what project should i choose 

andrew poon
poon andrew stanfordalumni org

abstract
this work analyzes the distribution of past cs    projects by applying hierarchical agglomerative clustering  the clusters reveal which topics are very popular
and which topics are more unique  tracking the clusters over time also provides
insight into how student projects have shifted over time  this knowledge will help
future students select interesting and unique projects 

 

introduction

choosing a project for cs    is difficult  students want to choose projects that are not only interesting  but are also unique from other projects chosen by current and past students  this work
attempts to address this problem by using text analysis and clustering to organize past projects  the
distribution of projects changes over time and provides insight into where student interests lie and
how the field is evolving  as a result  this work is able to identify popular topics worked on by many
students  and highlights the most unique projects that have been submitted over the years 

 

data set

the data set for this project comes from the archive of past cs    projects  these can be accessed at
the course website    the papers for each year have been collected  converted to text    and converted
into word frequency vectors  i e  a histogram of words  
   

document processing

converting project papers into useful word frequency vectors requires processing  standard processing techniques such as stop word removal     p       lower casing all words  and word stemming     p      were applied  by far  the most tedious part of this process was building a filter
list to remove contentless words  this problem was exacerbated by the fact that the texts were
cs    project papers with different vocabularies than standard english texts  for example  words
such as algorithm and learning would be relevant for ordinary topic modeling  but in this case 
every project applies some algorithm that attempts to learn something so these words do not indicate
anything particular about the project  words like this are considered contentless in this context
and were subsequently added to the stop word list 
after painstakingly removing as many irrelevant words as possible  each paper is converted into its
own word frequency vector  the resulting vector indicates the key topics of the paper  an example
of the top entries in a word frequency vector is shown in table    in this example  the paper    
investigates using neural networks to identify handwritten digits and training a robot to write those
digits  thus  is it not surprising that terms such as nn and digit appear frequently 
unfortunately  not every project paper was available for analysis  some papers  and all papers in
      were not available for download  additionally  some papers failed to convert from pdf to
 
 

http   cs    stanford edu
the unix utility pdftotext was used 

 

fitable    word frequency vector of a paper investigating digit recognition using neural networks 
nn
reconstruction

  
  

image
recognition

  
  

program
digit

  
  

motor
mnist

  
  

plain text  in the end       papers were converted to word frequency vectors to form a sufficient
data set 

 

clustering

hierarchical agglomerative clustering  hac      p       was used to cluster the projects  hac
starts by initializing each sample in its own cluster  on each iteration  the clusters that are most
similar are merged together  as the clustering iterates  the threshold for merging clusters becomes
more relaxed  the result is that the most similar clusters are merged first  less similar clusters are
merged later  and dissimilar clusters are left unmerged  this continues until the similarity threshold
drops below a certain value  or until sufficiently few clusters remain 
similarity between clusters was measured using cosine similarity     given by the expression
ab
  a    b  
where a and b are the two word frequency vectors  this essentially performs a vector dot product 
this approach is straightforward and provides a simple way to measure the similarity of two papers 
after two clusters are merged  the word frequency vectors are averaged and only the twenty most
frequent words are kept  this was done so that the new vector would only contain the most relevant
words  otherwise  the vocabulary of the word vector would grow after each merge  causing many
clusters to merge together quickly 
the standard k means clustering algorithm was also attempted  but did not perform well  the random initial clusters resulted in inconsistent final clusters  there were also cases with empty clusters
and cases with all samples in one huge cluster  after several attempts  k means clustering was
abandoned in favor of hac 

 

results

the clustering sequences for      and      are shown in figure    initially  each project is placed in
its own cluster  as the algorithm runs  the most similar clusters are merged first  these early clusters
are marked and keywords from those clusters are shown in table    as the algorithm continues to
run  the threshold for merging clusters relaxes and clusters with lower similarity are merged  this
results in a very large cluster which can be seen at the bottom of the graphs  this large cluster does
not have very distinctive keywords and is not meaningful  however  there are a handful of clusters
that remain separate from the large cluster  these remaining individual clusters can be interpreted
as the most unique papers 
the clustering sequences for each year follow the same pattern  similar clusters merge early on  but
eventually most clusters merge into a huge cluster while only a few unique clusters remain separate 
by analyzing the early clusters and the remaining unique clusters for each year  we can see how
popular topics and unique topics change over time 
clustering reveals several popular project areas such as  finance  robotics  music  image classification  text analysis  biology  sports  and movies  the cluster sizes over the years are shown in
figure    tracking these clusters over time shows some trends in student interest  for example  all
clusters  except for robotics  are generally growing over time  this is expected as the class size has
grown significantly over time 
in the case of robotics  student interest has shifted as more projects are attempting to control other
machines such as cars and rockets rather than the typical robotic arm  causing the keyword robot
 

fifigure    evolution of clusters 
table    keywords from early clusters 
grouping
   
   
   
   
   

    
keywords
clustering  mean  word  partition  node
image  feature  classify  skin 
object
note  music  component  instrument  microtiming
query  document  classify 
precision  default
reinforcement learning  light 
player  traffic  game

grouping
   
   
   
   
   

    
keywords
feature  josquin  note  music 
classify
tag  word  question  feature 
classify
stock  return  feature  price 
trading
game  team  season  prediction  play
click  query  search  rank  url

to decline in frequency  the cluster for movies fluctuates wildly during the early years  apparently 
the surge in popularity was due to the introduction of the netflix prize      in the years following a
glut of netflix projects  students were less interested in applying machine learning to movies  also
of interest is the finance cluster  surprisingly  there were no stock trading projects in       however 
there were two large peaks in      and       the first peak came after the housing bubble bust 
while the second peak is due to a sponsored project  investigating stock trading based on twitter
messages  finally  we can also see that image classification  plotted separately  is the most common
topic because of wide applicability in areas such as computer vision and medical imaging  oddly 
there was a drop in      in image classification projects  closer inspection shows that many projects
that year investigated astronomical applications such as detecting dark matter  causing the keyword
image to not appear as often 
at the other end of the spectrum  we also find many examples of unique projects by examining the
clusters that remain separate  some examples of unique projects include  ionospheric corruption
of radio waves  tracking vehicles using an autonomous helicopter  optimizing wind farms  and detecting arguments in online forums  ironically  this algorithm found a unique project     from     
that also analyzed past cs    projects in a similar fashion  this algorithm  while attempting to find
unique projects  discovered that it itself was not unique  this was truly a surprising result 

 

discussion

the clustering algorithm presented here is able to discover some general patterns in past projects 
however  the clusters are not very precise because they are based only on word frequency  the
biggest factor that could improve this system is to use better natural language processing  nlp  
 

supervised by mihai surdeanu and john bauer 

 

fifigure    topic trends over time 
the simple word frequency approach used here loses information that could be found in sequences
of words  n grams   named entity recognition  ner  could be used to identify topic keywords and
eliminate contentless words 
another problem encountered in this work is that project topics are becoming more broad as well 
for example  robot vision combines robotics and image classification  trading stock based on twitter messages combines finance and text analysis  these kinds of projects do not fit neatly into the
common topics and introduce a lot of ambiguity in the results  thus  a crude attempt at topic modeling was made by manually reading through past projects and labeling the paper with topic keywords 
this provided more precise labels for each project  but was too tedious to be scalable  with better
topic modeling  each topic would have a fingerprint of distinctive keywords which could be used for
more accurate clustering 

 

conclusion

this project performed unsupervised clustering on past cs    projects  this revealed several common topic areas and well as some unique projects  interest in the popular topic areas has also varied
with time and has been influenced by external factors such as sponsored projects  these results
provide guidance to future students by showing which topics have been very popular  and providing
examples of unique projects  this information should help students choose more varied and unique
projects in the future 

 

future work

this project has been described as very meta and ironically discovered itself to not be unique
when searching for unique projects  but why stop there  we must go deeper  in the future  more
projects can apply machine learning to past cs    projects  then  there could be a new project that
analyzes the other project analyzers 
regarding this system  the performance could be improved by incorporating better nlp and topic
modeling  this would lead to better clustering and could reveal finer details than the general trends
found in this work  the visualization of the clustering process also has much room for improvement 

references
    michael chang and ethan saeta 
analyzing cs     projects       
http   cs    stanford edu proj     changsaeta analyzingcs   projects pdf 
    john a  conley and my phuong le 
handwritten digit recognition 
investigation and improvement of the inferred motor program algorithm       
http   cs    stanford edu proj     conleyle handwrittendigitrecognition pdf 
 

fi    t  hastie  r  tibshirani  and j  friedman  the elements of statistical learning  data mining 
inference  and prediction  second edition  springer series in statistics  springer       
    christopher d  manning  prabhakar raghavan  and hinrich schutze  introduction to information retrieval  cambridge university press  new york  ny  usa       
    wikipedia  cosine similarity        http   en wikipedia org wiki cosine similarity 
    wikipedia  netflix prize        http   en wikipedia org wiki netflix prize 

 

fi