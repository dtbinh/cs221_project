cs     final  project  writeup  
  
localized  explicit  semantic  analysis  for  
concept based  information  retrieval  
  
francis  lewis  
glewis   cs stanford edu  
my goal is to  given a word  text fragment  or longer document  output a
set of conceptual ideas that best embody the meaning of the text as we humans
understand it  the utility of such a method is providing a reliable way to
incorporate knowledge of the world into semantic analysis of text  in doing so  we
can generate a set of concepts that relate to a text fragment  if these same
concepts could also be mapped to other forms of input  such as images or
sound  we would have a reliable and algorithmic approaches to linking related
forms of media together 
previous work has been done in learning an association between words
and high level concepts by estimating probabilities with text frequency occurring
in online encyclopedias  however  previous work has relied on either heavy
utilization of natural language processing techniques or processing entire corpora
of articles  my proposed method instead involves a localized approach  having
the algorithm estimate probabilities by trying to find and learn from articles
relevant to the query at hand  to test the efficacy of this approach  i train my
model on a selection of twitter data relating to two different hashtags  and
measure how well i can distinguish one set of training tweets from the other 
the data used to train the model involves parsing data from the wikimedia
api  which can be downloaded
at http   www mediawiki org wiki api main page  the data relevant to my model
is the title of a wikipedia article  the related plain text word tokens  and the links
to other wikipedia articles  before learning  i first parse the raw wikitext by
removing special tokens such as curly braces and brackets  normalize all words
to lower case  and stop and stem each token  each wikipedia article represents
some concept and has some associated words  by letting the model read lots of
articles and determine which words are most relevant to which articles  or which
words arent relevant at all   we can train a model to classify textual fragments
based on the strength of the text fragments correlation to a constellation of
wikipedia articles 
for testing purposes  i used     tweets from twitter
 https   twitter com    i chose this source for my testing data since tweets are

fiassigned natural categories based on a number of special tokens  hashtags  
because tweets can be modeled as a livestream of text data about a particular
current event  using tweets provides a good opportunity to test the discerning
power of localized esa against events and text too recent for the algorithm to
know anything about  my data was divided into two categories of    tweets
each  one for  raiders and one for  berkeleyprotests  since these two
categories were trending around the same time in approximately a similar
geographic location  they collectively provide a good challenge  i trained my
model with the first    tweets in each category  and then tested with the final   
in each category 
my number of features for learning concepts to words is high  since each
wikipedia article provides new textual data  individual word tokens  along with a
weight that represents how frequently a given word appears in an article 
comprise the features for learning wikipedia article titles  while the wikipedia
article titles themselves comprise the raw input features for the twitter training  i
experimented with feature selection in the realm of selecting the vector length of
wikipedia article titles that would give the best results on the twitter dataset 
though my current success has been mixed  it seems as of now that a length of  
is optimal 
i used primarily generative models in my project  to fit words to concepts 
i built an index of words and what articles those words belonged to  i then
learned the probability of an article given a particular word by finding the
probability of a word given an article weighted by the probability of a word
appearing across all articles  in this way  i was able to estimate a probability
model for what articles should be associated with what words  i modified this
technique by introducing a control parameter for increasing the number of articles
i scan for per word  in particular  the control parameter allows for a particular
depth to be searched  increasing the complexity of the model by allowing me to
learn probabilities for words given several articles  the current method im using
to allow the learning of significance is tfidf  for classifying twitter data  i fit a
basic nave bayes model to the concept vectors predicted by my esa algorithm 

fiin my results table  i have testing error listed along the y axis and the
different parameters i used to train my two models along the x axis  the level
deep represents the number of layers of articles i delved per example before
learning from them  the   vec represents the feature selection i used in
determining what size concept vector would give the best results in training and
testing twitter data 
  level
deep    vec

  level
deep    vec

  levels
deep    vec

  levels
deep    vec

   

  level
deep    vec
   

   

   

  levels
deep    vec
   

berkeley test
er 

   

raiders test
er 

    

   

    

    

    

    

i unfortunately have a very large amount of error  this error reaches a
minimum when i analyze only one level of articles and use a vector of size   to
classify twitter data  i originally hypothesized that i could minimize the error by
simply increasing the amount of text data i analyze as well as using larger
vectors with more concepts to reflect this increased learning  interestingly
enough  it didnt seem to have any more of an effect than using less article data
and smaller vectors  my suspicion is that since the most relevant information is
reflected closest to the data  i e  in the first level of article analysis and the first
few concepts in a concept vector   then adding both more article data and a
larger concept vector only introduces noise that distorts the prediction process  i
also noticed that localized esa almost completely failed on the raiders test set  i
believe its because the raiders set contains many more proper nouns than the
berkeley set  for which wikipedia may not have data and so i wouldnt have been
able to classify 
for the future  my first priority is to find a way to be able to process this
data quickly enough to allow even more experimenting and testing  i also would
like to experiment with using different probabilistic models for text frequency
analysis besides tfidf  in particular  i would like to replace the idf part with a
gaussian distribution by modeling the idf values for all words across all selected
articles as a gaussian  i would also like to obtain and test on more data from a
variety of sources  as i believe localized esa has the potential to work much
better on certain sets than others  as witnessed in the data sets above   finally 
wikipedia holds a wealth of other information such as edit places  edit times 
users  etc  i think finding novel ways to incorporate this data will make localized
esa a very useful semantic analysis tool in the future 
http   www aaai org papers ijcai      ijcai       pdf
http   snowball tartarus org 
http   www nltk org 
https   twitter com  lang en

fi