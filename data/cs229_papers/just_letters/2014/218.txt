neural network joint language model  an investigation and
an extension with global source context
ruizhongtai  charles  qi
department of electrical engineering  stanford university
rqi stanford edu

abstract
recent work has shown success in using a neural
network joint language model that jointly model
target language and its aligned source language
to improve machine translation performance  in
this project we first investigate a state of the art
joint language model by studying architectural
and parametric factors through experiments and
visualizations  we then propose an extension to
this model that incorporates global source context information  experiments show that the best
extension setting achieves      reduction of test
set perplexity on a french english data set   

 

introduction

the construction of language model has always been
an important topic in nlp  recently  language models
trained by neural networks  nnlm  have achieved
state of the art performance in a series of tasks like
sentiment analysis and machine translation  the key
idea of nnlms is to learn distributive representation of
words  aka  word embeddings  and use neural network
as a smooth prediction function  in a specific application
like translation  we can build a stronger nnlm by
incorporating information from source sentences  a
recent work from acl       devlin et al        
achieved a    bleu score boost by using both target
words and source words to train a neural network joint
model  nnjm  
in this project  we implement the original nnjm
and design experiments to understand the model  we
also extend the current nnjm with global context of
source sentences  based on the intuition that long range
dependency in source language is also an important
information source for modelling target language 
our contribution mainly lies in three aspects  first 
we present a deep dive into a state of the art joint
language model  and discuss the factors that influence
 

this project is advised by thang luong and it is a co project for
cs    n natural language processing with yuhao zhang 

the model with experimental results  second  we propose
a new approach that incorporates global source sentence
information into the original model  and present our experimental results on a french english parallel dataset 
third  as a side contribution  we have open sourced  our
implementation of both the two models  which could be
run on both cpu and gpu with no additional effort 
the rest of this report is organized as follows  we first
give a brief introduction on nnjm in section    then
in section   we present our extensions  we introduce
how we compute source sentence vectors and why we
make these design choices  we then spend more space
present our insights on nnjm gained from experiments 
and evaluation of our extended nnjm model in section
   we summarize related work in section   and conclude
in section   

 

neural network joint model

for statistical machine translation  we can augment target language model with extra information from source
sentence  an effective approach proposed previous work
is to extend traditional nnlms by concatenating a context window of source words into the input and train
word vectors for both source and target languages  in
section   we will also describe another extension of
nnlm of using source sentence vector as an extra source
of information 
   

model description

we use a similar model as the original neural network
joint model  to be concrete  we provide mathematical
formulation for the model together with a model illustration in figure    for more details please refer to the
original bbn paper  devlin et al         
one sample input to the model is a concatenated list of
words composed of both target context words  n   history words for n gram  ti and source context words si  
source words are selected by looking at which source
word target word ti is aligned with  say its sai   then we
take a context window of source words surrounding this
 

http   goo gl wizzcf

fip school   he  walks  to 

 

 

 

    s     s  

neural network joint model with global
source context

output
softmax layer

in this section  we show our attempts in pushing the
state of the art of nnjm by utilizing global source
context  global source sentence information  
for
simplicity  we will use nnjm global to refer to this
extension in the following sections 

hidden layer

word vectors
input

he

walks

to
walks

to

school

  s 

  s 

  s 

  s 

   
figure    neural network joint model with an example
 illustrated with chinese english  where we use   gram
target words    words history  and source context window size of    we want to predict the next word following he  walks  to and hopefully estimated probability of
the next word being school would be high 
aligned source word  when the window width is
we have m source words in the input 

m 
   

p ti   ti   si  
ti   ti         tin  
si   sai  m         sai        sai   m 
 

 

here we regard ti as output  i e  y  r as one of the target words  and concatenation of ti and si as input  i e 
x  rn m  of n    target words and m source words 
the mathematical relation between input and output is as
follows  where     l  w  b      u  b       linear embedding layer l  rd vsrc  vtgt   which converts words to
word vectors by lookup  where d is word vector dimension  in hidden layer  w  rh d n m      b     rh  
in softmax layer  u  rvtgt h   b     rvtgt and

weighted sum source sentence

our first attempt is to include sentence vector directly
into the input layer of the neural network  we calculate weighted sum of word vectors in source sentence 
and feed the result into our input layer  as shown in figure    specifically  we experimented with two different
approaches 
   uniform weights we assign each word a uniform
weight in the source sentence  in another word  we
take the mean of all the word vectors to form the
global context vector 
   zero weights for stop words instead of giving all
words the same weight  we identify top n frequent
words in the vocabulary as stop words  and assign
each of them with a zero weight  for all the rest
words in the vocabulary  we still assign them with
a uniform weight  the intuition is depress possible
noises introduced from those irrelevant words 
p school   he  walks  to 

 

 

    s     s     s 



  s   

output
softmax layer

hidden layer

weighted sum
word vectors

exp vi  
gi  v    pvtgt
k   exp vk  

input

optimization objective is to maximize the loglikelihood of the model 
m
x

log p y  i    x i      

i  

   

walks

to
walks

p y   i   x      gi  u f  w l x    b        b     

     

he

evaluation metric

we use perplexity as the metric to evaluate quality of a
language model 
 

p p  w     p w    w         wn   n

to

school

  s 

  s 

 s 

  s 

  s 

 s 

  s 
he

every morning walks

to

school

  s 

figure    an example for the nnjm with global context  where an additional source sentence is fed into the
model 
   

splitting source sentence into sections

in order to increase expressibility of sentence vectors  we
propose to split the source sentence into sections before
taking weighted sum  as shown in figure    we treat the
number of sections as a hyper parameter for this model 
specifically  we experimented with two variants of this
approach 
   fixed section length splitting the sentence vector
is first extended with end of sentence tokens so that
all the input source sentences are of the same length 

fithen the splitting is done on the populated source
sentences 
   adaptive section length splitting we use the original source sentence instead of extending all sentence vectors into a uniform length  thus  each section will have a variable length dependent on the
length of the entire sentence 


on training set  each batch contains     input samples 
each of which is a sequence of target words plus source
context words  there are around   k mini batches per
epoch  model parameters are randomly initialized in the
range of                we use early stopping to pick the
model with least validation set perplexity  at the end
of every epoch we do a validation set test and see if the
validation set perplexity becomes worse from last time 
if it is worse we halve the learning rate 

hidden layer

splitting
word vectors
source sentence

 s 
 s 

  s 
he

every morning walks

to

school

  s 

figure    an example of splitting source sentence into
  sections before calculating the global context vectors 
weighted sum is calculated on the first half of the sentence to form the first global context vector  and then on
the second half 
   

global only non linear layer

we can also add non linearity to the model by adding
another global only non linear layer between the global
linear layer and the downstream hidden layer  as it is
illustrated in figure    note that this non linear layer
is only added for the global part of the model  and has
no effect on the local part  we use the same non linear
function for this layer as in other layers of the model 

hidden layer

splitting
word vectors
 s 
 s 

  s 
he

every morning walks

to

school

  s 

figure    an example for the non linearity on the global
source sentence  a weighted sum is calculated to form
the intermediate global context vectors  yellow   and
then these intermediate vectors are fed into a global only
non linear layer 

 

both training and testing are implemented using
python  we use theano library for neural network modeling  the training runs on a single tesla gpu  training
speed is around       samples second and training one
epoch of data        k  takes around half an hour  for
reference  total training time for a basic nnjm model
over the entire corpus is around     hours when the full
gpu power is utilized 

 

experimental results

   

model training

following a similar strategy with bbn paper  we use
mini batch gradient descent to maximize log likelihood

nnjm

in this subsection  we focus on showing our understanding of the joint language model  more evaluation results
will be combined with nnjm global model in subsection     
     

non linear layer

source sentence

the data set we use is from european parallel corpus  our training set contains    k pairs of parallel
french english sentences  validation and test set each
contains  k pairs of french english sentences  for
nnjm model analyzing  we use the entire data set 
however  due to limit of time  we also use a   k subset
of the data for some of the experiments such as nnjm
and nnjm global comparisons 

effects of hyperarameters

tuning is on the entire    k training set  since a full
grid search is too time consuming we will start from a
default hyper parameter setting and change one of them
each time  in default  learning rate is      target n gram
size is    source window width is       source words  
vocab size is   k for both target and source language 
epoch number is    word vector size is    and there is
one hidden layer of     units 
as shown in figure    hyper parameters can have a big
difference on model performance  generally it helps to
increase word vector dimensions and hidden layer size 
source window width   or   is good and target n gram
size of   is optimal in our default setting 
for learning rate  we train the model until convergence
 around    epochs yet the decrease of valid set perplexity
is marginal after   to    epochs   we observe that while

fivalidation set perplexity

validation set perplexity

   
   
   
   
 
   

 

  

   

   

   

 

   

   
   
   
   
 
   

 

 

  
  
  
  
  
 

 

  

   

   

   

   

 

 

 

  

source window width
validation set perplexity

validation set perplexity

word vector dimension

   

   
   
   
   
 
   

 

 

hidden layer size

 

 

 

 

 

nnjm global

in this subsection we demonstrate experimental results
for each variant of the nnjm global model  and compare their results with the vanilla nnjm model  note
that all the models in this part are trained with the same
strategy described in previous section  by default  we
use a vocabulary size of   k  a source window size of   
a target n gram size of    an embedding dimension of    
a hidden layer size of      and a learning rate of     to
train the models 

target ngram size

     
figure    effects of hyper parameters
very large learning rate such as     and     leads to quick
convergence yet tend to rest at unsatisfactory local minimums  very small learning rate such as      converges
too slow  we think lr     achieves balance between convergence speed and stability 
     

visualizations and insights

in this subsection we use network parameter visualization to show how the neural network take advantage of
source context  specifically  we look at the linear transformation matrix w in the hidden layer  which can be
thought as a way to measure how much certain part of
input contribute to predicting the next word  in figure     we observe that the center source word  i e  the
one aligned with the next target word  contributes most
to the prediction  even more than the last history target
word  index      there is a trend of attenuating importance for source words far from the middle one 
  
  
  

comparing nnjm global with nnjm

the resulting perplexity achieved by different models
on the test set is shown in table    note that we also
include the result for a basic neural network language
model  nnlm  where only target words are utilized for
making predictions  to demonstrate the effect of global
source context information  it is observed that for each
setting of source window size  the nnjm global model
achieves smaller  better  test set complexity compared
to its corresponding nnjm model  the best performance is achieved when the source window size    under
this setting  a slightly better result is achieved when we
use a zero weights for stop words weighted sum strategy  there is no noticeable difference between the different settings of number of stop words in the nnjm global
model 
model

srcwin

perplexity

nnlm
nnlm global

 

     
     

nnjm
nnjm global

 
 

     
     

nnjm
nnjm global

 
 

    
    

nnjm
nnjm global

 
 

    
    

nnjm global   sw   
nnjm global   sw   

 
 

    
    

  
   
   
   

   

   

   

    

    

average hidden layer weight

    
   

target

source

    
   
    
   
    
   
    

 

 

 

 

 

  

  

  

word index

figure    top  heat map of absolute values of hidden
layer matrix w  rh d n m      bottom  average of
w s elements corresponding to each input words  words
  to    are from source with word   as the center one 
words    to    are history words in the target n gram 

table    test set perplexity for different models  srcwin represents the source window size that is used in
the model  sw n represents that n most frequent stop
words are removed from the global sentence vector  results for the nnlm model where only target words are
used for prediction are also included 
     

effect of splitting source sentence

both the two approaches for splitting the global source
sentence vectors are evaluated and compared to the basic nnjm and nnjm global models  the results are

fishown in table    the fixed section length splitting strategy with section number of   gives reduction of the test
set perplexity when compared to the basic nnjm global
model  while the adaptive section length splitting strategy gives almost the same result as the basic nnjmglobal model  and also achieves better result compared
to the original nnjm model  the performance is observed to deteriorate when the section number increases 
model

numsec

perplexity

nnjm
nnjm global

 

    
    

nnjm global   fixsplit
nnjm global   fixsplit
nnjm global   adasplit

 
 
 

    
    
    

table    test set perplexity for models with different
global context vector section numbers  numsec is for
section number in for global context vector  fixsplit denotes the model with fixed section length and adasplit
denotes the model with adaptive section length 

     

effect of global only non linear layer

generally  adding a non linear layer could add expression power to the neural network  table   shows the effect of adding non linear layer for generating sentence
vector under various settings  the best perplexity is
achieved with fixsplit of   sections and non linear size
     which is      lower than the basic nnjm model 
one possible explanation for the improvement is that
with the help of section splitting the non linear model
can gain additional expressive power from this combination of architecture settings that is not possible without
splitting or non liearn layer 

 

related work

while bbns work  devlin et al         on neural network join model focused on efficiency and mt result presentation  we investigate deep into the original nnjm by
study on hyper parameters and visualization of hidden
layer weights  we also extend the model with global
source context and achieves improvement in terms of
perplexity scores  our project have taken similar strategy in generating sentence vector with previous work on
using sentence vector to learning word embeddings with
multiple representations per word  huang et al         
however  we have also developed more complex models
and focus more on designing good architecture to improve joint language model quality 

model

ns

nlsize

perp

nnjm
nnjm global
nnjm global   nl
nnjm global   nl

 
 
 

  
   

    
    
    
    

nnjm global   fixsplit
nnjm global   fixsplit   nl
nnjm global   fixsplit   nl

 
 
 

  
   

    
    
    

nnjm global   adasplit
nnjm global   adasplit   nl
nnjm global   adasplit   nl

 
 
 

  
   

    
    
    

table    test set perlexity for models with global only
non linear layers  results for models with no global vector splitting  with fixed section length splitting  and with
adaptable section length splitting are shown  nl is for
non linear layer in the global part  nlsize represents the
size of the global only non linear layer 

 

conclusion

in this report we present our work in investigating a neural network joint language model and extending it with
global source context  our experimental analysis demonstrates how hyperparameters influence performance of
the model  we also use visualization of network weights
to show how source words influence prediction  furthermore  we have shown that incorporating global source
context  sentence vector  can further improve the performance of the language model in terms of perplexity  finally  we have open sourced our implementation of both
the original model and the extended model 

acknowledgements
the author sincerely acknowledge thang luong in stanford nlp group for his advising and thank yuhao zhang
for his collaboration and thank cs     staff for bringing
us such a rewarding class 

references
 devlin et al       jacob devlin  rabih zbib  zhongqiang
huang  thomas lamar  richard schwartz  and john
makhoul        fast and robust neural network joint
models for statistical machine translation  in   nd annual
meeting of the association for computational linguistics 
baltimore  md  usa  june 
 huang et al       eric h huang  richard socher  christopher d manning  and andrew y ng        improving word representations via global context and multiple
word prototypes  in proceedings of the   th annual meeting of the association for computational linguistics  long
papers volume    pages         association for computational linguistics 

fi