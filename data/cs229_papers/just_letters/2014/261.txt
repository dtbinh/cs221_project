information based feature selection
farzan farnia  abbas kazerouni  afshin babveyh
email   farnia abbask afshinb  stanford edu

 

introduction

feature selection is a topic of great interest in applications dealing with high dimensional datasets 
these applications include gene expression array analysis  combinatorial chemistry and text processing of online documents  using feature selection brings about several advantages  first  it leads
to lower computational cost and time  less memory is needed to store the data and less processing power is needed  feature selection helps improve the performance of the predictors by avoiding
overfitting  it can also capture the underlying connection between the data  and perhaps the most
important aspect  it can break through the barrier of high dimensionality 
to select the most relevant subset of features  we need a mathematical tool to measure dependence
among random variables  in this work  we use the concept of mutual information  mutual information
is a well known dependence measure in information theory  for any arbitrary pair of discrete random
variables  x  x and y  y  mutual information is defined as
i x  y    

x

px y  x  y  log

xx   yy

px y  x  y 
 
px  x  py  y 

   

the paper is organized as follows  in section   the method of maximum relevance minimumredundancy  mrmr  is presented along with maximum joint relevant  mjr  method  in section
   we present our method to solve the feature selection problem  section   presents the result of our
algorithm tested on madelon dataset  finally  section   discusses the conclusion 

 

mutual information as a tool for feature selection

as discussed earlier  mutual information is a powerful tool in measuring relevance among random
variables  hence  it can be a useful mathematical tool to find and select relevant features  in other
words  if our goal is to select no more than k features an optimal task is to solve
arg max i xs   y   
 s  k

   

where xs    xi   i  s   however  as k gets larger our estimation of mutual information becomes
less accurate  it is because for large ks we do not have enough samples to estimate mutual information
accurately  hence  the objective function in     should be modified so that it becomes estimable by
available samples  in the next sections  we first discuss a past approach to solve this issue and then
propose a new solution to improve such approaches 

 

fi   

max relevance min redundancy  mrmr  approach

as mentioned earlier  we aim to identify the most relevant subset of features whose size is limited
to a given factor  note that this is not the same as characterizing the k best features with the
most individual mutual information to the target y   in fact  different features may share redundant
information on the target  thus  redundancy is another important factor to be considered in feature selection  to balance the trade off between relevance and redundancy  the following modified
objective function  mrmr  has been suggested in     
 xs   y    

  x
  x
i xi   y   
i xi   xj   
 s  is
 s   i js

   

here  the first term measures the average relevance of features to the target  while the second term
measures average pairwise redundancy among selected features  therefore  maximizing  xs   y  
leads to identifying a well characterizing feature subset whose total information on the target is close
to the optimal feature subsets  to maximize this objective  they used an inductive approach where
first the most informative feature is chosen  and then next features are inductively added by solving
the following at every step 
arg max

xj x sm

i xj   y   

x
 
i xj   xi   
m    x s
i

   

   

m

maximum joint relevance

although mrmr is a well known feature selection method  there are several applications where the
test error rate never goes below some large thresholds like     which seems quite unsatisfactory 
note that     includes only up to pairwise interactions  by considering higher order interactions we
can become able to select a more informative feature subset which in turn results in smaller error
rates  to this end  maximum joint relevant  mjr  algorithm changes the inductive rule of     to a
more sensitive one     
x
arg max
i xj   xi   y   
   
xj x sm

xi sm

nevertheless  we may again encounter the issue of lack of enough samples to estimate the second
order mutual information appeared in the above formulation  as a matter of fact  a considerable
number of third order empirical marginals may become too small  and thus it requires a more accurate
estimation of mutual information than the empirical one  therefore  in next section we are going
to propose a new algorithm to estimate mutual information with higher accuracy  as an important
advantage  this estimation technique reduces the required sample size to estimate mutual information
within the same accuracy 

 

adaptive maximum joint relevant

in this section  we propose the adaptive maximum joint relevant  amjr  feature selection algorithm to tackle the instability problem in mjr  similar to mjr  we use the criterion in     to
iteratively select the most relevant features  however  we propose a new scheme to estimate the
mutual informations which stabilize the algorithm in small training set regimes  we build our estimation technique based on functional estimation method proposed in      specifically  in order to

 

fiestimate i xj   xi   y   at each step  we have to estimate the joint entropies according to the following
identity 
i xj   xi   y     h xj   xi     h y    h xj   xi   y   
   
in order to describe the estimation method in amjr  consider for example  estimating h xj   xi   
following from      first the empirical joint distribution of  xj   xi   is computed according to
n

pa b

 x
   xj   xi   t     a  b   
 
n t  

   

where n is the size of training set and  xj   xi   t  is the joint value of tth training example  note that
a and b are assumed to take value in some finte sets a and b  respectively  now  assuming that pa b
is the true joint probability of  xj   xi   at point  a  b   the true joint entropy would be
x
h xj   xi     
pa b log pa b  
   
aa  bb

in order to provide the estimator h xj   xi   of h xj   xi    one naive way is substitute each pa b in
    with its estimate pa b   this method which is used in mjr  is in fact the source of instability on
the performance since most of the estimated probabilities are very small  in amjr  we consider two
cases for the estimated joint probabilities 
 if pa b 

log n
 
n

we use it as an estimation of pa b in     

 if pa b   logn n   first we fit a polynomial f of order blog nc to the function x log x in the interval
    logn n    then  we use f  pa b   as an estimation for pa b log pa b in     
as we see in section    the approximation polynomial f introduces stability to the algorithm and
improves its performance  consequently  the estimation of h xj   xi   in amjr would be
 
x
x
h xj   xi     
pa b log pa b  
f  pa b    
   
n
pa b  log
n

n
pa b   log
n

similarly  the estimations h xj   xi   y   and h y   are provided for h xj   xi   y   and h y   
respectively  finally  the mutual information is estimated as
 j   xi   y     h xj   xi     h y    h xj   xi   y   
i x

 

    

numerical results

in this section we provide numerical results to confirm our theoretical analysis  we perform different
feature selection and classification methods on the dataset madelon released in nips      feature
selection challenge      this data set consists of      samples each containing     continuous input
features and one binary output response  here we have used      samples       as the training set
and used the other     samples       as the test set 
in order to explore the effect of sample size on different feature selection methods  we quantize the
input space into   and   levels  uniformly  thus  we have two scenarios  in the first one  the input
features are quantized separately into three levels which corresponds to the large training set regime
 

fi   
mrmr
mjr
    

classification error rate

   

    

   

    

   

 

  

  

  

  selected features

figure    svm classification error for   level quantization of input space 

 since each level happens too many times and we have small number of probabilities to estimate  
in the second scenario  the input features are quantized separately into   levels  the later scenario
corresponds to a small training set regime where there are a large number of probabilities to estimate 
figure   compares the misclassification error of mrmr and mjr feature selection algorithms for
different number of features  here  svm is used as the classification method and the input space
is quantized into   levels  since this scenario corresponds to large training set regime  the mjr
outperforms mrmr as depicted in the figure 
in fig     the svm misclassification error of mjr and amjr has been compared for different
number of selected features  here  the input space is quantized into   level which corresponds to the
small training set scenario  as depicted in this figure  mjr has unstable performance in this scenario
while amjr shows stable and better performance  this figure confirms our theoretical analysis of
instability of mjr and shows that our proposed method  amjr  removes the instability problem
almost completely 
the advantage of the proposed method amjr method is further described in fig     in this
figure  the svm misclassification error of amjr and mrmr methods are compared for differnt
number of selected features  here  the input space is quantized into   levels  small training set
regime   as depicted in this figure  amjr substantially outperforms mrmr for any number of

   
amjr
mjr
    

classification error rate

   

    

   

    

   

    

   
 

 

 

  

  

  

  

  

  

  

  selected features

figure    svm classification error for   level quantization of input space 

 

fi   
amjr
mrmr
    

classification error

   

    

   

    

   

    

   
 

  

  

  

  selected features

figure    svm classification error for   level quantization of input space 

selected features 
it worth mentioning that other than svm  we have also repeated the above experiments for
logistic regression and classification trees and the same relative results were obtained  since our
focus is on comparing the feature selection algorithms  and not the classification methods   and also
due to the lack of space  the results for these methods are not provided here 

 

conclusion

feature selection is an indispensable part of solution when dealing with high dimensional datasets 
one powerful tool to address this problem is mutual information  a common approach is to use maximum relevance minimum redundancy  mrmr  approach to solve the feature selection problem  in
this paper  based on insight from information theory  a new objective function is used  also  a novel
mutual information estimator is used enabling us to discretize the data into finer levels  combining
the novel mutual information estimator with the new objective function  an error rate   times lower
than that of mrmr is demonstrated 

references
    t  cover  and j  thomas  elements of information theory  john wiley   sons       
    h  peng  h  long  and c  ding  feature selection based on mutual information criteria of maxdependency  max relevance  and min redundancy  pattern analysis and machine intelligence 
ieee transactions on                       
    h  yang  and j  moody  data visualization and feature selection  new algorithms for nongaussian data  nips       
    j  jiao  k  venkat  y  han  t  weissman  minimax estimation of functionals of discrete distributions  available on arxiv       
    available online  http   www nipsfsc ecs soton ac uk datasets

 

fi