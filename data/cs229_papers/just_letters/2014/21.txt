probabilistic driving models and lane change prediction
tim wheeler
cs    fall     
abstract
probabilistic traffic models  providing a statistical representation of the future behavior of traffic
participants  are crucial for risk estimation in automotive collision avoidance systems  current research
has focused on large scale behavior  primarily in the form of lane change prediction  these models are
limited in their use for high fidelity driving propagation  this paper investigates a methodology for
dynamic model construction based on a bayesian statistical framework successfully employed in aviation
collision avoidance systems  machine learning techniques are also used to develop a lane change predictor 

 

introduction

as the automotive industry moves towards autonomous driving  it becomes increasingly necessary to develop
advanced collision avoidance systems  crash prediction systems  and the tools for their rigorous analysis 
the certification of any automated driving system will require a combination of driving tests and detailed
simulation studies to ensure system effectiveness and safety  driving tests are inherently expensive and timeintensive  simulation can be used to quickly and inexpensively test over the space of potential trajectories 
recent developments in collision avoidance in civil aviation have allowed for the creation of rich encounter
models based on a bayesian statistical framework from which optimal collision avoidance strategies have
been derived     these encounter models used dynamic bayesian networks for the correlated propagation of
aircraft in a close encounter 
the purpose of this project is to work towards the development of such statistical models for highway driving
and to apply machine learning techniques to develop a lane change predictor 
all work for this project was conducted with the julia programming language    

 

probabilistic traffic models

the driving prediction problem can be formalized as a joint probability distribution p  st     st   over the
future scene given the current scene configuation  where a scene s incorporates current and past information
necessary to leverage the markov assumption  this work focuses on obtaining a model of this probabilist
distribution which best fits the existing training data and performs well in simulation  prior work in collision
avoidance for aviation leveraged dynamic bayesian networks to model this state transition probability     
a dynamic bayesian network  dbn  is a bayesian network split into time slices  where indicators from time
t are used to condition a distribution over target values in t      using a dbn to directly model p  st     st  
is difficult due to the high variability in traffic structure and scene composition  one way to develop a model
applicable accross varying traffic structures and participant counts is to model the distribution over individual
vehicles actions  p  ait   st   where the individual actions can be used to deterministically propagate the scene
forward 
vehicle pose and kinematics are modelled assuming a rigid body in the euclidean plane  control is exerted
through the pedals and the steering wheel  thus  the target values for the dbn were chosen to be the
constant acceleration and turn rate over the next time step  afut and fut   deterministic propagation of a
particular vehicles state given these values is conducted using euler integration 
f 

f 

   

fut

afut

fk

figure    probabilist dynamics model in the form of a dynamic bayesian network
 

fimodelling the distribution requires solving several issues simultaneously  the set of relevant features  their
respective discretizations  the model structure  and the conditional probability tables must all be learned
from a limited set of driving data 

 

data processing

this work uses data generated from interstate      i      in the san francisco bay area of northern
california  i     presents a clean driving environment  lacking complicated merges and splits between palo
alto and south san francisco  and in particular always has four lanes in each direction for the sections used 
group

feature

units

description

global

t

s

ego

pg
x
pg
y
b
vx
vyb
g

m
m
m s 
m s 
rad

northing in the global coordinate frame
easting in the global coordinate frame
longitudinal velocity in the body frame
lateral velocity in the body frame
vehicle heading in the global frame

other

id
pb
x
pb
y

m
m
m s 
m s 

identification number  unique across frames
longitudinal relative position from the ego car
lateral relative position from the ego car
longitudinal relative velocity from the ego car
lateral relative velocity from the ego car

b
vx oth
b
vy oth

timestamp  unix epoch

table    a summary of raw data collected from drives
raw features were obtained from drive log files and are summarized in table    data had already been
resampled to    hz  high accuracy lane curves were used to project the given data to a road relative frenet
frame 
a total of thirteen hours of driving data spread over twenty one drive files were available for processing  split
roughly evenly between north  and south bound  data outliers were removed using a gaussian threshold test
and then smoothed using a gaussian kernel  resulting poses were projected to the lane relative frenet frame
with the use of lane centerlines  this frame has an x axis along the lane in the direction of travel and a y axis
perpendicular to the lane  positive towards the left  the resulting lane relative positions and velocities  pf
x 
f
f
f
pf
y      vx   and vy   were smoothed again to reduce noise from the projection process 
a set of     indicator features were extracted for the ego vehicle  features included inherent vehicle properties  relative features between vehicles  roadway relative features  and aggregated features over a vehicles
history  features were reflective of those used in the literature         for a complete list of features see table
  in the appendix 
due to limitations with the current sensing setup  data on other vehicles is not reliable figure    vehicle
positions are estimated by fitting to point clouds  resulting in problems with larger vehicles  occlusion  and
doubling up when a new section of the car becomes visible  this  coupled with the natural limitation of not
being able to see the entire environment  makes it difficult to create a predictor based on other vehicles 

figure    a projection to the frenet frame showing vehicle overlap  blue indicates the ego vehicle
instead  learning methods were applied to the ego vehicle instead  a more detailed analysis of other car
motion will be withheld until higher quality data can be obtained  
the problem was chosen to be modelled using discretized probability distributions  simplifying the inclusion
  information

from other vehicles is still leveraged  such as when computing features like the distance to the car in front

 

fiof hybrid variables and allowing for standard structure learning packages to be used  for the purposes of
this project discretization was conducted with manually chosen bin edges  typically five bins per variable 

 

feature selection for probabilistic traffic models

one primary objective for this project is to identify a set of features to be used in a probabilistic traffic
model  formally  one must identify the set of features which maximize the likelihood of the graph given the
data  p  g   d   this is equivalent to maximizing the log likelihood  also known as the bayesian score 

ln p  g   d    ln p  g   

n x
x
q
i j  

 



ln

i  

 ij   
 ij    mij   


 

ri
x
k  


ln

 ijk   mijk  
 ijk

 

where there are n random variables xi n   ri is the number of bins for xi   qi is the number of parental
instantiations of xi   mijk is the number of times xi   k given the jth parentalp
instantiation in thep
dataset d 
ri
ri
we use ijk for a dirichlet prior  same idea as laplace smoothing  and ij    k  
ijk   mij    k  
mijk  
for general graphs and input data  learning the structure is np hard  we make an independence assumption
between the two target variables to split the problem into two smaller structure learning problems 
f 

   

g 

fk

fut

   

gk

afut

solving each reduced problem is still difficult  there are  nindicators        possible graphs  one binary choice
on whether to include a particular edge for each indicator  note that the actual graph will likely only have a
small number of parents  due to a built in limitation on needing enough data to properfy specify
p  such a large
conditional probability distribution  even if we restrict ourselves to eight parents there are i      
   
i
trillion options 
three heuristic feature search methods were used  feature ranking  forward search  and graph search  each
seeks to maximize the log bayesian score on a reduced problem 
feature ranking is the simplest of the three  here the bayesian score is computed for each indicator target
pair  and the highest ranked indicators are successively added until the bayesian score ceases to increase 
algorithm   feature ranking
r  features sorted by indicator target bayesian component score
f 
while score f   r f         score f  do
f  f   r f     
end while
forward search starts with an empty feature graph and continuously adds the next feature which results in
the greatest increase in the bayesian score  the algorithm terminates when there is no feature which can
further increase the score  forward search can get stuck in local optima but in practice outperforms feature
ranking 
algorithm   forward search
i  set of all indicators
f 
while arg maxf i score f   f      score f  do
f  f   f  
i  i    f  
end while

 

figraph search is similar to forward search but allows for the removal of a feature  it also terminates when no
action will result in a higher score  graph search has greater freedom than forward search in traversing the
space but can also get stuck in local maxima  it tends to perform the best 
algorithm   graph search
i  set of all indicators
f 
while true do
scoreadd   maxf i score f   f   
scoredelete   maxf f score f    f   
if scoreadd   scoredelete and scoreadd   score f  then
f   arg maxf i score f   f   
f  f   f  
i  i    f  
else if scoreadd   scoredelete and scoreadd   scoredelete   score f  then
f   arg maxf f score f    f   
f  f    f  
i  i   f  
else
break
end if
end while
these algorithms were run on two separate feature sets  the full indicator set including all     features and a
reduced indicator set lacking the template features  totalling     a runtime comparison is given in table     
four target values were used  fut and afut each for a quarter second and half second horizon  the results
for fut
   ms are given below  see the rest in the appendix 
   features

score

indicators

feature ranking
forward search
graph search

      
      
      

  vyf     af
y
  vyf   dcl
  vyf   dcl

table    feature selection for fut
   ms with    features

    features

score

feature ranking

      

forward search

      

graph search

      

indicators

  af
y      ms  

f
  vy      ms  

  vyf      ms  



   ms   af
y    ms      ms
af
y    ms

   ms

table    feature selection for fut
   ms with     features
it was necessary to decimate the training data by a factor of two in order to get decent results  using the full
dataset resulted in over fitting and massive computation times due to the large number of features selected 
recall that the dataset is not iid  but sampled at   hz from continuous trajectories  so tossing every other
sample does not result in much loss of information and greatly reduces data redundancy 
these results are promising  the smaller feature sets contain values which can be extracted from a small
state space  they do not rely on a long history  and are thus candidates for online motion planning  the
larger feature sets can be used for higher fidelity models in the validation of safety systems 

 

fi 

maximum likelihood structure learning

the performance of three general structure learning algorithms were compared on a variety of indicator
feature sets  two of these algorithms used the smile jl package    the third  a greedy hill climbing method
using the k  prior  was implemented based on a method directly in julia   
algorithm

k  score

bdeu score

fut
deg      ms
 

deg   afut
   ms  

run time

bayesian search
greedy thick thinning
k  hill climbing

       
       
       

       
       
       

 
 
 

 
 
 

    s
    s
    s

table    algorithm comparison for full structure search with eight indicators

algorithm
bayesian search
greedy thick thinning
k  hill climbing

k  score

bdeu score

fut
deg      ms
 

deg   afut
   ms  

run time

         
         
         

         
         
         

 
 
 

 
 
 

    s
    s
    s

table    algorithm comparison for constrained structure search with eight indicators
f
tables   and   show results using the    ms targets and eight indicators      vyf   af
x   ay    v   dcl  
and ttcrmr   the first is a full structure search in which tiering was enforced  edges from target variables
to indicator variables were forbidden  the second table has the additional constraint of preventing edges
between indicator variables as they are going to be observed in the final dynamic model 

we observe that the k  hill climbing algorithm consistently results in the highest k  score  which it
is designed to maximize  the bayesian search method sometimes outperforms it in terms of the bdeu
score  which it is designed to maximize  greedy thick thinning is the fastest algorithm and produces the
lowest scoring graphs 

vyf
ttcrymr
dcl
af
y
kvk
af
x


fut
   ms

afut
   ms

figure    resulting graph structure from k  hill climbing under full search
  written
  method

by the author to provide access to the smile c   library   
adapted from code written by edward schmerling

 

fikvk

af
x

ttcrymr

af
y



afut
   ms

vyf



dcl

fut
   ms

figure    resulting graph structure from k  hill climbing under constrained search
the corresponding graph structures for k  hill climbing are given in figures   and    note that the parents
of the target values remain unchanged  the advantage of the second approach is the drastically reduced
search space  the resulting model is sufficient for modelling the posterior distribution when indicators are
observed but cannot be used for reverse inference 
model validation and tuning was the focus of work in an aa    project and is subject to further research 

 

predicting lane changes

given a traffic scene at time t  predict whether a target vehicle will be in the same lane  in a lane to the left
of its current lane  or in a lane to the right of its current lane at a time t   h for some horizon h on the
order of several seconds 
most methods in the literature focus on lane change detection for the ego vehicle due to the same limitations
of poor sensor measurements and incomplete scene information  the ego vehicle has very good knowledge
of its own dynamics and fairly good knowledge of its immediate surroundings  for other vehicles this is not
the case  for instance the ego vehicle may not know whether the car in front of itself has another car in front
of it which affects its behavior 
it was common in previous work to include features such as driver eye tracking to predict lane changes  note
that there was no human driver in this case  and other features such as blinker status is not available  only
the original position and velocitiy estimates  coupled with the lane curves  were used to derive the feature
set 
let us consider applying a naive bayes classifier  the naive bayes classifier is well defined for discrete and
continuous features  but it is not clear how to handle more complicated hybrid features  consider drel
x f o   the
distance to the car in front  this feature is typically continuous  but when there is no car in front it takes on
rel
  similarly  features like vx f
o   the velocity of the car in front  is typically continuous but is simply missing
if no vehicle is seen 

   

baseline algorithm comparison

we can establish a lower bound on performance using only well behaved indicators  table     was computed
using sci kit learn from julia via pycall  decimation by a factor of    was employed to avoid the problem
of non iid samples  there were a total of     samples with a lane change in a two second horizon  out of a
total of        samples  results are averages over    fold cross validation 
classifier
rbf svm
nearest neighbors
adaboost
random forest
decision tree
linear svm
naive bayes

auc

precision

accuracy

      
      
      
      
      
      
      

      
      
      
      
      
      
      

      
      
      
      
      
      
      

parameters
default
nearest  
default
   estimators  max depth    
max depth    
c        
gaussian

table    rough sci kit learn algorithm comparison on well behaved indicators
we see that naive bayes paradoxially has the highest auc metric score but by far the lowest precision  it
appears to be primarily predicting lane holds  which as we can see does fairly well in terms of auc but
poorly in terms of precision 
the highest precision was obtained by an svm with an rbf  gaussian  kernel  this prompted further efforts
to find a high qualitfy classifier using a larger feature set  all     indicator features were used to train a
 

fibetter rbf svm classifier  features with undefined behavior were arbitrarily set to their maximum values
when undefined values were present 

   

tuned rbf svm classifier

recall that the radius basis kernel function is given by 
krbf  x  x      exp kx  x  k 



given training vectors xi n  rp in two classes  and a truth vector y  rn such that yi          svm for
classification  svc  solves the primal problem    
pn
minw b     wt w   c i   i
subject to yi  wt  x    b      i
i   

 i              n

the resulting dual formulation is 
  t
   q

min

 et 

subject to y t     
   i  c

 i              n

where e is the vector of all ones  c     is the upper bound  and q is an n by n positive semidefinite matrix 
qij  k x  x    
the classification decision function is 
n
x

sign

 
yi i k xi   x    

i  

where  is an intercept term also obtained during training 
the advanced rbf svm used           we find an increase in auc outperforming all previous models 
we do see a slight drop in accuracy but an increase in precision  values extracted by averaging across   
cross validation folds 
rbf svm

feature count

auc

precision

accuracy

original
improved

  
   

      
      

      
      

      
      

parameters
default
        

table    comparison of tuned rbf svm with full indicator set to baseline

learning curves  svm  rbf kernel 

    
    

score

    
    
    
    
 

training score
cross validation score
    

    

    
    
training examples

    

    

    

figure    lane change auc performance on a naive bayes classifier versus horizon 

 

fian inspecton of the learning curve shows that the algorithm performance is converging on the training error 
which remains close to one  this is very good and suggests that a larger number of iterations on a larger
feature set would produce better results 

cross validation score

    
    
    
   
    
    
 

  

   

   

number of features selected

figure    automated feature reduction for rbf svm
running automated feature reduction shows that having access to most of the features results in best results 
but fairly good results can still be obtained with as few as twenty  this makes sense  most of the vehicles
motion is captured in a few variables  but the full description is often better 

   

predictive power vs  horizon

we can also get a sense for predictive power for certain horizons  see figure      here we see strong results
for a horizon of up to two seconds  after which the ability to predict begins to drop off 

figure    lane change auc performance on a naive bayes classifier versus horizon 

 

conclusion

this paper introduces a method for developing probabilistic driving models for use in traffic modelling and
automotive safety systems  dynamic bayesian networks were used to represent the distribution over acceleration and turn rate for the next time step  graph structure and parameters were learned from real world
driving data using existing bayesian methods and leveraging existing toolkits  in particular  the greedy
hill climbing algorithm with a k  prior consistently provided the best results in terms of maximizing the
bayesian score 
a lane change predictor using support vector machine classification was trained using a radius basis kernel
function  the classifier possessed an auc metric of        with the possibility of improvement given additional driving data  an analysis showed the decline in feature predictive performance with increased horizons
above two seconds 

 

fireferences
    m  kochenderfer  l  espindle  j  kuchar  and j  d  griffith  correlated encounter model for cooperative
aircraft in the national airspace system version      project report atc      lincoln laboratory       
    j  bezanson  s  karpinski  v  b  shah  and a  edelman  julia  a fast dynamic language for technical
computing  arxiv preprint arxiv                 
    m  j  kochenderfer  m  w  m  edwards  l  p  espindle  j  k  kuchar  and j  d  griffith  airspace
encounter models for estimating collision risk  journal of guidance  control  and dynamics  vol     
no     pp               
    j  schlechtriemen  a  wedel  j  hillenbrand  g  breuel  and k  d  kuhnert  a lane change detection
approach using feature ranking with maximized predictive power  in intelligent vehicles symposium
proceedings       ieee  pp          ieee       
    d  kasper  g  weidl  t  dang  g  breuel  a  tamke  a  wedel  and w  rosenstiel  object oriented
bayesian networks for detection of lane change maneuvers  intelligent transportation systems magazine 
ieee  vol     no     pp             
    m  j  druzdzel  smile  structural modeling  inference  and learning engine and genie  a development
environment for graphical decision theoretic models  in aaai iaai  pp               
    f  pedregosa  g  varoquaux  a  gramfort  v  michel  b  thirion  o  grisel  m  blondel  p  prettenhofer  r  weiss  v  dubourg  j  vanderplas  a  passos  d  cournapeau  m  brucher  m  perrot 
and e  duchesnay  scikit learn  machine learning in python  journal of machine learning research 
vol      pp                 
    p  spirtes  c  n  glymour  and r  scheines  causation  prediction  and search  vol      mit press       
    n  friedman  d  geiger  and m  goldszmidt  bayesian network classifiers  machine learning  vol     
no       pp               
     m  kochenderfer  aa     decision making under uncertainty  stanford bookstore       
     d  koller and n  friedman  probabilistic graphical models  principles and techniques  mit press       
     a  s  hesar  h  tabatabaee  and m  jalali  structure learning of bayesian networks using heuristic
methods  international proceedings of computer science   information technology  vol           

 

fi 

appendix
core features

f

rad

cont 

heading angle in the frenet frame



rad s 

cont 

turn rate

kvk

m s 

cont 

speed

f
vx

m s 

cont 

longitudinal speed in lane

vyf

m s 

cont 

lateral speed in lane

af
x

m s 

cont 

longitudinal acceleration in lane

af
y

m s 

cont 

lateral acceleration in lane
roadway features

lane
nll
nlr
dcl
dml
dmr
donramp
dof f ramp
ttcrml m
ttcrmr m

m
m
m
m
m
cont 
cont 

disc 
disc 
disc 
cont 
cont 
cont 
cont 
cont 
time to crossing of left lane marker
time to crossing of right lane marker

index of the closest lane
number of lanes to the left
number of lanes to the right
lateral distance between center of car and closest centerline
lateral distance between center of car and the left marker
lateral distance between center of car and the right marker
distance along the rhs until next onramp
distance along the rhs until next offramp

f
vx scene

m s 

cont 

the mean velocity of vehicles in the scene

areq
y

m s 

cont 

the acceleration required to stay in lane



m 

cont 

local lane curvature
vehicle relative features

drel
x r

m

cont 

longitudinal distance between observer and related vehicle r

drel
y r

m

cont 

lateral distance between observer and related vehicle r

rel
vx r

m s 

cont 

longitudinal relative speed between observer and related vehicle r

rel
vy r

m s 

cont 

lateral relative speed between observer and related vehicle r

r

rad

cont 

heading of the related vehicle

r

rad s 

cont 

turn rate of the related vehicle

areq
r
ttcx r
x r

m s 
s
s

cont 
cont 
cont 

longitudinal acceleration required to avoid a collision with the related vehicle r
time to a longitudinal collision with related vehicle r
timegap between observer and related vehicle r

af
x h

m s 

cont 

maximum longitudinal acceleration over history h

af
y h

h

m s 

cont 

maximum lateral acceleration over history h

rad s 

cont 

maximum turn rate over history h

af
x h

m s 

cont 

mean longitudinal acceleration over history h

af
y h

h

m s 

cont 

mean lateral acceleration over history h

rad s 

cont 

mean turn rate over history h

 af  x h

m s 

cont 

standard deviation of longitudinal acceleration over history h

 af  y h

m s 

cont 

standard deviation of lateral acceleration over history h

  h

rad s 

cont 

standard deviation of turn rate over history h

templated features

table    all indicator features

  

fi   

feature selection results
algorithm

   features

    features

     s
     s
     s

     s
     s
     s

feature ranking
forward search
graph search

table    feature selection algorithm runtime comparison  note that this is an offline process so the timing
difference for such small values is not of great concern 

   features

score

indicators

feature ranking
forward search
graph search

      
      
      

  vyf     af
y
  vyf   dcl
f
  vy   dcl

table     feature selection for fut
   ms with    features
    features

score

feature ranking

      

forward search

      

graph search

      

indicators

  af
y      ms  

f
  vy      ms  

  vyf      ms  



   ms   af
y    ms      ms
af
y    ms

   ms

table     feature selection for fut
   ms with     features
   features
feature ranking
forward search
graph search

score
       
       
       

indicators
  vyf     af
y   dcl   ttcrmr
f
  vyf     vscene x
  vyf     dmr   areq
stay

table     feature selection for fut
   ms with    features
    features
feature ranking

score
       

forward search

       

graph search

       

indicators


  vyf      ms      ms   af
y    ms


f
  vy      ms      ms   af
y  s




f
  af
y      ms      ms      ms   ay    ms      ms  

   ms

table     feature selection for fut
   ms with     features
   features

score

feature ranking
forward search
graph search

      
      
      

indicators
f
af
x   vx
f
af
 
v
x
x   nll
f
af
x   vx   nll

table     feature selection for afut
   ms with    features
    features

score

feature ranking

      

forward search
graph search

      
      

indicators
f
f
f
f
f
f
af
x   ax    s   ax    s   ax    s   ax    s   ax    ms   ax    ms  
f
f
af
x    ms   ax    ms   ax    ms
f
f
f
af
x    v   ax    s   ax    s   ax    ms
f
f
f
af
x    v   ax    s   ax    s   ax    ms

table     feature selection for afut
   ms with     features
  

fi   features

score

feature ranking
forward search
graph search

      
      
      

indicators
f
af
x   vx
f
af
x   vx   nll
f
af
 
v
x
x   nll

table     feature selection for afut
   ms with    features

    features

score

feature ranking

      

forward search
graph search

      
      

indicators
f
f
f
f
f
af
x   ax    ms   ax    ms   ax    ms   ax    ms   ax    ms  
f
af
 
a
x    ms
x    ms
f
f
af
y   vx   ax    ms
f
f
f
ay   vx   ax    ms

table     feature selection for afut
   ms with     features

  

fi