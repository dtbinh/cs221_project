how hot will it get  modeling scientific
discourse about literature
natalie telis  cs   
ntelis stanford edu

project aims
many metrics exist to provide heuristics for quality of scientific literature  some of which are
determined before publication  such as impact factor of journal  or affiliations of authors 
others  like citation count  are not  none of these metrics take into account the amount
of scientific discourse generated by an article  i aim to create a classifier that does  in
particular  in the wake of the rise of social media  more and more scientific discourse has
taken place in public  through blogging  facebook posts  and even tweets about papers  can
we predict how much discourse a paper will generate 

data preparation
i have annotated each of the         papers indexed within ncbis pubmed database in
the last   months with a tweeted score for each paper i defined as
ti     count tweetsi    

 count retweetsi  

a tweet is defined as a message  with or without content  posted on twitter which can be
linked  by url  which can be used to identify doi or pubmed database id  to a paper  in
assigning my scores  i use       a retweet is a tweet indicating that it has been sourced
from a previous tweet  with no additional content added  e g   a retweet which adds
commentary  such as rt  scientist twitter nice paper  http   link db  is considered
a tweet of its own  whereas a retweet with no additional commentary  rt  scientist twitter
http   link db  is considered a simple retweet  i set       
after calculating the tweeted score ti for our library  although the data is not normal  i
calculate a z score zi   ti tt for every tweet score  i do this as otherwise the score distribution is extremely noisy  varying from   across most of the set to scores in the hundreds of
thousands for a scant few papers 
once i have the normalized tweeted scores  i first aimed to understand the distribution of
discourse  unfortunately  across the body of papers  just shy of          or about     of
papers actually receive even a single tweet  this is understandable  as many papers come
from lower impact journals  average if in the set is       however  there is frequently lossy
data about exact impact factor due to new journals  which are therefore assigned an impact
factor median over all journals  which is      this may bias estimations of average impact
factor in a subset of papers  

 

fimoreover  some tweets may come from spammers  or from laypeople  however  manual inspection suggests that the fraction is low  moreover  most tweets in that category appear to
be retweets and are downweighted by     which suggests that they likely do not inflate
the scores enough to aect the quality of the data 

methods
methods used
topic assignment  latent dirichlet allocation   lda  is used for topic assignment  our
inferencer was trained on     million papers from ncbis pubmed with a dictionary consisting of all words in the titles and abstracts of those papers  with stop words removed 
the training process instantiates k latent topics  each with a dirichlet distribution over the
dictionary  and is refined through iterative gibbs sampling 
naive bayes  naive bayes is a binary classification scheme which can use binary data to
predict a binary response variable  probabilities of each word in a dictionary occurring in a
training example in one category  without loss of generalization  labeled    or in the other
 labeled    are defined according to bayes rule  with   added in the numerator  and the
size of the vocabulary on the denominator  to prevent undefined values in the case that a
word was not observed in the training dataset  laplace smoothing  
p wi  state   

counts wi   state     
total    v 

assuming each word occurs independently
of another  the probability of a document occuring
q
given a response variable state is i   p wi  state   this probability can be easily calculated
for both possible states  and the higher probability corresponds to the predicted state  i
implemented this method myself 
support vector machines  svm   the support vector machine relies on data points close
to the margin between two classification groups to identify maximal margin hyperplanes that
partition two classifications  i used the r package liblinears implementation 

preliminary approach
i began by fitting a generalized linear model to the standardized twitter z scores  however 
upon plotting  i quickly saw that this model would suer significantly because of the number
of papers with tweets 

 

fithis qq plot suggests that although the fit of the linear model is pretty okay for papers
with a median amount of tweets  the very long tail of papers with no tweets and the variable
high tail suggests that a linear model is not ideal for approximating those papers 
because of concerns about the reasonability of fitting a linear model to a dataset in which a
majority of the data is equal to zero  i decided to put o plans for a linear model and instead
to consider binary classification  high twitter activity     versus low twitter activity
     i annotated each paper in my set according to this paradigm  i considered papers with
  standard deviations tweet scores to be
i generate a training set and a testing set from my data  although this does not reflect the
actual proportions in the dataset  for demonstrative purposes  i selected     papers which
do have tweets  categorized as  s   biasing towards those which get a lot of tweets  and
    papers which do not have any tweets at all  categorized as  s   i do this by random
assignment  so my training set and testing set are usually slightly uneven  e g       and
        however  by fishers exact test i determine that the assignment of papers with high
tweets was not distinct between the training and testing sets 
i then define a dictionary based on words which appear more than once in the training and
testing sets  and use this dictionary to generate word counts for both training and testing
sets  then  i attempt to train several models to create a classifier  both naive bayes and
svm showed extreme signs of overfitting  so i readjusted my approach 

classification without topic partitions
i first attempted to use a naive bayes approach for classification  the naive bayes approach
is good for this kind of problem  as it takes into account the class prior during assignment 
and also aggregates potentially weak eects into a stronger signal of class membership  however  due to the extreme sparseness of the word matrix for high  and low activity tweeted
papers  this approach was poor  see below   no better than random  i decided to try another approach  focusing more on the borderline cases  a support vector machine 

 

fithe svm i trained suggested signs of serious overfitting  i theorized that this overfitting was
a result of poor concordance between training high activity papers and testing high activity
papers  as there is a deal of underlying structure in the documents  such as diering scientific subfields  and there are very few highly tweeted papers  the sets of high activity papers
might dier from sample to sample 
therefore  i repeated my previous analyses and data gathering methods  but i first restricted
my data set by using latent dirichlet allocation  topic assignments to create a soft classification for each paper  then  restricting to papers only within a particular topic  there are
a total of     topics   i repeated my training and testing procedure from above 

results
several datasets for training were used during the course of the project  denoted below 
 schema a  a random training and testing set of      papers subsampled from the
corpus of         total papers annotated in the last   months 
 schema b  a training and testing set made by randomly assigning all papers with
highest probability membership in a chosen topic  material science and technology  to
either training or testing   n        
 schema c  like schema b  but limiting membership in training and testing sets to
papers with probability       of belonging to material science and technology  more
conclusive assignment    n        
technique
naive bayes
naive bayes
naive bayes
naive bayes
svm
svm
svm
svm

data used
a
b
c
c
a
b
c
c

features included
all
all
all
words in   training document
all
all
all
words in   training document

training error
   
    
    
    
     
     
    
   

testing error
   
    
    
    
    
    
    
    

discussion
i believed much of the challenges in training resulted from the breadth of possible subject
material  this was confirmed by the vast decrease in overfitting once i restricted my training
to a particular topic  in the beginning  the results from both naive bayes and the svm
models suggest serious overfitting  this is likely because the relevance of training to testing
data was initially very thrown o by subsampling  as a sample of       done twice  from
        is not guaranteed to be representative at all  

 

fithese results suggest that my model could continue to improve with intelligent matching of
training and test data  the progressive tightening of the gap between training and test error
with the   dierent training and testing data generation schema suggests that by more and
more intelligent training strategies i could hope to improve the model even beyond this 
one way in which i plan to implement this is to consider the full topic probability vector
rather than simply the most probable assignment  this would help intelligently choose training data that does not confound  and there is a lot of correlation structure in similar joint
assignment  e g   many papers assigned with high probability to one topic are often assigned
with moderate probability to only one or two others  partitioning those sets in those ways
may further pare down error  
i also plan to consider improvements on my learning techniques  for example  a multinomial
naive bayes model may be more eective than one based simply on binary word occurrence 
in the bag of words model  likewise  i could potentially improve the model by including
binary metadata  such as high or low impact factor 
one question of interpretation of these results is methodological  what association do words
seem to evoke  and why do certain words mean papers get more tweets  in particular  for
material science and technology  the topic showcased in this report  the two most leading
terms are nanotube and pacemaker  this suggests to me that there are particular words
that are indicative of particular areas in a field that are hot  this suggests that similar prediction  with a much longer spread of social media data  could even lead to longitudinal
trend identification in terms of interests and directions in a particular field which are seen
as interesting or hot 
another final methodological question is to note that tweets may indicate outrage  curiosity 
confusion  or amusement  not just interest  since our repository also contains tweet text
as well as tweet authors  perhaps sentiment modeling  with some correction for longitudinal
average sentiment  could help partition papers into high positive  high negative  and
other sentiment and activity co categories 
in conclusion  although the current model performs well  there are many open problems in
the field of prediction of discussion around a text to continue to explore  and i hope to be
able to by expanding on the techniques outlined in this project 

references
   latent dirichlet allocation  blei  et al  journal of machine learning research   
                

 

fi