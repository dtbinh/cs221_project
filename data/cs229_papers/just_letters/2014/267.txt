star galaxy separation in the era of precision cosmology
michael baumer  noah kurinsky  and max zimet
stanford university  department of physics  stanford  ca      
november         

introduction
to anyone who has seen the beautiful images of the
intricate structure in nearby galaxies  distinguishing between stars and galaxies might seem like an
easy problem  however  modern cosmological surveys
such as the dark energy survey  des      are primarily interested in observing as many distant galaxies as possible  as these provide the most useful data
for constraining cosmology  the history of structure
formation in the universe   however  at such vast
distances  both stars and galaxies begin to look like
low resolution point sources  making it difficult to isolate a sample of galaxies  containing interesting cosmological information  from intervening dim stars in
our own galaxy 
this challenge  known as star galaxy separation
is a crucial step in any cosmological survey  performing this classification more accurately can greatly
improve the precision of scientific insights extracted
from massive galaxy surveys like des  the most
widely used methods for star galaxy separation include class star  which is a legacy classifier with a
limited feature set  and spread model  a linear discriminant based on weighted object size      the performance of both of these methods is insufficient for
the demands of modern precision cosmology  which is
why the application of machine learning techniques
to this problem has attracted recent interest in the
cosmology community      since data on the realworld performance of such methods has yet to be
published in the literature  we will use these two classifiers as benchmarks for assessing the performance of
our models in this paper 

figure    in the top false color image from des  it is
easy to tell that the large object in the foreground  ncg
      is a galaxy  but what about all the point sources
behind it  in the bottom images of our actual data  we
see sky maps of stars  right  and galaxies  left   the voids
seen in the map of galaxies are regions of the sky which
are blocked by bright nearby stars 

hst data  the catalog contains         sources 
of which               are used for training and
             are  randomly  reserved for crossvalidation 
from the     variables available in the catalog 
we selected    variables which appeared to have disdata and feature selection
criminating power based on plots like those shown in
we use a matched catalog of objects observed by figure    features include the surface brightnesses 
both des  a ground based observatory  and the hub  astronomical magnitudes  and sizes of all objects as
ble space telescope  hst      over   square degree observed through   different color filters  near ultraof the sky  the true classifications  star or galaxy  violet to near infrared   we also use goodness of fit
are binary labels computed from the more detailed statistics for stellar and galactic luminosity profiles
 

fi a  log   for a star like luminosity pro   b  log  size   full width at half   c  histograms of mean surface brightfile vs  a galaxy like profile 
maximum  vs  infrared magnitude
ness

figure    a selection of catalog variables  before preprocessing  selected as input features to our models 
 which describe the fall off of brightness from centers
of objects  
as a preprocessing step  we transformed all variables to have a mean of   and variance of    for certain variables   namely chi squares and object sizes we decided to use their logarithm rather than their
raw value  to increase their discriminating power and
make each feature more gaussian  see panels  a  and
 b  in figure    
in performing feature selection  it was important to us that features were included in physicallymotivated groups  after making the plot of learning
vs  included features shown in figure    we implemented a backwards search  using a linear svm  to
determine the relative power of each of the variables 
figure    a plot of training success vs  features used
we were surprised to find that our less promising pafor four of our models 
 
rameters   and surface brightness  were often more
powerful than magnitudes in certain color bands  and
method
   train    test
considered removing some magnitudes from our feagda   smote
     
     
tures  we ultimately decided against this  however 
gnb   smote
     
     
given that we wanted to select features in a uniform
lr
     
     
manner for all color bands to maintain a physicallylinsvm
     
     
motivated feature set 
gaussiansvm
     
     

methods and results

table    training and test error for multiple supervised learning methods 

to determine which method would obtain the best
discrimination  we initially ran a handful of basic
algorithms with default parameters and compared
results between them  given that we had continuous inputs and a binary output  we employed logistic regression  lr   gaussian discriminant analysis  gda   linear  and gaussian kernel svms  and
gaussian naive bayes  gnb  as classifiers  we implemented our models in python  using the sklearn
module     

we opted to employ the    regularized linear svm classifier linearsvc     regularization
gave comparable results   the naive bayes algorithm gaussiannb  the logistic regression method
sgdclassifier loss  log    and the gda algorithm  using class weighting where implemented
 namely  for svm and lr  to compensate for the
much larger number of galaxies than stars in our
training sample  the results of these initial classi 

fimethod
true g
true s

gda   smote
g
s
     
    
     
      

gnb   smote
g
s
     
    
            

lr
g
s
           
           

linsvm
g
s
          
           

gaussiansvm
g
s
          
           

table    confusion matrices produced on our test set for different supervised learning techniques  showing
the true false positive rates for galaxies in the top row and the true false negative rates for stars in the
bottom row of each entry 
fications can be seen in table    and the confusion
matrices from these runs can be seen in table   
the statistics in table   show that we have sufficient training data for the lr  gda  and svm methods  since our test error is very similar to our training
error  we decided to proceed further from here with
naive bayes gda and svm  in order to continue our
search for a successful generative model and optimize
our best discriminative algorithm 
gaussian naive bayes and gda with bootstrapping and smote we first describe the
generative algorithms we applied  eq      shows the
probability distribution assumption for the gaussian
naive bayes classifier  where xi is the value of the figure    training success vs  features used for data
i th feature and y is the class  star or galaxy  of the with equal numbers of stars and galaxies  produced
object under consideration 
using the smote algorithm 
p x y   

n
y

p xi  y  

 
xi  y  n  i y   i y
 

   

we also performed a synthetic oversampling of
stars using the synthetic minority over sampling
technique  smote  algorithm      the idea of
smote is to create new training examples which
look like the stars in our data set  specifically  given
a training example with features x that is a star  we
choose  uniformly at random  one of the   nearest
stars in the training set  where nearest means we
use the euclidean    norm on the space of our normalized features   denoting this chosen neighbor by
x    we then create a new training example at random
location along the line segment connecting x and x   
after oversampling our population of stars using
the smote algorithm to have equal numbers of stars
and galaxies  the training errors for gaussian naive
bayes and gda suffer  as they are no longer able to
succeed by classifying almost everything as a galaxy 
given that these generative models fail to classify a
balanced dataset well  we conclude that our data does
not satisfy their respective assumptions sufficiently to
warrant their use  this is not very surprising  as generative learning models make significant assumptions 
as we will see next  the discriminative algorithms we

i  

this makes stronger assumptions than the quadratic
gaussian discriminant analysis model  whose assumed probability distributions are described in eq 
    
x y  n  y   y   

   

in fact  this shows that the gnb model is equivalent
to the quadratic gda model  if we require in the
latter that the covariance matrices y be diagonal 
to correct for the effects of our imbalanced data
set  we applied the bootstrapping technique  whereby
we sampled from our set of galaxies  and trained on
this sample  plus the set of all of our stars  the intent was to take    such samples and average the
results of these different classifiers  however  each
individual classifier still performed poorly  misclassifying most stars  unless we heavily undersampled
from our galaxies  that is  we chose far fewer galaxies
than stars to be in our new data set   in which case
the classifiers misclassified most galaxies 
 

fifigure    results of two rounds of gaussian svm optimizations  varying the parameters c and  logarithmically  the leftmost plot shows c              and              while the middle and right plots show
c                 and                 the left two plots show training versus test success percentages 
while the rightmost plot compares success for stars and galaxies over the range of our better performing grid
search 
used performed much better 
grid search optimized svm with gaussian
kernel  l  regularization given the good performance of the linear svm and a preliminary run of
the svm with gaussian kernel  we decided to try to
optimize the     regularized svm by performing a grid
search across the parameter space c              
and             spaced logarithmically in powers
of       we employed the svc method of sklearn 
a python wrapper for libsvm  which provides automatic class weighting to counteract our unbalanced
training sample  we used the barley batch server to
run these grid samples in parallel  and had svc record
the training error  test error  and confusion matrices
for each model  the resulting scores from these optimizations can be seen in figure   
our model choice was based on of the requirement
for lowest generalization error  highest test success 
and best separation for stars  as a classifier which
classified every point as a galaxy would have a good
overall test performance but perform no source separation  and thus perform poorly on stars  in figure    this was performed functionally by selecting 
first  the model with the highest test success  then
among those the models closest to their correspond 

figure    histogram of the margin from the optimal separation plane  as determined by our gaussian
svm  of each training example  a negative margin
corresponds to a galaxy  positive to a star 

ing training success rates  to minimize overfitting 
then  among this subset of models  we chose the
model which performed best separating stars while
not compromising galaxy separation performance 

the sum total of these considerations leads us to
select the model with c                   which has
the confusion matrix shown in table   and an over  at the poster session  our tas suggested importance sampling or random sampling as alternatives to a grid search  all test success rate of        out of all methods
given the average run time of between   to    hours per opti  tested  this svm had the highest generalized success
mization  and the fact that importance sampling is an iterative rate and best performance separating stars  at    
process  we opted for grid search to reduce computing time  it
success  the distance of each example from the sepacan be seen in our figures that our optimization is fairly convex  so the grid search appears to be sufficiently accurate  see rating hyperplane of the svm can be seen plotted as a
also csie ntu edu tw cjlin papers guide guide pdf 
histogram separately for stars and galaxies in figure
 

fi   the factor which most limits the star separation
performance seems to be the multimodal distribution
of stars in our feature space  one of which is highly
overlapping with galaxies  it is interesting to note the
highly gaussian distribution of galaxy margins from
the plane  indicating that these were better modeled 
and overall had more continuous properties 

discussion
since we determined that the generative models
we attempted to use were not appropriate for our
dataset  we are left with three successful discriminative models  logistic regression  linear svm  and
gaussian svm  we used a receiver operating characteristic  roc  curve to characterize the performance of our benchmark models  class star and
spread model  since they both give continuous output scores  since our three models all give binary
outputs  they appear as points on the roc curve 
as shown in figure    we see that all three of our
models significantly outperform the two benchmarks 
as noted in the previous section  the multimodal
distribution of stars was the limiting factor in our
ability to separate these object classes  regardless of
the model  we rarely saw a true star rate greater
than about      suggesting this class of stars comprises on average     of observed objects  and may
be worth modeling  given the high dimensionality
of our features  it was not obvious what feature may
have set these stars apart  but it is clear that they are
confounded with similar galaxies  though galaxies do
not show similar substructure 

figure    an roc curve for our best three supervised
learning methods compared to benchmark classifiers
from the literature 
boundary indicates that there might be a class of
stars that is not properly modeled by our current feature set  this motivates the future work of seeking
out other astronomical catalogs with more or different features to enable better modeling of our stellar
population  in addition  though we chose to focus
on algorithms discussed in this course  deep learning also has great potential for improving star galaxy
separation  such algorithms are the focus of some
bleeding edge cosmology research      though their
performance on current generation survey data has
yet to be published 

references

conclusions and future work

    bertin   arnouts         sextractor  software
for source extraction  a as            
    chawla et al         smote  synthetic minority over sampling technique  jair      pp         
    des collaboration         the dark energy
survey  arxiv astro ph         
    koekemoer et al         the cosmos survey  hubble space telescope advanced camera
for surveys observations and data processing 
apjs            
    pedregosa et al         scikit learn  machine
learning in python  jmlr      pp            
    soumagnac et al         star galaxy separation
at faint magnitudes  application to a simulated
dark energy survey  arxiv          

we have shown that machine learning techniques
are remarkably successful in addressing the challenges of star galaxy separation for modern cosmology  though the assumptions of our generative
modelsgda and naive bayeswere not borne out
in the data  causing them to perform poorly  we
had success with logistic regression and svms  and
the largest challenge was in feature standardization
and svm optimization  our best model  the gaussian svm  achieved very good performance  classifying       of true galaxies correctly  while achieving
      accuracy in classifying true stars  surpassing
both of our benchmark classifiers 
the distribution of stars near the svm decision
 

fi