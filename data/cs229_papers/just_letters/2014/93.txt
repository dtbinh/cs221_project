life expectancy post thoracic surgery
adam abdulhamid  ivaylo bahtchevanov  peng jia
stanford university   cs    

abstract
operative mortality rates have been a topic of great interest among surgeons  patients  lawyers 
and health policy administrators  postoperative respiratory complications are the most common fatality
following any type of thoracic surgery  the exact incidence is most contingent upon the preoperative
health and lung function of the patient  and we would like to explore and understand how those conditions
can drive these complications  one particular metric that has been used to quantify mortality rates in
the past has been the thirty day mortality rate  this metric  however  may not be entirely comprehensive
because many patients die shortly after this time period or become very weak  having to be taken to another
facility before passing away there  as a result  many of these deaths are severely underreported  the scope
of our project is to examine the mortality of patients within a full year after the surgery  more specifically 
we are examining the underlying health factors of patients that could potentially be a powerful predictor
for surgically related deaths 

i 

data

the data was compiled by marek lubicz 
konrad pawelczyk  adam rzechonek  and
jerzy kolodziej at polands wroclaw thoracic
surgery centre for patients that were victims
of severe lung resections for primary lung cancer in      to       the database is part of
the national lung cancer registry and is administered by the institute of tuberculosis and
pulmonary diseases in warsaw  poland  the
data is represented as follows  the rows are
the patients      training examples  and the
columns are the features     features and the
true false labelling   the examples are labeled
with ground truth  i e  we know whether the
given patient lived or died  a  false  label indicates that the patient lived   year past the
surgery  while a  true  label indicates the patient died within   year after the surgery  the
features include both continuous data and class
data on the patients health 

ii 

features

as mentioned  our feature set includes both
continuous and classification data regarding to
the patients health conditions at the time of

the surgery  each patient has    variables associated with them  all    of them are known
for each of the     patients  as well as the
class label for each patient   some of the continuous data includes a patients forced vital
capacity  the maximum volume their lungs
exhaled  size of original tumor  and age at
surgery  in addition we have several classification features such as presence of pain before
surgery  haemoptysis before surgery  cough before surgery  whether the patient is a smoker 
whether the patient has asthma  and a few others  the classification predicts whether the
patient survived the following year long period 

iii 
i 

naive approach

strategy

initially  we simply ran naive bayes  svm 
and logistic regression to obtain a better
understanding of our data  we trained our
algorithms on the full data set and obtained
our testing values using using k folds cross
validation  setting k equal to     
we then implemented a bootstrapping ap 

fiproach on our three algorithms  we randomly
sampled from our old data set with replacement  i e  duplicates are allowed  in order to
force a       percentage labelling split in the
datasets  we then trained    models  obtained
   different predictions and then average those
predictions to obtain a final prediction  we
analyzed data sets of different sizes to see how
data set size affected our accuracy and recall 

ii 

results

our results can be seen in table   at the top of
the following page  and in figure   and figure
  below 
testing accuracy of different classifiers with bootstrapping

naive bayes
logistic regression
svm

accuracy

   

   

   

   

   
  

   

   

   

   
   
dataset size

   

   

   

   

figure    testing accuracy with bootstrapping

testing recall of different classifiers with bootstrapping
 
naive bayes
logistic regression
svm

   
   
   

recall

   
   

   

random forest

strategy

our next approach to tackle our variance problem was the random forest  using the tree
classification algorithm allows us to average
multiple deep decision trees that would be
trained on different parts of the same training set  this approach comes at the expense
of a slightly higher bias  and potentially some
loss of interpretability  but will ultimately improve the final performance of the model  the
classification tree algorithm works very well
when you have mixed categories of continuous and binary features  by taking random
subsets of features  examining all of the possible split points   the algorithm can make a
decision on which feature is the best and pick
that one  while still accounting for uncertainty 
we then combined our bootstrapping approach
to incorporate the random forest algorithm as
follows 

 a  draw a bootstrap sample z of size
n from the training data 

   
   

   

   

   

   
   
dataset size

   

   

   

   

figure    testing recall with bootstrapping

 

iv 

   for b           b 

   

 
  

analysis

our initial results showed moderately high accuracy but very poor recall  ability to properly
classify the positive examples  because of the
negative skew in the data  our svm maintains
high accuracy simply by returning  false  every
single time  the bootstrapping approach gives
a slight improvement to recall but still maintains the problem that each sampled model
will have the same dominant variables present 
each of the    samples will have trees that look
very similar  we are not significantly reducing
variance and still have an over fitting problem  

i 

 

   

iii 

 b  grow a random forest tree tb to the
bootstrapped data  by recursively repeating the following steps for each
terminal node of the tree  until the
minimum node size nmin is reached 

fitable    accuracies and recalls for different classifiers

training accuracy

testing accuracy

training recall

test recall

without bootstrapping
naive bayes
logistic regression
svm

      
      
      

     
     
      

      
      
      

     
     
 

with bootstrapping
naive bayes
logistic regression
svm

     
      
      

      
      
     

      
      
      

      
     
    

note  all results are based on dataset size of    
i  select m variables at random
from the p variables 

testing accuracy for randomforest classifier
 

   

ii  pick the best variable splitpoint among the m 
accuracy

iii  split the node into two daughter nodes 

   

   

without bootstrapping
with bootstrapping

   

   

   
  

   output the ensemble of trees

   

   

   

  tb   b  

   
   
dataset size

   

   

   

   

figure    testing accuracy with without bootstrapping

to make a prediction at a new point x 
 
b

b

testing recall for randomforest classifier
 

 tb   x  

   

b   

classification  let cb   x   be the class prediction of the bth random forest tree  then
crbf   x     majority vote cb   x    b  

without bootstrapping
with bootstrapping

   
   
   
recall

regression  frbf   x    

   
   
   
   
   

ii 

results

our results can be seen in table   at the top
of the following page and in figure   and  
below 

 
  

   

   

   

   
   
dataset size

   

   

   

   

figure    testing recall with without bootstrapping

 

fitable    accuracies and recalls for random forest

training accuracy

testing accuracy

training recall

test recall

without bootstrapping
random forrest

     

     

     

     

with bootstrapping
random forrest

     

      

 

 

note  all results are based on dataset size of    

iii 

analysis

random forest is a classification tree algorithm
that enables the averaging of multiple deep
decision trees that are trained on different
parts of the same training set with the goal of
reducing variance  this is very useful in our
case because trees that are grown very deep
will often learn irregular patterns  resulting in
an overfitting of the training sets  our earlier
trials demonstrated strong training accuracy
but poor testing accuracy for this exact reason 
when we combine bootstrapping with random
forest algorithm  what is referred to as bagging   we are essentially able to pick the best
features to run on a forced even split label data 
bagging worked well in our case because
we had a high variance  low bias procedure
of noisy trees  each tree is identically and
independently distributed  meaning the expectation of an average of any number of trees is
roughly the expectation of any one tree  by
averaging out the trees  we can maintain the
bias level of each original tree while seeing improvement on the variance side  an average of
b i i d  random variables  each with variance
    has variance b      since the trees are not
necessarily independent  the average would be
with positive pairwise correlation   the vari  
ance of the average is         b   from
this equation  we can see that a large b value
will make the second term disappear  thus the
size of the correlation of pairs of bagged trees
limits the benefits of averaging  our tree classification algorithm will reduce the variance
through reducing the correlation between the
 

trees  without increasing the variance significantly 

v 

feature selection

we implemented and ran a couple of feature
selection algorithms  we tried an algorithm
called  best first   which works by using a
greedy approach with backtracking  it can
work either searching forward  starting from
the empty set  or search backwards  from
the full set   the resulting feature set was as
follows  pre   pre   pre   pre    pre   
pre    pre    pre    pre    pre    age 
what this means is our  best first  feature selection algorithm selected these    features as
the optimal set of features 
we ran another algorithm called  rank
search   which works by training and testing
models on each individual feature  it then outputs which features performed the best  the
idea is that this indicates that it is an important
feature   it then tries with increasingly large
subsets  the best feature plus the next best feature etc  until it finds the best subset  this
algorithm reliably reported pre   and pre  
as the two dominant features  pre   is an indicator telling whether the patient has asthma 
and pre   is an indicator telling whether the
patient had a heart attack within the   months
leading up to the surgery 

vi 

conclusion

by the end of all of our iterations and improvements  we were able to achieve fairly good

firesults with the random forest and bootstrapping  these results have large implications in
the medical field  an analysis similar to ours
could be performed before a patient goes in for
surgery to see how high risk they are  which
could be crucial information  one particular
challenge we faced was the limited amount of
data  we received our data from the uci machine learning repository  so we were bound
by the variables and patient examples from the
specific study 

vii 

future

ultimately  we were able to improve our results
by averaging a series of identical and independently distributed trees  which we would like
to contrast with boosting  in which the trees
would be grown in an adaptive manner specific to the bias  not i i d    we would like to
recursively train on the residuals of each misclassification  a next possible step would be to
implement the following algorithm  bumping  
   bootstrap n models  with replacement 
forcing even ratios   where number of
models   number of features 
   train n models  with initially one feature
per model 
   test all n models on original data set  pick
the model with lowest error on original
data set  and define a new residual data
set on all misclassified examples 

   train your next n models on the residual
 i e  boosting    but no averaging at this
point 
   test on the very original data set and
pick the best one  continue process repeatedly 
hopefully this will further reduce our variance  in addition  we calculated the optimal
feature set as shown above  so it would be interesting to compare different results for all
of our implementations if we use only those
specific features 

references
 wroclaw university study  creators  marek
lubicz      konrad pawelczyk      adam
rzechonek      jerzy kolodziej    
     wroclaw university of technology  wybrzeze wyspianskiego            
wroclaw  poland 
     wroclaw medical university 
wybrzeze l  pasteura           wroclaw 
poland 
boosted svm for extracting rules from
imbalanced data in application to prediction of the post operative life expectancy
in the lung cancer patients  applied soft
computing 

 

fi