classifying legal questions into topic areas using machine learning
brian lao
stanford university
bjlao stanford edu

karthik jagadeesh
stanford university
kjag stanford edu

abstract
in this paper we describe the steps taken
to build a machine learning classifier that
successfully classifies legal questions into
the most relevant practice area  we have
created    different general categories that
legal questions fall into  categorizing legal questions into the correct practice area
has many useful applications such as facilitating improved realtime feedback  information retrieval  relevant lawyer recommendations  and responses to users asking
questions on q a websites 

 

introduction

we built a multiclass classification model for categorizing everyday users legal questions into the
relevant legal topic area  application of machine
learning to the legal domain remains a relatively
new task     with the increasing ubiquity of the
internet  individuals are looking more to internet
resources to find relevant attorneys and to obtain
answers to their legal questions  legal q a websites exist where users ask a question and are given
the choice of tagging their question with the relevant topic area  for instance  a users legal question may be  how do i file for divorce  the
relevant legal area tag would be family law 
however  people sometimes lack the legal expertise to select the topic areas most relevant to their
question  thus  classifying legal questions into
their associated topic area would facilitate     
the retrieval of more relevant information and answers to educate the user  and     improved recommendations of relevant lawyers with expertise
in the particular topic area  we conducted multiclass classification with   models      logistic regression  logistic       multinomial naive bayes
 mnb       linear svm with newton method
 svm  newton        svm with stochastic gradi 

figure    we used apache nutch to scrape
        user asked legal questions  the legal categories associated with each question  and the locations in which the questions were asked 
ent descent  svm  sgd    and     one layer neural network    layer nn   linear svm with newton method performed the best with a       test
accuracy in multiclass classification 

 

data

in order to successfully build a supervised
learning model to categorize legal questions 
we needed a large dataset of user asked legal
questions that have already been labeled with
the correct practice area  legal q a websites
contain publicly released user questions that have
been manually labeled   by users or lawyers   with
the relevant topic area  we used the apache nutch
web crawler  http   nutch apache org   to crawl
the following legal websites      avvo com legalanswers       ask a lawyer freeadvice com     
lawguru com legal questions 
we obtained
        user asked legal questions  the associated
legal area tags  and the locations in which the
questions were asked  figure   shows an example
of the different components of a legal question
that we scraped using apache nutch 
a typical example of a legal question may include  a  two sentences of background about the
users situation and  b  a sentence containing the
actual question such as what legal remedies do i
have after being hit be a negligent car driver  the
associated tag for this example would be personal
injury law and the location would be a u s  state
such as california  although the questions were

fias a feature  and created a feature vector consisting of the counts of how often each word appears
in the question  the intuition behind using this
feature vector is that questions with a shared topic
area will contain similar words that will allow us to
discriminate between legal topic areas  for example  the category intellectual property will likely
have a high proportion of questions that use the
word patent 
   

figure    distribution of our    legal category labels after preprocessing the data
often short in length  the relatively large number of
documents facilitated the training of a model that
could predict the legal area with relatively high accuracy  the rows in our raw data file are individual
questions  and the columns include     the legal
question      associated topic area  and     associated location 

 
   

features
data preprocessing

for the legal q a websites that we scraped  each
website used different vocabulary for their topic
area labels  for instance  avvo has the legal
categories  dui and speeding ticket  while
freeadvice has the legal category drunk driving  to create a unified set of topic area labels
for the legal questions  we created our own set
of    practice areas  we then used our legal
knowledge to manually map the legal q a websites different legal categories into our own categories  for example    of our    practice areas is
driving law  which encompasses avvos dui
and speeding ticket categories  freeadvices
drunk driving category  and other categories related to driving law  our set of practice labels and
the labels distribution amongst the legal questions
can be seen in figure   
we further processed the legal questions by    
removing stop words      stemming words with
the porter stemmer  and     lower casing all words
in the questions 
   

word unigram features

this is the most basic feature vector used for any
text based model  we look at each unique word

word bi gram features

while the unigram feature vector can be effective
for certain tasks  it is often useful to look at the
context of a word in a sentence  one straightforward method of capturing the context of a word is
to look at its surrounding words  rather than keeping track of individual word counts  the bi gram
feature vector keeps track of the number of times
that pairs of words appear in a question  intuitively  bi grams allows us to capture the discriminative power in two word terms such as home
accident  which would likely be associated with
the legal area of personal injury law  if only
unigram features are used  home could be associated with real estate law for instance  by
combining both unigram and bi gram features  a
much larger feature space is obtained  resulting in
a sparser set of features 
   

tf idf weighting

we use term frequency   inverse document frequency  tf idf  to give weights to unigrams and
bigrams based on their relative importance to our
corpus of legal questions  in the context of our
task  tf idf will give high importance to legal
words such as annulment that may show up with
high frequency within an individual family law
question  but may not be common in the context
of the entire corpus  legal terms are often topic
area specific  and we want to ensure that our features capture the discriminative power of the legal
terms being used 
   

word vec features

we used a pretrained set of word vector features
from the word vec project  this system contains
a vector for each word in the dictionary and uses
a neural network trained on new york times to
provide a context relevant vector representation of
the word  we represent a paragraph by looking at
the average of values of the word vectors in the
document 

fi 

models

in order to classify legal questions into   of the   
legal categories  we experimented with   different models and evaluated their label classification
abilities 
   

logistic regression

this logisistic regression model is a probabilistic classifer using the stochastic gradient descent
model of learning 
   

multinomial naive bayes    

this mnb model uses laplace smoothing for
maximum likelihood of parameters where we used
a laplace smoothing parameter      
nyi  
yi   ny  n
the standard naive bayes model is useful when
features have values of   or    binary   the model
described here allows us to generalize and use a
naive bayes appraoch for features that take on values of         k 
due to some of the assumptions made in this
model  skewed training data will result in a shift of
the weights to the biased classes  this is especially
problematic for us since there is a large difference
in   of samples in each class 
   

linear svm    

this svm model solves the optimization problem 
p
minw    wt w c li    w  xi   yi   where we used
the l  svm loss function max    yi wt xi      
and a penalty parameter c     
   

units  one solution to this problem can be to implement and run the neural network on a gpu 

 

results

we conducted   fold cross validation on our data
set  resulting in our svm model with newton
implementation performing the best in multiclass
classification with a       test accuracy  see figures   and    
the immigration and driving law categories possessed the best f  scores  as seen in figure   

svm with stochastic gradient descent

this svm model uses a hinge loss function of
l y    max     ty   a l   penalty  and employs
the sgd algorithm  w   w  qi  w 
   

figure    overall   layer neural network architecture  when we are using word frequency and
tf idf feature vectors  the   of input units is on
the order of   k 

one layer neural network

we designed a neural network     with   k input
units      hidden units  and a softmax function for
the output layer 
zj
 z j   pke z for j              k 
k  

e

k

this model is able to learn complex patterns if a
large training data set is available  it is especially
useful when we are trying to learn from a sparse
feature space 
with out current implementation of neural networks  the computational time is extremely slow
given that we have on the order of    k input

 
   

discussion
general

given that svms have performed well in many
text classification applications     we were not
surprised by the linear svm model performing
the best with a promising       test accuracy 
low f  scores for the topic areas of personal
injury  litigation  and government brought
down the overall accuracy  poor performance in
these topics may be explained by these topics having overlap with other topics  where  for example 
a classifier may have trouble categorizing a car
accident question because of the questions relevance to both the personal injury and driving
law categories  in the future  for the f  scores
for individual categories  we plan on obtaining f 

fifigure    training and test accuracy for our  
models 

scores using the   layer neural network as well 
our neural network implementation was initially extremely slow when we used the   k input units from the training data word corpus  this
prompted us to test out alternative input vector
forms where we filtered and only kept the top k
tf idf weighted features  thus  for computational purposes  a lot of useful information was
thrown out  which contributed to the lower test accuracy of     
we then implemented a method that incorporates word vec learn word representations as an
alternative method to reduce the dimensionality of
the input feature space while maintaining information about the data  we were pleased with the improvement to     test accuracy  and we plan on
tweaking the model more in an attemp to further
improve accuracy 
   
     

figure    error rate over    epochs starts to stablize around     after    epochs

sources of error
sentences related to multiple practice
areas

some legal questions inherently belong to multiple practice areas  for example  many questions might have relevance to both the consumer
  product and the tech   privacy topic areas  although each of the legal questions in our
dataset only has one topic area label  the dataset
inevitably contains legal questions that could fall
under multiple categories  our multiclass classifiers will have more trouble classifying these questions  which may explain the lower accuracy on
topics like litigation 
     

class imbalance

some of our topic area classes have fewer training samples than others  for example  intellectual property questions only have    k questions
compared with the average of     k questions in
other categories  to help remedy the unbalanced
class problem  we can try certain techinques such
as sub sampling and up sampling 
     

figure    f  scores for the individual topic areas 

sentence imperfections

a unique aspect of our data is that the questions
are unedited from users on the internet  thus 
there are issues regarding spelling  grammar  and
homonyms of words being used which lead to different meanings  we can try using publicly available spelling correction apis to find a correctly
spelled word with the closest edit distance 

fi 

future

there are various approaches that we plan to take
to improve our current results  we will look to
parse the input data in new ways and create a larger
set of features upon which we can build a model 
additionally  we will consider methods of feature
selection to reduce the feature space and look at
alternative statistical models and methods which
may be more conducive to the classification task
at hand 
   

location tags

for each question  we have information regarding
the location in which the question was asked  this
feature may prove to have discriminative power if 
for example  more driving law questions are
asked in california versus other states 
     

weighting

it may be useful to have features that give more
weight to words in sentences ending with a question mark punctuation  a legal question in our
dataset may contain multiple sentences  for example a single legal question may have     sentences
of background before a sentence that asks the legal
question itself  features based on the legal question itself may be more indicative of a legal topic
than the features based on the sentences about the
background of the users situation 
     

      pos tags
we plan to impelement semantic features such as
part of speech  pos  tags  in pos tagging  we
will use the definition of the words within the context of the specific legal question in order to annotate each word with its part of speech  such as
labeling the word as a verb or a noun  this added
information will help to differentiate between the
same words that are used with different meanings 

new features

we plan on modifying our feature set to     include the location tags associated with each question and     give more weight to words in sentences depending on the sentiment of a sentence 
and     using the part of speech tag for each word
in the sentence  we will test out various new features that are more tailored to the task of classifying legal questions 
     

a question of how do i incorporate my new business 

sentiment based features

we will also try to implement features that capture the sentiment of the legal question  for instance  we would apply sentiment analysis to assess the polarity of a given legal question  whether
negative  neutral  or positive  for example  we
would intuitively expect legal questions in personal injury law to be more negative  as the user
asking the question was likely physically injured 
however  we may expect questions under business law to be more neutral or positive  such as

   

feature selection

we plan on applying various feature selection
methods     such as      chi squared      information gain      removing frequent or rare words 
    clustering related word features  these methods will allow us to prune the noisy features and
only incorporate features that are most relevant to
the classification task at hand 

 

conclusion

we evaluated the ability of various different models to classify user asked legal questions into the
appropriate legal areas  as expected  a linear
svm model performed the best with a       test
accuracy in multiclass classification  we plan to
continue improving our current results using the
new methods and metrics that we have mentioned 

references
    r  nallapati  c d  manning  legal docket classification  where machine learning stumbles 
emnlp        
    c d  manning  p  raghavan and h  schuetze 
introduction to information retrieval  cambridge
university press          pp          
    r e  fan  et al  liblinear  a library for large
linear classification  the journal of machine learning research                      
    r d  goyal  knowledge based neural network
for text classification  ieee international conference on granular computing              
    t  joachims  text categorization with support vector machines  learning with many relevant features 
in proceedings of ecml   th european conference
on machine learning       
    g  forman  an extensive empirical study of
feature selection metrics for text classification 
jmlr                  

fi