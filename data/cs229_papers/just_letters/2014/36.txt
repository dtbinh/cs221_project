applications of machine learning to predict yelp
ratings
kyle carbon

kacyn fujii

prasanth veerina

aeronautics and astronautics
stanford university
stanford  ca
kcarbon stanford edu

electrical engineering
stanford university
stanford  ca
khfujii stanford edu

computer science
stanford university
stanford  ca
pveerina stanford edu

abstractin this project  we investigate potential factors that
may affect business performance on yelp  we use a mix of
features already available in the yelp dataset as well as generating
our own features using location clustering and sentiment analysis
of reviews for businesses in phoenix  az  after preprocessing
the data to handle missing values  we ran various featureselection techniques to evaluate which features might have the
greatest importance  multi class classification  logistic regression 
svm  tree and random forest classifiers  naive bayes  gda  was
then run on these feature subsets with an accuracy of      
significantly higher than random chance        for   class
classification   regression models  linear regression  svr  were
also tested but achieved lower accuracy  in addition  the accuracy
was approximately the same across different feature sets  we
found that across feature selection techniques  the important
features included positive negative sentiment of reviews  number
of reviews  location  whether a restaurant takes reservations  and
cluster size  which represents the number of businesses in the
surrounding neighborhood   however  sentiment seemed to have
the largest predictive power  thus  in order to improve business
performance  a future step would be to conduct additional
analysis of review text to determine new features that might
help the business to achieve a higher number of positive reviews 

i  i ntroduction
yelp  founded in       is a multinational corporation that
publishes crowd sourced online reviews on local businesses 
as of       yelp com had    million reviews and     million
monthly visitors      a portion of their large dataset is available on the yelp dataset challenge homepage  which includes
data on        businesses          users  and          
reviews from the cities of phoenix  las vegas  madison  waterloo  and edinburgh      for businesses  the dataset includes
business name  neighborhood  city  latitude and longitude 
average review rating  number of reviews  and categories such
as good for lunch  the dataset also includes review text and
rating 
our goal was to analyze what factors may affect the
performance of a business on yelp  specifically  we wanted to
investigate the effect of location and other business attributes
vs  quality of food and service  measured indirectly through
review sentiment  on the business rating  in this paper  we
investigate several models to determine which factors have
the greatest effect on a business yelp rating  which can then
be used to help them improve their yelp rating  to increase

the tractability of the problem at hand  we limit ourselves to
data from businesses and reviews in phoenix  az 
to determine potential features of interest  we explore
clustering of business location  as represented by latitude and
longitude   as well as sentiment analysis of reviews  we then
use methods of feature selection to determine which features
best predict business performance  as represented by star
rating  finally  we implement and evaluate different prediction
models 
ii  data
the dataset we used is available on the yelp dataset
challenge homepage  we limited ourselves to businesses and
reviews from phoenix  az  which included       business
records and         reviews  business and review data were
available in two separate files  each with a single object type 
one json object per line 
the business records had the following format 
business
 
type  business 
business id   encrypted business id  
name   business name  
neighborhoods    hood names   
full address   localized address  
city   city  
state   state  
latitude  latitude 
longitude  longitude 
stars   star rating  rounded to halfstars  
review count  review count 
categories    localized category names  
open  true   false  corresponds to
closed  not business hours  
hours   
 day of week    
open   hh mm  
close   hh mm 
  
   
  
attributes   
 attribute name    attribute value  
   
  

fi 

the reviews had the format 
review
 
type  review 
business id   encrypted business id  
user id   encrypted user id  
stars   star rating  rounded to halfstars  
text   review text  
date   date  formatted like
            
votes    vote type    count   
 

we imported these json files into python arrays using
numpy     for our analysis 
iii  f eature g eneration
a number of the features we evaluated came directly from
the yelp dataset  such as the number of reviews the business
has  specifically  the ones we chose were 
 latitude
 longitude
 review count
 price range
 accepts credit cards
 has take out
 has delivery
 is wheelchair accessible
 good for lunch
 good for dinner
 good for brunch
 good for breakfast
 takes reservations
in addition to these  we generated additional features using
clustering and sentiment analysis to determine if there were
other features that may be useful in predicting business performance 
a  k means clustering for location
k means clustering can be used to create additional labels
for each business which correlate to its location in a city 
for example  clustering could reveal downtown locations 
shopping malls  and other popular gathering places  this gives
additional information not apparent from just latitude and
longitude  such as the number of nearby businesses  determined by cluster size   this also makes it easier to implement
as a feature when compared to the business neighborhood
 formatted as a string   we ran the clustering algorithm for
k       which returned results which seem to overlap with
some neighborhoods  a visualization of the clustering results
is shown in figure   
the features we used from k means are the cluster labels
and sizes  which represent the neighborhood and the size of
the neighborhood  respectively 

fig     results from k means clustering on latitude and longitude for
businesses in phoenix  az with k   

b  naive bayes classifier for review sentiment analysis
we trained a naive bayes classifier to classify business
reviews as either positive or negative  we used        reviews
from the yelp dataset  with     of the data as the training set
and     held out as a development set 
in order to make this a binary classification problem  we
defined the negative label to be star rating less than or equal
to   and positive label to be star rating greater than    reviews
are labeled with a     star rating   we began with a simple
bag of words as our feature set  each review was processed
into a set of indicator variables for each word in the review 
this simple bag of words did not produce a very accurate
model  accuracy of         although the overall bag of words
introduces a huge number of features  we saw that the most
important features were actually very few in number 
most informative features
horrible
awful
disappointing
waited
worst
disgusting
terrible
wrong
worst
overpriced

neg pos
      
      
      
      
      
      
      
      
      
      

ratio
   
   
   
   
   
   
   
   
   
   

we tried to address this problem of having too many uninformative features by only using the top performing features 
this made a significant difference in our results  the table
below summarizes the accuracies achieved by culling the
number of features down to the n most important 

fi  feat 
all
     
    
   
   

accuracy
    
    
    
    
    

pos prec   recall
          
          
          
          
          

neg prec   recall
          
          
          
          
          

of features  recursive feature elimination was implemented for
svm  logistic regression  naive bayes  and gda  due to time
constraints  we were not able to implement our own version
of recursive feature elimination for regression algorithms 
c  tree based feature selection

note that when using all the word features we classify too
many reviews as negative  high negative recall  low negative
precision   but by culling the number of features we get
better overall classification because we dont classify negative
reviews as aggressively  lower negative recall  higher negative
precision and higher positive recall  
next we added bigram features with the intuition being
words like awful and great might be negated in cases
like awful good or not great  this actually decreased our
classifier accuracy to        our hypothesis is that adding a
huge number of new bigram features diluted the predictive
power of the unigram features 
we used the trained unigram classifier with     features
       accuracy  to generate a sentiment feature for the
businesses in our training  development  and test sets  this
was done by computing the ratio of positive reviews to
total reviews for each business  this sentiment feature is our
indirect way of measuring a businesses food and customer
service quality  since most of our sentiment features deal with
qualitative factors 

we used a tree based estimator to compute feature importances  specifically  we used the extra trees classifier from
scikit      which fits ten randomized decision trees on subsamples of the dataset and averages them to improve accuracy
and reduce over fitting  we used the default gini criterion
to evaluate splits and selected those features with the highest
importance score 

c  data pre processing

keeping all input features  we experimented using scikits
svm     with linear  polynomial and gaussian kernels  our
results are shown in table i 

before running our feature selection algorithm  we scaled
the data to have zero mean and unit variance  to handle
missing data  we performed imputation using scikits imputer 
we selected the mean strategy after observing that none of the
mean  median  and most frequent strategies significantly outperformed the others  categorical variables were represented
by   for true and   for false  it is worth noting that for the
multinomial naive bayes classifier  the data is unscaled since
the scikit implementation required positive valued features 

v  p rediction m odels
we implemented the following models using scikit libraries 
support vector machines  svm   logistic regression  multinomial naive bayes  gaussian discriminant analysis  gda  
decision trees and random forest classifiers  linear regression
with regularization and support vector regression  svr   most
algorithms were implemented with the defaults from scikit 
which can be found in their user guide      however  we investigated the types of kernels for svm and the regularization
parameters for logistic and linear regression 
a  support vector machine

kernel
linear
gaussian
polynomial  d    
polynomial  d    
polynomial  d    

training accuracy
     
     
     
     
     

test accuracy
     
     
     
     
     

iv  f eature s election

table i
c lassification results with regularization for logistic
regression  

after generating features  we explored various methods of
feature selection to determine which features might be most
useful in predicting business performance 

based on these results  we used linear kernels moving
forward 

a  univariate feature selection
univariate feature selection works by conducting various
univariate statistical tests on the feature set  then selecting the
features that performed best      we used the anova f value
as the scoring function for the feature set  then selected the
top two features with the highest score since the f values for
these features were significantly higher than the rest 

b  multinomial logistic regression
we implemented multinomial logistic regression     with
regularization using the entire feature set  scikit implements
regularization using parameter c  which is inversely proportional to the strength of regularization 
while regularization did have an effect on testing accuracy 
it was essentially negligible  so we used c       

b  recursive feature elimination
we used scikits recursive feature elimination implementation      which assigns weights to each feature and prunes the
feature with the smallest weight at each iteration  this was performed in a cross validation loop to find the optimal number

c  multinomial naive bayes
we used scikits implementation of a multinomial naive
bayes classifier      with a laplace smoothing parameter of
       

fic
   
   
   
   
   

training accuracy
      
      
      
      
      

test accuracy
      
      
      
      
      

table ii
c lassification results with regularization for logistic
regression  

d  gaussian discriminant analysis
we used the lda classifier available with scikit      for our
  class classification problem with all of the default settings 
e  decision trees and random forest classifier
for decision trees and the random forest classifier  we used
the scikit implementation with the gini criteria to pick the
best split            our random forest classifier averaged over
ten trees 
f  linear regression with regularization
using the ridge regression implementation in scikit      
we implemented linear regression with l  regularization 
parameterized by   once again  we saw little effect of the
regularization on the accuracies  table iii  

   
   
   
   
   

training accuracy
      
      
      
      
      

test accuracy
      
      
      
      
      

table iii
p rediction results with regularization for linear regression  

thus  in our final model  we selected the default parameter
of        

model
svm
logistic regression
naive bayes
gda
decision tree
random forest
linear regression w  regularization
svr

training accuracy
      
      
      
      
      
      
      
      

test accuracy
      
      
      
      
      
      
      
      

table iv
p rediction results using the entire feature set 

model
svm
logistic regression
naive bayes
gda

training accuracy
      
      
      
      

test accuracy
      
      
      
      

table v
p rediction results with recursive feature selection  

model
svm
logistic regression
naive bayes
gda
decision tree
random forest
linear regression w  regularization
svr

training accuracy
      
      
      
      
      
      
      
      

test accuracy
      
      
      
      
      
      
      
      

table vi
p rediction results with univariate feature selection  

model
svm
logistic regression
naive bayes
gda
decision tree
random forest
linear regression w  regularization
svr

training accuracy
      
      
      
      
      
      
      
      

test accuracy
      
      
      
      
      
      
      
      

table vii
p rediction results with tree   based feature selection  

g  support vector regression
similarly to svm  we used the scikit implementation of
svr with a linear kernel      
vi  r esults
a  prediction results
when making predictions using classification models  we
floored the business rating         z as part of the preprocessing  when using a regression model  we floored the
business rating only when comparing the prediction to the
actual rating  for all models  we implemented cross validation
over ten iterations  withholding     of the data as the test set 
in tables ivvii  we present our prediction results using
the entire feature set  recursive feature selection  univariate
feature selection  and tree based feature selection 

b  feature selection results
this section details the most important features selected 
recursive feature selection with logistic regression selected
the following eight features  sentiment  reservations  latitude 
longitude  review count  good for lunch  good for dinner 
and cluster size  the accuracy versus number of features
selected is shown in fig     meanwhile  univariate feature
selection determines that the features in table viii are the
most important  which is corroborated by tree based feature
selection in table ix 
vii  d iscussion
across feature selection techniques and multi class classification models  we observed an accuracy of       regres 

fiwhile others are clearly in the take out category  it is likely
that customers take this into account when reviewing these
businesses  as such  clustering can be used to determine
certain common categories of businesses  predictions can then
be made within each category for the yelp star rating  which
may improve the accuracy of predictions  conceivably  there
are competing features for certain businesses  as speed is
valued much more at a fast food option  whereas quality is
much more valued at a high end restaurant 
viii  c onclusion
fig     cross validation accuracy vs  number of features for recursive feature
selection using logistic regression 
feature
sentiment
no  of reviews
cluster size
reservations
longitude

anova f value
     
     
     
     
     

table viii
t he top   most important features according to univariate
feature selection using a svm with a linear kernel  

feature
sentiment
latitude
longitude
no  of reviews
cluster size

importance value
      
      
      
      
      

table ix
t he top   most important features according to tree   based
feature selection using a svm with a linear kernel  

feature selection methods found our sentiment classifier
to be an important feature  followed by location  number of
reviews  cluster size  and whether or not the restaurant takes
reservations  we observed accuracies of      across   class
classification and feature selection models  suggesting that
review sentiment had the most power in predicting business
rating on yelp 
ix  f uture w ork
future work will include investigating the generation of
new features based on review text and user data that may
hold predictive power  in particular  analyzing review text to
develop new features or stronger models for sentiment analysis
may be a promising direction  in addition to sentiment  the
review text and user data could also be analyzed to gather
other features about the business that can be improved  sorting
businesses into categories before running predictions may also
help to improve accuracy 
another future area of interest might be to pair yelp
data with sources of other data  like walkscore or weather
information for the location 
r eferences

sion techniques and multinomial naive bayes achieved lower
accuracy  for   class classification  where random chance
is        our accuracy results indicate that features such
as location  price range  and the option of take out have
significant predictive power  this means if a business improves
upon these features  they should be able to improve their
business rating 
as corroborated by multiple feature selection techniques 
the most important feature was the reviews sentiment  which
reflects upon the business quality of service  this ultimately
drives its rating on yelp  unsurprisingly  while other features
like location can help  a business looking to improve should
first focus on its service  as such  future work should involve
detailed sentiment analysis of review text  which is essentially
improving the feature set  additionally  the maximum test
accuracy achieved was        across all models  given that
a large variety of algorithms were used  new features are most
likely needed to further improve accuracy  future work should
not only improve the sentiment analysis but also examine other
possible features to use in rating prediction 
it should also be noted that not all businesses have the
same goals  there are restaurants that aim for michelin stars 

    yelp   wikipedia  online   available  http   en wikipedia org wiki yelp
    yelp
dataset
challenge
 online  
available 
http   www yelp com dataset challenge
    numpy  online   available  http   www numpy org 
    sklearn
select
percentile
 online  
available 
http   scikitlearn org stable modules generated sklearn feature selection selectpercentile html
    sklearn
rfecv
 online  
available 
http   scikitlearn org stable modules generated sklearn feature selection rfecv html
    sklearn
rfecv
 online  
available 
http   scikitlearn org stable modules generated sklearn ensemble extratreesclassifier html
    scikit
learn
user
guide
 online  
available 
http   scikitlearn org stable user guide html
    sklearn
svc
 online  
available 
http   scikitlearn org stable modules generated sklearn svm svc html
    sklearn logistic regression  online   available  http   scikitlearn org stable modules generated sklearn linear model logisticregression html
     sklearn multinomial naive bayes  online   available  http   scikitlearn org stable modules generated sklearn naive bayes multinomialnb html
     sklearn
lda
 online  
available 
http   scikitlearn org stable modules generated sklearn lda lda html
     sklearn decision tree classifier  online   available  http   scikitlearn org stable modules generated sklearn tree decisiontreeclassifier html
     sklearn random forest classifier  online   available  http   scikitlearn org stable modules generated sklearn ensemble randomforestclassifier html
     sklearn ridge regression  online   available  http   scikitlearn org stable modules generated sklearn linear model ridge html
     sklearn
svr
 online  
available 
http   scikitlearn org stable modules generated sklearn svm svr html

fi