better reading levels through machine learning
adam gall  aag     stanford edu  cs   
dec         

introduction
measuring the reading difficulty of a particular text is a common and salient problem in the
educational world  particularly with respect to new struggling readers  while common sense
measures exist for canonical texts  assigning an appropriate reading level metric to new resources
remains challenging  current systems have been widely criticized for misrepresenting the difficulty of
texts which causes frustration for students and educators alike  currently the most popular metric is
the lexile reading measure which is both proprietary and expensive  i aim to use machine learning to
reproduce the results of the lexile measure and hopefully improve it with the insights gained through
this exercise 

dataset
one major hurdle in constructing a data set for this project is obtaining texts that have been
rated by lexile  this is critical as the cost to have a new text rated is prohibitive  at the outset  i had a
set of     texts that were rated by lexile  these texts are approximately      words in length and
have lexile ranges from     to      
i intended to pad this set with additional texts from project gutenberg  an online collection of
public domain books and articles  however  i learned quickly through analysis that there are
insufficient texts available through project gutenberg that have lexile ratings to sufficiently pad my
dataset  thus deeming this source unhelpful  furthermore  i was unable to find another source of
large numbers of lexile rated texts and thus was left with my original     texts 

while my dataset is fairly
robust  it does have several issues 
first  lexile ranges from    l     l
but the vast majority of my texts range
from    l     l and clusters around
    and      so it is possible that i will
overfit this segment of the scale 
furthermore  my texts are
predominantly biographical
encyclopedia articles from a small
number of authors  these are also
potential sources of bias 

fifeatures and preprocessing
i have initially focused on four features  sentence length  paragraph length  word length and
difficulty of vocabulary  preprocessing for these features is relatively simple with the most effort going
into constructing a vocabulary list  i have chosen to use pythons nltk module for natural language
processing and have used it to assign part of speech and also to stem the words in my vocabulary list 
furthermore  i removed articles  conjugations and prepositions as they do not weigh heavily in any of
the open reading measure standards and are unlikely to provide much information  as one may
expect there are a large number of words that appear very infrequently in the data  i have ultimately
decided to include these as they may be difficult words that contribute significantly to the reading
level 
in order to determine vocabulary difficulty i cross referenced several standard state
vocabulary lists by grade level focusing on sight words  words that cannot be sounded out and
thus must be memorized   this left me with a list of      words rated by grade level       naturally 
this is a small sample of the        words that are present in the data  however  reading
comprehension relies heavily on understanding of vocabulary  thus any proxy that we may be able to
find for understanding should be useful 
from the outset i expected a strong correlation between sentence length and lexile score
while i assumed that vocabulary difficulty would be a harder feature to incorporate  this assumption is
supported by the data 
finally  i created   discrete classes for the lexile measures        to increase the universe of
available algorithms  i also believe that reading level is itself an imprecise concept and would
ultimately be better represented as a distribution rather than a concrete number  furthermore  the
lexile scale roughly corresponds to grade level which is how it is most commonly used referred to 

fimodels
i reserved     of my data for testing and utilized the remaining     to train 

locally weighted linear regression
because the correlation between sentence length and lexile score appeared strong in
my initial plots i had high hopes that adding the right feature would yield strong results for this model 
it follows that the most challenging aspect of this model was determining which features were likely
to provide additional information  i tried numerous combinations of my features and ultimately found
the strongest results came from using average sentence length as the only feature 

k means clustering
to implement this algorithm i started by separating the data into   clusters  one for
each discrete lexile score represented in the data   once this was complete i created a probability
distribution from the actual lexile scores of the documents in each cluster  finally  i assigned new
documents to the most likely lexile score for its given cluster  i attempted this approach using various
feature vectors from my full feature vector  to just words to just words for which i had a grade level
and was unable to achieve results that much better than a coin flip 
below is the probability density generated by using k means on the word vectors generated
from the raw texts  clearly  clustering in this fashion fails to generate definitive classifications 
however  it is interesting to note that the densities roughly center around     and    l as we would
expect from the word score scatter seen above 

class lexile

 

 

 

 

  

  

  

 

     

    

     

     

     

     

 

 

 

     

     

     

     

     

 

 

 

     

     

     

     

     

 

 

     

     

     

     

     

     

 

 

    

     

     

     

     

     

 

 

 

     

     

     

     

     

     

 

     

     

     

     

     

     

 

fisupport vector machine
svm provided the strongest results of any algorithm that i implemented  however 
even here we hardly performed better than the flip of a coin  one interesting observation that arose
from svm testing was that the algorithm performed similarly when given only the word frequency
features  this indicates that there is indeed information present there that i have been unable to
effectively describe in feature generation and furthermore gives me hope in my continued efforts 

results
below are the best results that i achieved with each model after exhaustively testing different
combinations of features  while my exact classifications are inconclusive it is worth noting that both
lwlr and svm performed very well at classifying each text within one lexile level  clearly  lexile is
using a feature that i have been unable to discover but we are able to recommend texts that are
approximately appropriate within one grade level with near certainty 
model

training error

within one class error

lwlr

     

    

svm

     

     

k means

     

     

future
i am very passionate about this subject and will continue to improve upon the work that i
have done in an attempt to create a better system for predicting reading difficulty  one thing that was
unable to sufficiently explore within the scope of this study is the effect that parts of speech 
conjugations  proper nouns  numbers and arguably objectionable words lend to reading level  these
features should be relatively simple to analyze using modern language processing tools  however  i
expect that the impact of such features will be difficult to quantify directly which is why i avoided
them for this project 
finally  i believe strongly that the best measure of reading level will incorporate textual
analysis to classify things like concept and tone that have traditionally eluded quantification  this was
outside of the scope of this study  yet i do not believe that we will have a truly effective system that
works for both students and educators until such features are incorporated 

fireferences
 dolch word list   wikipedia  n d  web     nov       
fry edward  dr  fry s spelling book levels      teacher created resources        print
 sight word lists   student resources  tarpey elementary school  n d  web     nov       
 sight words  grades k             pawnee cusd     simplified online communication system  n d 
web     nov       
u s  department of education  office of career  technical  and adult education  octae    dolch basic
sight word list   literacy information and communication system  lincs   n d  web     nov       
 wisd high frequency word lists by grade level   waxahachie independent school district  n d 
web     nov       
assessing the lexile framework  results of a panel meeting  national center for education
statistics  aug      
how lexiles harm students  mike mullen  october         
http   mikemullin blogspot com         how lexiles harm students html

fi