cs     project  using vector representations to augment sentiment
analysis training data
andrew mcleod   lucas peeters

abstract
while the accuracy of supervised sentiment classification algorithms has steadily increased in recent years  acquiring
new human labeled sentiment data remains expensive  in this paper  we explore the effectiveness of increasing the
training data set size for sentiment classification algorithms by adding unlabeled phrases whose sentiments are inferred
by their proximity to labeled training phrases  this study was carried out in the context of two softmax classifiers  each
trained on movie review phrases with sentiments labeled between    very negative  and    very positive   using pretrained vectors to represent individual words  we find that augmenting the training data of these softmax classifiers can
improve the classification accuracy of one of our models by up to     while the other neither improves nor deteriorates 

introduction

the stanford sentiment treebank  sst   a database of
movie review phrases  this database is comprised of
       full sentences parsed into a total of        
unique phrases  all of which have been annotated by three
human judges with a sentiment ranging from   to       
following previous work on this data set  we discretize
the sentiment into five evenly spaced bins  where   is very
negative and   is very positive  the data is split into      
training        development  and       test sentences
however  due to the fact that some phrases are found in
multiple sentences  individual phrases can be found in
one  two  or all three splits  for the same reason  some
phrases are repeated within the same split  accordingly 
accuracies can either be reported on the full  redundant 
splits  or on just the unique phrases occurring in them 
leading to very different reported numbers  unless otherwise noted  the accuracies reported in this paper were
computed using the redundant splits so that direct comparison could be made with the results reported in     

sentiment analysis is an active field of machine learning
research in which the goal is to find the opinion or emotion expressed by a text with regard to a specific entity
     this analysis can be carried out at the level of individual words  whereby one associates a sentiment to each
word separately  or at the level of full sentences or even
texts  methods aimed at performing the task of assigning
sentiments to full sentences will then commonly contain
two key components  a way to represent and predict the
sentiments of individual words  and a way to represent
and predict the sentiments of phrases using these word
representations 
in this project  we set out to improve the predictions
made by these types of sentiment classifiers in situations
where the amount of labeled sentiment data is limited 
this is attempted by representing words and phrases as
vectors and using the notion of distance in these vector
spaces to infer the sentiments of the nearest unlabeled
neighbors of the training phrases  these newly labeled
phrases can thereby be added to the original training data 
we tested this idea on two classifiersone in which the
vectors representing phrases were constructed by averaging the vectors corresponding the phrases constituent
words  and another in which these word vectors were concatenated 

we chose to represent the words within these phrases
using vectors trained by glove      an unsupervised
learning algorithm for obtaining such vectors  glove
is able to capture significant semantic and linguistic
relationshipsfor instance  the difference vector between
man and woman is found to be approximately parallel to the difference vector between king and queen
making it well suited to our task  the vectors we
used were pulled from a publicly available set of fiftydimensional vectors trained on wikipedia articles 

data
both of our classifiers were trained and tested on



ajmcleod stanford edu
lpeeters stanford edu

 

fimethods

also carried out using adagrad 

the softmax classifiers

word vector averaging model

both of our classifiers are trained using a softmax training
algorithm  denoting our phrase  or word  vectors as xk  
we optimize l for each sentiment bin l                 
by maximizing our objective function

our first classifier constructs phrase vectors by averaging
over all word vectors in a phrase  phrases are thereby represented in the same fifty dimensional space as individual
words  and both phrases and words are used to train a single set of model parameters l   word vector backpropagation is only carried out when training on words  not on
phrases   and words that are found in the sst but not in
the glove database are initialized to a random unit vector  training was carried out both on the full sst training set  and on restricted training sets with sizes logarithmically distributed between   and       when training
on these restricted training sets  we augment the training
data with the k nearest neighbors of each training phrase
 pulled from the remainder of the training set   labeling
these neighbors with the same sentiment as the original
training phrase 

j      




exp  l  xk  
  xx
  sk   l  log p
m
m exp  m  xk  
k

l

     

   

where the k sum runs over all training phrases  and sk
is the known sentiment for each training phrase xk   the
regularization term     is proportional to the square of
the euclidean norm of   defined as the vector formed by
concatenating the five l vectors  we carried out this optimization using the adaptive gradient algorithm  adagrad       which reduces the step size of gradient descent for a given parameter the more that parameter is
updated  convergence is assumed to be attained when the
euclidean norm of the change in  after one batch step is
smaller than a user defined fraction of the norm of  itself  once our l are trained  the predicted sentiment sk
of the vector xk is
sk   argmax exp  l  xk  

word vector concatenation model
conversely  our second classifier represents phrases of
different length in different spaces by setting phrase vectors equal to the concatenation of their constituent word
vectors  due to the different resultant vector lengths  a
separate set of softmax parameters l is trained on the
phrases of each length  this is done by first training the
phrase length one softmax using word vector backpropagation  and then computing the k nearest neighbors of
all word vectors in our current  possibly restricted  training set  where k may be     a softmax is then trained on
each set of phrases with length greater than one as well
as their nearest neighbors  where the nearest neighbors of
a phrase are constructed by replacing each word in the
phrase with its k nearest neighbors  never replacing more
than one word at a time   this gives us a  k  l  fold
increase in our training set size for phrases of length l 

   

l

where l again runs over our five sentiment bins 
while the pre trained glove vectors already capture
some of the semantic relations between the words occurring in the sst  they are not optimized for this particular task  thus  we initialize our word vectors to their
pre trained glove values  after normalization   but then
adjust them through backpropagation  that is  we maximize our objective function with respect to these word
vectors as well as   thereby training both sets of vectors
simultaneously  these word vector optimizations were
accuracy
   

   

   

   

   

                                                                                                                                

phrase length

figure    comparison of the accuracy achieved by our vector averaging and vector concatenation approaches as a function of phrase length
 without adding nearest neighbors   green represents the vector averaging model  blue the vector concatenation model  and dark turquoise their
overlap 

 

fibase model performance

nearest neighbor results

before testing the effect of augmenting our training data 
we ran both classifiers on the full sst training data to
compare our baseline results to previously reported accuracies achieved on this data set  this comparison is found
in table    although neither of our models is able to
match the best classifier reported in      our concatenated
vector model did outperform all non neural net models
reported therein  including naive bayes and svms   figure   compares the vector averaged and concatenated vector models for each phrase length separately  we see there
that the concatenated vector model mainly attains higher
accuracy on the phrases of length    to     for very long
phrase length  beyond      the number of training phrases
is limited and thus the statistics are not very reliable 

the effects of adding nearest neighbors to the training set
of our two classifiers was tested by running a large number of trials with variable training set sizes and number
of nearest neighbors  the restricted training set was randomly selected from the sst training phrases for each
trial  and the phrases within it were required to be unique 

model
vector averaged
concatenated vector
recursive neural tensor network

word vector averaging model
the accuracy achieved by our vector averaging model as
a function of training set size is shown for       and   
nearest neighbors in figure    gains of approximately
   are achieved by adding   nearest neighbors  but only
marginal improvements are made by going from   to
   nearest neighbors  note that these accuracies were
achieved by testing on only the unique phrases in the relevant data split  not on the full  redundant  set  the      
reported in figure    on the redundant set  corresponds to
      on the set of unique phrases  the relative accuracy
for   through    nearest neighbors is also plotted in figure    we see there that our classifiers with augmented
training sets do up to    better than our baseline  k     
model for training sets with     or fewer training examples  but that this improvement is mitigated to below   
once training set sizes grow to around      

accuracy
     
     
     

table    the accuracy achieved by our two classifiers when
trained on the full sst training set without adding nearest
neighbors  compared to the highest accuracy reported in     
which was attained using a recursive neural tensor network 
 

  
k  
k  
k   

accuracy  in   

  
  
  
  
  

  

   
   
training set size

   

    

figure    accuracy attained by the vector averaged model on the unique phrases in the relevant data split as a function of training set size  for
k        and    nearest neighbors  the plotted accuracies represent values averaged over     trials  and the error bars denote the standard error of
the mean of these trials  note that the obtained accuracies are obtained using unique phrases  which severely influences the accuracy  as discussed
in the text 

 

fik     k  
k     k  
k     k  
k     k  
k     k  
k     k  
k     k  
k     k  
k     k  
k      k  

relative gain in accuracy  in   

 
 
 
 
 
  

  

   
   
training set size

   

    

figure    relative gain in accuracy as a function of training set size for k     through k       plotted values were averaged over     runs  and
the error bars denote the standard error of the mean 

word vector concatenation model

set of five softmax classifiers to predict the sentiments of
nearest neighborsone for each sentiment label  of the
labeled training phrase   the input used to train these
classifiers was a concatenation of the original training
vector and the neighboring vector  training these classifiers on all phrases in the sst training split using the
five nearest neighbors of each phrase  and testing on the
five nearest neighbors of phrases in the sst development
split  we achieve an overall nearest neighbor classification accuracy of     

a similar set of trials was run for our vector concatenation model  however  unlike the vector averaged model 
no statistically significant increase or decrease in accuracy was observed 

improving the sentiment classification of nearest neighbors
by adding the k nearest neighbors to our training sets in
the above manner  we increase our training set size by a
factor of k  or more  in the concatenated model   the reason our gains remain limited despite this k fold increase
is that phrases are only loosely clustered by sentiment 
assigning the nearest neighbor of each phrase in the sst
training set  using a euclidean metric  only achieves an
accuracy of        to improve on our above results  we
therefore need to improve our ability to predict nearest
neighbor sentiments  our first attempt to do this was to
compare the confusion matrices of nearest neighbor assignments for different metrics on this space  some of
the results of this comparison are found in table    given
the accuracy achieved by each metric for each sentiment
label  we can maximize our overall accuracy by choosing the best performing metric as a function of training
phrase sentiment  however  doing this only boosts the
overall nearest neighbor sentiment prediction accuracy to
       a second approach consists of training a new

sentiment

n
ln
 

n
ln
 

n
ln


 
 
 
 
 

     
     
     
     
     

     
     
     
     
     

     
     
     
     
     

training set
fraction
      
     
     
     
     

table    the accuracies achieved by the l    l    and l norms
when assigning the same sentiment to the nearest neighbor of vectors in the sst training set  these accuracies can be compared to the
fraction of the training set associated with each sentiment label 

future work
there are a few directions in which this work can be
taken  a new set of trials should be run using our recently
developed nearest neighbor sentiment classifiers in conjunction with our vector averaged model to see if perfor 

finot find that this result extended to our vector concatenation method  at least for the types of nearest neighbors constructed in this model  without adding nearest
neighbors  our concatenation model outperforms all reported methods on this dataset except for neural network
methods  notably the recursive neural tensor network 
which achieves             whether or not it would be
possible to increase the accuracy of neural network methods by augmenting their training data with nearest neighbors remains a question for future work 

mance improves  we can also try adding different types
of nearest neighbors to our concatenated vector model 
for instance by selecting nearest neighbor phrases from
the remainder of the sst training data like is done in the
vector averaged model  finally  this line of research into
augmented training data sets can be continued on to neural net models  which will increase the overall accuracy
of our classifiers 

conclusions
we have shown that  for small training set sizes  the use of
nearest neighbor vectors allows for an increase in phrase
sentiment classification accuracy by several percent with
respect to our baseline vector averaging method  we did

acknowledgments
we would like to acknowledge richard socher for advising us on this project 

references and notes
    j  duchi et al   adaptive subgradient methods for online learning and stochastic optimization  jmlr     p                  
    r  feldman  comm  acm         p               
    r  socher et al   reasoning with neural tensor networks for knowledge base completion  adv  nips          
    j  pennington et al   glove  global vectors for word representation  proc  emnlp       

 

fi