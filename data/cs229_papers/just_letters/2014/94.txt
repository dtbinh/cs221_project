improving taxi revenue with reinforcement learning
jingshu wang  and benjamin lampert 

abstract in recent news there has been controversy surrounding uber and lyft taking business away from taxi drivers 
in light of this controversy  we explored how machine learning
could optimize a taxi drivers income by understanding the
flow of demand throughout the day  we first used historical
new york city  nyc  transaction data from      to build
summary statistics of the demand  fare  duration  and transition
probabilities for different zipcodes and times across nyc 
we then applied a reinforcement learning model to maximize
the total revenue generated by an individual driver  utilizing
dynamic programming we backtested our model on   k drivers
and found that if they followed an optimal policy they would
earn on average     more per day than following a nonoptimal policy 

i  introduction
with the emergence of traditional taxi competitors such
as lyft and uber  many new york taxi cab drivers are concerned with unfair competition  these companies leverage
analytics  mobile apps  and other data sources to adjust rates
based on a wide set of features  uber  for example  has a team
dedicated to analyzing demand and usage across cities    
providing similar services to normal drivers therefore would
be an important step to allow them to compete  our approach to this problem was first to analyze and preprocess
a collection of      taxi records  we used a linear model
to understand which features best predict demand  and a
reinforcement learning model to maximize an individual
drivers income based on a set of summary statistics 
ii  data
a  nyc transactions
the dataset we used is from the      nyc taxi records
made available by a freedom of information act and first
published by chris whong     the collection contains all
    million taxi transactions for       where each entry
details the relevant information for each taxi trip  a summary
of these features can be found in table i 
label
pickup datetime
dropoff datetime
pickup longitude
pickup latitude
dropoff longitude
dropoff latitude
total amount

example
                   pm
                   pm
          
        
          
         
  

table i
e xample of some of the features contained in the raw data  

  w 
  b 

jingshu jingshuw stanford edu
lampert lampertb stanford edu

fig     plot of each pickup location overlaid on a map of new york city 
each dot represents a transaction during a single day in january  points of
activity include manhattan  left middle  and the jfk airport  lower right  

to handle the computational difficulties in training on
large collections  we subsampled our data  we only examined
transactions from january which resulted in a dataset of     
million trips  for linear regression  we sampled and fit only
     of the data  for reinforcement learning  we trained our
model using the full dataset  but only backtested on sampled
drivers from wednesday records 
b  preprocessing
to improve our location features we decided to bin the raw
gps coordinates of the pickup and drop off location into a
zipcode  this was accomplished using the python package
geodis     we also applied a filter at this point to exclude
all data not within nyc  ie  not reported correctly or did not
successfully reverse geocode  
the next goal was to build a feature set that would allow
us to train a policy on the data  we first iterated through all
of the records to build a set of summary statistics related
to the fare information  these data are grouped by the key
 hour  pickupzipcode  dropoffzipcode   an example of one
field from the summary data is as follows 
the next step was to get an estimate for the searching time
for the next customer  because our dataset only observes
the time between pickup and drop offs  we lack direct
information related to how long the driver searched in an
area to pick up a new customer 
to infer this data  we looked at drivers who picked up
new customers in the same zipcode as where they had just
dropped off a customer  this provided a time delta associated

fistatistic
hour
pickupzipcode
dropoffzipcode
number of transistions
mean trip distance
mean trip duration
mean fare
transition probability

value
  am
     
     
   
     miles
    minutes
     
     

table ii
e xample entry in the summary statistic data for picking up a
passenger at zipcode        who is traveling to       
between

      am on w ednesday 

with how long they were searching between customers 
we then modeled the searching time as an exponential
distribution to get the expected waiting time for each zipcode
and hour  more details will be found in section iii 
with these summary statistics we were then able to apply
our model 
iii  m odel

in a traditional reinforcement learning model the intention
of the driver would be an important feature  in practice 
although a driver may intend to drive to a zipcode z  they can
pick up customer along the way  however  its impossible to
know a drivers intention from historical data as we only
know where he ends up picking the next customer  thus 
to reduce the difficulty in estimating the model parameters
from historical data  we consider a simplified policy 
 policy   z  t    z  the next pickup zipcode from state
 z  t   the driver goes to z  directly and will keep
searching inside z  until he picks up the next customer
at z 
then  we can write out the value function as 
v   z  t 
  e f   z  t   z     tpick     v   z     tnext   
 
 
x
 
 
  
  e
 p  z    z   tpick  f  z    z   tpick     v  z   tnext   
z 

 

 

  e r z    tpick    

yijk

     i   j   k     ij     ik     jk  
p
x
as xs   ijk
s  

where i are the main effects for zipcodes  j are the main
effects for monday  tuesday       sunday  and k are the  
hour intervals for a day   x    x         xp   are the additional
predictors of weather information 
b  reinforcement learning
   framework  when a taxi driver drops off a customer 
and is looking for new business  there are two actions they
can make  either they can stay in the area and wait for
passenger there  or travel to a new location  travelling to a
new location has an associated cost  but if chosen correctly
the new location will impact future trips the revenue for
the day  we find that a reinforcement learning model comes
naturally to quantify the above optimization procedure  using
zipcodes as geographical units  we can define the states 
reward and value functions as 
 states   z  t  a dropoff zipcode at a corresponding time
 reward  r z    t    average trip fair of one trip for a
pickup zipcode z  at time t 
 value function  v  z  t  expected total revenue to the
end of the day starting from state  s  t 

 

 



p  z    z   tpick  v  z   tnext  

z 

a  linear regression
to find the relationship between taxi demand and various
factors  geographical locations  time and date  weather etc   
we use a linear regression model to decompose the sources
of variances  for the response  we aggregated the number of
trips every three hours for each zipcode  for predictors  we
consider zipcodes  the hours in a day  and the days in a week
as three major factors along with their two term interactions 
we also include weather information during the   hour time
interval as additional predictors  the model is 

x

where z     tpick and tnext represent the random zipcode
that the customer at z  wants to go  the random pickup
time at z  and the random dropoff time at z   respectively 
f  z    z     tpick   is the trip fare from z  to z   at time t and
p  z    z     t  is a customers transition probability from z  to
z     the above equations hold after assuming that all the
random variables are independent  to further quantify tpick
and tnext   we have
tpick

 

t   travel  z  z      search  z   

tnext

 

tpick   trip  z    z    

where travel   search and trip are random time intervals
for the time cost traveling from z to z  without a passenger 
searching for the next customer at z    and driving a passenger from z  to z   respectively  finally  to approximate the
expectation in      we replace all the random time with their
expectations  as the value function is almost linear in t and
our estimates of r z    t    and p  z    z     t  will be piecewise
constant  described later  in hour  this approximation should
be accurate enough most of the time  we then define the
optimal value function as 
v    z  t    max v   z  t 


then  it will satisfy 
v    z  t 
 
 

max r z    tpick    
z 

 
x

 

 

 

p  z    z   tpick  v  z   tnext  

z 

where
tpick

  t   travel  z  z    t    search  z    t   travel  z  z    t  

tnext

  tpick   trip  z    z     tpick  

fi   estimating model parameters from historical data 
to estimate the model parameters  we bin the data by days
in a week and hours in a day  for r z  t   p  z    z     t 
and trip  z    z     t   we simply take from historical data the
average of the trip fare  the transit frequency and trip time
inside the bin that t falls into  for travel   we estimate it by 
travel  z  z    t 

z   

search  z    t   

 

 
 z    t   

the exponential distribution has a nice property  if we can
observe a lower quantile of search   p  search   x    p 
then
 ln    p 
 
x
for the historical data  if the actual searching time was short 
its likely that the driver would still be in the same zipcode
when he picks up the next customer  so we roughly observe a
lower quantile of search   and we can determine a reasonable
x as either the median or     depending on how large they
next
from data in the
are  of duration times for zdropoff   zpickup
corresponding time bin  to estimate p  we assume that drivers
are randomly searching  so if x is not too large  the drivers
would still be in zipcodes close to z  and the searching time
should be similar  thus 
  tnext
pickup  tdropoff   x  zdropoff   z   
  zdropoff   z   

   updating model parameters from future data  updating our model parameters with online data can be accomplished by recalculating the summary statistics based on the
new information  obtaining feedback on a drivers intention
would be an important new feature  assuming that drivers
follow our policy to maximize their revenue  we will know
both where they intend to go and where they actually pick
up the next customer  for a policy  z  t    z    instead of
saying that the driver must pick up the next customer at
z    we can estimate the transitions probabilities from z to
a pickup zipcode z   for policy  z    and the new value
function will be

z 

 
 p  z      z     tpick  f  z      z     tpick     v   z     tnext   

  trip  z  z    t   trip  z    z    t 

as the driver will start searching as soon as he arrives at any
point in z    finally  we need to estimate search   for this
we need to consider that drivers may drive away to other
zipcodes as they search for new passengers  our prediction
of search  z    t    is defined as the expected searching time
if the driver keeps searching in z  until he picks the next
customer  thus  simpily taking the average time duration
next
of zdropoff   zpickup
to estimate search will be a severally
downward biased estimate 
assume instead that search follows an exponential distribution exp   which is a common distribution for waiting
time  then

p z    t   

v   z  t 
 
x
x
  e
p  z   z  z      t 

notice that p  is the transition probability for the drivers
while p is the transition probability for the customers 
iv  outcome
a  understanding the demand
the linear regression has r          the anova table
is 
pickupzip
day 
hour
tempf
windspeedmiles
weatherdesc
factor winddir  point 
precipmm
humidity
cloudcover
heatindexf
pickupzip day 
pickupzip hour
day  hour
residuals

df
  
 
 
 
 
  
  
 
 
 
 
   
   
  
     

sum sq
       
     
       
    
    
     
     
    
    
    
    
      
       
      
        

mean sq
      
     
      
    
    
    
    
    
    
    
    
    
    
    
    

f value
      
     
      
    
    
    
    
    
    
    
    
    
    
     

pr  f 
      
      
      
      
      
      
      
      
      
      
      
      
      
      

table iii
a nalysis of variance   p ickup z ip is the zipcode factor   day   is
the days in a week factor and hour is the factor for the
three   hour intervals  

other factors are interactions and

weather information  

some of the main effects and interaction are shown in fig 
  
b  backtesting the optimal policy
the result of the reinforcement learning model is an
optimal policy that details which zipcode mazimizes a drivers
earning potential for the day  if a driver is looking for a
new customer  the inputs of the policy would be his current
zipcode and the time of day  the output would be the zipcode
where he should look for a new passenger to maximize his
revenue  fig    illustrates what such a policy looks like 
to back test our model we used transaction records to
follow drivers for their shift  we compared their revenue for
the day versus what they would have earned if they had used
our policy  by using dynamic programming  we were able to
back test over   k drivers and record their performance with
and without following our policy decision  fig    shows the
performance of     representative drivers 

fiweather

day

   



hour














   

    

 









   



   
    






   

   


    









  
am
      a
am m
        
am a
m
         
 a a
m m
      p  
pm m
      p  
pm m
        
pm p
m
         
 p p
m m 
  
am
 

   

m
on
d
tu ay
w esd
ed
a
ne y
s
th day
ur
sd
a
fr y
i
sa day
tu
rd
su ay
nd
ay

c
clolear
ud
y
ic
e
lig p fog
ht elle
dr ts
iz
m
zle
od
er m
a is
o te t
pa par ve rain
tc tly rca
hy c s
lig lou t
ht dy
sn
su ow
nn
y



 a 

 b 

 c 

dayhour interaction
sunday

   

  
  

saturday
friday

  

thursday

  

wednesday

  

  

  

effect

   
   

   

fig     following the optimal policy from our model produced higher
revenue than if a driver had used his own strategy  this result holds for a
variety of work durations 

   

   

  

   

tuesday

   

  

v  conclusions

  

monday

  a

m 

  a
 a

m 

m 

  a
 a

m 

m 

   

  p

am

  

m 

  

am

pm

 

 

  p
 p

m 

m 

  p
 p

m 

m 

   

pm

  

  

pm

 

am

 

 d 

fig     individual effects  a   c  are the main effects of variables weatherdesc  day  and hour in table iii  for  b  and  c   the red dashed lines
show the significance thresholds based on adjusted p values  in  d  the color
represents the estimate of each interaction effect  a smile face represents a
significant positive effect and a sad face is a significant negative effect  all
pvalues have been adjusted using bonferroni to accommodate for multiple
testing 

applying a reinforcement learning model to the nyc taxi
data we were able to show that an optimal policy can be
obtained that maximizes the revenue generated by a single
driver  given a starting location and a time of the day our
model can direct drivers to the optimal zipcode to find a
customer  backtesting results show that if a drivers were to
follow our policy  they would earn     more revenue 
vi  future
there are finite number of passengers  and if all taxi
drivers used this policy then they would not all see the
same gains  optimizing the entire system  and coordinating
between all drivers would be a better approach and could
ultimately benefit all parties  taxi companies could allocate
their medallions more efficiently  drivers would minimize
their vacancy time and working hours  and passengers could
see better taxi coverage depending on peak hours locations 
another important piece of our model that is missing
from traditional reinforcement learning algorithms is the
feedback from making a decision  tracking the intention of
the driver to drive to a location  compared to where they
end up  is an important piece of information  if this were
to become a mobile app  then it might be possible to obtain
this information and improve our model 
r eferences
    http   blog uber com uberdata   uberdata uber blog  uber  n d  web 
   dec       
    whong  chris  foiling nycs taxi trip data  foiling nycs taxi
trip data  n p      mar        web     dec       
    https   github com doat geodis geodis  n p   n d  web     dec       

fig     example policy from our model based on starting the day at  am
and ending the day at  pm  each node represents a starting zipcode  and
the edges show the expected revenue for the day if the policy is taken 

fi