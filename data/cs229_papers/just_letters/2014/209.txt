improving positron emission tomography imaging with machine learning
david fan chung hsu  fcdh stanford edu   cs     fall        
   introduction and motivation
high  resolution positron emission tomography  pet  systems can image the specific biological
activity of malignant tumors  making it useful for cancer screening  in pet imaging  the patient is first
injected with a radioactively labeled molecule that serves as a contrast agent  the most common agent is
  
f fluorodeoxyglucose  fdg   an analog of the sugar glucose  which is rapidly taken in by cancer cells 
every   f atom that undergoes positron decay in the cell creates two high energy photons simultaneously 
that travel in opposite directions  when both of these photons in any given decay event are picked up by
the systems detectors  the positron decay is assumed to have occurred on the line joining these two
detectors  called the line of response  lor   over time  the lines that join
the detected high energy photon pairs accumulate in image space  and  by
solving a mathematical inverse problem  an image representing the
biodistribution of the contrast agent is reconstructed  the intersection of the
lines  which show up as bright spots in the image  help doctors determine
where the tumors are likely located in the body  we call these photon pairs
true pairs for the purpose of this project  the illustration on the right shows
an example of a true pair 

figure    true pair

a problem arises when photon pairs are misidentified  leading to the
event being placed on an incorrect detector response line  the largest cause
of misidentification is the low detector efficiency in the system  combined
with high decay rate  meaning that often   non correlated photons that
originated from different decays are detected  misidentification increases the
background in the images  leading to loss of tumor contrast and errors in its
intensity seen in the image  we call all misidentified pairings as false pairs
for this project  the illustration on the right shows a false pair  where the
tumor is thought to be located along the dotted line  because of   decays

figure    false pair

which occurred simultaneously 

conventional algorithms do not have an easy way of distinguishing false pairings from true
pairings in pet imaging  the primary goal of this project is to use machine learning to find structures
that are embedded in the data  which may act as predictors for false pairings versus true pairings  in
imaging statistics  the contrast to noise ratio  cnr  of the image is generally considered as the primary
metric which determines the detectability of a tumor when examined by a radiologist  in terms of
machine learning  contrast to noise ratio is represented by the proportion of false positives in the final
dataset used to reconstruct the image  by using logistic regression  nave bayes  and support vector
machines  i aim to classify true pairings and false pairings with a higher accuracy than current
algorithms  in order to improve the cnr of the final image set 

fi   simulation data acquisition and pre processing
the problem addressed by this project can be tackled by supervised learning  to achieve this  we
perform a simulation of our system using gate  the most accurate and comprehensive simulation
program for tomography based imaging systems  the simulation outputs photon pairings and the
ground truth of whether or not these are true or false pairings  with the ground truth  we can use
supervised learning to train our model  and perform testing using our model  in order to introduce
hardware related uncertainties  such as jitter or energy blurring into the data   we pre process the
simulation dataset with various programs that are intended to add in uncertainties in order to better
mimic real world data taken with our system  i ended up with       pair samples  and the breakdown
is quantified in the table below  we see that the ratio of false to true pairs is very small 
table    summary of simulation data
training

testing

number of samples

     

     

number of true pairs

     

     

number of false pairs

   

   

   features
the raw features used in the supervised learning originate from the gate simulation program 
there are   main features that are generated by the simulation with each pairing    cartesian
coordinates x   x   y   y   z   z   as well as two energy parameters e  and e   the positional
information is relevant in learning because typical pet scanners have sensitivity and efficiency
profiles that are not uniform over the entire spatial imaging space  this means that there is some
dependency of the position of the pairings  the energy of the pair is relevant  because false pairs can
be correlated with some loss of energy  during scattering events  testing using these raw features  both
continuous and discretized  did not yield good results  so a new set of position features were generated
by re mapping the positions  in order to better approximate the spatial position of the pair over the
field of view  the position coordinates are converted from   features into    representing  x  x     
 y  y      and  z  z      which are the centroids of x  y  and z positions of the pair of coordinates 
compared to the situation when a linear mapping of cartesian coordinates of both endpoints of the are
used  a centroid location gives much more intuitive information regarding where the response line that
is drawn between the two points of a pair will pass through in image space  the horizontal and vertical
angular information of the pair is also derived  using tan    y  y    x  x    and tan    z  z    x x     in continuous space  there are a total of   generated features used in our learning  discretization
of the feature space is performed to turn these   features into      binary valued bins for testing 
table    summary of the number of features
raw features

discretized raw

generated features

discretized generated

position

 

   

 

   

energy

 

   

 

   

fi   learning process and results
we chose to use logistic regression and support vector machines with both continuous valued
and discretized features  while we used nave bayes  using a multinomial event model  with the
discretized features only  logistic regression and nave bayes are both implemented in matlab
using code i wrote myself  while svm was performed using the liblinear library provided at the
link listed in the reference  since the algorithms we used are all unmodified from what was covered
in lecture  i will leave out the equations in this report as they are unnecessary 
one very important point to note for this project is that the usual training error and testing
error equations do not apply in a straightforward fashion for this project  for radiology images 
absolute magnitude in the image pixels do not convey any information  but rather it is the contrast in
the images which provides the information regarding tumor locations  therefore  the magnitude of the
error does not matter  but rather it is the ratio of errors  or false positives  to true positives that is
important  i define below the equation for contrast to noise ratio  cnr   which is typically used
as the primary figure of merit in evaluating imaging performance  to evaluate the performance of
various algorithms in this project  it should be noted that the cnr takes into account all   main
categories of classified data  true positives  tp   true negatives  tn   false positives  fp   and false
negatives  fn   therefore  an algorithm with a low fp rate that throws out a lot of tps by categorizing
them as fns  will have a worse cnr than one which has a higher fp rate but keeps many more of the
tps 
cnr  

        
   

the baseline for all comparisons is the conventional algorithm  the majority rules algorithm
which assumes that all pairs are true pairs  the extreme skew in the ratio of true pairs to false pairs
means that majority rules already yields a high percentage of accuracy in predicting true pairs  the
error percentage is calculated in each test case  which is the ratio of false positives to true positives 
looking at table    we can see that we can already achieve a baseline cnr of       with majority
rules  the percentage of improvement in cnr is defined as 
 cnr algorithm    
  cnr improvement  

    using raw features
the first step i tried was to simply use the learning algorithms on the raw features  both continuous
and discretized  the results are summarized in table     below  we can see the performance is
marginally better than baseline  but not by much 
    using generated features
using the generated features defined in section    we observe much better results  which are
summarized in the table     below  in all situations  we used    iterations for logistic regression  we
see that logistic regression of the discretized generated features offers the best improvement in cnr 

fitable      raw features learning summary
continuous

discretized

logistic reg 

svm

logistic reg 

nave bayes

svm

true positives

    

     

    

     

     

false positive

   

   

   

   

   

error  

    

    

    

    

    

  cnr improve

     

     

     

     

     

table      generated features learning summary
continuous

discretized

logistic reg 

svm

logistic reg 

nave bayes

svm

true positives

    

     

    

     

     

false positive

   

   

   

   

   

error  

    

    

    

    

    

  cnr improv 

     

   

     

     

     

logistic regression   cnr improvement over iterations
 
   

testing
training

 

  improvement in cnr

   discussion
to put the numbers in perspective  the
best result achieved        in cnr  means
the cnr increased from       to       
this means that the learning algorithm was
able to reduce the proportion of false pairs
over all data by almost half  from      down
to       i plotted the learning curve of the
best case logistic regression  highlighted in

   
 
   

green above in the table   and the learning
 
curve is shown on the right  we can see that
   
in all cases  the training curve remains above
the testing curve  meaning it performs better
 
 
 
  
  
  
  
 larger improvement   we also observe a
iterations
characteristic zig zagging during training
of the logistic regression model  which i have not been able to explain fully  while the general trend
of the learning curve is for the improvement to slowly increase with the number of iterations  it seems
that the contrast to noise ratio does not increase monotonically with the number of iterations  another
interesting observation is that  in all cases  it seems that svms perform the worse  i believe this might
be due to the high imbalance between true positives and true negatives  where the true negatives
constitute less than    of true positives  

  

fithe improvement offered by the re mapping of raw features to generate a new set of features was
very clear  i believe this improvement comes about because of the intrinsic nature of pet imaging  in
that the locations of the lines drawn through imaging space are more important than the positions of
the actual detectors  for example  one detector  by itself  offers no information whatsoever regarding
the position of the cancerous tumor  because pet requires two end points in order to draw a line
through space  therefore  it makes logical sense that  during learning processes  we can re map the
two end points to new features which represent the line connecting the two end points  in my
implementation  i chose to re map the end points to a location centroid and angles representing the
direction of the line  but there might be even better ways to map the end points 
   conclusion and future work
in conclusion  the work in this project resulted in an improvement of       in the contrast tonoise ratio in the pet dataset  equivalent to a     reduction of identification error  from      from
      we find that  using a generated feature set composing of energy  positional centroids  and angles
between detectors  along with appropriate discretization of feature space  we are able to achieve the
best results 
the next steps that i am planning on taking with this project is to extend the learning to both
unsupervised learning  as well as deep learning  i have written a grant application  using ideas derived
from the work presented here  to use deep learning on pet data  the fact that logistic regression
performed so well on this dataset is encouraging  since it points to the potential success of using logistic
activation functions in the deep learning algorithm  unsupervised learning would allow us to learn
from actual measurement data  these data contains all the effects of various uncertainties in the system 
but does not offer ground truth labeling of data  a successful work in unsupervised learning would
have the ability to boost the capabilities of pet systems all around the world without requiring
hardware upgrades 
   acknowledgements
the author would like to acknowledge the help of wenbo zhang in explaining the use of gate to
simulate pet systems 
   references
gate simulation software 
http   www opengatecollaboration org 
liblinear library for svm 
http   www csie ntu edu tw  cjlin liblinear 

fi