learning facial expressions from an image
bhrugurajsinh chudasama  chinmay duvedi  jithin parayil thomas
 bhrugu  cduvedi  jithinpt  stanford edu

   introduction
facial behavior is one of the most important cues for sensing human emotion and intentions
among people  as computing becomes more human centered  an automatic system for accurate
facial expression analysis is relevant in emerging fields such as interactive games  for instance 
the games played using microsoft kinect   online education  entertainment  autonomous
driving  analysis of viewer reaction to advertisements  etc  for example  the reactions of gamers
could be used as feedback to improve the gaming experience on systems like the microsoft
kinect  similarly  analysis of facial expressions of drivers would help in determining their stress
level and such information could be used to alert drivers if they are stressed and in a state
unsafe for driving  with these applications in mind  this report describes our attempt to learn
facial expressions from an image using deep learning  we also contrast our deep learning
approach with conventional shallow learning based approaches and show that a convolutional
neural network is far more effective at learning representations of facial expression data 
this report is organized as follows  section   describes the problem in more detail along with the
dataset used to train and test our algorithm  section   describes the image preprocessing and
feature extraction stages  section   describes the various learning models that were applied to
this problem and section   presents our results  we analyze and discuss our results in section  
and conclude with future plans in section   

   problem description and dataset

figure   example images from the dataset

any learning algorithm that uses features extracted from data will always be upper bounded in
its accuracy by the expressiveness of the features  as a result  a motivation for deep learning
based approaches is that learning algorithms can learn  or design features much better than
humans can  with this background  icml      hosted a facial expression recognition challenge

fibased on a new dataset assembled using images crawled from the internet      in this project
we work on the same problem  to classify an image of a human face into one of   expressions 
the data consists of   x   pixel grayscale images of faces  the seven categories into which the
images are to be classified based on facial expression are angry  disgust  fear  happy  sad 
surprise and neutral  the training set consists of        examples  the public test set used for
the leaderboard consists of       examples  the final test set  which was used to determine the
winner of the competition  consists of another       examples  in our project we used the
public test set for validation and the private test set to report test error rate 
the number of examples of each class in the dataset are as listed in table    some example
images from the dataset are shown in figure   
table    distribution of labels in training  validation and test sets

label
angry
disgust
fear
happy
sad
surprise
neutral

training examples
    
   
    
    
    
    
    

validation examples
   
  
   
   
   
   
   

test examples
   
  
   
   
   
   
   

   features and preprocessing
    features for shallow learning models
for our experiments  we used two sets of features  in one case  we provided eigenfaces
 described below  as inputs to svm and shallow neural networks  in the second set of
experiments  we tried improving the accuracy of the svm and shallow neural net models by
providing gabor filters as the input features  gabor filters are linear filters used for edge
detection in image processing and the frequency and orientation representations of gabor
filters are similar to those of the human visual system  they have been found to be particularly
appropriate for discrimination 
   eigenfaces   we mean adjusted the images  performed pca on these and then extracted
the top    principal components from them  once this was done  the training test
samples were then mapped to these principal component dimensions 
   gabor filters with adaboost   we used    different gabor filters    frequencies   
orientations   the frequency parameter determines the thickness of the detected edge
and the orientation determines the angle of the detected edge  this gave us         
features for each sample  in order to reduce the dimensionality of this feature space  we
ran adaboost feature reduction algorithm  with decisiontree classifier of depth one  on
the training set        the adaboost classifier assigned an importance value to each
feature and we selected only those features with an importance value greater than   
this helped reduce the number of features from          to around      

fi  

figure  gabor filter outputs for one image

figure   adaboost results applied to a       image 
darker pixels indicate higher importance 

    preprocessing for the deep learning model
we first subtract the mean of each image from each pixel in the image  next  for each pixel  the
mean value of the pixel across all images is subtracted from the pixel and it is divided by its
variance  this is the only preprocessing done on the data before it is fed to our convolutional
neural network  the convolutional neural network learns the appropriate features from this
normalized data 

   learning models
our roadmap was to benchmark deep learning against conventional supervised learning
techniques

    shallow learning models 
we chose support vector machines  svms  and conventional shallow neural networks and
trained them using the extracted features 
   svm   we implemented a svm using the scikit learn     library in python  the
library presents an easy api for training  testing and cross validation for our svm 
since we have a multi class problem  the library implements a one against one
approach as described in      we did a grid search over various values of the svm
parameters to obtain the best possible accuracy from our feature set 
   shallow neural network   we implemented a standard three layered neural
network using the theano     library in python  the fully connected hidden layer has

fi     neurons and an output layer has   neurons  one corresponding to each class of
labels  the number of hidden layer neurons is chosen to be three times the number
of neurons in the input layer  the hidden layer implements a  activation
function  since the neural network takes a long time to train  we ran the training on
nvidia tesla c     gpus 

    deep learning model
next  we implemented convolutional neural network          for our problem  the network
architecture is loosely based on the approach described in      the network has an input layer 
three convolution   max pooling layers  one fully connected hidden layer and an output layer
with   neurons  one corresponding to each class  similar to the shallow net model   the number
of convolution kernels in the three hidden layers are         and    respectively and the size of
the convolution kernel in each layer is  x  

   results
in this section we compare the relative performance of the three implemented learning models
on our dataset  figure   shows the training error and validation error of the different models on
the dataset 

relative performance of different learning
algorithms
      
     
     

     

     
     

           

     

           
     

     
     

    

    
training error

test error

svm

shallow neural network

svm with gabor filters

shallow neural network with gabor filters

convolutional neural network
figure   training and test error rates for different learning models

clearly  convolutional neural networks outperform shallow learning approaches 

fi   discussion and improvements in the model
the low training error on the convolutional network compared to the high test error indicates
that the model is suffering from over fitting of the data points  to gain more insight into this
issue  we derived a confusion matrix  shown in table    each row in table   corresponds to an
expected class label  and each column corresponds to the predicted class label  thus the entry
        in the second row  first column indicates that the class disgust is misclassified as the
class angry with          probability  based on the analysis of this data  we arrived at the
conclusion that the maximum confusion occurred among the classification of the negative
emotions  fear  disgust  anger  sad   there was also some confusion in the classification of the
positive emotions  happy  surprised  and neutral   to overcome this issue we came up with a
two stage hierarchical classification approach 
stage    classify between positive and negative emotions
stage    classify among the emotions in each category
with this  we are getting around        accuracy for positive classes and around       
accuracy for negative classes  which is already around     improvement over the single stage
approach  in spite of the relatively inaccurate binary classifier  this takes us very close to stateof the art results  which is     for the entire set 
table    confusion matrix for deep learning

angry

disgust

fear

happy

sad

surprise

neutral

angry

     

    

     

    

     

    

     

disgust

     

     

     

   

     

 

    

fear

    

    

    

    

     

     

     

happy

    

 

   

     

    

    

    

sad

     

 

     

     

     

    

     

surprise

    

 

    

    

    

     

    

neutral

     

    

    

     

     

    

     

   future plans
we were resource limited in our shallow learning approaches  mainly  we could not run
adaboost on all our training data  in the future  we want to run adaboost on all training
samples  maybe implement the adaboost algorithm on the gpu  also  the stage   binary
classifier used in the   stage hierarchical classification approach mentioned above needs to be
fine tuned  if we can make this classifier reliable  with the accuracy we are getting for the
classifiers in the second stage  we would be able to get the same or higher accuracy than the
best results achieved on this dataset  our ultimate goal is to develop an end to end system out
of our model and take it to the mobile domain  mobile devices have already started shipping
with gpus  if we can have the convolutional neural network on the mobile device predict a
facial expression from a snapshot of the users face in soft real time  this could be useful in a
wide range of applications 

fireferences
   
https   www kaggle com c challenges in representation learning facial expressionrecognition challenge data
    bartlett  marian stewart  et al   recognizing facial expression  machine learning and
application to spontaneous behavior   computer vision and pattern recognition       
cvpr       ieee computer society conference on  vol     ieee       
    zhu  ji  et al   multi class adaboost   statistics and its        
    pedregosa  fabian  et al   scikit learn  machine learning in python   the journal of
machine learning research                      
    bergstra  james  et al   theano  deep learning on gpus with python   nips      
biglearning workshop  granada  spain       
    knerr  stefan  lon personnaz  and grard dreyfus   single layer learning revisited  a
stepwise procedure for building and training a neural network   neurocomputing  springer
berlin heidelberg              
    lecun  yann  and yoshua bengio   convolutional networks for images  speech  and time
series   the handbook of brain theory and neural networks             
    krizhevsky  alex  ilya sutskever  and geoffrey e  hinton   imagenet classification with
deep convolutional neural networks   advances in neural information processing systems 
     
    tang  yichuan   deep learning using linear support vector machines   workshop on
challenges in representation learning  icml       

fi