predicting mobile users future location 
john doherty  doherty   
introduction 
one of the most powerful features of modern smartphones is their ability to provide us with realtime  locationally
aware information  for example  our smartphones can predict when we are heading to work and give us an
estimate of the travel time  while these features can be useful  they typically do not generalize to users with more
complicated schedules  if our smartphones could predict our destination  they could give us more relevant
information about the places we are going  

 

imagine the following situation  you hop in your car to head to the gym after work  you dont turn on your phones
navigation because you know the way  but there is an accident on your route and a lot of traffic  if your phone
knows where you are going without you telling it  it could warn you before you start driving and help you reroute 
of course all this location prediction sounds a bit creepy  but if it is all happening on device  and never storing
your location history in the cloud  maybe people will feel a little more secure  for my project i created a system
that can predict a users next location using their current location and the current time  learning only from that
users location history  

 

this system is trying to answer the question  if i were to leave from my current location right now  what is my most
likely destination  this can be reduced to a multi class classification problem where the set of classes is the set of
previously seen locations  and where we try to predict the class  destination  that maximizes the following
probability  
p  destination   current location  date  time  

 

while this question seems like a straight forward machine learning question  a number of unique challenges arise
when working with real data  location history datasets are often small and noisy  even so  the results are
encouraging and with some tweaking it could have use in a real product  

 

data 
data for this project was collected using the gps location tracking app  moves     for this project  i used my
personal location data collected over the last two months and a friends location data collected over a summer 
this location data was obtained in the form of gpx files  gpx files are designed to store information about a
users movements throughout the day  so it tracks the latitudes and longitudes of locations visited  in addition to
the routes taken between those locations  

 

since i am trying to solve a classification problem  i need the output to take the form of discrete classes  this
means performing some sort of preprocessing to cluster continuous latitude longitude coordinates into a set of
discrete locations  i did this by looking at the physical distances between coordinates and clustering points that
were less than    meters apart  this clustering step is necessary because the actual latitude longitude
coordinates of locations do not have much meaning  the resulting discrete locations are more useful than raw
latitude longitude coordinates  but there are still far too many unique locations to preform any sort of useful
classification  for example  my personal location history contains    unique locations     of which were visited
just once  to fix this problem i filter out locations visited fewer than   times  

 

with a set of discrete locations on hand i am able to build my training examples  training examples are built using
pairs of consecutive location points from the location history  these pairs of points  a  b  are instances from the
dataset where the user traveled from a to b  my input vector consists of features of the date  time  and location of
the first point in the pair  the target variable is the location of the second point  

 

the performance of this system depends heavily on the dataset given to it  specifically  the performance is going
to be much better if we use the data of someone with a consistent schedule rather than that of someone with a
more irregular schedule  to make sure i am testing both ends of the spectrum i am using two datasets  my
irregular location history from this school year and my friends more regular location history collected while
working over the summer  a comparison of the distributions of locations can be seen in figure    

fi 
dataset    working data  
   data points after filtering

dataset    school data  
    data points after filtering

  
  
  
  
  
 

figure    this shows a comparison of the two datasets  on the left  dataset    is my friends location data
collected while she was working  on the right  dataset    is my location data collected while i was at school  in the
graphs  each bar indicates the frequency with which a unique location was visited  the long tail of the graph for
dataset   shows the large number of locations with just a few visits  

 

features 
i tested different combinations of binary features on the location  date  and time of input data  my final system has
the following features  

 





 
 

a binarized version of the input location  for example  if there are   possible location classes and the current
data point is from location    then we have the features                  
a binarized vector of the day of the week  this is intended to catch the weekly repeating patterns in schedules 
monday is represented as                        
a binarized vector representing the current time broken into   hour bins  this means that times are grouped
into the following bins    am to  am   am to   pm    pm to  pm   pm to   am  for example   am would be
represented as               
a binary feature indicating if the time is am or pm  
a binary feature indicating if the day is a weekend  

additional features that i tried include  the previous location and different combinations of weekdays  monday 
wednesday  friday or tuesday  thursday for example   these features did not perform as well  

model 
i tried a number of different classifier models on this problem  all of the models are implemented in python using
the scikit learn library      because i have binary features and a multi class classification problem  the following
models made the most sense  

  

fi





 
 

one vs rest classification with logistic regression 
naive bayes 
multi class svm 
linear discriminant analysis  lda  
random forest 

these are all standard classifier models with multi class variants  since i do not know much about the distribution
of my datasets  it is worth testing the performance of all of these models   

results 
in this section i will discuss how well these models perform on the two location datasets provided  the first step in
evaluating these models is to actually select the data for training and testing  i used two different methods to train
and test the models  the first method is k fold cross validation  it splits the data into k folds  trains on k   of them 
and evaluates on the remaining one  i average the results for k iterations  this method uses less data for
evaluation and saves more for training which is useful given that i have pretty tight data constraints   

 

the second method is intended to simulate online learning  in this method  i move through the dataset in
chronological order and train on all data up to the current datapoint n   and test on the datapoint n  the results of
each evaluation are averaged  this method is intended to simulate a real environment where this system would
be employed  in the real world  the model will take in more data over time so it is important to evaluate how it will
preform as the amount of available data increases  

 

i use a few different metrics to evaluate the predictions of the models on the test set  classification accuracy is the
most obvious metric and it simply gives the fraction of the test set for which the model predicted the correct
output  this can be useful  but is not always the most indicative of true performance  this is especially true in data
with an unbalanced output class distribution  like mine   

 

since this is a multi class classification problem precision and recall are important metrics  precision gives a
sense of the number of false positives for each class while recall looks at false negatives  instead of actually
reporting precision and recall for all classes and all models  i will just present the f  score  this score combines
precision and recall for all classes  i will also present confusion matrixes for the best performing models   

 

all of these models assign a probability to each possible output class for a given input  we can sort the classes by
this probability such that the first element of the sorted list is the most probable class and the one we would
ultimately return  while most evaluation metrics only look at the one returned output  the class with the highest
probability   it is interesting to evaluate the entire list of possible outputs  specifically  in this problem it might be
acceptable to return more than one result  to evaluate the list of possible classes for each model  i give the
average position of the correct output in the sorted list of possible classes  

 

below are tables of results evaluated on the described metrics  there are two tables for each dataset  one
showing the results of k fold testing and the other showing online learning  in these cases  dataset   is my
friends working location data  this is a more consistent dataset and should give better results  dataset   is my
school location data  this is much less consistent and the results reflect that  

 

svm
train accuracy

logistic
regression

random
forest

lda

naive
bayes

    

      

      

      

      

test accuracy

      

      

      

      

      

f 

      

      

      

      

      

average position
of correct output

      

      

      

      

      

table    results of k fold testing for dataset   

fi 
svm

logistic
regression

random
forest

lda

naive
bayes

test accuracy

      

      

      

      

      

f 

      

      

      

      

      

   

      

      

      

      

average position
of correct output

table    results of online testing for dataset   

 

svm

logistic
regression

random
forest

lda

naive
bayes

train accuracy

      

      

      

      

      

test accuracy

      

      

      

      

      

f 

      

      

      

      

      

     

     

     

      

     

average position
of correct output

table    results of k fold testing for dataset   

 

svm

logistic
regression

random
forest

lda

naive
bayes

test accuracy

      

      

      

      

      

f 

      

      

      

      

      

     

     

     

      

     

average position
of correct output

table    results of online testing for dataset   

  
  
  
  
  
  
  
 

figure    a comparison of results for logistic regression of the two datasets  dataset   left  dataset   right   these
graphs show the distribution of the position of the correct class in the list of predicted classes sorted by probability 
the skewed distribution shows that generally the correct class is close to the top of the list  dataset   has a
longer tail because it has more possible output classes with similar probability  

 

fifigure    confusion matrixes for logistic regression of the
two datasets  dataset   left  dataset   right   rows
represent the true class while columns represent predicted
class  the number at a position  i  j  is the number of times
we predicted class j when the true class was i  the result
from dataset   looks pretty good with most of the points
falling along the diagonal  the model struggled significantly
with dataset   and resulted to guessing class   in most
cases   

discussion 

 

overall logistic regression performed the best on both datasets  this is most likely because logistic regression is
pretty resilient to noise  and there is a lot of noise in both datasets  logistic regression is not trying to build hard
boundaries between classes  like svm  but instead uses a one vs all classifier system where it just assigns a
score to each possible classification  it is interesting that logistic regression  one of the simplest classifiers  can
out perform others on small  noisy datasets like these  svm on the other hand struggled to deal with the fewer
number of point  in fact  on dataset    the svm resorted to classifying every test instance as home  

 

there are a number of properties of location data that make it tricky to work with  and the results reflect  first 
these datasets are small  for reference  my personal location history collected over two months was reduced to
just     data points after filtering out rare locations  this is not a lot of data  especially when dealing with more
complex schedules that contain a lot of noise  in order to learn more complex schedules  we need more features 
but adding features increases the number of parameters that need to be learned which requires more data  as a
result  my system uses relatively few features to try to extract the core parts of the users schedule  

 

another challenge that these location datasets throw at us is the imbalance in the number of times each location
is seen  in any location history dataset there are going to be some locations that are visited far more frequently
than the rest  i discussed filtering out rare locations  but there will always be places like home and work that you
go daily and places like class or the gym that you go a few times a week  while some classifiers can handle this
imbalance  others  like svm  cannot      this is another possible reason for why svm performs so badly  

 

finally  peoples schedules tend to change over time  for example  class schedules change every quarter 
schedules for jobs are more consistent  but people can change jobs and engage in different routines after work 
while i did not encounter these properties in my datasets  a real word system would have to be modified such
that more recent points were weighted more heavily in training  

 

future 
while this system did show some encouraging results  clearly there is a lot that still could be done  there are
plenty of other features that could be added including ones external to the location history  these include things
like weather and category of location  residence  office  classroom  etc    even more interesting would be to try
modeling this data with latent variables  there might be certain clusters of locations which have similar properties 
additionally  modeling this dataset as a series of state transitions from one location to another could give better
results  

 

references 
    https   www moves app com  
    http   scikit learn org stable  
    wu  g  and chang e  class boundary alignment for imbalanced dataset learning

fi