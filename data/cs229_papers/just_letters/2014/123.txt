prediction of average and perceived polarity in online journalism
albert chu  kensen shi  catherine wong

abstract we predicted the average and perceived
journalistic objectivity in online news articles based on
the text of the article and data collected on the individual
user reading the article  in order to better understand
both how journalistic bias relates to the text of an
article alone  and how reader demographics and political
leanings influence the perceived objectivity of a given
article  using survey data on the perceived polarity of    
articles ranked by multiple readers using the mechanical
turk crowdsourcing platform  we applied supervised machine learning algorithms including l  regularized l loss svm  naive bayes  and linear regression to predict
the mean polarity for each article and the perceived
polarity of that article by any given reader  we were
able to identify the words most strongly correlated with
journalistic bias  predict a binary classification of mean
and perceived bias for each article with relatively high
accuracy  and determine the degree to which the article
text alone and reader demographic information impacts
perception of journalistic bias 

i  i ntroduction
a  background
objectivity is a significant underlying principle in
journalistic professionalism  and one that has become
less standard in an internet oriented and often deeply
partisan media environment  when anyone from a casual blogger to a major news outlet can disseminate information to the public  understanding both the polarity
of a given news article or media source  as well as the
inherent biases influencing the perception of that media
source by any given reader  presents especially interesting implications today  recommendation engines and
predictive algorithms filter the news presented to any
given reader to cater to his or her own interests  often
without the knowledge of the casual reader looking for
what he may believe to be objective information 
despite the importance of journalistic objectivity to
the integrity and perception of a given article  previous
machine learning research to automatically classify online news articles based on the presence of journalistic
a  chu  k  shi  and c  wong are undergraduate students at stanford university  stanford  ca   achu   kensens  catwong 
at stanford edu 

bias in a given article has been surprisingly sparse 
especially in detection of bias within articles not specifically focused on political campaigns and figures  sonal
gupta in          addressed a similar problem but
focused only on articles concerning american politics 
relying on the presence of previously identified political
memes within the articlesphrases that had been
manually identified as markers of a specific political
inclinationto assign a limiting binary classification
distinguishing between articles that favored the democratic and republican parties 
b  goals
using a combination of text analysis on online
news articles alone and survey data collected on the
demographics and subjective biases of the individual
user reading a given article  we predict    the mean
journalistic bias of a given online news article  based
on labelings assigned by multiple readers of the same
article  and    the perceived journalistic bias of a
given article  which is the bias labeling assigned by
a specific user to the article  our goal was to better
understand how journalistic bias relates to the text
of an article alone  as well as the degree to which
reader demographics and political leanings influence
the perceived objectivity of the same article 
ii  data s ets and f eature p reprocessing
since there is no prior dataset and little existing
research on this topic  we created an entirely new
dataset consisting of both articles and individual reader
responses to those articles 
a  article text collection
we collected urls for     online news articles
based on the top ten google news search results
for    high ranking keywords obtained from google
trends      which provides data on high volume search
keywords across various categories in a given year 
keywords were chosen to reflect a range of popular political and nonpolitical topics across categories 
including the top trending keywords in      overall 
the top five business people of       and the top male

fiand female politicians in       the complete list of
keywords can be found in the appendix 
we then used the mechanical turk crowdsourcing
platform to isolate the text from each of the     articles 
due to nonstandard formats used to present article
text online between various news sources  it is fairly
challenging to automate text collection for online news
articles  so the mechanical turk task was designed to
allow crowdsourced workers to manually scrape article
texts when presented with each article url 
article texts were preprocessed using the python
nltk toolkit to remove punctuation  normalize character case  and stem tokens using the english snowball
stemmer      certain common types of text features
 such as urls  dollar amounts  telephone numbers 
etc   were replaced with tokens representing each type 
we then filtered the resulting article texts to remove
broken links and articles with less than    words  which
corresponded to articles that simply contained a video
and article caption  and were considered to not contain
enough text for a reader to consistently judge the bias
of the article   after filtering  our final dataset consisted
of preprocessed texts from     online news articles 
b  bias labeling and reader demographics survey
using a mechanical turk survey  each article url
was provided to   different survey respondents  who
read and labeled articles on a discretized      scale
according to the perceived journalistic bias of the
article  where a ranking of   corresponded to no bias
and    corresponded to a completely biased article 
the survey also collected information on   additional
features corresponding to reader demographic data and
perceptions of the articles  reader age  education  gender  income  political leaning  on a discretized scale
of      corresponding to left and right wing leanings 
with an additional option for no political leaning  
reader location of residence  the perceived type of bias
and political leaning of the article  and perceived bias
ranking of the article source  complete information on
the survey features  including the options provided to
survey respondents  is available in the appendix 
prior to ranking the articles  survey respondents were
provided with standard definitions of the terms bias 
left wing politically  and right wing politically  to
ensure that survey respondents understood the terms
and were looking for standardized attributes in the
articles        
we assumed that the mean of the perceived bias
rankings for each article represent the ground truth

bias rankings on which we trained our models to
predict mean bias  the perceived bias rankings assigned
by a specific user to a given article were used to
train our models to predict individual perceived bias 
this dataset was collected through mechanical turk 
which resulted in     individual responses  across    
articles  after removing incomplete surveys 
iii  a rticle b ias p rediction m odels  
m ethodology
a  mean bias labeling prediction
our first goal was to predict an articles average
bias as reported by   readers per article only using
the articles text  we first used l  regularized l  loss
svm  support vector machine  for binary classification  with the java liblinear package      which we
modified slightly to control randomization across runs 
to predict average bias from the article text in bag ofwords format 
initially  we considered all       words that appeared
at least   times total and in at least   different articles 
we realized that the feature space was too large  with
only     training examples   so we applied forward
search on the svm to select     words to use as
features  used throughout the remainder of the study 
the number     was chosen because at this point 
the accuracy improvement during forward search was
tapering off  this search uses a mix of    fold cross
validation and leave one out cross validation  loocv 
with additional heuristics  based on word frequencies
in biased and unbiased articles  we found that carefully choosing the words during forward search was
especially importantin fact  using    fold cv in the
forward search produced a much lower peak classification accuracy  at each step of the forward search 
we first used    fold cv for efficiency and then used
loocv to narrow down the top words found  since
we only had     training examples  the accuracy was
always one of     choices  namely multiples of      
between   and     so many words would have the same
maximum accuracy  hence  the heuristic was used to
pick the single word to add at each step  instead of
choosing them arbitrarily  the top five words  picked
first by forward search  were senat  isnt  court 
subject  and council  note that these are stemmed  
each training examples feature data was normalized
before feature selection but not after  so feature i
actually represents the frequency of word i relative to
the other       common words 

fiwe then experimented with different numbers of
bias classes to see how accurate the predictions could
be  since bias was reported on a scale       we split
this range into equal parts to obtain the different bias
classes  we report the loocv percent accuracy for  
through    bias classes  where a correct prediction
must exactly match the expected class  standard nave
bayes with laplace smoothing and linear regression
were self implemented and run for comparison on the
binary classification problem 

fig     loocv accuracy of the svm  naive bayes  and linear

b  perceived bias prediction

regression models in predicting mean bias with varying numbers
of bias labeling classes 

in this phase  we used the same set of     words
found previously  the goal here is to predict a specific readers rating of an articles bias  using both
the article text and reader specific information  age 
education  gender  income  political leaning  and state
of residence   we ran svm on this new dataset  with
    examples   reporting loocv percent accuracy for
  through    bias classes computed as before  similarly 
standard naive bayes and linear regression were also
implemented and run for comparison 
we then repeated this procedure with two modified
datasets  one with only reader data and one with only
article text for comparison 

fig     loocv accuracy of the models in binary classification
of perceived bias labeling with text features alone  survey reader
demographic data alone  and a combination of the feature sets 

iv  r esults
using the final set of     text features obtained after
forward search feature selection  we predicted the mean
bias class labelings using the svm  linear regression 
and naive bayes models to obtain the results in figure
   as described earlier  we experimented with different
numbers of bias classes to determine how precise our
bias predictions could be  by splitting the original bias
labeling range  which was on a scale of       into
equal parts to obtain between   and    bias classes 
the results in figure    which show the mean loocv
accuracy of each model using the given number of bias
classes  are plotted along with a chance baseline  corresponding to the results that would have been obtained
by randomly assigning labels  to show improvement in
the accuracy obtained by each model in comparison
to random labeling  using svm with   bias classes
gives a        loocv accuracy  naive bayes gives
        and linear regression gives        
we also predicted perceived bias labelings  the bias
labelings assigned by a specific reader to an article 
using the same range of      discrete bias classes
corresponding to increasingly accurate discretizations
of the original bias labeling range  figure   shows the

comparative loocv binary classification accuracy of
the svm  naive bayes  and linear regression models
when trained with only the     text features  only
the reader specific demographic features obtained from
the survey  and a combination of these two feature
sets  for binary classification  svm with both text
and reader information achieves a        loocv
accuracy  with reader information alone  svm obtains
a        accuracy  and with text alone  it gets a
       accuracy 
for better comparison of trends and differences obtained by training the models on these different feature
setsthat is  when we trained on text features alone 
reader specific features alone  and the combination of
these two feature setsfigure   shows an expanded
graph of the resulting loocv accuracy of the svm
model on      discretized bias labeling classes 
v  c onclusion
after forward search feature reduction to reduce
overfitting  the performance of the svm and naive
bayes models showed that we were able to predict

fiand political leanings far more than the specific text of
the article itself 
vi  f uture w ork

fig     loocv accuracy of the svm model on varying numbers
of discrete perceived bias labeling classes  with the text features
alone  reader demographic features alone  and a combination of the
feature sets 

mean bias labelings  especially a relatively rough binary
classification of bias labelings  with surprisingly high
accuracy using text features alone  this suggests that
any article does in fact have a ground truth level
of journalistic objectivity based on the text  despite
variation in the bias labelings assigned by each of the
survey respondents to any given article  it does in fact
appear that there exists an overall degree of bias associated with the article  independent of the specific reader 
however  it is also interesting to note that perceived
bias for any one individual reader seems to be far more
strongly correlated with the specific demographic and
political leanings of that reader than with the article
text alone  even adding article text features to the
reader specific features does not strongly impact model
performance for perceived bias prediction 
additionally  the specific final text features chosen
after forward search feature reduction give insight
as to the words that appear most strongly correlated
with prediction of journalistic objectivityinterestingly 
although perhaps unsurprisingly  many of these were
associated with politics  a topic common to many of
the articles ranked as biased by readers 
overall  our research shows that it is possible to
assign mean bias labelings to articles based on their
article text  and demonstrates a preliminary method
to automatically score the journalistic bias present in
a given article  results which could be used to better
inform readers of the objectivity of news content presented online  the results also provide insight into the
degree to which the previous biases and background of
a reader influence his or her perception of the objectivity of a given article  showing that on an individual
basis  reader perception of article bias is a subjective
measure  influenced by a readers own demographics

future work could focus initially on expanding our
dataset to include a larger scope of news articles across
a greater range of sources and topics  with additional
bias labelings and survey respondents for each article 
a larger and more diverse dataset could reflect a more
balanced demographic of readers  with more accurate
ground truth labelings for mean bias overall 
additionally  some features were collected in our
bias labeling survey that were not used in this study 
including the specific class of bias in a given article 
the perceived political leaning of the article  or the
bias ranking of the article source  future work could
attempt to predict these labelings and also take into
account additional features into the prediction model 
such as the perceived bias ranking of other works by a
specific author or the general subject of the article  and
feature reduction could be used to optimize text feature
prediction to predict each of the additional possible
labels 
a ppendix
a  search keywords
articles collected were selected based on top google
news results for these search terms  selected based
on the following categories on google trends  top
   trending       top   business people  top  
energy company  top   female politicians  top   male
politicians  top   us governors 
search keywords  paul walker  boston
marathon  nelson mandela  cory monteith 
iphone  government shutdown  james gandolfini 
harlem shake  royal baby  adrian peterson  oprah
winfrey  willie robertson  charles r  schwab  bill
gates  steve jobs  bp  wendy davis  dianne
feinstein  kathleen sebelius  janet napolitano 
kay hagan  ted cruz  barack obama  hugo
chavez  rand paul  arnold schwarzenegger 
chris christie  jesse ventura  andrew cuomo 
rick perry
b  article bias labeling survey features
mechanical turk crowdsourced survey respondents
were provided with online news article urls  and
asked to complete a survey after reading the article with
their opinions on the article and demographic information  while the original mechanical turk survey cannot

fibe publicly accessed  a preview image of the survey 
along with information on the survey data features  is
available here  http   bit ly cs   biassurvey 
r eferences
    bias  http   www oxforddictionaries com us definition american
english bias  accessed  december         
    left wing politics 
http   en wikipedia org wiki left wing
politics  accessed  december         
    right wing politics  http   en wikipedia org wiki right wing
politics  accessed  december         

    steven bird  edward loper  and ewan klein  natural language processing with python  oreilly media inc        
    r  e  fan  k  w  chang  c  j  hsieh  x  r  wang  and c  j 
lin  liblinear  a library for large linear classification  journal
of machine learning research                   
    sonal gupta  finding bias in political news and blog websites 
http   snap stanford edu class cs   w      proj     report
sonal gupta pdf       
    google inc  google trends       http   www google com 
trends topcharts date       accessed  november          

fi