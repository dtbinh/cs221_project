robo brain  massive knowledge base for robots
gabriel kho  christina hung  hugh cunningham
 gdykho  chung     hughec  stanford edu
december         
abstract
the robobrain project is a collaborative effort to build a knowledge engine that learns and shares
knowledge representations  these knowledge representations are learned from a variety of sources 
including interactions that robots have while performing perception  planning and control  as well as
natural language  semi structured knowledge and visual data from the internet  this representation
needs to consider several modalities including symbols  natural language  visual or shape features 
haptic properties  and so on  the knowledge that robo brain accumulates is stored in a graph
database  the structure lends well to a data visualization  through the data visualization  we
crowd source comments on the graph to find problems with the data  this paper details an attempt
to create a learning algorithm based off of those comments so that we can predict fault nodes in the
graph and alert them to administrators 

 

introduction

the robo brain project is knowledge engine that will share and learn knowledge representations 
the knowledge is stored in a graph structure and created by crawling knowledge repositories on the
internet like opencyc    and wordnet           however  as with any automatic crawler  errors in the
graph can pop up  first  any errors in opencyc and wordnet will propagate to robo brains knowledge
engine  another important problem is that as data is being coaslesced from multiple sources  incorrect
representations will appear  for example  a node plant might have connections to the nodes roots 
leaves  and nuclear energy  what this entails is that the node plant needs to be disambiguated
into two different nodes  one for the biological and plant and another for a power plant 
in order to quickly see these problems  a data visualization was created since the structure of a graph
database lends to well to directed force layout  unlike row store databases  a graph database in based
on graph theory  each item in the database contains a pointer to its adjacent item  thus  we can think
of a tuple as a node  edge and node  each node and edge can have their own set of properties  in
robo brain  each node symbolizes a concept  such as the noun car or a verb slip  the name of these
nodes is stored in the attribute handle of these nodes  other relevant attributes include the source urls
that this node was generated from  upvotes and downvotes on the source  and the lines to other edges 
the only attributes we really care about on our edges is the pointer to the source node and the pointer
to the target node 
the solution we propose is to crowd source feedback on the graph  humans can intuitively tell if
there is a problem with a subsection of the graph  previous work has been done in trying courdsource
the information humans intuitively know and try to incorporate it into robots     thus  the data visualization built in d     was created with options for users to comment on the nodes and edges of
the graph  afterwards  the algorithm is planned to be put into a system to alert administrators to potentially problematic nodes  for this paper  we did all our implementation in scikit  a python library     

 

fifigure    visualizing the structure of the graph database

 

dataset

our dataset contains     comments over     nodes and     over     of edges  most of these are
collected by walking through the graph starting at the shoe node  a lot of the data has been collected by
getting feedback from people involved in the project  unfortunately  complications outside the classroom
led to delays of the data visualization being accessible to people around the world  thus resulting in a
much smaller dataset than we anticipated 
the feedback itself from the the graph was stored in a mongodb  the only we differentiated a node
from an edge was though the node id  otherwise  they had the same structure 
a challenging part of this dataset is we have conflicting data  some feedback might list a node as good
while others might list it as problematic  in addition  we have no idea what the underlying distribution
of the feedback is 
for a node  the feedback structure was such that there was an id node  which was a unique id that
identified the node  the f eedback type was just a coarse level on whether the user thought the node
was a good node or bad node  in this case  it either had the value of agree or disagree  then we had a
node handle which was the name of the node  due to the fact that nodes hadnt been disambiguated
yet  this handle identifier was unique at the time of the writing of this paper  however  to keep this code
easily integratable for the future version of the graph  we store id also which will remain unique  the
last field in the feedback is a f eedback type which is the type of feedback  for the nodes  this feedback is
either good node  split node  remove node or rename node  the idea behind
this was that when a node is good  everything is good  but when a node is bad  different things could
be bad 
for an edge  the node id takes the form of link sourcenodehandle to targetnodehandle   the
f eedback type is the same  the node handle contains the handle of the link  which takes values like
isahom on y m of or issam esy sn et   these values describe the relationship between the
source and target node and are not unique  the last field f eedback type is really either goodlin k
or badlin k  because when a link is bad  it should just be removed 

 

features and preprocessing

we use different features for node prediction and link prediction  the features used in node prediction
are the length of the handle name  number of non alphabetical characters  belief  number of links 
sources used for creation  we use length of handle name because some nodes come out with a name
like artifictwornbyhumans  our logic is that some nodes had their handles name incorrectly generated
and a length of the handle might be a good indicator if that happened  we then use number of nonalphabetical characters because we have nodes like heat m ap    and shoe jpg  so we think that the
number of non alphabetical characters would a good indicator of that 
we also want to try to predict if two nodes have been merged such as the plant and power plant
example mentioned above  we think the best indicator of this one is the number of edges a node has  a
high number of edges is probably a good indicator that two distinct nodes have been merged 

 

fithe source that a node is created from is used as a feature  we use an indicator variable to indicate
if a node used a particular source during its creation  this is possible because the number of sources
used in robo brain right now is very limited  the rationale behind this is that some sources are more
trustworthy than others  in fact  down the line  we might be able to use this to rank sources 
now  belief is a preprocessed feature that is generated by the pipeline at the creation of the node 
it represents the probability that a node is good  a lot of sources contain upvotes and downvotes on
the concepts that these nodes are created from  thus  a belief is created by taking a feature vector
containing upvotes  downvotes  weight of a project  and the log normalized number of times the feed
 the source where the concept was generated from  appeared  and then calculating the dot product with
a weight vector  upvotes and downvotes are smoothed using laplace smoothing  and then the result of
the dot product is thrown into a sigmoid function and this results in the probability  the reason we use
this preprocessed feature instead of doing more sophisticated learning on the raw data of the upvotes
and downvotes is a practicality reason  due to the way that data is stored  using the beliefs reduces our
calculation time from o n    to o n   thus even though it might not be the most sophisticated way of
doing the calculation  we use it 
in classifying whether a link is faulty or not  we build features outwards from a link through the
nodes that it connects  in particular  we use the product of the belief in each of the two nodes as well as
the number of links each connected node has  the intuition for the first feature is that if a link connects
two nodes that are believed to be faulty  then the link itself is likely to be faulty as well  additionally 
as the number of links on a node increases  it may become more likley that links are contain faulty
knowledge  this latter intuition is roughly equivalent to the inclusion of the number of links as a feature
for classifying whether a node is faulty  where a high number of links might indicate that a node should
be split  these few features represent an early baseline for classification of faulty links  and more
analysis will be needed in the future to develop additionaly features in this space 

 

models

we model the problem of predicting faulty nodes and links in the knowledge graph as a classification
task and consider both binary and multiclass classification  in the binary case  our prediction models
distinguish between good nodes and links and bad nodes and links  for the multiclass formulation
of the problem the bad node class is divided into clases for split  remove  and rename 
there is no multiclass formulation for predicting faulty links since the only feasible action when altering
a faulty link is to remove the link 
we use support vector machines to perform classification in both the binary and multiclass formulations of the problem  and compare various parameter combinations for svm classifiers with a baseline
multinomial naive bayes classifier  the svm classifier is well suited to our task since the low dimensionality of our feature set avoids the training time drawbacks of svms and allows us to easily capture
any non linear relationships between our features  grid search over the svm parameters kernel function 
slack cost c  and   used in rbf and other kernel functions  finds that the linear kernel k   hx  x  i
and a slack cost parameter c       minimized classification error for predicting faulty nodes in the
dataset  ablation analysis reveals that features derived from node handles contribute most to classifier
performance 

 

results and discussion

given the limited size of our dataset  we use leave one out cross validation  loocv  to evaluate
the performance of our models  we would ideally hold out a set of graph annotations on a separate
part of the knowledge graph to assess the generalizability of our models  but  as mentioned previously 
complications in the deployment of the wide release of the crowdsourcing interface has hindered our
efforts towards data collection  in addition  the graph is ever changing its nodes and edges  so we figured
it would be best to conduct our experiments on an isolated section of the graph  we report the average
loocv misclassification error of our classification models for predicting faulty nodes in the table below 
binary multiclass
model
multinomial naive bayes
     
     
svm  linear kernel  c    
     
     
svm  linear kernel  c      
     
     
table    misclassification error for predicting faulty nodes
 

fithe svm classifiers achieve low misclassification error on the nodes in the dataset  but perhaps the
more significant metric of success is the precision of the classifiers with respect to bad nodes and links 
for nodes  the svm using a linear kernel and a slack cost parameter c       achieves       precision
with respect to the bad class  a high precision indicates that the proportion of predicted faulty
nodes and links that are truly faulty is high  thus  using the predictions of a highly precise classifier to
alter the graph structure would result in alterations to mostly faulty nodes and links rather than already
correct ones 
the following table shows somewhat preliminary results for classification error with regard to predictiing faulty links 
model
multinomial naive bayes
svm  linear kernel  c    

classification error
     
     

table    misclassification error for predicting faulty links
for predicting faulty edges  the baseline naive bayes classifier outperforms the baseline svm model
in achieving a lower misclassification error  with a precision of       with respect to bad links  the
precision of the naive bayes classifier seems promising  but short of where we would like our model to be
before utlizing predictions for augmenting the knowledge base 

 

conclusions

the robo brain crowdsourcing project was ultimately a challenging project  real world complications
with the robo brain project made data collection challenging  in addition  a lot of verification in this
learning algorithm would have benefited from human evaluation  which just wasnt possible given the
man power we had 
ultimately though  our algorithm does its job of having a low number of false positives  by positives 
we mean classifying a node as faulty  the reason this is important is because most of an administrators
time should not be spent evaluating nodes that arent problematic  in that sense  this algorithm is a
success  and just needs more testing 

 

future

robo brain is currently being modified as we speak  in fact  on the subgraph we examined  a new
node with high number of edges appeared last week  in addition  more studies need to be done on the
hci level  for example  there is an imbalance of data related to the positive and negative examples  we
hypothesize that people just might not comment if a node or edge is good  and only comment on the
bad ones to save their time  however  to be sure of this  studies with a statistically revelant number of
users has to be conducted 
in addition  some of the handles on the edges might not be human readable to someone unfamiliar
with the project  thus  there might be discrepancies  this means that some of these might need to be
changed 
in addition  some feedback might be because some users just dont understand some of the words
presented to them  for example  gaucherie is connected to rusticity  the comments on such nodes
might be random noise 
further work will be done to try and incorporate this learning algorithm into the robo brain pipeline
so that it can alert administrators to potentially problematic nodes  as the knowledge base grows it
will become increasingly important to maintain a healthy knowledge graph  and developing additional
features for capturing relationships between potentially fault nodes and edges will similarly become
important for improving the performance of these models over an increasingly disparate dataset 

 

fi 

references

    a jain et al   planit  a crowdsourcing approach for learning to plan paths from large scale
preference feedback  arxiv                  
    a jain et al   robobrain  large scale knowledge engine for robots  arxiv                  
    george a  miller  wordnet  a lexical database for english  communications of the acm vol 
    no                  
    pederosa et al   scikit learn  machine learning in python  jmlr     pp                 
    sw opencyc org  opencyc for the semantic web         online   available  http   sw opencyc org  
 accessed      dec        
    y  zhu et al   reasoning about object affordances in a knowledge base representation 
stanford university       

 

fi