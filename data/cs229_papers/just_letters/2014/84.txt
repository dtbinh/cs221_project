cs    technical report      

deep learning architecture for univariate time
series forecasting
dmitry vengertsev 
abstract
this paper studies the problem of applying machine learning with deep architecture to time series forecasting 
while these techniques have shown promise for modeling static data  applying them to sequential data is gaining
increasing attention  this paper overviews the particular challenges present in applying conditional restricted
boltzmann machines  crbm  to univariate time series forecasting and provides a comparison to common
algorithms used for time series prediction 
  department

of computer science
stanford university
davenger stanford com

   introduction
forecasting future values of observed time series plays an
important role in nearly all fields of science and engineering  such as economics  finance  business intelligence  and
industrial applications  time series forecasting itself bares numerous complexity aspects  moreover  with the rapid growth
of big data  time series forecasting algorithms will have to
analyze increasingly massive datasets 
there has been extensive research on using machine learning techniques for time series forecasting  in     several
machine learning algorithms were presented to tackle timeseries forecasting problem  such as multilayer perceptron 
bayesian neural networks  k nearest neighbor regression 
support vector regression  and gaussian processes  in     
the effectiveness of local learning techniques is explored for
dealing with temporal data 
the main goal of this project is to explore the application
of the novel deep learning algorithms for the problem of time
series prediction  deep architecture allows us to construct
complex models that have high vc dimension and able to
describe complex time series  for example  it was shown that
the use of recurrent neural network improve accuracy of
energy load forecasting      for the overview of unsupervised
feature learning for time series modeling  refer to      
starting with traditional statistical approaches like autoregressive integrated moving average  arima   we further
increase vc dimension of models by using machine learning
techniques such as multi layer perceptron  mlp  and support vector regresison for time series forecasting  finally  the
results of arima  mlp and sv regression are compared to
the case of neural network with deep architecture  conditional
restricted boltzmann machine 
as a benchmark dataset for testing and comparison of
forecasting algorithms  we selected m  competition dataset
     in particular  we consider      monthly time series of

different length  each time series is unique with        
of datapoints marked as historical data and         marked
as data for prediction  data set includes several data types 
demographic  finance  industrial  macro and micro economy 

   time series forecasting
starting with algorithms with lower vc dimension  arima 
sv regression  and mlp  we go further and apply deep architecture such as pre training with continuous restricted
boltzmann machine  rbm  and conditional rbm 
    auto regressive integrated moving average
it was decided to start with statistically sophisticated model autoregressive integrated moving average algorithm  arima  
which is one of the traditional algorithms for time series forecasting       autoregressive models accurately predict shortterm temporal structures  but for the case of long term highlevel structures performance drops since these models assume
certain stationary properties of time series 
after tuning three parameters of arima   number of autoregressive terms  number of nonseasonal differences needed
for stationarity  and number of lagged forecast errors in the
prediction equation  high prediction accuracy was obtained 
table   
    multilayer perceptron and support vector regression
to address the drawbacks of arima related to stationarity assumption  it was decided to use machine learning approaches 
machine learning algorithms for time series forecasting
have obtained popularity  and among the most established are
support vector regression and multi layer perceptron      
    
support vector regression is a modification of svm 
where the hypothesis has the form hw b  x    wt x   b and

fideep learning architecture for univariate time series forecasting     

hidden layer

rbm 

hidden layer  h

input layer
input layer

output layer
visible layer  v

output layer

hidden layer

a 

multi layer perceptron

b 

restricted boltzmann machine for multi layer perceptron

figure    network topologies

the original margin constraint is modified to represent the
distance between the continuous output of training example
and our hypothesis output  multi layer perceptron  mlp 
is a heavily parameterized feedforward neural network  fig   
by selecting number of hidden units the complexity of the
model is controlled 
for mlp and sv regression  sliding window is used to
construct features from time series   the lagged time series
values utn          ut   and the value to be predicted is the
next value ut    for one step ahead forecasting   in addition 
smoothing window was applied to time series  to improve
the model generalization  but it introduces high bias in both
support vector regression and multi layer perceptron  so it
was not used in the final results presented 
for multi layer perceptron  sliding window of size n    
gave the model with the lowest combination of variance and
bias for given number of hidden units  which is consistent with
other papers where smaller sliding window shows smaller
bias  fig     table   summarizes performance of mlp and
sv regression 
comparing to arima  which underfits test data in longterm  mlp shows significant improvement  fig   

indeed  big reconstruction error from hidden units back to
visible units prevented us from using it 
there are possibly several reasons why pre training with
rbm did not work for time series     rbm extension to continuous valued input is not appropriate and    dependencies
between input parameters that are not modeled by rbms  regarding the first concern  typical rbm uses binary logistic
units for visible nodes  we used the trick described in     
where continuous valued inputs are scaled to        and then
are treated as probability for binary random variable to take
value    this approach worked for grayscale images      but
for the time series forecasting it was inappropriate  therefore 
another approach described in     was used in the next subsection  where the noise is added to sigmoid units to handle
continuous input to rbm  as for the second concern  a special
form of rbm  called conditional rbm       that takes into
account temporal dependencies was used 

    conditional restricted boltzmann machines
using conditional restricted boltzmann machines that was
presented by taylor           for human motion estimation 
we are aiming to capture most temporal structures of time
series and therefore enhance rbm  for more details refer to
    multi layer perceptron with continuous rbm pre  appendix a 
training
in crbm  connections between current time slice vt and
mlp involves random initialization of weights  therefore pre  several previous time slices vt    vt         vtn   are contraining allows the initial weights land in a better start  re  sidered  fig    these connections correspond to autoregresstricted boltzmann machines  rbm      can find such initial
sive component and therefore model short term dependencies 
weights  fig   b shows the process of pre training  for the
moreover  fixed connections from previous time slices to hidhidden layer in the mlp network  we construct an rbm that
den units are added to capture long term dependencies  in
trains on the inputs given for that layer  the final weights of
the taylors paper  where crbm was applied to a model of
the rbm are given as the initial weights of the layer in the
motion  isolated blocks of time series  mini batches  were
mlp network 
used for speeding up learning  for the motion estimation 
there are several examples when pre training has shown
sequential processing through the training data sequences was
that rbms can improve the final performance  for instance for
unnecessary and therefore mini batches were permuted for betmnist digits classification  however  in the case of lagged
ter learning  in our case  however  sequence of mini batches
time series features  rbm did not improve the prediction  does matter and therefore we do not perform permutation 

fideep learning architecture for univariate time series forecasting     

mini batches of size    were used  and the input time series
were rescaled to have zero mean and unit variance 
since training and testing crbm for      monthly series
takes approximately two days on high end laptop  computational expense was a limiting factor  and therefore limited
number of experiments were performed to determine parameters of the model  the best parameters from the few experiments were   delay       batch size       number of hidden
units     and number of epochs       not optimal  
dependence of prediction error on number of epochs is
shown at fig  
    conditional restricted boltzmann machines   error analysis
performance of crbm for time series prediction was compared to mlp performance fig   a  where testing rmse for
both algorithms are shown for all time series  by time series
id   for short time series  fig   b  series with id from   to    
that have length of around     vertical blue dashed lines  have
better prediction with crbm than mlp  also crbm shows
better performance for several time series with longer lengths 
for instance id      fig   a mlp and fig   a crbm  overall 
by looking at fig   a  even crbm without fine tuning of
parameters and pre training is at least comparable to mlp 

figure    comparing prediction from arima and mlp 

red train  blue test black ground truth
hidden layer  h

   
fixed
fixed

fixed

fixed

   

visible layer  v

   results and discussion
for simplicity of comparison of algorithms rmse metric was
used  although more sophisticated metrics exist     to gauge
accuracy of time series forecasting  table   summarizes rms
error for the different algorithms  cross validation was not
performed for crbm only due to limitation of computational
resources 
note that the algorithms arima  sv regression and
mlp   were severely tuned by empirical search of the best
model parameters  on the other hand rmse for conditional
rbm given above was not obtained by fine tuning  again due
to limitation of computational recourses  moreover  in order
to speed up the learning  number of epochs for training of

v  t   

v  t   

v  t   

v  t 

figure    conditional rbm with delay of size two

crbm was      that is smaller than optimal value  close to
      fig    
there are several ways to further improve performance of
crbm for univariate time series forecasting  first  additional
data cleaning can be performed to remove outliers  second 
by introducing conditional dependence of hidden units longterm structures should be captured better  though it can make
training more complex  third  stack of crbms can be used
to add more layers and to further increase vc dimensions of
hypothesis to capture more complex dependencies 

   conclusion
deep neural network are able to accurately predict time series 
it is clear from table   that conditional restricted boltzmann
machines are comparable or better than our competing methods  as mentioned above  the prediction accuracy can be
table    comparing crbm to other models

figure    rmse of mlp depending on various sizes of
sliding window

learning method

train rmse

arima
sv regression
mlp
conditional rbm 

      
      
      
      

test rmse
      
      
      
      

fideep learning architecture for univariate time series forecasting     

figure    a  comparison of test rmse for mlp and crbm  b  length of train part of time series

fideep learning architecture for univariate time series forecasting     

     b delay   h   hbias  where a delay  and b delay 
hbias
are weighting parameters from visible units to delayed visible
units  and weights from delayed visible units to hidden units 

references
   

ahmed  et al  empirical comparison of machine learning
models for time series forecasting  econometric reviews 
                    

   

bengio  et al greedy layer wise training of deep networks  advances in neural information processing systems         

   

bontempi  et al  machine learning strategies for time
series forecasting  business intelligence  springer        
    

   

busseti  et al  deep learning for time series modeling 
technical report  stanford university      

   

chen  et al  continuous restricted boltzmann machine
with an implementable training algorithm  vision  image
and signal processing             

   

hinton  et al  reducing the dimensionality of data with
neural networks  science                        

   

hinton  et al  training products of experts by minimizing contrastive divergence  neural computing     
              

   

hyndman  et al  another look at forecasting accuracy
metrics for intermittent demand  international journal of
applied forecasting               

   

langkvist  et al  a review of unsupervised feature learning and deep learning for time series modeling  pattern
recognition letters               

    

makridakis  et al  the m  competition  results  conclusions and implications  international journal of forecasting                    

    

taylor  et al  modeling human motion using binary latent variables  advances in neural information processing
systems      

    

sutskever  et al  learning multilevel distributed representations for high dimensional sequences  university of
toronto      

    

forecasters org resources time series data m competition 

figure    selecting optimal number of epochs 

improved by tuning parameters of crbm  computationally
intensive   moreover  further improvement is possible if more
complex models are used that take into account dependencies
among hidden layers and by stacking conditional rbms 
i was disappointed with the performance of multi layer
perceptron with rbm pre training  due to temporal dependencies of visual units is not considered in rbm  yet potentially
with the right lagged time series as an input mlp with pretrained rbm should work  another concern of mlp with
pre trained rbm is the way of handling continuous input 
probably the gaussian nodes with unit variance are not the
best distribution to represent many datasets 

appendix a   energy based models and
crbm learning
as mentioned in section      conditional rbm is a modification of rbm with additional connections fig    conditional
distributions that are used for propagation from visible units
vi to hidden units h j and vise versa are given by 
 j    v  w  j  
p h j     v    g hbias
 i    h  w i      
p vi  h    n vbias
where g is a logistic function  and n m     is normal distribution that models continuous valued input for rbm  loglikelihood is given by in form of free energy 
e v  h    log v  h    
i

 i   
 vi  vbias
 j  h j
  hbias
  
j

  vi h j wi j
i  j

the main idea is include directed connections between
visible units  data from previous time steps  into a dynami i   and the directed connections from
cally changing bias vbias
 i  
previous time steps of visible units to hidden units as hbias
for the learning rule we use standard update of weights and biases using contrastive divergence      update of dynamically
     a delay   v   vbias and
changing biases is given by  vbias

    

https   gist github com gwtaylor        

fi