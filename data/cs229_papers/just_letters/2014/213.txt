direct data driven methods for decision making under uncertainty

junjie qin
institute for computational and mathematical engineering  stanford  ca       usa

   introduction
we are constantly making decisions under uncertainty  a
widely used formulation for decision making under uncertainty can be summarized by the following optimization
program
u    argmin ep  f  u  x   
   
uu

where the future cost f depends both on the decision u  u
as well as the outcome of uncertain events  represented by
a random variable x  x   here the random variable x
follows a distribution p which is assumed to be known in
order to form the expectation in problem      examples
includes making an inventory decision with uncertainty future demand  purchasing stocks with uncertain information
about the future price  and deciding which phd programs
to choose to go with uncertainty in research directions  advisors and funding opportunities 
in practice  the distribution p is never readily available  instead  practitioners often resort to the following two step
procedure 
model
of
data learning
stochastic decision
 uncertainty 
 sn   methods
 u   
optimization
 p 
that is  given a historical data set sn   one would first apply
certain machine learning algorithms to obtain representations of the data  usually in the form of point prediction or
parameters for the distribution p  and then use these representations to form the stochastic optimization program    
which in turn would lead to an optimal decision  this
procedure has many known issues  first of all  off theshelf learning algorithms are derived using loss functions
that are mathematically convenient  but may have nothing
to do with the actual economic costs regarding the decision
that one is making  for instance  commonly used linear regression algorithm assumes a quadratic error loss  whereas
the actual cost function may be a different function such
as a piecewise linear function in the inventory control example  as such  the learning step in the above procedure
is sub optimal with respect to the true economic cost  furthermore  as the stochastic optimization step sees the model
of uncertainty which is a summary of the data instead of the

jqin   stanford   edu

data itself  it may make assumptions inconsistent with assumptions made in the learning step  an example would be
using gaussian error assumption in the stochastic optimization step whereas a     type regularization was used in the
learning step  which implies the assumption that the error
follows the laplace distribution   last but not the least  as
different assumptions and approximations may be used in
each step of the two step procedure  it is usually very hard
to theoretically gauge the performance of this two step procedure 
an attractive proposal is to integrate these two steps  a
couple of existing papers have explored this idea within
particular application domains  liyanage   shanthikumar        proposed the concept of operational statistics
that drives the optimal estimator for the newsvendor ordering target under uncertainty  their methods assume the
functional form of the distribution for the uncertainty  e g 
the net demand follows an exponential distribution  and
equivalence type argument to reduce the hypothesis class
of all possible estimators  kao et al         considers the
setting that the results of a regression are used for solving
decision problems whose cost function is an arbitrary convex quadratic function  it is shown in the paper that by
using a convex combination of the results produced by ordinary least squares and empirical minimization  the actual
cost generated by their algorithm is significantly lower than
directly using the results of ordinary least squares  a few
important question still remains open   i  what is a suitable general formulation for deriving methods that directly
identify the optimal decision from the data   ii  are there
algorithms that have provably guaranteed performances under this general formulation 
this paper makes progresses in addressing these questions 
in particular  we adopt and modify the framework of statistical decision theory  which is the root of all modern learning algorithms  to incorporate the actual economic costs in
section    this leads to a risk minimization problem which
again depends on the unknown underlying data generating
distribution  we show that empirical risk minimization can
lead to a class of efficient algorithms whose performance
can be theoretically analyzed  section   then applies these
general consideration to a specific problem on dispatching

fienergy generators to meet uncertain demand in the context
of renewable integration  we then propose algorithm that
follows empirical risk minimization paradigm as well as
variations of it  theoretical guarantees are then derived followed by empirical test results with data from bpa 

   formulation
given a family of distributions p    p        and
iid

samples x            xn  p   the goal of statistical decision
theory is to identify a good procedure  that maps the data
x    x            xn   to a decision d  d that has small risk 
the notion of risk is defined as the expected value of a loss
function l   d   i e  
r       ep  l    x    
where the loss function l   d  assigns preference to decisions given the value of the model parameter  and d  
 x   this theory is usually used for the purpose of estimating the parameter  itself of some known function of 
and so the resulting procedure  is often called an estimator 
to cast our problem into the framework of decision theory 
we use the following specification or modification
 we make no assumption on the form of the distribution p   that is  instead of say assuming the uncertainty follows a normal distribution and then estimating its mean and variance  we allow arbitrary type of
distribution in the family p   in a sense  this forms a
nonparametric estimation problem in the classical terminology  as such  we drop the index  and use p
itself to refer to an arbitrary member of the family of
the unknown distribution p 
 we set the loss function to be the true economic cost
of the problem  that is
l xn     d    f  d  xn     
where xn    p denotes the unobserved uncertainty
in when we are making the decision  i e  x in      
notice that this modifies the conventional loss function which is a deterministic function of the unknown
parameter  and decision d to a function that depends
on the random realization of xn     the resulting risk
function is
ep f  u  xn     
where we modified the notation from estimator  to
decision u 
 
we would still have to make implicit technical assumptions
such as the mean of the loss function exists 

the remaining problem is to design procedures that maps
the data to a good decision u that minimizes the risk  the
challenge is that since p is unknown  it is in general impossible to find procedures that are uniformly optimal with
respect to all possible members p  p  thus the bulk of
the classical decision theory concerns about alternative notations of optimality  which leads to uniformly minimal risk
unbiased estimators  uniformly minimal risk equivariant
estimators  optimal bayes estimators  and minimax estimators  cf  lehmann   casella        for a good treatment of
this subject   however  none of the above optimality criteria permits universal procedures to derive algorithms which
could identify the optimal decision  that is  calculation has
to be done based on the particularly assumed distribution 
and for different distributions the methods and results vary
significantly 
we propose 
instead  to minimize the empirical risk
pn
   n  i   f  u  xi   within a pre determined hypothesis 
class that contains functional forms for u  this would certainly generate efficient algorithms if for examples the cost
function is convex and the hypothesis class is linear  in
more complex situations  non convex optimization procedures may be deployed to identify the optimal hypothesis in the hypothesis class  we will show by an application in the next section that it is possible to prove theoretical performance guarantees for the resulting procedures
which suggests that the sub optimality with respect to the
true risk r  which is defined using the unknown distribution p  is bounded with large probability and the bound
approaches to zero with the number of samples increases
to infinity  the application consists of a specific choice of
the cost function for a practical situation  and the hypothesis class  but both the proposed algorithms and the performance analysis could be generalized to other cost functions
and hypothesis classes 

   application  electric power dispatch for
renewable integration
the rest of this paper concerns with an application that is
of an increasing importance both in the united states and
around the globe  as global warming becomes a growing
consensus  many countries around the world are pushing
deeper renewable penetration into their energy generation
portfolio and their electric power grids  this results in significant challenges in the operation of the grids as renewables are intrinsically variable  i e   they are intermittent 
uncontrollable and random  figure    a  depicts the wind
power generation in a bpa region over    days  and figure    b  gives common percentage forecast errors for the
 
this use of the term of hypothesis follows leaning theory
instead of the literature of hypothesis testing  although they are
closely related 

fiwind with different forecast horizons  it is evident that as
the forecast errors for the wind is significant at day ahead
 around     in figure   b    an explicit modeling of its uncertainty and its impact is necessary  see  qin et al       a 
and  qin et al       b  for more backgrounds 

the temperature  wind speed  and other relevant information about hour i  as well as some nonlinear transformations of the some of the features available  denote the historical data set by sn     x    d              xn   dn     where
xi  rm   we are also given features regarding the delivery hour which we are making a dispatch decision about 
denoted as xn     now our goal is to find a mapping from
xn   to u that has small risk using data sn   as in section
   the true population risk is defined using economic cost
of the system  which coincides with the objective function
of the stochastic optimization      i e  

    
    
    

    

forecast error

wind power generation  mw 

    

    

    
    

    
   
    
    
    

   
 
 

    

  
  
time  hour 

  

 a  wind generation for    days

r   ep  cu   q d  u     

forecast error
typical dispatch stages

 

 

 

 

  

  

  

  

time horizon  hour 

 b  forecast error     for wind      algorithms

figure    renewable is variable and difficult to forecast 

     problem statement
the problem that we are concerning is the dispatch of conventional generators  which are slow and have to be notified
and scheduled at    hours ahead of the delivery period  in
a grid with substantial amount of the renewables in the system  this is a challenging problem in the sense that when
the decision regarding the conventional generator is made 
no precise information about the net demand  defined as
the actual demand minus the renewable generation is available  we can formulate this problem as a specific instance
of the general decision making under uncertainty problem
that we discussed in the previous sections  in particular 
if the distribution for the net demand is p  then one would
like to identify the optimal amount to be scheduled for the
conventional generators by solving the following stochastic
optimization
u    argmin ep  cu   q d  u     

   

as the true population risk cannot be evaluated without the
knowledge about the underlying distribution p  we minimize the empirical risk instead  furthermore  we restrict
ourselves to the hypothesis class of linear functions of the
feature xn   for tractability  note that this does not reduce
the generality of our procedure a lot because as mentioned
before we can always add nonlinear transformed features
into the original list of features to capture any nonlinear
effects  within the linear hypothesis class  the dispatch decision u can
by a weight vector w  rm  
pmbe represented
j j
i e   u   j   w xn    
we proposes three algorithms listed as follows 
 algorithm    empirical risk minimization  erm 
n

min
w

with its solution denoted by werm  
 algorithm    erm with least squares regularization

uu

where u is the dispatched slow generator at day ahead with
cost per unit generation being c dollars  d   dmin   dmax  
is the future net demand for the system   d  u    
max d  u     is the unserved demand with per unit
penalty being q dollars  in california  the average per unit
cost for slow generators  i e  c  is around     per mw 
whereas the penalty for each unit of unserved demand  i e 
q  also referred to as value of lost load  is in the range of
           per mw  so we assume that q   c 
as a matter of practice  the actual distribution for the net
demand d is unknown  instead  we have observation of
the net demand over historical hours  together with other
relevant data that we refer to as features  for instance  for
a historical hour i  we may have records of the net demand
di and a vector of features xi with its entries recording

 x  
cw xi   q di  w  xi   
n i  

n

min
w

 x  
cw xi  q di w  xi     ls kdxwk   
n i  

with its solution denoted by werm ls  
 algorithm    erm with    regularization
n

min
w

 x  
cw xi   q di  w  xi        kwk   
n i  

with its solution denoted by werm     
here the firstp
algorithm is directly minimizes the empirical
n
risk r   n  i   cw  xi   q di  w  xi    within the
hypothesis class  algorithm   is proposed for the case that
some of the underlying data generating features may not

fibe included in the model  if the missing components can
be approximated with normal distribution  by the virtue of
the central limit theorem   the least square regularization
would improve erm  the last algorithm is proposed for
the case that certain automatic feature selection is needed 
if the number of features is large  the    regularization in
algorithm   would be useful to reduce over fitting 

bounds in theorem   to be informative as well  because
in general whenever the empirical risk and population risk
are close enough  which is ensured by the bound given  
the distance between their minimizers should not be far 
     numerical results

we test all three algorithms with one year of hourly wind
and demand data from bpa  http   transmission 
     performance guarantees
bpa gov business operations wind   
the
we have the following guarantees on the performance of
three proposed algorithms are compared with a benchmark
werm  
algorithm which is separated estimation and optimization
 seo   that is the two step procedure discussed in section
theorem    uniform convergence bound   for i i d  data
   here since for the deterministic case of     the optimal
 x    d              xn   dn    suppose that we restrict to the
solution is clearly u   d  the seo reduces to performs
weights satisfiying kwk   w max and suppose that
a least square regression for the demand  we have tested
ekxi k    x max      then with probability at least     
three sets of features  the first set of features consists of the
the excess risk of erm is bounded as
last net demand observation and hour of the day  where the
r
max max
hour of the day is used to capture seasonality  the second
 
log    
  q

c w
x

 
  set of features consists of the net demand over the last   
 r werm  r w     
n
n
hours and hour of the day  the last set of features includes
where w  is the minimizer of the population risk r 
the net demand over the last    hours  order statistics  of
the net demand and hour of the day  all three cases conthis result suggest that with probability
     the excess
tains a constant feature representing the intercept  so that

risk of erm diminishes as o    n   i e   as the number of
the number of features for these three cases are m     
samples grows to infinity  the result of erm is near optimal
m       and m       respectively  the tests are conin the hypothesis class with large probability  one unsatisducted in a rolling horizon fashion  for each hour in which
factory fact regarding the previous result is that it does not
a dispatch decision has to be made  the data corresponding
show how the algorithm performs as the number of feature
to the past n hours are used as the sample data set  the
m changes  using tools from algorithmic stability theory 
same procedure is repeatedly tested for all n  n hours 
the next results bounds the generalization error of algowhere n                  is the total number of hours
rithm   and explicit shows the dependence on the number
in the data set  
of features 
the results for our experiments are shown in table   in
theorem    algorithmic stability bound   under assumpthe form of the percentage cost reduction compared to the
tions of theorem   and w l o g  assume  dmax      dmin   
benchmark algorithm  from the results in the table  we can
with probability at least      the generalization error of
observe that with small number of features  m       the
erm is bounded as
cost saving of erm increases slowly as the number of samerm
erm
ples increases  while with more features  m        the av r w
   r w
  
r
erage cost saving grow dramatically as the number of samlog    
max m
max
ples grows  when the number of features is large erm l 
  q  c d
   q  c d
  m   
n
 n
over performs erm which suggests that the automatic feature selection is beneficial even with    features  in all our
where     q  c  c 
 

this result
shows that generalization error scales as

o m  n   note the second term in the bound  so that
when we have a large number of features  the sample size
has to grow much faster to ensure the same risk bound 
note that the generalization error is different from the excess risk and is only the difference of between the population risk and empirical risk both evaluated at the point
produced by the erm algorithm  as the algorithmic stability theory concerns the output of the particular algorithm 
it does not bound the risk difference with the true population risk minimizer  however  we would expect the error

the set of all linear combinations form so called l statistics 
they are widely used as estimators of quantiles  note that for
the problem of our interests  the optimal solution of the stochastic optimization can be solved analytically if there is no feature 
and the optimal solution is a quantile of the unknown net demand
distribution 
 
although this is not a conventional learning problem  in the
learning language  we would say the training data set has size n
and for each of the n  n experiments the test data set has size   
using a larger test set  i e   deciding the optimal decision for more
than one future hours would not make sense in our application as
the most recent piece of information has the largest information
content regarding the optimal decision 

fitable    percentage cost reduction for various number of samples and number of features 

n    
n    
n    

m  
erm
    
    
    

m  
erm ls
    
    
    

m  
erm l 
    
    
    

m   
erm
    
    
    

m   
erm ls
    
    
    

experiments  erm ls does not work well in general for
this application  especially comparing to erm l  
as all the cost numbers vary significantly from experiments
to experiments  figure   gives the box plots for two settings  which suggests that all the proposed methods have
much smaller spread  variance  in the realized costs and
erm has smallest over the three proposed algorithms 
 

   

 

x   

   

   

   
cost

 

cost

 

x   

 

 

   

   

 

seo

erm

erm ls

erm l 

 a  n        m    

 

seo

erm

erm ls

erm l 

 b  n        m     

figure    box plots  spreads  of the costs for various methods and
scenarios 

m   
erm l 
    
    
    

m   
erm
   
    
    

m   
erm ls
    
    
    

m   
erm l 
    
    
    

p

  log     n  bousquet et al          for the linear hypothesis class with bounded    norm  
the rademacher complexity is bounded by w max x max   n  the observation
that the loss function is lipschitz continuous with coefficient q  c translates the rademacher complexity of the
hypothesis class to that of the loss function class and completes the proof 
proof sketch of theorem    as routine algorithmic stability proofs  we have to establish that the algorithm is stable  in fact uniformly stable   one can show  with very
tedious algebra  that for our problem and the erm algorithm  the stability coefficient is    dmax  qc   m  cn  
furthermore  the cost function is uniformly bounded by
k   dmax  q  c   invoking a standard algorithmic stability theorem  bousquet et al         gives the bound that
p
 r werm  r werm         n k  log      n 
and completes the proof 

references
   conclusion and future directions
this project proposes  theoretically analyzes  and empirically tests methods that directly solve stochastic optimization using the data  instead of performing separated estimation and optimization procedure  the application of generator dispatch with renewables is studied in depth to serve
as an example of general formulation and methods proposed  empirical results show that our algorithms lead to
great cost savings        for the energy grid operator 

bousquet  o   boucheron  s   and lugosi  g  introduction
to statistical learning theory  in advanced lectures on
machine learning  pp          springer       
kao  y   roy  b v   and yan  x  directed regression  in
proc  advances in neural information processing systems  pp               
lehmann  erich leo and casella  george  theory of point
estimation  volume     springer       

future work that develops bounds for regularized version
of the algorithm  improves the theoretical understanding on
the effect of model mis specification  investigates how estimators from statistics and operational statistics can serve
as features and improve the performance  and tests extension of the algorithms such as kernelized versions are of
interest 

liyanage  liwan h  and shanthikumar  j george  a practical inventory control policy using operational statistics 
operations research letters                       
issn           

a  proof sketches

qin  j   zhang  b   and rajagopal  r  risk limiting dispatch with ramping constraints  in proc  ieee international conference on smart grid communications
 smartgridcomm    pp              b 

proof sketch of theorem    this result relies on a theorem
that bounds the excess risk with the sum of   times of
the rademacher complexity of the loss function class and

qin  j   su  h   and rajagopal  r  storage in risk limiting
dispatch  control and approximation  in proc  american
control conference      a 

fi