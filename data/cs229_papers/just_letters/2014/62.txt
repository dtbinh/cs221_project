cs    project final report

dec          

machine learning for continuous human action
recognition
tian tang
department of electrical engineering  stanford university
tangtian stanford edu
abstractin this term project  we consider the
problem of automatic recognition of continuous
human activity  our source data are short videos with
rgb and depth information of seven predefined
categories of human action as well as long videos that
contain a series of continuous actions  by extracting
frame level features that represent each action and
are invariant to small environmental noise  we train
the model with part of our data and test it with the
remaining data and then compare the precision of
svm and order representation model with different
choice of features 

i  introduction
this project falls into the domain of computer
vision and artificial intelligence  where continuous
human motion analysis and recognition is one of the
most attractive topics  there are tremendous amount
of work on this field during the past decade based on
static images and  d videos  with the development
of depth sensors  we can make better cognition
algorithms utilizing depth maps  to achieve this 
previous work proposed many ideas on feature
choice and extraction  such as sparse interest points 
dense trajectory and skeleton information  etc 
however  challenges lie in several aspects  the
difficulties in video and image processing still exist 
variations in the environments where the actions
take place are an important source of uncertainties
and failures in recognition tasks  feature selection
affects both accuracy and computational expenses of
the algorithm  which makes it crucial to the project 

the exact problem we address in this paper can
be described as follows 
to start with  seven basic action categories 
drinking  eating  using laptop  reading cellphone 
making phone call  reading book and using remote 
are defined and videotaped  these source videos are
provided in rgb d by gang yu et al online      by
modeling each motion  we aim to achieve the
following tasks 
    same environment action recognition 
training videos and test videos are taken in
same environment  each action is performed by
different people in their own ways which suggests
large intra class variations  the algorithm should be
robust to recognize each action precisely 
    cross environment action recognition 
training videos and test videos are taken in
different environment  we should be able to
eliminate the interference of environmental factors
and recognize the action correctly 
    continuous action recognition 
test videos contain a series of continuous
actions performed by one person  where we dont
have prior knowledge about the commencement and
the termination of each action  we are expected to
segment a continuous human activity into separate
basic actions as defined and correctly identify each
one of them 
after preprocessing the original video episodes 
we choose and extract frame level features that can
effectively represent each action in consideration of
computational complexity as well as model
precision 

fiin this paper  we adopt the frame level skeleton
method proposed in     that extracts four types of
features from rgb skeleton joints as well as depth
maps  the feature selection method will be further
elaborated in part   
in part    i will explain the order representation
model used  test results and discussion will be given
in part   and   correspondingly while conclusion and
future research interests will be shown in part   and
  
ii  data and implementations
the first table below shows the data sets used in
this paper  there are    performers who perform  
different actions in two environment settings in total 
each person demonstrates each action several times
with slight differences  which implies varieties  note
that s   s  and s  are in the same background
setting  and s   s  are in a different setting 
the second table shows the training and testing
set s  used for each task 

special configuration of    skeleton joints and
represent each action with four types of features
derived by calculating the relative distance of the
chosen joints  first  we need to preprocess the video
and segment the foreground in each frame of the
sequence and acquire its skeleton 
    preprocessing and skeletonization
skeletonization has been widely used in human
motion analysis to generate skeletonized image of
human subject in foreground  thus the accuracy of
preprocessing and segmentation of the subject in
each frame of the video sequence may affect the
overall precision of the algorithm  which makes it
crucial 
traditional skeletonization methods contain the
following steps  threshold segmentation which yields
a binary image  morphological operations  dilation 
erosion  connected component labeling etc   which
give the foreground of each frame  here  we use the
universal background subtraction algorithm proposed
in literature     called vibe  the overall idea of this
algorithm can be described as below 

table i  data set information
dataset

episodes

performers

action s 

time

set  

  

no     

   sit 

n a

   

no     

 

 

   

no      

 

 

   

no       

 

 

  

no       

continuous

 

 s  
set  
 s  
set  
 s  
set  
 s  
set  
 s  
table ii  implementation
task number

training set s 

test set s 

task  

s 

s 

task  

s   s 

s 

task  

s   s  and s 

s 

iii  feature selection
in this paper  we adopt the frame level skeleton
joints methodology proposed in     that selects the

picture i  flowchart of preprocessing method

    feature extraction
in this paper  we use expressions and characters
in consistent with previous papers 

fisuppose the training dataset has  videos 
r                         and         refers
to the label of the video  v    i    i     it    where
it   t         t refers to a frame of the rgb d
video sequence  for    the skeleton is denoted as
                     where              is
the coordinate of joint on the  th frame and
      is the total number of joints 
here  we have   types of features  three of
which are extracted from human skeleton  rgb
video  and one is extracted from the depth video to
get the object shape and position 
 i  pairwise joint distance 
      sit  sjt 

   

 ii  spatial coordinate of a joint 
      xit or yit or zit
   
 iii  temporal variation of a joint location 
      sit  sit 
   
where  is a time duration 
 iv  object feature 
it is crucial to gain object information  shape
and position  in this human object interaction motion
recognition problem  by using local occupancy
pattern  lop       we can obtain the potential object
positions  here  we give the extracted object feature
directly 
                   
              
   
where                           
 

          is a parameter   is the number
of cloud points of each cell for each potential object
position 

insusceptible to background noises  moreover  this
feature extraction method is comparatively robust
and insusceptible to background noises  which is
very useful in cross environment action recognition
tasks 
iv  models
as we mentioned before in introduction section 
there are large intra class variations in human actions
as same actions are performed differently in speed 
style and environment  this makes automatic action
recognition even more challenging 
to deal with this problem  we use order
representation model based on our selected features 
for each action  there are certain joints that play
more important roles than other joints in defining the
action itself  take making phone call as an
example  the hand ear distance should be small for
every demonstration no matter what the human
subjects leg positions are  that is to say  the
hand ear joint pair should be of the smallest norm for
action making phone call 
moreover  object features are also important in
helping recognize an action  for example  even with
the information of the object is in humans hand 
we still need more information about the object itself
to decide whether the person is eating  drinking or
using cell phone 
given a video episode with a certain label of
action type  we can get a series of video frame
corresponding to time  within each frame  we extract
four types of features from the rgb d video  denote
 f 



as the complete feature set of type f  then we

have                                  
                                 
here  we define size n order indicator p as in
    as 
p    op   k 
   
picture ii  examples of extracted joint features

the major merit of this feature selection method
is that it successfully transforms a large amount of
rgb d information into low level joint features and
their euclidian distances  moreover  this feature
extraction method is comparatively robust and

 f 

 f 

 f 

 f 

where op    i    i       in   is a subset of   
k is the index of element of op with the
minimum value  
the response of p on video frame it is
defined as 

fivp  it      

  

 f 

 f 

 f 

ik  ij for all ij  op

  

   

otherwise

thus  the representation of video v with t frames
on p can be written as 
vp  v    tt   vp  it  
   
this value of vp  v  should be high for relevant
videos and low for less relevant videos 
given a size    n      order indicator p and a
threshold p   we define the classification
function fp p  v  as 
fp p  v      vp  v    p  

   

then  to address the problem of different action
demonstration speed  we need to add a normalization
weight to each frame to get videos of same durations 
we define classification error as 
 

nr
p   n i  
  fp p  v   yi  
r

   

where yi         is the label of the video 
the optimal threshold value can be gained by
minimizing classification error p   minp p   for
simplification  write fp p with optimal p as fp  
based on the above classification error  we can
sort all size   order indicators   and remove the
redundant ones to form a compact set by using
threshold   if p    then we keep p and move on
until we finish with all indicators of size   and get
the updated size   order indicator set    
similarly  we can update size   order indicators
to size  order indicators with smaller classification
errors  finally we get an order indicator pool   
as output 
after obtaining distinct pools for the four types
of features  we need to combine them to get the final
order indicator pool                        
upon doing so  weights should be determined for
each    for each action category 
here  we use the boosting method in literature
     called weak learn  the weak learners are
trained to find a weak hypothesis while trying to

maintain a distribution over the training set 
after applying weak learn algorithm to
classification function fp p  v  defined in equation
     we get the revised classifier g v  
g v      m
m   m    fm  v        
m
m   m    fm  v       
    
where m is the number of weak learners  fm is the
m th weak classifier and m is its weight 
for multi class recognition problem we have
here  the number of categories is    i e  c     
for each video v  the classifier gives   and  
label on action category and the video belongs to the
action category c  with largest weighted sum
 one to one mapping  
c
c
c    arg maxc m
m   m    fm  v       
c  v 
  fm
     
    
                                            
algorithm order indicator pool                 
input  initial order indicator set    indicator size 
output  order indicator pool   
       
   for       do
       p  p     p    
  
  
  

         
     
    
 f 

  

r         op   where p   do

  

for             do

  

p    op           

 f 

   
        p
   
end for
    end for
    end if
    end for                                   

v  results
apply the order representation model
introduced in section   on our dataset  and compare
the result with svm  full feature sets   we get the
following table with training and test errors for each
model and task 

fitable iii  table of results
task    training set  s       samples   test set  s      
samples  
models
pairwise

joint

training error

test error

distance

     

     

spatial coordinate features

     

     

     

     

object features only

     

     

weak learn algorithm boosted

     

     

     

     

features only

only
temporal variation features

vii  conclusion
in this paper  we applied order representation
model as well as svm to same environment  crossenvironment and continuous human action
recognition tasks  the results of the first two tasks
are acceptable  and the comparatively low precision
of continuous action recognition indicates the big
challenges in computer vision where more effort can
be paid 

only

order representation
svm

task    training set  s  and s       samples   test set  s 
     samples  
order representation

     

     

svm

     

     

task    training set  s   s  and s       samples   test set 
s      samples  
order representation

 

     

svm

 

     

viii  future
we can further improve the algorithm to reach
higher precision and shorter running time  for
example  for the action drinking  we are able to tell
whether the tester is drinking water or soda and we
can make the recognition process real time   both
training error and test error are high for task   and   
we should try to use more features or try other
features  hidden markov model  hmm  also gives a
promising future in dealing with the temporal
relationships inherent in human actions 
references
    data source 

vi  discussion
the results gained using order representation
model are consistent with our common senses  the
overall precision of same environment action
recognition is the highest of all three tasks  followed
by cross environment action recognition and then
continuous action recognition  weak learn boosting
algorithm gives better precision than svm with same
feature choice while there is still room to improve
them both  this high bias problem can be fixed by
using larger set of features or by using other feature
choices  this also makes sense as the size of feature
pool we use to identify each action is much smaller
than the originally defined feature sets 
according to leave one out cross validation
results  pairwise joint distance feature contributes
most to our model accuracy  while weak learn
boosting contributes least 

http   research microsoft com en us um people zliu actionrecorsrc 
    gang yu  zicheng liu  junsong yuan  discriminative orderlet
mining for real time recognition of human object interaction  accv
    
    anjum ali  j k  aggarwal  segmentation and recognition of
continuous human activity 
detection and recognition of events in video       
    olivier barnich  marc van droogenbroeck  vibe  a universal
background subtraction algorithm for video sequences  image
processing  ieee transactions        volume     issue    
    j  shotton  a  fitzgibbon  m  cook  t  sharp  m  finocchio  r 
moore  a  kip  man  and a  blake  real time human pose recognition in
parts from single depth images  cvpr       
    j  wang  z  liu  y  wu  j  yuan  mining actionlet ensemble for
action recognition with depth cameras  cvpr       
    robert eschapire  a brief introduction to boosting  sixteenth
international joint conference on artificial intelligence       
    j  wang  z  liu  j  chorowski  z  chen  y  wu  robust  d action
recognition with random occupancy patterns  eccv      

fi