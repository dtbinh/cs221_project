identifying arrhythmia from electrocardiogram data
taylor barrella  samuel mccandlish
december         

overview

   
mlii lead hvl

 

in this project  we use machine learning to determine
when a persons heart is beating irregularly    this condition is known as cardiac arrhythmia  to do this  we
use data from an electrocardiograph  a device used to
measure a trace of a persons heartbeat via electrodes
placed on their skin  during a heartbeat  heart muscles
electrically polarize and depolarize as they contract  and
the machine records these events are deflections on an
electrocardiogram  ecg  trace  our goal is to analyze
ecg data to determine whether or not arrhythmia is
present 
a normal heartbeat occurs at a relatively steady rate 
with the various chambers of the heart contracting for
a certain amount of time and in the correct order  significant deviations from the normal timing of these contractions indicates arrhythmia  this deviation can be
classified by the specific area of the heart that is beating
abnormally  the heart rate  or the physiological cause 
to identify arrhythmia  we have taken two separate
approaches  the first approach  single beat classification  uses basic knowledge of physiology along with a
support vector machine  svm  to analyze each individual beat in an ecg  reporting any abnormal beats  this
approach was highly successful in analyzing an ecg from
a previously seen patient  and moderately successful for a
new patient  the second approach uses neural networks
to analyze the ecg in a more holistic way  without using
any knowledge from physiology  this approach  while
less successful  improved significantly over the baseline
and appears promising for future work 

 

p

t

qrs

   
   
   
    
   

   

   

   

   

time hsl

figure    a beat can be decomposed into waves known
as p  q  r  s  and t  which correspond to polarization
and depolarization of different parts of the heart 

 labeled   xx  were selected to display less common arrhythmias that would not be present in a random sample  the sampling rate is     samples per second  with
a reading made over a    mv range 
each record was annotated by two or more cardiologists  and the annotations were reconciled to yield a single annotation set per recording  each beat is classified
by type  and we group these classes into either normal
or abnormal 
for our first model  some pieces of the data were discarded  four of the    ecgs  come from patients using a
pacemaker  a device that electrically stimulates the heart
to beat normally whenever it fails to do so  paced beats
from these patients appear significantly different from
normal beats  and we exclude them to avoid difficulty in
patients without pacemakers  a more thorough analysis
would include these records 

data

all of the records include recordings from two leads
placed on the chest  one lead is always placed in the
mlii  modified lead    position  while the other varies
between the v   v   and v  position  because of the
inconsistency in the position of the second lead  we only
use the recording from the mlii lead  in addition  the
lead used in record     is not clear  so we exclude it from
our analysis 

for our analysis  we have used data from the mitbih
arrhythmia database         this database contains   
half hour excerpts of two channel ecg recordings  of
which we exclude five as described below  these were obtained from    different subjects studied by the bih arrhythmia laboratory between      and       twentythree recordings  those labeled   xx  are from a mixed
population of hospital patients  while the remaining   
 cs    cs    

barrella stanford edu
samsamoa stanford edu
  we thank cs    ta dave deriso for this suggestion 
 cs    

  records

 

                  and     

fimlii lead hvl

   
   
   
    
 

 

 

 

 

 

time hsl

figure    a sample of ecg data  taken from record       detected beats are denoted by a red dot  the fourth
beat shown occurred prematurely and is classified as apc  or atrial premature contraction 

 

beat classification approach

 delay between the current beats peak and previous
beats peak  this captures the heart rate 

arrhythmia is typically diagnosed by detecting irregular beats in an ecg      hence  in this approach we will
separate an ecg recording into its constituent beats and
attempt to classify each beat as normal or irregular  our
goal is to use a minimal amount of information about the
physiology of the heart to identify and classify beats using machine learning  rather than building a complicated
model of each beat 
in this approach  we will build two types of models 
the first model is an individualized model  built for a
single patient  this would be useful when a person is
being tracked routinely for arrhythmia  and a sample of
the patients data has already been evaluated by a cardiologist  the second model is trained on a collection of
ecg records  with the goal of identifying arrhythmia in
real time on a previously unseen patient  both models
use the same method  and differ only in the training set 

   

 mean and mean squared voltage of the beat  this
captures the amplitude and duration of the waves 
 maximum voltage derivative and its relative timing 
this captures the speed of the r wave 
 mean absolute derivative and mean squared derivative  this captures the total amount of deflection 
using only these simple features  we are interested to
see how well our model can learn the complicated set of
rules used by cardiologists to identify abnormal beats 

   

model

we use a support vector machine to classify each beat
in a given patients record as normal or abnormal  to
use the highest dimensional effective feature space possible  we used the radial
 basis function kernel k  xi   xj    

features

 

exp    xi  xj      where      is a tunable parameter  we also include a regularization parameter c  since
we do not necessarily expect our data to be linearly separable 
we used the libsvm library     to perform training
and prediction  to do this  we first scaled our features
to lie in the range         in order to use the radial basis
function most effectively      we then found the optimal model parameters c   by using the grid py script
included with libsvm to do a simple grid search  minimizing the error for   fold cross validation 
using the method described above  we build two different types of models  for the first type  we train a
personalized model svm on a single patient record 
we choose a random subset of     of the beats to be
test beats  and the remaining     to be training
beats  a typical record contains about      beats  we
train the svm on the training data from a single patient
record  so that the test beats can be classified as one
 maximum and minimum voltage during a beat as would expect in a scenario of ongoing monitoring 
well as their relative timing  this captures the main
for the second type of model  we train the svm on a
characteristics of the r wave 
collection of patient records  we choose a random subset

for this analysis  we model an ecg record as a series
of beats  because a beat appears as a sharp peak in the
ecg  we are able to extract a list of beats by finding
deflections with slope above a pre determined threshold 
given a peak  we extract a slice of the ecg signal centered at that peak with a pre determined width  the
collection of these slices forms a series of beats 
each beat can be separated into a series of waves
known as the p  q  r  s  and t waves      these waves
represent electrical activity in the various parts of the
heart  the r wave  which corresponds to depolarization
of the main mass of the ventricles of the heart  can be
identified as the sharpest peak in a beat  other waves
are identified as the peaks and troughs in the vicinity of
the r wave 
because we only want to use a minimum amount of
physiological knowledge  we extract the following fairly
generic features from each beat 

 

fias many abnormal beats as possible  in case they are signaling an emergency  the f  measures are shown in fig 
   the models for all but two records       and      
 
scored highly  these records contain a large number of
normal beats occurring in irregular rhythm  which are
 
difficult to categorize  in future work  it may be beneficial to devise a method to consider normal beats with
 
abnormal rhythm separately 
the real time model was  as expected  less success 
ful than the personalized models  seven of the nine
   
   
   
   
   
   
   
test records performed moderately well  while records
f  score
     and      had perfect precision but very bad recall  record      contained many abnormal beats that
looked normal in the mlii lead  an model of both leads
figure    histogram of f  scores for the    personalized
would likely have been more successful 
models  all but two records       and       were very
successful 
frequency

 

   

analysis

frequency

   

the personalized models were surprisingly effective in
this setup  this appears to signal that classifying a beat
as normal abnormal can be done relatively unambiguously when given prior access to a classified sample of
the subjects beats  we expect that this type of classifier could be useful when a patient is subject to ongoing
monitoring  a cardiologist could classify a small number of normal and abnormal beats  then allow the model
to automatically classify the rest rather than tediously
analyzing hours of ecg recordings 
the real time model was less effective than the personalized models  but still worked well for many of the
test records  the two records that it failed for      
and       were both in the group of recordings selected
to display less common arrhythmias  so it is not surprising that the classifier was not as successful  future
work should include a method for including uncommon
arrhythmias in both training and testing sets  perhaps
by including ecg data from other databases 
as a further refinement of the model  it would likely be
useful to use a sample dependent regularization weight 
this would allow us to ensure that rare forms of arrhythmia are not treated as anomalies by the svm  and penalize their misclassification more heavily  because our objective function does not take into account or preference
for recall over precision  we expect that using weights
would improve our results 
an interesting extension of this problem would be to
classify abnormal beats by their type of arrhythmia  this
could be done by training a multi class svm  which
works by training multiple sub models for each pair of
classes  we leave this for future study 

   
   
   
   
   

   

   

   

   

   

f  score

figure    histogram of f  scores for the real time
model  evaluated on   different patient records  all but
two records       and       were moderately successful 
of   of the patient records to be test records  and the
remaining    to be training records  we train the svm
on all of the data from the training records  so that the
test records can be classified as one would expect in a
real time hospital scenario 

   

results

the personalized models were very successful  we
trained models for    of the records  the other    had
either too few normal beats or too few abnormal beats
to choose a useful testing and training set  to evaluate their performance  we use the f measure  which is
defined for real  as
f             

precision  recall
    precision   recall

   

in particular  we choose the f  measure  which is ap   
neural network approach
propriate if we place roughly   times as much value in
recall as in precision  in our case  false positives are much instead of using intelligent learning based on our own
more acceptable than false negatives  we want to catch knowledge of physiology  the focus of this section is to
 

fibels from        

begin to develop a deep learning algorithm  in particular  we have implemented a neural network  or multilayer
perceptron  mlp  

   

 the model is evaluated four times  once using the
raw data as features  once using discrete ffts  once
using discrete haar wavelet transforms  and once using discrete db  wavelet transforms 

model

a multilayer perceptron uses logistic regression along
with additional intermediate layers  called hidden layers 
here  we will be using a single hidden layer  the details
of the mlp will be described after the data is explained 
for our current analysis  we take the first five minutes
of each half hour recording  originally  the data for each
example are a           matrix of values  the first
column is the time  in seconds   the second column is
the reading of the upper lead  in mv   the third column
is the reading of the lower lead  in mv   because the
time sampling was verified to be the same for each example  the first column has been stripped from the data 
to shorten the sample from thirty minutes to five minutes  the number of rows has been truncated to        
because there are two channels  there are thus       
parameters per example 
an mlp takes the input and learns the most relevant features  in addition to using the raw amplitudes
as input  the model was also run using two forms of preprocessing  discrete fast fourier transforms  ffts  and
discrete wavelet transforms  using pywavelets       we
used both haar wavelets and daubechies  in particular 
db   wavelets 
the target prediction is whether or not the patient has
arrhythmia as indicated by the recording data  i e    if
arrhythmia is present and   otherwise  for determining
this  the oracle is obtained by checking the records in the
mitbih arrhythmia database      for each record  we
check whether there are any beats before the five minute
mark that are classified as something other than normal  if there are no such irregular beats  the example is
labelled    for healthy  otherwise  the example is labelled
  
because the data set is small  instead of using simple cross validation  we use    fold cross validation  we
partition the training examples into    subsets si of
size    for each i                  the model is trained on
s       si   si        s   to obtain a hypothesis
hi   the hypothesis is then tested on si to get an error
si  hi    the estimated generalization error of the model
is then calculated as the average of the si  hi    this generalization error is the metric by which we will evaluate
our success 
ideally  we would use leave one out cross validation
 loocv   however  training the mlp just once takes
minutes  due to our project timeline  we didnt invest
the time for a four fold increase in training time 
here is a summary of what has been covered so far 

 the model is a multilayer perceptron  mlp  with a
single hidden layer  more details follow 
 the model is trained and then evaluated by using
   fold cross validation 
a tutorial was used to implement the mlp      this
uses the theano package for python     
with a single hidden layer  the mlp works by making
a prediction based off of an output vector


f  x    g b      w     s b      w     x   
   
the function g is for logistic regression  to accommodate multiclass classification  g is the softmax function
e w x b i
g x  w  b i   p  w x b   
j
je

   

the function s is a nonlinear activation function for the
hidden layer  here  we choose
s x    tanh x 

   

finally  a prediction is made by
h x    arg max f  x i  
i

   

the parameters w      a matrix  and b     a vector  are
weights  the additional weights for the hidden layer 
w     and b      are called hyperparameters  these four
sets of parameters are learned by training and using
backpropagation to calculate the error  cost function  
theano is able to calculate gradients so that backpropagation doesnt have to be implemented 
next we comment on the dimensions of these parameters  two of the dimensions are given by the dimension of
the input vector  d           and the number of possible classifications for the output  l       the other
dimension  dh   is the dimension associated with the hidden layer  i e   b    is a vector of dimension dh   w     is
a dh  d matrix  b    is a vector of dimension l  and
w     is an l  dh matrix  we chose dh       
there are additional comments to be made about the
details of the model  the mlp uses l  regularization 
i e   it tries to keep the l  norm of the weights low 
training is done using mini batch gradient descent with
a batch size of   and learning rate  step size  of         
five to ten iterations are made through the entire batch
of    examples    being held out for cross validation  

 training data is a            matrix of real valued
examples  along with a    dimensional vector of la 

filogistic regression
neural network

raw data
     
    

ffts
   
     

haar wavelets
     
    

db  wavelets
     
    

table    mlp errors after using raw data and three forms of preprocessing 

   

results

between heartbeats for the sample  truncating each sample to be the same number of heartbeats  and adjusting
using the raw data as features  the estimated generaliza  the samples to have the same initial time offset 
tion error is
  
  x
s       
    bibliography
 
   i   i
after training the mlp  this is not very good  but it is an
improvement over the baseline estimated generalization
error of            the baseline consists of running logistic regression without the hidden layer  and also without l  regularization   the results after preprocessing
are summarized in table   

    moody gb  mark rg  the impact of the
mit bih arrhythmia database  ieee eng in med
and biol              may june         pmid 
          http   www physionet org 
physiobank database mitdb  
http   www physionet org physiobank 
database html mitdbdir records htm 

   

    goldberger al  amaral lan  glass l  hausdorff
jm  ivanov pch  mark rg  mietus je  moody
gb  peng c k  stanley he  physiobank 
physiotoolkit  and physionet  components of a
new research resource for complex physiologic
signals  circulation         e    e     circulation
electronic pages  http   circ ahajournals 
org cgi content full        e            june
    

analysis

the baseline error      is high  and actually higher than
what would be obtained by using a majority prediction
rule on our data set  for the mitbih dataset  most
of the examples have arrhythmia   this is expected because the features  which are the        raw amplitude
readings in this case  are basically useless for a linear classifier  the point here is that even with useless features 
using an mlp by adding a hidden layer and nonlinear
activation function results in a lower error 
preprocessing the data improved performance when
ffts were used  however  the two chosen wavelet decompositions did not help  the performance depended
not on the type of input  but the number of parameters
in the input   for ffts  the number of input parameters was half  real input results in an fft with half
as many complex parameters  the imaginary parts were
dropped   it is possible that the chosen wavelet decompositions were inappropriate for our data  ffts may
have been more appropriate because of the nature of the
data  somewhat periodic graphs  where significant deviations from periodicity indicate arrhythmia  
our implementation of the mlp did help  but overall
it was still not very successful  unfortunately  we did
not figure out how to adapt other deep learning methods  such as convolutional neural networks  for image
classification   to our data 

   

    ashley ea  niebauer j  cardiology explained 
london  remedica        chapter    conquering
the ecg 
http   www ncbi nlm nih gov books nbk     
    http   deeplearning net tutorial mlp html
    http   deeplearning net software theano 
    chang cc  lin cj  libsvm  a library for support
vector machines  march         
http   www csie ntu edu tw  cjlin libsvm 
    hsu cw  chang cc  lin cj  a practical guide to
support vector classification
    http   pybytes com pywavelets 

future steps

given sufficient time  improvements could likely be made
by continuing to adjust the learning parameters  a possible idea for helpful preprocessing would be to normalize the time axis of the data  this would be done by
setting the unit for the time axis to be the average time
 

fi