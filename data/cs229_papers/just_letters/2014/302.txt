did we say the same word  towards identifying spoken word similarity
ossian oreilly  ooreilly stanford edu  and andreas mavrommatis  andreasm stanford edu 
december        

 

introduction

robust and accurate machine learning algorithms for speech recognition and natural language processing
must be able to handle numerous challenges  for example  variabilities present in human voice recordings
such as gender of speaker  tempo  pitch  pronunciation  accent  noise  etc  add complexity to the problem
of transcribing voice into text 
in the particular application of identifying whether two audio samples contain the same language
content  using the text transcribed from speech recognition as the similarity metric is susceptible to
inaccuracies during the transcription process  therefore  it appears that using the audio signals themselves
 without the intermediate step of transcribing audio to text  could offer some benefit  applications where
it might be beneficial  or even necessary  to never transcribe to text include searching for instances of
spoken words in an audio file  or a video lecture   or learning the pronunciation of a new language 
as a first step to address this problem  we perform binary classification for many pairs of words of the
english language  we explore various techniques to determine what features are important in classifying
spoken words 

 

data acquisition and processing

our data are audio waveforms from www forvo com  a user generated pronunciation database  we downloaded waveforms for    different words  each containing at least    individual pronunciations  examples  
we converted each waveform into a spectrogram of      frequency bands by     time samples  variable 
depending on original duration   we also computed mel frequency cepstrum coefficients  mfcc  

 

feature selection

we explored the following features      raw spectrograms      spectrograms aligned in time  to avoid
bias due to misalignment of each spoken word       centroids derived from k means clustering on both
raw and aligned spectrograms  to reduce dimensionality       principal components derived from pca
on aligned spectrograms  to reduce dimensionality        d haar wavelet transform of raw and aligned
spectrograms  efficient compression   and     mel frequency cepstrum coefficients  mfcc  
each features are associated with specific thresholds which we vary in order to select the best set
of features  the best features were derived as the ones that gave the lowest train and test errors after
classifying two words using a support vector machine  liblinear package   our training set consisted
of     of the dataset  the rest being the test set  for this stage  we focus exclusively on a single pair of
words  specifically  the words tumblr and anything  because they had the largest number of examples  

 

fi   

thresholded spectrograms

we increase the signal to noise ratio of each spectrogram by keeping only amplitude coefficients  a  that
exceed a threshold value  above some background noise level   given by
athr    a     a 

   

where  a  and  a  are the sample mean and standard deviation of the amplitudes  respectively  and
 is a scalar parameter that we vary  setting numerous amplitude coefficients to zero makes the feature
representation sparse  which improves the computational performance when using a classifier such as svm 
or when applying feature extraction using k means clustering 

figure    example of converting a raw spectrogram to aligned and rescaled spectrogram from a particular sample 

   

aligned and rescaled spectrograms

to avoid bias due to misalignment of each spoken word  caused by differences in the timing and tempo of
speakers   we discard time periods with low signal to noise ratio  we assume that each spectrogram can be
segmented into three segments  a silent  noise only  beginning  followed by a voice segment  and another
silent segment at the end  figure     to identify the voice segment  we compute and set a threshold on
  u i        where each u i  is a vector  along the frequency dimension  that contains the amplitude coefficients
at time sample i  we identify the voice segment as the part of the signal that exceeds a threshold on
  u i      for at least   consecutive samples  thresholds were determined empirically   in addition  to reduce
computational time  we resize each spectrogram to a fixed size of     by    elements  figure     as with
the raw spectrograms  we keep only amplitude coefficients that exceed a specified threshold 

   

k means clustering

we treat each point in the spectrograms and mfccs as a coordinate in  t  f  a  space  that is   time 
frequency  amplitude   and perform k means clustering in that space  we select features by varying the
number of clusters  for the raw and aligned spectrograms  we also vary the threshold on the amplitudes
 figure     more specifically 
 for the raw spectrograms  to avoid biases due to timing differences  we discard the time coordinate
of the clusters  the feature vector in this case contains the centroid coordinates  f  a  
 for the aligned spectrograms  we retain all three dimensions in the cluster space   t  f  a   since by
aligning the spectrograms in time we have corrected for timing differences 
 for the mfccs  we discard the time coordinate of the clusters  similar to the raw spectrograms 
the feature vector in this case contains the centroid coordinates  f  a  
 

fi   

principal component analysis

as an additional way of reducing the dimensionality of our features  we performed principal component
analysis  pca  on the aligned spectrograms  for feature selection  we varied both the amplitude threshold
and the number of principal components that are kept in the pca representation of the spectrograms 

   

wavelet transform

wavelets have been used in many signal processing and image processing applications to  for instance 
compress and denoise signals  many naturally occurring signals are suitable for compression using wavelets
as only a few set of coefficients contain most of the energy in the signal  currently  it is the preferred
image compression technique in the jpeg library  here we apply the  d haar wavelet transform to each
spectrogram and retain the maximum modulus coefficients by thresholding 

figure    examples of feature selection process  showing the variation of training error  blue  and test error  red  as
we vary the number of k means clusters and amplitude threshold if the raw  left  and aligned  right  spectrograms 
note that aligning the spectrograms lowers the training error and in some cases the test error as well 

 

results and discussion

after selecting the features that gave the best test and training errors in the word pair tumblr vs
anything  as described in the previous section   we then performed binary classification using a support
vector machine  liblinear package  on all pair combinations of the    words  our training set consisted
of     of the dataset  the rest being the test set  we computed training and test errors for each pair of
words  table   lists the results of all word pair classifications 
aligning and resizing the time axes of the spectrograms resulted in a significantly lower test error
 reduction from     to     on average  see table    figure     because it corrects for differences in
timing between different speakers  as well as differences in tempo 
to our surprise  reducing the feature dimensionality by performing k means clustering and pca on
the spectrograms and or mfcc did not increase performance by any significant amount  table     one
 

fidisadvantage of the k means clustering is that it produces clustered point clouds that are different for
individual samples of the same word  this results in further inconsistencies in the features of examples
with the same word label 
based on the alphabetical ordering of words in the confusion matrices  figure    panels as and
was   there are two distinct bands  one with low test error and one with large test error on average 
the band with the lowest error corresponds to words beginning with the sound s  which are apparently
easier to distinguish from other words  we note that we did not simply have more data for words
beginning with s  on the other hand  words that beginning with the sound m are the most difficult to
distinguish from other words  figure     we suspect that nasal sounds such as m and n do not produce
distunguishable auditory signals 
on average  words with more examples are more easily distinguished  regardless of the type of features
 figure    bottom row   therefore  incorporating more data should increase performance  one way to expand the dataset would be to create synthetic data by manipulating our current data  i e   changing pitch 
tempo  timbre etc    another approach would be to extract smaller phonetic components  phonemes 
from each waveform that represent individual sounds  instead of entire words   each word would then be
represented by an ordered sequence of phonemes  which would serve as a basis for words 

 

conclusion

we applied machine learning algorithms to address the problem of classifying audio samples based on the
words they contain  we find that aligned and rescaled spectrograms derived from the audio samples and
wavelet transforms of the aligned spectrograms are useful features for this problem  using a linear svm
classifier on a training set of    words  we find an average test error of       when we use the wavelet
transform  on average  words with more examples yield lower test errors  implying that incorporating
more data  i e   more audio samples per word  should increase performance  further development is
needed for implementing practical applications  such as voice search within an audio or video record  or
language learning software 
features
spectrograms
spectrograms
spectrograms
spectrograms
mfcc
k means on mfcc
pca on aligned spectrograms
wavelet transform on raw spectrograms
wavelet transform on aligned spectrograms
raw
aligned
k means on raw
k means on aligned

training error    
 
 
 
 
 
 
 
 
 

test error    
             
             
             
             
             
             
             
             
             

learning rate
    
    
    
    
    
    
    
    
    

table    error analysis for each of the feature selection methods  the learning rate is defined as the
decrease in test error per additional example 

 

firs

as

wrs

was

words starting
with s

rs

wrs

as

wrs

   

   

   

   

   

   

   

   

   

   

   

  
  
test error    

  

   

  

  
  
test error    

  

average test error    

average test error    

  

  

  

  
 

  

  

  

  

  

  

  
  
  
number of samples

  

 
 

  

  

  
  
test error    

  

   

  
  
  
  

  

  

  

 
 

   

  

  

  
  
  
  
number of samples

  

  

  

  

  
  
test error    

  

  

  

  

average test error    

  

  

  

 
 

   

  

  

counts

counts
  

  

  

 

 
 

average test error    

  

   

  

  

 
 

   

   

   

  

   

   

   

   

   

   

   

   

   

   

counts

counts

   

  

  

  
  
  
number of samples

  

  

 
 

  

  

  
  
  
number of samples

  

  

figure    results of word pair classification for four different features  raw spectrograms  rs   aligned spectrograms  as   wavelet transform on raw spectrograms  wrs   and wavelet transform on aligned spectrograms
 was   colored images are confusion matrices  showing the test error for each word pair  histograms are distributions of test errors for each feature  scatter plots show the variation of the average test error for each word as a
function of the number of samples for that word  red lines are linear fits 
 

fi