multiclass classifier building with amazon data to classify
customer reviews into product categories
yunzhen hu
yzhu   stanford edu

te hu
teh stanford edu

abstract

haier liu
haierliu stanford edu

several brands of skateboard shoes to the
twitter user  there could be a good chance that
this deal is made 

as one of the worlds largest online
retailers  amazon com has a sales
volume of tens of billions dollars and
as a result huge amounts of customer
reviews accumulate  these reviews are
great learning samples which could
help promote commercial prediction
and advertisement  

to build such a classifier  our project both trains
and tests on customer reviews form amazon 
we select amazon as our data source since it
has a detailed and clear product hierarchy and
also has plenty of customer reviews as samples 

ii  dataset

our project aims to build a multiclass
classifier based on the customer
reviews which are labeled by product
categories in amazon  different
feature selections and models are
tested when building this classifier  as
we will show below  the random forest
classifier performs the best 

our data is provided by our project advisors
prof  ashutosh saxena and mr  aditya jami
from snap 

i  introduction
figure   amazon product hierarchy

to make predictions about potential customers
needs and thus feed him her relevant
advertisement is one of the prevailing
applications of machine learning  for example 
some person posts something online  say  a
tweet expressing his her interest in a pair of
skateboard shoes  if a certain classifier could
recognize this potential need and then display

figure   shows the structure and classification
of goods in amazon  there are    root
categories and further subcategories belonging
to each of them  we only train and test on the
top level categories in our project 
we also have a large dataset related to        
items in amazon  information contained
includes description of each item  customer
reviews  relations to above categories  etc 

 

the net sales of amazon is         billion in      and
        billion in        amazon com inc      annual
report form     k    united states securities and exchange
commission  january          

 

fimulticlass classifier building with amazon data to classify customer reviews into product categories

iii  features   preprocessing
we extract           customer reviews from
the raw dataset  after labeling these reviews by
their corresponding root parent categories  we
construct a sample with       reviews for each
of the    root categories   the other   categories
are not included since there are not enough
reviews for them  
tokens are generated from these        reviews 
before which they are all converted into
lowercase and stemmed by porter stemming
algorithm  with the bag of words model  a
dictionary of all the        tokens is constructed
and each review is transformed into a booleanvalued         vector   an element is labeled
  if its corresponding token appears in the
review and   otherwise  

figure   distribution of mi scores of tokens

two feature filters as shown below  mutual
information  mi  and term frequency  inverse
document frequency  tf idf   are built to
calculate feature scores  different thresholds are
tested in running the classifiers in order to
decide the best feature sizes for different models 
 

 

 

   

 

 



 

  

 

   

   
 
     
 
 
   
figure   distribution of tfidf scores of tokens

then we split the        reviews into training
and testing samples  for each root category  the
training size is     and the testing size is     

iv  models
we have built   multiclass classifiers  random
forest classifier  rf   decision trees classifier
 dt   linear discriminant analysis classifier
 lda   linear supporting vector classifier
 svc  and nearest centroid classifier  nc  

 

fimulticlass classifier building with amazon data to classify customer reviews into product categories
table   model performances with features filtered by mi

decision trees classifier builds a decision tree
according to the information gain for each
attributes  it chooses the attribute with the
largest information gain at each node  and stops
adding node when every attribute has already
been included along this path through the tree 
or the training examples associated with this
leaf node all have the same target attribute value 

mi
    
     
     
     
     
     
     
     
     
     

random forest classifier repeatedly selects  by
bagging  a random sample with replacement of
the training set and fit      trees to the samples 
with each tree fitting with an individual
decision trees classifier  the random forest
classifier takes the majority vote in the case of
decision trees 

  

tfidf
   
    
    
    
    
   
    
    
    
    

                

          

dt
     
     
     
     
     
     
     
     
     
     

lda
     
     
     
     
     
     
     
     
     
     

svc
     
     
     
     
     
     
     
     
     
     

nc
     
     
     
     
     
     
     
     
     
     

table   model performances with features filtered by tfidf

linear discriminant analysis classifier sets a
common covariance matrix for conditional
distributions given different labels 
    

rf
     
     
     
     
     
     
     
     
     
     

rf
     
     
     
     
     
     
     
     
     
     

dt
     
     
     
     
     
     
     
     
     
     

lda
     
     
     
     
     
     
     
     
     
  

svc
     
     
     
     
     
     
     
     
     
     

nc
     
     
     
     
     
     
     
     
     
     

    

linear support vector classifier applies the
svm algorithm 
nearest centroid simply decide the labels by
computing and find the nearest centroid 
  
 

      
   


  
  

 
 
   

    

v  results
in table   and   we show the results of different
models with features filtered by mi and tfidf 
respectively  figure   and   are also shown for
readers easier understanding of the trend
between the test accuracy and the score
thresholds 

figure   model performances with features filtered by
mi

 

fimulticlass classifier building with amazon data to classify customer reviews into product categories

vii  conclusions
for this project  random forest classifier
performs the best  with      estimators   the
performances of random forest classifier and
linear supporting vector classifier will
improve as the number of features increases 
since random forest classifier has the
capability of preventing over fitting  a threshold
of      on tfidf works fine  we prefer tfidf
to mi because tfidf has a more evenly
distribution 

viii  future

figure   model performances with features filtered by
tfidf

to improve the accuracy of the multiclass
classifier  we are going to try bi gram tokens 
moreover  a hierarchical classifier can be built
on the dependences between labels at different
levels  also  cross domain learning can be
implemented with data from twitter and e bay 

vi  discussion
generally  random forest classifier performs
the best  for all the feature sets  random forest
classifier gives the largest accuracy  the
optimal value is       with a threshold of     
 filtered by tfidf  

acknowledgement

when more features are included in the model 
both random forest classifier and linear
supporting vector classifier perform better 
which shows that these two models have a good
capability of preventing over fitting 

we would like to thank professor ashutosh
saxena and mr  aditya jami  who have
provided us with so many suggestions and data 
we have learnt a lot from them in our regular
meeting every tuesday 

no evidence shows that the difference of feature
filters would significantly influence the
performance  with the number of features
constant  say  around        the filters generate
similar results for every model  with the
threshold on feature scores lowering  linear
discriminant analysis classifiers accuracy
firstly increases and then decreases  which
demonstrates that the filters are both valid  but
since tfidf values distribute more evenly than
mi values of the features  the application of
tfidf may be more convenient 

references
    teh  y  w   jordan  m  i   beal  m  j     blei  d  m 
        hierarchical dirichlet processes  j  amer 
stat  assoc            
    jiang  y     saxena  a         august  
discovering different types of topics  factored topic
models 
proceedings of the twenty third
international joint conference on artificial
intelligence  pp              aaai press 
    daum iii  h          frustratingly easy domain
adaptation  arxiv
preprint
arxiv  

 

fi