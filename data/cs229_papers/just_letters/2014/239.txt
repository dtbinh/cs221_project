

cs    fall  

classification of arrhythmia using ecg data 
giulia guidi   manas karandikar

dataset
overview
the dataset we are using is publicly available on the uci machine learning algorithm  it can be found
at https   archive ics uci edu ml datasets arrhythmia  it consists of     different training examples
and spans    different classes  out of the     training examples      is for normal people  we also
have examples of    different types of
arrhythmia  among those types of
arrhythmia  the most represented ones are
classes    coronary artery disease  and   
 right bundle branch block   

the     features include the age  the sex  the
height and the weight of the person as well as
some workable information extracted from
the electrocardiogram  this number of
features is relatively high compare to the
number of training examples available  
in order to understand the features we had to
take a close look at how electrocardiograms
figure    class distribution in dataset
are performed and what is measured  

as different parts of the heart  ventricles and the atria  contract  the electrical activity can be
recorded  thanks to multiple electrodes   d reconstruction of the waves is performed  all the
pre processing to convert raw data to workable vector angles amplitudes has already been done by
the creators of the dataset 
each channel in the dataset is obtained by taking a voltage difference between two electrodes  


approach to the problem
prepare the dataset
the first step had to perform was to prepare the dataset  we noticed that some missing values being
replaced by nan were crashing the algorithms  most of those values were found in feature     we
also got rid of the rows containing nan values  
with those changes  we ended up having     features and     training examples  


feature selection 
in biomedical applications  feature selection is very important as there are usually a lot of different
parameters to take into account  with our dataset  we have     features and     training examples 
with this ration of features to training examples  our algorithms cant be accurate  as it is
 

fi

computationally expensive to enumerate and compare          e    models  we decided to perform
feature selection with the help of trees classifiers  after running a tree classification algorithm we
used only the features that were showing up in the tree  


clustering
we created our training and testing sets by randomly sorting the indexes and spit  for training and
 for testing  each algorithm was run several times and the accuracy was the average each trial  



algorithms
we explored four main algorithms  tree classifiers  svm  naive bayes and random forest  
the results of these algorithms are presented in the following paragraphs 

decision trees
since the big advantage of decision trees is speed of computation and immunity to missing values  we
ran cart algorithm in r and in matlab to understand our feature and data set much better  to
optimize the depth of the tree  we minimized the cross validation error as a function of the depth of
the tree   number of leaves and then pruned the tree to that depth  a typical cross validation error
plot looks something like fig   





figure     i  cross validation error vs depth of tree  ii  decision tree trained on dataset

the prediction accuracy of trees on the testing dataset was around      this is expected to improve
by implementing boosting methods like adaboost   gradient boosting 
the classification tree constructed was a direct input for feature selection while running the next
algorithm  support vector machines  svm  
 

fi

support vector machines  svm 
using liblinear on matlab  we tried to run a svm algorithm on the entire dataset      features and   
classes   the accuracy of the model was only of      this poor result can be explained by several
factors  first of all the number of training examples that we used was comparable to the number of
features  in addition  there were only   classes well represented in the dataset  class   for normal
people  class   for artery coronary disease and class    for right bundle branch block   every other
class represented less than    of the dataset  see fig    
in order to improve the svm performance  we decided to combine feature selection and class
selection  
as far as feature selection is concerned  we used the    most important features that we had with
decision trees and we ordered them from most important  the first highest one on the tree  to least
important 
we decided to extract from the dataset only
the data for class        training examples 
and class       training examples  
starting with only the most important
feature  once the svm had finished to
create a model  we added the following
most important feature and ran again the
algorithm  on each iteration  we computed
the accuracy of the model and found that
we could maximize the accuracy by
selecting only    features  see fig     
figure    variation of accuracy with number of features

in addition to that  we implemented several types of kernels  we implemented a gaussian kernel and
polynomial kernels of degrees      and    the results can be visualized in fig   we plotted for the  
most important features  according to the decision trees  the separator as well as the support vectors 
the kernel which gave us the maximum accuracy was the polynomial kernel of degree   
n of features

n of classes

accuracy

kernel

   

  

   

 

   

 

   

 

   

 

   

polynomial deg  

  

 

   

 

  

 

   

rbf

  

 

   

polynomial deg  

 

fi


figure    visualization of  d plots for svm with different types of kernels

naive bayes and random forest
the naive bayes algorithm relies on a strong hypothesis  the value of any feature is independent of
the existence of any other feature  
in most of the real life examples  the naive bayes hypothesis is never satisfied but the algorithm
predicts with a good enough accuracy the classes  
the first step for us was to transform continuous data into multinomial  we used a kernel smoothing
density estimate to model the probability density function of continuous variables  
the accuracy of the algorithm was on average      which is fairly good for naive bayes on such a
large feature set  

 

fi

we tried to use a random forest algorithm running a matlab version of the brieman and cutlers
random forest package on fortran r  after running the algorithm    times  we averaged the accuracy
of each iteration of each model  we found that the average accuracy of random forest was      


conclusion
the accuracy   speed for various algorithms that we implemented is tabulated below 
algorithm

accuracy

training speed

testing speed

decision trees

   

    s

     s

svm for   classes and     features

   

   s

     s

svm for   classes and    features

   

   s

     s

naive bayes

   

   s

    s

random forest

   

  s

   s

note  training and testing speed are estimated with matlab time summary  it is the time to execute the training or
testing function 

as can be seen  classification trees provide a good accuracy with extremely fast computation time 
the predictive accuracy is expected to be improved by implementing more complex boosting
algorithms like adaboost and gradient boosting  svm with feature selection gives the highest
accuracy amongst all the algorithms implemented 
in spite of the fact that the dataset is immensely skewed towards a few classes and contains missing
values  the implemented algorithms exhibit good level of accuracy in prediction  performance is
expected to significantly improve with a larger and more distributed dataset  

contributions 
the idea for the project was conceived by dave deriso and was substantiated by the authors  the
implementation and analysis of the algorithms has been done by the authors 

references 
    r  e  fan  k  w  chang  c  j  hsieh  x  r  wang  and c  j  lin  liblinear  a library for large linear
classification journal of machine learning research                    
    h  altay guvenir  burak acar  gulsen demiroz  ayhan cekin  a supervised machine learning
algorithm for arrhythmia analysis   proceedings of the computers in cardiology conference  lund 
sweden      
    random forest matlab package https   code google com p randomforest matlab 

 

fi