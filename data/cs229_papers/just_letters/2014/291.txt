a general purpose sentence level nonsense detector
cs    final project report
december         

ian tenney
iftenney stanford edu

 

introduction and background

i have constructed a sentence level nonsense detector  with the goal of discriminating well formed english sentences from the large volume of fragments 
headlines  incoherent drivel  and meaningless snippets
present in internet text  for many nlp tasks  the availability of large volumes of internet text is enormously
helpful in combating the sparsity problem inherent in
modeling language  however  the derived models can
be easily polluted by ungrammatical text and spam 
and automated means are necessary to filter this and
provide high quality training data 
there is scarce precedent in the literature for a direct nonsense detection system  but similar problems
exist in the context of spam filtering and computational sentence completion  for spam filtering  recent
research has focused on the problem of bayesian poisoning  hayes         whereby naive bayes based
filters are defeated by the inclusion of random words
or text snippets that make the spam vocabulary appear
similar to normal text  solutions to this problem propose combining multiple sources of information  such
as higher n grams  in order to introduce greater context  upasana and chakravarty        
sentence completion is an example of a standard
nlp task where the system must consider a variety
of possible solutions and discriminate between sensical and nonsensical answers  while generally operating on a more restricted domain  e g  sat questions   the fundamental goal is similar enough that
similar techniques can be applied  the microsoft research sentence completion challenge  zweig and
burges        highlights several approaches to this
task  including neural networks and dependency parsing but also showing strong performance with lighterweight n gram and distributional models  zweig et al  
      
i take a lightweight approach  using a mix of heuristics  lightweight token based features  part of speech
features  and language model scores as features to
avoid computationally intensive parsing or neural networks  i structure my system as a binary classification

task on a heterogeneous feature space  consisting of
to produce a final answer of sentence or nonsense
for a given line of text 
i implement this project in a mix of java
and python  using java to interface with stanford
corenlp for feature extraction  and python  with the
excellent pandas and scikit learn libraries  for data
management  classifier implementation  and analysis 

 

data

   

source and labeling schema

the base dataset is a   tb corpus of sentencetokenized internet text derived from common
crawl  http   commoncrawl org   and provided by the
stanford nlp group  this is a relatively indiscriminate crawl  and contains text from a variety of web
pages  ranging from internet forums to pornography
 its the internet  to shopping sites and news articles 
in addition to coherent sentences  this contains a
large number of link titles  headlines  and other text
fragments  a typical few lines would be 
 p     panel      makede   ntshonge 
 if so  an authenticator may have
been maliciously associated with your
account by an unauthorized party 
 no new messages
c aspplayground net
 forum software 
advanced edition      
 living area

i annotate each line with one of the following labels 
  sentence  complete english sentences  not
necessarily with perfect grammar
  fragment  sentence fragments  headlines  or
captions  constituting coherent but non sentential
text
  nonsense  small fragments  noun phrases 
gibberish  spam  and anything else not counted
above
  none  no label  or foreign language text
this scheme is designed to roughly align with
downstream nlp tasks  nonsense is unwanted for any

fitask  while fragments are inappropriate for parsing
but  as they are subsequences of real sentence  are still
useful to build language models  i retain these labels
for visualization purposes and future use  but for our
supervised learning models we treat  sentence 
as a positive example and all other labels as negative 
   

datasets

i compile two labeled datasets from this corpus  one
by manual annotation and one by crowdsourcing  the
crowdsourced dataset consists of        lines of text 
each labeled by two distinct workers on amazon mechanical turk  of these       lines are labeled unambiguously as a single category by both annotators   
and thus can be used for training and evaluation  this
means that we drop the most ambiguous examples 
since turkers are more likely to disagree on lines that
are difficult to classify by any algorithm  as we see
in section    this tends to give results that are inflated
relative to real world performance 
to remedy this  i also compiled a manual dataset 
consisting of       lines of text  each labeled by a single highly trained user    all of these labels are assumed to be unambiguous  and the entire set is usable 
the class distribution for each dataset is 
dataset  size 
 sentence fragment nonsense none 

manual       
      
      
      
     

mturk       
      
      
      
     

where the discrepancy can be attributed difficulty
by mechanical turk workers in correctly identifying
fragments and nonsense  leaving relatively more sentenced behind when ambiguous data is discarded 

 

features and preprocessing

i implement five groups of features  for a maximum of
a     dimensional feature space 
   baseline sentence heuristic  first letter is capitalized  and line ends with one of        feature  
   number of characters  words  punctuation  digits  and named entities  from stanford corenlp
ner tagger   and normalized versions by text
length     features  
   part of speech distributional tags     tag 
words for
each penn treebank tag     features  
   indicators for the part of speech tag of the first
and last token in the text    x       features  
 
unfortunately  budget and time constraints prevented us from
using a third annotator  which would have increased the fraction
of usable labels 
 
myself  with some help from my roommate 

   language model raw score  slm   log p text  
slm
and normalized score  slm     words
     features  
for part of speech tags  i use the default english
tagger included with stanford corenlp  manning et
al          this is a sequence based maxent tagger
trained on a mixed corpus of newswire  technical  and
colloquial text  for language model scores  i build a
bigram model using the kenlm package  trained on
the entire english gigaword   corpus  parker et al  
      of newswire text 
before evaluating the model  all features are independently scaled to zero mean and unit variance
 across the training set  in order to improve performance of regularized and non linear models and allow
extraction of meaningful feature weights from logistic
regression 

 

t sne visualization

i used t distributed stochastic neighbor embedding
 t sne  to visualize our dataset and understanding the difficulty of the classification task 
tsne attempts to non linearly map high dimensional
data into a low dimensional  usually  d  space
by preserving local similarity  formally  it generp  p
ates a gaussian distribution  pij   j i n i j where
pj i  exp  kxi  xj k    i     over pairwise distances in the original space  then finds an embedding that  locally  minimizes the kl divergence between this and a heavy tailed t distributed similarity
qij in the low dimensional space  van der maaten 
       i use the barnes hut implementation of tsne  using the python implementation available at
http   lvdmaaten github io tsne   and retain our multiclass labels for visualization purposes 
i generate visualizations using the level   feature set  everything but language models   with scaling normalization applied 
on the mturk dataset  we find that restricting to unambiguous labels gives us a well separated space  regions of mostly sentences  blue  have a well defined
boundary from nonsense regions  red   and to a lesser
extend  fragments  green   on the manual dataset
 figure    this separation is much weaker  and sentences are interspersed with fragments and nonsense
in many clusters  reflecting the inclusion of more ambiguous data points 
because the t sne algorithm uses l  distances in
the feature space  it is sensitive to scaling  strong contributors to distance in the original space may be relatively uninformative for our classification task  to
better visualize the difficulty of the classification prob 

fiin which you can mouse over data points to see
the original text and label  in the scaled space  the
resulting clusters group text that share common
attributes or grammatical patterns  such as beginning
with a wh word or consisting of two nouns 

 

figure    t sne embedding of manual dataset  mturk
dataset visualization available online 

i implement five models for the binary classification
task  a baseline heuristic  logistic regression  linear
svm  rbf  gaussian  svm  and random forest  the
baseline system uses feature    section    directly  if
a line of text matches the heuristic  it is classified as
  sentence    otherwise negative  this achieves
a precision of     and recall of     on the manual dataset  reflecting that a large number of nonsense
fragments conform to this naive pattern 
all models are tuned using   fold cross validation
on the training set  using a grid search to choose the
best set of hyperparameters  for logistic regression
and linear svm  we tune the regularization strength c
and use either an l  or l  regularization penalty  for
the rbf svm  we use a gaussian kernel  and tune the
regularization strength c and the kernel width   for
the random forest  we use the scikit learn implementation with probabilistic prediction  pedregosa et al  
      and tune the number of decision trees and the
maximum tree depth 
i compute accuracy  precision  and recall for all our
r
models  and optimize for f  score  f      pp r
   the
f  score provides a balance between precision and recall that is less sensitive than either to the choice of
threshold  and so provides a good summary of overall
model performance 

 

figure    t sne embedding of manual dataset  scaled
features  mturk dataset visualization available online 
lem  we train a logistic classifier  l   c        on
the full dataset and use the resulting weight vector
    nonzero elements  out of      to scale the feature
 i 
 i 
space  xj  wj xj before running t sne  this emphasizes the dimensions most relevant to our task  and
shows a clean separating margin in both datasets  figure    
i
have
posted
interactive
visualizations
of
the
dataset
at
http   stanford edu iftenney nonsense  

supervised models

results and discussion

i present f  scores for all our models in tables         
and    the mturk dataset is split           train test 
while the manual dataset is split            train
and dev set results are averaged over   fold crossvalidation  while test set is from a single evaluation
on unseen data 
all results are presented with the best hyperparameters from the cross validation runs  which vary somewhat depending on the feature set  most notably  the
linear models  logistic and linear svm  perform best
with l  regularization and c       on features     
while l  regularization  with c      performs better
on the larger feature sets         and higher  that introduce more correlated or less informative features 
as mentioned in section    l  regularized logistic regression with c       yields     out of      nonzero
features  with most of the zeros mapping to group  

fiindicator features 
table    f  scores for each model  on manual dataset 
all features 
number of elements
baseline
logistic
linear svm
rbf svm
random forest

train
    
      
      
      
      
      

dev
   
n a
      
      
      
      

test
   
      
      
      
      
      

does not seem to help  performance is stagnant  or
slightly lower when these features are included  this
is likely due to the low order  bigram  of the model 
which makes it unable to capture sophisticated relationships across strings of words  and due to the
many high probability sequences found in nonsense
and fragments 

table    f  scores on mturk dataset  all features 
number of elements
baseline
logistic
linear svm
rbf svm
random forest

train
    
      
      
      
      
      

dev
    
n a
      
      
      
      

test
    
      
      
      
      
      

the more expressive non linear models also have
the best performance  although rbf svm only
slightly outperforms the logistic and linear svm classifiers at the expense of much longer training time 
random forest yields the strongest performance overall  despite nearly memorizing the training set  f 
scores of         on all tests  it still performs significantly better        than other models on unseen
data  notably  the model also trains quickly  within
a factor of   of logistic regression   and is relatively
insensitive to hyperparameters  performance with    
trees of depth    is within    of     trees of depth
    allowing for faster training and a more compact
model 
table    test set f  scores across all feature sets  on
manual dataset  feature groups as described in section   
feature grp 
baseline
logistic
linear svm
rbf svm
random forest

   
      
      
      
      
      

     
      
      
      
      
      

       
      
      
      
      
      

all
      
      
      
      
      

figure    precision recall curves for all models  on
the manually annotated dataset using features         
yellow dot shows performance of baseline heuristic 
auprc is the area under the precision recall curve  as
an aggregate metric of performance 
as expected from the labeling ambiguity  performance of both the baseline heuristic and our models is significantly higher on the mturk dataset than
on the manually annotated set  nonetheless  with
all marginal examples labeled  we are still able to
achieve nearly     f  using the random forest classifier  analysis of the precision recall curve in figure  
shows that the random forest clearly outperforms other
models in precision  reaching a precision of     at
    recall   performance which should be sufficient
for many practical applications 

table    test set f  scores  on mturk dataset 
feature grp 
baseline
logistic
linear svm
rbf svm
random forest

   
      
      
      
      
      

     
      
      
      
      
      

       
      
      
      
      
      

all
      
      
      
      
      

performance increases with additional features  as
shown in table    group   features outperform the
baseline  under the random forest model   and all
models improve slightly when adding group    partof speech distribution  and group    begin  and endtag  features  interestingly  adding the language model

   

feature importance

i can extract feature importances from the random forest by counting splits  for a given decision tree  a features importance is given by the number of training
examples that are decided using that feature  averaging this across the ensemble gives us the following as
our top    features 
  
  
  
  
  

f
f
f
f
f

sentence pattern   baseline heuristic
pos     fraction of   in pos tags
nchars   number of characters
nwords   number of words
rupper   ratio of   uppercase     words

fi   f rpunct   ratio of   punctuation     letters
   f pos in   fraction of certain prepositions
   f pos dt   fraction of determiners  the  those  an 
etc  
   f pos prp   fraction of personal pronouns
    f nupper   number of uppercase letters

the importance of the sentence heuristic is expected 
given the strong performance of the baseline  and the
importance of many of the group   features is also not
surprising  given that group   is a dense set with all
features active for a given line of text 
unfortunately  unlike with linear models  it is not
possible to tell directly whether each of these features
is indicative of positive or negative examples  indeed 
this is not necessarily well defined  
   

error analysis

while the learned model significantly outperforms
baseline  it is still weak in many cases where the
sentence heuristic is incorrect  this can go both ways 
many sentences from internet forums are not properly
capitalized  such as  co produced by danger
mouse     thats cool     while in other
cases the heuristic applies but is outweighed by
other features  such as  label is resistant
to these solvents  xylene  ipa 
dmso  ethanol  toluene   which is treated
as  other  presumably due to the large number
of capital letters and abnormally high noun fraction  in other cases  we see the limitations of the
distributional model  such as  microsofts
network certifications for another
article    which is erroneously called a sentence
as its lack of a verb is overlooked 
additionally  a large number of errors on the mturk
dataset are related to labeling  in many cases  the
model actually predicts correctly while the labels are
wrong  this highlights the difficulty of obtaining good
crowdsourced labels  there is a trade off between precision and time  and providing longer instructions and
other quality control measures can quickly ramp up
the cost of an annotation project 

 

conclusion and future work

i present a model that successfully discriminates complete sentences from fragments and nonsense  and is
capable of reaching     precision while maintaining
    recall  this is realized by a random forest classifier  which strongly overfits the training set with only a
modest model size  yet still generalizes well to unseen
data 
this model performs well enough for many practical purposes  where      noise is tolerable or can be

filtered by more intensive means  but could certainly
be improved  most notably  additional data would
permit the use of a richer feature set  such as an ngram model over part of speech sequences  alternatively  language modeling may be effective if trigrams
or higher are used to span over more syntactic constructs  however  this also risks biasing the model  as
it may learn to pick out text that resembles the training
corpus  regardless of grammaticality 

 

collaboration statement

this project was based on an idea proposed by gabor
angeli  angeli stanford edu  for cs   n  although
i am submitting this project to cs    only  gabor
provided me with access to the dataset  a budget for
crowdsourcing  and some advice on feature engineering  all work for the project  including data annotation  pipeline implementation  and analysis  was performed by me alone 

references
brian hayes        computing science  how many ways
can you spell vagra  american scientist           
     july 
christopher d  manning  mihai surdeanu  john bauer 
et al        the stanford corenlp natural language
processing toolkit  in proceedings of   nd annual meeting of the association for computational linguistics 
system demonstrations  pages      
robert parker  david graff  junbo kong  ke chen  and
kazuaki maeda        english gigaword fifth edition 
june 
fabian pedregosa  gael varoquaux  alexandre gramfort 
et al        scikit learn  machine learning in python 
journal of machine learning research              
october 
upasana and s  chakravarty        a survey on text classification techniques for e mail filtering  in      second international conference on machine learning and
computing  icmlc   pages       february 
laurens van der maaten        accelerating t sne using tree based algorithms  journal of machine learning
research              
geoffrey zweig and christopher j  c  burges       
the microsoft research sentence completion challenge 
technical report msr tr           december 
geoffrey zweig  john c  platt  christopher meek  et al 
      computational approaches to sentence completion  in proceedings of the   th annual meeting of the
association for computational linguistics  long papers
  volume    acl     pages         stroudsburg  pa 
usa  association for computational linguistics 

fi