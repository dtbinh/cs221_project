personalized web search
dhanraj mavilodan  dhanrajm stanford edu   kapil jaisinghani  kjaising stanford edu  
radhika bansal  radhika  stanford edu 

abstract 
with the increase in the diversity of contents available on the web  the need for
personalizing the search results has become more and more relevant  usually the ranking
of the pages is done with respect to query terms and page content alone  but if the ranking
is done considering query terms as well as user history  then the search engine performance
can be improved  in this paper  we try to learn the user interactions and aim to re rank
search query results based on the learned user interests 

   introduction
one criticism of search engines is that when queries are issued  most return the same
results to all users  different users may have completely different information needs and
goals  when using precisely the same query again  for example  a person interested in
programming may use the search query design pattern to find information related to
software and a person in fashion industry may use the same query for the design pattern in
clothes  with personalization  a search engine can find out which result to prioritize and
treat the two queries differently 

   related work
in the recent works like      knn and k mean based collaborative filtering is used for
enriching the user history by taking into account the history of similar users      shows that
user profile constructed based on modified collaborative filtering achieved the best
accuracy  in     it is shown that personalization is not effective for all queries  like the
queries with low click entropy  for which most people click on same url  does not require
personalization  for a user   query pair  give more weightage to the url clicked by the user
for that particular query  skip click model takes hints from not just url clicked  but also
from the skipped ones  suppose a user clicked url at position    then the three urls above
it are skipped urls  sat click introduced by fox et al      is measured by time between
click and next action  if time is less than a certain threshold  satisfaction is    otherwise
more satisfaction 

fi   experimental set up
a  dataset
data is publicly available here as part of the personalized web search challenge on
kaggle  the dataset includes user sessions extracted from yandex logs  with user
ids  queries  query terms  urls  their domains  url ranking  clicks and timestamps 
user data is made anonymous by using ids instead of actuals  we have used sample
from this dataset for faster experimentation and iterations of the learning
algorithms  for training  we randomly picked       unique users only  picked all
queries by those users and then split the kaggle provided train data into train
set first    days  and test set next   days   we are not using the test data provided
by yandex as it does not have any click information to compute ndcg  also we have
removed those queries which does not have any click information from both train
and test datasets  table     graphs in figure      show some basic characteristics of
our sampled dataset

characteristic

train

test

days

  

 

session count

     

    

unique users

     

    

query count

     

     

unique terms

     

     

unique domains       

     

unique urls

      

      

click count

      

     

total records

              

table   

fifigure   

figure  

figure  
in figure    we can see that the number of users who are doing multiple queries are
exponentially decreasing  so the scope of personalization is limited as the user
history is also limited 

b  evaluation metrics
we are evaluating the results from learning algorithms using ndcg  normalized
discounted cumulative gain  metric  it measures the performance of a
recommendation system based on the graded relevance of the recommended
entities  it varies from     to      with     representing the ideal ranking of the
entities 

k   the maximum number of entities that can be recommended 
idcg is the maximum possible  ideal  dcg for a given set of queries  documents 
and relevances 
k

fi   our approach
the search results in the dataset  provided by yandex  was not personalized  we worked
to re rank the search results based on the users search history  since we didn t have
information to make explicit relevance judgment  we used implicit relevance based on
clicks  here we assume that the clicks are a proxy for what user considers relevant  and
the user data provided by yandex is anonymized by providing only ids for sessions  users 
query  urls and their domains  so we cannot use any information on url content  query
content  user personal profile as suggested by many papers  we worked on creating
features to capture personal biases based on users search and click history and trained a
ranker to re rank the result 

    data labelling
for a given session and query  urls can be categorized as skipped  clicked or missed  all
the urls below the last clicked url are missed and all the not clicked urls above it are
skipped  here we are interested in clicked urls  as mentioned earlier  we dont have the
human judgment information to give relevance score to urls  we use the click information
and the amount of time spent on each click to label the clicked urls into three categories 




high relevance   url clicked and time spent on that is more than     units 
relevant
  url clicked and time spent is between    to     units 
irrelevant
  url clicked and time spent is less than    units or urls missed
 skipped

the amount of time spent is calculated from the timestamp information provided in the
dataset  the timestamp is given for a click and query w r t a session and by taking the
timestamp difference between the clicks  the time spent is calculated  if the click is a last
activity in a session  then the time spent is given     units  since it can be put in satisfied
category 

    normalization of query
since the yandex dataset has only term ids of a query  we couldnt apply normalization
technique in queries to identify same queries  instead we tried to find stop words using
term frequency in queries  top ten stop words are then excluded from the query terms 
also we sorted the terms to normalize queries like interstellar movie and movie
interstellar  this helped us to increase the correlation of similar queries from users past
history  there may be some cases where the meaning may be different  but to figure that
out we need the exact term  which isnt provided in the dataset 

fi    data features
this section describes the features we have derived from the given dataset  these
features are used in training and testing our model 

      navigational query
personalized search shouldnt be applied for navigational queries  ex  query facebook
indicates that people would always click the link facebook com  to find out navigational
queries  we use query entropy feature which measures the variability in clicked results
across individuals  click entropy is calculated as 
click entropy  q       p c  q    log  p c  q  
u

u

p c  q  is the probability that url u was clicked following query q
p c  q     number of times that url is clicked for that query     total urls got clicked for
that query 
u
u

a large click entropy means many pages were clicked  non navigational query  for the
query  while a small click entropy means only a few were clicked  navigational query  

      user preferences


users past preference on domains  some users have preference for certain domains
for a particular topic  for example some users prefer amazon while some prefer
ebay for shopping  from train data       domains  out of        clicked domains 
got clicked by the same user more than once  figure    

figure  

figure  

fifigure  


users past preference on urls   count for repetitive search handling  figure   
from train data       urls  out of        clicked urls  got clicked by same user
more than once 



users past preference on same query  user preference for a url and domain is
specific to query  for example  for tech queries user may prefer stackoverflow
result  but for movie  s he may prefer imdb  similarly for urls  user preference may
change with query type 
from train data       queries  out of         got queried by same user more than
once 

for each predicate  example  user domain pair   calculate the following three features
that correspond to three urls categorization 




high relevant prob    total high relevant urls for a predicate      total urls for a
predicate    
relevant prob    total relevant urls for a predicate      total urls for a predicate
   
irrelevant prob   total irrelevant urls for a predicate     total urls for a predicate
   

here we used laplace smoothing to handle sparseness of data 

      query based feature
url re ranking also depends on query ambiguity  for capturing query ambiguity  we have
included two features 
 query length   smaller the length more probability of ambiguity
 query average position in session   later in the session means less ambiguous as
user keeps on refining query

fifeature name

description

user domain
   userdomainhighrelevantprob
   userdomainrelevantprob
   userdomainirrelevantprob

for capturing user domain preference 
for a user domain pair find out the three
probabilities  high relevant  relevant  and
irrelevant 
calculation is given in sec       
example 
userdomainhighrelevantprob    total high
relevant urls for a user domain pair   total
urls for a user domain pair 

user domain query
  
userdomainqueryhighrelevantprob
  
userdomainqueryrelevantprob
  
userdomainqueryirrelevantprob

for capturing user domain preference w r t to
similar queries 
for a user domain query tuple find out the
three probabilities  high relevant  relevant 
and irrelevant 

user url query
  
userurlqueryhighrelevantprob
  
userurlqueryrelevantprob
  
userurlqueryirrelevantprob

for capturing repeated url history for a user
for similar queries 
for a user url query tuple find out the three
probabilities  high relevant  relevant  and
irrelevant 

user url
   
userurlhighrelevantprob
   
userurlrelevantprob
   
userurlirrelevantprob

for capturing repeated url history for a user 
for a user url pair find out the three
probabilities  high relevant  relevant  and
irrelevant 

   

unpersonalized rank

the rank at which the url is displayed in the
original result list  it contains extremely
valuable pieces of information of pagerank 
query document similarity  and all other
information yandex could have used to
compute its original ranking 

   

query entropy

for finding out navigational queries 

   

query length

measure for query ambiguity

   

query average position in session

measure for query ambiguity

table   

    test features
all the above feature extraction is done on training set based on click information  but we
dont have click information on test data  to assign feature values for test data set  we use
the values computed for train data where predicate value is matching 
example  for a given user domain pair in test data check if same pair appears in train data 
if it appears  use same feature values of train data  otherwise assign the average value of
that feature 

fi    handling missing feature
as mentioned in the data characteristics  most of the users searched only for a single query 
which means that for a user domain pair  there is just one url  so there are a lot of data
points where the features are not present in test data  also in train data there are many
cases where there is a single url for a predicate value  when we didnt do any smoothing
and assigned a score of one for predicates with single url  the model was giving very high
performance difference between test data         ndcg  and train data       ndcg  
indicating that the model is overfitting  also we found out that the features are biased and
not giving any importance to rank  we analyzed this overfitting on the train data and
handled such cases separately by giving them average score  similar to the test data  along
with laplace smoothing  these changes helped us in improving ndcg on test data to       
and remove the overfitting  train data ndcg       

    re ranking the search results
re ranking can be done by classifying the data points into one of the three classes and then
ordering the urls based on the class label  but     and     have shown that this approach
is highly unstable and ordering the urls by expected relevance is a more stable way  so
we are re ranking our urls using score function calculated using class probabilities to
maximize our ndcg s expectancy      has shown that ndcgs expectancy is maximized using
the following equation 

maximum is obtained by sorting urls by decreasing values of numerator 

that is the decreasing values of 

we modified the sort function to give a slight weightage to rank so that it will break the
tie when two urls have same class probabilities  we are giving some weightage to prob 
also to counter the cases where   url  has significant prob  and prob   but prob  is zero 
and url  has slightly low prob  and prob   but prob  is zero  using the original formula
 decreasing order of prob     prob    url  got better predicted rank even though it has
significant probability of irrelevant class  prob    so after these modifications our scoring
function is 
score          log rank     prob         prob            prob    low score is better 




rank is non personalized rank
prob  is probability of relevant class 
prob  is probability of high relevant class

fi

prob  is probability of irrelevant class 

we then sort the urls in increasing order of their score value so that the url with
predicted rank   has least score and has high importance 

  algorithms and results
we have approached this problem using a point wise approach  point wise algorithms
typically rely on either regression or classification methods  our approach was based on
the classification of the test urls into one of the   classes      missed  irrelevant skipped  
   relevant      highly relevant   for training our model to classify the urls into the above
classes  we have used the following algorithms 
 random forest
 gradient boosted trees 
we have used the scikit python library implementation of these algorithms 

for random forest  we have used the following parameters 
n estimators     max depth none  min samples split    random state none  n jobs   
for gradient boosted trees  we have used the following parameters 
n estimators     learning rate      max depth    random state  
model

zero entropy
restriction
 dont use
personalization
where query entropy
is zero 

baseline
ndcg
score on
test data

predicted
ndcg score
on train
data

baseline
ndcg on
train data

predicted
ndcg on
train data

gradient
boosting

no restriction

      

      

      

      

gradient
boosting

with restriction

      

       

      

      

random
forest

no restriction

      

      

      

      

random
forest

with restriction

      

      

      

      

table   

fi  

conclusion and future work 

from our results  we can see that gradient boosting gives better performance on test data
than random forest  for the dataset and set of features we have used  random forest is
giving more importance to predicate probability features than yandex rank  as a result of
which  it is giving better result on train data but not on test data 
from our observation  to handle this problem better  much more data needs to be
processed to have a better history  for that  map reduce approach may be used  the class
labels can be increased from   to   in order to distinguish between skipped and missed
clicks  further scope of work includes adding new features like similar user preference
using collaborative filtering  getting query similarities using knn  and trying some more
classification algorithms  svm  and algorithms specific to ranking   ranknet  lambdamart
using ranklib   if unencrypted data  like actual queries instead of ids  is available  nlp
techniques can be applied to analyze query terms and page content  for better
personalisation 

references
   j  teevan  s  t  dumais  and d  j  liebling  to personalize or not to personalize 
modeling queries with variation in user intent 
   s  fox  k  karnawat  m  mydland  s  dumais  and t  white  evaluating implicit
measures to improve web search
   milad shokouhi  ryen w  white  paul bennett  filip radlinski   fighting search
engine amnesia reranking repeated results
   learning to rank using classification and gradient boosting
http   research microsoft com pubs       boosttreerank pdf
   dataikus solution to yandexs personalized web search challenge
http   research microsoft com enus um people nickcr wscd     papers wscdchallenge    dataiku pdf
   a large scale evaluation and analysis of personalized search strategies
http   www     org papers paper    pdf
   adaptive web search based on user profile constructed without any effort from
users http   www     wwwconference org docs  p    pdf

fi