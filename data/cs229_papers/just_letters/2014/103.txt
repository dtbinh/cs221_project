hacking the hivemind 
predicting comment karma on internet forums
daria lamberson  darial   leo martel  lmartel   simon zheng  szheng  

 

introduction










virality on online platforms has a number of social and
monetary implications  to measure virality  many websites
such as hacker news and reddit allow users to upvote
or downvote others comments  and these votes are aggregated into a single numeric karma score for each comment  while these karma scores are a numeric proxy for
the popularity and virality of a users views  they are affected by a multitude of factors  and as a result  virality
is notoriously difficult to predict  in this project  we apply
machine learning techniques to build systems that can accurately predict comments success  using karma as a proxy 
we further analyze the content based and metadata features that are most weighted in determining a comments
popularity in order to provide insight into the role of such
features in comment virality 

 
   

   

text of comment
authors username
user the comment replies to  if any 
depth within comment thread
time of day posted
time elapsed since article posting
headline of article
url of article

data cleaning

we wrote a script in ruby to clean the comment text and
store important data  we removed html entities and tags 
all capitalization  punctuation  and stop words  contentless words like the   using this format of comment text 
we were able to ignore much of the insignificant parts of
the original text for content based features  such as our
unigrams for naive bayes  and thus improve the accuracy
of our content based predictions  we kept a copy of the raw
text for the stripped out elements for some of our metadata
features  such as the number of hyperlinks 

approach
data collection

for example  a raw comment 

we scraped        comments from hacker news using a
custom ruby script that pulls from both the official hacker
news api     and the scores from a prominent unofficial
api called algolia      because the former does not publish comment scores   for the reddit data  we parsed an
existing snap dataset of several million reddit comments
     our database of comments includes the text as well
as all available metadata  author  post time  parent comment post  and score  the score is our ground truth  posts
with positive scores are labeled good 

i  x   ve been looking into these issues for over
a decade  sarno  x   s theory is psychobabble   a
href   http   en wikipedia org wiki tension myosi
tis syndrome  rel   nofollow   tension myositis 
syndrome  a   p  i  quot the treatment protocol
for tms includes education  writing about
emotional issues  resumption of a normal lifestyle and  for some patients  support
meetings and   

we aimed to scrape recent comments to maximize the applicability of our algorithm to comments posted today  toorecent comments  however  do not yet have stable scores
because users may still vote on them  so we scraped a block
of comments from late september       an alternative approach would be to scrape a random subset of all accessible
comments throughout the history of the website  but we
opted to favor recent trends and practical usefulness over
historical fairness 

afterwards  the links are saved 
  http   en wikipedia org wiki 
tension myositis syndrome  
and the final clean comment becomes 
issues decade sarnos theory psychobabble
tensionmyositissyndrome treatment
protocol tms includes education writing
emotional issues resumption normal
lifestyle patients support meetings   

in addition to the full text and score of each comment  our
script collected various metadata 
 

fi   

features

   

 

features

results

this svm with smo  we used the weka svm java
package     
 linear svm with smo with nb classification output
as an extra feature  we implemented this by chaining
the previous two implementations 

after collecting and cleaning the data  we created a number of heuristics to create roughly two dozen features that
corresponded to popularity  these primarily fall into four
categories 
 raw timing features such as posted in the afternoon
and time since article posted
 raw relevance features such as popularity  score  of
article and depth within comment thread
 raw sophistication features such as average word
length and profanity count
 derived content features such as sentiment analysis
score and naive bayes classification

   

regression

we also took a regression approach to the problem  in which
the output of our system for each comment is a number
a prediction of the comments score rather than a simple
classification of popular unpopular  we implemented two
different regression algorithms and ran three experiments 
 n class classification  integer score prediction  using
naive bayes with unigram features  each class corresponds to a popularity level  e g  all one upvote comments  two upvote comments  etc  up to the highest
popularity level  
 linear regression using unigram features 
 linear regression using only the metadata features 

see figure     in the results section for the complete list of features  most are self explanatory  and the
rest we explain below 
 naive bayes prediction is the class predicted by
our naive bayes classifier  trained beforehand on the
same data set 
 sentiment is the emotional positivity negativity
score generated by a sentiment analysis library     
 comparative sentiment is the sentiment score normalized by comment length 
 depth is the thread depth  the size of the reply chain
between the top level article and comment 

 

evaluation metrics

   

binary classification metrics

to measure our performance in binary classification  we
used the following metrics 

 

model

 precision   tp  tp fp 
 recall   tp  tp fn 
 accuracy    tp tn   tp tn fp fn 

we considered many types of models for our task but ultimately decided to focus on binary classification and regression 

   

where tp is the number of true positives  fp is the number
of false positives  tn is the number of true negatives  and
fn is the number of false negatives 

binary classification

by using all three of these metrics  we can determine how
accurate our system is and what types of errors each of our
models is making 

in our binary classification model  we established a karma
threshold to distinguish between popular and unpopular comments  where popular comments have received
more upvotes than downvotes  the advantage of binary
classification is that it maps directly to the expected use
case of our system  a user asking should i post this comment  we tested three different binary classification algorithms 

   

regression evaluation metrics

to measure our performance in regression  we used root
mean squared error  rmse   which is defined as 
v
m
up
 
u
 yi  yi  
t
i  
rm se  
m

 naive bayes  nb  classification using unigram features  in the bag of words  order agnostic model 
pulled from the text of each comment  we process comments before extracting unigrams by removing html  punctuation  and stop words  we used
a python implementation of nb from the machine
learning package scikit learn     
 linear support vector machine  svm  with a sequential minimal optimization  smo  for binary
classification of popular vs  unpopular using
 only  the metadata features discussed above  for

for regression  the goal is to minimizing root squared error
to increase accuracy of predicted karma scores 

 

results

for each approach  we trained on a randomly chosen    
of the data and tested using the remaining      we used
 

fi   

binary classification

 

   

all         available comments in our data set for hacker
news classification  but used the most recent          reddit comments rather than the entire   million to speed up
training 

   

discussion

regression

train
rmse

test
rmse

lr  unigrams 

   

    

lr  metadata 

   

   

nb  n class 

   

   

binary classification

when comparing our three binary classifiers across both
reddit and hacker news  we found that the svm with
metadata features and smo had the highest recall for predicting hacker news comments and the highest precision on
reddit comments  further  our svm with naive bayes and
metadata features and smo had the highest recall on reddit comments  naive bayes solely with unigram features
never outperformed the other two in any of the categories 

these results show us that we achieve the best performance on our regression when predicting using linear regression based on our metadata features mentioned above
 with rm se        meaning that on average we can make
predictions within     points of the true popularity  
we also see that using linear regression based on unigram
features has extraordinarily high overfitting as demonstrated by having a training rmse of     and a testing
rmse of       this is discussed in the error analysis 

 
for binary classification on hacker news data  we find that
we have comparable performance in precision and overall
accuracy for all three models but the svm with smo with
no unigrams far outperforms the other two in recall      as
opposed to the nb classifiers     and svm nbs      

   

discussion
feature analysis

we extracted feature coefficients from our svm model and
found that there can be a large discrepancy in the features that are important in determining comment popularity across different platforms  in many ways  we can
see that the different weighting given to different features
reflects the dynamics of each of the networks 
note that the strongest predictor in determining popularity of both the hacker news and reddit comments is the
naive bayes prediction  this makes sense since the nb
output is based solely on unigram counts whereas most of
the other features are based on comment metadata  by incorporating the nb output  we allow our svm nb model
to take into account a vague sense of the comment content 
the weighting indicates that it gives it relative importance 
which causes our svm nb model to take on some of the
strengths and weaknesses of our nb classifier that the svm
alone does not 

for reddit data  we find that the svm model without nb
output far outperforms the other two in terms of precision
     versus     for nb and     for svm nb  the svm
model also has the highest overall accuracy but the difference is not as large      as opposed to nbs     and
svm nbs       however  in terms of recall  the svm
model with the nb output far outperforms the other two
     vs     for nb and     for svm without nb  

we use our feature weights to make some recommendations
regarding how to structure comments on each of these platforms to maximize popularity  according to the coefficients
for each feature  we recommend that on hacker news  one
could optimize karma by posting a long  pessimistic reply to a brand new article  on reddit  one should focus on producing short  profane responses to popular 
visible posts 
 

fi   

error analysis

 

discussion

figure      the weights that our svm model gives to each of our features 
similarly  examining naive bayes coefficients produced a
plausible list of best unigrams for each corpus 
for hacker news 
  
  
  
  
  
  
  
  
  
   

   

       
       
       
       
       
       
       
       
       
       

people
time
make
good
work
data
things
lot
code
years

algorithms  many of our misclassifications seemed to fall
victim to this  in real life  a quick post time and high word
count dont salvage a completely irrelevant comment 

for reddit 
  
  
  
  
  
  
  
  
  
   

       
       
       
       
       
       
       
       
       
       

that said  we find that our hacker news svms recall of
    based only on metadata is high enough to still be very
useful  if our classifier tells you not to post something  you
can be confident that your comment is bad  because such
high recall implies a low false negative rate  on the other
hand  the somewhat lower precision of     means that the
algorithm will endorse some bad posts as good  thus  our
system will filter out many  but not all  of your bad posts
and very few of your good ones  clearly a useful tool compared to the baseline behavior of posting every comment
you compose 

pts
coms
people
time
funny
mos
good
make
fuck
shit

furthermore  for reddit data  our svm without nb prediction has relatively high precision whereas our svm with
nb prediction has relatively high recall      in both cases  
these are both useful models to have based on whether we
want to minimize false negatives  to avoid wasting good
posts  or minimize false positives  to avoid the negative
attention drawn by a bad post  

error analysis

our classification results are good but not spectacular 
our features  which include post timing and average word
length  correlate with comment popularity  but we suspect
that the most direct triggers to upvoting and downvoting
behavior  such as agreement with a controversial opinion
or disapproval of an off topic tangent  would require a high
level of natural language understanding and therefore be
extraordinarily hard to detect with broad machine learning

in general  reddit data led to higher precision and hacker
news data led to higher recall  we hypothesize that bad
reddit comments are often more blatantly offensive  lead 

fi   

data collection challenges

references

   

ing to fewer false positives  while the hacker news community is less aggressive and arbitrary with downvotes  leading
to fewer false negatives 

most existing sentiment analysis libraries do not work well
with forum comments because of their high concentrations
of jargon  slang  and internet speak  our sentiment features were moderately helpful  but many comments were
given a sentiment score of    neutral unknown  despite
clear signs of enthusiasm or vitriol upon manual inspection 
similarly  we tried to use a few topic modeling libraries but
they were completely flummoxed by the vocabulary used
on these websites 

our worst performing regression model was linear regression based on unigram features  which had high overfitting as demontrated by having a training rmse of     and
a testing rmse of       this severe overfitting was expected  because unigram features produced an extremely
high dimensional feature space  we addressed this by running linear regression on the metafeatures instead  with
much better results 

 
   

data preprocessing challenges

future work

data collection challenges
in our next steps we would hope to 
   expand our content based features to try to attain
a higher level of natural language understanding  in
particular  we would like to measure the relevance of
comments to their parents 
   explore more complex multi class classifications than
binary  for example  unpopular  ignored  fairly popular  and uber popular 
   spend more time on regression and build more complex continuous output models  binary classification
provides the core use case  whether or not to post  
but finer grained outputs would allow optimization
for higher scores 
   explore less general  and potentially more useful 
data sets  such as restricting the reddit classifier to
one subreddit  subforum  

fig  the distribution of api accessible hn comment scores

references

the most significant complicating factor in data collection
was that both reddit and hacker news  and most internet
forums  hide and delete unpopular comments  so finding
a large training set of unpopular comments proved difficult  as such  our data set was mostly limited to mediocre
and better  but lacking in very inflammatory comments  
which makes decisive classification harder and can lead to
class imbalance  for example  while hacker news comments can become negative  we only saw those that were
only mildly disliked  karma score of    meaning one more
downvote than upvote  or neutral  karma score of     to
see this  we show the distribution of karma scores for our
hacker news comments below 

    f  saif and a  he  stopwords  proceedings of the
international conference on language resources and
evaluation       
    hacker news api 
https   github com hackernews api 
visited on october          
    hn search api  http   hn algolia com api  visited
on october          
    j  leskovec and a  krevl  snap datasets 
stanford large network dataset collection       
 http   snap stanford edu data  

the following figure demonstrates that a plurality of our
hacker news cmments have scores of exactly    and we
have relatively few  around      examples of truly bad
comments  score     however  we posit that this data still
holds value as a training set  in our experience  strongly
negative comments tend to be purposefully inflammatory
 trolling  racial slurs  etc   so an earnest user of our tool
is likely trying to differentiate between whether her comment will be popular or ignored  rather than popular or
reviled 

    m  hall  e  frank  g  holmes  b  pfahringer  p  reutemann  i  witten  the weka data mining software 
an update  sigkdd explorations  volume     issue
        
    m  pedregosa  et al  scikit learn  machine learning
in python  journal of machine learning research    
     
    sentiment 
https   github com thisandagain sentiment
 

fi