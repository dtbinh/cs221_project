cs     final project  fall     

 

language identification and accent variation detection in
spoken language recordings


shyamal buch   jon gauthier   arthur tsang
 shyamalb  jgauthie  atsang   stanford edu
computer science dept    symbolic systems program
stanford university

abstractwe develop a model for identifying languages and accents
in audio recordings  our hierarchical sequential nodule model
 hsnm  incorporates both short distance features  which capture
simple linguistic distinctions  e g  phoneme inventories  and longdistance features  which detect long distance suprasegmental patterns 
e g  tone and prosody  which help a classifier discriminate intelligently
among two or more languages 
we apply this model to the language identification  lid  and
accent detection  ad  tasks  and investigate the potential for simple
knowledge transfer between the two tasks  we demonstrate that the
hsnm model performs well on binary and multi class classification
tasks in lid and ad 

i 

i ntroduction

a  motivation
language identification  lid  systems are utilities for determining the language being spoken in an audio recording  many
modern lid systems operate on audio inputs of variable length 
and output a single decision or a probability distribution over the
possible languages being spoken in the recording      applications
for lid systems include emergency call routing and multilingual
translation systems          
accent detection  ad   a variant of language identification  can
be used in speech training and also to improve accuracy of lid
systems  which can suffer large accuracy losses on accented speech
     accented corpora can be small relative to traditional speech
corpora  due to the higher cost of accurate data collection     
thus  data light methods to improve accent detection systems can
be of value for the task 
b  prior work
research on the lid task can be broadly divided into two
approaches  the first approach focuses on building generative
language dependent models  passing audio through languagespecific tokenizers or speech recognizers in order to approximate
the probability of a particular recording being in a particular
language           systems built in this way yield high accuracy
results  but are slow and likely difficult to maintain  such pipeline
systems also have a critical failure point in that errors committed by
components early in the pipeline  e g   tokenizers which convert
audio into phone transcriptions  can yield unrecoverable systemwide failures 
the second approach is a lower level solution  where languageindependent audio features  and complicated conjunctions of such
features  are the primary source of information for classification
          this method is more appealing from an engineering
acknowledgement  thanks to sebastian schuster  stanford nlp group  for
offering useful advice throughout the course of the project 
note  appendix material available upon request 

corpus
ogi
cslu

train

german
dev

test

train

   
   

   
  

   
  

   
   

mandarin
dev
test
   
  

   
  

table i  number of recordings per split in datasets used  ogi
refers to the language identification data  and cslu refers to the
accent identification data  cslu numbers are approximate 

perspective  as it can be highly portable and robust among different
linguistic environments 
there has also been significant prior work in the field of
accent detection  particularly in the specific problems of shanghaiaccented chinese     and north vs  south accented american
english      these approaches typically rely on gaussian mixture
models  gmms   and adopt the second approach described above 
c  our work
this papers objectives are as follows 
   develop a model that can capture distinguishing language
patterns with only language agnostic audio based features 
   develop an lid system which integrates the model to give
accurate classifications 
   develop an accent detection  ad  system which incorporates both the model and the corresponding lid model to
leverage language data for additional accuracy 
we situate ourselves firmly on the second side of the dichotomy
mentioned earlier  employing only language independent features
drawn directly from the audio  along with conjunctions and
synthetic forms of those features   section iii gives a detailed
description of the model we develop centered around such features 
we apply this system first to binary lid classification between
german and mandarin audio recordings  german and mandarin
differ in everything from basic linguistic details like phoneme
inventory to suprasegmentals and long term patterns like phonotactics  syllable structure   tone  mandarin has lexical tone   and
prosody  for this reason we see the language pair as a useful first
target in developing an lid system 
ii 

dataset

a  language identification
the ogi multi language telephone speech corpus     is a
collection of thousands of phone call recordings in    languages 
the recordings in the corpus vary in duration from   seconds to
  minute  and are either responses to prompts or free speech 
some of the calls in the corpus have an associated basic phonetic
transcription  the result of a process of manual segmentation and

fics     final project  fall     

 

hierarchical sequential nodule model  hsnm 

annotation by linguists  we choose to not use this extra data for
the following reasons 
 this data requires expert intervention  and makes generalizing to new languages or different corpora difficult 
 no real time system could depend on this data  as such
annotation would not be available at runtime 
 annotations are provided for only a subset of all recordings 
and it would be impractical to depend on such partial data 
audio recordings vary in quality  extremely poor quality calls
are labeled as such in the dataset  we retain low quality calls in
our experiments 
the ogi corpus comes with a predetermined train   development
  test split  we use this split in all of our experiments  table i
gives the number of recordings in each split of the dataset for
both german and mandarin 
b  accent detection
the cslu foreign accented english corpus       like the ogi
corpus  contains thousands of phone quality calls  all calls are
english language free speech  by speakers native in each of   
foreign languages  some recordings are as short as   seconds  but
the majority are    seconds long 
in addition  for each recording  we have the strength of the
accent  as judged by three native english speakers on a   point
scale ranging from negligible no accent to very strong accent 
we randomly split our dataset into sections of approximately
          and      for train  development  and test respectively 
iii  m ethodology
a  hierarchical sequential nodule model  hsnm 
we develop the hierarchical sequential nodule model  hsnm 
to capture distinguishing linguistic features in audio speech recordings  the model is deliberately designed to capture both shortdistance patterns  phoneme distributions  fundamental frequency 
and long distance patterns  phonotactics and syllable structure 
tone  prosody  etc    while we wish to capture such phenomena 
we also wish to keep the system language independent  languagedependent features are costly to engineer and difficult to maintain 
by using only language independent features drawn from the
sound waves of the recording itself  we ensure that extending the
system to a different language amounts to simply collecting the
requisite audio data 
the hsnm is designed to operate at one abstraction level higher
than the low level speech features typically used in the literature
      as such  any standard low level feature set can be used
interchangeably  typical low level features include mel frequency
cepstral coefficients  mfccs   volume  pitch  and others      
in order to capture both short  and long distance phenomena
in speech  we aggregate the described low level audio features
in two ways  first hierarchically  collecting statistics from feature
distributions over multiple slices of audio  and second sequentially 
by passing forward information from one hierarchical aggregation
to the next  figure   shows an overview of the structure of the
model 
we first split an audio file into short segments of t seconds in
duration  the audio recording can then be modeled as a sequence
of these segments  we build a sequence of nodules above this
segment collection  each nodule ni has n associated consecutive
segments x            xn as children  furthermore  each pair of consecutive nodules ni  and ni have so number of segments that
overlap between the two  we construct a representation of a nodule
by combining features from its preceding nodule and its associated
child segments 



audio sample
 recording 

segments



frames



segment
features

 x   x   x   x   x   x   x 




feature
nodules
 sequence 

trans trans trans trans trans trans

fig     hierarchical sequential nodule model  hsnm  developed
for language identification and accent detection tasks  the hsnm
synthesizes high level hierarchical and sequential features from
more local audio based features  themselves drawn from small
splits of an audio recording 

ni   hn  t  so   trans  ni      x            xn  i 

   

here trans is a feature extractor function which draws features
from a nodule structure      extracts functionals for each feature
of the provided segments  following       the two high level sets
of feature aggregations we use for    in our experiments are as
follows  these functionals are calculated independently for each
sequence of values for a feature drawn from the child segments of
a nodule 
basic  mean  delta  change from first to last segment 
complex  basic   standard deviation  mean
formally  we can describe the hsnm as a tuple
h   hn  t  so     trans   ci  where c   n  l is a classifier
described below 
   classification  we classify a recording as being in a language
   l using a classifier at the nodule level  which maps a single
nodule ni to a predicted language    the language of the entire
recording is the language with the most nodule level votes  if a
recording has k nodules 
    arg max
 l

k
x

  c nj        

   

j  

   training  from every recording we construct k nodules from
fixed size audio segments  where k varies based on the length of
the recording   since each recording is labeled with the language
being spoken in the audio  we can construct for a given recording
with k nodules k different training examples  hni    i kj     where
  is the language of the overall recording 
  for

the first nodule  n    we assume trans  n         

fics     final project  fall     

 

language identification  lid  system
corpus

accent detection  ad  system

ogi  speech  corpus  

accented  speech  corpus  

      dev        train        test   

audio sample
 recording 

hsnm  lid 

hsnm

training

hsnm model

voting
aggregate nodulelevel votes

 g  m  m  g  g  m  g   

classification

german

accent classification
test

fig     the lid system based on the hsnm  speech samples
drawn from the corpus are processed into hsnm nodules  which
are used at training time  at test time  nodule level classifications
are aggregated to yield an overall classification 

we train the classifier c   n  l on the concatenation of all
training examples drawn from recordings  note that we train each
nodule vote independently of previous nodule vote information  by
avoiding constructing a full sequence model over a recording  we
keep training highly efficient and simple to implement  nodules
still do carry sequential features  defined by trans   but the
classification at the nodule level is trained without conditioning on
previous classifications  a key distinction from a typical sequence
model 
b  language identification  lid  system
we first perform minor preprocessing on the audio data  we
normalize each recording to have a peak amplitude of    db 
and drop segments below the required length t  specified as an
hsnm hyperparameter   as we discuss in section iv  we added
this preprocessing step after performing some basic error analysis
on early output from the model 
we proceed to extract our base low level audio features  here 
we use a standard feature set that has been demonstrated to work
well for speech recognition tasks       we use opensmile     
to extract these low level features  which include mfccs  pitch 
loudness  jitter  and line spectral pairs  among others 
we then apply and fine tune the hsnm described in the
previous section to the task of language identification  asking
a system to discriminate between mandarin and german voice
recordings  in this task our classifiers are thus functions c  
n  l  where l    g erman  m andarin   we also experiment to find optimal values of the other hsnm hyperparameters
t  n  trans  ni      x            xn   for our lid system 
we also develop a smart baseline in order to isolate the effects
of the hierarchical and sequential features within the hsnm  the
baseline uses the same low level features as described above to
yield language votes at the segment level  that is  there is
no aggregation of segments into nodules and no synthesis of
hierarchical or sequential features 
c  accent detection  ad  system
we perform the same preprocessing and feature extraction on
the accent audio data as we do on the language id data  described
in the previous section  
we then apply the language id model constructed in the previous
two sections to the task of accent detection  our classifiers now

fig     condensed schematic of the accent detection system  we
apply the same lid sequence voting model  see figure    to accent
data  and leverage lid models to improve performance further 

model

dev f 

test f 

baseline  t     
best model  t     

     
     

     
     

baseline  t     
best model  t     

     
     

     
     

table ii  performance of selected models on language identification  lid  development and test sets  varying segment duration
t  metric is macro averaged f   over two classes  

discriminate among a set of two foreign english accents  l  
 g erman e nglish  m andarin e nglish  
we also attempt a simple form of knowledge transfer in order
to leverage parameters learned on language identification data
for the accent detection task  this development is driven by an
intuition that patterns which distinguish a particular language from
its peers also appear in the speech of its native speakers in other
languages  for example  we expect trends in german prosody to
also distinguish german accented english speech 
with this motivation  we pose a knowledge transfer experiment
where a model trained on accent data is ensembled with a corresponding language identification model  we redefine the hsnm
as a tuple hn  t  so     trans   c  w i  where c is now a set of
distinct classifiers trained on the same feature sets extracted by
  trans   w is a collection of per classifier vote weights  where
higher weights indicate greater confidence in the classifier   we
determine the most likely accent for a recording by collecting votes
from each classifier at each nodule  extending equation      
    arg max
 l

k x
x

  c nj       wc  

   

j   cc

in practice  we find including a language id classifier increases
accent detection performance  but only when it is downweighted
relative to the main accent detection classifier  see section iv b
for more information 
iv  r esults   d iscussion and a nalysis
a  language identification  lid  results
table ii shows a brief overview of the improvement our model
achieves over the baseline described in section iii  varying the
hyperparameter of segment duration t  the following section
describes in detail results from several other hyperparameter experiments 

fics     final project  fall     

 

  

  
  
  
  

baseline
logistic
svm  gaussian k 

  
 

   

 

   

 

   

  

  
baseline
  sec
  sec
  

 

 

   

nodule size

 a  classifier experiments  svm outperforms logistic regression by a large margin  otherwise
identical feature sets   hsnm configuration 

accuracy  avg  f  

  
accuracy  avg  f  

accuracy  avg  f  

  

  

  

  

baseline
basic h s features
complex h s features

  
 

   

 

   

 

 

nodule size

   

 

   

 

   

 

nodule size

 b  segment size experiments    second segments help the hsnm best capture short  and
long distance features useful for language id

 c  feature set experiments  complex features
yield the greatest performance margin over the
baseline  at n      

fig     experimental results for lid hsnm system on german vs  mandarin classification 

silence
speech disfluencies

  of failed examples w  property
german
mandarin
     
     

     
     

table iii  error analysis  common properties of recordings
on which lid classification failed 

baseline
  way hsnm  ovr svm 
complex features  n      t     

  
accuracy  f  

property

  

  

  

   hyperparameter grid search  the hsnm developed in section iii a has a large number of degrees of freedom  the most
significant are 
 c  the classifier used to collect language votes from nodule
data 
 t  the duration of each segment drawn from a recording 
 n  the number of segments inherited by each nodule  and
  and trans   the feature extractors used to generate hierarchical and sequential features from low level segment data 
we perform a grid search over the possible combinations of
hyperparameters on a held out development set and report results
below  with accompanying visualizations  we vary nodule size and
a single hyperparameter in these graphs  unless otherwise mentioned  by default we configure with an svm  complex features 
and t     sec 
figure  a shows an experiment evaluating different classifiers
c   n  l for discriminating among languages at the nodule
level   the models in this graph all use the complex feature
set  with segment duration t       we find that an svm with a
gaussian kernel offers a significant boost over a baseline 
figure  b demonstrates the effect of different segment durations
t on development set performance  it is clear that in using   second
segments  we forfeit the benefits of the hsnm  in fact  it seems
we lose information by aggregating over segments of too large
a duration  with t      however  we see the hsnm features add
orthogonal information which increases classification performance 
figure  c compares the performance of the two feature sets
tested on different nodule sizes  the complex feature set trials
outperform by a large margin both the basic feature set trials and
the baseline  note that complex features have a clear peak at n     
while basic features peak at n     
   error analysis  table iii presents the results of an analysis
of all examples on which one of the best performing lid systems

german

mandarin
tamil
language

overall

fig     results from a proof of concept trial suggest that the
hsnm based lid system generalizes well to multi class classification  metric is average f  score 

fails  by far the properties most often appearing in these failed
examples are 
 silence  sustained pauses in speech  with no to little background noise 
 speech disfluencies  filler words like uh  eh  etc   which
often precede   succeed long silences 
there were    total recordings on which the lid system failed
for this analysis  we randomly sampled    recordings from those
on which the lid system succeeded and looked for the same
properties as listed above  we confirmed that the failed examples
have these properties far more often than the succeeded examples
 based on the analysis of the random sample  
another error not listed above is volume variance  we recognized a significant variation in volume among recordings early on
during development  and added a preprocessing step to remove this
variation 
   multi class classification  we also validate the performance
of the model with a preliminary experiment on a multi way
classification task  drawing on the tamil portion of the ogi corpus 
there is a similar number of recordings in each split of the corpus
for tamil as weve seen for german and mandarin 
figure   shows the performance of the hsnm models trained
with a one versus rest svm classifier  we train one versus rest
logistic regression models which show similar improvements over

fi 

model

dev f 

test f 

baseline  t     
best model  t     

     
     

     
     

baseline  t     
best model  t     

     
     

     
     

ensembling  t     

     

     

accuracy  avg  f  

cs     final project  fall     

table iv  performance of selected models on accent detection
 ad   metric is macro averaged f   over two classes   ensembling
 ad   lid  shows improved performance for both dev and test 

property

  of failed examples w  property
german accent
mandarin accent

background sounds
southern influenced accent

     
     

     
     

table v  error analysis  common properties of recordings
on which ad failed

the baseline  which is the same baseline as used in other lid
experiments  re applied to the three class problem  
b  accent detection  ad  results
   general results  table iv shows a brief overview of the
improvement our model achieves over the baseline described in
section iii  varying the hyperparameter of segment duration t 
like with lid  we also performed experiments to fine tune hsnm
hyperparameters  and observed similar trends as before  full results
for all the experiments are in appendix material 
   error analysis  our system failed on    out of     recordings
on the ad development set  we listened to all     recordings 
looking for characteristics common in the errors 
a significant portion can be attributed to problems in recording
quality  especially background sound  we can approach this issue
by more sophisticated preprocessing  in addition  some of the failed
examples  which were typically rated by the judges as a strong
accent  sound heavily influenced by southern us english  since
there is not much southern influenced speech in the training data 
it makes sense that the system missed these  however  since we use
an svm with a gaussian kernel  our model should theoretically
be able to handle these kinds of examples if given more southerninfluenced training data  see table v for percentages 
we also notice that the f  score among recordings rated as
negligible no accent by at least one judge is significantly lower
than average  for a german accent  this rate is       compared
to        and for a mandarin accent  this rate is        compared
to       
   ensembling  we see that ensembling boosts performance for
ad  as shown in table iv and figure    by admitting weighted
votes from the lid model with a wlid          we see
performance improvement in both the dev and test set  this shows
promise in the ensembling method 
v 

c onclusion   c hallenges and f uture w ork

we find that the hsnm structure manages to successfully
encapsulate useful representations of both long  and short distance
language independent features  we see boosts in performance after
adding more nuanced functionals which capture more structural

  
  
  
  
  
  
baseline

ad hsnm

ad hsnm 
 lid hsnm 

fig     results of accent ensembling experiment  we find that
wlid         gives perf  boost on both dev test 

 hierarchical  and time based  sequential  patterns  additional experiments with hsnm hyperparameters  segment durations  nodule size  other feature sets  show that performance is indeed dependent on the correct choices  we grid search over hyperparameters to
produce the reported results  preliminary experiments suggest that
the hsnm model generalizes well to multi class classification  this
is likely due to its flexible  language agnostic structure  we also
successfully ensemble an ad model with an lid classifier for the
corresponding language  effectively transferring learned knowledge
from the lid data to the accent task 
future work includes further developing our lid and ad system
to be even more robust by addressing specific issues found in error
analysis  such as silence and disfluencies  and examining potential
overfitting problems on the accent corpus  we also would like
to quantitatively compare the hsnm with standard models used
in speech processing  gmms  hmms  and other models such as
rnns to further support the value in the model developed here 
we are also interested in investigating other knowledge transfer
methods to further enhance the ad system 
r eferences
   
   

   

   

   

   

   
   

   
    
    

    

    

y  muthusamy  e  barnard  and r  cole  reviewing automatic language
identification  ieee spm  vol      no     pp        oct       
e  ambikairajah  h  li  l  wang  b  yin  and v  sethu  language identification  a tutorial  ieee circuits and systems magazine  vol      no     pp 
            
y  zheng  r  sproat  l  gu  i  shafran  h  zhou  y  su  d  jurafsky  r  starr 
and s  y  yoon  accent detection and speech recognition for shanghaiaccented mandarin  in eurospeech         
m  a  zissman  comparison of four approaches to automatic language
identification of telephone speech  ieee trans  on speech and audio
processing  vol     no     p           
m  a  zissman and e  singer  automatic language identification of telephone speech messages using phoneme recognition and n gram modeling 
in icassp      vol     ieee        pp  i    
p  torres carrasquillo  e  singer  t  gleason  a  mccree  d  reynolds 
f  richardson  and d  sturim  the mitll nist lre      language
recognition system  in icassp       mar        pp           
i  lopez moreno  j  gonzalez dominguez  and o  plchot  automatic language identification using deep neural networks  in proc  icassp       
j  t  purnell and m  magdon ismail  learning american english accents
using ensemble learning with gmms  in icmla    ieee        pp    
   
y  k  muthusamy  r  a  cole  b  t  oshika  and others  the ogi multilanguage telephone speech corpus       
t  lander  cslu  foreign accented english release      ldc       
d  bone  m  black  m  li  a  metallinou  s  lee  and s  s  narayanan 
intoxicated speech detection by fusion of speaker normalized hierarchical
features and gmm supervectors  in interspeech        pp           
b  schuller  s  steidl  a  batliner  f  burkhardt  l  devillers  c  a  mller 
and s  s  narayanan  the interspeech      paralinguistic challenge 
in interspeech        pp           
f  eyben  f  weninger  f  gross  and b  schuller  recent developments in
opensmile  the munich open source multimedia feature extractor  in proc 
acmm     acm        pp         

fi