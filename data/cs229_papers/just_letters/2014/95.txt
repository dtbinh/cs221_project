visual localization and pomdp for autonomous indoor navigation

chulhee yun
stanford university  stanford  ca       usa

chulhee   stanford   edu

sungjoon choi
stanford university  stanford  ca       usa

chois   stanford   edu

abstract

maximizes its expected reward or minimizes expected cost 

in this project  we present a system that simulates
a fully autonomous robot in an indoor environment  we built a virtual environment and simulated the robots monocular camera input by rendering the scene in the robots perspective  using
this simulated camera input  we trained the localization algorithm by uniformly sampling image
inputs and collecting a dictionary of orb descriptors and keypoints  we also discretized the
environment and formulated it into two pomdps
with different scenarios  and used qmdp and
fast informed bound  fib  method to calculate
approximate solutions to the problems  through
simulation of the policies we could see that fib
with one step lookahead strategy works the best
among tested algorithms  connection of localization algorithm with pomdp completes the
system for fully autonomous robot simulation 
and our policy was robust enough to perform
trained tasks even with large observation noise 

our original objective for this project was to combine a simultaneous localization and mapping  slam  algorithm
with pomdp to implement a fully autonomous robot 
however  most of slam algorithms calculate the position and pose relative to the starting point instead of the
global coordinate of the map  which makes application of
our pomdp policy difficult  moreover  the error of slam
algorithm accumulates over time  unless there is a loop closure detected during the algorithm  thus  we concluded
that we should separate mapping  training  and localization
 testing  for this particular problem  using images generated from points uniformly sampled in the virtual space  we
trained the localization algorithm by constructing a dictionary of orb  rublee et al         descriptors and true   d
keypoint coordinates  when the test image arrives  the localization algorithm finds matches of descriptors between
the test image and dictionary to estimate the robots position and pose 

   introduction
throughout recent years  the technology of autonomous
system is getting out of the laboratory and into the real
life  the market of unmanned aerial vehicles  uav  also
known as drones  is drastically growing due to its versatility and broad range of application  google driverless
car has achieved its license to drive inside california  and
amazon is currently shipping orders using its robot warehouse 
in these systems  autonomous navigation is the most challenging and important part of their artificial intelligence 
the task of autonomous navigation can largely be decomposed into three parts  mapping  localization  and decision
making  an autonomous system first should understand its
surrounding environment and locate itself in that environment  then  the system must make a proper decision that

this estimate of position and pose can be fed into partially
observable markov decision process  pomdp   astrom 
      as some observation values  pomdp is a generalization of markov decision process  mdp   in which agents
do not have the ability to be fully aware of their current
states  instead  they only have some observations  e g  estimated position  that help them guess what states they are
actually in  since the exact state is unobservable  we instead maintain a probability distribution of states  i e  a
vector  referred to as belief state  the policy of pomdp
is a collection of  vectors  each of which corresponding
to a particular action  inner product of an  vector with
a belief state produces the expected utility of selecting the
action corresponding to the  vector  thus  the optimal
action for a belief state is the action corresponding to the
 vector that maximizes the inner product 
there are a number of algorithms to solve pomdp exactly  smallwood   sondik        sondik         but it
is known that in general cases  pomdp is intractable to
solve exactly  thus  there have been active research on ap 

fivisual localization and pomdp for autonomous indoor navigation

 a 
figure    block diagram of our system 

proximation algorithms for pomdp  in both offline and online settings  two examples of the simplest and fastest offline approximation methods are qmdp and fast informed
bound  fib   hauskrecht         qmdp creates one vector for each action  and iteratively updates the  vector
assuming full observability after the current state  fib  on
the other hand  takes account of the partial observability in
 vector updates  it also creates one  vector per action 
and it is known to produce a tighter upper bound for the exact utility than qmdp  there are also online approximation
methods that are executed on run time  among them  onestep lookahead strategy combined with approximate value
functions is known to upgrade performance compared to
strategy without lookahead 
in this project  we combined visual localization algorithm
and pomdp to train a robot to autonomously navigate
through an environment and achieve given goals  in a
 d virtual living room  a robot observes its environment
through its camera  we generated these camera images by
rendering an indoor scene with opengl and pov ray  we
trained the localization algorithm using this simulated camera image and later used the algorithm to estimate the position  besides that  the indoor scene was discretized and
formulated into a pomdp problem  and approximate solutions were computed using qmdp and fib algorithms 
given that we have trained the localization model and have
solved pomdp  we can simulate a fully autonomous robot
that estimates its position and chooses actions to maximize
its utility  the block diagram describing our system is in
figure   
the remaining sections of the report are organized as follows  detailed explanation of our virtual environment and
simulated camera input is given in section    the algorithm
for environment mapping and robot localization is depicted
in section    section   illustrates the approaches taken to
formulate the environment into pomdp and to solve the
pomdp  the experimental results are presented in section
   and we conclude the report in section   

   camera simulation
visual localization algorithm requires an input from visual
sensors which is usually a camera  in real world situation 
a robot reads visual information from the camera and local 

 b 

figure    high quality synthetic input sequences  note the realistic characteristics which are challenging for the localization system in the wild   a  high exposure washes out the chair due to
low dynamic range making it hard to detect features   b  reflection on the window and the chrome plate changes along with a
robots position causing unreliable keypoints 

izes its global position  to simulate this behavior  we set up
a virtual indoor scene and rendered the scene using a ray
tracer  every time the robot makes movement  the global
position of the robot is given to the ray tracer as a viewpoint  a rendered image from the ray tracer is provided
to the algorithm  the resolution of the video sequences is
          vga  and the field of view is set to      which
simulates common consumer cameras for robots 
we found the dataset published by handa et al   handa
et al         is useful for our purpose  handa released
the mesh of a synthetic living room as well as the videos
recorded from several handheld trajectories   the videos
are not relevant in our case because we need on demand
video sequences  not pre rendered   the scene we used in
this project is shown in figure   

   localization
every time the robot makes movement  a synthetic color
image in an unknown viewpoint is given to the localization
pipeline  we begin with learning an indoor scene to estimate the pose of the robot  the information learned from
the scene is stored  and then a synthetic image from the
robots vision is used to localize the robot along with this
scene information 
     environment learning
environment learning is an one time process required to
run before a simulation  first  we uniformly sampled locations of the scene  we sampled     locations in total 
for each sampled location  we synthesize color inputs using the ray tracer described in section    we also produced
corresponding depth maps using opengl  because the ray
tracer is not capable of computing  d coordinates for each
pixel  we employed opengl and located a depth frame
buffer to get  d coordinates of the scene 
the pairs of color and depth images were then used to ex 

fivisual localization and pomdp for autonomous indoor navigation

figure    set of color and depth images      pairs of images were
used to learn the environment 
figure    discretization of virtual living room into        grid 
the grid at upper left corner is the origin         the lower right
corner being          

keypoints in world coordinate system

   approximate pomdp solution and policy
execution

pi

f

ui

     pomdp formulation

 d projections

r  t

figure    illustration of the location estimation problem 

tract orb features  opencv detected orb keypoints from
the color images  again  we compute orb descriptors for
each keypoint  in average      keypoints were detected per
color image  the length of descriptor was    
finally  we back projected the keypoints to  d coordinates 
we can compute the  d world coordinates because we
know the position of the cameras for each synthesized image  these  d world coordinates of the keypoints were
stored along with the corresponding descriptors  resulting
in              vectors 
     location estimation
we begin with extracting orb features given an image like
section      note that we have  d keypoint and   d descriptor pairs from a query image while there are  d keypoint and   d descriptor pairs in the dictionary 
the descriptors from the query image are matched with
the descriptors in the dictionary using l  distance metric
for each training location  given a set of correspondences
between  d points in the world coordinate and  d keypoints in the query image  we retrieve the pose  r and t  of
the robot  this is done by solving the perspective n point
problem which returns r and t  then  using the following equation  project the  d points into the image plane to
compute projection error and filter out outliers  the threshold we used is   px  finally  our pipeline outputs the pose
 r and t  where the number of inliers are maximized 

we will first clarify our notation related to pomdp 
a pomdp problem can be formulated with a tuple
 s  a  o  pt   po   r   where s  a  and o are sets of states 
actions  and observations  respectively  pt  s    s  a  and
po  o   s    are probability models of state transition and
observation  r s  a  is the reward function 
discretization of the state and observation space is illustrated in figure    we divided the x  y coordinates into
       grid  and marked the regions that the robot cannot
pass into red  in this way  the state space became a space
with     discretized states  in case of observation  we
counted all     grids into possible observation because the
localization algorithm may mistakenly output locations that
the robot cannot pass  we also discretized the action space
into eight actions a    n  w  e  s  n w  n e  sw  se  
the names of the action represent the direction the robot
moves inside the grid 
in case of transition model pt  s    s  a   we assumed that
there is a     chance of moving to the right direction  and
there are     chances each of deviating    from intended
direction of action  this is to take into account errors in
pose estimation  if the resulting state is unable to pass or
is outside the state space  the robot is blocked by the object
and stays in its original state  after investigation about the
performance of localization algorithm  we assumed that the
observation model follows gaussian distribution with mean
being the true state and standard deviation being the half
of grid size  we cropped the gaussian distribution so that
po  o   s    has nonzero values only at      neighborhood
of s   
we solved the pomdp algorithm in two different scenarios  an easy one first and a relatively difficult one next 

  
u
fx
v      
 
 

 
fy
 


cx
r  
cy  r  
 
r  

r  
r  
r  

r  
r  
r  

 
 x
t   
y
t   
z 
t 
 

   

   the robot needs to visit the grid          the upper
right corner  when the robot reaches the corner  it
receives a reward of      and then the scenario ends 

fivisual localization and pomdp for autonomous indoor navigation

   in this scenario  the robot needs to circulate around the
grids                            and          the order
of visit should be correct  each time the robot visits
the right grid  it receives a reward of      the scenario
does not end  and repeats until the end of simulation 
in this case   s           because the state needs to
carry information about the four different goal states 
on top of that  the robot receives a small negative reward
      unless specified in the scenario  this is to encourage
the robot to move faster to the goal 

figure    illustration of policies for scenario   learned from
qmdp and fib algorithms 

     offline learning algorithms
having set up the problem  we used qmdp and fib algorithm to solve the problem  both qmdp and fib algorithm
figure    illustration of policies for scenario   learned from fib
creates one  vector per action  and they are initialized to
algorithms
zero  the update rule of qmdp is
x
 k 
a k     s    r s  a    
pt  s    s  a  max
a   s        
 
s 

and the update rule of fib is

a

   experimental results
     policies learned

figure   shows a brief sketch of the policy obtained using
a k     s    r s  a  
qmdp and fib algorithms  the arrow assigned to each
x
x
 k   
    state indicates the resulting policy when we believe we are
 
 
 s
  

max
p
 o
 
s
 
a p
 s
 
s 
a 
 
o
t
a
a 
in that state with probability    we can see that the two alo
s 
gorithms produce largely the same policy  however  performance resulting from these policies may significantly dif     policy execution
fer depending on the problem  because of different values
in policy execution  we used an online method called
of  vectors  figure   illustrates the policies for scenario   
lookahead strategy  the standard decision strategy for the
clustered according to the goal position  we can note that
action was  b    argmaxa at b  which considers only
the robot tends to move to direction closer to the goals 
current time step  the lookahead strategy considers not
only the current but also the next step to determine the ac     simulation without localization algorithm
tion 
to verify the validity of our obtained policy  we performed
 b    argmax r b  a  
simulations on our scenarios using the same observation
a
x
model that we used to solve the pomdp  for each scenario 
   

p  o   b  a u  u pdate b elief b  a  o    
we tested four pairs of policy strategy   qmdp  standard  
o
 qmdp  lookahead    fib  standard   and  fib  lookap
head  
where
r b  a   p s r s  a b s  and p  o   b  a   
p
 
 
all the simulation cycle started at state          for the first
s  po  o   s   a 
s pt  s   s  a b s  
scenario  when the robot reaches the goal state the simula     simulating in continuous space
tion was reset and initialized to the starting state  we performed the simulation for       steps and accumulated the
although we discretized the state and observation space 
rewards for evaluation  since the second scenario did not
we can still run the simulation in continuous space using
have terminal condition  ten simulations were executed for
the policies obtained  given the true  continuous  state val    time steps each  the pair that has the biggest summed
ues  x  y    and observation values  x  y     we can calreward can be considered to be the best approach 
culate which discrete state and observation these belong to 
the results are summarized in table    in the first scenario 
and calculate actions and update beliefs in the same way as
we can notice that there is no large difference between the
discrete case  instead of sampling s  from pt  s    s  a  
four methods because the scenario is too simple to solve 
we update the state values in continuous space  by executthe minimum steps taken to finish scenario   is     which
ing the selected action with some gaussian noise 

fivisual localization and pomdp for autonomous indoor navigation
table    total reward without localization algorithm 

policy strategy
qmdp standard
qmdp lookahead
fib standard
fib lookahead

scenario  
       
       
       
       

scenario  
        
        
        
        

table     estimated  total reward with localization algorithm 

policy strategy
qmdp standard
qmdp lookahead
fib standard
fib lookahead

scenario  
       
       
       
       

scenario  
     
     
      
     

is hard to attain under partial observability and stochastic
actions  from the accumulated rewards we can conclude
that the algorithms took approximately      steps in average to finish scenario    which is near optimal 
as problem gets bigger and more difficult  however  it
is demonstrated that algorithm with tighter utility upper
bound is more likely to do better  fib outperforms qmdp
in scenario    also  we can notice that adopting lookahead
strategy improves the performance of the policy  the ideal
utility that a robot can get in this situation is          considering this  the performance of the algorithm degraded
due to partial observability and stochastic actions 
     simulation with localization algorithm
finally  we performed simulation of our policies using the
output of localization algorithm as observations  in reality  output values of localization algorithm contains many
outliers  especially when the camera is close to walls
or surfaces  the algorithm barely finds features to match
with trained dictionary  producing unrealistic estimates of
the position and pose  so we could see that the performance of the pomdp policy simulation got worse when
coupled with localization algorithm  despite frequent outliers in observation  however  we could observe that the
robot eventually achieves the goal  although more time
steps were required 
due to huge running time of the localization algorithm 
we could not have enough time and resources to run the
simulation with as many time steps as in section      for
scenario    we measured the performance by sampling the
number of time steps taken for the robot to reach the goal
and converting it to expected accumulated rewards in      
time steps  in scenario    we simulated the policy and accumulated the reward given in     time steps  the results
are summarized in table    even though  fib  standard 
seems to be the best option to opt for  we have to note that
this is only a sample from limited number of time steps 

   conclusion
in this project  we constructed a system that can simulate a
fully autonomous robot inside an indoor environment  using an existing mesh of indoor scene  we constructed a virtual environment where the robot lives  the camera input
to the robot can be simulated by rendering the scene from
the robots view  the images generated from random points
of the environment were used to train the localization algorithm  on the other hand  the indoor environment was formulated into two discretized pomdps according to different scenarios and approximate algorithms such as qmdp
and fib were used to solve the pomdps  given the policy  we could simulate the robots behavior in continuous
state space  simulation using the observation model verified that our solution works reasonably well  and that fib
with lookahead policy execution strategy performs the best
among tested methods  after plugging the output of localization algorithm into the observation of pomdp simulator  we saw that although the performance degrades due to
frequent outliers in observation  the robot robustly reaches
the goal after some increased number of time steps  if we
had more time  we would be willing to improve the localization algorithm and test variety of interesting scenarios
that can be formulated in pomdps 

references
astrom  karl j  optimal control of markov decision processes
with incomplete state estimation  journal of mathematical
analysis and applications                     
handa  ankur  whelan  thomas  mcdonald  john  and davison 
andrew j  a benchmark for rgb d visual odometry   d reconstruction and slam  in icra       
hauskrecht  milos  value function approximations for partially
observable markov decision processes  journal of artificial
intelligence research                   
rublee  ethan  rabaud  vincent  konolige  kurt  and bradski 
gary  orb  an efficient alternative to sift or surf  in computer
vision  iccv        ieee international conference on  pp 
          ieee       
smallwood  richard d and sondik  edward j  the optimal control of partially observable markov processes over a finite horizon  operations research                       
sondik  edward j  the optimal control of partially observable
markov processes over the infinite horizon  discounted costs 
operations research                     

fi