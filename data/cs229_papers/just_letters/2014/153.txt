prediction of price increase for magic  the gathering
cards
matthew pawlicki
aeronautics and astronautics
stanford university
pawlick  stanford edu

joseph polin
computer science
stanford university
jpolin stanford edu

jesse zhang
electrical engineering
stanford university
jessez stanford edu

abstract
magic  the gathering  mtg  is a trading card game that drives an extensive second hand
market  given the relatively high price and traffic volume of these cards  there is motivation to
investigate the parameters of the market with the hope of predicting its behavior  specifically 
this study aimed to use historical price  sales  and tournament usage data to predict dramatic 
short term price changes in specific cards  classification algorithms  such as logistic regression and support vector machines  proved effective in anticipating such price changes  on
average  investing in cards using the models detailed in this report would theoretically lead
to a substantial net profit 

 

background and motivation

magic  the gathering is a successful trading card game with roots going back to       the active mtg
community is composed of over    million players around the world and continues to grow at a rapid pace
     the game itself is highly strategic and requires players to acquire and maintain carefully crafted decks
of at least sixty cards  with tournaments paying out upwards of               active players often spend
close to        annually curating decks that suit their strategies  the contents of tournament winning decks
are posted publicly and tend to influence the dominant and popular strategies at any given time  players may
purchase packs of randomly assorted cards from major retailers as well as single cards available at card shops
or online vendors in the secondary market  the combination of a strong and growing mtg community  welldefined strategy influences  and the availability of secondary market data permits a legitimate opportunity for a
machine learning price prediction study 

 

data

the raw data set included card price history  tournament usage  sales volume history  and the intrinsic attributes
for each card  the tournament usage data was extracted from mtgtop  com which publishes the contents
of tournament winning decks from around the world  the price and sales volume history  which drew from
thousands of vendors around the country  was provided by a developer at mtgprice com  a website dedicated
to compiling this sort of data  card attributes were obtained from mtgjson com  a free card database api 
each sample of the data set represented a specific card on a specific date  samples missing price  usage  or
attribute data were omitted  further  only cards whose rarity  a denotation assigned by the card manufacturer 
was rare or mythic rare were included since these tend to be more expensive and exhibit higher potential
gains  ultimately  the data set consisted of m          samples  all of which were from may      through
august      
 

fi 

feature and label selection

each sample pertained to information for a specific card k on a specific date d and included the    features
shown in table    the expression for usage of card k on date d in tournament winning decks is given by 
k
ud
 

  times card k was used from date d    through date d
total   of cards used from date d    through date d

   

table    selected features for card k on date d
indices
   

feature s 
k
k
k
pd
  pd 
        pd 

      

k
k
k
k
k
k
 pd
 pd 
    pd
 pd 
         pd
 pd 
 

       

k
k
k
k
k
k
 ud
 ud 
    ud
 ud 
         ud
 ud 
 

       

k
k
k
k
k
k
 sd
 sd 
    sd
 sd 
         sd
 sd 
 

  
  
  

mk
k
rd
  k
 
 d

description
prices for day d and preceding week
price difference between day d and each of
the previous   days
usage difference between day d and each of
the previous   days
sales volume difference between day d and
each of the previous   days 
converted mana cost attribute
days until card loses tournament legality
variance of past weeks prices

in attempts to improve the performance of the classification algorithms mentioned in section    the timedependent features  price  usage  and sales volume  were low pass filtered and normalized to generate modified
data sets  the full data set was also divided into smaller sets by clustering on card type  such as creature or
enchantment    types total   or removing samples whose average daily price never exceeded a certain price
threshold 
labelings were constructed as binary classifications in an effort to detect profitable  investible situations  four
different labeling were considered  as shown in table    the inclusion of a       margin in labelings   and  
were intended to structure a profit margin into the classification that would offset transactional costs such as
postage upon resale 
table    various labeling schemes considered
index

labeling scheme

description

 

 k
 
k
y i     pd
  pd   d  

true if price on day d is less than average price
over following week

 

 k
 
k
y i     pd
  mind   d  d  d   pd
 

true if price on day d is less than minimum
price over following week

 

 k
 
k
y i     pd
          pd   d  

true if price on day d is at least       less than
average price over following week

 

 k
 
k
y i     pd
          mind   d  d  d   pd
 

true if price on day d is at least       less than
minimum price over following week

as a means for visualizing price  sales  and tournament usage metrics over time  a custom matlab graphical
user interface  gui  was developed  see figure     the gui allowed for the variation of parameters such as
filter coefficient  price threshold  and time window  this tool proved helpful in determining useful features and
gaining an intuition for how mtg card prices were affected by usage and sales volume 
 

fifigure    custom designed graphical user interface for examining price data

 

model selection

given that the primary goal of the study was posed as a binary classification problem designed for determining
whether or not to buy a card on a specific date  classification algorithms such as logistic regression  lr  and
support vector machine  svm  were deemed most appropriate  after initial classification attempts  the data set
was discovered to not be linearly separable  and regularization techniques were used to improve the robustness
of each model  both models were tested using a k fold cross validation implementation to minimize the effects
of exceptional trials 
   

logistic regression

regularized lr was implemented according to equations   through   based on      for the sake of comparison 
both l  and l  regularization were tested  the results from the liblinear     l   regularized lr code was
confirmed with an independently developed matlab implementation 

   arg max


m
x

log p y  i   x i             

 i 

p y  i   x i        h t x i   y     h t x i     y

t

   

i  



 i 

h  x      

 
    et x i 

 i 

   


     

   

for the l   regularized lr  the theta vector was derived using batch gradient descent whose update rule is
given by equations   and    in these expressions   is the learning rate   is the regularization parameter  and j
 i 
subscript corresponds to the j th element of an  n         vector  note that xo     and serves as the intercept
term 
m

o    o  



  x
 h t x i     y  i   xo i 
m i  

m

  x

 i 
 h t x i     y  i   xj     j
j    j  
m i  
m
 

   


   

fi   

support vector machine

a regularized svm was implemented according to equations   and   where     b wt  t holds the parameters
that svm attempts to determine and c  ranging from     to      is the weight applied to the regularization
term  the l  and l  norms were considered for both the objective and loss terms 
n
o
h t x i        wt x i    b    

   

m

x


ff 
 
w   min   w      c
max       y  i  w  x i 
w  
i  
   

   

performance metrics

to evaluate the performance of the lr and svm approaches  several metrics were considered  first  the
classification errors  sum of false positives and false negatives divided by m  of the models were compared
to the classification error obtained from the trivial classifier  which labeled all samples    never buy   after
outperforming the trivial classifier  performance was evaluated using the likelihood ratio  l  or the ratio of the
true positive rate  tpr  to the false positive rate  fpr   since true positives and false positives were the only
scenarios in which money was made or lost  there was justifiable motivation to maximize this ratio 
although the models were meant to maximize the percentage of correct buy dont buy decisions  a real world
application would be concerned most about maximizing the profit obtained  thus  models were further evaluated based on the amount of profit generated per buy  the amount of profit generated per card in the data set  the
return on investment  roi   and the percentage of maximum possible profit  ultimately  percent error  e  and
percentage of maximum possible profit  f   were selected to be the primary metrics by which various models
were assessed 

f     

prof itmax

m
x
 
 i 
 i  
h t x i    d  pd
prof itmax i  
m
n
o
x
 i 
 i  
 i 
 i 
  d  pd     d  pd
 

   

    

i  

 

results

filtering the time dependent data decreased the performance of all models  and normalizing the data sets resulted in marginal improvement  clustering by card type and removing low price cards resulted in data sets
that were too small to yield consistent models  the labeling corresponding to a change in mean price exceeding
      produced the best results for both accuracy and percentage of maximum possible profit recovered  see
table    

model
svm
lr

table    classification results
training set  m         testing set  m       
e      
f      
e       f      
e      
f      
e       f      

    of the data set was used for training and     was used for testing  the classification error is given by
e and the percentage of maximum possible profit  f   is given in equation    the data was labeled using label
  in table    using forward search  ud  ud   feature    in table    generated the highest percentage of
maximum profit  and sd  sd   feature     generated the greatest true positive to false positive ratio 
 

fi 

discussion

although svm and lr both produced positive profits  lr yielded the highest f with the lowest e  this may
be because rather than creating the optimal separation between the closest samples  the support vectors  in the
case of svm   lr factors in a degree of confidence that correlates to price difference  see figure    
  

  

recommend don t buy
recommend buy

d pd

  

 

   

   

   
    

  

    

 

tx

   

 
    

figure    price change vs feature weighting for logistic regression
of the    features used  the variance of the past weeks prices and the converted mana cost of a card contributed
the least  as expected  changes in price  usage  and sales volumes especially within   days of the current day
were the greatest indicators for upcoming price trends 

 

future work

a logical next step to improving the models would be to more thoroughly evaluate the performance metrics 
recasting these metrics as convex optimization problems and maximizing them would result in models that
prioritize generating profit over simply being accurate  also  it would be worthwhile to consider the impact of
other factors inherent to a real world implementation  such as transaction logistics  trading of multiple copies
of the same card  and strategies for re sale  also  the time frame over which the price was predicted was fixed
at a week  this time frame was likely sub optimal and presents an avenue for further investigation 

 

acknowledgments

we would like to thank mtgtop  com  for providing us the data for the tournament deck lists  mtgjson 
com  for providing us the card database api  and alasdair young from mtgprice com  for providing us the
price and vendor inventory data from may      to august       we would also like to thank andrew ng and
all of the cs     teaching assistants for their mentorship and guidance this quarter 

references
    y  lejacq         august  at     magic the gathering still going strong   online   available 
http   www nbcnews com 
           february  pro tour gatecrash event information 
    a  y  ng  logistic regression       
    r  e  fan  k  w  chang  c  j  hsieh  x  r  wang  and c  j  lin  liblinear  a library for large linear
classification  the journal of machine learning research  vol     pp                 

 

fi