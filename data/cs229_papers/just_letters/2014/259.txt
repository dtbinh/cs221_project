cs    predicting heart attacks
sihang yu  xuyang zheng  yue zhao
december        

abstract
in this paper  the use of multiple machine learning algorithms for arrhythmia analysis is explored 
we present different models built by multi class supported vector machines  svm   multi class nave
bayes  nb   decision tree and random forest  the performance of the various models in predicting the
presence of cardiac arrhythmia and further classifying the instances into    pre defined groups is tested
and presented  the random forest classifier outperforms other algorithms with a test accuracy of     
we provide a discussion on the results of different models  together with some insight about of data set 

 

introduction

to minimize the difference between the cardiologists
classification and ours with the machine learning alcardiovascular disease is the number one cause of gorithms 
death worldwide  claiming more lives than cancer and
hiv combined  in this project  our aim is to distinguish between the presence and absence of cardiac    feature selection
and also further classify one single instance into one
of the    predefined groups  class    refers to nor  we apply minimum redundancy maximum relemal  classes    to    refer to different types of ar  vancy  mrmr  feature selection method  which
rhythmia and class    refers to the rest of unclassified has a better performance than the conventional topones  we systematically investigated the multi class ranking method  to filter out the irrelevant features
classifiers built by the following machine learning as well as the highly correlated features 
in terms of mutual information  the purpose of
methods  supported vector machines  svm   nave
bayes  nb   decision tree and random forest  specif  feature selection is to find a feature set s with m
ically  two different algorithms are employed to ex  features based on two criterions reference   maximal
tend the binary svms into a    class svm classifier  relevance eq    and minimal redundancy eq    
one versus one algorithm and one versus all algorithm  the one versus all algorithm is also used to
build a    class nb classifier  decision tree is a logicbased algorithm  which can directly classify the instances into    classes  random forest is an ensemble method by constructing a set of trees and outputting the mode of the classes given by the trees 
the performance of various methods are presented
and discussed 
our data comes from uc  irvines machine
learning repository  this database contains    
rows  each representing a patient instance  there
are     columns  the first     columns denote the
    features and the last column is the classification
         it also gives us the cardiologists classification  which we take as a gold standard  we aim

  x
i xi   e 
 s 

   

  x
i xi   xj  
 s 

   

max d s  c   d  

xi s

min r s   r  

xi  xj s

where i xi   c  is the mutual information between
the ith feature and the classification  i xi   xj   denotes
the mutual information between the ith and j th feature 
we define the operator  d  r  to combine d
and r and consider the following simplest form to
optimize d and r simultaneously 
max  d  r      d  r
 

   

fifor each model  we try to select different num  prediction procedures using multi class svm  onebers of features and see which amount has the best versus one one versus all  classifier are presented 
performance 
     

 
   

modeling

each time  instances from class i and class j are
used to train a  class i versus class j  svm binary
classifier  which is referred to as i vs j classifier in
the following paragraphs for simplicity  therefore 
for k classes classification problem  there are in total
k  k      binary classifier trained  i e     versus  
classifier    k    versus k classifier  when predicting the class label for a testing instance  the instance
is supplied to all k  k      classifiers  and each classifier predicts a class label for this instance  the final
classification result is chosen to be the class that receives the most number of votes 

naive bayes

in this project  the features are continuous while the
corresponding class labels are discrete                
therefore  the standard nave bayes algorithm cannot be applied to classify these instances directly 
one method to solve this problem is to discretize
the features and then implement a multinomial nave
bayes classifier  another method is to combine kernel density estimation method into bayes  theorem 
the idea is illustrated in the following formula 
j fj  xi  
p  y   j xi     pk

k   k fk  xi  

     

one versus all

each time all the instances are first re divided into
two groups  class i and non class i  and then used
to train a  class i versus all the rest  svm binary
classifier  therefore  for k classes classification problem  there are in total k binary classifier trained 
i e     versus not   classifier   k versus not k classifier  when predicting the class label for a testing
instance  the instance is supplied to all k classifiers 
and each classifier returns a score which indicates the
probability of the instance being class i  the final
classification result is chosen to be the class that has
the highest score  probability  

in the formula  fj  xi   is the estimated density at the
value of xi  it is based on a kernel density fit and only
the observation from the class y   j is involved  j
is the estimate of the prior probability of class y j 
the idea is essentially like a discriminant analysis
without assuming normality  the nave bayes classifier computes a separate kernel density estimate for
each class of y based on the training data for that
class  both methods can allow nave bayes classifier to be implemented in this project  where the feature data is continuous  we choose to use the second
method with kernel density estimation  moreover  to
extend the nave bayes algorithm into a multi class
scheme  the same one versus all design used in the
multi class svm is employed      class i versus all
the rest  binary classifiers are trained  during the
predicting phase  each binary nave bayes classifier
returns a score indicating the probability of the instance being class i  the final classification result
is chosen to be the class that has the highest score
 probability   the resulting accuracy of multi class
nave bayes classifier is        

   

one versus one

     

training and testing procedures

in order to explain the training and testing procedures for each fold  the pseudo code is presented below with explanations 
 a   for each fold  in total    folds  
  use   features to     features  step size is   features  thus in total run    times 
 i use cross validation method in the training data
to select the best parameters  best penalty parameter
c and gaussian kernel parameter   as shown in the
formulas below  for the multi class svm classifier
 ii  train the multiclass svm classifier using the best
parameters selected above with the training data 
 iii  predict class labels on test samples and calculate
the test accuracy 
  choose the best accuracy from    accuracies  this

svm

firstly  two different designs of multi class svm classifier are explained  one design is based on the
one versus one algorithm and the other is based on
the one versus all algorithm  secondly  training and
 

figives the best accuracy that multi class svm classifier can produce for this specific fold  with optimum
feature size  
 b   average the best accuracies from all folds to get
an overall best accuracy that multiclass svm classifier can produce for this given data 
     

        which is noticeably lower than the overall accuracy of        one explanation is that the training
samples for classes   to    are remarkably fewer than
those for class    thus the characteristic of class   to
class    might be less captured by the model than
class    true negative rate  arrhythmia classified
as arrhythmia  is expected to increase given more
training data from class   to class    

svm results

for the parameter selection procedure mentioned
above  the penalty parameter c varies from     to
    and the gaussian kernel parameter varies from
   to       a picture for a typical parameter selection procedure is presented below  we choose the
best parameter pair based on the cross validation accuracies 
the training and test errors versus the number of
training instances for one of the classifiers are shown
in the picture below  the big gap between training
error and test error suggests that the method suffers from high variance problem and the test error is
expected to decrease given more data 

to conclude for svm method  multiclass svm
methods using one versus one classifiers or oneversus rest classifiers present comparable accuracies
of        and         respectively  the accuracies for both classifiers are expected to improve given
more training data  especially from class   to class
   

figure    svm parameter selection

the table below presents the confusion matrix for
one of the classifiers used  due to the limited number of testing samples  the       confusion matrix are
mostly filled by  s which posts a difficulty for reading
and interpreting the results  thus the numbers from
class   to class    are merged and are presented in
the category arrhythmia  from the table  we can
see that the overall accuracy of       is not due to a
large number of unbalanced data but the model actually captures the feature distributions for different
classes and gives a fair prediction for testing samples 
we might also notice that given an testing instance
belonging to arrhythmia category  the probability
that it will be predicted as arrhythmia is       or

   
     

decision tree
grow the tree

decision tree is a kind of logic based algorithm  which
is one of the most successful techniques for supervised
classification learning  figure n shows a diagram of
a decision tree  each interior node corresponds to
one of the features  the edges to children denote
the possible values or value intervals of that feature 
 

fi   

each leaf denotes the classification given the values of
the input features represented by the path from the
root to the leaf  here are the main steps to build a
decision tree  firstly choosing rules to split on  then 
dividing the training instances with the rule into disjoint subsets  repeating recursively for each subset till
the leaves are almost pure  for an ideal tree  the instances reaching the same leaf should be in the same
class  which means the leaf node is pure  however 
the ideal tree can cause severe over fitting especially
in our case when the training date is extremely insufficient 
     

     

grow the trees in the forest

random forest is an ensemble method for classification that operate by constructing a set of decision
trees and outputting the class that is the mode of
the classes output by individual trees  unlike single
decision tree  which is likely to suffer from high variance or high bias  random forests can find a natural
balance between the two extremes  for each tree in
the forest  we construct our training set by sampling
with replacement within the training instances  bootstrap sampling   besides  if there are m features  a
number m   m is specified such that at each node 
m features are selected at random out of the m and
the best split on these m is used to split the node 
the value of m is held constant during the forest
growing  unlike single tree  in the forest each tree is
grown to the largest extent  i e  there is no pruning
and minparent should be   

optimize the tree

in this part  we set the option prune on  change
the number of features  the complexity of the tree
 minparent   and the split criterions  gdi  deviance
or twoing   and calculate the corresponding accuracy 
minparent is a number k such that impure nodes
must have k or more observations to be splitted  the
smaller the minparent value  the more complex the
decision tree  in this way  we find the best feature
size and minparent  which leads to the highest accuracy  figure   shows the relations between feature
size  minparent of the tree and test accuracy  using
gdi as the split criterion  which has better performance than deviance and twoing   we find that this
model has the best performance when minparent is
   and feature size is      the corresponding test accuracy is         training accuracy       

     

error estimation

in random forests  there is no need for crossvalidation to get an unbiased estimate of the test
set error  the out of bag  oob  error estimation is
usually used in this forest model  since each tree is
constructed by bootstrap sampling from the original
data  some of the instances are left out and not used
in the construction of the tree  taking these left out
instances as test set and predicting their classifications with the corresponding tree  at the end of the
run  take j to be the class that got most of the votes
every time the certain instance n was left out  the
proportion of times that j is not equal to the true class
of n averaged over all cases is the oob error estimate 
this has proven to be unbiased in many tests 

    
   
    
accuracy

random forest

   
    
   
    
   
  

   
   

  
  

   

results

   

  
minparent

     

   
  
 

 

feature size

the following figure shows the test accuracy  calculated via the oob error estimation algorithm  varies
figure    the relations between feature size  minpar  with the number of features m with      decision
ent and test accuracy
trees in the forest  we can see as m increases  the
accuracy reach the limit about     
 

fineeds to be pruned to get the balance between bias
and variance  while random forest can achieve both
low bias and low variance  bagging and other resampling techniques are usually used to reduce the
variance in ensemble methods  to make a prediction  all of the models in the ensemble are polled and
their results are averaged  random forests works
by training numerous decision trees each based on a
different resampling of the original training data  in
which numerous replicates of the original data sets
are created  in random forests the bias of the full
model is equivalent to the bias of a single decision tree
 which itself has high variance   by setting prune off
and minparent    we minimize the bias of a single
tree  by creating many of these trees  and then taking the mode of their predictions  the variance of the
final model can be greatly reduced over that of a single tree  in practice the only limitation on the size
of the forest is computing time as an infinite number
of trees could be trained without ever increasing bias
and with a continual decrease in the variance 

    

   

    

oob error

   

    

   

    

   
 

 

  

   

   
m

   

   

   

conclusion
table    testing accuracy for different classify
model
testing accuracy
svm  one versus one 
      
svm  one versus all 
      
naive bayes
      
decision tree
      
random forest  mrmr 
      

 

future work

firstly  the current results of our models are limited
by the small amount of data and the missing data
in some features  incorporating more accurate and
precise patient instances may improve our accuracy
results 
secondly  based on the models we have optimized 
we can create our own ensemble methods  which may
improve the overall performance 
thirdly  we can try to improve our classifier to be
able to work on real time data such as ecg signal 

without loss of generality  in the above table we use
the test accuracy gained via    fold cross validation
for the random forest just as the other models  the
accuracy of random forest shown in the last section is
calculated via out of bag error estimation algorithm  
the test accuracy of random forest is significantly
better than other models  the single decision tree

reference
   peng h  long f  ding c  feature selection based on mutual information criteria of max dependency 
max relevance  and min redundancy j   pattern analysis and machine intelligence  ieee transactions
on                         
   chih chung chang and chih jen lin  libsvm   a library for support vector machines  acm transactions on intelligent systems and technology                     software available at
http   www csie ntu edu tw  cjlin libsvm

 

fi