better models for prediction of bond prices
swetava ganguli

jared dunnmon
abstract

bond prices are a reflection of extremely complex market interactions and policies  making prediction of future prices difficult  this task
becomes even more challenging due to the dearth of relevant information  and accuracy is not the only considerationin trading situations 
time is of the essence  thus  machine learning in the context of bond price predictions should be both fast and accurate  in this course project 
we use a dataset describing the previous    trades of a large number of bonds among other relevant descriptive metrics to predict future bond
prices  each of         bonds in the dataset is described by a total of    attributes  including a ground truth trade price  we evaluate the
performance of various supervised learning algorithms for regression followed by ensemble methods  with feature and model
selection considerations being treated in detail  we further evaluate all methods on both accuracy and speed  finally  we
propose a novel hybrid time series aided machine learning method that could be applied to such datasets in future work 

i 

introduction

key problem 
bond markets are generally characterized by a substantial
dearth of trading information with respect to the amount of
information available to equity traders  while equity traders
can access stock bids  offers  and trades within    minutes
of these activities  analogous information on bonds is only
available to those who engage a fee for data contractor  and
even then only in relatively small subsets compared to the
overall volume of bond trades  the asymmetry in required
versus available information leads to the current state wherein
many bond prices are in fact days old and do not accurately
represent recent market developments     
our goal 
the goal of this project is to use the techniques and algorithms of machine learning and a set of data describing
trade histories  intermediate calculations  and historical prices
made available  on kaggle  by benchmark solutions  a bond
trading firm  in order to more accurately predict up to date
bond prices using data that would be viable to obtain at
a particular moment in time      the high volume of data
characteristic of this problem is common in such financial
modeling endeavors  and hinders the formation of fully
descriptive a priori theoretical models  in this report  we
develop strategies to effectively utilize the data provided for
bond price prediction via thorough investigation of the space
of available machine learning models and combination with
methods from time series analysis  
strategy and methods 
feature selection  an important aspect of this task is creating class balanced training and test data sets while identifying appropriate metrics for assessment of prediction
success  critical features are analyzed and extracted using low order modeling techniques like principal component analysis  pca  and correlation analysis 
supervised learning methods  we first investigate computationally inexpensive techniques such as generalized
linear models  glms  and regression trees  we also assess the viability of methods like principal component
regression  pcr  and support vector regression  svr  
ensemble methods  since we have a regression problem at
hand  regression trees are combined as weak learners
in ensemble methods like bagging  ls boosting and
random forests to reduce overfitting and to potentially
take advantage of the large size of the dataset 
hybrid time series methods  because each bond includes
historical data on five different quantities for the last ten
trading periods  we investigate the possibility of feature
space augmentation or reduction using time series  ts 
  computing

analysis  ideally  predictions from ts methods would
either provide new features with additional explanatory
power or enable reduction of the feature set size while
retaining explanatory power 
neural networks  we experiment with applying neural networks to this problem  as they are known to fit even
highly nonlinear data well given sufficient neurons 

ii 

exploratory data analysis

the data used for this project contains    attributes observed
for each of         bonds    nominal     discrete ordinal   
observation weight and    continuous  ratio  attributes  including a ground truth trade price  to predict the bond price
 often called the  trade price    the data delineates a unique
id of the bond  nominal discrete attribute   a categorical id of
the bond  nominal discrete attribute   a weight importance
of each bond  continuous ratio attribute   the bond coupon
 continuous ratio attribute   years to maturity  continuous
ratio attribute   whether the bond is callable or not  nominal
discrete binary variable   seconds after the trade occurred that
it was reported  continuous ratio attribute   notional amount
of the trade  quantitative discrete attribute   the type of trade
that occurred      customer sell      customer buy      trade
between dealers   and a fair price estimate based on implied
hazard and funding curves of the bond issuer  continuous
ratio attribute   this last attribute is referred to from hence
forth as the  curve based price   in addition  the dataset also
has information about the last    trades that occurred on
each bond considered  including the time difference between
a trade and the previous trade  continuous ratio attribute  
the trade price  continuous ratio attribute   the notional trade
amount  continuous ratio attribute   the trade type  binary
discrete nominal attribute   and the curve based price  continuous ratio attribute  
correlated attributes 
we observe from the correlation matrices that attributes
price of the last trade and curve based price of the last trade
are strongly correlated at all time points  this is intuitively
expected  thus  this information can be used to inform
dimensionality reduction  the fact that the remainder of the
variables are minimally correlated implies that each of those
attributes should supply new information for our prediction 
a similar conclusion can be observed when autocorrelations
are computed for these different time series  specifically 
the mean autocorrelations for each variable are very low
           beyond the first lagged period  indicating that each
variable contributes unique information at every time period 
treatment of categorical attributes 
empirical pdfs of the nominal attributes have been analyzed  from the pdf of the attribute that denotes whether

time on stanford corn  barley and rye clusters is gratefully acknowledged

fithe bond is callable  we see that      of the bonds are not
callable whereas      of the bonds are callable  from the
empirical pdf of the attribute trade type of the current trade 
it is seen that the current trade has      of the type   trade 
     of the type   trade  and      of the type   trade  thus 
there is a relatively uniform sampling of the three trade types 
while preparing the cross validation datasets  this will be
taken into account such that they are class balanced  furthermore  in the ensemble methods used  which are regression
tree based since we are predicting a a continuous output  
these categorical variables are handled appropriately in cases
where they are nominal or ordinal 

upward and downward directions      we evaluate our predictions based on the simple weighted l   norm of the difference
between the actual price and our predictions per sample  i e 
per bond   thus  the model evaluation metric that we choose
is the weighted error in price per sample  weps  which is
defined as 

iii 

statistical significance 
let e  and e  be the errors obtained from two different models m  and m    since the number of records in the training
datasets and test datasets for all models is the same  say n  we
can write the observed difference in the error as  d   e   e   
the variance of d can be computed as

cross validation  feature selection and
model evaluation metric

cross validation strategy 
the key problems encountered in the process of feature
selection and in creating training and test data sets are 
   the time series length is either on the borderline or
below the minimum number of points required for a
statistically consistent time series prediction
   categorical attributes have non uniform distributions
   the amount of data characterizing different bond
weights is distinctly non uniform
points   and   are direct manifestations of the dearth of data
for bond price prediction  it is also important to correctly predict the highly weighted bonds well since they are usually of
higher priority in a portfolio  due to the problems mentioned
above  it is difficult to conduct typical k fold cross validation
wherein the training sets would be class balanced  instead  in
order to utilize all the data given  we utilize a       hold out
cross validation  we therefore create weight balanced training
and test sets using the following algorithm 
algorithm for cross validation 
step i randomly create   instances of weight balanced training and test sets 
step ii run machine learning algorithm on each of these  
training and test sets 
step iii report the appropriate metric  discussed below 
from each of the   independent runs 
step iv the final value of the evaluation metric is the average of these   values 
to demonstrate that our sets are indeed weight balanced  we
plot the pdf of the bond weights for one instance of the
training and test sets in figure   
tr aining set

tes t set
 

pdf of bonds

pdf of bonds

 

   

 
 

 

  

weight of bonds

 

  

weight of bonds

figure    demonstration of weight balanced training and test

datasets 

model evaluation metric 
given that errors in bond pricing are equally detrimental in

   

note that all prediction errors are calculated using our crossvalidation algorithm 

d   d   

 
 e     e      e      e      o      
n  

   

the     confidence interval in our case is then given by  dt  
true difference 
dt   d      d
   
importantly  this implies that improvements in weps out to
the fifth decimal place are indeed statistically significant 
feature generation and selection 
feature selection and generation is handled as follows 
correlation analysis  no attributes supplied are strongly
correlated  mild correlations exist in only   sets of
attributes  thus  this method is not particularly informative 
pca in supervised learning  pca is run on the full dataset
with the goal of determining if there exists a reduced
feature set that retains the majority of the explanatory
power of the full feature set 
scoring function for ensemble methods  random forests
 rf  are used for feature ranking  rf will select features
randomly with replacement and group every subset in
a separate subspace  called the random subspace   we
use a scoring function with the following methodology 
if feature x  appears in     of the trees  then score
it  otherwise  we do not consider ranking the feature
because we do not have sufficient information about its
performance  we then assign the performance score of
every tree in which x  appears to x  and average the
score  our search method is recursive  for example  if
we drop the worst     in the first round  we do so in all
following rounds until the desired number of features
is attained  the     parameter has been determined
via numerical experiment 

iv 

   

 
 

fi
fi
im   wi  fiytrue  y predict fi 
weps  
im   wi

models from supervised learning

we now proceed to explaining implementation and performance of the various models utilized here  all algorithms
were implemented in matlab for ease of workflow  and all
results referenced in the text can be found in figure   
generalized linear models 
several models from supervised learning were investigated 
first  an unweighted generalized linear model was implemented using two different link functions and the full feature
set in order to investigate the underlying distribution of the

fiwls test error vs  pca feature number
 
 
 

test error   

data  while financial data often has an underlying normal
variation  it is important to ensure that this assumption is
valid before proceeding  we report the results of ordinary
least squares  ols  regression using link functions for the
normal and gamma distributions  evaluating the training
and test errors for these different cases illustrates that normal
variation appears to best characterize the data  to improve on
these results  weighted least squares  wls  was performed
using the evaluation weights to appropriately govern which
points are treated with highest importance in the regression 
wls gives noticeable      improvement over ols  in the
context of errors on the order of     

 
 
 
 

principal component regression 

 

we next proceed to implementing pcr using the reduced
feature set  we reduce the size of the feature set to    features using this procedure  as the pca routine reports that
all principal components higher than    are nearly linearly
dependent  in order to make a prediction based on pca  we
extract the transform utilized in the pca algorithm and apply
this directly to the test data  once the data is transformed in
this way  we can run glm models as usual  it was explicitly
confirmed that transforming the regression coefficients back
to the original covariate space gives the same predictions
for ols  validating the prediction procedure we use  interestingly enough  despite the fact that the first few principal
components tend to explain variance in the input best  it is in
fact the independent principal component in our data with
the lowest eigenvalue  i e  the last one in the reduced feature
set  that provides the vast majority of the explanatory power
with respect to the bond price  this is illustrated explicitly
in figure    investigation of this features constituents reveal
that it exclusively contains all of the historical curve and
trade prices  implying that these variables have substantial
impact on correct prediction  in fact  the most explanatory
pca variables are so potent that while using    feature wls
gives an error of         in    seconds  a   feature wls using
only the three most explanatory pca components gives a
weps of only         in just   seconds  this     reduction
in speed could certainly become important in dealing with
massive datasets often encountered in this area of finance 

 

support vector regression 
we briefly investigate the possibility of using support vector
regression  svr  to predict bond prices  but found the model
estimation process to be so time intensive that it precluded
effective parameter tuning  specifically  the libsvm package
did not report an svr result in   days of computation time
while the liblinear package took   days just to estimate
a single model  it is possible that this has to do with the
size of the memory cache allocated to storage of the support vectors  regardless  given these model estimation times 
performing a parameter sweep over the critical svr model parameters proved impractical given the time constraints of the
project  and results for this method are therefore not reported 
regression trees 
regression trees are known to overfit the data  thereby
forming highly biased predictors  this behavior is observed
in our analysis as well  the weps on the training set is
extremely low      to    while the weps on the test set
is      to     to mitigate overfitting  we experiment by  i 
changing the number of data samples required at each node
to make a decision   ii  implementing different metrics for
growing and pruning the trees  e g  entropy  misclassification
errors  etc     iii  varying the number of predictors randomly
sampled at each node to make a decision  and  iv  controlling
the depth of the trees  however  the test weps does not
decrease in any of these cases  a full list of tested conditions
and representative results can be found in figure   

 

  

  

  

  

  

  

number of pca features
figure    wls weps versus pca feature number 

v 

models from ensemble methods

we use bagging of data samples  ls boosting  sequence
of decision trees   and random forests  bagged trees  as
our ensemble regression methods with regression trees as
the weak learner  again  note that all prediction errors are
calculated according to our cross validation algorithm 
random forests with regression trees 
random forests can be a useful method for feature selection
as outlined above  however  they are computationally very
expensive  furthermore  they generally produce a weps of
slightly above     which is inferior to even simple models
like glms  various methods have been experimented with to
reduce the overfitting in each weak learner and to accelerate
the convergence of the random forest algorithm  including 
 i  changing the number of data samples required at each
node to make a decision   ii  different metrics for growing
and pruning the trees  e g  entropy  misclassification errors 
etc    and  iii  number of trees grown for the majority vote 
however  none of these methods improves the performance
of the random forest  the training and test errors along with
the execution time shown in figure   are characteristic of the
experiments we conducted 
ls   boost with rt as weak learner 
we also experimented with the ls boost algorithm with
regression trees as the weak learner  boosting is known to
perform well due to exponential penalizations of observations
proportional to the error in their prediction  j  the number of
terminal nodes in trees  is the critical parameter that can be
optimized for a given dataset  hastie et al      comment that
typically    j    works well for boosting and results are
fairly insensitive to the choice of j in this range  we therefore
choose j      this algorithm performs better than random
forests in that weps stagnates at around    and the computation time is   times lower  characteristic values of weps
on the training and test sets after performing cross validation
are shown in the summary table of figure   

vi  models from hybrid time series methods
a relatively uncommon idea that was explored during this
project was the possibility of using time series analysis to
supplement the given feature set  ideally  using a time series
method to make a prediction for the new price could allow for
much of the historical data contained in the five time series
for trade price  curve price  trade type  trade size  and time
delay to be incorporated in a concise fashion  the goal of
our work was to implement a time series forecasting method

fithat would allow us to create a set of time series predictions
that could be used to either augment our feature set or even
replace all of the historical features in a concise fashion 
one issue that is immediately apparent with the dataset
utilized here is that each bond contains time series data for
the last ten instances of the five variables mentioned above 
ten points is a very small number for normal time series predictions  to the point that many standard time series analysis
methods cannot estimate model parameters with any degree
of statistical certainty from such a small dataset  knowing
this  we proceeded to investigate several potential options for
time series analysis in order to find even a very simplistic
model that could at least output a reasonable prediction in
the majority of cases with the goal of using it for feature set
augmentation as discussed above  for the sake of brevity 
these complex time series models will be concisely described
below  with provided references detailing full model specification 
cointegration models

another  slightly simpler method for approaching time series
forecasting is to predict future values of a variable based on
its historical behavior  while there exist a wide variety of
methods for accomplishing this task  arma models are quite
common due to their simplicity and intuitiveness  in particular  the arma model incorporates two separate modes of
capturing time series behavior  the first can be summarized
by considering that recent values of a time series variable can
be very good predictors of the present value  this is captured
by the ar  auto regressive  parameter in the model  the
second line of thought  the moving average  ma  portion of
the model  captures the fact that a large shock at a previous
period would not only affect that period  but periods in the
near future as well      the combination of these two models
can be written as the following in terms of a time series yt  
ar coefficients i   ma coefficients  j   and deviations from
the ar model et  
p

one method of time series prediction involves a procedure
known as cointegration analysis  briefly  a set of univariate
time series can be considered cointegrated if some linear combination of these series and their lags is statistically stationary
in time  a linear combination of time series  and lags  that is
cointegrated is known as a cointegration relationship  if a set
of cointegration relationships exists amongst a group of time
series  these relationships can be used to forecast values for a
subset of the time series at later points using historical data 
the canonical statistical test for determining whether or not
a set of time series data contains a cointegration relationship
is the engle granger test     
the fundamental assumption of the engle granger test is
that a time stationary linear combination of two time series 
yt and zt   is required for cointegration  such that 
yt  zt   ut  

   

with ut stationary  if ut were known a priori  one could use an
established statistical method such as the dickey fuller test
to evaluate stationarity      however  in this case  we estimate
ut using ordinary least squares and analyze the stationarity
of the estimated series  a second iteration of this procedure
is performed on the first differences of the each time series
with the lagged residuals included  the combined output
of the stationarity tests is presented as a single test statistic
that can be used to evaluate the existence of a cointegration
relationship 
we performed the engle granger  e g  test in matlab on
the time series data from each of the nearly one million bonds
in our dataset  given that we only have ten points for each
quantity for each bond  it is not unexpected that the majority
of the e g tests reported a p value substantially above any
reasonable significance boundary  in other words  this result
means that it is generally not possible to reject the null hypothesis of there being no cointegration relationship amongst
the different time series variables describing each bond  this
makes it difficult to specify a cointegration based model such
as a vector auto regressive  var  or vector error correction
 vec  model to forecast bond price based on cointegration
relationships amongst previous time series quantities  notably  however  there was a nontrivial proportion of the data
 around       for which the e g test did allow for confident
rejection of the null  implying the existence of at least one
cointegration relationship  this result suggests that with
access to additional historical data  it might well be possible
to form a viable prediction for each bond that would allow
for reduction of said historical data to a single time series
prediction that could then be input into the machine learning
model discussed here     
  referred

auto regressive moving average  arma  models

to as    feature wls  in figure  

yt  

 i yti   et  

i   

q

  j et  j  

   

j   

where p and q are the specification parameters of an
arma p  q  model  specification of p and q can be reliably achieved using the box jenkins methodology  a welldocumented procedure using easily computed autocorrelation functions      the form of several visualized autocorrelation and partial correlation functions implies that an
arma      model would potentially be appropriate for this
data  given the results of the cointegration analysis and the
fact that the majority of observations allow statistically viable
fitting to an arma      model  we pursue this approach in
our time series modeling  in terms of results  the arma
model was utilized in the following fashion  one of the data
series provided  but not originally used in the regression
is a categorical variable that is a relatively non informative
bond id number  this variable identifies bonds that are
of the same class  issuer  period  etc    because fitting an
arma      model to each observation independently would
take substantial computational resources and thus might
not be the most helpful from a predictive standpoint  we
used knowledge gained from our pca analysis to define
a method for efficiently integrating a time series analysis
into our models via this bond id number  specifically  our
previous analyses indicated that historical trade and curve
prices were responsible for most of the explanatory power of
the model  we therefore aimed to use time series estimates
of the difference between the trade price and the curve price
to extract additional information that would improve our
results  our algorithm is as follows 
 i  estimate an arma      model for    samples of each
bond type in the training set using a variable defined as the
difference in the trade and curve prices at each time point
 ii  average arma parameters to create an average ts model
for the difference in trade and curve price for that bond type
 iii  forecast one period forward from the historical data 
which gives a prediction of the difference between the trade
and curve price for each bond type for the prediction period
 iv  use this forecast variable as a new feature in glm models
the ultimate goal of this procedure was to integrate given
data on the type of the bond in a manner more consistent
with fundamental economic behavior as opposed to a simple
categorical label  theoretically  this series should have greater
explanatory power than the bond identification number series
alone because it incorporates time series data  we illustrate
this in practice by performing a reduced feature set glm 
using only data from the current time period  the curve and
trade price from the first historical period  and either the

fipotentially be appropriate for this data  given the results of the
for addressing non linear problems  we therefore experiment with
cointegration analysis and the fact that the majority of observations
two layer  one hidden layer  one output layer  neural networks
allow fitting to an arma      model  we pursue this approach in
trained with the levenberg marquardt optimization algorithm 
bond
identification
number or the arma      variable  as
exploiting
implementation
of these
algorithms
our
time series
modeling 
the trainingparallel
and testing
errors along with
the execution
time for
from
shown in figure    including the bond identification number
model tuning would greatly enhance our ability to make the
in
terms
of
results 
the
arma
model
was
utilized
in
the
folthis
exercise
is
shown
in
the
summary
table 
two layer
nns
changes training and test error only by       and       
most accurate predictions possible  all of these routes could
lowing
fashion for
one
the data series provided 
butinclusion
not used
perform
best on our to
dataset 
reducing
test error
to intend
   in only
respectively 
thisofreduced feature
case while
yield
improvements
our current
results 
and we
to
inofthe
is a variable
categorical
variable
that is
a relatively
  hours  the
reduction
with
sizemonths 
beyond    neurons
theregression
arma     
instead
lowers
training
and nontest
investigate
several
of these
in network
the coming
on  informative
stanford university 
cs     machine learning  
error by   bond
 andid
   
  respectively 
inclusion
of the
arma
number 
this variable
identifies
bonds
that
is quite gradual  we also
experiment with many hidden layers
variable
in
the
full
wls
model
similarly
yields
a
respective
are of the same class  issuer  period  etc    because fitting an
with varying number of neurons  but all are outperformed by the
ix  summary
of algorithm
performance
reduction model
in training
and
test errorindependently
by      and    
  the
summary
of performance
metric
arma     
to each
observation
would
take
  layer viii 
network 
methodology 
crosshvalida on and feature selec on 
fact that inclusion of the
time series variable enhances the
substantial
computational
resources
thus might
not errors
be the
the performances
methods
are are
summarized
below 
performance
of thebond price prediction  
glm
in bothand
training
and test
the
performancesofofthe
thevarious
various
methods
summarized
crosshvalida on approach   
arth  most
helpful
from
predictive
standpoint 
used knowledge
suggests
that
thisavariable
does
add new we
information
to the
graphically
figure
  and in tabular
formthe
in code
figure
comthe computeintime
is evaluated
by running
on  one
node
ires  gained
  create   instances of weightlbalanced training and test sets 
our of
pca
analysis
to define
a method
for efficiently
modelfrom
instead
simply
causing
overfitting 
importantly 
if
putation
time is corn
evaluated
using one node on the corn cluster 
on the stanford
cluster 
supervised 
ensemble 
hybrid time 
neural 
h  is  integrating
  run ml algorithms on each of these   training and test sets 
the time series
model for
each bond
is precomputed 
it is
a time series
analysis
into type
our models
via this bond
learning  to methods 
series 
networks  nn  
a simple
use our
thisprevious
feature to
provideindicated
supplementary
  report weps from each of the   independent runs 
id
number matter
specifically 
analyses
that hismethods
weps
weps
training
  glms about
  the
bootstrapping 
  arma in a simple
  twollayer 
information
bond price evolution
glm 
ning  torical
  final weps is the average of these   values 
trade and curve prices were responsible for most of the
train
test
time
 
pcr 
 
random forests 
 
var vec 
 
mul layer 
iate  explanatory power of the model  we aimed to use time series estipca feature selec on for glms    
 simple  
generalized linear models
svr 
  ls l boos ng 
mark  mates 
vii 
neural
of the difference
between
thenetworks
bond price and this variable
 
only    out of    pca features eec vely linearly independent 
backpropaga on  
ols
      
      
   seconds
ore  to extract additional information that would improve our results 
 
feature containing exclusively all previous prices and all previous 
wls
      
      
   seconds
neural
networks
are very well suited for function fithat  our
algorithm
is as  nn 
follows 
results 
intermediate curvelpredicted prices holds most explanatory power 
ting
problems 
a
neural
network
with
enough
neurons
can
gamma
      
      
   seconds
me  
best weps
best weps
training
fit any methods
data with arbitrary
accuracy 
they best
are model
particularly
generalized
linear
models
with
pca
 pcr 
time series  ts  for feature genera on   
train
test
time
   estimate
an addressing
arma     
model
for   problems 
samples of we
eachtherebond
well
suited for
non linear
   feature
wls
      
      
   seconds
generalized
linear
models
      
      
wls
  
secs
 
dickeylfuller and englelgranger tests used to assess sta onarity and
type
in   the
set using one
a variable
defined
the
differexperiment
with two layer
hidden
layer  as
one
output
report fore
december
    training
generalized linear models with
  feature
wls
      
      
  seconds
colintegra on of  me series  respec vely 
layer 
neural
networks
trained
with
the
levenberg marquardt
      
      
wls
   secs
ence in the trade price
and curve
price at   feature
time point
pca  pcr 
hybrid time series methods
optimization
algorithm
and
simple
back propagation 
the
 
specify an autolregressive moving average model  arma      
ous   hybrid
time series methods
      
      
wls w arma     
   secs
training
and testing
errors
along to
with
the an
execution
time
from
arma     
model
      
      
   hours
g function
ensemble
methods 
random
  foraverage
arma
parameters
create
average
ts model
 
fit arma to  me series describing dierence in curve and trade price
regression
trees  rt 
      
      
all predictors
   hours
thisare
exercise
are
shown
in figure
   two layer
nns
perform
orests  rf 
used
feature
ranking 
rf
willcurve
wls
w arma
      
      
   seconds
random
forests
 ensemble
for
the for
difference
in trade
and
price
for
that
bond
type
 
average model es mated from    historical samples per bond class 
      
    regression
trees
quite with
well
dataset 
reducing
test error
to     in
only   hours
elect features
randomly
with
replacement
and      
method 
rton our
  feature
wls
      
      
  seconds
   
used to predict dierence between current trade and curve prices 
hours 
weps
reduction
with
network
size
beyond
  
neurons
ls boost
 ensemble
method  
roup every
subset
in a separate
subspace
 called
      
      
    weak learners
   hours
  
forecast
one
period
forward
from
the
historical
data 
which
  feature
wls
w bond
id
  
    
      
  seconds
weak
learner 
rt
 
arma forecast for each bond class used as addi onal glm feature 
is quite gradual 
andom subspace  
we use a scoring function with
gives a feedprediction
of       
theappears
difference
trade and
neural networks
forward 
       between
two layerthe
   neurons
   hours
  feature
wls w arma
              
  seconds
 
no ceable decrease in weps compared to using bond id series 
he following
methodology 
if feature
x 
or 
a  of the curve
price
fortraining
each
bond
n    
trees test
then 
score
it  otherwise 
we
error
vs 
error type
test error vs  execution time
regression trees  rt 
feature selec on for ensemble methods   
do not consider ranking the feature because we  
all predictors
      
      
    hours
 
do not have
about vs 
its
   sufficient
use thisinformation
forecast
variable
asperatraining
new feature
in glm models
test error
error
 
random forests  rf  are used for feature ranking 
 
predictors
per
node
      
      
    hours
    then assign the performance score   
ormance 
we
 
  scoring func on scores each feature if it appears on     of trees 
  
predictors
per
node
      
      
    hours
f every tree in
x  appears to x  and aver   
  which
the
ultimate
goal of this  procedure was to integrate given data
  recursively assign average weps of every tree in which a feature 
ge the score  for example if performance tree  
   predictors per node
      
      
    hours
s  training
error
on  the
of the bond in  a     
manner
    type
appears to that feature as its score  
    weps
      performance tree  
per     more consistent with fun   predictors per node
      
      
    hours
damental
economic
as opposed
to a simple categorical
ormance tree  
       
then  behavior
the importance
of  
 
   
other
methods
tried
with
rt
also
overfit
the
data
conclusions 
eature x 
                    
       
our search
label 
theoretically   this
series
should have greater explanatory
   
   
random
forests
 ensemble
method 
with
rt
method
is recursive 
for
example 
lets
say
in
the
hods  power
than the bond identification number series
alone because
generalized
linear models   glm models generally perform well with low cost 
principal
component
regression
rst round we  drop the worst      second
too and  
   regression trees
      
      
    hours
 data 
it incorporates
time series
we
illustrate
this
in
practice
  
  
  
 
   
 
   
 
   
 
regression
trees by
 
augmen ng feature set with ts models improves performance 
o on until we get
the
desired
number
of features 
execution
time  hours
   
regression
trees
      
      
    hours
training
error 
 
performing aexperimented
reduced feature set glm using random
only data
from the
forests
his number has been
 
ensemble methods do not seem to substan ally improve results 
ls boost
generalized linearwith 
models
   
regression
trees
      
      
    hours
current time
period 
the    
curve
and trade priceneural
from networks
the first hisprincipal
component
regression
 
neural networks give most accurate results without overljng  
   
regression
trees
      
      
    hours
arma
regression
trees
torical
period 
and either
the bond identification
number or the
y  models
from
supervised
learning
glm w  arma
  neural networks and glms give best speedhaccuracy proles 
random
forests
 
ls boost
 ensemble
method  
weak
learner 
rt
arma     
variable  as  shown in table    including the bond
ls boost
neural
networks
   
weak
learners
      
      

  hours
hat all prediction
errors
are
calculated
according
to
identification
number
changes
training
and
test
onlytime 
bynumber
    
class average
arma
predictions
figure
   test
error
versus
training
error
anderror
training
arma
wls
test error
vs 
pca feature
future work 
   
oss validation
algorithm 
glm
w 
arma
 
   
weak
learners
      
      

  
hours
and        respectively     
for this reduced feature case while inclu   
 
improve  me series models  model type and specica on  
 
   
weak
learners
      
      

  
hours
sion of the arma      variable instead lowers training and test
alized linear models
   
 
explore deep learning  encouraged by performance of nns  
viii 
conclusions
and
future
work
 
   
weak
learners
      
      

  
hours
error by     and       respectively  inclusion of the arma variable
 
 
exploit parallel implementa on of algorithms for model tuning 
 
 
neural networks  feed forward 
in the
wls model
similarly
yields a respective reduction in
l models
fromfull
supervised
learning
were imple   
  
we
can
makegeneralized
several definitive
conclusions
regarding the
 
if possible  add more data to our  me series for each bond  
 
d  first training
an unweighted
linear
model
two layer
 
neurons
      
      
   hours
and
test
error
by
   
and
   
 
the
fact
that
inclusion
of
   
relative
performance
of
the and
tested
models  in predicting bond
training
error 
 
mplemented
using
various
link functions
the
   
the
variable
enhances
the
performance
of the glm in
two layer    neurons
      
      
   hours
price 
 i 
glm
models
perform
well
with
low
computational
  time series
references 
ature   
set in order
to investigate the underlying  
   
both
training
and
test
errors
suggests
that
this
variable
does
in
fact
two layer
  
neurons
      
      
    hours
cost
 order
of
seconds  
 ii 
feature
set
augmentation
with
ution
of the data 
financial data often has an  
ng
error 
    while
  
has e  t   et al  elements of sta s cal learning  springer  
ts new
models
improves
 iii 
ensemble
methods
do overnot
add
information
to results 
the
model
instead
of simply
causing
two layer    neurons
      
      
    hours
ying normal
variation 
it
is important
to ensure
that
 
 require much more compu   hull  j c   op ons  futures and other deriva ves  pren celhall  
substantially
improve
results 
and
  
 
  the
  report
  
 
   for  each
   bond
  
  
  
fitting 
importantly 
if
time
series
model
type
is
sumption
is valid
before
proceeding 
we
the
figure    summary of results 
arma trade curve dierence
forecast   networks
number
of pca
tational
 iv 
neural
give
veryfeatures
   shumway  r h   et al  time series analysis  springer  
of ordinary
least investment 
squares
regression
using to use this
precomputed 
it is ols 
a very
easy
matter
feature
toaccurate
provide
results
without
overfitting
in
reasonable
amounts
of
time
 ornctionssupplementary
for the normal  gamma 
poisson  and
binoinformation
about
the bond price evolution in a
references
derevaluating
of hours  
nnsand
and
glms
istributions 
the v 
training
test
errors give best results in terms
table    summary of results
simple
glm
model 
of
combined
speed
and
accuracy
 e g 
figure
   
se different cases illustrates that normal assumption
 

 

 

test error   

number of bond classes

test error   

test error   

test error   

for predic on of bond prices using machine learning 

exist
several
fruitful
in which to take future
  referred
s to bestthere
characterize
data  further 
normal
to the
as
   feature
wls the
indirections
table  
work 
first 
obtaining
a dataset
ution of the
residuals
reinforces
this conclusion 
to with longer time histories
would
for
specification of more
ve on these
resultsallow
in terms
of statistically
minimizing oursignificant
error
detailed
time series
for improvement of feature aug   
  weighted
least squares
 wls  models
was performed
mentation 
investigating
thegovern
performance of different classes
the evaluation
weights
to appropriately
of treated
time series
models
as machine
points are
with highest
importance
in thelearning feature generation
mechanisms
would
be useful 
second  the success of neural
sion  wls
gives noticeable
improvement
over ols 

    benchmark solutions  benchmark bond trade price
challenge  www kaggle com       

cation regression
of multilayer networks and deep learning methods to
pal component
this
problem may
bond price predictions  finally 
xt proceed
to implementing
pcryield
using better
the reduced
e set  we reduce the size of the feature set to   

    shumway  r h   time series anaysis  springer   rd ed 
     

networks on this dataset implies that investigating the appli 

    hull  j  c   options  futures and other derivatives 
prentice hall   th ed       
    hastie  t   tibshirani  r   friedman  j  h   the elements
of statistical learning  springer   nd ed       

fi