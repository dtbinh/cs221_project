applyingdeeplearningtoderiveinsightsaboutnoncodingregionsofthe
genome
avantishrikumar annasaplitski sofialunafrankfischer avanti annasaps luna   
cs   finalproject december    


motivation
mostcellsinthehumanbodyhaveessentiallythesamednasequence butthecellsinhearttissuebehave
verydifferentlyfromthecellsinlungtissue thedifferenceisepigenetics modificationsofthedna or
proteinsassociatedwithdna thatcontrolwhichgenesareturnedonandwhichareturnedoff one
importantexampleofsuchmodificationsistheactivationofregionsofthednacalledenhancers 
enhancersactiveinagivencelltypeareknowntoturnontargetgenes butthegenomiccodeunderlying
theirbehaviorislargelyunknown currentmethodstoidentifyenhancersrelyprimarilyonchemicalassays
thatdetectthemodificationsassociatedwithenhanceractivity however thelinkbetweentheunderlying
dnasequencesatanenhanceranditscelltypespecificactivityispoorlyunderstood previousworkhas
focusedontrainingsvmsusingstringkernelstodistinguishbetweenenhancersandrandomgenomic
segments butsvmsdonotscaleaswelltoterabytesofdataandrequirethatthefeaturesandkernelsbe
knownbeforehand deeplearning ontheotherhand allowsustoperformunsupervisedlearningthrough
pretrainingtoautomaticallydiscoverrelevantfeatures andtheweightslearnedforthesefeaturesmight
giveinterestinginsightsintothegenomiccodeatenhancers 

data
thepositivedatasetconsistsof nonpublic encodeenhancersidentifiedbyexperimentalassays we
usedthecentral   bpofallenhancersequencesthatwere     bplong whichretained      
sequences therationaleforrestrictingtothoseregions     bpwasthatlargerregionsmightactually
consistoftwosmallerenhancerregionsincloseproximity andwouldthuslookqualitativelydifferent there
weretwonegativesetsconsideredatdifferentpointsinourstudy thefirstwasrandompermutationsofthe
dnasequenceofthepositiveset andthesecondwasrandomsegmentsofthegenome inallcases the
hg  genome human wasused whilemoredatawasavailable preliminaryexperiments notreported
hereduetospace showedthatforourneuralnet doublingtheinputdatasizedidnotlowertheerrorrate
appreciably 

featuresandpreprocessing
torepresentrawsequences weusedonehotencoding whichrepresentseachbaseinthesequencewith
fourbinarybits exactlyonebitisset dependingonwhetherthebaseisana c g ort inadditionto
usingonehotencoding weusedaspectrumkerneltotransformthesequences thespectrumkernelis
builtonamappingwhich givenasubsequencelengthk returnstheoccurrencecountsofeachlengthk
subsequence orkmer inthestring weusedthekebabskernelpackageimplementationofthespectrum
kernel 

models
weexploredthreemaincategoriesofmodels thefirstwasmultinomialnaivebayestrainedon
subsequencesoflength   thesecondwasasupportvectormachinethatusedaspectrumkernelwith
kmerlengths  andcost    inordertoimplementthesupportvectormachine wereliedonthe
kernlabslibrary whichallowedustouseacustomkernel thethirdwasaneuralnetworkwitha class
softmaxoutput trainedwithstochasticgradientdescent batchsize   andearlystopping thedataset
wassplitas    training      validation      testing  variousarchitectureswereexploredfortheneural
network whichwewillexplainonacasebycasebasisinthesubsequentsections 

fi
resultsanddiscussion

shuffledsequencesasbackground
itwasknownintheliteraturethatenhancershaveahigherincidenceofgandcbases called
gccontent intheirdnasequencecomparedtorandomregionsofthegenome indeed running
multinomialnaivebayestrainedmerelyontheincidenceofa c g tbasesachieves   accuracy we
thereforehadthehypothesisthatbyusingshuffledversionsofthepositivesetsequencesasthe
backgroundset wewouldkeeptheproportionofg cbasesconstantandwouldtherebyforcetheneuralnet
topickoutfeaturesthatwentbeyondg ccontent 
usingtheonehotencodingrepresentationoftherawsequence wetrainedaneuralnetwithone
hiddenlayerandsigmoidalactivationscontaining           and    nodes attainingtest
misclassificationratesof           and   respectivelywithearlystopping whilewewere
encouragedbythisresult whenwescannedthesequenceofchromosome  togenerateadistributionofa
probabilityofagivenregionbeinganenhancerthedistributionweproducedfornonenhancerregions
lookedidenticaltothedistributiongeneratedforknownenhancerregions figure   contrarytoour
expectations theneuralnethadidentifiedfeaturescommontoallgenomicregions ratherthanfeatures
uniquetoenhancers ourinitialhypothesis fromlookingatthefeaturesdetectedbythemostinformative
nodesintheneuralnet wasthattheneuralnetwasidentifyingwide globalpatterns potentiallyrelatedtoa
biologicalphenomenonknownasnucleosomepositioning however asasanitycheck wetraineda
multinomialnaivebayesmodelonallsubsequencesoflength inthepositiveset usingshuffled
sequencesasthenegativeset  andthismodelwasabletoattaina    misclassificationrate inparticular 
the mercg whichishasbiologicalsignificanceascpgislands was   xlesslikelytooccurinenhancer
sequencesrelativetoshuffledversionsofthesequence itthereforeseemslikelythatmostofthe
discriminativepoweroftheinformativenodesinourneuralnetwasduetodetectingthisbiasinthe
occurrenceofcgdimer butmoreinvestigationwouldbenecessarytodetermineifthisistrue a
visualizationofthemostinformativenodeinthe   nodemodelisincluded highactivationofthisnode
correspondedtoclassifyingtheregionasnotanenhancer  figure   

neuralnetwithrandomgenomicsequences
oncewerealizedthattrainingagainstshuffledenhancerregionsdidnotpickoutthefeatureswe
wereinterestedin weswitchedbacktothenegativesetofrandomgenomicsequences wehopedthatthe
netswouldbeabletopickoutthefeaturesthatdistinguishenhancerandnonenhancerregions wetried
thefollowingarchitectures  hiddenlayerwithsigmoidalactivationfunctionsand       nodes  hidden
layerswithsigmoidalactivationsand   nodeseach andaconvolutionallayerwithrectifiedlinear
activationfunctionsand  channelswithkernelsspanningconsecutive bsegmentsfollowedbyafully
connectedlayerwith   hiddennodes unfortunately allofthesearchitecturesproducederrorsinthe
     rangewhichbarelyoutperformednaivebayestrainedongccontent weexaminedwhichfeatures
theindividualnodeswerepickingout andwediscoveredthatthehighlyweightednodeswereagainmostly
pickingoutgccontent 
next wetriedpretrainingwithdenoisingautoencoderstoseeifthatwouldgetthenettopickout
moreinformativefeatures wepretrainedusingandencodinglayerwith   hiddennodes usedsigmoidal
activationsfortheencoderanddecoder usedthemeanbinarycrossentropyforthereconstructionerror 
andsetthenoiselevelto  althoughpretrainingdidappeartoidentifyinterestingpatternsinatrichness
 figure   addingapretrainedlayerfollowedbyahiddenlayerwith   nodesandsigmoidalactivations
didntimprovetheperformancebeyondthe      range wewerestillintriguedbythenodesidentified
infigure andhypothesizedthattheywereassociatedwithabiologicalphenomenoncallednucleosome
positioningifso theymayhavebeenusefulindetectingananchorpointwithinenhancersequencesthat

ficouldhavebeenusedtorecenterthesequences whichwouldpotentiallyreducetheneedfordeveloping
modelsthancantoleratetranslationalinvariance totestoutthishypothesis weisolatedtheportionofthe
weightvectorsofinformativenodesthatappearedtobeinvolvedinnucleosomepositioningtomakean
adhocnucleosomepositioningdetector exampleinfigure  andusedittoscantheenhancerregionsif
ourdetectorwasindeedidentifyingnucleosomepositioning wewouldexpectonlyonestrongpeakper
enhancerregions asnucleosomepositioningdoesnotoccurmorethanonceper   bp unfortunately 
eachenhancerregionhad onaverage multiplepeaksforournucleosomedetector counteringour
hypothesissowedidnotpursuethisideafurther datanotshownduetothepagelimit  

benchmarkingwithsupportvectormachineandmultinomialnaivebayes
inordertobenchmarkourneuralnet weranbothmultinomialnaivebayes usingkmersas
features andasupportvectormachine withaspectrumkernel  fornaivebayes thetrainingsetinvolved
   ksequences balancedbetweenenhancersandrandomgenomicregions andlookedkmerlengths 
through  thebestperformancewasinkmerlengths through  whichhadmisclassificationratesof
about    withkmerlength  naivebayeshadanerrorrateof    andwithkmerlength ithadan
errorrateof    
wewerenotabletotestthesvmonthesamedatasetsizeasthenaivebayesclassifierbecause
theruntimeofsvmsscalesquadraticallyinthetrainingsetsize ourparticularimplementation whichused
rskebabspackage scaledparticularlybadly soourtrainingsetsizewaslimited inaddition usingthe
spectrumkernelwithhighkmerlengthswasexpensive soweonlyuseduptoakmerlengthof  inorder
togeneratealearningcurve weranthesvmwithtrainingsizesof k  k   k   k and  ksequences 
thetestingerrorwasabout   with mersand   for mersforbothofthebiggestdatasets the
learningcurvesforkmerlengths and areincluded figures and   wenotethatwemayhavebeen
underfittingourdata giventhatthetestingandtrainingerrorsconvergedcompletelyforkmerlength  
althoughthetrainingsetwasmuchsmaller thesvmstilloutperformednaivebayes bothtrainedonkmers
andtrainedongccontent andtheneuralnet withonehotencodedrawsequencesasinput  

neuralnetwithspectrumkerneltransformation
givenoursuccesswiththespectrumkernelsvm wedecidedtocreateaneuralnetwhoseinput
wastheresultofmappingtherawsequencetohigherdimensionalspaceusedbythespectrumkernel that
is wegavetheneuralnetthecountsofkmersoftherawsequenceinsteadoftherawsequence the
neuralnetarchitectureagaininvolvedonehiddenlayerwith   nodesandsigmoidalactivations the
neuralnetperformedbetterthanthesvm gettingatestingerrorof    itshouldbenotedthattheneural
netwastrainedonalargerdatasetwecouldnotdotheequivalentcomparisonofhowthesvmwould
performonthisdatasetbecausethesvmwouldnotscaletothatlevel however thelearningcurveinfigure
 ofthesvmsuggeststhatitwouldnotperformsubstantiallybetteronthelargerdataset itshouldalsobe
notedthattheneuralnetwastrainedonthesamesmalldatasetofthesvmwith mers thetestingerror
was    worsethatthatattainedbythesvm butitisalsotruethatneuralnetsaregenerallymosteffective
whentrainedonlargerdatasets 
wedidntinvestigatewhichkmersweremosthelpfulindistinguishingenhancerandnonenhancer
regionsbecausethisinformationisnotbiologicallyinteresting instead thiswasmostlyanexerciseto
demonstratethatifwecouldinduceourneuralnettodetectlocalfeaturessuchaskmers wewouldlikely
attainbetterperformance thereasonwewishedtoderivethekmerrepresentationdirectlyfromraw
sequence ratherthanusekmercounts isthatthekmercountrepresentationdiscardsalotofpositional
informationandinherentlypreventsthedetectionofalotofhigherorderpositionalpatternsthatwesuspect
existatenhancers convolutionallayersofferonewayofdetectinglocalfeatures butourpreviousattempts
attrainingconvolutionallayersdidnotproducefruitfulresultsoutofthebox wethusdecidedtoinitializethe
kernelsoftheconvolutionallayerinawaythatwouldpickoutthemostinformative mersidentified

fiaccordingtonaivebayes inthehopethatthismaycausethestochasticgradientdescenttofindbetterlocal
minima wechose mersratherthan merstomakethenumberofchannelsintheconvolutionallayer
moretractable  sofarwehavenotbeenabletoobtaintheperformancewiththekmercount
representation butweareactivelytryingtodebugwhy 

conclusion
althoughourfirstattemptswiththeneuralnetwerenotparticularlysuccessful wewereabletoimproveits
performanceusingunderstandinggleanedfromrunninganaivebayesclassifierandansvm oncewe
beganusingthekmertransformationaspreprocessing theneuralnetbegantoperformbetterthaneither
thenaivebayesorthesvm especiallygiventhatitcanhandlemuchlargertrainingsetsthanansvm 
however thereisstillasignificantdrawbackinusingthekmertransformationwithaneuralnetasopposed
toansvm becauseitrequiresdoingthetransformationexplicitlyinsteadusingakerneltransformation 

futuredirections
theimmediateorderofbusinesswouldbetofindouthowtoderivetheequivalentinformationinthe
kmercountrepresentationbyusingtheonehotrepresentationwithconvolutionallayers thethree
hypothesesforwhythisisnotyetworkingare    thatthenumberofchannelsweareinitializingthe
convolutionallayerwith wehavetriedvariousnumbers isnotenoughtocapturealltherelevant
information    thattheinitializationweareusingfortheweightsisnotactuallyeffectiveatpickingout
kmersinawaythatisusefultosubsequentlayersand   parameterexplosionfromusingalargenumber
ofchannelsiscreatinglotsoflocalminimaandourstochasticgradientdescentalgorithmisgettingtrapped 
ourpreliminaryexplorationof   suggeststhatitisnotthecase becausewhenwelimitthekmercount
representationtothenumberoftopkmersidentifiedbynaivebayes ournetworkhasbetterperformance 
ourpreliminaryexplorationof   ispromising buttheintricaciesofourexplorationdonotfitinthisreport 
wehavenotdoneanyexplorationof   yet butwewouldplantoinvestigateitbyaddingpoolinglayers
aftertheconvolutionallayer tohopefullyreducethenumberofparameters 
anotherideawouldbetofixtheissuewithourinitialnegativeset whichwasshuffledversionsof
thepositivesequence bykeepingboththeproportionofgsandcsaswellastheincidenceofthecg
dimerconstantbetweenthepositiveandthenegativeset thismayresultinanegativesetthathasthe
samegccontentbutisalsonotsoeasilydistinguishedbynaivebayes 

references
a kartzoglou a smola k hornik a zeileis       kernlabans packageforkernelmethods r 
journalofstatisticalsoftware           http   www jstatsoft org v   i    
i goodfellow d wardefarley p lamblin v dumoulin m mirza r pascanu j bergstra f bastien y 
bengio pylearn  amachinelearningresearchlibrary arxivpreprintarxiv           
 http   deeplearning net software pylearn   
j palme u bodenhofer       anrpackageforkernelbasedanalysisofbiologicalsequences r
packageversion       http   www bioinf jku at software kebabs  
m handi d lee m mohammednoori m beer       enhancedregulatorysequencepredictionusing
gappedkmerfeatures ploscomputationalbiology      e        
 http   www ncbi nlm nih gov pmc articles pmc         

explanationofweightprofilesatinformativenodes
weidentifyinformativenodesbylookingatwhichnodeshadthelargestoutgoingweightstothe class
softmaxlayer forthesenodes wenormalizetheincomingweightvectortohavemagnitude  sincewe
haveaonehotencoding every elementsintheweightvectorcorrespondtothefourpossiblebases
 a c g t atagivenpointintherawsequence wethusplottheweightscorrespondingtodifferentbasesin
differentcolors andstackthemsothatthesmallestweightisinthefront 


fifigures
figure 

figure 




figure 

figure 





avisualizationoftheweightsplacedonvariousbasepairs
alongasequence anexplanationofthisvisualizationisat
theendofthetext notethethehighweighsgiventoregions
withatrichflanks 

weightprofilefornodeusedasnucleosomepositioning
detector anexplanationofthisvisualizationisattheendof
thetext 

figure 

figure 


training blue vs testing red errorratesforthesvmwith
spectrumkernel kmerlength  




training blue vs testing red errorratesforthesvmwith
spectrumkernel kmerlength  

fi