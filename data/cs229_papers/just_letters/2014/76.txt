      

classification of cardiac arrhythmias patients
azar  
fatema  
algharbi   
batool  
haider  
azarfazel   
fazel 
fatema
algharbi 
batool
haider
cs     
final  
project  
report   
fall  
      
cs   
final
project
report 
fall
    

abstract 
cardiac arrhythmias are any of a group of conditions in which the electrical activity
of the heart is irregular or is faster or slower than normal  it is the leading cause of death
for both men and women in the world         in this project  we aim to classify heart
arrhythmias patients among    different classes based on ecg  electrocardiography 
data  after applying rigorous data pre processing
and feature selection techniques  we
  
used   different machine learning algorithms  svm  logistic regression  knn  random
forest and decision trees  our best accuracy was      obtained via svm  we also used
some of these methods to come up with the most important attributes that determined the
class of arrhythmia  this work can be of immense importance to researchers who are
exploring various techniques to capture the key pre informers of a potential cardiac
disease  well before it is too late 

data
the data has been taken from a well maintained
ecg
 electrocardiography 
database
 https   archive ics uci edu ml datasets arrhythmia   it
contains     attributes  ecg  patient related
variables  and     instances  the variable class is
our target variable  class    refers to  normal  ecg 
classes    to    refer to different classes of
arrhythmia and class    refers to the rest of
unclassified classes  figure     shows the distribution
of different classes in our database  as can be seen 
almost half of the instances are classified to class   

 

  

  

 

 

 

  

 

  

  

  

  

   

while there are few instances in other classes  thus  in
the database  we do not have much evidence for some
of the classes like class    or    
  
  
  
  
  
  
  
  
  
                             
  
classes
  
number of instances

introduction
heart diseases kill more than         people
annually  in the united states  someone has a heart
attack every    seconds      in this paper  we present
our methodology and the outcomes of developing a
machine learning system that is capable of classifying
a patient into    different cardiac arrhythmic
categories  this work has great potential to serve the
medicine industry  with the advancement in medical
technology  database will only continue to grow  the
evolution of smart body chips capable of sending real
time patients information are rigorously being
researched upon  algorithms such as these can be
have ground breaking impact regarding helping
researchers target the keys features that cause cardiac
arrhythmia and assist them in classifying patients in
right categories  so as to be able to take measures in
the right direction 

fig     distribution of instances in various classes

analysis   data pre processing
following are the main steps took to process the
data 
   normalization
since many algorithms require normalized dataset 
we normalized all the feature values except gender
    
using z score         normalization and scaled
them between   and    we then proceeded on towards
analyzing the data 
   feature selection 
we first ran various algorithms on the data set with
all the features  the performance of most was not

fi  
  
  
  
  
  
  
 
 

fig      variable importance for cardiac arrhythmias patients
using decision trees

we then applied principle components analysis
 pca  to these features to reduce the dimensionality
of the dataset while retaining most of the variation in
the data set  figure    shows the results of this
analysis 
 

 

 

 

 

 

 

 

 

  

  

  

  

   

v 

vd 
i i t
t 
di  wwaa
 t vev
 w e
av
e

e
av
 w

e
av
 w

 t

  
 r
v 

avf q wave

a
v  qrs
e avea
 wav  w rs
not ot  q e
v
v  rv  rnonsv 
cti
wa
e
 
fl
 
 de
 q
intr
v 
v  

wa
ve

   

 q

v 

heart rate

iii  feature importance score via decision trees 
we took two approaches  the first was to cluster the
classes into class   and not class    we then
found the features that best described the responses
for these two large classes  in the second approach 
we used all the classes in their original labeling in the
dataset and found out the most important features for
the response via decision tree  figure     shows the
results of decision tree  as seen in this figure  based
on the features importance score  only    features out
of     features were considered as the most important
features 

variable importance

i  invariant features
firstly  we removed some the categorical features
that were     of time indicating either all  s or all
 s  these seemed not to help a lot in decision making
as they were pointing to the same category most of
the time  as expected  their removal  did not disturb
the accuracy but reduced the feature space and
simplified the data base   
  
ii  mutual information 
next  we found tuples of features that were
correlated at least with      correlations  this is to
say the set of features giving approximately the same
information  and so we included only one feature
from each tuple  this reduced the number of features
to     

the heart rate is the most important variable followed
by v  t wave  v  t wave and v  q wave 

pc         explained var  

      
satisfactory  with the instances to feature ratio of
only      we decided to improve the performance of
algorithms by reducing the number of features 

av
r

 t
 w
av
e

   

fig     using pca
to visualize
by decision
   
    the features selected
   
   
pc         explained var  
tree

models 
we applied several models and algorithms to our
dataset  accounting for their merits and demerits
based on literature review  these are as follows 

fig      decision tree for cardiac arrhythmias patients

figure     shows the most important    variables for
the response using the decision trees  we noticed that

   knn  k nearest neighbors 
we used knn because it is simple to implement  
very straight forward  here  an object is classified by
a majority vote of its neighbors  with the object being
assigned to the class most common among
its k nearest neighbors      this could be done by
measuring distances between the object and its
neighbors  the following formula shows a
representation of simple euclidian distance  where a
and b are the respective positions of the object and
one of its neighbors 

fi      

  b  a  
i

 

i

i
  
knn is very sensitive to irrelevant or redundant
features because all features contribute to the
similarity and thus to the classification  this was
ameliorated by careful feature selection described
previously 

  
   decision trees
we used decision trees as they implicitly perform
feature selection   can tackle nonlinear relationships
between parameters  each leaf of the tree is labeled
with a class or probability distribution over the
classes 
a tree can be  learned  by splitting the source set into
subsets based on attributes and the recursion is
completed when splitting no longer adds value to the
predictions  the information gained is based on the
decrease in entropy after dataset is split  following
equation shows the formula for entropy  where p is
the probability of certain class occurring  given a
specific feature
  

e  s     p  p  log   p  p   p  n   log   p  n  
  
   random forest
we tried random forests as they are an ensemble
learning method that operate by constructing
multitude of decision trees  therefore their
performance is often better than decision trees alone
and can tackle issues like pruning  often an issue in
decision tree  automatically  since random forest
works quite well with several features  we first tried it
on the full set of features  table   shows its
performance class wise 
class

n cases

 
 
 
 
 
 
 
 
 
  
  
  
  

   
  
  
  
  
  
 
 
 
  
 
 
  

n instances
misclassified
   
  
 
 
 
  
 
 
 
  
 
 
  

pct  error
      
      
     
     
      
      
       
       
      
      
      
      
      

table    random forest performance on full feature set

  

since the error obtained was quite high  approx 
      we then tried it on the reduced features set
outlined previously  the error graph  figure    below
shows that the performance did not improve much 
    

    

out of bag classification error

n

d a b   

    

    

    

    

   

    

 

   

   

   

   
   
   
number of grown trees

   

   

   

    

figure    random forest performance on reduced
feature set

  
   svm  support vector machines 
we used svm with a cross validation of k     
to allow us average the error  among the    accuracy
we found  the highest that was reached is     and the
mean was approximately      
we tried both the polynomial and the linear kernels
for the svm and found out that the linear kernel
outperformed the polynomial kernel  the linear svm
with cv      gave the best accuracy among all the
other models we used  the features that were selected
for the svm where determined by using the decision
trees as explained in the feature selection section 
   logistic regression
since the logistic regression is used for binary
classification of datasets with categorical dependent
features  in order to apply logistic regression to our
multi class dataset  we firstly classified our instances
into two major classes  class    which contained all
the instances with class    label  and class not  
 which contained the instances for all the other
classes   we classified our data in this way  because
about half of our instances were labeled as class    
just like svm  we used cross validation with
k    folds to validate our model  although we got
accuracy of about      for the training set  the
accuracy for test set was about      

fi      
results
table   summarizes the results obtained from
each of the outlined methods 
as can be seen svm with linear kernel gave the best
performance  trees on the other hand did not
perform so well  one of the possible reasons may be
the presence of    classes  our discussion with some
of the other cs    teams with similar project
revealed that decision trees performance improved
drastically with reduction in the total number of
classes  other methods appear to be less sensitive to
this 
additionally  we used cross validation to obtain
error estimate on the test set  this helped us to be sure
that the error was the mean of all the various test sets
that could be obtained from the given data and so
results are less sensitive to the choice of test training
set  we used    k folds for the purpose  figure  
shows the comparison between errors that we got for
different models 
  
  
  
  
  
  
  

    

  
table    summary of various models performance using cross
validation of    folds

figure   shows the accuracy for the svm  knn 
decision trees  and logistic regression using
various folds  it is clear that the svm outperformed
all of the other models in our case 

figure    comparison of the accuracy of various models
using cross validation  

future work
since about     of the data were clustered in class
   it is believed that with more data on the patients of
the other classes  it is possible to learn more to get
more accurate classification  some of the classes had
only   to   instances in the data  which makes it
difficult to learn about these classes and hence their
misclassifications probability is high when using
various algorithms  it is clear that class   has the
dominant effect on the predicting models so
collecting more instances of patients in the other
classes is a goal for better predictions in future 
apart from this and based on our findings throughout
this project  here is what we would like to propose for
future work in this area   it will be interesting to group the features

based on their physical similarity  like also the
ecgs p wave variables together and all the q
wave variables together  and re check the
performance of these algorithms 
   though we used some rigorous feature
selection techniques  one more method that
can be tried is forward or backward
search techniques  where the features are
dropped or added to check their impact on the
algorithms accuracy  a great thing to do here
would be to combine all the feature selection
techniques described in this paper and average
out the score assigned to each feature in the
data set  this will give researchers a very
good idea of which features are most
important distinguisher of various cardiac
arrhythmias  these ranks could be discussed
with experts in the field of cardiology to
check how well did the data driven
assignments match expert opinion 
   it will be worthwhile to reduce the number of
classes based on literature review  group
similar classes together  to check if the
performance of the model improves  one
could break this algorithm into two parts  first
part can be used to give the user results based
on reduced number of classes  say    and then
on all    classes  this way  even if the
accuracy with all the    classes is not
extremely high  at least the user will know in

fi      
general which of the   broad categories he she
fall in   
conclusions
using linear svm  we built a predicting model to
classify the cardiac arrhythmia patients  the most
important features were selected via decision trees 
we were successful in reducing the variables from
    to    variables and using cross validation our
models accuracy is approximately      the data is
skewed as about     of the patients are classified as
class    hence  for prediction accuracy improvement it
is essential to gather more data on patients in other
classes 

references
    http   en wikipedia org wiki cardiac dysrhythmia
    http   www cdc gov dhdsp data statistics fact sheets d
ocs fs heart disease pdf
    cunningham       k nearest neighbor classifiers 
technical report ucd csi         university college
dublin
    roger vl et al  heart disease and stroke statistics
     update  a report from the american heart
association http   www cdc gov other disclaimer html

  

fi