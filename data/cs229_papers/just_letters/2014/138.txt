demand prediction of bicycle sharing systems
yu chun yin  chi shuen lee  and yu po wong
stanford university
bike sharing system requires prediction of bike usage based
on usage history to re distribute bikes between stations  in this
study  original data was collected around washington d c  area
in      and       original data is processed by several feature
engineering approaches based on analysis and understanding of
the data  ridge linear regression  support vector regression  svr   random forest  and gradient boosted tree are applied to
predict usage of bike sharing system in an hour based on the
conditions of the hour  test root mean squared logarithm error
 rmsle  is achieved at                   and      respectively
after model parameters optimization and feature selection 
keywordsbike share system  support vector regression 
radom forest  machine learning 

i  introduction
a bike sharing system is a new form of public
transportation systems  allowing its users to rent a bike from
a location and return the bike to another location  a typical
bike sharing system consists of an information technology
 it  system and bike sharing stations  the it system keeps
track of the registration database  payment  and number of
available bikes in each station  while a bike sharing station
has kiosk s  connected to the it system for user registration
and bike rental return  bike racks are connected to kiosk and
automatically release retrieve bike to from users  one major
task for maintaining a bike sharing system is to redistribute
bike to match bike demand at each station  this task requires
prediction of bike demand based on the usage record and the
current information about the targeting prediction time
window  the full prediction problem would be predicting the
rental return counts at each of the stations  in this project  we
simplifies the problem down to predict the count of total
rental bikes of the entire bike sharing system only 
the dataset used for learning is provided by fanaee t    
collected from capital bikeshare program in two years      
and       around washington d c  the dataset is also
hosted on uci machine learning repository      which
contains       rows of hourly count of rental bikes  crb 
with the corresponding features including date hour  seasons 
holiday working day weekday 
weather 
temperature 
feeling temperature  humidity  and wind speed  these
features will be manipulated and used to predict the hourly
crb  this report is organized as follows  section ii
describes the algorithms used for learning as well as some
methods to engineer the features to improve the
performance  the results predicted by each algorithm along
with the optimization methodology is shown in section iii 
section iv discusses some issues we faced during the
optimization of model parameters and future works 

ii  approaches
before feeding the raw feature inputs into the learning
algorithms  we first study some possible ways to engineer
the raw data to potentially increase the correlation between
features with target  i e  crb in this report  
    digitization  some features are discrete by their nature 
for instance  there are   seasons in any year  and they are
supposed to be independent  we can thus create   new
digital features representing the   seasons  xspring  xsummer  xfall 
and xwinter  a row with the season of spring corresponds to
xspring      xsummer   xfall   xwinter      the digitalization can be
applied to weekday  season  and weather 
    periodic function  another trial is to transform the
feature hour into cos      hour  and sin      hour  
since hour is intuitively a periodic function 
    conditional expectation value mapping  cevm   for
features with discrete values  e g  seasons   we can transform
them into their corresponding conditional expectation value 
  e y     where y is used interchangeably with crb
throughout this report 
next  the algorithms used for learning are introduced  the
primal optimization problems will be first described  then
the model parameters and their optimization strategies will
be discussed 
a  ridge linear regression  rlr 
primal problem 

 

 
    arg   
   
                 
 
     
where  is a penalized coefficient to adjust the trade off
between bias and variance  the higher the   the lower the
variance in general  the ridge function in matlab is used to
solve rlr problems  since the computation is very fast  we
optimize  by brute force 
b  support vector machine for regression  svr 
primal problem 


 

                

      


   

              
subject to        t  i       i
i   i               
where  controls the width of the allowed  insensitive zone 
and c is a penalty factor  which determines the penalty for
the data sitting outside the  insensitive zone  and the dual
problem is 

fi

 
                        
   


  

         
  

           
subject to  
   i   i            
where              i    j   is the kernel function 
and             is the vector of all ones 
details of the derivation of the dual optimization problem
and the algorithm to solve the problem can be found in     
in this project  we use a publicly available package provided
in     to solve the svr problem  parameter c determines the
trade off between model complexity  flatness  and the
degree to which deviations larger than  are tolerated in the
optimization formulation  parameter  controls the width of
the  insensitive zone and affect the number of support
vectors  qualitatively  if  is too small and or c is too big 
the model may lead to overfitting  if  is too large and or c is
too small  the model may result in underfitting 
to tackle non linear problems by svr  kernels are often
employed  because they introduce different nonlinearities
into the svr problem by mapping input data x implicitly
into a hilbert space via a function    where it may then be
linearly separable  while there are many options for the
kernels  in this project  we focuses on the gaussian radial
basis function  rbf  kernel because it gives the best
predictions  the rbf kernel can be expressed as
 
               
   
where  is the bandwidth of the kernel  the rbf kernel
spreads the influence of each training point to an area in the
high dimensional space  and  determines how far the
influence of each training point spreads  if  is too small  the
influence spreads far  which potentially leads to high bias  if
 is too large  the influence is in a narrow region  leading to
loss of mapping to higher dimensional feature space and
potentially high variance 
it is well known that the selection of   c  and  will
significantly affect the final performance  furthermore  we
observed that the computation time needed to solve the
optimization problem is also highly dependent on  and c 
empirically  we found that the computation time increases
rapidly with increasing c  and it can easily take hours to
search the optimal parameters without a reasonable initial
guess  according to      optimal choice of c can be derived
from standard parameterization of svm solution  resulting in
c    max  y  y   y  y    where y and y are the mean
and the standard deviation of the training targets respectively 
for the selection of   it is well known that the value of 
should be proportional to the input noise level  and as
suggested in      the optimal  can be estimated empirically
by       ln    where  is the input noise level and m
is the number of training samples  while  is usually
estimated from the squared sum of the fitting error of the
training data  here we estimate       given the data is
normalized  fortunately  as will be shown in section iii  the

model performance is not sensitive to the selection of  as
long as  is small enough  as for   empirically we found that
the computation time is less sensitive to   and the optimal 
value is usually somewhere between         as a result  the
optimization strategy for svr is as follows 
i  given a set of features f and a small set of training data
 e g              estimate  c          
 y  y        ln   
ii  check if   is small enough so that the test error does
not change with respect to  
iii  optimize  c    on f around  c      by grid search to
get  c      
iv  plot the learning curve on  f  c       i e  training error
and test error vs  the training sample size  
v  perform feature selection to obtain an optimal subset
features f  
vi  re optimize  c    on f to get  c        
c  random forest method for regression  rf 
random forest is an algorithm with ensemble learning
method  which constructs multiple decision trees  and the
output is the average of the output of the individual tree
 regression   this method combines the idea of bagging
 bootstrap aggregating  and the random selection of features
in order to improve the variance in decision tree learning 
which has low bias and very high variance  in rf  the tree
formation uses classification and regression trees  cart 
methodology  which minimizes     on the root node and
recursively on the child nodes        
min            min         
   




where index i is for size of training samples in the node and
index j is for candidate features in the   s is the critical value
used to split samples on a node  the optimization stops with
constraints on tree depth or the minimum number of data
points in the leaves 
a rf first chooses bootstrap sample di from d  the
original training data set  and then uses a modified tree
learning method that splits with candidate features randomly
selected from a subset of the total features  typically  for a
dataset with d features   features are used in each split
        finally  we will have ensemble of trees and the output
is the average of the output of the trees  which reduce the
variance  two key parameters are tree number and tree depth 
small tree depth will decrease the complexity of tree
structure that results in high error  bias   large tree number
will improve variance of individual trees  leading to smaller
error  computation time usually linearly increases
proportional to tree numbers  in this project  rf is
implemented by weka  a java based package available to the
public     
optimization strategy for random forest  rf  
i 
given a set of data f
ii 
optimize tree number and tree depth to get
 b  depth  
iii 
feature selection with forward search on  f  b  
depth   to get fs
iv 
re train model with   fs  b   depth  

fid  gradient boosted regression tree  gbt 
gradient boosting is an ensemble method  which enhances
performance of regression classifier  each iteration fits a
model to the residuals left by the previous classifier  output
is the sum of these classifiers  to prevent overfitting 
shrinkage rate   is included that the output would be sum of
the classifiers weighted by    smaller  can control
overfitting  however  it requires a large number of iterations 
which is often too time consuming to optimize 
   i  f  i   
   
f  i  f  i    l 
where f x i is a set of prediction  l is the error of the
prediction  we uses gradient boosting machine along with
random forest in our learning model 

the crb among all the features  the correlation is still low
 the noise is large   thus the rlr result suggests that this
crb prediction problem is highly non linear 
b  support vector regression  svr 
since this crb prediction problem is highly non linear 
we use the rbf kernel in the svr  both polynomial and
sigmoid kernels have been tested and the rbf kernel is
proved to be the best   in addition  digitalization of features
described in section i      is employed to achieve the best
performance  based on the optimization strategy described in
section ii b  we first optimize c and  around c  and   as
shown in fig   a to get c       and           one can
clearly see the trade off between bias and variance on the
plot  owing to the initial estimation of c  and    the
optimization by grid search can be performed in minutes 

iii  results

    

 

rmsle    
                  

 

   

where pi is the predicted crb  in fact  rmsle is the root
mean square of the logarithm of the ratio between the
predicted values and the actual values  i e  targets   one
reason for us to choose this metric is because this metric is
also used by a public competition on kaggle       which
allows us to compare our results against as a reference 
relative error  error divided by target  can be estimated from
rmsle  e g  rmsle     estimates that relative error  
     in the rest of this report  the       method is used to
measure the generalization error for the sake of
computational efficiency  unless otherwise specified 
a  ridge linear regression  rlr 
since rlr has a closed form analytical solution and only
one model parameter   the performance is determined by the
selection of features  fig   a shows the generalization error
 rmsle in short  before and after feature engineering 
various combinations of the feature engineering described in
section ii         were tested  and the cevm is found to
give the best results  however  rmsle     is still
considered very poor  in fig   b  the average with the
standard deviation of the crb vs  hour is plotted  even
though the hour feature indicates the highest correlation with

fig      a  generalization error of ridge regression vs  parameter  
the blue line represents the result after mapping features to
conditional expectation values as described in section ii        b 
hourly average with standard deviation of the count of rental bikes
vs  hours 

   
rmsle

this section shows results of applying the algorithms
described in section ii on the prediction problem of rental
bike demand  the performance metric used in this project is
the root mean square logarithmic error  rmsle  

train
test

    
   
       
  

 

  


 

  

fig      a  optimization of c and  in the svr  the contour
represents the generalization error given by the       method   b 
training and test errors vs  the loss function   showing that the
performance is insensitive to  for small s 

next  the test and training errors are plotted vs   in fig   b
to verify that the performance is indeed insensitive to  as
long as  is small enough  therefore        is used from
now on  fig   a shows the learning curve using  c       the
minimum test error         occurs when the full dataset is
trained  from the learning curve  one can conclude that  i 
acquiring more training samples and  ii  feature selection
may help  since the test error is still decreasing  and there is a
gap between the training and test errors  therefore  forward
and backward searches are carried out as shown in fig   b 
the forward search is found to achieve the better result  and
the resulting selected features are  from high to low
importance   hour   working day   season   year    
furthermore  we found that both forward and backward
searches reach the same order of the top four selected
features  indicating the importance of these four features 

fig      a  training and test error vs  the sample size after the
optimization of c and    b  forward and backward search for
feature selection 

fibike counts

   

non working day
working day

   

   

 
 

 

  

  

  

hr

fig      a  learning curves with errors estimated by       method
and    fold cross validation  showing similar trends   b  the mean
count of rental bikes vs  hour for working days and non working
days  two different patterns are clearly observed 

next  we select the top    features  denoted by fs  from the
forward search and then re optimize  c    to obtain  c       
              with the  c         learning curve is re plotted
in fig   a  test error is improved to       k fold crossvalidation  cv  with k    is also drawn on the same plot in
fig   a  showing that       method gives similar results to
cv  in addition  the minimum generalization error given by
cv is       lower than       method  due to the larger
training sample size being used  to seek for additional tricks
to improve the performance  we turn our attention to the top
two significant features  hour and working day  interestingly 
as shown in fig   b  a clear patterns are observed  during
working days  the peaks occurs at  am and  pm  suggesting
that the rental bike demand is dominated by commuting
workers  while during non working days  the peak occurs in
the afternoon  implying that the demand is dominated by
those who go out for fun  based on this observation  by
adding one more feature  hour working day into fs  the
generalization error is lowered to      
c  random forest method  rf 
the maximum number of the features in a node is choen
as    the feature engineering is applied including digitzation
of features and periodic function of hour  following the
optimization strategy described in section ii  we first
optimize tree number and tree depth by eveluating the test
error estimated by       method  the result of the
optimization of tree number and tree depth is shown in fig 
 a  with larger tree depth  the generalization error will be
smaller because the model complexity increases and the
training error  bias  is reduced  on the other hand  the test
error decreases with larger tree number because a large
number of trees can average out the variance  the
performance saturates when the tree number is increased to
    and the depth is increased to      the computation time
linearly increase with tree number  for the learning of    
trees  it take less than one minute  therefore we choose tree
number     and unlimited tree depth as our optimization
parameters  in fig   b  we compares the learnig curves
between the feature sets before and after feature selection by
forward search  the rmsle estimated by       method is
reduced from      to      after feature selection  and the
improvement is more pronounced for smaller training sample
size  the resulting selected features are  from high
importance to low   hour   working day   season   year  
weather     which is consistent with the result from the
svr 

fig      a  optimization of tree number and tree depth   b  learning
curve by       on optimized tree number and tree depth  green
line and the dash line are the test error of the given dataset before
and after feature selection 

d  gradient boosted tree  gbt 
we tried our learning model with    iterations and
shrinkage rate of     along with rf with tree number of    
and unlimited tree depth  the training error is about        
the test error for the full dataset is       which is similar to
the result of rf  gbt provides several knobs to control
overfitting including tree structure and shrinkage rate  
however  our model is not improved a lot with different tree
structure  optimization of shrinkage rate  needs larger
iteration number  which takes more than  hr for shrinkage
rate  of     and iteration number of      
iv  disscusion   future work
in this section  we discuss the issues we faced when
implementing the learning algorithms and optimization 
which leads us to the future works 
 from the rlr  we conclude that this prediction problem
is highly non linear  and the resulting rmsle is high 
by employing svr with rbf kernels  rmsle is
reduced significantly  interestingly  while applying
proper feature engineering techniques  section ii  can
greatly improve rlr performance  the same technique
does not help that much  rmsle from svr is reduced
from      to      after applying feature engineering  
since rbf kernels already map the features to a high
dimensional space  preprocessing the features may not
be as intuitive as the case of linear regression  and
requires more experience and prior knowledge in the
problem 
 while further improving the svr performance is
possible by optimizing the model parameters and more
feature preprocessing  e g  adding hour working day  
the difficulty arises from the fact that re optimization of
the model parameters is necessary but the optimization
is time consuming  e g  several minutes  even with a
judicious initial guess  what makes it worse is that the
performance can be severely affected by some arbitrarily
preprocessed features thrown into the program for
learning if they are not correlated to the targets  which is
hard to know in advance  therefore  feature selection is
often needed to reach the optimal solution  in short 
more sophisticated strategies to select model parameters
are required for rapid testing on adding new features 
 although svr performs well in this problem  the
random forest method seems to give a better result 

fiwhat makes rf even better is that the optimization of
the model  i e  tree numbers and depth  is
straightforward and less computationally expensive 
therefore  without applying sophisticated techniques 
we found that rf outperforms svr 
for future works  we plan to work on two aspects 
 data  we have identified several critical features 
including hour  working day  year  etc  one commonly
used trick for improvement is to mix these features by
some functions  e g  x y  sqrt x y   etc    another trick
we will try is to utilize the time dependence between the
data points  for example  the crb today may have
something to do with the crb yesterday at the same
time 
 theory  we have found that rf is the most promising
candidate to deliver best performance  hence it is
worthwhile to dive into rf and stretch it to the limit  for
svr  we have not fully explored the model selection so
far  by the fact that a linear combination of kernels is
still a kernel  we can combine various kernels to test the
limit of svr 
v  conclusion
table i  summarizes the results of the learning models that
we have tried  the final performance is measured by    fold
cross validation on the full        data points  the random
forest method is found to perform the best  in terms of both
prediction accuracy and training time  while each model has
been optimized as much as possible  the optimization is
limited by training time and our understanding of the
algorithms  not necessary to be the true limit of each one 
further study is needed to test the limits  as discussed in
section vi 
table iii
summary of results
method

performance  rmsle
by    fold cv 

computation time
for training

ridge regression

    

   sec

support vector regression

    

   s secs

random forest

    

   s secs

gradient boosted tree

    
 not optimized 

   s mins

reference
    fanaee t  hadi  and gama  joao   event labeling
combining ensemble detectors and background knowledge  
progress in artificial intelligence         pp        springer
berlin heidelberg 
   
https   archive ics uci edu ml datasets bike sharing dataset
    a  j  smola and b  scholkopf  a tutorial on support
vector regression  journal of statistics and computing 
vol      pp               
    chih chung chang and chih jen lin  libsvm   a
library
for
support
vector
machines 
 online  
http   www csie ntu edu tw  cjlin libsvm 

    v  cherkassky and y  ma  practical selection of svm
parameters and noise estimation for svm regression 
neural networks  vol      pp              
    breiman  leo   random forests   machine learning     
             
    witten  ian h   and eibe frank  data mining  practical
machine learning tools and techniques  morgan kaufmann 
     
    ho  tin          the random subspace method for
constructing decision forests   ieee transactions on
pattern analysis and machine intelligence                
doi                   
    mark hall  eibe frank  geoffrey holmes  bernhard
pfahringer  peter reutemann  ian h  witten         the
weka data mining software  an update  sigkdd
explorations  volume     issue   
     https   www kaggle com c bike sharing demand 

fi