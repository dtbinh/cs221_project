  

error  detection  based  on  neural  signals  
nir  even chen  and  igor  berman   electrical  engineering   stanford    

introduction    

brain   computer   interface    bci    is   a   direct   communication   pathway   between   the  
brain  and  an  external  device   in  professor  shenoys  lab  primates  are  trained  to  use  
bci   to   control    d   cursor   to   accomplish   different   tasks    the   primate   controls   the  
cursor   through   its   neural   activity    which   is   being   recorded   by   an   array   of   electrodes  
implanted  in  the  motor  cortex   
decoding   neural   activity   is   a   challenging   task   prone   to   error    in   this   work    we  
propose  a  binary  classifier  that  uses  the  same  neural  inputs  from  the  motor  cortex  
to  detect  errors     
this   classifier   can   alert   the   system   when   an   error   occurs   and   enable   auto 
correction    as   well   as   provide   feedback   regarding   the   primates   understanding   of  
failure   and   success    eventually    this   method   can   be   used   to   enable   paralyzed  
patients  to  communicate  with  their  environment   
in  the  first  part  of  the  project  weve  focused  on  reducing  the  datas  dimensionality  
through   preprocessing    in   the   second   part    we   compared   various   classification  
techniques  on  the  data   eventually  achieving  a  classification  error  rate  of          

experiment  setup  
in  this  project  we  will  use  data  from  the  spelling  task   which  includes  time  series  of  
neural   activity    and   labeling   of   the   trials    success       failure     in  this   particular   task  
the  primates  were  required  to  bring  the  cursor  above  a  highlighted  letter  and  hold  it  
there  within  a  set  timeframe      ms    if  successful  the  primates  received  a  reward   
in  the  case  of  failure   a  specific  sound  is  produced   figure         
hold     ms

target  appears

  

    

  

  

cursor  
movement

  

  

   ms

reward

success
   ms
failure

figure      experiment  setup   a  target  appears  and  the  primate  moves  the  cursor  using  bci   selection  is  made  when  
it  holds  the  cursor  on  top  of  a  circle  for     ms   than  the  target  is  vanishes   the  target  changes  to  blue  in  while  it  is  
being  help   feedback  appears     ms  after  selection   

for   our   initial    pre milestone    study    we   used   existing   data   from   previous  
experiments    where   the   reward   was   given   immediately   after   the   success    for   the  
sake   of   this   project    we   then   conducted   a   new   experiment    where   feedback   was  
delayed   for      ms    so   that   the   reward   doesnt   mask   the   primates   internal   error  
recognition   
  

fi  

the  data  

the  neural  activity  is  recorded  from  the  motor  cortex   areas  m   and  pmd   with  a     
khz   sampling   rate   from         electrodes    in   neuroscience   it   is   common   to   analyze  
neurons   spikes    so   our   raw   data   includes   is   discrete   in   time   and   binary   in   values   
with  x t     if  there  was  a  spike  during  millisecond  t   
based  on  previous  studies   we  expect  the  failure  signal  to  appear  at  most     ms  
after   the   actual   mistaken   selection    so   we   use   samples   from      ms   before   letter  
choice  to     ms  after   the  experiment  was  conducted  on  the  same  primate  for  two  
days         trials  were  recorded  with  a       failure  rate     

preprocessing  
dimensionality  reduction  
each   learning   examples   included         time   samples   in         channels    resulting   in  
                   dimensions    before   proceeding   to   run   learning   algorithms   on  
the  data   well  take  some  steps  to  reduce  dimensionality   
time  dimension   
based  on  prior  studies   we  expect  the  failure  signal  to  last  for  tens  of  milliseconds   
furthermore    the   usual   firing   rate   for   neurons   in   around        hz    meaning   that   the  
 ms   resolution   produces   very   sparse   data    based   on   this   evidence    and   as   is  
common  in  the  field   we  expect  not  to  lose  any  critical  information  by  downsampling  
the   data   and   grouping   the   samples   into     ms   bins   of   int   values    resulting   in   only       
time  samples  per  channel  per  trial   
number  of  channels   
in   the   channel   domain   we   applied   per time frame   pca   to   reduce   the   number   of  
channels   we  decided  not  to  normalize  the  variance   since  all  channels  represented  
the  same  type  of  data    spikes   ms    and  intuitively   active  channels  should  have  
more  influence  than  sparse  ones     
weve   tried   two   different   approaches   to   performing   the   pca    and   we   compared  
them  by  the     of  variance  represented  in  the  first  n  components   using  the  rule  of  
thumb  that  its  better  to  have  fewer  components  that  capture  more  variance   
nave   pca    our   first   approach   was   to   concatenate   all   the   timeframes   of   all   trials  
 success   and   failure     and   extract   the   primary   components   from   the   resulting  
covariance  matrix   the  result  is  shown  in  figure     below   red    demonstrating  that  
     components  are  required  to  capture       of  the  variance   
mean diff   pca   following  these  results   we  came  to  assume  that  the  first  primary  
components   we   capture   actually   represent   the   kinematic   signals   produced   by   the  
brain   which  are  strong  and  common  to  both  failure  and  success  trials   this  model  
can  be  represented  a  sum  of  three  random  processes   
x  t     t       t     t         
where     t   is   kinematic   component   that   controls   the   cursor        t   is   the  
component   caused   by   success failure   and    t   is   noise    we   assume   that   the   three  
processes   are   independent   and   have   zero   mean    to   extract   the   second  
component     t    we  applied  the  following  method   

  

fi  
   

 
 

   
    

 
 

                    
     

the   resulting   mean diff             captures   the   average   differences   between  
successful  and  failed  trials   since  the  kinematic  component  is  generally  orthogonal  
to   the    fail success    labelling    subtracting   the   two   averages   reduces   its   effect   
eventually    we   ran   a   pca   on   the   matrix    and   examined   the   resulting   vectors    as  
shown  in  figure     below   the     first  pcs  together  capture       of  the  variance   
   
pcamean
pcaall

    

 
 

  

  

  

  

  

  

  

cumulative  variance

explained variance

    

 
  

 pcs

figure      per vector dotted  line     and  cumulative   solid  line   variance  for  two  sets  of  pcs   the  
nave  pca   pcaall    red   and  mean diff  pca   pcamean    blue   

based   on   the   above    we   choose   to   proceed   with   the   second   set   of   pcs    generated  
from   the   mean diff    to   gauge   their   relevance   to   our   classification   problem    we  
project   the   successful   and   failed   trials   on   the   first   three   pcs   we   found    figure        
with  the  projection  of  failed  results  in  blue  and  successful  in  red     

  

figure      projection  of  successes  and  failures  on  first  three  pcs   failure  in  blue   success  in  red   solid  line  represent  
the   average   projection   for   every   timestamp    dotted   line   is   the   standard   deviation   of   the   mean    the   circles   represent  
the  time  evolvement  where  the  black  circle  is  target  selection  time   t      top    d  of     leading  pcs   bottom    d  view  
of  pc     and  pc      

we   can   see   that   the   two   processes   are   differentiating   from   target   selection   time  
 t      black   circle     in   the   bottom   figures   it   is   evident   that   the   mean   projection   on  
pc    and   pc    are   common   for   both   scenarios    however    pc    contains   the  
information  that  distinguishes  between  the  two  scenarios   
  

fi  

classification  
algorithm  comparison  
using  the  preprocessed  data   three  channel  with      samples  each   meaning           
we   compared   several   learning   algorithms    svm    c         logistic   regression    gda  
and  k nn   k      as  shown  in  figure      to  compare  the  different  methods  we  used  
   fold   cross   validation    maintaining   in   each   random   group   the   original  
success failure  rate   at  this  stage   we  ignored  the  temporal  information   treating  our  
samples  as  vectors  in      dim  space     
classifier comparison
  

error  

  
  
  
 
 

svm

lr

gda

   nn

figure      four  learning  algorithms  performance  
using     fold  cross  validation  

  

  
figure      svm  performance  as  function  of  
number  of  pcs   

we  can  see  that  svm  achieves  the  best  performance       classification  error    this  
can   be   explained   by   the   fact   that   our   data   is   still   in   a   high   dimension   space    and  
learning   algorithms   are   prone   to   overfitting   in   this   case    especially   where   the  
number   of   examples   is   low    the   exception   is   the   svm   algorithm    that   has   a   low  
effective   vc   dimension   and   can   produce   robust   classifiers    the   gda   algorithm  
performs   especially   poorly    indicating   that   the   data   distribution   cant   be   modeled   as  
gaussian   
effect  of  dimensionality  reduction  
based   on   the   above   results    we   selected   the   svm   method    and   revisited   our   decision  
to  choose  the  three  leading  pcs  to  project  the  data  on   as  shown  in  figure      
we   can   see   that       pcs   capture   almost   all   of   the   relevant   information   of   all        
channels   note  that  the  other  learning  algorithms  we  applied  would  not  be  able  to  
converge  to  a  solution  if  run  on  the  original  data  without  preliminary  pca   
effect  of  time  evolution  
finally    we   tried   to   answer   this   question       how   soon   does   the   primate   know   it   made  
a  mistake   to  asses  this   we  trained  per time window  classifiers      ms  windows   
every    ms   and  measured  their  performance   results  are  in  figure        
first   of   all    we   see   that      ms   before   selection    error   rate   is           which   is   the  
expected   performance   of   a   classifier   without   information    since   the   failure   rate   is  
      progressing  in  time   we  see  that  the  trials  are  classified  with  better  precision   
and  even  before  the  selection   a  prediction  can  be  made  with  around       error  rate   
the  minimum  error  of  classification  is  around     ms  after  the  selection   
  

  

fi  

  

figure      performance  of  time window  classifiers   windows  of     ms   blue      
classification  based  on  all  time  samples   red   

summary  
in  this  project  weve  shown  that  data  related  to  the  success  or  failure  of  a  trial  can  
be   extracted   and   expressed   in   three   primary   components    furthermore    an   svm  
classifier  can  be  trained  on  this  data  to  achieve  and  error  rate  of          indication  
about   target   selection   error   is   available   even   before   the   error   occurs    but   is   most  
evident     ms  after  the  selection   
error   detection   can   improve   the   speed   and   precision   of   speller based   bci  
applications     
for  future  work   we  would  like  to  implement  a  real time  error  detector  that  would  
prevent  wrong  selections   in  addition   we  would  try  training  separate  classifiers  for  
each  time  window   and  then  bagging  their  results  into  a  single  output   for  example   
by  using      svms  aggregated  by  a  logistic  regression  classifier     
finally    given   that   false positive   and   false negative   classifications   can   have   different  
costs   we  should  train  a  weighted  classifier   that  takes  these  costs  into  account  and  
minimizes  the  expected  cost     

acknowledgments    
the   experiments   were   done   in   prof    shenoys   lab   and   with   the   help   of   sergey  
stavisky  and  jonathan  kao   

reference  
   kao  j  c   stavisky  s  d   sussillo  d   nuyujukian  p     shenoy  k  v 
        information systems opportunities in brainmachine interface
decoders   
   bottou  l   cortes  c     vapnik  v          on the effective vc
dimension 

  

  

fi