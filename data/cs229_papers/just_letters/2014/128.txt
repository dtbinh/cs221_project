cs     machine learning  fall       stanford university

 

office appliance classification
from disaggregated plug load data
gerrit de moor  elissa goldner  brock petersen
department of civil and environmental engineering  atmosphere energy program
stanford university  stanford  ca         usa

abstractas buildings consume     of us primary energy 
energy efficiency through smart plug load management can have
a significant impact on us carbon emissions  in this paper we
investigate learning algorithms to classify plug loads based on
minutely energy plug load data and hourly weather data  one
month of data is preprocessed into    daily features which are
used to classify      training samples into    labels using a
range of multi class classification algorithms  of all models tested 
random forest trees performed the best  with a test error of     
this promising result can likely be further refined with improved
feature selection  and a larger dataset  future work will focus on
increasing the model performance and integrating it into a useful
product for plug load management companies 

i 

i ntroduction

energy consumption from carbon intensive energy resources
is a major contributor to anthropogenic climate change  while
transportation and industry contribute to this problem  buildings represent the largest energy consuming end use in the
united states at approximately     of primary energy 
historically building energy reduction focused on hvac
and lighting  as those loads have decreased  the importance
of appliance loads in buliding energy use has increased  office
buildings represent approximately     of the building energy
use in the united states and appliances consume     of that
energy 
enmetric is a plug load analysis company that grew out of
the need to give facility managers control over the appliance
energy use in office buildings  recognizing the challenge
faced by increased energy use  enmetric hopes to reduce
appliance energy use by eliminating energy consumption when
appliances are providing no service to the building occupants 
enmetric utilizes smart power strips that sense the appliance
load in virtual real time  using the smart power strips in conjunction with a software platform  building managers can view
the buildings disaggregated appliance energy consumption and
create rules to determine when plugs should power down 
learning more about enmetrics system  the group identified
two challenges that could be assisted by the use of machine
learning algorithms  classify the appliance plugged in to a
socket to detect when the appliance is moved and predict
the type of appliance that is plugged into a socket without
having to manually transcribe the appliance type  this paper
will explore different techniques to accomplish those goals 

ii  m odel f unction
the main goal of our model is classification of disaggregated
loads  currently  enmetrics labels each disaggregated load
by hand during installation of the smart power strips  if the
devices are moved  it is the responsibility of the building
manager to report the move to enmetric  since enmetrics data
analytics depend on accurate labeling of devices and because
it is often difficult for the building manager to keep track of
individual employees moving loads around  enmetric asked
the group to investigate an accurate method to detect if a
device had been unplugged or moved  the model developed
in this project learns device signatures from past data and
classifies each load as a particular device on a daily basis  by
comparing yesterdays device classifications to todays device
classifications  loads that have been classified as a new type
can be flagged for review 
iii  data
enmetric system smart power strips system takes plug load
measurements every second but aggregates the measurements
to be stored at the minute level  the energy data collected
included average power  minimum and maximum power during
the second  average frequency  average current  and average
power factor  see table below   for our project  we obtained
the minutely data for over     appliances in a        square
foot office building in silicon valley  the data covered one
year from           to                 gb   because of
computational limits  we were restricted to only analyzing one
month at a time     gb  
to capture the effects weather has on the appliance energy
use  hourly weather data was incorporated in the analysis
as well  the meso west weather database provided hourly
temperature and relative humidity for moffet airfield 
iv  f eatures and p reprocessing
all the numerical data provided by enmetric was processed
into features for our models  since minutely appliance energy
profiles vary significantly during the day  all data was aggregated and classified at the day level for each unique socket 
daily means and standard deviations of the minutely plug load
data were calculated and serve as features 
temporal features in the form of day of week  day of year 
month  and weekend served as additional features  all values
 except for weekend  which is binary  were normalized to
within the range          by using a cosine and sine to account
for the periodic nature of these feature inputs in the algorithms 

fics     machine learning  fall       stanford university

hourly weather data  temperature and humidity  were processed in a similar manner to the enmetric data  the daily
mean  minimum  maximum and standard deviation were calculated for the temperature and humidity  this brings the total
feature count to    and the number of training samples to     
 for one month of data   an overview of the data used and how
it was processed into features is shown in the table above 
the plug load data contained    different appliances  resulting in    labels to be used for classification  an overview of
the labels is shown is in the table below 

 

up the data  in this sense  it is very similar to the classic    
test data      training data rule  but with the difference that we
perform this   times with random permutations of the dataset 
the trade off between training and testing error for different
values of k is shown below for the random forest model  the
variance versus biased nature of the k value is shown  the
graph also supports our selection of a k value of   

fig     training and testing error from random forest for
different k values 

v  c lassification m odels
all learning algorithms considered for use for our project
fell under the category of supervised learning methods for
classification  because the dataset included several unique labels  the project is a multiclass classification problem  several
models were tested with varying parameters to find which algorithm resulted in the highest performance rates  the following
popular multiclass classification algorithms were tested for our
purposes  multiclass support vector machine  svm standard  
multi class svm one vs one  svm ovo   multi class svm
one vs rest  svm ovr   random forest  and gradient tree
boosting  a more detailed description of each algorithm  as
well as its strengths and weaknesses with respect to this
project  can be found below  all algorithms were implemented
in python  using the sci kit learn library 
the tuning of the parameters for each algorithm was done
by k fold cross validation  with a k value of    this rather
small value of k was chosen because we had sufficient training
samples and we wanted to speed up processing times  the
training samples were first randomly permuted before splitting

vi  s upport v ector m achines
support vector machines  svms  are a set of supervised
learning algorithms commonly used for classification  svms
are useful for this particular problem because they are very
effective in a high dimensional space  because they allow
the user to choose the kernel function to act as the decision
function in the model  svms are also flexible to what types
of classification problems they are suited for  despite their
effectiveness and versatility  svms become less effective when
the number of features is much greater than the number of
samples  since this was not the case for our classification
problem     labels       training samples   the group started
out by classifying using support vector classification  svc 
with a linear kernel and varied different parameters until we
found our best results  the general algorithm for a support
vector machine is as follows 
given training vectors xi  rp i       n   in two classes  and
a vector y  rn such that yi          svc solves the
following primal problem 
x
 
i
min wt w   c
w b   
i   n

   

yi  wt  xi     b      i  

   

i     i           n

   

 
min t q  et 
  

   

subject to 

its dual is

fics     machine learning  fall       stanford university

 

subject to 
yt     

   

   i  c  i           l

   

where e is the vector of all ones  c    is the upper bound 
q is an n by n positive semidefinite matrix  qij  k xi   xj  
and  xi  t  x  is the kernel     
there are two types of approaches for multi class svms 
one vs the rest  ovr  and one vs one  ovo   one vs therest considers training a single classifier per class while the
one vs one trains n n    binary classifiers for a n way multiclass problem      a more detailed description of each method
can be found in the subsequent sections  research has shown
that ovr and ovo are among the most suitable methods for
practical use     

fig     training and testing error from one versus one support
vector classification when varying c value 

a  one versus one svm
the first type of svm that was used for this project was
the standard svc with a linear kernel found in the sci kit
learn package  by default  svc implements the one versus one
approach to multi class classification  ovo classification will
build n n    classifiers  one classifier to distinguish between
each pair of classes i and j by marking i as the positive example
and j as the negative  when it is time to make a prediction
for a particular unseen sample  all classifiers are applied to
the sample and the class with the highest number of positive
predictions will be chosen 
rbf kernel   exp  x  x      

fig     training and testing error from one versus the rest
support vector classification when varying c value 

   

for the ovo svm  we started by tuning the penalty parameter of the error term  c   which can also be seen as the
hyperplane separation term  while initially  increasing c lead
to smaller prediction errors  values larger than    resulted in
overfitting the training sample and hence an increase in testing
error  it was expected that small values of c would result
in more misclassification  but we found that imporvements
reached their limit as c approaced a value of    the best
results for our model were found by using a radial basis
function  rbf  kernel  also know as the gaussian kernel 
varying values of gamma  the kernel coefficient used with rbf
kernels  produced no improvement in predictions  a graphical
view of the effects of varying c values on the ovo support
vector classification model can be found in figure   
b  one versus the rest svm
as opposed to ovo where a sample is marked as in class
i and not in class j  one vs the rest will build n different
binary classifiers to determine if the example is either in class
i or not in class i      for svm ovr  the outputs and ideal
tuning parameters were very similar to standard svm  as seen
in figure    similar to ovo svm  the ideal parameters for the
ovr svm were found to be c    with a rbf kernel 

vii  e nsemble m ethods
since svm results were still showing significant error
rates  the group decided to investigate ensemble methods for
classification  the goal of ensemble methods is to combine
the predictions of several base estimators built with a given
learning algorithm to improve the robustness over a single
estimator      by combining multiple hypotheses that may
produce weak results for a particular classification problem 
ensembles methods aim to build a stronger  more accurate
hypothesis tailored for the individual problem  the two types
of ensemble methods used for our classification problem were
random forest classification and gradient tree boosting 
a  random forest
the random forest classification method falls under the
category of an textitaveraging ensemble method  the driving
principle behind averaging methods is to build several estimators independently and to average the resulting predictions
together in order to reduce the variance in predictions      in
random forest classification  a series of randomized decision
trees in the ensemble are built from a random subset of the
training set  each subset used is a sample drawn from the
feature vector that is replaced after use for continued use in

fics     machine learning  fall       stanford university

subsequent tests  when splitting a node in the construction of
the tree  instead of choosing the best split that is best fit for
the entire training set  the split chosen is the best fit among
the subset of data  although the randomness of these subsets
usually produces results with higher bias  the nature of the
averaging ensemble method also decreases the variance among
results  typically much greater than the increase in bias 
when running random forest classification  we varied the the
number of estimators and maximum depth used in the algorithm until we found optimal results  intuitively  we expected
the performance to increase as we increased the number of
estimators  i e  the number of trees   but this came with the
trade off of a longer computational time  as can be seen in
figure    the improvement in training and testing error as we
increased the number of estimators was not as great as we had
expected  surprisingly for the group  varying the max depth of
the trees had a much greater effect on performance  as seen in
figure    the optimal values of max depth and n estimators
found for our model were    and      respectively 
given training vectors xi  rn i         l and a label vector
y  rl   a decision tree recursively partitions the space such
that the samples with the same labels are grouped together  let
the data at node m be represented by q  for each candidate
split     j  tm   consisting of a feature j and threshold tm  
partition the data into qlef t    and qright    subsets
qlef t       x  y  xj    tm
qright      q   qlef t   
the impurity at m is computed using an impurity function
h    the choice of which depends on the task being solved
 classification or regression 
nlef t
nright
h qlef t      
h qright    
nm
nm
select the parameters that minimises the impurity
g q     

   g q   

 

fig     training and testing error from random forest classification when varying max depth 

b  gradient tree boosting
the first of two algorithms that we looked at is called gradient boosting classification  gradient tree boosting combines
weak learning methods in an iterative manner until a single 
stronger learning method is created  the gradient tree boosting
method was chosen for our classification problem because it
is typically very good at handling data with heterogeneous
features  more concern with this algorithm was around the
scalability of its results  although it proved effective for our
data sets in month long intervals  it was thought that combining
that data into a full year could cause issues due to the sequential nature of boosting limiting its ability to be parallelized
    
since gradient tree boosting is another ensemble method  the
group expected that varying the number of estimators and the
max depth of the tree branches would act in similar manners as
they did in the random forest algorithm  as seen in figures  
and    our assumptions were largely correct  more interesting
however  was the effect that learning rate had on the model
results  shown in figure    after varying the parameters in
the gradient tree boosting algorithm  the optimal values of nestimators  learning rate  and max depth found for our model
were          and     respectively 

recurse for subsets qlef t     and qright     until the maximum allowable depth is reached  nm   minsamples ornm  
  

fig     training and testing error from random forest classification when varying number of estimators 

fig     training and testing error from gradient tree boosting
classification when varying number of estimators 

fics     machine learning  fall       stanford university

 

vs the rest  did not improve the performance  the random
forest algorithm performed the best in terms of speed and test
error  the algorithm ran in under    seconds and resulted in
a test error of      gradient tree boosting resulted in the
lowest training error and performed only slightly worse than
the random forest algorithm in terms of test error 
our work has shown that machine learning algorithms can
be a useful  efficient tools in classification of office appliances
from plug load data  future work should be able to further
improve the classification to more acceptable levels of test
errors  in particular  improved feature extraction and selection 
and a larger dataset look promising 
fig     training and testing error from gradient tree boosting
classification when varying max depth 

fig     training and testing error from gradient tree boosting
classification when varying learning rate 

viii  r esults
all three svm algorithms produced the greatest training and
test errors seen out of the five algorithms used  trying different
binary svm decompositions  ovo and ovr  did not improve
the performance  the random forest algorithm performed the
best in terms of speed and test error  the algorithm ran in under
   seconds and resulted in a test error of      gradient tree
boosting resulted in the lowest training error and performed
only slightly worse than the random forest algorithm in terms
of test error 
the graphs show that tuning the main parameters of the
algorithms has a significant impact on the test error  biasedvariance trade off does show in some of the tuning  as can
be expected when tuning certain parameters such as the c
parameter in svm models 
the performance of the random forest confirmed the fact
that random forests usually work faster  but need a sufficient
dataset size to work its randomization concept 
ix  c onclusion
svm performed worst of the algorithms we tested  trying
different binary svm decompositions  one vs one and one 

x  c ontinued w ork
the work presented in this paper shows that learning
algorithms can be an effective tool for enmetrics systems
to use in their smart plug analytic services  the group will
be presenting our findings at enmetric systems next month
with the intentions of continuing further and producing a
useful tool for the company to possibly use  based on the
successful results developed throughout the course of this
project  the following are potential actions that may be taken
in the continuation of the project 
   run the five learning algorithms presented in this poster
on the larger dataset  varying the necessary parameters
to find the best fit model for classification 
   develop an iterative algorithm that learns based on
a collection of past data  predicts for the day  and
compares to known to identify any location where
deviced could have been moved or unplugged 
   explore the space of prediction to eliminate the need
for enmetric to record by hand the type of device at
each plug at installation 
acknowledgment
we would like to thank jake masters and brad degnan of
enmentric systems  inc  for providing us with the data and
motivation for this project  we would also like to thank romain
juban of c  energy for his advice and guidance on the use of
learning algorithms with load disaggregation and classification 
finally  we would like to thank professor andrew ng and the
cs    teaching and project assistants for the help  guidance 
and encouragement 
r eferences
   
   

   

   
   
   

h  kopka and p  w  daly  a guide to latex   rd ed  harlow  england 
addison wesley       
machine learning applications for load  price and wind power prediction in power systems  michael negnevitsky  senior member  ieee 
paras mandal  member  ieee  and anurag k  srivastava  member  ieee
h  lei and v  govindaraju  half against half multi class support
vector machines  state university of new york at buffalo  http   blue 
utb edu hlei research papers half halfsvm pdf
f  pedregosa  et al  scikit learn  machine learning in python  journal
of machine learning research  vol      pg                 
http   www mit edu       spring   classes multiclass pdf
http   ieeexplore ieee org xpl login jsp tp  arnumber         
url http  a  f  fieeexplore ieee org  fxpls  fabs all jsp 
 farnumber  d       

fics     machine learning  fall       stanford university

 

a ppendix a
o ffice a ppliance c lassification m odel
different versions of this simulation were used throughout
this project  this version utilizes the most recent version
of the office appliance classification algorithm  this script
was written in python  utilizing the pandas and sci kit learn
libraries 
listing    code  
  c o d i n g   u t f  
     data input
  read i n t h e d a t a and i n d e x by t i m e
stamps
 

  in      
  c r e a t e one column o f u n i q u e ids f r o m t h e
node hid and c h a n n e l number
df    channel number      df    channel number 
    apply   s t r  
df    node hid      df    node hid      apply   s t r  
d f    i d      d f   a p p l y   lambda x   x    n o d e h i d 
   x    channel number        
df    i d      df    i d      apply   i n t  
df   df   drop      node hid     channel number 
      
df   head    

  in     

  in      

  import warnings
  warnings   f i l t e r w a r n i n g s   i g n o r e   
c a t e g o r y  d e p r e c a t i o n w a r n i n g  

  s t r i p t h e name column o f l o c a t i o n numbers
and c a l l i t new name
d f    new name      d f   a p p l y   lambda x   x   
name         
d i c t d a t a   pd   r e a d c s v      u s e r s   g e r r i t  
documents   d i c t i o n a r y   c s v   
d i c t i o n a r y   d i c t   z i p   d i c t d a t a   old name  
d i c t d a t a   new name    
d f   d f   r e p l a c e      new name    d i c t i o n a r y    

  in      
import p a n d a s a s pd
import numpy a s np
import s y s
import s t r i n g
from d a t e u t i l import p a r s e r

  take out unlabeled data
d f   d f   d f   new name      unknown   
df   head    

i n p u t p a t h      u s e r s   g e r r i t   documents   
  read i n a l l data f i l e s from d i r e c t o r y
  f i l e n a m e s                               
                                         
     
  d f   pd   c o n c a t     pd   r e a d c s v   i n p u t p a t h  
x s a m p l e   s t r   f i l e n a m e      c s v   
low memory f a l s e   f o r f i l e n a m e i n
filenames      
  r e a d i n one d a t a f i l e   o n l y f i r s t nrows
  f i l e n a m e    x sample      
f i l e n a m e    x sample      
d f   pd   r e a d c s v   i n p u t p a t h   f i l e n a m e     c s v
   low memory  f a l s e  
      data m a n i p u l a t i o n
  here we p l a y a r o u n d w i t h t h e d a t a   and
s e t up t h e x and y v e c t o r
  in      
df   head    

  in      
d f    o c c u r r e d a t      pd   t o d a t e t i m e   d f   
occurred at    
d f   day     d f    o c c u r r e d a t      a p p l y   lambda
x   x   t i m e t u p l e       tm yday  
d f   cos day      np   c o s   d f   day               
np   p i  
d f   sin day      np   s i n   d f   day               
np   p i  
d f   weekday     d f    o c c u r r e d a t      a p p l y  
lambda x   x   weekday      
d f   cos weekday      np   c o s   d f   weekday
           np   p i  
d f   sin weekday      np   s i n   d f   weekday
           np   p i  
d f   month     d f    o c c u r r e d a t      a p p l y  
lambda x   x   month  
d f   cos month      np   c o s   d f   month           
  np   p i  
d f   sin month      np   s i n   d f   month           
  np   p i  
d f   weekend       d f   weekday          

fics     machine learning  fall       stanford university

 

d f m i n   c o l u m n s     x  min  f o r x i n d f  
e l e c t r i c f e a t u r e s     columns  
  in      
c a l e n d a r f e a t u r e s     cos day   sin day   
cos weekday   sin weekday   cos month
  sin month   weekend  
e l e c t r i c f e a t u r e s      power avg    
power min     power max     e n e r g y u s e d 
   frequency avg     voltage avg    
current avg     power factor avg   
  in      
df mean   d f   g r o u p b y      i d    day      
e l e c t r i c f e a t u r e s     a p p l y   lambda x   x  
mean      
df mean   c o l u m n s     x  mean  f o r x i n d f  
e l e c t r i c f e a t u r e s     columns  
d f s t d   d f   g r o u p b y      i d    day      
e l e c t r i c f e a t u r e s     a p p l y   lambda x   x  
std     
d f s t d   c o l u m n s     x  s t d  f o r x i n d f  
e l e c t r i c f e a t u r e s     columns  
d f c a l e n d a r   d f   g r o u p b y      i d    day      
c a l e n d a r f e a t u r e s     a p p l y   lambda x   x  
mean      
d f l a b e l   d f   g r o u p b y      i d    day         
new name        a p p l y   lambda x   x   i l o c
    

  o t h e r s t u f f b a s e d on
  h e a r t b e a t time s e r i e s c l a s s i f i c a t i o n
with support
  v e c t o r machines
  a r g y r o kampouraki   george manis   and
c h r i s t o p h o r o s nikou   member   ieee
 

r o o t mean s q u a r e o f s u c c e s s i v e
differences
def rmssd   x    
n   len   x  
r e s u l t   np   s q r t   sum   np   e d i f f   d   x        
    n    
return r e s u l t
d f r m s s d   d f   g r o u p b y      i d    day      
e l e c t r i c f e a t u r e s     a p p l y   lambda x  
rmssd   x    
d f r m s s d   c o l u m n s     x  r m s s d  f o r x i n d f
  e l e c t r i c f e a t u r e s     columns  
d f s d s d   d f   g r o u p b y      i d    day      
e l e c t r i c f e a t u r e s     a p p l y   lambda x   np
  s t d   np   e d i f f   d   x      
d f s d s d   c o l u m n s     x  s d s d  f o r x i n d f  
e l e c t r i c f e a t u r e s     columns  

  in      

  in      

import numpy a s np

d f     pd   c o n c a t     d f l a b e l   df mean   d f s t d  
df max   df min   d f n i g h t d a y   d f r m s s d
  df sdsd   df calendar        

d f a v g d a y   d f   g r o u p b y      i d    day      
e l e c t r i c f e a t u r e s     a p p l y   lambda x   x
                      mean      
d f a v g n i g h t   d f   g r o u p b y      i d    day      
e l e c t r i c f e a t u r e s     a p p l y   lambda x   x
            and                       mean      
df night day   df avg day   df avg night
d f n i g h t d a y   c o l u m n s     x 
n i g h t d a y r a t i o  for x in df  
e l e c t r i c f e a t u r e s     columns  
df max   d f   g r o u p b y      i d    day      
e l e c t r i c f e a t u r e s     a p p l y   lambda x   x  
max      
df max   c o l u m n s     x  max  f o r x i n d f  
e l e c t r i c f e a t u r e s     columns  
d f m i n   d f   g r o u p b y      i d    day      
e l e c t r i c f e a t u r e s     a p p l y   lambda x   x  
max      

      t e m p e r a t u r e data
  in      
w e a t h e r d a t a   pd   r e a d c s v   i n p u t p a t h  
weather data clean   csv   
  in      
w e a t h e r d a t a   c o l u m n s      time     temp    rh 
   hour   
w e a t h e r d a t a   day     pd   t o d a t e t i m e  
w e a t h e r d a t a    time        a p p l y   lambda x  
x   t i m e t u p l e       tm yday  
m e a n w e a t h e r   w e a t h e r d a t a   g r o u p b y     day
         temp    rh        a p p l y   lambda x   x  
mean      

fics     machine learning  fall       stanford university

std weather  
         temp 
std     
min weather  
         temp 
min      
max weather  
         temp 
max      

w e a t h e r d a t a   g r o u p b y     day
  rh        a p p l y   lambda x   x  
w e a t h e r d a t a   g r o u p b y     day
  rh        a p p l y   lambda x   x  
w e a t h e r d a t a   g r o u p b y     day
  rh        a p p l y   lambda x   x  

 

       kmeans
  in      
  f r o m s k l e a r n   c l u s t e r i m p o r t kmeans
  i m p o r t numpy a s np

w e a t h e r   pd   c o n c a t     mean weather  
s t d w e a t h e r   min weather   max weather
     

 
 
 
 

  in      

  kmeans   c l u s t e r c e n t e r s
  max   kmeans   l a b e l s  

d f     d f     r e s e t i n d e x       merge   w e a t h e r   how
   l e f t    l e f t o n   day   r i g h t i n d e x  
true  

kmeans   kmeans   n i n i t        
kmeans   f i t   x  
l a b e l s   kmeans   l a b e l s
c e n t r o i d s   kmeans   c l u s t e r c e n t e r s

       s u p p o r t v e c t o r machine c l a s s i f i e r
  in     

  in      
d f     t o c s v   i n p u t p a t h   f i l e n a m e   p r o c e s s e d
  csv    index  false  
  in     
  i m p o r t p a n d a s a s pd
  i m p o r t numpy a s np
  i n p u t p a t h      u s e r s   g e r r i t   documents   
  f i l e n a m e    x sample      
  d f     pd   r e a d c s v   i n p u t p a t h  
x sample      processed   csv  
  d f     s e t i n d e x      i d     day      i n p l a c e  t r u e  
  in      
d f     np   random   p e r m u t a t i o n   d f    
      u s i n g machine l e a r n i n g a l g o r i t h m s
  once we h a v e r e a d i n t h e d a t a p r o p e r l y  
we can u s e t h e r i g h t m a c h i n e l e a r n i n g
a l g o r i t h m   f i r s t we i m p o r t t h e s c i  k i t
l e a r n   s k l e a r n   l i b r a r y which has a l l
t h e good svm s t u f f f o r t h i s   and a l l
o t h e r a l g o r i t h m s as w e l l      
  in      
x   df             
y   df             

from s k l e a r n   svm import svc
from s k l e a r n   e n s e m b l e import
randomforestclassifier
from s k l e a r n   e n s e m b l e import
gradientboostingclassifier
from s k l e a r n   m u l t i c l a s s import
onevsoneclassifier
from s k l e a r n   m u l t i c l a s s import
onevsrestclassifier
  in     
k  
xcv   np   a r r a y s p l i t  x  k  
ycv   np   a r r a y s p l i t   y   k  
xcv     np   v s t a c k   t u p l e     xcv   j  
range   k   i f j     i       f o r i i n
  
ycv     np   v s t a c k   t u p l e     ycv   j  
range   k   i f j     i       f o r i i n
  

for j in
range   k  
for j in
range   k  

  in      
  s e l e c t one model
  c                              
c       
  c                                            
for c in c 
 
model   svc   c      k e r n e l   r b f   
gamma          

fics     machine learning  fall       stanford university

 

 
 
 

 
 
 

t u n i n g   c     
model   r a n d o m f o r e s t c l a s s i f i e r  
n e s t i m a t o r s        max depth      
m a x f e a t u r e s    a u t o    n j o b s    
t u n i n g   n e s t i m a t o r s        
m a x d e p t h     
model   g r a d i e n t b o o s t i n g c l a s s i f i e r  
l e a r n i n g r a t e           n e s t i m a t o r s      
m a x d e p t h       m a x f e a t u r e s     
tuning   learning rate        
e s t i m a t o r s        max depth       or    
  m a x f e a t u r e s     
    to add
model  
n j o b s    
model  
n j o b s    
tuning  

 

p r i n t    number o f m i s p r e d i c t i o n s i s  d  n   
sum   p r e d i c t i o n     y              
  np   h s t a c k   t u p l e   p r e d i c t i o n   y            
np   h s t a c k   t u p l e     p r e d i c t i o n   y                
p r e d i c t i o n    y        
  in      
  import m a t p l o t l i b   p y p l o t as p l t
   m a t p l o t l i b i n l i n e

t o svc model
o n e v s o n e c l a s s i f i e r   model  
  in      
o n e v s r e s t c l a s s i f i e r   model  
c      

error train        c
error test        c
 
 
 

 

xcv   np   a r r a y s p l i t   x   c  
y c v   np   a r r a y s p l i t   y   c  
xcv     np   v s t a c k   t u p l e     xcv   j   f o r
j i n r a n g e   c   i f j    i       f o r i i n
range   c      
y c v     np   v s t a c k   t u p l e     y c v   j   f o r
j i n r a n g e   c   i f j    i       f o r i i n
range   c      
f o r i i n range   c    
model   f i t   xcv   i     ycv   i              
  p r e d s t r   model   p r e d i c t   xcv   i    
  print training error for fold
             f o r m a t   i      model  
s c o r e   xcv   i     y c v   i      
e r r o r t r a i n   i      model   s c o r e  
xcv   i     ycv   i    
  p r e d s t e   model   p r e d i c t   xcv   i    
  p r i n t  test error f o r f o l d    
       f o r m a t   i      model   s c o r e  
xcv   i     y c v   i      
e r r o r t e s t   i      model   s c o r e   xcv  
i     ycv   i    
p r i n t c    t r a i n i n g e r r o r        cross
v a l i d a t e d e r r o r         format   np  
mean   e r r o r t r a i n     np   mean  
error test    

  in      
p r e d i c t i o n   model   p r e d i c t  x 
print   prediction  

 
 
 
 
 

plt  
plt  
plt  
plt  
 plt

plot   errors  
x l a b e l   c  
y l a b e l    cross v a l i d a t e d e r r o r   
t i t l e    t u n i n g o f p a r a m e t e r c  
  s a v e f i g   t e s t  

  in      

fi