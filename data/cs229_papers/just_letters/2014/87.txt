cs    project midpoint
do a barrel roll
steven ingram  tatiana kuzovleva
december     

 

motivation

we implemented an algorithm to navigate
foxs arwing space ship from the start of level
one to the boss of level one without taking any
damage  the problem can be described as navigation of a  d space with only  d information  without actually knowing distance  the
craft must consistently avoid crashing into terrain  hitting buildings  trees  and must avoid
laser fire from enemy ships  fox ship is constrained to go forward at a constant speed and
so we can only vary its position by moving up  figure    sample screenshot
down  left  and right 
we implemented a reinforcement learning algorithm to train the pilot  fox mccloud  to nav   
supervised learning
igate a portion of the first level  the goal was
to have the ship not crash  we let our algorithm     classifying arrows and death
play the game over and over  making use of a
our reinforcement learning algorithm resimple reward function 
quired certain pieces of information to compute
the reward function for each state visited  this
information indicated whether starfox had died
  introduction
or had maneuvered too far left  up  or right 
in order to tell when the level had to be
the problem of navigating  d space with restarted  we considered the boost box  in only
only  d information is a common one  the diffi  two cases does the boost box disappear  either
culty in our problem lies in the large amount of if fox has died or if the boss has been reached 
information we must process during each run of
furthermore  if arrows appeared on the top
the algorithm and the large number of possible of the screen  for example  that meant fox has
states  we did not classify the objects in our field flown too far up and the reward for that state
of view in advance  nor did we access in game in  was decreased accordingly  not allowing starformation about the placement of objects in the fox to maneuver too far in any direction was a
 d game space  we only allow the algorithm to challenging constraint to meet because it would
process screenshot data 
force him to fly near terrain and buildings  but
 

fistill avoid them  the problem is much harder
than just allowing starfox to fly high up into the
corner of the screen where almost nothing will
collide with him 

either the exhaust of starfoxs engine was in the
region or if enemy laser fire was also in the region 
these situations would would cause our hypothesis to incorrectly guess if there was arrow when
there actually wasnt  fortunately  our ability
to identify if starfox died or not was over      
    data collection
percent accurate  with over        screenshots 
we gathered screenshots of both the boost  only   were misidentified 
box portion and each arrow portion manually 
for each classification problem  the boost box 
emulator controller
left arrows  top arrows  right arrows  we gathered  
over a thousand screenshots of positive and negwe automated reinforcement learning of
ative examples  our algorithm then considered
starfox
using a sequence of communication steps
only a cropped  relevant portion of each screenbetween
three applications  matlab  autohotkey 
shot as input 
and project    matlab processes the screenshots
taken by project   of starfox  computes which
    algorithms
next action for starfox to take  relays that information to autohotkey  and temporarily pauses 
for each set of arrows  the screenshot was
autohotkey continuously waits for a command
converted to greyscale and run through matlabs
from matlab to issue to starfox  sends the comedge function  only   in   of the      pixels from
mand using its sendinput function  and signals to
the cropped region were considered in training
matlab that the project   emulator has received
and classification  this resulted in each training
the command  once matlab is notified that the
sample having a dimension of      the output
previous command has been sent by autohotkey
for each training sample was manually set to be
it unpauses and is ready to issue another comeither   or    indicating whether one or two armand  the overall loop of the algorithm is as
rows were present  or no arrows were present in
follows  pause the game  take a screenshot  prothe cropped region  finally  stochastic gradient
cess screenshot for information  compute action 
descent was applied to the data to compute theta 
unpause game  issue action  and repeat 
the hypothesis we used for h   was the sigmoid
matlab and autohotkey communicated with
function 
each other using three files  two permanent contained the commands from matlab  the third
    results
was created and deleted by autohotkey to signal
matlab 
after computing theta and computing h  
for all the training examples  we determined a
threshold for h   for each case  with which to differentiate if there were arrows or not  or if starfox
had died or not  for determining if there there
left arrows or not  the dividing value for h   is
     for top arrows or not       and for right arrows        with these values we were able to
correctly identify all of the training examples as
having arrows or not and if fox had died or not 
while running our reinforcement algorithm 
our hypothesis for arrow identification were accurate over    percent of the time  the error in
arrow identification occurred consistently when figure    flow of information
 

fi 
   

reinforcement learning

of the image  and if that was above a certain
threshold  we registered the ship as hit 

data set

after data was collected from the project  
emulator  our software processed the screenshots  screenshots were cropped and then converted to gray scale  we used the matlab function edge to simplify the input to a         pixel
long logical vector  however  this still left a lot
of possible states to explore 

figure    red screen after collision

   

algorithm

because of the complexity of the state space 
we implemented a reinforcement learning algorithm to navigate the level  we ran into trouble
attempting to express the transition probabilities
because of the huge number of states available to
our system  therefore  we chose to implement
the sarsa algorithm which allows us to only
take into account the action we chose instead of
considering all actions available to the ship in
each state 
the sarsa algorithm allows us to consider
only two states at a time with the following algorithm 

figure    cropped screenshot

figure    grayscale image

initialize q s  a  as empty hashmap  
for each level of starfox do
fetch and process first screenshot 
for each screenshot do
choose action based on q 
do action a 
take new screenshot 
calculate the reward 
choose action a based on q 
update q s  a  
first screenshot   second
screenshot 
end
end

figure    post edge function
furthermore  we had to keep track of the
health bar  this was vital for our reward function  we compared the current health bar to previously gathered healthbar images to determine
how much health was left  we also registered a
hit to our ship by determining if the screen had
turned red  we considered only the red content
 

fihere  the actual update rule for sarsa is 

therefore the reward function can have a value of
a any time the health bar changes at all  this
reflects the fact that all changes to health are
bad  and should be penalized 
our reward function also penalizes the ship
when it moves away from the main path of the
level  fortunately  the game provides guiding arrows on the screen when you stray too far from
the path  therefore  our reward function takes
on a value of b whenever those arrows appear 
when both these conditions are met  i e  the
health bar changes and theres an arrow on the
screen  the ship has really messed up its navigation  and the reward function is evaluated at
 a   b   otherwise  the reward function is   

q s  a     q s  a     r   q s    a     q s  a  
unlike most other reinforcement algorithms 
sarsa doesnt require us to calculate the transition probabilities for our states  which grealty
reduces the time and space needed by our algorithm 

   

state space

the input for our algorithm is the processed
screenshot  because of how complex the game
world is  we can not simply boil this down to integer valued attributes  we considered using clustering and supervised learning algorithms to prep

the information from our screen capture to allow
a
starf ox hit



b
us to input integer valued parameters  however 
arrow appeared on screen
we decided to prioritize the reinforcement learn
 a   b  both health and arrows


ing aspect of our project as was suggested in the

 
otherwise
feedback to our proposal 
in order to speed up access time to our states
and action values  we implemented a hashmap     termination
in matlab  our first implementation used a maour algorithm terminated when either we
trix to store our state values  and we found that
reached the end of the level  or our health dipped
searching for states took too long  the hashmap
to zero and we lost the level 
implementation allows us to play the game in
we measured success of the flight by how
real time 
much health remains once the level is complete 
or how long we stayed alive for  if we did not
    actions
complete the level 
in the current implementation of the algorithm the possible actions to be taken in any state     results
are movements up  down  left  or right  these
we used the following constants without opmovements are implemented through the presstimization 
ing and holding down of the left  right  up  and
down arrow keys 
a  
in the future  we will expand this algorithm
b      
to include shooting and doing defencive maneu      
vers such as the barrel roll which allows the ship
to fly through enemy fire unharmed 
       

we found that starfox encounters a significant amount of states even when traversing even
our reward function penalizes both a loss of small portions of the level  the graphs below
health and maneuvers that take the ship too far show in even one simple area of the level  that
away from the main path  we assume that no in every single run of the level  new states were
maneuvers increase the health of the ship  and encountered 

   

reward function

 

filearning algorithm to navigate the first level of
starfox in its entirety 

 

future work

we still hope to find a better way of storing states  as things stand  too much memory is
required to store all possible states  even more
importantly  a single pixel difference between two
screenshots can cause us to classify the state differently  this requires the algorithm to play starfox many times before it starts learning  fox
simply does not see states more than once often
enough  a different approach to classifying states
from screenshots would be worth considering 
we would also like to add more possible actions for starfox  specifically the famous barrel
roll that allows the pilot to avoid incoming laser
fire  other actions like the ability to shoot enemies can be added as well 
the reward function could be expanded to
reflect the existence of buffs and save points in
the game 

figure    number of new states encountered in
each level
one reason our algorithm encountered a new
state at almost every step was due to the fact
that communication with autohotkey combined
with project   emulator behavior is not deterministic  sending the same command in the same
state over hundreds of iterations still resulted in
hundreds of new states  in our algorithm  despite
earlier efforts to reduce the state space as much
as possible without resorting to feature classification  all it takes is one pixel out of        to
merit creation of new state action vector 
in the small fraction of steps when a state
had been visited before  such as in the initial
state or much less often  a state later along the
level traversal  our algorithm would look up the
refrences
reward obtained and add that value to the ap   
propriate state action pair value  in only a few
kaelbling  leslie pack  micheal l  littman  and
of the over        states visited was the ship able
andrew w  moore  reinforcement learning 
to try moving in all four directions 
a survey  journal of artificial intelligence research                   web    oct       

 

conclusion

liao  yizheng  kun yi  and zhe yang  reinforcement learning to play mario          n 
pag  web    oct      

we applied both supervised learning techniques and a reinforcement learning algorithm
to attempt navigation of the first level in the
videogame starfox     we had high success rate
of determing the level of health of fox  which arrows were on the screen if any  and if fox had
died or completed the level 
ideally  we will optimize for the various reward constants in our algorithm and determine
whether it is possible for a simple reinforcement

eden  tim  anthony knittel  and raphael van
uffelen  reinforcement learning  algorithms 
unsw  n  d  web     oct       
ng  andrew  cs     lecture notes  part ii 
classification and logistic regression        
web     oct       

 

fi