sentiment analysis using semi supervised recursive autoencoders
and support vector machines
bahareh ghiyasian and yun fei guo
december         

 

 

introduction

dataset

our dataset     consists of      sentences which is
converted to        english phrases from movie reviews  we split our dataset randomly into a training set and a testing set  so that     of the phrases
are in the training set and     are in the testing
set  our goal is to classify the phrases in the testing set into five categories  very negative  negative 
neutral  positive  and very positive   all phrases
from the dataset  both training and testing  are labelled with integers from   to    where   indicates
very negative and   indicates very positive 

sentiment analysis is an important field that aims
to extract opinion or other subjective information
from sources such as text  with the advent of
social networks  blogs  reviews  and online shopping  sentiment analysis has garnered significant attention from both the industry and the research
community     being able to accurately identify
sentiment from online text allows businesses to better understand their customers  it gives businesses
invaluable insight about what were the pain points
in their products and services  what potential improvements can they make on their products and
services  and etc  because of its potential to obtain
high level insight from large amounts of data  sentiment analysis has applications in various domains
     sentiment analysis can be used to summarize
user reviews on review related sites      it can be
used as sub component of software systems such as
question answering systems  in organizations such
as business and governments  it can also be used for
reputation management and public relations     

 

features and pre processing

for the svm the features are the count of each word
in each phrase  for the neural network  the features
for each phrase are a vector that is generated by
the recursive autoencoder and encodes the semantic
and syntactic information of the phrase 

 

past work on sentiment analysis has often focused
on classifying text into two classes  positive and
negative      some recent work     has tried to classify text into more than two classes  in our experiments  we will classify data into five classes  very
positive  positive  neutral  negative and very negative  we will make use of the sentiment analysis
dataset on kaggle      which contains phrases and
sentences from rotten tomatoes movie reviews 

   

models
support vector machines

one algorithm we used on the dataset is support
vector machines with linear kernel  we used the
package liblinear to train and test our dataset 
to prepare our data  we found a list of all unique
words in our dataset  we then created a matrix
a  where each row is a training sample  a phrase 
and each column is a unique word  for each row in
 

fithe matrix  we have the count of each unique word
in that particular phrase  initially we made our
features case insensitive so that both upper  and
lower case forms of a word would map to the same
feature  but we found that this didnt give us good
results  also  we tried using the unique words from
    as our features but we found that this didnt give
us good results  so instead we used the original
words with their case information as features  the
number of unique words or features is       
because we have five label classes  we cannot run
a simple svm  instead  we ran one vs all svm  so
for each label class  we let its training and testing
samples be the positive class and all other training
and testing samples be the negative class  we then
run linear svm on the dataset and the modified labels 

   

evaluation 
 

   

where w     is a  n by n matrix and b    is a  nby   vector  the goal of the autoencoder is the
minimize the reconstruction error 
erec   c    c      


 
   
 

 c    c      c    c   
 

   

for each pair of adjacent words in the phrase  e g 
x  and x  followed by x  and x     we calculate the
reconstruction error  we pick the pair with the lowest reconstruction error and replace both word vectors with their code p  we then perform the same
reconstruction error calculation on the new list of
word vectors again and find the pair with the lowest error  we continue in this greedy fashion until
there is only one vector left in the list  this vector
is the code that represents the phrase 

semi supervised recursive autoencoders

another algorithm that we used was semisupervised recursive autoencoders      the algorithm consists of an unsupervised part and a supervised part  the unsupervised part is essentially
a recursive autoencoder that creates a code or
an n dimensional vector that represents the phrase 
to do this  each phrase in the training set is converted to a list of n dimensional vectors where each
vector represents a word  in our experiments  we
build each vector by drawing each element in the
vector randomly from a uniform distribution in the
range of                we also choose n to be    
although n can be any positive integer  for each
pair of adjacent words in the phrase  we calculate
the following value 
p   f  w      c    c      b     

 

 c    c      w     p   b   

after obtaining an n dimensional code for each
phrase from the unsupervised recursive autoencoder  we wish to use supervised learning to classify
this vector into a particular class  many standard
supervised learning algorithms such as naive bayes
and support vector machines can be used here  in
the case of multi class classification  one can choose
to use softmax regression  in our case  we chose to
run one vs all logistic regression  for each of the
five label classes  we label its training and testing
examples as being the positive class and all other
training and testing examples as being the negative class  we then ran logistic regression on this
data to determine what percentage of the testing
examples were correctly classified 

   

where w     is an n by  n matrix  c  and c  are
vectors corresponding to each word in the pair  and
b    is an n by   vector  the function f is an activation function such as the sigmoid function or
tanh  in our case  we used tanh 
after computing p  we want to obtain the original
word vectors from p by performing the following

 
   

results
support vector machines

for the linear svm we have the following results 
 

fi 
positive class

accuracy

very negative
negative
neutral
positive
very positive

     
     
     
     
     

number of correct
labels
     
     
     
     
     

   

semi supervised recursive autoencoders

for the semi supervised recursive autoencoder we
have the following results 

positive class

accuracy

very negative
negative
neutral
positive
very positive

     
     
     
     
     

impact of data format

in our experiments  the neural network has a
slightly lower performance than the svm  we hypothesize that this may not be the result of the neural network being less effective in sentiment analyses  when we manually scanned the training and
testing examples  we found that many of the labels didnt make sense  for instance  the phrase
introspective and has the label positive  while
the phrase introspective has the label neutral 
also  the example is worth seeking   has the label positive while the example is worth seeking has the label very positive  our theory is
that the svm works better for examples similar
to the latter because it can easily recognize that
a dot contributes to increasing the positivity scale 
the neural network is more proficient in recognizing the semantic and syntactic relationship within
the phrase  and therefore would be weaker in recognizing one off cases like these  given that the
dataset labels are hard to understand even by human standards  it is fair to say that the neural networks worse performance on this dataset is actually
an indication that it is a more effective algorithm
overall 
another reason why the neural network performed worse might be because of the size of each
phrase  the dataset used in the paper     contains
training and testing examples with large amounts
of text  our dataset consists of phrases that were
in comparison much shorter in length  we hypothesize that the neural network works better when the
size of the phrases sentences are large  the figure
  shows the distribution of the size of each phrase
in data set for different labels 

the first column indicates the class that was used
as the positive class in the one vs all svm  so the
data for the first row results from treating training and testing examples with label very negative
as   and all other training examples as    the second column in the table is simply the percentage of
testing examples that were correctly labelled  the
third column is the actual number of testing examples that were correctly classified  the total number of testing examples is        the average accuracy of all five classes is        it took about    
iterations for the svm to converge 

   

discussion

number of correct
labels
     
     
     
     
     

   
the table has the same format as svm table 
the average accuracy of five classes is
       the maximum iteration was     and  
sub models reached the maximum  very positive
model stoped at    

improvements on support vector machines

the svm can be improved in several ways  in our
experiments we used a linear kernel we can try using another kernel such as the gaussian kernel in
future experiments   however  since the size of the
 

fifigure    distribution of the size of each phrase in words in training and testing data sliced by the label 
is more similar than the pair of words apple and
car  to obtain this information  one can use pretrained word vectors such as     that are trained
over a large corpus of text such as google news articles or freebase  words that tend to co occur in
the corpus would have vectors that are very close
to each other in terms of distance 

feature set        words  is large  we doupt that
kernel could improve the performance   we can also
try to adjust the regularization parameters  we
used a large number of features  it is possible that
many of these features were not effective in predicting sentiment  therefore one can perform forward
search or other algorithm to select only the features
that are useful  we can also spend some time trying to develop meaningful features that correlate
strongly with certain sentiment labels 

   

one thing we noticed while running the neural
network is that for several of the label classes the
program did not terminate until the max number of
iterations      was reached  therefore it is possible
that if we set the max iterations variable to be a
higher number  the neural network would perform
better 

improvements
on
semisupervised recursive autoencoders

we believe that if we had used softmax regression
instead of one vs all logistic regression  the neural
network performance would have been better because the labels are mutually exclusive  this is
because softmax regression gives a probability distribution over the set of label classes  the sum of
probabilities will be     which means that theres a

in our method  we used randomized numeric vectors of dimension     while they are sufficient for
giving us decent results  they dont capture the fact
that some words are more similar to each other in
terms of meaning or the concepts they represent 
for instance  the pair of words apple and orange
 

fidefinitive answer as to whether a particular example is from a particular class  class with the largest
probability   however  in one vs all logistic regression it is possible that more than   sub model return true for the same sentence  like both poitive
and very positive would return true for the same
sentence  in this case  there is at least one error  so
one vs all logistic regression introduces unnecessary
errors 
another thing that one can try is to alter the
embedding size  or the size of the word vectors  in
our experiments we used     but there is no reason why this should be the number  there might
be other embedding sizes that yield better results 
similarly  one can try to tweak various parameters
in the recursive autoencoder to see if one can yield
better results  for instance  one can try to alter  
the parameter that controls the relative weighting
between the reconstruction error  error of the unsupervised part  and the cross entropy error  error
of the supervised learning part 
we could try increasing the training dataset size 
because we believe recursive neural network is a
more complex algorithm  non linear   it needs more
training data to reach a certain upper bound on the
error with high probability  therefore  we hypothesize that if we increase the size of the training data 
the marginal improvement of neural network would
be more than svms 

ysis has been to categorize text into groups that
range in posivity      few works have been done to
classify text in terms of subjective categories that
are not positive or negative  it would be interesting for future study to try to classify text into categories that have varied meanings  for instance 
for online forums that address customer concerns 
one may try to classify the posts as feature request 
software bug  account issue  payment issue  and etc 

 

    r  socher  j  pennington  e  h  huang  a 
y  ng  and c  d  manning      b  semisupervised recursive autoencoders for predicting sentiment distributions  in emnlp 

conclusion
work

and

references
    a  l  maas  r  e  daly  p  t  pham  d  huang 
a  y  ng  and c  potts        learning word
vectors for sentiment analysis  in acl 
    b  pang and l  lee        seeing stars  exploiting class relationships for sentiment categorization with respect to rating scales  in acl  pages
       
    b  pang and l  lee        opinion mining and
sentiment analysis  foundations and trends in
information retrieval              
    google 
     
word vec 
https   code google com p word vec  
    kaggle        sentiment analysis on movie reviews  https   www kaggle com c sentimentanalysis on movie reviews 

future

on our dataset  the neural network performs
slightly worse than support vector machines  based
on our discussion  we believe that this may not be
an indication that the neural network is a less effective algorithm  but that its lower performance
might be due to the nature of the dataset  more
rigorous work in the future will be needed to determine under what conditions are neural networks
more effective than conventional algorithms and by
how much 
lastly  much of the work done in sentiment anal 

    r  socher  a  perelygin  j  y  wu  j  chuang 
c  d  manning  a  y  ng and c  potts       
recursive deep models for semantic compositionality over a sentiment treebank  in enmlp 
    stanford 
     
ufldl
tutorial 
http   ufldl stanford edu wiki index php ufldl tutorial 

 

fi