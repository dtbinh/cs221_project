collaborative filtering recommender systems
 rahul makhijani  saleh samaneh  megh mehta
abstract   aim to implement sparse
matrix completion algorithms and principles of
recommender systems to develop a predictive
user restaurant rating model  in particular  we
implement the two primary forms of
collaborative filtering   neighborhood and
latent factor models to our yelp data set  in the
k neighborhood models  we are looking to
exploit the similarity of our items and users so
as to group them in clusters and make
predictions based on the available ratings 
contrastingly  in the latent factor model  we use
matrix factorization techniques to derive the
hidden similarity layer between the users and
the items  all our model comparisons and
evaluations are done using the rmse metric 





description and evaluation of our kneighborhood and latent factor models
against the initial baseline model 
conclusion and future prospects 

   data set description
we implement our various models on the
yelp dataset publicly available online for all at 
http   www yelp com dataset challenge  it contains
actual user  business and users  review data from
phoenix  las vegas  madison  waterloo and
edinburgh  to put into context  we have data for   
    businesses          users with          
reviews  the data was given in the following format 

   introduction
high quality recommender systems are the
main drivers for success of many online services such
as netflix  yelp  ebay and amazon among others 
the   star rating parameter in yelp is a critical
parameter in determining whether a user will click to
find out more about a particular business  or just
scroll on  in fact  a harvard study states  a one star
increase in yelp rating leads to a      increase in
revenue   hence it is apparent why there is such a
strong commercial incentive for good user
recommender systems  they have a direct impact on
a growing number of businesses 
over the years  yelp has managed to greatly
increase the amount of clientele information in its
database and our goal is to evaluate the predictive
performance of various collaborative filtering
techniques on this data set  this paper will take on
the following structure 



short explanation of the data set used for
testing our recommendation systems 
brief discussion on the performance metric
used in our model evaluation 

for the purpose of this paper  we essentially
make use of the review json and business json files
from which we extract the relevant data in order to
create our user business ratings matrix 

despite the large amount of data available to
us  as we notice in the graph above  the distribution
of reviews per user is heavily skewed to the low end
and so this presents a challenge to our collaborative
filtering methods that rely heavily on a high number
of shared ratings between users or restaurants in
order to create reliable vector based similarity
metrics  to a certain extent  we try to circumvent this

fiproblem by restricting our attention to restaurants in
the city of las vegas only  this gave us a slightly
denser matrix on which we could better perform our
collaborative filtering techniques 

instead of just taking the means  this is where we
would look to incorporate our k nearest neighbor
collaborative filtering methods  which exploit any
similarity existing between users items 

   error metrics

there are many possible ways of measuring
the similarity between users items and in this case the
pearson r correlation would be the most reliable
metric 

the error metric we look at throughout this
paper is probably the most popular one when
evaluating recommender systems  the root mean
squared error is a prediction accuracy metric that tries
to answer the question of how close the prediction
ratings are to the true ratings  it uses squared
deviations and so larger errors tend to get amplified 
n

 p  a  
i

rmse  

similarity between users u   v
sim u v   

 

 



 r  r u   rv i  r v  
iu v u i



 r  r u     rv i  r v   
iu v u i

 

 

 

 

i

i  

n

pi   predicted rating
ai   actual rating

we evaluated all of our models through a  cross validation rmse  where we train the model on
    of the data and then test its accuracy on the
remaining     

   predictive methods
given our user business ratings matrix for
restaurants in las vegas  we aim to fill in the missing
ratings in our sparse matrix  we will look at both
nearest neighbor and matrix factorization methods in
order to assess their predictive performance 
however  since most collaborative filtering methods
start off with a simple baseline model  we will begin
by implementing the following model to our data 

rui      u   i
   global average rating
u   user bias
i   restaurant bias

r u   mean rating user u
ru i   rating user u assigns to item i

for example  the jacard distance metric would
not be the most effective  as it wouldn t take into
account the   to   ratings assigned to each restaurant
by each user  once we calculate the pearson r
similarity correlation metric  we make clusters of the
k most similar users items 

method    user user k neighborhood model
in order to implement this initial method  for
any given user i  we begin by calculating the
similarity between this user and all other users  then
we select its k nearest neighbors based on this
similarity measure and finish by computing the
predicted rating for i based on a weighted
combination of the k nearest neighbor ratings 




pu i   r u

 s u u    r  r
 
   s u u     
u   i

u 

 

u  n

u  n

pu i   user u s prediction for restaurant j


r u   mean prediction for user u
s u u      similarity between user u and user u 

in this model  we take into account the mean
rating given by the user and the mean of the
restaurants in order to remove the presence of any
possible bias when predicting   i e  a user has the
tendency to rate everything higher than it s  true 
rating   we get an rmse of       on our predictions 

this model gives us an rmse of       on our
predictions  which is a slight improvement on our
baseline model 

an issue we encounter is notably with
outliers  such as very low  sandbag  ratings that bring
the average down  especially when dealing with
sparse data such as ours  one solution to this problem
would be to assign weights to the different ratings

issues we encounter while running this model
is that at times we find users with no ratings history
except for the one rating we are trying to predict  or
we find users whose neighbors had never rated the
restaurant for which we want to predict  this model

ru   i   rating of user u  for restaurant i
n  neighborhood of user u

fialso suffers from scalability problems  as the user
base grows  hence  it might be more beneficial to
focus our attention towards the item space instead of
the user space 

baseline estimate from the ratings matrix  we get a
normalized matrix to which we just need to apply the
similarities 

method    item item k neighborhood model
itemitem cf uses similarities between the
rating patterns of items  if two items tend to have the
same users like and dislike them  then they are
similar and users are expected to have similar
preferences for similar items  again  as in the user
case  we look at the k nearest neighbours of any
given restaurant from the set of restaurants each user
has already rated  using the pearson r correlation
similarity metric and then take a weighted average of
the selected k restaurants to compute a prediction for
the rating of the targeted restaurant 

pred mn  



sim m  j r ju



  sim m  j  

jnuk  m 

we get an rmse of        which is a minor
improvement compared to our previous model 
we must note that in this method it is possible
for some of the ratings averaged together to compute
the predictions to be negative after the weightings  as
the similarity scores could be negative whilst the
ratings are constrained to be non negative 

jnuk  m 

nuk  m     j   j belongs to the k most similar restaurants m that user u has rated 
pred mn   prediction of restaurant m by user u
sim m j    adjusted pearson correlation for restaurants m and j
we get an rmse of       which is a marked
improvement compared to the user user kneighborhood model 
considering the item space rather than the
user space takes care of several crucial problems 
namely  the search for similar users in the highdimensional space of user is computationally more
expensive and restricting our attention to the itemspace takes into account the problem of new user
profiles having comparatively fewer ratings for
businesses  however  sparseness of the matrix is a
recurring challenge we encounter  since the reliability
of vector based similarity matrices depends on the
number of shared ratings 

method    item item collaborative filtering
 baseline model included 
this method is identical to the previous one
except for the fact that we include the baseline model
this time  we manage to correct the negative
predictions by averaging the distance from the
baseline predictor  it also helps extend collaborative
filtering to large user bases  since we subtract the

above  we see a summary of the rmse
results for various k neighborhood parameters when
training and testing for the item item collaborative
filtering model just discussed  we observe that the
optimal rmse is achieved for a neighborhood of    
however  given the sparsity of our ratings
matrix  it would be interesting to have a closer look at
some matrix factorization techniques  recommender
systems rely on many different types of input data

fiand since our  explicit feedback   ratings given by
users for the various businesses  is sparse  it would be
useful to implement matrix factorization algorithms 
which would infer user preferences using  implicit
feedback   the intuition behind using matrix
factorization to solve this problem is that there should
be some latent features that determine how a user
rates an item  for example  two users would give
high ratings to a certain restaurant if they both like
that type of food  or if the restaurant has a nice
atmosphere for example  hence  if we can discover
these latent features  we should be able to predict a
rating with respect to a certain user and a certain
item  because the features associated with the user
should match with the features associated with the
restaurant  basically  these models assume a
similarity layer between users and restaurants is
induced by a hidden lower dimensional structure
latently present in the data 

method    stochastic gradient descent
 svd 
at first we were looking to find the singular
value decomposition in order to perform matrix
factorization on our ratings matrix  however this
wouldn t be a very good idea as we have a very high
number of unknowns in our matrix and computing
the svd would require us to set these unknowns to   
this would hamper our predictions  as a user not
having a rating for a given restaurant would be
equivalent to them giving the restaurant the lowest
possible rating  hence  instead of directly factorizing
our ratings matrix  we can use stochastic gradient
descent in order to derive a matrix p containing the
latent factors of the user and a matrix q containing
those of the restaurants without having to set any of
the unknown entries to    the algorithm for this
method is given below 
to get the prediction of a rating of an item q j by p i   we can calculate the
dot product of the two vectors corresponding to q j and p i  
rij   pit q j  fitted rating 
 

eij     rij  r ij     squared error 

however  one of the issues with this method
is that it can lead to some serious over fitting of our
data and in order to solve this problem  we can try
and regularize our svd stochastic gradient descent
method 

method    regularized stochastic gradient
descent  svd 
in this method  we add a new regularization
parameter to the model above in order to avoid overfitting the data  in order to learn the factor vector pi
and q j   the system minimizes the regularized squared
error on the set of known errors  the system learns
the model by fitting the previously observed ratings 
so as to predict future unknown ratings  the constant
 controls the extent or regularization as we try and
regularize the learned parameters whose magnitudes
are penalized 
 

eij     rij  r ij     

 k  
 
  p   q   where p and q give a good approximation of r 

  k  

the new update rule is as follows 
pik   pik      eij qkj   pik  
qkj   qkj      eij pik   qkj  

our new rmse goes down to       as we get
our best result till now  we see below that the number
of iterations before which our regularized svd model
minimizes the rmse is     here the ideal number of
latent factors would be    
     
     

regularized  svd  results  

     
     
     
r
m
s
e  
     

fatcors     
fatcors     

we want to minimize the error and so using stochastic gradient descent  we      
get the following update rule 
p ik   p ik     eij qkj

     

qkj   qkj     eij p ik

     

fatcors     

   

this method greatly improves our predictions
with our rmse going down to       

                                                                              
no   of  steps  

fiin order to improve our predictions  we add biases to the regularized svd
model  one parameter ci for each user and one d j for each movie 
 

rij   ci   d j   u tiv j  improved rsvd model 
we train the weights ci and d j are trained simultaneously
with uik and v jk  
ci   ci      rij     ci   d j  global   mean  
d j   d j      rij     ci   d j  global   mean  

 is the learning rate
 

rij is the predicted ratings matrix
this method gives us a slightly improved rmse of       on our predictions 

method    alternative least squares
the previous two methods lead us to the
alternative least squares optimization problem 
which works in the same way as the svd stochastic
gradient descent algorithm excluding the fact that it
keeps rotating between fixing the qkj and the pik  
since the system computes each q i independently of
the other item  restaurant  factors and computes each
p j independently of the other users and other user
factors  this gives rise to the possibility of massive
parallelization of the algorithm  also  since the
training set is large and sparse  instead of looping
over every single training case like gradient descent 
als provides a more computationally cost efficient
method  however  we see here the als gives us an
rmse of        which is poorer than our previous
svd models because at times stochastic gradient
descent converges faster and is easier to compute 

surprise that the matrix factorization techniques
performed a lot better than the vanilla collaborative
filtering ones  our best result was achieved  in our
improved rsvd model  interestingly  the alternating
least squares method performed worse than the
rsvd  however  we must note that the als is only
more efficient in certain cases when parallelization is
required or when stochastic gradient descent is very
slow 

   future
the two main issues faced by collaborative
filters were the sparseness of data and the cold start
problem  item cannot be recommended unless it
hasnt been rated before   given the amount the data
available to yelp  a possible solution would be to
combine collaborative recommendation systems with
content based filters to exploit the information of the
items already rated  on its own  content based filters
wouldn t provide great insight as collaborative filter
have certain advantages over them      they have the
ability to provide recommendations for items both
relevant and non relevant to the user s profile     they
can also perform in domains where not much content
is available about the users items  a  content boosted
collaborative filtering model  would help us get the
best of both methods  also  implementing a
combination of our models should help improve our
predictions  rmse 

   references 


   summary of results



model

rmse

baseline

     

baseline   user  user cf   knn

     

item  item   knn

     

baseline  item   item cf   knn

     

svd with stochastic gradient descent

     

regularized svd  rsvd 

     

improved rsvd

     

alternating least squares

     

the results we got were in line with what we
expected to get  as the different collaborative filtering
techniques performed better than the baseline model 
also  given the sparseness of the data  it was no










recommendation system based on collaborative
filtering   zheng wen
improving
regularized
singular
value
decomposition for collaborative filtering   arkadiusz
paterek
data mining methods for recommender systems xavier amatriain  alejandro jaimes  nuria oliver  and
josep m  pujol
mining of massive datasets   stanford lectures
matrix factorization techniques for netflix  koren
collaborative filtering recommender systemsmichael d  ekstrand  john t  riedl and joseph a 
konstan
content boosted collaborative filtering for
improved recommendations   melville and mooney
and nagarajan
http   officialblog yelp com         harvard studyyelp drives demand for independent restaurants 
http   ucersti ieis tue nl files papers   pdf
collaborative filtering models for recommendations
systems  nikhil johri  zahan malkani  and ying wang

fi