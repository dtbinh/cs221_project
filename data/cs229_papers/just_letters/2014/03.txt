classification of bad accounts in credit card industry
chengwei yuan
december         
introduction
risk management is critical for a credit card company to survive in such competing industry  in
addition to operational expenses  provisional loss is a major driver to a companys expense  the
provisional loss arises due to the bad accounts booked  bank lends the money to customers
who eventually do not have capability to pay back  in the risk management  there are generally
two stages a company can take to manage and control credit risks  the first stage occurs when
booking a customer  an aggressive underwriting strategy could book and approve high risk
population who seeks for credit card  while a conservative policy may only focus on upmarket
and affluent population  as expected  the first strategy could generate both high revenues
 interests charged  and high expenses due to bad accounts booked  resulting in trivial incremental
net income  and the second strategy could generate not only low revenues but also low losses 
resulting in incremental net income be trivial as well  there is always trade off for different
strategies in terms revenue generation and loss control  finding an optimal strategy is often
difficult and needs to be adjusted accordingly due to internal or external factors such as macroeconomic change  for example  almost all credit companies suppressed their approval rates for
high risk segments of population and incurred huge financial loss during           economic
depression  the second stage happens in customer management after the customer is booked 
although booked customers pass the first screen of risk control  the chance of false negative
 false good accounts  could still be high  however  in the second stage  by leveraging their
performance such as credit card utilization  payment information  risks can further be managed to
control provisional loss  in our project  we will focus on the second stage of risk management 
and particularly are interested in classifying if a booked account will be a bad account within
   months since booked  since an internal classification model is already available  our second
interest is to train a better classifier to outperform the benchmark model 
dataset
data are not publicly available  there are three sources for both training and test samples  credit
bureau data  from one of the largest three bureaus transunion  experian or equifax   consumer
purchase behavior data  internally summarized purchase information  and customer experience
data  information about how customers use digital like website or mobile on their accounts   the
credit bureau and customer experience data contains   statements  from current snapshot
statement  of historical observation  and consumer purchase behavior data have summarized   
statements historical observation  there are three vintage snapshots for training sample 
august       k accounts   march      k accounts  and june       k accounts   for the test
sample  the snapshot comes from more recent vintages on january    k  and march    k    
year data  the target variable is dichotomous with value   or   indicating if an account is bad or
not  the bad definition indicates that an account is either delinquent or charged off in the future
   months from current snapshot statement  the benchmark score  in the format of probability
score  is also provided  in each data source  the unique account id serves as the key index for
combing all datasets 

 

fifeatures and preprocessing
features
there are about     features per snapshot in the credit bureau data  resulting in     x        
features  and    features per snapshot in the customer experience data  resulting in    x        
features  and     features in the consumer purchase behavior data  therefore  there are a total of
    features in the raw training test samples 
preprocessing
features selection
since there are many available features in our samples  we decide to perform variables selection
first before we train the models  as there are three sources of dataset  we choose to conduct
variable selection separately for each of them  for credit bureau data  we only choose the current
snapshot features      features  in variable selection while creating additional trend features as
described in the next section  for customer experience data  we summarize the statement wise
features by either taking the max indicator features  or mean continuous features   thus reducing
the features from     to    before the variable selection processing  the formula below gives the
definition of customer experience data summarization 
 

 

 

 

 

where
represents the current snapshot feature  and
statement features 

and

indicate the last two

we use the treenet function in software spm v     salford systems  san diego ca  to perform
quick variable selection  we also create an additional random variable  uniformly distributed  in
the training sample and include it in variable selection  in the treenet output  the variables are
ranked by their predictive importance to the target  we apply two rules to determine the variables
selected  first  for credit bureau variables  we aim to select at most top    variables  while for
purchase behavior and customer experience data  we aim to select at most top    variables 
based on our understandings  bureau variables generally have stronger predictive power to risks
than the other two types of data  thus we decide to keep more variables for bureau data  second 
only variables that are ranked above the random variable will be kept in the selected variables list 
we believe that any variables that are ranked under the random variable are more likely random
noises  by applying rules above  we select a total of    features for model training purposes    
features from bureau data     features from transaction and    features from digital data 
additional features creation
as described above  for bureau data  we only choose the current snapshot features into the
variable selection  however  we also think the trend of some features may have incremental
values to predict risks  for example  the outstanding utilization trend  outstanding balance divided
by credit line   the payment ratio trend  payment divided by last statement outstanding balance  
the fico trend and etc  we also create some indicators based on previous statements information
such as if an account has made purchase or not  or if an account has made any payment or not and
etc  as a result  we create a total of additional    features 
therefore  there are a total of    features in our dataset for training purposes 
 

fifeature treatment
we apply simple missing treatment and capping flooring to features selected  if a feature has
missing values  the missing observations will be imputed by medians  and the feature is capped
by its   th percentile and floored at  st percentile 
models
in this project  we test four classification machine learning models  which are implemented in
sas      sas institute  cary nc   salford predictive modeler  spm v     and e     package of
the r tool  r core team         logistic regression  stochastic gradient boosting  random forests 
and support vector machine  svm  
the logistic regression  mccullagh and nelder        is a popular method in banking industry 
particularly in credit scorecard development  thomas        siddiqi         in a binary
 
  as a sigmoid function of
classification problem  the logistic regression models the  
 
 
input features by  
  the classification can be determined by the
 

 

   
decision boundary  
  where is a pre specified threshold  generally
  
the logistic regression is easy for interpretation  but is usually limited by its constraints on
capturing nonlinear relationships 
the gradient boosting method was proposed by friedman        and was later improved to
stochastic gradient boosting by using the bagging procedure  the purpose of boosting methods is
to sequentially construct a sequence of weak classifiers and then ensemble them through a
weighted majority vote to produce the final prediction 
the random forests method was introduced by breiman         it is an ensemble method to
improve the variance reduction of bagging by reducing the correlation between the trees  without
increasing the variance too much  the de correlation is achieved via the random selection of
variables at nodes when growing trees  random forests method has several advantages such as its
capability of capturing the non linear boundary between event and non event  naturally embedded
out of bag validation  no special treatment to input features  suitable for unbalanced data
classification and etc 
the svm  cortes and vapnik        classifier generally transforms the input attributes into a
high dimensional feature space by introducing a mapping  linear or non linear  via a kernel 
when training the classifier  svm only uses the related support vector points in feature space to
find the optimal separating hyperplane  one popular kernel choice is the gaussian kernel
 

 

 

  

  

   which represents less parameters than other kernels  for a binary

classification problem  the probability output can be generated by
 

   

 

 

 

 

before fitting the svm model  the input data is first standardized to a zero mean and one standard
deviation  also  we enable the probability   true in the svm   function of e     package to
obtain probability type scores 

 

fithe model performance is compared by roc and auc  area under roc curve   bradley       
on the test sample  the classifier that results in largest auc will be treated as the best one  our
model will also be compared to the internal developed model to show the incremental
improvement 
results and discussion
results
we calculate areas under roc curve on the test sample after four models are trained  the aucs
from benchmark and four models are provided in table    since random forests method generates
the largest incremental values to the benchmark  we also provide the rocs as shown in figure   
table    the aucs of benchmark model and new models
models
logistic regression
svm
stochastic gradient boosting
random forest  rf 

auc
benchmark
new
model
     
     
     
     
     

  increase of
auc to
benchmark
     
     
     
      

rf vs other
models
     
     
     
 

figure    roc curves of benchmark and random forest models 
left  benchmark  right  random forest

discussion
the obtained results for the four models are shown in table   in terms of auc metric computed
on the test sample  the best result is achieved by the random forest model which outperforms
logistic regression  improvement of         svm  improvement of        and stochastic
gradient boosting method  improvement of         while all models can beat the benchmark
model  it is expected that all models could beat the benchmark  since the benchmark model only
applied the logistic regression on credit bureau data  the new logistic regression model has
smallest incremental value in terms of auc to the benchmark  while svm  stochastic gradient
boosting and random forest have substantial improvement to the benchmark 
we also check the top    variables that have most predictive power to forecast the likelihood to
be charged off  they follow into three categories  balance utilization  fico and the carried
 

fitotal highest balance  all of which are business intuitive  for example  the higher utilization
indicates the higher risk to be bad account  high fico score means the low risk population  and
carrying higher balance explains the customers tend to revolve their balance and are lack of
capabilities to pay off and thus are more risky 
although the sophisticated machine learning methods can beat the benchmark  logistic regression 
in terms of model performance  it may give rise to a problem with respect to model
implementation and interpretability  the credit card industry is highly regulated by occ  office
of the comptroller of the currency   so companies are required to provide transparency of model
structure and interpretation to regulators  however  random forest and stochastic gradient
boosting  as ensemble methods  are quite complicated as trees are ensembled together and thus
encounter difficulties for interpretation  in addition  their implementation into the internal
production system could also be challenging 
future
we have been able to apply machine learning techniques to train better models that can
outperform the benchmark model  however  considering the difficulties of model interpretability
and implementation  in the future  we can do more researches on 
   find creative solutions to provide enough transparency of ensemble methods to
regulators and enable implementation in the production system 
   improve logistic regression under the framework of gradient boosting methods so that the
trained model can have both improved model performance and clear interpretation 
reference
    bradley  p  a          the use of the area under the roc curve in the evaluation of machine
learning algorithms  pattern recognition                  
    breiman  l          random forest  machine learning              
    cortes  c  and c  vapnik          support vector networks  machine learning                
    friedman  j  h          stochastic gradient boosting  technical report  dept  of statistics 
stanford university 
    mccullagh  p  and j  a  nelder          generalized linear models  second edition  london 
chapman and hall 
    r core team          r  a language and environment for statistical computing  r foundation
for statistical computing  vienna  austria  available at http   www r project org  
    salford systems  available at http   www salford systems com products spm 
    siddiqi  n          credit risk scorecards  developing and implementing intelligent credit
scoring  john wiley   sons  inc  hoboken  nj 
    thomas  l  c   d  b  edelman  and j  n  crook          credit scoring and its applications 
siam  philadelphia  pa 
 

fi