predict influencers in the social network
ruishan liu  yang zhao and liuyu zhou
email  rliu   yzhao    lyzhou stanford edu
department of electrical engineering  stanford university

abstractgiven two persons and their social network
features  our job is to predict which one is more influential 
in our project  we collect training samples from kaggle
based on human judgement  we use several different
models to make predictions  such as logistic regression 
svm  naive bayes and neural network  we also use some
auxiliary techniques like cross validation  feature selection
and data preprocessing  in the results section  we compare
the performances of different models and provide analysis
and suggestions for future works  we implement different
learning models using matlab 

     followers
     mentions received
     retweets sent
      network feature  

     followings
     retweets received
     posts
      network feature  

    listed
     mentions sent
    network feature  

since we have two persons in each data sample 
we have    features in total  whats more  there is a
binary label representing a human judgement about
which of the two individuals is more influential in
each training sample  label   means a is more influential than b  label   means b is more influential
i  i ntroduction
than a  there are      training samples and     
in recent years  social network plays an increas  testing samples in our dataset  given a test sample 
ingly significant role in our daily lives  we share our our job is to predict which individual in this test
experiences and opinions with our friends on face  sample is more influential 
book  twitter  instagram and so on  when youre
b  data preprocessing
browsing your friends posts  you may find that
before applying different models on our data  we
some of them are more influential than others and
we call them influencers  according to the research may want to preprocess it first  for linear models 
in sociology  influencers have a great impact on the hypothesis function has the following form 
other peoples lives because people always tend to
h  x    g t x     x  r  
   
follow the opinions of these influencers in social
networks  therefore  it is important for us to figure which results in a linear decision boundary  i e 
out which persons are influencers and how they when t x  b  we predict    otherwise  we predict
shape public opinions  in our project  the goal is    note that for a training example  if we change the
to find influencers in a specific social network order a and b  that is  exchange the last    features
twitter 
of the training example with its first    features  
ii  dataset
a  original dataset
we use the dataset from kaggle provided by
peerindex  consisting of a standard  pair wise
preference learning task      each datapoint
describes two individuals  a and b  for each
person  there are    pre computed  non negative
numeric features based on twitter activity provided 
which include 

the label should also be reversed    becomes   
and   becomes     thus the coefficients of the
first    features must be opposite numbers of the
coefficients of the last    features  i e 
j   j      j                

   

where j is the jth entry of   so there are only   
independent parameters in   and thus we can use
only    features to represent an example  we choose
z as our new representation of training example as
follows 

fizj   xj  xj      j                

   

where zj and xj are the jth attribute of z and x
respectively  note that this preprocessing method
will only be used in linear models  i e  logistic
regression and svm  besides this method  we
also use other preprocessing methods such k means
algorithm  which will be discussed in their own
models 
iii  s ystem m odels
a  follower count benchmark
first  we tried a quite straightforward and trivial
method based only on the number of followers 
i e  if a has more followers than b  then a is
more influential than b  using this method  the
test accuracy is about        this result shows
that the number of followers is a strong indicator
of the influencers  however        is not good
enough for our prediction and this result will be
mainly considered as a benchmark  after adding
more features and using more general models  we
hope to get a better prediction on the test samples 

which is greater than   to make it more influential
than other features on the prediction 
for this logistic regression problem  we have two
choices gradient ascent and newtons method
to achieve the parameter   since the number of
features n      is not very large  so it will not take
much time to compute the inverse of a nn matrix 
thus newtons method should converge faster in
this problem and we choose newtons method as our
implementation  the update rule of the newtons
method is 
      h    l  

   

where h  rnn is the hessian matrix and hjk  
p
 i   i 
 i 
 i 
 m
 
i   h  z      h  z   zj zk    l  
z  y  h  z   
furthermore  we will add cross validation and
feature selection to this model  which will be discussed in details in section iv  we write our own
code to implement the newtons method 

c  svm
first  we also use the preprocessing method
mentioned in ii b  we implement svm
through libsvm  a library offered online at
b  logistic regression
http   www csie ntu edu tw cjlin libsvm  
with
l
regularization
and
linear
kernel
function
   
first  we preprocess the original dataset using the  
previous method  after preprocessing the original      using svm model  the problem formulation
dataset  different attributes have different ranges becomes 
which vary a lot  thus the first thing to do is to
m
x
handle the data to make it more uniform and easy
 
 
kwk
 
c
i 
min
to deal with  so in order to achieve this  we have a
w b   
i  
normalization on the dataset  for each feature  we
   
 i 
t  i 
s t  y  w z   b      i   i              m
do the following normalization
i     i              m
 i 
z
j
 i 
the box constraint c in     is customizable and
  z              n
   
zj   qp
 i  
m
we
let c     as its default value 
i   zj
moreover  we add cross validation and feature
where m is the number of training examples and selection to this model  which refers to section iv 
n      is the number of features 
because our job is to predict who is more influen  d  naive bayes
tial given two persons  the number of followers  as
after using discriminative learning algorithms 
we know  plays a significant rule in the prediction  we may want to try some generative learning ala person with more followers than the other is more gorithms  the distribution of the original data is
likely to be judged influential by users  so we will probably not gaussian distribution  so gaussian
multiply the normalization of the first feature  i e  discriminant analysis can hardly work  therefore 
number of followers  by a positive constant factor we choose the naive bayes model  in this case  we

fiusing the formula in      we can predict on
the testing set  however  one thing we havent
specified is the numbers of classes of each attribute
and changing this parameter  we may get different
testing accuracies  this parameter can be regarded
as a    dimensional vector  i e  c    c    c         c    t  
if we use a deterministic initial state instead of
random initialization in the k means algorithm  the
clustering result is also deterministic given the number of clusters  in this case  all parameters are determined by the vector c and we call it discretization
parameter  thus  the testing accuracy is a function of
c  say f  c   by changing c  we may achieve different
accuracies and thus we can optimize the accuracy 
we provide a coordinate ascent based algorithm as
follows 
t
 initialize c                   
 repeat until convergence 
for j                
cj    arg maxcj f  c    c         cj        c    
where cj   cj  d       cj        cj   d
the parameter d denotes the maximum step we
can take in one iteration  which is adjustable  this
algorithm has high computational complexity and
thus we havent tested and improved it  which could
be considered as a choice in future work  in section
v  we choose some specific values of c and illustrate
pm
 i 
 i 
i x
 
k

y
 
  
the
performance of naive bayes  we write our own
j
jk y     i  pm
 
k
 
  
  
    
c
j
 i      
code to implement the k means and naive bayes
i   i y
    algorithm 
where j                 corresponding to    features 
y is probability of y      jk y   is the probability e  neural network
that the jth attribute is class k given y      jk y  
the naive bayes model introduced in the precan be derived similarly and thus we dont give its
expression here  with the above parameters and a vious section is a nonlinear model  however  this
results from our nonlinear preprocessing methods 
test sample y  i    we predict   if and only if 
which makes naive bayes itself not that nonlinear  besides linear models  we still want to try
p y  i      x i     p y  i      x i    
some nonlinear models with high capacity  for this
p x i   y  i      p y  i        p x i   y  i      p y  i      reason  neural network might be a good choice 

use the multinomial naive bayes model  which is
more general 
before we apply naive bayes algorithm  we need
to discretize our dataset first  for this purpose 
we choose k means algorithm to discretize each
attribute into several clusters and each cluster corresponds to a class  applying clustering algorithm
like k means can help us distinguish users from
different levels  for instance  a film star may have
a million followers while a normal user can only
have hundreds of followers and in this case  their
difference cant be ignored and we need to put them
into different classes 
in the original dataset  some attributes have very
large ranges  thus  we consider using logarithm
function on the original dataset before applying kmeans  we will compare the performance using
logarithm function to not using it in section v 
once we have discretized the data  we can apply
multinomial naive bayes model to it  assume the
jth attribute has cj classes             cj    we give
the parameters achieving maximum likelihood as
follows 
pm
i y  i      
   
y   i  
m

 y

  
y
j  

jy i   y        y  
j

  
y
j  

jy i   y  
j

   
the decision boundary in     is linear  however 
this model is nonlinear because k means algorithm
and logarithm function are nonlinear mappings 

fig     neural network architecture in matlab

fiwe use the matlab neural network pattern
recognition toolbox to implement the neural network algorithm  which is designed for classification
problems  the network architecture is given in fig 
   which is a two layer feed forward network  with
sigmoid hidden neurons and softmax output neuron
    the number of neurons in the hidden layer is
customizable and we will compare the performances
using different numbers of hidden neurons in section
v 
iv  m odel s election
a  cross validation
we use hold out cross validation and     of the
training set are considered as validation set  whats
more  we randomly split the training set for    
times  we get     hypothesis functions  and pick
the one with the smallest cross validation error  we
also use k fold cross validation and we will assign
different values to k in section v in order to choose
the best one 

fig     test accuracy vs  cross validation options for linear models

and compare their test accuracies  it is shown that
cross validation can improve the test accuracy of
svm  while having no strong effect on logistic
regression  whats more  the performance of svm
is better than logistic regression using cross validation 

b  feature selection
in our dataset  it is fairly possible that some features are much more indicative than other features 
e g  the number of followers is a strong indicator 
we have    features for the linear models as stated
above  although the number of features are not very
large  the test error is large and there may be only
a subset of these    features that are relevant to the
result 
in this problem  we use forward search as our
feature selection algorithm  which is designed to
find the most relevant features  in addition  the
maximum number of features is regarded as an fig     test accuracy vs  discretization parameter for linear models
input argument in our implementation  thus  we can
change this parameter to optimize the performance
fig    shows the test accuracies of svm and loand sort features from the most relevant to the least gistic regression with different numbers of features
relevant  which will be discussed in section v 
selected  as is shown above  the performance of
svm is better  compared with logistic regression 
v  r esults   d iscussions
the best performance of svm is achieved when
a  logistic regression   svm
  features are selected  which shows that some
in this section  we apply cross validation and features are weak indicators  after feature selection 
feature selection to the linear models  i e  logistic we sort the features from the most relevant to the
regression and svm  the following figures com  least relevant as follows 
pare the performance among different models 
svm                   
 
  
in fig     we use different cross validation options
lr
     
 
               
on linear models  such as hold out and k fold 

fitraining roc

validation roc

 

 

   

   

   

   

   

   

true positive rate

true positive rate

where the corresponding feature name refers to section ii a  from this table  we can see that although
the sort results shown above are quite different for
these two methods  their test accuracies are very
close  using linear models  the best accuracy we
can achieve is         highest test accuracy in fig 
  and fig     

   
   
   

   
   
   

   

   

   

   

   
 

   
 

   

   
   
false positive rate

   

 

 

 

   

   
   
false positive rate

   

 

   

 

b  naive bayes
all roc

 

 

   

   

   

   

   

   

true positive rate

true positive rate

test roc

in this model  we compare the performances of
different discretization parameters  here we assume
that all features have the same number of classes
 the coordinate ascent based algorithm takes too
much time   we also mentioned in section iii d that
logarithm function is used to preprocess the data 

   
   
   

   
   
   

   

   

   

   

   
 

   
 

   

   
   
false positive rate

   

 

 

 

   

   
   
false positive rate

   
with logarithm
without logarithm

    

fig     receiver operating characteristic

    

accuracy

    
    
   
    
    
    
    

 

 

 

 
 
 
  classes of each attribute

 

 

  

fig     test accuracies vs  number of classes of each feature

in fig     we also compare the performance with
logarithm preprocessing and the one without logarithm processing  we can see that logarithm processing helps increase the test accuracy and when the
number of classes of each feature increases  their
performances approaches  in addition  it turns out
that different discretization parameters dont have
much impact on test accuracy  in this case  the best
test accuracy is        
c  neural network
in this model  we use    neurons in the hidden
layer  we also apply hold out cross validation with
    as the validation set  the roc  receiver
operating characteristic  curves are shown in fig 
   after training the two layer network  we get the
test accuracy         which corresponds to the
area under the test roc curve 

vi  c onclusion   f uture w orks
from the above results  we can conclude that the
best accuracy we can achieve is about     using
linear models  which is not much better than our
benchmark        this suggests that the testing
examples might not be linearly separable  the test
accuracy of naive bayes is close to linear models 
however  if we can apply the coordinate ascent
based algorithm  we may achieve much better performance  which is a good choice for future works 
furthermore  the accuracy of nonlinear models such
as neural network is better than linear models 
although its not as good as we expected  moreover 
there might be data corruptions since sometimes
human judgement can be highly biased  in order to
achieve better performance  we can either try more
nonlinear models or use decision trees to improve
it 
r eferences
    j  furnkranz and e  hullermeier  preference learning  a tutorial
introducton  ds       espoo  finland  oct      
    hsu c w  chang c c  lin c j  a practical guide to support
vector classification j        
    chang c c  lin c j  libsvm  a library for support vector
machines j   acm transactions on intelligent systems and technology  tist                  
    swingler k  applying neural networks  a practical guide m  
morgan kaufmann       

fi