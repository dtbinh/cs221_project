carveml  application of machine
learning to file fragment classification
andrew duffy
stanford university
agd cs stanford edu
abstract
we present a learning algorithmic approach to the problem of recognzing the file types of file fragments 
with the purpose of applying this to file carving  the reconstruction of partially erased files on disk
into whole files  we do so through the use of     calculated features of an input fragment  applying the
support vector machine  multinomial naive bayes  and linear discriminant analysis models to our
problem to see which produces the most accurate method of classification 

i 

introduction

have carving methods which examine the fragment contents  keeping in mind that different file types have different formats  and there
is not much consistency between formats  we
must derive some statistical similarity measures to tell how similar or different two fragments are from one another  having derived
these features  we would then apply any of
our array of classification algorithms to try and
train a model on our fragment data  and see
how well we perform at prediction new fragments types 
we begin our dissection of our research
with a description of the dataset we used  a
walkthrough of the set of derived features that
we calculated per fragment  the statistical models we used to represent the data  and end with
a discussion of our results and possible future
work 

in the forensics community  a common scenario faced by digital forensic investigators is
piecing back together useful information from
a disk image  digital forensic investigators
examine these disk images in an attempt to
uncover any suspicious activity  including any
traces of digital materials that can be used as
evidence  however  this is often challenging 
as perpetrators may delete information off
of their hard disk  these sorts of deletions 
though  are not permanent  they simply mark
each block of the file as unallocated and available for use in other files      these unallocated
file blocks can be recovered using a tool called
a file carver  an application which can take
these fragments and write them back out to
disk 
most file carvers depend on header and
footer information specific to certain filetypes
     for example  when we look at jpeg images  we know that all jpegs begin with the
byte sequence  xff  xd   so a header footer
based carver will look for fragments that contain this pair of bytes near their start to determine that a fragment is a jpeg  while this
might be a good method for classifying fragments towards the start or end of a file  it will
fail for classifying the internal fragments  because of this  it often becomes necessary to

ii 

data set

we draw our data from the govdocs  corpora  a   million file dataset developed by
garfinkel et al       the corpus was developed
by crawling through various  gov sites  which
means the dataset is in the public domain and
freely distributable  this is uncommon in the
forensics field  as many researchers will use
either difficult to reproduce datasets or completely private datasets      thus  performing
 

 

fiour research against a widely available freely
distributable corpus allows anyone to compare
our results to others using the same set of files 

   

h     p  x   i   log  p  x   i  

while we are forunate to have such a large
corpus  it also makes training and testing on
the entire corpus unfeasible  leading to questions of data sampling  luckily  the maintainers of the dataset have gone and created     
pre packaged threads       file randomly
generated subsets of the main corpora specifically meant for further use in research  our
training thread was composed of     files with
   different labels  and we tested on a set of
    files  with one randomly chosen fragment
of random size from each file  to echo the environment we might see in a real disk dump 

i   

where p  x   i   is the probability of any
single byte in the fragment taking on value i 
it becomes immediately clear that these probabilities are simply the byte proportions we
calculate for our first     features  so our code
for calculating shannon entropy becomes something like 
def s ha nn o n  e nt ro p y   bytes    
hist   byt e hist ogram   bytes    
returns a dictionary of
byte value     probability
sum    
for i in range        
sum    hist   i     math   log  
hist   i       
return     sum

while we performed our testing on file fragments  for parameter estimation we trained
each model on entire files  this was done with
the goal in mind of creating models that could
recognize whole files  and then assume that
fragments we give it will have the same distributions  in general  as whole files with the
corresponding type  this method performed
fairly well at allowing us to classify the random testing fragments  as we show below in
the results section 

iii 

one can think of shannon entropy as a measure of the amount of information packed into
some size of bytes  it does not correspond
directly to  information per bit   but its values do correlate strongly with file type  we
found in our research that for text files such
as  html   txt and  tmp  the calculated entropy was somewhere between            while
for more compressed formats  including  jpeg 
 pdf  and  gz formats  this measure was closer
to      other research used a combination of
these features and others  including byte ngrams which take into account byte ordering 
whereas we instead treat each input as a bag
of bytes similar to the bag of words model
we normally associate with text classification
     while this is making a very strong assumption about our dataset  it in no way hampered
our ability to realize fairly strong results using
just this minimum set of features 

features

we did not use any direct features of the
files or fragments  rather  we calculated a    dimensional feature vector for each training
and testing input  and used those to represent
each file to our learning models 
these     features came from two sources 
the first     features consist of the byte histogram  where for each of the     possible byte
values         we had one real valued feature
that represented the proportion of fragment
bytes that took on that specific value  this
has been used by other researchers with strong
results      the other measure that we chose
to extract from each input source is the shannon entropy  the formula for calculating the
shannon entropy  h  of a file or fragment is 

iv 

models

for this application  because the goal was
classification  we had the universe of classification methods at our disposal  as the work
 

fiin this area has been somewhat limited and
no specialized algorithms were found for this
topic  in this paper we only consider the use
of support vector machine  mulitnomial naive
bayes and linear discriminant analysis models 
because the research in this field is limited 
and no specialized algorithms exist to solve
the fragment classification problem  we chose
svms because they are generally a good candidate for any classification problem  naive
bayes was used to see how well we could
model this problem like a text classification
problem  treating each of the bytes as if they
were randomly occuring words in some text
stream  lda was used just as a method of comparison to the other two  as it is also a generally
strong classification algorithm  it assumes that
the conditional distribution of features are distributed gaussian  but we see in the results
section that this does not harm our classification accuracy 
for the support vector model  we use the
linear kernel  and apply l  regularization
with a parameter c       found empirically
through multiple test runs  our classification
accuracy appeared to drop for values less than
     and for values much higher than     we
also found decreased accuracy  so     was
found to be optimal within a few fractions of a
percent 
also note that svms are not invariant to
scaling         so we improve our results by
clamping our features in the range of         we
do this for our     dimensional byte histogram 
possibly contributing to our strong results 

v 

chine model had the strongest performance in
our tests  exceeding maximums set by other
scientific work with the same corpus      it was
initially surprising that the naive bayes model
performed so much more poorly than our support vector machine  however  it is likely the
case that the naive bayes assumptions may
have been too strong to correctly model our
dataset  at least with our given set of features 
meanwhile  support vector machines  often the favorite for a variety of classification
problems  were unsurprisingly successful in
this scenario 
here you can see the confusion matrix for
the svm  note how the diagonal entries are
the brightest  and the bright off diagonals indicate file types that were often mis classified as
another type by the svm  ex  html files were
often misclassified as python files  etc  

while we initially only used lda as a
comparison  we can see that it performed
even better than naive bayes  our secondary
model  considering that our features are all
real valued and can be considered more or less
random  though  it makes sense that ldas
gaussian assumption will actually perform relatively well at finding a linear boundary between classes 
for readers interested in pursuing this research further  lda might be interesting to
look into for this task as more features are
calculated and included in the feature vector 
while most works we consulted seemed to rely
on svms  these results imply that use of lda
in this setting shows some promise for future

results

after training our models and testing
against     distinct fragments  we found the
following results  which we show in tabular
form 
accuracy
svm
      
multinomial naive bayes
      
lda
      
we can see that our support vector ma 

firesearch 

vi 

fraction of mislabeled files has never been investigated  but after the authors individual
probing  we found that most of the files we
opened by hand matched their expected format  the most variance between actual and labeled format being between different plaintext
files  for example  there were a few simple
text files encountered that were labeled with a
 html extension 
the other issue with the dataset is that in
the end  the number of file types we train on
does not even come close to the total nubmer
of file types availabe out in the wild  while
it might not be feasible  or even useful  to try
and build a classifier that recognizes all the
thousands of file types  a mere    is not quite
enough to cut it  while most research we consulted for this project had a similar or lesser
number of file classes  eventually this work
should be extended to include at least     common file formats for the tool to be at all practically useful 
for future research  the classification performed here is far from optimal  and lacks intelligence to distinguish well between multiple
different types of the same format  for example  the classifier often misclassified  html files
as  py files  this is in part due to the issues
with the dataset as noted above  but in theory
we can add a set of features for plaintext files
which are calculated from syntax analysis of
the text  this will allow us to eventually become even more precise in our classifications 
as opposed to the accuracy that we were able
to realize 
eventually  this fragmentations process will
not only be more accurate  but fully automatable and combinable with an algorithm to perform the fragment reassembly phase to achieve
full carving capabilities 

discussion and future work

the fields of computer forensics and computer security are quite large  theres a
plethora of ongoing research in both of these
areas  but despite all the attention it gets  machine learning has scarcely been applied to it 
we found very few sources that cited the use of
machine learning techniques for security and
forensic applications  and those that did were
usually in relation to anomaly detection based
intrusion detection systems 
more common techniques for solving the
fragment classification problem were systematic  and required an intimate knowledge of the
format of each type of file  such as the headerfooter method of file type detection described
at the top  while this works for a small enough
group of file formats  for general classification
machine learning is surely more scalable  as
hard coding of headers and footers is unnecessary  and indeed impossible for recognizing
some formats  json  etc   
our results are a promising sign that machine learning has a strong future in forensic
and security applications  and can compete
with less intelligent methods of classification
and estimation  which is necessary if we wish
to scale up to not just a few dozen  but hundreds or even thousands of file types 
there were a few issues with our dataset
that may skew our results  the first is that the
extensions of some files in the dataset are inaccurate  and while this does not give cause to
disregard our results wholly  it does mean that
there is some amount of error in our approximations  considering the size of the dataset 

references
   

c  beek  introduction to file carving  mcafee foundstone professional services       

   

d  ariu et al   machine learning in computer forensics  and the lessons learned from machine learning in computer security   proceedings of the  th acm workshop on security and artificial
intelligence       

   

s  fitzgerald et al   using nlp techniques for file fragment classification  digital investigation  vol          

   

s  garfinkel et al   bringing science to digital forensics with standardized forensic corpora  digital investigation  vol          

   

pedregosa et al   scikit learn  machine learning in python  jmlr     pp                  

 

fi