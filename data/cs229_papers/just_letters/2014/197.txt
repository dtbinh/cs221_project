predicting user following behavior on tencent weibo
jinfeng huang  jinfeng stanford edu   hai huang  hai    stanford edu   zhaoyang jin  zjin stanford edu 

abstract

tencent weibo  the chinese twitter like service gains its popularity in recent years  enormous commercial
potential lies in the user behavior on the social media such as tencent weibo  in this project  different machine learning
algorithms were applied to predict user following behavior on tencent weibo  the accuracy of prediction  true positive rate and
true negative rates are estimated  the performance of algorithm in various training set size was evaluated  with experiments on
five different algorithms  svm with rbf kernel was determined to be the most effective method  as for feature analysis  user
profile is shown to be the key feature subsets that make the biggest difference  number of tweets stand out as the best indicator
for the behavior of the user  with the findings in this project  tencent weibo was advised to further improve its recommendation
practice with more emphasis on user information generally and the most active user group specifically 

   background introduction
tencent weibo is a chinese twitter like service
launched by tencent  one of the biggest internet
companies in china  since its launch  tencent weibo
has become a major platform for sharing interests
online  there are more than     million registered
users on tencent weibo by far  generating over   
million messages each day  this scale benefits the
tencent weibo users but it can also flood users with
huge volumes of information  hence putting them at
risk of information overload 
reducing the risk of information overload is a priority
for improving the user experience and also presents
opportunities for novel data mining solutions  thus 
capturing users interests and serving them with
potentially interesting items such as news  games 
advertisements  products  is a crucial feature social
networking websites like tencent weibo 

   project description
the goal of the project is to predict whether a user will
follow an item that has been recommended to her  an
item is a specific type of user on tencent weibo  which
can be a person  an organization  or a group  that is
selected and recommended to other users  typically 
celebrities  brands  or some well known groups were
selected to form the items set  the raw data set
contains two categories of data  user data and item data 
user data includes user profile  id  age  gender 
personalized tags    of tweeting times   following
history  and users keywords with relative weights  item
data includes item id  category and item keywords  we
aim to use different classification algorithms to predict
user behavior  by learning whether a specific user with
certain background will recommend and follow a
specific item  we can improve the personalized
recommendation accuracy 

   data
     data selection
with experimentation on different selection of features 
we decide to keep the following seven features for our
prediction model 
category

feature

description

user data

gender

integer value of       or    for unknown 
male  or female  respectively 

year of birth

integer selected by user when she registered 

number of
tweets

integer that represents the amount of tweets
the user has posted 

item data

 st layer in item
category

 nd layer in
item category
 rd layer in
item category

items are organized in categories  each
category belongs to another category  and all
together forming a hierarchy  for example 
 st layer category science and technology 
second layer category internet  third layer
mobile and forth layer company name

 th layer in item
category

table    training features for predicting whether a user will follow an item

we choose our features mostly based on performance
on empirical trials  besides  there are two more reasons
which help us to make feature decisions     relevance 
for example  data on follower followee relationships
history without detailed description does not provide us
with useful information on a users interest in a
particular item     certain feature is too sparse to be
useful  for example  there are millions of possible
tags keywords for a user  but one user may just have
several even zero of them 

fi     data statistics
we randomly select a considerable portion of data
from whats provided by tencent 
  of total data set from tencent

          

  of total data we use with random selection

       

  of training data in our model

       

  of testing data in our model

      

table    the sizes of training data and testing data

   methods
     algorithms
       support vector machine
a support vector machine constructs a hyper plane or
set of hyper planes in a high or infinite dimensional
space  which can be used for classification problem  a
good separation is achieved by the hyper plane that has
the largest functional margin  since in general the
larger the margin the lower the generalization error of
the classifier 
we use two kernel functions in this report  radial basis
function  rbf  kernel and linear kernel 
radial basis function kernel is defined as 
  must be greater than   
linear kernel is defined as
 
       logistic regression
logistic regression is a linear model for classification
rather than regression  in this model  the probabilities
describing the possible outcomes are modeled using a
logistic function  our implementation fits a logistic
regression with l  regularization  as an optimization
problem  binary class l  penalized logistic regression
minimizes the following cost function 

       naive bayes classifier
naive bayes methods are a kind of supervised learning
algorithms based on bayes theorem with a naive
assumption  in this report  we implement the gaussian
naive bayes algorithm for classification  the
likelihood of the features is assumed to be gaussian 

the parameters
and
maximum likelihood 

are estimated using

       stochastic gradient descent
stochastic gradient descent  sgd  is a simple yet very
efficient approach to discriminative learning of linear
classifiers under convex loss functions  sgd has been
successfully applied to large scale and sparse machine
learning problems often encountered in text
classification and natural language processing  in this
report  we use soft margin linear support vector
machine  i e  hinge  as our loss function 
     implementation and data processing
we implemented the five algorithms using scikit learn
library version      in python       in this report  svm
with rbf kernel  svm with linear kernel  logistic
regression  naive bayes classifier and sgd with
hinge loss function 
for comparing the accuracy performance of different
algorithms  we randomly chose         lines of data
from our raw data as our training set  and randomly
chose another        lines of data as our test set  after
we learned from training set and predicted labels for
test set  we calculated three kinds of accuracy for the
predicting result 
  accuracy  defined as the number of successful
predicting cases over the number of total test cases  we
used the average value of accuracy of    times
experiments 
  true positive rate  defined as in worst case  the
number of successful predicting cases over the total
number of positive cases  user follow item  
  true negative rate  defined as in worst case  the
number of successful predicting cases over the total
number of negative cases 
note that true positive rate and true negative rate are
defined under worst case  i e  the lowest value of true
positive accuracy   true negative accuracy in the   
times experiments  we used these two numbers in
order to measure the accuracy for unbalanced data  e g  
the positive cases exceed the negative cases  or vice
versa  an algorithm in which true positive rate and true
negative rate are both close to the total accuracy is
regarded as a better one than algorithms in which these
two rates are relatively low  or have large gap between
them 
then we tested the performance of different algorithms
under different sizes of training sets in terms of the
three accuracies defined above  training sizes were set
to                                                    
       and         lines of data  and test case is set to
       lines of data  these data were randomly chosen
from the raw data  we plotted three accuracies versus

fitraining size  and investigated how the increase of
training size affects the performance of algorithm 
in order to analyze how each feature affects the
performance in terms of accuracy  we choose the best
algorithm from previous experiments  and run it  
times  on each experiment  we left one feature out of
our training features  and recorded the corresponding
accuracy  true positive rate and true negative rate  the
more the three accuracies decreases  the more
important the leave out feature is  since we have
already selected our features  the accuracies should not
increase as one feature is left out   we listed the top
three important features as well as the corresponding
changes of accuracies 

   results
the following table summarizes the performance for
all five methods we tried 
method

svm  rbf 

svm  linear 

logistics

nave bayes
 gaussian 

stochastic
gradient
descent
 hinge 

metrics

perfor
mance

accuracy

    

true positive rate

    

true negative rate

    

accuracy ratio

    

true positive rate

    

true negative rate

    

accuracy

    

true positive rate

    

true negative rate

    

accuracy

    

true positive rate

    

true negative rate

   

accuracy

   

true positive rate

    

true negative rate

    

speed

medium
 around   
minutes 

slow
 around   hours 

fast
 less than  
minutes 

prediction algorithm because such performance comes
with the price that its true negative rate is low  meaning
the naive bayes algorithm is strongly biased towards
positive results 
a second significant result from our experiment is that
stochastic gradient descent is not a solid method for
this problem  with the worst case rendering only   
for true positive rate and true negative rate  further
more  its average accuracy is barely      no better
than any random prediction  in high dimensional space 
stochastic gradient descent  as linear classifier  does not
fit the problem 
another interesting pattern worth noticing is that the
true positive rate is generally higher than true negative
rate  and the accuracy  resulted from the combining
effect from true positive rate and true negative rate 
stays in between  we deem this pattern as desirable  as
in real life  tencent weibo will definitely be more
interested in getting things right  namely how they
could get people to follow an item  we will discuss
more of this phenomenon in the following section 
finally  a better algorithm in terms of accuracy may
suffer in efficiency  svm with rbf kernel is the high
performing algorithm but it took thirty minutes to run 
other algorithms do not perform as well could be very
fast with run time less than a minute  there is trade off
between accuracy and efficiency 
     accuracy  effects of increased size of training
data
we also want to explore  for each method  how the size
of training data will affect the performance of the
algorithm  to make results comparable  we keep the
     ratio between training data and test data across all
experiments  the following chart summarizes the result 

fast
 less than  
minutes 

fast
 less than  
minutes 

table    algorithms accuracy  true positive rate  true negative rate and implementation
speed for         training examples and        testing examples
figure    algorithms accuracy over different training size

for a fixed amount of         training data  we notice
that svm with rbf kernel delivers the best result for
two metrics  accuracy        and true negative rate
        its true positive rate        is high too  but only
ranks second  naive bayes gives the highest true
positive rate        however  this is not a good

except for stochastic gradient descent  all other
algorithms deliver significantly better performance
with more training data  for best algorithm  namely
svm with rbf kernel  the performance boost from

fi    training data to         data can be a dramatic 
from     to     
from the chart above  we could also draw an important
conclusion that for this particular application  svm
algorithm will deliver significantly better performance
with a rbf kernel than with a linear kernel 

converge to overall accuracy  using svm with rbf
kernel as an illustrating example  other algorithms also
generate similar trends except naive bayes  
data size

   

   

    

    

     

     

     
 

     true positive rate and true negative rate 

accuracy

    

    

    

    

    

    

    

true
positive

    

    

    

    

    

    

    

true
negative

    

    

   

    

    

    

    

effects of increased size of training data
the following two charts summarize each algorithms
performance measured in true positive rate and true
negative rate respectively 

table    accuracy trends with increasing training set in svm  rbf kernel 

this shows that all algorithms bias for positive results
come to diminish with the help of big training data size 
     feature analysis
we also try to explore which feature contributes most
to our prediction so that we could understand the
tencent weibo user behavior in depth  the following
chart summarizes the top   features measured in how
much accuracy will be compromised if we remove the
feature in the prediction model 
figure    algorithms true positive rate over different training size

figure    top   critical features for model performance
figure    algorithms negative positive rate over different training size

we could conclude from the charts above that for all
five algorithms  true positive rate beats true negative
rate with the same training data size almost under every
situation  it means that with the seven features  these
algorithms tend to successfully predict positive results
 i e  follow or recommend an item   which is preferable
in applications  the gap between true positive rate and
true negative rate is usually not significant except in
nave bayes classifier 
another significant trend as training data increases is
that true positive rate and true negative rate tends to

there are two important insights from the above chart 
firstly  number of tweets a user had sent is the best
indicator of whether she would follow a recommended
item  given this important information  tencent weibo
could consider concentrating more recommendation
efforts to the most active user group 
secondly  all top   features  number of tweets  year of
birth  gender  concern the user profile rather than item
characteristics  this fact illustrates that to determine
whether a user will follow a particular item  the
information on the user herself is more valuable than
the information on the item 

fi   conclusion
with experiments on five different algorithms  we are
able to determine the best one  svm with rbf kernel 
this algorithm delivers     accuracy         training
data and a test set of        data  the positive true rate
      and negative true rate       is also reasonably
similar  showing no bias between positive and negative
results 
we are also able to demonstrate interesting trends with
increased data size  except for sgd  accuracy for all
other four algorithms experience considerable
improvement in performance as training data size
increases  the true positive rate and true negative rate
also shows similar patterns  furthermore  although true
positive rate is significantly higher than true negative
rate with smaller training set  such bias is diminished
as training set gets larger and both metrics converge to
overall accuracy with the         training set size 
as for feature analysis  we show that user profile is the
key feature subsets that make the biggest difference 
among three user profile features  number of tweets
stand out as the best indicator of whether a user will
follow a recommended item  tencent weibo is advised
to further improve its recommendation practice with
more emphasis on user information generally and the
most active user group specifically 

   further work
     use user following history
constrained by the data source released by tencent 
some of the findings in our project seem to be trivial 
more interesting results could be found if we have the
knowledge of what the users interests are  for
example  if we could get keywords and tags of items a
user follow in the past  and thus match with the users
own keywords and tags  we could hopefully raise the
model performance to the next level 
     use social graph to determine following
patterns
in our model  we did not explore the relationship
between the following behavior of a users friend and
the behavior of the user herself  an interesting
hypothesis that could be tested is that ones interest is
similar to ones friends 
     use the popularity of each item
we did not differentiate the items own popularity in
our model  this is a high potential improvement
possibility for us because a popular item is intrinsically
more likely to get following when presented to a user 

taking into account this effect  we could change the
current situation where item characteristics do not
contribute enough to the prediction 

reference
   data source  http   www kddcup     org 
   scikit learn library documentation  http   scikitlearn org stable documentation html
   cs     lecture notes  stanford university      
   c  m  bishop  pattern recognition and machine learning 
springer      
   k  p  murphy  machine learning  a probabilistic perspective  the
mit press      
   t  hastie  r  tibshirani and j  friedman  the elements of
statistical learning  data mining  inference  and prediction   nd
edition   springer      
   c  j  c  burges  a tutorial on support vector machines for
pattern recognition  data mining and knowledge discovery      
   j  shawe taylor and n  cristianini  kernel methods for pattern
analysis  cambridge university press      
   what is an rbf kernel 
https   charlesmartin   wordpress com            kernels part   
    kernel functions for machine learning applications 
http   crsouza blogspot com         kernel functions for machinelearning html
    j  m  hilbe  logistic regression models  chapman   hall press 
    
    andrew y  ng  feature selection  l  vs  l  regularization  and
rotational invariance  icml conference      
    s  russell and p  norvig  peter  artificial intelligence  a modern
approach   nd edition   prentice hall      
    l  bottou  stochastic gradient descent tricks  microsoft
research      

fi