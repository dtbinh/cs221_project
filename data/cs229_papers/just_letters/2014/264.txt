sentiment analysis of yelps ratings based
on text reviews
yun xu  xinhui wu  qinxia wang
stanford university

i 

introduction

a 

background

challenge dataset the yelp dataset has information
on reviews  users  businesses  and business check ins 
we specifically focus on reviews data that includes
            user reviews of businesses from five different cities  we wrote a python parser to read in the
json data files  we only extract text reviews and star
ratings and ignore the other information in the dataset
for simplicity  we store the raw data into a list of tuples 
where an example tuple is of the form   text review 
star rating   and star ratings are integers in the range
from   to   inclusive  a higher rating implies a more
positive emotion from the user towards the business 
we use hold out cross validation and run our algorithms on a sample size of         we randomly
split this sample set into training      of the data 
and test  the remaining      sets  we assume that the
reviews stored in the json files are randomized in business categories  so we could sample our subsets of size
n by simply extracting the first n reviews  possible
improvements in sampling could be done by bernoulli
sampling to reduce possible dominance of training set
by certain business categories 

yelp has been one of the most popular sites for users to
rate and review local businesses  businesses organize
their own listings while users rate the business from
     stars and write text reviews  users can also vote
on other helpful or funny reviews written by other
users  using this enormous amount of data that yelp
has collected over the years  it would be meaningful if
we could learn to predict ratings based on reviews text
alone  because free text reviews are difficult for computer systems to understand  analyze and aggregate
     the idea can be extended to many other applications where assessment has traditionally been in the
format of text and assigning a quick numerical rating
is difficult  examples include predicting movie or book
ratings based on news articles or blogs      assigning
ratings to youtube videos based on viewerscomments 
and even more general sentiment analysis  sometimes
also referred to as opinion mining 

b 

goal and outline

the goal of our project is to apply existing supervised
learning algorithms to predict a reviews rating on a
given numerical scale based on text alone  we look at
the yelp dataset made available by the yelp dataset
challenge  we experiment with different machine
learning algorithms such as naive bayes  perceptron 
and multiclass svm     and compare our predictions
with the actual ratings  we develop our evaluation
metric based on precision and recall to quantitatively
compare the effectiveness of these different algorithms 
at the same time  we explore various feature selection
algorithms such as using an existing sentiment dictionary  building our own feature set  removing stop
words and stemming  we will also briefly discuss other
algorithms that we experimented with and why they
are not suitable in this context 

c 

ii 

results and discussion

a 

evaluation metric

we use precision and recall as the evaluation metric
to measure our rating prediction performance  our
oracle is the metadata star rating  we compare our
prediction with the metadata star rating to determine
the correctness of our prediction  precision and recall
are calculated respectively by the equations below 

precision  

tp
tp   f p

   

recall  

tp
tp   f n

   
   

where tp  f p  f n are the number of true positives  false
positives  and false negatives respectively 
we record our data as shown in table    where the
 i  j th entry represents the number of actual rating i
being predicted to be rating j 

data

the data was downloaded from the yelp dataset
challenge website https   www yelp com dataset 
 

firating

 

 

 

 

 

 
 
 
 
 

  
  
  
  
  

  
  
  
  
  

  
  
  
  
  

  
  
  
  
  

  
  
  
  
  

 i e  reducing a word to its stem root form  to remove
repetitive features using the porter algorithm readily
implemented in natural language toolkit  nltk  
the results of the various feature selection algorithms on the test data are shown in fig    each column corresponds to precision or recall for ratings  
through    from left to right  we observe that building
a dictionary from the dataset followed by removing
stop words and stemming gives the highest prediction
accuracy 
the advantage of using an existing lexicon is that
there is no looping over the dataset  also  the feature
set consists exclusively of adjectives that has sentiment
meaning  the disadvantage is that the features that
we use are not extracted from the yelp dataset  so we
might include irrelevant features while relevant features are not selected  for example  many words in
the text reviews are spelled wrong  but still contain
sentiment information  using such a small feature set
causes the problem of high bias 
building the feature set using training data results
in a larger feature set  selects only relevant features
from the yelp dataset itself  and improves both precision and recall significantly  however  looping over
the training set to select relevant features can be slow
when our training size becomes large  if we loop over
a small training set though  the features selected might
have high bias and not representative of the entire yelp
dataset 
a large feature set also has the problem of high variance  in other words  while the training error reduces
with a larger training set  the test error remains high 
this motivates us to remove stop words  i e  common
words with no sentiment meaning  and use stemming
to reduce redundancy in the feature set that we built 
this further improves our prediction accuracy by a
noticeable margin 
negation handling by appending not  was motivated by putting more information of the sentence
context into each word  the results however did not
improve  this could be caused by overfitting from
adding more features  since we append not  to all
the words following punctuation  all the nouns following negation were also processed and added  and such
manipulation may generate noise on our testing 

table    illustration of precision and recall calculation 

thus in our context  precision and recall of rating i
are calculated by the equations below 
precision  
recall  

m i  i  
 j  

m  i  j 

m  i  i  
 i  

m  i  j 

   
   

an additional evaluation metric to consider is runtime of our predictor  which becomes particularly important when the dataset is huge and optimization
of runtime becomes necessary  which we will discuss
further later 

b 

preprocessing

in our data preprocessing  we remove all the punctuations and all the spaces from the review text  we
convert all capital letters to lower case to reduce redundancy in subsequent feature selection 

c 

feature selection

we implement several feature selection algorithms  one
using an existing opinion lexicon  the others building
the feature dictionary using our training data with
some additional variations     
our most basic feature selection algorithm uses
bing liu opinion lexicon available for download
publicly from http   www cs uic edu  liub fbs 
opinion lexicon english rar  this opinion lexicon is often used in mining and summarizing customer
reviews      so we consider it appropriate in our sentiment analysis  it consists of      adjectives in total 
where      are positive       negative  we combine
both the positive and negative words and define these
words to be our features 
the other feature selection algorithms loop over the
training set word by word while building a dictionary
that maps each word to frequency of occurrence in the
training set  in addition  we implement some variations      appending not  to every word between
negation and the following punctuation      removing
stop words  i e  extremely common words  from the
feature set using terrier stop wordlist      stemming

d 

perceptron algorithm

we consider a review not as a single unit of text  but
as a set of sentences  each with their own sentiment 
with this approach  we can address our sub problem
on the sentiment analysis of one sentence instead of
the whole review text  we use perceptron learning
algorithm to predict the sentiment of each sentence 
 

fi    
precision

    

recall

psercentage

    
    
    
    
    
   

basic

with dictionary

stop word

stop word
  stemming

figure    comparison of test error for different feature selection algorithms using naive bayes 

rating

where the hypothesis is defined as the following 

h   x     g  t x  

precision    

recall    

    
    
    
    
    

    
    
    
    
    

 
 
 
 
 

   

and g is define to be the threshold function 
table    perceptron algorithm results on test dataset 

 
g z   

 

x 

 

x  

we observe that the precision and recall results are
significantly better for ratings   and    the two extreme
cases  since we train the features based on only positive
or negative sentiment    categories   it is difficult for
our algorithm to predict how positive or how negative
the entire sentence is using these features 
another observation is that the ratings are predicted
to be consistently lower than the actual rating  to fix
this problem  we scale the predictions to have the same
mean and standard deviation as the actual star ratings 
however  this did not improve our prediction accuracy 
when we trained the weights for the features  we separate the reviews into two groups      star as positive 
    stars as negative  on the other hand  the mean
rating is around      thus  this manual separation in
the training step affects the weights calculated and the
rescaling step later might counteract the information
that we gained from the training earlier 

   

we use stochastic gradient descent to minimize the
loss function  each sentence is predicted to be positive
 p  if the hypothesis is computed to be   or negative
 n  if the hypothesis is    finally  we compute the star
rating for the entire review based on the number of
positive and negative sentences in the review 

rating    

p
       
p n

   

where p and n are the number of positive and negative sentences in the review respectively  the equation
above ensures that the rating is scaled in the        
range to be comparable to the metadata rating 

e 

we built the feature set by looping over the training
dataset with stop words removed and porter stemming
and this gives us a total of       weights  the precision
and recall for the test set are shown in table   

naive bayes

we use the naive bayes algorithm in the scikit learn machine learning library to predict star ratings  similarly 
the features are selected by looping over the training
 

fiset with stop words removed and porter stemming 
naive bayes is traditionally used and proved to be
the most suitable for text classification  in our naive
bayes algorithm  we represent a review via a feature
vector whose length is equal to the number of words
in the dictionary  we use laplace smoothing to avoid
over fitting  in addition  we implemented a variation of
naive bayes  i e  binarized naive bayes using boolean
feature vector  in other words  instead of counting the
frequency of occurrence of the words  we use   or   to
denote whether the word occurred or not  this is motivated by the belief that word occurrences may matter
more than frequency 
the precision and recall for the training and test set
for binarized naive bayes are shown in table   and
table   
rating
 
 
 
 
 

precision    

recall    

    
    
    
    
    

    
    
    
    
    

svm  and nearest centroid algorithms  both were
implemented using the scikit learn machine learning
library 
multi class svm is a generalization of svm  where
the labels are not binary  but are drawn from a finite
set of several elements  however  the predictions have
extremely low accuracy  even on the training dataset
itself  therefore  we conclude that it is not suitable in
the context of sentiment analysis 
the nearest centroid algorithm is a classification
model that assigns to observations the label of the class
of training examples whose mean  centroid  is closest
to the observation  in the training step  given labeled
training samples   x    y       x    y            xn   yn   where yi s
are the ratings and xi s are feature vectors in the high
dimensional feature space  the per class centroids are
computed using the formula below 

r  

 
 
 
 
 

precision    

recall    

    
    
    
    
    

    
    
    
    
    

y   arg minry kr  x k

    

however  the precision and recall on the test dataset
are found to be low  this is expected  because in many
cases  our data are represented in a very high dimensional space with only few components being non zero 
there is little sense of clustering for this model  because
when we calculate the average position of the points
that are classified as the same group  it might become
a point in space that is not close to any of the point
in the cluster  but closer to some point that in another
group 
we also experimented with the natural language
toolkit  nltk  to tag each word into different group
based on parts of speech  however  this results in a
very low training speed without much improvement
on classifying the test data 

table    naive bayes algorithm results on test dataset 

the training error is significantly improved  implying a much lower bias error as compared to the
perceptron algorithm  although the precision and recall for the test set are not very high  we observe this
is due to the fact that star   and   reviews are difficult
to be distinguished from each other  same for star   
   and   reviews  for example  more than one third of
the star   reviews are predicted to be star   and vice
versa  this is expected  because star   and   reviews
are difficult to be distinguished from each other in the
first place  therefore  if we combine reviews of star  
and   into one classification category  our prediction
accuracy will be significant improved 

f 

   

where cr is the set of indices of samples that has rating r  in the prediction step  the class assigned to an
observation x is computed by the formula below 

table    naive bayes algorithm results on training dataset 

rating

 
xi
 cr   i
cr

g 

comparison of algorithms

a comparison of precision and recall on the test dataset
using different learning algorithms is shown in fig   
multi class svm and nearest neighbor both have
low precision and recall  perceptron algorithm has the
highest precision and recall for star   and   ratings 
but the predictions are poor for star       and    it also
suffers from high bias on the training dataset  naive
bayes  binarized  has the best overall performance  but

other algorithms

other algorithms that we have also considered so far
are multi class support vector machine  multi class
 

fi    
precision

    

recall

percentage

    
    
    
    
    
    
   
                   
naive bayes

                   
                   
                   
perceptron
nearest neighbor multiclass svm

figure    comparison of test error for different learning algorithms 

further error analysis by running the algorithm on different sample sizes shows that it has the problem of
high variance  this is evident from the learning curve
plotted in fig    where as the sample size increases 
the margin between training and test accuracy remains
large 

categories and use customized feature sets for each
category  because different word features might be
more or less relevant in different business categories 
runtime of the algorithm could possibly be improved
by training and testing within each business category 
because of a smaller feature set  we could also try
using parts of speech in feature selection process to
differentiate between the same word features that are
used as different parts of speech 

     
    
    

percentage

    

references

    
    
    

   

g  ganu  n  elhadad  and a  marian  beyond the
stars  improving rating predictions using review
text content   webdb  no  webdb  pp           

   

n  godbole  m  srinivasaiah  and s  skiena 
large scale sentiment analysis for news and
blogs  icwsm       

   

b  pang  l  lee  and s  vaithyanathan  thumbs
up   sentiment classification using machine learning techniques  proceedings of the acl    conference on empirical methods in nlp       

   

k  dave  s  lawrence  and d  pennock  mining
the peanut gallery  opinion extraction and semantic classification of product reviews  proceedings of the   th international conference on world
wide web  pp               

   

m  hu and b  liu  mining and summarizing
customer reviews  proceedings of the   th acm
sigkdd international conference on knowledge
discovery and data mining       

    
    

training accuracy

    

testing accuracy

   
      

      

       

       

                

sample size

figure    learning curve for binarized naive bayes algorithm 

iii 

conclusion and future work

in conclusion  we have experimented with various feature selection and supervised learning algorithms to
predict star ratings of the yelp dataset using review
text alone  we evaluate the effectiveness of different
algorithms based on precision and recall measures  we
conclude that binarized naive bayes combined with
feature selection with stop words removed and stemming is the best in our context of sentiment analysis 
possible improvement could be extracting additional information from the dataset such as business
 

fi