a bigram extension to word vector representation
adrian sanborn and jacek skryzalin

   background
glove is an algorithm which associates a vector to each word such that the dot product of two words
corresponds to the likelihood they appear together in a large corpus   psm      glove vectors achieve
state of the art performance on word analogy tasks  v king   v man    v woman   v queen    but
they are limited to capturing meanings of individual words  in our project  we develop biglove  a
version of glove that learns vector representations of bigrams  using the full english wikipedia text
as our training corpus  we compute     million bigram vectors in     dimensions 
to evaluate the quality of our biglove vectors  we apply them to two machine learning tasks 
the first task is a      semeval challenge where one must determine the semantic similarity of two
sentences or phrases  we used logistic regression using as features the cosine similarity of the average
sentence  bi glove vectors and found slightly better performance in one challenge when glove and
biglove were combined  but generally  the usage of biglove vectors did not increase performance 
second  we applied biglove vectors to classify the sentiment of movie reviews  training with naive
bayes using bag of words  svms  and random forests  we found that naive bayes or an svm with
glove vectors performed the best 
applications of biglove vectors were hindered by insufficient bigram coverage  despite training    
million vectors  at the same time  examination of nearest neighbors revealed that biglove vectors were
indeed capturing semantic relationships unique to bigrams  suggesting that the method has promise 
training new vectors on a much larger corpus such as common crawl is likely to improve performance
of biglove vectors in tasks 
   bigram word vectors
the glove algorithm  glove associates to each word w a vector v w   rn   where typically n
takes values between    and      this collection of vectors is trained first by computing xij   the
number of times word i occurs in the context of word j  within some large corpus of text c  we used
the english wikipedia  
xij  

x

  c c    wi      c d    wj           c  d      
 c  d 

c d   m 

 

where c k  denotes the kth word in c  m is the number of words in c  and wi represents the ith word
in the lexicon obtained by collecting all words which occur in c  note that the word c k  cannot occur
in the context of itself  that we consider the context of a word to be all words that appear at most
   places before or after the word in question  and we weight contextuality by the inverse distance
between the words in the article 
let wi  w be the ith word in the set of words w   glove uses gradient descent to minimize over
all possible assignments of v wi    v wi    rn and bi   bj  r the quantity
j 

 w  
x


 
xij     v wi    v wj     bi   bj  log xij  

i j  
 

fi 

adrian sanborn and jacek skryzalin

biglove  we develop a technique called biglove  bigram glove   biglove associates to each bigram
b a vector v b   rn  we choose n       for this project   we hypothesize that by associating a vector
to each bigram we can capture new bigram specific meanings 
to train biglove  we download all articles in english wikipedia  removed all non article content  i e  
pictures  urls  captions  metadata   converted all letters to lowercase  and removed all numbers  with
some experimentation  we noticed that the bigram frequency was strongly biased towards bigrams
containing one significant word  e g   removal  tendency  and one insignificant word  e g   the 
of   in order to increase the information density of our bigrams  we removed    common stop words
which do not add to the meaning of a sentence  finally  we trained biglove via the same gradient
descent algorithm as glove  except that the cooccurrence xij is calculated via 
x
  c c    bi      c d    bj           c  d     
xij  
 
 c  d 
c d   m  

where c k  now denotes the kth bigram in c  which consists of the kth and  k     th word in c  
and where bi represents the ith bigram in the lexicon obtained by collecting all bigrams which occur
in c  we performed fifty iterations of stochastic gradient descent  which took around    hours on a
   cpu node with   gb memory 
bigram vector space  we trained vectors for any bigram occurring more than     times in the
corpus  resulting in           vectors  to probe the semantic structure captured in these vectors  we
computed the ten nearest neighbors for several hundred of the most frequent bigrams  measured by
cosine similarity of the corresponding vectors  v   v   kv  kkv  k  we observed that many neighbors captured new bigram specific meaning  for example  nearest neighbors of united nations included security council  secretary general  human rights  nearest neighbors of st louis included kansas
city and saint louis  and nearest neighbors of major league included professional baseball and
baseball player 
   semantic similarity prediction
we use glove and biglove to produce feature vectors for a semantic similarity task  the data  downloaded from http   ixa  si ehu es sts data trial tgz  was used for the semeval      task   
challenge  the challenge contains     pairs of sentences in each of three groups  annotated with a
numerical label ranging from    indicating that the sentences have no overlapping meaning  to   
indicating that the sentences have the exact same meaning 
we usedplogistic regression using a small feature vector  minimizing over all  and c the quantity
   

   c ni   log exp yi  xi     c        where we set c          for weak regularization  we
 
considered four features  the lengths of the two sentences  and the cosine similarity between the
two sentences using the glove biglove vectors obtained by averaging the glove biglove vectors of
all words bigrams in the sentences  we tested three versions  one just using glove  one just using
biglove  and one using both glove and biglove  the percentage of correct similarity rankings is
given below 
group  
group  
group  
train success test success train success test success train success test success
glv
     
     
     
     
     
     
bglv
     
     
     
     
     
     
glv bglv
     
     
     
     
     
     
figure    success rates using various algorithms and feature vectors for semantic similarity
we find that biglove vectors alone perform poorly  suggesting insufficient coverage of bigram vocabulary  however  adding biglove to glove did improve performance slightly for one group of sentences 

fia bigram extension to word vector representation

 

   sentiment analysis
the data  the data  downloaded from http   nlp stanford edu sentiment    spw        consists of       movie reviews with a train dev test split of                 each movie review is
assigned a score from   to    the interval        is split evenly into fifths  and each fifth is assigned a
sentiment  i e   very negative  negative  neutral  positive  and very positive   it is our goal to correctly
categorize each review into its designated category  we record the percentage of reviews that have
been classified correctly 
we also consider a two category system in which we consider reviews with assigned scores in the
interval          to be negative and those with assigned scores in the range          to be positive 
we apply all algorithms to both the five category and two category schemes  algorithmic adaptations
necessitated by having a non binary classification scheme are detailed below 
before applying all algorithms  we filter all text as done in bigram vector training  we consider three
featurization schemes  bag of words  glove  and biglove  the bag of words  bow  model associates
to each review r a sparse vector  r  indexed on the words in the article  the wth entry of  r  is
the number of times the word w appears in r  to construct glove and biglove feature vectors  we
average all word   bigram vectors corresponding to words in the review 
naive bayes  we use a multinomial naive bayes  nb  model with laplace smoothing  we use the
bag of words feature vectors only with this algorithm  to train the model with training set ttrain   we
calculate the values
p ttrain  
    k  
  c   category rk      rk   x 
p word x   category   c   
p ttrain  
 vocabulary    k     c   category rk     nk
p ttrain  
  c   category rk   
p category c    k  
 
 ttrain  
where rk denotes the kth review in ttrain   category r  gives the category associated to review r  and
nk gives the number of words in the kth review rk   given a test review r with bow feature vector
 r   we predict the category with the highest posterior probability 
support vector machines  we experimented with a variety of support vector machines  svms 
and found that a  svm  as constructed in  sswb     with a gaussian kernel demonstrated optimal
performance  the parameter  is an upper bound on the fraction of training errors and a lower bound
on the proportion of support vectors 

concretely  given a training set ttrain consisting of pairs  i    y  i    where y  i       we solve the
following optimization problem 
maximize

 ttrain  
  x
i j y  i  y  j  k  i     j   
w      
 
i j  

subject to

 
 
 

i
 ttrain  

 ttrain  

 ttrain  

x

x

i  

i y  i     

i  

 
    bag of words used
i    
     glove used

 


     bag of words used
where k       exp k  k    where   
 
   
 glove used
given a new feature vector   we assign the category determined by



 ttrain  
x
x
 
sign 
i y  i  k    i    
k sv    i     
 sv  
i  

where sv is the set of all support vectors 

sv sv

fi 

adrian sanborn and jacek skryzalin

for the case where we have   classes  we use the one vs one approach  we construct    svms to
separate each pair of classes  upon receiving a new feature vector   we test  against each svm and
tally the number of times that  is assigned to each class  we classify  according to the category to
which it is most frequently assigned  if ties occur   is assigned the class based on the classification
provided by the furthest hyperplane 
random forests  to train our random forests  rf   we create     decision trees  as outlined in
 bre      we create the decision trees via the cart algorithm  minimizing gini impurity  to test
our random forest  we output the most common decision of the     decision trees 
results  we provide the computed success rates for the various  learning algorithm  feature vector 
pairs  the best test success rates for each classification scheme are bold and underlined 
two sentiments
five sentiments
train success test success train success test success
nb
bow
     
     
     
     
bow
     
     
     
     
glv
     
     
     
     
svm
bglv
     
     
     
     
glv bglv
     
     
     
     
glv
 
     
     
     
rf
bglv
     
     
     
     
glv bglv
 
     
     
     
figure    success rates using various algorithms and feature vectors for sentiment analysis
one of the most important messages gleaned from these results is that naive bayes  which is both
simple and easily implemented  is also highly effective  also important is that  svms can improve
success rates over regular svms  using a  svm produced success rates around    higher than those
of a standard svm using glove features  using  svms with the glove feature model produced higher
success rates than naive bayes    and success rates around     higher than those of a standard svm
using bow features  standard svm results not shown   it can be concluded that  svms allow us
to harness the power of svms when the data is sparse and or very high dimensional 
interestingly  we found that neural networks generally produced relatively low success rates  success
rates not shown here   this comes as a surprise  other experiments not reported here involving the   
newsgroups dataset showed that using neural networks with the glove feature model can rival naive
bayes on document classification tasks 
   discussion
we effectively implemented biglove and trained bigram vectors on the full english wikipedia text 
and the resulting vectors captured new bigram specific meaning  however  when our biglove vectors
were used in semantic similarity and sentiment analysis tasks  they performed poorly on their own
and did not substantially improve performance in combination with glove vectors 
because the number of bigrams needed is on the order of the square of the english vocabulary
size  achieving ample vocabulary coverage is a particular challenge in training biglove vectors  upon
closer inspection  despite training over     million bigrams  we found that algorithm performance was
hindered by low vocabulary coverage of bigrams  for example  over     of bigrams in the sentiment
analysis movie reviews were not in our biglove dictionary  furthermore  expanding the vocabulary by
lowering the corpus co occurrence threshold is not sufficient to achieve significantly greater coverage 
training a vector for any bigram occurring more than    times in the wikipedia corpus  rather than

fia bigram extension to word vector representation

 

     would produce about     million vectors but only reduce the percentage of missing bigrams from
    to      see figure  

figure    bigram coverage of movie review text as a function of training corpus
occurrence threshold
by comparing the movie review text and biglove vocabulary  it also became evident that some notuncommon colloquial bigrams were missing from the vocabulary  it appears that the formal nature of
the wikipedia text also biased the biglove vocabulary away from the text in our applications 
although our implementation of biglove didnt perform as well as expected  we nonetheless feel
that the algorithm is sound  and that examination of biglove neighbors suggests that the model still
has potential  as such  as a next step  we would like to train biglove on a much larger corpus  ideally
the combination of wikipedia  common crawl  and gigaword    we estimate that a vocabulary
of around    million biglove vectors should provide sufficient coverage for our applications  and we
would expect then to see improved performance when biglove is combined with glove  additionally 
the semantic space of bigrams is more complex than the space of individual words  and we expect that
biglove performance will continue to improve as the vector dimension is increased  at least to    
dimensions  and perhaps to      dimensions 
once second generation biglove vectors have been trained  we would also like to experiment with
other statistical methods for using the vectors in applications  for example  instead of averaging all
word vectors in order to create a vector for the entire review  it would be interesting to instead analyze
the distribution of all word vectors in a review  using more advanced statistical techniques  one might
then be able to correlate certain distributions of word vectors with certain sentiment classes 
references
 bre   
 psm   

leo breiman  random forests  mach  learn              october      
jeffrey pennington  richard socher  and christopher d  manning  glove  global vectors for word representation  in proceedings of emnlp       
 spw      richard socher  alex perelygin  jean y  wu  jason chuang  christopher d  manning  andrew y  ng  and
christopher potts  recursive deep models for semantic compositionality over a sentiment treebank       
 sswb    bernhard scholkopf  alex j  smola  robert c  williamson  and peter l  bartlett  new support vector algorithms  neural comput                   may      
e mail address  asanborn stanford edu
e mail address  jskryzal stanford edu

fi