cardiac dysrhythmia detection with gpu accelerated neural networks

albert haque
computer science department  stanford university

   introduction
the human heart is controlled by an electrical system
which stimulates blood movement via contractions  cardiac dysrhythmia  or arrhythmia  occurs when the electrical activity of the heart is irregular  effects of arrhythmia
range from discomfort to cardiac arrest  although most
arrhythmias are harmless  arrhythmia is still responsible
for about         deaths in the us  annually  early detection and treatment of arrhythmia can reduce the number of
deaths by      hoefman et al         
identifying patterns in arrhythmia has been studied for several years and many statistical approaches have been attempted  these approaches can be grouped into two categories   i  statistical learning based on explicit feature extraction and  ii  recognizing patterns from the raw time series data 
most attempts fall into the first category of extracting features using human intuition  many studies use classical machine learning algorithms such as support vector machines
this document is part of a term project for cs     machine learning  stanford university  autumn       and was created on december          

ecg signal

cardiac dysrhythmia is responsible for over half
a million deaths in the united states annually  in
this work  we evaluate the performance of neural networks on classifying electrocardiogram
 ecg  sequences as normal or abnormal  arrhythmia   using neural networks as our primary
learning model  we explain our models performance and discuss hyperparameter tuning  comparing the results of our model to svms  random forests  and logistic regression  we find that
our neural network outperforms the other three
models with a binary classification accuracy of
       for the multi class classification task  we
achieve an accuracy of        the use of gpus
accelerates the neural network training process
up to an order of magnitude over cpus 

qrs

x 
t

p

u

st

class  

x 
pr interval
qrs interval

class  

x 

qt interval

additional
features

abstract

ahaque   cs   stanford   edu

pca

layer  

   
   

x 

 

layer  

   
   
   
    
    
   

    

   
 

    
   
    

layer  

layer  

   

figure    proposed neural network classifier  the original ecg
record and hand crafted features are used as inputs 

 song et al          the second category of approaches
are centered around time series analysis  time series approaches use wavelet transforms and attempt to minimize
the noise present in the data  thakor   zhu         some
models note the periodic interval between the qrs complex and pr qt intervals  see figure     autoregressive
neural networks have also been proposed for forecasting
time series data  zhang        
in this work  we are concerned with two problems  first 
we want to classify a record into binary classes  normal or
abnormal   second  we want to classify a record into multiple classes  depending on the specific case of arrhythmia
present  multi class classification   we construct a neural
network topology using significant hyperparameter tuning
and leverage the parallel computing power of gpus to accelerate the training process 

   dataset and feature extraction
we use the arrhythmia dataset found at the uci machine
learning repository  bache   lichman        guvenir 
       it consists of     records with     attributes per
record  each record is assigned to   of    classes  a class
label of   indicates normal ecg patterns while a class label
between   to    indicates abnormal ecg patterns or arrhythmia of varying types  the datasets class distribution
is shown in figure   
in our experiment  we use principal component analysis to
extract features from the dataset and is detailed in section
     using these principal components  we train several
models and compare their results 

ficardiac dysrhythmia detection with gpu accelerated neural networks
figure    regularized cost function for multi layer neural networks  let     contain the network parameters for layer    h  x   rk
be the output probabilities where k is the number of classes   h  x  i denotes the ith output  l denotes the number of layers and s 
denotes the number of neurons in layer   

  m k
 
l  s s   
  x x  i 
 x x x      
 i 
 i 
 i 
j     
 ij  
yk log h  x  k       yk   log     h  x  k     
m i  
 m
i   j  
   

   
   
   
  
 
 

 

 

 

 
  
class label

  

  

  

    
   
    
    
    
    
 

figure    class distribution

   
    
    
number of layer   neurons

    

figure    number of neurons vs mse

   models

    
    
accuracy

we compare our neural network model to three commonly
used approaches   i  support vector machines   ii  logistic
regression  and  iii  random forests  for support vector machines and logistic regression  we use a one vs all classifier
for the multi class classification problem 

binary classifer
multiclass classifier

    

binary  test 
multi  test 
accuracy

number of instances

   

mean squared error  mse 

k  

    
    

multi  train 
multi  test 

    

    

    
    
 

   
   
principal components

   

      
  

 

  
regularization parameter

     neural networks

 a  pca

we employ a multi layer neural network for both binary
and multi class classification  the output of each neuron is
the sigmoid function 




x 
 
 x  


 

             
h  x   
tx  x   




 
 
  e
xn
n

figure    hyperparameter tuning results

we train the network using back propagation and stochastic
gradient descent to minimize the cost function in figure   
due to the large number of hyperparameters  a significant
portion of this project was devoted to exploring the effect
of hidden layer size  depth  learning rate  and regularization
parameters 
perhaps the most important hyperparameter  we analyze the
effect of the number of neurons on performance  figure  
shows the effect of the number of neurons on mean squared
error  mse   as the number of neurons grows  the performance decreases  this is apparent for both the binary
and multi class case  based on these results  we use    
neurons at each layer  although the analysis is not shown 
deeper networks do not necessarily produce better results 
most often  performance suffered as the number of hidden
layers increased  based on this analysis  we select two hidden layers for our network topology 

 

  

 b  regularization

instead of using features from the raw dataset  we use principal components as inputs into our neural network  as
shown in figure   a   we analyze the number of principal components versus the classification accuracy  too few
 less than     or too many  greater than      principal components result in poor performance across all models  only
nn results are shown   based on these results  we use    
principal components for our final model 
the regularization parameter  attempts to prevent overfitting by reducing the magnitude of parameters in   fixing
all other parameters  we vary  for various values  see figure   b    and set         
in summary  we use the following hyperparameters 
  
  
  
  

regularization parameter         
two hidden layers l    
one hundred neurons at each layer h    h       
first     principal components are used as features

     support vector machines
due to its popularity in practice  we evaluate the performance of svms on our dataset  the svm attempts to find

ficardiac dysrhythmia detection with gpu accelerated neural networks

the maximum margin hyperplane that separates the dataset
in a higher dimensional feature space  finding this optimal
margin reduces to solving the following convex optimization problem 

before we began training of our models  we performed a
singular value decomposition over the dataset  extracted the
first     principal components  and used these as input features int our models 

m

min

 w b

x
 
  w     c
i
 
i  

   

subject to the constraints 
s t  y  i   wt x i    b      i   i           m
i     i           m

   
   

     results
surprisingly  our neural network outperforms the svm 
random forest  and logistic regression model in the binary
classification problem  see table     in the multi class case 
our neural network still outperforms svms and logistic regression with a test accuracy of       

where the i above allows for slack in the event the
dataset is not linearly separable 

table    classification accuracy
binary
multi class
model
training
test training
test
neural network
            
            
svm
           
           
random forest
           
           
logistic regr 
            
            

     logistic regression
closely related to our neural network  logistic regression
aims to minimize the cost function 
m

j    


  x
h  x i     y  i 
  i  

   

where h x    t x and  is defined in      to learn the
parameters   we minimize j   using stochastic gradient
descent 
     random forests
random forests apply the technique of bootstrap aggregating  i e  bagging  to learn a model  b decision trees are
created where each decision tree is trained on a random
sample of the training data  equation     shows how y is
computed for an unseen example in a regression problem 

because the cost of false diagnoses can be expensive  in
monetary terms  we wanted to see our models false positive rate  figure   shows our models roc curves  on the
test set  our network is able to achieve a true positive rate of
    with a false positive rate of about      after that  higher
false positive rates provide marginal gains in true positive
detections 

   discussion

   

as indicated by the training accuracies  the neural network 
random forest  and logistic regression tend to fit  or closely
fit  the training data perfectly  however  we know these
models are not memorizing the input data since the test accuracies are above     and     in the binary and multiclass case  respectively 

where yb is the prediction for decision tree b  for classification problems  the majority vote of all b trees is taken
 instead of the average  to output the predicted label 

we wanted to further analyze the performance of our neural
network by looking at the convergence rate  figure   shows
the mean squared error as a function of number of iterations
for different training sizes 

y  

b
  x
yb  x 
b

we separated the dataset into a training set consisting of
    records and a hold out test set containing     randomly
selected records  both matlab and python were used for
preprocessing  training  and testing 
our dataset consists of     records  of these     records
are incomplete and missing at least one feature  excluding
the vector angle to j feature   because we do not want
to discard these records  we instead impute the values by
using the features mean value as this tends to improve performance  jerez        

mean squared error  mse 

   experiment

    

  
   
   
   
   
   

   
    
    
    
    
 

  

  
iteration

  

 a  binary task

  

mean squared error  mse 

b  

   

  
   
   
   
   
   

   
   
   
   
   
 

  

  

  

iteration

 b  multi class task

figure    convergence rate of the neural network model  training   because the neural network was trained on       iterations 
only the first few iterations are shown above 

fitraining roc

  
  
  
  
 

binary
multiclass
classification task

figure    cpu vs gpu runtime

as previously mentioned  we used both cpus and gpus
for training our neural network  svms  logistic regression  and random forests were trained using cpus only 
due to the large number of cores available on gpus  gpus
are able to quickly train neural networks  especially those
with a large number of neurons  we used a geforce gtx
    ti gpu with     cuda cores clocked at     ghz with
  gib of gddr  memory  the cpu used was an intel i     k   core processor clocked at     ghz and    gib of
ddr  memory 
the plot shown in figure   show the results of our multiclass classification task by training on a neural network
with two hidden layers containing     neurons each  the
training phase consisted of       iterations  for gpus  the
bottleneck is the data transfer to and from the device  once
the training data has been transferred to the gpu  the computation is very fast  because our training data is so small
 on the order of a few mib   the gpu runtime does not
experience long initial or post processing delays  for networks with more hidden nodes  we can expect the gpu
vs cpu runtime margin to increase  juang et al        
steinkrau et al         
     error analysis
to visualize the results from all models  we generated confusion matrices depicted as heat maps in figure    both
logistic regression and neural networks achieved a training
accuracy of       this is denoted by values present only
on the diagonal  for svms and random forests  we see
incorrect classifications present on the off diagonal entries 
looking at the test set  we see that svms tend to incorrectly categorize normal heartbeats  label    with labels
      this results in a high false positive rate  our neural network has few errors on the test as stated in table
   this is confirmed in the confusion matrix above as most
true predicted labelings lie along the diagonal 
confusion matrices are a good indicator of false positive
and false negative rates  depending on the application  one
can be more expensive  in monetary terms  than the other 
to analyze these rates  we plotted receiver operating characteristic  roc  curves for the binary classification case

test roc

 

gpu    ghz      cores 
cpu    ghz    cores 

 
class  
class  

   

   
true positive rate

  

true positive rate

training time  seconds 

cardiac dysrhythmia detection with gpu accelerated neural networks

   
   

   
   

   

   

 

 

 

   
false positive rate

 

class  
class  
 

   
false positive rate

 

figure    roc curves for the binary classification task  class  
corresponds to the normal ecg case and class   corresponds to
arrhythmia is present 

in figure    we do not plot the multi class case as some
classes contain one or two training examples and this skews
the roc curves for these labels 
figure   shows a perfect roc curve for the training phase 
as expected  looking at the test roc curve  if we are willing to tolerate a false positive rate of about      we are able
to achieve a true positive rate of      in this particular application  in the domain of medical practice and arrhythmia
detection   early detection has the potential to save lives 
as a result  the cost of a false negative is greater than the
cost of a false positive  if a patient is diagnosed as arrhythmia present  predicts positive   we can perform further tests
to correctly label this patient 

   conclusion
due to the time series nature of ecg data  future work
can explore recurrent and autoregressive neural networks
 zhang         these networks are well suited for predicting future time series and can be applied to ecg signals 
additionally  as with any learning problem  more  training  data would benefit our research  a longer term project
would aim to utilize smart health sensors  fitbit  jawbone 
microsoft band  etc   for real time cardiac monitoring 
in this work  we used principal component analysis to
transform the original     features from the dataset  we
then analyzed the effect of number of principal components  hyperparameter tuning  and the runtime effect of
gpus  results show that our neural network outperforms
other methods on the binary classification task  for multiclass arrhythmia classification  our neural network performs well and is beaten only by random forests  future
work can explore the use of ecg time series data and let
the neural network extract features from the raw time series
data on its own 
as previously mentioned  the cost of false negatives is very
high  building a classifier to minimize the false negative
rate can bring many benefits to the medical and healthcare
industry  this work allowed us to deeply analyze learning

ficardiac dysrhythmia detection with gpu accelerated neural networks

      

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

        

 

 

 

 

 

 

 

 

 

 

   

   

 

 

 

 

 

 

 

 

 

 

 

 

   

   

      

 

 

 

 

 

 

 

 

 

   

 

 

 

   

 

 

 

 

   

 

 

 

 

   

 

 

 

 

 

 

 

    

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

    

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

    

 

 

 

 

 

 

 

      

 

 

   
   
   
   
   

      

   

 

        

 

 

 

   

 

 

 

 

   

 

 

   

 

 

    

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

      

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

    

 

 

 

 

 

 

 

      

 

 

     

   
   
   
   
   

    

 

 

 

 

 

 

 

 

 

 

 

 

   

    

 

 

 

 

 

 

 

 

 

 

 

 

   

    

 

 

 

 

 

 

 

 

 

 

 

 

   

    

 

 

 

 

 

 

 

 

 

 

 

 

   

    

 

 

 

 

 

 

 

 

 

 

    

    

 

 

 

 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

             

   

svm  train 

 

 

 

 

 

 

 

 

             

   
 

 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

 

  

 

 

 

   

   

   

  

 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

 

  

  

  

  

   

   

   

logistic regression  train 

   

 

 

 

 

 

 

 

 

 

 

 

        

 

 

 

 

 

 

 

 

 

 

   

    

 

 

 

 

 

 

 

 

 

   

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

    

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   
   
   
   

   

 

 

 

 

 

 

 

 

 

 

 

    

 

 

 

 

 

 

 

      

 

 

    

 

 

 

 

 

 

 

 

 

 

 

 

   

    

 

 

 

 

 

 

 

 

 

 

 

 

   

    

 

 

 

 

 

 

 

 

 

 

    

 

 

 

 

 

 

 

 

             

   

   

random forest  train 

      
   

   

   

neural network  train 

 a  confusion matrices  training set 
      

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

    

 

 

 

 

 

 

 

 

    

 

 

    

 

 

 

 

 

 

 

 

 

 

 

 

 

 

    

 

 

 

 

 

 

 

 

 

 

 

 

    

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

             

svm  test 

   

      

 

 

 

 

 

 

 

 

 

 

 

   

   

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

    

 

 

 

 

 

 

 

 

 

 

 

 

   

    

 

 

 

 

 

 

 

 

 

 

 

 

   

   
   
   
   
   

   

 

    

 

 

 

 

 

 

 

 

 

 

 

 

    

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

             

   
   
   
   
   
   
   
   
   
   
   

random forest  test 

   
 

  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

   

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

 

  

 

 

 

  

 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

 

 

 

 

 

  

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

  

  

  

  

   

   

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

   

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

 

 

 

 

    

 

 

 

 

 

 

 

 

 

 

 

 

    

 

 

 

 

 

 

 

 

 

 

 

 

   
   

 

 

   

logistic regression  test 

   

   

   

   

      
   

    

 

 

 

 

 

 

 

 

 

 

 

 

    

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

             

   
   
   
   
   

   

neural network  test 

 b  confusion matrices  test set 
figure    confusion matrices for all models for the multi class classification problem  predicted labels are on the y axis and true labels
are on the x axis  numbers indicate the number of records with a specific true predicted labeling  labels         and    are not shown
because these labels do not appear in the dataset 

algorithms using real data  we hope future researchers and
students can build on this work 

references
bache  k  and lichman  m  uci machine learning repository        url http   archive ics uci 
edu ml 
guvenir  h altay et al  a supervised machine learning algorithm for arrhythmia analysis  in computers in cardiology       pp          ieee       
hoefman  emmy  bindels  patrick je  and van weert 
henk cpm  efficacy of diagnostic tools for detecting cardiac arrhythmias  systematic literature search 
netherlands heart journal                      
jerez  jose m et al  missing data imputation using statistical and machine learning methods in a real breast cancer
problem  artificial intelligence in medicine           
          
juang  chia feng  chen  teng chang  and cheng  weiyuan  speedup of implementing fuzzy neural networks
with high dimensional inputs through parallel processing on graphic processing units  fuzzy systems  ieee
transactions on                     

song  mi hye  lee  jeon  cho  sung pil  lee  kyoung joung  and yoo  sun kook  support vector machine based arrhythmia classification using reduced features  international journal of control automation and
systems                 
steinkrau  dave  simard  patrice y  and buck  ian  using
gpus for machine learning algorithms  in proceedings of
the eighth international conference on document analysis and recognition  pp            ieee computer
society       
thakor  nitish v and zhu  yi sheng  applications of
adaptive filtering to ecg analysis  noise cancellation and
arrhythmia detection  biomedical engineering  ieee
transactions on                     
zhang  guoqiang peter  an investigation of neural networks for linear time series forecasting  computers  
operations research                        

fi