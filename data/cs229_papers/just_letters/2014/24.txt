predicting the diagnosis of type   diabetes using electronic
medical records
oliver bear dont walk iv  david joosten  tim moon
december         

 

introduction

   

feature selection

the vast majority of features extracted from the
dataset were binary features related to diagnoses and
prescriptions  in order to reduce the number of features to a manageable number  filter feature selection
was applied to find which of these binary features
were most relevant  specifically  the mutual information with diabetes diagnosis was computed for the
    most common diagnoses and the     most common prescriptions  the    binary features with the
greatest mutual information  listed in table    were
used for the learning algorithms 

as of       over     million people worldwide have
diabetes      diabetes puts patients at a higher
risk for blindness  kidney failure  heart disease  and
stroke and it is especially prevalent in the united
states in racial groups with low access to healthcare 
such as native americans          african americans         and hispanics              although the
onset of diabetes mellitus type    dmt   can be prevented or delayed with behavioral changes  e g  physical activity or dietary changes  an estimated       of
people with dmt  in the united states are undiagnosed  in order to improve diagnosis methodologies 
supervised and unsupervised machine learning algorithms trained on electronic medical records  emr 
were implemented and evaluated for effectiveness 

 
   

models
unsupervised learning

an unsupervised clustering algorithm was applied to
provide insight into the distribution of positive dmt 
  features
cases in the feature space  the gap statistic  as defined in hastie  et al       was computed for the data
    dataset and feature extraction
using k means clustering with k     to k      this
this study uses a publicly available emr dataset re  was implemented using the fpc package for r      loleased by practice fusion in      for a kaggle com  cal maxima in the gap statistic were interpreted as
petition      it consists of de identified records for the optimal numbers of clusters 
      patients  among whom       have been diagnosed with dmt   the data was extracted from     supervised learning
   database tables  which include diagnosis histories  medication histories  physician visits  lab re  methodology the emr dataset was used to train
ports  smoking histories  and demographic character  several supervised learning algorithms implemented
istics  we had four original features from the raw in r      in order to evaluate algorithm effectiveness 
input  age  gender  weight  bmi   in addition to indi     fold cross validation was applied to compute the
cator variables for dmt  diagnosis  binary features test error  precision  and recall for each of the models 
were added in the form of indicator variables for med  the train error was also computed to provide insight
ication prescriptions  diagnoses  and anomalous lab into the variance and bias of the models  learning
report results  however  upon inspection  it became curves were obtained by varying the size of the trainclear that practice fusion stripped some data related ing set and computing the test error  precision  and
to lab reports  possibly to de identify patients or to recall 
make the kaggle contest more challenging  thus  features related to lab reports were not included in this naive bayes in order to provide a baseline with
study 
which to compare the results of future models  the
 

fifeature
       mixed hyperlipidemia 
       benign essential hypertension 
lisinopril
       unspecified essential hypertension 
zocor  simvastatin 
       other and unspecified hyperlipidemia 
       chronic kidney disease  stage iii  moderate  
       edema 
     essential hypertension 
lipitor  atorvastatin calcium 
simvastatin
        coronary atherosclerosis of unspecified type of vessel 
        coronary atherosclerosis of native coronary artery 
        osteoarthrosis localized primary involving lower leg 
       peripheral vascular disease unspecified 
       abnormality of gait 
       congestive heart failure unspecified 
lasix  furosemide 
coreg  carvedilol 
cozaar  losartan potassium 

type
diagnosis
diagnosis
prescription
diagnosis
prescription
diagnosis
diagnosis
diagnosis
diagnosis
prescription
prescription
diagnosis
diagnosis
diagnosis
diagnosis
diagnosis
diagnosis
prescription
prescription
prescription

mutual information
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table    binary features with the greatest mutual information with diabetes diagnosis  diagnoses are
indicated with icd   codes  generic names for brand name prescriptions are indicated 
throughout the multi dimensional feature space  knearest neighbors  knn  is a reasonable alternative
to the parametric models explored thus far  by using an odd number of patients mapped in the feature space nearest  using minkowski distance with
parameter    to a sample that requires a class assignment  knn can model highly localized phenomena
and nonlinear behavior  knn is implemented with
the kknn package for r     

naive bayes algorithm was applied to the classification of dmt  diagnosis  as the only generative learning model applied in this paper  calculating p y x 
through estimating p x y  and a prior p y    naive
bayes makes the strong assumption that features are
conditionally independent  it is implemented in the
e     package for r     
logistic regression following on the results of
the naive bayes model  logistic regression was investigated since it make weaker assumptions concerning
the conditional probability distribution of features 
specifically  as a generalized linear model based upon
the conditional mean p y x  subject to the bernoulli
distribution  it does not require features to be conditionally independent nor multivariate normal  further  the data provides a sufficient number of samples
 almost         for the effective use of logistic regression using the glm package for r     

decision trees finally  in order to better communicate the hierarchy of indicators of dmt   this
paper explores a single white box approach using decision trees  by applying this non parametric greedy
algorithm whose objective it is to maximize information gain in a top down search of features  this paper
is able to provide visualization of some of the decision boundaries with respect to individual features 
while this approach is unlikely to capture feature in 

support vector machines  svm  support vector machines  svms  are powerful classifiers that involve constructing a hyperplane decision boundary in
the feature space that maximizes the functional margin with the data  they are especially well suited for
modeling nonlinear behavior since one can use kernels to project data into high dimensional  possibly
infinite dimensional  feature spaces  since the data
was not expected to be separable     regularization
was applied  svms are implemented in the e    
package for r     
k nearest neighbors  knn  since it is possi  figure    decision tree  pruned  showing diagnosis 
ble that patients with dmt  are present in clusters medication and demographic features 
 

fiteraction  since each features decision boundary is
calculated in isolation   it should provide insight into
the feature space unavailable from the other models
explored  this is implemented with the rpart package for r      see figure   for an example decision
tree 

 
   

kernel
linear
polynomial
radial
sigmoid

equation
ut v
ut v   c 

test error
     

d

kuvk 

e

tanh ut v   c 

     
     
     

table    results from    fold cross validation on
svms with several kernels  the cost function parameter is c      the parameter values are        
and c      

results
clustering analysis

results for the gap statistic analysis are shown in
figure    the gap statistic for varying k applied to
the k means cluster algorithm indicates no optimal
number of clusters between k     through k     
specifically  the gap statistic for varying k applied
to the k means cluster algorithm did not yield any
local maximum  and this indicates no optimal number
of clusters between k     through k      further 
the cluster sizes when k     were observed to be
approximately equal  which does not correspond to
the relative sizes of samples with         and without
        a positive diagnosis of dmt  
figure    test error  precision  and recall of an svm
with different values of the cost function parameter 

   

cross validation

results from    fold cross validation are summarized
in table   
model
naive bayes
logistic regression
svm
k nearest neighbors
decision trees

figure    gap statistic indicates data cannot be
meaningfully separated into two classes 

   

test error
    
    
    
    
    

train error
    
    
    
    
    

precision
    
    
    
    
    

table    results from    fold cross validation on supervised learning algorithms 

svm analysis

   

   fold cross validation was used to compute the test
error of svms with several kernels  results are summarized in table    note that results are similar with
the linear  polynomial  and radial kernels  the radial
kernel was chosen for the svm since it projects into
an infinite dimensional feature space  and hence may
better reproduce the nonlinear behavior of the data 
the test error  precision  and recall of the svm were
also calculated as the cost function parameter for   
regularization was varied  results are shown in figure    the error was minimized by choosing the cost
function parameter to be c      although it did not
vary significantly 

learning curves

several learning curves for the naive bayes  logistic
regression  and svm classifiers are shown in figure
  

 
   

discussion
method evaluation

inspecting table    we see that svms and logistic regression are the methods that yield the smallest generalization error  approximately      the
svm is particularly interesting because changing the
 

recall
    
    
    
    
    

fi a 

 b 

 c 

figure    learning curves for naive bayes  logistic regression  and svm classifiers  showing the effect of
training set size on  a  test error   b  precision  and  c  recall 

   

cost function parameter for    regularization causes
a tradeoff between precision and recall  specifically 
increasing the cost function parameter decreases the
precision and increases the recall  this suggests that
the cost function parameter can be adjusted to tune
the precision and recall to match the needs of doctors 

bias and variance

one can estimate the relative contributions of bias
 model limitations  and variance  overfitting  to the
error of a model by comparing the test error and train
error  from table    we see that the test error and
train error are very close for naive bayes and logistic regression  suggesting that the bulk of the error is
due to bias  this is corroborated by their flat learning curves  which indicates that there is some inherent error in the models even when the training set
    data implications
is large  on the other hand  the train errors for knearest neighbors and decision trees are fairly small
each models performance may also indicate charac  compared to the test error  implying that the error is
teristics about the underlying data  firstly  the poor largely due to variance  finally  the svm has a train
performance of the naive bayes model relative to the error that is moderately smaller than the test error 
others may indicate that each feature is not condi  indicating that both bias and variance contribute to
tionally independent of every other feature  further  error in the svm 
the relative weakness in the results of knn combined
with the results of our unsupervised clustering model
indicate that the data is not best described by several  
conclusion
independent clusters in the feature space  instead 
those models that applied a discriminative decision electronic medical records  emr  were used to train
boundary  namely logistic regression and svm  per  learning algorithms for dmt  diagnosis  a variformed best in classifying patients 
ety of supervised learning algorithms were evaluated
 

fiand it was found that svms and logistic regression
produced the smallest error  svms are especially
promising since one can adjust their behavior with
different choices of kernel or cost function parameter
to suit the needs of medical practitioners trading off
false negatives and false positives 
based upon the results  future studies should attempt to reduce the bias present in the logistic regression and svm models  specifically  new features
such as genetic markers  lifestyle factors and more
relevant lab tests  e g  glucose  which was crucially
missing  would provide additional dimensions along
which to separate classes  furthermore  future work
should incorporate time series data  which is crucial
for identifying the onset of dmt  in a particular year 
this will account for the possibility of internal structure that is currently not captured 

statistical computing  vienna  austria       
http   www r project org 
    schliep  klaus and klaus hechenbichler 
kknn  weighted k nearest neighbors  r
package        http   cran r project org 
web packages kknn 
    therneau  terry  et al  rpart  recursive
partitioning and regression trees  r package        http   cran r project org web 
packages rpart 

references
    dimitriadou  evgenia  et al  misc functions
of the department of statistics  e       tu
wien  r package              http   cran 
r project org web packages e     
    hastie  trevor  et al   estimating the number
of clusters in a data set via the gap statistic 
journal of the royal statistical society series
b  vol    no     p           
    hennig  christian  fpc 
flexible procedures for clustering  r package version  
       
     http   cran r project org 
web packages fpc 
    identify patients diagnosed with type   diabetes  kaggle  july           accessed october          https   www kaggle com c 
pf     diabetes 
    idf diabetes atlas   th edition  international diabetes federation        accessed
october           http   www idf org 
diabetesatlas introduction 
    national diabetes statistics report 
estimates of diabetes and its burden in the
united states  centers for disease control
and prevention        atlanta  ga  u s 
department of health and human services 
accessed october           http   www 
cdc gov diabetes pubs statsreport   
national diabetes report web pdf 
    r core team  r  a language and environment
for statistical computing  r foundation for
 

fi