classification of musical playing styles using midi
information
chet n  gnegy
center for computer research in music and acoustics  ccrma 
stanford university
chet ccrma stanford edu
abstractthe automation of playing style classification is
a crucial step in the analysis of musical information  in this
work  style is taken to be a more functional description of a
specific instrument within a song rather than a description of
the genre or articulation in which that instrument is playing 
it is shown that by using a supervised learning approach  we
can classify the playing style of an instrument as bass  rhythm 
lead  or fingerpicking with a very high accuracy  several machine
learning algorithms are tested  the most successful one was the
support vector machine with a gaussian kernel  was found
to provide an accuracy of       using a leave one out crossvalidation technique  additionally  we use a recursive feature
search to analyze which subset of a larger feature set is most
important for this classification task 

i 

examples of songs by bach  the beatles  duke ellington 
eminem  led zeppelin  metallica  tim mcgraw  slayer  and
willie nelson  each song consists of one or more tracks  each
hand tagged with one of the four class labels  an analysis
of the data is done and a vector of features is collected  the
features will be discussed in detail in a following section 
we investigate the use of several different learning algorithms  namely decision trees  k nearest neighbors  logistic
regression  quadratic discriminant analysis  and support vector
machines  the most successful of these will be selectively
tuned using a feature search 
ii 

i ntroduction

in this study  we investigate the use of machine learning as
an automatic classifier for playing styles in midi tracks  more
specifically  our goal is to use a supervised learning approach
to identify whether a segment of midi data is best described as
a bass part  a lead part  a rhythm part  or a fingerstyle part  we
have seen the application of machine learning to style classification before for different definitions of the word style  two
notable examples are the modeling and replication of the genre
of a piece of music  and the classification of the articulation
of the playing style   rather than looking at these aspects of
music  we will look at the function of a music track in a
greater context  i e  the baseline  or the melody   these classes
will be designated bass  lead  rhythm  and acoustic 
respectively  as humans  we can easily recognize each class
to have the following characteristics  bass  typically a single
line of deep tones being in support of the harmonic structure
of a song  lead  a melodic line such as a vocal performance
or guitar solo  rhythm  typically repeated chords to provide
rhythmic and harmonic structure to a melody  and acoustic 
characterized by playing multiple individual notes or melody
lines  often demonstrated by independent musical lines being
performed on the same instrument  it should be mentioned
that while the tracks by prominent guitarists such as chet
atkins or tommy emmanuel are intended to be classified as
acoustic  the acoustic class is also well suited for piano
performances  the reason is that both styles will likely have
a melody line supported by harmonic structure and or a bass
line  even though the class names were picked based on guitar
related terminology  the instrument for which these tracks were
intended is not of interest in this study  several examples of
data samples are shown in figure   
the training data consists of roughly    midi tracks
spanning many genres  to name a few  we have chosen

data c ollection

the midi songs that were used were either exported from
the guitar tablature software  tuxguitar  or obtained from
an online guitar tablature database  or from an online midi
fingerstyle database    each track is tagged with a class in logic
pro using the text tool  to reduce the number of outliers in
the data set  each track was scrutinized to make sure that the
class label was representative of the entire track  for example 
  tuxguitar 

sourceforge net projects tuxguitar 
tab database  http   www ultimate guitar com 
  fingerstyle guitar file collection  http   www acousticfingerstyle com 
  guitar

figure    an examples of each class type  the samples from
top to bottom are intros to pink floyds money  led zeppelins
since ive been lovin you  fleetwood macs go your own
way  and john loudermilks windy and warm 

fiif a rhythm track has a two or three measure stretch in which
a brief solo is played  that solo section is omitted 
the tracks are partitioned into sections  typically twelve
measures long  this increases the amount of data points
provided to our algorithms  at the expense of the assumption
that the data is identically and independently distributed 
and allows us to maintain short time characteristics in the
performances  in other words  if we did not partition the
tracks  any measure to measure variation would be averaged
out during feature computation  each feature is normalized in
a preprocessing step such that all examples of any particular
feature are within the range          even though it is typical
to normalize to a mean of zero and a standard deviation of   
this was shown to produce somewhat higher accuracy rates 
iii 

f eatures

many features were proposed  ranging from simple averages and standard deviations to more complex ones that were
often designed for separating two specific classes  most often
for distinguishing rhythm from lead or acoustic  to
aide in the definitions of our features  we will first discuss
terminology  first  readers should be aware of standard midi
notation for pitch    for example  bb  is a b  in octave
   midi pitch      and c  is middle c  midi pitch     
there will be some discussion of note groupings  which we
will define to be any set of notes that are played simultaneously
in a single midi track  note groupings will be designated
by     where i is the ith note in some set  a note will be
designated using a superscript  for example ij is the j th note
in note grouping i  each note evaluates to its defined midi
pitch  the median pitch in a note grouping is designated by
  the number of note groupings in a set is designated by
      the pitch class  often called chroma  of some note  i  
will be designated c    pitch class is similar to pitch but is
invariant to changes in the octave of the note  i e  a  and
a  are of the same pitch class  a  
the features that have been considered for this classification problem are as follows 
mean pitch  the average of the midi pitch values
median pitch  the median of the midi pitch values
lowest pitch  the minimum of the midi pitch values
highest pitch  the maximum of the midi pitch values
pitch standard deviation  the standard deviation of the
midi pitch values
duration  the average of the midi note lengths  rather
than use the raw durations  we scale the durations and
take the base   logarithm such that sixteenth notes  eighth
notes  quarter notes  and half notes have durations of    
       and    respectively 
duration standard deviation  the standard deviation of
the midi note lengths  we use the scaled log of the raw
duration data as seen in the duration feature 
play count  the average number of note groupings per
measure
note count  the average number of notes per measure
repetition  the average number of notes that were played
in the previous note grouping
  midi

numbers  http   newt phys unsw edu au jw notes html

polyphony  average number of notes being played simultaneously  this is measured using the overlap of notes
and not simply counting the number of note on events
occurring at any given time 
polyphonic separation  given a time in which multiple
notes are playing  the average difference between consecutive midi pitches  zero for monophonic tracks 
polyphonic repetition nthresh    given that a note is polyphonic  greater than nthresh notes in a grouping   the
average number of pitches that are common between a
note grouping and the previous note grouping  zero for
monophonic tracks 
percent polyphony nthresh    the percentage of the sample
in which the number of playing notes exceeds nthresh  
coverage  the probability distribution       for pitch class
is computed on a per measure basis  for each note
grouping  we sum the probabilities corresponding to each
note  coverage is the average of this metric for all note
groupings in the sample as computed in equation    this
represents the amount of tonal diversity in the sample 
repeatedly strumming the same chord will produce a high
coverage  playing a scale will produce a low coverage 
note that even though coverage is probability based  the
coverage of a sample will exceed one if multiple octaves
of the same pitch class are routinely played  let n
represent the number of notes groupings  each of size
qn   in    
c       

n qn
  xx
p c np        
n n   q  

   

jump size  the absolute value of the difference between
midi pitch numbers from one note grouping to the next 
each note grouping is represented as the median of its
pitches  let n represent the number of notes groupings
in    
n
  x
 n  n   
   
j       
n n  
lyricality  a measure of the singability of a sample  lines
that alternate in pitch are given a negative lyricality 
and lines that monotonically increase or decrease are
given high lyricality  repeated notes are scored as zeros 
this metric scales according to the jump size and the
length of the alternating or monotonic segment  the exact
computation of a lyricality score for a length l subset of
  is seen in equation    each alternating  monotonically
increasing  and monotonically decreasing sequence of
note groupings in a sample is given a score         we sum
the scores from the monotonic sequences and subtract the
scores from the alternating sequences  averaging by the
number of note groupings  as seen in equation   
        l

l
x

 n  n   

   

n  



x
  x
 i     mono 
 j     alt 
l       
    
i
j

   

fiother features were used  some based on pitch autocorrelations  repetitions  and onset quantization to a quarter note or
eighth note grid  we will avoid discussion of these features
because they were eliminated during feature selection  the
means of feature selection will be discussed in a later selection 
iv 

data s eparability

the features in the previous section  especially the latter
ones  were designed in response to looking at the distributions
in early stages of this study  using polyphony  note count  and
mean pitch alone  we can make a rudimentary classification
and correctly predict the majority of bass data points  as well
as a decent fraction of the lead points  however  discerning
the acoustic class from rhythm and even lead with any
reasonable accuracy is a hopeless endeavor  figure   shows the
distribution of the data with respect to these simple features 
the more complex features are quite useful for distinguishing
between parts labeled rhythm and acoustic  figure  
shows a subset of the feature space in which the rhythm
and acoustic classes are fairly separable 
it is worth mentioning that an additional reason that the
data is not completely separable is because the class assignments are not completely objective  while there are parts that a
human would unquestionably recognize as a lead part  such as
a guitar solo  there are other parts with similar characteristics
that function as a rhythm part  consider the thrash heavy
metal genres  during a verse  it is typical for a distorted
guitar to repeatedly pick a low  often drop tuned  open string 
occasionally playing a fifth chord or a complex riff  even
though this may be a low polyphony  even technically intricate
guitar lick  it still functions as a rhythmic backing track  the
classification algorithm is likely to mistake it for lead or even
bass if the mean pitch is low enough  in this case  the context
of the song matters  similar ambiguities exist between rhythm
and acoustic parts  there is a grey area between finger picking
and strumming  most notably  a hybrid picking approach in
which a guitarist will hold a pick between their thumb and
index finger and pluck different strings using their other three
fingers  simple piano parts can also walk the line between
the rhythm and acoustic labels  for these cases  it would
not be uncommon for human musicians to debate the ground
truth labeling  we therefore do not expect perfect classification
from our algorithm in these cases  of course  we will look at
the errors of our algorithm to make sure that they are indeed
examples of a disputable ground truth 
v 

example  it does that to each one of the training examples
in turn and computes the accuracy as the fraction of correct
classifications  rather than testing on individual data points 
we test on all of the data points that came from an individual
song  we will refer to this as loocvsong   this is done
because we cannot robustly estimate the test error of the
algorithm if there are data points from the same track in the
training and test set  recall that the tracks are partitioned into
measure groupings and that we get a small cluster of points
from a single track rather than a single point   this would
cause the accuracy estimate to be inflated  by ensuring that
no song appears in both the training and test set  we can be
confident that the test data are not being overfit  it has the
added benefit of being much faster than traditional loocv 
algorithm
decision tree
logistic regression
k nearest neighbors  k     
k nearest neighbors  k      
quadratic discriminant analysis
svm linear
svm polynomial kernel
svm rbf kernel

accuracy
     
     
     
     
     
     
     
     

table    we have several algorithms that perform reasonably
well  it is clear that decision trees and qda are not good
candidates for a learning algorithm 
table   shows the results  it is clear that k nearest neighbors  k       performed the best of all of the algorithms 
but does that make it the most appropriate to use  to learn
about how the class labels were being decided  the algorithms
were tested on a subset of the feature space  two features at a
time  and the decision boundaries were viewed on a  d plot 
some of these results are shown in figure    note that we
dont expect performance that is representative of training on
the whole feature space  we only want to look at the nature
of the decision boundaries  as we can see from the figure 
the k nearest neighbors decision boundary is quite jagged  it
is expected that even a noise in the test set could cause different classes to be assigned  the decision boundaries for the
support vector machines and for logistic regression look much
more smooth  the linear svm had decision boundaries that

c hoosing a l earning a lgorithm

now that we have a set of features to test on  it is time
to pick a learning algorithm  some immediate choices come
to mind  logistic regression  quadratic discriminant analysis 
and support vector machines  svm   the scikit learn  python
module came with a wealth of other options  so the data were
tested using decision trees and k nearest neighbor algorithms
as well 
the testing procedure uses a modified version of leaveone out cross validation  loocv  to estimate the accuracy of
the learning algorithm  standard loocv removes a single
training example from the training set  trains the algorithm on
all but that example  and then tries to classify that training

figure    using note count  mean pitch  and polyphony  we
can see a high degree of separation for the bass class  but
the acoustic class is not separable 

fifigure    using polyphonic separation  jump size  and coverage  the rhythm and acoustic classes become more
separable  almost none of the bass examples have polyphony
and are therefore clustered on the axis 

were straight lines  as the name suggests  as did the logistic
regression algorithm  the svms with polynomial and gaussian
kernels had curvature and provided a more appropriate looking
fit  a fifth order polynomial kernel and a radial basis function
 rbf  kernel were used for the latter two svms  respectively 
the linear svm uses a one vs the rest multi class strategy  and
the other svms use a one vs one strategy 
it was expected from looking at the data that a nonlinear decision boundary is well suited to fit the data  for this
reason  though we include an analysis of the linear svm in
the feature search section  we ultimately expect to pick an
algorithm with smooth  non linear decision boundaries for the
final implementation 
vi 

f eature s earch

we have seen decent performance across several different
algorithms and would now like to get the best performance
over each particular algorithm  we do this using a common
approach  a recursive feature search  the procedure is as
follows 
  
  

  
  

find loocvsong accuracy   using some feature space
backwards search
a  remove the ith feature
b  find loocvsong accuracy  i
c  replace the ith feature
find highest k values of i
for feature i in top k features  such that i    
a  remove the ith feature
b  recurse using feature space without feature i for
top k features
c  replace the ith feature

figure    the decision boundaries above were generated
by training on the entire dataset and excluding all but two
features  this does not give us an estimate of the accuracy of
training on the full feature space  but it will give an idea of
what decision boundaries look like  the k nearest neighbors
decision boundary is very jagged and is not expected to be
very robust to noise  the svm boundary is much smoother 

algorithms  as it is fitting a curve to an infinite dimensional
feature space 
algorithm
logistic regression
nearest neighbors  k      
svm linear
svm polynomial kernel
svm rbf kernel

table    after an individual tuning using the recursive feature
search  we have optimized the performance of five  high
performing algorithms 

the optimal feature space for our svm was found to be
mean pitch  median pitch  lowest pitch  highest pitch  pitch
standard deviation  duration  play count  note count  repetition  polyphony  polyphonic separation  polyphonic repetition
 n     percent polyphony  n     coverage  and jump size  it
was a bit surprising to not see higher accuracy using some
of the more complex features  lyricality  onset quantization 
and a couple autocorrelation based features were not especially
useful for this classification task 

vii 
of the    or so features that were designed for this
classification task  approximately    remained in the optimal
subset for any particular algorithm  fortunately  we see in
table   that the accuracy results have improved quite a lot 
the choice of learning algorithm is now quite obvious  the
support vector machine using the rbf kernel has an accuracy
of        and the smooth  non linear decision boundary that
was discussed in the previous section  it is not surprising
that the svm with the rbf kernel outperformed the other

accuracy
     
     
     
     
     

m isclassifications

the confusion matrix for the svm using the rbf kernel is
shown in table   with the precision and recall statistics  most
of the contents of the matrix lie on the diagonal  indicating
correct classification  it is clear that the algorithm doesnt have
much difficulty classifying bass examples  but it makes the
occasional mistake on other classes  there were generally three
types of observed error in the cross validation  we will now
look at examples of each 

figuess

correct class

r
l
b
a

r

l

b

a

   
 
 
  

 
   
 
 

 
 
   
 

 
 
 
   

r
l
b
a

prec 

recall

     
     
     
     

     
     
     
     

figure    the introduction to slayers angel of death is classified as lead rather than rhythm even though its function
in the song is to support the vocal line 

table    the confusion matrix for the svm  and associated
precision and recall statistics

the first  and largest sources of error are between the
acoustic and rhythm classes  this not too surprising  as it
was discussed above  these two classes are not easily separable
and share many similar properties  an example of this is shown
in figure    the sample is most likely played with a pick with
the exception of the higher notes that have the same onset as
the bass notes  which are are probably played with the ring 
pink  or middle fingers  an example of hybrid picking   a quick
listen to this sample in its original context should be convincing
enough that the most appropriate label is rhythm  but out of
context one could reasonably debate that acoustic is a good
label 

figure    no doubts dont speak is classified as acoustic 
but the correct class is rhythm  taken out of context  perhaps
neglecting that no doubt is a punk band   it would not be
surprising for a human to mistake this as a fingerpicked part 

there are also a few instances of lead being mistaken for
rhythm  one of which is shown in figure    for a lead part 
this example exhibits quite a bit of polyphony  it is no surprise
that the algorithm was fooled  the other lead parts that were
misclassified were for the exact same reason 

figure    an excerpt from a guitar solo in led zeppelins since
ive been lovin you  the high polyphony near the end of the
sample causes this lead part to be classified as rhythm 

finally  repeated riffs with low polyphony that function as
rhythm parts are often labeled as lead  figure   shows an
example of this  slayers angel of death  the full sample
is the shown riff repeated a couple of times  this isnt
an easily singable melody that we would typically label as
lead  metallicas enter sandman riff is actually classified as
acoustic by the algorithm 

viii 

c onclusions

using a support vector machine with a gaussian kernel 
we can successfully classify musical data into categories of
playing style  an accuracy of       is shown  but we will
still bother to mention that for some examples  the ground
truth labeling is not completely objective  some musicians may
argue that some of the mislabeled samples are actually correct
 and for that matter that some of the correctly labeled samples
should be labeled otherwise  
ix 

f uture w ork

this study is intended to be the first part of an algorithmic
composition application that does composition in a similar
style to some given set of midi files  once the computer
has labeled the input tracks according to playing style  it can
analyze the tracks appropriately  extracting rhythm  chords 
and tonality information from the rhythm and bass tracks 
melodic and harmonic information from the leads  and a bit
of everything from tracks with the acoustic label 
acknowledgment
thanks to roger dannenberg  anders oland  ryan rifkin 
and julius smith who provided helpful advice throughout the
course of this project 
r eferences
 

shlomo dubnov  gerard assayag  olivier lartillot  and gill bejerano       
using machine learning methods for musical style modeling  computer
        october              
  dannenberg  thom  and watson  a machine learning approach to musical
style recognition  in      international computer music conference 
international computer music association  september        pp          
  pedregosa  f  and varoquaux  g  and gramfort  a  and michel  v  and
thirion  b  and grisel  o  and blondel  m  and prettenhofer  p  and weiss 
r  and dubourg  v  and vanderplas  j  and passos  a  and cournapeau  d 
and brucher  m  and perrot  m  and duchesnay  e   scikit learn  machine
learning in python  journal of machine learning research  vol     pp 
               

fi