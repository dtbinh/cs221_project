predict  seizures  in  intracranial  eeg  recordings  
  
linyu  he   linyu   stanford edu   and  lingbin  li   lingbin stanford edu   

 

abstract  

this  project  aims  to  predict  seizures  in  intracranial  electroencephalography   ieeg   recordings  using  four  algorithms   the  data  are  a  series  of     
minute   ieeg   clips   labeled   preictal    positive    for   data   recorded   prior   to   seizures   or   interictal    negative    for   data   recorded   between   seizures    our  
goal  is  to  distinguish  between  the  two  states   the  major  challenge  is  that  the  data  are  highly  imbalanced   i e    the  number  of  positive  examples  is  less  
than       of  that  of  negative  examples   our  work  is  to  make  modifications  to  each  of  the  four  models  and  analyze  the  corresponding  performance  
gain     

 

introduction  

spontaneous  seizures  are  the  typical  symptom  of  epilepsy   which  is  a  common  but  refractory  neurological  disorder  that  afflicts  nearly      of  the  
worlds   population    anticonvulsant   medications   are   administered   to   many   patients   at   high   doses   to   prevent   seizures    but   their   effectiveness   is  
limited   and   patients   often   suffer   their   side   effects    even   for   patients   whose   epilepsy causing   brain   tissue   is   removed   via   surgery    spontaneous  
seizures  still  persist   due  to  the  seemingly  unpredictable  occurrence  of  seizures   patients  with  epilepsy  experience  constant  anxiety        
this   project   aims   to   make   it   possible   that   devices   designed   to   monitor   patients   brain   activity   can   warn   them   of   impeding   seizures   so   that   patients  
are   able   to   take   appropriate   precautions    it   is   also   helpful   to   reduce   overall   side   effects   caused   by   anticonvulsant   medications   taken   by   these  
patients   by  providing  them  with  devices  with  the  ability  to  predict  an  impending  seizure   anticonvulsant  medications  could  be  administered  only  
when  necessary   thus  lowering  the  doses  given  to  patients   
multiple   researches   support   the   notion   that   the   occurrence   of   seizures   is   not   random    according   to   evidence   shown   by   related   researches    for  
patients   with   epilepsy    the   temporal   dynamics   of   brain   activity   can   be   classified   into       states    interictal    between   seizures     preictal    prior   to  
seizures    ictal   seizure   and  postictal   after  seizures    the  brain  activity  of  each  state  can  be  recorded  by  ieeg        our  goal  is  to  employ  machine  
learning  techniques  to  learn  from  ieeg  data  the  characteristics  of  preictal  states  and  then  distinguish  these  states  from  the  interictal  states   after  
one  preictal  state  is  identified   a  warning  should  be  sent  to  the  patient  to  prepare  him  or  her  for  an  impending  seizure   

 

data  and  feature  extraction  

kaggle  provides  ieeg  data  collected  from  canine  subjects   the  data  of  each  subject  is  organized  into     minute  clips  labeled  preictal   positive   for  
data   recorded   prior   to   seizures   or   interictal    negative    for   data   recorded   between   seizures    each   clip   contains        channels   of   ieeg   data   where  
each  channel  corresponds  to  one  electrode  implanted  in  the  subjects  brain   for  each  training  example                         is  the  label  and         is  a  clip  in  
which  each  row  corresponds  to  one  channel  and  each  column  corresponds  to  ieeg  readings  at  one  sampling  time  point   
since   seizures   in   most   patients   are   associated   with   a   stereotypic   eeg   discharge   with   characteristic   spectral   pattern    we   employed   the   following  
feature  extraction  procedure        
apply  fast  fourier  transform  to  each  channel  in  a  clip  and  divide  the  resulting  power  spectrum  into     bands   delta              hz    theta           hz    
alpha                hz     beta                 hz     low gamma                 hz     and   high gamma                  hz     in   each   band    sum   the   power   over   all   band  
frequencies   to   produce   a   power in band    pib    feature    therefore        features   are   obtained   in   each   channel   and        features   are   obtained   in   one   clip   
the  procedure  above  is  also  illustrated  in  fig      where      is  the  power  spectrum   
delta           hz               
theta         hz             

channel     
channel     

alpha          hz      
        

fft  

beta           hz      
         

  
  
  

low gamma           hz      
         
high gamma            hz       
         

channel      
fig      feature  extraction  using  fft  

 

cross  validation  

there   are          examples   in   total    in   which          are   negative   and         are   positive    for   each   time   of   cross   validation    we   randomly   pick         of   the  
negative  examples  and       of  the  positive  examples  for  training  and  use  the  rest  for  testing    this  process  is  repeated  for       times  to  calculate  the  
average  evaluation   

  

   

fibesides   the   training   error   and   the   testing   error    the   following   values   are   adopted   to   evaluate   the   performance   of   each   model   since   the   data   are  
highly  imbalanced   
  
table      values  chosen  to  evaluate  the  performance  
name  
definition  
   
accuracy   acc   
  
         

positive  predictive  rate ppv  precision  
  
   

true  positive  rate   tpr  recall  
  
   

false  negative  rate   fnr  miss  rate  
  
   

false  positive  rate   fpr  fall out  
  
   
  
in   table               and    are   the   number   of   true   positives    true   negatives    false   positives   and   false   negatives    respectively    finally    the   receiver  
operating  characteristic   roc   curves  and  precision recall  curves  will  be  plotted  based  on  values  in  the  table  above   

 

learning  algorithms  

in   out   attempt   to   seek   a   solution    three   models   covered   in   cs         were   first   adopted    which   are   logistic   regression    nave   bayes   classifiers   and  
support   vector   machines    svms     later    we   employed   a   model   inherited   from   communication   systems    which   makes   a   prediction   based   on  
correlation  coefficients  between  the  test  example  and  all  training  examples   modifications  are  made  to  each  model  to  improve  their  performance  on  
an  imbalanced  data  set   in  the  following  discussion   we  use                 to  denote  each  training  example  where              is  the  set  of  features  and  
             is   a   label        corresponds   to   a   negative   label   and       corresponds   to   a   positive   label    in   svms                where        corresponds   to   a  
negative   label   and       corresponds   to   a   positive   label     we   use     to   denote   a   query   point    test   example    and     to   denote   the   label   predicted   by   a  
model   
   

logistic  regression  

in  logistic  regression   the  hypothesis  is      

 
    where     is  the  parameter   the  probability  of  
        

     conditioned  on     and  parameterized  by  

  is                   
usually   the  prediction  that        is  made  if            since  the  data  set  is  highly  imbalanced   i e    the  number  of  negative  examples  is  much  
larger  than  that  of  positive  ones   we  consider  it  more  important  to  correctly  classify  more  positive  test  examples  than  to  have  a  few  false  positives   
therefore   the  decision  threshold  of       can  be  less  than        which  makes  it  more  likely  to  classify  a  test  example  as  positive   we  set  the  decision  
threshold  to  be     where                and  plot  the  cross  validation  results  when  choosing  different  values  of      which  is  shown  in  fig       

fig      the  relation  between  the  decision  threshold     and  accuracy   recall  and  precision  of  the  logistic  regression  model  
  
it   can   be   seen   in   fig        that   when            both   accuracy   and   precision   are   high    but   the   recall   is   not   satisfactory    when          we   achieve   the  
maximum  recall  but  the  accuracy  and  precision  are  very  low   so  a  trade off  has  to  be  made  between  recall  and  precision accuracy  by  choosing  an  
appropriate  value  of      for  example   if            both  accuracy  and  recall  are  high  and  close  to  each  other   meaning  the  accuracy  for  classifying  all  
test  examples  and  the  one  for  classifying  positive  test  examples  are  close   the  low  precision  when           is  caused  by  the  increased  number  of  
false  positives   which  is  acceptable  to  some  extent  since  false  positives  are  less  important  than  true  positives  in  seizure  prediction   

  

   

fi   

nave  bayes  

the  multinomial  distribution  is  used  to  model  the  features  of  each  ieeg  clip   since  the  value  of  each  feature  is  continuous   we  first  discretize  the  
values  into     groups  where    

    
 

         is  the  maximum  value  of  the  features  of  all  clips  and     is  the  group  size   similar  to  the  modification  made  

to   logistic   regression    a   test   example   is   labeled       if                   where     is   a   positive   constant   specified   to   overcome   the  
imbalanced   data   when   making   predictions    fig        indicates   that   the   nave   bayesian   model   cannot   make   satisfactory   predictions   no   matter   what   the  
value  of     is   

fig      the  relation  between     and  accuracy   recall  and  precision  of  the  nave  bayesian  model    
   

support  vector  machine   svm   

in  the     regularized  svm   the  hypothesis  is                 where  parameters     and    are  obtained  by  solving  the  primal  optimization  problem  
whose   objective   is  min   

 
 



 

 

 
          where  

 
        is   the   cost   term    since   it   is   more   important   to   correctly   classify   more   positive   test  

examples   than   to   have   a   few   false   positives    the     cost sensitive   svm     c svm          is   adopted   in   which   two   different   costs   are   assigned   to   negative  
and  positive  examples   respectively   in   c svm   the  objective  of  the  primal  optimization  problem  is  min   
 

 
 



 

   

 

    

      

         is  a  trade  off  between  the  classification  margin  and  misclassified  or  non separable  examples  and  the  cost  factor    

  
  

    

    where  

  is  the  ratio  of  

costs  between  positive  and  negative  examples   we  employ  the  libsvm  library       as  our   c svm  implementation   

  
fig      the  relation  between         and  accuracy   recall  and  precision  of  the   c svm  model  
  
different   values   of        and       are   chosen   and   the   corresponding   results   of   accuracy    recall   and   precision   are   shown   in   fig         satisfactory  
accuracy  and  recall  can  be  achieved  when        is  chosen  as                               or                 etc   
   

correlation  decision  

we  derived  the  correlation  decision  model  from  a  few  concepts  in  communication  systems   the  previous  three  models  discard  the  training  set  after  
a  hypothesis  is  built   however   in  correlation  decision   we  do  not  use  training  examples  to  build  a  hypothesis  and  we  keep  the  entire  training  set   

  

   

ficonsider   a   given   query   point        calculate   its   correlation   with   each   training   example   and   assign   a   score   to   each   of   them   

                           find   the   training   example   with   the   maximum   score               since          is   the   training  


example  that  the  query  point  is  most  similar  to   we  can  make  a  prediction  that              
in   order   to   classify   more   positive   examples   correctly    in   other   words    to   output   more   positives    we   increase   the   scores   for   positive   training  
examples  by  a  factor        namely               for  all  positive  training  examples     

fig      the  relation  between     and  accuracy   recall  and  precision  of  the  correlation  decision  model    
   

others  

we   also   tried   extracting   features   using   pca   and   ica    using   different   kernels    such   as   the   gaussian   kernel   and   the   sigmoid   kernel    in   svms    and  
capturing  non linear  behaviors  of  features   such  as  log       and         but  we  didnt  achieve  improvement  in  performance   

 

results  and  discussion  

table     shows  the  results  of  the  models  discussed  above   where  for  each  model   the  results  before  and  after  our  modification  to  these  models  are  
compared    for   each   modified   model   except   nave   bayes    the   decision   parameters   that   achieve   an   acceptable   performance   are   shown    among   all  
chosen  models   logistic  regression  with  a  threshold  of        works  best   in  whose  results  both  accuracy  and  recall  are  close  to        
  
table      performance  results  of  different  models  
decision    
training    
test    
model  
fnr  
fpr  
acc  
precision  
recall  
parameters  
error  
error  
         
        
        
        
        
     
     
     
logistic  regression  
          
        
        
        
        
     
     
     
nave  bayes  

q       

        

        

   

   

     

nan  

   

q          

        

        

        

        

     

    

     

        

        

        

        

     

     

     

        

        

        

        

     

     

     

r         
       

   

        

        

        

     

     

     

            

   

        

        

        

     

     

linear  
kernel   
c       
 c svm  

r       
linear  
kernel   
c         

correlation  decision  

     
  
fig      shows  the  roc  curve  and  the  precision recall  curve  of  each  model   the  area  under  curve   auc   is  calculated  by  using  the  trapezoidal  areas  
created  between  each  point        it  can  be  seen  that  logistic  regression  outperforms  others  since  its  aucs  for  roc  and  precision recall  curves  are  the  
highest     correlation  decision  model  also  provides  high  aucs  for  both  kinds  of  curves   which  indicates  its  performance  is  close  to  that  of  logistic  
regression     
seizure  prediction  is  usually  performed  in  real  time   table     gives  the  comparison  of  average  runtime  for  different  models   the  test  is  performed  on  
the  same  pc  with  a     ghz  cpu   it  can  be  seen  that  logistic  regression  consumes  the  least  time   since  it  also  has  the  highest  prediction  performance   
it   is   the   most   cost efficient   algorithm   in   this   situation    although   the   performance   of   the   correlation   decision   model   is   as   good   as   that   of   logistic  

  

   

firegression    it   runs   much   more   slowly    since   it   has   to   keep   track   of   all   training   examples   during   the   prediction   process    therefore    we   consider  
logistic  regression  with  threshold  modification  as  the  best  model  in  this  project   

  
fig        the  roc  curve  and  precision recall  curve  of  each  model  
  
table      the  average  runtime  of  different  models      of  training  examples               of  test  examples            
model  

logistic  regression  
             

nave  bayes  
  q           

 c svm  
  c         r          

correlation  decision  
               

runtime   in  seconds   
          
          
          
         
  
the  major  challenge  of  this  project  is  the  imbalanced  data   what  weve  done  so  far  is  to  sacrifice  false  positive  rate  to  achieve  a  low  false  negative  
rate  because  a  false  negative  is  far  more  dangerous  than  a  false  positive  in  seizure  prediction   the  current  results  are  within  our  expectations  but  
they   are   not   good   enough   since   we   believe   the   information   extracted   from   the   limited   number   of   positive   examples   is   not   enough   to   perfectly  
distinguish  between  the  two  classes   

 

conclusions  and  future  work  

three   supervised   learning   models   covered   in   cs      and   one   model   inherited   from   communication   systems   are   employed   in   this   project   to   predict  
the   occurrence   of   seizures    modifications   are   made   to   these   models   to   deal   with   the   highly   imbalanced   data    among   the   four   models    logistic  
regression   outperforms   others    which   obtains   the   highest   aucs   for   roc   and   precision recall   curves    meanwhile    when   choosing   the   decision  
parameter  properly   both  accuracy  and  recall  of  logistic  regression  are  close  to        
an   important   method   to   deal   with   imbalanced   bits       and       in   wireless   communication   is   to   code       into        and       into        so   that   the  
numbers   of   both   classes   become   balanced    weve   been   trying   to   apply   similar   ideas   to   the   project    but   have   yet   got   a   satisfying   result    so   the  
exploration  will  be  continued  to  seek  better  solutions   
also    instead   of   using   a   single   model   to   build   a   classifier    attempts   can   be   made   to   combine   the   predictions   of   different   models   and   develop  
strategies   to   make   a   final   decision    models   involved   in   the   combination   may   differ   in   their   feature   extraction   process   since   it   is   possible   to   develop  
for  each  model  the  features  that  best  fit  the  model   

 

references  

      kaggle  inc           kaggle   the  home  of  data  science    online    http   www kaggle com c seizure prediction  
      j   jeffry  howbert  et  al     forecasting  seizures  in  dogs  with  naturally  occurring  epilepsy    plos  one   vol       no       p   e               
      yun   park    lan   luo    keshab   k    parhi    and   theoden   netoff     seizure   prediction   with   spectral   power   of   eeg   using   cost sensitive   support   vector  
machines     epilepsia       no        pp                      
      chih chung   chang   and   chih jen   lin   
http   www csie ntu edu tw  cjlin libsvm   

        

libsvm  

    

a  

library  

for  

support  

vector  

machines   

 online    

      jesse  davis  and  mark  goadrich    the  relationship  between  precision recall  and  roc  curves    in  proceedings  of  the    rd  international  conference  
on  machine  learning          pp             
  

  

   

fi