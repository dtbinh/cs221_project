hierarchical classification of amazon products
bin wang
stanford university  bwang  stanford edu

shaoming feng
stanford university  superfsm  stanford edu

abstract   this projects proposed a hierarchical
classification strategy for amazon products 
classification strategy and the inference efficiency
were the two focuses  a strategy to apply general
classification
algorithms
to
hierarchical
classification problem was proposed  parallel
programing and hashing techniques were applied
to improve inference efficiency 

i  introduction
automatic hierarchical classification is drawing
more attentions nowadays  with the explosion of
information on the internet  managing information
by classifying them into hierarchical categories is
becoming more and more important  internet
encyclopedias like wikipedia  medias like
newspapers and online retailers like amazon are the
major demand sources 
this project aims to solve the hierarchical
classification problem for amazon products  in this
problem  each product will be classified into a
hierarchical category where both the destination
category and the parental categories are clearly
specified  there are two major challenges in this
problem  efficiency and accuracy  due the huge
size of amazon product catalog  there are enormous
possible assignments for a product  hence finding
the most possible assignment will be a computational
consuming as well as time consuming process  an
efficient classification strategy must be developed so
that each product can be classified fast enough to
keep retailers and customers on amazon from
waiting for too long  besides efficiency  accuracy is
also an important consideration  if the products
cannot be classified accurately  there will be troubles
for both customers who want to search for a product
and for retailers who want to maintain their catalog 
these two challenges in hierarchical classification
are also the focuses of this project  after the problem
formulization in section ii  our methods will be
presented in section iii and be evaluated in section

iv  conclusions and future works are presented in
section v 

ii  problem formulization
a  data set

the dataset is a collection of amazon products with
highly detailed product information and their preassigned labels  categories  
the labels are organized hierarchically as shown in
figure    it is different from a simple tree structure
since a node may have multiple parents  e g  node
      in figure   has node     and node   as its
parents  such structure is called directed acyclic
graph  dag      as what the name implies  the
relations between nodes are directed and there is no
cycle in the graph 
 
 

 
   

   

   
     

     

figure    category hierarchy

there are      labels in total in the directed acyclic
graph  each label is a node in the graph and its sublabels  sub categories  are its children nodes  the
graph has a single root  most of the label nodes has a
depth  distance from root node  with each edge a
length of    of    where the maximum depth is   
also  each product may multiple labels which locate
at different places in the graph 
the label information of a product is stored in the
attribute browsenodes  they are described as a
path from root node to its label  since the product
may have different labels  there may be multiple
paths in this attribute 

fieach product has    attributes where each attribute
contains multiple properties and sub properties with
rich information for a certain aspect  in this
project pruned editorial reviews was the
particular attribute that was used to build our base
classifier  they are the concise product description
provided by the vendor or some trusted parties 
b  accuracy improvement

improving the accuracy for hierarchical classification
in general is too broad a topic for a term project 
hence we narrowed down the scope to finding a
good strategy for applying a given base classification
algorithm 
given a classification algorithm as our base
classification algorithm  such a nave bayes or
logistic regression  there are many ways for them to
be applied to a hierarchical classification problem 
and for different problems  the best base algorithm
may also vary  hence  it is more important to find a
good strategy for applying the base algorithm  which
was set as the major goal for accuracy improvement
in this project  detailed strategies will be shown in
next section 
c  efficiency improvement

there are around        categories and more than
        products in the catalog  efficiency will then
be an important concern  to improve the inference
efficiency  i e  to minimize the time used for
prediction  both programming techniques and
algorithm improvement were explored 
d  evaluation metrics

both accuracy and efficiency were evaluated in this
project 
accuracy per layer  depth  was the major criteria for
different strategies  this is because this metric does
not only capture how well a strategy is doing  but
also reflects the advantage and disadvantage of a
strategy in detail 
inference time and training time were the major
criteria for efficiency evaluation  inference time
refers to the time that an algorithm needs to make a
prediction while training time stands for the time
needed on training  inference time is a more crucial
requirement for applications since it affects user
experience and system setup directly  a shorter
training time will make development less expensive 

iii  methodology
a  efficiency improvement

i  parallel computing
the training and prediction of every nodes were
applied in a breadth first search  bfs  order  it
means for nodes at the same layer  depth   the data
selection  training and prediction could be done in
parallel  due to the existence of global interrupt lock
 gil  in python  only a single core can actually be
utilized in pythons multi thread module  thus multiprocess module must be applied to speed up the
performance  however  writing speed to global data
then became a problem for different processes  so the
balance between computing and writing speed needs
to be carefully handled  till now   x speedup was
achieved on a   core cpu with hyper threading 
ii  classifier and data hashing
after training a classifier  it will be saved to disk for
later use  since a label may appear multiple times
during prediction  hashing the classifiers will boost
up the speed 
in the original program  all data were saved as a list
which was processed one by one in order to filter out
those that belong to a certain label so they can be
trained  however  during the prediction process  if
the grandchild of a node needs to be predicted  there
is no need to search for the data again since we
already have all the needed information when
predicting that nodes child  thus a data hash table is
created to hold the data for later use in next layer in
the bfs order 
b  accuracy improvement

i  base classifier
multinomial nave bayes classifier was chosen as
the base classifier in this project  pruned editorial
review of each product was used as the input
feature  the reviews were first encoded into ascii
characters and then stemmed  after stemming  some
stopwords were removed to reduce the feature
dimension  at last  term frequencyinverse
document frequency  tf idf  was applied to weight
each word properly 
ii  classification strategy
flat strategy
flat strategy is brutal style strategy used as a base
line  in this strategy  the hierarchical structure is

fiignored  in other words  all nodes in the graph are
considered equally  for example  book and art
book doesnt have any relationship anymore  so all
the       labels will be trained together in this
strategy 
top down strategy 
in a top down strategy  a path to a label from the root
will be predicted for a given  considering the
directed graph of the labels  several algorithms can
be applied on generating a label path 
 greedy search 
greedy search is a special case of k beam search
which will be covered in the section below 
in greedy search  when predicting from any label
node  the child with most probability will be chosen 
 k beam search 
k beam search is more complicated and has lots of
variations  the general idea of k beam search is that
instead of choosing the child with most probability
directly  it will choose k children with highest
probability  then at the very end  leaves of the graph 
the most possible one will be picked 
the variation of this algorithm locates where when
counting the probability  multiple ways of
calculation can be used  node probability  path
probability and discounted probability  figure  
shows an example for future explanation 
given a product  assuming it has     probability to
be a book and     probability to be kitchen for
classifier root  and for classifier book  it has a
    chance to be art book  and     chance to be
fiction book  similarly for the classifier kitchen 

root
   

   
book

art
   

kitchen

fiction
   

dish
   

chop 
   

figure   classification example

o

node probability 

node probability doesnt have any historical
information when counting probabilities  in the

above example with k    under classifier root  both
book and kitchen will be kept in the result
because they are the   node with highest
possibilities  however  when predicting the next
level of labels  the result would become dish     
probability  and fiction      probability   which
are higher than art      probability  and
chopsticks      probability   in other words 
information about their parent will be ignored 
o

path probability 

for path probability  the multiplication of the
probabilities of every node on the path would be
used to choose the highest k probabilities  in k  
case  book and kitchen would be kept for the
first step without any change comparing to node
probability  but for the next level of prediction 
art and fiction will be kept as their probabilities
are                  and                   which are
higher than the probabilities of dish             
      and chopsticks                    
o

discounted probability 

discounted probability is a variation of path
probability with weight on the probability of
different levels  in path probability in the example 
art is calculated as                   here a
coefficient is added  so the probability becomes
           where the value of  and  needs to be
adjusted at run time 
combined strategy
since different strategies may achieve the best
performance at different layer  it is better to combine
strategies to maximize the overall performance 
first of all  different variation and parameters of kbeam search will be tested  to find the parameters   
 and k  with highest accuracy for each layer  then
for a given product  if all the variations give a label
in same layer  the result from the methods that has
the highest test accuracy in that layer will be chosen 
if different variations predict a label in different
depth  only those results that are at the best
performance of each method are considered  among
all the considered results  the one with highest testing
accuracy with deeper layer will be chosen 
lets take the following situation where   beam
search performs best at depth      beam search
performs best at depth   and greedy search performs
best on reset of the depths as an example  for a given
product  if all methods predict the label to be in

fidepth    the result of   beam search would be chosen
since it performs best at depth    if   beam search
predicts the result to be a label at depth      beam
search predicts a label in depth   and greedy search
predicts the label to be in depth    only the result of
  beam search and greedy search would be
considered since only greedy search and   beam
search gave predicts at the layer where their testing
accuracy is better than other variations  and for this
case  the result of greedy search would be chosen
since it performs better at the deeper layer 

extremely low  which means flattening the labels and
train them in a traditional way doesnt fit the needs
of hierarchical classification problem at all 
c  top down strategy

table   shows the prediction accuracy for different
top down strategy with various parameter settings 
after experimenting with different discounting
parameters  no discount performs best in this case 
table   top down strategy performances

greed
y

k   beam

k   beam

k   
bea
m

k   beam

combined
strategy

l 

     

     

     

    

    

     

l 

     

     

     

    

    

     

l 

     

     

     

    

    

     

l 

     

     

     

    

    

     

l 

     

     

    

    

    

     

l 

     

     

    

    

    

     

l 

     

    

    

    

    

     

l 

     

    

    

    

 

     

l 

     

    

    

    

 

     

this methodology ensures that no degrading will
happen in the balance of accuracy at different layers 

iv  result and evaluation
a  feature engineering

table   shows the feature selection process for the
base classifier in this project  though improvement
of base classifier is not our major concern  some
effort was spent to make sure it has a moderate
performance 
table   feature selection for base classifier
     
data

accuracy in  

      
data

      
data

train test train test train test
reviews
coding

in

reviews
coding

in

utf  
    

         

         

    

    

         

         

    

reviews with stemming     

         

         

  

reviews with stemming
and stop word
    

         

       

    

ascii

as shown in the table  the greedy search  k   beam  provides a fairly good accuracy comparing to
k beam search  however  through layer   to layer
   k    beam performs better than greedy search 
hence the greedy search will be major parameter set 
when combined strategy was applied  it achieve the
best overall accuracy 
d  efficiency improvement
table   efficiency improvement

b  flat strategy
table   flat classification performance
accuracy in  

      data        data        data
train test train test train test

mono layer prediction     

   

    

         

original

single
product
prediction
time

total
prediction
time

training
time

       ms

       s

   day

      s

   hours

improved           ms

   

as we can see from the result of flat strategy  the
accuracy of both training and testing data are

here is a comparison between initial version of our
k    beam prototype and the final optimized version
with hashes and parallel computing 

fitotal prediction time was improved by   x and
training time was improved by   x  which is
reasonable with   processes and hashing  also  due
to the limitation of multi threading module in python
interpreter as explained in above section  it is
believed that a great leap in prediction time will be
achieved with c or other language where multi cores
and good memory sharing efficiency are available 

v  conclusion and future work
this project explored the strategies of applying base
classifier on a hierarchical classification problem as
well as methods to boost both inference and training
speed  combining greedy search and k beam search
with different parameters after experiments is the
best strategy that was found  parallel programing and
hashing techniques were proposed as the solutions to
improve efficiency 
but it is interesting that the accuracy of k beam
search does not increase with the increase of k  this
implies the fact that the discounted path probability
cannot truly reflect how good a path is  hence
looking for a better way of scoring different search
strategies for each path will be the most important
work for the future 
acknowledgement  we want to say thank you to prof 
ashutosh saxena and aditya jami for providing the
project materials and the weekly advice  also we want to
say thank you to prof  andrew ng and cs    tas for
their help during the quarter 

reference
    sun  aixin  and ee peng lim   hierarchical
text classification and evaluation  data
mining        icdm       proceedings ieee
international conference on  ieee       
    babbar  rohit  et al   on flat versus
hierarchical classification in large scale
taxonomies   advances in neural
information processing systems       

fi