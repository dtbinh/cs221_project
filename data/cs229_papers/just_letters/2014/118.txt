contours and kernels  the art of sketching

dan guo  paula kusumaputri  amani peddada

abstract
given sketches of everyday objects  we seek to
accomplish two goals  the first is to classify the
object depicted within the sketch  the second
is to study different styles and varied interpretations in the depictions of the same object category  we employ supervised learning algorithms
as the main method of classifying sketches into
their respective classes  with our fine tuned multiclass svm attaining       accuracy  to understand varied styles and portrayals of objects 
concepts that inherently are subjective and contain more vague metrics  we use unsupervised
learning algorithms to discover common structure in the data  we find that by clustering  we
group sketches with stylistically or structurally
similar features  these groupings yield important insight into how we can obtain computational interpretation of more general  subjective
qualities 

   introduction
sketching has existed as one of the original ways humans
have used to depict their narratives and journeys  despite
being such an ancient media of expression  this art form
has been tremendously unexplored through computational
methods  with only introductory work having been completed  eitz et al          in our study  we aim to explore
the domain of sketches in two ways 
first  we interpret sketches as carriers of information  depicting real life objects  in this part of the study  we want to
see exactly how well sketches can be equated with their real
life counterparts  namely  we attempt to classify a sketch
according to the object that is drawn 
secondly  we consider sketches as an art form  where the
individuality and creativity of the artist are emphasized 
given drawings of objects within a certain category  we
note that there is a large degree of variability depending
on the author of a sketch  we thus seek to understand what

stanford university

 dguo       paulaksp  amanivp   stanford   edu

types of styles might be used in the creation of these drawings and what are different ways that artists might represent
the same everyday object 

   dataset
dataset  we base our analysis on an already existing crowd sourced dataset  collected and released with
the paper  eitz et al          the dataset consists of
       human produced drawings  depicting     object
categories  each sketch is labeled with the correct object category  representing our ground truth values  the
sketches are limited to only isolated objects  with no backgrounds  surrounding contexts  or colors  and are provided
in png and svg  temporal order of strokes  format 
features  given the common use of bag of features feature extraction  sivic et al         in image  and photobased classification  we apply the same techniques to
sketch based classification  the features of sketches are
derived using the histogram of gradients  hog  technique
 dalal   triggs         this featurizing algorithm divides
the png images into a grid of cells  within each cell  the
technique first builds a visual vocabulary of common line
orientations  the method then counts frequencies of gradient orientation and builds a histogram from which    
features can be extracted  the hog descriptors are invariant to local scale geometric transformations  as the features
are derived from a particular cell  dalal   triggs        
we then use this feature representation  in the form of    dimensional real valued vectors  to represent each sketch 

   methods
we utilize various supervised learning algorithms for object classification in sketches  we test the algorithms  all
except one vs one svm  with    fold cross validation  reducing data loss  we also use unsupervised learning algorithms to understand the different styles and representations in a sketch  because there does not necessarily exist a
metric to test against  our style analysis is fairly qualitative
and examines individual images to understand structure between sketches 

ficontours and kernels  the art of sketching

     supervised learning
note that knn and gda were algorithms used mostly for
rapid prototyping  we implemented these to test the   
fold cross validation wrapper algorithm and error analysis
infrastructure 
       k n earest n eighbors
to serve as a guiding diagnostic  we implement a variant
of the k nearest neighbor classifier  the naive knn classifier examines the training instance closest in euclidean
distance to a given test example by features  and then outputs that training example s category as the test example s
prediction  this corresponds to the case when k      guo
et al         
we made several enhancements to this naive classifier  first
it seemed less accurate for the label of the test example to
be determined by only one training example  therefore 
we learned the optimal value of k by varying the values of
k  finding that the best performance is achieved with k    
 see figure    

classifying a test example is equivalent to finding the class
where the probability of observing the test example s features is maximized 
       m ulticlass svm
with the high success of svm s in classification related
problems  we implement two variants of the multiclass
svm as described in  hsu   lin        
one vs all svm for each object category in our dataset 
we train an l  regularized  soft margin svm that classifies whether a given sketch belongs in that category or not 
thus  we learn a total of     svm s  for a test example
x  we loop through every classes s svm  classifying x as i
where i is
x
ji k sij   x    b
arg max
i          

j

of the different kernels experimented  the quadratic kernel
k x  y     xt y   c   performed optimally 
one vs one svm for all pairs of categories x  y   we
generate an svm   sv mx y   that classifies whether a
sketch is of x or y   we classify test example z as the
class
x
c   arg max
i sv mx y classifies z as c 
c

x y  x y

because this requires an svm for all unique pairsof objects  the one vs one svm algorithm requires    
svm
 
classifiers 
figure    plot of how classification accuracy varies with the value
of k  number of neighbors   the accuracies are the average of
   fold cross validation runs  with the modified voting scheme
discussed 

we simultaneously modified how the k neighbors voted
on the classification of the testing instance  a naive approach would be to tally up the classifications of the k
neighbors and assign the test example the majority s classification  guo et al          we instead enhanced the vote
so closer neighbors have more weight in the classification
of the test example  we categorize a test example x with n
   k nearest neighbors of x by euclidean distance dist   
p
as the class i that maximizes nn i class n  i 
dist x n   
       g aussian d iscriminant a nalysis
the inspiration behind gda is that our task is  fundamentally  predicting data on the similarity of input features 
so it follows intuition to construct general models for our
classes  thus we describe the distribution of features for
each of the     possible object categories  from this point 

       n eural n etwork
the superlative performance of neural networks within
other computer vision tasks suggested they would be appropriate for this study  in specific  we build a single layer
neural network with     nodes  another with     nodes 
a deep neural network with two layers of varying sizes 
and     binary neural network classifiers for each category  we find that the network with two layers performed
best  with     and     nodes  respectively  which follows
a heuristic that the number of hidden nodes should be approximately the mean of the input and output layer sizes 
each neural network is a supervised feed forward  backpropagation network  where the output layer is a softmax
classifier  and the hidden layer contains a standard nonlinear function  the tanh function  applied at each level
to our input features  during training  examples are fed
through the network  and the results are used to calculate
gradients that we then apply iteratively in stochastic gradient descent to derive our matrices  biases  and other parameters  as is common with back propagation  rojas         

ficontours and kernels  the art of sketching

testing involves feeding new examples through the layers
to obtain a vector of probabilities  which is then translated
into the predicted category 
       h ierarchical c lassification with svm
with the success of one vs all svm  we decided to develop a more intuitive way of choosing which subsets of
categories to train our svms on  to obtain more fine tuned
class recognition  we therefore chose to construct a hierarchical tree of categories  see figure    from our data  we
do so in the following manner  begin with a set of    
nodes  where each represents one object category   iteratively create subtrees by setting the next two nodes  or
other subtrees  that are most commonly misclassified for
each other as children of a new internal node  this eventually generates a tree whose structure describes the level of
misidentification between various sets of object categories 
with entries sharing the same parent node being frequently
misidentified  for every internal node in the tree  we then
train a binary svm to predict whether a test example belongs to the left or right child  classifying a given example
involves passing it through the tree  and outputting the first
leaf node the sample reaches 

similarity of i to any other cluster which i is not a member 
we run k means clustering using different numbers of starting centroids  each for multiple runs  and pick the number
of cluster centers that routinely achieves the largest silhouette value over all data points 

   results
     supervised learning results
we test the data with various supervised learning algorithms  with the results below in table   
for sketch classification  we find that one vs all svm produces the best results  with an accuracy of       that improves over the     performance provided by the paper
 eitz et al         
     unsupervised learning results
when we apply k means to the object categories  we find
optimal cluster numbers range from   to     we stop at this
point  since further grouping begins to assign individual
points to unique clusters  which does not yield additional
information 

   discussion

figure    svm hierarchical tree  the root of the tree is in the
top left  note that nodes that share a direct parent are more likely
to be misclassified  we have two closeups showing the misclassifications in groups of vehicles and objects that look very similar 

     unsupervised learning
       k m eans c lustering
to understand what styles or representations of objects are
commonly found  we strive to understand structure within
an object category  for each object category  we run kmeans clustering on its constituent points  however since
a given object category may differ in the number of unique
clusters  for each object category  e g  mug  seagull  etc  
we learn the optimal number based on the data s silhouette
b i a i 
values  rousseeuw        of point i  s i    max a i  b i  
where a i  is the average similarity of i with all other data
within the same cluster and b i  is the lowest average dis 

knn  knn is a simple classifier examining the categories
of the closest neighbors in order to make a classification  in
a feature space where many different object categories exist in close proximity to one another  predicting simply by
the nearness of neighbors is susceptible to noise  the error
from knn appears to be a variance problem that can likely
be reduced with more data  a larger dataset from a representative distribution of artists can improve performance
levels  we can understand this by recognizing that as the
number of training sketches grows  a given test example
will look more similar to a now larger set of training examples drawn from that same category  at the moment  we
find that a test example is close in distance to a few sketches
of the same class  but is also near drawings of other objects 
which is the source of many misclassifications 
gda  observing the results in table    it appears that gda
is susceptible to overfitting  as its training error is fairly
low while its test error remains high  suggesting a variance
problem with gda  specifically  gda makes the assumption that the distribution of features given a class label is
multivariate gaussian  in many object categories  however 
this assumption does not hold  since there are several common representations of the same object  thus the variation
of sketches may not follow a canonical gaussian curve  in
fact running a normality test confirms that most features
with statistical significance are indeed not gaussian given
the class labeling 

ficontours and kernels  the art of sketching
table    training and test accuracy  all models  excluding one vs one svm  were evaluated with    fold cross validation 
m odel

t raining accuracy

t est accuracy

data s et s ize

gda
knn
one   vs   all svm
one   vs   one svm
n eural n etwork
h ierarchical c lassification

      
    
    
    
      
    

      
      
     
      
      
     

      training        testing
      training        testing
      training        testing
      training        testing
      training        testing
      training        testing

svm  the one vs one svm is a cleaner algorithm to train
as we build classifiers between only two distinct object
categories  instead of the one vs all variant  the drawbacks  however  seem to outweigh this potential benefit 
for instance  one vs one requires an svm
 for every pair
of distinct object categories  which is    
svm s to train 
 
this makes testing and handling the one vs one system extremely inefficient  such that    fold cross validation is infeasible  additionally  it would not scale well to larger
numbers of categories  another drawback is our one vsone does not keep track of the score outputted by each
svm  instead  the current implementation simply classifies an object by the class that the greatest number of pairwise svm s decides on  this does not allow for more nuanced decisions  where the relative confidence from different svm s is taken into account 
the one vs all svm excels in the task of classification  outperforming the other algorithms by a considerable margin 
this is perhaps because svm s generally excel at binary
classification tasks more so than other methods and are generally more robust  placing fewer restrictions on the structure of the data  as with gda  for instance  despite regularization  interestingly  the set of svm s attains     
accuracy on the training set  this implies that the data are
linearly separable and do not have very large outliers  at
least in the quadratic kernel space  it is also interesting to
note that there are certain test examples where every one
of the     svm s achieves a negative score  this suggests
that for each object x  the binary svm classifies the test
example as not of the object x  in many cases  this is
an expected outcome  since the negative examples for each
svm encompass a larger range of features spanned by    
categories  as opposed to those of the positive examples 
which span only a single category  this could perhaps be
addressed by picking random subsets of our negative examples for classifier training 
looking at close ups in figure   of our hierarchy tree 
the common misclassifications of objects by the one vsall svm demonstrate that object category labels play large
roles in the accuracy  as there is significant overlap between
some classes  either very similar to each other  barn and
house  or one a subset of the other  chair and armchair  

figure    confusion matrix of multiclass svm     fold cross
validation   warmer values  or larger values correspond to more
correct classifications  the left axis is truth classes and bottom
axis is predicted classes 

this may suggest that we can actually build a set of categories that will allow us to classify more accurately  without losing additional information 
hierarchical classification  the idea behind implementing hierarchical classification is that with a clear hierarchy
in object categories  svm s can be used to decide between
large groups of objects  and more specific svm s can then
progressively produce finer classifications  the central issue with this method is that the hierarchical tree of object
categories that we generated is less balanced than what we
had anticipated  specifically  for the majority of the tree 
an internal node has two children  one which is a leaf node
and one which is the rest of the tree  because of this structure  when we build an svm classifier for each internal
node  the classifier will often decide between a single object category  leaf node  and the rest of the objects  rest
of tree   and if the svm were to achieve a positive score
on that object category  the algorithm simply classifies the
test example as such  since we do not consider the rest of
the tree during our testing  there are many chances for a
test example to incidentally be misclassified as the single
object category  the hierarchical system thus did not lend
itself well to classification because its structure is prone to
premature classification decisions 
neural network  it is also interesting that the neural

ficontours and kernels  the art of sketching

network s performance was not as superlative as expected 
given nn s common use in computer vision classification
tasks  this might be simply due to the parameterization
of the network  more finely tuned functions and weights
may be needed  but most likely it is because of the large
variability in the training and testing data  such that the network tries to capture too large a range of relationships between components within the sketches  thus producing less
definitive results  therefore  a standard network may not
actually be suited to this task  and different variants of the
network might need to be considered  convolutional neural networks  for example  
k means clustering  to understand styles and different
interpretations in sketches  we cluster within particular object categories  using pairwise distance as a metric  these
clusters capture quite well the variability with which the
object is drawn  quite often  the clusters within an object
category mirror the diversity of drawing style and representations of the same object  to give a few examples  in
figure    we have samples from the two bear clusters  cluster one represents more realistic bears with four paws  in
cluster two  the bears are standing on its hind legs  these
are fundamentally two different positions that a bear can
assume and the clustering algorithm clearly separates these
two cases  as a second example  the two clusters of vans
are based on the level of detail in the sketch  the vans in
cluster one are all very busy  with many lines sketched onto
the side of the vehicle  the vans in the second cluster have
an empty body and are more simplistic in flavor  the style
of the sketch  in this case the rhetorical choice of the level
of realism or abstraction to depict the van  is a way the
algorithm has clustered the sketches  we likewise see in
our clusters a similar trend with object orientation  where
sketches within a category are grouped by alignment 

line orientation distributions will be more sparse and the
features are able to distinguish between very busy and very
cleanly drawn objects  note however that there are object
category clusters that do not have any apparent differences 
for instance  we expected that clustering in the category of
books would result in opened and closed books  but instead
each cluster has both opened and closed books  and with
varying viewpoints of the object 

   conclusion and future work
we have presented various algorithms to classify a dataset
of sketches into their object categories  we have demonstrated that we are able to achieve reasonable classification
of       using multiclass one vs all svm  we also evaluated different classifiers with varying degrees of success 
though none outperform the one vs all svm 
we believe sketching is a vast domain to explore  and
would like to continue improving performance on object
recognition within sketches  we aim to further pursue the
approach of a hierarchical cluster by building the hierarchical tree in a more sophisticated manner  this might produce a more balanced structure  thus improving classification 
another possibility is to experiment with more elaborate features that include spatial and temporal information
about the sketch  to better learn the more subjective components and drawing style  with a more complex feature extraction method  we could additionally cover up portions of
the image  extract features from the image  and classify the
image to understand which parts of the image are most correlated with its identity  this yields insight on what makes
a certain category what it is and perhaps how we ourselves
internally identify everyday objects 

references
dalal  navneet and triggs  bill  histograms of oriented gradients for human detection  in proceedings of the      ieee computer society conference on computer vision and pattern
recognition  cvpr      volume     volume     cvpr     ieee computer society       
eitz  mathias  hays  james  and alexa  marc  how do humans sketch objects  acm trans  graph 
 proc  siggraph                         
guo  gongde  wang  hui  bell  david  bi  yaxin  and greer  kieran  knn model based approach
in classification  in on the move to meaningful internet systems       coopis  doa  and
odbase  pp          springer       

figure    vans  top  and bears  bottom  clustering  clusters by
k means clustering 

hsu  chih wei and lin  chih jen  a comparison of methods for multiclass support vector machines  neural networks  ieee transactions on                mar       issn           
doi                    
rojas  raul  neural networks  a systematic introduction  springer       

because our features are related to the distribution of lineorientations  we have confidence in the clustering results 
diverse orientations and representations of the same object
produce varied orientations of lines  which the hog features are able to capture and k means is able to cluster between  with more abstract or simple sketching styles  the

rousseeuw  peter j  silhouettes  a graphical aid to the interpretation and validation of cluster
analysis  journal of computational and applied mathematics                      issn
          
sivic  j   russell  b c   efros  a a   zisserman  a   and freeman  w t  discovering objects and
their location in images  in computer vision        iccv       tenth ieee international
conference on  volume    pp         vol     oct      

fi