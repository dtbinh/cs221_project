cs     final report  december     

making sense of the mayhem 
machine learning and march
madness
alex tran and adam ginzberg
stanford university
atran  stanford edu ginzberg stanford edu

i 

introduction

the goal of our research was to be able to accurately predict the outcome of every matchup in
a march madness bracket  this is an extraordinarily difficult problem because of the high
amount of variance in college basketball and
the sheer number of games that are played in
the tournament  in the history of march madness  no one has ever created a perfect bracket 
last year  warren buffett agreed to give    billion to anyone that submitted a perfect bracket
to the yahoo bracket challenge and         to
the top    best performing brackets  it only
took    games for the last perfect bracket in
the challenge to be eliminated  we wanted to
see how a well formulated machine learning
model and algorithm would perform on this
problem compared with the success of humangenerated brackets  our model hoped to produce the highest performing bracket as defined
by the yahoo bracket challenge 

ii 

data

we scraped espn for the tournament game
data for the last four years of march madness 
we then scraped over    features relating to
each team  for each year we had    training
examples  the total number of matchups in
the tournament  we randomly decided which
team in a matchup to list as team    and then
we classified the specific matchup with label y
if team   won or lost    or   respectively   this
structuring gives us     training examples in
total with approximately half having output
label   and half having output label   

iii 

model

we modeled our problem as a classification
problem  we utilized supervised learning to
train on a set of previous tournament games
and then aimed to classify future games as either a win or a loss for the indicated team  we
characterized each game using a number of features relating to each team  the first main set
of features that we used were seasonal game
statistics such as field goals  rebounds  assists 
etc  this information established a base profile
for a team  and provided a reasonable estimate
as to how a team performed on a game to game
basis  the second set of features we used were
more complex metrics including strength of
schedule ranking  margin of victory  win   in
last    games  etc  these statistics when used
in combination with our seasonal game statistics proved very useful because they were able
to account for the fact that college basketball
teams often have vastly different schedules and
play disjoint sets of teams  see the appendix
for a comprehensive list of the features that
were available to us 
we implemented a feature extractor that
served several functions for our model  first 
our feature extractor returned a dense map
of the feature values because a feature vector
which is too large proves to be unwieldy for an
algorithm such as logistic regression  then  our
feature extractor normalized all of the seasonal
game statistics by dividing by the number of
games played so that every teams statistics
could be interpreted in the same context  because of the limited size of our dataset  we then
chose to use the difference of team feature val 

fics     final report  december     

ues  where appropriate  in order to reduce the
size of our feature vector  we then rescaled all
of the features to roughly fall into the range    
   so that each feature would have equal footing and so that an algorithm such as logistic
regression would converge more quickly  finally  we derived several new features from our
original raw feature values  for example  in order to account for the quality of a given teams
schedule when weighting its record  we created
a feature which multiplied the teams strength
of schedule ranking by its win percentage taking into account potential interaction between
these two features 
until now  we have only discussed the formulation of our training examples in describing our model  however  another critical aspect of our model was our scoring function 
recall that our goal was to optimize the score
our bracket would receive when submitted to
the yahoo bracket challenge  thus  when creating a bracket  any decision that one makes
in the earlier rounds must persist throughout
the tournament  thus incorrectly predicting a
single upset in the first round can completely
ruin a bracket because the incorrectly predicted
team will appear in future matchups while the
actual advancing team will not  in addition
because our objective was to optimize the score
achieved by the yahoo bracket challenge  we
evaluated the success of our bracket by summing over the scores associated with every correct classification according to this function 
score        round    this scoring function
gives exponentially higher weight to correctly
predicting matchups in the later rounds of the
tournament  we chose to maximize total score
rather than minimize test error 
we now briefly describe the baseline and
oracle that we used in comparison with our
model  our baseline classified the team with
the higher seed as the victor  breaking ties arbitrarily   this baseline is a very natural choice
because it follows the expectation of the selection committee with respect to each team
subject to random noise due to predetermined
seeding rules  the baseline certainly does not
perform poorly  it typically averaged among
 

the   th percentile of human brackets  however it remains a naive approach to bracket
construction  for our oracle we chose the highest performing human bracket for a given year 
this bracket is the best out of millions of entries and reflects an upper bound on the score
we hope to obtain 

iv 

algorithms

the two main algorithms that we used were
logistic regression and an svm 
we briefly mention how we approached
training and testing over the data  we used kfold cross validation  this method segmented
our data into   tournaments to be used as training data and   tournament to be used as testing
data  we trained our algorithms on every possible combination of the   tournaments  and
then tested each on the corresponding tournament that was left out  we then output the
average of our training error  test error  and
points scored over all   folds 
for logistic regression  we trained the
model on our training data using stochastic
gradient descent  we chose to run logistic regression on our data because it is a relatively
simple algorithm that is quite effective  logistic regression creates a linear decision boundary  this property is both an advantage and
disadvantage of the algorithm  because of its
simplicity logistic regression is unlikely to overfit the data  however  at the same time the algorithm has a relatively low ceiling in terms
of performance at least in the base dimension
of our feature space  this is a problem of high
bias  we used annealing to reduce our learning
rate after every update to our parameters  the
update rule for logistic regression 
 i  

 j     j    y i   h   x  i     x j

for support vector machine  we utilized
the library libsvm to run the algorithm on
our data  we worked in an infinite dimensional feature space because we used the gaussian kernel  this is the primary advantage of
svm  working in an infinite dimensional feature space and using the kernel trick allowed

fics     final report  december     

us to capture the interaction between features 
this is particularly valuable for our dataset 
one disadvantage of svm is that it is susceptible to overfitting because in working in an infinite dimensional feature space it is extremely
likely that the extracted data will become linearly separable and hence the model will learn
hidden attributes of our data rather than of the
problem more generally  to fine tune our use of
svm  we ran grid search and cross validation
to find the optimal cost c and  parameters
which are fixed when we optimize the primal
problem 

min

w b 

l
  t
w w   c  i
 
i   

subject to

y  i    w t    x  i       b        i  
 i    

with gaussian kernel 
k   x  i    x   j      e k x

v 

 i    x   j   k   

results

our results are in table   
for logistic regression  we used annealing
to reduce the learning rate after every iteration
according to the equation 
  p

seemed to completely defy reason  for example  last year the two teams in the tournament
final were   and   seeds  an extremely unlikely
event 
the results of folds   and   inspired us
to run a slightly modified experiment  for
each matchup we assign a probability to the
event that a given team wins using our sigmoid function  typically in the range             
originally  we classified that team as going
to win if the probability was greater that     
for our modified version we recognized that if
we believe team   is going to win     of the
time we also believe it is going to lose     of
the time  thus  we decided to simulate     
brackets using these probabilities  and see how
well the best of these brackets performed  i e 
we flipped a coin weighted according to our
generated probabilities at each matchup and
then advanced the appropriate team  this result proved to be quite impressive  in the     
run  our      bracket simulation produced a
bracket that would have performed better than
every single human bracket last year except for
  out of over    million generated 

for svm  we used grid search to tune the
parameters c and  in our algorithm  see
figure   for a graph of these parameters 

 
  of updates made so far

see figure   for a graph of the learning rate 
we were quite pleased with the results of logistic regression  we made significant progress
on the baseline  outperforming it for every fold 
in addition  for folds   and   we were able to
correctly predict the winner of the entire tournament  achieving impressively high scores of
     and       these folds corresponded to the
years      and      respectively which had far
fewer upset teams reaching the later rounds 
folds   and   marginally outperformed the
baseline but the results were not too impressive  in those two years the tournament results

we were also very pleased with our results
for svm  it did not perform as well as logistic
regression in terms of average score  an artifact
most likely due to an inadequate amount of
data  however  svm still correctly predicted
the eventual tournament winners for folds  
and    this suggests that when the tournament
results are more predictable  our current features are sufficient for learning a reasonably
good model  we anticipate that if we were
able to acquire more years of data our svm
would continue to improve  we posit that using the infinite dimensional gaussian kernel
on this small of a dataset is why our svm is
underperforming logistic regression 
 

fics     final report  december     

table    errors and scores for different algorithms and feature vectors

model algorithm

avg  training error

avg  test error

avg  score

    
    
    
    
n a

    
    
    
    
n a

   
    
   
    
    

baseline  choose highest seed 
logistic regression
svm  c               
best of      simulation
oracle  i e  best human bracket

 a  convergence of theta using annealing versus fixed alpha

 b  average score using various
cost and gamma values for
svm

 c  sample verbose output

figure    graphs for annealing and grid search and error analysis output

vi 

analysis

once we had extracted our features and coded
up working algorithms and a scoring function 
we began work in analyzing our results to see
which examples we were misclassifying and
why 
to discover what kinds of features were
relevant we compared the success of various
subsets of features using forward and backward search  we would add or subtract a
feature  run our algorithm to see if our results
improved  if so take action  and repeat  we
discovered that of the game statistics nearly all
of them were useless except for fg   ft   and
 pt   which were marginally helpful  thus 
we discarded statistics like assists  rebounds 
steals  etc  this approach of forward and
backward search only took us so far because
while we had an intuition for what kinds of
features might be useful  our intuitions were
often wrong and a feature that might be helpful in one subset turned out to not necessarily
be helpful in another 
 

our next approach led us to look specifically at which matchups we were getting
wrong and just how wrong we were  i e  how
high of probability did we think a certain team
had of beating another that actually did not  
to do this  we added a verbose option in our
output  see figure   
the verbose output allowed us to pinpoint exactly which matchups we were getting
wrong  which round that matchup occurred in 
and what probability we thought the team we
chose had of winning  because of the nature
of our scoring function  the first matchups that
we looked at were the ones in the later rounds 
reversing whether we correctly classified the
winner of the tournament had a huge effect
on our score  we then looked at matchups
which we were very unsure of i e  those who
we were predicting a certain team to win with
probability      for example  after identifying these such matchups  we then looked at
the features corresponding to each team  our
thought process was  team a has a good sos 

fics     final report  december     

low tos  high fg   etc  and  team b has a
bad sos  bad scoring margin  but really good
win   in their last    games   if team b wins
the matchup  then maybe we should be emphasizing win   in last    games more  we went
along like that iterating our feature extractor
until we were performing as well as we could
without overfitting 

vii 

discussion

our best results with logistic regression averaged a score of       which would be in
the   th percentile of any year  the oracle for
our problem  i e  the best performing human
bracket each year  averaged a score of      
further  the score for a perfect bracket is      
thus  while our average score of      is impressive  we are still incorrectly labeling a large
number of matchups  and we are only about
halfway to the score of a perfect bracket  the
two major factors for our inability to make
further progress are the availability of data and
the inherent difficulty of predicting a perfect
bracket 
as we mentioned earlier  espn only has
detailed data available for the last four years 
this is not much data at all and limits the
amount of learning we can achieve with our
classifiers  we were not able to truly utilize the
power of svm since we would end up simply
overfitting the small set of training examples 
further  since college basketball is an amateur
sport  quality data is generally less available
than the data for professional sports 
beyond the data  this problem is quite simply very hard  basketball is an extremely high
variance sport  and even though you may be

able to determine which team is generally better  there are absolutely no guarantees about
what will happen on one given night  this is
especially impactful because march madness
uses single game elimination  further  the progression of matchups in a bracket makes errors
extremely harmful  for example  incorrectly
predicting that the overall champion will lose
in the first round means that you will lose
out on the points for every matchup that the
champion is involved in 
in the end  while we didnt get close to our
goal of creating the perfect bracket  we have
to be happy with our results as there are simply too many challenges to this problem  it
would be interesting to see how these algorithms would perform a decade down the road
when there are more tournaments and data
available on espn 

viii 

future work

while we have made reasonable progress  there
is still much future work to do  at this juncture 
we feel that time would best be spent collecting data for more tournament years and more
features  from there  it would be very interesting to see what kind of additional features
may prove valuable for this problem  perhaps
qualitative features about the players on a specific team or inter temporal features regarding
a coach and a specific program may be useful  a next step may be to try improving the
algorithm perhaps via a neural network  at
the end of the day  this specific problem is so
complex and suffers from such high variance 
it seems unlikely that any amount of data or
learning could ever generate a perfect bracket 

 

fi