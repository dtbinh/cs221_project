predicting heart attacks
luyang chen  qi cao  sihua li  xiao ju
stanford university
lych  qcao  sihua  xju stanford edu
abstract
this paper aims at a better understanding and application of machine learning in medical
domain  in this paper  we modify three classical models for multiclass problems  logistic regression  naive bayes and svm  and then implement them to predict cardiac arrhythmia based
on patients medical records  first  we use all features provided to build the models  by comparing the accuracy of different models  we find that multiclass naive bayes and svm models
have better performance  afterwards  we implement feature selection to improve the accuracy
of prediction  forward search procedure is used to choose different amount of features for each
model  by comparing the accuracy between using all features and using features selected  we
find that feature selection can significantly enhance the performance of our models 

i 

introduction

in the medical industry  machine learning
algorithms can be used to diagnose some serious diseases  among all diseases  cardiac
arrhythmia is the top one cause of death in
the world  claiming more lives than cancer and
hiv combined  thus  how to predict cardiac
arrhythmia in real life is of great significance 
both to research and application 
in this paper  we apply machine learning
algorithms to predict cardiac arrhythmia based
on a patients medical record  we use the uci
arrhythmia data set for both training and testing  we are provided with     clinical records
of patients  each record contains     attributes 
such as age  sex  weight and information collected from ecg signals  the diagnosis of
cardiac arrhythmia is divided into    classes 
class   refers to normal case  class   to   
represent different kinds of cardiac arrhythmia  such as ischemic changes  old anterior
myocardial infarction  supraventricular premature contraction  right bundle branch block
and etc  class    refers to the rest 
our objective is to classify a patient into
one class according to his or her clinical measurements  this paper applies logistic regression  naive bayes  and svm algorithms to this
realistic problem  compares their accuracy  and

modifies the models to get the best possible
results 

ii 
i 

features and preprocessing

extract valid columns

the dataset contains     columns representing     features  but some data in  
columns are invalid or missing  so these  
columns are completely ignored  we extract
the rest     columns to form a new examples
matrix for modeling 

ii 

data discretization

after observing the raw data provided  we
notice that some features have discrete values 
such as age  sex  etc  while the other features
have continuous  real values  such as the features extracted from ecg signals  e g  amplitude of waves of each channel   since we plan
to use naive bayes algorithm  in which the input examples are discrete valued  we have to
turn all the raw data into discrete values before
computing the probabilities  for the convenience of computation  we divide the values of
each feature into    intervals  for the features
that can only take two possible values   or   
we consider   lies in the first interval and  
 

filies in the   th interval  then we convert each
value into the order number of the interval it
lies in  for example  if the height of one patient
is    cm and it lies in the  th interval  which is
                we assign   to the height of the
patient  thus  all features take values from  
to    

iii 
i 

models

multiclass naive bayes

after data preprocessing  all the features
can only take values from   to     also 
 

k   p y   k 

   

j s y k   p  x j   s y   k 

   

  

 k    

   

 j s y k    

   

logistic regression and svm

since the supervised learning algorithms
we have learnt can only classify examples into
  classes  our first idea is to check whether an
example lies in class   to    one by one until
we find the class it lies in and then stop  we
try both logistic regression and svm 
first  we relabel class   to be   and class  
to    to be    then we can use logistic regression or svm to get a decision boundary  if
this decision boundary tells that a test example
belongs to    we predict it belongs to class  
and then stop  otherwise  we relabel class   to
be   and the other classes to be    we can get
a new decision boundary  which tells whether
the test example belongs to class   or not  we
continue doing this until we find a class for
this test example  if we cannot find a class for
it after the first    trials  we predict it to be in
class    
these algorithms can work  but there are
two flaws  first  why should we start from
class    if we start from a different class  we
might get a different prediction  second  why
should we stop immediately after we get the
first prediction  we can continue  pretending
we havent got a prediction  it is possible that
two different decision boundaries tell an example belongs to class i and class j respectively 
which one should we believe  due to these
two flaws  we want to modify svm later so
that it becomes more reasonable and reliable
when used for multiclass classifications 

ii 

we would like to classify training set into   
classes  to parameterize a multinomial over k
outcomes  we can use k parameters          k
specifying the probability of each of the outcomes  we define k and j s y k as follows 

k   
  

s   

we assume that all x j  y   k are independent  then using formulas of conditional probability  we have the probability of the whole
training set 
m

p  x  y   



n

  j x j i   y y i 

i   

j   


y i 

   

maximizing p  x  y  under the constraints
    and      we get the following formulas 
k  

j s y k  

im     y i   k 

   

m

im     y i   k     x i   s 
j

im     y i   k 

   

and we can also use laplace smoothing to
modify the formulas above 
to make predictions  we use bayes formula 
p y   k  x    

p  x  y   k  p y   k 
p  x  

   

we predict y to be arg maxk p  x  y  
k   p   y   k   

iii 

multiclass svm

in the lecture  we talked about using svm
to classify our training examples into two
classes  we want to modify the algorithm so
that it can be used to make multiclass classifications 
first  we draw k hyperplanes in the feature
space  so that the jth hyperplane  jt x   b j    

fican separate training examples labelled j from
those not labelled j  this can be easily achieved
by using svm algorithms  each hyperplane
tells whether a training example belongs to
class j or not  however  chances are that the ith
hyperplane tells a training example belongs to
class i while the jth hyperplane tells it belongs
to class j  we want to know which one is more
reliable 
we define the geometric margin of a training example with respect to the jth hyperplane
as  j  
  jt x   b j  
j  
   
k j k

not belong to class green  therefore  we predict it to be in class blue  we can easily check
other cases and obtain the same results as the
algorithm achieves  which is quite reasonable 

if  j is small  then  jt x   b j is likely to change
its sign even if the jth hyperplane changes a little bit  therefore  if  j is larger  the conclusion
the jth hyperplane makes whether it belongs to
class j is more reliable 
after we calculate all the geometric margins  we sort them from the largest to the smallest  we first look at the largest one j    if it tells
this example belongs to class j    we trust it and
assign y   j    if it tells this example doesnt
belong to the class j    we also trust it and then
we look at the next largest one j    if it tells this
example belongs to the class j    we trust it and
assign y   j    otherwise  we look at the next
largest one j    until we find some j and it tells
this example belongs to class j  if none of all
the hyperplanes tell this example belongs to
some class  we predict it should be in the class
with the smallest geometric margin 
please look at the following simple example  if we consider   classes with  d feature
space  it is what the algorithm above attains 
first  we use svm to separate blue dots from
the others with a blue dashed line  separate
red dots from the others with a red dashed line
and separate green dots from the others with
a green dashed line  then we draw three lines
which are the angle bisectors and divide the
plane into three areas  now we want to classify
the black dot  we find its geometric margin
to the blue dashed line is the smallest  the
red dashed line tells it does not belong to class
red and the green dashed line tells it does

figure    simple example

so we conclude our algorithm as follows 
table    algorithm

pseudo code
for j     
if y i    j
set y  i      
else
set y  i      
end
run svm using   x  y   to get  j and b j  
end
for j     
calculate  j  

  jt x  b j  
 
k j k

end
sort  j from the largest to the smallest 
find the first j such that  jt x   b j     
if such j can be found
assign y   j 
else
assign y   j    
end
in addition  if the training examples are
not well separable  we can also use svm with
 

fil  regularization  just need to modify this algorithm and run svm with l  regularization for
   times to get    hyperplanes 

iv 

feature selection and results

for classification problems  the accuracy
is a vital performance measurement of the
classifier  to test the accuracy     fold crossvalidation technique is used in our experiments 
since the results we obtain by using all

features are unsatisfactory  we decide to implement feature selection to our above mentioned
models to improve the accuracy 
in class  three kinds of heuristic search
procedures used for feature selection were introduced  forward search  backward search 
and filter feature selection  we choose forward
search because of its easy implementation and
good performance 
for our four models  accuracy before feature selection  after feature selection and numbers of selected features are shown in the following table 

table    results

model
logistic regression
svm
multiclass naive bayes
multiclass svm

accuracy  before 
      
      
      
      

figure    feature selection for logistic regression

v 

discussion

in our project  before feature selection 
naive bayes achieves lower cross validation
error than svm  while after feature selection 
svm achieves lower cross validation error than
naive bayes  we think the problem may lie
in the lack of enough training examples     
 

number of selected features
  
  
  


accuracy  after 
      
      
      


figure    feature selection for multiclass naive bayes

and excessive amount of features       just
as we can see in problem set   when we use
both naive bayes and svm to classify spams
and non spams  svm algorithm works better if
there are more training examples  however  in
this problem  only     training examples are
available 

fias shown in the table    we didnt implement feature selection for multiclass svm  this
is because all the codes are written by ourselves
and they are not efficient enough to complete
feature selection within acceptable time 

vi 

future

there are certainly rooms for improvement  first  we can extract new features  since
we are provided with ecg raw data  methods
like fft and wavelet decomposition can be
used to gain new features that cannot be easily
recognized in time domain  then we need to
give these features some physiological explanations  moreover  we can also use deep learning
methods to generate new features  also  we

can use pca to reduce the dimension of feature space and figure out what features are
informative 

references
    guvenir  h  altay  et al   a supervised machine learning algorithm for arrhythmia
analysis   computers in cardiology      
ieee       
    mishra  binod kumar  prashant lakkadwala  and naveen kumar shrivastava 
 novel approach to predict cardiovascular disease using incremental svm  
communication systems and network
technologies  csnt        international
conference on  ieee       

 

fi