cs    final paper

 

speaker recognition for multi source singlechannel recordings
jose krause perin  maria frank  and neil gallagher

abstract we have applied speaker recognition algorithms to
the problem of speaker classification in multi source  multiple
speakers  of speech signals recorded on a single channel  one
microphone   the goal is to separate the speakers in a singlechannel recording by classifying short time frames of the
recording as one of the speakers  we have evaluated three
different supervised learning techniques commonly used in the
speaker recognition  non negative matrix factorization  vector
quantization  and gaussian mixture models  the feature space of
the first technique is the spectrogram representation of speech
signals  whereas the last two are based on mel frequency cepstral
coefficients  initially  we compared these techniques based on the
classification error rate for a designed recording  then  we
selected the best performing technique and applied it to noisy
recordings of seven speakers in a teleconference meeting 

i  introduction
n todays globalized work place an increasing number of
team projects are executed using online communication
tools for meetings  those online meetings often include a
mixture of native and non native speakers and multiple
speakers talking simultaneously  and are recorded on a single
channel  in addition  different network speeds and locational
background sounds create varying sound qualities for those
speakers  in order to be able to retrace meeting topics
organizations have an increased interest in using recordings to
locate specific parts of the conversation  for this purpose 
among others  it is useful to separate speakers as sources from
those single channel recordings 
the aforementioned scenario is an extension of the classic
cocktail party problem     where  typically  the number of
microphones is greater or equal than the number of sources  in
our problem  however  only one microphone is used to record
a mixture of different independent sound sources 
the global shape of the dft magnitude spectrum  known
as spectral envelope  contains information about the resonance
properties of the vocal tract and has been found out to be the
most informative part of the spectrum in speaker recognition
     thus  speech signals are typically analyze in the frequency
domain  we have studied two different approaches to
representation in the frequency domain  the first one is based
on spectrogram  made up of fourier transforms of short time
frames from the speech signal  this representation tells us how
the spectral shape evolves over time  the second approach is
based on mel frequency cepstral coefficients  mfcc   the key
idea behind the calculation of mfcc is to use a filter bank
spaced according to the mel frequency scale  which
approximates the human auditory system s response more
closely than the linearly spaced frequency bands     

i

we have evaluated three different supervised learning
techniques commonly used in the speaker recognition  nonnegative matrix factorization  nmf   vector quantization
 vq   and gaussian mixture models  gmm   the feature
space of the first technique is the spectrogram representation
of speech signals  whereas the last two are based on mfccs 
initially  we compared these techniques based on the
classification error rate for a designed recording  then  we
selected the most efficient technique and evaluated
performance for varying training sequence lengths 
furthermore  we applied it to actual recordings of team
meetings  one of the project members was a participant in
those recordings  those data contain up to   speakers  some of
whom are non native speakers  per recording at different
locations that are recorded as one monaural video 
the remainder of this paper is organized as follows  in
section ii we described our approach to speech separation by
speaker identification of short time frames of speech  in
section iii  we described in more detail the frequency domain
representation based on spectrograms and mfccs  in section
iv we described the three different machine learning
algorithms used in this project  in section v  we compare the
performance of these different algorithms and perform more
in depth analysis and simulations one the most
computationally efficient of these  moreover  we apply this
algorithm to actual meeting recordings with up to seven
participants  in section vi  we discuss the impact of the results
we found and possible areas for future work  section vii
concludes the paper 
ii  speech separation strategy
when the number of microphones is equal or greater than
the number of speakers we have the well known cocktail party
problem  this problem is generally solved by the unsupervised
learning algorithm called independent component analysis
 ica   although ica is a powerful algorithm  the requirement
of a number of microphones equal or greater than the number
of speakers is rarely met in practice  thus different strategies
must be adopted  several algorithms  both assuming
supervised and unsupervised learning  have been proposed
towards that goal e g         
supervised learning techniques are typically based on
breaking the training data of different speakers into
dictionaries and trying to find the dictionary that best fit a
given sample of recording  this is either accomplished by
variations of non negative matrix factorization or support
vector machines as discussed in      for instance 
for unsupervised learning  speech separation is typically
done by independent subspace analysis  isa       roughly

fics    final paper
speaking  in isa the spectrogram of an audio recording is
calculated  the magnitude of the spectrogram for the different
time frames is used as different outputs for the ica algorithm
that separates the independent components of the data  after
this the data must be clustered so that each frame can be
assigned to the right speaker 
for this application  however  where we wish to separate
speakers in a meeting recording  i e   in a dialog   we can
count with a further simplification that speakers do not overlap
each other for long periods of time  thus  this simplifies the
problem to a speaker recognition problem  that is  given a
training sequence for each speaker we can take frames of the
single channel recording and classify them as one of the
speakers 
fig    illustrates this process  the sound waves of the
speech signals of each speaker is additively combined at the
microphone  the combined speech signal is then passed to the
speaker recognition algorithm that based on the training data
classifies a certain time frame to be spoken by one of the
speakers 
a shortcoming of this approach is that it will not be able to
correctly separate frames where more than one speaker is
speaking  rather  it will classify the entire frame as one
speaker  however  we can minimize the errors induced by this
overlapping by selecting very short time frames at which only
one speaker is speaking or there is one clearly dominant
speaker  indeed  our simulations were based in time frames of
the order of   ms 

 
          fft              

   

where      is a window function  typically the hamming
window   and   are the discrete set of frequencies  fig   
illustrates a spectrogram of a   s speech signal  this graph
shows how the spectrum information changes with time  for
this plot we chose the number of samples per window        
     which corresponds to approximately    ms time frame
for the sampling frequency of      khz   hamming window 
and no overlapping between frames  note that most of the
signal energy is confined within      khz 
for analyzing the speech signals we normally work only
with the modulus square values of          this is motivated
by the fact that our auditory system does not perceive
differences in the phase of speech signals  moreover 
         is real valued  which facilitates the analysis 
however  the speech signals of different speakers are modeled
as additive signals in the time domain  and therefore in the
frequency domain  however  this condition is not generally
true for the modulus  i e   the modulus of the sum is normally
different from the sum of the modulus   nonetheless 
algorithms based on the modulus typically work fairly well 
since          is discrete in both frequency and time we
can write it in matrix form 
               

 

   

 

fig     diagram illustrating speaker recognition approach to speech
separation 

iii  speech signal representations
the global shape of the dft magnitude spectrum  known
as spectral envelope  contains information about the resonance
properties of the vocal tract and has been found out to be the
most informative part of the spectrum in speaker recognition
     we will use two different approaches to frequency domain
representation of speech signals  one based on spectrograms
and the other based on mel frequency cepstral coefficients
 mfccs  
a  spectrogram
the spectrogram is a widely used frequency domain
representation of speech signals  it is based on the short time
fourier transform  that is  instead of taking the fourier
transform of the entire signal  which could be hours long   we
take the fourier transform of short time frames  typically tens
of microseconds   this enables us to analyzed localized
frequency domain characteristics that are more useful for
speaker recognition and other speech processing tasks 
more formally  the spectrogram of a discrete time signal
     is defined as

fig     spectrogram of a   s speech signal  note that most of the signal
energy is confined within      khz 

where    corresponds to the ith non negative frequency and   
corresponds to the jth time frame  thus    is a 

     
 

   

          where         is the number of time frames 
moreover  each column of   corresponds to the spectral
information of a particularly time frame 
b  mel frequency cepstral coefficients  mfccs 
a more sophisticated approach to representing signals in
the frequency domain is based on mel frequency cepstral
coefficients  mfccs   the key idea to mfcc representation is
to use a set of bandpass filters to do energy integration over
neighboring frequency bands  the filter spacing is set
accordingly to the mel frequency scale which better
approximates the human auditory system response      lower

fics    final paper

 
  
  

cepstrum index

  

 
 

 
 

fig     calculation of mel frequency cepstral coefficients 

 
    

     

    

     

    

     

    

     

time  s 

frequencies carry more energy  thus they are represented with
higher resolution by allocating more filters with narrower
bandwidths 
fig    illustrates the process of calculating mfcc 
initially a time frame from the speech signal is selected
 similarly to spectrogram calculation   and the modulus square
of its fourier transform is calculated 
after this the
preemphasis filter is used to mitigate the low pass frequency
characteristic of the vocal tract and also from the microphone 
this is intended to enhance the power of high frequency
components that are naturally more attenuated  after this  we
have a mel frequency filter bank  roughly speaking  this filter
bank integrates the signal energy in certain important
frequency ranges that mainly characterize the speaker 
intuitively  more filters are put in the low scale frequency
where most of the signal energy is confined  as the frequency
increases the spacing between filters is reduced as less
discriminative speaker characteristics are presented in high
frequencies  lastly  a discrete cosine transform  dct  is
calculated and we have the mfcc coefficients  typically  in
speech signal processing no more than    coefficients are
used  in our simulations we have used    and a group of   
mel frequency bandpass filters  an example of mfcc
coefficients is shown in fig     this representation the matrix
has dimensionality                  where        is the
number of cepstral coefficients  typically               this
reduction in dimensionality allows us to use techniques such as
vector quantization and gaussian mixture model 
iv  models
we have studied and implemented three different models
for speaker recognition   i  non negative matrix factorization
 nmf    ii  vector quantization  vq   and  iii  gaussian
mixture models  gmm   these methods are commonly used in
speaker recognition applications  methods based on support
vector machine  svm  are also commonly used in speaker
recognition applications 
our goal is to identify the best performing algorithm for
the application and data set we have  and then proceed to more
in depth analysis of that algorithm  the next subsections
describe each one of these methods 

fig     mel frequency cepstral coefficients for a speech signal  the color
indicates the intensity of a certain coefficient 

a  non negative matrix factorization  nmf 
this method is based on a factorization of the spectrogram
matrix given in      the non negative s matrix is factorized
into two non negative matrices
       

   

where   is interpreted as the dictionary matrix that
characterizes a speaker  and   is the weights matrix  this
way  a certain sound uttered by a speaker is decomposed as a
sum of weighted sounds from a dictionary  the size of the
dictionary is an important design parameter  moreover  note
that this method cannot be applied to the mfcc matrix
because it is not necessarily non negative 
the   and   matrices are obtained through the update
equations     
    
    
 
     
   
 
  
   
 
     

where   and     denote element wise product and division

respectively 
for a certain frame     a column of the spectrogram
matrix  we calculate the weighting vector      corresponding
to the dictionary     of the ith speaker according to
         arg min                     
     

   

the frame    is then classified as being from the speaker
who led to the minimum mean square error  i e   minimization
over i  
results in the literature suggest that        or the
corresponding   matrix  should be sparse so that utterances
are decomposed as a combination of just a few dictionary
sounds      however  requiring sparsity is difficult in
optimization problems  nonetheless            is commonly
minimized instead  even though this condition does not
necessarily imply sparsity it typically leads to good
performance  as a result  the optimization problem in     is
solved by regularized least squares instead of conventional
least squares 

fics    final paper

 

            

 
 

 

min             

   

     

   

for some distance measure        often  a clustering algorithm
is used to reduce the number of feature vectors in each
reference set       when this is done  each vector    is a
summary of each cluster  rather than an individual frame  in
the case of k means clustering  each    would be a cluster
centroid 
in our case  we assume that the times when each speaker
begins and ends a segment of speech is unknown  so instead of
averaging an entire segment of speech  we chose to classify
each test frame individually  as if it were at the center of a
segment one second in length  the frame is classified as the
speaker that minimizes 
 
   
 

          

    

min             

 
      
 

     

   

  where    is the number of frames per second  k means
clustering with     clusters was used to decrease the number
of reference vectors for each speaker 
c  gaussian mixture model  gmm 
gmm are widely used in speaker and speech recognition
tasks e g            gmm can be considered as an extension of
the vq model  in which the clusters are overlapping  that is  a
feature vector is not assigned to the nearest cluster as in vq 
but it has a nonzero probability of originating from each
cluster 
the mfcc of each speaker is modeled as a mixture of n
multivariable gaussian random variables     
 

   

 

            

   

   

   

where     is a   dimensional vector corresponding to the
mfc coefficients of a certain time frame     are the mixture
weights  and           is well known multivariate gaussian
distribution  i e             is the distribution of             
each speaker is characterized by a set of parameters
                                 these sets of parameters are
estimated for speaker user based on their training sequence
using the em algorithm     

after every user is modeled we wish to classify what speaker
          spoke a certain frame       this is done by maximum
likelihood according to
        arg  max pr       

 

     

pr            pr     
     
pr       
   
   arg  max pr          
   arg  max

   

     

where the last equality follows by assuming that all speakers
are equally likely  i e   pr            this is a reasonable
assumption in long recordings  and since pr        is the same
for all speakers 
v  results
initially  we implemented and tested all these methods for
test case of one male and one female speaker  the training sets
were     seconds from recordings of each user reading a
segment of a book  the test data was    seconds of the same
users reading from a play script  the play was designed so that
each speaker would speak for the same amount of time  due to
time constraints  the parameters for each algorithm were not
systematically optimized 
table   gives a summary of the comparative results of the
different methods presented 
table    comparative results summary
error rate    
  
  
  

method
nmf
vq
gmm

a learning curve was generated for the vq algorithm  fig 
    the test set was     seconds of the same play reading as
used in the comparative tests  the training sets were also taken
from the same book recordings as in the comparative tests  the
learning curve displays a clear minimum at     seconds of
training data  with a corresponding error rate of       
    

    

   

error rate

b  vector quantization  vq 
vector quantization is a simple classification technique that
assumes the probability distribution of each class is well
modeled by the distribution of training examples  classically
in speaker recognition problems  this technique is applied to an
entire utterance in which it is known that only one speaker is
talking      we denote the feature vectors representing the
frames of the test segment as                          and the
reference features vectors of speaker   as                         
the test segment given by s is classified as the speaker that
minimizes 

    

    

    

    

   

 

   

   

   

   

   

training sequence duration  s 

fig     learning curve for vector quantization algorithm

   

fics    final paper

 

finally  vector quantization was tested on team meeting
data made up of seven different participants  training data was
generated by splitting    minutes of the recording into seven
separate recordings  each containing only one speaker  these
training recordings were then truncated so that all speakers had
the same amount of training data  the vq algorithm was then
used to classify an additional   minutes of the original
recording  using the truncated training data  in this setting  our
implementation of vq does not perform much better than
chance 
 

   

error rate

   

   
vector quantization

   

random classifier
   

it is interesting that the learning curve for vector
quantization displayed a clear minimum  indicating overfitting 
lastly  we note that the performance of vq on the meeting
recording was worse than on the male female play recording 
it is particularly important to note that performance was worse
versus the male female recording even when classifying only
two speakers in the meeting recording  likely reasons for
lower performance are noticeable background noise in the
meeting recording  overlap of individual speakers talking  and
presence of more than one speaker of the same gender  it is
also worth noting that performance of the vq algorithm gets
closer to that of a random classification as the number of
speakers increases  this makes sense  as one would think that
classifying more speakers makes for a more difficult problem 
the most important area of future work is to re test the
algorithms using a cross validation framework to optimize
model parameters  we hypothesize that doing this would
significantly improve the performance of all three algorithms 
other additions that might help performance when dealing
with complex recording environments and multiple speakers
would be techniques to classify when multiple speakers are
talking at the same time  and when no speakers are talking 

   

vii  conclusion
 

 

 

 

 

 

no  speakers being classified
fig     error rates for multi speaker classification on teleconference
recording

we also tested the effect of increasing the number of
speakers in the recording on performance  this was done by
using removing portions of the test data where one or more
speakers are talking  then running vq on the modified test set
without taking into consideration the training sets of those
speakers that were eliminated  a plot of performance for vq
with each possible number of speakers classified is given in
figure    the error rate when each frame is classified
randomly is given for comparison 
vi  discussion
in our comparative test  we found that of the three
algorithms tested  vector quantization and non negative matrix
factorization demonstrated similar levels of performance 
while classification using a gaussian mixture model performed
significantly worse  when analyzing these results  we must
take into consideration that parameters for each model were
not systematically optimized 
we hypothesize that the superior relative performance of
the vq and nmf algorithms is due to the fact that
performance is less heavily dependent on variations in model
parameters 
the poor performance of the gmm and was somewhat
surprising  we  again  attribute this to sub optimal parameters 
in the case of gmm we also conjecture that performance
suffered because we were able to run the algorithm only for a
small number of gaussians        while results in the literature
show that best results are obtained for mixture of a larger
number of gaussians       

we tried three different algorithms for speaker separation 
the vector quantization as the simplest algorithm yielded very
good results  the assumption is that this algorithm requires the
least optimization  under the time constraints we were able to
generate error plots for speakers and time of training data for
the vector quantization but not the other approaches 
we achieved reasonable error rates for the male female
speaker data however the current algorithms were not able to
achieve comparable performance for the seven speaker data 
this appears to be an issue of different audio qualities of the
individual speakers as well as due to a lack of dedicated
training data and more overlap 
references
   
   
   
   
   
   
   
   
   

z  haykin  s  chen  the cocktail party problem  neural comput  
vol      no     pp                 
t  kinnunen and h  li  an overview of text independent speaker
recognition  from features to supervectors  speech commun   vol     
no     pp        jan       
h  makino  s   lee  t  w   sawada  blind speech separation        pp 
       
m  a  casey and a  westner  separation of mixed audio sources by
independent subspace analysis  mitsubishi electr  res  labs       
t  virtanen  unsupervised learning methods for source separation in
monaural music signals   
b  wang and m  d  plumbley  musical audio stream separation by
non negative matrix factorization 
m  n  stuttle  a gaussian mixture model spectral representation for
speech recognition       
r  reynolds  douglas  rose  robust text independent speaker
identification using gaussian mixture speaker models  ieee trans 
speech audio process   vol     no     pp             
a  ng  mixtures of gaussians and the em algorithm  in cs   
lecture notes        pp     

fi