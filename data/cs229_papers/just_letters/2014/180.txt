stay alert   creating a classifier to predict driver alertness in
real time
aditya sarkar  julien kawawa beaudan  quentin perrot
friday  december         

 

problem definition

driving while drowsy inevitably leads to road accidents  there are about         crashes every year attributed
to drowsy driving in the us  with an associated     b in annual losses  in fact  as many as one third of fatal car
accidents are linked to drowsy driving  it would be extremely useful if cars could determine whether the driver
is alert or not in real time  the objective of this investigation  then  is to create a classifier that will determine
if a driver is alert or not alert 

   

describing the ford dataset

the dataset was provided by ford  and shows the result of many observations of driver behavior  for each
driver  there are   minutes of sequential data  recorded every    ms  there are samples from     drivers from
diverse backgrounds  for each observation  an output isalert is labeled  isalert     indicates that the driver
is alert  while isalert     indicates that the driver is not alert or drwosy  for each observation  the data has
   different features values    of these are physiological  features start with p      are environmental  features
start with e  and    are vehicular  features start with v  
in addition to the training data  we were also provided data to test with  not only did this give us more
data to work with  it also took away the decision on how to split the data into training and testing datasets 
this pre divided testing set eventually turned out to be problematic  as will be discussed later 

   

key challenges

one challenge to creating a classifier was that we could not use intuition in our predictions about which features
to use  since the features are only labelled p  e  or v  if we knew what each feature described  we could
predict  for example  that it might be useful to include the feature for time of day  instead  we had to perform
some statistical analysis of the data to determine which features had the largest difference between when isalert
    and isalert      we also challenged ourselves to use as few physiological features as possible  this makes
sense  since it is difficult to observe the drivers physiological features   like heart rate  for instance   in normal
cars  finally  the data contains a mixture of continuous and discrete features  for example  p  is continuous 
while e  is discrete  and has a range of   to    instead of discretizing the data  we decided to assume all the
features were roughly normally distributed for certain models  such as the gaussian naive bayes 

 
   

preliminary processing
first round  feature selection using    features

unsurprisingly  using all the features provided did not improve the performance of our naive bayes or linear
classifier  instead  we experimented with using subsets of the    features provided to us  to select the best
features  we did two things  the first thing we did was to compare the distribution of each features when the
driver was drowsy and when the driver was alert  in particular  we assumed that many of the features had a
normal distribution  and that we could compare by simply calculating the difference in the mean of each feature
divided by the total variance  this proved extremely useful in narrowing down which features to use  we tried
to include features with a ratio of difference in mean to total variance greater than      and ignored the others 
which narrowed the down the feature set to   features  we experimented with using different subsets of these
features  naturally 

 

fithe second thing we tried was to feed the data through feature selection algorithms  the first was a
univariate feature selection  which selects the best features based on univariate statistical tests  in particular 
we used selectkbest which is part of the scikit learn package  we also used l  based feature selection  which
uses a transform method to reduce the dimensionality of the data by selecting the non zero coefficients in the
sparse solution  the culmination of all these feature selection algorithms is represented in the image above
where we used ensembles of a decision tree  here we used an extra tree  to compute the relative importance of
each attribute  the three attributes that were ranked to have the highest importance were the ones we used in
order to get our lowest testing error  as you can see from the graph  there are certain stand out features that
we used across different feature sets 

   

second round  feature selection using new features

the data  as explained above  already contained    features about the drivers physiological  environmental 
and vehicular behavior at each time step  we also chose to create features describing each feature over time 
the minimum and maximum value  as well as the size of the range  of each feature over a given time span 
simply processing  not mention training any models on  this many features took an extraordinary amount of
time and memory so we had to narrow down which features to use 
as with the first round features  we manually selected features by comparing the ratio of difference in means
 between drowsy and alert observations  to total variance  this narrowed down the original     features to
only    features  which was much more tractable 

   

third round  feature selection dependent on model

although this is not a preprocessing step  we feel like it is important to discuss this last round of feature selection
here  both the first and second rounds of feature selection described above look at the data from a higher level 
and then attempt to create superior sets of features  however  we have found that different features perform
better or worse depending on the model used  hence  our third round of feature selection is an iterative process
where we use intuition built from our first two rounds and create new sets of features that are adapted to each
model 

 

models and results

using the sets of features developed using our various feature selection techniques  we looked to train our learner
on the ford training dataset  to do this  we explored various models  including stochastic gradient descent
and randomized decision trees  for each model  we trained the learner on   different feature sets 

 

fi a basic set  this set was arrived at using our first round  where we looked at features with the largest
difference in drowsy and alert mean to variance ratios  this set has   features  with   physiological   
vehicular and   environmental 
 univariate set  this set has a total of only   features  where none are physiological  as desired by our
challenges   two of the features in this set overlap with the basic set 
 l  based set  this feature selection method was less successful at picking out features and had a total of
   features  only   eliminated  
 time change features  these features were arrived at from our second round  there are a total of   
features 
 method dependent set  inherent in the name  this set varied across models  for sgd  for example  our
method dependent set has   features  two environmental and   vehicular  
training our learner on each set yielded some very striking results  for relevance  the table below only shows
the best training and test errors and specifies which feature set yielded these results 
method

feature selection

train error

test error

sgd w  hinge loss
sgd w  logistic loss
naive bayes
naive bayes
randomized trees

model dependent
model dependent
time change
l  based
l  based

     
     
     
     
     

     
    
     
     
     

train
false
pos 
  
     
     
     
    

test
false
pos 
     
     
     
    
     

next  we will describe each model  its implementation and the relevant results 

   

stochastic gradient descent

we implemented a standard stochastic gradient descent algorithm  using the features selected above  we
experimented with two different losses  a hinge loss  and a logistic loss function  we eventually used an
implementation provided in the scikit library 
early on  we found that stochastic gradient descent on the hinge loss function showed a lot of promise  as
a result  we iterated a lot on this model using parameter tuning and choosing our features carefully  our best
results came with a set of   with features e   e  and v    our test error was a highly satisfying       
  the lowest test error in all our results  however  the learner developed had a surprisingly high training error
of         this is strikingly high  and is related to the distribution of both training and testing data sets 
in training data  there were more drowsy outputs and in test data there were less  as a result  our predictor
predicted alert for a very large percentage of samples and this led to a high number of false positives  predicting
alert but driver is in fact drowsy   in fact  out of all our wrong predictions         were false positives  these
high false positive rates occurred across different feature sets  in our analysis  we will delve into this further 
when we used a logistic regression instead  results were indeed very similar  test error was higher         but
training error was lower          

   

naive bayes classifier

the nave bayes model assumes that all    features  selected as described above  are dependent on the alert
or drowsy variable  and are independent of each other  the model was trained by calculating the distribution
of each feature  separated by alert or drowsy observations   a major assumption that we made because most
of the features were continuous was that the distribution of each feature was normal  so  the distribution of
each feature was described by its mean and variance  and the probability of any given value of a feature was
calculated using the probability density function for that features normal distribution  classifying was done
by simply choosing the assignment  drowsy vs  alert  which had the higher probability of giving the observed
values of the features 
naive bayes offers an alternate way of learning  unlike the surprising results found in sgd   with very high
false positive rates across different feature sets   naive bayes had varying rates with different feature sets  our
best test error occurred using our l  based feature set  this was generally surprising because this set contained
a total of    features  and we expected overfitting  our test error on l  based feature set was        with once
again a high training error of         a more balanced result came using our time change feature set which
 

fiyielded a test error of        and a training error of         for the l  based set  there were high false positive
rates         whereas in the time change feature set the false negative rate was high          

   

ensemble methods  randomized decision trees

using the scikit learn package  we were able to implement an averaging algorithm based on randomized decision
trees called the randomforest algorithm  this algorithm creates a diverse set of classifiers by introducing
randomness in the classifier construction  then  prediction is done by averaging the prediciton of the individual
classifiers  this has the benefit of reducing the variance of the base estimator and thereby reducing overfitting 
unlike other models  these decision trees yielded very low training error  this is a characteristic of this model
because the learner can create overly complex trees that do not generalize the data well   effectively overfitting 
low training error  however  did not translate into low testing error  the best performing randomized tree
occurred on time change feature set with        test error and a very low       training error  false positive
and negative rates in this model were more evenly distributed 

   

neural networks

we were actually very excited about implementing neural networks  the set up of our problem  in particular
the unlabeled features  made the concept of using neural networks extremely attractive to us  in addition to
this  we always suspected that if we combined some of our features together  we would get a more realistic
model of the situation that we were trying to mimic  while we could not use our intuition to put these features
together  the promise of neural networks was alluring and hence we implemented it with great anticipation  the
results however were less than satisfactory  with a       training error and a       testing error  this was not
sufficiently accurate to even make it to our summarized table of results  retrospectively  we should have taken
more time setting the initial weights vector for the algorithm but it was obviously hard without the knowledge
of what each feature represented 

 
   

analysis and discussion
distribution of training and test data

our high training error was something that was extremely alarming to us  at many points we were sure we
were implementing our algorithms wrong in some way so after triple checking each line of our code to ensure
correctness  we were curious as to why this was the case  we tried many different learning algorithms and often
received a similar anomalous pattern  our training error was at times up to a factor of three higher than our
test error 
we decided to splice the dataset that we were using as our training set into two parts  one that would
continue to be used as training data while the other would be used as test data in place of the dataset we were
given  we finally started getting more expected results where the testing error and training error were similar 
after further statistical analysis of the training and testing datasets  we realised that these had a significantly
different distribution from one another  training had less alert outputs than testing 
this was a relief for us in multiple ways  apart from assuring us that our algorithms were indeed not to
blame  it also explained why our feature extraction  and subsequent testing on our models  showed that the
best results achieved were when we used very few of the features we had available to us  as mentioned  we
received the best results with only three out of the thirty features  this can be explained by the difference in
distribution  the more features we used  the more our model was fitting to the distribution of the training set
which of course was very different from that of the testing dataset 

   

false positive and false negative rates

consider the following table that describes our findings under the lens of our wrong predictions 
method

feature selection

sgd w  hinge loss
sgd w  logistic loss
naive bayes
naive bayes
randomized trees

model dependent
model dependent
time change
l  based
l  based

train
false
pos 
  
     
     
     
    
 

train
false
neg 
    
    
     
     
    

test
false
pos 
     
     
     
    
     

test
false
neg 
    
    
     
    
     

fiin our search for the reason behind the appalling performance of our training set to the models we were
creating  we started looking at where the errors were coming from  were we predicting drivers as drowsy when
they were actually alert or vice versa  in the above table  we determine false positive and false negatives for
each of our models  a false positive rate is defined as the percentage of errors that were predicted as alert but
were actually drowsy  similarly  false negative rate is the percentage of errors that were predicted drowsy but
were in fact alert  what we found was while looking at linear classification with both loss models  we were
achieving a similar rate of false positives and false negatives  upon further probing we realised that the training
set indeed had a much lower proportion of positive labels  in fact  the training data had fit the model in a way
that it would almost always predict that the driver was alert  this of course was advantageous to the test data
where around     of the examples represented an alert driver  hence  we see that our predictor yields high
false positives because it often predicts alert  this leads to a high training error because the training data
only has     of samples as alert 

   

usefulness of time variation features

after our first experiments with the regular feature set  we were disappointed by the high training and testing
error of our models  especially with stochastic gradient descent with hinge loss and logistic loss functions  at the
time  we werent sure why the linear classifier did poorly  and we were even more surprised that the physiological
features were bad at predicting whether the driver was alert  we expected  when we began the project  that
physiological features would be the most effective at predicting driver alertness  because intuitively  alert people
have higher heart rates  for example  than drowsy people 
one explanation for the poor predicting power of physiological features was that the normal value of each
feature might vary drastically depending on the features  for example  one persons resting heart rate might be
a high heart rate for someone else  so  instead of using the instantaneous value of each feature  we decided to
include other features  such as the minimum  maximum  and total range of each feature over a certain time
frame  usually   seconds  we expected the range of a feature over a time period to be especially useful  since
intuitively  a change in alertness in a driver should reflect a greater variation in physiological features over time 
surprisingly  then  the time variation features were not extremely useful  after running the same analysis on
these time variation features as on the original features  we found that only a few differed significantly between
alert and drowsy observations  even more surprisingly  the features were significant were not physiological
features  the useful range features were e   e   e   e   and v   and both e  and e  were already
present in the set of normal useful features  similarly  the useful minimum features were v    e   e   only
e  was not in the set of normal features   and the useful maximum features were e   e   e   e   e  
v   and v   only e   e   and v  were not in the set of normal features   so  perhaps it is not surprising
that using these time variation features did not affect the training and testing error very much 
however  the time variation features did help in some cases  for example  our best naive bayes model using
only features in the original dataset only achieved     training error and       testing error  while the using
time variation features reduced the error to       training error  while the testing error stayed the same        

 

future direction

another model we would have liked to explore  with a lot more time  would have been a hidden markov model
which calculates the probability of transitioning from alert to drowsy or vice versa  given the value of the
features  this model would have been interesting because we had several positive results with our time change
features  furthermore  although we have already tried  we plan on contacting ford so that they can give us
labels to the features  one focus of ours was to create the classifier so that it can run in real time   that is
why we tried to ignore phsyiological features as much as possible  in a car environment  the data passed to the
learner would have labels for the features  for example  data about the speed of the vehicle would be labeled
vehicle speed  it would be very interesting to be able to see what the features actually referred to   especially
our optimal feature set of e   e  and v   for sgd  with this intuition  perhaps we could better combine
features leading to superior predictions 

 

references

scikit learn  machine learning in python  pedregosa et al   jmlr     pp                   stochastic
gradient descent wikipedia  wikimedia foundation     nov        web     nov        naive bayes
classifier wikipedia  wikimedia foundation     nov        web     nov       

 

fi