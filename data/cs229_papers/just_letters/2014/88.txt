constructing personal networks through communication history
ryan houlihan and hayk martirosyan
stanford university

 cs     final project 
 dated  december          
this study aims to predict a users relationships with his or her contacts based solely on the
words used in electronic communications between them  software was created that reads in bulk
exported email data and builds a comprehensive graph database of people  the words communicated
between them  and their relationships to each other  several stages of preprocessing and feature
selection were applied  and then various classifiers were shown to effectively predict relationships
using    fold cross validation  logistic regression showed      testing error on a sample set of
     emails and     contacts  the study shows promising results and suggests future work that
incorporates message meta data  other mediums of communication like text messaging  and larger
data sets would only improve upon our results 

i 

introduction

in the modern technological age communications with
friends and acquaintances are no longer transient  instead  each of us amasses a large collection of electronic
communications whether through email  cell or video
recordings  in everyday life ones speech and behavior
around others directly reflect the kind of relationship one
has with another  whether they are friends  lovers  family 
strangers  superiors  inferiors  etc  as a team we questioned whether this circumstantial speech and behavior
was also present when communicating electronically  we
hypothesized that the content of electronic communications alone would allow us to classify inter personal relationships  something humans can easily do when observing two strangers or old friends interacting with one
another 

ii 

platform

to test this hypothesis we decided to use the mass of
a persons email as our data set  and classify their email
contacts into friend and acquaintance categories  email
is the most widely used form of electronic communication
and we believe one is more likely to communicate with all
types of people  whether friends or strangers  via email
then they are via mobile mediums like text messaging
or snapchat  as features we used individual words and
their respective frequency counts in all emails to a given
contact  this was chosen over phrases as it greatly simplified our process  for our models we decided to focus
on supervised learning approaches after briefly exploring
unsupervised approaches such as k means  the details
are outlined in the following sections 




rhouliha stanford edu
hayk stanford edu

fig     example subgraph for a single word node  people
who have heard this word  and their relationships 

a 

data

our data set consists of bulk exported emails from
gmail which are parsed and curated through software
we developed  we first parse the email data in mbox
format into a standardized yaml file containing message entries with fields for date  to  from  cc  bcc  and
text fields  during this process  we only consider plaintext email sections  a thorough set of regular expressions
are used to lowercase  eliminate links  numbers  punctuation  and all non word tokens  we decode all encoded
data and output in full unicode  we further process the
words by selecting     random words from each email
and applying a stemming algorithm  when extracted
for classification  the term frequency inverse documentfrequency transformation is also applied to normalize the
data 
we then load this data into the neo j graph database 
creating person  message  and word nodes  and role  relation  alias  and heard edges  heard edges keep a count
of the frequency of stemmed words spoken between people  and eventually become our feature vectors  relation
edges are used to describe person to person relationships 
and correspond to our classification labels  role edges are
used to label a person as either the sender or receiver of

fi 

fig     an example subgraph of alias edges that combine
person nodes corresponding to the same human but with different email headers 

a given message  to from cc bcc  
when we first developed our graph  we noticed that for
a given contact  there might be      distinct email headers  where either the name or email address was changed 
this noisy data meant we were classifying the same person many times  to alleviate this issue  we built a heuristic module which parses through person nodes  applies
fuzzy string matching  and with some user assistance 
connects all people nodes that belong to a single person
using an alias edge  the result is that for each person 
one canonical person node contains all of their communications  and the auxiliary person nodes connect to the
canonical node with alias edges  this result can be seen
in  fig     
finally  we have a module that allows the user to label their contacts as friends or acquaintances  updating
relations edges in the database  all contacts are classified  and subsets of these are extracted and marked as
unknown during training and cross validation 

b 

features

our features for every sample  person  are the frequency of each word that this person said to our user 
the vocabulary begins as the entire stemmed collection
of words from the data set  then is curated to lower the
number of features  we explored three methods to extract the most important features 
   a minimum and maximum total frequency cutoff 
to eliminate overly rare and overly common words 
   principal component analysis  pca         to extract the most relevant features 
   the chi squared statistic to extract the k best features 

to set our minimum and maximum frequency limits we
first computed the total frequency  times each word was
heard or said  of all distinct words  we then calculated
the mean    and standard deviation    of the words on
their total frequencies  the maximum frequency cutoff
was set as a chosen number of standard deviations above
the mean frequency  the low frequency cutoff was set as
a fixed constant value  this proved to be a good way to
remove        of features early on in the process  especially because many words are said only once or twice 
and contribute little to classification 
the pca method performed linear dimensionality reduction using singular value decomposition of the data
and keeping only the most significant singular vectors to
project the data to a lower dimensional space  the svd
used factorizes the matrix a into two unitary matrices
u and v   and a   d array s of singular values  real 
non negative  such that a    u  s  v   where s is a
suitably shaped matrix of zeros with main diagonal s 
this method did not prove to be useful to us for feature
selection 
finally  we selected the k best features using the chisquared statistic  which measures dependence between
stochastic variables and eliminates features most likely
to be independent of the known labels  this method was
very effective for reducing the number of features without
losing much in classification performance 

c 

models

we explored the following classifiers  svm with linear
kernel  svm with exponential kernel  logistic regression  using batch and stochastic gradient descent   ridge
classifier  and multinomial naive bayes  below we show
the basic method of a few of these 
   svm with linear kernel     given a set of
instance label pairs  xi   yi    i              l  xi  rn
yi          it solves the primal unconstrainted
optimization problem
l

x
 
 w  xi   yi  
min wt w   c
w  
i  
where c     is the penalty parameter and loss
function
 w  xi   yi     max    yi wt xi      
   logistic regression     performs same minimization as svm with linear kernel but the loss function is instead defined as
 w  xi   yi     log     eyi w

t

xi

 

   svm with exponential kernel given a training vector xi  rp   i              n and a vector

fi 

start

extract plain text email data
from  mbox format and output
to yaml format

remove all html and non word
tokens

choose optimal min and max
word frequency  select only
words with total frequency
within that range

remove stop words and stem
all words

weigh all features using
tf idf

use well formed features for
training and testing

extract relevant features using
chi 

fig     process for extracting features from email dataset

yi          it solves the dual optimization problem
 
min t q  et 
  

classifier
accuracy
logistic regression
     
logistic regression w  sgd
     
svm w  linear kernel
     
svm w  exp  kernel
     
ridge classifier
     
multinomial naive bayes
     

subject to y t     
   i  c  i              l
where e is a vector of all ones  c     is the upper bound  q is a nxn positive semidefinite matrix 
   
qij   k xi   xj   and  xi  t  x    e xx   is the
kernel 

iii 

results

all results were obtained using grid searching in scikitlearn  in addition to the named classifiers  all trials used
a tfidftransformer to normalize the data and a selectkbest feature selection algorithm using the chi squared
test  all results are from    fold cross validation  using
the following data set 
 emails      
 total words spoken        
 samples  contacts      

higher than the number of samples  nonlinear classifiers
tended to over fit  and would often have      training error while having     testing error  reducing the vocabulary by usage frequency was more effective than pca 
but the chi squared test was the most effective method of
feature selection  reducing the number of features with
the chi squared test reduces the resulting accuracy  but
we can get a significant reduction in the feature count
without much of a hit  for example  using the top     
features we can still achieve over     accuracy  and using
the top     features we achieve       accuracy 
we think our accuracy would certainly improve with
a larger data set  however  much of the difficulty comes
because it is hard to classify people in black and white as
friend vs acquaintance  and there is always a gray area 
we can directly see this by looking at the misclassified
contacts  it may be more effective to use a continuous
friendliness scale  perform a regression  and then compare our results using some closeness test to the labeled
value 
finally  it is interesting to look at the most useful features for classification  here are the top twenty word
stems for this data set 

 min  frequency cutoff   
 max  frequency cutoff stddev    
our results show excellent classification of the sample
data  logistic regression was the most accurate classifier 
at      testing error  in general  linear classifiers did a
little better  likely because the number of features was

 putnam unbeliev red there boop iphon re
subject probabl fwd forward candid guy drink
im hugh sox nun i yeah 
these words provide some nice insights  fwd  forward  re are the tokens we replaced forwarded and
replied sections in emails with  and signify that how much

fi 
we forward reply vs send new emails is an important
factor  yeah  i  im  and guy suggest that informal
language is very important  candid likely comes from
candidate  which points towards job interviews  and is
likely a huge marker towards acquaintance 
iv 

future

in the future there are a variety of things we would like
to accomplish 
   use meta features from the email header and message statistics 
   classify into more categories or use continuous
friendliness value 

bag of words model of the communicated words  this
method can potentially be used to improve email or
mobile clients by predicting relationships with contacts 
there is great promise for future work that incorporates
different communication mediums  message meta data 
and larger data sets  the greatest challenges are dealing
with the noisy data  preprocessing  alias creation   time
required for labeling  and the difficult gray area between
friends and acquaintances  we plan to extend this work
into an open source tool that will allow users to build
a graph of their personal network and visualize various
aspects of it to gain useful insights 

vi 

references

   improve insert performance to handle       s of
emails 

    jones e  oliphant e  peterson p  et al  scipy 
open source scientific tools for python      

   create a useful open source visualization tool 

    scikitlearn  machine learning in python  pedregosa et al   jmlr     pp                 

one possible improvement would would be to add message statistics and meta features as part of our feature
vector  adding features such as email length  email frequency  number of people to  ccd  and bccd on an email 
and who was typically sent an email together might lead
to interesting results  it would also be interesting to consider heard vs sent words separately 
another interesting area would be to move away from
only a binary classification such as friend or acquaintance and into multinomial classification set such as
 friend  family  acquaintance  newsletter   alternatively 
we could stick with friend vs acquaintance but explore
a continuous friendliness scale to avoid the tough gray
area with contacts who are almost but not quite friends 
one way to provide insight into this is to further explore
clustering algorithms such as k means and see if we can
produce some compelling patterns in the data 
another useful goal is to improve the speed of our
database queries and inserts  especially when dealing
with email datasets on the order of   k    k emails 
while our current speeds are fine for our own testing
if we ever want to allow other users to begin to take
advantage of our software this shortcoming will have to
be taken care of  this would also allow us to load in a
much larger sample set which could help improve both
supervised and unsupervised approaches 
finally  integrating a tool which would allow us to
better visualize our results would be enormously helpful  while neo js features are sufficient for our current
needs it is overall quite slow and only able to display a
small number of nodes  again if we plan to have external
users using our system such a feature is a necessity 
v 

conclusion

we are able to very effectively classify email contacts as friends vs acquaintances based exclusively on a

    r  e  fan  k  w  chang  c  j  hsieh  x  r  wang 
and c  j  lin  liblinear  a library for large linear classification journal of machine learning research
                   

fi