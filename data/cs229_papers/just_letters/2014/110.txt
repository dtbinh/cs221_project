recommendation based on user experiences
hai vu
december         
abstract

depedent  we first discuss standard recommeder system that
discover time depedent features in order to make suggestion
to users  we then discuss new model  introduced in      to
deal with time depedent feature that we named user experience  we experiment with a movie data set  evaluate the
improvment over standard recommender system  and discuss
other benefits of discovering user experiences 

latent factor model is one common technique to build recommendation systems  standard latent factor model however
does not take into account the order in which each individual
user makes the ratings  modeling the shift in user behaviour
over time will not only allow making better recommendations
to users  but also discover their hidden categories  level of
experiences or progression stages  in this project  we apply the standard latent factor technique to a movie review
data set and then apply the new technique to model user
experience on this data set 

   modelling users time indepedent latent
features
rating is modelled as a sum of average score  item and users
biases  and the similarity of user and items in the latent feature space 

we find that the improvement in prediction depends on the
time span of the dataset  but more importantly  we can use
the model to draw interesting insight from the discovered
user experiences   guiding user to the next level of experiences 

 rbui     bi   bu   qt i pu
the baseline algorithm

in principle  the factorization method maps users and items
onto the feature space and the similarity on this space is mearecommender systems follow   main strategies  content  sured by inner product  the higher the product of these  
based filtering and collaborative filtering  collaborative is projected vectors the better the user rates the movie  teh
often the preffered approach as it requires no domain knowl  hidden feature space has k dimensions 
edge and no feature gathering effort  the   primary methods
for collaborative filtering are latent factor models and neighthe challenge is to find the feature space  or in other word 
borhood methods 
to decompose the user item matrix into the product of   matrices  the first matric links users to the hidden features and
in user user neighbourhood methods  similarity between users the second matric links products to the features  given that
is measured by transforming them into the item space  simi  the user product is sparse we dont try to factorise the matrix
lar logic applies to item item similarity  in latent factor meth   using singular value decomposition approach   instead we
ods  both user and items are transfomed into a latent featuee minimize the error on available ratings 
space  an item is recommended to a user if thu are simip
 min u i  rui      bi   bu   qt i pu        qi        pu      
lar  their vector representation in the latent feature spase is
relatively high 
 where     qi        pu       is regularization

   introduction

  is the mean rating  bi and bu are bias for items and users
respectively  hidden feature space has k dimensions 

we select latent factor model because it allows us to identify
the hidden feature of the users  these features are time in 

fiwe will use stochastic gradient descent to find the optimal the algorithm
solution  we chose it over alternating leaset square due to
 step     initialization  for each user  assign experience
is easy implementation and fast running time  taking derivalevel evenly so that each user at each level makes rating
tion to each parameters we arrive at the following update at
appoximately equal number of times   while statisfying
each iteration 
monitonicity constraint 
 iu      riu       b itemi   b useru  qi pu t  
 step     filter ratings according to exprience level and
train model for each individual experience level
 qi   qi     iu pu      qi  
 training smaller experience levels first   collect all rat pu   pu     iu qi      pu  
ings belonging a specific level and apply the standard
recommendation method to this dataset  in this step we
 b itemi   b itemi    bias  iu     b itemi
keep parameter set   fixed while optimizing parameter
set   
 b useru   b useru    bias  iu      b useru  
 step     reassign ratings to experience levels  for
each user  reassign rating times to exprience levels to
minimise the error  in this step we keep parameter set  
fixed while optimizing parameter set   

  can be calculated from the dataset 

   modeling time depedent user experiences
when a user rates a product  she stays at specific experience
level  denoted by an integer e where e       k  k is the highest
exprience level   we assume that

 repeat steps     until convergence 

to avoid overfitting we introduce a smoothing component
into
our error function so that parameters of experience level
 users at the same exprience level will behave similarly
k
will
not significantly different from those of experience level
 apart from their hidden features 
k     this change impacts the derivation of error and the
update rule as follows 
 experience level never decreases over time
 qi   qi    iu pu     qi   q otheri   

our intuitive expectation of user experience is that its level
never decreases  we believe that monitonicity is natural assumption and will not narrow too much the scope of application  in addition  the monitonicity constraint will also help
to avoid overfitting because only a limited set of experience
sequence could be mapped to the sequence of ratings  now
the new model consists of   set of parameters  

 pu   pu   iu qi     pu   p otheri   
 b itemi  
b itemi   bias  iu
b item otheri   
 b itemu  
b itemu   bias  iu b item otheru   

 

  b itemi   b itemu  

 set      b item  b user  q  p for each user experience
level
while fitting model for exprience k  we use q other  p other 
b item other  b user other parameters of the model k   
 set    the assignment of each rating to an experience
level  for each user  the assignment is subjected to the
monotonicity constraint 
for step    we use dynamic programming to reassign each
rating to new experience level 
to train the model we need to find the above set of parameters to minimize error expression as in previous section  as    implementation details
objective function is not convex  we will find the local optimum by alternatively optimize one parameter set while keep  data set
ing the other set fixed and repeate until the error becomes
small enough 
availabel at  http   grouplens org datasets movielens 
 

fiformat of data files  each row includes user id   movie id  avoid overfitting
rating  timestamp 
overfitting is avoided by
 using regularisation to make the model simple   parameter of the experiences level k should be similar to that
of level k    we arrive at      

dataset    k  number of users        number of movies  
      number of ratings            timespan       days 
rating        number of rating   user  at least   

 forcing simple asssignment   monotonicity constraint on
experience level for each user

dataset  m  number of users         number of movies  
      number of ratings              timespan        days 
rating        number of rating   user  at least   
number of hidden features

each user has k number of hidden features that are time indepedent and e number of experience levels  the maximal
value of k is    and e is    we dont see rmse improvement
we select only users whose have been at least    days  latest with large k or e value 
rating and earliest ratign are done at least    days apart  in
the system and make at least    ratings  users who spend less
time in the system have not potentialy reveal her behaviour dynamic programming
and therefore will not be considered 
for step     we use dynamic programming to reassign each
rating to new experience level  dynamic programming is feasible because the best assignment for ratings r    r       rn conlearning rate and convergence
tains the best assginment for ratings r    r       rn  as its subas we run standard latent factor model repeteadly at step   of set   here r    r       rn are ordered according to the time of
the algorithm above  we want to use the same learning rate  ratings 
and bias for each iteration  we obtain these values by running
a baseline algorithm on the data set without considering the
   result  discussion
user exprience at all  for this dataset  we get  bias         
        note  we use different learning rate  for p  q and for how much improvement does the new model bring  
bias parameters because the bias parameters are simpler to
learn 
if we keep only users from dataset    k  whose timespan is
more than    days  we achieve rmse of       versus the
baseline rmse        the baseline model is the standard recinitialisation
ommendation which does not consider user experience   we
argue that users who just spend very little time in the movie
at step   of the algorithm  the initial value of q and p are system will not exhibit behaviour shift  instead they exhibit
randomly generated so that  q p  is in comparable range of a noise behaviour  once they spend more time in the
the maximal rating value 
system  their behaviour will be shaped and follow the monotonicity trend  therefore removing these noise behaviours
will help identify the key user experiences 
cross validation  testing
filtering relevant users

for validation  we split the data set so that we can make
   fold cross validation  regarding testing  for each tuple
 user  movie  timestamp  in the test dataset  we find the user
experience level by looking up the level associated with the
closest timestamps in the traingning set  giventhe level   the
parameters    b item  b user  q  p  are then obtained and
root mean square rmse is calculated 

dataset    k  baseline rmse          keeping users who
spend at least    days in the system  user experience based
model  rmse          improvement over baseline      
number of experience level     regularisation      number
of time indepedent latent features k       learning rate for
bias parameter bias        for other parameters         

 

fidataset
baseline rmse
user exprience based model rmse
improvement over baseline

   k
     
     
   

 m
     
     
   

distinctive benefit over traditional recommendation
system  guiding to the next level
while traditional system recommend the product the user
most probably like at the current state of their experience 
our user exprience based system can do more  give them the
product so that they progress one level and also like it 

does time depedent behaviour exist in the dataset 

this a fundamental question  based on our experiements
we must remove noisy behaviour  see above   and consider here is the outline 
only users who have been engaged with the system for minum
 step    for a user u  find his current exprience level  say
period of time  this duration is application dependent and
e  find the averate time a user spend in exprience level
could be measured in different ways  for a movie system in
e 
this project with timespan          days  data set    k and
 m  we observe that the threshold of    days is feasible 
 step    if user user u has spent much longer than average
at this experience level  then use parameter set e   to
impact of model smoothing
make offer   instead of using e as normal recommednatio system does 
when users jump from one experience level to the next  we
dont expect too sudden change in their behaviour  therefore
it is reasonalbe to smooth the model parameters 
   conclusion
      b itemi   b useru  qi   pu   to keep the sum we have experimented with user exprience based recome   e  e     small  
mender system as suggested in      we confirm that the
improvement in term of rmse over baseline recommender
this term is part of our error function describe in section    system is in the range of        
however  to see how much we have gained with smoothing we our work reveals that 
also try to minimise
 for the model to work efficienlty and delivering improve e   e    instead of e   e  e      
ment over baseline recommenders  the dataset needs to
be cleaned upfront to remove users who spend too little
time in the system  we introduce the conjecture about
we observe that the smoothing gain in rmse reduction is
noisy behaviour that could reduce the performance im    from        down to       on dataset     
provement 
benefits of discovering user experiences

 we suggest to amek use of the learnt user experience to
make offers to users so that they progress faster  the
steps are outlined in section   

instead of modelling user experience and then using it to predict future user ratings  one can also just take the latest ratings of each indivudual users and run a standard recommen we discuss the model smoothing that is necessary to redation on that filtered data set  which one has better preduce overfitting  we observed that model smothing will
diction power   if all users have already reached the highest
increase the performance about    in one of our dataset
experience level  then our user experience model provide less
advantages  however  we expect that this is not the case in as future work  the model could be extended to model differtypical systems 
ent classes of progresstions  see       
another benefit is to identify a group of users who got stuck
in certain experience level for longer than average time  do
they need special treatment   can they churn   or can
we help them to progress so othat they stay with us   this
question leads to the next topic below 
 

fireferences
    y  koren  collaborative filtering with temporal dynamics 
commun  acm       
    y  koren and r  bell  advances in collaborative filtering 
in recommender systems handbook   springer       
    from amateurs to connoisseurs  modeling the evolution
of user expertise through online reviews by j  mcauley 
j  leskovec  acm international conference on world wide
web  www        
    finding progression stages in time evolving event sequences by j  yang  j  mcauley  j  leskovec  p  lependu  n 
shah  acm international conference on world wide web
 www        

 

fi