re clustering of constellations through machine learning
shanshan xu   kaifeng chen   and yao zhou 
 
stanford university  department of physics
stanford university  department of applied physics
 
stanford university  department of materials science and engineering
 dated  december          
 

introduction

since thousands of years ago  people around the world
have been looking up into the sky  trying to find patterns
of visible stars distribution  and dividing them into different groups called constellations  originally  constellations are recognized and organized by peoples imaginations based on the shapes of the star distribution  the
most two famous groups of stars is the big dipper and
the orion  in modern astronomy  the international
astronomical union  iau  has defined constellations as
specific areas of the celestial sphere  these areas have
their origins in star patterns from which the constellations take their names  in total  there are    officially
recognized constellations 
on the other hand  certain stars are grouped together
primarily because they are close to each other and far
away from other stars  in other word  one can approximate constellations as the clusters of stars on the celestial sphere  then it would be quite interesting to see
what would constellations  clusters  look like if one uses
some totally objective clustering methods regardless of
traditions and human imaginations  for example  would
the seven stars in the famous constellation big dipper
still be classified into the same cluster  this gives us an
inspiration of re clustering of constellations using unsupervised machine learning techniques 

non single stars  for our purpose  we extracted star
names  positions in the form of galactic coordinate and
magnitudes information from this database 
the feature is stars  galactic  coordinate on the celestial sphere  in addition  we set a threshold apparent
magnitude mc and only analyze the stars with magnitude  mc    a brighter star has smaller magnitude   in
the following  we choose mc       and     respectively 
for mc        there are    stars which belong to    real
constellations  for mc        there are     stars classified into    real constellations  the formal serves the
purpose to gain a quick result of the clustering algorithm
while the latter is the approximation of the stars in the
sky 

clustering methods
spherical k means  

first  we run the spherical k means algorithm for constellation re clustering  standard k means uses the euclidean distance as the measure  however  in our problem  since all the stars are on the celestial sphere  sphere
distance seems to be a better measure than ordinary euclidean distance  the spherical k means     algorithm 
which was initially developed for clustering large document collections  fits our problem perfectly  in this algorithm  the distance or dissimilarity measure is

data

d x i    x j         coshx i    x j  i  
in this project  we used data from bsc p database
table  which contains data derived from the bright star
catalog   th edition  preliminary  it is widely used as a
source of basic astronomical and astrophysical data for
stars brighter than magnitude      roughly every star visible to the naked eye from earth  the database contains the identifications of included      stars in several other widely used catalogs  double  and multiplestar identifications  indication of variability and variablestar identifiers  equatorial positions for b       and
j        galactic coordinates  ubvri photoelectric photometric data when they exist  spectral types on the
morgan keenan  mk  classification system  proper motions  j         parallax  radial  and rotational velocity
data  and multiple star information  number of components  separation  and magnitude differences  for known

   

where x i  is the unit vector indicating the star is coordinate on the celestial sphere  hx i    x j  i is the angle
between two unit vectors x i  and x j    the measure
    is also the distance or dissimilarity used in our other
clustering algorithms for constellation classification  in
company with this measure  the centroid coordinate i
associated with the cluster ci is then updated by
 j 
j x j  ci x
 
i   p
k j x j  ci x j  k  

p

   

for our purpose  we let the algorithm specify k automatically  ref      proposed a function f  k  to select
k and compared its performance to other methods of k
selection including aic  bic and the gap statistic     

fi 
here  we use f  k  defined in     as the criterion but replace the euclidean distance by the sphere measure     
the other improvement we made is about the initialization of k centroids  rather than the uniformly random
sampling  we adopted the seeding method in k means  
    
we apply the spherical k means   algorithm to the
    brightest stars when mc        we iterate k from
  to    and compute the corresponding f  k   which is
summarized in fig    

 b 
mc        input k      

   

fig     results of spherical k means    stars with
the same color are in the same cluster  the radius of
the circle represents the magnitude  i e  larger circle
means brighter 

   

affinity propagation

 

f  k 

 a 
mc        input k      

   

   
 

trial  
trial  
trial  
 

 

k

  

  

  

fig     f  k  values obtained by spherical k means   
only stars with magnitude      are considered 

affinity propagation  ap     is a clustering algorithm
that does not require the number of clusters to be determined or estimated before running the algorithm  unlike
k means  ap simultaneously considers all data points as
potential cluster centroids  exemplars   to find appropriate exemplars  ap updates two evidence matrices as 
r i  k   s i  k   max
 r i  k       s i  k     
k    k


 
a i  k   min    r k  k     max     r i   k  
i    k
i  i k 
 

 according to f  k   the optimal k is   because f    
is significantly smaller than other values  this suggests
that the brightest     stars on the sphere actually form
two clusters  however  k     is obviously too small for
the purpose of re clustering constellations 
 in a more reasonable range like k           f  k 
does not change significantly  this could be the sign that
constellations may not own clear clustering structure on
the celestial sphere 
 the number of real official constellations for these
stars is k       as shown in fig     if we choose
k      and k      corresponding to the real values
when mc       and mc        respectively  the stars
in the big dipper constellation are split into two clusters  compared with other constellations  the seven stars
in the big dipper are relatively isolated from its surroundings and thus are more recognizable  indeed  all the
civilizations in history group these seven stars together 
therefore  if we take the criterion that a good clustering should keep big dipper unseparated  then the real
value k      is too large 
in sum one problem when running k means algorithms
is to select appropriate k  we have shown that selection
k by the function f  k  and by the real value are not
good choices  therefore  we would better adopt an algorithm without the need to prespecify k 

a k  k   max
    r i    k    
 
i   k

   

finally  larger the r    k    a    k   more probability the
point k as a final cluster center  the algorithm involves
three matrices 
 the similarity s i  k   input matrix indicating how
well the data point with index k is suited to be the exemplar for data point i  for our problem  we set
s i  k       d x i    x k          coshx i    x k  i  

   

 the responsibility matrix r i  k   sent from data
point i to candidate exemplar point k  reflects the accumulated evidence for how well suited point k is to serve
as the exemplar for point i  taking into account other
potential exemplars for point i 
 the availability matrix a i  k   sent from candidate exemplar point k to point i  reflects the accumulated evidence for how appropriate it would be for point
i to choose point k as its exemplar  taking into account
the support from other points that point k should be an
exemplar 
the clustering results are shown in fig    a  and  b  
  clusters are obtained in fig    a  and    for fig    b  
the number of clusters given by ap is much smaller than
the real values and the seven stars in the big dipper
stay in the same group 

fi 

 a  mc        output 
k  

 b  mc        output 
k     

 a  mc      

 b  mc      

fig     results of dbscan 
fig     results of affinity propagation with damping
factor      preferences are the median of the input
similarities

other methods

although we need not prespecify k when running ap 
the number of outcome exemplars  clusters  is influenced
by the values of the input preferences s k  k   the diagonal
elements of the input similarity matrix  in our setting 
preferences are set to the median of the input similarities 
resulting in a moderate number of clusters 

just for completeness  we also call other clustering
methods in scikit learn     hierarchical clustering and
spectral clustering  as k means  these two methods need
to prespecify k 
the resulting dendrogram and clusters for stars when
mc       by agglomerative approach of the spectral clustering are shown as follows  the number of intersects

dbscan

dbscan      short for density based spatial clustering of applications with noise  is another clustering
algorithm that does not need to presepcify k  it is based
on local densities of the data and requires inputs eps and
mindist values to determine within what distance a point
will belong to a cluster  reachable   and how the points
that belong the same cluster are connected  but dbscan will also detect the noise data from the dataset 
namely the points that dont belong to any cluster 
as stated in ref      the suggested optimal mindist
value for two dimensional data would be    however 
in our problem  we found that mindist     is not the
best choice  the number of the resulting clusters is so
small that most of the stars are treated as noises  instead  we searched the parameter space of mindist and
eps to reach the number of clusters as close as the real
number  for mc        the best pair eps       and
mindist     will give    clusters  which is close to    
when mc        running dbscan algorithm for the
chosen stars for eps        and mindist     will give   
clusters with    noise points  the following two figures
show the result for the clustering of the big dipper
and orion  we can see that the big dipper could
be easily recognized except for one star in the neighboring cluster  the constellation orion belongs to a large
cluster with a high density of stars  since the algorithm
sets a global density based on input arguments eps and
mindist  it tends to choose the stars in high density region as the clusters and those in low density region as
the noises 

 a  dendrogram

 b  clustering results

fig     results of hierarchical clustering when mc      
with ward linkage and input k      and ward linkage 
when a vertical line is drew across the dendrogram will
be the number of clusters  inputing the real value k     
as one of the arguments of hierarchical clustering results
in fig     b   again  as k means  the big clippers are
no longer classified in the same cluster 
spectral clustering techniques first make use of the
spectrum  eigenvalues  of the similarity matrix of the
data to perform dimensionality reduction  then followed
by a k means in the low dimensional space     although
still running k means  spectral clustering has the ability
to detect non convex boundaries of the clusters  however  for the constellation clustering  we do not need the
non convex boundaries 

gui implementation

in order to facilitate the clustering process  we also
build up a gui to call the algorithms in scikit learn    

fi 
to zero  we then discount the expected rand index of
random labelings 
ri  

ari  
 a 
mc        input k      

 b 
mc        input k      

fig     results of spectral clustering 

a b
n

c  samples

ri  e ri 
max ri   e ri 

   

   

n

where c  samples is the total number of possible pairs in the
dataset 

silhouette coefficient

as well as a  d visualization  the screenshot is seen in
fig    

if the ground truth labels are not known  evaluation
must be performed using the model itself  the silhouette coefficient  sc      is an example  the sc score
is bounded between   for incorrect clustering and   
for highly dense clustering  scores around zero indicate
overlapping clusters  the score is higher when clusters
are dense and well separated  which relates to a standard
concept of a cluster  the sc is defined for each sample
and is composed of two scores a and b  a is the mean distance between a sample and all other points in the same
class and b is the mean distance between a sample and
all other points in the next nearest cluster  the sc for a
single sample is then given as 
s 

fig     gui screenshot

ba
max a  b 

   

the sc for a set of samples is given as the mean of the
sc for each sample 

comparison table
comparison of all algorithms

mc      
ari sc
k means          
hierarchical          
spectral          
affinity          
dbscan          
algorithms

adjusted rand index

adjusted rand index  ari      is to compare the result with the ground truth labels  ari has a bounded
range         where negative values are bad  independent labelings  and similar clusterings have a positive
ari  random label assignments have a ari score close
to    while     is the perfect match score  in this work 
we use the real constellations as the ground truth labels 
the mathematical formulation is as follows 
if c is a ground truth class assignment and k is the
clustering  we define a as the number of pairs of elements
that are in the same set in c and in the same set in
k  and b as the number of pairs of elements that are in
different sets in c and in different sets in k  to guarantee that random label assignments will get a value close

mc      
ari sc
         
         
          
         
          

to compare with the ground truth labels  all the prespecified k  if needed  are taken to be the same as the
number of real constellations  from the comparison table  we found that spherical k means  affinity propagation  and hierarchical clustering have better performance 
especially  they have similar sc score  note that the
reason for the relatively low ari score of affinity propagation is the difference between the number of generated
clusters and the real number of constellations 

fi 
future work

in this work  we use various clustering algorithms to
re clustering constellations  for each algorithm  we hope
to provide further justifications to pick appropriate input
parameters 
stars magnitude is simply treated as a threshold 
there maybe a more reasonable way to deal with the
magnitude  for example  in spherical k means  magnitude can be used as a weight factor in the dissimilarity
measure  or in affinity propagation  we may relate the
input preference to each stars magnitude 
finally  we can improve visualization of the clustered
constellations by methods like  d rendering  it would
provide us with a real life view of how the generated
constellations distribute in the universe 

    i  s  dhillon  j  fan  and y  guan  in data mining for scientific and engineering applications  springer        pp 

       
    d  t  pham  s  s  dimov  and c  nguyen  proceedings of
the institution of mechanical engineers  part c  journal
of mechanical engineering science                 
    r  tibshirani  g  walther  and t  hastie  journal of the
royal statistical society  series b  statistical methodology                 
    d  arthur and s  vassilvitskii  in proceedings of the eighteenth annual acm siam symposium on discrete algorithms  society for industrial and applied mathematics 
      pp           
    b  j  frey and d  dueck  science                 
    m  ester  h  p  kriegel  j  sander  and x  xu  in kdd 
vol            pp         
    http   scikit learn org stable modules clustering  
    a  y  ng  m  i  jordan  and y  weiss  in advances
in neural information processing systems
 mit press        pp         

fi