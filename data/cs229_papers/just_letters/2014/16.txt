reinforcement learning with deeping learning
in pacman
shuhui qu  tian tan  and zhihao zheng
shuhuiq stanford edu  tiantan stanford edu  zhihaoz stanford edu
abstract a new method to approximate the true value in
reinforcement learning by using deep neural network is
proposed  we simulated the pacman by using this method 
keywordsreinforcement learning  deep learning  q learning 

i  introduction 
nowadays  the machine learning has become a hot research
area with a lot of different algorithms  reinforcement learning 
different from most machine learning methods  requires
learning agent to acquire its own input data that repeatedly
choose an action to the dynamic environment  and get some
rewards and information about the action feedbacks  due to the
limitation of human experience and current technology 
collected data usually cannot cover all highly related features 
therefore  the learning agent should be able to explore data
features and possible hidden features by itself and gradually
improve its behavior by considering the consequences of its
past actions 
currently  there are several methods for reinforcement
learning  including model based monte carlo method  sarsa
 bootstrapping methods   q learning  as well as q learning
with function approximation  linear regression  svm and
other methods have been applied to function approximation for
q learning  however  in real world  some data such as spacial
and time series data may not have markov properties 
considering the fact that in deep learning  creating architecture
of hierarchical feature representations that become more and
more abstract for each additional layer of the architecture  thus 
learns features  hidden features could be generated during the
process 
this paper proposes an algorithm that applies deep neural
network and deep learning method for function approximation
so that hidden features as well as weights for each feature could
be derived to achieve optimal q function  first of all  the
relations between supervised learning and reinforcement
learning will be derived  secondly  deep learning will be
introduced to find optimal q functions  next  a simulation of
pacman will be applied to compare the method with other
reinforcement learning algorithms  finally  conclusions and
discussions of potential future work will be provided 
ii  literature review 
j  schmidhuber    studied the applicability of deep learning
for feature learning in combination with reinforcement learning 
he developed a new learning approach using stacked
autoencoders as feature learning method  s  dini and m 
serrano     implemented q learning by using a q table to store
data  with increasing complexity in the environment and the

agent  which will cause fail to scale in this approach  in order
to avoid this problem  they investigated an alternative
implementation in which use an artificial neural network as a
function approximator and eliminate the need for an explicit
table 
j  telecom     approach to the problem of autonomous mobile
robot obstacle avoidance using reinforcement learning neural
network  in the project  the author integrated these two
methods aiming to ensure that autonomous robot behavior in
complicated unpredictable environment  the result shows that
the combination of reinforcement learning and neural network
can improve learning ability and also make robot finish tasks
under more complicated environment 
iii  theory 
a  relation between supervised learning and reinforcement
learning
   supervised learning 
given a training set  try to learn a function h  x   y  so that
h x  could predict y values corresponding to new input of x 

fig     supervise  learning  model  
in order to make values of y to be as close to the hypothesis
of h x  as possible  a cost function is defined as 
 
h x     
  
where  is value of y for a given example in the training set 
considering the fact that in some cases it is not clear to find the
hypothesis function exactly we want  we add on regularization
term to make the algorithm less sensitive to outliers 
j 

j 

 
h x 
  

 
  

 

 

 
 

in order to minimize the value of cost function  we take
derivation of j as a function of   and set the value to be zero 

fi 
         
 

 j   

q function update
q

by deriving it  we could have the value of k  th iteration of
  gradient descent  
   reinforcement learning 
in reinforcement learning  each state is determined by
previous state  action that it takes and some random noise  the
process could be represented by 
  

x 

  

x 

  

x 

  

 q
q

               f

x 

q

 

   

   

 

  

q

 q

 

 

  fq

 

  q

 

q

 

q     x  a   q     x  a   q     x  a   r x  a     v x    
where  is the transition probability 
therefore  we can have 
err   q     x  a   r x  a     v x  

x 

where qopt x a  is the prediction  while r x a       is our
target  which is denoted by x  here  we could find that it is
similar to supervise learning 
                   
consequently  the cost function could be
                        

 
          

fig     reinforcement  learning  model  
x       f x    a    w 

b  q learning with deep learning

suppose that everything happens with reason and logic 
each state could be mapped to next state with the consideration
of action and random noise  where x is the state  a is action
that being taken under current state  w is random noise factor 
q  x  a    r    

                
                         
where  is the set of parameters in hypothesis learning
function  since we will use deep neural networks  dnn  in
our model   refers to a set of all parameters in dnn 
   reinforcement learning with deep neural network
architecture

t      x  a  x  v   x    
    

we also define the optimal value function as 
v  x   max  x  q  x
therefore  we could define the update function of v as 
v   tv
q function could be updated in the following format 
t x   x  a v    x 

q x  a   r x  a   
   

t x   x  a minq x  a 

q x  a   r x  a   
   

v x   r x  a   t x x  a v x
optimal value function update
v   tv
initiate 
v

 

 

  q

  

in each iteration  we could have 
v

   

  tv

 

q

   

 fq

 

fig     architecture  of  q learning  with  dl  
figure   shows the structure of the learning system with deep
neural network  initially  the network estimates the data
roughly  after interacting with the environment and getting
rewards  the estimation of value would converge to true value
and noise of the network could be minimized 
   deep neural network
the methods this paper discussed above  q learning  qlearning with linear function approximation  etc   have poor
performance when collected data do not have markov
property or have many hidden features that could not be
computed by superficial data  neural network sometimes
could give relative good prediction as it could also use one
hidden layer  however  neural network is shallow network 
features  the hidden layer of activation a  are computed by
using only one layer  deep neural network has multiple hidden
layers  this allows us to compute much more complex
features of superficial input  because each hidden layer
computes a non linear transformation of the previous layer  a
deep network can have significantly greater representational

fipower than others  by using a deep network  it could contour
and detect complex features 
   learning method    


   

    

 

fig     deep  neural  network  
in our deep neural network model  we have the input layer l
as ll   and use layer nl as output layer  nl is the number of total
layers   parameters for the network  w  b     w     b     w    
b       w nl     b nl      where wij l  denotes the weight for the
connection between unit j in layer l and unit i in layer l   
the activation  output value  of unit i in layer l is
represented by ai l   computation of activations is given by 
 

   

    

   

                

where  is sigmoid function  used in our model  and n
denotes total number of units in layer  l     xjs are inputs to
unit i in layer l  the output of the network can be represented
as 
    

          

   

 

   

    

  

    

     

 

in the paper  we use back propagation algorithm to train our
dnn model 
first  define the cost function with respect to one single
training example  x  y  to be 
 
 
                 
 
thus  if we also denote sl to be the number of nodes in layer l
 not counting the bias unit   then overall cost function with
respect to the whole training set can be written as 
   

 
  


 

   
   


        
 

            

  

 

 

           

where the second term is a regularization term  and  is weight
decay parameter 
so  the optimization problem now becomes minimization of
the above overall cost function as a function of w and b  we
initialize parameters w l s and b l s to small random values
near zero and then apply batch gradient descent  one iteration
updates w  b as follows 

   
   
          
    
   

   
   
          
       
   
where  is the learning rate  and we use back propagation to
compute above partial derivatives 

 
     


 



   
   

         

 
 


 

          

 

 

   

         

          

 

 
 
   
thus  we can update the network with batch gradient descent 
in this paper  we also update the network in a mini batch of
data in certain number of iterations 
   action selection
we use greedy action selection  when q x a  converge to q  
then we select the policy by               
   learning procedure for the q learning algorithm with
deep learning
step   initialize the deep neural network with n levels with
non linear activation function  combination of linear
activation is still linear activation 
step   input state data  and extract as many features
according to experience as possible
step   select action according to q x a  for aa where a is
action set
step   environment performs the action and response
rewards and states
step   update q function according to
                
                         
where                       is a parameter set
contains all parameters in dnn
step   repeat step  to step  until converge
iv 

simulation 

to test the performance of the combination of q learning and
deep learning  an example of pacman is applied in the game 
pros 
l
the  pacman  is  easy  to  implement  and  open  to  public  
l
the  state  space  and  the  action  space  of  this  game  are  
discrete  and  could  easily  define  each  state  
cons 
l
the   state   of   pacman   seems   to   have   markov   property   
each   decision   could   be   made   based   on   current  
conditions   without   consideration   of   past   steps   
therefore    this   is   not   a   perfect   example   for   deep  
learning    because   the   layers   might   be   shallow   and  
only  a  few  neutrons  in  each  layer  would  be  required  
to  solve  the  problem   
l
the   game   is   deterministic    the   q learning   with  
function  approximation  could  give  us  good  result   
a  introduction of simulation program
state  a state s consists of the map  pacmans location 
ghosts location  remaining foods location and current score 
action  there are five actions  go left  right  up  down and
stop  however  the pacman might be blocked if it encountered
a wall 
reward  if a ghost eats the pacman  the pacman gains     
points and game ends  if he gets a dot  he gains    points  if he
successfully eats all dots  he gains extra     points and he

fiwins the game  if he eats a ghost  he would gain     points 
any step would cost    points 
b  method we use
in order to evaluate the performance of q learning with
deep learning  we need to set up some benchmarks and
compare with other methods  q learning  approximation with
linear regression method and q learning with deep learning
 our proposed method  were applied  we could also play
pacman by using game method  therefore  minimax and
expectimax method were used to compare with the
performance of reinforcement learning 
   data
for minimax and expectimax method  there is no
requirements for data 
however  for q learning and the other two approximation
methods  data is gathered by running simulation 
   features
there are some basic feature inputs  distance from closet
food  whether ghost is close  whether to eat food  whether
ghost is eatable  number of food remaining and others 
for deep learning  there are hidden features to be
discovered  however  the number of hidden features or
perceptron needs to be carefully selected in order to reach
optimal result 
   environments
different methods performed differently under different
circumstances  two maps were implemented in the simulation 
small map 

fig     small  map  
medium map 

fig     medium  map  
   structure of deep neural network
the structure of deep neural network might influence the
score that could be obtained in the game 
there are two parameters that could be tuned  the number
of layers of network  the number of perceptron in each layer 
due to the fact that pacman works in a non complex pattern 
the number of layers and number of perceptron should be
restricted in order to avoid over fitting  therefore  we would
set the structure of the network as input layer       output
layer  input layer       output layer  input layer    output
layer  where    is the number of perceptron in that layer 
   deep learning implementation method
deep learning is implemented as 
we explore the map and gather data by running approximation
method  then these data was trained by using deep neural

network  after gathering last layer before the output  we plug
this layer back to predict q value and update parameter of
this layer 
c  result comparison
parameters for simulation  iteration is       the learning
rate is      exploration rate is       and discount factor is     
as we can see in the chart  q learning did not perform well
in the medium map  approximation method converged
quickly during the learning process and can reach score
around      
deep learning method converged after      iterations  it
did not outperform the approximation method during the
simulation   horizontal axis represents number of iterations in
hundred  e g     represents      iterations 

score  obtained  in  medium  classic  map  
      

q learning  

      

approximation  
method  

   
                                        
       

deep  learning
m        

fig     score  in  medium  map  
algorithm
average score
q learning
    
minimax
      
expectimax
       
approximation with linear regression        
approximation with deep learning
       
table     running  score  withour  exploration  
after setting exploration probability to       more
simulations were run to test the performance of each method
whose result is shown in the table
the score of small map is shown as follow  q learnings
performance is improved compared with medium map  qlearning with approximation perform well in the game  qlearning with deep learning does not perform well under     
iterations  however  it improved a lot after      iterations 
the reason behind this could be that q learning with deep
learning requires enough data to converge and make right
decisions  in a small map  the amount of data can be obtained
in each iteration is much smaller than in the medium map 
therefore  it requires more iteration  about       for deep
neural nets to converge and improve performances 
     

score  obtained  in  small  map  

   
                                            
      
       

q learning  
approximati
on  
deep  
learning  
m        

fig     score  obtained  in  small  map  

fi     

comparison  between  different  
hidden  layers

   

deep  
learning  
m        

                                                 
      

deep  
learning  
m     

       

fig     comparison  between  no   layers  

comparison  between  different  
number  of  perceptrons
     
deep  learning  
m        

   
      

                                           

deep  learning  
m        

       

 
fig          comparison  between  no   perceptron  
this paper also investigated the structure of deep neural
network as shown in fig  and fig    the network with more
layers converges quicker than the one with fewer layers  the
network with more perceptron in each layer converges quicker
than the one with less perceptron 
d  discussion
from previous results we can discover that 
l
the   pacman   problem   fits   better   by   using   q learning  
with  linear  regression  approximation   
l
during   the   construction   of   deep   networks    it   is  
discovered  that  parameters  dissipated  quickly  as  the  
number  of  layers  and  perceptron  increase   
l
the   computational   cost   of   deep   learning   is   large    that  
it   takes   longer   for   deep   learning   to   calculate  
parameters  and  update  values   
l
for   simple   problems    linear   regression   model  
performs  better  than  more  complex  models   
l
compared   with   linear   regression    the   deep   learning  
method  takes  more  iterations  to  converge   
l
deep  learning  method  requires  much  data  input   
l
for  simple  problems   the  more  layers  and  perceptron  
a   network   has    the   faster   it   converges    however   
these   numbers   should   be   carefully   selected   to   avoid  
dissipation   
v 

conclusion 

in this paper  we tried to apply mature supervised learning
methods to reinforcement learning for the purpose of helping
solve complex non markov problems in daily life  in
particular  we combined deep learning  dnn  with q learning

and proposed a new learning method for reinforcement
learning  rl   at the beginning  we investigated
reinforcement learning in details and found similarities
between rl and supervised learning  which proved that
indeed dnn could possibly be used in q learning  then we
constructed our new model by using dnn  finally  to test the
performances of our model and compare with other common
methods  we used the game of pacman as a platform and ran
simulations on it 
from the results shown in previous sections  it seems that
deep neural nets require large amount of data to converge 
thus  our model  q learning with dnn  deep learning  
performed better and converged much faster in medium map
than small map 
compared with other methods  our model in general
performed better than q learning  however  it did not
outperform q learning with linear approximation  for the
medium map  our model tied q learning with linear
approximation in performance  scores   but in the small map 
linear approximation is better  the game of pacman may be a
relatively simple problem and almost deterministic  thus 
linear approximation performed quite well for this specific
problem  while using our model  it took longer to converge
and the pacman in the game seems to be more hesitated when
making next decisions or actions  think too much as our
model takes into account more features and hidden features 
the overall results were not as good as linear approximation 
we also found out that the structure of the dnn matters 
parameters dissipated quickly as the number of layers and
perceptron increase  thus  each layer needs to be subtly
designed  in our simulation  the network with two hidden
layers and    neurons in each layer performed well 
the computational cost of our proposed model with dnn is a
major concern  for future work  we will strive for and focus
on reduction of computation costs  it may help in the future  if
we could design a better set of input features or design in great
details the structure of dnn  designing the structure of the net
in a way that state features and actions features can be
separated in hidden layers may help improve the performance
of q learning with deep learning model 
references
   

j  schmidhuber  deep learning in neural networks  an overview 
p      apr       

   

s  dini and m  serrano  combining q learning with artificial
neural networks in an adaptive light seeking robot q learning 
no  fig         

   

j  telecom  reinforcement learning neural
network to the problem of autonomous mobile
robot obstacle avoidance  no  august  pp             

fi