diagnosing malignant versus benign breast tumors via machine
learning techniques in high dimensions
danielle c  maddix
cs     machine learning
final writeup
december         

 

introduction and predicting

machine learning applications are vast  one such particular application to be investigated is in regards to
classifying whether a breast tumor is malignant or benign  even though there are predictive medical procedures that are available for diagnosis  one test may not be definitive enough and there can be error margins
of either false positives or false negatives  an algorithm which could take into account many test diagnostics and make a prediction has potential to have a broad impact in the medical field  in fact  the medical
literature is already becoming rich in such methods  with the potential goal of submitting patients to fewer
extensive testing  in terms of machine learning  this is a binary classification problem with continuous input
feature vectors xi that can be solved via supervised learning  since the corresponding labels yi     for negative
benign or   for positive malignant  are also given  forming m training examples  xi   yi    the output is either a
negative label   for benign or positive label   for malignant  it is clear that some features are very predictive 
such as the size of the tumor  but only considering this feature cannot give definitive results  the following
approach considers the eect of a larger number of features or dimensions  n 

 

data and features

the comprehensive dataset utilized is available from the breast cancer wisconsin  diagnostic  dataset on
the uc irvine machine learning repository  the dataset is fairly rich in examples  considering m      
patients  it consists of a matrix with    columns  where the first such column is the patient id and so
ignored in this study and the second column is the label m     for malignant and b     for benign  the
remaining    columns form the vector xi in the training example  xi   yi    there are ten distinct continuous
features measured  namely the radius  texture  perimeter  area  smoothness  compactness  concave points 
concavity  symmetry and fractal dimension  for each of these  the average  standard error and the worst case
measurements are reported  the class distribution is given by     benign samples        and     malignant
samples         note that this is fairly representative of the positive learning malignant samples  which
can be difficult in medical based datasets  such as with aids diagnoses  another dataset to be explored for
future research focuses other physical and also biological features  such as clump thickness  since cancer cells
tend to form multilayers  uniformity of cell size and shape  epithelial cell size  bare nuclei  bland chromatin
and mitoses  a comparison of the results from training on this dataset to the prior one could help distinguish
the characteristics that are most relevant in the diagnosis process 

 

models

the approach to this binary classification problem was to implement several supervised learning algorithms
and compare and contrast their results and error properties  the first such algorithm was logistic regression 
 

fiwhich is a discriminative learning algorithm directly modeling the conditional probability  p y x   the fitting
parameters    rn     including the intercept terms are computed via the maximum likelihood estimators
and then an optimization algorithm is used to find the optimal   both the second order newtons method
and gradient ascent were explored  newtons method was preferred to both stochastic and batch gradient
ascent  even though the implementation of the gradient ascent is simpler and each iteration is cheaper 
since it only requires calculating the gradient rather the hessian  it took far more iterations to converge 
newtons method already had error  as measured by the norm of the gradient  of approximately machine
precision  after      iterations  in comparison to the hundreds for both gradient ascents  another challenge
of gradient descent is choosing the proper step length  to expedite convergence  this could be done via
parameter fitting or by choosing an adaptive  via a bisection method  however  a downside to newtons
method is that it is more subject to round o errors  the hessian must stay negative semi definite and not
be poorly conditioned  since it is being inverted in the algorithm  in order to avoid these poor numerical
properties  the feature data in the design matrix x   rmx n    was normalized for logistic regression  since
the original feature data given had very dierent scales of magnitude  note that this normalization was
not necessary the gaussian discriminant analysis  gda  algorithm and so the implementation of gda was
fairly straightforward in such that no modifications needed to be made 
gda is a generative learning algorithm  which contrary to discriminative algorithms  first builds a model
for p x y       the positive class of malignant tumors and also builds a model for p x y       the negative
class of benign tumors  it then learns p y x  using bayes rule 
p y x   

p x y p y 
p x y     p y        p x y     p y     

   

in linear gda  the posterior densities are assumed to be multivariate gaussians with means   and    
respectively and same covariance matrix  and the prior density p y  is assumed to be bernoulli distributed 
in other words  x y      n         x y      n        and y  bernouilli     in the quadratic case 
the means and prior remain the same  but there are two distinct covariance matrices    and   such that
x y      n          and x y      n           note that these parameters were computed  using their
corresponding maximum likelihood estimates  mle   an advantage of the quadratic case is that it allows
the decision boundary to be nonlinear  which can help in models with high bias to increase the dimension of
the hypothesis space  h 
the last supervised learning algorithm implemented was support vector machine  svm   this algorithm
is designed to maximize the functional and geometric margins and so it is known as the optimal margin
classifier  this is a desired property to prevent the dataset points from clustering around the decision
boundary  where the margin for misclassification is the highest  thus  it solves an optimization problem to
maximize the distance between the points and decision boundary  as the primal is displayed below 
min

 w b

 
kwk 
 

s t  y  i   wt x i    b 

   i             m

   
   

the train and predict functions within liblinear      were used for implementation purposes in matlab  note
that all of the other above described algorithms were implemented from scratch in matlab 

 

results and analysis

below are the error tables comparing the results from the various algorithms  namely linear gda  quadratic
gda  logistic regression and svm  note that the error measurements reported for logistic regression are
using the second order newtons method  rather than stochastic or batch gradient ascent  since it produced
more accurate results with a fewer number of iterations  to explore the usage of the best number of features 
the errors were tabulated for a various number of features 
 

fitable    results  various tests versus various number of features for gda
linear gda
hold out cv
k fold cv
recall
precision

 
     
      
     
      

 
     
     
      
      

  
     
     
      
      

  
     
     
      
      

table    results  various tests versus various number of features for gda
quadratic gda
hold out cv
k fold cv
recall
precision

 
     
      
      
      

 
     
      
      
      

  
     
      
      
      

  
     
      
      
      

table    results  various tests versus various number of features for newton logistic regression
logistic regression
hold out cv
number of iterations
k fold cv
recall
precision

 
      
 
      
      
      

 
     
  
     
      
       

  
     
  
     
      
      

  
     
  
     
      
      

table    results  various tests versus various number of features for svm
svm
hold out cv
optimization accuracy
k fold cv
recall
precision

  
     
        
      
      
      

  
     
        
      
      
       

  
     
        
     
      
      

the hold out cross validation approximation to the generalization error was computed by training on    
of the data  namely     samples and testing on the remaining      the recall  the ratio of true positives to
actual positives  as a measure of the lack of false negatives  and the precision  the ratio of true positives to
labeled positives  as a measure of the lack of false positives were also computed on the above data sampling 
it is clear from the above tables that gda produces higher precision than logistic regression and svm 
whereas logistic regression produces highest recall  in this problem in particular  the higher recall may be
more valuable  since a false negative could be more dangerous to the care of a patient  who then may not be
treated  whereas with a false positive  the patient would most likely undergo more testing before treatment 
furthermore  we also note that in all cases  the hold out cv error decreases as the number of features increase 
which is indicative of a high bias problem  this error for gda is generally lower  which can be explained by
gdas property to use data more efficiently  since it can learn more quickly on smaller datasets 
for another error metric  the k fold cv was also calculated  which only holds out m k  for k       of the
training data each time for testing and trains on the remaining and takes the average of these k errors  as
expected  logistic regression has lower error in the   and    features case  since it is generally asymptotically
more efficient and robust with larger data  we note that the k fold cv error is higher than the hold out
 

ficv error and it also decreases as the number of features increase  for more features than     the hessian
becomes ill conditioned due to the dependency of the features and so that error is not reported 
to visualize the results  a comparison of two two dimensional cases for the algorithms trained on the first
    of the training examples are displayed below  along with their training errors and in addition for logistic
regression the number of iterations it took newtons method to converge  the plots on the left are perimeter
versus texture and the plots on the right are texture versus radius  note the decision boundaries and for both
versions of gda and the contours of each multivariate gaussian modeling the positive and negative classes 

 b  linear gda  training error         

 a  linear gda  training error         

 d  quadratic gda  training error         

 c  quadratic gda  training error        

 e  logistic regression  training error          niter      f  logistic regression  training error           niter    

 

fi 

discussion

the gda algorithm seems to product the best overall results  which are even better than logistic regression
and svm  at first  this may seem surprising  since in general the latter two algorithms make less assumptions
and are more robust  a possible explanation for this is that the posterior data is actually distributed as a
multivariate gaussian  this is collaborated by the alignment of the contours in the plotting section  if this is
the case  then gda will clearly outperform both algorithms  moreover  to compare linear vs quadratic gda 
we see that the assumption linear gda makes a good approximation to data  namely that both distributions
share the same covariance matrix   we observe that   and   are very close in values of elements and
so quadratic gda appears as linear  since the quadratic term    xt           x is very small in terms of
machine precision and linear term dominates  furthermore  generally for all models  there is decrease in the
error measurements with increased dimensional feature space  this demonstrates that solving this binary
classification problem in higher dimensional feature spaces positively aects the results  note that svm
specifically does better in higher dimensional feature space and so these are the results included in the svm
table 

 

future work and conclusions

it is evident from the error decrease with increased number of features that the linear models suer from
high bias  moreover  the small gap between the training error and approximate generalization error indicates
high bias  this implies that the hypothesis class of linear separators is not rich enough  future work includes
investigating using a nonlinear decision boundary with higher dimensional polynomials 
alternate approaches to consider are regularization with bayesian logistic regression or in terms of generative learning algorithms a multinomial naive bayes model with laplace smoothing  where the features
are discretized within certain ranges  moreover  it is key to find the optimal number of features relevant to
tumor diagnosis to solve the problem in a smaller subspace and so feature selection should be implemented 
lastly  investigating the other two datasets available in the repository would be interesting  as stated
in the data section  a comparison of this more physical based dataset with measurements from medical
images to the dataset with    more biological based features could be used to indicate which of these types
of characteristics is more highly correlated in diagnosis  another available dataset can be used to solve a
slightly dierent binary classification in supervised learning of whether a tumor is more likely to recur of
non recur 
it is clear that there are promising results in the area of applying supervised learning algorithms into the
realm of cancer diagnosis for potential use in collaboration with the established medical tests and to help
avoid evasive diagnostic tests on patients 

references
    uci machine learning repository  center for machine learning and intelligent systems 
breast cancer wisconsin  diagnostic  data set  wdbc data 
http   archive ics uci edu ml 
machine learning databases breast cancer wisconsin wpbc data       
    uci machine learning repository  center for machine learning and intelligent systems 
breast cancer wisconsin  diagnostic  data set  wdbc names  http   archive ics uci edu ml 
machine learning databases breast cancer wisconsin wdbc names       
    andrew ng  lecture notes      http   cs    stanford edu materials html       

 

fi