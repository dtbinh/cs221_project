predicting helpfulness ratings of amazon product
reviews
jordan rodak

minna xiao

steven longoria

stanford university
jrodak stanford edu

stanford university
mxiao   stanford edu

stanford university
sx      stanford edu

a bstract
in this paper  we outline our approach to predicting the
helpfulness of amazon com product reviews  specifically in
the case of electronics products  our developed classifier
assesses a dataset of up to      product reviews randomly
culled from a preprocessed dataset of over         reviews to
predict whether a given product review is either helpful or
unhelpful  performing feature selection on anatomical  metadata and lexical features  and using naive bayes and svms
with various kernels  we were able to achieve up to      
accuracy on product reviews 
i  i ntroduction
with the growing popularity of e commerce  online retailers
such as amazon com and yelp com are increasingly relying on
the ubiquity of user supplied product reviews to crowdsource
the online shopping experience  providing information to consumers and feedback to manufacturers  with the inundation
of reviews and varying degree of quality of such reviews on
popular online sites  we aimed to develop a machine learning
approach to automatically assess and rank the helpfulness of
reviews 
for the purposes of our project  we will focus on amazon coms product review platform  after a consumer purchases a product  amazon prompts him or her to write a
review of the product and rank it based on a star rating scale
from one to five stars  in order to differentiate reviews based
on their helpfulness  amazon has implemented an interface
that allows customers to vote on whether a particular review
has been helpful or unhelpful  the fraction of customers
who have deemed the review helpful is displayed with the
review  and amazon uses these ratings to rank the reviews 
displaying the most helpful rankings on the products front
page  the drawback is that more recently written reviews are at
a disadvantage since less people have voted on the helpfulness
of the review  because of this  reviews with few votes cannot
be effectively ranked and will not gain visibility until they
have accumulated adequate votes  which can take some time 
as a result  we would like to assess the helpfulness of reviews
automatically  without having to wait for users to manually
vote over the course of time  if we can do this  we would be
able to give users the most relevant  helpful  and up to date

reviews possible  without any delay in more helpful reviews
being displayed  moreover  such an automatic classification of
reviews would be able to help in rooting out poorly written
reviews lacking helpful information to other consumers 
ii  dataset
to implement our project  we focused on a subset of
amazon com product review data  which we obtained from
snap stanford edu  in particular we are using the data set for
electronic products sold through amazon  for which there were
over     million electronics reviews that span a period of   
years up to march       the raw data includes for each review
the product id  title of the product  price of the product  user
id and name of the reviewer  the fraction of users who found
the review helpful  the reviewers star rating of the product  out
of five   time of the review  review summary  the text of the
review  and the products description 
iii  p rocess
a  preprocessing
before drawing features from the data  some preprocessing had
to be done  to begin with  we iterated through the     million
reviews and narrowed down our search to those reviews that
had more than    helpfulness ratings  because the percent
helpful attribute on our reviews would not be accurate and robust a measure for reviews with few helpfulness votes  beyond
that we also limited our search to reviews for products had at
least    reviews  because automated helpfulness classification
itself is not necessary if the given product for which the
reviews are being displayed has few reviews to begin with 
after narrowing down our search this way we parsed out
all of the information we wanted  mentioned above   after
this preprocessing we were still left with upwards of        
reviews  in addition  we went through our revised dataset
and calculated the average star rating for each product by
aggregating across all the reviews for each product  so we
could have this data present for feature extraction 
b  feature selection
from our raw data  for each review we obtained the text of the
review  the reviewers product rating  rated out of five stars  

fithe reviews helpfulness rating  fraction of helpful votes out of
the total number of votes   and the product description 
to derive the features for our purposes  we examined samples of amazon product reviews and their corresponding
helpfulness ratings and observed that product rating  content 
and presentation of the reviews were key in assessing their
helpfulness with readers  we incorporated several groups of
features as our main approaches 
anatomical features 






length of the review
sentence count
character count
number of exclamation and question marks
number of words in all caps

our motivation for these structural features were to capture
token based and syntatic based analysis of the text of the
reviews  the perceived helpfulness of a review will be
influenced by the reviews length  for example  short reviews
most likely will not contain much information about the
product   additionally  we used the exclamation marks and
all caps word counts as a rough measure of extreme emotion
in reviews   reviews with high volume of exclamation
sentences or all caps tend to be overly emphatic and may
prove to be not as helpful  moreover  we purported that
reviews that ask too many questions may not be too helpful
to other consumers  since rather than offering information
and assessment about the product they are instead asking
additional questions 

training body of reviews  and instead weigh more heavily
words that appeared frequently in the single review at hand 
we computed the tf idf statistic for word t in review r using
the following formulation    
tf idf   tf  idf
where the tf term is calculated by the augmented term frequency  i e  the tf factor normalized by the maximum tf of a
word in the review r  and further normalized to lie between
    and     which is normalized to prevent bias towards longer
reviews 
f  w  r 
tf  w  r             
max f  t  r  
tr

and the inverse document frequency is measured by  
idf  w  r    log
with 









meta data features 



number of stars  score rating 
deviation from popular opinion

unrelated to actual text analysis  we also decided to capture
the rating a reviewer gave the product at hand  in addition 
we also considered a review ratings deviation from popular
opinion  calculated by taking the absolute value abs stars avgerage stars   our intuition was that reviews which give
extreme ratings that deviate considerably from the average
rating will not be as helpful to a reader 



to assess readability of each review  we considered two such
readability tests to assess review reading difficulty    


flesch reading ease score  computes reading ease
of the material on a scale from   to      with lower
numbers indicating a text that is more difficult to read 




totalwords
totalsyllables
            
    
totalsentences
totalwords



automated readability index  calculates an approximate
representation of the u s  grade level needed to comprehend the text  essentially how many years of education
are needed to understand the text  




characters
words
ari       
    
     
words
sentences

unigrams
readability

we captured the unigrams features of each review text by
computing the tf idf weight for each word in a review  rather
than just using the basic bag of words model that just takes
into account word frequency  using tf idf  term frequency 
inverse document frequency   we were able to scale down
the weighting of stopwords such as a  from  the and
other common words that appear frequently across our entire

n  the total number of reviews in the training data
r  a specified review in the training collection of reviews
r
w  the specified word in review r that we are calculating
the tf idf statistic for
tf  w  r    f  w  r  denotes the the raw frequency of
word w in review r  the number of times w occurs in
r 
max f  t  r    the maximum raw frequency of any word
tr
t in the review r
   r  r   w  r   the number of reviews in the training
set r that contain the word w  adjusted by   to avoid any
possible division by zero error

additionally  we captured the readability of each review  which
gauges the comprehension difficulty of each review  i e  how
easily a reader can read and process a passage  our intuition
behind using this feature was that users will generally not find
reviews that are too complex or difficult to read  or at the other
extreme too simple or immature diction wise  helpful 

lexical features 


n
     r  r   w  r 

fiwe also experimented with a feature to capture the extent
with which a product review contained mentions of words in
the products description  with the intuition that reviews that
a higher volume of description overlap  such as mentioning
certain specs of a product like cpu or aperture  would
contain more helpful information to the consumer reading the
review  however  we observed that incorporating this feature
into our feature set did not improve classification accuracy  we
purport that our unigrams feature was sufficient in subsuming
product description word mentions  particulary since the use of
tf idf weighting gave less weight to common words and more
weight to review  and product specific diction  as a result  we
dropped this feature from our final classification evaluation 
fig     training data accuracy of models against various training set sizes 

iv  m odels
in our work we modeled our problem as a binary classification
problem  where we wanted to be able to take in a given review
and classify it as either helpful or not helpful  to do this we
took the      reviews that we randomly selected from our set
of         and looked at whether or not they were helpful 
where we said a given review was helpful if its helpfulness
rating  the percentage of people who claimed the review was
helpful as opposed to not helpful  was greater than      note
here that we made sure to balance these      reviews  where
we had an equal amount of helpful and non helpful reviews so
that our classifiers would not be biased to varied class sizes 
we then took these      reviews and selected a set of     
to test on  we left the remaining     as our training set 
where we iterated through increasingly large set sizes from
this training data and trained multiple classifiers to see how
they would perform on classifying our test data  we moved
our way up in set size as                                
     
the classifiers we used were naive bayes  and support vector
machines with linear  sigmoid  radial basis function  rbf  
and polynomial kernels  these classifiers are very powerful
and helped model the data well  in earlier iterations of our
project we attempted using linear regression  which did not fit
our problem well because binary classification nature of the
problem 
v  r esults
train size

naive bayes

linear

sigmoid

rbf

poly

  
   
   
    
    
    
    

     
     
     
     
     
     
     

     
     
     
     
     
     
     

     
     
     
     
     
     
     

     
     
     
     
     
     
     

     
     
     
    
    
     
     

table i
t est accuracy of various classifiers

figure   displays our results for each of the models we tested 
with our most consistent models we were able to achieve

fig     classification test accuracy of models against various training set
sizes 

about     prediction accuracy on the helpfulness of product
reviews  in particular by using rbf and polynomial kernels for
svm  earlier in our work we had a much higher accuracy
around      but we then realized that it was because of a
strong bias in our dataset  we had been training on a set of
reviews that were     helpful  and so our classifiers were
simply prone to predicting helpful almost all the time  upon
balancing our data our accuracy has dropped a bit more  but
as we can see now there are clearly some patterns that are
being detected within the data itself 
as a baseline for our discussion it helps very much to note
that the sigmoid svm  while having a bit over     accuracy
on small sample sizes of the training set  oscillates right at
the     mark on our test set regardless of training set size 
this makes sense with respect to the fact that a sigmoid
function is not a very helpful model for our type of data 
and because the data is perfectly balanced we should be able
to expect     accuracy while knowing that no patterns are
being extrapolated 
using this as a baseline we can see that our other four classifiers are doing better in most cases than      which tells us
that they are extrapolating some pattern s  from the data given 

firbf and polynomial svms interestingly have achieved the
exact same accuracy regardless of sample size  because rbf
and polynomial kernels are much more similar to one another
than linear kernels  sigmoid kernels  or nave bayes  this makes
sense to a certain extent  another interesting pattern we see is
that using a linear svm leads to oscillations in accuracy level
that are dependent upon training size  though we would expect
that as our training set grows much larger the linear kernel will
perform better than other models  the interesting patterns for
our purposes though lie within a comparison between nave
bayes and the rbf polynomial svms  as we can see both
improve in accuracy over time  but we also notice that nave
bayes tapers off in accuracy gain much sooner than rbf poly
svms  we have a gain in accuracy of      for nave bayes
from    samples to      and      for the rbm poly svms 
nave bayes has its largest accuracy gain before we hit     
samples though  while we see the rvm poly duo continuing
to gain accuracy into the      and      sample sizes 
vi  c onclusion
to conclude we are definitely encouraged by our results 
we are seeing that our classifiers are definitely extrapolating
patterns from this data  and this shows promise for being
able to successfully classify review helpfulness upon review
publishing  an important take away though is with respect to
which of these classifiers was most successful at classifying
our data  now  while linear svm did in fact achieve the
highest accuracy at certain points  it was unreliable as well 
and as such we would not recommend it for the task at hand 
using the sigmoid kernel did not at all model our data well 
and so we are left with naive bayes or rbf or a polynomial
svm  now this comes down to the type of available data 
naive bayes is definitely helpful for very small sample sizes 
and while it does not attain the accuracy of a polynomial
svm  it gets pretty close  however  though we are able to
achieve around a    percent accuracy  we would like to see
if this statistic could be improved with hyperparameter tuning
for the rbf and polynomial kernels  and if more optimized
feature selection and additional features could achieve similar
improvements 
vii  f uture w ork
future extensions of our work include focusing on the area of
features improvement  more specifically we would like see
if syntactical features such as parts of speech tagging  and
sentiment analysis of the polarity of sentences in reviews 
or capturing the subjectivity vs  objectivity levels of reviews 
would improve our classification errors  and while our accuracy was promising  there is room for growth in extrapolating
even deeper patterns from our data in this way 
one future goal of ours would be to incorporate pairwise
ranking using the svm linear kernel  if we could compare
any two given reviews and rank one above another  this would

help for specific ordering of reviews  there is some difficulty
in this  as ranking reviews would require other reviews to
already be present  which most of the time will be the case  
otherwise  simple binary classification as we have already
accomplished can yield impressive results for times where
there are no other reviews to compare to  or if we would like
to provide immediate automatic feedback to a reviewer about
the helpfulness of his or her review 
additionally  we would like to generalize our classifier to
any product category  i e  furniture or movies   to see if
our feature set could be applied succesfully to any subset of
review data  not just the electronics reviews  or if developing
specialized classifiers for different categories would be more
optimal 
r eferences
    kim  s m   p  pantel  t  chklovski  and m  pennacchiotti  automatically
assessing review helpfulness  in proceedings of the      conference
on empirical methods in natural language processing  pages         
sydney  australia  association for computational linguistics      
    w  h  dubay  the principles of readability   costa mesa  calif  impact
information       

fi