

adaptivespacedrepetition

sheilaramaswamy
stephenkoo



i introduction
from weekly podcastsandactiveforumsforpolyglotstofulllanguagelearningsuiteslikeduolingo differentapproachesand
philosophies to language learning abound  however  little has been done to adapt languagelearningsystemstotheuserandher
learning patterns wefocusedinparticularonflashcards awidelyusedandpowerfultoolformemorization anumberofstudies
over the past century revealed that the productivity of memorization methods such as flash cards can be greatly augmented by
spaced repetition  a technique that involves spacing reviews of previously learned material over successively longer time
intervals  thus saving time by exploiting the fact that we take longer to forget something after each successful repetition 
moreover  computer programs now allow us to manage scheduling foranunlimitednumberofcards neverthelesseventhebest
options available  such as supermemo andanki  stillemploysimplealgorithmsthatdolittlemorethantoincreasetherepetition
intervals each time by a crudely derived coefficient  meanwhile  the users interactions with the system actually provide a large
amount of potentially useful information  with which a smarter system could theoretically gain a better understanding of the
users mastery of the material  through the use of machine learning techniques  we aim to build models of the relationship
between user performance statistics and the card repetition intervals  in an effort to more accuratelypredictmoreoptimalcard
displayfrequencies 
in this project weusethepublisheduserlogsofaflashcardprogramcalledmnemosyneasthebasisforourpreliminarydata
analysis and model training  we then build a working prototype of an online flash card system thatemploysthetrainedmodel 
while additionally updating its parameters with the new input fromtheonlineuser inordertoprogressivelyfitthemodeltothe
individual users learning patterns  the algorithms we used in our system were largely provided by the scikitlearn machine
learningpythonpackage 


ii data

mnemosyne is aflashcardapplicationfirstdevelopedin    asfreealternativetotheproprietarysupermemoprogramand
simultaneously as a research project into the nature of longterm memory   anonymized user logs are uploaded
automaticallyafter initial voluntary consentto a central database  which has thenbeenpublishedonlineperiodically weuse
a version released on january           which contains            log entries for flash card repetition  i e  display  events  
each log entry consists of information suchastheuserid timestamp andtheamountoftimesincethepreviousrepetitionofthe
particular card  for the purposes of this project  we evaluate the log entries two at a time  each training example consists of
features extracted from a pair of consecutive log entries for the same card and user  the set of features are as follows  we will
refertothechronologicallyfirstentryasthefirstrepetition andthesecondasthesecondrepetition  

  feature

description

  previousgrade

anintegerscoreintherange     providedbytheuseroneachrepetitionasanassessmentofhisorher
performance takenfromthefirstrepetition

  nextgrade

sameaspreviousgrade buttakenfromthesecondrepetition

  easinessfactor

afloatingpointvaluecomputedfromthecumulativehistoryofgradesonthegivencard withhighervalues
correspondingtohigheroverallgrades  ascomputedpriortothesecondrepetition

  retentionrepetitions

thenumberoftimestheuserhasgivenherselfagradeof orhigherthecardontwoconsecutiverepetitions 
priortothesecondrepetition

  acquisitionrepetitions

thenumberoftimestheuserhasgivenherselfagradeof or onthecard priortothesecondrepetition

  lapses

thenumberoftimestheuserhastransitionedfromaretentionrepetitiontoanacquisitionrepetition priorto
thesecondrepetition

  retentionrepetitions
sincelastlapse

thenumberofretentionrepetitionssincethelastoccurrenceofalapse priortothesecondrepetition

  acquisitionrepetitions
sincelastlapse

thenumberofacquisitionrepetitionssincethelastoccurrenceofalapse priortothesecondrepetition

  thinkingtime

theamountoftimeinsecondsthattheuserspentreadingthecardonthefirstrepetition

   interval

theintervaloftimeinsecondsbetweenthetworepetitionsinthepair
figure  descriptionsandenumerationsoffeatureset 

 

http   ankisrs net 
http   rockyfortress     herokuapp com
 
http   mnemosyneproj org 
 
thelogscanbefoundat https   archive org details         mnemosynelogsall db
 

 



theexactformulaisbasedonthesupermemo algorithm whichisdescribedindetailat http   www supermemo com english ol sm  htm

fi

insomecases wemaydiscretizetheintervalonthescaleenumeratedinfigure  inorderto
smoothoutnonlinearityandtoreducetheamountofnoiseinthedata theintervalranges
thatwechosearebasedontheintuitionthatsinceforgettingcurvesappeartofollow
exponentialdecay therepetitionintervalsshouldfollowaroughlyexponentialgrowth
pattern 
inaddition sinceusing  millionentriesisexcessivewithinourtimeconstraints we
partitionthedataintoindividualdatasetsbyuser wecapeachdatasetat    training
examples andforeachdataset weholdout   examplesasthetestset leaving   
examplesasthetrainingset wefocusononeuseratatime sinceourgoalistotrainsystems
thatcancatertothelearningcurveofeachindividualuser 


iii modelsandanalysis

ourmainframeworkconsistsofpredictingtheintervalgiventheotherfeatures
described wevariouslyapplydifferentpredictionmodels suchasthesupportvector
machine svm  togeneratepredictors toapplyapredictortoschedulingthenextrepetition
foraflashcard wecomputeallthefeaturesbasedontheusershistorywiththecardsofar 
whileartificiallysupplyingatargetgradeasthenextgrade thustheoutputintervalshouldbetheintervaloptimizedtoward
helpingtheuserobtainthegiventargetgradeonthenextrepetition onemaytheoreticallychooseanytargetgrade inouronline
system wearbitrarilychoseatargetgradeof  whichintuitivelyaimsforcomfortablerecallwithoutwastingtimeandmakingit
tooeasyfortheuser 

featureanalysis
asapreliminaryanalysis weusedarecursivefeatureeliminationalgorithmfrom
scikitlearn whichusesrecursivefeatureelimination todetermineanoptimalsetof
features givenanestimator theprocedurereturnstheoptimalnumberoffeaturesalong
witharankingofthegivenfeatures wechosethelinearregressionastheestimatorforthis
analysis becauseofitssimplicityofinterpretation higherrankedfeaturesexhibitstronger
 linear correlationswiththerepetitioninterval onourdatasetofsize     wefoundthe
optimalnumberoffeaturestobe  withtherankingsaslistedinfigure  inallsubsequent
models wethusonlyincludethefeaturesinthetoptworankcategoriesinthefeatureset 


linearregression

asourfirstmodel weagainchooselinearregression wethusattempttoexpressthepredictedinterval y oftheithrepetition
pairasanaffinefunctionofthefeatures f  
y i    wt f  i     
where w   areparametersthatminimizetheleastsquaresresidualsbetweenthepredictedintervalsandtheactualintervalsin
thetrainingdata topredictanoptimalintervalforachievingatargetgrade g target  wecanpullthesecondgradeoutofthefeature
vectorandrewritetheexpressionas 
y   wt f    w
next grade g target    
wecomputedthelinearregressiononadatasetforanarbitrarilychosenuser werecordedthemeansquareerror mse from
aftertestingandthecoefficientofdetermination r  whentestingthemodelonboththetestandthetrainingdataset 

supportvectorregression
analternativetosimplelinearregressionthatwetriedissupportvectorregression svr  thesvrbuildsamodelbysolvingthe
optimizationproblem 
minimize      w    
subjectto y  i   wt f  i     
and wt f  i      y  i    
where  isafreeparameterthatservesasathresholdfortruepredictions theresultingmodel similartosvms dependsonlyon
asubsetofthetrainingexamplesbyignoringanyexamplesclosetothemodelprediction 

supportvectormachine
finally wetrainsixdifferentmulticlasssvmclassifier svc modelsforeachuser witheachmodeltrainedonlyondatawiththe
samevalueforthenextgrade thisreflectstheintuitionthatthedistributionofrepetitionintervalsmaybeconditionally
independentgiventhevalueofthenextgrade wealsodiscretizethesetofpossibleoutputintervalsinto  classes asdescribed
previously weusetheonevsonevariantofthemulticlasssvmclassifier whichoptimizesthestandardbinarysvmobjective
foreachpair k ofthe  classes 



fi
minimize      wk       c   k  i  
i

subjectto y  i   wk t f  i     k        k  i  
 k  i      i           n 
whereweusearegularizationcoefficient c        thefollowingdecisionfunctionforeachkthbinarysvmisthenappliedtothe
featurevector f  

y k   sign wk t f     
andtheclassthatispredictedbythegreatestproportionofthebinarysvmsconstitutestheoverallprediction ofcourse the
algorithmsareactuallyimplementedintermsofthedualformofthegivenprimalobjectiveinordertotakeadvantageofthe
kerneltrick inourtests wetrythelinearkernel polynomialkernel andthegaussian rbf kernelwith       offeatures 
k rbf  x  y     exp     x  y     

 

 


clustering
understandingthatdifferentpeoplemayhavevastly
differentlylearningcurves wespeculatedthattrainingasingle
modelasthepriorforallnewusersonoursystemwouldnotbe
veryeffective wethustriedtoclustertheusersinourdata
basedontheirrepetitionlogstheresultingclusterscouldthen
potentiallyallowustotrainmultiplemodels eachconditioned
ontheusersclosestcluster 
wecomputedthreeaggregatefeaturesforeachuser 
retentionrate acquisitionrate andlapserate thesearethe
ratiosoftheircumulativeretentionrepetitions acquisition
repetitions orlapsestotheirtotalnumberofrepetitions 
respectively werankmeansclusteringon   usersinthe
databaseusingthesefeatures with k     clusters however we
foundnoconvincingclusters andavisualizationofthedata
corroboratesthefact asseeninfigure  wenonethelesstrain
andtestourmodelsondatapartitionedbytheclusters toseeif
accuracycouldbeimprovedbyassumingindependenceofthe
parametersbetweenclusters 


iv results
model

datadescription

training r  train  mse train 
setsize

testset r  test 
size

linearregression

datafromuser
c      a

   

    

               
 

    

           
    

svr

datafromuser
c      a

   

    

               
 


    

           
    


linearregression
conditionedonlapse
rateclusters 

useddatafrom 
     
userinthecluster
fortraining another
userfromthesame
clusterfortesting

mse test 

                         
  


                         
     e   
     e   

linearregression
seeaboverow
conditionedon
retentionrateclusters

    

                         
  


                         
     e   
      e   


linearregression
conditionedon
acquisitionrate
clusters

    

                         
 


                         
     e   
      e   


 

seeaboverow

forthisandallsubsequenttest weonlyconsideredusersthathadatleast    samples astheyappearinthemnemosynelogs andhadlookedatat
most   cards 
 
approximately   becausewesampledrandomusersfromthelogsfortesting ranthemodel times andtooktheaverageaccuracyoverallthe
clustersandruns 



fi
linearregression
conditionedon
acquisitionandlapse
rateclusters

seeaboverow

    

model

datadescription

svcconditionedon
lapserateclusters

useddatafrom userin     
theclusterfortraining 
anotheruserfromthe
sameclusterfortesting

                         
 


                         
     e   
     e   

trainingsize trainingaccuracy testsize testingaccuracy
        

    

        

svcconditionedon
seeaboverow
retentionrateclusters

    

        

    

        

svcconditionedon
acquisitionrate
clusters

seeaboverow

    

        

    

        

svcconditionedon
acquisitionandlapse
rateclusters

seeaboverow

    

        

    

        

svcconditionedon
grade   rbfkernel 

trainwithuser
   
c        testwith
usersc      a 
c     f  c  b ed  
c  e eb  c  fcf b

            

    


            

svcconditionedon
grade   rbfkernel 

seeaboverow

   

            

    

           

svcconditionedon
grade   rbfkernel 

seeaboverow

   

            

    

            

svcconditionedon
grade   rbfkernel 

seeaboverow

   

            

    

            

svcconditionedon
grade   rbfkernel 

seeaboverow

   

            

    

            

svcconditionedon
grade   rbfkernel 

seeaboverow

   

            

    


            

svcconditionedon
trainwithuser
   
grade   linearkernel  c        testwith
usersc      a 
c     f  c  b ed  
c  e eb  c  fcf b

            

    

            

svcconditionedon
seeaboverow
grade   linearkernel 

   

            

    

           

svcconditionedon
seeaboverow
grade   linearkernel 

   

            

    

            

svcconditionedon
seeaboverow
grade   linearkernel 

   

            

    

            

svcconditionedon
seeaboverow
grade   linearkernel 

   

            

    


            

svcconditionedon
seeaboverow
grade   linearkernel 

   

            

    

           

svcconditionedon
trainwithuser
   
grade   polykernel  c        testwith
usersc      a 
c     f  c  b ed  
c  e eb  c  fcf b

            

    



            

fi
svcconditionedon
seeaboverow
grade   polykernel 

   

svcconditionedon
seeaboverow
grade   polykernel 

   

svcconditionedon
seeaboverow
grade   polykernel 

   

svcconditionedon
seeaboverow
grade   polykernel 

   

svcconditionedon
seeaboverow
grade   polykernel 

   

    
            

            
    

            

            
    

            

            
    

           

            
    

            

            




v discussion
therelativelydecentcoefficientsofdetermination     ontestset fromthelinearregressionindicatethatthereexiststo
somedegreeacorrelationbetweenthevaluesofourfeaturesandtherepetitioninterval thisprovidedthegreenlighttocontinue
workinthisdirectionandextractasmuchworthaspossiblefromthecurrentfeatureset andalsogaveagoodbaselinefrom
whichtostart however theextremelyhighmeansquarederrorsalsoindicatethatthepredictedintervalsaretypicallynowhere
nearthecorrectintervals thismayindicatethattherelationshipbetweenthefeaturesandtherepetitionintervalisnotcorrectly
characterizedaslinear oraffine  thismaybeaddressedinanumberofways addfeaturesthatarenonlinearfunctionsofthe
currentsetoffeaturetoincreasetheexpressivityofthemodel oruseacompletelydifferentmodel perhapsageneralizedlinear
model thatmoreaccuratecapturestherelationshipbetweenthefeaturesandtherepetitioninterval asmentionedpreviously 
wedirectlyflattenedthenonlinearitiesintherepetitionintervalusingadiscretizationscheme transformingtheproblemintoa
classificationprobleminsteadofaregressionproblem moreover thesvrmodelperformedevenworsethanthenavelinear
regression thuswesubsequentlyfocusedonundertakingthenewclassificationproblem 
theresultsofapplyingamulticlasssvmtotheproblemvariedconsiderablydependingonthesetup ourattemptstocluster
usersbasedontheirlapserate etc  yieldedfewresults thesvcmodelstrainedonclustereddatacouldnotpredictwithan
accuracymuchbetterthanhalf whichislikelybecausetheclustersarenottrulyclusterstobeginwith thelackofdistinctive
clustersisunsurprising giventhatafeaturespacewiththreedimensionsistypicallyhardlyenoughtoclusteranysignificantset
ofsamples moreover asoftperformancemetricsuchaslapserateismorelikelytoformagaussiandistributionratherthan
separableclusters ascorroboratedbyourplots thisawareness however washelpfulinselectingmorerepresentativeusersto
selectfortrainingandtestingoursubsequentmodels 
conditioningthemodelsonthenextgradeprovidedgreatgainsintheclassificationperformance wecanseethatdepending
onthegivennextgradeandthechosenkernel wecanachieveupto   accuracyonthetestset whiletrainingaccuracyis
generallyquitehighregardless thelargediscrepancyinperformancebetweenthetestsetandthetrainingsetindicatesthatthe
learnedweightsdonotgeneralizewellacrossusers sincethedatasetsarepartitionedbyuser  however thisisfineforour
intendedpurpose wherewemaintainindependentmodelsforeachuser andthusoverfittingtoasingleuserisinfact
necessary inthefuture itwillthenbeimportanttousedatapointsheldoutfromthesameuserasourtestsetsinstead 
overall itisalsounderstandablethatourresults evenonthetrainingset isatbestonlydecent westillonlyhaveasmall
numberoffeaturesthatdontnecessarilyhavealinearrelationshipwiththeoptimalrepetitioninterval andwemaybeabletodo
muchbetterbybuildingoffofexistingmodelsofhumanmemoryinpsychologyliterature nonetheless wecansafelyconclude
thatitispossibletoapplymachinelearningmethodsoutoftheboxtobuildausableflashcardscheduler 

vi futurework
ifwehaveanother monthstoworkonthisproject wewouldfirstdelvedeeperintoexistingliteratureabouthumanmemory 
languageacquisition andforgettingcurveswecouldthendeviseevenmoreappropriatemodelsforthequalityofan
individualsmemoryofagivenitem wewouldalsoopenupourapplicationtomoreusers collectingmoreusagedataandeven
collectingadditionalfeatures suchasthinkingtime  wecouldalsodevelopawaytoassessausersgradeonaflashcardthat
doesnotdependontheirfeedback tomakethecalculationofthegrademoredeterministic finally weneedtodevelopbetter
metricsforthequalityofourmodels e g atleastincorporatingrecall precision andf scoresintoourclassificationevaluation 

vii additionalreferences
p a wozniak       may    applicationofacomputertoimprovetheresultsobtainedinworkingwiththesupermemomethod
 online  available http   www supermemo com english ol sm  htm

pavlikjr  p i  presson n   koedinger k r        optimizingknowledgecomponentlearningusingadynamicstructuralmodel
ofpractice inr lewis t polk eds   proceedingsoftheeighthinternationalconferenceofcognitivemodeling annarbor 
universityofmichigan 



fi