blowing up the twittersphere  predicting the optimal time to tweet
zach ellison  zellison stanford edu 
undergraduate bs computer science candidate 
stanford university

seth hildick smith  sethjhs stanford edu 
undergraduate bs computer science candidate 
stanford university

data

abstract
as social media plays a growing role in society  more and more
people are depending on social media for outreach and advertising  as this dependence grows  so does the importance of
understanding how to make posts as effective as possible  in
this project  we explore some of the factors that make posts
effective  create a predictor to determine how successful posts
will be  and then use that predictor in order to determine how
to optimize one of those factors  time  specifically  we examine tweets on twitter and attempt to give the time that will
optimize the effectiveness of a tweet with given text and by a
specific user 
keywords  machine learning  regression  twitter  tweet

introduction
we can separate our problem into a few different steps  first 
we need to model information about a tweet and how successful a given tweet is  second  given a tweet  user  and
post time  we must predict how successful that tweet will be 
finally  we then need to use our predictor to determine the optimal time for a given user to post a specific tweet  i e  what
time maximizes our success prediction for a specific user and
tweet 
we considered two papers that address similar problems
of using machine learning to understand interactions in social media and predict success of online content  lakkaruja 
mcauley  and leskovec consider the connections between title  content and community in social media  from their work 
we saw the benefits of breaking features into different models in order to better understand which types of features were
having the greatest impact on our final predictions  they also
did reasonably extensive language modeling when considering titles  which we considered when designing our own language model to examine the effect of the text of the tweets 
 h  lakkaraju   leskovec       tsagkias  weerkamp  and
de rijke wrote a paper considering a similar problem to ours
as they attempted to predict the number of comments an online news article would receive  we were able to draw from
their technique of first classifying an article to determine if it
would receive any comments before then running a regression
to determine how many comments it would receive when we
were handling issues of sparsity in our data set   m  tsagkias
  rijke      

our data comes from scraping tweets using the twitter search
api  we utilize the tweepy py python package  to connect
and query the twitter api  we utilized portions of scripts
from an assignment in ms e        taught by sharad goel  
we work with two datasets         original tweets  i e  not
themselves retweets  that all contain the word stanford  our
stanford dataset  and         original tweets that all contain the word california  our california dataset   both
datasets comes from the six day period of november        
  th
to november          and are split into a testing set of   
the total dataset and a training set of the remaining tweets 
we use the testing training split as a preliminary test of generalizability and ultimately apply k fold cross validation to
test the generalization error of our algorithms  our dataset
is notably sparse  only        of the         total tweets
collected achieved a non zero number of retweets 
from the very outset we were able to identify several interesting patterns in the data by simply plotting the average
retweets per tweet and the total traffic over the day  see figure     the figure demonstrates that retweet rates are not
driven by traffic  in fact we observe that a number of points
through the day there is an inverse relationship between the
retweet rate and the number of tweets posted  the plot additionally demonstrates through filtering by retweet success
that the shape of the retweet rate curve is not a result of large
retweet outliers 

features
there are a few different components we need to take into account when working with twitter  because twitter is a social
media site  we need to take into account the user themselves
and their relationship to the twitter community  in order to
measure the efficacy of tweets  we also need to consider the
quality of the content of the tweet  finally  we need to consider the time that the tweet was posted to take into account
the value of a given time  we therefore have three different
feature categories that utilize different raw inputs to address
these factors  we have user features to account for a users
relationship to the twitter community  language features to
account for the quality of the tweets  and a time feature to
account for the activity of the twitter community at a given
  roesslein
  goel

fitable    derived features
feature feature
class

description

user features
indicator on f ollowers count in
bucket i           bucketing polynomial degree     step     
indicator on f riends count in
f riends bucket j
bucket j          
bucketing
polynomial degree     step   
listed bucketk
indicator on listed count in bucket
j          bucketing polynomial
degree     step   
f avourites bucketl indicator on f avourites count in
bucket l           bucketing polynomial degree     step    
indicator on statuses count in
statuses bucketm
bucket l           bucketing polynomial degree     step    
language features
sentiment
binary indicator on sentiment polarity for very positive         positive         neutral     negative        and very negative        
objectivity
binary indicator on sentiment objectivity for objective         and
subjective        
parts of speech tags binary indicator on the presence
of each part of speech tag for the
number of times that part of speech
occurs                        
f ollower bucketi

figure    retweet rates and traffic over a day
time 
table    raw input
feature
followers count
friends count
listed count
favourites count
statuses count

text
created at

description
user input
number of followers a
user has
number of friends a user
has
number of lists a user is
on
number of times a user
is favorited
number of statuses that
user has posted
language input
the text of the tweet
time input
unix timestamp the
tweet was created at

type
int
int
int
int

time bucketn

int

string
int

raw input
the raw user data includes  f ollowers count  f riends count 
listed count  f avourites count  statuses count  text and
created at  see table     we take this data from the json
formatted tweets in our dataset 
we use statistics about a users popularity to and the amount
of interaction they have with twitter to account for their relationship with the twitter community  we would expect that
users that are more well liked or more popular  higher followers  more friends  higher listed and favourites counts  to

time bucketn
and f eaturex

time features
indicator on created at in bucket
n          
interaction variables
indicator on the union of
time bucket n and feature x 
derivedfeatures   timefeatures

generally have more successful tweets because they have a
larger number of people watching for their tweets  similarly 
we expect users with a high number of posted statuses to have
more retweets because they receive more exposure within the
twitter community by virtue of having more information out
there 
the time feature includes only  created at  this model is
relatively simple  it keeps track of the time at which a given
tweet was created  we convert all times to pst from utc 
in this conversion step we assume all tweets in our stanford
and california datasets are posted in  or directed at california 
this assumption does not add noise to the dataset as all times

fiare converted uniformly  we believe this assumption is justified by the traffic pattern we see which matches to pst users 
time is important to our model as after building a regression
we predict the optimal input for time 
note that we do not capture a users features as node in a
graph beyond their degree  we will consider the effects further in the discussion section  but would like to acknowledge
that a model that better considers the graphical nature of the
twitter community would better model these factors 

derived features
we derive features to overcome three problems with our raw
input data  implied linear relationships between real input
values and retweets  lack of interaction between input features  and unstructured information in the text  to remove the
implied simple linear relation between real valued input variables and output values  predicted retweets  we discretize the
range of input variables  we bucket inputs on a polynomial
bucketing scheme such that ith bucket for feature class c of
size
 i     kc  bucket sizec    ikc  bucket sizec  
  we found through experimentation that polynomial feature
bucketing out performs linear feature bucketing  we tuned
bucket size and polynomial degree for each feature class by
plotting the average retweet rate per bucket and tuning the
variables to produce a plot with strong signal  figures   and
  are an example of this process 
feature interaction between time features and other features is important to our predictive analysis of optimal time
because without feature interaction we would predict the
same time for all tweets  we took two step to overcome this
issue  for linear models we add derived interaction variables
by taking the inner product of the time feature vector and the
each of the other feature class vectors transpose i e  define
feature set f containing features class vectors vc   we take
interaction variable features
t
 vtimevcf v
 
time

as both vectors in the inner product contain only one nonzero entry  which has value    the resulting matrix has a single non zero entry with value   
the language features include  sentiment  subjectivity  and
parts of speech tags  we used the python textblob package  to help with our natural language processing  it takes
a line of text and returns different nlp tools  the first feature we considered is sentiment  which is returned as a float
from    to    we separate it into five binary variables corresponding to very negative sentiment           negative sentiment          neutral      positive          and very positive
         second  we consider subjectivity   which is returned
as a float from   to    we break it into two binary variables
for subjective         and objective          finally  we consider part of speech tagging  textblob returns a list pairing
  loria

figure    average retweet rate per user listed bucketslinear bucketing
each word in the text with its corresponding part of speech
tag  we then create binary features for each possible part of
speech tag  based on the penn treebank project  and whether
there are                   or       occurrences of a given tag 
we combine these three sets of features to build our complete
language model 

models
our problem has two main parts to it  first  we need to predict
the success  measured as the number of retweets  for a given
tweet  i e  given user info  tweet text  and time   second  we
need to determine the optimal time to tweet given user info
and tweet text  i e  argmax the prediction of retweets over
time  we apply four models each an attempt to minimize
mse between predicted retweets and true retweets  we utilized the scikit learn python package  to perform the regressions  while we wrote code to process the input and output
of the regressions  we also built more complex classifierregression hybrid moodels around the scikit learn classification and regression implementations  lastly  wed like to note
that scikit learn acts as a wrapper to the libsvm library for
support vector machines 

linear least squared regression
we use a linear regression algorithm to predict the success of
a given tweet  and then take the argmax over that linear regression to determine the optimal time  our linear regression
model finds the weight vector
w   min   xw  y    
w

  pedregosa

fisupport vector classification   support vector
regression hybrid
similarly to the logistic regression   least squares regression hybrid  in this model we again train a classifier to classify tweets as receiving zero retweets or not and then build a
regression model on non zero retweets  in this case we train
a support vector classifier on the following dual problem 
 
min t q  et  
w  
 t
y   
conditions  
   i  c  i            

results

figure    average retweet rate per user listed bucketspolynomial bucketing

we found that linear regression was unable to overcome the
challenge of highly sparse data well 

logistic regression   least squares regression
hybrid
in an attempt to overcome the problems presented by our
sparse dataset better than linear regression we applied a
regression classification hybrid  we first attempt to classify
tweets as receiving zero or non zero retweets and then apply
a regression model trained on all non zero retweet tweets to
the tweets we classify as receiving non zero retweets  we use
logistic regression to classify zero or non zero  i e  we find
weight vector w such that
n
 
min wt w   c  log exp yi  xit w   x       
w c  
i  

we then apply the least squares regression defined previously
to find a regression output of predicted retweets 

we apply the above models to predict the number of retweets
for each tweet in our testing set or alternatively in the testing fold of our k fold cross validation to evaluate our success  we use mean square error as a measure of correctness
of our regression  i e  our error is computed as the average
of the square of the difference between the predicted number
of retweets and the actual number of retweets  performing
a number of iterations of training on a fixed training set and
testing on a fixed testing set we found that our testing error
regularly beat out training error  after further inspection we
found that large outliers in the training data pulled the training
mse up  our testing error was not a good test of generalization error  to correct for this we joined our testing an training
data and ran k fold cross validation to estimate generalization
error  we found that our regression was able to beat the naive
baseline of predicting zero  the naive baseline here is not a
trivial bar to beat as about     of our data has zero retweets
and of those with non zero retweets very few have values over
   
table    training error
svr
lsr
svc svr
lr lsr
naive

support vector regression
we apply support vector regression to find a more reliable
regression model  our support vector calculations depend on
taking 
 
min   w    
 
such that

conditions  

yi  hw  xi i  b
hw  xi i   b  yi  

mse
       
       
        
       
       

zero mse
      
      
 
 
 

non zero mse
        
        
           
       
        

table    testing error
svr
lsr
svc svr
lr lsr
naive

mse
     
       
       
    e   
       

zero mse
      
      
      
      
 

non zero mse
           
        
        
    e   
        

we visualize our argmax solution to finding the optimal
time in figure   by plotting our prediction of retweets for

fitwo real tweets side by side over the course of a day  the
points in this plot indicate the true values 

connected followers or how retweets propogate through the
network ought to be answered a research continues on this
problem 

evaluation

figure    predicted retweets over time

discussion
our data is highly skewed toward tweets that receive
zero retweets  this skew has complicated our regression
goal  to overcome this obstacle we have worked with two
classification regression hybrid algorithms in which we first
attempt to classify   vs non zero tweets and then perform a
regression on the non zero values  this method did not improve our results because the classifier was still struggling to
classify correctly because of the sparsity of retweets  we see
four primary weaknesses in our approach which may be improved to achieve better results 

language
the wording of a tweet determines much of its success  although we employed sentiment and subjectivity classification
as well as parts of speech tagging to model the language of
a tweet  a human takes much more information to decide
whether to click retweet  we struggled to derive wit  humor
and apropos qualities of tweets which greatly influence success  with greater computing power it may be valuable to use
a bag of n gram type model of the text to predict on text important to retweets  another potential solution to extracting
more structure from text which would require less computational power is to apply a k clustering algorithm to the text
of tweets and include the assigned cluster as a feature in the
regression modeling  this type of solution would hopefully
be able identify certain types of tweets which receive more
retweets than others 

graph structure
twitter is fundamentally a social network  from user statistics we were able to gauge the outgoing degree of a node
 user  in the graph however that is the limit of our treatment
of the graph nature of twitter  we believe that further analysis of each user as a node in a graph will yield better results
than our method thus far  questions such as how to represent
a user with highly connected followers vs a user with poorly

while mse is a reasonable evaluator of our regression it does
not evaluate our goal to predict optimal posting time  it is difficult to find a good evaluator of optimal time as a user posting a tweet fundamentally changes the audience and could
not expect to receive the same reaction twice  we believe the
mse on the regression is a reasonable proxy for evaluating
optimal time as a perfect regression for retweets would receive   mse and also perfectly predict the optimal time  it is
also important to note that a an imperfect regression can still
predict optimal time perfectly as long as the argmax of predicted retweet values is the true optimal time  mse evaluates
to a higher standard than a theoretical evaluator against true
optimal time but remains a proxy for an evaluator of our goal 

regression goal
retweets are not a very good estimator of success for two reasons  tweets are rarely retweeted  about     of our dataset
has zero retweets  a retweet changes the audience of a tweet 
twitter has recently come out with a new metric called views 
which is a count of the number of other users that have seen
 ostensibly meaning read  the tweet  which will hopefully be
available to the api soon  utilizing this metric would allow
us to further tease apart distinctions between high exposure
and high success  i e  how many people saw it and out of
those  how many people felt strongly enough to retweet or
favorite it  from this  we would hopefully be able to further
separate exposure and virality from popularity as measures of
success 

acknowledgments
wed like to thank andrew ng and all of the cs     tas for
their help on this project throughout the quarter  wed also
like to thank sharad goel for his ms e     assignment that
helped us utilize the twitter api to collect our data  finally 
wed like to thank twitter for their public api that made this
project possible 

references
goel  s          assignment    mse      computational
social science 
h  lakkaraju  j  j  m     leskovec  j          whats in a
name  understanding the interplay between titles  content 
and communities in social media  icwsm 
loria  s          textblob  simplified text processing 
m  tsagkias  w  w     rijke  m  de          predicting the
volume of comments on online news stories  icwsm 
pedregosa  f   varoquaux  g   gramfort  a   michel  v  
thirion  b   grisel  o   et al          scikit learn  machine learning in python  journal of machine learning
research               
roesslein  j          tweepy 

fi