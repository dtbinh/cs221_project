artist attribution via song lyrics
michael mara
december         

 

introduction

structing our own dataset 
song lyrics were obtained via the genius api using
the ruby gem rapgenius rb    they were then processed using the python natural language toolkit
 nltk     for each artist dataset  we downloaded the
lyrics to all available songs by each artist  and created an ad hoc blacklisting mechanism in python to
remove translated lyrics and non songs  rap genius
sometimes has transcripts from movies or interviews
with the artist   we also initially excluded songs that
featured other artists even if our target artist was the
primary artist on the track  in order to mitigate corrupted data from verses from the featured artist  obtaining the   artist  initial  dataset  consisting of    
songs  upon examination of the learning curve from
the dataset  which suggested more data would give a
significant benefit  we relaxed the requirement  then
obtaining the   artist  extended  dataset      songs 
and the    artist dataset       songs  
as a final test  we also tried testing on a dataset
made of all songs by all artists appearing on
wikipedias list of hip hop musicians  with over   
songs available on genius  this resulted in a dataset
with     artists         songs   table   summarizes
our three datasets 

song lyrics  separated from the audio signal of their
song  still contain a significant amount of information  mood and meaning can still be conveyed effectively by a pure textual representation  there
has even been somewhat successful previous work on
genre classification from song lyrics     building on
previous work  we seek to build an artist attribution
system for song lyrics 
this task is in the same vein as classic author attribution tasks  which often are trained and evaluated
on extremely large datasets     providing more data
per author than it is possible to get for most songwriters  in order to focus our task  we focus only on
rap  as the songwriter and performer are usually the
same  and there is a heavy emphasis on distinctive
forms of lyricism  we actually enshrine that first assumption in our statement of the classification task 
given a textual representation of the lyrics of a rap 
return the name of the artist who raps it  this is
a limitation we will have to live with for now  there
does not exist any large public database that provides
ghostwriting information for rappers 
potential use cases for such a classifier would be
for detecting misattributed songs in a music library
or as part of an auto tagger in a music management
system  along with other uses of author attribution
systems  a lyric only classifier could also be used in
an ensemble method that includes audio only classifiers 
following previous work     we initially attempt to
distinguish between   prolific rappers  eminem  nas 
jay z  and nicki minaj  before expanding the classification task to encompass more artists  testing thoroughly on a    artist dataset  and eventually testing
on over     rappers at once 

 

 

features and preprocessing

given the raw lyrics to a song  we first filter out song
descriptors  such as  chorus    verse   via a simple handcrafted regex  then tokenize the remaining
lyrics  all features are extracted from this tokenized
representation  except in our final experiments  we
stick to a simple bag of words model  which has
proven to work very well on related tasks     often
beating painstakingly handcrafted features 
in order to obtain the bag of words representation 
we stem the tokens using the nltk snowball stemmer  construct a vocabulary consisting of every word

dataset

 

https   github com timrogers rapgenius
previous work has run into the issue that there ap  http   www nltk org 
  http   en wikipedia org wiki list of hip hop 
pears to be no reliable large dataset of lyrics with
author attribution     so we follow their lead in con  musicians  accessed           

 

fiartist
t i 
 pac
snoop dogg
ice cube
nelly
lil jon
sir mix a lot
ying yang twins
eminem
nas
kanye west
nicki minaj

song count
   
   
   
   
   
  
  
  
   
   
   
   

the word 
   w    max    w  a 
a

we then chose n features by choosing the n words
with the highest value of    
feature weighting in naive bayes with the
kullback leibler measure    was briefly considered 
but postponed due to the relative ineffectiveness of
our initial feature selection methods 
in our final experiments  we add on part of speech
 pos  bigrams to test the value of adding a proxy
for syntactic structure  first each token in a song is
converted into a pos tag using the ntlk  and then
the count of each bigram of the resulting tags is used
table    the artists and song counts from the     as a feature 
artist dataset  the   artist  extended  dataset consists of just the songs from the final four rows 

 

dataset
  artist  initial 
  artist  extended 
   artist
    artist

song  
   
   
     
      

vocab  size
     
     
     
      

models

we use our own matlab implementation of a multiclass naive bayes classifier using the multinomial
event model and laplace smoothing as our main
model  this was chosen based on its widespread success in many text classification tasks 
as a sanity check  we also implement a model based
on support vector machines  we use the built in
matlab fitcsvm   to train an ensemble of   vs all
binary svm classifiers  on the same features used for
naive bayes  we do multi class classification by selecting the artist whose corresponding svm returns
the highest score  the default c parameter  known
as the boxconstraint parameter in some texts and
the matlab documentation  causes severe overfitting    training error       test error   so we also
train with hand tuned smaller c values 

table    the artists and song counts from the   artist dataset  the   artist  extended  dataset consists of just the songs from the final four rows 

in the dataset  and construct a feature vector for each
song consisting of the count of each instance of each
word in the vocabulary appearing in the song  the
resulting in a bag of words representation is ideal for
our naive bayes classifier    
on top of this  we implemented two feature selection methods in order to hopefully improve generalization error     first a simple document frequency
results
thresholding  which removed words from the vocabu   
lary if they did not appear in at least   songs  second 
note that all results use    fold cross validation unwe computed the   statistic for feature selection    
less otherwise specified  taking a cue from computer vision  specifically the imagenet classification
tasks      we report not only the standard error rate
x
x  ne e  ee e  
w a
w a
   w  a   
for our larger datasets  but also some of the top n
e ew ea
ew       ea      
error rates  where an example is counted as misclassified if its correct label was not among the n rated
is computed for each artist word pair  where ew is as most probable by the model  note that the top  
the occurence of the word w    when it occurs    error rate is identical to the standard error rate 
when it does not   ea is the occurrence of the artist
a  new ea is the observed frequency of co occurence of
    initial results
the events and eew ea is the expected frequency of the
co occurence of the two events if the two events were our initial experiments used the   artist  initial 
independent 
dataset  running the same classification task as guo
we then assigned a   score to each word by taking et al      with this dataset  and using all      feathe max over all   from artist word pairs involving tures  our naive bayes classifier achieves a test error
 

fidataset

learning curve from initial   artist data
    
training error
test error

   


model

error rate

    

   

  artist  initial      
songs       features 

  artist  extended 
     songs      
features 

   artist
      songs       features 

training
error

test error

training
error

test error

training
error

svm  c    n      

 

      


 

      


 

svm  c        n      

     

      

     


      

      

svm  c        n      

     


      


     

      

nave bayes  all features 

      

      

     

nave bayes  n      

      

      

nave bayes  n     

      

      

    

test
error

top   test
error

top   test error

      


      

      

      

      

      

      

      


      

      

      

      

      

      

      

     


      

      

     

      

      

      

      

      

      

      

      

   

table    our results on our   main datasets using all
of our models  naive bayes is the best model in all
of our tests 

    

 
  

   

   

   

   

   

   

   

  training examples

error type
training error
test error
top   error
top   error
top    error
top    error
top     error

figure    the learning curve from the initial data
suggested more training examples would continue to
decrease our test error rate 

error rate
      
      
      
      
      
      
      

table    error rates for our naive bayes classifier on
the     artist dataset 

   

main results

table     is a table of our main results  and figure     gives the learning curves for the   artist  extended  and    artist datasets on our highest perfigure    the test error rates for our naive bayes
forming model  figure     provides a visualization of
classifier on the    artist dataset as we very the numthe confusion matrix  averaged over the    fold cross 
ber of features  using our  selection criteria 
validation  of our highest performing model  the
only anomalous result is the unusually poor classification of the yin yang twins  which our model hardly
of         slightly lower than guo et al  at       ever uses as the predicted label  this may be partially
in order to improve results further  we plotted the attributable to the fact that the yin yang twins have
learning curve  figure       and saw that more train  the lowest song count in all of our datasets at    
ing examples would likely benefit our model  this
is when we got rid of the no featured artists constraint  and obtained our other datasets 

   

scaling up

in order to get a taste of how our best model performs
on a dataset more than an order of magnitude larger
 both in song count and artist count   we ran our
in figure      we show that our feature selection naive bayes model on a dataset of        songs across
method does not seem to help results significantly      artists  due to the fairly high error rate  though
even on our    artist dataset  though it does allow fairly low compared to chance   and high computathe removal of many features without negatively af  tional cost  we report results only for naive bayes
fecting our error rates 
using all available features  see table      

   

adjusting feature count

 

fimean percentage choices

   artist confusion matrix visualization

learning curve from extended   artist data
training error
test error

 
nicki minaj
kanye west
nas
eminem
yin yang twins
sir mix a lot
lil jon
nelly
ice cube
snoop dogg
 pac
t i 

chosen label

   
    

error rate

  

t i 
 pac
snoop dogg
ice cube
nelly
lil jon
sir mix a lot
yin yang twins
eminem
nas
kanye west
nicki minaj

   
    

   

intended label

figure    confusion matrix for our naive bayes classifier on the    artist dataset  using all features   the
one anomalous result is the high misclassification of
the yin yang twins  who have the least number of
songs in our dataset 

   
    
   
    

bag of words
 
 

   

   

   

   

   

   

   

model
naive bayes
svm  c   
svm  c       
svm  c       

   

  training examples
learning curve for    artist data
   

   

table    comparison between our base model and
our model augmented with pos bigrams  our tests
showed no improvement  in fact a deterioration  from
adding pos bigrams to our model  although it improves the test error slightly on our svm c        
the best error for the svm still comes without the
use of pos bigrams 

   

error rate

      
      
      
      

  pos
bigrams
      
      
      
      

   

   

   

 
 

   

   

   

   

    

    

    

    

    

   

    

  training examples

adding features

as a quick final test  to try and get more information
out of our limited number of training examples  we
tried augmenting our bag of words model with partof speech  pos  bigrams  generated using the nltk 
as a proxy for local syntactic structure 

figure    the learning curves for our naive bayes
classifier using all available features on both the  artist  extended  and    artist datasets  note the
similarity to the learning curved for the   artist  initial  dataset 

 

discussion

judging by figure      our feature selection mechanism seem to be at best not harmful  theres no noticeable improvement in the error rate by selecting
smaller feature sets  though it also doesnt hurt until
      features on the    artist dataset  examining
our main results  table      the naive bayes classifier does fairly well on the task  having a significantly
 

fireferences

lower error rate than the svm  which takes far longer
to train  given previous successes using naive bayes 
this is not entirely unexpected 
our classification methods hold up fairly well even
when tripling the number of categories  with our best
classifier achieving       top   test error on the   artist dataset  we outperform guo et al    on the
  artist task  likely due to our larger quantity of training examples  via our programmatic data extraction 
inspection of the learning curves  figure      suggests more examples could lead to further reduction
of our error rate  as training error is much lower than
our test error  however  this is infeasible  artists only
put out a finite number of songs  and most have a
smaller discography than any of the twelve in our
toughest classification task 
in order to get better classification  to the point
where we could perhaps get reasonable classification
on our     artist dataset  we must make better use of
the current data  raw bag of words  though elegant 
throws away a lot of information that could be captured by higher level features  such as rhyme scheme 
sentiment  and syntactic construction  although we
found no benefit from adding pos bigrams  there are
many other features that could be tried in a more extensive analysis  including rhyme and style features
used in previous work    

 

future

 

acknowledgements

    boulis  c   and ostendorf   m  text classification by augmenting the bag of words representation with redundancy compensated bigrams  in
in proc  of the fsdm        
    guo  s   and khamphoune  s  im different  yeah im different  classifying rap lyrics by
artist  in stanford cs    final project papers
       
    john  g  h   kohavi  r   and pfleger  k 
irrelevant features and the subset selection problem  in machine learning  proceedings of the eleventh international
        morgan kaufmann  pp         
    krizhevsky  a   sutskever  i   and hinton 
g  e  imagenet classification with deep convolutional neural networks  in advances in neural
information processing systems  p       
    lee  c  h   gutierrez  f   and dou  d 
calculating feature weights in naive bayes with
kullback leibler measure  in proceedings of the
     ieee   th international conference on
data mining  washington  dc  usa        
icdm     ieee computer society  pp      
     

    manning  c  d   raghavan  p   and
schutze  h  introduction to information retrieval  cambridge university press  new york 
future work should seek to extend the bag of words
ny  usa       
model  perhaps by implementing redundancy compensated bigrams     sentiment analysis  and rhyme
    mayer  r   neumayer  r   and rauber  a 
and style features     class label reduction by using
rhyme and style features for musical genre classome clustering of artists could also prove useful  or
sification by song lyrics  in in proceedings of the
perhaps necessary  as the sheer number of artists may
 th international conference on music informadrive the error rate far too high for even the most sotion retrieval  ismir           
phisticated of models  
this paper purposefully avoids the question of at      stamatatos  e  a survey of modern authorship
tributing ghost written songs to songwriters  which
attribution methods  j  am  soc  inf  sci  techcould be an interesting challenge  our datasets are
nol         mar                
also automatically generated from a crowd sourced
lyric repository  the error rate of our ground truth is     telecommunications  v  m   and metsis 
v  spam filtering with naive bayes  which naive
unknown  and worthy of study in order to continue
bayes  in third conference on email and antithis line of research 
spam  ceas         

the author would like to thank professor ng and the
cs    course tas for the fascinating introduction
to machine learning over the course of the last few
months 
 

fi