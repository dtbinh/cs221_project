cs     machine learning

stanford university

cs    final project  k means algorithm
colin wei and alfred xue

sunet id  colinwei axue

december         

 

notes

this project was done in conjuction with a similar project that explored k means in cs     
relevant parts of our paper are shared between the two  however  the majority of the two
papers are different  the cs     paper focuses on theoritical research that has been done on
k means  while this project focuses on the differences between k  means and single link   
with a more practical approach 

 

introduction

the k means algorithm is an algorithm used commonly for clustering points in rn   while
this algorithm works quite well in practice  there are two aspects of this algorithm that are
hard to grasp theoretically 
first  it has been hard to prove any meaningful upper bound on the running time of
this algorithm  the worst case running time of k means has been shown to be   n      
although in practice  k means takes a sublinear number of iterations to converge for real
datasets  smoothed analysis has given polynomial time bounds to this problem  but even
these bounds are much higher than what has been observed empirically  we will summarize
some of the smoothed analysis work on k means 
second  it has been hard to quantify the accuracy of the solution that k means converges
to  although k means has guaranteed convergence because each step of the algorithm performs coordinate descent on the k means objective  the algorithm rarely converges to the
exact optimal solution because it mostly gets stuck at local minima  modifications of the kmeans algorithm with different centroid initialization procedures  such as k means    have
improved the accuracy of convergence 
we are interested to see if k means performance correlated with notions of stability we
discussed in class  we were interested in  perturbation stability in particular  unfortunately  the proof of the  c    stability discussed in     does not extend to  pertubation
stability  upon intital testing  we found little indication that gamma stability increased
k means performance  we also noticed that k means seemed to get stuck on local minimum more when the pertubation stability factor increased  to explore this  we implemented
k means and compared its performance to single link   

colin wei and alfred xue  colinwei axue stanford edu

 

fics     machine learning

 

stanford university

definitions

the k means clustering problem is as follows  given a set p of n points p         pn  rd   choose
a set c of k centers c         ck  rd to minimize the objective function
x
c  p    
min   p  c   
pp

cc

the k means algorithm for this problem works as follows 
   randomly initialize cluster centroids c         ck  rd  
   until convergence  repeat 
i  for every point in the data set pi   let wi   arg minj   pi  cj    
pn
  wi   j pi
 
ii  for every cluster centroid cj   set cj   pi  
n
i     wi   j 
the single link   clustering method is implmeneted as follows 
   construct a tree as follows
i  place every point in its own tree 
ii  select the two points not already connected with the smallest distance between them 
iii  connect the heads of the two trees that corespond to these points   that is  we
construct a binary tree 
iv  repeat step ii  and iii  until there is only one tree remaining 
   prune the tree by removing k links starting from the head of the tree  where a link can
only be broken from a head  that maximize the objective function
x
min   p  c   
c  p    
pp

cc

where the center of each cluster is the most optimal point for said cluster 

 

k means vs  single link  

we wanted to test the performance of k means and single link    due to     on data satisfying  perturbation stability to see how the two algorithms would compare  furthermore  we
wanted to see if  perturbation stability was a particularly useful notion for k means at all 
as it seemed like something that applied much better to hierarchical clustering algorithms 
we generated our own data to satisfy  perturbation stability as follows 
   we let k   
    and we placed cluster
centers in r  at  k             k                  k  k     

 k    k    k       k    k    k     

colin wei and alfred xue  colinwei axue stanford edu

 

fics     machine learning

stanford university

   for      total points  we chose one of the   points and inserted a random point in a
sphere of radius one around that center point  these      random points formed our
dataset 
since our data was randomly generated  it might not have satisfied  perturbation stability
perfectly if one of the actual centers was off  but we believed it approximated this well enough
for our purposes  for each of the    values of  from                          we generated   
such data sets and ran single link   and k means on both data sets   even though perturbation stability is not a valid notion for       we still used it as a parameter for
controlling how separated our clusters were  the following chart shows the average value of
the objective function for those data sets 

   
   
   
   
   
   
   
   
   
   

sl   avg 
       
       
        
        
        
        
        
       
       
       

k means avg 
        
      
        
        
        
        
        
        
       
       

sl   std  dev 
        
        
        
        
        
        
       
        
        
        

k means std  dev 
        
        
        
        
        
       
        
        
        
        

we found it really interesting that the performance of single link   dropped so quickly 
we were surprised to see such a dramatic change  the other interesting thing we noticed
about our results was how k means became more and more erratic as perturbation stability
increased  which seemed like a counterintuitive idea at first  the standard deviation of the
k means objective increased significantly from        to         however  this makes

colin wei and alfred xue  colinwei axue stanford edu

 

fics     machine learning

stanford university

sense because as data becomes more and more well separated  local maxima to the k means
objective become worse relative to the optimal solution 
before we ran these tests  we believed that stability notions would have some positive
impact on either k means runtime or approximation ratio  however  our tests showed no
correlation between perturbation stability and the number of iterations k means took and
also negative results for approximation in terms of increasing standard deviation  granted 
our results could have been different had we chosen to implement k means   instead of kmeans  however  it seems like in general  k means is not very tractable even under stability
assumptions  this makes sense  as k means is used a lot for real world data that isnt
well separated 

 

conclusion

k means does well through all data  as far as time efficency goes  k means is far superior  as
even in the worst case of its run times only had    iterations  since our dataset was large 
even if k means was run    different times  k means would still be superior in time efficency 
we notice that for low values of   the mminimum objective value of sl   steadily increases
until it reaches around      this can be explained by the idea that the number of misses
remain constant  but each miss is punished more due to the larger pertubation factor  it
is likely the case that for pertubation factors above      sl   returns the optimal value 
but has yet to be proven to be the case  the larger standard deviations as the pertubation
factor increases for k means could either indicate that it fails more as the pertubation factor
increases  or simply that k means is getting punished more for failed pertubations 
in conclusion  k means seems to be the optimal method in practice  even for data with
large pertubation factors  sl   remains a popular tool  however  for theoritical purposes
and bound exploration 

 

future research

primary future research should focus on divising theory to support our researchw  other
areas include exploring sl   and  c    stability 

 

references

    agarwal  manu  ragesh jaiswal  and arindam pal  k means   under approximation
stability  theory and applications of models of computation     
    arthur  david  bodo manthey  and h  roglin  k means has polynomial smoothed
complexity  foundations of computer science        focs      th annual ieee
symposium on  ieee       
    arthur  david  and sergei vassilvitskii  worst case and smoothed analysis of the icp
algorithm  with an application to the k means method  foundations of computer science        focs      th annual ieee symposium on  ieee       

colin wei and alfred xue  colinwei axue stanford edu

 

fics     machine learning

stanford university

    ragesh jaiswal and nitin garg  analysis of k means   for separable data  in proceedings of the   th international workshop on randomization and computation  pp 
              
    david arthur and sergei vassilvitskii  k means    the advantages of careful seeding  in proceedings of the   th annual acm siam symposium on discrete algorithms
 soda      pp                 
    ostrovsky  rafail  et al  the effectiveness of lloyd type methods for the k means problem  journal of the acm  jacm                  
    andrea vattani  k means requires exponentially many iterations even in the plane  in
proc  of the   th acm symp  on computational geometry  socg   pages         
     
    awasthi  pranjal  avrim blum  and or sheffet  center based clustering under perturbation stability  information processing letters                     
    balcan  maria florina  avrim blum  and anupam gupta  approximate clustering without the approximation  proceedings of the twentieth annual acm siam symposium
on discrete algorithms  society for industrial and applied mathematics       

colin wei and alfred xue  colinwei axue stanford edu

 

fi