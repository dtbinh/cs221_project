cs    fall     

 

a binary classification of beatles song authorship
miles bennett  casey haaland  and atsu kobashi

i  i ntroduction

m

achine learning algorithms have a long history with
the problem of text classification which has been used
extensively for a variety of applications including spam classification as well as the identification of the disputed authorship
of several federalist papers         in our project  we attempt
to use the tools of machine learning to identify the primary
author of a given beatles song using the lyrics as features 
since a large majority of beatles songs were written by either
john lennon or paul mccartney  we restricted the scope of
our project to a simple binary classification problem  though
text classification is a well known and widely used application
of machine learning  the aim of this project is to analyze the
performance of various classification algorithms and quantify
their suitability for this problem 
ii  m ethodology
a  data collection and preprocessing
after converting a pdf of beatles lyrics to plain text format
we first separated the set of lyrics into separate text files for
each song      in the first iteration of testing  we also removed
words we believed were content free  in an attempt to improve
the accuracy of our classifier  since some basic words were
likely to appear in a vast majority of the songs  we felt it was
likely that these words would not be informative features in
determining authorship so we would be justified in removing
them  as such  we defined the following words which would
not be included in the feature space 
   the
   a
   and

   or
   but
   if

in addition to removing these so called stop words  we
also lemmatized the song lyrics using a library provided by the
natural language toolkit  nltk   an open source module for
python      the lemmatization process converts a noun  verb 
or adjective to the canonical form of the word  for example 
the plural noun dogs would be converted to the singular
noun dog and the word went would be converted to go 
by lemmatizing the aggregate list of all words that appear in
the corpus of beatles songs  the size of the vocabulary was
reduced from      to       this is a similar preprocessing
technique that was used in problem set    
lastly  because there are only    songs written by either
lennon or mccartney  our data set was relatively small  for
this reason  in the algorithms to follow  all accuracies are
reported under leave one out cross validation  specifically  we

evaluated our experimental accuracy as 
n
o
  x n   i  
  h x
   y  i 
loocv     
n i  
since    of the songs were by mccartney and    by
lennon  there was very little class imbalance and this seemed
to be an appropriate metric for the classifier 
b  algorithms
there are a variety of well known algorithms in the field of
machine learning for the purpose of binary text classification 
one possibility that our group hypothesized was that songs
written by lennon and songs written by mccartney would
lie close together in a cluster  for this reason we wanted to
explore the use of non parametric clustering algorithms such
as k means or k nearest neighbors as well as investigate the
effect of using different distance metrics  e g     and      on
our classification accuracy 
another well known algorithm considered to be both simple
and effective for text classification problems is naive bayes 
the large volume of literature concerning the use of naive
bayes for textual analysis seems to suggest both the bernoulli
and multinomial event model would be well suited for our
classification problem 
the last two algorithms we seek to investigate in this
project are support vector machines and regularized logistic
regression  svms have many desirable properties such as
large margin  the ability to be kernelized  and a relatively low
tendency to overfit  all of which would allow it to perform well
on this problem  logistic regression is generally accepted as an
algorithm that performs well on binary classification  with the
addition of a regularization term we can obviate the problem
of overfitting that logistic regression tends to suffer from 
iii  r esults
a  initial classification approach
before attempting the algorithms listed above  we first
attempted to see if there was a simple feature space in which
we could visualize and perhaps find structure in the data  one
feature that was often cited as being useful for the purpose
of text classification was the number of unique words per
document  in accordance with this  we created a scatter plot
of the number of unique words vs  song length 
as evidenced by figure    this two dimensional feature
space was very densely populated with little discernible pattern
that would enable us to distinguish lennon from mccartney 
therefore  we found that lack of linear separability or distinct
clustering meant this feature space was not rich enough to
accurately perform the classification  this observation was

fics    fall     

 

fig     the circles denote songs composed by john lennon while the xs
denote ones written by paul mccartney

verified by our initial attempts using k nn which yielded an
accuracy of        
in hopes of better capturing more information that could be
used to discriminate between the two artists  we chose to create
a design matrix x  r       and a classification vector
y  r  

 x    


  
x 

 
 x     



y    


y        


y     

 i 

where xj gives the number of times the j th word in our
vocabulary v appears in the ith beatles song in our training
set where y  i      if mccartney was the artist of song i and
y  i      if lennon was the artist  

b  k means   k nearest neighbors
the k means algorithm is an unsupervised learning algorithm that clusters data based on a chosen distance metric 
this algorithm is ideal in the case that the convex hulls of
the points from each class are mutually disjoint   the knearest neighbors  k nn  algorithm is another non parametric
algorithm in which the class label of a new example is
predicted by a majority vote of the k nearest neighbors  where
nearest was measured by either the    or    norm   we chose
to break ties by using the class label of the closest example to
the test point  it is also worth noting that k nn exploits local
clustering of data and as a result tends to perform well on data
in which classes lie in a number of dense clusters     

  for the case of the svm  the class designations are swtiched so that
y  i      identifies a song composed by lennon rather than  
  the convex hull of a set of points s    x        x   is simply the set
n
 
of all convex combinations of points in the set  i e  conv s     t x 
t
            

fig     graphical depiction of the k nearest neighbors algorithm  the dashed
circle corresponds to the chosen distance metric

initially  we supposed that the data for the beatles songs
written by each author might lie in two distinct clusters of
our feature space  this situation might occur if  for example 
mccartney used one subset of the vocabulary much more
frequently than lennon did  another hypothesis for the structure of our data that we thought was reasonable was that the
classifications might lie close together in a number of smaller
clusters in the feature space  since both of these seemed to be
plausible assumptions about the data  we used the k means and
k nn matlab implementations  iterating over the number of
neighbors and utilizing the   norm and   norm  the results of
which are shown in the table 
algorithm
k means
k nn
k nn
k nn
k nn
k nn
k nn

k
 
 
 
 
 
 
 

accuracy
      
      
      
      
      
      
      

metric
  
  
  
  
  
  
  

as evidenced by the poor performance of k means  it was
clear the data did not lie in two distinct clusters of our chosen
feature space  this result might be explained by lennon and
mccartney having similar writing styles and therefore the
different classes were highly interspersed amongst each other 
the negative results of both algorithms could also be attributed
to the curse of dimensionality  as the feature space is over     
dimensional while the number of test examples was just    
c  naive bayes
naive bayes is a simple generative learning algorithm that
is easily applied to binary classification problems  the key
assumption of the algorithm is the conditional independence
of each of the words given the documents classification  i e 
p xi   xj  y    p xi  y p xj  y   the algorithm then seeks to
learn a model by computing
n
y
max p x  y    max p y 
p xi  y 




i  

where  is the vector of parameters p y       p xi  y       and
p xi  y      which are to be estimated  in our initial attempts

fics    fall     

 

to apply naive bayes we implemented both the bernoulli
and multinomial event models with laplace smoothing  the
results of which are summarized below 
event model
bernoulli
multinomial

accuracy
      
      

as was the case with the previous learning algorithms  naive
bayes only slightly outperformed random guessing  the reasons for the failure of this particular learning algorithm is most
likely due to the small training set as well as the conditional
independence assumption being violated 
d  svms   regularized logistic regression
the last two algorithms that we attempted were support
vector machines and regularized logistic regression  both of
which are supported by the liblinear package      in order
to fully investigate the range of classifiers available  we used
a variety of objectives for the svm algorithm such as    loss
with    penalty 
j       kwk     kk  
as well as    loss with    penalty 
j       kwk     kk 
among other options supported by the software package  for
the logistic regression learning algorithm we also experimented with values of the regularization term for the objective
m



x
jlr     
log     exp y  i  t x i    kk  
i  

h
t it
with   x i   r   where x i    x i      assuming we
have a cost function j for the unregularized algorithm  our
objective for the regularized learning algorithms take the form
j     fp  kxkp  
where kxkp is defined for p    as
kxkp  

n
x

   p
p

 xi  

i  

and fp   r   r  is the mapping
 
z if p    
f  z   
z   if p    
the value   r is the regularization parameter which helps
stabilize the objective j      to choose the value of the regularization parameter  we iterated over a logarithmic spacing
of values from        to      and computed the resulting
accuracies for each value of   the accuracies reported in
table below are for
  arg max loocv


specifically  we selected the value of  that produced the
maximum accuracy on our set of examples under leave one
out cross validation 

algorithm
logistic
svm
svm
svm

loss
log loss
  
  
  

penalty
  
  
  
  

accuracy
      
      
      
      


      
       
     
    

iv  a n a lternative f eature s pace
in light of the poor performance of many of the algorithms
we used to perform the classification  we sought alternative
feature mappings for the lyrical data  in the course of our
research  we discovered term frequency   inverse document
frequency  tf idf   is a standard feature mapping that performs
well in text classification  to perform the feature mapping we
define the following quantities    
nw  x    number of times word w appears in doc x
nw  x 
  term frequency
f  w x    p 
 
w nw  x 


  of docs    
w  x    f  w x  log
  of docs with word w    
the term frequency portion of the mapping gives a weighting corresponding to how often word w appears in document
x while the inverse document frequency portion serves to
deemphasize words which appear in a large fraction of the
documents  the addition of one in both the numerator and
denominator of the idf term acts as a form of smoothing and
prevents division by zero in the case that a word does not
appear     
using this new feature mapping we reran a selection of our
previous algorithms to see if the new feature mapping w  x 
garnered any improvement in classification accuracy and perhaps be worth pursuing  the following table summarizes the
results of implementing the tf idf frequency feature mapping 
algorithm
logistic
svm
k means

loss
log loss
  
 

penalty
  
  
 

accuracy
      
      
       


   
  
 

we speculate that the reasons for the poor performance of
tf idf for this particular application are similar to the reasons
why many of our aforementioned algorithms failed  more
specifically  we were using very few training examples relative
to the size of the feature space we were attempting to learn 
additionally  since each training example consists of the lyrics
to a song  rather than a much larger body of work  say an essay
or a book  the examples will not be as informative as would
be the case for classification of larger documents 
v  r educing the f eature s pace d imensionality
lastly  with most of the algorithms unable to achieve any
significant accuracy  we explored the possibility of reducing
the dimensionality of the data  liang et  al  remarks  lowcontent stop words have been shown to be very useful
statistical cues of sentiment and psychology which led us

fics    fall     

to believe that perhaps we were not justified in removing the
stop words from our vocabulary     
futhermore  fung discusses in his paper the disputed
federalist papers  svm feature selection via concave minimization the use of a smaller set of    function words rather
than a complete vocabulary for improving text classification
     in fact  fung demonstrates the ability to classify   
disputed federalist papers using only a three dimensional
feature space consisting of the words upon   would  and
to 
in order to choose the subset of say     words  that provide
the best accuracy
to use
space  we would need
 as our feature

       
  

 

  
different
combinations
to try     

  
  
of words  a clearly intractable problem already  let alone for
a vocabulary of size     in light of this  we decided that
implementing a simple forward search would be the best way
to generate of low dimensional representation of our data 
starting with an empty vocabulary  we built up a lexicon
by iterating over the inclusion of every additional word that
was not already in the set and subsequently included the word
that achieved the highest value of loocv   the number of
words we ultimately included in our vocabulary was selected
as the number of words after which the addition of several new
features no longer improved training accuracy  figure   shows
how the training accuracy increase as a function of the vocabulary size under forward search  for the svm algorithms  the
first item in the legend denotes the regularization norm and
the second specifies the loss function for the objective 

 

figure   that are worth drawing attention to  first  the   
regularized    loss svm learning algorithm  shown in light
blue  learned far better with a small feature space than any
of the others  initially gaining nearly     accuracy for each
word added to the vocabulary  after the first word   however 
it ended up performing the worst by the completion of the
forward search  with the learning curve tapering off to     
after only   words  on the other hand  the    regularized   
loss svm  which ended up performing the best  experienced
one of the slower learning rates  coming in with consistently
lesser or equal accuracies to the other three learning objectives 
in addition to examining the accuracy vs  number of
features curve  we also looked at the vocabularies generated 
the words obtained from the forward search using the   
regularized     penalty svm is displayed below
  
  
  
  
  
  

its
would
can
alone
stop
do

  
  
  
   
   
   

bag
everybodys
fill
float
mr
after

   
   
   
   
   
   

answer
remember
sky
about
above
accidents

though using forward search with the other three objectives
did result in different vocabularies for each algorithm  the first
several words generally tended to be the same across all the
algorithms indicating that there are certain words which are
particularly important in distinguishing the two composers 
seeing that the forward search provided such dramatic improvements in classification accuracy for the svm and logistic
regression based algorithms  we also implemented a forward
search using naive bayes   as we had come to expect from
our research as well as our success using svms  the naive
bayes approach saw a remarkable increase in classification
accuracy when used in conjunction with feature selection 
while the naive bayes algorithm using the original     
dimensional feature space only achieved loocv          
using feature selection to build up a    word vocabulary we
were able to increase this number to nearly     

fig     training accuracy as a function of vocabulary size for various
optimization objectives  all algorithms were trained using regularization
parameter c    

it can clearly be seen from the graph above that using this
greedy approach to choose a subset of our feature space had
a profound effect on classification accuracy  as can be seen
from the plot  the accuracy grows quickly as new words are
added to the vocabulary  at least    with each added word
across all algorithms  and then almost all of the algorithms
training accuracies begin to plateau after    words  with the
   regularized    penalty svm performing the best  attaining
a classification accuracy of       
before moving on  there are some interesting patterns in

fig     the learning curve for naive bayes plateaus after a    dimensional
feature set is achieved  attaining an accuracy of     at saturation
  multinomial

event model

fics    fall     

 

having markedly improved our accuracy for svm  logistic
regression  and naive bayes we revisited the tf idf feature
mapping to see if it similarly improved with the use of feature
selection  interestingly  retraining on all the algorithms  not
including naive bayes  achieved the accuracy profile shown
below 

potential application of this classifier is the identification of
the principal author of songs written by multiple individuals
and for songs with disputed authorship  a further extension
to the project would be to include audio data in the feature
space or to examine if audio data alone  without lyrics  could
be used to determine the author of a song 
r eferences
    mehran sahami  susan damais  david heckerman  eric horvitz  a
bayesian approach to filtering junk e mail       dec   
    glen
fung 
the
disputed
federalist
papers 
svm
feature
selection
via
concave
minimizationavailable
at
http   pages cs wisc edu  gfung federalist pdf
     dec   
    fred  the beatles  complete lyrics of all songs available at
http   www gratuit cours com      dec   
    bird  steven  edward loper and ewan klein         natural language
processing with python  oreilly media inc       dec   
    john cast  chris schulze  ali fauci music genre classification 
available at http   cs    stanford edu proj          
dec   

fig     shows how the tf idf feature mapping affects training accuracy 
interestingly  the curve was identical for the three svm algorithms and
regularized logistic regression

    r  e  fan  k  w  chang  c  j  hsieh  x  r  wang  and c  j  lin 
liblinear  a library for large linear classification  journal
of machine learning research                     available at
http   www csie ntu edu tw  cjlin liblinear

overall the tf idf feature mapping exhibited a similar increase in accuracy from the forward search but overall was
not noticeably better than our initial feature mapping consisting of raw word counts  the general shape of the learning
curve shows the same general trend as the svm and logistic
regression accuracies  flattening out after a little over a dozen
words 

    tommi jaakkola  course materials for       machine learning  fall
      mit opencourseware  massachusetts institute of technology 
available at http   ocw mit edu       dec   

vi  c onclusions
the results of our project indicate that it is indeed possible
to distinguish between songs composed by paul mccartney
and songs by john lennon based only on songs lyrics  from
our results we have concluded that support vector machines
yield the best performance for this binary classification problem  more important than the particular algorithm however 
is choice of features  through the course of this project we
discovered that many of these algorithms will only perform
well provided you have chosen a good feature space in which
to represent the data 
additionally  much of the time spent preprocessing the data
 i e  lemmatization and removing stop word  provided no
noticeable performance increase as compared with selecting
the proper features  the results of our project seem to suggest
that much of the intuition that underlies the removal of socalled stop words is not applicable in this setting as many of
the words we suspected of being content free were in fact the
highest precedence features with regard to the forward search
feature selection 
regarding future work  our binary classifier could in principle be expanded to a multi class classifier  this multiclassifier could include all of the members of the beatles  paul
mccartney  john lennon  george harrison and ringo star  a

    george forman         feature selection for text classification
available at http   www hpl hp com techreports     
    dawen liang  haijie gu  and brendan oconnor       music genre
classification with the million song dataset      dec   

fi