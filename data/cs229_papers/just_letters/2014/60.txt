cs    fall       final project report by  xiao cai and ya wang
sentiment analysis on movie reviews
introduction
sentiment analysis  the process defined as aims to determine the attitude of a speaker or a writer with respect to
some topic in wikipedia  has recently become an active research topic  partly due to its potential use in a wide
spectrum of applications ranging from american idol popularity analysis to product user satisfaction analysis  with
the rapid growth of online information  a large amount of data is at our fingertips for this kind of analysis  however 
the sheer volume of information was a daunting challenge itself  to separate relevant information from the irrelevant 
and to gain knowledge from this unprecedented deluge of data  automatic algorithm is essential  in this project  we
explored the use of various supervised machine learning algorithms in learning sentiment classifier and tested the
effectiveness of different feature selection algorithms in improving those classifiers 
input data
the input data for this project originated from the rotten tomatoes dataset  a corpus of movie reviews originally
collected by pang and lee     for sentiment analysis  according to the information on kaggle com where we
downloaded the data  socher et al      used amazon s mechanical turk to create fine grained labels for all parsed
phrases in the corpus during their work on sentiment treebanks 
one pre labeled training dataset and one unlabeled testing dataset are included  the sentiment labels use integers
from   to    with   being the most negative and   the most positive  the training dataset contains         data
instances and the testing dataset         each data instance comprises attributes  phrase id  sentence id 
phrase  the phrase attribute contains the actual content of the represented text phrase  which were transformed
into numeric vectors usable by machine learning algorithm using the feature extraction process discussed below 
feature extraction
for text analysis  one standard feature extraction approach is to represent text phrases as n grams  i e  subsequences
of n words with or without skips  to account for the fact that a word preceded by a negative word such as barely
has opposite sentiments  we experimented with both unigram and bigram features in this project  the feature vectors
thus produced span a very high dimensional feature space and hence is expected to be very sparse  the special nature
may cause deterministic machine learning algorithms to overfit  this issue will be revisited later in the feature
selection section 
methodology
nave bayes algorithm  softmax  logistic  regression and support vector machine are all known to be suitable for
text classification but varies in its assumption and formulation of the problem  all were explored in this project 
nave bayes classifier  nbc 
as a generative learning algorithm  the nave bayes algorithm models class prior p y  and conditional feature prior
p x y  from training samples  nb classifiers learned in this project is based on the multivariate multinomial event
assumption 
assumption 
the multivariate multinomial event assumes that  given a sentiment of a phrase or data instance   word in one
position in the phrase tells us nothing about words in other positions  word appearance does not depend on position 
formulas for prior estimates 


the p x y  for a feature k is modeled as 
of all features appeared in a data instance  
th

  

  



 



    

  

 

 

  here 

denoting the total frequency

  the word or more precisely the index of the word at position j of

data instance   k  index for the k word in the vocabulary dictionary 
m  number of data instances        class label for instance  

   v 

  vocabulary and its size 

fics    fall       final project report by  xiao cai and ya wang
the laplace smoothing is applied to guarantee that the basic probability axioms  probability being nonnegative and
sum to    will still hold in the case when a data instance contains an n gram not found in the dictionary and the
estimate without smoothing will otherwise result zero for all sentiment classes  the estimate without the laplace
smoothing is basically the fraction of times a word k appears across all data instances of label c 
    



the prior p y  can be modeled as the fraction of data instances with sentiment label c 

 

 

with the priors thus modeled  the bayes rule is used to derive the posterior distribution on y given x and the class
with highest posterior probability is picked as the prediction of the sentiment class  give a data instance vectorized
as      
   the mathematical formulation of the predication is as follows 
 
 
 
   
 
 

 
 
 
 
 
   
 
softmax  by stochastic gradient descent 
softmax algorithm is another algorithm other than nave bayes  which we implemented using matlab scripting
language for this project  unlike naive bayes  it belongs to the deterministic algorithm family and aims to learn
mappings  using a set of parameters such as below  directly from the input space x to labels y 
the model assumes

 

 

and



 

  

   

 





  

 

 



 

 

  



is its estimate 

the softmax problem of this project is formulated as 
  



   



  



  

the partial gradient of     with respect to
   

with
  

   

  

 

  

    

  

 

 

  

  

 

 

 

  

 

    

 

can be derived as  due to page limit  no detailed steps are included  

  

being the parameter for class c and word k 
  





  

  the frequency of word k in data instance  

   vector representing how frequent each word in v appeared in data instance  

considering that the whole data instances from big data often cant be fit in the memory  stochastic gradient descent
algorithm is implemented to solve the parameters for softmax problem 
initialize to vector of zeros for each label class
 
 
loop through each data instance i   for i   to m   
  
          for each feature k and each label class c 
      
 
  
support vector machine
support vector machine is another machine algorithm fit for text classification problem for reasons elaborated by
joachims      the implementation of svm in this project leveraged a third part library  liblinear      the problem is
formulated using the standard linear svm formulation with l  regularization 
 
s t 

 


 

 

fics    fall       final project report by  xiao cai and ya wang

feature selection
realizing most words in a phrase do not convey sentiments  two feature ranking algorithms are implemented to
automatically identify the corresponding uninformative features 
mutual information  mi  rank
mutual information  mi  index is the feature ranking method related to naive bayes algorithm  high mi indicates
that a feature xi is very informative about y  in contrast  a very low mi indicates non informative features 
 
 
 
    
 
       
mi calculated in two different ways are used in this project  one uses the priors modeled in nbc to calculate mi k y 
for each word k as 
 
 

 
 
 
 
 
 
 
 
here   
    
is the probability of word k appears in phrase i at position j 
 
which based on nb assumption is the same for all data instance i and word position j 
the other is to use the formula given in the chapter    of mannings book      the same formula as given above can
be used by changing the definition of conditional prior of word k into the following 


  

 


    

 

 the p x y  modeled when nb used multivariate bernoulli event assumption  

 

f score rank
the f score is the correlation coefficient between one of the features and the label  it is very similar to f test statistics
that measures the difference between variance between different population groups and among each population group 
intuitively  it is a good measure of how discriminative a feature is regarding sentiment classes  the formula of fscore used in this project is a variant of the one used in chang      which instead is for two classes 
   

   




    

 



   

  

  
  



   

 

    

  

  with 

   



  

    

    

 

 

and 



  

 

results  discussion and conclusion
the models trained using algorithms discussed above are evaluated on both the training data and the test data set  the
prediction for the test data set is scored online by kaggle com  in the following  some interesting results are presented
and interpreted 
table    comparison of prediction accuracy
unigram features
nbc

bigram features

training

      

softmax
      

test

      

      

svm

nbc

softmax

svm

      

      

      

      

      

      

      

      

table   compares the use of unigram vs bigram and the use of three aforementioned algorithms  on the training data 
softmax has the best performance and achieves almost perfect predicting accuracy whether using unigram or bigram
features  svm achieved almost perfect prediction accuracy when using bigram  however  nbc consistently beat the
other two algorithms on the test data in terms of prediction accuracy  the phenomena can be explained by vaponic

fics    fall       final project report by  xiao cai and ya wang
chervonenkis  vc  dimension analysis  the hypothesis function for softmax use  v   c    v   number features   c  
number of classes  number of parameters  while the hypothesis for svm uses  v    c     number of parameters  since
each set of parameters that defines a hyper plane to separate one class from the rest uses has  v  dimension  more
parameters for linear or generalized linear models like softmax  the decision boundary for softmax is essential
linear   and svm translate into a hypothesis space with higher vc dimension  which itself translates into higher upper
bound on the discrepancy between the hypothesis error and the true error  or overfitting on testing data  this is one
cause which contributes to better performance of softmax over svm on training data and overfitting of both on test
data  on the other hand  the higher dimensional space resulted from bigram renders features vectors sparser and
hence higher chances to be separable  which contributed to the improved prediction accuracy of svm on the training
data in the case with bigram in comparison to the unigram case  nave bayes  on the other hand  is a generative
model  which is not as susceptible to the curse of vc dimension or overfitting  instead  according to central limit
theorem  the empirical probability based nbc tends to become quite accurate with a large number of data instances 
however  the more rigorous assumption of nbc also makes it less accurate in modeling the training data 
figure    prediction accuracy of nbc and svm vs  number of features
   

top    
top     
top     
top      
all features

   
   
   
   
nbc unigram
model training

nbc unigram
model testing

nbc bigram
model training

nbc bigram
model testing

svm  unigram  svm  unigram 
training
testing

figure   illustrates the effectiveness of feature selection by mi index for nbc and by f score for svm  it shows that
the top      features turn out to do an almost as good job as        features  in addition  the prediction accuracy
doesnt improve proportionally with the increase in features included in the model  the larger the number of features
already included the smaller the improvement in accuracy by adding more features  on the test data  the model
sometimes performs better after irrelevant features had already been tossed out but still having enough relevant
features kept 
table    most informative vs least informative by feature rank

nbc features ranked by mi
  most informative
  most informative
escapades
 infantilized 
 adage 
 unhappiness 
 goose 
 glosses 
 gander 
 hunky 
 amuses 
 relish 

svm features ranked by f score
  most informative
  least informative
flopped
henry
repugnant
tearing
execrable
pedigree
under inspired
harrison
not at all good
buddy

note    the mi shown here is calculated using the formula given in the chapter    of mannings book     

top   svm tokens in order based on f rank turned out to be quite interesting  the top informative token flopped
unambiguously conveys negative feeling  while the top least informative token henry is a name  names usually
dont convey any feeling unless it happens to be something like voldemort  on the contrary  the result of feature
ranked by mi doesnt seem to be as intuitive  after some investigation  we found out that the high mi rank for
sentiment neutral words like goose and adage is due to the fact that these words seem to concentrate in the
phrases with class label    hence  mi algorithm identified those as very informative about class label    which
happens to represent neutral sentiment and also constitutes the largest proportion of the document  this inspired the
thought that maybe a modified mi ranking which identifies features most informative about the four more extreme
sentiments could have served a better job  due to time limitation  the idea is not pursued further in this project 

fics    fall       final project report by  xiao cai and ya wang

norm theta n  theta n   

figure    convergence of softmax parameters with passes of data with learning rate     

the convergence of theta for softmax
     

     

     
     
     
     

    

    

    

    

 

 

 

    
 

 

 

 

 

passes

figure   shows the convergence rate of softmax parameters with the number of passes through the training data using
    as the learning rate and with        top features selected using f score  though the algorithm converges quickly 
due to the large number of data instances  each pass takes a long time to finish  in this aspect  the softmax is the least
efficient among the three algorithms tested 
the feature selection using f score turns out to work well for softmax algorithm  the model with the top       
unigram features achieves a        accuracy rate on the test data which is better than the accuracy rate       
shown in table   for softmax model using all features  the model with the top      features resulted     accuracy
rate on the test data  which is almost the same as the model using all features 
in conclusion  through this project  we learned that the deterministic algorithms like softmax and svm is liable to
over fitting in a high dimensional feature space  on the other hand  with its relative rigorous assumption of
conditional independence of features  nave bayes algorithm doesnt perform too well on the training data but also
has less problem of over fitting  furthermore  feature selection methods can improve performance and the top
features identified by different feature selection methods could differ 
future work 
in the future  other feature selection methods or hybrid of feature selection methods will be explored  for instance
combining the top features from the mi ranking with those from f score ranking  in addition  more sophisticated text
analysis algorithms  such as maxem or deep learning neural network or others will be implemented 
reference 
    pang and l  lee        seeing stars  exploiting class relationships for sentiment categorization with respect to
rating scales  in acl  pages        
    recursive deep models for semantic compositionality over a sentiment treebank  richard socher  alex
perelygin  jean wu  jason chuang  chris manning  andrew ng and chris potts  conference on empirical methods in
natural language processing  emnlp       
    thorsten joachims  text categorization with support vector machines  learning with many relevant features 
http   www cs cornell edu people tj publications joachims   a pdf
    http   www csie ntu edu tw  cjlin papers libsvm pdf
    yin wen chang and chih jen lin  feature ranking using linear svm
http   www csie ntu edu tw  cjlin papers causality pdf
    andrew ng  stanford cs    course notes  http   cs    stanford edu materials html
    christopher d  manning  prabhakar raghavan  and hinrich schtze       introduction to information retrieval 
cambridge university press 

fi