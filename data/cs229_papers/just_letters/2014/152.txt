predicting lecture video complexity  analysis of supervised regression
nick su
ismael menjivar
njsu stanford edu
menjivar stanford edu
december        

abstract
in the past decade  use of massively open online courses  moocs  has rapidly risen  providing millions of
students access to higher education  moocs have also changed the education paradigm  with most courses
involving a combination of short video lectures with moderated online discussion  this project evaluates
performance of different supervised regression algorithms and features to predict lecture video complexity  we
find that there is a weak relationship between our predictors and labels  but much promise for future work with
more advanced natural language processing  nlp  techniques  we hope the results of this work will help
mooc instructors tailor their teaching style to virtual audiences 

introduction
the analysis performed by this project is based on data from stanfords openedx online learning courses 
these courses consist of many short videos each around    to   minutes in duration  while the courses cover
a broad range of topics  many of them share the same video structure  an instructor visible on the screen speaks
in the foreground or corner of the screen while a slideshow is shown in the majority of the screen space 
instructors will often move and handwrite text onto the screen  but the structure is otherwise very uniform 
our goal was to design a model that provides information to an instructor on where students are
having difficulty following a lecture by examining the progression of text visible on the screen and the
progression of closed caption text up until that point  this function is constructed using a supervised learning
algorithm informed by a dataset of user interactions with the video during different time intervals 

data
our project used three data sets that provide analysis on forty     to    minute course videos for cs     intro
to computer networks 
   a dataset from the vice provost office of online learning contained almost six hundred thousand
user interactions with the videos when they were offered on openedx 
   data of text on screen was extracted from frames of each video screen using matlabs optical
character recognition  ocr  tool 
   closed caption text files were extracted from each video online using keepsubs 
video interactions data
the dataset for user interactions with the videos consisted of a single  csv file with the following information
for each of the         interactions 
o
o
o
o
o
o

event type  the type of interaction  this could be pausing the video  playing the video or seeking to a
new section of the video 
video current time  the current playhead time  tells us when a user paused or played a video 
video new time  for video seek  new playhead time 
video old time  for video seek  old playhead time 
video code  machine code name for video  also video code for youtube 
o youtube com watch v video code
anon screen name  unique id for a viewer

ocr on screen text data
screen text was manually extracted using optical character recognition at three second intervals in the video 
data is stored in a txt file with lines labeling the video time  in seconds  that ocr was performed followed by
a line of all the text identified by matlabs built in ocr function  while it recognizes a majority of text on
a given frame  it contains some inconsistencies correctly identifying human writing and squiggles 

ficlosed caption text data
closed captions are done manually by openedx and are consequently very clean and error free  each caption
has the following associated data 
   caption number
   time duration that it appears on screen
   text displayed

label extraction
labels were extracted from the video interaction data by analyzing the pause and seek event types  we
judged these interactions to be most indicative of where a student may be struggling to keep up with the lecture 
for each video  we kept a count of how many interactions occurred at each twenty second interval 
each pause event at current time t increments the count of events during the twenty second interval in which it
took place  for seek type interactions  we first check if a student rewinds the clip  new time   old time  and
increase the count of interactions for each twenty second interval within the rewound region 
to normalize the counts  we divide them by the number of users that interacted with the given video
to get an estimate of how many times each segment was watched by the average user  these can then be used
as a labeling to mark which sections students are pausing and viewing the most  and therefore most likely to be
struggling to follow the lecture 

feature extraction
extracting meaningful features proved to be difficult due to the nature of the problem  unlike typical nlp
features that typically operate on bounded datasets and perform classification  obtaining nlp features that
reflect time dependence and are suitable for regression is more difficult  in this project  we present and
evaluate a set of parameterized base features and evaluate feature strength  parameter sweeps  and alternative
feature sets 
our base feature set contains    features  consisting of the frame time  a   term set of features drawn from
closed captions  and a   term set of features drawn from the ocr dataset  aside from the frame time  the other
features are described as follows 
o closed caption features examine speech rate and word occurrence and are parameterized on a word
depth w  the   features are listed as follows
o time depth td is the time taken by the lecturer to speak the last w words 
o cumulative term frequency ctf  looks at each word in the last w and examines how often
it has occurred prior to time t  the last w words are analyzed  and the   th   th   th
percentile and gini index are produced as the four features 
o term frequency tf  generates the same four terms but instead looks at the occurrence of a
word throughout the entire duration of the video  the same four features are produced
o ocr features examine text changes from frame to frame and are parameterized on frame depth f 
o average frame change measures the number of new words per frame and generates two
averages as features  one over the last f frames  and another over all previous frames 
o average frame text measures the average number of words per frame over the last f as well
as all previous frames
in addition to evaluating algorithm performance on the base feature set  three additional tests are made 
   parameter sweep  word and frame depth are swept over a suitable region to determine the optimal
depth the feature set should be generated over 
   expanded feature set  an expanded feature set is generated that simply creates n and f size vectors
of individual word and frame frequencies  this set is run on pls and sv regression in the hopes of
identifying dependencies between variables not easily quantified in a small feature set 
   tf idf  is compared to tf to examine whether inter data information can improve performance 

algorithms
linear regression

filinear regression characterized different features  feature parameters  and feature strength  providing us with
varied correlations within our dataset  this model served as our basis for comparison across the various
configurations of cross validation and feature selection 
partial least squares regression  plsr 
plsr was used to specifically evaluate the size of our feature set  measuring our original    term feature
vectors against reduced versions and our highly expanded feature vectors  this model was used for reducing
feature size of our base feature set and examining whether an expanded set could yield better prediction results 
vapniks support vector regression  svr 
svr allowed us to see how our other regression algorithms performed against highly optimized solutions  if
svr was able to correctly predict our labels to certain margin  we would hope to replicate the performance in
our optimization of variables using other algorithms 
binary decision tree  bdt 
we did not have high hopes for bdt  because our    dimensional feature set would naturally lead to over
fitting the limited amount of examples we had  but we wanted to confirm our intuitions about the performance
of these models 

figure    learning model implementation

figure   depicts a high level block diagram the learning implementation used in this project  tests of
parameter and feature strength were based around linear regression  while plsr was used to evaluate larger
feature sets 

results and discussion
algorithm performance

figure     top left  convergence of test and training error as number of training videos is increased from   to the    of
   videos   top right  scatterplot of observed vs expected values for a training set of    videos and a test set of   for lin 
reg  and svr   bottom  table of convergence for training and test error of binary decision tree  linear reg   partial
least squares  and support vector reg 

figure   displays a set of summary charts and tables comparing the performance of the four learning models
used  test and training error on the left side figure were obtained using hold out cross validation on varying
numbers of videos  noise is observed on small hold out sets  which we have attributed to high levels of feature
and label variance between videos  and both plsr and linear regression have test error below training error as
a result  both linear models converge  while svr and bdt do not reach convergence 

fias seen in the scatterplots on the left  a large amount of variance is observed with both lin reg  and svr  and a
correlation coefficient of      is extracted from the linear regression model 
feature evaluation

figure     left  training mse as a function of word depth  top  and frame depth  bottom    top right  stem plot of
increase of training mse with individual features removed   bottom right  simplified table of training mse increase
with removal of individual feature sets 

figure   presents another set of figures and tables evaluating the strength of different features and parameters
in the    term base vector that we used  the two plots on the left depict the changes in mse based on different
feature vector parameterizations  as shown on the plots  improvements in mse level out beyond     words
and    frames 
the stem plot on the right depicts the impact of removing individual features on changes in training mse  the
three strongest individual features are the word time        th percentile cumulative term frequency  and
median term frequency  a similar analysis was conducted by removing different sets of features  and the
closed caption features are observed to be the dominant contributor to algorithm performance 
varying feature sets

figure    comparison of    component pls regression using original    term feature vector blue  and expanded    
term feature vector  red   predicted vs  observed values seen on left scatterplots  comparison of relative strength and test
mse shown on the right 

figure   presents a comparison of pls regression performance on the original    term vector and an expanded
vector of     terms  as seen on the left three graphs  performance is roughly the same for the two algorithms 
however  test error is observed to be significantly higher for the larger feature vector  suggesting that interfeature noise dominated the pls algorithms attempt to discern shared information between features 

table    training and test mse using    fold cross validation comparing tf and tf idf feature performance 

table   depicts a separate analysis comparing the base feature vector to a modified vector replacing term
frequency with term frequency inverse document frequency  tf idf   tf idf factors in occurrence of words
across different videos  and    fold cross validation confirms that including this information reduces both test
and training error  suggesting that including more features that represent shared information can improve
performance 

ficonclusion   future work
our results showed that linear regression and svr find correlations in the data  but are afflicted by the amount
of noise in our features and labeling  there was also not enough data for binary tree regression to capture the
    decision paths generated by our feature set 
noise issues in data are generally difficult to remove with linear regression algorithms  reducing
noise from our feature set would call for improving the classification of on screen activity  relying on ocr
simplifies the activity  by failing to distinguish between various cases such as a screen of just text and a screen
where the speaker is writing on the board  improving our labeling would call for better classification for when
students are struggling in a lecture  a spike in user interactions might signal issues with the video instead of
lecture complexity  accounting for outliers in both the feature set and labeling would also improve any future
algorithms 
our parameter sweep of frame depth and word depth also provide useful information for future work 
the results show that looking beyond the last    seconds of video displayed or the last     spoken words the
performance of our algorithms levels off  future models could utilize these optimized parameter settings in
their implementations 
regarding parameter selection  the plsr comparison of the base and expanded feature sets hint that a
better feature set could be constructed from reducing noise found in tf ctf tfidf  a moving average tfidf feature generator should be evaluated in future studies as a potential vector set  as should more advanced
nlp driven features 
numerous smaller machine learning projects could be conducted to reduce noise in the final
regression model  particularly  classification algorithms to classify videos by viewership trends would help
reduce variance between videos  the ocr extraction also contained a lot of noise  and a separate study could
be done to classify the importance of the visuals being displayed in the video 

references
    vapnik  v   golowich s   smola a     support vector method for function approximation regression
estimation and signal processing      adv  in neuralinform  proces  syst                    
    b baharudin l h lee k khan  a review of machine learning algorithms for text documents
classification  journal of advances in information technology                
    foster  d  et al  featurizing text  converting text into predictors for regression analysis wharton
school of the university of pennsylvania

fi