indoor positioning system using wifi
fingerprint
dan li  le wang  shiqi wu
stanford university

abstract
indoor positioning system aims at locating objects inside buildings wirelessly  and have huge benefit for indoor location aware
mobile application  to explore this immature system design  we choose ujindoorloc database as our data set  use pca for
feature selection  and build prediction models based on decision tree  gradient boosting  knn and svm  respectively  our
experiment results indicate that combination of knn and gradient boosting provides high prediction accuracy for indoor
positioning  knn shows good performance for large volume of data set with sample size greater       and gradient boosting
has small cross validation error for small data volume and is robust to missing data 

i 

introduction

indoor positioning system  ips  aims at wirelessly
locating objects or people inside buildings based on
magnetic sensor network  or other source of data 
the major consumer benefit of indoor positioning is
the expansion of location aware mobile computing
indoors  such as augmented reality  store navigation 
etc  as mobile devices become ubiquitous  contextual
awareness for applications has become a priority for
developers  most applications currently rely on gps 
however  and function poorly indoors  up till now 
there is no de facto standards for ips system design
     due to the proliferation of both wireless local area
networks  wlans  and mobile devices  wifi based
ips has become a practical and valid approach for ips
       that does not require extra facility cost  however 
wifi based position system as  wps  accuracy depends
on the number of positions that have been entered into
the database  the possible signal fluctuations that may
occur can increase errors and inaccuracies in the path
of the user 
mike y  chen  timothy sohn  et al have explored
the influence of data size and prediction algorithm
on location predicting accuracy  and has proposed
that with centroid algorithm  a limited size of data
set can provide provide a highly reliable result    
sunkywu woo  seongsu jeong  et al have chosen fingerprint methods for wifi positioning system     by
adapting comparison algorithm and using rfid de 

vice as receiver  they achieved locating accuracy of
within  m  william ching  rue jing teh  et al have
conducted similar result using t mobile g   phone 
and suggested that the predicting accuracy would be
improved with the user contribution  in other words 
by constantly increasing the data size     joaquin
torres sospedra  raul mntoliu  et al  have proposed
ujindoorloc database for a common public database
for wlan fingerprint based indoor localization    
inspired by previous work  we plan to use fingerprint
of web access points waps  as features to predict the
position of mobile device holder  the fingerprint of
wap we use is the received signal strength indicator rssi   in this project  we locate the floor level of
a mobile device using wifi fingerprint via machine
learning methods  and explore the data size  feature
dimension  model combination and parameter selection to maintain  if not improve  prediction accuracy 
for different test environment 

ii 
i 

methods

data preprocessing

we use ujindoorloc database     for this project  it
consists of     rssi fingerprints detected using    different phone models by    users from   to   different
floors of   buildings  as shown in figure    for each
building  this dataset gives us thousands of rssi samples generated at various locations inside the building 
 

fithus  with given roadmap of fingerprints as training
set  we could generate a model using machine learning
techniques  with which we would be able to predict
the floor number of unknown mobile device holder in
a certain building 
figure    feature size vs  error for knn
i  
figure    source     left  map of uji riu sec campus
and tx buildings  middle  red indicates estce   tx
building  right  example of a reference point 

i  

dimensionality reduction

however  the high dimensional feature space with
redundant features would hurt computational efficiency  therefore  we used principal component analysis pca  to extract principal features  the energy
levels of the first     principal components are shown
in figure     we found that the top three principal
components contain most of the energy  and for the
components beyond the first      each of their energy
levels is less than one  as we can see from figure   
there is a tradeoff between prediction accuracy and
number of features required  which indicates the computation complexity  for the learning task  depending
on the characteristics of each learning algorithm  we
choose       and     top features for the suitable algorithms to compare and explore for the best prediction
accuracy 

the original dataset has more than      samples for
each building  which would require a lot of efforts for
data collection before building a model for another
building in real life scenario  therefore  we randomly
select a subset of the original sample space to explore
the effects of data size to the accuracy of the model 
data sizes we explored here in various experiments
consists of                           and      samples 

ii 

model selection

we implement four classification methods in machine
learning  including k nearest neighbor  knn   decision tree  gradient boosting and support vector machine  svm   all the four methods are applied with
   fold cross validation to avoid overfitting 
ii  

knn

knn seems to be a good candidate for classification of
this sort  it is due to the fact that knn tries to make
the classification by calculating the distance between
features  while the intensity of various rssi signals depends on the physical distance between wifi source and
mobile phones  in this case  closeness in feature space
is a good indication of closeness in physical space 
ii  

figure    energy levels of components in pca

sample size reduction

svm

we apply multi class svm to determine the decision
boundary between classes  however  the results are
worse than decision tree or knn  a potential reason
of svm failure is because of irrelevant variables with
high dimensional dataset  high prediction accuracy
can hardly be achieved even we reduce the feature dimension from     to      to solve this problem  we
further explore the effort of dimension reduction using
pca 
 

fiii  

decision tree

decision tree is then implemented  which has the advantages of fast training process  easy interpretation
and resistance to many irrelevant variables  but decision tree has the disadvantage of inaccuracy compared
with knn  even cross validation is used  in decision
tree  two criteria are applied to prune the tree  one is
cross validation and the other is one stand error  surrogate splits are used in construction of the optimal tree
as a missing value strategy  which encourages variables
within highly correlated sets 
ii  

gradient boosting

to maintain most advantages of trees while dramatically improve accuracy  bagging algorithm could be
a good choice  here gradient boosting is chosen to
improve the accuracy  also  to handle missing data  we
use surrogates to distribute instances  best number of
iterations  number of trees  are identified using cross
validation and the depth for each simple tree is set to
be four  this parameter could be further studied get a
better accuracy 

iii 

results   discussion

figure    errors vs k for knn methods using euclidean
distance

ii 

svm

svm does not perform well for this problem  here we
explore both linear kernel and third order polynomial
kernel  and decided to use polynomial kernel for better
accuracy  from figure    we can see the descending
trends of svm error with increasing of sample size 
and data with     principal features perform better
than data with    principal features 

we tested our models using data from three buildings
separately  as mentioned before  at each building  we
performed a pca on the feature space to reduce its
dimension  and randomly selected samples to perform
a ten fold cross validation on the four classification
models  both of the number of reduce dimension and
the number of samples in the sample space are tunable for each model to achieve the least error  each set
of parameters was performed for five rounds and the
error is averaged among those rounds to reduce noise 

i 

knn

we first explore the number of nearest neighbors we
need to consider for classifying a testing set  as shown
in figure    the error of classification increases with
increasing of k  then we look into the influence from
the number of samples and the number of top principal features in knn  k    classification  the error
reduces dramatically with increase of sample size  but
not much improvement is seen from adding more principal features 

figure    svm error vs  sample size  dashed line denotes result using    features  solid line denotes result
using     features
we did not test the experiment with      samples
because svm tends to perform better for small dataset
given enough margin data  therefore  experiment in
large feature and sample space will increase the difficulty of convergence for the optimization problem 
 

fiiii 

decision tree

we implement the decision tree in r  first of all a
full tree is grown with complexity parameter to be
   the we utilize two criteria to prune the tree  the
first criterion to find an optimal complexity parameter
is to choose the cp with minimum cross validation
error  one standard error rule is used as the alternative
criterion  note from figure    the total number of
splits to minimize cv error is     and the total splits
according to the one standard error rule is twenty 
cross validation criterion has the advantage of smallest
expected prediction error resulting from the smallest
bias  while it shows the disadvantage of more complex
tree structure and relatively higher instability  or higher
variance  for one standard error criterion  we obtain
a simpler tree structure with relatively low variance 
and it is proved to be able to screen out noise in the
data  the disadvantage of one standard error is larger
bias since it has less split than cv error minimized tree 

figure    statistics of decision tree
surrogate splits are used when the predictor used
to determine the split is missing  when considering a
predictor for a split  we use only the observations for
which the predictor is not missing  then we form a list
of surrogate predictors and split points  when sending
observations down the tree either in the training phase
or during prediction  we use the surrogate splits in
order  if the primary splitting predictor is missing 

iv 

figure    prune the tree with cross validation

figure    prune the tree with one standard error

gradient boosting

a gradient boosting  gb  model is fitted based on the
training data  with cross validation  optimal number
of iteration is determined to be      figure   shows the
misclassification error risk versus number of iterations 
the misclassification error is calculated to be      for
test set  figure    shows the relative importance of each
variables and figure    shows the partial dependency
of the most important variable v   from this figure we
could see that the floor number is strongly dependent
on variable v   which indicates that v  comes from a
strong signal source  figure    shows the overall error
rate for each floor  it is found that gb has very low
error rate  which is   for floor         for floor        
for floor   and   for floor   

figure    misclassification error risk versus number of
iterations for gradient boosting methods
 

fiv 

figure     relative influence of all predictors

future work

since gradient boosting is robust of missing value  the
effect of missing value for current knn model is to be
investigated  also  beyond the current models  tracking
of moving user  type of phones and minimum number
of wifi sources required for accurate positioning will
be explored in the future work 

references
    zhou  junyi shi  jing  rfid localization algorithms and applications  a review  journal of intelligent manufacturing                    
    ferris  brian fox  dieter lawrence  neil d  wifislam using gaussian process latent variable
models  ijcai                     
figure     partial dependence on v 

    marques  nelson meneses  filipe moreira  adriano  combining similarity functions and majority
rules for multi building  multi floor  wifi positioning  ieee xplore       
    chen  mike y sohn  timothy chmelev 
dmitri haehnel  dirk hightower  jeffrey hughes 
jeff lamarca  anthony potter  fred smith  ian varshavsky  alex 
practical metropolitan scale
positioning for gsm phones  ubicomp      
ubiquitous computing               

figure     error rate for each floor
we also investigated the performance of gb on
small data set  here a small data volume of only    
randomly selected samples are used to fit the model 
and we got a cross validation error of      

iv 

conclusion

as demonstrated in this paper  the simplest knn
model gives good accuracy  given a relative small feature space and reasonable large data space  however 
svm performs poorly on this classification algorithm 
although one decision tree does not give satisfying result  bagging of multiple trees through gradient boosting could highly increase the prediction accuracy  to
acquire high accuracy  while maintaining the capacity
for predicting both small and large data set  we suggest
the combination of knn and gradient boosting for the
indoor positioning system 

    woo  sunkyu jeong  seongsu mok  esmond xia 
linyuan choi  changsu pyeon  muwook heo 
joon  application of wifi based indoor positioning system for labor tracking at construction sites 
a case study in guangzhou mtr  automation in
construction                 
    ching  william teh  rue jing li  binghao rizos 
chris  uniwide wifi based positioning system 
technology and society  istas        ieee international symposium on               
    torres sospedra 
joaqun
montoliu 
ral martnez us  adolfo avariento  joan p arnau  toms j benedito bordonau  mauri huerta 
joaqun  ujiindoorloc  a new multi building
and multi floor database for wlan fingerprintbased indoor localization problems 
 

fi