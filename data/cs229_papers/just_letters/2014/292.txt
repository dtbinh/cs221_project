finding undervalued stocks with machine learning
ramneet singh rekhi
rr     nyu edu

tucker l  ward
tlward stanford edu

huan wei
huanw  stanford edu
department of statistics
stanford university  ca

acknowledgement
michael vincent
downs

   introduction
    past work
there has been a great amount of studies with machine learning on financial market  however  a majority of the studies focus
on short term market performances and high frequency trading strategies  nevertheless  there are studies that focus on long
term strategies  one study shows that nonlinear support vector machines can systematically identify stocks with high and low
future returns     other studies also suggest that svm would a preferable methodology  this will be also confirmed through
our analysis 
    predicting
we are predicting if a stock  given certain features  is significantly undervalued  this will be a classification problem  we define
a significantly undervalued stock to have a     increase annually  the annual s p     return is     on average     
      labeling
we decided to employ three different labeling techniques for our study  we label the stocks based on a  time 
return threshold  pairing            significantly undervalued stock  which is a stock increases     in a year  however  we are
also experimenting on having various input for return threshold  for now  we have                                 or       
  not a significantly undervalued stock  in logistic regression  this will be labeled as       and in svm  this will become        
however  as huerta and corbacho point out  this approach does not control for risk      therefore  we borrowed two labeling
schemes they used in their study  first we calculated sharpe ratios for each stock  a sharpe ratio is defined as 

 

this adjusts the return of a given stock by a benchmark risk free rate  b  for which we used the us treasury constant maturity
three month series available from the federal reserve  and divides the asset by its standard deviation  we then rank ordered
stocks at time t  based on their sharpe ratios  and gave the top     of this list a   label  and everything else a        label  the
final labeling technique we employed  was from the capital asset pricing model   this theory proposes the return of a stock
can be decomposed  into market return  measured by its covariance with a market index and an idiosyncratic component
represented by alpa  whose expected value is zero  hence the return of a stock can be viewed as 
 

 

therefore we regressed each stock onto the sp    index  at a given time interval  and ranked ordered them by their alphas  and
mapped the top     to a   ranking and all else to a        ranking 
   data
our primary data set contains       stocks with approximately     features reported quarterly from      to       for a total of
nearly    million data points  the data for some stocks does not cover the entire time period   the     features encompass
everything from earnings to capital expenditures to taxes  we primarily collected data from a morningstar database and
whartons wrds databases  after downloading the raw data  we organized and processed it into a common format  while

                                                                                                                

  http   en wikipedia org wiki sharpe ratio  
  see http   www sciencedirect com science article pii        x         for more details
  from http   en wikipedia org wiki beta    finance     

    

   

ficonceptually simple  completing just this preprocessing required nearly       new lines of python code and a significant time
investment 
selecting which stocks to analyze was a difficult and deliberate choice  on one hand  well known and widely followed stocks
 eg  apple  exxon  will tend to be more stable and price may more accurately follow commonly accepted valuation metrics  on
the other hand  such stocks present fewer opportunities for significant  market outperforming growth or decline precisely
because they are so well followed and tend to be large  well established firms  we thus compromised and started our analysis
with the s p     stocks  which encompass both big names like apple and relatively unknown names like abbvie  after our
initial analysis  we decided to expand our research to mid cap stocks in the s p      range to provide yet more examples of
relatively unknown companies with higher growth potential 
we employed two techniques for feature selection at the data collection stage  first  we research common market indicators
and ranked features according to how likely we thought they were to provide useful information  this ranking helped identify
several important features such as momentum and short interest  second  we used a shotgun approach to test all of the
features available to us and see which were most useful  after eliminating features that were a function of stock price  eg  market
cap  
we tracked two dependent variables  market cap and maximum stock price attained within a given quarter  both dependent
variables are necessary because of a fundamental basis of stock price  company valuation divided by outstanding shares 
tracking market cap gave us an indication of how a companys estimated fundamental worth changed according to the features
 the company valuation  while maximum stock price attained gave us a more direct measure of stock price  a savvy investor
carefully tracks both valuation metrics  and anomalies in either metric can indicate strong buy or sell signals 
other relevant sections will speak more specifically about stocks  features  and independent variables  but significant thought
was given to careful data selection long before we ran any machine learning algorithm 
    data quality
the nature of the stock market data  which consists of various market indicators and accounting measures  is less structured
and needs to be regularized in order to work with various models  so far we have used chi square discretization to process data 
fortunately  accurate stock and company data is readily available through morningstar and wharton  and in general we need
not worry about data accuracy  finding relevant data was also never a problem  although as stated it required significant
preprocessing 
    features
we employed two approaches for feature selection in our algorithms  first  our background research indicated that two features
specifically  momentum and short interest  were most likely to be useful features  momentum measures how much a stock has
been up or down in the recent past and has been shown to be a reasonable predictor of immediate future success  although we
target longer term returns  short interest  or the percentage of shares that investors are betting will decrease in value  reflects a
sense of sentiment about whether traders think the stock will increase or decrease in the coming one to two months  we paid
special attention to those features and several others that showed promise 
second  we also evaluated each of the available     features to see if we could find useful information among the less obvious
features  those features encompass everything from earnings to capital expenditures to taxes  we also implemented several
feature selection methods  namely k means  principle component analysis  singular value decomposition  entropy 
divergence and n factors  to help resolve the more promising features 
   models and results
we tested on the following models  decision tree  knn  support vector machine  random forest  adaboost  naive bayes 
logistic regression  linear discriminant analysis  we found that svm with backward search  whether the search is being
performed by naive bayes or svm  and adaboost with is a powerful combination  we implemented a scoring system for all
the models and feature selections methods 
            
    
      
interpretation of the formula  if you had      precision and      productivity and         misclass  i e if you predicted
     of possible y s with      precision and      accuracy  you would score a     that means  numbers around       are
good  numbers around      are more likely to be resulted from over fitting and will likely decrease with cross validation 

    

   

fiheres an abstract of the scores across models and features 
features
score top svm feat
prod top svm feat
precis top svm feat
misclass top svm feat

dt knn svm rf ada nb logr ida
                                       
                                       
                                       
                                       

the following heatmaps generated from the scores shows how well each algorithm performs on different feature sets  the
lighter the color  the better the scores  we can see that adaboost  knn and svm perform consistently well across various
feature sets on two different timeframes 

please see attachment for a detailed list of results 
   discussions and analysis
    model and feature selection analysis
after evaluating the models and feature selection methods with our unique scoring system  we found the following conclusions 
 backward search appears to work better than other supervised and unsupervised feature selection approaches 
 un optimized logistic regression can still perform if focused on the right features 
 results suggest there is enough variability of errors that mixing data sets and algorithms in an ensemble of votes will
likely outperform any individual algorithm 
    discussion on model productivity
while training on different models and feature selection process  we want the model to be able to encounter a variety of
economic circumstances and still predict  therefore  we developed a number of data sets and are finding that the algorithms
behave differently to each one  for instance  svm and adaboost each has its favorite datasets which it will be able to predict
consistently  that leads us to believe that one way to ensure generalization to a variety of economic environments will be to use
a range of datasets and algorithm combinations to develop ensemble predictions that will be more accurate than any individual
one 
we ve also automated both feature selection and algorithm parameterization  regarding the latter  for our top performing
algorithms  we run iteratively over each data set changing parameters using a  tournament  model  so  for example  in svm
algorithm  it has a  fast  mode which uses the best all around parameter set  but it also has an  accurate  mode where it starts
by fitting based on model type  then moves to kernel type  then cost  then gamma  then nu  this results in algorithms that
could be specifically dialed into each data set to maximize the individual result 
    discussion on memory on prior performance
noting that our data set was in a panel format  because we have observations for multiple individuals  over multiple time
periods  we decided to exploit this property by incorporating time specific and individual specific effects into our model  to
accomplish this we created an augmented feature vector as outlined by dundar  krishnapuram  et al   this entailed creating
dummy variables for each quarter and each stock  this helped control the bias in our model parameters and also had the added
effect of giving the model memory for each stock it looked at  over a given time period  when conducting cross validation  on
our test set we zeroed out all of the time dummy variables  before they were fed into the model  because we were afraid this

    

   

ficould introduce out of sample information  however  we kept the stock dummies  exploiting any conclusions the model might
have come to apriori about a given stock 
    real world stress test on svm with linear kernel
we found that we are outperforming the market at     of the time on a quarterly basis  with our culmultive portfolio
returning over      in the time period from                      while the benchmark s p     is up about        the
best results actually came from optimizing our model to the alpha labelings described above on a support vector machine
algorithm with a linear kernel  which was one of our highest scoring algorithms based on our earlier analysis 
date

market shape

portfolio sharpe

outperformance

alpha

tstat

beta

         

      

      

true

    

    

    

         

      

      

false

    

     

    

          

     

      

true

    

    

    

         

      

      

true

    

    

    

         

      

     

false

    

     

    

         

      

     

false

    

    

    

          

      

     

false

    

    

    

         

      

     

true

    

    

    

         

      

      

true

    

    

     

         

      

       

false

    

     

     

          

      

      

false

    

    

     

         

      

      

true

    

     

    

         

      

     

false

    

    

    

         

      

       

false

    

     

    

          

      

     

false

    

     

    

         

     

      

true

    

    

     

         

      

      

true

    

    

     

         

      

      

true

    

    

    

to construct this stress test we used a moving    quarter window  where we estimated the model  then tested this model on
the following quarter  to map to actual stock predictions we employed a similar technique as huerta and corbacho  we rank
ordered the stocks by the output of the decision function  and constructed an equal weight index of long the top     highest
ranked stocks  and short the lowest     stocks 
   future discussion
the model needs to be further improved  it optimizes against a single set of y s based on    prior quarters of x s  it s using a set
of features that worked during a particular window based on value ranges that may have only occurred in that window  the
ultimate objective is to finalize to a model that works across time periods  as such  we suggested the following for future
implementations 

    

   

fi    technical functionality
we discovered that  for  in controller loop can t accept character variable names and it could be fixed with sapply or lapply  we
also like to modify the codes to ensure our models and results are replicable across different datasets  different time frames or
when implemented by different end users 
    data
firstly  if we can have new dataset that take in new x and y variables from tyler scaling models  we can then build new x
variables from the value managers  original spreadsheet   for example earnings acceleration 
on feature selection  we could implement reverse search on naive bayes  knn to validate their settings  feature sets  we
would also like to refine clustering to improve results on entropy and divergence  as well using other clustering algorithms
including latent class analysis to evaluate features  investigate missed predictions  false pos then false negative  to uncover data
and algorithm inadequacies  smear y influence over key variables 
    algorithms
we would like to modify our current models  each algorithm can be optimized further  we need to do k fold cross validation
to find best algorithm parameters  in addition we should write an algorithm to run the optimization of each algorithm varying
parameters  we will also move to multivariate predictors consistent w  new multi y dataset 
for learning algorithm  we need to build feature memory and stock memory  c b beta  into algorithm  different algorithms
have their own strength  some algorithms  like logistic  let you set priors  others  like adaboost  lets you weight features
and or observations  it takes more time to decide between using current algorithms or modifying existing algorithms 
we will also implement ensemble voting weighing votes for each record  for instance  having the following formula within an
algorithm based on settings and data set or assess across algorithms based on strengths and weaknesses 
 svm vote   svm weight     naive bayes vote   naive bayes weight       
we are also interested in incorporating new algorithms  for instance localized logistic regression  neural networks  lda
and additional clustering  including latent class or latent drichlet 
we will also like to diversify our portfolio strategies  we will consider both long and short portfolios strategies  we would also
to incorporate tools to separate models in order to find worst performers and emulate profits of pair trades 
   attachments
model evaluation score results
https   drive google com a stanford edu file d  b avk  hpeptzww ndbfa hrb e view usp sharing
   reference
    huerta  r   corbacho  f     elkan  c         february     nonlinear support vector machines can systematically identify
stocks with high and low future returns  retrieved december          from http   biocircuits ucsd edu huerta af huerta pdf
   damodaran 
a 
      
january
   
retrieved
http   pages stern nyu edu  adamodar new home page datafile histretsp html

december

  

     

from

    a stock selection model based on fundamental and technical analysis variables by using artificial neural networks
and support vector machines          review of economics   finance  retrieved december           from
http   www bapress ca ref ref        a stock selection model based on fundamental and technical analysis variables by
using artificial neural networks and support vector machines pdf
    varian  h         january      machine learning and econometrics  retrieved december          from
http   web stanford edu class ee    abstracts        slides machine learning and econometrics pdf
    useful lists   n d    retrieved december           from https   www quandl com resources useful lists
    dundar  m   krishnapuram  b   bi  j     rao  r   n d    learning classifiers when the training data is not iid 
retrieved december          from http   people ee duke edu  lcarin ijcai       pdf

    

   

fi