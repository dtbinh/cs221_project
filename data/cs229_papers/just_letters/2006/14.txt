minimizing system correlation in svm training
luciana ferrer

  description of the problem

we will consider a binary classification task for which two separate classifiers are available  each classifier
may use different input features and different modeling techniques  in a setup like this  the final decision
is made based on a combination of the outputs generated by both classifiers with the hope that the final
performance will be better than the performance of the two individual classifiers  this  nevertheless  is not
necessarily the case  in the extreme  if both classifiers were generating exactly the same output for each
sample  the combined classifier could never have a better performance than the individual ones  independently
of the combination procedure used  intuitively  what we wish is to have two classifiers for which the within
class correlation is small  this way  both classifiers contribute independent information leading to a better
final decision 
in this work we will study the case in which one of the classifiers is given to us  we can consider this
system as a black box which simply gives us a value for each sample  and the other one is an svm which
we need to train  our goal is to modify the training criteria for the svm so that the score resulting from
this system is as little correlated as possible to the scores from the black box system 

  anti correlation kernel
in this section we will derive the optimization problem we need to solve in order to achieve the combined goal
of minimizing the error of the svm system  which we will call s  while also minimizing the correlation of this
system with the original black box system  which we will call b   given a training set t   f x i    y i     i  
        mg  the standard svm problem is 
p
minimizew b  j w         wtw   c i i
   
subject to
y i   wt x i    b     i i           m
i   
i           m
we want to modify the objective function by adding a term   in the objective function  where  is a
tunable parameter and  is the within class correlation between system s and system b  given the scores
fb i   i           mg from system b for the training set t  we can compute the within class correlation between
the scores produced by the svm and these scores the following way 
cov b  s jy   
   
    var b
jy  var s jy  
where cov b  s jy    var b jy   and var s jy   are the within class covariance and variances  these can be
approximated by the within class sample covariance and variances in the training set t   the within class
sample covariance can be calculated as 
x x i y i    ff  b i  b   s i  s  
   
cov b  s jy    m 
ff
ff
ff      i
where bff and sff are the sample means for each class ff         the value s i  is the output of the svm 
i e    s i    wtx i    b  replacing this into     we get 
cov b  s jy    wt k
   
 

fiwhere 
k   m 

x x i y i

   

ff   

 

i

  ff  b i  bff   x i  x 

   

where x is the vector of feature means  k is simply the vector of within class covariances between each input
feature and the scores from system b  similarly  we can compute var s jy   and get wt mw where m is the
within class sample covariance matrix of the training set t   calling v   var b jy    we can write   as 
t
tw
    wv wkk
tmw

p

   

the new objective function then is j w         wt w       wv wkkmww   c i i  this function is not convex 
on the other hand  if instead of trying to minimize the within class
p correlation weptry to minimize the withinclass covariance  we get j w         wt i   kk t  w   c i i      wt aw   c i i   where a   i   kk t
is a symmetric positive semidefinite matrix  now  by doing a change of variable w    bw  with a   b t b
 i e    b is a matrix square root of a   we can write the new optimization problem as 
t

t

t

minimizew b 
j w 
         w  tw    c
 
subject to

pi i

y i   w t z  i    b     i
i   

i           m
i           m

   

where z  i     b    t x i   this is simply another svm problem with kernel k x  y    xt a  y  the
matrix a   can be computed extremely eciently using the matrix inversion lemma by which a    
 kk t   thus 
i   k
k
t

 xtkyt k
   
k x  y    xt y     k
tk
we can give an intuitive interpretation to this kernel  when  is small this kernel is close to the linear
kernel  when  grows to infinity the kernel substract the projection of the points x and y into the vector k
from the linear kernel  the resulting value of the kernel will be small if x and y are both aligned with k 
since the svm will only make an effort to separate points which give a high kernel value  this means we are
considering vectors which direction is close to that of k to be unimportant and  in consequence  emphasizing
the importance of the vectors which direction is different from that of k 

  simulation
in order to test this idea we created a toy problem  we generated data for two classes with model x  
cy   mff   where the yi are generated independently with a normal distribution with zero mean and unit
variance  c is a random matrix intended to create correlation between the features and mff is a vector of
zeros for one class and a vector of ones for the other class  we then took half of the features and trained an
svm  which served as system b  the remaining features were used to train system s with varying values
of   we created two separate sets  one for training and one for testing 
figure   shows the scatter plot of scores  on the training data  for both systems with      and        
we can see that for the large value of   the within class correlations have been reduced  eventhough we
are actually minimizing the covariance and not the correlation   we can also see from this picture that the
separation of the two classes is better for the larger  which implies that the performance of the combination
should be better in this case  figure   confirms this observation  in this figure we see the error rates for
system s  system b and the combined system and the correlation between system s and system b as a
function of the value of  for the test data  the error for system b  blue line  does not depend on  
the error for system s  red line  grows with the value of  since we are tradding off poorer performance in
 

fi 

  
 

s scores for lambda           

 

s scores for lambda    

 
 
 
 
 
 
 

 

 

 

 

 

 
  
  

 

 
 

 

 

  

b scores

 

 

 

b scores

figure    scores from system b versus scores from system s for two values of 
exchange for lower correlation with system b  the black line shows that  in fact  we achieve lower correlations
 reaching a value of zero  for higher values of   finally  the green line shows the performance measure we
care about  the performance of the combined system  the combination is performed using another svm
which is trained on the training set with the scores from the two svm systems  b and s  for each value of  
the combination performance improves for higher values of   from       to        this is a     relative
improvement  similar results were found by changing the random seed  the number of features  the number
of training and test samples  etc 

  application to speaker verification
speaker verification is the task of deciding whether a speech sample was produced by a certain target
speaker or not  it is a binary classification task where the two classes are true speaker and impostor  we
are considering a speaker verification system composed by two subsystems briey described below  the
ubm gmm and the mllr svm  the goal is to use the method explained above to train the mllr svm
subsystem taking into account the fact that the final system will be a combination of both these systems 

    universal background model gmm  ubm gmm  system

the ubm gmm system is a generative classifier  the input features are given by the mel frequency cepstral
coecients  mfcc   which serve as a description of the vocal tract characteristics of the speaker  each
class  impostor and true speaker  is modeled using a gaussian mixture model  gmm   the impostor gmm 
usually called background gmm  is trained using data from a large set of held out speakers  the truespeaker models for each speaker  called target models  are trained by adapting the background gmm to the
speaker s training data  the score is finally computed as the logarithm of the ratio between the likelihoods
of both models given the test data     

    maximum likelihood linear regression svm  mllr svm  system

state of the art speech recognition systems use methods to adapt the models for the acoustic units to the
speaker of the utterance being recognized  one strategy for performing this adaptation is to compute a
linear transformation of the means of the gaussians of the acoustic models such that the transformed models
maximize the likelihood of the utterance  the parameters of this mllr transform encode a description of
the speaker specific characteristics of the acoustic models  i e   they describe how the speaker differs from
the average population of speakers used to train the original acoustic models  thus  these parameters can
be used to train the svm target models for speaker verification systems         the mllr svm system is
currently one of the best performing speaker verification systems 

 

fi 
 

system b error
system s error
combined system error
correlation   

 
 
 
 
 
 
 
 

 

  

 

  

 

  

figure    error and correlation as a function of 

    application of the proposed method to the speaker verification problem

in speaker verification we need to train one model  svm  for each target speaker  in this work we will
consider the case in which around     minutes of a telephone conversation are available for training and
another     minutes are available for testing  the mllr svm system converts each sample into one long
vector of dimension      containing the parameters of the mllr transforms for each class of acoustic models 
this implies that when training a target svm we have only one vector labeled true speaker  on the other
hand  we can use all the held out conversation used for training the ubm model in the ubm gmm system
as samples for the impostor class   
to implement the proposed method to train a target model we need to estimate the vector k of withinclass covariances between the ubm gmm system and each of the mllr features  since only one sample is
available for the true speaker class  we will estimate vector k as the covariances for the impostor class only
 i e   in equation   the first sum is done only over ff       to estimate this covariance we need to have
the output of the ubm gmm system corresponding to the speaker for some significant amount of impostor
samples  for this  we select a set of     held out utterances from different speakers  which are never target
speakers  and obtain scores from the ubm gmm system  using the adapted model corresponding to the
target speaker   these samples are used to obtain an estimation of the vector k according to equation   
this procedure has to be done separately for each target speaker  this implies that a different kernel is used
to train  and test  each target svm 

    results

we applied this method to the data from the      speaker recognition evaluation organized by nist  the
correlation vectors are estimated using speakers from the      evaluation  the resulting mllr svm systems
for different values of  are combined with the ubm gmm sytem using a linear perceptron  figure     shows
the curve of false rejection versus false acceptances for five different systems    the ubm gmm system  two
mllr svm systems  one for      and one for        results do not change for larger values of   and the
two corresponding combined systems  we can see that a significant improvement  of around    on most
operating points  on the combined system is achieved by using the proposed kernel 
curves similar to the ones presented in section   could be shown here  we do not show them due to
lack of space  but we want to point out that eventhough the correlation between both systems goes down
when  increases  the value never reaches    for      the average correlation for all speakers is       for
large values of  the correlation decreases to       on the other hand  the inner product between the vector
  in order to compensate for this very unbalanced number of samples for the classes we use a weight of     for the slack
variable corresponding to the positive sample and a weight of   for the slack variables corresponding to the negative samples in
the objective function of the svm
  here we use the complete curve that is obtained by sweeping a threshold on the scores to show that improvements are
obtained at any value of the threshold  speaker recognition systems are usually used on the left upper corner of this plot  where
false acceptances are very low 

 

fi  

false rejection  

ubmgmm
mllrsvm    
mllrsvm     
combined    
combined     

  

  

 

 

   

   

   

 

 

 

  

  

false acceptance  

figure    false rejection versus false acceptances for the ubm gmm  two mllr svm systems and the
corresponding combinations
k and the resulting weight vector w does reach    which is what we expect this kernel to achieve  this
indicates that the svm is finding the right weight vectors for the estimated k vectors  but that these vectors
that are estimated from the held out data are not predicting the correlations between the features and the
ubm gmm system accurately  this might indicate that we should use more impostor samples to estimate
k  that we need better matched data       and      databases where somewhat different data collections 
or that we need to include true speaker samples in the estimation of k  in any case  this is an issue that
needs further investigation 
there are two ways in which the results presented in this section are somewhat optimistic  first  the
optimal value of lambda was chosen to optimize the test performance  and second  the combiner was trained
on the test data  we believe that both are relatively minor problems  in the case of the choice of lambda 
we saw in the simulations that the performance of the combination converges to a certain value as lambda
grows  this same effect is seen in the speaker verification performance  thus  the exact choice of lambda is
not important  as long as the value is large enough  the problem of the combiner being trained on the test
data is somewhat more important  but since the combiner is simply a perceptron with only three parameters
and the number of training samples is large        samples      of which are true speaker samples  the
possibility of overfitting the parameters to the test data is very small  for these reasons  we believe that
when we test the system on new unseen data we will obtain results similar to the ones presented above 

    conclusions

the presented method shows to be effective in finding svms that lead to improved performance for the
overall combined system with respect to the performance obtained by training the svms to optimize the
performance of the subsystem by itself 

references

    douglas a  reynolds  thomas f  quatieri  and robert b  dunn   speaker verification using adapted gaussian
mixture models   digital signal processing  vol      pp              
    a  stolcke  l  ferrer  s  kajarekar  e  shriberg  and a  venkataraman   mllr transforms as features in speaker
recognition   in proceedings of the  th european conference on speech communication and technology  lisbon 
sept        pp            
    a  stolcke  l  ferrer  and s  kajarekar   improvements in mllr transform based speaker recognition   in proc 
ieee odyssey      speaker and language recognition workshop  san juan  puerto rico  june      

 

fi