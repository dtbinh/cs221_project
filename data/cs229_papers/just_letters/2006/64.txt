dimensionality reduction using noisy distance data
pratik biswas         



december         

 

motivation

the basic idea behind most dimensionality reduction algorithms is to find a low dimensional embedding
of high dimensional data while roughly preserving the local relationships between the data points  the
inputs to these algorithms are usually the point coordinates in the higher dimensional space or some form
of mutual distance data between the points in the higher dimensional space 
however  very often  we are unable to capture the distance relationships between the points exactly 
or for that matter between all the points  the examples i consider in this project are inspired from
distance geometry  that is  to find a map of the relative positions of points in a low dimensional space
given some incomplete and inaccurate information between the points  this model can be applied to
sensor network localization or to molecule structure determination 
for example  in a wireless sensor network  the sensors are able to communicate with only a few sensors
in a close neighborhood of its own  using received signal strength or time of arrival measurements  each
sensor can arrive at a rough distance estimate of its neighbors  the challenge is to find the positions of
all the sensors in space using these measurements          the situation is similar in a molecule structure
prediction problem  using nmr or x ray crystallography measurements  we can estimate distances
between some pairs of atoms  the objective is to determine the complete structure of the molecule in
  d space         
the general distance geometry problem can be stated as such  given an incomplete and inaccurate
distance matrix between a set of n points  where xi   i                n are the positions of the n points in
space  we consider the case of   d in this report   can we recover their relative positions in space 
while this problem may at first glance seem somewhat unrelated to dimensionality reduction  variants
of dimensionality reduction techniques have been shown to have applicability in this space          this
is because the problem involves finding a low dimensional representation of points since the distance
information is generated from a low dimensional space in the first place  that respect the given distance
constraints 
it should also be borne in mind that the distance matrix d  where dij   kxi  xj k    can be expressed
in terms of the gram matrix g   x t x 
dij   gii   gjj   gij  
therefore  when there is complete and exact distance data  the exact positions of the points can be
recovered by performing a matrix decomposition of the gram or distance matrix  this idea is similar to
what the pca and mds algorithms perform in practice 
the problem when we have noisy distance data is that the distance information which was corresponding to a lower dimensional space is distorted  quite possibly  the distance information available to us no
longer corresponds to a low dimensional embedding anymore  the dimensionality reduction techniques
that do depend on distance information attempt to find low dimensional embeddings that preserve the
given distance information as much as possible 
 electrical

engineering  stanford university  stanford  ca        e mail  pbiswas stanford edu

 

fithese algorithms need to be evaluated in terms of their sensitivity to inaccurate distance data  in
this project  i chose   techniques that use only incomplete distance information to find low dimensional
embeddings   isomap     laplacian eigenmaps    and maximum variance unfolding    
i also considered using other methods pca  mds     and locally linear embeddings          etc 
but some of these methods use either complete distance information  or knowledge of point positions in
the higher dimensional space  for this project  i will stick to the   algorithms mentioned above 

 

algorithm descriptions

the isomap algorithm first creates a k nearest neighbor graph and assigns each edge a length that
equals the euclidean distance between the two nodes connected  in our case  if there are less than k
neighbors  all the neighbors are used  the second step is to compute the pairwise distance ij   for all
pairs of nodes i and j  as the length of the shortest paths connecting them on the graph  using djikstras
algorithm   in the third step  it uses the pairwise distances ij as inputs to mds  more specifically  it
computes the gram matrix g from the distance matrix   estimates the dimension r by the number of
significant eigenvalues of g  and constructs the low dimensional representations 
the laplacian eigenmaps method also begins by creating a k nearest neighbor matrix  however  the
edge weights are set to create a weighted laplacian  v    
wij   exp d ij       i  j    
the eigenvectors of the laplacian can be used to represent the variation in the geometry of the
graph  an optimization problem is solved to find a lower dimensional representation of the points while
maintaining the structure of the laplacian  in particular  the bottom eigenvectors encode most of this
information and it turns out that we can extract a lower dimensional embedding from them 
the mvu algorithm attempts to unfold the manifold by pulling the data points apart as far as possible  while faithfully preserving the local distances between nearby input data  it does so by maximizing
the distances between all the points while ensuring that the given neighborhood distance relations are
satisfied  this problem is formulated as a semidefinite program and the resulting distance matrix is used
in the same way as in isomap to obtain a relative map of the points 
it should be noted that in all cases  since only a relative map is obtained  i perform a least squares
fitting method at the end to find the best affine mapping that maps the points as closely as possible to
the original points  these   algorithms offer a reasonably complete picture of dimensionality reduction 
isomap is a variant of the mds based methods that use the top eigenvectors of the gram matrix 
the laplacian method looks instead at the connectivity graph and infers structure using the lowest
eigenvectors  and mvu is like a bridge between the   methods as explored in      

 

results

the input is some mutual distance information between a set of points x    x            xn   generated randomly
from a uniform distribution in   d space  only distance data upto a certain radius r is available  that
is  we will have distance information only between points which are within the cutoff distance r from
each other  the given distance information is further perturbed by a multiplicative gaussian noise  that
is  dij   dij        where dij is the corrupted distance between xi and xj   dij is the true distance  and 
is n        here when we refer to     noise  it means that        
the analysis examines how closely the results with the noisy distance data will match the actual
point positions  an appropriate choice of the measurement metric might be the rmsd error in the point
positions  the number of points n  noise  and the radius r are varied and its effect on the rmsd error
is observed   
  code

available at www stanford edu  pbiswas css   project 

 

fiin terms of accuracy  the mvu always outperforms the other   methods  followed by isomap 
especially when the noise is low less than      and there is enough distance information r       for
   points   it seems that the mvu technique best captures the distance information  isomap and
laplacian eigenmaps introduce other ideas such as the structure of the neighborhood graph to infer the
point positions  while this might be a good idea when the distance information is very unreliable  for
more accurate distance data  the eigenmap technique  in particular  is unable to exploit all the information
given to us 
the example shown in figure   shows the performance of the algorithms on a random graph of   
points  in a square region of             using r       and     noise  the red stars are the results with
erroneous distance data  the green circles are the actual positions of the points  and the blue lines show
the discrepancy between the actual positions and estimated positions  as can be seen  the mvu approach
has the best performance  it is  however  the slowest approach as well  since a large sdp is required to
be solved 
isomap   rmsd          

laplacian eigenmaps   rmsd         

mvu   rmsd          

   

   

   

   

   

   

   

   

   

 

 

 

   

   

   

   

   

   

       

 

   

   

   

       

 

   

   

   

       

 

   

   

   

figure      dimensional embeddings for a set of    points obtained from distance corrupted by     gaussian
noise  radius    

when the noise is very high     or greater   the story is very different  the neighborhood graph
is the most reliable for graph structure and the eigenmaps technique works very well  the isomap
technique has the worst performance  this could be because possibly the error propagation is very high
in the step where distances between unconnected points is found by using shortest paths  so much so that
the distance matrix obtained is simply not valid 
the eigenmaps method  however  is far less susceptible to noise  by taking the smallest eigenvectors
of the laplacian  this method captures the more regular variations in the structure of the graph  just like
the lower frequency components in a signal  the higher eigenvectors correspond to the more irregular
variations  the assumption is that if the set of points is in a low dimensional space  most of the information
will be encoded in the smallest eigenvectors  in some sense  it is the connectivity information that is more
criticial than the exact distance information  infact  for more irregular graph structures  it was observed
that the eigenmaps method performed far worse than the other   methods 
the example in figure   shows the performance on     points with r        corrupted by     noise 
it is interesting to observe how the points which are close to each other tend to cluster together in all
these algorithms  as future work  it might be interesting to investigate how to cluster points beforehand 
and do the dimension reduction on the a reduced point set corresponding to just the clusters  this could
help in reducing the computational times  especially for the mvu 

 

fiisomap   rmsd         

laplacian eigenmaps   rmsd         

mvu   rmsd         

   

   

   

   

   

   

   

   

   

 

 

 

   

   

   

   

   

   

       

 

   

   

   

       

 

   

   

   

       

 

   

   

   

figure      dimensional embeddings for a set of     points obtained from distance corrupted by     gaussian
noise  radius    
  

   
isomap
eigenmaps
mvu

  

    

isomap
eigenmaps
mvu

   
  

rmsd error

times sec 

    

  

   

    
  
   
 
    

 
  

   

   
number of points

   

 
  

   

 a  computational times

  

  

  

  
  
noise pct error 

  

  

  

  

 b  noise sensitivities

while the mvu technique provides very accurate results  the computational effort spent was significantly larger and for very large sets  the computational time was clearly too large to offer any advantage
over the other methods  figure   a  shows how the methods scale with the size of the point set  note
that r is not the same for all sets of points  since a high r for a large set corresponds to a far higher
connectivity  r is scaled such that connectivity stays with     on average 
the graph  figure   b  shows how the rmsd error varies for the different methods for a set of    
points with r        and varying noise  it captures the sensitivities of the different approaches to noisy
distances 

 

fi 

conclusion

based on the results  i would recommend using mvu when the distance information provided is not too
noisy  and the number of points is small  if the number of points is high and computational time and
effort are an issue  but the noise is still low  isomap is a better alternative  but if the noise is high and
the point set is large  eigenmaps is the best option 

references
    andreas savvides  mani srivastava  lewis girod  and deborah estrin  localization in sensor networks  wireless sensor networks  pages              
    pratik biswas  tzu chen liang  kim chuan toh  ta chung wang  and yinyu ye  semidefinite
programming approaches to sensor network localization with noisy distance measurements  to appear
in ieee transactions on automation science and engineering  special issue on distributed sensing 
    gordon crippen and timothy havel  distance geometry and molecular conformation  wiley       
    pratik biswas  tzu chen liang  kim chuan toh  and yinyu ye  an sdp based approach for
anchor free  d graph realization  technical report  dept of management science and engineering 
stanford university  submitted to siam journal on scientific computing  march      
    kilian weinberger  fei sha  qihui zhu  and lawrence saul  graph regularization for maximum
variance unfolding  with an application to sensor localization  in b  schlkopf  j  platt  and thomas
hofmann  editors  advances in neural information processing systems     mit press  cambridge 
ma       
    m  w  trosset  applications of multidimensional scaling to molecular conformation  computing
science and statistics                  
    j  b  tenenbaum  v  de silva  and j  c  langford  a global geometric framework for nonlinear
dimensionality reduction  science                      december      
    m  belkin and p  niyogi  laplacian eigenmaps for dimensionality reduction and data representation 
     
    w  weinberger  b  packer  and l  saul  nonlinear dimensionality reduction by semidefinite programming and kernel matrix factorization       
     trevor cox and michael a  a  cox  multidimensional scaling  chapman hall crc  london        
     s  roweis and l  saul  nonlinear dimensionality reduction by locally linear embedding       
     d  donoho and c  grimes  hessian eigenmaps  locally linear embedding techniques for high dimensional data  proc  of national academy of sciences                         
     lin xiao  jun sun  and stephen boyd  a duality view of spectral methods for dimensionality
reduction  in icml     proceedings of the   rd international conference on machine learning 
pages           new york  ny  usa        acm press 

 

fi