learning to test

michael munie
department of computer science
stanford university
stanford  ca      
munie stanford edu

  introduction
our motivation for this project was that we wanted to be able to quickly tell if the cs qualifying
exams or the ee qualifying exams are more likely correctly pass or fail a student  this question
is difficult to answer without a formal model  so we will first describe how the exams are actually
conducted  and then propose our formal model  in the electrical engineering department a given
student would go from professor to professor  each professor would examine him her independently 
and at the end of the day the judgments of the professors would be aggregated into an overall score 
in contrast  in the ai quals in the computer science department  the student would be examined by
a group of professors who would meet as a committee  in the course of our research we have found
that finding the expected utility for either model is np hard and even simply determining which
methods utility  without computing the difference  is greater is np hard   

  formal model
our formal model is based on bayesian networks  in this paper we consider only binary networks 
in which each node can take on exactly two possible values  true and false  which well sometimes
denote   and     in the single agent case the network has the following structure  the nodes in
the network are partitioned into three disjoint sets  s  u   and the singleton set r  intuitively  u is
the set of hidden knowledge nodes  each node captures whether the student does or does not know
a particular topic  s is the set of questions one can ask  and r is the decision  or result  node
determining whether the student should pass the exam  consistent with this intuition  we have the
following constraints on the network  among nodes in u there can be arbitrary correlations  for
example  the node representing the students knowledge of probability theory might influence the
likelihood that the student knows machine learning  the parents of nodes in s are restricted to the
set u   thus  the nodes in s are all independent given u and the nodes in s have no children  r is a
special node  and represents our criterion for passing  or failing  the student  it is the child of exactly
every node in u and expresses a set function        u             that is  r has a deterministic cpt  
for this paper r will be    the student will pass  if more than  percent of the nodes in u are    and
r will be   otherwise 
this is a complete description of the students knowledge  and whether he ought to pass  for any
given choice of questions and answers to those  figure   shows an example of such a network 
to complete the model we must specify constraints on the questions asked  to this end we associate
with each node a cost  every si  s has an associated cost ci   which will be assumed to be the unit
cost unless explicitly mentioned  all nodes in u and r have cost   finally  we assume an given
budget b  such that the aggregate cost of the questions asked will not be allowed to exceed b 
 

the motivation could be set up equally well in terms of sensor networks  but we stick to the exam story
both because it was our motivation 

fir

x  

y  

x  

y  

x  

x  

y  

figure    sample single agent network
in this paper we will focus on learning which of two models  the sma  i e  ee  and smac  i e 
cs   will perform better 
we capture the multi agent  sma and smac  models by duplicating each observable node as many
times as there are professors  attaching a separate cpt to each node  and assigning each copy to
a distinct professor  each of these duplicated nodes is said to belong to the same question  the
intuition behind this construction is that these duplicates represent a specific question that any of
the professors could ask  but that they each draw different conclusions based on the answers  the
individualized cpts mean that the assumption that they all have access to the same set of questions
is w l o g    each professor has a budget of  pb     where p is the set of professors  in the sma
 sequential multi agent  mechanism  each professor i first selects a node  then based on the value
of the node observed  the professor makes another observation until he fills his individual fraction
of the budget  each professor makes the observations independently  the results of the observations
made by all of the professors are then used to compute the posterior on r  and make a pass fail
decision 
the smac model is a particular way to capture the operation of committees  in this model  all
the professors are put into a committee  each professor is still given the budget of  pb     inside the
committee  observations must be made by question group  not by individual node  what we mean
by this is that all the nodes in the same question group that belong to professors inside the committee
must be observed simultaneously  or none of the nodes observed at all  a committee will observe
question groups until its budget is exhausted  after all the nodes in a particular question group have
been observed  the choice of the next question group may be conditioned on the results of previous
observations 
the intuition is that although fewer question groups will be examined in aggregate than in the sma
model  the total professor time spent will be the same 
the following will be stated without proof  but provide us our motivation to try and find under what
conditions the sma mechanism beats the smac mechanism 
theorem  
 given any budget b and set of researchers p   there are networks for which
the best gsmac mechanism outperforms the best sma mechanism  the best smac yields
an expected utility of    the highest possible  and the best sma yields the expected utility of
 p  
     b p
     
 given any budget b and set of researchers p   there are networks for which the best sma
mechanism outperforms the best smac mechanism  and it does so by the widest possible
margin  the best sma yields an expected utility of    the highest possible  and the best
smac yields the expected utility of the trivial mechanism which decides based on the prior
of r 
theorem   deciding if  on a specific network  eu  sm ac   eu  sm a  is np hard 

  implementation
the data generation code is written in python  and the machine learning code is in matlab  the data
generation is implemented as follows  we first create a random directed graph and reject the graph

fiif cycles are present  this graph will represent the set of u nodes  then we populate the probability
tables of the links with random values  after this is complete  we add a r node with a random 
value ranging from    to    for each node in u we add a group of question nodes with random
probability tables  one for each professor  corresponding to a question group 
now we have a random student network  we would like to know which mechanism  sma or smac 
has the higher expected utility on this network  using the bayesian network  we compute exactly
the utilities of the sma and smac mechanisms and output the result to a file 
in addition  for each network  we record the following features 
 the number of simply connected components in the u network 
 the diameter  largest shortest path between any two nodes  of the u network 
 the number of edges in the u network
 the actual average link strength in the entire network
 
 prior on r
 maximum of rs prior
 number of nodes with              outgoing links
 number of nodes with              incoming links
 number of nodes with              total links

 

results

overall  the results were fairly inconclusive  using svm implemented with the smo algorithm we
were able to get     classification error on the test set  and using logistic regression we were able
to get     error  we also tested with gda  but the error was over     which seems to suggest that
the data is not gaussian  the test data had     of the examples with a true negative classification 
so the results of our machine learning are not much better than guessing uniformly according to the
prior  both svm and logistic regression performed almost as well on the test set as they did on the
training set  it was difficult to find informative features so we also ran svm with a quadratic kernel
to get more features  but the error rate was still relatively high at      the testing and training were
made more difficult by the fact that generating enough training data was very slow  so for all of these
examples we were training on data sets of size     and testing on data sets of size     however 
since the error rate was similar on the test and training sets  it is more likely that our features are our
problem  not the small number of training examples 
below is a selection of our results with the error on the test set 
algorithm
svm
logistic regression

  nodes and   observations
     
     

  nodes and   observations
     
     

we graphed the outputs of both logistic regression and svm according to the true classification of
the data to see if there was any natural break  but the output looks very hard to classify and in fact
may be independent of the labels  the data is presented in figures    and   

 

conclusion

the initial motivation for applying machine learning to this problem was that in other np hard
problems like k sat  there appears to be a natural phase transition when measured according to the
number of variables per clause  instances that have a ratio less than this phase transition are very
likely to be satisfiable  and instances that have a ratio greater than this phase transition are very likely
to be unsatisfiable  ideally  we would be able to find this phase transition via automated methods 
but it is very dependent on the feature  in the particular problem we worked on  it doesnt appear
that we have found the right feature  if one exists 

fi 

 

   

   

   

   

   

   

   

   

   

   

   
   

 

   

 

   

   
   

 

   

 

   

figure    logistic regression on   node networks with budgets of    left  and    right 

 
   

    

 
   
   
    

 
   

 

 

    

   
   
 
    

   
 
   

 

   

 

   

   
   

 

   

 

figure    svm on   node networks with budgets of    left  and    right 

   

fithe problem of predicting which structure of professors has greater expected utility proved to be
a difficult problem to apply machine learning to  the results are better than simply going with the
prior  but perhaps not good enough to be dismissed as more than a random variation  through this
project i personally have learned a lot about applying machine learning  and hopefully  with some
more work  i can produce some more positive results on this problem 

 

future work

the initial results are a little unsatisfactory  but there are areas in which i can hopefully improve
them  first  my data set was a little too small  and with more time i should be able to generate
a larger set  secondly  and more importantly  the features i have come up with are insufficient 
hopefully  through a little more work to gain insight into the structure of these networks  hopefully
i can find a better set of features  i would also like to acknowledge that this work is based on joint
work done with yoav shoham and with advice on the machine learning from haidong wang 

references
    andreas krause and carlos guestrin  optimal nonmyopic value of information in graphical
models   efficient algorithms and theoretical limits  in nineteenth international joint conference on artificial intelligence       
    andreas krause and carlos guestrin  near optimal nonmyopic value of information in graphical models  in   st conference on uncertainty in artificial intelligence       
    d  kempe  j  kleinberg and e  tardos  maximizing the spread of influence through a social
network  in sigkdd       
    t h  cormen  c e  leiserson  r l  rivest and c  stein  introduction to algorithms  mcgrawhill  boston  ma       
    robert j  mceliece  the theory of information and coding  cambridge university press  cambridge  united kindom       

fi