face orientation estimation in smart camera networks
chung ching chang

   introduction

a single
image x y t 
camera node
head strip matching

an important motivation for face orientation analysis derives from the fact that most face recognition algorithms
require face images with approximately frontal view to
operate efficiently  such as principle component analysis
 pca       linear discriminant analysis  lda       and hidden markov model  hmm  techniques     
in a networked camera setting  the desire for a fontal
view to pursue an effective face analysis is relaxed due to
the distributed nature of the camera views  instead of acquiring frontal face image from any single camera  we propose an approach to face reconstruction in a smart camera
network by collaboratively collecting and sharing face information spatially 
the framework of spatiotemporal feature fusion for face
reconstruction and analysis is shown in fig     in node
feature extraction in each camera node consists of low level
vision methods to detect features for estimation of face orientation or the angular velocity  these include the hair face
ratio and optical flow  which are obtained through discrete
fourier transform  dft  and least squares  ls   respectively  another feature extracted locally is a set of head
strips  which is used to estimate relative angular difference
to the face between cameras by a proposed matching technique  a markov model is designed to exploit the geometic
connectivity between strips  and a viterbi like algorithm is
applied to select the most probable displacement between
head strips of the two cameras  therefore  the proposed
technique does not require camera location to be known in
prior  and hence is applicable to vision networks deployed
casually without localization 
a spatiotemporal feature fusion is implemented via keyframe detection  and a spatiotemporal reinforment learning 
the key frames are obtained when a camera node detects a
frontal face view through a hair face analysis scheme and
this event is broadcasted to other camera nodes so that the
fusion schemes for face analysis can be adaptively adjusted
according the the relative angular estimates  in order to
maintain a high confidence level  the proposed spatiotemporal reinforment learning cooperate the temporal correlation into the state transition matrix and the spatial correlation into a cost function design  and choose the trellis with
the minimum cost 

in node feature
extraction

optical
estimation

head strips

face strip
matching

hair face
estimation

key frame
notification
and receiving

key frame 
y

other network nodes

decision

spatiotemporal
reinforced learning

face orientation
estimates

relative angular
difference to the object
spatiotemporal
data fusion

figure    framework of spatiotemporal feature fusion for face orientation analysis 

   in node feature extraction
local data processing algorithms in each camera node
consist of low level vision methods to detect features for
estimation of face orientation  including optical flow and
hair face ratio as introduced in the following subsections 
these techniques are developed to be of low computational
complexity  allowing them to be adopted for in node processing implementations 

     optical flow estimation
this analysis project the motion of the head into several
orthogonal dimensions and estimate the projected vector by
least square estimation  in order to reduce computational
complexity  we only consider the motion vector of the edges
of a face  in our experiments  we assume the head turns
without tilt and pan  so we decompose the head motion into
only translation and rotation in y axis to simplify the analysis  the decomposition model is as follows 
vi   t   rcos i  

   

where vi is the norm and direction of the motion vector in
the direction orthogonal to the heads vertical axis  where
positive sign indicates the direction is to the right  and negative sign to the left    t is the translation factor  r is the
transversal radius of the head   is the angular motion  and
rcos i   represents the distance from the point of the motion vector to the longitudinal axis of the head in the  d

fi a 

 a 

 c 

         

 b 

 b 

zero padding

figure    procedure for the hair face ratio estimation  illustration
of how the head ellipsoid  right  is transformed into a sequence
of hair face categorized image slices  middle   and into a ratio sequence with zero padding  left   

 d 
  

head

  

body

  

 vi 

  
 
   
 

  

  
  
  
rcos t  for red and rsin t  for blue

  

  

figure    opical flow estimate  a  image x y t    b  image x y t    
 c  image x y t  and the motion vectors   d  least squares estimates 

image plane  by placing eq     in vector form  we have
 


  rcos    
v 

 v      rcos      
 

 t
   
          

  
      
 
 
   z  
vn
  rcos n  
z
   z    
 z
 
v

a

minimizing the mean square error of the motion vectors under the model yields the least squares solution of x as 
zls    at a   at v

   

where the first element of zls is the translational velocity
and the second element is the angular velocity of the head 
the residue of the least squares analysis yields a measurement of the confidence of the estimation  experimental results are shown in fig     where the slope indicates the angular velocity  and the intersection on y axis indicates the
translational velocity 

     hair face ratio estimation
to estimate the hair face ratio  we first classify the head
region into face and hair regions by color  there is much
previous work on using skin color model and hair color
model for face detection              in this paper  we simply apply similar method with value applied well in the sequence  further research on exploiting model bias between
cameras and skin color model estimation based on the em
application  paper evaluation problem  proposed by john
platt 
based on the hair face classification  face orientation is
analyzed in the following procedures as shown in fig    
consider the head as a ball in  d space  and cut the surface

of the ball into n equally spaced strips along its longest axis
direction  as shown in figure    in each camera frame  we
can only see m of the n strips of the ellipsoid  calculating
the ratio of the hair region to the face region in each of the
m strips and padding zero to the strips that cannot be seen in
the current frame  we form a ratio sequence of length n   by
analyzing dft of the ratio sequence and considering only
the phase of the fundamental frequency in the frequency domain  we can estimate the face orientation based on the assumption that the hair face ratio is symmetric in the frontal
face and is approximately a sinusoidal curve along the surface of the ellipsoid 

   head strip matching and the relative angular difference to the face between cameras
instead of calculating the ratio of the hair region to the
face region in each of the m strips  we sample each of the m
strips with n samples  prior to the sampling  a  d median
filter is applied to reduce noise as well as introduce correlation between sampling points and the nearby pixels  geometrically  if all cameras are deployed at the same horizon 
the relative angular difference to the head between two cameras would cause a shift in their strips  therefore  matching
the head strips of the two cameras and finding the displacement of the strips give us the  quantized  relative angular
difference to the object between the two cameras at a given
time 
the head strip mapping is based on a markov model
and a viterbi like algorithm as shown in fig     considering two sets of head strips y and y    corresponding to the
head images captured in two cameras  c and c    let y  

   where yi   yi  rn
 y  y        ym   and y     y  y        ym
correspond to n sample points in a single strip  our problem now is to map the strips in y  to the strips in y with the
constraint that yi   yi are in some spatial order 
we now introduce the concept of the states s  let s  
 s  s        sn   denote all states for the strips of a head     o   
for example  s  representing the strip that includes the nose

fi b 

 a 
si  

 a 

m

sm

sm

sm

sm

sm

m  

sm  

sm  

sm  

sm  

sm  

 c 

  

si  

  
y

pxixi  

si  

 

s 

s 

s 

s 

s 

 

s 

s 

s 

s 

s 

 

s 

s 

s 

s 

s 

 b 

  

yi  
pxixi  

si  
pxixi  
pxixi

  

yi  

yi  
si

input

y 

y 

  

y 

figure    illustration of the markov model and viterbi like algorithm   a  the viterbi like model generated by the head strip set
in camera c   b  the trellis of the viterbi like algorithm  sm 
in the rightmost row is the state with the minimum cost  and the
corresponding trellis is marked in thick  red  line 

trail  for each of the captured head images  the corresponding head strips y should map to a consecutive subset of s 
denoted sy   which is not known in prior and is approximately of length m  in other words  y is a representation of
the states sy   as we scan vertical sampling lines through
the head horizontally  we are actually going from state to
state  for example   from si to si     ideally we will get
yj and yj   for certain i and j  however  due to the fact
that the head is not a perfect ball  we may as well get yj
and yj k for certain j and small k     the latter constraint
showing that the two states should be near and cannot occur in a reverse order as we scan through the head  in other
words  the probability of p si si     the probability of going
from the current state to the next state as we scan through
the head  is not necessarily    the probability of the transition between states forms a markov model  as shown in
fig   a   in our experiment  the choice for the probability is
 k     
  u k   u k         
   
where u is the unit step function and  is the bandwidth
parameter  further normalization is required 
as we match the set of strips y  to y   we first assume that the representation y is ideal  corresponding to the
states sy one by one  under this assumption  we transform
the viterbi algorithm  a supervised learning algorithm  into
an unsupervised way of learning  which we call a viterbilike algorithm  for each given input yi   we can sum the cost
in each of the previous states and the cost to go w  in each
branch  and choose the branch with the minimum cost as
the path from the previous states to the current states  the
cost of the branch w is calculated as follows 
psi si k   exp 


  si si k   
wsi si k    ln psi si k  yi k

  si si k  
 yi k

   

is calculated by the inverse of the
where

mean square error between strips yi k
and yi k  
the initial states are assumed to be equally likely  meaning that the matching can start from any of the states in sy  

   y   

  

figure    example of strip matching   a  background subtracted
image in camera y     b  background subtracted image in y   c 
the trellis with minimum cost  blue  and the corresponding cost
in each step of the viterbi like algorithm  red  

the first and the last states in sy may be regarded physically as the not in y  not in current face  states  therefore 
some exceptions for the probability model are made in the
first and the last states  where p s  s  is given higher probability and p si sm is   when i   m  and zero otherwise 
according to the viterbi algorithm  the path with the
smallest cost is chosen  for example  as in fig     assume sm  in the rightmost column is the state with the
minimum cost  and the corresponding previous paths are
marked with thick  red  lines  showing that the paths are
 s  s  s        sm     in fig   an example of head strip matching is shown  the tellis of the viterbi like algorithm is shown
in the right figure with blue dots  where red dots represent
minimum branch cost  w  in each viterbi like step  notice
that the trellis  excluding those in states s  and sm   intersect the x axis around     which means the displacement
between two head images is    strips  or    degrees in the
example 

   spatiotemporal data exchange
collaboration between cameras is achieved by data exchange  correlations in temporal domain is exploited since
face orientation and angular velocity  one being the derivative of the other  in consecutive frames are continuous provided that the time lapse between frames is short  data exchange in spatial domain would also be helpful in sharing 
comparing  and validating data since for any time instance
the captured image in each camera should reflect the same
motion in  d 

     key frame detection
key frames are the frames that include feature or estimates with high confidence  the hair face ratio based on
the phase of the fundamental frequency is sensitive to the
face angle  although the true face angle is not a linear
function of the estimates  a frontal view with the hair face
ratio approximately symmtric to the face center can be detected accurately  under the assumption that head motions

fihair face
estimates  degree 

 

a

b

 
t  
 

t time

 
calculate the face
o
 
key frame orientation by adding  
key frame detected  notification with the relative angular
difference

through
the
indicating that the
  calculate the face
camera has the frontal network
orientation by adding  o
view   o 
with the relative angular

difference  



time of a frontal
face detected  tff 

t ff

itively  the result is similar to that interpolated by the optical flow estimates  however  the corresponbdence between
cameras are achieved through the l  norm penalization 

b
a
 t    
t
ab
ab

figure    illustration of the key frame detection procedure

are piecewise linear between samples  the time of a frontal
view  defined as a key frame event  can be determined by
interpolation  once a key frame is detected  the time of its
detection is notified to other cameras  since the key frame
is associated with relatively high confidence  other cameras
would assume the received key frame orientation estimation
to be true and calculate the face orientation by adding that
with the relative angular difference to the object between
themselves and the camera that broadcasted the key frame 

     spatiotemporal reinforcment learning
in our framework  the face orientation between key
frames are determined by the accumulation of the angular
motions  recall that the optical flow estimation is a prediction over tens to hundreds of motion vectors according
to the law of large numbers  the pdf of the estimation itself
will be gaussian distributed  no matter what the original
error distribution is  assuming that the optical flow estimates in different time are independent  the variance of the
accumulated optical flow estimates equals the sum of the
variance of each individual optical flow estimates  therefore  while the orientation estimates are deterministic when
a key frame occurs  the orientation estimates between key
frames are stochastic with uncertainty increase over time 
hence  we apply a spatiotemporal reinforcement learning
algorithm to choose the best trellis with the minimum cost
as the uncertainty increases 
in a markov decision process mdp   we are usually
given the states s   the state transition probability psa   
the discount factor    and the cost function c   and want
to determine the optimal policy   and its correponding
actions a   in our face orientation estimation settings  s
are the  discretized  face orientations  each column of psa is
the probability density function of a gaussian random variable  with mean and variance determined by the optical flow
estimation  the cost function is the sum of the absolute difference of the estimates between cameras  choosing the
cost value to be a l  norm of the difference of the estimates
can avoid the effect of an outlier  different from the ordinary mdp  the psa here may change over time  and thus
the action also changes over time  the spatiotemporal reinforcement learning proposed here cooperate the temporal
information into psa   and spatio information into c  intu 

   comparative experiments
the setting of our experiment is as follows  three cameras are placed approximately on the same horizon  one
camera  camera    is placed in frontal direction to the
seat  and the other two are with about    o  camera    and
  o  camera    deviations from the frontal direction  the
experiment is conducted with a person sitting still on a chair
with the head turning from right   o   to left    o   and
then to the front    o   without much translational movement  the time lapse between consecutive frames in each
camera is half a second  the resolution of the cameras is
   x    pixels   
the result of the estimated relative angular difference to
the face between cameras is shown in fig     the mean
and the std of the estimates in camera   are      o and
     o   and those of camera   are       o and     o   both
of the time averaged estimates are close to the ground truth 
four examples of the reconstructed face based on these estimates are shown in fig     the example in  c  fails to match
well in the nose trail region due to an under estimation of
the relative angular differnce to the face between the cameras  in the example in  d   one should notice that the left
and the right ears are not in the same horizon  indicating
that the in node signal processing fails to capture the head
geometry in fitting an ellipse to the head region  which in
turn deteriorates face matching performance 
the results of the collaborative face orientation estimation are shown in fig     and fig      with camera locations assumed known and unknown  respectively  the dotted lines in the figures show the ground truth face orientation at each time instance  although the estimation is
degraded without camera location known in prior  the estimates without prior knowledge of the location still predict
the face orientation in an acceptable level 

   conclusions
in this project  the face model reconstruction and analysis problem is approached in a networked camera setting 
based on the distributed nature of the network  we propose
a spatiotemporal feature fusion framework to address the
problem which involves two aspects  first  a collaborative
technique for head strip matching based on a markov model
and a viterbi like algorithm predicts the relative angular differences to the face between cameras and reconstructs a face
model without having to know camera locations in prior 
second  spatiotemporal collaboration is embodied through
identification and exchanging of key frames  and the design of the state transition matrix and cost function in a re 

fi  

cam  estimation
cam  estimation
cam  estimation

   

  

estimated orientation  degree 

est  angular diff  to the face between the camera and cam 

hairface ratio estimation
cam 
cam 
cam 

  

  
 
  
  
  
  

   

  

 

  
 

 

 

 

 
frame  

 

 

 

 

figure    estimated relative angular differences to the face between the cameras 
 a 

   

 

 

 

 

 
frame  

 

 

 

 

figure     face orientation estimation by hair face ratio estimates 

 b 
spatiotemporal collaborative estimatation without camera location known
   
cam  estimation
cam  estimation
cam  estimation

 c 

estimated orientation degree 

   

 d 

  

 

  

figure    examples of the reconstructed head model   a  and  b 
successful examples   c  fair example   d  unsuccessful example 

cam  head
cam  head
cam  head
cam  body
cam  body
cam  body

  degree consecutive frame 

  

 

 

 

 

 
frame  

 

 

 

 

figure     spatiotemporal face orientation estimation without
knowing camera locations 

optical flow estimation

  

   

also like to thank chen wu for her contribution on the realtime optical flow algorithm and her idea on the framework 
furthermore  i would like to thank prof  andrew ng and
daniel chavez whom i consulted for project direction and
machine learning algorithms 

  
 
  
  

references

  

 

 

 

 

 

 

 

 

frame  

figure    face angular motion estimation by optical flow estimates 

inforcement learning algorithm  comparative experiments
with and without spatiotemporal collaboration indicate that
the proposed technique can successfully predict the face orientation within an acceptable level without prior camera location information 

   acknowledgement
i would like to thank prof  hamid aghajan for his help
with this project  i consulted him for the design of the
framework on the data fusion between cameras  i would

    q  chen  h  wu  t  fukumoto  and m  yachida   d head pose
estimation without feature tracking  in ieee conference on
fgr         
    d  kurata  y  nankaku  k  tokuda  t  kitamura  and
z  ghahramani  face recognition based on separable lattice
hmms  in proc  of icassp         
    b  kwolek  face tracking system based on color  stereovision
and elliptical shape features  in ieee conference on avss 
       
    c  liu and h  wechsler  enhanced fisher linear discriminant
models for face recognition  in proc  of icpr  volume   
pages                  
    f  liu  q  liu  and h  lu  robust color based tracking  in
proc  of third conf  on image and graphics         
    m  turk and a  portland  eigenfaces for recognition  j  cognition nueralscience                    

fi