learning bayesian networks in presence of missing data
narges bani asadi
motivation  signal transduction networks  the study of signal transduction
networks is one of the major subjects of interest in systems biology  signal
transduction pathways are means of regulating numerous cellular functions in response
to changes in the cell s chemical or physical environment  signal transduction often
involves a sequence of biochemical reactions inside the cell  which are carried out by
proteins  the network of interacting proteins and phosphates relay information as well as
amplifying them  their interaction is by activation or inhibition of the next molecule in
the chain  there are usually more than one pathways functioning as well as interacting in
a cell at the same moment  the target proteins that can be gene regulatory proteins  ion
channels  or components of a metabolic pathway have important affect on the cell
behavior 
in this project we try to show how bayesian network learning methods are helpful in
inferring the causal interactions between the molecules  to study the signal transduction
network one needs to have a way of measuring proteins involved in the process and
should also have a way to silence or force each of them or groups of them to a specific
value in order to produce intervention data that is necessary to distinguish correlation
from causality  although microarray technology has had a lot of progress and people are
able to measure the expression of all the human genes in parallel  measuring the proteins
that are active in cell is still a very challenging task  current technology is capable of
measuring only a few  around     proteins simultaneously in a single cell  therefore it is
of great value to develop methods that can extract the information about the structure of
network as much as possible from the partially observed variables  this fact motivated
me to work especially in learning the bayesian network in presence of missing data 
the data i have used to test my algorithms is from flow cytometry experiments  facs  
in facs experiments they put fluorescent markers inside the cells that bind to proteins
floating around inside  then they put them inside a tiny tube with a fluid flowing through
it that causes the cells to pass one by one through a number of lasers that can detect the
quantity of proteins with markers bound to them  giving one an idea of the amount of
each protein in the cell  what the scientists have done is to apply a number of activators
and inhibitors to cell samples and then checking    different proteins in this manner 
each experiment has between     and      cells in it and there were   different
experiments  this data is published in     
bayesian networks  bayesian networks are useful models in representing and learning
complex stochastic relationships between interacting variables and their probabilistic
nature is capable of modeling the noise that is inherent in biological data  a bayesian
network is a dag consisted of two parts    the structure or the directed edges that
encode the causal relations and conditional independencies between the variables     the
local parameters or the distribution function and parameters that encode the distribution
of a child value given its parents  cpds   bayesian networks can include continuous and
discrete variables as vertices      in this project i have focused on the discrete value case 
one of the most challenging tasks relating to bayesian networks is to infer the graph
structure from experimental data  finding the most probable structure is a np hard

 

fiproblem in the complete data case  in complete data imposes both foundational problems
as well as computational complexities in the already challenging task of structure
learning  we assume that the data is missing at random  mar assumption  to simplify
the missing data likelihood function  in this case the likelihood function is composed of
different likelihood functions each for one completion of the missing values  we have an
exponential number of completing these values and in the worst case each of them will
contribute a different mode to the overall likelihood 
l     d     p  d  h     
h

as a result we lose the unimodality of the likelihood function  the closed form
representation and its decomposition to product of parameter likelihoods     
that is why we try to convert the missing data problem to a complete data problem first
by guessing the missing values using the observed variables 
the most referenced algorithm for structure learning with missing data is the structural
em  sem  proposed by nir friedman  in sem we start with a random structure and its
parameters  we run parametric em on it to impute the missing values and update its
parameters  then we use this completed data to score the next possible graphs that can be
reached with local changes to the current graph and for each of the candidates we use the
ml parameter assignment  after several updates to the structure we run em again to recalculate the missing values  one problem with sem apart from its slow convergence is
its stickiness  since we update the graph based on one possible imputation of the data it
can easily get trapped in local minima  one might try different initializations and multiple
restarts but it becomes intractable when we have a large number of missing variables 
also the greedy search in the graph space is not the best one can do especially as the
number of graph vertices grows  we try to develop an algorithm that finds the best
possible structures based on multiple completions of the data to escape local minima 
also unless one has a huge number of completed data cases it is impossible to learn a one
best scoring graph that is why we will infer a data base of probable graphs in each
iteration and use all of them to infer the missing values  so i will discuss the algorithm in
two steps     learning the probable graphs given the multiple versions of the complete
data     imputing the missing values given the graph data base 
learning the probable graphs  learning is usually done by scoring the candidate
graphs  different scoring metrics have been developed such as bic  ml and bayesian
score  these different metrics all prefer the more probable graphs given the data and
usually penalize the more complex graph to avoid overfitting 
i used bayesian score in the implementation 

p  g   d   p   d   g   p  g  
   

p   d   g      p  d   g    p    g  d

the likelihood integral in     has a closed form if we choose the parameters distribution
from the exponential family with conjugate priors  since we are using discrete variables
we choose the multinomial distribution with dirichlet priors 
 

fi ijk   
p    g        ij  
i    j   
k       ijk  
q

n

   

n

q

i   

j   

p  g   d    

r

   ij  
   ij   n ij

ijk

r

   ijk   n ijk  

k   

  ijk  


 

we use the bde metric for the alpha parameters to have score equivalence  the nijks in
    are sufficient statistics that can be calculated by going through all the data cases and
counting the number of times node i has value k while its parents are in the state j 
the space of possible graphs is a super exponential function of number of variables in the
network hence the exhaustive search in this space is computationally intractable if we
have more than say   variables  you can see the growth of the search space by number of
variables in table   
vertices
graphs
therefore people usually use heuristic
search methods to explore the space 
   
 
markov chain monte carlo is a general
     
 
optimization method that performs a
          
  
random walk in the space of networks
           
  
and by applying metropolis hastings
  
           
theorem will ultimately converge to the
  
         
posterior probability  i e  in the mixed
  
           
mcmc chain each network will be
sampled with the frequency proportionate to
table    growth of search space
its probability 
using metropolis hastings rule we accept
the proposed graph g with probability a g  g 
p g     d t  g    g    
in which t g  g  is the transition
p g   d t  g      g  
probability  but instead of running mcmc in the graph space we decided to use the
order space  by order we mean a topological order of the variables of a graph such that
each node is placed after its parents  it has been shown by cooper et al that finding the
probable graphs is no longer np hard if we know the correct ordering of the nodes     
but finding the correct ordering is still a difficult task unless one has enough domain
knowledge  so we score the orders in a similar way we score graphs and try to find the
more probable orders given the data  the mcmc in the order space has been studied by
friedman et al in      using mcmc in the order space rather than the graph space has
several advantages     the order space is a much smaller space and hence easier to
   

a g    g      

o   n log n  

  n   

explore  
vs   
     we can have more efficient local moves such as
swapping the place of two nodes     the transition probabilities need to be calculated in
the graph space based on all the possible dags that can be reached by applying the local
change operators to the current and proposed graphs but one can assume the transition
probabilities to be equal in the order space and avoid the expensive acyclicity checks   

 

fiwe can use the domain knowledge on ordering of the variables to define priors on the
orders 
using the dynamic programming technique we can compute the order score efficiently
    
   

n

n

p p  d     p g   d    score  x i   pai   d    
gp

gp i  

 score  x   pa   d 
i

i

i   paipap

to compute this score efficiently one can make a database of pre computed local scores
indexed by the parent set for each node      in this data base we will list the score of all
possible families for the nodes  in order to score a particular order we only accumulate
the entries that have a compatible parent set  and finally we multiply the nodes total
scores to have the order score  in case of missing data and having multiple imputations
we can score the order given each of the completed data bases separately and then take
the average of these scores as the final order score 
as the number of graph vertices grows it gets intractable to list all possible parent sets for
nodes  that is why people usually limit the nodes in degree by k  a small number like  
or   as the data base grows polynomially with the k  in order to have a more informative
way to list the parent sets and also allow some larger parent sets for some nodes of the
graph i used the regression tree idea  we can use the regression tree modeling technique
to find the most informative nodes for each node and list all subsets of those nodes as a
possible parent set  i ran the regression tree algorithm using the stop criteria of having
less than    data points assigned to each node trying to minimize the error

  c
 yi in a greedy way for a data base of      data
c i   
cleaves ic
cases in the network studied in sachs et al paper      you can see the results in table  
that shows regression tree has captured most of the relevant parents for each node in the
network in figure    the numbers in the table indicate at what depth of the tree the nodes
branch out and the lower numbers indicate more informative nodes  this is not the
perfect method to find the parent set as it does not capture the causal relations but can be
useful along with the nave way of listing all possible size k parent sets 
to help mixing of the mcmc chain i used a more complex mcmc method called
parallel tempering  in parallel tempering we have several mcmc chains each running at
a different temperature  the higher temperature chains accept the bad moves easier
and hence explore the space faster  each chain can get a proposed sample from its
neighbor chain and accept that using the metropolis hastings rules  i implemented the
parallel tempering algorithm for a simple   node network for which i generated
observational and perturbation data  i used the cooper yoo    method for calculating the
scores in case of perturbation  and i was able to capture the high scoring orders 
graph sampler  in the second stage of the algorithm assuming we have captured the
high scoring orders we will sample graphs by sampling the parents of each node based on
the pre computed scores given these orders 
e 

  y y
i

   

  where y    

 

fip  pai   xi    

p  xi   pai   p  pai  
 p  xi   pai   p  pai  
pai p

the orders do not partition the graph space and each graph is compatible with different
number of orders  this will introduce bias in the graph sampling process      to solve
this problem we will maintain a database of the unique graphs that cover most of the
probability space of the high probable orders  we will sample graphs from each order and
store them in the data base until we cover     probability of that order and then we will
go to the next order but we will not restore the repeating graphs  this way we will end up
with a data base of probable graphs that can be used in the next step of the algorithm that
is the missing data imputation 
missing data imputation  we will use gibbs sampling method to infer the missing
values  in gibbs sampling we iteratively fix the values of all data cases instead of one
data case  we use the fixed data cases to learn the parameters of the graph and then we
use the partial observed case to run bayesian network message passing algorithm     and
complete that partially observer data case  since we are having a data base of graphs
instead of one graph we need to run gibbs sampler in all of them and infer the missing
data using the probability that is the average probability of all the graphs  then we pick
another case and re assign its values and we repeat this until convergence  we will run
the gibbs sampler multiple times on the data to have multiple imputations and will use all
of those to compute an average score in the order scoring phase in the mcmc order
sampler  figure   shows the steps of the algorithm 

raf
raf
mek

mek

plcg

 

predictors

pip 

erk

pka

pkc

p  

 

 

 

 

 

pip 

jnk

 
 

pip 

 

 

 

 

 

 

 

erk

 

 

akt

 

pka

 

 

pkc

 

 

p  

 

jnk

akt

 

plcg

figure    network of    proteins in t cell

pip 

 

 

 

 

 

 
 

 

 

 

 

 
 

 

 

 

 

 

 

 

 

 

 

 

table     list of candidate
parent sets

 

figraph
data base

graph sampler

gibbs sampling
and bn message passing

order
data base

multiple
versions
of
complete data

mcmc in order space

figure    algorithm steps

acknowledgement
i have started to work on this project this quarter under supervision of my advisor
professor wing h  wong 
references 
   karen sachs  omar perez  dana peer  douglas a  lauffenburger  garry p 
nolan  causal protein signaling networks derived from multiparameter
single cell data  science vol          
   david heckerman  a tutorial on learning with bayesian networks      
   daphne koller  nir friedman  probabilistic models in artificial intelligence 
    
   nir friedman  daphne koller being bayesian about network structures 
proceedings of the   th annual conference on uncertainty in ai  uai   pp               
   gregory e cooper  edward herskovits  a bayesian method for the induction of
probabilistic networks from data  machine learning                 
   byron ellis  wing h  wong  learning bayesian network structures from
experimental data  to be published     
   gregory e cooper  changwon yoo causal discovery from a mixture of
experimental and observational data  proceedings of uncertainty in artificial
intelligence  p             
   j  pearl  probabilistic reasoning in intelligent systems  networks of plausible
inference  morgan kaufmann  san mateo  ca      

 

fi