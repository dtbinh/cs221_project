rahul choudhury  stanford id          

cs    machine learning final project report

segmenting descending aorta using machine learning
   introduction
atherosclerosis is one of primary causes of adverse cardiac events today  plaque  a mixture of calcium  cholesterol
fibrin and other substances accumulated in the vessel lumen causes stenosis or occlusion of the vessel  depending
on the location  atherosclerosis may result in strokes  heart attacks  aneurysms  and peripheral artery occlusion
diseases 
the descending aorta is one of the most important vessel structures in the body  it is the largest artery in the body
that runs down through the chest and the abdomen  the descending aorta begins below the arch of the aorta and
ends by splitting into two great arteries  the common iliac arteries   which go to the legs  it has been observed that
atherosclerosis of the descending aorta is a useful predictor of cardiovascular events    
with the advent of multi detector ct  mdct  modality  computer tomography angiography  cta  is routinely used
to image aortic vessels  cta is ideal for preoperative evaluation of thoracic and abdominal aortic aneurysms  as it
demonstrates their position  extent  and relationships to the renal and iliac arteries  ct is especially advantageous as
it acquires information of the whole  d volumetric data with high resolution rather than  d projections in conventional
angiography 
geometric processing of ct vessel data has become increasingly more important for visualization  diagnosis and
quantification of vascular diseases  the first step towards building patient specific vessel geometry is reliable
segmentation  however manual segmentation of a volumetric vessel is a very tedious task  the problem is
exacerbated by the fact that cta produces a huge number of tomographic slices of the aortic vessel  it has been
shown in past studies that manual segmentations vary significantly between experts as well as when an expert
segments the same image at different times    
the problems mentioned above underscore the need for reproducible and accurate automatic segmentation methods
for the descending aorta in ct datasets  the aorta is complex in shape and appearance and varies significantly
across individuals  in addition poor image contrast  noise  and missing or diffuse boundaries add to the complexity of
the segmentation task  the goal of this project is to use machine learning algorithm s  to build a probabilistic model 
which can enable automatic segmentation of descending aorta in the ct volumetric datasets 

   methodology
the segmentation approach is based on the observation that blood filled regions are usually more easily recognizable
in contrast enhanced ct images     if we can isolate the blood filled regions from the non blood filled ones  the task
of detecting the descending aorta becomes much simpler  based on this observation  a machine learning algorithm is
first used to estimate the probability distribution of three main classes of volumetric data  we then find the region filled
with blood  which includes aorta  pulmonary trunk  heart chambers  and coronary arteries  using a bayesian
classification  we finally apply a level set based active contour model to extract the descending aorta and obtain a  d
geometric model  all these steps of the methodology are explained in detail below 
the following notations have been used in describing the methodology 

i is the input image

n is the number of voxels in i

m is the number of classes of voxels

s    s      s  n   is a set of integer indices into i



y    ys   s  s   is the set of intensities of i  ys is the observed intensity at the s th voxel of i
wsc is a binary indicator variable that indicates the membership of a voxel s to class j  wsc    if voxel s
belongs to class j otherwise it is   



   

ws is an m dimensional indicator column vector whose c th component is the indicator variable wsc
step    estimating the probability densities

we assume that there are three classes of voxels in the ct dataset  a  blood filled regions  b  myocardium  c 
lung  given a set of unlabeled training ct slices  we model the volume as a mixture of three gaussians  if we

 

firahul choudhury  stanford id          

cs    machine learning

know the mean  c and standard deviation  c for each class where

c   blood filled regions  myocardium 

lung   we can estimate the probability that the intensity of a voxel given that it belongs to a specific class  in other
words we can find out

p  ys   v   wsc       

 

 c  

exp 

 v   c    
  c

 

two separate algorithms  k means and expectation maximization  em  have been implemented to estimate the
densities of the gaussians  these two methods give us the mean

 c and the standard deviation  c for each

class  step   of the methodology is executed offline on unlabeled training data once and the results  estimated
densities  are stored for online usage 

unlabeled training slice

output of em

figure    result from em

   

step    calculating the posterior probability using bayes rule

we can then use bayes rule to find out the probability that a voxel belongs to a specific class c given its voxel
intensity  thus

p  ys   v   wsc   p  c 
  here we assume that the prior data p  c   is
p  ys   v   wsc   p  c  

p  wsc   ys   v   
c

uniformly distributed across all three classes  the classification of the voxels is then obtained by maximum a
posteriori  map  estimation  in other words

wsj     if j   arg max p wsc   ys     here
c

p  wsc   ys   represents the result of applying an anisotropic diffusion filter on the posterior p  wsc   ys   before
doing the map estimation  a gradient based anisotropic diffusion filter is used to reduce noise  or unwanted
detail  in the posterior probabilities while preserving specific image features 

   

step    extracting the aorta using level set based active contour

once we have classified each voxel as belonging to one of the three classes  we turn all non blood filled voxels
to zero and assign a non zero value to all the blood filled voxels  now our job is to extract the descending aorta
from these blood filled voxels and discard other regions  such as coronary arteries  pulmonary trunks  heartchambers etc   we employ a level set based active contour model to detect the descending aorta  a detailed
analysis of the level set model is available in literature     below we give a brief overview of the model 
the paradigm of the level set is that it is a numerical method for tracking the evolution of contours and surfaces 
instead of manipulating the contour directly  the contour is embedded as the zero level set of a higher
dimensional function called the level set function    x   t     the level set function is then evolved under the

 

firahul choudhury  stanford id          

cs    machine learning

control of a differential equation  at any time  the evolving contour can be obtained by extracting the zero levelset    x    t         x   t        from the output  the main advantage of using level sets is that arbitrarily
complex shapes can be modeled and topological changes such as merging and splitting are handled implicitly 
the governing level set equation is

d
   a  x     p  x     z   x  
dt
and z is a spatial modifier for the mean curvature

where a is an advection term  p is a propagation term

   the scalar coefficients    

and



weight the relative

influence of each of the terms on the movement of the interface 

   implementation
training
ct volume

input
ct
volume

expectation
maximization

density
functions

anisotropic
diffusion
filter

bayesian
classifier

binary
threshold

binary
edge
image

seeds
fast
marching

input
level set

distance

inflation
strength

length
penalty

geodesic
active
contour

output
level set

binary
threshold

segmented
binary
image

figure    implementation pipeline
the above diagram presents how the methodology explained in section   was implemented using a pipeline
approach  first we apply expectation maximization algorithm to the unlabeled training ct dataset to find the
density functions of all the three groups of voxels mentioned earlier  these density functions are fed to the
bayesian classifier to classify the test ct volume  however before applying the bayesian classifier  we first apply
an anisotropic diffusion filter to smoothen the test ct volume while preserving the edge information  the
smoothened image is passed as the input to a bayesian classifier to classify each voxel in one of three
categories  this classified volume is then passed to a binary threshold filter  which sets all the non blood filled
voxels to zero and assigns all the blood filled voxels to the same non zero number 
at this point we are ready to apply the geodesic active contour filter  this filter expects two inputs  the first is an
initial level set and the second input is a feature image  the initial level set is computed by a fast marching
filter  a set of user provided seeds is passed to a fast marching image filter in order to compute a distance
map  a constant value is subtracted from this map in order to obtain a level set in which the zero set represents
the initial contour  this level set as well the output of the bayesian classifier are passed as inputs to the
geodesic active contour level set image filter  finally  the level set generated by the geodesic active contour
filter is passed to a binary threshold image filter in order to produce a binary mask representing the segmented
object  for the geodesic active contour filter  several scaling parameters are used to trade off between the
propagation  inflation   the curvature  smoothing  and the advection terms 
microsofts  net technology and c    c  language on windows xp platform have been chosen to implement
the methodology described above  mergecom  toolkit from merge emed has been used to load dicom
datasets     itk has been used for low level image processing tasks   

   results
as explained in the methodology section  we run two different unsupervised learning algorithms to compute
density functions of three groups of voxels  blood filled regions  myocardium and lung   the training data for k 

 

firahul choudhury  stanford id          

cs    machine learning

means and em is a ct volume of heart acquired with contrast agent from a single patient  there are    slices in
the volume  each slice is     by      each pixel is represented by    bits  a modality lookup transformation was
applied to the original voxel data to convert them to hounsfield unit 
we found that both k means and em are dependent on the order in which the training image slices are
presented  as well as on initialization points  we also found that as the bone  including rib cages  voxels have a
very similar intensity profile as that of blood filled regions  both em and k means usually cluster bones and
blood filled regions under one cluster 
for this application  we ended up using the density function results from em as the k means algorithm implicitly
assumes that the data points in each cluster are spherically distributed around the center  less restrictively  the
em algorithm assumes that the data points in each cluster have a multidimensional gaussian distribution with a
covariance matrix that may or may not be fixed  or shared  we run em using different initial values and then used
average means and standard deviations for three classes as inputs to the bayesian classifier  the final mean
and variance for all the three density functions computed by em are mentioned below  in hounsfield unit  

myocardium
lung
blood filled region

mean
      
       
      

variance
        
       
        

the bayesian classification and geodesic active contour parts of the algorithm were executed online  we tested
this part on a  d ct data set of cardiac images  the images were acquired using a siemens scanner with a slice
spacing of   mm and an in plane resolution of       mm  for the purpose of establishing ground truth we
manually identified the contour as well as marked all the pixels belonging to the descending aorta on each of the
test slices  because we applied the geodesic active contour to a binary volume  we found that the overall
processing time for each slice is quite reasonable      seconds   below is an example of a single test slice and
its corresponding segmentation result 

original test slice

segmented slice

figure    example of segmented slice
several problems were discovered during the implementation of the geodesic active contour algorithm 


we found that the fast marching filter is highly susceptible to the initial seed point as well as to the
distance parameter used to specify distance from the seed point to the input level set  in the beginning
we supplied only a single seed point  roughly at the center slice in the descending aorta  whose
distance from the input level set contour was set to      with those parameters  we found that the
algorithm provides best sensitivity at that center slice  but the sensitivity gradually decreases as we
move away from the center to the two extreme ends of the aorta  to improve the overall sensitivity of
the algorithm  we initialized the fast marching level set algorithm with multiple equidistant seed points
in the volume  a distance of      was used for each one of the seed points  below is a graph for the
improved sensitivity plotted against slice position for a test volume of    slices 

 

firahul choudhury  stanford id          

cs    machine learning

   

sensitivity

   
  
  
  
  
 
                                                 
slice number

figure    sensitivity for each slice in the test data set



we also found that the propagating surface leaks into the coronary artery  at the location where the left
main  lm  coronary artery bifurcates from the aorta  we had to prevent this leakage by adjusting the
weight between the curvature and the inflationary constant  one could also cut the linking sites of the
left main coronary artery and the aorta in a few slices of the data where the linkage exists  and evolve
the surface solely inside the descending aorta  however this technique necessitates manual
intervention 

   conclusions and future work
we proposed a novel approach to segmenting and reconstructing the human descending aorta using machine
learning  this method was tested on a data set of cardiac ct images  the application of our method results in a
reconstructed geometric model of the descending aorta  which provides an improved comprehensive view of the
vessels  this could potentially assist clinicians in achieving more accurate clinical diagnoses of atherosclerotic
diseases in the descending aorta  future work would involve the following items 

currently the user supplies the initial seed points for the fast marching filter  we would like to
automate the seed point selection process 

for the geodesic active contour level set filter  currently the scaling parameters are found out by trial
and error  in future we would like to learn these parameters by applying an appropriate machinelearning algorithm 

we would like to test the overall algorithm for larger test datasets

we would like to measure clinically significant parameters from the descending aorta model  such as
the centerline and diameters of vessels 

   references
     albert varga  noemi gruber  tams forster  gyrgyi piros  klmn havasi  va jebelovszki  and miklos
csandy  atherosclerosis of the descending aorta predicts cardiovascular events  a transesophageal
echocardiography study
     warfield  s   winalski  c   jolesz  f   and kikinis  r       automatic segmentation of mri of the knee  in
ismrm sixth scientific meeting and exhibition  page     
     j a  sethian  level set methods and fast marching methods  cambridge university press      
     http   www merge com
     http   www itk org
     yang y   tannenbaum  a   giddens  d        knowledge based  d segmentation   reconstruction of
coronary arteries using ct images  proceedings of the   th annual conference of the ieee embs 

 

fi