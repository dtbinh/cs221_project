clustering wordnet senses utilizing modified and novel similarity metrics
cs    final project report
christopher thad hughes  sushant prakash
 the neural activation profile algorithm  which returned some of the more interesting results  was not completed in time
for the poster presentation  

introduction
we approach the problem of clustering senses in princeton s wordnet  fellbaum        a manually created
dictionary thesaurus which attempts to model the structure underlying human concepts  a synset  the fundamental unit
in wordnet  is represented by a group of synonyms and a gloss definition  and is connected through a variety of semantic
links  such as hypernyms  type of  or meronyms  part of   to other synsets  a particular word is associated with one or
more synsets  each representing a particular sense of the word  while this electronic database provides an inventory with
which to do word sense disambiguation  the fine grainedness of the senses  which sometimes even humans have
trouble distinguishing between   has posed a problem in achieving reasonable performance 
our goal is to learn to automatically cluster senses of a given word that are sufficiently close in meaning  we pose the
problem as the decision  given two synsets  of whether or not those synsets belong in the same cluster  we train and test
a binary logistic regression classifier on a set of hand labeled clustering data from the gale ontonotes project  using a
variety of synset similarity metrics as features  we used     of the nouns and verbs for training  and put the other    
of nouns and verbs aside for testing  to obtain data points  we looked at all possible pairs of senses for each given word 
for our train set  we had      negative      positive noun pairs  and      negative      positive verb pairs  for test  we
had     negative     positive noun pairs  and     negative      positive verb pairs 

previous work
researchers have used a variety of measures on the information contained within wordnet to determine similarity  one
simple heuristic is just the shortest path between the two synsets following semantic links  slightly more complicated
measures  such as  wu and palmer  take into account the least common subsumer  lcs  of the synsets and their
relative depths  first introduced by  resnik   several measures  including  lin  and  jiang and conrath   estimate
probabilities over concepts from a standard corpus  and then compare the information content of the two synsets and
their lcs 
while all of these rely on semantic links  a very important source of information  and the one most useful to humans
performing the task  is the similarities in the glosses and examples of the synsets  the lesk measure attempts to deal
with this by counting the word overlaps  banerjee and pedersen        modified the measure by also counting word
overlaps present between synsets semantically connected to the original two  i e  the hypernyms  the hyponyms  and so
forth  even with this modification  the measure remains somewhat crude and calls for improvement 

our features
lesk overlap generalization  log 
one obvious shortcoming of the lesk measure is its inability to deal with synonyms  since it superficially measures
overlap with word equality  glosses that express the same concept  but with different synonymic words  will appear to
have low similarity  having sense tagged wordnet glosses would help immensely with this problem  but since that is
not the case we must consider all combinations of the senses of the two words in question  given a set s of similarity
measures between synsets  we calculate a similarity vector vij for each pair  i j  where i exists in senses w    j exists in
senses w    we then keep the vector vmax  which is the vector that maximizes the most entries  this corresponds to an
optimistic attitude for the relatedness between the two words  after doing this for each pair of words  we return the top
n similarity vectors as the pairwise features for the synset 
this measure provides a great deal of flexibility over the previous lesk measure  as it not only allows for high similarity
between synonyms  but also takes into account soft relations between words that almost or somewhat mean the same
thing  moreover  as s  the set of synset similarity metrics is added to or improved  so will the performance of this
measure increase 

filesk with information overlap  lio 
two more shortcomings of the lesk measure are clearly evident  firstly  any overlap is treated the same  no matter the
word s  contained in the overlap  even if the standard stop words  the  of  etc   are filtered out  some word overlaps
will just be more common than others  and will thus not imply as great a deal of similarity 
the second shortcoming involves the length of the overlap  there is an intuition that a long overlap should be weighted
very highly  arguably even more than the sum of several short overlaps  in the lesk formulation  this intuition is
implemented by squaring the overlap length before adding it to the cumulative score  this decision  however  seems
quite arbitrary as it does not have any theoretical foundation beyond that of the original  vague intuition 
to deal with both of these problems  we propose that a language model be built to estimate the probabilities of sequences
of words  we can then weight an overlap w  w    wn with the information of that sequence  defined as  log  p
  w  w    wn      notice that this will give less weight to common sequences of words  which would be more likely to
overlap   and more weight to rare sequences  this method also captures the intuition behind higher weighting of long
sequences  as a sequence of several words will naturally have a much lower probability than the occurrence of a single
word  for our experiments we trained a unigram  bigram  and trigram language model on all the wordnet glosses and
examples to use for this method 

relation weight learning  rwl 
with banerjee and pedersen s modification to the lesk algorithm  overlaps in the synsets of the hypernyms  meronyms 
and other semantically related nodes  are also taken into account  but then these are summed up either uniformly or by
hand chosen weights  we modified their wordnet similarity package to output each of the measures separately so that
we could perform gradient descent to automatically learn the weightings of the different relations  we applied this
technique not only to the similarity  lesk measure  but also the similarity  vector pairs measure which also looks at the
similarity of extended glosses  consequently a significant bug in vector pairs was found and reported  siddharth
patwardhan put out a new release the next day  

classifier results
train max f score

test f score

nouns  original lesk measure

      

      

nouns  lesk with rwl

      

      

nouns  lesk with information overlap

      

      

nouns  log with original metrics

      

      

nouns  rwl with original metrics

      

      

nouns  all original similarity metrics

      

      

verbs  original lesk measure

      

      

verbs  lesk with rwl

      

      

verbs  lesk with information overlap

      

      

verbs  log with original metrics

      

      

verbs  rwl with original metrics

      

      

verbs  all original similarity metrics

      

      

looking at the above table  we immediately see an anomaly in the f scores for verbs  the test set performance is better
than the train set performance  we are not sure why this is the case  but we suspect it has to do with the fact that we have
relatively few data points  and thus may not be getting a representative sample in either or both sets  in general we can
see that the verbs have a higher score than the nouns  most likely caused by the fact that they have a higher rate of
clustering 
modifying the lesk measure with the information overlap  lio  did not help training performance much and slightly
hurt test performance in both cases  one reason for this may be that the language model was not trained well enough
from only the wordnet glosses  a supplemental corpus could be used to obtain more data  in addition  a more complex

filanguage model  incorporating higher n grams or utilizing parses could help  it is a bit of a circular problem in the sense
that a proper lexical resource would better allow computers to understand language  but the computers need to
understand the language in the gloss definitions to build the proper lexical resource 
lesk with learned weights for the different relations  rwl  turned out to increase training performance a notable amount
for nouns  but only slightly helped the test performance  and hurt a little bit for the verbs  disappointingly  when we
substituted this modified lesk measure into the ensemble of metrics  performance actually decreased  a likely cause
being overfitting of the training data 
generalizing the overlaps for lesk  log  seems promising considering the improved performance obtained on nouns 
however  it is the most computationally intensive of our measures due to it computing similarities for all pairs of senses
for all pairs of words between the glosses 

neural activation profile  nap 
late in the project  we had the insight that a concept is intricately connected to many other concepts  and that wordnet
models at least some of this structure using a graph of synsets and the semantic edges between them  interestingly  most
of the work we have seen on measuring word sense similarity  including many of the measures described above  assumes
 or even imposes  a tree like structure on the synset graph  including a root node and a strong inheritance hierarchy  our
insight was that moving away from the tree mentality and towards a true graph or network might be useful 
we were partially inspired by our simple understanding of the neurons in the brain  which seem to function as at network
that passes energy between the nodes in a complex feedback loop  we imagined interpreting wordnet as a similar
structure  with the synset nodes representing neurons and the edges between them representing synapses 
with this analogy in mind  we developed the concept of a neural activation profile  nap   an nap is a vector of real
numbers representing the activation energy for every single node in the wordnet graph  for any synset  we can compute
the synsets nap by initializing the energy of each node in the graph to zero  and then assigning a positive  unity 
activation energy to that synset  we then iteratively spread the energy from the initial node throughout the entire
wordnet graph  during each iteration  each node that contains energy keeps some fraction of its energy for itself  and
passes the rest of its energy to its neighbors   this process of iteratively passing information to neighbors is realized
efficiently using an algorithm similar to the bellman ford shortest path algorithm   we say the algorithm has converged
with the angle between two successive nap vectors  energies as each node  dips below a threshold  even with a
wordnet graph of over         nodes  convergence typically took between       iterations  in total lasting less than a
second in our java implementation  the edges over which the energy is passed can be arbitrarily weighted  we used
unity weights for all edge types except antonym edges  which were weighted with negative one  this means that
antonyms of the starting synset can receive negative energy and pass it onward 
the figures below  generated by producing graphviz  dot files from our java program  show the nodes with the highest
activation energy over the first few iterations of nap computation for the word sense power  powerfulness  red color
is used for positive energy  blue for negative  and intensity of the color shows the amount of energy present at the node 
only the    nodes with the highest energy are displayed  although many thousands of others have non zero energy after
the first iterations 

iteration  
iteration  

iteration  

iteration  

fiiteration  

iteration   

iteration  
iteration   

once we have computed a nap vector for two different word senses  we can compare the senses by computing the
cosine similarity between the nap vectors  the advantage of using this method  as opposed to previous methods that
examine only a single path between senses  is that many different paths of influence between nodes are taken into
account 

neural activation profile results
the classification results based on nap similarity are interesting in that they reveal ambiguities in the word sense
clustering task  many of the false positives generated by the nap similarity algorithm show examples in which the
hand labeled test data has been mislabeled  in fact  the top    word sense pairs predicted to be most similar by the nap
similarity are hand labeled as not being similar senses  although we believe that some of them are  see appendix   the
maximum f score achieved for nouns using only the nap similarity was      for nouns and       for verbs  precision
and recall curves for the nap similarity are shown below 

nap precision recall
   

   

   

   
v erb nap
noun nap
   

   

   

 
 

   

   

   

   

 

r e c all

the tables in appendix a show the best examples of false positives  true positives  false negatives and true negatives
computed with the nap similarity measure  interestingly  the false positives comprised the word sense pairs with the
overall highest nap scores  all of the examples below had an angle of zero degrees between their nap vectors  to
within the precision of the machine  however  one has to wonder if a basketball center and a hockey center
represent two different senses of the word center  similar reasoning would seem to imply that wheat bread and
white bread are two different uses of the word bread  the same can be said for many of the rows in the table below 
however  the rent row in the table below does actually represent two different  and opposing  senses of the word
rent  note that in the focus row  the implied row was also present 

fithe table below lists the word sense pairs with the highest nap similarities that were marked in the hand labeled data as
being the same word sense  we argue that the similarity in sense between these sense pairs and most of the sense pairs
from the false positives table above are not that different  meaning that many of the false positives are actually
mislabeled data  again  all of the sense pairs in the table below had a difference of zero degrees between their nap
vectors 
the table below shows the sense pairs that were labeled as being the same sense  but that had the lowest nap similarity
scores  we realize that all of theses sense groupings are appropriate that that the computed nap similarity does not
reflect the similarity of the senses  these truly are false negatives 
the senses pairs below are those with the highest difference in nap similarity that were also marked in the hand labeled
data as not being the same sense of the word in question  we are quite happy that the nap similarity measures for all
these sense pairs are different  because they do indeed represent different word senses  one of the most interesting
examples below is the sense pair for wear  which contains two senses with opposite meaning 
interestingly  the sense pairs with the highest nap differences are all verb sense pairs  we hypothesize that this is
because wordnet imposes a tree like hypernym hierarchy on all nouns  having them all descend from a single root node 
this means that activation energy trickles up to the root nodes in the noun hierarchy and then back down again  where
some of it moves to completely unrelated or even antonym nodes  the verb hypernym hierarchy is much weaker  and
because the verbs dont all join at a single root  the nap vectors for unrelated nodes dont loose energy to the upper
nodes in the hierarchy 

nap analysis
one of the most obvious facts about the performance of the nap sense clustering is that the recall seems to be poor 
most of the false positive assertions of similarity are somewhat excusable  however  the false negatives are clearly
examples of word sense similarity that was missed by the nap similarity measure 

future work
there are severals ways to try to improve on this work  including the better language model for lio and efficiency
speed ups for log  for nap  one things we would like to try is to automatically learn edge weights for propagating
energy  while the function may not be differentiable with respect to those parameters  eliminating the possibility of
gradient descent or newton s method  approximating those algorithms with the secant method could provide better
results 

acknowledgements

thanks to rion snow for an initial discussion about the project and professor dan jurafsky for providing the handlabeled data 

references
c  fellbaum        wordnet  an electronic lexical database  cambridge  ma  mit press 
t  chklovski and r  mihalcea        exploiting agreement and disagreement of human annotators for word sense
disambiguation  proc  of ranlp      
mihalcea  r    d  moldovan        automatic generation of a coarse grained wordnet  in  n wordnet   pp         
pedersen  t   patwardhan s   and michelizzi j        wordnet  similarity  measure the relatedness of concepts 
dem  of the human language technology conf       
resnik  p        semantic similarity in a taxonomy  an information based measure and its application to problems of
ambiguity in natural language  jair     pp         
lesk  m        automatic sense disambiguation using machine readable dictionaries  how to tell a pine cone from an ice
cream cone  proc  of sigdoc      

fiappendix a  nap performance examples
nap false positives
sense  
center    
field   

focus    focal
point    nidus  
acknowledge   

rent     lease  
remember   
report   
describe    
account   
report   

sense   gloss
the position on a hockey team of the
player who participates in the face off
at the beginning of the game
somewhere  away from a studio or
office or library or laboratory  where
practical work is done or data is
collected   anthropologists do much of
their work in the field 
a central point or locus of an infection
in an organism   the focus of
infection 
accept as legally binding and valid 
 acknowledge the deed 

sense  
center    

let for money   we rented our
apartment to friends while we were
abroad 
mention favourably  as in prayer 
 remember me in your prayers 

rent    hire    
charter    
lease   
commend    
remember  

to give an account or representation of
in words   discreet italian police
described it in a manner typically
continental 
make known to the authorities   one
student reported the other to the
principal 

report   

field    field of
operation    line
of business  

sense   gloss
a position on a basketball team of
the player who participates in the
jump that starts the game
a particular kind of commercial
enterprise   they are outstanding in
their field 

focus  

a fixed reference point on the
concave side of a conic section

acknowledge    
recognize    
recognise    
know   

accept  someone  to be what is
claimed or accept his power and
authority   the crown prince was
acknowledged as the true heir to the
throne    we do not recognize your
gods 
hold under a lease or rental
agreement  of goods and services
mention as by way of greeting or to
indicate friendship   remember me
to your wife 
make known to the authorities   one
student reported the other to the
principal 

report     
cover   

be responsible for reporting the
details of  as in journalism   snow
reported on china in the      s  
 the cub reporter covered new york
city 

sense   gloss
the compartment that is suspended
from an airship and that carries
personnel and the cargo and the power
plant
where passengers ride up and down 
 the car was on the top floor 

sense  
cable car    car   

unite or bring into contact or bring
together the edges of   close the
circuit    close a wound    close a
book    close up an umbrella 
mark with diacritics   point the letter 
mark with diacritics   point the letter 

close     

sense   gloss
a conveyance for passengers or
freight on a cable railway   they
took a cable car to the top of the
mountain 
a conveyance for passengers or
freight on a cable railway   they
took a cable car to the top of the
mountain 
bring together all the elements or
parts of   management closed ranks 

report   
describe    
account   

to give an account or representation of
in words   discreet italian police
described it in a manner typically
continental 

report     
cover   

hang glide   
soar   

fly by means of a hang glider

sailplane   
soar   

nap true positives
sense  
car    
gondola   
car     elevator
car  
close up    
close     
point    
point    

cable car    car   

point    
point    

mark  hebrew words  with diacritics
mark  a psalm text  to indicate the
points at which the music changes
be responsible for reporting the
details of  as in journalism   snow
reported on china in the      s  
 the cub reporter covered new york
city 
fly a plane without an engine

fiswitch over   
switch    
exchange   

change over  change around  as to a
new order or sequence

mailman   
postman    mail
carrier    letter
carrier   
carrier   

a man who delivers the mail

interchange   
tack     switch   
alternate    
flip     flip flop  
carrier    
newsboy  

reverse  a direction  attitude  or
course of action 
a boy who delivers newspapers

nap false negatives
sense  

sense   gloss

sense  

sense   gloss

focus  

cause to converge on or
toward a central point 
 focus the light on this
image 
harmony of people s
opinions or actions or
characters   the two parties
were in agreement 
put  something somewhere 
firmly   she posited her hand
on his shoulder    deposit
the suitcase on the bench  
 fix your eyes on this spot 
compatibility of
observations   there was no
agreement between theory
and measurement    the
results of two tests were in
correspondence 
cause to be firmly attached 
 fasten the lock onto the
door    she fixed her gaze on
the man 

focus    
focalize    
focalise    
sharpen   
agreement  

put  an image  into focus   please
focus the image  we cannot enjoy
the movie 
the verbal act of agreeing

     

fixate     fix  

make fixed  stable or stationary 
 let s fix the picture to the frame 

     

agreement    
accord  

harmony of people s opinions or
actions or characters   the two
parties were in agreement 

     

situate    fix     
posit   
deposit   

    

become one  become
integrated   the students at
this school integrate
immediately  despite their
different backgrounds 
take on titles  offices  duties 
responsibilities   when will
the new president assume
office  

integrate   
incorporate  

put  something somewhere 
firmly   she posited her hand on
his shoulder    deposit the
suitcase on the bench    fix your
eyes on this spot 
make into a whole or make part of
a whole   she incorporated his
suggestions into her proposal 

     

peace  

the state prevailing during
the absence of war

peace  

the state prevailing during
the absence of war

peace    peace
treaty   
pacification  
peace   

take on a certain form  attribute 
or aspect   his voice took on a
sad tone    the story took a new
turn    he adopted an air of
superiority    she assumed
strange manners    the gods
assume human or animal form in
these fables 
a treaty to cease hostilities   peace
came on november   th 
harmonious relations  freedom
from disputes   the roommates
lived in peace together 

     

nap
angle
     

agreement    
accord  
situate   
fix      posit   
deposit   
agreement   
correspondence  
 

fasten     fix   
secure   

integrate   

assume   
adopt     take
on     take
over   

assume   
acquire   
adopt     take
on    take   

nap
angle
    

     

     

nap true negatives
sense  

sense   gloss

sense  

sense   gloss

head   

form a head or come or grow
to a head   the wheat

head     head
up  

be the first or leading member of
 a group  and excel   this student

ficlose     shut   

rise    lift     
rear  
rise     
jump     climb
up   
scale  

headed early this year 
become closed   the
windows closed with a loud
bang 

rise    lift     
rear  

rise up   the building rose
before them 
rise in rank or status   her
new novel jumped high on
the bestseller list 
measure by or as if by a
scale   this bike scales only
   pounds 
rise up   the building rose
before them 

sharpen    
taper    point   

give a point to   the candles
are tapered 

break    
wear    wear
out    bust    fall
apart  

go to pieces   the lawn
mower finally broke    the
gears wore out    the old
chair finally fell apart
completely 
develop into   this idea will
never amount to anything  
 nothing came of his
grandiose plans 

come       add
up     amount  

close    

move up    
rise   
move up    
rise   
scale   
arise    rise    
uprise     get
up     stand up  
point    
wear    hold
out     endure   

total    number   
add up    
come      
amount   

heads the class 
be priced or listed when trading
stops   the stock market closed
high this friday    my new stocks
closed at     last night 
be promoted  move to a better
position
be promoted  move to a better
position

    

    
    

size or measure according to a
scale   this model must be scaled
down 
rise to one s feet   the audience
got up and applauded 

    

be positionable in a specified
manner   the gun points with
ease 
last and be usable   this dress
wore well for almost ten years 

    

add up in number or quantity 
 the bills amounted to         
 the bill came to        

    

    

    

fi