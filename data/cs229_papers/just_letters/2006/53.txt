matrix factorization for collaborative prediction
alex kleeman
nick hendersen
sylvie denuit
icme

 

introduction

netflix  an online video rental company  recently announced a contest to spur interest in
building better recommendation systems  users of netflix are able to rank movies on an
integer scale from   to    a rating of   indicates that the user hated it  while   indicates
they loved it  the objective of a recommendation system  or collaborative filter  is to
provide users with new recommendations based on past ratings they have made 
there are several methods for approaching this problem  many of which have been
extensivley documented such as k nearest neighbors and mixture of multinomials  we are
particularly interested in exploring different matrix factorization techniques  namely
    fast  maximum margin matrix factorization       
   incremental svd    
   repeated matrix reconstruction

 

background

to support the contest netflix released a large dataset with about     million ratings
collected from about         users regarding        movies     it is useful to think of
the data as a matrix with rows corresponding to users and columns corresponding to
movies  this matrix only has ratings for approximately    of the elements  the rest of
the elements are unknown  the problem is to predict these unknown values based on the
known ratings supplied by the users 
a natural way to solve the recommendation problem is to lump movies into genres
and users into groups  ratings can be predicted based on how certain groups typically
rate certain genres  it can be assumed that there are far fewer groups and genres when
compared to the absolute numbers of unique users and movies in the netflix dataset  low
rank matrix approximations are well suited for this type of problem  they attempt to
decompose the ratings matrix into smaller factors  analogous to genres and user groups  
which can be combined to provide rating predictions for any user movie combination 

 

fi 

fast maximum margin matrix factorization

the fast maximum margin matrix factorization was proposed by srebro et al         
instead of constraining the dimensionality of the factorization x   uv t   srebro et al 
suggested a method that consists in regularizing the factorization by constraining the
norm of u and v where the matrices u and v can be interpreted as follows  the rows of
u take the role of learning feature vectors for each of the rows of the observed matrix 
y   the columns of v t   on the other hand  can be thought of as linear predictors for the
movies  columns of y   
matrices that can be factorized by low frobenius norm are also known as low trace
norm matrices  the trace norm of a matrix x  kxk is equivalent to the sum of the
singular values of x 
this leads to so called soft margin learning  where minimizing the objective function
is a trade off between minimizing the trace norm of x and minimizing the sum of the
square of the errors between the elements of the observed matrix y and its approximation
x   uv t   we therefore obtain following optimization problem 
min kxk  
x

x

 yij  xij   

i js

where s is the set of known entries in y   notice that constraining the trace norm of
x rather than its dimensionality leads to convex optimization problems as opposed to the
non convex problems that occur when considering dimensionality
constraints 


as the trace norm of x can be bounded by    kuk f ro   kv k f ro   we can replace
the complicated non differentiable function kxk by a simple objective function  our
minimization problem now becomes 
min
u v

 

x 
 
yij  ui vjt
kuk f ro   kv k f ro  
 
i js

we will therefore be searching for the optimal pairs of matrices u and v rather than the
optimal approximation matrix x  the optimization of u and v is performed with the
conjugate gradient method 
we implemented the fast maximum margin matrix factorization using a hinge loss
function for the error as described in sebro et al  the data yij we are trying to predict
are ordinal ratings whereas the estimates xij are real valued  we will therefore make use
of thresholds and implement the following optimization problem 
r 



x x 
 
h tijr ir  ui vjt
kuk f ro   kv k f ro   c
u v   
r   ijs

min

 

incremental svd

the singular value decomposition  svd  can be used to compute low rank approximations
to a matrix a  this is done by taking the svd and keeping only the top k singular values
 

fiand the corresponding left and right singular vectors  the result is the closest rank k
matrix to a in the least squares sense  l   norm  
let r represent our ratings matrix with rows corresponding to users and columns
corresponding to movies  a low rank factorization to r can be computed using the known
ratings in r and the incrementally computed predictions to the unknown values     ideally
this low rank approximation would complete the matrix or fill in the unknown values 
predictions could be computed efficiently by taking the appropriate linear combination
of the factors 
the problem is to figure out how to compute this factorization  the standard svd
requires access to the complete matrix  missing values are not allowed  and has time
complexity o m     this is obviously not acceptable for the size and nature of the netflix
database  we need a method of computing or approximating the svd of a matrix that
does not require access to the entire matrix and handles the missing values appropriately 
we opted to use a method of computing the thin  low rank  svd with successive
rank   updates  the thin svd is written a   uv t   if a is m  n  then u is m  r 
 is r  r  and v is n  r  where r  n  m is the rank of the approximation  lets say
we already have a low rank svd which we desire to update with a new movie by adding
a column c  first we must fill in the unknown entries  partition c into c  and c    where
c  contains the known values and c  the unknown  also partition the rows of u into u 
and u  according to c  and c    the unknown values can be predicted using the normal
equations   
  



c    u   u t u  



u t c   

in the implementation this need not be explicitly computed  with the completed
vector c carry out a few steps of modified gram schmidt  v   u t c  u   c  uv  form
the  r      r      matrix
k 

 v
    u  

 

now compute the svd of k and update the left and right singular matrices
k   u v t
 

 

u
u   v    v    v
u  u
  u  
to maintain the same rank  simply remove the smallest singular value from  and
the corresponding columns from u and v    in the bootstrapping process we allow the
rank to increase to the desired value   now the process of computing the svd consists
of cycling through the data matrix and updating the low rank approximation  each step
can be made computationally feasible for large datasets  also  the predictions used to fill
in the unknown values in the incoming column get progressively better as more data has
been evaluated 
 

fi 

repeated matrix reconstruction

the most difficult obstacle in using low rank approximations to predict missing values of
these sparse matrices is dealing with zeros introduced in the matrix due to a user not having rated a particular movie  thus  an ideal matrix factorization method for collaborative
prediction would be able to distinguish between un rated and rated entries when creating
these low rank approximations  although full singular value decomposition does not account for this  several methods can be used to mimimize the effects of missing entries  in
our implemetation we chose to use a zero mean method  in which each of a users ratings
are shifted relative to each movies average rating  leaving any unrated entries at zero 
once the matrix has been shiffted to zero mean  a low rank approximation is made  this
effectively initializes all unrated entries to the movies mean before continuing with the
svd 
once the initial matrix has been formed  the svd is used to form the low rank
approximation  because this approximation is not gaurenteed to preserve the known
ratings  all values in the low rank approximation are then reset to the known values  a
new low rank approximation is then made on this matrix and the procedure continues
until either convergence or a predefined cutoff 
the resulting matrix is now the approximation that will be used for our predictions 
before the matrix can be used it must first be shifted back to the original mean 
note that after the first iteration the matrices in consideration are no longer sparse 
therefore a more efficient method of svd approximation would be needed for a large
dataset  such as netflix  for testing purposes the svd matlab function was used 

 

results

because all published results for the methods we implemented were based on the movielens     data base  we decided to use it as a basis for comparison  this data set contained
         ratings  and was split into two sets  one with         ratings used for training 
and another of         used for testing  the best recorded rmse and mae values for
these tests are found in table   
despite the simplicity of the repeated matrix reconstruction method  it was found
to perform quite well in comparison to the other implemented methods  experimenting
with rank and number of iterations showed that rank    with approximately    iterations
performed best  in an attempt to pinpoint the sources of error  several visualizations of
the error were made  by taking an average of the absolute difference in predictions for
each movie  it was found that movies with fewer known ratings performed considerably
worse than those with a large number of reviews  figure     with this in mind  future
algorithms would likely perform better if they were to use a matrix factorization to fill in
missing values in the denser portions of the matrix and resort to a more versitile method
for the extremely sparse sections 
the fast maximum margin matrix factorization method underperformed our expectations with an rmse of       while this could be due to minor differences in our

 

fiimplementation such as the optimization routine used  our result for the nmae      
was comparable to the values published           which leads us to believe that even if
the published values had been exactly matched  the rmse error still would not have
improved upon the results for repeated matrix reconstruction 
a similar result was found for the iterative svd method  again  the mae values
matched the published values     but the rmse values were worse than the repeated
matrix reconstruction 
rmse
method
fmmmf
    
iterative svd
    
repeated matrix
    

mae
   
   
    

table    rmse and mae on the movielens data set for the three methods tested 

 

acknowledgements

we would like to thank chong do for his help with this project  and for introducing us
to the majority of the papers we used as references 

references
    srebro  n   rennie  j  d  m   and jaakola  t  s  maximum margin matrix factorization  in neural information processing systems  nips           
    rennie  j  d  m  and srebro  n  fast maximum margin matrix factorization for
collaborative prediction  in proceedings of the   nd international conference on
machine learning  icml        
    m  brand  fast online svd revisions for lightweight recommender systems  in proc 
siam international conference on data mining       
    gorrell  g  generalized hebbian algorithm for incremental singular value decomposition in natural language processing      
    the movielens dataset  grouplens research  http   www grouplens org 
    the netflix prize dataset  netflix  inc  http   www netflixprize com 
    m  brand  fast low rank modifications of the thin singular value decomposition  in
linear algebra and its applications      

 

fifig    here we display the distribution of error values from the repeated svd
reconstruction  the x axis corresponds to the error value or difference between
predicted value and true value of an item in the test set  the count shows the number of
times that error value occurred over the entire test set 

fig    here we show the relationship between the rank and error metrics from the
repeated svd reconstruction  rank    provided the best results on the movielens
dataset 

 

fifig    this figure compares the average error with the number of ratings made on a
movie  the x axis indexes the movies  they are sorted in descending order from left to
right  movies on the left have the most ratings in the dataset whereas movies on the
right have the least ratings  this plot shows that the repeated svd reconstruction
performs far better on movies that have many ratings when compared to movies that
have few ratings 

fig    this is the sparsity pattern of the movielens data when the users and movies are
sorted so that the user who made the greatest number of ratings is the top row and the
movie with the most ratings is the leftmost column  the low density part  to the right 
of this figure corresponds with the region of large error in the figure above 

 

fi