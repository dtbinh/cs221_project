yusuf ozuysal
andrew wong

super resolution
 

introduction

super resolution deals with the construction of highresolution images using a set of low resolution images
obtained from a scene with subpixel shifts  these lowresolution images are typically obtained from a jittery
camera source  such as a camera mounted on a vibrating aircraft  or a slowly moving subject  such as a few
frames of a standing person in front of a surveillance
camera  the low resolution images have small translations and rotations from the high resolution reference
image  the problem consists of constructing a model of
linear transformations for each image  and then piecing
together the images to form the high resolution image 
super resolution image construction has applications in
remote sensing and medical imaging  and in scenarios
where directly capturing high resolution images is not
feasible 
in this project  we attempt to solve the super resolution
problem with a map approach  following hardie  et  al
     we construct a model of the low resolution images
and attempt to learn the high resolution images  effects
of model parameters are discussed 

 

model

the initial part of the problem consists of constructing a model relating the high resolution image to the
set of low resolution images  from a training set of
p low resolution images sized m   m    we represent
the kth low resolution image as a vector yk with length
m   m   m  and the high resolution image n   n 
image as a vector z with length n   n   n    lowresolution images are modeled as a linear transformation of the high resolution image followed by a downsampling step with factor l   m   n    m   n    an

additive gaussian noise random variable   n     
accounts for camera noise effects     the linear transformation is 

here wk is an m  n matrix containing the weighted
contribution of each pixel in z to the pixels of yk   wk
is determined by the relative movement of the lowresolution image to the reference image  specified by a
translation parameter vector sk and rotation matrix rk  
and a point spread function that models the diffusion of
light to pixels  to determine wk   we calculate each
weight wijk as the value of a gaussian point spread function between the reference image and the shifted lowresolution image     
wijk   exp 

  vi  uj    
 
 

here the width  of the point spread function determines
how much leakage there is from high resolution pixels
to low resolution pixels   is a constant that characterizes the camera system and should be chosen to match
the type of sensor being used  vi is the pixel location
in the high resolution image and uj is the center of the
point spread function obtained by 
uj   r  vj  v    v   sk
here v is the center of the image  note that if there
are no rotations involved in the construction of lowresolution images  this relationship becomes
uj   vj   sk
given a set of yk we wish to obtain the corresponding
motion parameters s and the high resolution image z  to
simplify notation  we will construct y  a column vector
containing all k low resolution vectors  similarly  we
denote w as all of the wk stacked on top of each other 
allowing us to write our model as y   wz    with 
now a pm  dimensional vector  we formulate the map
estimate of s and z 
z  s   arg max pr y z  s  pr z  s 
z s

assuming z and s are independent  and minimizing the
negative log likelihood function  we obtain 

z  s   arg min l z  s 
z s

yk   w k z   

  arg min  log pr y z  s    log pr z    log pr s   
z s

fics    final project

 

figure    high resolution image        and test low resolution image      with added noise

the prior on z was chosen to be a gaussian to model the
statistics of photons hitting a light detector     

pr z   

 
n

       cz     



 
exp  zt cz  z
 

we can think of the covariance matrix cz as describing
the similarity between pixels in the targetimage  we can

pn pn
rewrite zt cz z as a product   i  
d
z
 
i j
j
j  
where di   j controls the shape of our similarity between
pixels  and  can control the weighting 
the prior of s is dependent on the motion characteristics
of the camera system and can be tailored to specific applications  in the general case  we dont know how the
system is moving  thus we will assume the prior to be a
uniform distribution 
we therefore write our log likelihood function as 
 
t
 y  wz   y  wz 
  
 

n
n
  x x
di j zj 
 
  i   j  

l z  s   

 

implementation

we created test low resolution images yk from a highresolution image for training by gaussian blurring a
         high resolution image  creating a shift  only
translations  by choosing a          window  and then
subsampling it by l      fig    shows an example test
image 

since we are minimizing the likelihood function with
respect to two sets of parameters  z and s  one method
would be to devise a coordinate descent like algorithm 
optimizing cyclically between z and s     we can use a
gradient descent for minimizing the gradient of the loglikelihood with respect to z  however because the dependence on s is implicit in our model  we dont have an
expression for the derivative of the log likelihood with
respect to the shifts  so estimating the shifts are done by
taking a block from the upsampled low resolution image
and matching this block to the high resolution image by
maximizing the   d correlation between the two blocks 
the algorithm begins by initializing z to gaussian noise
with mean zero and standard deviation   and initial
shifts s to    we then calculate the w given these
shifts  given w and the initial value of z gradient descent is run to minimize the log likelihood with respect
z  to minimize the log likelihood we take the gradient
of l z s  with respect to z 
l z  s 
 
zk
pm
n
x
  x
wm r  s zr  ym    
w
 s  
m k
   m  
r  
n
n
x
  x
di j zj  
di k  
  i  
j  

the first term in the gradient expression is the sum of
differences between the predicted and the actual lowresolution image vectors  each term in the sum is
weighted by the contribution of zk to that low resolution
pixel wm k  s    the second term is the prior gradient
which is in fact simply a linear combination of the pixels

fics    final project

in the high resolution image and can also be computed
via a convolution operation  using the gradient expression given above  the update rule that this routine uses
becomes 
n
  zn
zn  
k  z l z  s  z zn
k
k  s sk

gaussian kernel and then downsampling with a predefined factor  the images generated were used as input
vectors to the algorithm and the performance of the algorithm was tested for different parameter ranges  during these procedures convergence parameters of the algorithm  initial   the initial tolerance for gradient descent convergence  were kept constant between runs 

n
th
where zn
step
k and sk are z and s estimates at the n
and  is the step size determined by annealing  starting
from relatively high step sizes and decreasing the step
size as the algorithm approaches convergence 

a single gradient descent run is assumed to converge
when the maximum among the absolute value of the entries in the gradient is below a defined tolerance value 
after the gradient descent to the current tolerance value 
the shifts are estimated by maximizing the   d crosscorrelation of two blocks extracted from the high resolution estimate and upsampled low resolution input  the
shifts giving the maximum correlation values are stores
as sn
k 
since the accuracy of the shifts is relatively low at the
initial iterations of the main loop  this tolerance value is
decreased by half at every run of gradient descent starting from a predefined initial value  thus as z approaches
the original value and the accuracy of the shifts increases
the tolerance value is decreased  forcing the gradient descent to achieve a more accurate z estimate 

   

rotations

we tried to incorporate rotations into the same algorithm
above  again  the gradient with respect to shift rotation
could not be calculated directly so a search through multiple cross correlations of rotated images was performed
to estimate s and r   also  updating z in the gradient
descent step was done by rotating taking the difference
between the low res images and a rotated z and then rotating this difference back before applying w  unfortunately  the number of cross correlations and rotations
proved to be formidable in matlab and made the code
very slow 

 

results

the algorithm was tested using synthesized lowresolution images generated by first convolving with a

 

                 p     

                 p     

                 p     

absolute mean error vs number of frames absolute mean error vs 
   
  

   

  

 

absolute mean error vs 
 

 
  

   

   
 

  

   

 

   

  

 

  

  

  

 

 
 

  

  

  

   

 

   

    

figure    example of change p    and 
and the mean pixel error between z and
original high resolution image 

the parameters for which the performance of the algorithm was tested were    the variance of the additive
gaussian noise in the low resolution images  the variance of the prior for z  p the number of low resolution images used  the performance criterion used for
comparison was the absolute mean error between the resultant high resolution estimate z and the original highresolution image  results for selected parameter sets can
be seen in figures   and   
according to the results seen in the figure the number
of low resolution images used in general increases the
accuracy of the final estimate z  moreover  note that the
changes in  and  effects the magnitude of the gradient
of z in the gradient descent update rule directly  thus
the effect of changing one of these parameters in general depends on the value of the other parameter  the
ratio between    and  signifies the importance of the
error values over the information coming from the prior
of z  which explains how values of nearby pixels behave with respect to each other   thus making  smaller
with respect to    means relying mostly on the prior information and not depending on the error between the

fics    final project

figure    more estimated zs varying p    and  

figure    learning curves for different parameter values

figure    training on real video shot by handheld digital video camera 

 

fics    final project

 

estimated and original low resolution images 

 

the learning curves for some parameter combinations
can be seen in figure    the spiking behaviour in the
curves is due to the tolerance criteria used  since the
shifts are estimated again before each gradient descent
run  this changes the w matrix used during the gradient descent  making the initial delta much bigger than
the previously obtained lowest value  at each run of
gradient descent  the accuracy of the shifts increases 
when the shift values are close to convergence the
spikes disappear allowing the gradient descent to continue from the previously obtained smallest maximum
absolute value 

we found that estimating z with a map framework
proved to be very successful when we generated test
images our selves  however  with real video images 
we could not successfully capture all motion parameters into the model and thus found the z estimates to
be at best  smoothed interpolated versions of the lowresolution images  if a more accurate  computationally
efficient estimate of rotation could be found  we would
think the map estimator would be a good solution the
super resolution problem 

the algorithm was also tested on some videos by taking
       blocks from each frame as the low resolution input images  for these inputs because we dont have any
prior information on the point spread function used  effects of changing  value  the width of the point spread
function  was observed  the results can be seen in fig 
   as can also be observed from the figure changing 
didnt improve the result to a great extent although some
minor artifacts  vertical lines  seen for      are canceled for      
we speculate that the poor results from video footage
can be explained by our failure to capture rotations in
the low resolution images  as mentioned in the previous section  attempting to incorporate rotations into our
algorithm led to inconclusive results because of the increased run time and inaccuries in estimating the rotation between frames  in fig     we show the results of
attempting to estimate z from shifted and rotated test images  inaccuracies in our rotation estimator made it very
difficult to run the algorithm to convergence  causing the
resulting estimate to be much worse than our estimates
from a shifted only training set 

figure    estimate z from rotated test images 

 

conclusion

references

   r c  hardie  k j  barnard  e a  armstrong  joint
map registration and high resolution image estimation
using a sequence of undersampled images  ieee
transactions on image processing                  
     
   m tipping c bishop 
bayesian image superresolution 
advances in neural information processing systems              
mit
press cambridge ma      

fi