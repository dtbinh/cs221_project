hierarchical sparse coding
ian post
december         

 

introduction

a number of researchers have theorized that the brain may be employing some form of hierarchical
model of features in visual processing  nodes at the bottom of the hierarchy would represent local 
spacially oriented  specific features  while levels further up the hierarchy would detect increasingly
complex  spatially diffuse  and invariant features  with nodes in the uppermost layers corresponding
to invariant representations of objects and concepts  for example  mumford and lee have outlined
such a system employing hierarchical bayesian inference to combine sensory input at the lowest
levels with feedback from priors higher up     
models have been developed based on the idea of sparse coding that seem to mimic many of the
observed features of area v  in the visual cortexthe lowest layer of the hierarchy  specifically  we
assume that natural images can be represented as a sparse linear combination of over complete basis
functions  using unsupervised learning techniques and optimizing for sparseness  olshausen and
field succeeded in generating such a set of bases that resemble the localized  oriented lines detected
by simple cells in v       bell and sejnowski used independent component analysis  ica  and the
infomax principlemaximizing the information preserved by the decompositionto produce bases
with similar characteristics     
these models are good as far as they go  but they cannot be readily extended to generate
higher layers  in particular  we have assumed that the data is a linear combination of independent
components  which limits the complexity of the structure that can be captured  simply generating
a new sparse code for the output of the first layer yields no new information 

 

topographic ica

several related algorithms have been developed that attempt to extend bell and sejnowskis ica to
capture additional  nonlinear structure by relaxing the independence assumption  i have primarily
experimented with topographic ica  a model proposed by hyvarinen et al      the basic idea is to
group ica bases into neighborhoods  such that components within a given neighborhood tend to
be simultaneously active 
let x   as  as in the usual ica model  where x is the observed data and s the hidden  mixed
sources  the si are optimized for independence  so little correlation exists between their actual
values  however  we can capture the idea of simultaneous non zero values through their energies
s i   specifically  we want cov s i  s j     e s i s j    e s i  e s j    
    
we slightly relax the independence assumption of ica and assume that the correlated energies
within a neighborhood are due to the influence of a further layer of independent latent variables u
 

fithat determines the variances of s  hidden variables uj are mixed with neighborhood weights hij
and fed through a nonlinearity  to determine the variance i  of si  
x
i    
hij uj  
j

si is then generated as si   i zi   where the zj are mutually independent variables with the same
distribution as sj with unit variance 

figure    topographic independent component analysis  independent sources u are mixed by
neighborhood and run through a non linearity  which determines the variances of s  the si are
then mixed as in standard ica 
the si are then conditionally independent given their variances  but are dependent due to
relations between variances  within neighborhoods components are uncorrelated
e si sj     e zi  e zj  e i  e j      
but will tend to have correlated energies since
e s i s j    e s i  e s j     e zi   e zj    e i  j     e i   e j    
p
the covariance of i and j is
k hik hjk varuk which is positive if si and sj are in the same
neighborhood  constraining  to be monotonic  then   is too  so applying   the covariance is
still positive  so cov i    j    is positive  which implies the above equation     
derivation of the learning rule is complex but is worked out in      learning is done using a
 
gradient  for simplicity in the calculations  is chosen to be  x    x    

 

tica results

i experimented extensively with the neighborhood function and to a lesser extent with the nonlinearity  as noted in      with sufficiently large  overlapping neighborhoods and enough bases 

 

fifigure    topographic ica with a   by   neighborhood  neighborhoods resemble complex cells in
that they exhibit phase invariance as well as limited in rotation and translation invariance 
components become group by orientation  and neighborhoods resemble the receptive fields of v 
complex cells  the exhibit phase invariance as well as limited translation and rotation invariance 
large neighborhoods and more bases result in better defined neighborhoods  whereas smaller
numbers of bases or tiny neighborhoods  such as the linear topography below  result in irregularities 
fault lines of abrupt changes in basis orientation  etc  this appears to be due to limitations in the
model  namely forcing components to fit the predefined topography or using too few bases to cover
the image space  rather than any real correlations  such disjunctions become increasingly rare with
more components and larger neighborhoods  nor do they appear if the neighborhoods are disjoint 
a special case of tica known as independent subspace analysis     
more exotic neighborhoods including those with mixtures of positive and negative weights do not
appear to add anything  they may even cause the bases to decay into gibberish with no discernible
features  strange topography generally violates the assumptions of the algorithm   adjustments
in the nonlinearity also do not qualitatively change the results  although this function is highly
constrained in its form due to technical issues in the learning rule  in general  the model is difficult
to modify or generalize due to difficulties with intractable terms in learning 
i had hoped that creative fiddling and adjustments might allow learning of more interesting
features  e g  corners  but i think that would require an entirely new model  at least for the natural
images i trained on  it is possible that artificial images with more distinct edges and corners would
discover such structure  i experimented briefly with a further generalization of tica proposed by
karklin and lewicki     allowing much more general mixtures of the second layer of latent variables 
but as hinted at in their paper  convergence can be tricky  and using their own code from karklins
website  i was unable to reliably train the model  even in the best of cases  results are extremely
difficult to visualize 
 

fifigure    topographic ica with a linear   by   neighborhood  wrapping around to the next row 
some structure is discernible  but there is not enough overlap between neighborhoods to really
bring it into relief 

 

markov random fields and future work

one issue with tica is that both the topography and weights must be specified a priori  forcing the
user to guess what might be interesting and then forcing the data to conform to that model  this
recently led me to begin experimenting with markov random field models in the hope of learning
optimal neighborhoods and weights  learning is slow  but hinton et als contrastive divergence
algorithm makes models of this scale and type practical                briefly describes an experiment
that appears to produce results similar to tica 

figure    markov random field model 
mrf models also offer huge advantages in extendibility over hierarchical ica models  whereas
the second layer in tica was laboriously constructed and is difficult to generalize  mrfs can  at
least conceptually if not practically  be extended to arbitrary depths with only slight modifications 
given that oriented edges have emerged as bases with a number of different objective functions
 

fisparseness  mutual information  predictability  etc it is likely that similar components can be
learned with an appropriate mrf model  yielding a unified framework for the entire hierarchy 
as of now  i have completed only highly simplified proof of concept  treating ica output as
probabilities of binary mrf variables  i can reproduce some of the correlations revealed by tica 
although with far less sparsity in the connections  however  this model appears to offer much more
promise 

references
    anthony j  bell and terrence j  sejnowski  the independent components of natural scenes
are edge filters  vision research                        
    g  hinton  s  osindero  and k  bao  learning causally linked markov random fields       
    g  e  hinton  s  osindero  and y  teh  a fast learning algorithm for deep belief nets  neural
computation  pages                  
    aapo hyvarinen and patrik hoyer  emergence of phase  and shift invariant features by decomposition of natural images into independent feature subspaces  neural computation 
                     
    aapo hyvarinen  patrik o  hoyer  and mika inki  topographic independent component analysis  neural computation                       
    yan karklin and michael s  lewicki  a hierarchical bayesian model for learning nonlinear
statistical regularities in nonstationary natural signals  neural computation               
     
    t  lee and d  mumford  hierarchical bayesian inference in the visual cortex  journal of the
optical society of america a                    
    b  a  olshausen and d  j  field  emergence of simple cell receptive field properties by learning
a sparse code for natural images  nature                   
    m  welling and g  hinton  a new learning algorithm for mean field boltzmann machines       

 

fi