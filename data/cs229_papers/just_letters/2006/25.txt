object recognition and classification from  d point clouds
antoine el daher

abstract  in most modern robotic applications  having
accurate object recognition and classification is
becoming an ubiquitous requirement  to deal with the
latter  several techniques have been proposed  using
either monocular computer vision  stereo vision  and
even laser arrays  recently  new laser devices
introduced into the market  are able to capture the
whole environment as a sequence of several hundreds
of thousands of points per second  sampled from the
environment  along a cylinder around the laser device 
essentially  with correct pose estimation  those point
clouds give us exactly  d point coordinates for a very
fine grain sampling of the scene around us  in this
paper  we present two techniques for recognizing
objects based on those point clouds  the first one is
based on using support vector machines  and extracting
features from the scene  the second is based on boosting
a set of weak decision tree classifiers to recognize the
object  we also present a framework for performing
accurate simulations of the laser output 

i  introduction and previous work 
several techniques have been used to recognize objects
using computer vision  currently  viola jones advertise
the most successful ones  which are based on making a
boosted cascade of simple haar like feature classifiers
         there are successful attempts at recognizing
objects from images using svm  as described in        
and      using typical mercer or non mercer kernels 
all those methods have proven to be highly accurate for
simple  d images  and under certain conditions  but the
accuracy given by computer vision is not remotely
sufficient for actual robotics  for example  a car
detection module that used boosted classifiers  would
always return several false positives  and several false
negatives  and even if an object was detected using the
camera  then it would be even more difficult to exactly
pinpoint the object to  d space  and find its bounds
correctly  many factors contribute to this inaccuracy 
suffice to say that lighting conditions  colors  radial
distortion  all affect the quality of the reading  and
hence of the recognition  the idea behind lasers is that
they are extremely accurate  and rarely off by more than
 mm  they emit their own light  so do not suffer from
lighting fluctuations  we expect points coming out of a
laser detection to be precise  also  they are correctly
scaled  which is different for the camera  since a same
object can appear in different sizes for the latter 
depending on its distance  there has been some work

sehyuk park

on recognition from range images              describes
a more recent approach  which uses the laser data in
conjunction with camera data to normalize the
camera data  in order to account for lighting changes  in
this paper  we will describe our implemented simulator 
as well as some of the techniques that we experimented
with  for the simple case of testing whether an input
object is a bottle or something else  we were able to get
a classifier accuracy of        using     examples 

ii  simulating readings 
because such lasers are mostly at the prototype level 
and still prohibitively expensive  we decided to design a
simulator that would behave like the laser would  so we
would feed in a scene of the environment  and then the
simulator would give readings  as it would see them
from its own point of view  if it were a set of   
spinning lasers  along the y axis 
the code for this was done mostly using opengl  we
implemented a tool that would render scenes  we also
implemented a laser simulator  which would spin
around  and sample the scene at discrete intervals  and
mark those points  we then ran the simulator on several
 d models of bottles  and recorded the outputs given by
that simulator 
because we only had around    models of bottles at our
disposal  we recorded those same models in several
other poses  sometimes applying some distortion in  d
space to them  in order to get more general results  we
ended up with a few hundred  d models of bottles 
measured by  d point clouds 

fiwe experimented with various kinds of kernels  but
found that the most accurate results came when we used
the radial basis kernel 

figure    the simulator running on certain bottles 
the results given by the simulator were very
satisfactory  the interface has also been made such that
one can navigate through the scene  look around  and
one can easily distort the points read out by the
simulator  store them in file  and test classification on
them based on the reading 

the following picture shows the information that wed
extract out of such an image 

iii  svm for recognition
we have done a lot of work at the implementation level
on svm object recognition  using svms  we tried to
solve multi class classification problem   d features
 x y z  are not well known to the image classification
problems but there are many well known methods for
 d images 
first of all  there are several ways of gathering features
from  d images  we devised a simple way of gathering
features that is applicable to general learning algorithms
easily 
as preprocessing  we shift the  d points so that the
mean of z value becomes    simply  we can subtract
mean z value from every points z value  using this
way  we can also rescale every object to have similar
sizes  basically by dividing every coordinate by the
maximal coordinate that has been read throughout the
reading 

figure    the max min and mean features
and here is a an example that shows exactly what is
currently going on with the inputs 

based directly on the range scan  we divided the  d
image into n by m regions and for each region 
computed the maximum  minimum and mean z value
among the points in the region  for example  taking
into account a  d bottle similar to the one before 
similarly  we could compute minimum and average z
value for each region  the three measures  max  min
and mean  are a very good way to explain the features
briefly 
then  we have n m   dimensional features  using the
features of the training examples  we can train an svm
model and predict on test examples  this is a first very
simple approach at the problem 

figure    sample bottle

fiand assuming that the camera reader is somewhere on
the xz plane  away from the yx plane  we get the
following readings 

thanks to chih chung chang and chih jen lin  we
used some parts of the code libsvm  a library for
support vector machines  
the experimentation procedure is described in the
section about results 

iv  boosted classifiers for recognition 
i  technique 
in the spirit of what was done by viola jones      and
nuchter      we thought about using boosted weak
learners to classify  d objects  we did not really do any
implementation on this part  but gathered most of the
ideas already 
for purely depth images  the idea is to use the same set
of features as those found in  d images classification 
namely simple haar wavelets  then one can run
adaboost to combine those in order to correctly be able
to classify the input  note that the input would then
simply be the depth reading 
figure    depth map for the bottle
in this image  each reading is a depth reading  as given
by the laser scanner 

in order to solve the scaling problem  we devise a
hierarchical model  similarly to what is described in     
in other words  we gather features from m x n regions 
 m    x  n      m    x  n    and so on  in other words 
we divide the image into several sub samples  and get
features from that 
the question one needs to ask oneself at this point is 
how big should m and n be  bigger regions can solve
the locally translation invariant problem but will lose
much information  on the other hand  small regions can
contain more information and capture more important
features  the latter might nevertheless be affected by
small translations in the input 
note that all of the above mostly refers to depth images 
as opposed to actual  x y z  coordinate images  we will
be working on extending the algorithm to take all of
this into account  the scaling problem would disappear
under those conditions  since we know that a bottle can
only be of a specific size 

however  this would basically be forcing the object to
be recognized from this particular point of view  but we
have much more information from the  d points and
the pose of the laser  than we have from depth
information 
for this reason  it seems natural to extend the haar
wavelets typically used in  d computer vision  to take
depth into account  the basis is too big to fully list  here
are a couple of vectors in the basis 

figure     d haar wavelets 
so given a particular input  whenever a point that is
read falls within the black area  it is added to the sum 
and whenever it falls within the white area it is
subtracted  similarly to what the haar features do 
the idea is that we have several such features  and so in
the test cases  we find the feature which maximizes the
information gain  then we re assign weights for the
whole sample space using the standard adaboost
technique  and find the next feature which maximizes

fiinformation gain  and so on  in the end  we combine all
of them to create a classifier 
next  given a test image  all we need to do is evaluate
that classifier for those images 
ii  reducing the run time 
to speed things up during training  as was done using
the standard  d haar  we can use an integral  d
image  which stores for any  x y z   the total number
s x y z  of points contained in the box     x  x     y  x
    z   so that the number of points contained in  a  b  x
 c  d  x  e  f  can be calculated using o    accesses to
the  x y z  table 
computing the  x y z  table can be made efficiently 
we used the following formula 
s x y z    s x   y z    s x y   z    s x y z   
  s x   y   z   s x y   z     s x   y z   
  s x   y   z      p x y z 
where p x y z      if there is a point inside the bin
 x y z  and   otherwise  this clearly allows us to find
the whole of s x y z  in o n   time 

figure    finding s x y  in o   
looking at the above table  which is a  d simplification
of the problem  we notice that the sum of the points in
the range  a  x  x  b  y  is simply 
s x y   s a y   s x b    s a b 
extending this to the case of a  d feature  it is easy to
see that the sum of pixels in the range  a  x  x  b  y  x
 c  z  is 
s x y z   s a y z   s x b z   s x y c 
  s a b z    s a y c    s x b c   s a b c 
with that in mind  we can evaluate each feature at a
specific location  and get a certain value for it 

samples  and all of the negative samples  use the
feature as a classifier  and find the threshold which
maximizes the information gain  then find the feature
which maximizes the information gain  which is simply
given as 

in the above  n  is the number of negative examples that
would make the value bigger than the threshold  p  is
the number of positive examples whose feature
evaluation would yield a value smaller than the
threshold 

so in the end  the problem is simply one that counts
take into account the density of points  and their
configuration  to be able to make accurate predictions 
the training time for this method was understandably
very slow  because were forced to go over several
combinations  for   kinds of features  and different
kinds of scaling in  x y z  there are approximately o n  
combinations  each of which needs to be tested on the
samples which number possibly as much as a thousand 
in addition  storage for the sum tables tend to be also
very large  occupying a cubic amount of memory for
each of the test samples 
this made it difficult to train on a very large set  and so
we had to restrict ourselves to sets with sizes as small
as    elements  and test sets of size    elements  this
only yielded accuracies of         with more
computing power  we could have trained it on much
larger sets and seen what the results would look like 
which is something that we have included in our future
work 
iv  accounting for various transformations 
some factors need to be taken into account in order to
make sure that objects are correctly recognized  first of
all  the range readings are all converted to position
readings in x y z  this makes sure that objects are
correctly scaled  and that an object nearby wont look
ten times bigger as it would in a camera 
second  when an object needs to be classified  the mean
depth is subtracted  so that the object now has a depth
centered at z      this allows for some translation
issues to be resolved 

iii  training 

v  results
training occurs as follows  for each possible feature 
try placing the feature at every single location in the
image grid  evaluate the feature for all of the positive

let us first describe the testing scenario that we used 

fibecause the results of the svm algorithm were the
most successful  we will report them here  the previous
section describes the results of the boosting method 
we generated     models of bottles  and     models of
non bottles  which included various random objects 
such as cylinders  spheres  distorted planes  random sets
of points  we then ran it with various cross validation
coefficients  for example  with     of the samples used
for training  and     of the data used for testing 
the initial accuracy using         validation was
        with     out of     examples being classified
correctly  the number of support vectors on the    
strong training set was     on average 
for various other distributions of the cross validation
percentages  the accuracy varied between     and     
to make more evident the effect of the sample training
size  we ran it on several different sample sizes  which
we increased progressively 
here are the results for       cross validation 
training size
sv
test accuracy
  
 
   
  
  
   
  
  
      
  
  
   
  
  
     
   
  
      
   
  
      
   
   
      
   
   
      
table    svm testing results
the number sv represents the number of support
vectors that were used in the final classified  here is the
corresponding learning curve 

figure    svm learning curve

vi  conclusions and future work 
the results given by svm were quite accurate  up to
      and we are convinced that given more time and
computing power  the results given by boosting weak
classifiers would also have been far more accurate 
there were some problems with the training that wed
like to address in the future  the first one was
obviously that the bottle samples are quite different
from the non bottle samples  which made the problem
easy to classify  even though we tried having some
similar shapes  such as cylinders in the non bottles 
however  because everything was run of the simulator
we did not have an infinite amount of data  and the
inputted negative samples would probably not work if
run on an actual data  an actual device would have
given far more representative results  either way  the
methods are solid and fast at runtime at this theoretical
level 

references 
    paul viola  michael jones   rapid object detection
using a boosted cascade of simple features        
    viola  jones   robust real time object detection 
    c  bahlmann  b  haasdonk  and h  burkhardt  online handwriting recognition with support vector
machinesa kernel approach  in proc  of the  th
iwfhr  pages            
    sabri boughorbel  jean philippe tarel
non mercer kernels for svm object recognition
    chikahito nakajima norihiko itoh
object recognition and detection by a combination
of support vector machine and rotation invariant
phase only correlation
    farshid arman  model based object recognition
in dense range images  a review
    paul j  besl and ramesh c  jain  threedimensional object recognition
    andreas nuchter  hartmut surmann  joachim
hertzberg automatic classification of objects in  d
laser range scans
    j  mutch and d  lowe  multiclass object
recognition with sparse  localized features in  proc 
ieee conf  comp  vision patt  recog      
     h  zhang  a  c  berg  m  maire  and j  malik 
svm knn  discriminative nearest neighbor
classification for visual category recognition  in proc 
cvpr       
     kristen grauman  trevor darrell  unsupervised
learning of categories from sets of partially matching
image features  proceedings ieee conf  on computer
vision and pattern recognition       

fi