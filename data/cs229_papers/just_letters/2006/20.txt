predicting movie preferences
ian baker

 

introduction

accurately predicting customer preferences is a highly sought after goal  for it allows
businesses to market a product to individuals in a way that is most likely to generate
a sale  this can be particularly important in the movie industry  where individual
preferences can vary greatly  recognizing this  the online movie rental business netflix
has sponsored a contest to find better ways of making these kind of predictions 
specifically  the problem is as follows  for a given customer and a given movie 
predict how that customer would rate the movie on a     scale  using information on
how that customer has previously rated other movies  and other customer ratings 
more formally we wish to estimate
rcm   e r c   c  m   m 
where r  c  m are random variables representing the rating  customer and movie 
respectively  equivalently we estimate the probability p  r c  m  and use this to
compute the expectation 
one possible approach to this estimation is to assume that while individuals
preferences may vary greatly from person to person  each person belongs to one of a
small number of classes z  and members of a given class tend to rate all movies in
approximately the same way  probabilistically we write
x
x
p  r c  m   
p  r  z   z c  m   
p  r z  c  m p  z c  m 
z

z

we assume that the class z captures all the information about how an individual will
rate a given movie  i e  p  r z  c  m    p  r z  m   and that the class of a customer
does not depend on the movie rated  i e  p  z c  m    p  z c   assuming each member
of a class rates a given movie about the same  we model p  r z  m    n  zm   zm  
and p  z c    zc   for fixed parameters zc  
then given a set of training data  customer ratings   r i   ci   mi      we want to
find a set of parameters that maximizes the likelihood
x
l        
log p  r i ci   mi        
i

 

x
i

log

x

p  r i  z  mi   zmi   zmi  p  z ci   zci  

z

 

fifigure    the probablistic model of ratings  each customer c is given a class z with
probability zc   then the rating for a given movie m is generated from a normal
distribution n  zm  zm   
this optimization can then be performed by using the expectation maximization
algorithm  for the expectation step we compute
wji   p  z   j r i  ci   mi  
the optimal values of the parameters are then given by
p i
i
i wz i c   c 
zc   p
i ci   c 
p ii i
w r i mi   m 
zm   pi z i
w i mi   m 
p i ii z
 r  zm    i mi   m 
 
i wzp
 
i
i
 
zm
i wz i m   m 

in the maximization step  once we have optimized these parameters by iterative
updates  we can then make a prediction for a customer  movie pair  c  m  by
x
e r c  m   
rp  r m  c 
r

 
 
x x
x
x
 
r
p  r z  m p  z c   
p  z c 
rp  r z  m 

 

r

z

x

zc zm

z

z

 

r

fi 

results

we implemented the algorithm described above  using training data consisting of
approximately     million training examples  which netflix has provided from their
database as part q
of the contest  the error metric we use is the root mean squared
pm
 
i
i
i
i  
error rmse  
j      r     where  and r are the predicted and actual
m
values respectively 
a priori the number of classes  z  is unknown  so it needs to be determined
from the data  a straightforward method of choosing a good value is to train the
algorithm for increasing values of z and use hold out cross validation to choose the
z that minimizes the error on some held out test set 
initially we performed this using a relatively small training set  about   million
examples out of the     million available   we found z     had the smallest test error 
though the error remained nearly constant as z increased  however  the training
error steadily decreased as z increased  e g  test error was       for z            for
z       and training error was       for z     and      for z      
the discrepancy between the training and test error seems to indicate significant
variance in the method  and given the abundance of training data  it seemed likely
that the larger z values would perform better for larger training sets  we therefore
trained the method for increasing z and increasing numbers of training examples 
the results of which are summarized in table    as is evident from the data in that
table  the larger z values do outperform z     as the training set size increases 
given this dependency on training set size  validating on the largest training set
possible seems to be an important goal  however  since the runtime requirements of
the algorithm increase linearly in both time and space with the number of training
examples  using tens to hundreds of millions of examples quickly becomes infeasible
on a standard workstation without signficant performance tuning  significant parts
of the algorithm are parallelizable  and we implemented a parallel version which
outperforms the serial version by as much as     per iteration 
yet  there is still the issue of the memory consumption  even using data structures
designed to minimize the size of the data in memory  if all of the needed data is kept
in main memory  the algorithm consumes more memory than a typical workstation
has available when more than a few tens of millions of examples are used  in this case
most of the time is spent waiting for the operating system to page data to and from
the hard disk  and the time needed becomes impractical  within these constraints
we were able to train the algorithm for z     with    million examples and z    
with    million examples 
after training on these subsets we obtained a test error of       for z     and
      for z     on other randomly chosen subsets of the data  on a probe subset
of the data  specified by netflix  we obtained a test error of       for z      training
set      million  and       for z     training set      million   finally on the quiz
set we obtained an error of      for z       this is a set of data not included in
 

fiz
training examples  mil 
training

  

  

  

 
 
 

         
         
         

 
 
 

         
         
         

 
 
 

              
         
         

test
   

probe

table    rms error on the training  test  and probe sets for varying values of z and
increasing numbers of training examples 
the training set  but for which netflix will calculate the rms error  given a set of
predictions   the probe and quiz sets are not randomly chosen  but likely are chosen
because they represent more difficult sets of data to predict  as a reference  the
most naive method  always predicting the average rating  achieves an error of     
on the quiz set  and approximately      on the entire training set 

 

discussion

the method presented shows clear performance gains over the simplest of methods 
however  there is also clearly still significant room improvement  one of the main
problems with the method seems to be high variance  particularly as z increases 
indeed  as z increased the training errors were consistently and significantly reduced 
yet the test errors only showed improvement for larger training sets  given the
abundance of training data  over     million examples available  plus netflix total
database contains billions   this is not necessarily a major problem with the method 
however  the computational resources required is a more significant hurdle  in order
to really test the potential of the method  it is necessary to train it with more than
a few tens of millions of examples  yet to do this  one either needs more powerful
hardware than a typical workstation  or a highly specialized implementation designed
to parallelize the data across multiple machines or efficiently cache data to the disk
during the run  unfortunately neither of these is really an option for a one quarter
student project 
in addition to improving the implementation  it seems likely the method can be

 

fiimproved in other ways as well  one possibility is classifying not only the customers 
but also the movies  we can then model the problem by assuming not only are classes
of customers likely to rate the same movie similarly  but that generally movies of the
same class are likely to be rated similarly by customers of a given class  classes
of movies could then be determined not only through internal consistency with the
ratings from the training data  but also through external features  such as actors 
directors  genre  etc 

 

conclusions

we have presented a method for predicting how an individual will rate a movie  given
information about their previous ratings and other customers ratings  the method
uses a probablistic model that postulates various classes that customers belong to
and which determine how customers will rate a given movie  then given example
data we can find a maximum likelihood estimate to the model using the expectation
maximization algorithm 
even for relatively small samples  this algorithm shows improvement over the
simplest methods  however  in order to determine the true potential of the method 
further work is required in order to reasonably train on significantly larger training
sets 

 

fi