submitted by brian sa and patrick shih for cs    fall     

k means for netflix user clustering
i  introduction and model

unsupervised method for classifying users in the vast

in the netflix collaborative filtering problem  the goal is 
given a set of training data x     ui   mi   t i   ri    consisting of
a sample of prior movie ratings ri  an integer from   to   
associated with user u i   movie mi   time t i

 to

accurately predict the rating that should be associated
with a new point  u  m    

as part of a larger stanford

netflix data set because it converges extremely quickly
in practice 
the method of k means as applied to incomplete user
vectors x i  s is as follows 
    initialize cluster centroids by one of two methods 
a 

effort  we seek to use k means to cluster users with
similar movie preferences 

assign

                      k  

to k randomly chosen

x i  s 
b 

intuitively  this means that users have intrinsic user types

paper

wherein all users of that type have aligned preferences
across all movies  and furthermore that individual user

or use the heuristic described in the k means  

    repeat until convergence 

vectors of that type reflect choosing from some unknown

a 

let

c  i      arg max f score   x  i        j    

for all i 

j

distribution specific to the users class  the variance of
where

ratings for any particular movie for users within a class

f score   

is a scoring function described in

more detail later 

can be interpreted as the natural variation of user ratings

m

given known absolute preferences for a movie 

  c

as

opposed to the mixture of multinomials group  c f 

b 

for each j  let

 l  j    

project by dimitris  hau jia  and raylene   we make no
assumptions about the underlying model other than the

x

 i  
l

 i  

  j   x l i  

i   
m

  c

 i  

  j   x l  i      

i   
n



is x i  projected from

most basic premises  that users rate movies in a

for l        n 

predictable way and that for a given user  different movie

d a n   with all added dimensions padded with

ratings are not all conditionally independent 

 s  in effect ignoring incomparable ratings

more formally  each user has a vector of ratings over all

where either the centroid or the user is missing a

movies

x   

n

which

comprises

the

ratings that the user would rate each of the      
movies in the netflix library 

x

 i  

m



j  c        f score   x   i        c    

d

d  n   that are the projections of the complete user

vectors x i   onto some arbitrary lower dimension d  it is
usually the case that d    n  with an average across the
training set of

the objective maximized in this case is 

the netflix prize ratings

database provides incomplete user vectors 
with

rating for movie l 

hypothetical

d       furthermore  d varies with each

user i as it is not the case that all users have rated the
same number of movies nor have all users rated the

 i  

i   

for comparison  the traditional method of applying kmeans to a sparse data set is to fill in missing vector
elements with default values  furthermore  k means is
typically formalized as minimizing a distortion function
that represents either the angle between vectors  cosine
similarity  or the euclidean distance 

same set of movies 

m

j traditional  c        x  i     c   i  

our original motivation in pursuing k means was to

i   

 
 

perform linear regression within clusters  although the
scope of clustering a dataset of this scale and sparsity


proved enough of a challenge  k means is also an ideal

arthur  david and vassilvitskii  sergei  k means    the
advantages of careful seeding  to appear in soda      

fisubmitted by brian sa and patrick shih for cs    fall     
  size  is the number of users in the cluster
  span  is the number of ratings spanned by all users in the cluster
  overlapping  is the number of ratings rated on by two or more users
  outcasts  is the number of users who share no movies in common with other users
 result of kmeans with k      run on training set     x smaller training   dat
cluster
size
span
overlapping
outcasts
 
  
    
   
 
 
 
   
  
 
 
  
    
   
 
 
  
   
   
 
 
  
    
    
 
 
 
   
 
 
 
  
    
   
 
 
  
   
   
 
 
  
   
   
 
 
  
    
    
 
  
  
    
   
 
  
  
    
   
 
  
  
    
    
 
  
  
   
   
 
  
 
   
   
 
  
  
    
   
 
  
 
   
  
 
  
  
    
    
 
  
  
    
   
 
  
  
   
   
 
  
  
    
    
 
  
  
   
   
 
  
  
    
   
 
  
  
   
   
 
  
 
   
   
 
  
  
    
    
 
  
 
    
  
 
  
  
    
    
 
  
  
    
   
 
  
 
   
   
 

examining cluster       number of users    
  
    
   
   
 
     
   
   
   
   
     
   
   
   
   
   
 
   
   
    
    
    
    
    
    
 
    
   
    
 
    
 
    
    
 
    
    
    
 
 
    
    
        
    
    
 
    
    
 
    
 
    


 
 
 
 

 
   
 
 
 

     

 

 
 
 
 
 
 

 
 
 
  

 

 
 

 
 
 
 

 

 

 
 

   

    
 

 
     

 
 
   

       
 
 
 
 

 
 

   
 

 
 

 

 
 

figure    left  a graphical depiction of a cluster  movie ids are on the y axis  users along the x axis   s represent the presence of a rating
for a particular user and movie  right  a typical distribution of clusters  k     number of users        scoring function used was mmp
continuous  see below 

we will explain why this cannot be applied to the netflix
data set  without some tweaks  in the section on scoring
functions 

ii  parameters 
the parameters investigated in this project are the
number of clusters k  heuristic initialization  h   and the
scoring function

ii a  the effect of heuristic initialization  h 

movie rating distribution

number of movies

f score     

   

the k means algorithm is dependent on the initial

   

centroids and as such is not guaranteed to discover the

   

global optimum  that is  the quality of the clusters  as

   

quantified by the objective function described earlier  is

   

highly variable for different trials  a common method to

  

overcome this is to run the algorithm multiple times with

  

different initial centroids and return the best clustering

  

found  since clustering on the complete netflix data set
  

is computationally expensive  it is beneficial to start with
 
 

   

   

   

   

   

   

   

   

   

    

clusters chosen by some heuristic so as to speed up

number of rat ings

convergence while also guaranteeing the quality of the
figure    data set size          users  scoring function  mmp
continuous  k     notes  shown is the distribution of one
cluster  the tail of the distribution is not shown 

resulting clusters 

fisubmitted by brian sa and patrick shih for cs    fall     

heuristic initialization as described in the k means  

number of iterations until convergence

paper was implemented for these purposes  at least in

  

theory  carefully choosing initial centroid values has the

  
  

advantages of reducing the number of iterations until

relatively consistent when repeated  the heuristic assigns
the first centroid by choosing a user randomly  for each

  
iterations

convergence and of guaranteeing a clustering that is

  
  
  
  

successive centroid  it chooses a user with probability

 

proportional to its euclidean distance to centroids that

 
  

   

have already been assigned  the motivating idea is to
choose centroids that are maximally distinguished from
each other leading to more meaningful clusters on the

    

     

number of users

figure    scoring function  mmp continuous  notes  each data
point is the average over   trials run on different data sets 

first iteration  the results are compared with the
standard method of initialization whereby k user vectors
are selected randomly from the data set and used as the

ii b  choosing the right scoring function fscore  
to preamble our discussion of scoring functions  we will

initial centroids 

start by explaining why the traditional method of filling
incomplete vectors with default values fails in this
application  density  or sparsity  as is the case here  of

iterations until convergence

effect of k   heuristic initialization
  

the training data determines the ratio of default values to

  

actual ratings 

for the netflix data set  which is

approximately    dense  there are about     default

  

random
initialization

values for every actual rating  depending on the size of

  
k  
heuristics
  

the training set m and the number of clusters k  the
centroid vectors are filled with a significant proportion of

 

default values unless m is large or k is small  this allows
comparisons

 
 

  

  

  

  

between

two

default

values 

which

  

represent the maximum similarity attainable by either

number of clust ers  k 

figure    data set size       users  scoring function  mmp
continuous  notes  number of iterations capped at     each
data point is the average over   trials run on different data sets 

euclidean distance or cosine similarity 

since scoring

functions like euclidean distance or cosine similarity
make no distinction between faux values and real values 
the resulting signal to noise ratio is very low 

the

according to the data  figure        k   heuristic

number of such comparisons between default values is

initialization decreases the number of iterations until

related to the density of the centroid  a measure of which

convergence for all k and m 

this implies that the

can be found in a statistic we call the span  the span

heuristic starts the algorithm off with centroids closer to

represents the number of movies in a cluster that have

ideal than a random selection of users 

been rated by at least one person  even if an attempt is
made to improve the signal to noise ratio by decreasing
the weight of default values  a significant problem still
remains  it turns out that user vectors will always try to
maximize the number of comparisons between default



arthur  david and vassilvitskii  sergei  k means    the
advantages of careful seeding  to appear in soda      

values since these achieve perfect similarity  i e  result in

fisubmitted by brian sa and patrick shih for cs    fall     

a euclidean distance of   or a cosine similarity of   

the difference between a user rating and a centroid

thus  unless the span covers virtually the entire set of

rating  discretizes it to integral values  and returns as

movies  the clustering is utterly useless  see figure   for

output a number reflecting the similarity of the two

a typical clustering using k means with default values  

ratings 

the modification to k means as implemented in this
paper can be viewed as assigning a similarity score for
each pair

  x l i      l  j      

non comparable pairs  which

will be defined in this context as any pair where either
 i  
l  

x

  



  j 
l

  or both are missing  are given a similarity of

on an intuitive level  this represents the default

condition whereby no inferences about similarity can be
drawn  then for comparable pairs  the scoring function
returns a value that rewards  a positive similarity  for
rating differences within a certain threshold  and returns

the crucial insight we made was to treat the

non comparable case as a baseline from which to reward
for small mismatches and penalize for large mismatches 
the motivation behind a discretized scoring function is
to compensate for the granularity of user ratings  that is 
if for example the centroid rating is      the user rating
 an integral value  can be at best   or    yielding an
absolute difference of     

a scoring function that is

discretized in the same increments as the user ratings
returns consistent scores even if the centroid ratings
fluctuate somewhat 

 xl  i     l  j  
 x   i       j  
l
f mmpdiscrete   xl i      l  j        l  i  
xl   l  j  
  i  
  j 
 xl   l

a value that penalizes  a negative similarity  for rating
differences that exceed the threshold 
four scoring functions were evaluated for clustering

   
      
       
       

quality and secondarily for the root mean squared error
 rmse  of predictions made  the prediction for user i at
movie j  given by

x  ij      

closest centroids rating 

m mp discrete

is calculated by using the

  jc

 i  

 

    

  if it exists  or resorting
   

to an average rating calculated over the entire data set

the average over the movie  

rmse

    

 an average adjusted by the average over the user and

training

   

testing

    
 

ii b i  mismatch penalty  mmp  using a discrete scoring

    
  

function

  

  

  

  

  

  

  

number of clust ers  k 

the problem of matching sparse user vectors to ideal
clusters is analogous to that of sequence alignment for
dna or rna  in the case of sequence alignment  a given

figure    data set size       users  scoring function  mmp
discrete  notes  rmse for predictions made both within and
outside of span  each data point is the average over   trials run
on different data sets 

pair of bases under consideration can be assigned a
score via a scoring matrix  which contains a pre 

ii b ii  mismatch penalty  mmp  using a continuous

enumerated grid of all permutations of the   possible

scoring function

bases with scores reflecting their similarity or affinity  in

although discretization works quite well  it also has its

protein sequence alignment substitution matrices like

disadvantages  namely that the arg min of a step 

pam or blosum serve the same purpose for scoring

function

evolutionary

our

theoretically  this translates to a looser clustering 

inspiration from these methods  we wrote our own

since a difference in ratings of as much as   is tolerated

scoring function that takes as input the absolute value of

 or more accurately  rewarded  

sequence

divergence 

drawing

is

a

range  rather

than

a

single

value 

an ideal continuous

fisubmitted by brian sa and patrick shih for cs    fall     

scoring function would address this shortfall while

significantly as k and m increase  k        on all

maintaining the characteristics of the mismatch penalty

       

scoring system 

proportionally to k 

users 

because

specificity

increases

further  the sparsity of the data

f mmpcontinuous   x l i      l  j         x l i     l  j             

places a constraint on the minimum size of a cluster  we

this scoring function was constructed to fit the following

thus  increasing k requires a proportional increase in m 

specifications     it must return   if the scores match

if indeed a high value for k is required for optimal

exactly     it must assign a negative penalty for any

prediction  this implies that current values of k will have

absolute difference exceeding      and    it must assign

high bias  and this is borne out  the training error and

increasingly

negative

generalization error on k means run for constant k   

differences 

it is interesting to note that as originally

constructed 

penalties

f mmpcontinuous   

for

was

a

larger
cubic

absolute
function 

must ensure a sufficient span to make valid predictions 

and

increasing

m

indicate

that

our

algorithm

is

underfitting the data 

however  this proved too penalizing for large absolute
differences in ratings and the algorithm did not converge 
training and generalizat ion error

the mmp continuous scoring function was in fact able to
    

achieve tighter clusters  as can be seen from its testing

   

rmse for k     figure     however  it was also sensitive

    

to values of k that were too high 
rmse

   

training error

    
   

generalization
error

    

mm p cont inuous

   
    

   

   
  

rmse

    

   

    

     

      

number of users

   

training
testing

    
   

figure    scoring function  mmp continuous  notes  rmse
calculated for predictions within the span of the centroid  each
data point is the average over   trials run on different data sets 

iv  future work

    
 

  

  

  

  

continuing forward  a short term goal is to tune the

   

parameters of the mmp continuous scoring function for

number of c lust ers  k 

optimal clustering  long term goals include proceeding
figure    data set size       users  scoring function  mmp
continuous  notes  rmse for predictions made both within and
outside of span  each data point is the average over   trials run
on different data sets 

with linear regression within clusters and utilizing
information contained in movie content  such as from
www imdb com  to improve performance 

iii  discussion

v  acknowledgements

the ultimate goal of the netflix prize is to minimize the
rmse for predictions 

however  it is evident that k 

means as the sole method for rating prediction  as
compared to the current leading predictive algorithms  is
limited at the current values of k and m  number of
users  

we

expect

that

prediction

will

improve

we would like to acknowledge ted hong and dimitris
tsamis  our collaborators  and the entire stanford netflix
prize team  led by tom do and thuc vu  additional
thanks to tom do for his assistance in writing a version
of k means utilizing mpi 

fi