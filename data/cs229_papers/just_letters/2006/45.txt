rotten tomatoes  sentiment classification in movie reviews
alyssa liang
cs      december         

   introduction
text classification plays an large part in today s world  where the amount of information available is
overwhelming  although most text classification work is related to topical classification  the categorization of more
subjective documents that depend more on style and the author s opinion is also important  websites such as
amazon  imdb  and rotten tomatoes rely on opinions and reviews to keep their sites running  classification of
documents by sentiment  i e  positive or negative  could also be useful in recommender systems and customer
service 

   previous work
most of the work in text classification has been topical classification  however  there has also been work
in classifying text based on issues beyond subject matter  such as author identification and sentiment classification 
previous work in sentiment classification has mostly focused on classifying reviews as positive and negative  pang 
lee  and vaithyanathan have used learning algorithms such as naive bayes  svm  and maximum entropy to
classify reviews  however  their work focused on considering on the presence of terms  rather than their frequency 
and using only a limited number of features  pang and lee have also gone further and worked on classifying
reviews using a  multi point  scale  such as the number of stars for a movie review  in addition  there also has been
previous work  turney        using unsupervised learning algorithms to classify reviews 

   movie review data
i m using movie review data from http   www cs cornell edu people pabo movie review data   the data
consists of       positive reviews and       negative reviews  the data does not include reviews that could be
considered  neutral   movie reviews are an ideal source to experiment with sentiment classification  reviewers often
have strong feelings about movies and even better  they usually provide a numerical rating along with the review 
one would first approach the problem by choosing words that would easily distinguish positive reviews
from negative reviews  such as  bad    awful   or  wonderful   the list of most discriminatory words  i e   with the
highest mutual information  bears this out  interestingly  negative words dominate the list  from the reviews  it
appears that negative reviews are more likely to be at an emotional extreme 

bad
perfect
worst
life
stupid
supposed
boring
memorable
ridiculous
dull
waste
poorly
awful
excellent
outstanding
perfectly
mess
script
lame
plot
table       words in the dataset with the highest mutual information

   machine learning algorithms
in this project  i m focused on two learning algorithms  multinomial naive bayes classification and support
vector machines  for these algorithms  i used the bag of words framework  although i also used word bigrams to

fitake context into account  each feature represents a unigram  or bigram  and its value for a document is the number
of times the feature appears in that document 

naive bayes
one of the simplest approaches to text classification is the naive bayes algorithm  it makes the assumption
that the probability of one word appearing in a document doesn t affect the probability of another word appearing 
obviously  this isn t the case  but naive bayes still works well  for this project  i used the multinomial naive bayes
classifier  which takes multiple occurrences of terms into account  training the classifier is fairly quick and simple 
it estimates the prior probabilities for a class c as

p  c   

nc
n

where n is that total number of documents and nc is the number of documents that belong to class c  the classifier
also estimates the conditional probabilities that a term x appears in class c 

p  x   c   

n x  c    
nc   v

where nx c is the total number of times term x appears in all the documents that belong to class c  nc is the number of
terms in all the documents that belong to class c  and  v  is the size of the vocabulary  the conditional probabilities
using laplace smoothing to avoid zeros 
to classify a test document  we find

c   arg max p  c j   p   xi   c j  
c j c

ip

where p is the set of all positions in the test document that contain a term in the vocabulary  and xi is the term that
occurs at position i 
in this project  i used the implementation of multinomial naive bayes from the libbow toolkit  as well as
my own implementation 

support vector machines
svms tend to work better than naive bayes  unlike naive bayes  which is a probabilistic classifier  svm
attempts to find a hyperplane that divides the training documents with the largest margin  it finds the hyperplane
using the smo algorithm  and then it classifies a test document by finding
m


i   

i

y   i   k   x   i     x    b

where i and b are the parameters for the hyperplane  if the quantity is greater than zero  the the class y for the test
document is set to    otherwise it is set to   
in this project  most of tests with the svm classifiers used a linear kernel  however  i also ran svm with a
 
radial basis function kernel   k   x  z     exp   x  z where gamma           previous work  siolas  d alche buc 

 

 

      has suggested that an rbf kernel works well  i used the svmlight package to implement the svm classifier 

   evaluation   results
the data was split into   equal sized folds      documents each   so that results are the average of the trials
from eight fold cross validation  i focused on using word unigrams and bigrams as the features  unigrams  
bigrams appears to have only a    or    improvement of unigrams only  the documents were parsed so that
punctuation was removed  and unigrams that were less than   characters were removed from the vocabulary 

unigrams
unigrams   bigrams

vocabulary size
      
       

nb
      
      

svm  linear 
      
      

table    accuracies for naive bayes and svm using unigram and unigram bigram freatures 

fiinitial unigram and bigram results
as expected  using both unigrams and bigrams improved the performance of the classifiers  since bigrams
would take context of a word into account  interestingly enough  though  naive bayes greatly outperformed svm 
perhaps the multinomial model is better at modeling the data when using only simple unigrams and bigrams 

   improvements
unigrams   bigrams

naive bayes
      

svm  linear 
      

svm  rbf 
      

sentence upweighting  last     
sentence   adj adv upweighting

      
      

      
      




tf idf
tf idf   upweighting
combined

      
      


      
      
      

      
      
     

table    accuracies for learning algorithms with various improvements 
i attempted to improve the classification results using a number of different techniques  including
upweighting and normalizing the counts 

sentence position upweighting
intuitively  the beginning and the ends of a movie review would be helpful in classifying it as positive or
negative  since reviewers tend to summarize their views at those points  as a result  the classifiers might improve
the performance if we put more weight on these sentences 

   

     
     
     
    

    

     
    

     
     

    

     
    

    

     
    

     
 

   

   

   

   

   

fraction of sentences upweighted
svm  begin

svm  end

nb  begin

nb  end

figure    accuracies based on fraction of sentences being upweighted 

accuracy  nb

accuracy  svm

    

fieach term that appeared in one of the chosen upweighted sentences was counted twice  instead of only
once  the graph above shows that when upweighting the ending sentences  both classifiers had the best results
when they upweighted the     of the sentences  however  upweighting the beginning sentences actually decreased
the accuracy  the movie review data showed that although the ends of the reviews did tend to summarize the
author s opinion  the beginning the of the reviews usually summarized the movie itself 
the ideal proportion of sentences to upweight appears to be the last      implementing this upweighting
improved both naive bayes and svm  more sophisticated measures of position might help improve the accuracy
even more  such as somehow incorporating a terms position into the term features  ultimately though  the goal of
the sentence upweighting is to identify parts of the review that give a good description of the reviewer s opinion 
identifying important parts of the reviews  perhaps by using summarizing techniques  might help improve the
classification even more 

part of speech upweighting
the most discriminatory terms in the dataset were usually adjectives  such as  worst  or  outstanding  
adverbs  such as  poorly   also seemed helpful in determining how a reviewer felt about a movie  therefore  i also
applied upweighting based on whether a unigram was adjective or an adverb  i used the stanford nlp group partof speech tagger to get the part of speeach for the all the unigrams  if a unigram was an adjective or an adverb  i
multiplied its count by     
pos upweighting improved the accuracy slightly  compared to the other improvements  it appears to be
relatively useless  intuitively though  using part of speech would seem to be a useful component to incorporate 
perhaps upweighting based on part of speech could be limited to only the most  useful  adjectives and adverbs 

tf idf
instead of using the frequency counts of a term  i scaled it using its tf idf weight  term frequency inverse
document frequency   which gives a better measure of how important the term is to the dataset  term frequency was
set as the number of times a term appeared in a document divided by the total number of terms in the document 
inverse document frequency measures how many documents a term appears in  an unimportant term  such as  the 
or  and   would appear in many documents  and thus it would have a low idf 

tf  

  of occurrences of term in the document
  of occurrences of all terms in the document

idf   log

  of documents
  of documents the term appears in

the new value for a term is set as tf  idf   if we used only frequency counts  a term such as  the  would
have a high value  however  tf idf would scale the fequency count so it would have a much lower value 
for the svm classifier  tf idf weighting gave the single biggest improvement in accuracy  which points to
its importance in improving accuracy  however  the accuracy of the naive bayes classifier actually decreased
slightly  i speculate that using different equations for tf idf would transform the data so that it would match the
multinomial model better  for example  other work  rennie        has suggested using

tf   log       of occurrences of term in the document 
combining learners
i hoped to combine the naive bayes and svm classifiers to improve the results  i ran another svm
classifier over the outputs for the naive bayes and svm classifiers  the inputs for the new svm classifier were the
two probabilities  for the positive and negative class  given by naive bayes and the output from the svm with a
linear kernel  and the svm with a rbf kernel  the new svm classifier used a linear kernel  it also used the results
from the classifiers using tf idf weighting and upweighting 
combining the learners gave a very slight improvement  however  the improvement was so small that it
only correctly classified two more documents compared to the original svm classifier with a linear kernel 

fi   discussion
with the addition of the improvements  svm outperformed naive bayes  interestingly enough  the naive
bayes accuracy increased only a little with the addition of the improvements  by less than     in contrast  svm
showed huge improvements  by almost      without the improvements  svm with the rbf kernel gave much worse
results  however  once tf idf weighting was implemented  it gave similar results to svm with a linear kernel  and
even slightly better 
the misclassified documents tended to be ones that were more ambivalent  and pointed out both positive
and negative aspects of a film  in addition  misclassified documents would include the author s opinions on how
contrasting viewers might react to the film  for example  an author might give a positive review  but say   people
who don t like gross out humor will hate this movie   while it would be very easy for people to identify this
sentence as irrelevant to the author s overall opinion of the movie  it seems as if it would be a very difficult task for a
classifier  sentiment classification could be improved by identifying the most relevant sentences in the review  such
as sentences where the subject is the movie  which will probably be related to how the author feels about the movie  

references
grilheres  b   brunessaux s   leray p  combining classifiers for harmful document filtering  in riao      
http   www riao org sites riao      proceedings      papers      pdf
joachims  thorsten   svmlight       
http   svmlight joachims org 
mccallum  andrew kachites   bow  a toolkit for statistical language modeling  text retrieval  classification and
clustering        
http   www cs cmu edu  mccallum bow 
pang  b   l  lee  and s  vaithyanathan  thumbs up  sentiment classification using machine learning techniques  in
emnlp            
http   www cs cornell edu home llee papers sentiment pdf
rennie  j   shih  s   teevan  j   karger  d  tacking the poor assumptions of naive bayes text classifiers  in
icml      
http   haystack lcs mit edu papers rennie icml   pdf
siolas  g   d alche buc f  support vector machines based on a semantic kernel for text categorization  in
ijcnn      
http   ieeexplore ieee org iel                      pdf
toutanova  k   the stanford nlp group part of speech tagger        
http   nlp stanford edu software tagger shtml
turney  p  thumbs up or thumbs down  semantic orientation applied to unsupervised classification of
reviews  in proceedings of the   th annual meeting of the association for computational linguistics  acl  
         pg          
http   oasys umiacs umd edu oasys papers turney pdf

fi