cs      dimensionality reduction of neural data
zuley rivera alvidrez
introduction
in the experiments in the shenoy lab  monkeys make reaching movements while neural data is simultaneously recorded
from    electrodes  our ultimate goal is to understand how the population activity of the neural signals relates to the monkey s
behavior  recent advances in neural recording have allowed us to collect data from many neurons simultaneously  however  data
analysis techniques have not yet been modified to be able to take advantage of this  what is the best way to represent the relevant
information encoded in populations of neural activity  on an instant by instant basis  recent work in this field suggests that the
information encoded in the activity of the population of neurons can be represented in a lower dimensional space  in this project 
we use machine learning techniques to help us evaluate an experimental hypothesis involving neural plans encoded by the activity
of neurons in the premotor cortex of monkeys 
background
lets say youre in the forest  hunting for your dinner  there are a number of different animals you might choose  but you
have only one spear  in the distance  coming towards you  you see two animals  as the animals get closer  you instinctively
prepare yourself to throw your spear  how do you decide which animal to target  do you simultaneously plan throws to both
locations where the animals may emerge  thereby allowing the maximum flexibility based on the incoming sensory stimuli  or do
you plan your throw to one location  then the other  and alternate back and forth as it becomes clearer which animal is the better
target 
we are interested in understanding how the brain forms plans about competing actions  on an instant by instant basis 
instead of studying hunters choosing their prey  our lab studies monkeys making sensory guided reaching movements to targets on
a computer screen  see figure     while a monkey plans these reaches  we record the activity of neurons in the motor and
premotor  pmd  cortex of his brain  using experimental data from a more complex task  we seek a method that will afford a better
description and predictions than are possible with the current methods used in this field 

a 

    acquire touch
acquire fix

    target cue
delay period

    go cue     movement period
target acquired

b 
figure    schematic of delayed reach task   a  a monkey is trained to sit in front of a computer screen and reach to targets at locations
specified by a yellow square  the monkey is rewarded for touching this target within a fixed time window after a go cue is given   b 
neural activity is recorded while the monkey performs this task  the top panels indicate the different epochs of the task  and the black
tick marks represent the spiking activity of the population of cells recorded  sorted by tuning direction  the encircled region contains
spikes tuned to the upcoming reach location 

behavioral task
the data we will use for this project comes from a modified version of the delayed reach task  see figure     in this new
task  the   target task  two possible target locations are shown on the computer screen  after a delay period  one possible target is
removed and the go cue is given after a second delay  since the monkey is not rewarded unless he touches the target fast enough 
he cannot wait until the targets are disambiguated to make his plan 
during this delay period  where is the monkey planning to reach 

fifigure    schematic of   target task  this task is similar to the delayed reach task  but includes delay period during which the monkey
does not know to which target he will be cued to reach  at the end of the delay period  the   potential targets are disambiguated and the
go cue is given 

hypotheses
specifically  we want to know whether the monkey is planning a reach to both targets at once  or whether he is changing
his plan back and forth between plans to each target  there is evidence in the literature that would support either of these options
 cisek   kalaska        evidence of co coding two motor plans  horwitz   newsome        evidence for flipping plans  
by representing the firing rate  activity level  of each cell as a point in a multidimensional space  for each point in time 
we can plot out how the monkeys plan trajectory evolves through time  in fig   a  each axis represents the firing rate of one
neuron  in the delayed reach task  the point near the origin is the baseline activity of the cells recorded  and as time elapses  the
firing rates of the cell create the path towards the grey cloud  when the firing rates of the population fall into the grey cloud  the
monkey is ready to make the movement corresponding to this subspace  here a rightwards reach  this optimal subspace
hypothesis is a common way our group thinks about preparatory motor activity  and is described in churchland et al        
in the   target task  if the monkey is planning to both targets at once  we should see a trajectory that looks similar to that
shown in fig   b  conversely  if the monkey is flipping his plan between targets  we should see a trajectory in this space that looks
more similar to the cartoon in fig   c 

a 

b 

c 

figure    cartoons of the optimal subspace hypothesis   a  plan trajectory in firing rate space  for the delayed reach task   b  expected
plan trajectory for the delay period of the   target task  if the monkey is planning reaches to both target locations simultaneously   c 
expected plan trajectory for the delay period of the   target task  if the monkey is flipping his plan between both target locations 

dimensionality reduction techniques
in this project  we use linear dimensionality reduction techniques  the use of popular non linear methods  lle and
isomap  was investigated previously and their performance was found to be inadequate when compared to the performance of
linear methods such as principal component analysis  pca   linear methods have the advantage of reliability and low
computational complexity  they are guaranteed to show genuine properties of the original data and to produce axes that are
meaningful in that they are linear combinations of the original axes 
pca  principal component analysis  pca  is one of the most popular and well understood methods for dimensionality
reduction  the data is projected into a set of uncorrelated variables called the principal components  each principal component
accounts for as much of the variability of the data as possible  the disadvantage of the technique is that it does not learn a
generative model for the data  and one cannot compute the likelihood of the data 
factor analysis and sensible principal component analysis  factor analysis  fa  and sensible principal component
analysis  spca   have a similar underlying static model of the data 

y   cx   v

x   n     i  

v   n     r 

 y   n     cc t   r 

where x is the underlying or latent state vector  y is the vector of observations  and v is the observation noise with covariance
matrix r  the key assumption of fa is that the matrix r is diagonal  that is  it puts the variance unique to each coordinate in r

fiand the correlation structure in c  spca  however  assumes that r is a multiple of the identity matrix i  where  represents the
global noise level 
both models are learned using the em algorithm  see roweis        
e step  q i   x   i       p   x   i     y   i     c   r     n    y   i     i   c     x   i   where    c t   cc t   r     
let

x be the expected value of the latent state vectors x found in e step  then the m step becomes 

m step  c

new

 

t

  y x      r new ii   yy t  c x y t

 

ii

  n  where    ni  nc   xx

t

where y is the p x n mean subtracted matrix of n observations of p variables 
the em algorithm for spca is the same  except that r is now computed as i where
p

 


  yy t  c x y t

 

jj

 p

j

weighted pca  while pca and spca had been previously found to produce results that best reflect the expected trajectories of
the neural data  both methods suffer from sensitivity to outliers  furthermore  being unsupervised methods  they do not allow for
incorporation of known labels or structure of the data into the projection algorithm  because the directions of the principal
components that maximize the variance are not necessarily the same as the directions that maximize the separation between
classes  pca does not always perform well when used as a classifier  in order to address both shortcomings  the concept of
weighted pca has been recently proposed  koren         the objective of pca can be expressed as finding the p dimensional
projection that maximizes the sum of the squares of the euclidian distances in the p dimensional space between vectors i and j of
the training set 

using the concept of laplacian matrices and basic linear algebra  this objective can be generalized to the weighed sum 

where dij are the elements of the dissimilarity laplacian matrix  laplacian matrix is a symmetric positive semi definite matrix
with zero sums of rows and columns  by adequately defining this weight matrix  one can incorporate labels  supervised pca  
and or reduce the sensitivity to outliers by making the weights inversely proportional to the euclidian distance  normalized pca  
the n by b laplacian matrix can be easily constructed by setting 

the new objective function can still be solved optimally using an eigenvalue decomposition approach 
results
the em algorithm for factor analysis was implemented and applied to neural data for two specific target locations for the simple
delay reach task of fig     the same projections were found using spca  the trajectories obtained using both methods are shown
in figure    while  it is expected that factor analysis which assumes different noise variances per coordinate will perform better
than spca  which assumes equal variances  the trajectories for this data set were not significantly different  figure    a b  and c 
shows the trajectories obtained for the modified task including an ambiguity period  the planning end point clusters corresponding
to the two targets shown in the screen are presented in the figure as well as the neural trajectories during the ambiguous period in
which the monkey does not know to which target he will be reaching  for two sets of targets  fig   a  and  b   the trajectories
appear to be in a region in between the two target clusters  for a third pair of target locations  fig   c   the trajectories are clearly
biased towards one of the target clusters  figure  d shows the trajectories when the monkeys guess of the target location is wrong 
once the target is disambiguated  the monkey corrects its neural plan 
in order to understand whether something interesting was occurring in the trajectories shown in figures  a and  b  the endpoints of
the trajectories at the end of the ambiguous period were projected to a   dimensional space using pca and weighted pca  if the
monkey is planning then the end points should be separable from the baseline state of the trajectories  furthermore  if the plan
takes into account the location of the targets presented  the end points of the two sets of trajectories should be themselves
separable  figure  a  shows that the pca projection is dominated by outliers in the data  a projection using supervised pca with

fibinary weights  weights of vectors belonging to the same set of targets were set to zero  and those belonging to different target sets
were set to one  was produced  but as shown in figure  b  it still suffers from high sensitivity to outliers  the results of normalized
pca  which is designed to reduce the effect of outliers  are shown in figure  c  the mean of the clusters is different for the two
sets of targets  and also separated from the mean of the baseline state  finally  maximum separation was obtained with a mixture of
normalized and supervised pca  as shown in figure  d  figure  e shows the result of projecting the data for all sets of targets
using normalized and supervised pca  the projected end points show three classes which correspond to the pairs of targets that
were presented together  these results support the idea that the monkey is planning  and its plan is different depending on the
targets presented 

 a 

 b 

figure    neural trajectories for simple delay reach task for two target locations projected into a three dimensional space using spca a 
and factor analysis  b   the initial state is shown in red and blue dots for each target  the end points of the planning period are shown
in black and green  and the end points at the movement onset are shown in blue and yellow 

 a 

 b 

 c 

 d 

figure    neural trajectories for the task with ambiguity period  the planning clusters corresponding to the pair of targets presented are
shown in green and black dots  the pink traces are the trajectories followed during the ambiguous period for the three possible locations
of the two targets presented  a  b and c   panel  c  suggests monkey is planning to reach to a particular target  panel  d  shows the
trajectories when the target chosen by the monkey is not the one that he is instructed to reach  the monkey corrects his plan 

conclusions
the objective of this study was to provide insight into the neural mechanisms of decision making by the examination of the neural
activity of a monkey that is presented with a choice  for two sets of targets locations  it appears that while the monkeys planning
activity is different from baseline  it is not biased towards any of the targets  supporting the idea that the monkey is planning to
both targets simultaneously  for the third set of target locations  the monkeys neural plan is clearly biased towards one of the

fitargets  current work included correlating the neural data with behavioral measurements  such as reaction time  in order to provide
more insight to the results obtained 
different projection methods were employed to evaluate the results  it was found that for this particular dataset factor analysis and
spca produced very similar results  this is likely due to the fact that most of the neurons included exhibited similarly high firing
rates and hence  similar noise variances  assuming a poisson model in which variance and mean are the same   for data recorded
from neurons with more different firing rates  it is expected that factor analysis will outperform spca by assuming different
variance for each neuron  our results also show that weighted pca can be used to overcome the shortcomings of simple pca with
the same advantages of linear projection methods 

 a 

 b 

 c 

 d 

 e 
figure     a d  end point analysis of the trajectories corresponding to figs   a  green dots  and b  blue dots  obtained using pca  a  
weighted or supervised pca with binary weights  b   normalized pca  c   normalized and supervised pca d   the black dots represent
the mean of each cluster  the red dots are the initial state  panel e shows the output of normalized and supervised pca when all of the

fitrajectory endpoints are provided  the clustering method produced three clusters  green and purple  yellow and blue  and cyan and red 
corresponding to targets that were shown together 

acknowledgements
this work was done in collaboration with rachel kalmar  background  motivation   afsheen afshar  discussions  help with data
processing   and various other students from shenoys lab who contributed with data collection and analysis 

references
afshar a and cunningham jc  using embedding algorithms to find a low dimensional representation of neural activity during motor
planning cs    final project  fall      
churchland mm  yu bm  ryu si  santhanam g  shenoy kv         neural variability in premotor cortex provides a signature of motor
preparation  j neurosci  apr                   
cisek and kalaska  simultaneous encoding of multiple potential reach directions in dorsal premotor cortex  j neurophysiol                
horwitz and newsome         target selection for saccadic eye movements  prelude activity in the superior colliculus during a direction
discrimination task  j  neurophysiol                
koren y  and carmel l          robust linear dimensionality reduction  ieee transactions on visualization and computer graphics        
         
roweis s  and ghahramani z          a unifying review of linear gaussian models  neural computation                

ficode
the matab code produced for this project built on the extensive code for data processing produced by several students in shenoys lab 
furthermore  plotting routines  preprocessing of neural data sets  and various other tasks utilized code written by agsheen afshar and john
cunningham  such code is not included here for length considerations  also not included is code that was only slightly modified  additional or
complete code can be provided upon request 

  this matlab script produces three dimensional projections using scpa or pca of the
neural data saved in specified mat files for the time period specified  planning 
ambiguous  all  etc   it also performs end point analysis using pca  supervised pca 
normalized pca and supervised normalized pca 
option    
loaddata     
addpath  formark   
 load data
if loaddata 
load  matfiles r       mat   
load  matfiles bu       mat   
end
 order data according to target location
or   trialorderarraygrid rnew  
 get the target coordinates
for i     max size or   
targets i    or i  r    trialparams targetangulardirection 
tar cor   i     or i  r    trialparams target x   or i  r    trialparams target y  
end
 clear or
if option     
 specify which targets are to be projected as well as the neural parameters of the data
 that is to be projected
tars          
tars        
colorcue        
colorcue          
 delay         
 delay        
 get
tp  
cc  
td  
tt  

neural data satisfying the parameters specified 
 rnew trialparams  
 tp timecolorcue  
 tp timedelay  
 tp targetangulardirection  

 build new r based on specs
 valid     cc    colorcue      td    delay      ismember tt targets tars      
 valid     cc    colorcue      td    delay      ismember tt targets tars      
valid     cc    colorcue      ismember tt targets tars      
valid     cc    colorcue      ismember tt targets tars      

fir new   rnew valid    valid   
tp    r new trialparams  
cc    tp timecolorcue  
for i   max size r new  
if  cc i     colorcue  
r new i  trialparams target x   r new i  trialparams target x   
end
end
or new   trialorderarraygrid r new  
clear t coor 
for i     max size or new   
t coor   i     or new i  r    trialparams target x 
or new i  r    trialparams target y  
end

 print stats
fprintf    found    sprintf   d   size r new        trials out of    sprintf   d  
size rnew        n    
fprintf   for specs    found     sprintf   d   sum valid       n    
fprintf   for specs    found     sprintf   d   sum valid       n    
 get data for projection
tartmp     max size or new   
fprintf   n getting data     n   
 data  tbegs  tgo  color data  tbounds r    funcmarkdata or new  bestunits   tars  
tartmp    distoperimove     
fprintf  done  n   
figure    
ax      
fprintf  projecting onto lower dimensions    n   
 specifying the option dofa produces projections using factor analysis  the
 default option is spca
 y  newbounds  alltrajs  color trunc pm mu    funcrunprojectz data  tbegs  tgo 
color data  tbounds  ax   shouldplot     

elseif option      
 end point analysis
 specify targets
tars          
tars        
 get data 
colorcue           
colorcue          
delay       
tp    rnew trialparams  
cc    tp timecolorcue  
td    tp timedelay  

fitt    tp targetangulardirection  
 build new r based on specs
 valid     cc    colorcue      td    delay     ismember tt targets tars      
 valid     cc    colorcue      td    delay     ismember tt targets tars      
valid     cc    colorcue      td    delay    ismember tt targets tars      
valid     cc    colorcue      td    delay     ismember tt targets tars      
r new   rnew valid    valid   
tp    r new trialparams  
cc    tp timecolorcue  
for i   max size r new  
if  cc i     colorcue  
r new i  trialparams target x   r new i  trialparams target x   
end
end
or new   trialorderarraygrid r new  
for i     max size or new   
t coor   i     or new i  r    trialparams target x 
or new i  r    trialparams target y  
end
 find correct order
 print stats
fprintf    found    sprintf   d   size r new        trials out of    sprintf   d  
size rnew        n    
fprintf   for specs    found     sprintf   d   sum valid       n    
fprintf   for specs    found     sprintf   d   sum valid       n    
 get data for projection
tartmp     max size or new   
fprintf   n getting data     n   
 data  tbegs  tgo  color data  tbounds r    funcmarkdata or new  bestunits   tars  
  max size or new      onlydisamb     
fprintf  done  n   
figure    
ax      
fprintf  projecting onto lower dimensions    n   
 pmw  pmn  pmnw  pca  x    pointwpca data tbounds  color data    max size or new       
end

  get some rts
 
clear meanrt 
for i     max size  or new   
meanrt i    mean  or new i  r marksrt   
end

fi factor analysis em algorithm

  this function performs factor analysis on the data matrix given 
  see roweis      
 
  varargin 
 
shouldplot    if should plot 
 
function  pm  r out    facan  data k  varargin 
shouldplot     
assignopts who  varargin  
  note k   d  the desired embedding dimension
  note p   n  the size of the input dimension
p   size data    
n   size data    
y   data  
tol likelihood    e       e   is safe and thorough
  em algorithm
c     rand p k  
r   diag rand   p     initialize diagonal matrix to random values
likelihood       
converged     
iter     
learning      
 iterate em
while converged         iter     
iter   iter     
fprintf   factor analysis em iteration    sprintf   d  iter     likelihood value 
  sprintf   d  likelihood    n    
  e step
beta   c   inv c c  r   
x hat   beta y 
v  eye k  beta c 
delt   y x hat  
gam   x hat x hat  n v 
  m step
c new   delt inv gam  
r new   diag diag y y  n c new delt  n   
 
c new
 
r
  check for convergence
s   c new c new  r new 
likelihood new   sum       sum y   inv s  y       sum log svd s      log   pi      
assume y i are iid     log likelihood of whole training set  for comparison with proper
likelihoods  this equation needs revision

fiif  abs  likelihood new likelihood  likelihood  tol likelihood 
converged     
end
  update model parameters
c old   c 
c   c new 
r old  r 
r   r new 
likelihood old   likelihood 
likelihood   likelihood new 
learning iter  likelihood 
end
r out   r 
pm  c  inv c c  r    beta is the projection matrix
pm   pm  
  have now learned the model y n   c c    eps i   so we can evaluate test data 
  plot learning curve
  pc   c  inv c c  eps  eye p   
  pc  pc  
if shouldplot     
figure 
plot learning  
xlabel  em iteration   
ylabel  log likelihood   
title  learning curve of spca em algorithm   
end

 weighted pca
 for as many targets as desired  finds projection that maximizes distance between
target endpoints    
 or distance between startpoints and target endpoints    
function  pmw  pmn  pmnw  pca  x    pointwpca datatrunc  tbounds  color data tars 
option  
mu   repmat mean datatrunc      size datatrunc     
 just retain end points
n   size datatrunc      num of vectors
m   size datatrunc      num of vars
x    datatrunc mu      nxm

if  option     
 pick only endpoints
x   x tbounds            x is now trials xm
n  max size tbounds   
 figure out how many trials per target from color data
p     
for i    max size tars  
ta i    sum  color data       i   
targ i    find tbounds        ta i  p  

fip  p ta i  
end
lap   ones n n  
ind     targ   end       
ind  targ 

sta  cumsum ta  

 give weights of zero to data from same class  same target 
for i   max size targ  
lap ind  i  ind  i  ind  i  ind  i      
end
 ensure laplace conditions
for i   n
lap i i    
lap i i     sum lap i     
end
 solve using svd decomposition
ss   x  lap x 
 u d v     svd ss  
p   v         
pm   p 
yn   x p 
tr    datatrunc mu   p 
colors      b   g   r   c   m   y   k   
figure    
for i   max size tars  
plot  yn ind  i  ind  i      yn ind  i  ind  i     yn ind  i  ind  i       colors i 
 o    
hold on
st i      target  num str tars i    
end

hold off
legend st  
 plotembedded y newbounds  color trunc  
 do regular pca
sig    x  x 
  v  d    eig sig   
 u  s  v    svd sig   
pca   v         
ypca   x pca 

fifigure    
for i   max size tars  
 plot  yn   t      yn   t     yn   t       go   
plot  ypca ind  i  ind  i      ypca ind  i  ind  i     ypca ind  i  ind  i     
 colors i   o    
hold on
end
hold off
 ntrials      
ntrials   max size tbounds   
tbounds    tbounds     ntrials  
tr    tr   tbounds  end end     
colort   color data     tbounds  end end   
 plotembedded tr   tbounds  colort  

elseif  option       pick starting points  put them in one class and separate from
other endpoints

x    x tbounds            x is now trials xm
x    x    x tbounds            append starting pts
n  max size tbounds     
 figure out how many trials per target from color data
p     
for i    max size tars  
ta i    sum  color data       i   
targ i    find tbounds        ta i  p  
p  p ta i  
end
sta  cumsum ta  
 give weights of zero to vectors from same class
lap   ones n n  
ind     targ   end      targ end     
ind   targ n  
for i   max size ind   
lap ind  i  ind  i  ind  i  ind  i      
end
 enforce laplacian conditions
for i   n
lap i i    
lap i i     sum lap i     
end

 solve using svd decompostions
ss   x  lap x 
 u d v     svd ss  

fip   v         
pmw   p 
yw   x p 
tr    datatrunc mu   p 
colors      b   g   r   c   m   y   k   
yn   yw 
meani    
 plot data and compute means of clusters
figure    
for i   max size ind   
meani i     mean  yw ind  i  ind  i        
plot  yn ind  i  ind  i      yn ind  i  ind  i     yn ind  i  ind  i     
 colors i   o    
hold on
if i   length tars  
st i      target  num str tars i    
else
st i     target onset  
end
end
legend st  
title   binary weighted pca   
plot  meani       meani       meani         k   linewidth    
hold off
tr    datatrunc mu   p 
  ntrials     
ntrials   max size tbounds   
tbounds    tbounds     ntrials  
tr    tr   tbounds  end end     
colort   color data     tbounds  end end   
 plotembedded tr   tbounds  colort  
                                                                        
 do regular pca
sig    x  x 
 u  s  v    svd sig   
pca   v         
ypca   x pca 
figure    
for i   max size ind   
meani i     mean  ypca ind  i  ind  i        
plot  ypca ind  i  ind  i      ypca ind  i  ind  i     ypca ind  i  ind  i     
 colors i   o    
hold on
if i   length tars  
st i      target  num str tars i    
else
st i     target onset  
end
end
legend st  

fititle  pca   
plot  meani       meani       meani         k   linewidth    
hold off
                                                                  
 do normalized pca
clear dist lap
t      decay factor
dist        distance x  x        
lap   dist 
 enforce laplacian conditions
for i   n
lap i i    
lap i i     sum lap i     
end
 solve using svd
ss   x  lap x 
 u d v     svd ss  
p   v         
pmn   p 
yn   x p 
 plot
figure    
for i   max size ind   
meani i     mean  yn ind  i  ind  i        
plot  yn ind  i  ind  i      yn ind  i  ind  i     yn ind  i  ind  i     
 colors i   o    
hold on
if i   length tars  
st i      target  num str tars i    
else
st i     target onset  
end
end
legend st  
title  normalized pca   
plot  meani       meani       meani         k   linewidth    
hold off
 do normalized plus binary weighted pca
                                                
lap   dist 
for i   max size ind   
lap ind  i  ind  i  ind  i  ind  i      
end
for i   n
lap i i    
lap i i     sum lap i     
end
ss   x  lap x 
 u d v     svd ss  

fip   v         
pmnw   p 
yn   x p 

figure    
meani      
for i   max size ind   
meani i     mean  yn ind  i  ind  i        
plot  yn ind  i  ind  i      yn ind  i  ind  i     yn ind  i  ind  i       colors i 
 o    
hold on
if i   length tars  
st i      target  num str tars i    
else
st i     target onset  
end
  plot  meani     meani     meani       k   linewidth    
end
legend st  
title  normalized and supervised pca   
plot  meani       meani       meani         k   linewidth    
hold off

tr    datatrunc mu   pmnw 
ntrials   max size tbounds   
tbounds    tbounds     ntrials  
tr    tr   tbounds  end end     
colort   color data     tbounds  end end   
plotembedded tr   tbounds  colort  

 option    separate targets gradually with time
elseif option    
lap  ones n n  
 figure out how many trials per target
p     
for i    max size tars  
ta i    sum  color data       i   
targ i    find tbounds        ta i  p  
p  p ta i  
end
tr   size targ    assume for now two targets

for i     targ   
 get trial i matrix
tsi  tbounds   i  
tei   tbounds   i  
mi   x tsi tei    
ni   size mi    

fi 

lap tsi tei tsi tei   intt ones ni ni  
for j   targ       targ   
tsj  tbounds   j  
tej   tbounds   j  
mj   x tsj tej    
nj   size mj    
mit   mi 
 matrices must have same dimensions
if ni  nj 
mj   mj   ni    
nt   ni 
else
mit   mit   nj    
nt  nj 
end
 compute distance between trials
 dtr         sum  abs mit mj                        

dtr   exp       nt   
 dtr         sum  abs mit mj                         
 dtr    e  ones   nt  
mat   ones nt nt    eye nt    diag dtr  
lap tsi tsi nt   tsj tsj nt     mat 
lap tsj tsj nt   tsi tsi nt     mat 
end
end

 laplace conditions satisfied 
for i   n
lap i i    
lap i i     sum lap i     
end

ss   x  lap x 
 u d v     svd ss  
p   v         
wpca   p 
y   p  x  
  plotembedded y tbounds color data  
tr    datatrunc mu   p 
ntrials   max size tbounds   
  ntrials   max size trials   
tbounds    tbounds     ntrials  
tr    tr   tbounds  end end     
colort   color trunc     tbounds  end end   
plotembedded tr   tbounds  colort  

end

fi