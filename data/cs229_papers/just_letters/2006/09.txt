cs    project  factor analysis with partially
observed training data
paul csonka  barrett heyneman  salomon trujillo

   motivation
factor analysis is typically applied to problems when we have a set of m training
examples x i   rn   where n  m  we then model the x i  s as generated by a
latent random variable z  i   rk and noise term   rn with a diagonal covariance
  where
x i       z  i    
z  n     i 

   

  n      
or equivalently
x  n    t    

   

consider a similar situation in which  instead of the full vector x i    we only
 i 
observe some part of x i    a vector v  i   rn   n i   n  such that
t

v  i    j  i  x i 

   
 i 

j  i   rnn is a full rank matrix  with orthonormal columns  if we let r i  
n i 
r
consist of the indices of x i  which are observed  i e  present in v  i    then
   

j  i   

h

er i 
 

 

we can then express the distribution of v
   

   

er i 
 i 

as

i




t
t 
v  i   n j  i    j  i  t    j  i 

if we define the following variables

t

 i    j  i  
   

t

 i    j  i  
t

 i    j  i  j  i 
then we can express the joint distribution of z  i  and v  i  as
 

fi 

paul csonka  barrett heyneman  salomon trujillo



   

z
v  i 



n



  
 i 

  
 

i
 i 

t

 i 
 i   i  t
 
   i 

  

   the e step
to apply the e step of the em algorithm to our modified factor analysis problem
we need the conditional distrubition z   v  i     i     i     i   n  z i   v i    z i   v i   
where
  

t
 i   i     i 
v  i    i 

 
t
t
  i   i   i   i     i 
 i 

z i   v i     i 
z i   v i 

t



therefore  qi  z  i    is simply the pdf of this distribution 
   the m step

the m step is not as straight forward as the e step  and requires results found
in appendix a  after removing terms that dont depend on the parameters and
replacing the integral over z  i  with an expectation  the algorithm must maximize
the log likelihood of the data  which is
m
x
i  

   

 

h

i
ez i  qi log p v  i    z  i     i     i     i 

m
x



n
 
log  i     log    
 
 

t


   i 
 i 
 i   i 
 i   
 i 
 i 
 i   i 

v   z

v   z
 

ez i  qi

i  



here ez i  qi indicates the expectation is with respect to z  i  drawn from the
distribution qi   since there is no ambiguity  we will drop the z  i   qi  subscript 
     m step for   first  the log likelihood is maximized w r t    we must
t
t
replace  i  and  i  with j  i   and j  i   respectively and then take the gradient
w r t    dropping terms with no  dependence we get
m
x

 
t


 
   i 
v   i    i  z  i   i 
v  i    i    i  z  i 
 
i  

m
x
t
 
t
 
 e  tr z  i  t j  i   i  j  i  z  i 
 
 
i  


t
 
t
  z  i  t j  i   i 
v  i   j  i  



 

m
x
i  

e

i


h
t
t
 
t
t
 
v  i   j  i   z  i 
e j  i   i  j  i  z  i  z  i    j  i   i 

fics    project  factor analysis with partially observed training data

substituting the definition of  i 
 

m
x



 

t

t

  tr j
 

from    yields

e  j  i  j  i    j  i  j  i  z  i  z  i 

i  

m
x

 

 i 

j

 i  t

t

 



j

 i 



v

 i 

j

 i  t



 i  t
 z

i


h
t
t
e  i     i  z  i  z  i     i    j  i  v  i    i   z  i 

i  

using the fact that diagonal matrices commute  and the result from    we get
m
x

 

i  

h


i
t
t
e    i  z  i  z  i       i  j  i  v  i    i   z  i 

because  i  is not necessarily full rank  we can not at this point set the equation
equal to zero and solve for   however  if we set the equation equal to zero and
multiply by ej t   for j      n  we are left with n independent equations 
m
x
i  

h


i
t
t
e ej t    i  z  i  z  i    ej t    i  j  i  v  i    i   z  i     

since   and  i  are both diagonal  with j th entries
t

 

then ej 

 i 



ij
t
j ej  

 

 
j

and ij respectively 

also  using    gives us

m
x




ij  t  i   i 
ij t  i 
t
t
 i  t
ej z z i   
e 
 
ej j v  ej  z
j
j
i  

since ij     iff i  sj   we can instead sum just over the set sj and drop the
ij s 


x   
t
t
   t  i   i 
ej j v  ej t  z  i 
e  ej t z  i  z  i   
 
j
j
isj

ej  selects j t   the j th row of   allowing us to independently solve for the
 i 
n rows of   additionally  ej t j  i  v  i    xj and ej t    j   which allows us to
further simplify the equation as
t


 x  

 
  t  i   i  t
 i 
 i  t
e
 
e
j z z
xj  j z
j
j
isj
isj
i

i x h
x h
t
t
 i 
e xj  j z  i 
e j t z  i  z  i   
x



isj

isj

j t

x

isj



j t   

x

isj

 i 

h

e z

 i   i  t



z

i

 

x

isj

h

xj  j ez i  qi z

 i  t

i

 h ti
 i 
xj  j e z  i 




x

isj

 
i
h
t
ez i  qi z  i  z  i  

fi 

paul csonka  barrett heyneman  salomon trujillo

and after applying the rules for the expectation of z  i 

   



j t   

x

isj





 i 

xj  j tz i   x i   

x

isj

t

t

and z  i  z  i   
 

z i   x i  tz i   x i    z i   x i  

since each row j t deals with a different set sj   each must be calculated seperately and concatenated to form the full  
   m step for  and 
similarly  the log likelihood must be maximized w r t   and   these two
derivations are very similar  and are therefore worked out in appendix b  only the
resulting equations are shown here 
    

j  


  x   i 
t
x



 i 
 i 
j
z  x
j
m j  is
j

    

j  



 
  x   i 
xj  j  j t z i   v i    j t z i   v i  j
m
isj

   algorithm results
the algorithm was implemented in matlab  both pseudo code of this implementation and more in depth results of tests and netflix iterations are presented in
appendix c and d  respectively 
several extensive tests were conducted over a wide range of parameter values 
in order to validate the algorithm  the parameters include   m n  k    fraction
of remaining training data   and relative noise  the bulk of our empirical testing
was done on three windows machines  one to run full sized netflix predictions  and
two desktops for parameter variation  the approximate times for one test were   
hours for a single netflix iteration  and      hours for the two desktops running
one complete multiple parameter test  results are shown in appendix d 
our netflix results indicate that the algorithm does in fact converge  with returned accuracies reported by netflix to improve from r        to       from
the  st to the  th iteration  however  the accuracy appears to be approaching an
asymptotic limit  leading us to believe that there may be several improvements
required before we could achieve better results  specifically  next we should remove outlier users  and take the penalty of predicting for them inaccurately  with
a  matrix that does not include those users  however  presumably with the improvement in accuracy for the bulk of the users  our overall error decreases  we
have the cutoffs determined  but have not implemented this portion thus far  we
have also seen that for a given randomly generated data and user preferences set 
specific users consistently contribute to high condition numbers k for the inverted
matrix t     removing these users in the manner described should improve our
predictions  this has not been tested yet 
the two desktops ran a battery of tests to find optimal performance based on
varying parameters  first  we have confirmed a few obvious properties of the algorithm  in order to verify that we are running correctly  for example  the r  value

fics    project  factor analysis with partially observed training data

 

remains relatively high until about    percent of the training data is retained 
whereupon most m n ratios converge to the same final prediction accuracy  we
tested m n ranging from     to        the accuracy of predictions improved for the
sets with more data  as the ratio increased  after about       though improvement
is observed  it is not significant  for the netflix challenge  this actual ratio of the
provided data is about       this is inside our found optimal regionat least  for the
algorithm we presented  and for the data generated  however  with approximately
only   percent of the training data present in netflix  we can safely confirm that
this is a hard problem   
as the number of features  k  increases  computation becomes much more expensive  going at least as k     based on observing the iteration time for a single
convergence  for a representative range of parameters  weve found that for the
majority of m n  roughly k  n   gives the fastest results  with not much significant improvement for very small k  the overhead of the rest of the code might be
dominating here  for roughly k  n  the computation becomes many times more
lengthy  especially for large m n ratios  please see fig    
it appears that when taking into account r  accuracy and trying to limit computation time  the optimal values  assuming parameters other than m n fixed  are
roughly n    k  n  if this holds for the netflix data  then the maximum k     
we have used for the full data set is too small  ks larger than    crash our computers  and so aside from a single iteration becoming impractically long  memory
limits prohibit testing this result on our challenge submissions  note that with a
larger fraction of data left out during trainingmore than    percentand with high
m n ratios  for small k  k      the iteration times are found to be roughly    
times as high as for runs with k       essentially  this expresses what weve noted
previously  when there is lots of missing data  m  n   having a small k means
that it is very difficult to accurately reproduce the underlying structure and correlation matrix  leading to more cycles to convergence inside each iteration  hence 
the iteration times increase  however  once we have sufficient k  roughly k  n  
as shown previously  data reconstruction becomes easier  and iteration times drop
several fold despite the larger k 
with these empirical results  we have found some bounds that improve the performance of this specific algorithm  the cumalitive results have not been added to
our algorithm yet  and remains for future work 
   conclusions
the e m algorithm  as defined by these updates  runs until convergence as indicated by either very small changes in the frobinius norms of consecutive     and
  or r  prediction error  important observations are
 optimal performance is reached when at least     of the data is observed 
 training outliers noticably effect prediction accuracy 
 higher m n ratios improve prediction accuracy  all other parameters fixed 
 larger k improves prediction accuracy  all other parameters fixed 
 computation time becomes prohibitively long for both large m n ratio and
large k  implying a criteria for optimizing the choice of k 
in summary  we believe the developed and tested algorithm is sound  and a useful
method for attacking the netflix challenge 

fi 

paul csonka  barrett heyneman  salomon trujillo

appendix a  useful definitions and relations
this appendix defines expressions and relations which are used in the derivation
of the e and m steps of the modified factor analysis algorithm 
a    properties of j  i    because j  i  is full rank and its columns are basis vectors
in rn   we can define a diagonal matrix  i   rnn such that
t

j  i  j  i     i 

    

t

j  i  j  i    in i 

from these two definitions we can see that
t

j  i    j  i  j  i  j  i 

    

   i  j  i 

a    inverse of  i    given the definitions of j  i  and  i    we need to compute
 
 i    since  and  i  are both diagonal  we expect that
 i 

    

 

t

  j  i    j  i 
 

to prove that this is the case  compute  i   i  and check the result  recall
that       and  i  are diagonal  and that diagonal matrices commute 
 i   i 

 

t

t

   j  i  j  i    j  i    j  i   
t

  j  i   i    j  i 
t

  j  i   i    j  i 
t

  j  i   i  j  i 
t

t

  j  i  j  i  j  i  j  i 
 i
clearly  because of the commutative property of diagonal matrices  the same
 
result holds for  i   i 
a    relation between  i    j  i    ij   and the set sj   consider what the
 i 
columns of j  i  mean  if ej is a column of j  i  then xj is observed  i e  is present
in v  i    therefore   i  has another interpretation 

    

 i 
j

 
ij
 
 

if j   
if j  
  

where

    

 
 i 
  if xj is observed
ij  
 i 
  if xj is unobserved

fics    project  factor analysis with partially observed training data

 

from this definition one can see that
 

 i     i 

    

we can then define a set sj equivalently as the set of all training example indices 
i  such that





 i 

xj is observed
ej is a column of j  i 
ij    
 i 
jj    
 i 

finally  we can define the number of examples for which xj was observed as
    

m j     sj  

fi 

paul csonka  barrett heyneman  salomon trujillo

appendix b  m step for  and 
the derivations for  and  are presented in this section  only the resulting
equations were shown in the paper body 

b    m step for   the log likelihood must be maximized w r t    first ret
t
place  i  and  i  with j  i   and j  i    and then take the derivative w r t   
dropping terms with no  dependance we have

m
t


 x
    i 
 i  t
 i  t
 i 
 i   
 i 
 i  t
 i  t
 i 
e 
v j
j
z

v j
j
z
 i  
 
 

m
x
i  

i

h
t
 
t
 
v  i   j  i  z  i 
e j  i   i  j  i     j  i   i 

using the definition of  i 

 

m
x
i  

 

m
x
i  

 

and  i  from    and    respectively we have

i

h
e  i     i      i    j  i  v  i    i  z  i 
h

i
e    i        i  j  i  v  i   z  i 

to maximize the w r t   we set the equation equal to zero  once again  the
equation is simplified if we isolate the elements of   one at a time  by multiplying both sides by ej t   and then deal with the resulting n independent equations 
following similar steps as in the  derivation we are left with
x

e  j    

isj

x

isj

h i
 i 
xj  j t e z  i 

since j does not depend on the summation over sj and is constant  we can
replace
the left hand side with m j  j   on the right side we substitute the definition
  i 
for e z   the update for j becomes
    

j  


  x   i 
t
x



 i   x i 
j
z
j
m j  is
j

b    m step for   to estimate  we maximize   with respect to   expand
t
t
t
  by replacing  i     i    and  i  with j  i    j  i    and j  i   respectively 
then drop terms with no  dependence  take the gradient w r t    and distribute
j  i  to get

fics    project  factor analysis with partially observed training data

 


m
x
t
 
e  log j  i  j  i   
 
i  

t


   i 
 i 
 i  t
 i  t
 i 
 i  t
 i  t
 i 
 i  t    i 

v j
j
z
v j
j
z
j
 j
 


m
t


x
e      j  i  v  i    i     i  z  i  j  i  v  i    i     i  z  i   
 


i  

if we set this equal to   and both left and right multiply by   which is invertible 
we find that the equation is maximized by
 


m
t 

  x
e j  i  v  i    i     i  z  i  j  i  v  i    i     i  z  i 
m i  

at this point we use equation    to replace the last remaining j  i 
 



m
t


t
  x
e  i  j  i  v  i     z  i  j  i  v  i     z  i   i 
m i  

however  we have restricted  to be diagonal  so let us select only the diagonal
elements of both sides of the equation by left and right multiplying by ej t and ej
respectively  following the same steps as used in the  and  derivations  recall
 i 
that ej t j  i  v  i    xj and ej t    j   we see that


m
t


t
  x
e ej t  i  j  i  v  i     z  i  j  i  v  i     z  i   i  ej
m i  
i
h

h i
 

t
  x   i 
 i 
 
xj  j     xj  j j t e z  i    j t e z  i  z  i  j
m

j  

isj

plugging in for the definitions of the expectation of z  i  and z  i  z  i 
plifying yields
    



 
  x   i 
t
t
j  
xj  j  j z i   v i    j z i   v i  j
m
isj

t

and sim 

fi  

paul csonka  barrett heyneman  salomon trujillo

appendix c  implementation details
the application which motivated development of this algorithm was the netflix
challenge  a challenge to predict users ratings of unseen movies based on all known
rating data  the netflix data consists of sparse ratings for        movies by        
users  at first we considered a training example to be the vector of all users
ratings for a given movie  giving n            and m            even considering
t
the sparsity of the data  the square matrix   i   i     i    would have been  at
worst  in r                 having to repeatedly invert those matrices would have
been computationally prohibitive  severely limiting the number of test iterations
we would have been able to run 
for that reason we applied the algorithm to the transpose problem  i e  assuming a training example is a vector of a particular users ratings for all movies 
in this format we have many more training examples than the dimension of each
training example  which is contrary to the typical motivation for applying factor
analysis 
however  we feel that the algorithm is still applicable because the underlying
model still holds  specifically  we feel it is reasonable to assume that users can
be decomposed into a weighted sum of k eigen users and that the rating for a
given movie can be generated as a linear function of a particular users eigen user
components 
c    matlab  matlab was chosen because it is very good at evaluating an
expression of the form a  b  which is the primary bottleneck of the algorithm  the
algorithm can be parallized across various computers  but to do so  they would need
to share intermmediate values  which can amount to approximately     gigabytes for
the netflix test data  while this is feasible on todays computers  its not practical
to implement on shared resources 
in order to reduce memory contraints   and j were not stored as matricies 
but rather as vectors  since  is diagonal  it can be stored as vector of its diagonal
entires  j is also stored as a vector of indicies that map the element of x to the
elements of j t x  in matlab  this is accomplished by executing the code x j 
for vectors or x j    for matricies 
in order to solve a  b  we used the fact that t    is positive definite 
and therefore a more efficent algorithm can be applied to solving it  matlab
implements a processor optimizied blas software package  so it stands to be faster
than any code we write  matlab uses the command linsolve to calculate a  b 
linsolve will take additional flags that indicate that a is postivite definite  this
method was the fastest of several tested 
the following psuedo code was used to update the parameters of the algorithm
allocate memory for variables row  mat  inv col and inv submat
allocate and initialize to zero variables for
newlambda  newpsi  newmu and m j
for i      to m 
load j and x from disk for user i

fics    project  factor analysis with partially observed training data   

if dimension x    max dimension
choose max dimension random ratings from x
reform j to match x
end if
solve the e step for this user
 inv col inv submat   
 j lambda   lambda   psi j         x   jmu  j lambda 
mu zi xi   lambdaj   inv col
sigma zi xi   i   jlambda   inv submat
apply this users contribution to the m step
mu term   x   jlambda   mu zi xi
mat term    mu zi xi   mu zi xi    sigma zi xi
for j    set of movies user i rated 
row term     x j    jmu j      mu zi xi
row j    row j    row term
mat j    mat j    mat term
newpsi j    newpsi j   
jlambda j    sigma zi xi   lambda j j  
 mu term   mu j    
newmu j    newmu j    mu term
m j j    m j j     
end for
end for
for j      to n 
newlambda j    row j    mat j    
newmu j    newmu j    m j j 
end for

the algorithm took ten hours for a single iteration on the netflix data  this was
acceptable since we want to submit the prediction of each iteration to netflix  and
their system only allows one predicition every twenty four hours 

fi  

paul csonka  barrett heyneman  salomon trujillo

c    extensions of the algorithm  it would take a quarter tetrabyte to store
t
the largest   i   i     i    matrix in double precision  even while taking advantage of symmetry  however  for iterative linear solving algorithms  such as the
conjugate gradient method  the whole matrix would not need to be stored  the
algorithm would calculate the residual r   ax  b and use r to update the value
of x  ax can be calculated in matlab without ever storing its explicit form 
lambda    lambda   x    diag psi    x   however  as it turns out  as x s
grow to the         element size  round off error becomes a problem and the algorithm fails to converge 
appendix d  results
the following section presents more detailed results supporting claims made in
the conclusion of the main paper 
d    repeatability and effects of noise  a data set was generated according
to the underlying model for which the algorithm was designed  with parameters
m   n        k       the rows of   j  rk   and the latent variables z  i   rk
were independently drawn from a gaussian  n     i   the noise terms    rn   were
also drawn from a gaussian  n        i   the algorithm was run to convergence  
times for varying    values  fraction training data observed  and the assumed value
of k  the resulting prediction errors used to generate average and sample variance
data 
prediction accuracy drastically increases around     observed data  and again
around     observed data  prediction with less than     is virtually impossible 
being not much better than randomly guessing  by comparing figs  d   and d  
we see that  as would be expected  when the noise used to generate the data is very
small  prediction is much more accurate 
 

  

k     
k     
k     
 

 

  

 

r  error 

  

 

  

 

  

 

   

   
   
   
fraction of training data observed

 

figure    prediction error for moderate noise        
figs  d   and d   indicate that the algorithm is sufficiently repeatable for more
than     observed data  while suggesting that final prediction is very sensitive to
initial value of     and  for any less 

fics    project  factor analysis with partially observed training data   

 

  

k     
k     
k     

 

  

 

  

 

r  error 

 

  

 

  

 

  

 

  

 

  

 

   

   
   
fraction of training data observed

   

 

figure    prediction error for moderate noise            
 

  

k     
k     
k     

 

  

 

 

r sample variance

  

 

  

 

  

 

  

  

  

  

  

  

  

 

   

   
   
fraction of training data observed

   

 

figure    sample variance of prediction error for moderate noise 
     
d    further results for optimizing performance  the second desktop computer ran further tests on locating more parameters that improve the convergence
speed and accuracy  some results are highlighted below in figs     

fipaul csonka  barrett heyneman  salomon trujillo

 

  

k     
k     
k     
 

 

  

  

  

 

r sample variance

  

  

  

  

  

 

   

   
   
fraction of training data observed

   

 

figure    sample variance of prediction error for moderate noise 
          
iteration times  m n ratios decrease as                        missing data 
   
   
   
single iteration time  s 

  

   
   
   
   
   
 

 

  

  

  
  
  
feature vector count  k 

  

  

  

figure    time required for an iteration loop  i e   time until convergence for a given set of parameters  the plots are  in decreasing
order  m n ratios of                    and      which straddles one
of our empirically found optimum of       please note that the
apparently short times  near small k  are due to the algorithm
terminating early from very high r   

fics    project  factor analysis with partially observed training data   

iteration times  m n ratios decrease as                        missing data 
   
   

single iteration time  s 

   
   
  
  
  
  
 

 

  

  

  
  
  
feature vector count  k 

  

  

  

figure    same as fig    but showing decrease in computation
time as k increases  for larger m n values  this increase in performance becomes very small past our determined threshold of
m n        

r  vs  m n ratio vs  data retention
   
   
r value against known results

   
   
 
   
   

 

   
   
 
 
  
   
   

   

   

   

   

   
data retention fraction

figure    prediction accuracy vs  data retention and m n ratio 
note the profile shows increasing accuracy  decreasing r    as m n
and percent data retention both increase

fi