predicting the outcome of nfl games using machine learning
babak hamadani
bhamadan at stanford edu
cs      stanford university

   introduction 
professional football is a multi billion industry  nfl is by far the most successful sports league in
america  the games are very exciting because outcomes of the games are very unpredictable  the injuries
to players  psychological factors and leagues rules to create parity by allowing weaker teams to draft first
make it even harder to guess the winner 
the goal of this project is to predict the winner of an nfl game  the outcome of no game can be
predicted deterministically  therefore the goal is to come up with a system that is comparable or better than
human prediction 
   data 
data is based on nfl seasons             and      scores and statistics  i crawled the web for
the data  different statistics are available on different web pages  i wrote some code in java and perl to
extract required links and parse the html pages  i also wrote a fairly big java program to combine and
process the different statistics for each team and output them in the required format 
   training and testing methodology 
i used a scheme similar to leave one out classification with an additional constraint  you can not
use the data from the games that happen in the same week or future weeks  we can technically use the
statistics from games that happen in the earlier hours of the same day  but it would make the
implementation much harder 
i did not classify the first two weeks of each season  although those weeks are used as training
data  i didn t classify the first two weeks because some of my features depend on averages in past   weeks 
   algorithms 
at the beginning i was not sure if machine learning is applicable to this subject  therefore i started
with the simplest models to gain an intuition about the problem  i chose to try logistic regression and svm
with different kernels 
    logistic regression
i used my own implementation of logistic regression in matlab  which used newton raphson
method  it was by far the fastest algorithm in all my experiments 
    svm with different kernels
i used svm light with linear  polynomial and tangent kernels  i also tried the sigmoid kernel for
one or two experiments  however the results were not very good  i tried different parameters for each type
of kernel with two different basic feature sets and settled on the following parameters for the rest of the
experiments 
kernel
linear
poly

formula
 s a b c  d

params
c  
s default  c    d    c  

fitan

tanh s a b   c 

s      c    c  

table     c  trade off between training error and margin 

    feature sets
i started with two groups of features and experimented with different feature sets from each group 
for each feature i calculated that features average  i calculated the average in two different ways  see    
for an explanation of averaging methods 
      win ratio features
the first group was what i call win ratio features  here are all the features in the win ratio group 
for different experiments i omitted some of the features 
 weekno  vt teamno vt bothwin ratio vt homewinratio vt awaywinratio vt  weekwinratio
vt avgpointsscored vt avgpointsallowed ht teamno ht bothwinratio ht homewinratio
ht awaywinratio ht  weekwinratio ht avgptsscored ht avsptsallowed  
      game statistics features
these were mainly composed of statistics for each game averaged over different games 
there are too many game statistic features to mention all of them here     for each team plus weakno  
here is a sample of these features 
 weekno vt teamno   final score  avg yards by passing  avg yards by penalty   
avg third down efficiency  avg fourth down efficiency efficiency    total net
yards  total offensive plays average gain per offensive play  net yards rushing 
total rushing plays  average gain per rush  tackles for a loss number     
    backward forward selection
i implemented backward forward selection mechanisms in matlab  since these two mechanisms
are wrappers and non linear svm kernels take a long time to train i only tried them with logistic regression
 also logistic regression almost always performed the best  
   in each season try feature selection for some games and apply it to the whole season 
   use feature selection on a specific season       for example  and use the best features on different
seasons 
the logic behind    is that if a feature is important in one season it should be important in another
season because the game doesn t change much  this is a not a fool proof assumption  i used the second
approach with the game statistics sets  however in practice it didn t give very good results  for each season
i got a different set of features and when i tried classification with feature sets chosen based on feature
selection of another season  the result was not consistently better 
i also tried the selection algorithms with the win ratio features  backward selection eliminated
week no as the first feature for each season and forward selection never chose it as a good feature  after
removing the week no  the results immediately improved for logistic regression  and almost all linearkernel tests  on every experiment that i had done  it even improved the results for feature sets that were
derived from game statistics 
   new techniques
    momentum features 

fithere are many variables that are hard to include in our models  one example would be a team s
recent adjustment in strategy or personnel  to account for such changes i introduced the idea of momentum 
the idea is that a team is more likely to have a closer performance to its recent performance  similar to
locally weighting   i chose to add two features for each game vt  weekwinratio  visitor s past   week
win ratio   ht  weekwinratio  host s past   week win ratio   initially the result seemed positive almost in
every model  however after i eliminated the week number it turned out that the best feature set didn t have
any   week averages  i believe week number was a bad feature and initially when i added   week
averages only lessened week numbers negative effect  however i don t rule out that it can be useful with
different feature sets 
    using averages up to the test week 
i started with an initial set of win ratio features  for each sample the averages were computed up
to the week of the game  for example to classify games in week     i would need to create training samples
for all the games that happened between week   and week    including a game between team a and b in
week    to compute the average winning ratios of that game i would calculate the average winning ratio of
team a and b up to week   and the training sample would look like 
 result    a  visitor  win ratio up to games week        b  host  win ratio up to w         
 where   means visitor won   this increased the number of training samples where teams with lower
winning ratios actually won 
instead of using the averages up to week   for a game that happened in the  nd week  i used
averages up to week     it is completely fair because we want to classify week    s games and by then we
have all the outcomes of week       using this approach the training sample for the previous game may
look like 
 result    a  visitor  win ratio  up to test week           b  host  win ratio  up to w          
this technique increased the accuracy for all models  intuitively we use teams true strength  winloss ratio   assume team a had a tough schedule at the beginning of the season and team b had an easy one 
therefore b had a better win loss ratio  however in reality team a is a much stronger team and that shows
up in its results as the season progresses 
    using week    training data of a different year 
the idea is to train a model based on all    weeks of a prior season for classification  i tried this
approach with svm  linear kernel  and it gave very good results  that experiment included the team
numbers as features  it would be interesting to know the results after removing the team numbers  the
intuition behind removing team numbers is that teams may be very different from one season to another
 given player and personal change  although the counter argument is i haven t had time to completely
explore this option 
   results and analysis 
    results 

set

feature types

 
 
 

win ratio
win ratio
game stat

avgmethod
t
t
t

weekno a feature 
yes
yes
yes

fi 
 
 
 
 
 

game stat
win ratio
win ratio
win ratio
win ratio
game stat

t
t
t
g
g
g

no
no
no
no
no
no

table    t  average up to test week  g  average up to game week 

set
 
 
 
 
 
 
 
 
 

sl   
     
     
     
    

sp   
     
     
    
     

     
     
     
     

    
     
     

lr   
     
     
     
  
     
     

st   

     
     
     
    
     

sl   
     
     
     
     

sp   
    
     
     
     

     
     
    
    

     
    
     
     

lr   
  
     
     
     
     
     

st   

     
    

sl   
     
     
     
    

sp   
    
     
     
     

  
     
     

     
     
     

     
     
     

lr   
     
     
     
     
     
     

st   
     
     
     
     
     
     

table    sl  svm linear kernel  sp  svm poly kernel  lr  log regress  st  svm tan kernel 
    analysis 
      best results 
best results were obtained using set    win ratio features without week number  by logistic
regression                                               this is another reason why only performed
feature selection with logistic regression 
      best algorithm 
over all logistic regression is the best method  although svm with linear kernel has a similar
performance  an interesting observation is if any of the methods performs better on set of features a than it
does on b  it is almost guaranteed that other methods will perform better on a as well  so one can pick the
best set of features using the fastest method and try it with other algorithms 
      comparison of averaging methods 
in the left column table   you can see feature sets calculated using averages up to the games week 
and on the right column you can see feature sets whose averages were calculated up to the tests week  if
you refer back to table   you will see that in every case the set whose average was calculated up to the test
week had better results 
avg to game week
 
 
 
table  

avg to test week
 
 
 

approximate improvements
     
    
    

fi      impact of removing week number from the features 
again  on the left are the sets with week number  and on the right are the exact same sets without
the week number  in this case there is always an improvement on the best case  logistic regression  
with weekno
without
 
 
 
 
 
 
table  
    expert picks 
unfortunately i dont have the results of expert picks for           seasons  here is their
accuracy for their picks for      season through week     including          bear in mind that in their
picks they have the luxury of not picking a winner for some games hence for games that are too close they
may not pick 
expert
right        wrong       
right       wrong      
accuracy       
theisman
   
  
  
  
      
salisbury
   
  
  
  
      
hoge
   
  
  
 
      
jaworski
   
  
  
  
      
schlereth
   
  
  
  
     
allen
   
  
  
  
      
mortensen
   
  
  
  
      
golic
   
  
  
  
      
accusoure
   
  
  
 
      
table  
   conclusion 
when i started this project i was not sure if machine learning can be applied to this problem 
given my current results                                              i believe machine learning
can be a reasonable solution specially compared to humans  however any solution will be able to reach
very high accuracy 
given the time constraints i have not explored all the possible algorithms and features  bcs like
formulas can be used to account for unbalanced schedules  it would also be nice to compute a confidence
score to improve the results by not predicting close games  i think a couple of new things that i tried such
as using averages up to the test game can be useful for similar experiments 
    http   sports espn go com nfl features talent

fi