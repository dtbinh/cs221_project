squint  svm for identification of relevant sections in
web pages for web search
siddharth jonathan j b 

riku inoue

jonsid stanford edu

department of computer science
stanford university
rikui stanford edu

department of computer science
stanford university

abstract
we propose squint  an svm based approach to identify
sections  paragraphs  of a web page that are relevant to a query in
web search  squint works by generating features from the top
most relevant results returned in response to a query from a web
search engine  to learn more about the query and its context  it
then uses an svm with a linear kernel to score sections of a web
page based on these features  one application of squint we can
think of is some form of highlighting of the sections to indicate
which section is most likely to be interesting to the user given his
query  if the result page has a lot of  possibly diverse  content
sections  this could be very useful to the user in terms of reducing
his time to get the information he needs  another advantage of this
scheme as compared to simple search term highlighting is that  it
would even score sections which do not mention the key word at
all  we also think squint could be used to generate better
summaries for queries in web search  one can also envision
squint as being able to create succinct summaries of pages of
results  by pulling out the most relevant section in each page and
creating a meta summary page of the results  the training set for
squint is generated by querying a web search engine and hand
labelling sections  preliminary evaluations of squint by k fold
cross validation appear promising  we also analyzed the effect of
feature dimensionality reduction on performance  we conclude
with some insights into the problem and possible directions for
future research 

jyotika prasad

department of computer science
stanford university
jyotika stanford edu

in this paper  we propose squint  an svm based approach for
identifying the sections in a web page relevant to a user s query to
help the user jump right into the most relevant section of the web
page  here  we define a section to be a paragraph  squint works
by making use of the top k results returned in response to a query
from a web search engine  we use the google api   to learn more
about the query and its context 
the problem can be formalized as follows  given a query q  a
standard web search engine returns a set of pages ranked in
descending order of relevance  we call this set  p  given p  let s be
the set of all sections in every page in p 
s    s  s is a section in some page in p 
given q  we generate s  and use it to train a support vector
machine  during testing  given a particular section  we can predict
a numerical score indicative of the relevance of the section to the
query  q  the scores of all the sections in a page with respect to a
query enforce an ordering among the sections in terms of relevance
to the query  we use an svm with a linear kernel in squint 
support vector machines have been shown to work very well for
high dimensional learning problems similar to the ones
encountered when dealing with text     
the svm is trained with binary class labels for relevant and
irrelevant classes and we use the svms predicted margins during
testing for scoring sections 

keywords

in some sense  we identify related concept words to the query
based on term frequency and proximity statistics  we then build
query independent features based on occurrence of these high
frequency words of the top k results  in the section to be classified 

   introduction

we begin by discussing some related research in section    in
section    we first present a high level overview of the squint
framework and then subsequently drill down into the individual
sub components in the framework  in section    we report results
obtained during the training and test phases for various
experimental settings  in section    we present some insights into
the problem that we gained during the course of our
experimentation  we end with section    where we present our
conclusions and possible future directions for research 

artificial intelligence  machine learning  supervised learning 
squint  information retrieval  web search  support vector
machine 

currently for web search  the de facto model is that a user inputs a
query and gets multiple pages of links to results  then the user 
based on the snippets that he sees  clicks on one of the results  the
result page could have a lot of content in it and it is left to the user
to figure out where in the page  his answer to the query lies  some
shallow techniques that search engines at times use to offset this
problem are to highlight the search terms in the document 
although this is a step in the right direction  the results are often
very unsatisfactory especially if the search term is not a very
differentiating term or if it does not appear in the section at all 

   related work
gelbukh et al      have presented a model for recognition of
relevant passages in text  using relevance measures and structural
integrity  liu and croft     have explored passage retrieval using a
language and relevance model  this problem finds applications in
web question answering and summary generation  and has been

fiaddressed in these contexts in                yu et al      discuss a
vision based page segmentation  vips  algorithm to detect the
semantic content structure in the page  akin to identifying the
sections  teevan et al      have discussed search methodologies
that focus more on contextual information than just keyword
occurrences 

   overall architecture

as shown in fig   below  the training phase begins with querying
google and receiving the ranked set of relevant pages  these pages
are then cleaned and split by the page processor module  which
outputs a set of sections  the feature generator then computes the
features for each section  every section is then manually labelled in
the labelling phase  we use a binary labelling scheme where the
labels are    relevant  and    irrelevant   the output of these two
phases gives us the training set  which is used to train the svm 
after learning is complete  we proceed to the testing phase where 
given a query and a set of result pages  we are able to score the
sections in the result pages based on the features we extract from
the result set  the svms predicted margins are used as scores
here 

we define the rank of a word to be its position in the list if the
words were ordered by frequency of occurrence in the top k
results  we would have a feature each for say  the top     most
frequent words in the top k results  for the ith ranked word  this
feature would basically have the value for the frequency of this
word in the current section  one possible option that we have to
limit the dimensionality of the input vector is by bucketing words
by a certain range of ranks  for example  we can bucket ranks     
              etc to aggregate word counts  and come up with a
feature vector of reduced dimensionality  another option for
limiting the dimensionality of the input vector is to simply limit the
range of ranks that appear in the vector  figure   shows the effect
of dimensionality reduction on the accuracy of the test result 
bucket size here is   since it worked the best among various bucket
sizes tried  after testing various settings for bucketing and the
range of ranks  we decided to use a bucket size of   and rank
coverage of     which achieved the highest accuracy  in other
words  we use top     ranking words with no bucketing  we also
normalize for the length of the section since we do not want to be
biased towards long sections 

figure    dimensionality reduction in word rank features
       bigram rank based features

figure    squint  high level view of the framework

     feature generation

we define a bigram to be two consecutive words occurring in a
section  this feature is computed in a manner similar to the
previous set of features  this feature is based on the intuition that
the correlation between two words might be more informative than
the words taken individually  for instance   machine learning 
suggests a stronger relation to a query  ai svm  than the
individual words  machine  or  learning   for this feature as well 
we adjust the dimensionality by bucketing and limiting coverage of
ranks  figure   shows the effect of dimensionality reduction on
accuracy  a bucket size of   worked best for this feature as well 
from these results  we decided to use a bucket size   and rank
coverage    

in order to effectively score sections of a web page using the svm 
we need to select features that capture each sections
characteristics  among the information extracted from sections  the
frequency of certain important words and the location and
frequency of the query words are considered to be useful indicators
of relevance to the queried topic  the intuition here is that certain
words which are strongly related to the topic will occur frequently
in relevant sections  and also relevant sections are located near the
query words  in the proposed method  we currently have five
possible types of features that capture word frequency and word
location 
       word rank based features

figure    dimensionality reduction in bigram rank features

fibigram frequency based
       coverage of top ranked tokens
relevance to a topic may also be captured by the coverage of top
ranked token types in the section  for example  if we have a bucket
size of    we might be interested in knowing how many of the top  
ranked words occur in this section  how many of the next   highly
ranked words occur in this section and so forth  specifically  if the
top   ranked token types are  learning    machine    data  
 access   and  database   and a section contained  learning  and
 data   the corresponding value for this feature is    we use bucket
size of   and dimensionality of    for this feature 
       distance from the query
the intuition here is that the closer a section is to the query in the
web page  the more likely it is to be relevant  thus we compute the
section wise distance between the section in question and the
nearest section which contains the query  we feel that although this
is not a necessary condition for relevance  it could well be a
sufficient one  an ablative analysis with respect to this feature
confirmed this intuition 
       query word frequency
last but not the least  the frequency of the query word in the
section seems a reasonable indicator for relevance  since there are
many possible ways to evaluate importance of query word
appearance  we tried two types of measurements for this feature 
first  we used the query term frequency in the section  in this
setting  if there are   query words that appear in a section  the
count is   normalized by the number of words in the section 
second  we used a sum of weighted counts based on the distance
from the beginning of the section  research from text
summarization has shown that typically the gist of a paragraph is
given by its first few sentences  in other words  a match in the first
few words of a section counts more than a match much lower down
in the section  weighted count is computed by the following
equation 

in other words  we discount the count linearly as a query word
occurs in the latter part of the section  for instance  if a query word
occur as the   th word  and the total number of words in the section
is      the weighted count is      after testing both settings  we
decided to use the weighted count setting 
       the final set of features
the features discussed thus far are generated for each section in the
top k result pages obtained by querying google  we put the
features together and evaluated each setting by comparing k fold
cross validation accuracy to decide the optimal combination of the
features  after feature selection  we decided to use the set of
features shown in table    we will explain the details of feature
selection in       
table    the final set of features
feature name

parameters

rank based

dimensionality     
no bucketing

dimensionality    
no bucketing

coverage of tokens

dimensionality    

distance from the query

dimensionality   

     training set generation
the training set required is a set of sections of web pages and
corresponding binary labels indicating    relevant  and   irrelevant   we created the training set by hand labelling the
sections of pages returned by google on a few sample queries  the
basic steps are 
  
  
  
  

query google to get a set of pages
clean each page  remove scripts  pictures  links etc 
break each page into sections 
label each section of every page 

step   uses the googlesoapsearchapi  a quick way to do step   is
to get a lynx dump of the web page  lynx being a text based
browser cleans up scripts and pictures  and gives text with
numbered parts  where each part is a distinct html element  we use
this for step    the page is broken up into candidate sections based
on the numbering  candidates which have less than   lines of text
are eliminated  as we are only interested in significant chunks of
text  lynx also groups all the links on the page under visible
links or references  both of which are removed 
one caveat here is that we need to distinguish between training
labels and test labels  for training  we hand labelled every section
as     relevant  or     irrelevant   during testing  our task is
actually to detect on a per page basis  the most relevant section in
that page  therefore our labelling is slightly different  for every
page  the most relevant section s  is are  labelled as    while all
others are labelled as    note that we label the test set for the
purposes of evaluation only 
we generated the data set from   queries  machine learning 
gene sequencing  oregon missing family  space shuttle
discovery launch  ipod nano and google buys youtube 

     learning algorithm
as mentioned earlier  we use a support vector machine with a
linear kernel to learn to detect the most relevant section in a given
page  using the training set mentioned in the preceding section 
the training set contains results for   queries which comprises of
   web pages and     sections  given the relatively high
dimensionality of our feature vector  it is a reasonable choice to use
an svm  note that our purpose is to specify the most relevant
section  not just classify many relevant sections  to get a nonbinary metric of how relevant sections are  we use the predicted
margins for each sample  in other words  given parameter w and
feature vector x  we detect a sample that has the largest wtx as the
most relevant section in the page  also note that the way the
learning algorithm deals with data is different between the training
phase and the test phase  in the training phase  all the result pages
for a query are processed all at once  but in the test phase  the
algorithm examines the data  page by page  to determine the most
relevant sections for each page 

fiwe evaluate the performance of the learning algorithm using four
common metrics namely  k fold cross validation  learning curve
with respect to number of training data  ablative analysis for
features  manual error analysis  in each case  we use three different
settings for the evaluation  strict  relax   and relax   
these three settings can best be explained with an example  let s  
s  and s  be the top three highest scoring sections in a particular
page p  as returned by the svm 
under the strict setting  a result is deemed correct only if s  is the
most relevant section in the page  as indicated from the labelling in
the test data  under relax    the result is deemed correct if either s 
or s  is the most relevant section in the page  similarly  for relax   
the result is correct if either s  or s  or s  is the most relevant
section in the page 
    k fold cross validation
we do k fold cross validation with k      the   datasets were the
results for the   queries  we evaluate the accuracy for the strict 
relax   and relax   settings  in each case  we first evaluate the
accuracy per query as the percentage of pages within the query for
which the svm returned a correct result  the k fold accuracy is
then the average of the accuracies obtained for all the   queries 
the following results were obtained 

we also measured the significance of each feature through forward
search on the feature set  we started with the base case  lower
bound   where the algorithm randomly picks a section as the most
relevant  we added the frequency based features  word frequency
and bigram frequency  next  followed by coverage and distance
from the query  the last feature added was the frequency of the
query word in the section  we were able to eliminate the section
size feature  since we observed that we did not get any gain in
accuracy by the use of this feature 
the following chart shows the results of the forward search   
indicates the frequency based features    indicates the coverage of
top ranking words    is the distance of the section from the query
and   is the frequency of the query terms in the section 
    
   
   
   
accuracy

   evaluation

   

strict
relax  
relax  

   
   
   
   
   
  
base case

k fold accuracy

strict

      

relax  

      

relax  

      

    

       

          

feature sets

table    k fold accuracy for the best configuration
best configuration

 

figure    forward feature search
an unexpected observation was that the use of the query frequency
feature actually marginally hurt the accuracy of prediction under
the strict and relax   settings  we hypothesize that the lack of a
gain in accuracy with this feature could be because this information
is captured somewhat noisily by the rank frequency and coverage
features 

    learning curve

   insights

we examine how much training data we need  to get reasonable
accuracy by plotting the learning curve with respect to the number
of training examples  the horizontal axis is the size of training set 
and the vertical axis is the accuracy as measured by k fold cross
validation 

there might appear to be a potential problem for squint in the
case of queries which result in unequally sized clusters in the result
set  for example  for the query michael jordan one might expect
a majority of the results returned to talk about the basket ball
player  however  we would like to identify sections mentioning the
professor from berkeley as also relevant  given the current
squint framework it seems reasonable to expect that it will be
highly unlikely that relevant sections belonging to the minority
cluster will be correctly identified  however  we claim that this
problem is orthogonal to the one squint attempts to solve and
the preceding scenario can be easily resolved by allowing squint
to operate on a per cluster basis 

figure    learning curve
    forward feature search

another observation we made was that the frequency based
features were not as useful as we had hoped  in fact  the word
coverage feature is more critical to the accuracy of the scoring  we
think that this might be explained by the fact that since we do not
do idf weighting  our frequency features are susceptible to noise
resulting from low idf words  we do stop word filtering but that
may not be enough  the coverage feature is a little less susceptible
to this effect since it is bucketed and on inspection of the most

fifrequent words more often than not the top ranking words are
words that are highly correlated with the query 
given the limited size of our training set  we attempted to reduce
the dimensionality of some of our features  as shown in an earlier
section  we observed that reducing the dimensionality did give us
gains in accuracy  presumably because of the reduced number of
parameters that need to be fit for the training set 

acknowledgements 
the authors would like to thank prof  ng  jeremy kolter  samuel
ieong  catie chang  haidong wang and eric delage for their
support and feedback throughout the course of the project  we also
wish to thank dr andreas paepcke of the stanford infolab for his
insightful comments and suggestions 

references

we realize that there are quite a few reasonable extensions to our
feature set  we intend to explore some of these in the future  one
obvious upgrade to our suite of frequency features is to weight it
by idf  in addition  the query frequency features can be encoded in
a number of ways  for example  we could have a feature that looks
for all the words in the query to occur within a specified window of
words and counts occurrences only when the query words occur
within that window  one can imagine that this might be useful for
a query like data mining  wherein if the words data and
mining occur far apart  the meaning conveyed is not quite the
same as the query  one can also imagine designing features that
penalize absence of any of the query words in the section  many of
these features are similar to the query based features that a web
search engine or any information retrieval system might employ 

    jimmy lin  aaron fernandes  boris katz  gregory marton 
stefanie tellex  extracting answers from the web using
knowledge annotation and knowledge mining techniques 
in proceedings of the eleventh text retrieval conference
 trec        gaithersburg  maryland       

another interesting observation that we made was that the
relevance of a section did not seem to be too correlated with the
length of the section  the use of the length feature did not hurt
accuracy but it did not give us noticeable gains either 

    brian ulicny  lycos retriever  an information fusion engine 
in proceedings of the human language technology
conference of the north american chapter of the acl  pp 
        june      

we realize that squint offers a value add for a specific category
of queries in web search and information retrieval namely 
 information seeking  queries  in such queries  the focus of the
query is reasonably broad and good result pages comprise many
sections of text  for example   gene sequencing   however  for say
commercial queries  where a good result page is a home page of a
dealer or a hub page with lots of outgoing links  graphics and
animation  the value add is questionable 

    hang cui  renxu sun  keya li  min yen kan  tat seng
chua  question answering passage retrieval using dependency
relations  in proceedings of the   th annual international
acm sigir conference on research and development  pp 
               

   conclusion and future work

we proposed squint  an svm based approach to identify
relevant sections in a web page to a user s search query  this
problem has been relatively less studied in the literature  but we
believe that its solution will have a large impact on the user s
overall search experience  as a result of our evaluation  we see that
using information retrieval inspired features and some basic hints
from summarization give respectable accuracy with respect to
detecting the most relevant section in a page  some possible future
directions include qualitative comparisons of the summaries
generated using squint with the snippets generated by web
search engines and other summarization algorithms  it will also be
interesting to evaluate the impact squint has on user
productivity with regard to satisfying user information need 

    alexander gelbukh  namo kang  and sangyong han 
combining sources of evidence for recognition of relevant
passages in texts  issads       pp              
    xiaoyong liu  w  bruce croft  passage retrieval based on
language models  in proceedings of the eleventh international
conference on information and knowledge management  pp 
               

    shipeng yu  deng cai  ji rong wen  wei ying ma 
improving pseudo relevance feedback in web information
retrieval using web page segmentation  in proceedings of
the   th international conference on world wide web  pp 
       may      
    jaime teevan  christine alvarado  mark s  ackerman and
david r  karger  the perfect search engine is not enough 
a study of orienteering behavior in directed search  in
proceedings of the sigchi conference on human factors in
computing systems  pp           april      
    thorsten joachims  text categorization with support
vector machines  learning with many relevant features 
in proceedings of the   th european conference on
machine learning pp                
    chih chung chang and chih jen lin  libsvm   a library
for support vector machines        software available at
http   www csie ntu edu tw  cjlin libsv

fi