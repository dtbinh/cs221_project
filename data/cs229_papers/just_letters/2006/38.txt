dimension reduction of image manifolds
arian maleki
department of electrical engineering
stanford university
stanford  ca         usa
e mail  arianm stanford edu

i  i ntroduction
dimension reduction of datasets is very useful in different application including classification  compression 
feature extraction etc   linear methods such as principal
component analysis  have been used for a long time and
seem to work very well in many applications  but  there
are much more applications in which the dataset doesnt
have a linear structure and so the linear methods do not
work very well for them  fig      and fig      illustrate
what linear and nonlinear structures mean  when the
points of the dataset lie close to a linear subspace of the
ambient space  its structure is called linear and if they are
close to a smooth manifold as in fig       then it is said to
have nonlinear structure  nonlinear dimension reduction
methods try to recover the underlying parametrization
of scattered data on a manifold embedded in high
dimensional euclidean space  in the next section some
of the linear and nonlinear methods will be explained
very briefly  but  the focus of this report will be on
image processing applications of dimension reduction
algorithms  usually the image databases  each image
is considered as a very large dimensional vector  do
not have linear structures  donoho and grimes     have
shown that in some image databases nonlinear methods
will lead to very reasonable parameters and this proved
the efficiency of these methods in processing image
databases  in this report  nonlinear methods will be used
in three different image processing applications  image
synthesis image classification and video synthesis  it
will be shown that the methods which are proposed here
can easily outperform the linear methods  the structure
of this paper is as follows  in the next section  different
algorithms of dimension reduction will be reviewed 
in section iii two ways are explained for synthesizing
images  in section iv an image classification is proposed
and compared with the other methods and finally in
section v  i will explain how to synthesize a video by
using the manifolds 

 
 
 
 
 
 
 
 

 
 

   
 

 
 

fig    

   

linear data structure

  
  
 
 
 
  
  
  
  

  

  
 

  

 
 

fig    

 
  

nonlinear data structure

during this project several different databases have been
tested  but in order to use the same database during
the whole report  most of the figures in this report are
generated from the same database  this image database
is shown in fig      
ii  d imension r eduction a lgorithms
in this section some of the linear and nonlinear dimension reduction algorihtms will be explained 

fidij be the distance between xi and xj and dij shows the
distance between y i and y j   the closeness of dij and dij
is measured with stress which is defined in this way 
stress  

n x
n
x

 d ij  d ij  

   

i   j  

fig    

some of the patches in the first database i used

a  linear methods
   principal component analysis pca   pca     is a
way of changing the coordinates such that the first coordinate is the maximum variance direction  i e  among
all the directions that you can project the data on 
projection on this direction may have the maximum
variance   the second coordinate of pca is orthogonal
to the first direction and among all the directions that
are orthogonal to the first direction it has the maximum
variance property  the other directions of pca can be
defined in the same way  for more information please
refer to     
   multidimensional scaling  multidimensional scaling  mds  deals with the following problem  for a set
of observed similarities  or distances  between every
pair of n items  find the representation of the items in
fewer dimensions such that the similarities or distances
of these new items is close to the original similarities  it
is obvious that in most of the situations the similarities
are not exactly the same as the original similarities and
one should just does his her best to make them as close
as possible  assume that the similarities are euclidean
distances between the points x    x         xn   these points
are embedded in an m dimensional space rm and the
question is  can these points be embedded in the ddimensional  d  m  euclidean space such that their
structure or the distances between them  doesnt change 
assume that all the vectors are zero mean  obviously  the
solution of this problem is not unique and is invariant to
translation and rotation of the data  in order to make the
solution unique  some more constraints are imposed the
constraint is that the lower dimensional points are found
from an orthogonal projection of the original data on
a d dimensional space  assume that the data in ddimensional space is represented by y     y          y n   let

an interesting point about this quantity is that   the
elements of summation are not in the absolute value  the
reason is that y i is an orthogonal projection of xi and
so dij is always less than dij   the advantage of defining
stress in this way is that the global optimum of this
problem can be found explicitly and there is no need for
an iterative algorithm  the optimum projection would be
the first d principal components of the covariance matrix
of x    x         xn      in other words  y     y          y n can be
found by the multiplication of x with the matrix which
is found from the principal components of the covariance
matrix  for more information refer to     
b  nonlinear methods
   isomap  isomap     is an algorithm that tries to
preserve the geometry of the data  mds also tries to keep
the geometry of the data unchanged  the term geometry
was defined by the similarity matrix in section ii  it was
also mentioned in section ii that  the euclidean distance
is usually used as a similarity matrix  but  as it is obvious
the euclidean distance is not at all  representative of the
geometry of a manifold  instead  another distance called
geodesic distance can represent the geometry very well 
geodesic distance between two points on the manifold
is the length of the shortest path on the manifold from
one point to another  this is a brief description of the
isomap algorithm 




find some neighbors of each point  two different criteria are considered for neighborliness  
one is epsilon neighborhood and the other is kneighborhood  in epsilon neighborhood  a real value
epsilon is considered and every point that has euclidean distance of less than epsilon to the point 
is recognized as its neighbor  in k neighborhood
algorithm  the k closest points are considered as the
neighbors of the point 
in order to find a geodesic distance between two
points we try to connect every two points with
a path which is constructed from connecting the
neighbors of the points  the geodesic distance is
the length of the shortest path 

fiapply the mds to this similarity matrix and reduce
the dimension which has geodesic distances as its
elements 
   locally linear embedding  locally linear embedding lle      is another algorithm to retrieve the
parameter space of a manifold  again lle is trying to
somehow preserve the geometry of the data which is in a
high dimensional space  but  the method this algorithm
chooses to do so is different from that of isomap  the
algorithm is as follows 
 find the optimum value of wij such that 


opt
wij

  arg min
wij

n
x

 xi 

i  

n
x

wij xij   

j  

subject to these constraints 

w
if xj 
  nk  xi   
pij     
w
 
 
i
j ij



   

second image

  

  

  

  

  

  
  

  

  

  

average image on geodesic

  

simple averaging

  

  

  

  

  

  

  
  

  

  

  

  

  

fig     two images and their geodesic average as defined in this
report and their simple average 

   

in which nk  xi   shows the k neighborhood of the
point xi  
find y    y       yn in a d dimensional space such that
this cost function is minimized 
n
n
x
x
opt
error  
 yi 
wij
yij   
   
i  

first image

j  

as can be seen each of these two steps are simple least
squares problems and so the global optimum of each of
these two problems can be found easily  this algorithm
tries to keep the neighborhood of each point  or local
structure of manifold  almost unchanged 
iii  i mage s ynthesis
assume that two images are given  from the same
database  and we want to somehow find an average of
these two images  how can it be done  it is obvious
that  the simple average doesnt work at all and will not
generate any reasonable image  for example consider the
two images shown in the first row of fig      are given
and the goal is to find the average of these two images 
considering the average to be the midpoint on the line
that connects the vectors of these two images will result
in an image which is not a natural image at all this is
equivalent to simple averaging   for example the simple
average of these two images is also shown in fig      
two different algorithms are proposed here to solve this
problem 
   all the images are on the manifold that was found
before  by isomap or lle   there is a path
on the manifold that connects these two images

and has a shortest length  the geodesic   find
the midpoint of this path and thats the average
image  obviously if the density of the points on
the manifold is not enough  the average image will
not be very natural but for most practical purposes
the density of points is high enough  the result of
this algorithm is shown in fig      
   assume that the parameter space of the manifold is
convex  also assume that the parameters that correspond to our two images are   and     i define
the average image as the image that corresponds
to the parameter 
     
avg  
   
 
but avg may correspond to a point which is not in
our data base  in order to approximate it with the
points in the database  assume that              k
are the k nearest neighbors of avg  
 calulate w opt in this way 
x
wopt   arg
min
  avg 
wi i    
 w   w       wk  



find the image from this equation 
x opt
imageavg  
wi xi

   
   

in which xi is the image that corresponds to
i  
the result of this algorithm when applied to two
images is shown in fig       it is obvious that the second
method is more general in the sense that with this method
for every value of  in the parameter space one can
synthesize an image corresponding to this parameter 

fiimage  

image  

    

  

test error in terms of dimension for dataset  
test error for linear svm in ambient space
test error in terms of dimension for dataset  
test error of linear svm in ambient space

  
  

    

  
  
  
  

  

  

  

  

  

  

  

  

  

  

    

  

average image

    

    
  

  

  

  

  

  

    

fig    

two images and their average by the second method
    
 

iv  i mage c lassification
support vector machine  svm       or      uses the
fact that by transforming the data to higher dimensional
space we can make it more linearly separable  therefore
in svm method  we first transform the data to a higher
dimensional space and then try to find the optimum
separating hyperpalne between the two classes  making
the data more separable  in the linear sense  is the main
reason behind the success of svm  now  assume that
the data is on a manifold and is not separable in that
high dimensional ambient space  how can we find a
good kernel or equivalently transform  to make this data
more suitable for linear classifiers  a very reasonable
answer to this problem is to use the p aram et er
sp ace   there are several advantages in using the
parameter space  first  it is usually very low dimensional
space and whatever algorithm we use for classification 
overfitting is much less likely of course we havent lost
much information about the relative distances of the
points and so the likelihood of underfitting in parameter
space is not much more than that of the ambient space  
on the other hand  in many situations when we transform
the data to the parameter space because we flatten the
curls of the manifold  the data becomes more separable 
all these observations lead us to use parameter space for
classification  here is a method for classification of the
data 





find the lower dimensional coordinates of the training data 
by using svm find the best hyperplane that separates the data in lower dimensional space
find the nearest neighbors of each test point in the
training data 
find the optimum linear weights that approximate
the test data from the training data

 

 

 

  

  

  

fig     test error of classification  s and  s in terms of dimension
for two datasets

use the same weights to approximate the parameters of the test data
 use svm classifier in lower dimensional space
in order to understand the best dimension for classification  one can use cross validation  but as it will be
seen later the performance of this algorithm is usually
not very sensitive to the dimension we choose  as far
as the dimension is reasonable  there are other parameters in the algorithm that can be chosen through cross
validations  but  on the datasets that i have done my
simulations  the performance of the algorithm was very
robust to the small variation of those parameters 
as an example  the classification of  s and  s is shown
here  a set of      images of handwritten  s and  s is
used to train the model  the size of each image is   x   
two datasets of     images are considered as test sets 
the test errors in terms of the dimension of parameter
space is shown in the following figure 
as it can be seen  as the dimension changes from  
to    the error doesnt change very much  in terms of
cross validation the best dimension is    but as mentioned
before    d     is acceptable  it can also be seen that
if the dimension is chosen correctly  this algorithm can
easily outperform svm in the ambient space 


v  s ynthesizing v ideo
again assume that we have a database of images  for
example assume that these images are photos of a face
from different horizontal and vertical directions  also
assume that two images are given  i  and i    from this
database  the goal is to synthesize a video to show us

fia way that the camera can move very smoothly from
one position to another  the idea to achieve this goal is
very similar to the idea which was proposed before  for
finding the average  again we find the geodesic between
the two patches  then we divide the geodesic into n
points n is the number of frames that we want in the
video and synthesize these frames from their neighbors 
to be more specific assume that x    x         xk are the
image patches from the dataset that lie on the geodesic
of the two images x    i  and xk   i    also assume
that the geodesic distance of x  and xk is equal to d 
the distance of each new patch from x  is calculated
from this formula distance is calculated on the geodesic
from x  to xn   
d j    
d x    zj    
   
n
in which zj is the jth synthesized frame and d x    zj  
represent the distance of zj and x  on the geodesic of x 
and xk   in order to synthesize zj assume that i is found
such that 
d x    xi    d x    zj    d x    xi    

   

then use this formula to reconstruct zj  
d x    xi      d x    zj  
d x    xi      d x    xi  
d x    zj    d x    xi  
 xi  
d x    xi      d x    xi  

zj   xi

    
fig      shows an example of this algorithm  leftmost
frame and rightmost frame are given and   frames are
synthesized 
vi  c onclusion and f uture w ork
in this report some of the methods which are used
for linear and nonlinear dimension reduction have been
reviewed and applied to some of the simple databases
of images  two algorithms have been proposed for
generating some synthetic images that can work well
if the manifold of images has been sampled at a high
density  this also led us to the video synthesis to show
smooth variation of one image to another image  also 
i proposed a method for classifying high dimensional
data by using its low dimensional parameter space  this
algorithm needs more thought and more simulations 
also it should be tested in different applications of
classification and on other databases  in this report only
binary classification was considered  definitely  one of

fig    

  frames of video to show the rotation of the face

the future directions would be to extend this algorithm
to more general classification problems  in addition to
classification there are some other interesting questions
that shall be answered  for example  is there any better
method for synthesizing patches  another problem that
shall be answered is how can someone compare the
result of lle and isomap  in other words  a measure
is needed to say which parameters match better to the
database  this problem seems very interesting  because
there are many methods of nonlinear dimension reduction and it is not easy to say which one works better
than the others 
r eferences
    d l  donoho  c  grimes image manifolds which are isometric to euclidean space  journal of mathematics imaging nad
vision              
    a  ng  lecture notes of cs     stanford university 
http   www stanford edu class cs    
    t f  cox  m a  cox  multidimensional scaling  second edition 
     
    d l  tennenbaum  v  de silva  a global geometric framework
for nonlinear dimensionality reduction  sciencs            
    s  roweis  l  saul nonlinear dimensionality reduction by
locally linear embedding   science           
    t  hastie  et  al the elements of statistical learning  springer 
    
    d  donoho  wedgelets  nearly minimax estimation of
edges anals of stat   vol      pp               

fi