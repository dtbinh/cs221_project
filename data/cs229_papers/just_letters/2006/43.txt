detecting corporate fraud  an application of machine learning
ophir gottlieb  curt salisbury  howard shek  vishal vaidyanathan
december         
abstract

niques for this purpose  algorithms for automatated detection of patterns of fraud are relatively recent     and
this paper explores the application of several ma  the models currently employed are fairly simplistic  in
chine learning algorithms to published corporate this work  we have tested variants of logistic regression 
data in an effort to identify patterns indicative naive bayes  and support vector machines svm   we
of securities fraud  generally accepted account  conclude with a comparison of the predictive accuracy of
ing principles  gaap  represent a conglomerate these methods with the best performing solution used in
of industry reporting standards which us pub  industry today 
lic companies must abide by to aid in ensuring
the integrity of these companies  notwithstanding these principles  us public companies have method
legal flexibility to maneuver the way they disclose
certain items in the financial statements making
it extremely hard to detect fraud manually  here
we test several popular methods in machine learning  logistic regression  naive bayes and support
vector machines  on a large set of financial data
and evaluate their accuracy in identifying fraud 
we find that some variants of svm and logistic
regression are significantly better than the currently available methods in industry  our results
are encouraging and call for a more thorough investigation into the applicability of machine learning techniques in corporate fraud detection 

the data the attribute data consist of financial metrics covering  disclosed financial statements  management
discussion and analysis  md a   financial filing footnotes and most other required corporate filings    k  s   
   a  etc    these metrics represent real instance data
 such as accounts receivable over sales   data volatility
 eg    year and   year changes in accounts receivable over
sales   and indicators of comparison to appropriate peer
groups   eg  accounts receivable over sales compared to
other similar companies   there are approximately    
attributes per company per quarter  these attributes
represent nearly all financial and related corporate information publicly available  attributes can vary widely
between different types of companies and missing or incorrectly entered data is frequent in financial reports  to
handle this  we represented each attribute as a percentile
in the distribution for the appropriate peer group  when
attribute data is missing  the missing value is assumed
to be the   th percentile  the response data consists of
fraudulent and non fraudulent labelings of the public us
companies for all quarters over the same time period  a
company is defined as fraudulent for a given quarter if
it was successfully litigated  or settled  by the securities
and exchange commission  sec  for material misrepresentation or false forward looking statements for a period
including that quarter 
data exist for each of approximately       publicly
traded us companies and about     american depository receipts  adrs  over a period of    quarters             in this work  we treated each company quarter

introduction
regulators have attempted to address corporate accounting fraud for decades  from the formation of the sec
in the     s through the recent sarbanes oxley legislation  generally accepted accounting principles  gaap 
represent a conglomerate of industry reporting standards
established as an attempt to minimize accounting fraud 
however  the continued incidence of fraud and financial
misrepresentation  is evident in sec enforcement actions 
class action litigation  financial restatements and  most
prominent in the recent past  criminal prosecution  ironically  the complexity within gaap provides sufficient legitimate room for maniuplations that make it extremely
challenging to detect corporate fraud  human analysis of
financial data is often intractible and an incorrect analysis can result in staggering financial losses  it is therefore
natural to explore the ability of machine learning tech 

  generated

 

by reuters

fiof each of the methods employed  in what follows  the
vector x represents the attribute data for a single company quarter and y represents a label indicating whether
that company quarter was fraudulent 

as a separate data point which gives us about        
data points  within this set  there were approximately
      instances  company quarters  of fraud  this small
proportion highlights one of the major difficulties in detecting fraud  

logistic regression logistic regression is a discriminative learning algorithm which directly attemps to
estimate the probability of fraud given the attribute data
for a company quarter  our examination included several
hypothesis classes of increasing complexity within the setting of logistic regression  the hypothesis used in logistic
regression is defined by

data preprocessing to make computational manipulations easier for this work  we reduced the dimensionality of the problem by pruning out the attributes which
were perceived to contribute little to fraud prediction 
the evaluation of the contribution of a given attribute
to fraud prediction was done using an odds ratio  the
odds ratio      similar to the mutual information  assigns
a simple score to each datum to indicate how informative
that datum is about the likelihood of fraud  to compute
the odds ratio  both the fraud status  response data  and
related attribute status of all companies are required 
prior to computing the odds ratio for each attribute 
the attribute value was mapped to   or   based upon a
subjective estimate of malfeasance implied by the value
of the attribute for a given company relative to the data
of peer companies     in particular  for the revenue  asset  high risk event and governance metrics  large values
    th percentile relative to peers  were considered to
implicate a greater chance for malfeasance and their attribute would be cast as a    for the expense and liability
related metrics  small values     th percentile relative to
peers  were considered risky and their attribute would be
cast as a    to understand why those attributes cast as a
  are more likely to be fraudulent  consider if a company
were fraudulent it would probably overstate its revenue
 to make the companys financial condition seem stronger
than it really is  and it would probably understate an expense item 
of the     original attributes      were identified as
having an odds ratio greater than   and were significant
for fraud prediction  to see if we could further reduce
the dimensionality by removing linearly dependent attributes  we did a principal component analysis on the
remaining attributes  our results showed that to get    
of the variance  we needed to include the top     principal components  since this does not result in a large
reduction in dimensions we chose to start with all    
components identified by the odds ratio for training the
machine learning methods discussed below 

h x      g x    
 
g x   
    e x 
one can choose different forms for the function  x 
depending on the number of parameters one wants to fit
i  linear
 x          x          n xn
ii  quadratic
 x          x          n xn   n   x            n x n
iii  cubic
 x          x          n xn   n   x            n x n
  n   x            n x n

naive bayes naive bayes is a generative method
that determines a model of p x y  and then uses bayes
rule to generate p y x  rather than fitting parameters to
a model of p y x  directly  consequently  this approach
can be expected to work well if the conditional distributions of the attributes are significantly different for different values of the label  intuitively it is reasonable to expect that a company that has decided to act fraudulently
in a given quarter would behave in a manner similar to
other companies who have made the same decision  and
in a manner inconsistent with those companies not acting
fraudulently 
the training set for these experiments consisted of
machine learning algorithms we decided to evalu  the first         chronological company quarters and the
ate some of the most popular machine learning techniques test set consisted of the remaining           companyavailable in the literature today  we give a brief summary quarters  the objective of the chonological separation
was to mimic the natural chronological separation that
  this casting was done only for the calcuation of the odds ratio  the true attributes values were used for training learning algo  would be observed in the application of the algorithm 
rithms 
in particular  one desires to predict fraud in the current
 

fiquarter using current and all previous quarter information  including known fraudulent behavior in past quarters 
two variations of a bayesian learning algorithm were
evaluated  they differed only in the general model suspected to describe p x y   the first model was a multivariate bernoulli  it was selected for its simplicity 
the percentile scores of the attributes were cast to    if
perc    or perc     or    otherwise   the hypothesis
was that fraudulent companies would have metrics that
deviated more from their means than their non fraudulent
counterparts  this algorithm was naive in that it assumed p xi   xj  y    p xi  y p xj  y  for all i    j  otherwise  the algorithm would need to fit an unreasonable
number         of parameters 
the second model was a product of one dimensional
gaussian  it was selected to allow the algorithm to have
more flexibility in the selection of a decision boundary
and to compare the performance of the simplified multivariate bernoulli model to this more rich model  since the
attribute data are percentiles  one way to represent that
data as gaussian would be to recast it back to a gaussian 
we chose a simpler method of fitting a gaussian directly
to the percentile data 

to be fraudulent to least likely  in the case of svms  we
use the margins as substitutes for probability   the cap
curve indicates the fraction of fraudulent data points as
a function of the fraction of all ranked data considered 
for example  to say that the cap value at     is    
would mean that in the top     of all companies ranked
in order of likelihood of fraud  we correctly identified    
of all fraudulent companies  a cap allows one to determine how many false positives result when identifying any
desired percentage of companies as high risk  conversely 
we can identify how many false negatives result at each
percentile level of low risk companies 
the accuracy ratio  ar  is a metric for summarizing
the cap  the cap for random guessing has an ar of
zero and the cap for perfect prediction has an ar of   
formally the ar is defined as a ratio of areas under the
cap curves     
we find that even when algorithms fail to accurately
predict probabilities or labels  they perform much better in ranking the data by likelihood of fraud  for example  the logistic regression algorithms did not identify
any companies as fraudulent but produces a reasonable
ordering of companies  since a ranking is often of equal or
greater practical value in making financial and other decisions  we use the ar to assess the efficacy of the different
methods we tested 

support vector machines support vector machines  svms  are often considered to be the best offthe shelf tools for classification problems  binary classification svms calculate a separating hyperplane in feature space by solving a quadratic programming problem
that maximizes the distance between the separating hyperplane and the feature points  to handle data that is
not linearly separable  the svm method can be kernelized which allows the generation of non linear separating
surfaces  in this work  we employed the following kernels 

results and discussion
a summary of our results can be seen in table   
we briefly discuss the results of each machine learning
method below 

model
industry proprietary 
logistic regression

i  linear
t

k x  y    x y
ii  quadratic
k x  y     xt y  

naive bayes

iii  gaussian

svm
k x  y    exp 

 x  y  
 
   

variant

ar

linear
quadratic
cubic
bernoulli
gaussian
linear
quadratic
gaussian        

    
    
    
    
    
    
    
    
    

model evaluation the models mentioned above were
table    performance of machine learning methods
evaluated using a hold out test set  in order to assign accuracy measures and to compare different models 
we used the financial industry standard for quantitative logistic regression
model validation called the cumulative accuracy profile
linear classifiers further reduction was performed
 cap       to obtain the cap  the predictions of the
models are used to rank the data in order of most likely on the dimension of the attribute data by using forward
 

fiulent than the lowest risk      the similarity between
the in  and out of sample accuracy gives little reason to
believe that substantive variance has been added to the
model  with this in mind  we looked at the cubic feature
mapping 

search feature selection with a p value of      as the inclusion exclusion rule  the resulting forward selected
model revealed     of      metrics that met the p value
requirement          the logistic regression algorithm
was trained on                     training examples 
each of dimension    and tested on                    
validation examples  each of dimension     for the linear
hypothesis class  the ar was       see fig     further  the
highest risk     of companies were      times more likely
to be fraudulent than the lowest risk      the overall
accuracy for the logistic regression with linear classifiers
is fair  clearly the learning algorithm has produced results well above a random separating hyperplane  the
variance in this initial model  and corresponding risk of
overfitting  is small  the in sample and out of sample results are similar and the restriction to linear classifiers
all but guarantees we are not overfitting the data  the
possibility of bias does exist  the idea that a     dimensional training set is fit linearly to all of the predictors is
unlikely and requires further analysis 

cubic classifiers the cubic classifiers demonstrated a smaller improvement over the previous hypothesis classes with an ar of       the highest risk     of
companies were     times more likely to be fraudulent
than the lowest risk     
higher order polynomial hypotheses did not seem to
achieve significantly better results with the test set  suggesting that overfitting occurs beyond the cubic classifier 

naive bayes the mulitvariate bernoulli based
bayesian algorithm resulted in an ar of       see fig
    the gaussian based bayesian algorithm resulted in
a better ar of       this performance is comparable
to the current industry standard  to the extent that the
gaussian based algorithm performed much better than the
mulitvariate bernoulli based algorithm  the casting algorithm from percentiles to  s and  s failed to capture all
of the meaningful data  potential improvements to the
bayesian approach would be to recast the data from percentiles back to a gaussian and then estimate p x y  and
exclude the naive assumption  estimating the joint probability p x    x         xn  y   multivariate gaussian random
variables are quite amenable to estimation as the number
of parameters grows polynomially  rather than exponentially  with the dimension of the gaussian model 

figure    cap for logistic regression using various hypothesis classes 

the maximum probability of fraud placed on any one
company during the    year period was just over     meaning the linearly classified logistic model did not identify any companies as having a higher risk of fraud than
not  however      does imply a       higher probability of fraud than the average company since the prior
probability of fraud in the real world is     
quadratic classifiers the quadratic classifiers
demonstrated a marked improvement over the linear one  figure    cap for naive bayes using bernoulli and gausthe ar is      for the test set  further  the highest risk sian models 
    of companies were     times more likely to be fraud 

fisvm we implemented a version of the smo algo  perhaps more surprising is that a simple model like lorithm to construct svms      it is difficult to train svms gistic regression gave results not too far behind gaussian
on the entire data set  consequently a subset of data svms  this preliminary work strongly encourages furcontaining      non fraud and     fraudulent companyquarters were chosen for training purposes   this size
was chosen based on tradeoffs between training time and
improved accuracy  given more training time  we could
potentially build better svms   the regularization parameter  tolerances and other training parameters were
chosen to minimize the number of support vectors obtained at convergence 

figure    cap for best performing learning methods
ther investigation  future directions include developing
more sophisticated models for the hypotheses within the
framework of the learning methods explored here and designing systematic schemes for optimizing top level parameters  further  we have only touched upon a handful
of machine learning methods in this paper and it is extremely intriguing to consider the wealth of other methfigure    cap for svms with different kernels 
ods available in the literature  we conclude on the bright
the ar of      obtained from the linear svm is note that the problem of identifying fraudulent financial
quite disappointing and suggests that the data is not statements shows great promise of being succesfully adlinearly separable  training a simple linear svm is typi  dressed in the near future 
cally unable to reach convergence unless tolerances are
increased to unacceptable levels   a quadratic kernel acknowledgements
does marginally better  the results of the gaussian kernel
we are grateful to audit integrity  a quantitative risk
svm are dependent on the parameter   larger values
analytics firm based in los angeles  ca  for providing
of  tend to underfit while smaller values tended to overthe financial data set used in this work 
fit  the data shown here was obtained by hand tweaking the regularization and  parameters  for optimal
parameters the ar of the gaussian kernel svm is a dra  references
matic improvement over not only the other kernels  but
the other machine learning methods as well  scoring an     audit integrity  agr white paper       
ar of      in the best case  this is a very significant
improvement over the current industry standard of           sobehart j  keenan s  and stein r  benchmarking
quantitative default risk models  a validation methodthe encouraging results seen here behoove a deeper exology  moodys rating methodology       
ploration of better svm optimization algorithms  choice
of kernels and parameters 
    engelmann b  hayden e  and tasche d  measuring
the discriminative power of rating systems  deutsche
conclusion
bundesbank  discussion paper series       
we find that machine learning methods are quite easily     platt j  fast training of support vector machines
able to outperform current industry standards in detectusing sequential minimal optimization  advances in
ing fraud  not surprisingly  svms gave the best results 
kernel methods   support vector learning       
 

fi