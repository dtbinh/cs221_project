explaining preference learning
alyssa glass
cs    final project
stanford university  stanford  ca
glass cs stanford edu

introduction
although there is existing work on learning user
preferences in various systems  the outputs of such systems
tend to be confusing to users  studies of users interacting
with such systems show that when the system incorrectly
predicts a user s preference  the user may attempt to correct
the views of the system  for instance  by randomly
providing more training examples   in many cases 
however  users lose patience with this approach  largely
due to a misunderstanding of how the underlying system is
using the data to make predictions  ultimately  they may
stop trusting the system entirely  even when the system is
correct  users view such outcomes as  magical  in some
way  but are unable to understand why a particular answer
is correct  or whether the system is likely to be helpful in
the future 
in this paper  we describe the augmentation of a preference
learner  designed to provide meaningful feedback to the
user in the form of explanations  we begin by describing
the preference learning problem in the specific context of
semi autonomous meeting scheduling  and the preference
learning system currently being used to solve the problem
in the calo research project  we describe current
research on usability in the face of active learning systems 
including a study that we conducted on learning and trust 
we then describe how this preference learning system was
modified to store meta information about its learning
during the course of system execution  and how this metainformation is used to provide explanations of answers
concluded by the system  finally  we show how these
explanations can be incorporated into the larger scheduling
system to provide transparency into the learning system 
enabling trust between user and system 

motivation
the problem of scheduling meetings among groups of
busy  over committed professionals is a well known
problem for anyone who has worked in an office
environment  often  a series of emails or phone calls
among desired meeting participants are required before a
jointly acceptable time has been chosen  central to this
problem is the incorporation of preferences  different
participants may prefer particular meeting lengths 
particular times of the day  particular days of the week  etc 

complicating the problem of incorporating user
preferences is the problem of dealing with conflicts  the
ptime personalized scheduling assistant  berry  et al 
      is one system built to automatically handle these
scheduling problems while taking into account user
preferences  as part of the cognitive assistant that learns
and organizes  calo  project 
in ptime  a user initializes the system by indicating some
initial preferences for meeting scheduling  for instance 
preferring morning vs  afternoon meetings  whether to
overlap meetings if necessary  and whether to drop
individual meeting participants in order to fit a schedule 
as well as information on how these preferences should be
traded off with each other when they conflict  for instance 
whether a meeting should be shortened in order to not
overlap another existing meeting   to schedule a meeting 
a user enters both broad constraints  such as any time
before next thursday  as well as some suggested
relaxations  such as joe is an optional participant  
ptime then attempts to solve the constraint problem
defined by the meeting specification entered by the user
and the preferences of the user and other meeting
participants  presenting the user with multiple possible
schedules based on these constraints  some of which may
relax the constraints along varying problem dimensions 
these possible schedules are presented to the user roughly
in preference order   in some cases  less preferred
schedules are presented ahead of other schedules in order
to provide the user with more variety of options and to
allow the preference learner to better explore the space 
details can be found in  yorke smith et al          when
the user then selects one of the provided schedules  the
systems model of the users preferences is updated 
this update of the users preferences is computed by
pliant  preference learning through interactive
advisable nonintrusive training   gervasio et al        
pliant uses support vector machines  svms  to update
the preference model  using each selection of a schedule as
a new data point  as described in the next section 
although ptime  with on line learning computed by
pliant  has been deployed in restricted settings and used
by several users for accomplishing limited tasks  adoption
of the system has been slow  a study of ptime reported
in  yorke smith et al        and a larger calo wide user
study conducted by us  in conjunction with deborah
mcguinness and michael wolverton  both confirm that a

fisignificant barrier to the wide adoption of a system like
ptime is the problem of transparency  most users simply
do not trust a system that is constantly modifying itself
through learning unless they are able to ask questions and
be provided with information about the inner workings of
the system  ptime has begun to address this problem by
explicitly indicating where constraints are being violated 
but much more work is needed to provide explanations of
the preference learning  in particular  since users view the
ranked schedules provided by ptime as suggestions for
which schedules the user is expected to prefer 
explanations providing transparency into why particular
schedules are recommended over others is vitally needed
for the usability of the system  to solve this problem 
then  we attempt to provide transparency into the pliant
learning system 

this function is then combined with the initial preferences
elicited from the user by the full evaluation function 

f  z      az         wz
where z is the full schedule being evaluated  w is the full
set of a i and aij weights learned above  a is the initial set
of preferences elicited from the user  and  is a parameter
indicating the impact of the elicited versus learned
preferences  the  parameter is decayed over time  both
to accommodate a users changing preferences and to
acknowledge the difficulty many users have with explicitly
indicating their own scheduling preferences without the
use of examples 
the overall system  then  has the following work flow  the
system elicits initial preferences from the user  the a
vector above  once  during ptimes first use  then  for
each new meeting  the user specifies meeting parameters
and constraints  the ptime constraint solver generates
several candidate schedules  zs   relaxing constraints
when necessary  according to the stated constraints and the
users existing calendar  the candidate schedules are
presented to the user in  roughly  the calculated preference
order  and the user selects a single schedule  z  the users
preferences  the ai and aij weights  are then updated based
on the users choice 

active preference learning
active learning in pliant relies on a starting feature set
of   criteria  initialized through input from the user  the
criteria are 
   scheduling windows for requested meeting
   duration of meeting
   overlaps and conflicts
   location of meeting
   participants in meeting
   preferences of other meeting participants

the data used to perform the preference update is both the
chosen schedule  and the other schedules considered but
rejected by the user  for example  if three schedules are
presented in ranked order to the user  and the user chooses
the third schedule  a partial ordering is imposed on the
schedules indicating that the third  chosen  schedule is
preferred to both the first and second schedule  gervasio et
al         these partial orderings are then added into the
optimization problem solved by the svm  in the same way
that search engine results are ranked according to past
clickthrough data in  joachims       
joachims
svmlight is used as the basis for pliants update of the
preference weights  it is this update of the preference
weights  and the resulting ranked ordering presented to the
user  that we seek to explain 

the first time ptime is started by a user  the system
elicits initial preferences for these six criteria through a
wizard type interface  for example  for overlaps and
conflicts  the system asks the user whether he prefers
meetings to never overlap  or to allow the system to
double book him in two meetings at the same time when
necessary  the system also asks the user to rank the
importance of these six criteria relative to each other  for
example  if the users preference on duration and location
cannot both be met  the system stores pair wise
information about which criteria is more important 
given the partial utility functions implied by each of these
criteria  pliant combines these utility functions into a
single function using a   order choquet integral 

usability and active learning

f z     zn    i ai zi   i j aij  zi  zj 
where each zi   ui xi   the utility for criteria i based on
value x i   note that this is essentially a function over

as a guide for how best to explain the preferences learned
by pliant  we considered the results of three user
studies  relevant points from each of the three studies are
summarized below 

schedules  not individual meetings  for a given schedule 
each xi is a measure of the degree to which the full
schedule  containing multiple meetings  matches the given
criteria   thus  the learning task for pliant is to learn
the weights ai and aij associated with each individual and
pair wise matching of the six criteria  justification for the
use of a   order choquet integral is in  yorke smith et al 
      

first  outside of the work done for this project  we
conducted our own study of calo usability  joint work
with deborah mcguinness and michael wolverton  not
yet published   focusing on pliant and ptime  in
particular  we found a general lack of understanding
among users of how preferences are being updated  or
even that they are being updated at all  although there are
many reasons why a recommendation from pliant may

 

fiviolate the initial preferences entered by the user  for
example  pliant may have more recent learned
preferences that violate the initial preferences  pliant
may be deliberately violating preferences in order to
explore the search space  or the user himself may not have
correctly indicated his true preferences  a common
problem with explicit preference elicitation   users
nonetheless expressed surprise and confusion whenever
their true preferences were violated by the ranking of
schedules presented to them  many users commented that
it seemed that the system was ignoring their preferences 
leading them to view the system as untrustworthy  one
user commented  i trust  ptimes  accuracy  but not its
judgment  this type of complaint specifically about the
use of preferences was common among many of the users
we studied 

dictionary style feature vectors of several thousand words 
and why some words and not others were considered
important by the system  in our preference learning
domain  as noted above  we are instead considering a
relatively small space of features  each of which has
previously been explained to the user in simple terms  in
this domain  we are thus able to create similarity based
explanations which may enjoy the trust noted in the third
study  while still staying in the realm of familiar domain
concepts  these similarity based explanations can be
created naturally from the computation done by the svm 
as described in the next section 

providing transparency into preference
learning
starting with the pliant code base  we augmented the
use of svmlight with code to gather additional metainformation about the svm itself  for the purposes of
creating explanations  we considered the following svm
meta information 

next  we considered a study conducted by the pliant
team  in which users were interviewed about their
requirements for an adaptive scheduling system  one of
the top requests made by these users was transparency
into assisted scheduling decisions  e g   explanations of
conflicts and learned preferences   yorke smith et al 
       the authors continue by noting that the preference
model must be explainable to the user  in terms of
familiar  domain relevant concepts 
this last
requirement  in particular  is often a problem with attempts
to understand and explain statistical learning methods  a
problem that we address in our formulation 








the final study that we considered is by  stumpf et al 
       this study considered explanations of statistical
machine learning methods  focusing on a nave bayes
learner and a rule learning  classification  system  both in
the context of learning the proper foldering of email  they
found that rule based explanations  taken directly from the
rule learning system  were most easily understood by
users  but were not trusted to the same extent  they also
found that similarity based explanations  which users had a
very hard time understanding  were nonetheless considered
more natural by the users  and were easily trusted to an
extent beyond what would be expected  given the level of
understanding  these similarity based explanations were
roughly of the form  this message is really similar to the
message a in folder x because they have important words
in common   common words highlighted   the authors
also noted that many users appreciate a less formal  nontechnical style for explanations 



the support vectors identified by the svm
the support vectors nearest to the query point
the margin to the query point
the average margin over all data points
the non support vectors  i e   other data points 
nearest to the query point
initial preferences elicited from the user  along with
the current value of 
the kernel transform used  if any

in the pliant system  the kernel is linear  so no
information about the kernel transform was needed  if a
non linear kernel is used by the svm  the problem of
explaining the results becomes much more complicated 
we do not address the issue here 
the next step was to represent this meta information 
along with underlying information about how svms learn 
in a way suitable for producing justifications of the
computation performed in pliant  we represented this
information in proof markup language  pml   pinheiro
da silva  et al        because of its generic justification
representations for general reasoners  the existing use of
pml in the calo system  and work we have done over
the past year in expanding pml to represent the results of
machine learning  glass   mcguinness       
for the representation in pml  we added logical rules for
the deduction performed by an svm  both to generate a
conclusion given a query point and a data set  and to
describe the resulting decision plane and space of data
points  first  we added the base rule hasrank which has
three antecedents  the initial preferences elicited from the
user  the calculated weights  which indicate the support
vectors in the svm  and the new query point  which is a
schedule proposed by the constraint reasoner  and results
in a conclusion indicating the ranked preference for this

we posit that the understandability problem with
similarity based explanations noted in this last study is
related directly to the findings in the ptime study which
found that explanations must be tied directly to domain
concepts that are known to the user  in particular  users
complained that the similarities identified by the nave
bayes system seemed arbitrary  indeed  to users not
familiar with statistical machine learning methods  it
would be difficult to understand similarities among

 

fischedule  which is the result of the evaluation function
defined above  the antecedent representing the support
vectors contains an iw learnedsourceusage  with a link to
the data set used by the svm  along with information
about when this data set was generated   see pml
specification referenced at the end of this document   the
additional meta information gathered from the svm is
represented through additional rules indicating how they
are concluded  for example  hasclosedatapoint takes
the second two antecedents mentioned above  and
concludes a single data point near the query point  with
details about which dimensions are closest to the query 
depending on context  then  the expanded svm can
conclude as many of these hasclosedatapoint
conclusions as necessary  in our system  we limited this to
  points because of the relatively small size of the full data
set  additional rules for each of the other metainformation types mentioned above were also added 
finally  we added a rule hasinitialrank which is similar
to hasrank  but only considers the preferences initially
elicited from the user  and provides a rank based solely on
these initial preferences  without any learning 









the result  then  is a formal justification for why pliant
recommended a particular schedule 
given this
justification  we then created strategies for abstracting the
justification for presentation to a user  methods for
abstracting justifications in pml already exist within the
inference web  iw  infrastructure  mcguinness and
pinheiro da silva        we expanded these methods to
cover justifications of the form created by our augmented
svm 

this schedule closely follows the preferences you
told me when we started   used when  is large 
or when the rank generated by hasinitialrank is
particularly high  
this schedule is similar to other schedules that you
have chosen in the past  and i have observed your
preference for avoiding overlaps and conflicts 
 used when several of the nearest preferred data
points are particularly close along a particular
feature dimension  
this schedule seems to be strongly preferred  based
on your past selections  and i have observed that you
often have a preference for considering other
participants preferences   used when the margin
to the query point is particularly large  relative to the
average margin  this strategy also attempts to call
out which feature contributes the most to the margin 
but often leaves out the last clause when no features
stand out  
this schedule does not closely match your
preferences  but listing it here will help me to
understand more about your preferences  so that i
can do better in the future   used when the ranks
computed both by the learned weights and the initial
preferences are poor  but pliant is exploring the
space  

thus  to provide transparency into pliants preference
learning  we envision a link into ptime which enables
users to ask questions about why a particular schedule is
being recommended  the justification for a particular
schedule  in pml  is stored when the ranking is generated 
when a user then requests an explanation  this justification
is parsed  an explanation strategy is chosen  and the
resulting explanation is presented for the user  in the past 
we have designed systems which are intended to engage
the user in a full dialogue  enabling the user to ask followup questions and gather additional information as desired 
here  we instead show a system where a single explanation
is generated for each schedule  with no possibility for
follow up questions  since our goal is trust in the overall
preference learner  rather than trust in the recommendation
of a particular schedule  we feel that the sum of all
explanations over all of the generated schedules better
accomplishes our goal  and fits more cleanly in the
existing interface without distracting the user from their
main goal of selecting a schedule  the resulting
architecture  showing the main pliant architecture along
with the explainer component described here  is shown in
figure   

as discussed in the usability section above  we chose to
focus on similarity based explanations  taking advantage
of the small feature vectors and the clean mapping into
user understandable terms 
we also generated
explanations in the first person  as if the system were
informally talking to the user about its observations 
because of the preference for non technical explanations
noted in  stumpf et al         the strategies that we
created take a pml justification  and consider which of the
above meta information rules provides the strongest reason
for presenting a given schedule to the user  note that this
selection is a bit subjective  we chose fairly
straightforward heuristics for choosing a strategy  without
making claims that the best possible explanation is given
in all cases  instead  our goal was simply to provide an
explanation that is reasonable enough  so that the system is
not immediately dismissed by the user 
once a strategy is chosen  it generates a single explanation
in english  these explanations are generated in a simple
fill in the blank format  as such  the explanations are
domain dependent  and can occasionally be a bit awkward 
but avoid the issue of full natural language generation 
examples of some of the explanations generated by the
strategies 

for the purposes of this project  the integration of our
expanded pliant system and explanation component
into the larger ptime system was shallow  and completed
only to a degree to validate proof of concept  using
scheduling and preference data previously gathered during
the calo year   critical learning period  clp 

 

fifigure    system architecture  four boxes on left are the original pliant system  component svm explainer has been
added  and additional data flow is shown   original pliant architecture from  gervasio et al         

 containing scheduling and preference data for    users 
gathered over approximately two weeks   we generated
meta information for select examples as described above 
used the meta information and pml rules as described to
generate justifications for a subset of this data  and created
explanations from these justifications using the strategies
described above 

clp data  and karen myers for related discussions 
support  and ideas 

references
berry  p   conley  k   gervasio  m   peintner  b   uribe  t   and
yorke smith  n  deploying a personalized time management
agent  aamas    industrial track  pages                 
calo        http   www ai sri com project calo 
gervasio  m t   moffitt  m d   pollack  m e   taylor  j m   and
uribe  t e  active preference learning for personalized
calendar scheduling assistance  proceedings of conference
on intelligent user interfaces       iui           
glass  a  and mcguinness  d l  introspective predicates for
explaining task execution in calo  technical report  ksl       knowledge systems  artificial intelligence laboratory 
stanford university       
joachims  t  optimizing search engines using clickthrough
data  proceedings of the acm conference on knowledge
discovery and data mining  kdd   acm       
mcguinness  d l   and pinheiro da silva  p        explaining
answers from the semantic web  the inference web
approach  journal of web semantics                
http   iw stanford edu
pinheiro da silva  p   mcguinness  d l   and fikes  r  a proof
markup language for semantic web services  information
systems  volume     issues      june july       pp         
pml specifications 
http   iw stanford edu         pml provenance owl
http   iw stanford edu         pml justification owl
stumpf  s   rajaram  v   li  l   burnett  m   dietterich  t  
sullivan  e   drummond  r   and herlocker  j  toward
harnessing user feedback for machine learning  conference
on intelligent user interfaces       iui      to appear  
yorke smith  n   peintner  b   gervasio  m   and berry  p  m 
balancing the needs of personalization and reasoning in a
user centric scheduling assistant  unpublished manuscript
 under submission       

conclusion
we have described the design of a system for explaining
the results of an svm active preference learner  and how it
can be integrated into an adaptive scheduling assistant 
we based our design on several user studies which indicate
both the need for explanations of statistical machine
learning systems  and the types of explanations that are
required by users to build trust  future work includes
system engineering to provide a full integration into the
ptime system  analysis of whether additional metainformation or explanation strategies not studied here
would also be appropriate for generating explanations  and
a user study to validate the usability of the explanations
being generated  we also plan to consider the extensions
of this work to other svm systems  particularly systems
that are learning over larger feature sets 

acknowledgements
we gratefully acknowledge funding support for the calo
project from the defense advanced research agency
 darpa  through contract               to   r   we
thank melinda gervasio  pauline berry  neil yorke smith 
and bart peintner for access to the pliant and ptime
systems  and for helpful collaborations  partnerships  and
feedback on this work  we thank deborah mcguinness 
michael wolverton  and paulo pinheiro da silva for the
iw and pml systems  and for related discussions and
previous work that helped to lay the foundation for this
effort  we thank mark gondek for access to the calo

 

fi