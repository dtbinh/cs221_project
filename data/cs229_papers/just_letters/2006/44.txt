breaking it down  the world as legos
benjamin savage  eric chu
to devise a general formalization for identifying objects via image processing  we
suggest a two pronged approach of identifying principal parts and model fitting to encode
spatial relationship  we begin with a relatively simple object  the cube  additional
motivation for this choice was the fact that much of the man made world is composed of
cubic shaped structures  so cube recognition could serve as an input to later  more
complex object recognition tasks  our recognition formalization begins with training
svms to identify the principle parts of an object  for our project we used an svm to
pick out cube corners like those appearing in man made objects  in the second stage of
our formalism  information from the detected locations of the principle object
components is used to fit a pre defined model to the location data  we then used the
minimal least squares affine transformation to fit an arrangement of seven cornersour
cube modelto various permutations of the detected corners  this model contains
information about the spatial relationships between the constituent corners and
successfully distinguishes between possible cubes and impossible cubes 

we have successfully implemented very basic
elements of each of these three components in
our attempt to construct a better object
recognition algorithm  figure   diagrams the
general flow chart devised for this particular
scheme 

introduction
current object recognition algorithms face the
challenge of generalization  it is possible for a
machine to identify a teacup if the teacup is
presented to the machine in the same size and
orientation  otherwise  the machine falters in
its recognition  in an attempt to model human
vision and human spatial cognition  the
purpose of this project is to take a step
towards making object detection and
recognition rotationally invariant  scale
invariant  and partially occlusion invariant 
by constructing object models and
discovering a least squares mapping from a
suspect object to its model  we hope to
achieve that end 

image in

svm
ranker

model fitting

object out

figure   flow chart of algorithm 

the svm step performs the operation of
decoding contextual information as well as
identifying fundamental parts of an object 
the model fitting step provides the computer
with a mathematical framework with which to
rotate  scale  translate  and skew the object  it
also encodes the spatial relationships between
the fundamental parts of an object  using
these steps in conjunction with each other  we
are able to identify very basic cubes 

the overview
after a cursory observation of the human
visual system  we generated a list of three
major elements that contribute to the
flexibility of human object recognition 
   using parts and their relationships to
identify the whole
   using context to identify the object
   being able to mentally manipulate  
rotate the object

from now on  we shall discuss how the
algorithm applies to identifying cube like
objects and conclude by elaborating on how it
can be generalized to a broader framework 

 

fithe svm

these subspaces found trends in various
dimensions  while remaining invariant to a
good deal of noise  furthermore  they allowed
us to highlight significant characteristics of a
corner in a   x   patch  such as an edge
emanating from the center pixel   using this
technique gave superior results to using pca
to create a basis of eigen corners  and far
superior results to using the direct   x  
cutout 

since the simple building blocks of a cube are
its corners  the main purpose of the svm  
machine learning algorithm is to reinforce or
discourage certain classifications of pixels as
cube corners  these corners will be used
later in object mappings 
in early incarnations of this project  we
attempted to use the harris corner detector to
detect cube corner candidates  this
approach was unsuccessful because it only
used local pixel data and did not use
contextual clues to make predictions  we had
much more success with the svm classifier
by using large image patches and geometrical
information to encode contextual knowledge
into our classifier 

in the final step  the     vector given by
projecting into this subspace is scaled to unit
length to eliminate the variation caused by
differences in lighting  this     element
vector was used as training and test vectors in
an svm  using a gaussian kernel in our svm
with a  of    we were able to obtain less than
    error on a leave one out cross validation
test 

we first converted our color image to an
intensity image  and then took the gradient of
that image  to recognize cube corners in
this image we extracted   x   pixel image
patches to make sure that some amount of
context was observed  experimentation with
training an svm to directly classify image
patches of this kind showed little promise for
many reasons  therefore  to both endow the
svm with a sense of the spatial ties between
the various pixels in this patch  and to
substantially reduce the dimension of the
space  we found the projection of this     vector    x           into a     dimensional
space  the projection basis chosen was a
collection of geometric masks that were
blurred significantly  below  figure    are
artistic renditions of   such spaces 

as another form of validation we ran the svm
on every single   x   image patch in an
image not used in our training set  every
pixel is labeled with its margin from the
support vectors  more confident points are
highlighted in red  the results are shown
below  these results were even more
encouraging than the loocv test error  not
only did the svm find most cube corners in
the image  but the confidence of the prediction
seems to be well correlated with how well
centered the corner is in the cut out  we
described this image as a corner heatmap 
 note  our svm was further improved after
using it to create this image  but as the image
took    minutes to generate we decided not to
update this image  

figure   artistic renditions of our basis masks
applied on to   x   image patches 

 

fithus  we will model the positions of all of the
corners of the cube in the image as an affine
transformation of the x j   added to a   d
gaussian error term  as in equation   
z   i     ax   i     

   

note that x is in homogenous coordinates  so
that a contains translation information   this
model allows us to find the likelihood of this
arrangement of the corners given the
parameters of the affine transformation  we
even briefly toyed with the idea of viewing
this problem in a bayesian framework where
there was some prior distribution on the
transformation a  although we didnt have
time to explore this idea in depth  we found
the maximum likelihood estimate of the affine
transformation to be as follows 

figure   original photo  left the cornell box
and corner heatmap  right  

cube regression
we developed mathematical machinery to
grapple with the task of identifying cubes
from our detected corner data  the general
idea is laid out in figure    we modeled a
perspective of a cube  or even more generally 
any object with highly distinguishable
principle image components such as corners 
as a set of   d homogeneous points x    x    
x        x m    when appearing in an image 
this set of points may be rotated  scaled  and
translated  on top of this there may be
random noise  or perhaps distortion due to the
fact that the cube is being viewed from a
slightly different perspective 

w   s  x t  x  s  x t  

   

ai   z   i    w

   

 

in equation      z i  is a   x m matrix of the
observed corner locations  of image i  in
homogenous coordinates   see figure     x is
also a   x m matrix but this time of the model
points  and s is a diagonal matrix of
weightings used to weight the error of each
point relative to one another   it is the inverse
covariance matrix when the distribution of
noise around each point is assumed to be a
circular gaussian  

cube model

observed
corners with
wireframe
maximum
likelihood
estimate cube 
figure    diagram of cube regression in action 

 

figenerated by the same model under an affine
transformation a i  for i     to n  and treating
the a i as the latent variables  we were able to
find the best cube model  we were also
able to find the variance of error for each
individual point to create the weighting matrix
s  experimentation showed that using this s
gave superior results to the non weighted case
of simply using the identity matrix for s  the
em algorithm was extremely efficient and
converged in around    steps  figure   shows
the results of this algorithm after convergence 

z

 i  

 
 
 
  
      

                          


                         

figure     top  an example training image   bottom 
the z i  matrix used in equation    note that the  nd
point is missing  and hence all zeros  

we were pleased with the resemblance of this
weighted regression formula to the normal
equations  in our tests we found that this
worked quite well  this formula allowed us to
quickly find the likelihood that a set of points
was generated by a given model  as desired it
is completely invariant to translation  rotation
in the plane  scaling  and is mildly robust to
minor distortion  we even found a method of
fitting less than the full m points by setting
ones to zeros in the homogenous coordinate 
if the svm did not observe a corner  either it
was not in the image  or the svm simply
made an error   a was still calculated  using
the remaining data from the other points  with
the zero ed column   this allowed us to
realize the dream of making a model robust to
partial occlusion 

figure    note the seven major clusters
corresponding to the seven prominent corners of a
cube  also  note that the center corner has the
most variance 

this mathematical side of our project was far
more satisfying than the image processing
portion  we were able to apply the techniques
taught in class  linear regression  em
algorithm  multivariate gaussians  maximum
likelihood estimation   to achieve the goal of
storing the spatial relationships of parts of an
object in a mathematical model  and to create
an affine transformation invariant object
detection framework  we encountered one
truly difficult challenge however when it came
time to implement this methodology  factorial
blowup  unfortunately  the order of the corner
points is important  and so there was a
factorial blow up  when     cube corners
were detected in an image  say  we had to try
all

to generate the cube model we derived a
form of the em algorithm  by finding the
ordered locations of all visible corners in
hundreds of observed cubes  z    z     
z n   we generated a training set  by
assuming each training example was

 

fidifferent corners of a cube  this would have
to be done in the general object case   for
example  using this framework to detect
human faces  one would train a nose svm  a
right eye svm  a mouth svm  etc   this
drastically reduces the number of
permutations of image locations to try in the
model fitting step of the paradigm  we lacked
enough training data to create robust svms
for each type of corner  we would have needed
several hundred more images of cubes  the
other problem   particular to the cube  is that
every principle part is the same as every
other one  just rotated in   dimensions 

   c        p         c        p         c   
    p         c        p   
possible permutations  each bracketed term
corresponds to a certain number of unobserved corners  
there is still hope that one might solve this
problem  it makes sense to use the confidence
of the svm prediction to try the most
confident corners first  there are other
optimizations and heuristics to try here for
ordering that search  but we didnt have time
to implement any of these methods 
it may also be possible that this algorithm is
useful just not in this particular context  it
might be more suited to eliminating options
rather than to find the optimal one 

tiering is another idea to for future research
to pursue  once the method in this paper is
perfected to recognize small objects  we treat
collections of objects in the same fashion to
recognize more complex  compound objects 
such as cities  freeways  or airplane
formations  by constructing  say  a model of a
collection of cubes to represent a city 

using the simple heuristic  ordering by svm
confidence   we were able to identify a cube in
simple cube images  see figure    

finally  the weakness of the svm is that it is a
binary classifier  ultimately  to create a
machine capable of identifying all objects  one
would like to not enter image patches into
thousands of various svms  ideally one
could create an n object classifier  whose
running time did not vary with n  this might
improve detection of every kind of object 
cube corners would be better defined in
contrast to all other objects than with simply
cube corners and non cube corners 

acknowledgements
wed like to acknowledge geremy heitz for
initial guidance  carl erickson for
mathematical assistance  and thorsten
joachims for svm light  and  of course 
andrew ng for wonderful instruction that
made this possible 

figure     top row  shows the result of running
simple heuristics on the given cube before finally
choosing the top     bottom row   shows the results
of model fitting to the top   or   points 

the future
one idea we had and experimented with was
to train multiple svms to identify the

 

fireferences
cornell box  the  cornell university program of computer graphics     dec       
 http   www graphics cornell edu online box  
corner detection  wikipedia     dec         http   en wikipedia org wiki corner detection 
derpanis  konstantinos g  the harris corner detector     dec       
 www cse yorku ca  kosta compvis notes harris detector pdf 

 

fi