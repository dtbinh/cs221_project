training log linear models using smoothed hamming loss
olga russakovsky  in collaboration with samuel gross  chuong do  serafim batzoglou
december         

 

introduction

in a paper that is to appear in nips this year      we proposed a new objective function for training conditional random fields  crfs   when using the traditional log likelihood training  the training objective
function is fundamentally different from the testing objective  the accuracy of the resulting parse   we
wanted to develop a method of training crfs where the training and testing objectives would be more
similar  we proposed using a differentiable approximation of the hamming loss as the training objective  in
order to make the approach feasible  we derived a dynamic programming algorithm to efficiently calculate
the gradient of this new function  we then used bfgs to train the crf with respect to this objective 
and test its performance on sequence labeling problems  using the trained parameters  we parsed new 
unlabeled sequences by choosing  for each position independently  the label with the highest posterior probability  the results were very promising  showing that especially on difficult problems  where at each position
the posterior probabilities of the different labels were very similar  the maximum accuracy function significantly outperformed the log likelihood objective  we also tested our idea on a large scale problem of gene
prediction  and showed that this new objective function is much more resistant to noisy labels in the data 
for this project  we wanted to see if this idea can generalize to other applications as well  such as rna
folding or nlp part of speech parsing  however  both of those problems cant be solved with a linear
sequence labeling model  and so we are using a more general tree structured model instead 

 
   

algorithmic idea
notation

let x l denote an input space of all possible input sequences  and let y lxl denote the space of all possible
output labels  a label can be assigned to a span  i  j  of the sequence  

   

conditional log linear models

we define the conditional probability of a labeling y given an input sequence x as
p


t
exp
nt  l y w f    x 
exp wt f  l  x  y 
p
 
pw  y   x    x
 
t
z x 
exp
nt  l y  w f  n  x 

   

y  y lxl

where we define n  ta b y as the nodes in the parse tree corresponding
p to the labeling y covering the span
a  b of the sequence x  the summed feature mapping  fa b  x  y    nta b y f  n  x  and where the z x  is
the partition function used to normalize the distribution 

 

fi   

relationship to stochastic context free grammars

cllms can be used in the framework of stochastic context free grammars  scfgs  scfgs include production rules r    r        rn and probabilities p    p         pn corresponding to each rule  consider sequences
generated according to this grammar  then for a particular parse y corresponding to some sequence x 
y n  y 
pi i
   
p  y   
i

where ni  y  is the number of times ri occurs in the parse y 
now consider a cllm model incorporating this scfg  the features of each node in the parse tree thus
depend on the production rule used  furthermore  consider setting
f  l  x  y    n

   

w   log p 

   

in other words  the feature count of the ith feature in the cllm is ni   and the weight of that feature is the
log pi    then it can be shown that the two expressions for the probability of the parse  corresponding to
the cllm and the scfg  are equivalent 
finally  from now on assume that the grammar were working with is unambiguous  so there is a unique
translation from the given labeling y to a parse tree 

   

maximum accuracy

now  instead of training this model to maximize the log likelihood of the data  as is conventional  we instead
want to train it to maximize the total accuracy of the labeling  which is the objective we care about in the
first place   in particular  let d    x t    y t   m
t   be the set of training examples  we wish to maximize
a w   d   

m
x

x

  p  yn   yn t   x t      p  yn   a x t    for all a    yn t   

   

t   nt  l y t 

in other words  for each node in the parse tree corresponding to an input labeling y  t    we add   to the
objective if our cllm correctly predicts the label for the span of that node  however  we cant do the
maximization directly on a step function  instead we let q be a differentiable function arbitrarily similar to
the step function i x      x       and instead maximize
a w   d   

m
x

x

q p  yn   yn t   x t      p  yn   a x t    for all a    yn t   

m
x

x

q 

   

t   nt  l y t 

 

t   nt  l y t 

x

 t 

yy lxl  yn  yn

exp wt f x t    y  
 maxa  y t 
n
z x 

x

yy lxl  yn  a

exp wt f x t    y  
 
z x 
   

now we use the chain rule to compute the gradient of a with respect to w  for the sake of space  we
omit the discussion of the dynamic programming algorithms used to efficiently compute the gradient of this
function  however  the equations are fairly straight forward generalizations of the method presented in     
except that now they are similar to the inside outside algorithms of context free grammars instead of the
forward backward algorithms  for the function q  we use
q x     

 
    exp x 

   

as     q x       x       so a w   d   a w   d   however  the approximation a w   d  is smooth
for any      
 

fi   

training

within this framework  were currently using the l bfgs algorithm to optimize the parameters of the model 
in the future  were considering experimenting with different algorithms  however  this algorithm worked well
for the linear sequence labeling case  so were continuing to use it for now  note that one drawback of the
proposed objective function is its non convexity  in an attempt to avoid falling into a local minimum while
training  we first optimize the parameters using the convex likelihood function  and then attempt to improve
on them using the maximum accuracy algorithm  this is the same approach that was used for the gene
prediction experiments 

 

rna folding

we are planning to apply this model to various different applications  the one were focusing on currently
is rna secondary structure prediction  in particular  we would like to extend the contrafold    project
to encorporate this new training algorithm 

   

biology background

an rna is a single stranded linear molecule of nucleic acids that conveys genetic information  the nucleic
acid alphabet consists of just   characters  a  c  g and u  the rna molecule can fold on itself as bonds
between these nucleotides form  the resulting secondary structure of rna is very important for its function 
as it determines the types of interactions it can have with other molecules around it  therefore  being able
to accurately predict rna secondary structure is a necessary step to being able to understand the function
of the molecule  for the purposes of this project  were interested in correctly predicting
   the pairings within the molecule  nucleotide i pairs with and bonds to nucleotide j 
   the unpaired nucleotides
the tradeoff between the value of these two occurrences will be controlled by a parameter   because
predicting a nucleotide pairing correctly is much more important to determining the overall structure of the
molecule than predicting that a certain nucleotide is unpaired  however  it should be strictly worse to predict
an incorrect pairing than to not make a prediction 

   

context free grammars

the problem of rna secondary structure prediction fits very nicely into the framework of context free
grammars  in particular  consider the following grammar proposed by     and referred to as the g  grammar  
s  ls l

   

l  af a a
f  af a ls

    
    

where a  a can correspond to any of the   nucleotides  this grammar is the first step in testing out the
algorithm  since it is much simpler than a real grammar which would be used by an rna folder  yet it is
unambiguous and has been shown to work quite well nevertheless     

   

limitations

one of the limitations of this approach is that it cant predict pseudoknots or any other structure that is
not completely nested  however  no method is currently able to predict these structures with a reasonable
time complexity  and  furthermore  these structures are fairly rare so high accuracy can be achieved even by
programs that dont take them into account 
 

fi 

results

the results are still pending  however  there are a couple of experiments that were in the process of running 
for these tests  were using rna sequences from the rfam database  using the same selection criteria as in
     thus only the structures that have been generated by biological experiments and not those predicted by
a computational program are going to be used 
in the immediate future  we plan on accomplishing the following tasks 
   comparing the performance of maximum accuracy and likelihood parsing using the simple g  grammar
   encoding the more complicated grammar used in contrafold  and comparing the performance
directly to the published results
   comparing the speed of our more generalized model with contrafold
preliminary results should be available by the end of next week 

 

future directions

furthermore  some other options that were going to explore are
   evaluating the tradeoff of   the potential gain in accuracy from using a more exact approximation
versus the longer training time and even the potential loss of accuracy as the function becomes harder
to optimize  one option to consider is increasing  slowly as the training progresses 
   evaluating the performace of other objective functions besides approximate accuracy and likelihood 
e g  max margin methods
   generalizing from rna secondary structure prediction to a more complicated model of nlp part of
speech tagging 
overall  this is really just the beginning of this work  the goal is to conclude the project and produce
publishable results within the next two quarters 

 

references
   gross  s s  russakovsky  o  do  c b  batzoglou  s  training conditional random fields for maximum
labelwise accuracy  to appear in neural information processing systems  nips       
   do  c b  woods  d a  and batzoglou  s          contrafold  rna secondary structure prediction
without energy based models  bioinformatics         e   e   
   dowell  r d  and eddy  s  r  evaluation of several lightweight stochastic context free grammars
for rna secondary structure prediction  bmc bioinformatics             

 

fi