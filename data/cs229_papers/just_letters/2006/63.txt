evaluating the effectiveness of regularized logistic regression for the
netflix movie rating prediction task
adam sadovsky
sadovsky cs stanford edu

 

xing chen
xingchen cs stanford edu

introduction

netflix prize is a competition created by netflix that challenges individuals to predict how a user
will rate movies he or she has not seen  given the ratings for movies he or she has seen  the
computer science department at stanford has built a team that aims to tackle this problem by
developing and testing various machine learning approaches such as matrix factorization 
clustering  various types of linear and logistic regression  etc  one of the most successful
prediction methods thus far has been logistic regression with l  regularization  which achieves
approximately      root mean squared error  rmse  on the probe set  however  because l 
regularization imposes a gaussian prior on feature weights  it tends to assign non zero weight to
all features  for various reasons  l  may outperform l   furthermore  because l  regularization
results in zero weight for a large majority of features  it can also help us construct more complex
models  such as markov networks with movie potentials  

 

data representation

netflix provides contest entrants with several datasets      a sparse matrix consisting of       
movies and          users  with integer ratings    to    for select user movie pairs      a
separate probe set  which includes ratings  on which to test prediction accuracy  and     a set of
user movie pairs for which predictions must be sent to netflix for assessment  in uncompressed
form  the training set size is approximately     gb  the sheer size of the training set makes it
difficult to run queries and generate features from the raw data  furthermore  because a database
 sql  representation of the training set wasnt available to us  we had to write our own perl and
matlab scripts to parse the training set  extract ratings vectors for individual users  and construct
generic feature matrices for individual movie classifiers 

 

applying logistic regression

to apply logistic regression to the netflix rating prediction problem  we consider each movie to
be a separate problem requiring its own classifier  for any given movie m  our training data thus
consists of all users who have rated m  more specifically  each training instance corresponds to a
distinct user  this users feature vector contains the ratings given by that user to every other
movie in the dataset    if not seen   in addition  each movie rating is accompanied by a binary
feature that indicates whether the user has seen the movie  thus  supposing there are m movies
in the database  a single feature vector contains   m    features  including the intercept term  
since we do not include the rating of the movie we are classifying as a feature  the
corresponding label for each training instance is simply the rating given to movie m by this user 
additional details on feature definition  including normalization and scaling  are detailed in the
section    first  however  we discuss the two types of logistic regression used in our work 

fi 

regularized logistic regression models

unregularized logistic regression often overfits parameters to training data  logistic regression
finds the parameters  that maximize the following expression 

where r   is a regularization term     for standard logistic regression   for l  regularization 
r   is the sum of the norms of the components of theta  for l   r   is the sum of the squares of
these components     
because the l  norm is not differentiable at zero      we cannot use simple gradient descent
to optimize the l  regularized objective function  furthermore  because of the size of the netflix
training set  we cannot use newtons method to run l  regularized logistic regression
 intractable computation of inverse hessian   and gradient descent  implemented in matlab 
takes an extremely long time to converge  as a result  we eventually resorted to using pre written
packages to compute optimal parameters for the l   and l  regularized logistic regression
problem  for l  regularization  we used the lr trirls package      for l  regularization  we
used a matlab implementation of the irls lars method     
although both l  and l  regularization serve to prevent overfitting  they result in vastly
different feature weights  l  regularization imposes a gaussian prior on feature weights  as
mentioned in the introduction  the resulting weights are essentially all non zero  l 
regularization  on the other hand  results in non zero weight for less than    of the features  our
hope  therefore  is that l  regularized logistic regression will outperform l  on at least some
subset of the complete data  the two can then be combined  i e  using ensembl  to create a
universally better classifier  we initially hoped to also use l  weights as a basis for constructing
a markov model that encodes relationships between movies  but the task of training l regularized lr on the netflix dataset proved quite challenging in its own right  see future
work for more information on markov model construction 

 

feature design

as explained in section    in order to predict what a user u will rate some target movie m  we
build a training set consisting of vectors of movie ratings for all users that have rated movie m 
each training examples feature vector consists of pairs of features  viewed  rating  for a users
rating on each movie  viewed is either   or    depending on whether the user rated the movie
or not  and rating is a normalized  centered user rating  or   if the movie was not rated by this
user  
to center the ratings  we tried several approaches 
   user average  centers the rating around the users average rating  the advantage to this
approach is that it scales for user bias  for example  some users will tend to rate all
movies relatively high  while others will rate on a more conservative scale  the

fidisadvantage of user average centering is that some movies are better than others and
thus get higher overall ratings 
   movie average  centers the rating around each movies average rating  this method takes
care of movie bias  but does not account for user bias 
   weighted combination         movieavg         useravg  
centers the rating around a linear combination of both sources of bias  weights chosen
based on empirical performance 
   adjusted movie average   movieavg   useravg   avg useravg  
centers the rating around a movie average that is scaled by a users relative average
rating  for example  if the user usually rates movies lower than other users  this approach
adjusts the users rating for a given movie to a level that is proportionally below that
movies average rating 
the rating for the target movie rm is then centered around the same baseline and translated
into a binary         label  where label     if rm      label      otherwise  the feature vectors for
all users who rated the target movie m along with their corresponding labels are then passed as
input to l  regularized logistic regression 

 

rating prediction

the output of the trained logistic regression classifier represents the probability that user u would
label movie m    where label           in our case  label     corresponds to user u rating
movie m above the baseline rating  where the baseline is defined as described above   using this
probability  we can now predict the actual rating for this user movie pair 
to compute the predicted rating  we consider the full distribution of movie ratings given by
user u and compute an expected value for two sets of ratings  those above the baseline and those
below it  more specifically  let e    e ratings below baseline  and e    e ratings above baseline  
we compute the predicted rating as 
predicted rating   h x  e       h x   e 

 

testing methodology

to compare l   and l  regularization for logistic regression  we generate l  and l  predictions
for sets of movies with approximately                       and      ratings  where each bins
movie set contains    movies   we also compute simple baseline predictions  e g   predict a
users rating to be average rating  for another basis of comparison 
for each movie  we hold out     of the data for testing and train a classifier on the
remaining     of users who rated that movie  we then judge the accuracy of the ratings by
computing the root mean squared error  rmse  on the test set  due to time and memory
constraints  we were unable to compute rmse for the full netflix probe set  it takes

fiapproximately   hours to generate input features and train a classifier for a single movie with
     ratings   the rmse values shown below are therefore estimates of the true test set rmse 

 

results and discussion
rmse vs number of ratings for target movie

   

average rmse

   

avg l  w  baseline  
avg l  w  baseline  
avg l  w  baseline  
avg l  w  baseline  

 

avg baseline  
avg baseline  

   

   
   

   

    

    

    

approximate number of ratings for target movie

figure    average rmse for movie prediction as the number of ratings increases

figure   highlights several important trends  note first that  as expected  a larger training set
generally results in better performance  for example  the average rmse for all classifiers for
movies with       ratings is       lower than average rmse for movies with        ratings 
rmse vs number of ratings
   

 

   
rms e

more importantly  however  l 
logistic regression starts to significantly
outperform other classifiers when the size
of the training set increases  on movies
with      ratings or less  l  averages
rmse        baseline     which is worse
than the rmse       achieved by the
baseline   predictor  however  when the
size of the training set is greater than
      l  regression  baseline    averages
rmse        while the baseline  
predictor yields rmse       thus  as we
might expect  l  performance increases
dramatically as the training set size
increases  surprisingly  though  the l regularized logistic regression classifier
does not exhibit similar behavior  figure    

   
        
         

   

   

 
baseline  

baseline  

l 
l 
l 
l  w 
w  baseline   w  baseline   w  baseline   baseline  
classifier

figure    average rmse across classifiers for large and
small training sets

fifigure   also shows that l   and l  regularized logistic regression classifiers offer little or
no benefit over simple baseline prediction methods for movies with very few ratings 
consequently  in a full blown netflix prediction system  we can avoid training lr classifiers for
these sparsely rated movies and instead rely on simple baseline classifiers  however  for movies
with more ratings  l  regularized logistic regression clearly offers significantly improved
prediction accuracy  furthermore  supposing l  regularized logistic regression performs
acceptably well on movies with           ratings  we can make the prediction task more
tractable for movies with many  e g          ratings by training on only a subset of the users
who rated them 

 

future work

l  regularized logistic regression worked surprisingly well for the netflix movie rating
prediction task  unfortunately  our method of extracting features and training the l  classifier
are essentially infeasible for the full netflix dataset  however  the feature extraction task can
certainly be optimized  we currently use a perl script to parse the raw data file   and the l 
training code  currently implemented in matlab  can probably be ported to c    which would
result in orders of magnitude speed increase  once we are able to train l  regularized lr
classifiers for all of the movies in the netflix dataset  we can use ensembl methods to
combine the l  classifier with other prediction methods  including the l  classifiers  
furthermore  the l  classifier itself can probably be tweaked further via changes to the
normalization and probability to rating transformation steps 
another direction we wish to explore  as mentioned in the regularized logistic regression
models section  is the use of l  trained feature weights as a basis for a markov model 
consider a trained l  classifier for some movie m  movies whose features were assigned nonzero weights can be viewed as highly correlated with m  we can thus use these feature weights
to construct a simple markov network  where nodes represent movies and each movie is
connected to all other movies it is related to  note that  by adjusting the weight of the l  prior 
we can force a smaller number of non zero weights if necessary   edge affinities can also be
computed as functions of the learned l  weights  we can then run inference on this markov
network to compute the likelihood of each possible rating for a target movie  given ratings for a
subset of the other movies in the network 

   references
    http   ai stanford edu  ang papers icml   l l  pdf
    http   www stanford edu class cs    proj rakshekumarl regularizedlogisticregression pdf
    http   komarix org ac lr  lr trirls
    http   www stanford edu  hllee aaai   l logreg pdf

fi   appendix
table    classifier rmses
num
ratings

   

   

    

    

    
avgs

id
    
    
    
    
    
    
    
    
    
    
    
    
     
    
    
    
    
     
     
     
    

baseline 
      
      
      
   
     
     
     
     
     
     
      
     
     
     
     
     
     
     
    
     
     
           

baseline 
      
      
      
     
     
     
     
     
     
     
      
     
     
     
     
     
     
     
     
     
    
      

l  w  baseline  
      
      
     
     
   
     
     
     
     
     
      
     
      
       
      
      
      
      
       
      
      
           

l  w baseline  
      
      
     
    
   
     
     
     
     
    
      
     
      
       
      
      
      
      
      
      
      
           

l  w  baseline  
      
      
     
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
           

l  w  baseline  
     
      
      
      
      
      
      
      
      
     
      
     
    
      
      
      
      
      
      
      
     
           

table    average rmse for number of ratings
num ratings
   
   
    
    
    

l  w 
baseline  

l  w 
baseline  
       
       
        
       
      

l  w 
baseline  
     
     
     
     
     

l  w 
baseline  
     
     
     
     
     

baseline  
     
     
     
     
     

baseline  
     
     
     
     
     

baseline  
     
     

l  w baseline
 
      
      

l  w baseline
 
        
           

l  w baseline
 
      
           

l  w 
baseline  
       
           

    
     
     
     
     

table    average rmse
number of
ratings
        
         

baseline  
      
     

fi