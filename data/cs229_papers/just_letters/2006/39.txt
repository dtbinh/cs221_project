stock trading with recurrent reinforcement
learning  rrl 
cs    application project
gabriel molina  suid        

fi 
i  introduction
one relatively new approach to financial trading is to use machine learning algorithms to predict the rise and fall of
asset prices before they occur  an optimal trader would buy an asset before the price rises  and sell the asset before
its value declines 
for this project  an asset trader will be implemented using recurrent reinforcement learning  rrl   the algorithm
and its parameters are from a paper written by moody and saffell   it is a gradient ascent algorithm which attempts
to maximize a utility function known as sharpes ratio  by choosing an optimal parameter w for the trader  we
attempt to take advantage of asset price changes  test examples of the asset traders operation  both real world
and contrived  are illustrated in the final section 
iii  utility function  sharpes ratio
one commonly used metric in financial engineering is sharpes ratio  for a time series of investment returns 
sharpes ratio can be calculated as 

st 

average  rt  
standard deviation  rt  

for interval

t          t

where rt is the return on investment for trading period t   intuitively  sharpes ratio rewards investment strategies
that rely on less volatile trends to make a profit 
iv  trader function
the trader will attempt to maximize sharpes ratio for a given price time series  for this project  the trader function
takes the form of a neuron 

ft  tanh  w  xt  
where m is the number of time series inputs to the trader  the parameter w   m      the input
vector xt     rt       rt  m   ft      and the return rt  pt  p t    
note that rt is the difference in value of the asset between the current period t and the previous period  therefore 

rt is the return on one share of the asset bought at time t     
also  the function ft        represents the trading position at time t   there are three types of positions that can
be held  long  short  or neutral 
a long position is when ft      in this case  the trader buys an asset at price p t and hopes that it appreciates by
period t     
a short position is when ft      in this case  the trader sells an asset which it does not own at price p t   with the
expectation to produce the shares at period t      if the price at t    is higher  then the trader is forced to buy at
the higher t    price to fulfill the contract  if the price at t    is lower  then the trader has made a profit 

 

j moody  m saffell  learning to trade via direct reinforcement  ieee transactions on neural networks  vol     no    july
     

fi 
a neutral position is when ft      in this case  the outcome at time t    has no effect on the traders profits 
there will be neither gain nor loss 
thus  ft represents holdings at period t   that is  nt    ft shares are bought  long position  or sold  short
position   where  is the maximum possible number of shares per transaction  the return at time t   considering the
decision ft     is 

rt    ft    rt   ft  ft   
where  is the cost for a transaction at period t   if ft  ft    i e  no change in our investment this period  then
there will be no transaction penalty  otherwise the penalty is proportional to the difference in shares held 
the first term     ft    rt   is the return resulting from the investment decision from the period t      for example 
if      shares  the decision was to buy half the maximum allowed   ft          and each share increased

rt    price units  this term would be     the total return profit  ignoring transaction penalties incurred during
period t   
v  gradient ascent
maximizing sharpes ratio requires a gradient ascent  first  we define our utility function using basic formulas
from statistics for mean and variance 
we have

st 

e   rt  
e  rt      e  rt     
 



a
b a

 

where a 

  t
 rt
t t  

and

b 

 
t

t



t  

rt

 

then we can take the derivative of s t using the chain rule 

 ds t da ds t db
ds t
d 
a






dw dw  b  a    da dw db dw
t
t
 ds da ds t db  drt
 ds da ds t db   drt dft
drt dft   
  t 


  t





drt
db drt  dw
db drt   dft dw dft   dw 
t    da
t    da drt
the necessary partial derivatives of the return function are 

drt
d
  ft    rt   ft  ft     d      ft  ft        

dft dft
dft
  

ft  ft     
ft  ft     

    sgn  ft  ft    
drt
d
  ft    rt   ft  ft       rt  d      ft  ft        

dft   dft  
dft  
   
   rt    sgn  ft  ft    
then  the partial derivatives dft dw and dft   dw must be calculated 

ft  ft     
ft  ft     

fi 









dft
df 
d
d

tanh  wt xt        tanh  wt xt       

 wt xt      tanh  wt xt         xt  wm    t   
dw dw
dw
dw 

note that the derivative dft dw is recurrent and depends on all previous values of dft dw   this means that to
train the parameters  we must keep a record of dft dw from the beginning of our time series  because stock data
is in the range of           samples  this slows down the gradient ascent but does not present an insurmountable
computational burden  an alternative is to use online learning and to approximate dft dw using only the previous

dft   dw term  effectively making the algorithm a stochastic gradient ascent as in moody   saffells paper 
however  my chosen approach is to instead use the exact expressions as written above 
once the ds t dw term has been calculated  the weights are updated according to the gradient ascent

rule wi    wi    ds t dw   the process is repeated for n e iterations  where n e is chosen to assure that
sharpes ratio has converged 
vi  training
the most successful method in my exploration has been the following algorithm 
   train parameters w   m    using a historical window of size t
   use the optimal policy w to make real time decisions from t  t    to t  t  n predict
   after n predict predictions are complete  repeat step one 
intuitively  the stock price has underlying structure that is changing as a function of time  choosing t large
assumes the stock prices structure does not change much during t samples  in the random process example
below  t and n predict are large because the structure of the process is constant  if long term trends do not appear to
dominate stock behavior  then it makes sense to reduce t   since shorter windows can be a better solution than
training on large amounts of past history  for example  data for the years ibm           might not lead to a good
strategy for use in dec        a more accurate policy would likely result from training with data from           
vii  example

sharpe  ratio

price  p t 

 

    

    
    
    
   
    
    

   

  

   

   

  

   

  

   
t

   

  
training iteration

   

  

figure    training results for autoregressive random process 

   

  

   

    

  

t         n e    

the first example of training a policy is executed on an autoregressive random process  randomness by injecting
gaussian noise into coupled equations   in figure    the top graph is the generated price series  the bottom graph
is sharpes ratio on the time series using the parameter w for each iteration of training  so  as training progresses 
we find better values of w until we have achieved an optimum sharpes ratio for the given data 

fi 
then  we use this optimal w parameter to form a prediction for the next n predict data samples  shown below 

figure    prediction performance using optimal policy from training  n predict      
as is apparent from the above graph  the trader is making decisions based on the w parameter  of course  w is
suboptimal for the time series over this predicted interval  but it does better than a monkey  after      intervals
our return would be     

sharpe s ratio

price series  pt

the next experiment  presented in the same format  is to predict real stock data with some precipitous drops
 citigroup  
  
  
   

   

   
t

   

   

   

   
    
 
  

  

  

  

  
  
training iteration

figure    training w on citigroup stock data  t        n e     

  

  

  

   

fi 
 

returns  rt

 

  

   

   
   

   

   

   
t

   

   

   

   

   

   
t

   

   

   

   

   

   
t

   

   

   

 

ft  decisions 

   

 

    

  
   

percent gains    

  

  

  

 
   

figure    rt  top   ft  middle   and percentage profit  cumulative  for citigroup  note that although the general
policy is good  the precipitous drop in price  downward spike in rt   wipes out our gains around t       
the recurrent reinforcement learner seems to work best on stocks that are constant on average  yet fluctuate up and
down  in such a case  there is less worry about a precipitous drop like in the above example  with a relatively
constant mean stock price  the reinforcement learner is free to play the ups and downs 
the recurrent reinforcement learner seems to work  although it is tricky to set up and verify  one important trick is
to properly scale the return series data to mean zero and variance one   or the neuron cannot separate the resulting
data points 
vii  conclusions
the primary difficulties with this approach rest in the fact that certain stock events do not exhibit structure  as seen
in the second example above  the reinforcement learner does not predict precipitous drops in the stock price and is
just as vulnerable as a human  perhaps it would be more effective if combined with a mechanism to predict such
precipitous drops  other changes to the model might be including stock volumes as features that could help in
predicting rises and falls 
additionally  it would be nice to augment the model to incorporate fixed transaction costs  as well as less frequent
transactions  for example  a model could be created that learns from long periods of data  but only periodically
makes a decision  this would reflect the case of a casual trader that participates in smaller volume trades with
fixed transaction costs  because it is too expensive for small time investors to trade every period with fixed
transaction costs  a model with a periodic trade strategy would more financially feasible for such users  it would
probably be worthwhile to try adapting this model to this sort of periodic trading and see the results 

 

gold  carl  fx trading via recurrent reinforcement learning  computational intelligences for financial engineering 
      proceedings       ieee international conference on  p           march       special thanks to carl for email
advice on algorithm implementation 

fi