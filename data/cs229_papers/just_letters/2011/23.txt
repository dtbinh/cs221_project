promoting student success in online courses

chuan yu foo
yifan mai
bryan hooi
frank chen

cyfoo stanford edu
maiyifan stanford edu
bhooi stanford edu
frankchn stanford edu

   introduction

       automatic tagging

online education has become popular as an effective
method of imparting knowledge to a wide audience
efficiently and at low cost  unlike traditional offline courses  online courses open up the possibility
of fine grained tracking and monitoring of students
activities  progress and performance  in turn  this
creates the possibility of of analyzing the factors involved in student success  retention and progress 
an understanding of these factors could allow online
educators to better tailor their courses to promote
student success 

we attempted to predict tags on forum topics with
related keywords  allowing users to more easily
search for infomration on a particular topic  simply
by selecting particular keywords 

in this project  we will examine these factors in the
context of the machine learning and databases online courses  ml class and db class  that stanford
university is offering in fall       specifically  we
aim to examine the factors behind student success
and student retention in these two courses 

   methods
     tasks
for this project we looked at three different tasks
pertaining to our online education platform 
       student success
we attemtped to predict students midterm examination scores based on the data collected for the
databases course  this task was done using l regularized linear regression 
       student retention
we attempted to predict whether students will continue on  defined by submitting at least half of
the quizzes for that week  in the machine learning course in week   given their activities in weeks  
     this task was done using l  regularized logistic
regression from the liblinear library      

automatic tagging can be modelled as either a supervised or an unsupervised learning problem  in
the supervised scenario  the algorithm learns in an
online setting  in which the algorithm suggests tags
whenever a forum post is made and receives feedback
when the user actually selects a tag  in the unsupervised scenario  the algorithm attempts to automatically cluster forum posts and tags each cluster with
a keyword that is representative of the cluster 
we have used both supervised and unsupervised
learning methodologies for our project 
     data collection
over the course of    weeks  students taking the
online machine learning and databases classes engage in a number of learning and non learning related activities on the respective course websites 
these include watching lecture videos  completing
review quizzes  exercises  and programming assignments  students may also ask questions and reply to
questions on a question and answer  q a  forum
on the website 
for the purposes of this project  we collected data
the following activities of individual students  the
times at which the student watches a lecture video 
attempts a quiz  exercises or programming assignment  how frequently the student switches between
video speeds during lecture  pause and seek events
during lecture  attempts and scores on quizzes  exercises  and programming assignments  content of
forum postings on the integrated q a forum  how
many views the students questions received  how
many votes the students questions and replies received  when the student registered and what track

fi basic or advanced  only applicable for the machine
learning course  the student is on 

model
unweighted lr
lwlr
neural network

     preprocessing
before passing the data into the learning algorithms 
the following preprocessing steps were taken 

train
 
 
           
           
           

test
 
 
           
           
           

figure    model accuracies for unweighted linearregression  unweighted lr  and locally weighted
linear regression  lwlr   percentage of scores predicted to within indicated error  for the student success task

       student success and retention
for the student success task  students who did not
submit the midterm were removed  data on student
activities after the midterm began was also removed 
after this      features and      examples remained 

   results

for the student retention task  students who were
deemed to dropped out before week    i e  had not
submitted the one quiz for week    were removed 
data on students activities after the end of week  
was also removed  after this      features and     
examples remained 

     student success
for the student success task  our goal was to predict students midterm scores based on the features
described above  the results for this task are summarized in figure    the learning curves can be seen
in figure   

following this  the remaining examples for each of
the tasks were divided into train  cross validation 
and test sets in the ratios                  the
training data was then normalized for each feature
by subtracting the feature mean  and dividing by the
feature standard deviation  missing features  e g 
where students did not attempt quizzes  were replaced by   or the mean for that feature as appropriate  the normalization values  means and standard deviations  were saved and re used for testing
on the cross validation set  after cross validation 
the learning algorithm was retrained on the training
and cross validation set and tested on the test set 

the first model we tried was l  regularized linear regression  the regularization parameter   
     was chosen using cross validation  the model
successfully predicted       of students midterm
scores exactly  and       of students midterm
scores to within    this includes the students
whose scores were predicted exactly   the accuracies on the training set were similar  with       of
scores predicted exactly  and        predicted to
within    this suggested that the algorithm was
suffering from high bias 
as it was not possible to obtain more features  we attempted to address this issue by using a more powerful model  regularized locally weighted linear
regression  the regularization parameter       
and the bandwidth parameter       were chosen
using cross validation  performance on the test set
was still poor  with       of scores predicted exactly 
and       predicted to within    performance on
the training set was similar  with       of scores
predicted exactly  and       predicted to within   
this suggested that the high bias problem was still
unresolved 

       automatic tagging
for the automatic tagging task  we first preprocessed
the forum text using a standard stemming algorithm  replaced common symbols with special tokens  and removed the most frequent words as well
as the words and tags that appear at most   times 
this left      words  distributed among      forum
topic titles and message bodies  the resulting input
to the algorithm is a document word matrix containing the frequencies of each word in each message
body  and a separate document word matrix containing the frequencies of each word in each topic
title  the output of the algorithm is the list of tags
it predicts for each document 

 

in another attempt to resolve the high bias problem 
we used a regularized neural network to model
the data  the neural network we used had     input units  one for each feature     hidden units  and
  continuous valued output unit for the predicted
score  the hidden layer used a sigmoid activation
function  while the output layer was linear  the
regularization parameter        was chosen using cross validation  compared to unweighted lin 

fi a  unweighted linear regression

 b  neural network

figure    learning curves for two of the three models in the student success task  the graphs plot training
 blue line  and test  red line  accuracy  number of scores predicted to within    against number of training
examples 
ear regression  the neural network did not perform
much better  on the test set        of scores were
predicted exactly  and       predicted to within
   training accuracy was similar  with       of
scores were predicted exactly  and       predicted
to within    this suggested that the high bias
problem was still not resolved 

gistic regression  the regularization parameter 
      was chosen using cross validation  the model
was not successful  accomplishing an accuracy of
      on the test set  which is at chance for the data
set  the entire data set  and the test set in particular 
both contain dropped out and continuing students in
the ratio        making chance performance about
        accuracy on the training set was similarly
low at        suggesting that the logistic regression
model might be suffering from high bias 

in the light of this  we conclude that the features
we have are not sufficiently predictive of students
midterm grades  interestingly enough  since our features included assessment data  including the number of time students attempted each quiz and the
minimum  mean and maximum scores they obtained
for each quiz  this might suggest that the current assessment methods are not sufficiently discriminative
of students understanding of the material  under
the assumption that the midterm is a ground truth
indicator of students understanding of the course
material  

since it was not possible to obtain additional features for the dataset  we attempted to address
the high bias problem by using a more powerful model  the support vector machine  svm 
with gaussian kernels  however  performance
was still poor  with the svm yielding an accuracy
of       on the test set and       on the training
set  suggesting that the high bias problem was still
not resolved 
in the light of this  we conclude that the features
we have are not sufficiently predictive of whether
students drop out of the course or not  this could
be explained by the fact that students may drop out
of the course for many external reasons such as loss
of interest  lack of time  other commitments  and so
on  reasons which are not captured by the features
in our dataset 

     student retention
model
logistic regression
svm with gaussian kernel

train
     
     

test
     
     

figure    model accuracies for the student retention
task

     automatic tagging  supervised

for the student retention task  our goal was to predict whether students would continue in the course
or drop out of the course in week    based on features collected regarding their activities in weeks      the results for this task are summarized in figure    the learning curves can be seen in figure  
above 
the first model we tried was l  regularized lo 

       modifications to standard naive
bayes
for the tag prediction task  we use the naive bayes
algorithm with two significant modifications 

 

fi a  logistic regression

 b  svm with gaussian kernel

figure    learning curves for the two models in the student retention task  the graphs plot training  blue
line  and test  red line  accuracy against number of training examples 
incorporating message titles and bodies  we
assumed titles and bodies are conditionally independent given a class  then we can use the naive bayes
assumption to model probability of title text and
body text separately  as such  we pick the class

 is a variable constant which would be   in regular
laplace smoothing  in our case  we chose  by crossvalidation to minimize test error  this modification
was performed because we observed that the sparseness of the data meant that the ones added during laplace smoothing seemed to be overwhelming
 or at least adversely affecting  the sparse frequency
data that was actually collected  and thus we felt
that a smaller parameter  might be more appropriate  it turns out that this modification to laplace
smoothing can in fact be justified as a linear interpolation between a maximum likelihood estimator and
a uniform prior     

i
which maximizes the posterior probability of the
text  such that 
argmaxi  p yi  p t  yi  p b yi     


y
y
p tj  yi  
p bj  yi  
argmaxi p yi  
j

       training results

j

for each tag  the vast majority of the forum posts
belong in the untagged category  as such  we measured the performance of our algorithm using precision and recall  combined into the f measure score 

where y  is the untagged class  y  is the tagged class 
and t and b are the title and body vectors for the
message we are currently processing  specifically  tj  
the jth entry of t   is   if the jth word appears in the
message title and   otherwise  and bj   the jth entry
of b  is   if the jth word appears in the message
body and   otherwise  justifying this formula requires an extension to the naive bayes assumption 
we need to assume that the message titles and bodies
are conditionally independent given the class of the
message  which we feel is a reasonable assumption 

model
nb
nb titles
nb titles smoothing

pm

 i 

  x      y  i         
pm j
 i          
i     y

i  

and similarly for j y     here y  i  is the class of the
 i 

ith training example and xj is   if the jth word appears in the ith training example  and   otherwise 

test
      
      
      

figure    f  score for the tag prediction task

modified laplace smoothing  in our version of
laplace smoothing  we estimated j y     the conditional probability of the jth word being present
given the class y      as follows 
j y    

train
     
     
     

 

naive bayes  in our first attempt  we used naive
bayes with the usual laplace smoothing and only
using the message bodies as input  as can be seen
from the table  the predictors training and test fmeasures were both quite poor  due to the large
difference between training and test errors we hypothesized that overfitting was occurring  as no
more data is readily available  however  we decided
to incorporate the topic titles into our predictor as

fiused the lda c implementation by blei to process
our data  the topics produced were able to distinguish between admin related  theory related  and
implementation related content  we give some examples below 
admin topics week  date  slide  post  due 
schedul  note  materi  miss  got  solut  accuraci  submiss  review  incorrect  check  accept  temp
figure    learning curves for the final tag prediction algorithm  the graphs plot training  blue
line  and test  red line  f measure against number
of training examples 

theory topics valid  test  model  cross  curv 
real  high  sampl  select  overfit  layer  output  nn 
hidden  paramet  classifi  unit  digit  given
implementation topics plot  gnuplot  after 
call  window  close  paus  execut  matlab  window  command  instal  undefin  script  near  version 
linux  type

described in our methodology  hoping to give our
algorithm more information to work with 
naive bayes  with titles   therefore  for our
second attempt  we incorporated the topic titles using the modified prediction formula described in our
methodology  as can be seen from the table  this
led to a small improvement in both training and
test performance  note  however  that this modification does not directly tackle the overfitting problem  it does provide more data  but simultaneously
enlarges the space of possible models  since the prediction model for each tag now includes separate
conditional probability vectors for the title and message body   this is consistent with the observation
that the training f measure also improved  whereas
normally adding more data would be expected not
to improve training performance 

   conclusion
for student success and retention  quiz scores and
other features are not very predictive  therefore  we
conclude that student retention and success is hard
to predict with the set of features we have  we have
identified the errors to be poor accuracy due to high
bias and we plan to acquire more features such as
student survey and time spent on quiz  etc    in the
future  in addition  we could collect more features
by initiating voluntary student surveys throughout
future courses in order to understanding any external factors influencing their performance and success
rates throughout the class 

naive bayes  with titles and modified
smoothing   on our third attempt  we used the
variable smoothing constant            chosen using cross validation  as a result  the algorithm did
significantly better in both training and test settings 
the large difference between training and test performance that still remains  as can be seen from the
learning curve  indicates that the algorithm is still
significantly overfitting  considering that even the
most frequent tags appear in only about    messages  this is not completely unexpected  as the algorithm does not have enough data in a particular
tags tagged class to build up accurate prior and
conditional probabilities for that class 

for text identification  we again found that the results were not as good as we had expected  we identified this as severely overfitting due to the large difference between training and test f scores  we will
attempt to get more forum data over the next iteration with more classes and try different algorithms
 such as regularized naive bayes  as we move on
with the project 

   acknowledgments
we would like to thank professor andrew ng for
giving us the opportunity to work on the online education project  we would also like to thank jiquan
ngiam for his generous assistance 

     automatic tagging  unsupervised
for the unsupervised automatic tagging task  we
used latent dirichlet allocation  lda  model proposed by blei et  al          specifically  we

references
 

    d  blei  a  y  ng  m  i  jordan  latent dirichlet allocation  journal of ma 

fichine
learning
research
 
         
http   www cs princeton edu  blei papers bleingjordan     pdf
    r  e  fan  k  w  chang  c  j  hsieh  x  r  wang 
and c  j  lin  liblinear  a library for large
linear classification  journal of machine learning
research                     software available at
http   www csie ntu edu tw  cjlin liblinear
    m  d  smucker and j  allan  an investigation of
dirichlet prior smoothings performance advantage 
http   www mansci uwaterloo ca  msmucker publications smuckersmoothing ir    pdf

 

fi