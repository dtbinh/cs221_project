unsupervised morphological segmentation
with recursive neural network
minh thang luong  cs   n cs      final project report
   introduction

parse tree for a word could be derived from the rnn 

recent works have been successful in applying recursive neural network  rnn  architectures to predict
hierarchical tree structures of scene images and natural language sentences  socher et al                in
this project  we focus on the natural language modality
and explore how rnns could address the morphological segmentation problem 
motivated by  socher et al              s work in syntactic parsing of natural language sentences  where the
input is a sequence of words  our goal is to learn similar
hierarchical parse trees but for words instead  treating each character as a unit  by recursively grouping
characters together  we aim to achieve unsupervised
learning of not only the shallow morphological segmentation  i e  breaking words into morphemes  but
also the deep structure of word formations 

figure    recursive neural network architecture 
 a   the recursive process of constructing  n     new
nodes  p              p n     from the original character nodes
 c              c n    of a word  e g  cats   b   the merging process of combining children nodes  c lk     c rk     into a
parent node p k  with a local decision score sk  

unlike them  we explore learning the segmentation
task in an unsupervised manner  two novel types of
information  lexical and structural  are proposed to incorporate into the rnn that helps boost performance 

let x be a word of length n  i e  x   c        cn  
and a parse tree y corresponds to an ordered set of
 n    local decisions in the form of merging triplets 
 c lk   c rk    p k     where lk and rk are indices of the
left and right children respectively  the parent node
p k  is also considered as c n k    at the end of the
merging process  the rnn tree will have a total of
 n    nodes  with  c              c n    being the original
nodes  figure     the score for each local decision is
denoted as sk and computed as 
 
 
sk   g p k     s
   

the report is organized as follow  section   formulates the rnn architecture  while details of the backpropagation process are given section    unsupervised
learning is described in section    followed by our discussion in section   about incorporating lexical and
structural information into the rnn  experimental
setup and results are given in section    we suggest
future work in section    and conclude in section   

   recursive neural network  rnn 
     representation

p k    f  z  k   

let v be an ordered set of all characters in a language 
which is parameterized by the matrix w c of size d 
 v    specically  the ith character is in d dimensional
space  represented by the ith column of w c  

z

 lk  

  w  c

   
 rk  

 c

    

   

where  s and w are part of the rnn parameters  w
is of size d    d       in which d is the dimension
of character vectors  we use tanh as our activation
function f   for exibility  we abstractly denote the
score function as g which takes in parameters  s  

     structure prediction formulation
suppose the rnn parameters have been learned  details in section     we discuss how the most probable

 k 

 

the score s x  y  of a word x and a parse tree y is

fin 


k     sk  

simply the score sum of all the local decisions 
s x  y   

n 


sk

where the gradients for each local decision
are addressed in section   
   

k  

lastly  given t  x  be the set of candidate parsed trees
for a word x  its rnn score is maxyt  x  s x  y  
note  follow  socher et al                we use
g  x   s       s   x 

   unsupervised learning
     training examples
our input is a set of m distinct words x  
 x              x m    in a language  which we will treat as
positive training examples 
to employ unsupervised learning  negative examples
are articially constructed by corrupting the input
words  given an input word x i    one way to corrupt it is to randomly select a position and replace
the character at that position with a newly random
one  other options could be to change multiple characters at a time  or scramble the characters of x i    or
both  we opt to use the former option of corrupting
one character at a time to create a controlled setting
that could teach the rnn gradually  in fact  initial
experiments show that the system learns poorly when
multiple mutations take place at the same time 
     subgradient methods
from section    our parameters include   
 w c    s   w    follow  collobert   weston         the
cost function is designed so that the rnn will be optimized towards giving higher scores for correct word
forms while penalizing corrupted ones  specically 
our ranking type cost attempts to boost the score of
each positive example x i  up to a margin   set to    
experimentally  towards its negative example x i   
j    

m


note  we approximate the best parse tree y  by performing a greedy search as in  socher et al         
which iteratively nds a pair of adjacent nodes with
the highest score and combine them to yield a new
set of adjacent nodes for the next iteration  contextaware greedy search was experimented as well  however  without any further information about the word
structure or the morpheme distribution  the search
continues cluelessly  resulting in parameters that just
do not correlate with the segmentation task objective 
we further justify these in the later sections 

   back propagation through structure
to compute the gradients for each local decision score
sk   backpropagation through structure  bts   goller
  kuchler        is employed 
     error term derivation
as an intermediate step  we dene the error terms
 h 
 k    z h  sk   related by a recursive formula   
   
 
 p h  
f  z  h     w        k
  if left split
 h 
 
 
k  
      p h  

 h 
f z
   w    k
  if right split
   
   
   
for h  k  where w    w w b   at the base case 
 
 
 k 
 k   f  z  k    s
   
     bts gradient formulae
the gradients of the model parameters    
 w c    s   w   with respect to sk could be derived as 
sk
 k 
   k  c lk     c rk       
w
sk
  p k 
 s

 

 
max      s x i      s x i   

   

i  

due to hinge loss  the objective function j is not differentiable  hence  subgradient method  ratli et al  
      is employed instead of gradient ascent ones 
 
 
m
s x i   
j    s x i 
 

 
   



i  
for a word x  to compute  s x   we rst decode x to

nd the best tree y    we have  from eq        s x   


   
    

for w c   let wc be the column vector representing for
a character c  let  p i              p ih     be all the nodes
under the subtree rooted at p k   inclusive   which are
arranged in the order added by the rnn  using the
chain rule technique for ordered derivatives suggested
in  werbos         we have 
 
 


 i  
 ij  
s
 
z
 
z  ij     k j
s
 
k
k
c
c
 ij  
wc
w
w
z
j  
j  
h

h

 
h  k indicates that the node p h  is part of the tree
rooted at p k    p h  is a child of the node p p h    

filen
 
 
 
 
 
 
 
 
  

type
   
    
    
    
    
    
    
    
   

token
    
    
    
    
    
    
    
    
   

es
in
er
re
ed
ng
at
ti
te

morpheme subsequences
    ing     tion   
ation
    ion
   atio   
tions
    ati
   ness   
ating
    ers
   ting   
iness
    ate
   ling   
ities
    ess
   ions    ement
    tio
   ator    house
    ies
   ally   
alize
    ent
   ates   
ously

  
  
  
 
 
 
 
 
 

table    training data statistics   examples   left   the type and token counts of morpheme subsequences of
lengths   to    characters   right  the top frequent morpheme subsequences of lengths   to   with their token counts 

here   indicates simple derivatives that ignore the
fact that z  ij   depends on z  i              z  ij     


 w      
c   c lij     c rij  



 rij  
   

 w  
  c lij  
  z  ij  
 
  c c
 

wc
 w          w      
c   c lij     c rij  




 
otherwise
    

   morpheme prior and word
structure information
     character subsequence distribution
the rnn could potentially learn the distributional
representation of characters through w c   and their
compositional patterns by means of the parameters
w   however  it is not clear how the cost function
could drive the model towards optimizing the morphological segmentation task  especially in the context of unsupervised learning  without labeled data 
the model needs some form of prior information about
what could possibly be a morpheme unit and what
may not  to help it bootstrap the learning process 
hence  we incorporate the distribution of morpheme
subsequences into the model objective 
specically  we rst collect counts of all character subsequences of each word across the entire dataset  after that  ml estimates conditioned on the subsequence
length  in terms of characters  are derived  which gives
us the prior probabilities lex m  of each morpheme
subsequence m  such lexical information is incorporated into model by modifying eq      to become 
 
 
sk   g p k     s    lex p k   
where  is the lexical weight  which we set to    experimentally  table   gives the training set statistics
on the type and token counts of morpheme subsequences with some examples of the top frequent ones 

     minimum height parse trees
linguistically  words are constructed by building up
from minimal meaning bearing units  which are morphemes  however  the rnn has no clue about the
underlying structure of a word  which we could think
of as a hidden layer of morpheme labels  as a result  it could end up in a wrong left skewed parse
tree as in figure   a   to alleviate this problem  we
enforce a minimum height constraint on each node 
specically  recall that a binary tree of n nodes will
have a minimum height of   log   n            hence 
for each subtree rooted at a parent node p  spanning over n leaf nodes and possessing a height of
h  the structural score of that parent node will be
struct p      log   n           h  
we incorporate such structural scores into the rnn
model similar to the addition of lexical information in
the previous section  note  however  that only nodes
in the parse trees of the positive training examples
will include structural scores  as these scores are nonpositive  such structural constraints are expected to
prevent the rnn from considering unbalanced trees
for good words  eq      for the positive training
examples is extended to 
 
 
sk   g p k     s    lex p k       struct p k   
where  is the structural weight and set to     experimentally  while ad hoc in nature  this method  to
some extent  has helped alleviated the problem  figure   b  shows an example in which our model could
nd a correct tree with a more balanced structure 

   experiments and results
     data and evaluation metrics
we test our model on the english portion of the morphological segmentation data are publicly available at

fid
d

i

s

r

e

g

a

r

d

e

i

s

r

e

g

a

r

d

e

d

d

 a  height    

 b  height  

figure    parse trees of the word disregarded  shown are   a  a wrong left skewed parse tree of height   and  b 
a correct parsed tree of height   

the morpho challenge        semi supervised and unsupervised analysis website  there are a total of     
words with gold standard segmentation  and those do
not contain punctuation  e g   quotes or hyphens  are
retained  which leaves us with a dataset of      words 
we train and test on the same set  but no segmentation information is used during training 
without label information  we need to approximately
evaluate how good or bad a parse tree is  with respect
to the true segmentation of a word  e g   dis regard
ed  to do that  a simple metric is designed to look
at indices of the word span that each node in a tree
covers  and compare them with the true segmentation
spans  in our example  the true segmentation results
in spans               and          while those of the tree
in figure   b  are                                    etc 
if a span of the correct segmentation appears in the
tree spans  a segment score of one is given  if all correct
spans of a word are in the tree spans  a word score of
one is awarded  normalizing the segment and word
scores by the total number of correct spans and words
respectively gives the segmentation and word accuracy 
     results
our nal segmentation results are presented in table  
in terms of word and segment accuracy  we start with
a baseline where a greedy search strategy is used without any other information  which gives        and
       in word and segment accuracy respectively 
at this point  one might wonder  by means of our metrics  how a random system would score  to make it
fair  we give some explicit gures here  there are    
words without any segmentation out of      words 

and     segments with a single character out of     
correct segmentations  these will give a random system a score of                   in word accuracy
and                           in segment accuracy 

greedy
greedy lex
greedy lex struct
context lex struct

word
      
      
      
      

segment
      
      
      
      

table    experimental results  shown are the word and
segment accuracy of  a  the base system using greedy
search   b  adding lexical information   c  constraining on
the word structure  and  d  greedy  context aware search
with lexical and structural information 

gradually adding the lexical information and constraining on the word structure consistently improve
the system performance  with a gain of       and
       in absolute word and segment accuracy respectively compared to the baseline  context aware greedy
search further boosts the results by another       in
absolute word accuracy and       in absolute segment
accuracy  achieving the best performance of       
and        in word and segment accuracy 
     discussion
compared to the performance of state of the art unsupervised methods  in which the best result for english has  precision  recall  f measure  of         
                   our performance is considered modest  however  it is worth noting that we have not made
 
http   research ics tkk fi events 
morphochallenge     comp  results shtml

fiuse of the full word list of    k words with frequency
information that these unsupervised methods utilize 
at the same time  we only model each character with
a     dimensional vector  while in principle  it could
be scaled up to a much larger value since our vocabulary size is small  only    characters   due to time
constraint  the vector dimension is kept small so that
we could experiment with dierent training parameter
congurations  in reality  the set of parameter combinations to choose from is enormous  which makes it
very tricky to get the rnn to work  as such  it is encouraging to start seeing the model produces sensible
analyses though with errors  sample segmentations of
several long words in the dataset are shown in table   

gold
anthrop olog ical
collect iv iz ation
co religion ist s
rational iz ation
re conciliat ion s
respons ibil iti es
transmogric ation

rnn
anthrop olog ical
collecti viz ation
core ligion ist s
ration al iz ation
re conc iliat ion s
respons ibilit ies
trans mogri cation

table    sample segmentations of several long words in the
dataset  shown are the true and automatic ones 

   a final thought   future work
while adding lexical and structural information does
help limiting the search space during parsing  it does
not  however  directly involve in the optimization process  as a result  the rnn sometimes could scale
up the parameter values  diminishing the lexical and
structural inuence  hence  we suggest for future work
an approach that tackles the problem of nding the
underlying structure of words 
in  creutz   lagus         a codebook of all
morphemes mi in the data is maintained  and
an em like algorithm was used to minimized a
cost
 function as the data likelihood  cost data   
morph tokens mi  ln p mi    given segmented data 
p mi   could be reestimated using ml estimates 
we mimic by maintaining list of morphemes  which
will be iteratively updated  and introduce a logistic
layer for the rnn to give each node a probabilistic
score of being a morpheme unit or not  given parameters of the rnn  morpheme labels could be inferred for
a tree by labeling nodes with the classication scores
sorted in descending order  each subtree will be ignored once its root node has been labeled  for each
morpheme labeled node  a term  ln p m   where m is

the character sequence that node covers  will be added
to its score formula in eq       which essentially incorporates the data likelihood into the rnn cost function 

   conclusion
in this project  we have achieved a better understanding of how rnn could be applied and interact with
the morphological segmentation task  especially in unsupervised context  while the performance is modest  we have demonstrated the eectiveness of using
morpheme subsequence distribution and tree height
constraint  by making the rnn cost function better correlated with the task objective  these additional
information has yielded non trivial improvements for
the task  capturing the underlying structure of words
in a more principle manner is a worthy goal that we
plan to pursuit in future work 
acknowledgement  we thank richard socher for
providing the code base and feedbacks for the project 

references
collobert  r  and weston  j  a unied architecture for
natural language processing  deep neural networks
with multitask learning  in icml       
creutz  mathias and lagus  krista  unsupervised discovery of morphemes  in workshop on morphological and phonological learning of acl       
goller  c  and kuchler  a 
learning taskdependent distributed representations by backpropagation through structure  ieee transactions on
neural networks                 
ratli  nathan d   bagnell  j  andrew  and zinkevich  martin a   online  subgradient methods for
structured prediction       
socher  richard  manning  christopher  and ng  andrew  learning continuous phrase representations
and syntactic parsing with recursive neural networks  in nips      workshop on deep learning
and unsupervised feature learning       
socher  richard  lin  cli c   ng  andrew y   and
manning  christopher d  parsing natural scenes
and natural language with recursive neural networks  in icml       
werbos  p  back propagation through time  what it
does and how to do it  in proceedings of the ieee 
volume     pp                 

fi