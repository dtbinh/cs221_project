news feed optimizer
final report
brennan burns
vijay harid
fadi zoghzoghy

fiburns harid zoghzoghy

final report

december         

   introduction
each day  millions of people across the globe read thousands of news stories regarding many of the
different  ever changing realities of the world  as such  the news industry has become a business like
any other   one of maximizing profits by determining and delivering the most useful or desirable
product to the customer  a major part of this is the proper advertising of news stories to the end user 
the more refined and personalized the advertising  the more effective 
it is for this reason that the entire idea of a news feed was created  by supplying the customer  or user 
with a constant stream of not only current news topics  but those specifically of interest to that
individual user  the likelihood that the user will read an article on the news companys site increases 
obviously  in order to accomplish this goal  there needs to be an efficient automated way to determine
what articles are most likely to interest a user  enter machine learning 
   pre processing
as with all machine learning problems  the biggest hurdle is acquiring a good data set  this is more
than simply having a sufficiently large data set for the different methodologies and techniques to work
well  but also encompasses the problem of having well posed data  in an ideal world  all data would be
recorded and stored in such a form  but this is obviously not the case  and as such  quite a bit of preprocessing was required to massage the data into a working form 
the first step of the pre processing was to parse and reformat the original data  part of this process
included the recognition and removal of duplicate entries  which were rampant throughout the data 
the next step was to build a dictionary of all the words in the data set  for this dictionary  all of the
words were forced into lower case  all symbols and punctuation were removed  and the resultant words
were stored in a single large vector of approximately        words  with this vector in hand  the tfidfs for each user for each day were calculated  and used in the initial implementation  discussed
below  this giant feature set  however  led to the over fitting of the data  requiring the number of
features to be reduced  as will be discussed in section   
   initial approach
with all of the tf idf vectors created  logistic regression was applied to each user for each day 
allowing for the basic prediction of which articles the user will read  while this extremely simple
approach results in a seemingly excellent accuracy of       it is a meaningless result  which is
obtained by always predicting that a given user will not read a particular article  this comes from the
fact that a given user will only be exposed to  let alone read  an extremely small number of articles
relative to the total amount  in order to accurately predict what a given user would read in a meaningful
way  the articles considered for that user need to be reduced to those which he is likely to have been
exposed to  since all of the articles in the data set come from a specified feed url  it is reasonable to
assume that a given user was only exposed to articles from the same feeds from which he accessed the
 

fiburns harid zoghzoghy

final report

december         

articles he read 
the first step in this reduction was to separate the articles by their respective feed urls  of which there
were approximately      once the feeds were segregated  logistic regression could be applied to only
the articles associated with the feeds a given user accessed  which was  on average  between four to
seven feeds per user  while this greatly reduced the total number of articles considered  the number of
articles in those handful of feeds a user accessed could still be quite a bit larger than the user could
have reasonably been exposed to 
in order to balance the negative and positive instances in the training and testing process  it was
assumed that for every article a user read  there was a second article that he was exposed to  from the
same feed  that he did not read  this second  non read  article was randomly selected from the
associated feed  this balancing prevents the user from being punished for not reading articles which he
was not exposed to 
with the number of articles
sufficiently reduced  both
logistic regression and svm 
via liblinear  were applied to
predict which articles a given
user would read  while the
hurdle of the false    
accuracy had been overcome 
it became immediately
apparent that data was being
over fit by these algorithms 
figure   shows the training
and testing error from the
application of logistic
regression 
the application of svm
resulted in very similar
figure    training and testing error of logistic regression  in green is the mean
testing error  which should be decreasing over time  and the mean training error is in behavior  meaning that the
total number of features  or
blue  the red and black curves are the respective training and testing error for eight
individual users 
words in the dictionary 
needed to be reduced  as discussed previously in section    from the initial         words  the
dictionary was reduced to approximately       words by eliminating stop words  such as articles  and
those that rarely occurred throughout the month  while this reduction greatly increased the
computational efficiency of the algorithm  the shear amount of data that needed to be process was so
large that most pcs were unable to run the computations  these computations were only successfully
run when using a    core computer cluster  which still took quite a bit of time to process everything 
making it difficult to debug and develop  furthermore  this feature reduction did not cure the overfitting problem  which required a more intricate approach 
 

fiburns harid zoghzoghy

final report

december         

   clustering approach
the problem of over fitting 
mentioned in the previous section  is
caused by having too little data per
user  since the data set only covers
the reading habits of the users
throughout the month of august  the
total number of articles a given user
read is relatively small  by clustering
users based upon their reading
preferences  this problem can be
avoided  since the total data volume
per cluster is significantly larger than
the volume per individual users 
figure    user comparison relative to user   throughout august  the figure
another major advantage of clusters on the left shows the distance between the tf idf vectors of user   and seven
is that once a user has been identified other users  users      and all stories  which is the base case in which a user
as belonging to a particular cluster  reads all the stories every day  throughout the month of august  the figure on
articles can be recommended to that the right is similar  but computes the dot product instead 
user based upon what other users have read  for example  if user i has been identified as being in the
same cluster as user j  articles that user j has read will be recommended to user i  since both users are
from the same cluster and are likely to be interested in the same articles 
figure   shows a comparison of several
different users and how they can be
justifiably clustered  for example  user  
can be clustered together with user  
 blue  and user    red  as the euclidean
distance between their corresponding
cumulative tf idf vectors is relatively
small  and the dot products of the
normalized tf idf vectors are large  this
indicates the presence of strong similarities
among these users  allowing them to be
clustered via k means clustering by cosine
distances of the tf idf vectors  as shown
in figure   

figure    k means clustering

these clusters were found by taking the tf idf vectors of     users on day     and finding the cosine
distances between these users  using them as parameters for k mean clustering into eight clusters 
which was found to be a good compromise between accurate clusters and computational efficiency 
these eight clusters  shown in figure    were then further optimized with     additional users by
identifying their most appropriate cluster based on the cosine distance between their tf idf vector 
and the average tf idf vectors of the clusters  each of the remaining     users are then identified as
being part of a cluster every day  using the same method  it was found that most users did not change
 

fiburns harid zoghzoghy

final report

december         

clusters after the fifth day they read stories  meaning that the stories they read could be accurately
predicted 
once the users were
clustered  several different
approaches could be used to
actually predict whether a
specific article would be
read by a given user  as
shown in figure    a given
article is tested by user i s
cluster  be it by majority
voting or treating the cluster
as a super user  as discussed figure    resultant user clusters   this image shows several of the top keywords from
each of the eight user clusters formed via k means clustering  labels were added based
below  depending on the
on the general theme of the keywords 
resultant decision by the
cluster regarding the given article  it will be recommended to the user in question 
the first of the two decision
methodologies used  that of majority
voting  was implemented by using a
system of weak  uncorrelated
classifications  the main idea being
that for the given article was tested via
svm for every user within the cluster 
all of the resultant decisions for each
figure    cluster prediction process
individual user were tallied  and
whichever decision had a simple majority of votes  won  and was passed on as the recommendation for
the article 
the second decision methodology was to treat each cluster as a super user of sorts  meaning that all of
the stories and all of the feeds that the users within a given cluster were treated as if they were accessed
by one giant super user  in this case  the decision regarding the given article was based on the
application of svm to the entire group of articles related to this super user  and was then used as the
recommendation for the article 
   results
the application of majority voting via the clusters turned out to be less than stellar  the resulting f 
scores  shown in figure    averaged approximately     over    days for each of the eight clusters  a
large part of this is due to the fact that svm is being applied for each article to each user within the
appropriate cluster  these votes are then tallied to determine the proper recommendation  since svm is
still being applied on an individual basis  the expected accuracy is on the same order as individual
prediction  and therefore still random  but is averaged over a large number of users 
 

fiburns harid zoghzoghy

final report

december         

as expected  the application of a super user via the
clusters turned out to have slightly better accuracy
than both majority voting and individual
predictions  as demonstrated by the f  scores of the
clusters  shown in figure    this is largely due to
the fact that the super user approach has a far larger
quantity of data than what is available to individual
users  this indicates that a far larger data set is
desirable to obtain more accurate results 
as discussed in section    the vast quantity of data
required for the calculations ran on a    core
figure    f  scores of clusters with majority voting   this cluster  often for many hours at a time  these time
plot shows several f  scores of clusters used in majority
voting over the month of august  the blue curve is of a large scales limited the amount of optimization that
cluster  the red of a small cluster  and the green is the mean could feasibly be done  one such future
f  score of all clusters  as can be seen  larger clusters tend optimization would be to change the base time
to have better f  scores  which increase over the course of
scale from days to hours  which would improve the
the month 
accuracy of these predictions  since articles read
in the morning by one user could be recommended to a different user in the same cluster later in the
day  it would  however  increase the computational requirements of the predictions 
   conclusion
while the articles that a given user will read can
be predicted on an individual scale  it was found
that this method resulted in rather poor accuracy
due to a lack of data for each individual  as
such  the users were clustered together by their
reading habits  and these clusters were then used
to determine the whether a given article would
be recommended to a given user  two methods
of determining this recommendation were used 
majority voting and super users  with majority
voting  each article was tested on each user
within a cluster  and the  votes  were tallied  with
figure    f  scores for super user clusters   this plot shows
the majority vote determining the
several f  scores of clusters used for the super users over the
recommendation  for the super users  that data month of august  the blue curve is of a large cluster  the red of a
from all the users within a cluster were
small cluster  and the green is the mean f  score of all clusters 
combined and treated as if one user had read all similar to the f  scores from majority voting  the large cluster
of the corresponding articles  the given article has a better daily f  score than the small 
was then tested on this super user  with the result being the recommendation given  as discussed above 
the utilization of super users via clusters was found to provide the most accurate predictions 
 

fi