empirically verifying and innovations on several machine learning
theorems and techniques
yisheng jiang  cs    

tel              

e mail  yishengjiang yahoo com

abstract
in this project i will perform several machine learning techniques on a training set of     samples and   
features  along with     of their perspective labels  the techniques i use include  stochastic gradient
descent for linear regression  closed form linear regression  forward search feature selection  using highorder variables in linear regression  k fold cross validation  training error vs  generalization error  and lastly 
an innovative combination of logistic regression and multi tier cross referencing that allows us to predict
continuous variables with binary classification methods 

introduction

first i used alpha          the error margin
improved for the first    iterations  figure     but
got worse after that 

in cs     we learnt a massive array of tools to
operate on data  this paper is motivated by the
desire to put these tools to work and possibly 
using trials and error  to improve upon these tools 

stochastic gradient descend for linear
regression
i implemented the stochastic gradient descend for
linear regression as follows  starting with a zero
vector of coefficient  i look for the direction of
steepest descend in adjusting that vector  as
expressed in the cost function  i then scale that
vector with an learning rate i most learning rates i
tried caused the prediction to diverge with each
iteration  attempts were made to  scale  the size
of the adjustment  normalizing the vector to keep
the direction of the adjustment the same  but the
magnitude smaller than    of the original vector  it
did not work  the error gap was still diverging 

figure   error margin at alpha       
similarly  learning rate at        also diverged and
gave terrible predictions  figure    

then i tried something i learned on the pca lecture
notes  normalizing the x matrix so they are
expressed with zero mean and in terms of their
standard deviations from the meanii  this confined
the x values from    to    and it allowed me to get
much better empirical results 
figure   prediction at alpha       
 

fithen i used alpha            the error margin
improved for the first    iterations  but got worse
after that  figure    

figure   a learning rate          that worked

figure   error margin at alpha         

figure   a not so great prediction after      
iterations

figure     prediction at        learning rate after
   iterations before it diverged 

having tried a few other learning rate values  i was
able to find one that does not cause the prediction
to violently diverge  using learning rate of         
and iterating       times  i was able to make a
somewhat better prediction of the data 

as you can see here the error gap dropped sharply
in the first few iterations  and they continue to
drop slowly up to       iterations  our conclusion
regarding the stochastic gradient descend is that
for our set of data  it is hard to a small enough
learning rate that the prediction does not diverge 
and once you find a small enough learning rate  it
takes many iterations to find a good prediction 

 

filinear regression closed form

the linear regression in closed form is
implemented as theta    x  x  x  yiii  the graph of
the predicted labels vs  actual labels is shown in
figure  a 

figure  a  closed form linear regression
using the error measurement metric of
norm  actual predicted   actual   we find that we
have a score of         this will be the starting
point of improvement when we do feature
selection 

x   three of those are in higher order dimensions 
using the same prediction error metric as the last
section  we have a score of        an immediate
improvement over the raw linear regression with
first order terms 
using this method to choose n top most relevant
features  the table   shows the training errors we
found by varying the parameter of n top 

number of top features tr training error
 
      
 
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
  
      
table   average error as a function of num of top
features
we see that our method of feature selection is
robust enough that selection of up to    top
features will get better results than the raw firstorder linear regression  our prediction got much
better  as can be seen from figure  b 

feature selection
the previous sections linear regression uses only
first order variables  what if we combine that with
second  third  and fourth order feature variables
and perform linear regression on that  concretely 
we would have the sample matrix of  x  x     x    
x     for a total of    features  then we use
forward search feature selectioniv  starting with a
zero feature vector  making predictions with an
extra feature and picking the most relevant one 
repeating until we have n top most relevant
features  we will test if that improves our
prediction 
having implemented the feature selection
algorithm  we find that the   most relevant
features are x     x      x    x    x   x    x     

figure  b closed form linear regression with top
   features
 

fitraining sample size vs  generalization
error

upper bound theorem and will not always be the
case  as i saw empirically  figure    

i now attempt to find the relationship between
training sample size and the generalization error by
using the k fold cross validation mechanism  which
randomly splits our training set into k randomly
permutated subsets  looping through these sets 
using one set for generalization and other sets for
training  we average the generalization errors with
trained parameters in each permutated subsetsv 
by controlling the number of sets  we also control
number of training examples 
we know that training error will decrease as the
sample size get largervi  but what about the
generalization error  figure   shows the empirical
relationship of the mean generalization error vs 
the training sample size 

figure   generalization error variance

multi tier cross referencing logistic
regression

figure   generalization error average vs sample
size

i was also told in the lecture notes that the
variance in generalization error will increase as the
training sample increasevii  variance in
generalization error was obtained by randomly
permutating the sample size in    different trials 
which produces a set of    generalization errors in
each iteration  i understand that is more of an

even though our data is not binary  we can still
make a series of binary predictions on the data
using logistic regression  and then cross reference
these predictions to group these data sets into
several different intervals  i call this multi tier
cross referencing logistic regression  concretely  if
make predictions on whether y     y     y    and
so on  and cross reference these predictions  we
can group the labels into several buckets  and if
the number of buckets is large enough  we can
simulate continuous value predictions with binary
classifications 
the actual predictions employ newtons method
for logistic regressionviii  the result is surprisingly
good  below is the graph of actual vs predictedlabel with several different number of buckets
 figure     

 

fifigure    multi tier cross verification 

conclusion
we find that several theorems and techniques
taught in cs     are empirically sound  stochastic
gradient descend is volatile  closed formed linear
regression is both faster and more reliable  higherorder feature variables can be relevant  and can be
found using forward search feature selection  kfold cross validation is a good way to verify
generalization errors and its relationship to sample
size  and that with multi tier cross referencing
logistic regression  we can model continuous
variable labels with a series of binary
classifications 

acknowledgement
i like to thank professor ng and the tas  richard
socher  marius iordan  david kamm  abhik lahiri 

andrew maas  peter pham and tao wang who are
all brilliant and helpful 

references
i

andrew ng  cs     lecture notes  supervised
learning
ii
andrew ng  cs     lecture notes  primary
component analysis
iii
andrew ng  cs     lecture notes  supervised
learning
iv
andrew ng  cs     lecture notes  regularization
and model selection
v
andrew ng  cs     lecture notes  regularization
and model selection
vi
andrew ng  cs     lecture notes  learning
theory
vii
andrew ng  cs     lecture notes  learning
theory
viii
andrew ng  cs     lecture notes  supervised
learning

 

fi