music mood classification
cs     project report
jose padial
ashish goel

introduction
the aim of the project was to develop a music mood classifier  there are many categories of mood into which
songs may be classified  e g  happy  sad  angry  brooding  calm  uplifting  etc  people listen to different kinds of
music depending on their mood  the development of a framework for estimation of musical mood  robust to
the tremendous variability of musical content across genres  artists  world regions and time periods  is an
interesting and challenging problem with wide applications in the music industry 
in order to keep the problem simple  we considered two song moods  happy and sad 

database
as with any learning project  the size and quality of the data set is key to success  we initially underestimated
the difficulty in acquiring a music database labeled by mood  building the labeled happy sad database
proved to be a challenging journey for a number of reasons  not the least of which being the difficulty in
making the subjective decision to label songs as strictly happy or sad 
we began by analyzing songs from our personal music collection and soon realized the need for a larger and
more comprehensive database  after spending some time searching for a suitable database  we found the
million song database  msd   a freely available collection of audio features and metadata for a million
contemporary popular music tracks  the msd was compiled by labrosa at columbia university with the
help of analysis done using echo nest api  an open source platform for analysis of audio files   each track
data file contains a wealth of tempo  mode  minor major   key and local harmony information  this is the
information we planned to extract ourselves via time domain and spectral methods  and were thus very
excited to find it in this database 
the entire database of a million songs is    gb in size  downloading and unpacking the database alone took
several days  and crawling through the database within the timeframe of this project turned out to be an
infeasible task  hence  we largely operated with a subset of the database containing        songs 
the most challenging task was generating accurate happy sad labels for the songs contained in this database 
tags from the website last fm were available for the songs contained in the msd  out of the   million msd
songs  nearly        had a happy tag  and over        a sad tag  however  upon inspection of these songs 
we discovered that the majority of happy sad tags were incorrect 
ultimately we hand labeled songs from the        subset to generate our training set  the final data set
comprised     sad songs and    happy songs  the drop from        to     is a result of most songs being
unfamiliar to us  and many of those we knew are not clearly happy or sad 
hold out cross validation was used for testing the performance of our learning algorithm      of the final
data set was used for training and     of it was used for testing purposes 

feature selection
the following were considered as candidate features for the classification process

fi





tempo  the speed or pace of the piece  measured in beats
beats per minute
minute  bpm   this is a time domain
feature which captures the rhythm of the song 
energy  obtained by integrating over the power spectral density  psd  
mode  indicates if a piece is played iin major or minor key 
key  identifies which of the    keys the song has been played in  fig     
harmony  relative weighting between notes  characterized as chords or modes 

figure       note musical scale 

harmony
while feature elements such as tempo and energy were easy to obtain and use  a lot of time and effort was
spent on sensibly extracting the harmony information from the data  the msd provided us with the psd of
    seconds long segments of the song arranged in    bins co
corresponding
rresponding to the frequencies of the   
different notes 
s  hence a song of duration     seconds was divided into      segments   yielding a pitch
matrix of size   x     for each song
song 
this local harmony information could be processed and used in several ways  if we had a large enough
training set  approximately    times the size of the feature vector   we could have simply passed the huge
    x   matrix into the classifier  however  since the data set was limited  we had to intelligently capture the
harmony information in a small sized
sized feature vector  the need for doing this will be more evident from the
learning curve analysis  fig     which sshows that we were suffering from the problem of high variance 
variance the
motivation for the approach we adopted came from the concept of modern musical modes as shown below in
fig    

figure    musical modes   each corresponding to a   note subset of the total    musical notes 
note
we hypothesized that extracting
xtracting the above modes from the harmony information would contribute to the
mood detection significantly  several
al attempts were made to associate the song with one of the   musical
modes  we switched to the time domain and tried working over segments of different lengths but couldnt
succeed in assigning a mode to a majority of the songs in our database  eventually  w
wee picked the   most
important notes for each of the     se
seconds long segments  averaged over the entire song and subtracted the
key from each of the notes to obtain a  
  dimensional feature vector for each of the songs  although there
might be better ways of capturing the harmony information  the us
usee of these   dominant notes as elements of
our feature vector did significantly
tly aid the classification task 

fimodel selection and supervised learning results
at different stages of the project when different features were being tested  the mutual information metric
was used to evaluate their usefulness  the kl distance was used for computing the mutual information  while
computing the kl distance is straightforward for the case of discrete feature vectors  the continuous feature
vectors were dealt with by binning them and then using the discrete approach  the following figure  fig    
lists the mutual information for each of the feature vectors considered 

figure   mutual information for different feature vectors
having obtained a rough idea about the usefulness of the various features at hand  the forward search process
was used to find the optimum set of features for classification through supervised learning using a soft
margin svm  the following table  table    shows the progress at some of the steps in the forward search
process  from the table  though it may seem that the feature vectors beyond energy and tempo didnt add
much to the classification process  one must remember that marginal improvement of performance gets
successively harder 

table    soft margin svm performance for some of the candidate feature sets and svm kernels 
depending on the set of feature vectors used  either linear or a radial basis function  rbf  kernel seemed to
give the best performance  in the case of simple features such as energy and tempo  where the relationship
with mood is quite straightforward  a linear kernel performed best  the addition of harmony information
introduced much more complexity to the feature space  and subsequently the rbf kernel gave the best
results 
it was crucial for us to use the soft margin svm because the training set was labeled manually  since the
perception of mood varies from person to person  there was a strong likelihood of some of the examples
being labeled incorrectly  we varied the c parameter to minimize the generalization error  in fact  the svm
module of matlab that was used for classification scales the c parameter for different training examples to
account for the difference in the number of training examples for each of the classes 

fianalysis
having finalized the composition of our feature vector  choice of svm kernel etc   we performed k fold cross
validation in order to arrive at better estimates of the generalization error  we decided not to use k fold cross
validation for model selection since that would be computationally expensive and cumbersome  we also
varied the size of the training set and averaged over the results of the iterations to obtain the following
learning curve  fig     

figure   learning curve obtained through k fold cross validation
the curve suggests that we are suffering from high variance  while we felt that with     training examples
and a    dimensional feature vector we would be okay  it turns out that we are indeed over fitting 

unsupervised learning
in order to gain more insight into our problem  we attempted unsupervised learning  if unsupervised
learning worked well in clustering the dataset into happy sad songs based on harmony alone  it would
suggest that what we subjectively consider as being happy or sad  correlates well with our harmony
feature vector 
k means clustering was run on the dataset with two clusters  harmony being the only feature vector  based
on the fact that the rbf gave best results for the features with harmony data  we hypothesized that k means
would not be able to do a great job clustering along the lines of happy and sad songs  however  we wanted to
test it and see how well it could do 
as expected  if we assign labels to the clusters  the classification thus obtained was poor with an accuracy of
        in order to gain some visual understanding of why the clustering might be so difficult  we plotted the
rank   approximation of the harmony feature data 

  d visualization of harmony only feature space
for visualization purposes  and as a sanity check on the data  we projected all of our   d harmony feature
vectors into   d space  to project our higher dimensional data into   d  we computed the svd  singular
value decomposition  of the nx  data matrix for each feature vector  we then selected the two eigenvectors
of ata corresponding to the largest singular values of our data matrix  taken from the first two columns of the
right singular matrix  we then projected each songs   d harmony feature vector onto the first and second

fiprincipal directions to obtain the coordinates of the feature vector in the   d space  fig    provides a good
visualization for the high inseparability of the data  albeit visualized in   d  this helps to explain why kmeans would do so poorly in separating the data  further  it helps to verify why the rbf kernel worked best
when harmony data was included in the feature vector  i e  the rbf was able to carve out a complex decision
surface for the best separation of the data 

figure      d low rank approximation of   d harmony feature data  red points correspond to songs
labeled  happy   blue points correspond to songs labeled  sad  

conclusion
the performance and capability of our algorithm can be significantly improved if we have access to a larger
dataset because a larger dataset would allow us greater freedom in playing around with different ways of
capturing the harmony information  considering the subjective nature of mood classification  we believe that
    success is a good result  the success of our algorithm is comparable to the results obtained by different
research groups around the world  papers in literature quote anywhere from     to     as the level of
success achieved by their algorithms        though it should be noted that the classification results listed in
the literature typically involve multi class classification as opposed to our binary classification task 

references
    cyril laurier and perfecto herrera         audio music mood classification using support vector machine 
in proceedings of the international conference on music information retrieval  vienna  austria 
    lu  liu and zhang         automatic mood detection and tracking of music audio signals  ieee
transactions on audio  speech  and language processing  vol           january     

acknowledgements
we thank prof  andrew ng  andrew maas and other members of the teaching staff for guiding us through the
project  we also thank abhishek goel for helping us classify the list of        songs in our database  finally 
we thank mayank sanganeria for his valuable suggestions and help regarding feature selection 

fi