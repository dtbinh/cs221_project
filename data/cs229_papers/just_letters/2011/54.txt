applying machine learning to product categorization
sushant shankar and irving lin
department of computer science  stanford university

abstract
we present a method for classifying products into a set of
known categories by using supervised learning  that is  given
a product with accompanying informational details such as
name and descriptions  we group the product into a
particular category with similar products  e g   electronics or
automotive  to do this  we analyze product catalog
information from different distributors on amazon com to
build features for a classifier  our implementation results
show significant improvement over baseline results  taking
into particular criteria  our implementation is potentially able
to substantially increase automation of categorization of
products 
general terms
supervised and unsupervised learning  e commerce

software and sophisticated methods  most companies that
sell goods online do not have such resources  we propose to
use machine learning as a way to automatically classify
products 

   data
    initial dataset
for our project  we have acquired multiple datasets with
thousands of products and their respective data from ingram
micro  the leading information technology distributor  which
list of product descriptors and resulting categorizations in
the form of a detailed csv document separating multiple
features seen in table    the datasets we have chosen to use
for the project comprises of the categories seen in table   
table      product  descriptors  

keywords
amazon  product categorization  classifier

product descriptor
sku
asin
model number
title with categorization
description
     tech details
seller
unspecified number of images
url

   introduction
small to medium sized businesses who sell products online
spend a significant part of their time  money  and effort
organizing the products they sell  understanding consumer
behavior to better market their products  and determining
which products to sell  we would like to use machine
learning techniques to define product categories  e g 
electronics  and potential subcategories  e g   printers  
this is useful for the case where a business has a list of new
products that they want to sell and they want to
automatically classify these products based on training data
of the businesses other products and classifications  this
will also be useful when there is a new product line that has
not been previously introduced in the market before  or the
products are more densely populated than the training data
 for example  if a business just sells electronic equipment  we
would want to come up with a more granular structure   for
this algorithm to be used in industry  we have consulted with
a few small to medium sized companies and find that we will
need an accuracy range of     when we have enough prior
training data and a dense set of categorizations 

   prior work
there has been much millions of dollars spent developing
software  e g   ibm ecommerce  that maintain information
about products  buying history of users for particular
products  etc  as such there is much work being done on
how to create good product categories  however  while large
companies  like amazon  can afford to use such expensive

  
table      company  a   left   and  company  b   right   catalogs  
category
electronics
camera   photo
video games
musical instruments
kitchen   dining
computer   accessories
automotive
health   personal care
office products
cell phones   accessories
home   garden
home improvement
sports   outdoors
patio  lawn   garden
software
movies   tv
toys   games
beauty
everything else
clothing
baby

 
    
   
   
   
   
  
  
  
  
  
  
  
  
  
 
 
 
 
 
 
 

category
health
beauty
electronics
home
toys
office
sports
pc
personal care appliances
books
kitchen
wireless
grocery
pet
video
apparel
lawn
baby
automotive
shoes
jewelry
watches
software
music

 
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
  
  
  
  
  
  
  
 
 

    parsing the data set
each product comes with a large amount of accompanying
data that may not be relevant or may add unnecessary

ficomplexity that we feel would be detrimental to our
classification purposes  of the numerous product
descriptors  we decided to keep sku as a way to uniquely
identify products  title with categorization  the description 
and tech details  the title in particular contained a high
concentration of important product information such as
brand and categorization  as a result  we were able to parse
out the categorization  but decided to temporarily leave the
brand as part of the title and treat it as another word  as a
side note  we decided not to incorporate the images as we
felt that the task would be more suited to computer vision
and image processing 

    splitting training and test sets
before creating our training and test sets  we decided to
merge all of the categories with less than      of the overall
products into one other category  and then decided to
train on a     random subset of the data and testing our
error rate on the rest 

   nave implementation
our predominant source of data is in the form of text
descriptors about a product  thus  we decided to implement a
simple nave bayes classifier and focus on features described
in section    the simplicity of nave bayes makes for a great
testing environment and feature building  which helped to
define the scope of the overall project 

    first implementation  nave bayes
to begin  we implemented a simple multinomial event model
naive bayes model with laplace smoothing on just the title
specification  our accuracy using this implementation was
around     

    modified implementation  nave bayes
we realized that our classifier was classifying everything as
electronics because the probability of the electronics
category was astronomical compared to everything else 
thus  we decided to remove the prior class probability out of
the equation in naive bayes 

electronics  in other words  for the large categories there
was high recall but low precision  for the smaller categories
there was low recall but high precision  this effect was also
seen in product catalog b  but to a much lesser degree as the
skewness of the catalogs is much less than in product
catalog a 

    filtering items
one issue we noted for the this classification scheme was
that often times  an item just didnt have enough information
to classify it properly  or that the words just didnt provide
enough information to really provide a situation such that
even a human would be able to properly classify one way or
another  in the prior case  if there werent enough words  we
just removed the item from consideration  and didnt
incorporate that into the classification accuracy  this was
especially true for the company bs catalog  which included
books whose titles were often     words or less in length 
and were absolutely failing in their categorizations  the latter
situation leads up to the next subsection 

    multiple classifications   strong accuracy
upon manual inspection of miss classifications  the classified
found arguably equivalently good classifications  in this case 
the classification was electronics but we classified it as
office products  which also makes sense 
ex  fellowes powershred ms    c safe sense   sheet
confetti cut shredder

we decided to redefine success if the top three categories the
classifier spit out were the correct categorization  for each
sku  we took up to three categories that were greater than
or equal to the range of the probabilities  categories   max min  
accuracies were low when looking at product catalog b
 around       including multiple classifications increased the
accuracy to around      this is not just a fudge statistic  but
actually makes sense in the context of these catalogs  when a
company gets a user interested in an analogous category  we
want to be able to show him a product to buy if it the
analogous category also makes sense 

with these changes  our results immediately improved to an
accuracy of about     

using multiple classifications  if only one category was
returned  this means our classifier is reasonably certain that
this is the correct categorization as the probability this
belongs to other categories is much less  in fact  less than
    of the top categorization according to our heuristic  
we term this the strong accuracy  indeed  if we look at our
strong accuracy for product catalog b for multiple category 
our accuracy is       while our strong accuracy is       

    precision   recall

   features

we found that for product catalog a  there was a large bias
towards classifying towards the large categories  e g 

as we use the human generated text content to define a
product  we decided on using the bag of words model as

however  when we break up errors by category  we find the
less training data we have on this category  the worse the
accuracy  thus  from our previous observation  we decided
to only use categories that have      of the total number of
products  so       products  

fifeatures for each item  however  because the text descriptors
are human generated  we encounter a lot of human
eccentricities and differences in word choice semantics for
similar words  thus  we incorporate a couple of domain
specific ideas to help with classification  the first is per
feature processing  and the second is determining which
features to use 

    feature processing
we apply some standard techniques in text processing to
clean up our feature set  stemming lemmatization 
lowecasing  and stop word  punctuation  and and number
removal 
      stemming lemmatization
we use stemming to narrow our overall feature space so as
to help with the lack of features per item 
ex  case  casing  cases    cas

overall  we see mixed results for stemming  as seen in table
   while stemming shows improvement for both companies
over the baseline without any feature processing  it clearly
negatively affects the overall accuracy when combined with
other feature processing for company a  while it positively
affects overall accuracy for company b 
      stop word removal
we found as in much text  that there were many common 
short function words  often prepositional terms and other
grammatical syntax fillers that were found in many of the
product descriptions  while we have not yet attempted to
build a domain specific  i e  for product descriptions   we
used a standard set of porter stem words 
ex  the  is  at  who  which  on

stop words turned out to be by far the most impressive
feature processing technique we used  in so much as
dominating both catalogs for accuracy boosts  table     in
fact  for company a  it resulted in nearly a     increase in
overall accuracy 
      number removal
the decision to remove numbers was a result of looking at
our data and realizing that many numbers had little to no
relevance to a given category  and that the scatter  and range 
of numbers only would adversely impact a given
categorization  because there are a limited and finite number
of training examples  but an infinite number of numbers  we
believed that we could not fully capture the information of
numbers for a given category 
ex                  

the slight performance increase for company a is offset by
a slight performance decrease for company b  table     we

attribute this to the fact that for certain categories  specific
numbers may show up more  and thus affect the resulting
categorization 
      punctuation removal
removing punctuation was another result of looking at our
data and noticing that humans have a very large variance in
phrasing and word choice  this is especially true for words
that may have multiple accepted forms  or words with
punctuation in them  also  because we parse the item
descriptors on spaces  any punctuations that are in the
phrase are left in  including ellipses  periods  exclamation
points  and others  in addition  words that are concatenated
are often used differently 
ex  dont  dont   d    d  period   period 

punctuation removal was the second most effective feature
normalization method used for company a  while it was a
 close  third best for company b  table    
      lowercasing
sentences in the english language tend to have the first word
capitalized  in addition  different people will capitalize
different words intentionally or otherwise  depending on
their intent interpretation of the word  of choice of
capitalizing acronyms  clearly  this is not always a useful
normalization  and it can go very wrong especially when
dealing with product names vs  generic objects 
ex  president  president  cd  cd  windows  windows

as expected  lowercasing doesnt help very much  table    
most likely because any benefits we receive from lowercasing
occurrences such as knives are great to knives are great
are offset by windows vista to windows vista  which
have very different meanings  perhaps it wouldve been more
useful to just lowercase the word of the item if theres no
chance that its a product name  but that wouldve been
particularly difficult given the infinite amount of product
names out there 
table      showing  feature  to  nb  classification  accuracy  
feature normalization
stemming
stopword removal
number removal
punctuation removal
lowercase
include certain descriptors
none
all

company a
nb accuracy
    
    
    
    
    
    
    
    

company b
nb accuracy
    
    
    
    
    
    
    
    

    choosing features
secondly  we use domain specific knowledge about the
product catalog andwhat is contained in it  in addition to the
title  items have brands and other descriptions such as
specifications that we may use  we felt many of the

fispecifications such as size and weight wouldnt benefit the
overall classification  but we felt that the brand might have a
large impact  and we when we tested it  we saw that the
impact really depends on the dataset used  table    

    calculating the importance of features
in order to    understand what features we can add and   
and weighting the features we have by how important they
are in distinguishing between categorizations  when we used
the conventional mi formula  we found that two of the four
terms in the summation were actually skewing the mi
calculation for company catalog a    in particular  the case
of calculating the term of not this feature and in this class
and not this feature and not in this class  this is due to the
skewness in the distribution of categories  where electronics
categories are  of the catalog  and the modification is not
applicable for company catalog b as the distribution of
categories is more uniform  the modified mi formula we
used for this 
       

      log
               

     
    

m i 
       
       
       
       

least informative words
m  p  
pz   
superspee
stick

m i 
   e   
   e   
   e   
   e   

table      most  and  least  informative  words  in   health     personal  care   
for  company  a s  catalog  
most informative words
black
pack
set
cas

m i 
      
      
      
      

we surmise that this weird behavior is due to the mutual
information not properly describing or capturing the least
informative features  due to the skewness of the distribution
of categories in company a product catalog   

   different classifiers
we decided to continue working with our version of naive
bayes after trying a few different classifiers  we used the
orange  http   orange biolab si   machine learning library
for python to try out a few different machine learning
algorithms  we tried this on company as catalog and the
table below has the accuracy and time taken for each
algorithm
table      different  classifiers  and  accuracies  

table      most  and  least  informative  words  in   health     personal  care   
for  company  a s  catalog  
most informative words
tim
beard
wahl
hair

accuracy features  when we do use the least informative
features  the accuracy halves 
  for the company b product catalog  when we do not
include the least informative features  the accuracy
drops  however  when we do include the least
informative features  there is a slightly insignificant jump
of      accuracy 

least informative words
rapid
wet
orthot
champ

m i 
      
      
      
      

    feature selection
as we are using a modified set of the words to describe
products as our features  not all these features will be useful
for classification or differentiation between categories  we
considered forward or backward search to select our
features  but as our features run in the order of thousands 
we decided to try filter feature selection instead  to do this 
we first looked at the distribution of mi in our two catalogs 
in company catalog a  the spread between the most and
least informative features was large  we had some interesting
findings  both with using max of   possible categories  
  using the most informative features increased the
accuracy in both company catalogs 
  for the company a product catalog  not using the least
informative features along with the most informative
helped  we found a slightly significant jump of   

algorithm
nave bayes
k nearest neighbors  k     
tree classifier

company a catalog
dataset accuracy
     
     
     

time taken
   seconds
   minutes
   hours

here in table    we can see that the tree classifier
performed the best  but took a few hours to complete  in a
real commercial situation  we see an algorithm like the tree
classifier  or a more sophisticated algorithm as random
forests  as a back end deamon  in addition  this tree gives a
potential to more deeply understand a product catalog  more
so than a fixed set of categories   however  naive bayes
achieved almost the same accuracy as the tree classifier in
significantly less time  we thus stuck with our naive bayes
classifier to develop features for  see section      and select
features from  see section       we tried the knn  k nearest
algorithm  for different values of k  but they all performed
poorly  with k   to   performing the best 

   conclusions
we found two main difficulties working with two datasets 
in company as product catalog  the distribution of
categories was severely skewed towards one category
 electronics  and we tended to always classify products into
the larger categories  as such  the larger the category the
better the accuracy  in company bs product catalog  we
found many of the products had very short descriptions and
there many fewer words than in company as catalog 
along with implementing regular text processing techniques
 stemming  etc   we found that using mulltiple classifications 
the notion of strong accuracies  and filtering products that
had greater than a certain number of words    helped the

fiaccuracy significantly and made sense in the context of
categorizing products  in particular  we believe that using
multiple classifications is one of the main ways this can be
extended to incorporating user data from these company
websites and offering collaborative filtering capabilities 

    witschel  hans f   and fabian schmidt   information
structuring and product classification   tstt      beijing 
web       

we plan to continue this project in the context of a startup
and we would like to work towards a collaborative filtering
platform for small and medium sized businesses by first
making our classification and categorization of products
better for the current catalogs and robust for different types
of product catalogs  we would also like to implement
unsupervised learning to obtain sub categorization of
existing categories 

     ecl ss   the leading classification system   content
development platform  paradine     june       web       
 http   www eclass cdp com  

    product dependent features
so far  we have use the bag of words model and have not
incorporated features that are domain specific 
 

 

bi words  many brand names and descriptions have
words that occur together  e g   digital camera  we
would like to capture these 
upweight the title and pick the best percentage of
upweighting through a model selection method 
use our mutual information features on how to
weight the features  the ones that differentiate
between classes are more important   we already
have this  but this will be important when we have
different types of features 
add spelling correction as we noticed there are
many misspelled words in the catalog 

    explore more sophisticated methods
we chose naive bayes as it is the simplest method we could
implement  when we tried a few other algorithms using
orange  the tree classifier looked promising  however  since
it took around   hours on the entire set of data  we did not
pursue working with this algorithm for this project  we
would like to pursue variants of the tree classifier  such as
random forest  by exploring a subset of the data 

    unsupervised
supervised classification with product catalogs is limited to
the categories that we train on  to find more granular subcategories  we need to be able to discover structure within
categories of products  we are considering using an
unsupervised hierarchical clustering approach  in addition 
this would allow us to categorize and give structure to
catalogs where there is no existing product catalog  for
example  products for a new or sparsely explored market  

   references

    unspsc homepage         http   www unspsc org   

fi