finding answers to non factoid questions using a recursive
neural network
deepak merugu  reed nightingale  imran thobani

abstract
retrieving answers to complex questions is an unsolved problem in computing  surdeanu et al  have shown that by
identifying key features in the sentences in both a question and set of candidate answers  a combination of
discriminative learning and class conditional generative algorithms can correctly identify the best answer  as
determined by a human  with a baseline precision of      and mean reciprocal rank  mrr  of           we have
produced a prototype  alternative model by using an adaptation of the recursive autoencoder  rae  model presented
by socher et al     while our model does not currently achieve a testing precision or mrr better than surdeanus
model  with some changes to the approach to training the model  it may still be possible to match or exceed
surdeanus results 
introduction
in recent years there has been a significant growth in user generated content on the web  specifically  there is a huge
collection of questions and answers  written and rated by users  on social question answering websites such as
yahoo  answers and quora  this has led to interest in building a question answering system using machine learning
techniques  ibm watson and apples siri are examples of such systems that have received a lot of attention 
recent work by surdeanu et  al     shows that certain linguistically motivated features improve the
performance of the ranking module of a complete question answering system  in a related paper  richard et  al     use
recursive auto encoders  raes  for the task of paraphrase detection  building on this work  we propose to use a
similar model with different pooling techniques to learn similarities between questions and answers and thus  rank
answers from a given pool  at a high level  our model represents each sentence  from a question or an answer  as a
tree of words  leaf nodes  and phrases  parent nodes   words and phrases are represented as n dimensional vectors in
a vector space  and phrases are generated by recursively applying a neural network to child nodes  we use a
similarity matrix combined with a pooling technique to compute a fixed number of similarity features between the
sentences  which are then used to classify a question answer sentence pair as correct or incorrect 
approach
the rae model presented by socher et al was originally designed to detect paraphrased sentences  because a good
answer to a question will often repeat key words and phrases from the question  we suspect that the same general
model may be trained to match questions and answers 
the model has three main units  at the lowest level  a rae parses and processes sentences into trees
 representing the sentences syntactic structures   this piece of the model uses vector representations of words and
phrases  and is designed to train semantically and syntactically similar words and phrases to have similar vector
representations 
interfacing with the rae is a sentence comparing and pooling layer  in our tested implementation  this layer
compares two sentences by calculating the squared euclidean distance between each word  phrase  or word phrase
pair of vectors to generate a similarity matrix 
because the size of the similarity matrix varies based on the length of the sentences provided to it  there is a
pooling layer on top of the similarity matrix that averages roughly equal sections of the similarity matrix to produce a
fixed size pooled similarity matrix 
page  

fiat the top level  a softmax classifier uses the pooled similarity matrix to determine the models classification
of the two sentences as a good or bad question answer sentence pair  to do this  the softmax classifier is trained on a
data set that is labeled as   for sentence pairs in a good answer for a question and   for sentence pairs in a bad answer
for a question 
any discrepancy is error that is back propagated from the softmax
classifier to the pooled similarity matrix  down to the source similarity
matrix  and finally to the sentence trees and word and phrase vectors 
stochastic gradient decent  sgd  is then used to minimize the cost
function and cluster the word vectors  thus the two ends of the system
can simultaneously be trained in a single pipeline 
to test our model  we used a terrier ir bm   algorithm    to
generate a list of    candidate answers for each question selected for
relevance  for each trained model  we tested all of the questions the
model was trained on  and also tested the model against a fixed set of    
questions used exclusively for testing  given a question  we used the
learned word vectors and softmax classifier weights to determine a
classification probability for each answer  which was translated into ranks with the highest softmax classification
answer receiving a rank of    the lowest     and any ties broken by randomly ranking the answers within the
appropriate rank range  from the rankings  it is straightforward to calculate the precision and mrr for the model 
our code was developed in java on top of a suite of code originally written by richard socher  and
translated to java by jean feng  after determining the requirements for our model  we had to modify several
portions of the original framework in order to allow our code to interface with the existing framework 
training and testing results and analysis
we designed unit testers to check the correctness of each individual component in the system  after each component
was successfully tested  we did a numerical computation of the gradient and verified that the back propagation phase
indeed returns the true gradient  in using sgd  we also checked that the cost was decreasing with every update of the
parameters for different learning rates 
we now present some results and observations for different test
cases  figure   shows precision and mrr for a training data set of  
question answer lists  about    question answer pairs   we can
observe that the model fits the training sets almost perfectly after
about     iterations  the initial weight matrices have been randomly
initialized differently for the case where we did not use any
regularization of the parameters and the case where we did  this
shows that the model does indeed learn to rank correctly  at least for
a tiny data set 
figure   shows the learning curve on a training set of size   
questions  about    question answer pairs  with and without the
regularization of the parameters  in both cases  the parameters have
been initialized to the same values  it is interesting to observe very
similar performance in both cases   showing that the regularization
is in fact not hurting the model performance 
figure   shows the learning curve on a training set of size    
questions  about     question answer pairs  for about     iterations 
the learning curve does not show any reasonable improvement in the performance 
figure   and  

page  

fifigure   shows the performance on a training set of size     
       question answer pairs  questions comparing it to the case
where we only test on the original answers for a question  which is
variable 
figure   shows the performance with the size of the training set 
while we see a slight improvement in the performance as the
training size increases to                question answer pairs 
questions  there is also a dip for a training size of               
question answer pairs  questions 
figure   shows a sample set of pooled similarity matrices between
several sentence pairs  darker blues indicate closer vectors  while
dark red indicates large differences  medium values are shades of
yellow and green  fig  a b c compare the sentence does china
view north korea as a potential threat to its security  to one
random sentence  a  and two sentences identified by the ir  b c  
we see that there is a greater amount of bluer regions in b and c
than in a  fig  d e show the similarity matrices for two best
answers and their respective questions  we note that d appears
rather dissimilar  and e has regions of extreme similarity but also
several dark red regions 

a

b

c
figure       and  

d
e
fig    a e   sample similarity matrices for different question answer pairs 

we also tried training the model over      question answer pairs using collobert and westons word vectors as
initial values for the model  but found no improvement in over    batch iterations  we suspect that this could still be
an effect improvement to the model with a superior optimization method such as l bfgs because the pre trained
word vectors we used were not pre trained sufficiently based on nearest neighbors analysis  we tried to optimize our
model using l bfgs  but the optimizer we used did not appear to update the weights of the word vectors  a future
goal would be to include a superior optimizer 
conclusion and future work
there are several variables left to be tested  but three in particular
may have significant impact on the models performance  the first
of these is the training algorithm  our results all use sgd  which
did not demonstrate an appreciable learning curve for data sets
larger than five questions  however  it is possible that an optimizer
such as l bfgs would have led to better results  while we have
implemented and unit tested a series of pooling styles  the only one
used to obtain these results so far is the average pooling  it may be
page  

figure  

fithat one of the other three types  minimum  minimum gaussian weighted  and exponentially weighted  produces
consistently better rankings and or precision  finally  due to the relatively low number of training iterations 
questions trained over  and word vector size  it is difficult to determine how our model will compare to surdeanus
when given more ample iterations to learn word features 
to improve the performance of the model  we might consider adding additional features  for example  tfidf 
number of sentences in answers to question  as seen in figure    or named entity recognition 

acknowledgements
we conducted this research within the deep search natural language processing group at stanford university 
under the supervision of richard socher  in collaboration with rahul pandey and arun prasad  using a framework of
code developed by jean feng 
references
   
surdeanu  mihai  massimiliano ciaramita  hugo zaragoza  learning to rank answers to non factoid questions
from web collections  computational linguistics  vol      no     june      
   
socher  richard  eric h  huang  jeffrey pennington  andrew y  ng  christopher d  manning  dynamic pooling
and unfolding recursive autoencoders for paraphrase detection  advances in neural information processing
systems          
   
terrier v                 http   terrier org 

page  

fi