predicting outcomes of nfl games
albert shau
dec         

the purpose of my project is to learn how to predict the outcome of nfl
 national football league  games  being able to do so accurately would be
of interest to many nfl followers and could have implications in gambling if
i am able to a good amount better than the average nfl fan  it could also
be used in conjunction with a study on penalties called per game as a way
to validate or invalidate claims of referees fixing games  if it is possible
to show a pattern of favored teams losing due to an abnormal amount of
penalties  that might be data in support of referees deliberately guiding the
outcome of a game  to build the classifier  i took game data from     
to now  generated features from that data  and then ran naive bayes and
svms over the data to predict the outcome of games  the evaluation metric
used was  which is the number of games predicted correctly out of the total
number of games  success was judged by comparing classifier accuracy to
human accuracy  some measure of human accuracy is available on popular
sports sites  for example  according to http   sports yahoo com nfl picks 
so far in       yahoo  sports users predicted correctly        of the time 
whereas the accuracy of writers range from        to         my goal was
to beat them all by a good margin and hit     accuracy 
game data from      to      was taken from http   www pro footballreference com by scraping various pages available on the site  for each week 
the boxscores of the game contained information such as the winner  loser 
hometeam  score  passing statistics  rushing statistics  turnovers  penalties 
and sacks  the site also contains information about pro bowlers  outstanding players  throught the years  from this data  for each team and year  i
generated several statistics about each teams performace so far in the season  for example  a teams average points scored per game and average
 

fipoints allowed per game  using those statistics and using the outcomes of
the games  i ended up with a training set of    features  these features were
just statistics that i would use when considering what team would win in a
matchups  they included things like the difference in average points scored 
difference in win   between the teams  statistics comparing passing offense
of team a versus passing defense of team b and vice versa  rushing offense of
team a versus rushing defense of team b  tendency of each team to commit
penalties  difference in turnover ratios  whether a team is playing at home 
comparing first downs gained and given up  etc  for brevity i have not listed
all the features  but they all followed the same vein  comparing the two teams
in areas that most people think are impactful in winning or losing football
games 
the first approach was to use a naive bayes classifier  where all the features
were modeled as multinomial distributions  and were discretized to take on
values from   to        was chosen arbitrarily and no cross validation was
done  since each game must have a winner and a loser  the priors for winning
and losing are the same and can be ignored  the conditional probabilities
 k  y    
can be calculated simply and efficiently as p xi   k y          xi  y
   
and the analagous calulation for p xi   k y       laplace smoothing was
also used  so   was added to the numerator and the cardinality of each feature
    for most of them in this case  added to the denominator  at the time of
the first approach  i only had   features  using this approach     accuracy
was achieved  this accuracy is about as good as the worst yahoo sports
writer  not good  but definitely not terrible for such a simple approach  the
most recent     of the data was used for testing and the rest for training 
the next approach was to use an svm to classify  the same   features
were used  except none of them were discretized for this  and the labels were
changed to    and     the liblinear package was used to train and test the
data  the table below shows accuracy as a function of the cost parameter 
l  regularized l  loss support vector classification was used with a linear
kernel 
c
  
 
   
    
     
              
accuracy                                                 
so this is slightly better than the naive bayes approach with the right cost
 

fivalue  but still not better than humans  at this point  i decided to scrape
more data and generate more than   features  the reasoning was that i had
orders of magnitudes more training examples than features  therefore it was
unlikely getting data from before      would help much  and that is not even
considering the fact that the game was fairly different back then than now 
also  both algorithms performed similarly poorly  so the thought was that
there was something fundamentally wrong about the modeling process  what
was wrong was that i was not capturing all the different factors that are
involved in who wins a football game  i also briefly considered doing feature
selection  but that did not seem like it would be useful on a set of   features 
all of which were standard metrics people look at when comparing teams 
also considered using different kernels for the svm  and also normalizing
the features to all be on the same scale  as suggested by a paper written by
the libsvm folks   but that seemed like it could only squeeze out tiny amounts
of lift  whereas i needed a huge improvement  so after this point i went and
got more data in order to generate the final set of    features  some of which
were mentioned earlier in this report  since the svm performed better than
naive bayes  i only used svms after this point 
at this point  i also decided that in order for the comparison to be fair  i
should be using only the      season as the test set because that is what i am
comparing my classifier to  i would have used user and writer accuracies from
previous years but could not find that data  the test set is much smaller 
but it is still     examples so it is not too small  however  overfitting would
still be a concern  so i would train the model using data from      to     
with k fold cross validation  but will only be reporting accuracy on the     
test set for comparison purposes  these numbers  however  are higher than
using multiple years as the test data  however  it is possible      is just an
easy year to predict and that writers and average fans are having an easier
time predicting as well 
with the new features  svm performance increased to        this matches
yahoo users  so it is a decent classifier  i also tried normalizing the data so
it was all on the same scale  and using a radial basis kernal exp  u  v    
as suggested by the libsvm folks  i tried a grid of values for c and gamma 
exponentially scaling c and gamma up and down independently  but none of
those outperformed the linear kernel  at this point  i did a little study on
training data size to see if adding more data would help  the results are
 

fishown in the figure below 

the x axis depicts the earliest year used in the training data  last year used
was      in all cases   the y axis depicts accuracy  from this graph  you can
see that accuracy levels off before       so adding more data would probably
not be useful  there are not many more years to add anyway  one interesting
thing to note is that the performance is best when using my full dataset  it
is a pretty popular sentiment that the game is very different today than it
was decades ago  the basic thought is that it is much easier to score today
and passing and offense is much more important than it used to be  when
running and defense would dominate  based on that reasoning  one might
expect performance to improve if the earlier years are left out  however 
as evident in the graph  that is not the case  one could therefore argue
that what determined the winner of a game    years ago still determines
the winner of a game today  you could argue that the game has not really
changed all that much 
one final thing i looked at was dividing the season up into chunks  the idea
behind this is that early in the season  there is not much data on each team 
so the statistics early in the season are less meaningful  so once were at
week     maybe its better to ignore what happened in weeks      the figure
below shows the results 
 

fithe x axis shows the earliest week of data used  weeks before that were
filtered out from training and test sets  and test sets consisted of   years of
games minus the filtered out games  as you can see from the graph  there
is a sizable jump in accuracy when earlier weeks are left out  using this
observation  we can create multiple models and use them in an intelligent
way depending on the week we are trying to predict  this would boost
overall accuracy 
i was not able to reach the original goal of     accuracy  but was able to
create a classifier on par with regular sports fans  however  this is not useful
at all  because a much simpler classifier than mine would simply go to the
webpage and make a decision based on who the users and writers think will
win  i would spend future work on getting even more features and seeing
if performing feature selection would help  there are definitely important
features that i have not included  such as injury information  that is vital
in predicting the outcome of games  other ideas for features include using
the aggregate win percentage of all the teams a team has beaten in the past
to account for some teams having an easier schedule than others  there are
many many more features that could be used  something i learned was that
building a good  clean  dataset can be the bulk of the work in these sorts of
machine learning problems 
 

fi