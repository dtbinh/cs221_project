predicting star ratings of movie review comments
aju thalappillil scaria  ajuts 
rose marie philip  rosep 
sagar v mehta  svmehta 
   introduction
the growth of the world wide web has resulted in troves of reviews for products we wish to purchase  destinations
we may want to travel to  and decisions we make on a day to day basis  using machine learning techniques to infer the
polarity of text based comments is of great importance in this age of information  while there have been many successful
attempts at binary sentiment analysis of text based content  fewer attempts have been made to classify texts into more
granular sentiment classes  the aim of this project is to predict the star rating of a users comment about a movie on a
scale of          to     though a user self classifies a comment on a scale of   to    in our dataset from imdb  this
research can potentially be applied to settings where a comment about a movie is available  but no corresponding star
rating is  i e  the sentiment expressed in tweets about a movie   we also explore if analyzing movie metadata  like
director  actors  budget  production house etc   and using it to augment the model built only on review text would
improve the performance of the algorithm 

   data collection
a data set containing text based movie review comments from imdb and the star rating corresponding to each
comment was obtained from http   ai stanford edu  amaas data sentiment  this dataset contained reviews which had
star ratings between     and      stars as it was used for binary sentiment classification  we extended the dataset with
review comments having   and   star ratings from imdb  the number of training samples per class that were added was
in proportion to the existing dataset   to augment the star prediction algorithm  the following movie metadata was also
collected  director name  actor names  revenue  budget  production company  number of votes on imdb  genre  and
opening week revenue 

   text classification from user comments
the dataset we obtained had a default dictionary containing all the words that appeared in the training samples  this
dictionary had around       words  since the performance of classification algorithms on such a huge dictionary was
relatively poor  we evaluated the following strategies to reduce the dictionary size 
 eliminating stop words  common words  like a  an  and  are  as etc   appear with a high frequency in comments 
but add very little value to the classification problem as they do not have any specific polarity 
 identifying negation of context  the usage of terms likes not good  not interesting  wasnt interesting etc  have
a negative connotation  revised dictionary combined these negation bigrams into a single feature 
 stemming  stemming condenses the dictionary by identifying root words 
 eliminating features with low information  the ratio of number of occurrences of each word in positive comments
             star ratings  and negative comments             ratings  were counted and those words with
    or
    were selected 
    models evaluated
i  naive bayes  naive bayes with multinomial event model was trained with a default dictionary and it served as a
baseline for the project  we evaluated several options to enhance the algorithm as described in the feature
selection section 
ii  multiclass svm  apart from naive bayes  we used svm in order to determine the ratings for a given movie
comment  svm is traditionally a binary classifier  however there exists methods to adapt it to the multiclass setting 
the two methods used in this study were one vs  all and one vs  one using a linear kernel and the same bag of
words frequencies from naive bayes as our features  all svm models were generated using the liblinear package 
suppose we have classes
  the one vs  all method trains models  each model trains a binary
classifier considering all points in a class
as positive examples and all points in the remaining
classes
as negative examples  during testing  each sample is tested against each of these models  and the class
which creates the greatest separation from the decision boundary for that data point is chosen as its class 

fian alternative method is one vs  one  here
models are trained and during testing  each sample is
tested against each other  concretely  for the case of imdb ratings on a scale of      we train    svms  one for each
pair of ratings  each time a test point is placed into a class   it is considered a vote for that class  the training
sample is then classified as belonging to the class with the most votes  or in the case of ties  the mean of the star
ratings of the classes rounded to the nearest class  
    features evaluated
nave bayes and svm were evaluated with different dictionaries to identify the best feature set  features being words  
 default  the default dictionary that came with the dataset 
 unigram    default dictionary with cleaned up words from default dictionary 
 stem    dictionary with stemmed words 
 stem    dictionary with stemmed words by treating degree differentiators like er and est separate from root
words 
 stopword    dictionary used in unigram    with stop words removed 
 bigram       dictionaries used in unigram     stem   augmented with top     bigrams  respectively 
 negation    dictionary used in unigram   augmented with negated words 
 feat elim    dictionary in unigram   after removing words having low information 
    feature and model selection
feat elim   gave the best results on training and cross validating using nave bayes classifier      of the training
data was used for cross validation  the metrics obtained are as follows  accuracy with   tolerance is the fraction of
cross validation samples whose star ratings were predicted exactly correct  mean deviation is the average absolute value
deviation of predicted star rating from the actual star rating of comments  
 
   

      

 

      

      

      

      

      

      

      
      

   
 
   

                          
                    
      
      
                           
                           
            
                           
                          

accuracy with
  tolerance
accuracy with
tolerance  
mean
deviation
standard
deviation

 

the best cross validation metrics obtained after training svm with the different feature sets is as follows 
method

accuracy with   tolerance

accuracy with tolerance  

average deviation

standard deviation

one vs  all

   

   

    

    

one vs  one

   

   

   

   

as evident from the table above  both the multiclass methods provided worse results than the original nave bayes
approach  in an attempt to improve the result  we tried various parameters for the l  normalization constant and also

fitried to modify the feature set  i e  different settings for stop words  stemming  etc    however this did not prove to be
useful 
    analysis of cross validation results
i  nave bayes  word stemming reduced performance  this is because different tenses usages of word have different
semantics attached  for example  the word recommend can be used in both positive and negative ways like in i do
not recommend the movie and i recommend the movie  but  the word recommended can be used only in
positive context like i recommended the movie and not like i did not recommended the movie  removing stop
words improved performance by around      identifying negation of context did not improve performance  this
may be because since there are   different classes each within positive and negative comments  the negation of
context couldnt accurately capture the difference in usages of words like not in comments that have similar ratings
 either positive or negative   eliminating words that have very less information resulted in improvement in
performance by around    
ii  multiclass svm  an interesting observation is that the one vs  one with voting performed considerably worse than
the one vs  all method  this is possibly due to the fact that the voting model used was too simplistic  in particular  in
simply taking the frequency of a classification we ignore the actual separation between the test data point and the
decision boundary  a better approach would have been to have a probabilistic interpretation for the actual margin
created between the test point and the decision boundary for each of the svms  this would allow for a better
estimate of the posterior probability that the point belongs to that class  in contrast the one vs  all method does
pick the point which has the greatest margin  which could explain why it performed significantly better 
    test result
nave bayes gave better results as compared to svm  hence it was chosen to predict star rating of a user
comment on the test data  the performance is as below 
accuracy with   tolerance
      

accuracy with tolerance  
      

mean deviation
      

standard deviation
      

   movie star prediction from metadata
this section attempts to recognize how movie metadata affects individual user comments  our intuition was that
even though the text review would reflect a persons opinion of the movie  his rating might be influenced by factors like
the general opinion of the public  we try to incorporate this into our prediction by including the average movie rating
 similar to the one predicted by imbd   which is a general indicator of public opinion towards a movie  since the average
rating might not be available for all movies  we try to predict this based on the metadata available  like gross revenue 
number of votes on imdb  director  genre  production company  opening weekend revenue  budget  and main actor 
    models evaluated
k nearest neighbor algorithm was used to predict overall rating of movies with k    and using hamming distance
to estimate the distance between two data points  hamming distance is the percentage of features that are different 
these handle nominal features and the continuous features like revenue were discretized by grouping them based on
their frequency distribution  from the comments that we collected  we used only those movies      movies  for which
we could obtain most of the features from imdb  we used     samples for training  and used five fold cross validation
for feature selection  taking     movies out at a time 
    features evaluated
the movie features evaluated were budget  genre  opening week revenue  main actor  director  gross revenue 
production company  and number of votes 

fi    feature selection
forward search was used for selecting five best features which came out to be number of votes  production
company  gross revenue  director  and main actor  since the prediction of movie star rating based on metadata with  
tolerance gave poor results  accuracy less than       features were evaluated with a tolerance of    the results for
cross validation are shown below 

   
   
   
   
   
   
   
   
   
 

votes
production company
gross revenue
director
main actor
budget
round   feature
selected 
votes

round   round   feature
feature
selected 
selected 
production gross revenue
company

round   feature
selected 
director

round   feature
selected 
main actor

genre
opening week revenue

after selecting the best   features  cross validation gave     accuracy with a mean deviation of      
    analysis of cross validation results
we believe the high error observed for   tolerance was mainly due to other features that affect a movies goodness
value like its plot which cannot be quantized and hence not captured in our feature list  during cross validation  genre
did not give good results probably because we included only one genre for each movie  but a movie can be classified
under more than one genre at the same time  like humor and drama   budget and opening weekend revenue details
were not available for all movies  we assigned the mean value of all other movies to these features  which might have
resulted in them being rejected  while selecting features  actor was one of the last features to be selected  we had
selected only the first actor in the credits which might not be the best strategy  future work should include obtaining a
more exhaustive data set for budget  revenue and actors 
    test result
accuracy on test data was measured at two tolerances  allowing a deviation of   from actual value obtained an
accuracy of        and a deviation   from actual value gave an accuracy of         in the first case  the mean
deviation was        and standard deviation      

   combining nave bayes and movie metadata
as a final step  we attempt to combine the prediction from the naive bayes  which provides the rating for an
individual comment  and the metadata  which provides the overall rating for the movie  to provide a new prediction for
an individual comment  the motivation for doing so is that naive bayes assumes prior overall rating classes  which we
attain from the frequencies of classes in the training data  however  it is also true that not all movies are created equal 
movies which are considered good overall will  in theory  tend to have better individual comments as well  in
mathematical terms  the decision to combine metadata results with naive bayes was 
the data was combined linearly in the following fashion 

that is  the new prediction for a given comment is given by the naive bayes prediction times some factor plus the
overall movie prediction times
  the coefficients were chosen as and
so that the new value will be bound

fiby the earlier two predictions and would tend towards the better predictor  the parameter was chosen to minimize the
sum of the absolute differences between the true value and the predicted value using a simple grid search with     step
size  using this model  we shade our prediction upwards if the metadata suggests that a movie is better than as suggested
by the nave bayes predictor  and vice versa for a bad movie  the result of testing the new model is as follows 
model

accuracy with   tolerance

average deviation

nave bayes

      

      

nave bayes   metadata

      

      

incorporating metadata into the naive bayes predictor actually resulted in roughly similar to slightly worse results
overall  the optimal value for determined from the training set was     indicating that a high weight was placed on the
naive bayes model and low weight on metadata  this might be due to the fact that most of the movies had similar overall
ratings  the information gleaned from the metadata did not provide better insight into an individual reviewers comment
as we thought it might 

   conclusion
nave bayes with a feature set obtained after removing stop words and features with low information content gave
the best performance  it predicted the star rating with        accuracy from amongst    different classes on the test
data  the mean deviation of        stars indicates that the algorithm on the average does a good job of identifying the
polarity of the user comment 
we presume that some of the reasons for not getting higher accuracy while predicting individual rating could be the
following  the star rating assigned by a reviewer on imdb can be affected by other factors like general impression of that
movie among the public  it is possible a person liked the movie for its uniqueness  or plotline  or prejudice of the reviewer
towards an actor or a genre  these would vary from person to person and hence would not show up as a pattern during
text classification  it is also possible that a person seeing an extreme rating for a movie does not agree with it and tries to
contribute towards altering this by giving an individual rating in the opposite extreme even though he found the movie to
be of average value  for example  if a movie was rated   and a user thinks it should be a    he might give a   or   just so
that it brings down the overall rating 
we also tried to find a direct relation between individual rating and general perception of a movie as indicated by its
average star rating  it was observed that the linear combination of parameters that we evaluated did not establish any
relationship between the two  however  it is possible that they are related in some higher degrees  maybe  second or
third degree polynomial functions of the parameters  further work could be done in this area  probably using svm 

   references
    andrew l  maas  raymond e  daly  peter t  pham  dan huang  andrew y  ng  and christopher potts  learning word
vectors for sentiment analysis  the   th annual meeting of the association for computational linguistics  acl
     
    mahesh joshi  dipanjan das  kevin gimpel  and noah a  smith  movie reviews and revenues  an experiment in
text regression  proc of the north american chapter of the associatoin for computational linguistics human
language technologies conference         
st
    tsutsumi k  shimada k  nedo t  movie review classification based on a multiple classifer  the    pacific asia
conference on language  information  and computation  paclic        

   credits
the data set used in the project was obtained from andrew maas  we extend our sincere gratitude for his support
and guidance during the course of the project  we would also like to thank dr  andrew ng  for providing us with the
opportunity to work on this project  the material covered in the lecture and the notes gave us very good insight about
machine learning concepts that helped us to come up with algorithms to complete this project 

fi