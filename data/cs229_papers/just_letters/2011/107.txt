  
  

  
  

seyed  reza  mir  ghaderi      
nima  soltani  

forecasting   stock   market   behavior   based  
on  public  sentiment  
project  overview  
it   is   believed   that   public   sentiment   is   correlated   with   the   behavior   of   the   stock   market          the  
general   objective   of   the   project   is   to   characterize   this   correlation   and   use   it   to   predict   the   future  
behavior  of  the  market   we  assume  that  people  express  their  mood  in  their  twitter  posts   tweets   
and  approach  the  problem  by  performing  large scale  analysis  on  these  tweets   the  ultimate  goal  of  
this  project  is  to  predict  how  the  market  will  behave  tomorrow  given  a  large  set  of  tweets  over  the  
past  few  days       
previous   work   by   bollen   et   al          uses   sentiment   analysis   tools   in   conjunction   with   a   non linear  
model    self organizing   fuzzy   neural   network    to   predict   the   changes   in   dow   jones   industrial  
average    djia     here   we   design   and   build   the   learning   and   prediction   system   making   only   basic  
assumptions   about   the   relationship   between   tweets   and   the   market    we   do   not   assume   that   the  
moods  of  the  tweets  are  their  only  important  feature   and  instead  look  for  informative  features  that  
might   not   be   specifically   mood related    the   reason   for   doing   so   is   to   give   enough   freedom   to   the  
algorithm  itself  to  determine  which  words  are  most  pertinent  to  the  stock  values     

data  
when   starting   this   project    we   had   data   for       months   in           this   included   data   obtained   from  
twitter  over  the  course  of  the     months  as  well  as  daily  stock  values     

tweets  
we   had   training   data   in   the   form   of      gb   of   twitter   posts         over   the   timespan   of   june   to  
december  of         given  the  large  volume  of  data   we  needed  to  format  the  data  into  a  consistent  
format   we  first  tried  using  stanfords  nlp  java  application  to  parse  the  documents  and  get  the  data  
in   a   readable   format    the   advantages   of   this   approach   was   that   it   was   already   written   and  
debugged   as  well  as  containing  features  like  grouping  words  together  by  their  root  word   the  main  
disadvantage  of  this  approach   however   was  that  it  took  much  more  time  and  space  to  run  than  we  
could  allow  in  our  large scale  problem     
thus   we   decided   to   write   our   own   parser tokenizer    by   use   of   regular   expressions   in   python    we  
filtered   out   tweets   with   non english   letters    tokenized   urls    numbers    twitter   usernames    and  
emoticons   converted  everything  to  lowercase  and  removed  all  punctuation   we  then  ran  a  python  
stemming   tool    stemming          to   remove   the   suffixes   and   attempt   to   find   the   root   word   of   the  
words  in  the  tweets   also   since  each  tweet  has  a       character  maximum   we  decided  to  make  each  
tweet      words  long   where  we  truncated  the  tweet  if  it  contained  more  than      words  and  padded  
the  tweet  with  null  words  if  it  contained  less   we  ran  this  script  on  all  our  data   training  and  test  
data   so  we  can  read  words  with  consistent  formatting   

fi  
  

  
  

seyed  reza  mir  ghaderi      
nima  soltani  

stocks  
for   all   the   learning   algorithms   except   linear   regression    we   used   daily   open close   values   of   dow  
jones  industrial  average   djia   from  june           dec             for  the  linear  regression  we  used  
djia   hourly   values   for   the   same   period    obtained   from   price data     we   tried   different   labeling  
definitions  to  form  the  classification  problems   in  particular  for  one  bit  representation  of  the  state  
of  the  stock  market  following  day     we  tried    
                 
                 
                     
similar  approach  was  tried  for  the  growth  computation  in  match score  algorithm     

feature  selection  
one  of  the  main  differences  between  our  work  and  the  original  work       is  that  we  try  to  generalize  
the  feature  set   the  original  feature  set  used     features  from  two  sentiment  analysis  tools   a  general  
positive negative  classification   and  mood  states  in     dimensions  of  calm   alert   sure   vital   kind  and  
happy    also    they   took   out   tweets   that   did   not   explicitly   express   the   authors   mood    our   goal   in  this  
project    however    was   to   remove   these   constraints   and   allow   the   algorithm   to   select   the   most  
informative  content  in  the  day     

clustering  
to   select   the   features    we   found   the   statistics   of   the   words   appearing   every   hour    despite   having  
tens  of  gigabytes  worth  of  tweets   we  realistically  had  only       training  data  points   so  we  could  not  
confidently  train  a  hypothesis  with  more  than        input  features   in  order  to  make  good  use  of  the  
data    we   then   used   a   k means   clustering   algorithm   to   find   words   that   varied   consistently   with   each  
other  so  as  to  maximize  the  number  of  useful  words  to  use  as  features     
for   the   clustering    we   tried   clustering   based   two   different   sets   of   features          the   ratio   of   the  
hourly  word  count  to  the  total  hourly  word  count   or       the  ratio  of  the  hourly  word  count  to  the  
total   count   of   the   word   across   all   time    running   the   clustering   algorithm   on   these   two   sets  
produced   fundamentally   different   features    the   clusters   using         lumped   words   of   nearly   equal  
frequency  together   this  did  a  good  job  of  clustering  words  that  are  extremely  common  and  have  
seemingly   no   correlation   with   the   market    such   as   the    of    and    etc    the   clusters   using        
lumped  words  that  fluctuated  similarly  together   regardless  of  the  number  of  times  they  appeared  
each   hour    the   common   words   were   still   mainly   lumped   together    but   there   were   some   mixed   in  
other  sets   it  was  also  interesting  to  see  that  in  both  cases   the  algorithm  assigned  spanish  words  to  
their   own   cluster    after   coming   up   with         clusters    for   each   cluster      we   then   calculated   the  
mutual   information   as                               where    is   the   number   of   words   that   day    we  
then  sorted  them  according  to  mutual  information  and  took  the     most  informative  clusters     

fi  

  
  

  

seyed  reza  mir  ghaderi      
nima  soltani  

surprisingly    this   method   of   calculation   only   made   a        change   in   the   performance   of   our  
algorithm    in   retrospect    the   clustering   strategy   should   have   somehow   considered   the   mutual  
information  more  explicitly   also   one  problem  with  this  method  was  that  it  was  almost  chaotic   any  
small   perturbation   in   the   algorithm   parameters   changed   the   resulting   clusters    clustering   based   on  
a  more  robust  feature  of  the  word  also  could  have  helped   

market  behavior  learning inference  models  
the   inputs   to   the   algorithms   were   counters   of   clusters   with   daily   and   hourly   resolutions  
               and   the   stock   values indicators    in   all   the   models   we   used   the   tweets   of   one   day   to  
predict  the  stock  behavior  on  the  next  day   in  all  the  algorithms   we  used       of  data   tweets  and  
djia  up down  indicator   for  training  and  the  rest  for  testing    we  tried     different  algorithms   nave  
bayes   svm   linear  regression  and  our  own  heuristic  algorithm  score match   

nave  bayes  learning  
given   its   fast   running   time    nave   bayes   was   our   best   choice   as   the   baseline   algorithm    we  
predicted  the  probability  of  each  cluster  given  each  label  and  the  label  prior  as  
  
numtrainingdays

p cluster i   y   b   



t  
numtrainingdays



  yt   b xday  t i 

  yt   b 

t  

numtrainingdays

p y   b   



numclusters



xday  t  j 

j  

  yt   b 

t  

numtrainingdays

  

and  used  the  following  rule  to  predict  the  days  in  testing  data  
numclusters


xday  t  i  
 p   y       p cluster i   y     

i  
yt     

 

numclusters
 p   y        p cluster i   y      xday  t  i  


   
i  

given   the   large   occurrence   of   each   word   and   each   class    up down    we   did   not   need   ant   smoothing   
given   the   relatively   large   level   of   error    see   the   error   table    and   the   strong   assumptions   made   in  
nave  bayes   we  decided  to  apply  a  discriminative  algorithm  as  the  next  step     

support  vector  machine  
the   feature   vectors   we   used   for   svm   were   normalized   to   give   the   relative   occurrence   of   each    
cluster   in   a   day    we   used   cvx   for   convex   optimization   and   set   the   regularization   factor          to  

fi  

  
  

  

seyed  reza  mir  ghaderi      
nima  soltani  

avoid   overfitting   we   turned   the   real   frequency   vector   to   a   binary   vector   showing   whether   each  
frequency   is   higher   or   lower   than   its   average   value   over   all   the   training   days    o   can   see   slight  
improvement   see  the  error  table   as  compared  to  nave  bayes   however   the  error  was  still  large   
we   decided   that   problem   may   come   from   is   the   loss   of   valuable   information   by   discretizing   the  
stock  value   therefore  we  used  a  linear  regression  model   

linear  regression  
we  tried  to  predict  the  value  of  the  stock  by  running  a  linear  regression  on      
 the  stock  value  in  the  past  few  hours  
 the  average  stock  value  in  the  past  day  
 the  hourly  count  of  clusters  in  the  tweet  data  of  the  past      hours  
we  considered  predicting  the  afternoon  stock  value     to  have  data  for  the  past  few  hours    the  result  
indicated   that   with   the   size   of   data   we   had    linear   regression   was   prone   to   overfitting    while   the  
training  error  was  very  low   we  got  orders  of  magnitude  higher  testing  error   

our  own  heuristic   score match  algorithm  
as  explained  in  the  svm  section   we  can  obtain  a  binary  feature  vector  by  discretizing  the  frequency  
vector   doing  so   we  get  binary  vectors  of  length      to  each  vector  we  assigned  a  score  equal  to  the  
stock   growth   following   that   vector    following   that   day     if   one   vector   happened   multiple   times   in  
our  data  we  assigned  the  average  of  the  stock  growth   negative  if  decline   values   we  then  labeled  
the  testing  sequences  by  comparing  their  feature  vector  to  the  ones  in  the  training  set   finding  the  
ones   with   minimum   hamming   distance   and   finally   comparing   the   average   score   values   of   the  
closest  vectors  to      see  the  diagram    this  algorithm   which  was  inspired  by  minimum  hamming  
distance   decoding   in   communication    lead   to   an   error   which   was   lower   than   all   the   other  
classification  algorithms   

  

results  
nave  bayes  
training  error     
test  error       
  

linear  svm  

linear  regression  

training   error         training  mse       
test  error       
test  mse          

score match  
test  error         

fi  

  
  

  

seyed  reza  mir  ghaderi      
nima  soltani  

conclusions  
although   we   did   not   meet   the   performance   of   the   original   paper    we   were   solving   a   slightly  
different   problem    the   result   of           accuracy   is   quite   good   considering   the   generality   of   the  
assumptions  we  were  making  and  the  limited  training  data  that  we  had   the  original  paper  had     
months  worth  of  training  data    

future  improvements  
there   is   room   for   improvement   and   we   are   interested   in   pursuing   research   on   this   topic    the  
following  are  potential  methods  of  improvement  that  we  plan  to  use   

retrain  with  a  sliding  window  
due   to   the   scarcity   of   data   in   our   project    we   tried   to  maximize   the   amount   of   data   we   could   use   for  
training   but  one  problem  was  that  we  did  not  update  our  training  data  once  we  tested  it   testing  on  
a  finite  window  of  the  past  could  provide  a  more  realistic  dataset     

use  pairs  of  words  as  features  
the   mutual   information   will   increase   even   more   with   the   joint   distribution   of   the   words   in   the  
cluster  as  opposed  to  the  distribution  of  the  union  of  the  words     intuitively   this  should  correspond  
more  to  a  contextual  clustering  of  the  words   as  opposed  to  purely  coincidental   

use  a  different  classification  structure  
the  current  system  only  considers  the  change  as  an  up down  quantity  and  does  not  distinguish  a  
    change  in  the  market  from  a       change   we  could  make  a  ternary  system  where  we  add  an  
insignificant  change  level   corresponding  to  a       to      change   alternatively  we  could  still  use  a  
binary  classification  but  only  train  using  data  points  corresponding  to  large  changes   

acknowledgements  
we   would   like   to   sincerely   thank   ali   reza   sharafat    who   helped   with   the   initial   bring up   of   the  
project   and   provided   advice   on   python   coding   later   on   after   he   withdrew   from   the   course    we  
would  also  like  to  thank  mihai  surdeanu  and  john  bauer  for  proposing  the  topic   

works  cited  
       j    bollen    h    maoa   and   x    zeng     twitter   mood   predicts   the   stock   market     journal   of  
computational  science   pp                  
       j   yang  and  j   leskovec    patterns  of  temporal  variation  in  online  media    in  acm   international  
conference  on  web  search  and  data  minig   wsdm    hong  kong            
  
  

  

fi