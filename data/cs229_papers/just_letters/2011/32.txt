classification of video game user reviews
derek huelsman
stanford cs     project
december         

abstract
using a set of     video game user reviews  i was able to train a supervised learning algorithm to classify
user opinions of video games as worth buying or not worth buying with an accuracy of         i
used this algorithm to find the user percent approval of eight different games currently in the market as
well as the average scores given by those who believed the games was worth buying and not worth
buying  the results differed greatly from the critical consensus  suggesting that user reviews provide a
valuable alternate perspective on video game quality 

   introduction
the internet has made it possible for millions of users to submit personal reviews for nearly all available
consumer products  the impetus for websites to provide this feature is two fold  it attracts those who
wish to have their voices heard and it attracts those who are looking for a reliable source of public
opinion  the video game industry in particular is greatly affected by this social internet feature  this is
because buying a video game is a non negligible investment of both time and money  some of the best
games of this generation cannot be completed in     hours and cost as much as     new  since the
video game industry provides few opportunities to get free hands on experience with a game prepurchase  website and magazine reviews are perhaps the consumers most valuable resource 
websites like gamerankings com and metacritic com attempt to calculate the average scores of all
published professional reviews for each game  however  there is no requirement that different sources
rate games on comparable scales  as a result  the average review score from each of
gamerankings coms     review sources ranges from        to         and these are often converted
from letter grading schemes  this suggests the same score often means something completely different
coming from two different sources 
the problem is arguably worse at the user level  gaming websites that allow users to give scores to
accompany their reviews rarely provide a qualitative scale to guide users  as a result  each user draws
the line between recommendation and condemnation in a different place  below are some samples of
negative and positive reviews from gamespot com with provided scores and summaries 





    negative quick review of a game that didnt live up to the hype 
    negative ruined by a rushed release  bugs  sloppy meshes  oh my 
    positive the best of the unpolished games of     
    positive excellent sequel

another problem is that some users are not afraid to give the maximum or minimum allowable scores
for games they do dont like just to have the maximum impact on the displayed average user score  as a
result  these reviews are effectively weighted more than the reviews from those who choose to use the
entire range of scores  its not even a covert practice 

 

fi

   i don t actually give this game a     its to level out the ratings of people who put        i give
it an   like gamespot

occasionally users simply enter a rating they didnt intend 


    i have never played a game this good  this amazing  its perfection

this makes it difficult for visitors to these websites looking for game recommendations to get players
overall impression of a game in a reasonable amount of time 
these problems can be avoided by classifying reviews as positive or negative  or more explicitly 
categorizing reviews based on whether the user believes the game is worth buying or not worth
buying  the average of many of these classifications is akin to a probability that your investment will
pay off  with this information  a more valuable consensus can be made available to interested
consumers  no gaming website that i know of has implemented a system like this  but  using machine
learning and text analysis  it is implemented here using existing user reviews 
i will be using user reviews and not professional reviews primarily because there are many more of
them  and they currently receive much less focus than their professional counterparts  despite being
equally valid and equally valuable opinions of games 

   pre processing
the training set used consists of     positive and     negative reviews  approximately ten of each from
   different top selling video games between      an       these were scraped from gamespot com
and gamefaqs com  reviews were included from such a large quantity of games in order to avoid bias
from genre specific text  the data stored for each review includes a positivity label of   or    the users
rating on a     point scale  and the review text itself  the following steps were used to preprocess the
data 







the stanford tokenizer provided by the stanford natural language processing group
 http   nlp stanford edu index shtml  was used to remove empty lines from the text and put
each term or token on a separate line 
all meaningless punctuation was removed   periods  commas  quotation marks 
stop words were removed using a modified list provided by cornell university
 ftp   ftp cs cornell edu pub smart english stop   a different set of stop words was used
depending on if bigrams were included in the term document matrix  since common words like
not and is become valuable when attached to other words 
the porter stemming algorithm was used to combine words with the same stems
 http   tartarus org martin porterstemmer    for example  addiction  addicting  and
addictive would all be reduced to addict 
a vocabulary was created for the entire training set  this included        unigrams and       
bigrams 

although it was considered  including trigrams or other n grams increases computation time
dramatically  making it infeasible to include them in this project 

   term document matrix creation
 

fia term document
document matrix is made up of the number of occurrences of term j in document i  or in this
case  review i  from this  several learning algorithmss can be applied under the assumption that the
probability
lity of a term occurring in a review is independent of any other term occurring in the review 
eight      distinct term document
document matrices were created using the training data in an effort to test how
much different combinations of three matrix preparation methods would affect review classification
accuracy 




including bigrams in addition to unigrams  increasing the number of columns from        to
        
doubling the term weight of the first    and last     of each review  these are approximately
the locations
ions where reviewers were most likely to giving their overall opinion of a game  several
different weight distributions were tested 
counting only if a term occurs in a document or not  rather than the number of occurrences 

i also tried several implementations
tions of tf idf
tf
 term frequency inverse
inverse document frequency   but none of
them improved accuracy 

   training results
i used nave bayes  logistic regression  and support vector machines along with leave one out
leave
cross
validation  loocv  to find which algorithm provided the highest classification accuracy for the training
data  l  regularized
regularized logistic regression performed on average about       better than the best svm
algorithm  which is nearly negligible  so i will provide the training results for nave bayes and logistic
regression with the assumption that the best logistic regression and svm results are identical 

loocv naive bayes

loocv logistic reg 

      

      
basic

      

basic

      

bi
accuracy

accuracy

bi
      

wt
bi wt

      

      

wt
bi wt

      

sng
      

sng
      

sng bi

sng bi

sng wt
      
           

sng wt
      

sng bi wt

           

review count

bi  bigrams used

sng bi wt

review count

wt  weights used

sng  single counts used

with the set of     reviews  using bigrams  weights  and single counts
count with l  regularized
regularized logistic
regression achieved the highest accuracy          it is interesting to note that using only unigrams and
single counts achieved the second best accuracy of         and the best accuracy using only    
 

fireviews   thus  this would be a reasonable alternative if the increased computation time of including
bigrams became a problem  logistic regression benefitted the most from increased review count  and it
is likely another increase in review count would increase accuracy a little further 
among the most common tokens indicative of a positive review were must have  awsome  sic 
finest  underrated  addicting  is awesome  personal favorite  must buy 
highly recommended  and great addition  among the most common tokens indicative of a negative
review were disgrace  horrible  unacceptable  overrated  lame  not buy  is boring 
not worth  bargain bin  and huge disappointment 

   testing results
in order to apply the best performing algorithm to a real world situation  i acquired the    most recent
reviews  or the total number of reviews available  for some of the most highly anticipated games of
      games like batman  arkham asylum  the elder scrolls v  skyrim  and the legend of zelda 
skyward sword received nearly unanimous praise from critics and users alike  so they were excluded
here  i was interested in games that received critics scores in the high   s and   s  where it may be
unclear for consumers what the score actually means  adjusted   approval was calculated from the test
  approval by taking into consideration the classification error from the training results       of bad
games were classified as good while      of good games were classified as bad  using these values  we
can find the expected value of the actual percent approval by solving the equation 
              

         

  

for
  as the accuracy of an algorithm increases  the adjusted   approval approaches the test  
approval  critics scores were aggregated by gamerankings com 

  

user
review
count
  

battlefield  

    

  

     

     

    

    

dark souls

    

  

     

     

    

  

resistance  

    

  

     

     

    

  

dragon age ii

     

  

     

     

     

     

madden nfl   

    

  

     

     

    

    

assassin s creed  revelations

    

  

     

     

    

    

call of duty  modern warfare  

     

  

     

     

     

     

game title
saint s row  the third

critics
average

     

average
approval
score
    

average
disapproval
score
    

test  
approval

adjusted  
approval

     

the average approval score is the average rating provided by those who believe the game is worth
buying  the average disapproval score is similar  this provides some additional useful information about
the game  for example  even though dark souls received only the third highest adjusted approval  it
received the highest average approval score  indicative of the fact that it tends to be a love it or hate it
game 
 

fiuser approval

test result comparison
      
     
     
     
     
     
     
     
     
     
    

saint s row  the third
battlefield  
dark souls
resistance  
dragon age ii
madden nfl   
assassin s creed  revelations
  

  

  

  

  

  

   

call of duty  modern warfare  

critics  average score

perhaps surprisingly  there is no clear correlation here between the critics average score and the user
approval percentage  in fact  assassins creed received by far the highest approval  despite having the
third lowest critics average  it would become more obvious that a correlation exists if we also included
unanimously praised and panned games  but the fact that there are significant outliers in this score
region suggests that critics scores may not be enough to get an accurate read on the quality of a game 

   discussion
the information acquired in the testing results is particularly valuable because it took critics scores
spread over a range of less than    on a scale of      and spread them to a range between     and
     it is now much easier to distinguish between games that one may otherwise have believed were
equally valued  while professional reviews remain the most valuable resource for consumers interested
in the technical aspects of games  it makes more sense to get information on how worthwhile a game is
from as many different sources as possible  in this case  the vast number of user reviews freely available
on the internet should suffice 
there remain some unresolved issues that lower classification accuracy 




some reviews simply offer no personal opinion on the game  how can these be ignored   and
should they be ignored  
since the video game industry attracts a younger crowd  rampant spelling mistakes can leave
little for the classifier to work with  would a spellcheck implementation be worthwhile 
theres a problem with users repeatedly referring to other games with the opposite reaction
 that was the greatest game of all time but this one is not   which can throw off the classifier 
im not sure of an easy way to work around this problem 

with more time  i would have liked to try using a parser to extract adjectives from sets of reviews for
games in order to obtain consensus game descriptors 
all websites referenced are given at the relevant location in the text  the cs    lecture notes were used
to understand and implement all learning algorithms  matlab was the primary computing environment 
 

fi