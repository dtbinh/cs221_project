a recommendation engine for wikipedia articles based on
constrained training data
john rothfels

brennan saeta

which we wish to generate recommendations  we
imagine a situation in which a user has potentially
liked only a small number of articles   li       i  
based on what we believe to be a reasonable assumption about how much a user can be expected to
manually curate their preferences  while some users
might actively curate their interests  others may not 
we wish to generate good recommendations in either
case  without resorting to collaborative filtering techniques such as generating recommendations based on
what other  similar  users have liked 

abstract
we consider the problem of generating recommendations for wikipedia articles
based on constrained data  modern recommendation systems commonly use a combination of collaborative filtering techniques and
content based methods to predict new items
of interest for a user  we focus our problem
on generating recommendations via contentbased analysis using only a small set of liked
articles as training data and no other information about user preferences or other users
in the system  we find that our methods are
promising for users with many likes  but that
our algorithms do not generalize well to more
constrained data  concretely  other methods
are needed to produce good results when a
user has a small set of likes 

 

emin topalovic

 
   

preliminary steps
generating a dataset and subset of
wikipedia articles

our first task was to work towards generating a
dataset on which to test our recommendation algorithms  we needed not only an unbiased dataset
which reflects the diverse preferences of multiple
wikipedia users  but also a more tractable subset a
of wikipedia articles on which to test our approach 
at the time we started our project  there were over
   million articles on wikipedia  comprising about   
gigabytes of data  given this large amount of data 
retrieving a similarity measure of each of these articles to users preferences would be computationally
infeasible  with our hardware setup  

introduction

the popularity of wikipedia has skyrocketed since its
inception  people use the site to learn  to research 
and to explore  given its prevalent use  and the previously successful applications of machine learning to
recommendation problems  we propose to build a recommendation engine for wikipedia articles  we formulate our problem as follows  given a  non empty 
set li of wikipedia articles that a user i has enjoyed
or liked  we wish to generate a non intersecting set ri
 li  ri     of wikipedia articles the user will also
find interesting or enjoyable to read  our challenge
is in generating measures of similarity between articles that generalizes to allowing our recommendation
engine to accurately suggest articles related to the
users preferences  as well as out of the box articles
which  possibly unrelated to the users likes  he or
she may still enjoy  an additional challenge is working with the massive dataset that comprises the    
million articles of wikipedia 

since we found no preexisting data for wikipedia user
likes  to generate a dataset we asked real users  via a
web survey which we wrote and distributed  to select
wikipedia articles that they would be interested in
reading  to make this survey more feasible for users
to complete  we again wanted a reasonable sized subset of articles for them to choose from which would
allow for diversity of articles and topics but not incentivize users to rush through the survey  causing
them to submit disingenuous preferences 

unique to our problem is the nature of the data over for the reasons given above   a  was set arbitrarily
 

fiat      however  we felt that choosing this subset
should not be done randomly  since many articles in
wikipedia are stubs or pages redirecting to other articles  we wanted to choose only articles with high
content value so that the selections would be rich and
more inclusive of users actual interests  towards this
end  we implemented and ran the pagerank algorithm      we first processed our articles into a matrix
g such that gi j     if article i links to article j  after a little less than     hours  our implementation 
had converged such that the change in    norm was
less than          given each articles pagerank 
we selected the top m       articles to be a 

binary variable indicating existence of the ith dictionary word in a  we produced separate feature vectors
for nouns  verbs  and adjectives 

we received    submissions to our web survey  each
containing at least    liked articles  with some submissions including more than     selections  from
each users submission  we randomly chose     of
the selections to constitute a like set  li   which we
use for generating recommendations  the other    
constitutes the articles we hope to recommend and
which all reported data is based on 

 articles of confederation  american civil war 
abraham lincoln
 ancient egypt  avicenna

on each vector type  we evaluated the capability of kmeans to produce meaningful clusters of articles  we
found that while a few clusters were in fact informative  see below   most were either very sparse or too
large to be useful no matter the number of clusters
generated  this motivated our exploration of better
feature choices 
some examples of  qualitatively  interesting clusters 

   

revised strategy

we decided to build a new dictionary containing the
stems of all useful words in a  more concretely 
we ran porters stemmer over all a  a  and further
note  in the recommendation domain  it is often in  introduced a list of standard stop words to remove
feasible to give more than a small number of rec  uninformative english words from a such as the 
 
ommendations  e g  due to space limitations on a giving us a set of words a   the dictionary generated
 
web page  and in our situation  we are indeed con  is the union of all a   finally  to reduce the dimension
cerned with returning a reasonable number of rec  of the dictionary  we removed words whose frequenommendations  moving forward  our decision was cies across all articles fell out of a specified range
to try and create algorithms which learn to return  arbitrarily set at        and were able to reduce the
good recommendation sets without artificial pruning number of words in the dictionary from          to
or post processing steps  so recommendation sets are under          this choice was motivated by the intuition that the most useful words to use for generatreasonably sized by default 
ing content similarity measures between articles are
those that are neither extremely rare nor extremely
  method and experiments
common 
   

initial strategy and results

we further decided to switch from the naive approach
of generating feature vectors from count existence of
nouns adjectives verbs and instead began considering methods that have proven to be successful in related works          we chose to do term frequencyinverse document frequency  tfidf  as a measure
of word importance within each article  this allowed
us to introduce a more intelligent notion of word importance  rather than count  to introduce semantic
information  we considered latent semantic analysis
 lsa  which encodes the contextual usage of words
     we further considered latent dirichlet allocation  lda  as a way to consider a different feature
paradigm  namely one which models a mixture of deduced topics as the representation for each article     

given a  the next task was to convert each article
into a feature vector  we chose to start by considering the verbs  nouns  and adjectives appearing across
all articles in a as separate sets of features  as a
preprocessing step  we formed a dictionary of verbs 
nouns  and adjectives which appear in our corpus 
to standardize the dictionaries  we used stanfords
part of speech tagger     to select nouns  adjectives
and verbs  we used porters stemming algorithm    
to reduce dictionary dimension 
for each article a  a  we produced feature vectors of
the form va    v         vn   where each vi is either the
count in a of the ith dictionary word or i vi  a   a
  written

in python  leveraging the numpy package 

 

ficontinuing on  we will differentiate between the dif  clustering and choose the dislike set di to be those
 tf idf  lsa lda 
articles least similar to articles in li   concretely  we
ferent features choices as va
 
define the weight of a positive cluster cp to be the
we applied clustering to our feature vectors using k  number of articles from l that are in that cluster 
i
means  we utilized k means in two ways which we for each positive cluster  we sort all non positive cluswill refer to as naive and hierarchical  for naive k  ters by distance to the centroid  the farthest clusmeans  we cluster va into k                     clus  ters are weighted more than the closer ones  finally 
ters  where va is the set of feature vectors for a  for once weve calculated this for all positive clusters  we
a given user i  the recommendation set ri returned is sample articles from the negative clusters according
the union of clusters containing articles from li   this to their total weights  these chosen articles are the
method gives us an idea of how well our features allow dislike set 
simple clustering to not only cluster potential articles
of interest  but further prune out irrelevant articles
to give a manageable recommendation set  hierar    results
chical clustering is a more direct way of producing
a targeted and manageable recommendation set  we
begin as we do with the naive approach  namely clus  given our choice to look for algorithms which return
tering va   for a given user i  we then iteratively appropriately sized recommendation sets  we chose
re cluster the union of clusters containing articles in f  score as a metric of performance  this score reli until we achieve convergence  the recommendation wards recommendation sets that contain articles we
set ri returned is the union of all re clustered clus  know a user will like  based on the     held out
ters containing articles in li   convergence is reached data  but penalizes for recommending other articles 
when subsequent runs stop adding a significant num  most importantly  the f  score will penalize us for reber of new articles to the recommendation set which turning large recommendation sets when the user has
would be returned  concretely  we define convergence specified only a small number of likes  for each of our
   data points  and for a given choice of algorithm  
as 
we compute an f  score from the recommendation set
returned by the algorithm  numbers reported below
n
nn    
are average f  scores across all    data points 
 
where n is the set of number of articles returned
in the four iterations previous to the current one 
namely  ni is the number of articles added to the recommendation set going from iteration i    to i  the
following is an outline of hierarchical k means 

to determine the significance of our results  we computed t tests  unpaired  heteroscedastic  two tailed 
between the f  scores of our algorithms and found no
statistically significant difference between any of our
algorithms or feature choices  p        

def h i e r a r c h i c a l k m e a n s   a r t i c l e s    
    clustering
r e c   s e t       a r t i c l e s t o recommend
  add a l l a r t i c l e s t h a t a r e c l u s t e r e d
  
  
   
   
  with elements in the l i k e s e t
v tfidf           
           
f o r a r t i c l e in l i k e s e t  
v lsa
   
                 
r e c    a r t i c l e s i n c l u s t e r   a r t i c l e  
i f l e n   r e c     num recs or c o n v e r g e d  
v lda
                 
    
return r e c
table    f  score of naive clustering versus cluster size
return h i e r a r c h i c a l k m e a n s   r e c  
  
   
we also wanted to see how supervised learning algov tfidf            
rithms could perform in generating recommendation
v lsa
    
    
sets  namely  we considered the efficacy of logistic regression and svms with gaussian and linear kernels 
v lda
     
    
this presented the unique challenge of deducing negative  or disliked examples  given that our input is only table    f  score of hierarchical clustering versus cluster
li   a set of likes for user i  to do this  we leverage size
 

fi   

figure    more input data gives us better results  graph
of svm with linear kernel 

supervised methods
v tfidf
v lsa
v lda

gaussian
    
     
     

linear
     
     
     

regression
    
     
     

we see an almost identical graph for all of our algorithms  leading us to suspect that one of our fundamental problems is the constraint of our problem 
namely to have algorithms which perform well given
small like sets  we may simply need more training
data in order to perform well 

table    f  score of different supervised methods

 
   

   

conclusion

classification sensitivity to negative article selection

recommendation set sizes

we wanted to see how much of a difference a simple
negative article selection performs against our handacross most of our algorithms  hierarchical cluster  crafted one   we ran this for tfidf features  using
ing the exception  we found that recommendation set a gaussian kernel 
sizes were generally quite large  often greater than
hand crafted
              
     while this made the recall of our algorithms
simple
               
quite good  our precision was often very poor  despite various changes to our algorithms  e g  increastable    tfidf features  gaussian kernels  two different
ing the number of clusters  increasing the size of the negative generation systems 
dislike set   recommendation sizes continued to be
large  leading us to suspect that post processing on we find that our negative article generator improves
recommendation sets is necessary to reduce size  we the f  score of our gaussian svm  this is statisconsider options in the future work section 
tically significant result   using a better negativearticle selection algorithm can potentially increase
    small input sizes
our f  score 
as we looked over numbers  we noticed that our algorithms consistently perform better for users with
larger like sets  in order to understand this  we plotted the f  score as a function of the size of a users
like set  li   we find a practically linear relationship 
as users tell us more information about what they
like  our system works better and better 

   

overall

recommending interesting wikipedia articles given
only a handful of good articles is not an easy task 
limiting ourselves to content based recommendation
systems  and only a few training examples per person is an especially challenging problem  holding all
other variables constant  we found that the f  score
increased almost linearly as a function of the size of a
users like set  further  we found that the classification algorithms were sensitive to the negative articles
selected 
a general concern we have is about the dataset itself  although we found a way to associate numbers
with our algorithms  we feel that the dataset is far
from perfect  specifically  we have assumed that the
survey results from each user contains every article
in a that the user finds interesting  since there is no
way for us to enforce this  one can imagine that the
true  or unbiased  set of results should contain many
more articles than a user actually selects during the

  the simple negative article selection simply returns one article no matter what positive articles are passed in  we chose
this over randomly selecting articles because this allows for a consistent measure across runs of the algorithm 
  running a unpaired  heteroscedastic  two sided t test with the null hypothesis that the two means are the same  we find
that we reject the null hypothesis  p           

 

fisurvey  either because he she forgot to select some
articles  or tried to complete the survey in haste  or
any other of a multitude of reasons  a more interesting example of this is somewhat philosophical  that
often times users dont actually know what they want
or might like  they may think they know  and will
act accordingly  but there is no reason why some of
our recommendations should be wrong in the way
we say they are  i e  if the recommendation is not in
a users     held out set  

recommendations they can give   with the exception
of our hierarchical clusterer  our algorithms  generally  return recommendation set sizes over    despite
our efforts to prevent this from happening  while
this boosts our recall  it negatively affects our precision  this suggests the need to modify our approach
to find algorithms which generate good recommendation sets  but require additional post processing
steps which prune the recommendation sets to reasonably sized subsets  there are many feasible methods and heuristics to accomplish this  one suggestion
is to have our svms not return the set of articles
  future work
classified positive  but rather the k articles given the
we found one of the major shortcomings of our ap  highest probability  margin  of being positive examproach was that we could successfully generate good ples 
recommendations when a user has a large set of preferences  but that when there is limited data  our references
problem is extremely hard to solve  for users with
    david m  blei  andrew ng  and michael jordan  lasmall like sets  it is possible  and potentially desirtent dirichlet allocation  jmlr                  
able  to use collaborative filtering techniques to augment their like set before running our algorithms      t  k  landauer  p  w  foltz  and d  laham  an
introduction to latent semantic analysis  discourse
more concretely  in a preprocessing step  we can augprocesses                  
ment a users like set by finding the k users with most
similar preferences and considering the union of their     larry m  manevitz and malik yousef  one class svms
for document classification  j  mach  learn  res  
like sets as we run our algorithms   alternatively 
          march      
we can base the recommendations themselves on the
like sets of similar users  vis a vis a others like you     raymond j  mooney and loriene roy  content based
book recommending using learning for text categoenjoyed    system 
rization  in fifth acm conference on digital libraries 

dl     pages         new york  ny  usa       
our research suggests that another shortcoming of
acm 
our system is using feature vectors whose dimensions
are too large  previous work has obtained compelling     lawrence page  sergey brin  rajeev motwani  and
terry winograd  the pagerank citation ranking 
results deducing article topic with only    to    feabringing order to the web  technical report     tures  this same work found that svms can be
    stanford infolab  november      
highly sensitive to feature vector size  and futhermore  that the optimal size can depend on kernel     wongkot sriurai  phayung meesad  and choochart
haruechaiyasak  recommending related articles in
choice      even after removing all words in our
wikipedia via a topic based model  in iics  pages
dictionary which occur less than   and more than
             
   times across all articles  we still have more than
        features 
    kristina toutanova and christopher d  manning 
enriching the knowledge sources used in a maxi 

finally  we feel that our strategy for returning reasonmum entropy part of speech tagger  in proceedings
ably sized recommendation sets can use significant
of emnlp  emnlp     pages       stroudsburg 
improvement  recall our approach to this problem
pa  usa        association for computational linguistics 
was to generate algorithms which  by default  return
not only good recommendations  but also a small     c j  van rijsbergen  s e  robertson  and m f  porter 
number of them  given the requirement that most
new models in probabilistic information retrieval  unrecommendation engines are limited in the number of
known       

  there are various measures of textual similarity which can be considered  some of which use the feature vectors we have
already generated for each article 

 

fi