predicting borrowers chance of defaulting on credit loans
junjie liang
 junjie   stanford edu 

abstract
credit score prediction is of great interests to banks as the outcome of the prediction
algorithm is used to determine if borrowers are likely to default on their loans  this in turn
affects whether the loan is approved  in this report i describe an approach to performing
credit score prediction using random forests  the dataset was provided by www kaggle com 
as part of a contest give me some credit  my model based on random forests was able to
make rather good predictions on the probability of a loan becoming delinquent  i was able
to get an auc score of           placing me at position     in the contest 

  introduction
banks often rely on credit prediction models to determine whether to approve a loan
request  to a bank  a good prediction model is necessary so that the bank can provide as
much credit as possible without exceeding a risk threshold  for this project  i took part in a
competition hosted by kaggle where a labelled training dataset of         anonymous
borrowers is provided  and contestants are supposed to label another training set of
        borrowers by assigning probabilities to each borrower on their chance of defaulting
on their loans in two years  the list of features given for each borrower is described in table
  
variable name
seriousdlqin yrs

revolvingutilizationofunsecuredlines
age
numberoftime    dayspastduenotworse
debtratio
monthlyincome
numberofopencreditlinesandloans
numberoftimes  dayslate
numberrealestateloansorlines
numberoftime    dayspastduenotworse
numberofdependents

description
person experienced    days past due
delinquency or worse
total balance on credit cards and personal
lines of credit except real estate and no
installment debt like car loans divided by
the sum of credit limits
age of borrower in years
number of times borrower has been      
days past due but no worse in the last  
years 
monthly debt payments  alimony living
costs divided by monthy gross income
monthly income
number of open loans  installment like car
loan or mortgage  and lines of credit  e g 
credit cards 
number of times borrower has been   
days or more past due 
number of mortgage and real estate loans
including home equity lines of credit
number of times borrower has been      
days past due but no worse in the last  
years 
number of dependents in family excluding
themselves  spouse  children etc  

table    list of features provided by kaggle

 

type
y n

percentage
integer
integer
percentage
real
integer
integer
integer
integer
integer

fi  method
random forests is a popular ensemble method invented by breiman and cutler  it was
chosen for this contest because of the many advantages it offers  random forests can run
efficiently on large databases  and by its ensemble nature  does not require much
supervised feature selection to work well  most importantly  it supports not just
classification  but regression outputs as well  however  there are still performance
parameters that need to be tuned to improve the performance of the random forest 

    data imputation
the data provided by kaggle were anonymous data taken from a real world source and
hence  it is expected that the input contains errors  from observation  i determined that
there were three main types of data that required imputation 
   errors from user input  these errors probably came from a typo during data entry 
for example  some of the borrowers ages were listed as   in the dataset  suggesting
that the values might have been entered wrongly 
   coded values  some of the quantitative values within the dataset were actually
coded values that had qualitative meanings  for example  under the column
numberoftime     dayspastduenotworse  a value of    represents others 
while a value of    represented refused to say  these values need to be replaced
so that their large quantitative values do not skew the entire dataset 
   missing values  lastly  some entries in the dataset were simply listed as na  so
there is a need to fill in these values before running the prediction  in particular 
na values were found for features numberrealestateloansorlines and
numberofdependents 
since the choice of method of data imputation could significantly affect the outcome of the
prediction algorithm  i tested several ways of doing the imputation 
   just leave the data alone  this method can only be used for errors in the first  
categories  since the random forest algorithm works by finding appropriate splits in
the data  it will not be adversely affected even if there were coded keys with large
quantitative values in the input  however  this would not work for filling in the na
entries in the data  so for this method  i removed the features
numberrealestateloansorlines and numberofdependents completely from the
dataset 
   substitute with the median value  another way to handle the missing data is to use
the median values among all valid data within the feature vector  in a way  this is
using an available sample within the dataset that would not affect predictions
greatly to fill in the missing entry 
   coded value of     since the randomforest library that i was using would not take
na for an input entry  i simply replaced them with a value of     essentially  a value
that did not appear anywhere else  to ensure that na is seen as a separate value in
the data 
as will be described below  from tests it was determined that method    filling in missing
entries with     worked the best in giving prediction accuracy 

 

filastly  the labels in the dataset were skewed so that there were about    times more nondefaulters than defaulters  this is to be expected since most people do not default on their
loans  or try not to  however  this will tend to skew the predictions made by the random
forest as well  to overcome this problem  i   rebalanced   the input by repeating each row
with defaulters    times  so that overall  the input file contained an even number of
defaulters and non defaulters 

    parameter tuning
one of the nice features of random forests is that there are relatively few parameters that
need to be tuned  in particular  the parameters that i tested for optimizing were 
   sample size  size of sample to draw at each iteration of the split when building the
random forests 
   number of trees  the number of trees to grow  this value cannot be too small  to
ensure that every input row gets predicted at least a few times  in theory  setting
this value to a large number will not hurt  as the random forests should converge 
however  in my tests it appears that a value that is too large reduces the accuracy of
the prediction  possibly due to overfitting 

    experimental setup
for the test setup  i set up a data pipeline that first took the raw input and passed it through
the data imputation stage  thereafter  the parameters to the random forests are chosen 
and a   fold cross validation is done on the labeled inputs  an auc score is calculated for
each of the   runs  and the average auc is used to get an estimate of the performance of
the algorithm on the actual test data  the experimental setup is shown in figure   

figure    experimental workflow

  results
    methods of data imputation
i shall now present some of the findings from my experiments  among the three methods of
doing data imputation  it was clear that the method which used coded values gave the best
results  this could be because borrowers who had na in their entries were grouped
together and used to predict each others credit reliability  which would implicitly mean a
nearest neighbor like match was being done  the results of the three methods can be
seen in figure   

 

fiaverage auc from   fold cross
validation

comparing methods for data
imputation
     
      
     
      

auc     trees

     

auc      trees

      
     
      
leave alone

median

coded value

figure    comparing different ways of doing data imputation

    number of inputs sampled at each split
this input number refers to the number of inputs sampled at each iteration when building
the forest  i tested the prediction performance with different sample size  all using the
coded value heuristic for data imputation  since it gave the best performance among the
three methods   from the results we see that a larger sampling size gives less accurate
prediction  this could be because of overfitting to the dataset  the results of the tests can
be seen in figure   

average auc from   fold cross
validation

comparing sampling size
     
      
     
      
     
      
     
    

    

     

     

     

sampling size
figure    comparing different sampling size

    number of trees grown
lastly  from the best performers in the past two parameters  i ran a test for     and     
trees grown  actually  most of the learning occurred within the first     trees that were
grown  and only incremental changes appeared thereafter  nonetheless  it appears that the
performance was slightly better when we grew only     trees as opposed to      trees 
again  i suspect this was due to overfitting of the data  results are shown in figure   
 

fiaverage auc from   fold cross
validation

comparing number of trees
grown
       
       
       
       

   

    

figure    comparing number of trees built in the forest

    submission to kaggle
the above results were calculated from doing a   fold cross validation on the labeled data
provided by kaggle  however  after all the parameters were chosen and a best set was
found  the parameters were used to train on the labeled training data  and used to predict
the unlabelled test data  this was then submitted to kaggle which did the final auc scoring 
using the parameters chosen above  i got an auc of           as a reference  the top team
got a much better result  an auc score of          

  discussion
simply by tweaking a few parameters  i was able to get rather good prediction results on the
dataset  this is an example of the strength of random forests  its default parameters are
generally quite good  from discussions that occurred on kaggle after the competition is
over  some of the teams also using random forests were able to get much better results 
with smarter ways of doing data imputation  and by combining random forests with other
ensemble methods like gradient boosting 

  references
   bastos  joao   credit scoring with boosted decision trees   munich personal repec
archive           print 
   chen  chao  andy liaw  and leo breiman   using random forests to learn imbalanced
data  
   dahinden  c     an improved random forests approach with application to the
performance prediction challenge datasets   hands on pattern recognition          
print 
    description   give me some credit   kaggle   data mining  forecasting and
bioinformatics competitions on kaggle  n p   n d  web     dec       
 http   www kaggle com c givemesomecredit  
   leo  breiman   random forests   machine learning                    print 
    random forest   wikipedia  the free encyclopedia   wikipedia  the free encyclopedia 
n p   n d  web     dec         http   en wikipedia org wiki random forest  
   robnik sikonja  m    improving random forests   lecture notes in computer science
                        print 
 

fi