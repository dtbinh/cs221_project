can a machine learn to teach 
brandon rule         
december         

 

introduction

computers have the extroardinary ability to recall a lookup table with perfect accuracy
given a single presentation  we humans are not so fortunate  to learn  we must see
the entries of a lookup table many times  however  it is not sufficient  nor efficient  to
simply see the entries several times in one sitting  we must repeatedly be reminded of
an entry at spaced intervals  the spacing is not arbitrary  if we wait too long  we forget
the entry  not long enough  and we waste time with a familiar entry  the spacing is
also not constant  more difficult entries must be reviewed more frequently  the process
of learning a lookup table in this manner is called spaced repetition 
the goal of spaced repetition is to maximize the number of lookup table entries
stored in a students memory at a given time  however  it is not possible to have
complete confidence that any particular entry is known  so we consider two alternatives 
goal    maximize the expected number of entries known at a given time 
goal    maximize the number of entries that we are highly confident the student knows
at a given time 
it is not clear that either goal is superior in all circumstances  if the student wants to
score well on a simple knowledge retrieval test  then we might argue that we should
target the first goal  because this would maximize the expected score on the exam  on
the other hand  if the lookup table consisted of the vocabulary for a language  then it
might be superior to target the second goal  since the first may be prone to leaving the
student with a vocabulary of partially known words across a range of topics  rendering
her unable to speak fluently about any single one 

 

our model

to clarify the problem  we specify a probabilistic model  we are given a set of students
s    a  b  c        and a lookup table t     x    y              xn   yn     here the set s is
arbitrary  and the xk and yk are also arbitrary  the reader may take xk and yk to
be numbers  words  names  or any other objects that a person might be interested in
committing to memory  associated with each student and entry is a history consisting
of times h    t    t            rn  more accurately  a sequence of elements of an affine

 

fispace acted on by r   indicating when the student is exposed to the given entry  for
example  suppose adam is trying to learn spanish  and has seen the flashcard
hello   hola 
at     pm      pm and      pm  in this case  we could represent adam as student a 
the flashcard as entry  hello  hola   and the history as            
we model the experiment of testing student a on entry  xk   yk   at time t given
history h using a bernoulli random variable x whose probability is a function of a  k 
t and h  we set x     if student a knows entry  xk   yk   at time t given history h 
and   otherwise  we denote the probability that x     by f  a  k  t  h   in symbols 
we have
def
f  a  k  t  h    pr x      a  k  t  h  
with our new definitions  we see that the task is to construct for each student and entry
a history h  given our knowledge of the outcome of a series of bernoulli experiments 
we thus restate goal   as follows  given student a  vocabulary t   and time t  find
arg max
h

n
x

e x  a  k  t  h    arg max
h

k  

n
x

f  a  k  t  h  

k  

goal   can be stated using an additional parameter   indicating what we mean by
highly confident  for example  we might say were highly confident a student knows
an entry if we believe there is at least a     chance that she knows the entry  in this
case  wed set        given   a  k and t  our goal is to find
arg max
h

n
x

   f  a  k  t  h      

k  

for this project  we focus on the latter goal 

 

existing solution

our data was collected by a program used by a single student to learn the language
xhosa  in this case  the entries of the lookup table consisted of pairs of words indicating
the translation from english to xhosa  for example  dog  inja   the program
uses a simple algorithm intended to maximize the number of entries with confidence
greater than      for each word  the program keeps track of the the students past
performance  for example  if at a given point in time  the student has been presented
with a given word five times  answering incorrectly the first two and correctly the last
three  the students performance on the word would be                  the algorithm
associates with a given history a feature called the words streak  defined to be the
value and length of the longest constant suffix of the history  for example  the history
                would have streak         intuitively  this says that the student has answered
the word correctly the past three times in a row  the history           would have streak
        indicating she has answered incorrectly the past two times in a row 
associated with each type of streak that has occurred  the program stores a number
indicating the number of milliseconds that it should wait before presenting the student
 

fiwith any word with the given history  for example  if the student has history          
for a particular word  the student last saw the word at     pm  and the program has
a time of   hour associated with the streak type         then the the student will be
scheduled to see the given word again at     pm  note that the repetition interval
selected by the algorithm is purely a function of the streak of a particular word  taking
no other features of the word or its history into account 
in order to target the goal of maximizing the number of words with confidence above
     the program tunes the times associated with the various streak types as follows 
whenever the student answers correctly after a given streak type  the time associated
with the streak type is multiplied by       when the student answers incorrectly  the
streak type is multiplied by         thus  if a student is answering correctly after a
given streak type     of the time  then on average  out of    answers    will be correct 
and   will be incorrect  thus  the time will be multiplied by
                 
causing the time to oscillate  if she is answering more than     of the words correctly 
the time will increase until it starts to oscillate  similarly  it will decrease if she answers
correctly less than     of the time  the data demonstrates that this technique appears
to work well  in a history consisting of        answers  we observed that the student
answered words correctly       of the time  on average 
however  this model takes into account only a single feature of the word  its current
streak  it makes no distinction between histories     and                  we decided to
investigate the impact of other features on the probability of answering a word correctly 

 

testing other features

given our data of        answers across     words  with times selected by the algorithm
described in the previous section  we trained a logistic regression algorithm to predict
whether the student will answer a word correctly given the words history  testing
the predictive capabilities of various features  however  it wasnt possible to treat all
histories uniformly  because the way that times were selected was not uniform  for
instance  we initially attempted to find a correlation between time since last seeing a
word and probability of answering correctly  it was difficult to find any correlation 
however  this was to be expected  because the times were carefully selected by an
algorithm to target a     probability of answering correctly 
to overcome this bias  we split the data according to streak types  this way 
within a single streak type  there is no bias as to how the time was selected  we then
tried various features to determine which might have an impact on the probability of
answering correctly  although we tried more than a dozen features  only a few ended
up being predictive  we give seven here  though as well see in the data  not all of them
were particularly predictive 
 the time since the student last saw the word
 the number of times the student has answered the word incorrectly
 the number of times the student has answered the word correctly
 

fi the longest streak of incorrect answers the student has had for the given word
 the number of times the student has answered the given word incorrectly after a
streak of the current type
 an exponentially weighted count of times the student has answered the current
word correctly  answering correctly the previous time counts for    the time
before for   before that      etc  we found       to be most effective 
 an exponentially weighted sum of the total amount of time the student has gone
between seeing the word while still getting it correct 
we tested the features using         hold out cross validation  using the area
under the roc curve as our metric  to select features for a particular streak type  we
used forward search 

 

results

we present our results in the figure      we note that for different streak lengths 
different features tend to be more predictive  for short correct or incorrect streaks 
we see that the exponentially weighted count of correct answers  as well as the longest
wrong streak  tends to be indicative  while for long correct streaks  the simple count of
total wrong answers for the word tends to be most indicative 

 

future work

in future work  wed like to try to incorporate the features we tested into a new model
for selecting times to show a word  it would also be interesting to attempt to come up
with a model that optimizes goal    the expected number of words known  it would
also be useful to collect data that is not influenced by a selection algorithm  since this
would allow us to test whether the streak length itself was a good feature to use 

 

fi 
   
   
true
positive    
   
rate
 
                 
false positive rate
 a  wrong streak of  

 
   
   
true
positive    
   
rate
 
 

 
   
   
true
positive    
   
rate
 
                 
false positive rate
 c  right streak of  

 

 
   
   
true
positive    
   
rate
 
 

 
   
   
true
positive    
   
rate
 
                 
false positive rate
 e  right streak of  

                 
false positive rate
 b  wrong streak of  

                 
false positive rate
 d  right streak of  

 

 
   
   
true
positive    
   
rate
 
 

                 
false positive rate
 f  right streak of  

 

exp time
 
   
   
true
positive    
   
rate
 
                 
false positive rate
 g  right streak of  

 

past streak

exp count

correct

wrong streak

time

wrong
 h  legend

figure      roc curves for different types of streaks

 

fi