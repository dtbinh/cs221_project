optical character recognition  classification of handwritten digits and computer fonts
george margulis  cs    final report
abstract
optical character recognition  ocr  is an important application of machine learning where an algorithm
is trained on a data set of known letters digits and can learn to accurately classify letters digits  a
variety of algorithms have shown excellent accuracy for the problem of handwritten digits    of which
are looked at here  additionally  we attempt to extend these techniques to the harder problem of
classifying various characters written in different fonts  and achieve accuracies of     and     on these
two data sets  respectively  using support vector machines  finally  we implement pca to see how the
algorithms will fare using a smaller dimensional space and find the interesting result that the more
difficult to classify data set  computer fonts  can be accurately classified using a smaller dimensional
projection space 
data sets
for this project  i used two data sets of digits  a subset of the mnist database of handwritten digits  and
the not mnist  database of various fonts images of computer writing  perhaps something that can be
found from trying to read an image   each set
contained    various characters digits      
instances of each classification and     testing
examples
for
each
letter digit 
with
representative images shown in fig     each of
the digits is a   x   pixel image  resulting in a
    dimensional space  the mnist database is
well studied and has had many algorithms
applied to it for letter classification   and serves
fig    images of representative digits and as a baseline for implementing our algorithms 
characters from the mnist  left  and notmnist the notmnist database appears to be a harder
datasets 
task  and so we want to understand why that is
so and where our algorithms fail 
algorithms implemented
knn
we implemented k nearest neighbors  where the classification of a point only depends on the closest k
points in euclidean distance  varying the values of k  the best results were obtained by using small
values of k and results are shown for k   
c svm with  th degree polynomial kernel
with the help of libsvm   we implemented c svm which attempts to solve the following optimization
problem 
s t 

fiutilizing a  th degree polynomial kernel
  once the optimum margin classifier is
known  binary classification can be performed  since we are classifying a set of    digits characters  we
separately implemented one vs  one classification  where each training setoff each character is trained
against each other training set  finally  during testing  the character is classified according to all the
optimum margin classifiers and then the one which occurs the most often is taken as the classification 
values for c and gamma were determined by getting low training error 
c svm with linear kernel
using liblinear   we also implemented an svm using a linear kernel  very similar to above  however in
this case  the kernel was a linear function  and the c term in the above equation was
  ie  l 
regularization rather than l   again  we used one vs  one method of making a multi class classifier 
l  regularized logistic regression
we used liblinear to implement logistic regression with l  regularization  where there is a term in the
maximized likelihood  we would like to maximize
w r t 
where
is a sigmoid function 

this is once again a binary classifier 

and we implemented a one vs  one extension to classify our    digits characters 
it should be pointed out that while the first   techniques lead to non linear classifiers  the   th degree
polynomial kernel maps to a high dimensional space  where it fits a linear classifier  but that boundary
will not be linear in the original space   while the second two decision boundaries are hyperplanes in the
original space of the characters  the set of handwritten characters is in an extremely high dimensional
space      pixels   but is without a doubt  much lower dimensional  in addition  this space is highly nonlinear  one might imagine that space of characters are a k dimensional manifold mapped into rn  where
n in this case is      and k is much much less than n 
results of classifiers
the results of our   techniques are shown below in table    with our polynomial svm outperforming the
other techniques by a reasonable margin  additionally  knn performed the second best  which may be
rather surprising due to the extremely unsophisticated nature of the technique  however  it is
interesting that the
error
algorithm
classification
error classification
best techniques are
mnist
mnist
ones
where
the
logistic regression
    
     
classifier boundary is
linear svm
     
     
not linear in the
svm   poly
    
    
original space of the
knn    
    
     
characters  this might
table    classification error
be
expected  and

fipoints to the fact that the data set is highly non linear  however  it is interesting that the gap between
the non linear techniques and linear techniques seems to go down a bit for the notmnist dataset 
despite the fact that is more challenging to classify 
it should be pointed out that current state of the art ocr techniques are able to get sub   
classification error on the full       training example mnist database  our best technique had   
error on a smaller subset of the data set  on the full dataset  we were able to get errors below    
although classification took so long that we were unable to repeat it for our final best parameters c and
  however  our goal was to use these techniques to classify and understand the datasets rather than
purely get the best performance  additionally  this performance is fairly close to the best reported
performances on the mnist dataset using these techniques  which suggests that our performance on
the notmnist dataset is pretty good  for these  
techniques  
dimensionality reduction  pca 
while classification using the values of all the pixels
gives good results  the extra dimensionality can lead
to random noise which will make our
measurement less accurate 
additionally  by
lowering the number of dimensions of the data  we
are able to run our classification algorithms much
faster and on larger data sets  a technique that can
reduce dimensionality is principle component
analysis  or pca  where the data is projected on its k
principle components  or k directions of largest
variance  in addition  pca is a good tool for

fig    projection of  s and  s from the mnist
data set onto first   principle components  x
and y axes 
visualizing high dimensional data  we show
the projection of a subset of the mnist
data set on the first two principle
components of our full training set in fig    
and we see the separation between  s and
 s in the dataset 

fig    relative size of the projection onto the first  
principle components for the mnist and notmnist data
sets 

we performed pca on      training
examples from each of the datasets and the
magnitude of the first    principle
components is shown below in figure   
interestingly enough  we see that the
relative size of the first few principle
components for the mnist data set is
smaller than the size of the notmnist
projection onto the first few principle

ficomponents  this is particularly interesting because while our results show that the notmnist data set
is more complex  and harder to classify  it actually can be described better by a projection onto fewer
components  this is intriguing  as it suggests that the notmnist data is less non linear and that
projecting onto a small dimensional space  once can still get accurate classification  upon thinking about
this further  this seems to make intuitive sense  handwritten characters have interesting nonlinear
embellishment such as additional loops  or curvature  while there is more variation in fonts  there are
more linear transformations  such as widening a character and less variation around the border 
using this intuition  we decided to
see if that meant we can get high
classification accuracy was still high
using a smaller number of principle
components  using the same  
algorithms  but with a smaller
number of training examples and
testing examples       and     
respectively  for speed   we
obtained
the
following
classification accuracy vs  number
of principle components used  fig 
    as would be suggested by our
pca  we find that the classification
accuracy drops off quickly below
about    principle components for
the mnist data set and about   
components for the notmnist data
set  this seems to be consistent
with our interpretation that the
notmnist data set is more linear 
which allows pca to be more
effective  additionally  this might
be the reason that the linear
classifier techniques used are
closer to the non linear techniques
on this data set  it is pretty
interesting that despite the
fig    classification accuracy of the   algorithms as a function of  
notmnist data being harder to
of principle components projected upon  top graph is for the
classify  it appears to exist in a
mnist data set  bottom is for the notmnist dataset 
smaller dimension euclidean space 
i would guess this to be interpreted that there must be more variance in the various fonts of the
notmnist data set as compared to the standard mnist dataset 

fiimprovements additional work
there can be a variety of improvements on the work to investigate the notmnist dataset
further  there has been a wealth of algorithms implemented for ocr   with varying degrees of success 
up to classification errors of less than     neural networks  various other svms with different kernels  it
would be interesting to see how these various algorithms fare on the notmnist dataset  also  nonlinear dimensional reduction using algorithms like isomap   lle  or nonlinear extensions of pca can be
used to project the data onto a nonlinear subspace  in order to understand the intrinsic dimension of
the data  from here we might be able to further understand why the classification of the notmnist
data set is a hard problem and figure out algorithms that allow for better classification accuracy 
conclusion
we have applied a variety of algorithms to digit character classification using   dataset  one of
handwritten characters  mnist  and one of a variety of fonts  notmnist   both of these are interesting
for the problem of ocr  where one might try to read a handwritten letter or address  or read the font
on a banner image in a website  svms were able to achieve the best accuracy on both of the data sets 
and this was attributed to the ability to handle nonlinearity  however  pca showed that the linear
component of the notmnist dataset was actually less than that of the mnist and we were able to
achieve high classification accuracy with slightly less principle components  which was a bit interesting
considering the difficulty of classifying this data  which i thought it was interesting tidbit of insight 
references
 
mnist digit database of handwritten digits  yann lecun  corrina cortes 
http   yann lecun com exdb mnist 
 
the notmnist data set  yaroslav bulatov  http   yaroslavvb blogspot com         notmnistdataset html
 
y  lecun  l  bottou  y  bengio  and p  haffner   gradient based learning applied to document
recognition   proceedings of the ieee                    november     
 
chih chung chang and chih jen lin  libsvm   a library for support vector machines  acm transactions
on intelligent systems and technology                       software available at
http   www csie ntu edu tw  cjlin libsvm
 
r  e  fan  k  w  chang  c  j  hsieh  x  r  wang  and c  j  lin  liblinear  a library for large linear
classification  journal of machine learning research                     software available at
http   www csie ntu edu tw  cjlin liblinear
 
j  b  tenenbaum  v  de silva and j  c  langford  a global geometric framework for nonlinear
dimensionality reduction  science                      
 
sam roweis  and lawrence saul  nonlinear dimensionality reduction by locally linear embedding 
science                        

fi