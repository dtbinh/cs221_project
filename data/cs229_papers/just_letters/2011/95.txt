sign language classification using webcam images
ruslan kurdyumov  phillip ho  justin ng
december         
abstract
immediate feedback on sign language gestures can greatly improve sign language education  we seek to classify the
english sign language alphabet using webcam images  we normalize and scale the images to    x   px  and use the
binary pixels as features  we can successfully classify up to         of the signs using a linear or gaussian kernel
svm  outperforming k nearest neighbor classification by about      using a custom set of features we developed  we
can successfully classify         of the signs with an svm 

 

introduction

upward to make hand gestures  but this is a feature we hope
to include in the future  we tackle these challenges in postprocessing  explained in a later section  of the image captures 
current solutions incorporate a variety of machine learning techniques to classify hands  using a pyramid of histogram of oriented gradients as a feature for a svm with
      cross validation  knight is able to distinguish between
a hand and non hand      shariff applies a combination of   d 
shape based and size based features to recognize the configuration of the hand in the scene      tracking data gathered
over time with a  d camera  marx devises a svm based system to classify between one finger  two fingers  a closed fist 
and open palm gestures      hidden markov models  commonly used in handwriting recognition  achieve a      accuracy in classifying    words in american sign language
    

implementing techniques in machine learning and image processing  we hope to obtain a high level of accuracy in distinguishing between the letters in the english sign language
alphabet  our system receives input from webcam images and
classifies them based on features defined by post processing
the images 
the primary application for sign language recognition is
to improve sign language education  a person who wants to
learn and practice sign language today does so by hiring a
sign language instructor or watching instructional videos online while practicing in front of a mirror  the former method
can be expensive and inconvenient for people with busy schedules and the latter does not provide the user with necessary
feedback on correctness 
our recognition system can be implemented into a desktop
or browser application that can provide immediate feedback
to a users hand gesture  such a tool would be inexpensive
and convenient  requiring the user to practice in front of their
webcam rather than attending expensive sessions with an instructor  unlike the mirror  the system can also provide instant feedback on the correctness of the hand gesture 
the alphabet were classifying introduces several interesting challenges  first  many letters  e g  a  m  n  s  and t  are
represented as very subtle nuances of a clenched fist  since we
use the silhouette of the hand shape to classift  these letters
are likely to be commonly misclassified  second  the letter j
and z are signaled by motion in the hand  which cannot be
represented in a single image  the starting pose of the letter
j is identical to the letter i  so we removed this letter from our
data set  the letter z  which starts in a distinct pose with the
pointer finger  is represented by its starting pose 
our choice of using webcam images as our input format also creates some difficulty in correctly classifying hand
shapes  depending on the quality of the webcam  captured
image data typically contains noise from compression  since
we cannot constrain where the users hand will be in relation
to the camera  our recognition system needs to be invariant
to scale and position  rotational invariance is less important
because the users arm will typically be pointing vertically

 

data collection

figure    a sampled set of image processed training data 

our dataset comprises over      webcam images of hand
gestures  see figure     the images were taken from a standard laptop webcam at     x    px resolution  using a custom matlab script to expedite the process  each team member took       images of his right hand signing each of the   
letters  skipping the letter j   while capturing images  the
hand gestures were moved or rotated slightly to avoid taking
 

fisets of images that were too similar  by introducing some
variance into the dataset  we help ensure that any new data
we test on does not need to look exactly like the training data
in order to be correctly classified 
a single image of the background without a hand in the
foreground is also taken for both the training and test data 
this background plate allows our system to extract the foreground from the images much more easily by performing background subtraction  which we describe later 

the next step is to get the silhouette of the hand  the
image is binarized by setting all non black pixels to white    
and leaving the remaining pixels as black      we remove
holes and remove all but the largest connected components
to get a single  solid silhouette of the hand 

we normalize the translation and scale by relying on some
assumptions about the wrist  first  we locate the wrist by
starting from the bottom most row  assuming the hand protrudes from the bottom edge of the image  and checking the
width of the arm in each row until the width reaches its minimum  which is assumed to be the wrist location  the hand
  image processing
is repositioned such that the wrist is centered at the bottom
of the image and padded by a factor dependent on the wrist
we apply image processing to better extract features from our width to fit into a square  the final image is resized into a   
input images  in particular  the features that we extract from x   px image  forming a     member feature vector  the full
our images should be invariant to background data  transla  effect of processing can be seen in figure   
tion  scale  and lighting condition 

 

approach

our overall approach to the classification problem is summarized in figure   

figure    image processing timeline  we start with a raw image
 a   subtract the background  b   binarize  c   remove small white
and black holes  d   remove white patches and crop the hand  e  
cut out the wrist  f   normalize the hand size  g  

first  images are converted to grayscale  while this makes
other operations much more simple and quicker to compute 
we lose color information from the original images  since our
system needs to be lighting invariant  the color information
should actually be ignored because we cannot rely on the coloration of the hand to be consistent between test and training
image data 
since the background data is not relevant and should not
be trained nor tested against  we first remove the background
from the foreground  by subtracting the background image
from an input image  we find the intensity change of each
pixel  intensity changes above some threshold are classified
as foreground pixels and the remaining pixels are background
pixels  the threshold was set to ensure that hand pixels would
not be subtracted out 

figure    the classification process 

after obtaining our full data set  we shuffle the data and
split into         train test  which still leaves us with over
    test examples  since its not obvious which image size will
provide the most distinguishing features  we varied the resized
image dimension and examined the    nearest neighbor test
error  we find that a    x   px image is optimal  and these
results are summarized in the cross validation subsection 
once we have an optimal image size  we run    fold crossvalidation on the train examples to determine optimal svm
parameters c and   for the gaussian kernel   given an optimal set of parameters  we then train on the full training set 
forming a training model  and test on the test set 
 

fi 

results

   

gaussian kernel cross validation log search

pixel features

  
    

image scaling

    
accuracy    

     

to determine the optimal image dimensions to use for classificiation  we ran k nearest neighbor  with k       classification
on our full test set with pixel dimensions                    
we chose to use k nearest neighbor classification rather than
svm since using an svm would require tuning model parameters for each pixel dimension  significantly complicating the
cross validation  the results can be seen in figure   

    
    
  
    

    
     
    

   
   

     
   

    

   
   

     
gamma

  
    

 

c

test error vs  image dimension using   nearest neighbor classification
  

figure    the cross validation accuracy for a range of gaussian
kernel svm parameters 

  

test error    

  

  

     

  

the summary of the classification results can be seen in table   

 

 

classification results

 

  

  

  
  
image width   height  pixels 

  

table    pixel feature classification results

  

classifier

figure    image resize dimension test error using    nearest
neighbor classification 

c v  accuracy

test accuracy

  
signs

  
signs

  
signs

  
signs

linear
                       
we find that    pixels is not enough to robustly classify
kernel
among the signs  as expected  when scaling the image down
gaussian
                       
that severely  many distinguishing features are lost     and
kernel
   pixel image dimensions have improved performance  error
kn a
n a
           
around      and the    pixel image dimension has the best
nearestperformance  we chose to use    x    pixel images because
neighbor
enough distinguishing features should remain and we can filter
out some noise by scaling down to that size  in addition  with
the gaussian kernel slightly outperforms the linear kernel
a training set of around      images  we could ensure that our svm when testing the full    sign set  but they are both      
number of features       was still significantly smaller than accurate on the condensed    sign set  they both outperform
the training set size 
the baseline k nearest neighbor classifier  with k        our
test error is actually slightly lower than the cross validation
error  which seems somewhat anomalous  the likely explanation is that when we randomly shuffled our data and split
      cross validation
into training and test sets  the training set happened to get a
after creating the feature vectors for all of our test and train disproportionate amount of poor examples 
examples  we ran    fold cross validation to determine the optimal soft margin parameter c and the gaussian parameter
  for the gaussian kernel   we used a log  space search  in
either   or   dimensions  and picked the parameter  or parameter pair  with the highest cross validation accuracy  in       learning curve
general  we first run a coarse search  and if necessary  a fine
search  for example  the search for optimal c   parameters to get an idea of how well our algorithm was learning  we
is shown in figure   
plotted the learning curve  seen in figure   
 

fiby the fingers  so the position of the fingers in the hand gives
a unique spectral signature which can be used to recognize
the sign 
the classification results using our custom features can be
seen in table   

training and test error vs  training set size  linear svm 
  
training error
test error
  

error    

  

table    custom feature classification results

  

c v  accuracy

test accuracy

classifier

  
signs

  
signs

  
signs

  
signs

linear
kernel
gaussian
kernel
knearestneighbor

     

     

     

     

     

     

     

     

n a

n a

     

     

  

  

 

 

 

   

   

   
   
    
number of training samples

    

    

    

figure    learning curve  the test error appears to be asymptotically decreasing  suggesting a high variance situation 

with our custom features  the gaussian kernel is actually less effective than the linear kernel  suggesting that some
overfitting is taking place in the higher dimensional feature
space  we have a higher test accuracy than c v  accuracy for
the linear kernel  but not for the gaussian kernel  which is
difficult to explain  the linear kernel svm outperforms the
   nearest neighbor classifer for both sign sets 

the plot shows an asymptotic decrease in the test error 
coupled with a slow increase in the training error as the training set size increases  it appears that we have a high variance
situation   lots of features and not enough training examples 
since our train error is fairly low  high bias is not a problem 

   

custom features

to compare the performance of our pixel based feature vector  we also created a    feature vector from the images  to
collect the features  we used the normalized  but not resized 
image  as in figure    g   except with the hand pixels in
grayscale  three features we collected were the relative area 
height and width of the normalized hand  we also determined the width of the top of the hand and the gap in the
top the hand  these two features could be useful in telling
us how many fingers that the user was holding up  we also
obtained features by running fourier transforms on various
rows in the image and saving the magnitude at an optimal
frequency  running the fourier transforms provides a rough
alternative to edge detection  the edges are primarily caused

 

further work

there are several key extensions of our work  while our
method works well on webcam images  ideally  we would like
to classify live webcam video of sign language  this would require a different image processing scheme to isolate the hand
in the image  in particular  our background subtraction algorithm would have to be modified  since the background would
be dynamic  an alternate approach would be to use scale invariant features derived from webcam stream images  derived
from an algorithm such as sift      which would be less sensitive to background noise and rotations of the hand 

references
    knight  d   tang  m   dahlkamp  h   and plagemann  c  a framework for recognizing hand gestures  cs    final
project paper       
    shariff  s  and kulkarni  a  identifying hand configurations with low resolution depth sensor data  cs    final
project paper       
    marx  m   fenton  m   and hills  g  recognizing hand gestures with a  d camera  cs    final project paper 
    starner  t  and pentland  a  real time american sign language recognition using desk and wearable computer
based video  ieee transactions on pattern analysis and machine intelligence vol      no      december     
    lowe  d g  object recognition from local scale invariant features  the proceedings of the seventh ieee international
conference on computer vision       vol   

 

fi