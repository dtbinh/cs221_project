cs    final report  unsupervised learning of temporally
coherent features for action recognition
zhiheng huang  robin jia  and kris sankaran

introduction
human vision is remarkably well adapted to object and action recognition tasks  recent literature suggests that at least some of the brains abilities to process visual cues can be explained by
temporal coherence  the principle that signals closer together in time are more similar than those
that are not  this idea arises from the observation that while models of vision used in machine
learning often use still images as training data  humans learn to recognize objects based on a continuous stream of visual input  this continuous stream encodes not only the appearance of an object
at a given moment in time  but also the frequencies of various types of transformations  importantly  the frequency of a transformation correlates negatively with how important it is for object
recognition  this fact may contribute to the formation of invariance in the human visual system
     in this project  we extend existing strategies for object recognition with temporal coherence to
the task of action recognition in video files  we try to learn  in a completely unsupervised manner 
filters that respond differently to different types of actions but are invariant to irrelevant transformations  we evaluate performance of our approach using the benchmark kth action recognition
data set  obtaining        classification accuracy  further  we experiment with motion detection
and interactive response map applications  we conclude this document with potential directions
for developing this project 
sparse linear auto encoder algorithm
to learn features in video data  we construct a sparse linear auto encoder  sla  corresponding
to the first layer network outlined in      as shown in figure    we train the network on small
video cubes to obtain weights that connect visible units and hidden units  these weights can be
viewed as filters that we can later use to generate features  when building these filters  we pool
pairs of hidden units so that they form quadrature pairs  together  such pairs allow for superior
features and invariance  for the t th training example x t    the response of the hidden units is given
by h t    w x t  and the response of the output units is given by y  t    w t w x t    where w is
the matrix of weights connecting visible units and hidden
units  if h is our pooling matrix  then
p
the activation of the pooling units is given by p t    h w x t       where we apply the square root
and squaring operators element wise  given this formulation and n training examples  one way to
construct a standard sla is to minimize the objective function    
j 

n
x

kx t   w t w x t  k     

t  

n
x

kp t  k 

   

t  

where the first term minimizes reconstruction error and the second term promotes sparsity  however 
we wish to learn features that use information regarding temporal coherence  in particular  we want
date             
acknowledgements  the authors are grateful for the guidance of will zou in giving us the idea of this project 
theoretical instruction as well as his invaluable advice for us to identify problems and get the algorithm work 
 

fifigure    a graphical representation of our sla  the links between the input and
the hidden layer correspond to multiplication by w  
our hidden units to fire in a way that they retain the features of the data that change least over
time  this way  they will be invariant to transformations of the data that disrupt the continuity of
the scene being shown  suppose that the n training examples are n sequential video blocks taken
of some scene or object  perhaps in motion  we can concoct a new objective function that also
penalizes differences in activation between sequential video blocks  more precisely  we let
j 

n
x
t  

kx t   w t w x t  k     

n
x
t  

kp t  k    

n
 
x

kp t   p t    k   

   

t  

we optimize this objective with lbfgs  we see that we wind up with edge detectors that pair
up  due to pooling  as can be seen in figure   a   an example visualization of the threedimensional learned features  including moving edge detectors  can be viewed at http   stanford 
edu  kriss  weightmatrices mp   when preprocessing  we first scale the data to         subtract
out means and then whiten using pca  we also tuned our parameters  and  to balance our
emphasis on the reconstruction error  sparsity  and temporal coherence 
visualizing learned filters and generating response maps
once we have generated the weight matrices for the hidden units in our auto encoder  we can
generate response maps via convolution  each of the learned filters can be convolved with a video
cube to generate a response map  when applied to a series of video frames  these response maps
can then be used for motion detection or action recognition tasks  a screenshot of several response
maps  along with their corresponding filters  is displayed in figure   b   the complete video can
be viewed online at http   stanford edu  kriss  walkingresponsemap mp  
experiments  motion detection and interactive response map generation
in addition to using our learned features to develop a classifier for action recognition  we experiment with using our features for motion detection and with generating response maps in real
time  in the motion detection experiment  we first locate video blocks whose activation vectors
have relatively large l  norms with non overlapped sliding window  we then calculate a centroid for all these blocks  with their x  y coordinates weighted by l  norms of activation vectors  further  we adjust aspect ratio of the bounding box according to variances of the x and
y coordinates  a clip of the motion detector applied to several kth video clips is available
at http   stanford edu  kriss  motion detection attention mp   note also that we have
used motion detection to selectively dense sample motion intensive areas in our action classification
pipeline described below 
a second experiment was designed to interactively generate response maps  we implemented a
system that takes a video stream from a webcam and uses an already learned set of filters to create
response maps in real time  we thus have a means of applying our algorithm to our own video
 

fi a  visualization of filters learned using the sla 

 b  screenshot from video of response maps 

figure    filter visualization and response maps
data in real time  which could be useful for debugging and visualization purposes  a video of this
application being used is available at http   stanford edu  kriss  interactivemovie mov 
experiments  classification on benchmark kth data
to evaluate the effectiveness of this approach  we employed our learned features to classify actions
in the kth video data seta standard benchmark test for action recognition algorithms  the kth
data set contains      clips depicting   types of actions  running  walking  jogging  hand waving 
hand clapping and boxing     different people perform each of these actions  all against plain
backgrounds  we follow the procedure proposed in     to obtain training and testing data  the
classification pipeline we implemented includes five parts  data sampling  sla feature extraction 
k means clustering of sla featues  histogram feature extraction  and svm classification using
histogram features  as shown in figure    in essence  we use a bag of words model to avoid
processing the astronomical number of sla features directly 
sampling and sla feature extraction  sla feature vectors are formed by feeding video
data to the sla network and collecting activation values from all the pooling units  to obtain
input data for the network  we employ a sliding window approach that takes relatively large stride
to sample video cubes  as we have access to only limited computational resources  to further
boost performance  we also developed a scheme that densely samples only in areas with the most
significant sla activation  i e  interest points with activation vectors that have relatively large
l   norm  in practice  this method has not brought significant improvement due to reasons that
warrant further exploration 
sla feature clustering and histogramization  we use k means clustering to discover centroids that may be representative of all extracted sla features  as k means can become trapped
in local minima  we may run the algorithm several times with different initializations and use cross
validation to pick a good set of centroids  the centroids are then used as the vocabulary for a
bag of words model  where we bin all the sla features from each video clip to create a histogram
feature for it  different schemes could be employed for the histogramization process  in our implementation  for each sla feature  we increase the counting value of its corresponding bin by the
features l   norm 
 

fifigure    this is an overview of our classification pipeline for working with action
recognition data 
support vector machine classification  the last step in our pipeline is to classify video clips
using histogram features the classifier we employ is libsvm  which essentially takes a one versus all
scheme for multi class classification  following      we use chi square kernel 
results
we are able to obtain an average of        accuracy in the kth classification task using     
centroids for k means clustering  the filters we used are trained on            video cubes 
with parameters                 hidden units and pooling size    we apply pca to reduce the
dimension of input data to      retaining roughly     of variance  see figure   a  for visualization
of the filters  also note that we do not train our filters on the kth dataset but on a different dataset
provided by will zou  we might examine in the future how training directly on kth dataset will
impact performance  in classification  we sample with a spatial stride of   and temporal stride   
generally using smaller stride would improve performance 
table   a  compares our performance with published results using the same data set  note
that the state of art accuracy in     is        which uses a similar sla framework  but without
temporal coherence  this does not mean that temporal coherence hurts performance  as     employs
a second layer network  multi scale sampling  and possibly more k means iterations  we were not
able to implement these features due to limited time and computational resources  nevertheless 
we compared our performance with a reduced version of     that omits the second layer and multiscale sampling  has a vocabulary size of       and uses one k means iteration  we also retrained
their filters using our training data  we repeated the the experiment for   times  the best and
average accuracy can be found in table   a   these results suggest that temporal coherence may
help  although more convincing evidence would require beating state of art performance  besides 
table   b  explores the impact of different parameter settings on performance  lastly  we note
that k means sensitivity to local minima can have an approximately    influence on the result 
error analysis
we analyzed the examples that our algorithm misclassifies  a large number of errors come from
misclassifying similar looking actions  e g  hand waving  clapping  and boxing  jogging and running   in some cases  such as hand waving and hand clapping  the key to differentiating between
actions depends on small details  such as whether the hands touch each other  in other cases  like
running and jogging  the difference might be in how fast the person is moving  our classifier also
performs somewhat poorly when significantly different looking videos belong in the same classification group  for example  our algorithm is good at recognizing that a person is walking if he she
is moving horizontally  but is prone to errors when the person walks in a diagonal direction 
future directions
sla training data selection  we hypothesize that we might be able to reduce error by selecting
better training data for our sla  ideally  our data would be chosen to produce a balanced number
of low frequency and high frequency filters  or we could train our sla filter on the kth dataset
for this particular classification task 
 

fitable    performance
 a  average accuracy by various algorithms on kth dataset

algorithm
harris d   hog hof
harris d   hof
cuboids   hog d
dense   hof
hessian   esurf
hmax
 d cnn
plsa
grbm
q  le et  al      method with dense sampling
q  le et  al      method with norm thresholding
our method  vocabulary size      
our method  vocabulary size      
reduced q  le et  al      method  vocabulary size
     

average accuracy
     
     
     
     
     
     
     
     
     
     
     
        best        
        best        
        best        

 b  performance using different parameter settings

k means iteration      
k means centroid   average accuracy
    
      
    
      
     
      

k means centroid         
k means iteration   average accuracy
 
      
 
      
 
      

multi scale classification  to better model the zoom in and zoom out effect as well as the
changing size of moving objects in different clips  we might try to set up multi scale sampling for
our pipeline 
second layer network  following the example of      we would like to add a second layer to
our network  we plan to construct a network similar to that of our sparse auto encoder described
above  but with some non linearity  this could be achieved by utilizing the pooling output of
first layer or using sigmoid function as activation function  hopefully this layer would be able to
aggregate information collected by the first layer filters and render a cumulative perspective on
invariances in the data  in fact  we already have a preliminary implementation of this layer  though
we have not yet had time to obtain reasonably looking filters with it 
references
    einhauser  w   hipp  j   eggert  j  korner  e   and konig  p  learning viewpoint invariant object representations using a temporal coherence principle  biological cybernetics     no     may               http 
  www springerlink org
    hateren  j h  van and schaaf  a  van der  independent component filters of natural images compared to the
primary visual cortex  proceedings  biological sciences      no        march                
    wang  h   ullah  m  m   klser  a   laptev  i   schmid  c  evaluation of local spatio temporal features for action
recognition  in bmvc      
    le  q   zou  w   yeung  s   ng  a  learning hierarchical spatio temporal features for action recognition with
independent subspace analysis  cvpr      
 

fi