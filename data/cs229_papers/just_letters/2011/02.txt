tactical and strategic game play in doppelkopf
daniel templeton

   abstract
the german card game of doppelkopf is a complex game that involves both individual and team play and requires use of strategic
and tactical reasoning  making it a challenging target for a computer solver  building on previous work done with other related
games  this paper is a survey of the viability of building a capable
and efficient game solver for the game of doppelkopf 
   introduction
throughout human history  games have served an important role  allowing real life problems to be abstracted into a simplified environment where they can be explored and understood  today  games continue to serve that role and are useful in a variety of fields of
research and study  including machine learning and artificial intelligence  by researching
ways to enable computers to solve the abstracted  stylized problems represented by games 
researchers are creating solutions that can be applied directly to real world problems 
     doppelkopf  doppelkopf is a game in the same family as schafkopf and skat played
mostly in northern areas of germany  the rules are officially defined by the deutscher
doppelkopf verband      but optional rules and local variants abound  the game is played
with a pinochle deck  which includes two each of the nines  tens  jacks  queens  kings  and
aces of all four suits  for a total of    cards  as in many games  like skat  schafkopf  spades 
bridge  etc   the general goal is to win points by taking tricks  with each trick going to
the highest card  trump or non trump  played  as in schafkopf  the highest trump card
is determined not only by strict rank  but also by suit  e g  the jack of clubs is a higher
trump than the jack of spades  in addition to points won through taking tricks  a second
tier point system rewards taking tricks in specific situations  often governed by optional
rules 
the complexity of the game is further multiplied by partnering and soloing rules  in
a regular  non solo game   the four players are arranged in teams of two based on which
players hold the two queens of clubs  although team membership is not revealed  discovery
date     december            pst 
 

fi 

daniel templeton

of the team arrangements happens either through gameplay  when a player plays a queen
of clubs  or through calling  which amounts to accepting an increased scoring burden
in order to declare team affiliation   special rules exist for the case when one player
holds both queens of clubs  called a hochzeit or wedding   until team membership has
been definitively revealed  the game is effectively four players playing against each other 
once the team arrangements are known  proper team play commences  in some cases an
in between state may exist where only one players team membership is known  offering
its own unique gameplay dynamics  its also worth noting that a player not holding a
queen of clubs has no way to definitely signal his or her team membership other than by
calling 
at the beginning of a hand  any player may opt to play a solo  meaning that the usual
partnering rules are discarded  and that one player plays as a team of one against the
remaining players as a team of three using a soloist selected alternate trump arrangement 
in an official tournament game  each player plays four non solo hands and at least one solo
hand 
     related research  the complexity and regional popularity of doppelkopf have presumably kept it from being a subject of mainstream machine learning research  a handful
of desktop or online doppelkopf games are available on the market that include doppelkopf
ais  taking the freedoko ai as a representative example  those ais make use of well understood algorithms  such as decision trees and game heuristics  related research in similar
card games and large state model imperfect information games does provide a foundation
for approaching this problem space from the perspective of machine learning 
tsitsiklis and van roy     and baird     lay important groundwork proving the soundness of
applying reinforcement learning to problems with very large state spaces via approximation
functions  sturtevant and white     present an algorithm for playing hearts that makes
use of reinforcement learning with a perceptron as the approximation function  using the
maxn algorithm from luckhardt and irani     rather than reducing the state space through
feature approximation  sturtevant and white train against an extremely high dimensional
space composed of boolean combinations of atomic features  with good results  koller
and pfeffer     take a different approach to managing the large state space by reducing
the complexity of the problem by restructuring the entire problem space around transition
paths rather than states  yet another approach  taken by buro et al      is to leverage the
sparse state model by translating states into indexes into a lookup table 

   implementation approach
for this paper  the decision was made to apply reinforcement learning to a limited variant
of doppelkopf  in this doppelkopf variant  solos are not allowed  and weddings result in a
redeal  other complex special cases of non solo games  such as armut  or poverty  also

fitactical and strategic game play in doppelkopf

 

result in a redeal  calling is allowed only during the first trick  and its effect on scoring is
ignored 
because the state space for doppelkopf is untenably large  as is the case with most other
card games  an approach such as a vanilla markov decision process  that attempts to have
complete knowledge of the system could not be used  instead  an approximation function
is required to estimate the values represented by the states in the model  for this paper 
the function used was simple linear regression 
as an avid doppelkopf player  i was able to apply my understanding of game play to
developing an approximate feature set that models the relevant details at any state in
game  by reducing the specific cards to an approximate feature set  not only is the size of
the state model reduced  but a level of game intuition is built into the learning algorithm 
the final set of features included aspects of each players hand  such as number of trump
or number of aces  aspects of the cards already played in the trick  and aspects of the cards
that are held by the other players 
for this paper  it was decided that the game of doppelkopf would be approached as an
imperfect information game  where each player only sees the cards in his hand and the
cards played in the current and previous tricks  a side effect of this approach is that
transitions in the state model become highly nondeterministic  for a game state s  s
with player p set to play  the selection of a card c to be played can result in a very large
number of possible subsequent states s   s 
doppelkopf is a zero sum game  at the completion of a hand  two players will win the
point value of the hand  and two players will lose the point value of the hand   in a solo 
the soloist will win or lose three times the point value of the hand to preserve the zero
sum quality of the game   one approach to reinforcement rewards would be to only issue
rewards in the final trick  that approach tends to dilute some basic gameplay wisdom 
such as valuing winning a trick with a fox in it  instead  the approach to state rewards
taken was to award points for each trick taken with bonus points for second tier scoring
options  such as capturing or losing a fox  because partnerships may not be known until
late in the game  this reward scheme cannot be zero sum  a trick that is taken by a player
before that players partner is revealed must count as positive points only for the capturing
player 
to jumpstart the learning process  an anonymous online correspondence doppelkopf  or
doko  site has donated records of more than a thousand games played online by four
human players  because the doko site allows the customization of rules  and no single
combination of rules represents a clear majority  one of the leading optional rule sets was
selected  all games played with alternate rule sets were ignored during training  because the
training set data is from players of varying skill levels  the data is somewhat noisy  the hope
is that the data noise should be minimal and would be outweighed by the demonstration
of more subtle gameplay techniques like signaling and hunting the fox 

fi 

daniel templeton

     implementation details  the first implementation step was to ingest and process
the donated training data set  provided as a sql dump of roughly      games  the data
needed to be converted into a format amenable to the reinforcement learning algorithm  a
process then had to be created to replay each game  card by card  tracking the full game
details so that they could be used in training the approximation function 
after a game is played to completion  the game is then played back in reverse  allowing the
expected values of each state along the way to be calculated and recorded in a straightforward fashion  because of the choice to handle doppelkopf as an imperfect information
game  the state model is constructed based only on the data visible to a single player  i e
that players cards and the cards thus far played by all players  in effect  each game in
the training data set is processed as four separate training example games  one for each
players imperfect information state model 
as each state revisited  the state features and calculated expected value are stored for use
as training data for the linear regression parameters  after the entire set of donated games
is processed  the linear regression parameters are trained against the entire data set  the
resulting parameters are make logical sense for the most part  the features that most
strongly correlate to a high value trick  from a given players perspective  are the number
of dullen  the highest trump  held  and whether the players partner is winning the trick 
both are clearly good indicators of expected success  oddly  the next highest indicator of
high value trick is the number of foxes held by the player  which is counterintuitive at
first glance  winning a trick that contains a fox played by the opposing team results in
additional second tier points for the winning team  because the training data is drawn
from games played by experienced players  the risk of losing the fox may be mitigated and
even turned to an advantage by smart and careful use of the card 
as the training set is very small compared to the state space  the linear regression parameters are only trained by the training data against a tiny percentage of the possible states 
at this stage  the algorithm is therefore a pretty poor doppelkopf player  to provide additional training data  four copies of the solver are set to play against each other in groups
of        games  producing         training data elements for each batch of games  after
each batch of games  the parameters are trained against the new data combined with the
previous data  the parameters thus trained appear to better match with the expected
relevance of the state features  the number of dullen and whether the players partner is
winning the trick are still strong indicators  but the number of foxes is a negative valued
feature  in a system that has not yet learned how to properly play such a card  it very
logically represents more of a risk than a value 
beyond the first batch of self play games  the linear regression parameters change very
little  indicating that the regression model has learned as much as it can from the data 
unfortunately  the value approximation of the linear regression is still quite poor  resulting
in a poor doppelkopf player  the algorithm remains only slightly better than a player that
selects cards at random 

fitactical and strategic game play in doppelkopf

 

given that the trained parameters seem to match logical expectations of relative magnitude
and sign for feature relevance  my theory for the poor performance of the algorithm is that
the approach is too simple for a game as complex as doppelkopf 
   conclusions and future work
while failing to produce an efficient expert level doppelkopf solver is a disappointing result  it is not at all unexpected given the complexity of the problem and the limited time
and resources available for the project work  as the opportunity presents itself to continue
this project in subsequent coursework  the next steps will be to explore significantly different feature representations for game states and to investigate the use of other variants
of reinforcement learning  such as t d    i have found this project to be exciting and
challenging and look forward to the opportunity to develop it further 
   references
    deutscher doppelkopf verein  turnierspielregeln 
http   www doko verband de download turnierspielregeln rtf       
    j  n  tsitsiklis and b  van roy  an analysis of temporal difference learning with
function approximation  in ieee transactions on automatic control  vol      no    
pp                
    l c baird residual algorithms  reinforcement learning with function approximation  in
machine learning  proceedings   th int  conf   july      prieditis and russell  eds 
san francisco  ca  morgan kaufman       
    n r sturtevant and a m white  feature construction for reinforcement learning in
hearts  in  th international conference on computers and games iccg      
    c a luckhardt and k b irani  an algorithmic solution of n person games  in aaai   proceedings      
    d koller and a pfeffer  generating and solving imperfect information games  in proceedings of the   th international joint conference on artificial intelligence  ijcai  
pp                 
    m  buro  j  r  long  t  furtak  and n  sturtevant  improving state evaluation  inference  and search in trick based card games  in proceedings of the twenty first international joint conference on artificial intelligence  ijcai          

fi