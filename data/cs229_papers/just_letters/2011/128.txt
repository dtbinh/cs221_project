structured completion predictors applied to image segmentation
dmitriy brezhnev  raphael joel lim  anirudh venkatesh
december         
abstract
multi image segmentation makes use of global and local features in an attempt to classify every pixel in an
image into a semantic region  one important relationship is inter class spacial interaction between small local
regions we call superpixels  while complex models have been built in order to provide for such semantic
understanding and define visual grammars  we explore the usefulness of a new technique we name stacking 
this method performs multi class svm learning several times by first constructing label probabilities for each
superpixel based on extremely local superpixel features  such as raw pixel information  and increases semantic
understanding through stacked predictors that augment the feature set with predictions on the surrounding
context 
the results of the algorithm with our constructed features are marginally successful  demonstrating that the
algorithm can be used to improve semantic understanding  this report analyzes the success of constructed features
and analyzes reasons for a lack of strong improvement 

 

introduction

the goal of multi image segmentation is to classify every pixel into a semantic region  unlike single object recognition
algorithms that aim to find a particular object  innovative works in the recent years have applied several strategies
for this goal      conditional markov fields  that encode probabilistic preferencs      close to medium range effects
to impose consistent layout recognized to be part of a known object      among others 
in our paper  we base our algorithm on a strategy presented by gould et  al      there  gould et al  proposes
to decompose the images into small consistent regions called superpixels  for which around      local features are
computed  which include raw rgb values and averages of surrounding pixel information  subsequently  the authors
use this superpixel decomposition to build a complex model to learn the local and global environments  however 
due to the complexity of the model  the inference becomes an expensive operation 
another approach to this problem is to use lower order models to obtain more features from context probabilities 
we use a multi class svm to get a probability distribution for surrounding pixels labelings and we capture this
environment classification to design features  this provides semantic understanding without drastically increasing
the complexity of the model  in the case of image segmentation  images of natural scenes tend to have a certain
structure that we can leverage to make good predictions  for example  the regions of images containing trees 
buildings  or mountains are more likely to be backed by sky  we examine this approach in its application to
computer vision  analyze possible features and provide analysis for how the algorithm can be improved 

 
   

approach
theoretical approach

constructing features from predictions  stacked learning  goes back to           the essence of the approach is to
infer context from a baseline model m  to construct new features for models m    m          with later models having
better data separability and increased accuracy 
the problem of image segmentation can be formulated as prediction of y                  corresponding to the
region label  given a feature vector x for a certain superpixel  to represent our algorithms steps  consider three
models  m  represents the model with the region predicted solely from pixel data  based on model m    two models
are constructed m  and m    model m  appends context features to represent the semantic local data  such as the
superpixel above being region    note that information about the surroundings is encompassed in these additional
features and not aboout the superpixel itself  m  is not realistic in practice  as we dont have the correct labelings  
we instead use the probability distributions for each region  hence the predictions of mo will be encoded in the
 

fidistributions p i     p y x i     see the figure for a graphical representation of the algorithm  after computing m 
and m    the algorithm uses the former as training and the latter for testing to produce a stacked predictor  as we
will see  this has marginally positive improvements 

   

technical approach

we first trained a baseline multiclass svm  m        on a random selection of    percent of the data set  then
for each feature vector in our dataset  we obtained from m  eight scores indicating how likely a given superpixel
belonged to some class  these scores were converted into probabilities by performing logistic regression on the set
of scores corresponding to each class  as suggested by     
to each training example feature vector we then appended features computed from the region labelings of superpixels surrounding the training example  the same features were computed and appended to the test examples by
using the probability distribution we obtained from m    a second svm  m    was then trained on the new training
examples with augmented feature vectors and tested on the new test examples with the appended features  this
yielded the performance for the first stack in our algorithm  the process can be repeated as many times as one
likes with new features appended to the feature vectors at every level of stacking to obtain mn   which takes as input
a feature vector augmented by features computed using the probability distributions obtained from m            mn   
as we will see  the merit of using more than just one stack is questionable  as the inaccuracy of earlier predictions can
accumulate and cause performance to decrease  this approach is based on the idea that while with locally constructed
features the algorithm classifies each superpixel independently from others  up to estimation by m            mn    the
algorithm can leverage patterns that are exhibited by groups of superpixels 
the motivation for using logistic regression to estimate probabilities from the svm scores is that each positive score indicates a positive prediction for the corresponding class  whereas a negative score indicates a negative
prediction  since this divides the scores into two categories  logistic regression is perfectly suited to obtaining a
probability estimate that predicts that category from a given score  however  for a given superpixel  distinctions
between svm scores that clearly indicate a ranking for the most probable region are often blurred when because of
the exponential growth of its denominator  the sigmoid function obtained from logistic regression maps these scores
to numbers very close to either   or    hence  it may be useful to investigate other methods of obtaining probability
estimates from scores  in particular  where there is a clear most probable class according to the svm scores  we
would like that distinction to be as clearly reflected in the probabilities as well   in this case it may be enough simply
to transform the probability distribution such that the aforementioned property is attained  we will later discuss
one such transformation 
true positives
to measure the success of our algorithm  we compute segmentation score  given by true positives false
positives false negatives
averaged over all classes  on     hold out test set and     training set 

   

feature construction

since outdoor scenes tend to be subdivided along horizontal lines  e g  sky above trees  trees above grass   we aimed
to capture this relationship by considering superpixels surrounding pixel label probabilities to by constructing several
features that make use of the semantic context  the most remarkable constructed features are presented in table 
table    several constructed features
name
baseline
layer above
layer below
window above
window below

   

description
m    computed using features provided in    
for each superpixel  compute the above pixels  one layer  and average the predictions for those pixels
for each superpixel  compute the below pixels  one layer  and average the probability of each region
for those pixels
same as layer above but for a varying window of      pixels
same as layer below but for a varying window of      pixels

dataset

the data set we obtained from     contains a set of roughly     images of outdoor scenes  already decomposed
into superpixels  see the attached figure for an example of superpixel decomposition of an image  for any given
image  each pixel belongs to a superpixel  and each superpixel is given one of eight semantic labeling labelings 
 sky  tree  road  grass  water  building  mountain  object  

 

fithe data set provides pre computed features for each superpixel     features computed from rgb values  and
the average and variance for each feature in  x  pixel window  several gentleboost algorithms are used to construct
a total of      features per superpixel  this data is available at     

 

results

the methods described above produced minimal improvements  but improvements nonetheless  that demonstrate
that stacking can be applied to increase semantic understanding  especially with the choice of good features 

   

feature analysis

the constructed features were simple  and hence did not show dramatic improvement in the segmentation score on
the test data  however  all tests suggest a positive improvement whenever stacking was used in comparison to the
baseline  m    see the corresponding figure for details on each constructed feature 
overall  the lack of feature improvement suggests several things  first  the constructed features  given their
simplicity and the resulting scores  are successful in capturing only a small segment of the surrounding context 
secondly  the linear multi class svm fails to capture significant separability in the data  furthermore  when m 
produces wrong probabilities  this severely undermines the predictions made by m   see sections below   from

figure    number refers to stack level  feature names refer to the window feature and its orientation above  below 
the constructed feature set  the most impressive improvement was seen in the combination window above  window
below  regardless of being implemented on the same or stacked models  this generated a segmentation score of
       half a percentage point above baseline 
we wont list all of the segmentation scores  but only the most notable ones  window above  window below
performed over several layers of stacking  see graphic 

   

scoring function analysis

as mentioned in the approach section  it seemed reasonable to investigate methods of converting the multiclass
svm scores into probabilities that would preserve a clear ranking of classes from least to most probable  to this
end  rather than performing logistic regression  we assumed that given svm similarity score s i  x  for class i and
example x 
k log p y   r x   s r  x 
where k is some constant  yielding
p y   r x   e

s r x 
k

 

under this assumption  for different values of k we could achieve distributions that appeared more sharply peaked
wherever the svm scores were higher  this conformed to the property we desired  but for k                            
our algorithm yielded exactly the same segmentation scores            which were also no different from the score
achieved when we computed the probabilities using logistic regression  the experiments listed in the table below
were performed with one stack  using the window above and window below features 
 

fitable    several constructed features
k value
k  
k     
k      
k       
k        

seg score achieved
      
       
       
       
       

the uniformity of the results lends evidence to the possibility that our features failed to increase separability
as we had hoped  in any case  the results from testing were invariant under this particular transformation of the
probability distribution 

   

class accuracy

segmentation score is computed by taking the average of the segmenation scores for each semantic region  hence
if one semantic region is classified extremely poorly  since all classes are weighted equally  the segmentation score is
pulled down 
this was particularly true for class   in which the baseline score was        this class corresponds to the label
mountain  which is heavily underrepresented in the training set  however our algorithm was able to raise this
segmentation score to       for class    by accounting for the context  additionally  foreground object accuracy
was increased  as demonstrated by the sample image below  we can see that the boats waves are represented as a
foreground object  but our algorithm picks out the hull of the boat  and correctly classifies the waves as water  the
rest of the classes were left largely unimproved 

figure    shows improvement made by stacking  first image is the original  second is the correct labeling  third is
the labeling made by the baseline model m    fourth is with one level of stacking using window above and window
below 

   

repeated stacking of one feature

incidentally  we tried stacking a feature on top of itself  but as expected  this did not increase predictor accuracy
after the first few levels of stacking  after the first level of stacking  appending the same feature in subsequent stacks
adds little new information  and differs from the same feature appended in previous stacks in that it is computed
from a probability distribution obtained from an svm trained in a later stack level  probabilities from later levels
are not necessarily more accurate  especially where m  made wrong predictions  and the graph below indicates that
the probability distributions might converge as the number of stacks approaches infinity 

 

discussion and future work

stacking with the features we designed yielded small improvements of around      in segmentation score over the
baseline model  while these improvements are small  they do suggest that we successfully managed to capture at
least some of the interaction between superpixels  a heuristic reason stacking failed to give larger improvements is
that our features were themselves too simple and did not interpret the context in a way that is linearly separable for any given superpixel  our features at best gave an estimate for the composition of the regions above and below
that superpixel  but the variability of this composition in over     images of nature might hide any pattern that an
svm could possibly learn  put simply  we largely failed to design good context features 
 

fifigure    behavior of segmentation score as number of stacked levels increases
as mentioned before  another direction for further research is determining good ways of obtaining probability
estimates given scores from multiclass svms  in order to make accurate predictions in later levels of stacking  it
is important that the probabilities of the correct context are kept comparatively high  so that accuracy of models
in previous stacks are not blurred by our a bad probabilistic interpretation of svm scores  aside from logistic
regression  we gave one possible method to estimate probabilities  but it is unclear whether this method works  since
our features might not have been good enough to make use of more accurate probabilties  and since our predictor
accuracy neither improved nor worsened with the transformation 

 

acknowledgements

we express a special thank you to daphne kollers lab and especially to huayan wang for providing initial guidance
for this project 

references
    s  gould  r  fulton  d  koller  decomposing a scene into geometric and semantically consistent regions 
proceedings of international conference on computer vision  iccv        
    http   users cecs anu edu au  sgould index html
    http   svmlight joachims org svm multiclass html
    s  gould  j  rodgers  d  cohen  g  elidan  d  koller  multi class segmentation with relative location
prior  in icml       
    j d  lafferty  a  mccallum  and f  c  n  pereira  conditional random fields  probabilistic models for segmenting and labeling sequence data  in icml       
    j  winn and j  shotton  the layout consistent random field for recognizing and segmenting partially occluded
objects  in cvpr       
    d  h  wolpert  stacked generalizations  neural networks                  
    l  breiman  stacked regressions  machine learning                
    b  zadrozny  c  elkan  transforming classifier scores into accurate multiclass probability estimates  acm 
     
     t  joachims  t  finley  and c yu  cutting plane training of strcutural svms  machine learning              
     

 

fi