characterizing densities from semi analytical
models of galaxy formation
alex ji

amir kavousian

steve lesser

statistics
email  alexji stanford edu

civil engineering
email  amirk stanford edu

computer science
email  sklesser stanford edu

i  i ntroduction
the naive way to model galaxy formation is a computationally intensive process involving large n body simulations
of hydrogen atoms interacting in a very complex manner 
this is not feasible for modeling a large number of galaxies
in a variety of environments  which is needed to compare
theoretical models of galaxy formation to empirical observations from galaxy surveys and to discern which physics
processes affect what galaxy properties  astrophysicsts have
dealt with these issues by developing semi analytical models
 sams  where the important stages of galaxy formation are
filled in with parameterized analytic recipes describing the
underlying physics  the sam takes input parameters and
outputs a population of galaxies  to summarize a population
of galaxies  astrophysicsts usually do something similar to
histogramming the various galaxy properties  for example  one
can histogram the log mass of a population of galaxies  which
is called the stellar mass function  the height of each bin is
then a summary statistic of the galaxy population   there are
standardized ways of normalizing these counts in a given bin
width  

distribution  running the parameters through the sam to create
a set of possible universes  each with their own stellar mass
function 
in this project  we have taken a sample of      z    
stellar mass functions  each with    log mass bins  and each
predicting an identical z     k band luminosity function 
these stellar mass functions populate a    dimensional space 
where each dimension is the height of the stellar mass function
in a given mass bin  our goal is to model the density of this
distribution  once we have a density  we can calculate the
probability of the true universe occuring in this model  the
ability to validate a sam in different data sets is extremely
valuable  as it can further constrain sam parameters and
suggest where a sam incorrectly models galaxy formation
physics 
ii  d imensionality r eduction
the stellar mass function  smf  data is highly correlated
in some dimensions  see figure     to reduce the dimensionality of the model and to enable better visualization of the data 
we performed principal components analysis  pca  
the results of our pca show that using only the first three
principal components  we can explain     of the variance in
smf data  see figure    
a  details of principal components analysis
the calculation is done by a singular value decomposition
 svd  of the centered and scaled data matrix  compared with
using eigenvalues of the covariance matrix  the svd method
generally offers more numerical stability and accuracy  the
svd of the n p matrix x is given by 
x   u dv t

fig     various stellar mass functions from the same semi analytical model
showing the variance of the smf despite all corresponding to the same
luminosity function 

one can use a procedure like the metropolis algorithm to
find the parameters that fit a particular histogram  also called
a data set  once we fit parameters to a particular data set  we
are interested in the predictions of the sam for another data
set  we can take the posterior distribution of sam parameters
from fitting to the first data set and sample from this posterior

where u and v are n p and pp orthogonal matrices
with the columns of u spanning the column space of x  and
the columns of v spanning the row space  d is a pp diagonal
matrix  with diagonal entries d   d        d     singular
values of x  the eigenvectors vj  columns of v   are principal
component directions of x  the first principal component
direction v  has the property that z    xv  has the largest
sample variance amongst all normalized linear combinations
of the columns of x  the subsequent principal components
are calcualted in the same way      we used the first three
principal components to fit our models as three components

fitest between training data and test data  to perform k means
clustering we treated each stellar mass function as a single  dimensional point using pca components  we then performed
standard k means clustering on a subset of all of our available
generated points  we tried various ratios of splitting up our
data into training and test sets and settled on a random    
of our points being used for training and the remaining    
used for testing  to predict density values for the test set
we clustered the test points using the trained clusters and
let the density be proportional to the number of points in a
cluster  once clusters were found we planned to fit continuous
distributions to each individual cluster 
we were interested in whether our k means clusters were
stable predictors of the probability density function  to determine this we used a chi squared test on the ratio of the size
of each cluster to the total size of the either the training set
or the test set  let k be the number of clusters  m be the test
set size  n be the training set size  mi be the fraction of test
points found in cluster i  and ni be the fraction of training
points found in cluster i
fig     histograms of stellar mass density data  and correlations between
different density distributions 

x   

k
x
 n mi  m ni   
i  

ni n m

we then took the p value of our chi squared value as a
performance metric of the k means as a good fit or bad fit
where the p value is defined as
 
p      p  x      xk 
 

fig     pca results on smf data  the eigenvalues drop quickly as more
principal components are added  as a result  using only the first three principal
components  we are able to explain     of the variability in smf data 

for each candidate cluster size k we ran k means for         iterations of random separations of training and test data 
we then histogramed the resultant p values of the chi squared
performance metric as seen in iii a  we can see there is a
wide variance in most of the p value distributions implying
that the k means clustering is highly dependent on which
points are split into the training set versus the test set  we can
see when k is    the p values reach essentially their maximum
average and have relatively few iterations with p values less
than     

explained     of the variance in our data  note  this assumes
our observed stellar mass function has variability similar to
predictions from the sam 
iii  d istribution f itting m ethods
our goal is to parameterize the probability density of our
semi analytical model  since the distribution has unclear
structure we used unsupervised learning for density estimation 
a  k means clustering
one unsupervised method we investigated was k means
clustering  we did not know the number of clusters we should
use so we tried many different cluster sizes and evaluated their
effectiveness using the p value of chi squared independence

fig     histograms of the chi squared p values for repeatedly running kmeans clustering with different numbers of clusters  cluster size starts at  
on the top left and increases to the right continuing again at the beginning of
each row  due to the high variance in p values we determined k means was
not an acceptable method for density estimation 

fihowever  even with the best k values we think k means
is not stable enough to definitely show a strong and stable
probability density model hence we do not consider it to be
appropiate as a final modeling tool 
b  kernel density estimation
in order to have a definitely accurate distribution we implemented kernel density estimation  however  this method is
slow and inefficient due to requiring the complete training
dataset to produce a single density value as opposed to
requiring a smaller number of parameters such as in mixture
of gaussians  it also generally overfits the density 
kernel density estimation places a small kernel  in our
case a multivariate gaussian  on each data point and then
computes an average probability based on existing data and
a bandwidth matrix h  let  x    x         xn   be an independent
random sample drawn from f  x   the general form of the
kernel estimator of f  x  is

kl p k q   

r

p  x   x   x   
dx  dx  dx 
p  x    x    x   log q x
   x   x   

numerically calculating the three dimensional integral with
matlab quad proved incredibly time consuming  so we
decided to use monte carlo integration with       points and
report the error  this method introduces some error because
a uniform sample of a distribution with many features may
not adequately sample all the features  however it makes the
numerical calculation tractable 

n

fh  x   
 

 x
kh  x  xi  
n i  
 

where kh  x     h    k h   x   k x  is a multivariate
gaussian function  and h is a symmetric positive definite d 
d dimensional matrix known as the bandwidth matrix  after
principal component analysis  the dimensionality of our data
is    we used a diagonal bandwidth matrix and computed the
values on the diagonal based on the assumption that data was
observed approximately sampled from a multivariate normal
density  this is a strong assumption our data does not satisfy 
but it still gives a reasonable result  the ith diagonal of h is
calculated described by    zhang    as
hi   i

fig     left  log likelihood of test set  right  log likelihood of training
set  blue line represents the mean log likelihood over     runs of mixture
of gaussians  red lines represent     one standard deviation from the mean 
green line represents maximum log likelihood  note that at k      vertical
dotted line   the log likelihood for the test set begins to flatten  so we choose
k     for our model 

   d   
 
 d     n

where i is the standard deviation of the ith variate and d
is the dimensionality of the data  the probability density can
be seen in the level contours of figure   
c  mixture of gaussians
as our third density estimation method  we decided to use
mixture of gaussians  abbreviated gm   we used the first  
principle components to fit this model  we used the matlab function gmdistribution fit to fit the mixture of
gaussians  which gives us a density function  to choose k  we
decided to compare the log likelihoods on a hold out test set
     of the data   in figure    we plot the log likelihood of
the test set and the training set  we determine the number of
clusters by finding an elbow in the likelihood  in our data  this
occurs at k     
one might expect that when you have reached an optimal k 
adding an additional gaussian component will not significantly
change the resulting distribution  to test this hypothesis  we
needed a way to compare two different distributions  we
decided to use kullback leibler divergence  kl divergence  
the kl divergence between two distributions p  x  and
q x  is defined as follows 

fig     the results of the kl divergence to compare gms with different
number of clusters  each plot shows the results of the kl divergence of one
specific k gm model  indicated on the y axis  against models with k  
                 clusters  as the plots show  the kl divergence bottoms out
around k     and stays that way for higher k  this means increasing k does
not significantly change the probability distribution and confirms k     is a
good number of clusters for our gm 

we computed gm models for k     to k      and
calculated the kl divergence between all models  plotted in
figure    note that when the gm has k     the kl divergence
is flat for k     suggesting that the probability distribution
does not change significantly after a   component model  the

fiprobability density can be seen in the level contours of figure
  
d  dirichlet process
using the dirichlet process as a prior for the number and
relative weighting of gaussians in a gaussian mixture model
allows us to have an arbitrary number of gaussians in our
prior for the distribution we want to estimate  this allows
us to avoid the process of tuning the number of clusters     
markov chain monte carlo  mcmc  methods can be used to
find this dirichlet mixture model  we used algorithm   from
neals paper     with prior distributions for new gaussians set
according to      and we quickly describe the algorithm here 
the state of the markov chain is determined by a cluster
assignment for each data point and the mean and covariance
matrix parameters for all gaussians containing a data point 
to step the markov chain  the algorithm essentially does two
things  first  it redistributes the points into different clusters
using metropolis like probabilities  with some probability 
points can be put into a randomly initialized new cluster 
second  we use a maximum likelihood estimate to update the
mean and covariance matrix of each cluster 
we encountered an underspecified part of these markov
chain algorithms  the mixture of gaussians algorithm does
fuzzy clustering  so it gives the probability of each data point
being in a given gaussian  however  all mcmc algorithms
we found for the dirichlet process gaussian mixture model
 dpgmm  used hard clustering  since the state of the markov
chain is determined by the cluster label of each point  when a
cluster has less than p      number of dimensions  points in it 
then the maximum likelihood covariance estimate is singular 
to sidestep this problem  we simply did not reestimate the
covariance matrix when there were too few points in the
cluster 
initializing the markov chain with k     and a distribution
given by a gaussian mixture fit  we ran the mcmc while
checking whether the distributions given by the last three
markov states were significantly different as determined by
kl divergence  however our results showed that our markov
chain converged whenever the monte carlo integration error
was large  rendering the results unreliable  given the time
constraints  we decided to run the monte carlo simulation
for     times and fit our model based on that  the probability
density can be seen in the level contours of figure   
iv  c onclusion and n ext steps
we have done a survey of useful methods for density
estimation  we investigated a simple k means model  a kernel
density estimation model  a gaussian mixture model  and a
dirichlet process mixture model 
for this particular data  we found that a mixture of gaussians model with k     works fairly well  slightly increasing
k does not significantly change the distribution  as seen by
kl divergence  additionally  the dirichlet process gaussian
mixture model provides an automatic way of determining k 
we are fairly confident that our gaussian mixture with k    

accurately estimates the density  and we can compute values
from the density very quickly 
now that we have a density  the next step is to consider
the observed stellar mass function from our universe to this
probability density  we can find the magnitude of the density
for the observed universe  and in future work we will find a
way to interpret that number to constrain the underlying semianalytical model 
thank you to richard socher for his advice on exploring
dirichlet processes for density estimation  dr  yu lu for providing the data and useful discussions  and the rest of the cs
    course staff for their general advice and encouragement 
r eferences
    x  zhang  m  king  r  hyndman  bandwidth selection for multivariate
kernel density estimation using mcmc       
    c  rasmussen  the infinite gaussian mixture model  advances in neural
information processing systems     mit press       
    y  teh  dirichlet process  encyclopedia of machine learning  springer 
     
    r  neal  markov chain sampling methods for dirichlet process mixture
models  technical report no        department of statistics  university
of toronto       
    t  hastie  r  tibshirani  robert  the elements of statistical learning 
springer       
    y  lu  h  mo  m  weinberg  n  katz  a bayesian approach to the semianalytic model of galaxy formation  methodology  monthly notices of
the royal astronomical society           

fifig     level contours of variuos density functions including from top to bottom  kernel density estimation  mixture of gaussians with k      and dirichlet
process  point colors represent different gaussian components  note the similarity to kernel density estimation with both mixture of gaussians and dirichlet
process despite them containing many fewer parameters and being much more efficient 

fi