predicting probability of loan default
stanford university  cs    project report
jitendra nath pandey  maheshwaran srinivasan
          
abstract  extending credit to individuals is necessary for markets and societies to function smoothly 
estimating the probability that an individual would default on his her loan  is useful for banks to decide
whether to sanction a loan to the individual and is also useful for borrowers to make better financial
decisions  in this project  we used supervised learning to estimate the probability of loan default for
individuals  training and test data were drawn from a competition on kaggle      the metric used to
judge the efficiency of a solution was the auc  area under the roc curve  calculated on probabilities of
default for the test data  we tried logistic regression  probit regression  support vector machines
 svm  using an rbf  radial basis function  kernel  classification trees and finally random forests 
we obtained the best result  auc score of           with random forests regression  using oversampling to mitigate class     imbalance and using carefully chosen coded values for missing values 

   introduction  our goal was to estimate the probability that a person applying for a loan will
experience    days past due on loan payments or delinquency or an even more serious financial crisis 
the training data  drawn from the competition give me some credit on kaggle      contained records
on         people  each training record had information on    input features and a binary      
dependent variable  the test data had records for         people containing information on the above   
features and we had to predict probability of loan default for these people based on the input features  the
effectiveness of the prediction was judged by calculating the auc  area under the roc curve  from the
probabilities of default for the test records  the auc calculated on all the         test records  which
shall be referred to as auc full  was made available to us only at the end of the competition  just two
days before the project deadline  an auc calculated on     of the test records  which shall be referred
to as auc part  was available to us throughout the competition  our decision making on algorithm
selection was hence guided somewhat by auc part in addition to auc scores computed on held out
training data  luckily  our auc part scores correlated very well with our auc full scores 
the auc metric for a   class problem is calculated as  sort the estimated probabilities in ascending order

  

   
         

   

 

where    is the sum of the ranks of class   records and

       is the number of items which are actually class     

   data  the training and test data suffered from the following problems 





missing values for   features  as much as     of the records have missing features 
coded values for   other features  only      of the records have coded values 
most of the input features had outliers  for instance  ruul  a feature with high predictive power 
normally in the range        sometimes had anomalously high values          
class imbalance in training data i e  number of records with class   labels was only    of the
total records 

   algorithms and results
    logistic and probit regression  in logistic regression             


 


    

in probit regression                  where  is cdf of n       standard normal 
      multiple models  since the parameter  could be quickly computed even when the training data
had more than         records  to solve the problem of missing data in the test records  we developed  
different models  each model using a different subset of input features  for instance one model used all   
features and was employed when all features were present  another model used only input features     
and was employed when feature   was missing and so on  thus depending on the features that were

ficlean in the test data record  we would use the appropriate model to compute the probability of a
positive  class    
      feature saturation and under sampling  we were able to get a dramatic leap in performance by
saturating an important feature with high predictive power  ruul  to a maximum value  ruul max  that
was determined by    fold cross validation in the test data and throwing away training records which had
ruul   ruul max  we obtained a further small increase in the auc part score by under sampling
class  records so that ratio of class  records to class  records was      found by    fold cross validation  
figure  

table  
approach

logistic

probit

 auc part 

 auc part 

multiple
models

       

       

multiple
models with
feature
saturation

       

       

multiple
models  feature
saturation  data
balancing

       

      

      diagnostics  as a diagnostic procedure  we plotted auc curves  training auc and test auc with
  fold cross validation  versus size of training data   see fig   for both logistic and probit regression
with multiple models  ruul saturation and data balancing  from the auc curves  we saw that training
and test auc were nearly the same for reasonable number of training samples  hence we concluded that
high variance was not a problem  since we were targeting an auc of above       we concluded that
relatively high bias might be a problem which led us to explore kernelized approaches 

     support vector machines
       choice of svm  kernel and class probability estimation  we used the c svm with l 
regularization and rbf radial basis function  kernel  we focused on rbf kernel since  we seemed to be
suffering from high bias and this kernel maps input samples into an infinite dimensional space  also rbf
kernel has relatively fewer model parameters to optimize  is numerically stable compared to high order
polynomial kernels and some other kernels like linear and sigmoid kernel are special cases of the rbf
kernel under certain conditions     
we used the libsvm library     to implement the svm with rbf kernel for our problem  in order to
generate posterior probabilities  libsvm uses an improved form of platts method     which consists of
approximating the probability by a sigmoid function of the form 
            a        where 
        x    b and  is the sigmoid function

fi       methodology  we opted to train only three models due to the computational complexity of model
training  again each model using a different subset of input features  for each test record depending on
the clean features available  we used the appropriate model  in the test data  we replaced coded values
with their median  which was    we hoped that this would not have a significant negative impact since the
number of such records was only      of the total  we used   fold cross validation with auc as the
metric to estimate optimal c and  for each of the   models  we also saturated the ruul parameter to a
maximum value ruul max for all test records and threw out training records with ruul   ruul max 
       under sampling to handle imbalance of class   and class   records  since cross validation is
computationally expensive we used a smaller training data set of        clean records with a     ratio
of class  records to class  records for c and  estimation  once optimal parameters were obtained  each
model was retrained on a larger data set containing        clean records with again a     ratio of class  to
class  records  we computed posterior probabilities with these   optimized models and obtained an
auc part score of         
       class weights to handle imbalance of class   and class   records  instead of using undersampling  we also tried applying a more severe misclassification penalty for class   records  more
specifically  libsvm gives the option of applying penalties c w  for errors in classifying class   records
and c w  for errors in classifying class   records as opposed to a common penalty of c  we generated a
training set of        records by randomly sampling from the clean training data  then we set w  to  
and tried to find optimal values for c   and w  by brute force search over a well designed search space
and using   fold cross validation with auc as metric  we found optimal parameters for each of the three
models  once optimal parameters had been obtained for the three different models  we re trained the three
models with these optimal parameters for c   and w  over the entire clean training data containing
around         records to predict class probabilities  we computed posterior probabilities and obtained
an auc part score of         

     classification trees  cart 
       description  in a classification tree  the input feature space is partitioned into a set of rectangles
 regions  using recursive binary splits and then in each rectangle  a simple model like a constant is
fitted     given a region r containing training records  to split the region into r  and r   all possible
binary splits  x i   s  x i    s  are examined and the  feature  value  pair  x i    s   is selected that optimizes
a particular criterion   minimize gini index or cross entropy deviance or maximize twoing metric     
this process is repeated recursively for each child and we stop splitting if a node is pure  i e  contains
observations of only one class   or it has fewer than minparent records or if any split imposed on this
node would result in a child with less than minleaf records  for predicting class probabilities for a test
record  the test record is run down the classification tree until it reaches a leaf node and then the class  
probability is estimated as the fraction of class   elements in that leaf node 
       avoiding over fitting  deep trees are susceptible to over fitting and there are two ways of
controlling their depth and reducing generalization error  one method is to find an optimal value for the
minleaf parameter     different values  varied logarithmically from    to       using    fold crossvalidation  the other method called pruning  is to grow a full sized deep tree and then to reduce the tree
depth by recursively merging leaf nodes  the optimal tree depth to prune is decided using    fold crossvalidation 
       handling missing values using surrogate variables  while building a classification tree  at each
non leaf node r  having chosen an optimal  primary  feature f and split point  we also choose secondary 
tertiary features and split points ranked according to the ability of the feature to mimic the split of the
training data at node r according to the primary feature f  when a test record is run down the tree and
hits node r  if it has primary feature f missing  then the best ranking non missing feature is used for
finding the child node to go to 
       results from using classification trees  we implemented classification trees using matlab
statistics toolbox  we experimented with three different node splitting criteria  gini  deviance and
twoing and both methods of controlling over fitting  finding optimal value for minleaf and pruning 

fiwe were able to obtain best results with split criterion set to twoing  using optimal value for minleaf
and turning on the option to use surrogate variables at each node  we obtained auc part         

     random forests
       description  random forest is an ensemble supervised learning method  which builds a large
collection            of de correlated trees and then averages them      each tree in the ensemble is
constructed by taking a boot strap sample of size n b from the training set and growing a classification or
regression tree with that sample  while growing the tree  at each node  m out of the p input features are
randomly selected to determine best feature for splitting at that node  reducing m  reduces the correlation
between the different trees but it also reduces the strength of each individual tree      while predicting 
a test record is run down each tree and the results over all trees in the ensemble are averaged for
regression and a majority vote taken for classification  usually the mean square error is used as the
splitting criterion while building regression trees and the gini index is used as the splitting criterion while
building classification trees 
       application to our problem and results  we decided to use the r package randomforest    
since matlab statistics toolbox did not seem to support random forests  the important parameters to
optimize for both types of random forests were m and minnodesize  for each of the trees grown   in
order to optimize these parameters we drew bootstrap samples of size n b         grew      trees and
estimated effectiveness of the parameters on held out cross validation data with auc as the metric  we
found optimal values for m and minnodesize by using brute force search over a limited search space  we
used the random forest with optimized parameters to predict class probabilities for the test data 
we found that attempts at cleaning data like saturating the ruul parameter or replacing coded
values with medians backfired  since the package randomforest in r  did not handle missing values
gracefully  we started by training three different models similar to svms  we did this for both regular
data and balanced data  created using under sampling  and used both regression and classification based
random forests but were unable to significantly improve on a simple reference solution that used random
forest regression on only the clean input features  we achieved a key breakthrough by replacing missing
values with coded values     was used  and training  optimizing a single model using all    input features 
we also gained some improvement by using over sampling of class   training records to mitigate classimbalance as opposed to under sampling of class   records  using random forest regression  replacing
missing values with   s and over sampling class  records we were able to get an auc part of
        which was our highest 

   discussion  logistic regression  probit regression and svms are traditionally not very effective
when there is a significant amount of missing data  as in the case of our problem      the non competitive
performance of logistic and probit regression can also be attributed to the fact that  they produce linear
decision boundaries using only    input features even though there is an abundance of training data
         records   as for svms  they are not specifically encouraged while training to optimally predict
class probabilities  rather the goal is to maximize separation from the separating hyper plane in the
higher dimensional space  significant missing data along with this fact might explain the relatively lower
performance of svms on this dataset 
classification trees are effective at handling missing data and outliers  they are also invariant to
monotone transformations of an input feature  this might explain why ruul parameter saturation  which
produces significant benefits for logistic  probit regression and svms  is not necessary for
classification trees  classification trees are notorious for having high variance     and we were pleasantly
surprised that a well tuned classification tree could out perform tuned c svms 
random forests were invented as a method to reduce the variance problem of classification trees
and it is no surprise that they out perform classification trees  we were able to obtain significant
benefits by tuning m  the number of input features selected at random at each node as split candidates 
and the minimum node size  however we are unable to satisfactorily explain the following results 

fi

random forest regression was better at predicting class probability estimates than random forest
classification  our best guess is that this might have to do with the split criterion   mean square
error  used for regression as opposed to the gini index that is used for classification 
 the hack of replacing missing values with    performed significantly better than a multiple model
approach  where depending on the clean features available in the test record  an appropriate tuned
random forest was used 
we tried three approaches to handle the imbalance between class  and class  records   under sampling of
class   records  over sampling of class   records and using different weights for class  and class  records
when the algorithm permitted it  we obtained best performance with over sampling of class  records 
table   final summary
approach
logistic regression  multiple models  data cleaning and undersampling
probit regression  multiple models  data cleaning and undersampling
svm with rbf kernel  multiple models  data cleaning and undersampling
classification trees using optimal minleaf  twoing for splitting and
surrogate variables
reference solution using random forest regression with only
clean features
random forest classification using coded values for missing values
and under sampling
random forest regression using coded values for missing values and
over sampling
the winning team  not us  we were    th out of     teams 

auc part
       

auc full
        

       

        

       

        

       

        

       

        

       

        

       

        

na

        

   conclusion  based on our experience in this project  given a binary classification problem with
missing data  we would recommend first trying random forests  tuning m and minimum node size and
addressing class imbalance by over sampling  after the results were announced we learned that the best
scores were obtained by blending averaging the results from different schemes like random forest
regression and classification  gradient boosted regression and classification trees and neural
networks  also the winning team combined the    given features to produce as many as    features to use
in their algorithms 
references 
   http   www kaggle com c givemesomecredit
   chih chung chang and chih jen lin  libsvm   a library for support vector machines  acm
transactions on intelligent systems and technology                      
   h  t  lin and c  j  lin  a study on sigmoid kernels for svm and the training of non psd kernels by
smo type methods  technical report  department of computer science  national taiwan university 
     
   john c platt  probabilistic outputs for support vector machines and comparisons to regularized
likelihood methods         advances in large margin classifiers 
   http   www mathworks com help toolbox stats
   hastie  tibshirani and friedman  the elements of statistical learning  data mining  inference and
prediction   nd ed      
   http   www stat berkeley edu  breiman randomforests
   a  liaw and m  wiener         classification and regression by randomforest  r news              

fi