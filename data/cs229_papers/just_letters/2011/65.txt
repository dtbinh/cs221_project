pulse project  user interest based news prediction
yinan na

jinchao ye

abstract
pulse is a news recommendation app available on both
iphones and android phones  predicting news of users
interest according to their reading history has always been
a hot topic  in this project  we used tf idf vector and
logistic regression to predict news of users interest on
pulse  the feed of news has been proved to be a key factor
for predicting news  special measures have been taken to
handle unbalanced training data  we use word stemming
and other techniques to reduce the dimension of tf idf
vector  we also compared logistic regression with
support vector machine  moreover  instead of treating
each user independently  we also tried unsupervised
learning methods such as k means on users 

yijun liu

train on day      and test on day                

the classifier predicts whether the user will read or click
a certain story in the following days  we employ f  score
as criteria for evaluation 

   approach
     data pre processing

   introduction

we removed the malformed or duplicated data in
stories log  by malformed data  we mean those entries
which lack parts such as story title  story url  feed title 
feed url or timestamp  by duplicated data  we mean
those entries which are exactly the same except for the
timestamp  in this case  we only reserve the one with the
earliest timestamp  there are        stories left out of
        raw entries after the processing  we also adjust
the other two files according to stories log 

     motivation

     baseline  tf idf vector and logistic regression

today  many users love to read news from their mobile
devices  where they can easily pick up news stories
recommended by app softwares like pulse  in such
applications  the program predicts what kind of story the
users likely to read  by applying various machine learning
algorithms on the dataset  the recommendation system is
like netflix challenge      our project aims at developing
and improving such prediction algorithm  to provide an as
accurate as possible result 

the tf idf     is a weight used to evaluate how important
a word is in a document  it is often used in information
retrieval  more specifically  let denote a word  denote a
document and is the set of documents  then

     dataset
we use the pulse data in august     days  as our dataset 
there are three files  stories log  user story read log and
user story clickthroughs log  in stories log  each story is
stored in the format  story url  story title  feed url  feed
title  timestamp   there are total         raw entries in
this file  the read log is stored in the format  user  story
url  story title  feed url  feed title  timestamp   there
are         reading records in this file  the click log is
stored in the same format as read log  there are      
clicking records in this file 

here
is the term frequency in the documents 
the higher the score is  the more important the word is in
the document  before we use word stemming and other
techniques  the tf idf vector is an        dimension vector 
when computing the tf idf vector  we ignore the
non english characters 
as taught in class  logistic regression is a common to
predict the probability of an event when no prior
information is available 
we compute the tf idf vector for each story  then for
each user  on each day  we train a classifier for predicting
whether a certain story will be viewed or clicked by the
user 

     handling the unbalanced training data
     problem definition
for each user  we build a classifier and evaluate it over
the time series 
train on day    and test on day          

one tricky part of the problem is that during the training
process  there are much more negative samples than
positive samples  as shown in figure    if we use these data

fidirectly  the classifier we get might judge every test sample
as negative 
we employ an iteration scheme to solve this problem 
for the first iteration  we first randomly pick up the same
number of negative samples as positive samples and feed
them to the logistic regression  then we use the classifier to
test the whole training data and pick up top    hard
negative examples to replace   original negative samples 
then we feed the newly generated negative samples along
with positive samples to logistic regression  we did
iteration for     times and get the final classifier  the idea
is similar to boosting      we did not replace all the
negative samples at one time because that will make the
classifier change dramatically and not converge  just as
figure   shows  we replace   negative samples each time
and this makes the decision boundary moves gradually
towards the ideal place  just as figure   shows  this
handling increases the f  score 

figure   unbalanced data

we use our own method instead  we find a vocabulary
table of      words  these words are frequently used and
are sufficient for most uses  we first removed the words
that are not in the      words vocabulary  then we did
word stemming on the      words and got      words
instead  in the meantime  we also record the word hierarchy 
the idea is similar to wordnet      these dimension
reduction methods increase the f  score 

     removing false positives using feed
despite that we have incorporate feed url and feed title
when we compute tf idf vector  the feed title can be used
more efficiently as a cue to remove false positives 
we have noticed that some readers only read news from
certain feeds  such as usa today  hacker news  this
is easy to explain just as most people usually buy clothes
from certain stores  therefore  for each user  we maintain a
set   which records the feeds the user have read news from 
during testing process  we directly label those stories of
which feed title are not in as negative  this means we
assume that the user only read news from feeds in   this is
proved to remove a lot of false positives while only slightly
lower the recall rate  therefore it improves the
performance 

     support vector machine

figure   boundary moving towards ideal place

we also use support vector machine  svm  as a
comparison method to logistic regression  as taught in
class  support vector machine finds line with the largest
margin between positive samples and negative samples 
svm is usually performs better than logistic regression 
this is also confirmed in our experiment 

     user clustering using k means
figure   boundary oscillate dramatically

     word stemming and dimension reduction
use the        dimension tf idf vector as feature not
only seriously lowers the computation speed  but also
causes the problem of over fitting 
we first remove the stop words such as a  the  here 
there  etc 
word stemming is a common method of reducing the
dimension of tf idf vector  for example  it will map the
words process  processes  processor  processing 
processed  procession  to the same word process 
however  after we have implemented word stemming 
the dimension of tf idf vector is still at a relatively high
level  so we tried latent dirichlet allocation  lda  
however  since the packages which implemented lda are
all too large and required  gb memory  we find it hard to
employ them in our own package 

as for unsupervised learning algorithm  we tried using
k means to cluster     users to k clusters  in our
experiment k       the vectors that are used for clustering
are        dimensional binary vectors  the jth component
is   if the user has read the jth story before certain day
otherwise it is   
once we have clustered the     hundreds users  we can
assign each of the remaining user to one of the k centers
according to their previous reading history  then on a new
day  for a user s belonging to cluster l s   we recommend
the top n stories most read by other l s  users on that day
for user s 

   experiment
we use the dataset described in section     and      the
statistics of the processed dataset is shown as table   
table   statistics of processed data
number of stories in    days       

finumber of users
number of read times
number of click times

    
       
     

we find that many users seldom click through a story on
many days and it is very difficult to predict whether a user
will click through a story  so we do not incorporate the third
file  user story clickthroughs log  i e  we do not
incorporate the clicking data 
also  we find that in the      users  many users neither
reads or clicks through any story  we pick up the most
active     users for our experiment 
as stated in section      for each user  on each day j  we
train on the data on the first j days and predict whether or
not the user will click the new stories in the following    j
days  so for each user on each day  we can get a precision
score  recall score  as well as f  score  here

average f  performance comparison of different methods with     users
    
   
    
   
lr
lr feed
lr stemmer feed
lr iteration
svm feed

f 
    
   
    
   

 

 

  

  
day

  

  

  

figure   average f  for     users
we have implemented and compared five methods  the
five methods are explained as following 
lr  this simply uses tf idf vector and logistic regression 
this does not employ handling of unbalanced data  during
training  the ratio between negative samples and positive
samples are      the negative samples are randomly picked 
this is the baseline 
lr   iteration  in addition to the baseline  this method
handles the unbalanced data use iteration method described
in section     
lr   feed  in addition to the baseline  this method uses
feed titles as cues to remove false positives  as described in
    
lr   stemmer   feed  this method uses logistic
regression and tf idf vectors as well  but the tf idf vectors
are reduced to low dimension using word stemming and
other methods described in section      it also uses feed as a
cue to remove false positives 
svm   feed  this method uses svm as learning
algorithm and tf idf vector as feature  besides  it also uses
feed title to remove false positives 
we used liblinear and libsvm packages for logistic
regression and svm 

average precision performance comparison of different methods with     users
    
lr
lr feed
lr stemmer feed
lr iteration
svm feed

   

    

   
precision
    

   

x    
y       

    

   

 

 

  

  
day

  

  

  

figure   average precision for     users
average recall performance comparison of different methods with     users
 
   
   
   

table   precision  recall and f  comparison
f 
precision
recall
lr
      
     
      
lr feed
      
      
      
lr stemmer feed
      
      
      
lr iteration
      
      
     
svm feed
      
      
      

   
recall
   
   
lr
lr feed
lr stemmer feed
lr iteration
svm feed

   
   
   
 

 

 

  

  
day

  

  

figure   average recall for     users

  

fitable   records the average precision  recall and f  for
    users  despite it is the average precision of different
users  it pick up the peak value during    days 
from the f  curves  we can see that svm   feed out
performs other methods  and methods with feed
outperform those without feed  lr with iteration
outperforms that without iteration 
from the precision curves  methods with feed have
better precisions than those without using feeds  this is
because by using feed  we can remove a lot of false
positives 
from the recall curves  we can see that baseline method
has the highest recall  it is because we only pick up
randomly picks up a small number of negative samples
during training and the classifier therefore is easier to tends
to categorize the training data as positive  so it has high
recall but extremely low precision  and hence the lowest f 
score  when we add iteration during the training process to
handle the unbalanced data  the recall drops while the
precision rises and the overall performance f  rises  the
svm   feed method balances precision and recall well and
therefore has the highest f  score 
another point is that different users have really different
performances  for some user as shown in figure    the f 
score is really high  of course  there are also users whose
behavior is hard to predict 
user     f  curve using lr feed
 
    
   
    
   
f 
    
   
    

   future work
currently we have done the dimension reduction via
word stemming and lookup tables  which helped us
recognize all those common english words and named
entities and eliminate those non sensical strings  in the
future  we want to try lda and compare with our own
method of dimension reduction  principle component
analysis  pca  might also be used to further reduce the
dimension of tf idf vector  we can also try named entity
recognition to reduce the dimension 
despite that the clicking data is really sparse  we may
also use it to predict whether a certain story will be read or
viewed by a user  in fact  we can use weighted logistic
regression and assign higher weight to clicked through
story  this is because people click through a story because
they are more interested in this story than those who are
only viewed 
our implementation of user clustering using k means
seems to have problems  so we still needs to investigate
why we cluster so     users to   cluster while other clusters
only have   or   users each  moreover  we might also use
other unsupervised learning algorithms such as
collaborative filtering 

   acknowledgement

   
    
   

in accuracy while it needs a little more computing time
during training 
in order to acquire good performance  we need to
employ feed title as an important cue to remove false
positives  the dimension reduction of tf idf vector  such as
using word stemmer or lda  is also critical to this problem
as it not only speed up the computing process  but also
avoid over fitting  as for unbalanced training data  we need
to iteratively pick up equal number of negative samples and
positive samples and train on them  the rule for picking up
these samples is similar to the idea of boosting 

 

 

  

  

  

  

  

  

day

figure   f  curve for user     using lr feed
as for user clustering  our implementation is problematic
because it clusters     users into one cluster while other
clusters each have only   or   users  we are still
investigating the problem 

   conclusion
the above experiments have shown tf idf vector is an
effective feature for information retrieval or text mining 
more importantly  logistic regression and svm have both
been proved to be effective for large scale machine
learning projects  svm is better than logistic regression

we would like to express our gratitude to all the cs   
teaching staff who gave us the possibility to complete this
project  we want to thank prof andrew ng for all the
ineffable lectures  along with the meticulously articulated
course notes 
we want to thank dr richard socher for his altruistic
and comprehensive instructions in this project  without his
trenchant advices  the progress of our work could have
ceased to move at the very beginning  all his instrumental
advices greatly incited our minds and helped us bring about
this final project 

references
    karen sprck jones  a statistical interpretation of term
specificity and its application in retrieval  journal of
documentation              

fi    robert m  bell  yehuda koren and chris volinsky  the
bellkor solution to the netflix prize 
    yoav freund  robert e  schapire  a decision theoretic
generalization of on line learning and an application to
boosting  journal of computer and system sciences    
       
    george a  miller  wordnet  a lexical database for english 
communications of the acm  volume    issue     nov 
     

fi