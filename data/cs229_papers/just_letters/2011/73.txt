learning from high dimensional fmri data using random
projections
author  madhu advani
december         

introduction
the term the curse of dimensionality refers to the difficulty of organizing and applying machine
learning to data in a very high dimensional space  the reason for this difficulty is that as the
dimensionality increases  the volume between different training examples increases rapidly and
the data becomes sparse and difficult to classify  so  the predictive power of a machine learning
algorithm decreases as the dimensionality increases with a fixed number of training examples  which
is known as the hughes effect 
one way of dealing with the curse of dimensionality is by projecting data into a lower dimensional
subspace  the statistically optimal way to do this  assuming the data is on or near a linear subspace  is pca  which projects the data into a subspace that preserves as much of the variation as
possible  however  pca is computationally expensive for high dimensional data compared to the
method of dimensionality reduction through random projections 
in fact  the distortion of the data when it is compressed via random projections can be bound by
the jl lemma  however  empirical testing of this technique will demonstrate how well it performs in
practical machine learning problems  the major benefit of using random projections are that they
are a computationally less expensive than pca particulary when the dimensionality of the data
becomes too large for matrix diagonalization to be feasible  as is the case for very high dimesionality
data such as fmri 
the main focus of this paper is applying machine learning algorithms to classify fmri data and
attempting to empirically and theoretically predict the feasibility of applying random projections to
the supervised classifying of fmri data  to that end  we first give a review of some of the applicable
theory for random projections  then we empirically examine how logistic regression classification
error varies when the data is compressed via random projections  we then explain a theoretical
method for estimating an assymptotic bound on the generalization error  estimated using k fold
cross validation 

 
   

theory   random projections and machine learning
random projections preserve relationship between data   jl lemma

random compression of data points x    x         xp from n to m dimensions via random projections
is computed simply by letting

 

fix i   axi
n

   

m

where a is a matrix a   r  r with each aij a normally distributed random variable with
mean   and variance   m   in fact the distribution need not be normal  and the jl lemma will still
hold true if we instead let each aij equal  m or   m with equal probability  for computation
simplicity  our results used the latter method for
 compression 
the jl lemma state that m   o    log s  is sufficient so that with a high probability  error
exponentially decaying with m  we have a bound on the distortion of two points xi   xj
fi
fi
fi ka xi  xi  k   kxi  xj k  fi
fi
fi
   
fi
fi
kxi  xj k 

where s is the number of pairs of point  or vectors to be preserved   here s   o p     so
m   o    log p    
a detailed proof can be found in      but to provide a rough sketch  we consider taking the
vector between each pair of points vi and compressing them  it is not hard to show that the mean
value of the squared length of each vector has the same expected value in the compressed space 
and we then use the fact that each dimension is independent to show that the we can compute a
chernoff style bound on the distance of the compressed vector from its mean length 

   

predicting machine learning ability using random projections

now  we want to use the jl lemma to predict the effect of randomly compressing data on machine
learning  we will assume the case of a classification problem where each data point xi is labeled
by yi as   or    we can assume that a machine learning algorithm will separate our data equally
well if the data is rotated or shifted  without altering relative distanes between points  thus we
will consider shifting and rotating our data so that the means of the data   and   lie on the z 
axis  equidistant from the origin 
now  as in     we will define that data is separated by a margin  if there exists a unit length
vector w that bounds the angle separating the two distributions of the data 
p  y  w  x   kxk        

   

note  we will also call yi  w  xi    kxi k the margin of a data point xi
first consider s data points with some margin  and randomly project this data from n to m
dimensions 


for some m   o log s 
and letting u and v be normalized vectors in our dataset
 

fi  fi
fi  fi
fi
  fifi
ka u  v k   ku  vk  fi   fikauk    fi   fikavk    fi
 
 
 
thus  we have a high probability that
 u  v  au  av  



ku  vk        
 
it is not hard to extend this result  as in      to show that for sufficiently small 
fi
fi
fi
fi
fiu  v  au  av fi   
fi
kaukkavk fi
 u  v  au  av  

 

   

   

   

fithus  if we choose m large enough such that the maximum distortion likely to be less than is
     then we have
fi
fi
fi axi  aw xi  w fi
fi
fi
   
fi kaxi k  kxi k fi    
thus  we can ensure 
the margin is reduced my no more than a factor of

for some m   o

 
   


 

with high probability

log p  
 

empirical testing of learning on high dimensional fmri
data
high dimensional fmri data

the starplus fmri data used in this paper was originally collected by marcel just and his colleagues
in carnegie mellon universitys ccbi  in the experiment  fmri imaging was performed on a fraction
of the brain  while the subjects were told to either stare at a blank wall or a sentence or picture 
which they viewed seperately 
fmri scans measure the level of oxygen in the the blood flow of the brian  and is correlated
with brain activity in different regions of the brain  the brian of the subject was scanned every
   seconds over a period of about half a minute for each trial and about   seconds of looking at
a picture and   seconds of looking at a sentence  the results of the brain scans in each    second
time step is stored in a vector of activity measurements about      voxels  volume elements  of the
brain 
the most obvious application of machine learning was to use a machine learning algorithm to
classify whether the subject was looking at a picture or a sentence based on the brain activity
during the course of their fmri 
to do this i concatenated the    vectors of voxels collected while a subject stared at a sentence
or picture for   seconds followed by a blank screen for   seconds  then  i reduced the dimensionality
of the data by restricting the data to certain regions of interest suggested to be indicative  thus
learning was performed on    training examples  each        dimensional with an equal number of
examples corresponding to sentence and picture viewing 

   

machine learning and results

logistic regression gave an acceptable level of classification accuracy  generally between    and   
percent  when applied to the full dimensional data set  and performed better than gaussian naive
bayes  due to the high dimensionality of the data and low number of data points  machine learning
algorithms intrinsically have a high variance  so i applied k fold cross validation to the    training
examples to approximate the generalization error of the algorithm 
when i randomly compressed the data and applied logistic regression i found that the mean value
of the error and variability of the error decayed as i varied the number of projected dimensions
m between     and      and the expected error for     dimension was actually lower than the
expected generalization error for the full dimensional data set  eventually  we would expect the
error to rise for some larger m as the jl lemma predicts that the two results will converge when
m becomes large enough 

 

fifigure    plot of cross validation error as a function of number of random dimensions m

 

discussion of theortically approximating machine learning generalization error

using the previous theoretical ideas we attempt to approximate an assymptotic bound on the
generalization error of learning from randomly compressed data  because the data is so highdimensional  if we apply logistic regression to the data  it results in a positive margin and zero
error  which is an artifact of overfitting  so  we will estimate the generalization error using holdone out cross validation 
assuming that we are running a learning algorithm in the full n dimensional space and applying
hold one out cross validation to test it  we will generate a classifier w
 i  learned on the other p   
points  for each x i   now  by appling the results of the jl lemma for margins to the set of pairs
of vectors  w
 i    x  we see that we can take the fraction  of wi and xi that result in a margin
larger than   and then projecting these pairs of vectors down tom dimensions 
we can maintain

log p  
a margin separating the data with high probability with m   o
  if we assume that aw 
 
where w is the classifier learned in the high dimensional space  has a generalization error that
greater than or on the order of the error of our learning algorithm on the compressed data  then
we have approximately bound the error from above by     with a high probability
one indication for why margins  as defined in this paper  may be important for predicting the
number of projections required is as follows  how many projections are required for a machine
learning algorithm to learn from data that is compressed via random compressions is a function of
the probability distribution of the data  one interesting result is that if we call let the ideal generalization error be the classifier that results in the minimum expected number of misclassifications of
data and we let the data be distributed as two gaussians with an identical  isotropic variance  then

 

fiwe find that projection onto a single random dimension will compress both the separation between
the distributions and the spread so that the generalization error remains the same  however  if
we choose our variance to be non isotropic  it is more difficult to separate in projections that lie
approximately parallel to the large variance  thus  a larger number of random projections may
be required to achieve the same generalization error  this illustrates that the non isotropic nature
of the variance of data contributes to needing a larger m to accurately separate the data  this
relats to our margin discussion since non isotropic variance  in directions orthogonal to the vector
between the means of the distributions  correspond to a larger fraction of data points having a low
margin 

conclusions
in conclusion  random projections are a useful way of reducing the dimensionality of data sets
that are too high dimensional to apply pca to  the jl lemma guarantees preservation of stucture
with large enough m  so that the generalized probability distribution will be preserved up to any
degree required by large enough m  however  the number of generalized dimension it will require
is really a property of the data set  one useful measure of which is the distributions of margins of
the data points 
one important property of high dimensional data is that it appears to be more seperable than
it actually is  i e  we can find a hyperplane separating a small set of data with zero error  but this
not mean that the generalization error of the hyperplane is low  so predicting the effects of random
projections on the generalization error of the distribution remains an interesting problem  which
may require some extension of the jl lemma to accurately bound  as the jl lemma applies to a
finite set of points rather than a distribution 

acknowledgements
thanks to professor surya ganguli for mentoring on this project

references
   bingham  e  mannil  h  random projections in dimensionality reduction  applications to image
and text data  helisinki university of technology
   blum  a  random projections  margins  kernels  and feature selection  department of computer science  cmu 
   dasgupta  s   gupta  a  an elementary proof of the johnson lindenstrauss lemma  international computer science institute 
   karnin  z   et al  explicit dimension reduction and its applications  proceedings of the   th
annual ccc      

 

fi