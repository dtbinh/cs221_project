cs       project final report hooyeon haden lee  hlee                        

title  using twitter to estimate and predict the trends and opinions

 

introduction

was set to zero  hence  predicting the same day trends  
in another related work from tweets to polls     the authors
attempt to solve a very similar problem using the twitter data 
the authors trained a linear model after applying the sentiment
analysis over each tweet by checking whether a tweet contains
a positive word and or a negative word using a standard lexicon 
each tweet could be marked either positive or negative or both 
for predicting obamas job approval rate  they limited the input space to the set of the tweets that contain a word obama 
on which the sentiment analysis was performed  a simple linear
regression model was slightly better than their baseline prediction  for predicting other tredns such as the consumer confidence
index  the authors report that their model did not work well 

the most common  traditional way to collect peoples opinions is
random sampling and asking survey questions by phone  if a news
media is interested in knowing how popular obama is  it would
call up a thousand people and ask how they would rate obama 
the data collected in this manner is considered a gold standard 
however  this approach bears shortcomings such as costs  limited
targeted people  etc 
in this paper  we attempt to predict peoples opinions and trends
by analyzing the web data such as twitter  there are several
interesting questions we could ask  given billions of tweets  how
people feel about economy  how people rate obama  what people
think about the top five contestants left in the tv show  american idol  collecting polls by asking people in person or by phone  
data
is costly  but if we could get the same results from the freely available web data  it could supplement or supplant the conventional     data collection
we used the publicly available tweets from jul   to nov   
way of collecting polls    
which were crawled via public twitter apis  for the trend data 
  problem description
we only had time to work with the obama job approval rate data
to formalize our problem  let us define a time series variable p t  from gallup 
that we would like to predict  for instance  obama job approval     data statistics
rate that ranges in    and      could be such a time series vari  in this work  we used all public tweets from           to
able  then our problem simplifies to predicting p t  using the             inclusive  and the gallups obama jab approval
twitter data 
polls data from           to             inclusive  during this
since the tweets are also time series data  we can denote another period  we have the following statistics  here  qualified tweets
time series variable q t  that can be obtained by some text anal  refer to the tweets that contain obama  case insensitive  as a
ysis over the twitter data  this could be  for instance  a certain substring  
words frequency at time t   hence  we could try to correlate these
  of days
   
two time series p t  and q t  via learning models such as linear
  of tweets  total 
              
regression model  for  day forecast  we would correlate q t  with
  of tweets  daily 
          
p t      e g  predicting  days ahead  
  of qualified tweets  total 
         
  of qualified tweets  daily 
      
  related work
figure
 
shows
the
time
vs 
volume
of
tweets plot while figure
google recently developed an online tool called google flu trends
 
shows
the
fraction
of
qualified
tweets 
the green line in figure
     which attempts to predict the number of influenza patients
 
 the
number
of
qualified
tweets 
is
scaled
up by multiplying
based on the search queries on google com  the actual number
by
    
the
left most
vertical
line
is
when
osama
bin laden was
of patients in the united states is reported by the center for diskilled 
the
number
of
qualified
tweets
bursts
out
on that data
ease control  cdc   but the report comes out with a   week lag 
while
the
total
number
of
tweets
does
not
change
much 
on the
in this work  the authors attempt to train models and predict via
other
hand 
during
the
week
of
           
the
occupy
wall
a simple linear regression model over the log odds of two events 
street
movement riot
was
prevailing
around
the
globe 
and
the
p t  which is the fraction of physician visits among all visits and
total
number
of
tweets
spiked
during
this
week
 the
right most
q t  which is the fraction of influenza related search queries  this
simple model provided a good estimate of the number of flu pa  vertical line   while the fraction of qualified tweets did decrease
tients  which matched the actual cdc reports  in their model   by a bit  these two plots together suggest that people do share
their opinions via twitter 
table    top words with respective train test errors with     

rank
 
 
 
 
  
  
  

word
cong
nov
     
 thatawkwardmoment
 nv
 vote    
 whyivote

train err  single 
       
       
      
     
      
      
       

test err  single 
       
      
       
       
       
       
       

 

train err  agg 
       
      
       
      
       
       
      

test err  agg 
       
       
       
       
       
      
       

fi 
   

method
assumptions and pre processing

first of all  given a prediction problem  or a topic   we must admit that not all tweets are related to the given topic  in fact  the
majority of tweets would be irrelevant  hence  it is important
to limit our input space to a proper subset of tweets  this introduces a different but relevant problem called topic analysis 
given a tweet  can we identify what topic s  this tweet belongs
to  since this problem itself is an interesting  difficult problem  we
do not attempt to solve this problem in this work  but we would
simply filter the tweets as a pre processing step  for instance 
for the obama job approval prediction  we could simply consider
the tweets that contain obama 
second of all  as the authors in     pointed out  twitter has its
own language in the sense that people post tweets using hash
tags  urls  internet slangs  etc   in order to express their opinions within     characters  considering that each tweet consists
only of    words on average  this is barely a short sentence  hence 
a simple sentiment analysis may not work well for analyzing the
figure    time vs qualified tweet fraction
twitter data  in this work  we propose that the term frequency
     
linear
regression   aggregate words
count  uni grams  bi grams  etc   and aggregated term frequency
 top k uni grams  etc   could be used in learning the time series now  suppose we learned  i  for all m words  we can sort them
by the training error to get the best k words that work best on
prediction model 
training set  and use these top k words altogether to learn a new
linear regression model  in this case  the parameters  k  we want
to learn would be in dimension k      learning  is identical to
learning  in that we just have to solve the normal equation 
 k      x  k   t x  k      x  k   t  y
where x is the design matrix containing the top k feature vectors
as rows  this approach mimics the work in     
      time parameters
notice that the above linear regression models try to predict   day
forecast  same day   however  it would be more useful and practical if we could produce  day forecast  for instance  the tweets
posted today may be a good indicator of the true polls data in  
days or a week  hence  we introduce the time parameter  that
indicates how far in the future we would like to predict  in our
work  we would consider the values                           means
predicting the trends   days ahead   then  we would instead try
to minimize the sum of squared errors 
pk
 k 
 k   i 
  yj        i   i xj     

 

figure    time vs tweet volume

   

   

learning models

experiments and findings
experiment setup

single word model
the time parameter  varies between   and     since we have
    days of data  we train our model on days  ts       te   and test
our model on days  te             our days are   indexed   with
these three parameters fixed  we learn single word linear regression models for each word in our vocabulary v  recall that
 v                and we sort the words by how well they fit the
target variables  obama job approval rates  
aggregate model
based on this measure  we pick the best k words  where k  
             and train an aggregate word linear regression model as
explained earlier  our findings focus on using these aggregateword models  and our single word models play a role as a feature
selector for ggregate models 

we used linear regression model where the features are word frequencies and target variables are obamas job approval rate  to
be more formal  n       is the total number of days  in our
vocabulary  english words and hash tags   we have m    v    
         words  for each word wi   the corresponding feature vec i 
tor x i   rn is defined as  xj is the word frequency of wi on
pm  i 
day j  in particular  i   xj     for all j  i e  the sum of word
frequencies of the same day equals to one    y  rn is defined as
the gallups poll data 
      linear regression   single word
we can easily think of a linear regression model to learn the parameters  i   r  using just a single word wi by solving the
famous normal equation 
 i      x i   t x i      x i   t  y

 

fi      effects of size of aggregate model
for fixed   ts   and te   an aggregate model is built on top of the
best x words measured by single word models  by varying x  we
can see the effects of having fewer or more words in aggregate
model  naturally  we would expect that our models will fit the
training data better as we have more words  hence more features 
 we can overfit  however  for the test error  it will initially decrease as our model can capture the trend better  but eventually
it overfits and blows up 
in figure    we can clearly see this effect  as we add more words
to the aggregate model  we see that the train error continues to
decrease  in blue   while the test error  in purple  bold  is minimized at around     words  and then starts to blow up  after
having     words  the aggregate models test error was too huge
to fit the plot  the red line represents the train error of each word
in single word model  and this is monotone increasing because we
sorted the words by their train errors  the green line represents
the test error of a single word  which appears to be a bit random
as single word is clearly not a good predictor 
figure    training set size vs error

   

results

table   summarizes the train test errors when       showing
some of the top    words due to the limited space  notice that
we see some politically relevant words such as cong  congress  
 vote       whyivote  etc  the following sections will discuss
what each of the parameters tells us about the data 

figure    prediction using    words

figure    aggregate model size vs error

      effects of training set size
for fixed  and x  the number of words used in aggregate model  
as we vary t   te  ts  the size of training set   we would expect
that the larger training set will help our model learn the target
trends  training on   days of data would make our model too
simple  while training on     days of data would yield a more
reasonable model  figure   shows how train test errors change
when we vary te from     to      x axis  while we fix parameters       ts      x       in this plot t   te represents the
split point of train set and test set  as we see  the test error  in
bold green  has a decreasing trend in that our models benefit from
more data  while train error  in red  increases and decreases but
does not fluctuate much 

figure    prediction using    words

 

fifor comparison  figure   and figure   show how our aggregate
models predict the trends when we keep all parameters the same
except for the number of words used in aggregate model  in these
two plots  the red curve is the target trend  gallup data   and the
green curve is our models prediction  for these plots  we fixed
      ts      te        but the numbers of words used are    and
    respectively 
notice that figure   does capture the trends in training set quite
well  while figure   captures the trends in training set almost perfectly  the    word model captures the trends in test set much
better than the    word model 
      comparison with random models
previous results confirmed that aggregate models do work better
as we use more words  which begs the following question  what
happens if we just randomly pick words from our v and train
aggregate models  would these model fit the trend curve just as
perfectly as our models  if the answer is yes  then our models are
not any better than randomly picking a set of words and learn
linear regression models  here  we fix all the parameters   ts   te  
and x while we train our aggregate models with a different set
figure      of words vs l  error
of words  the best x words  our model  and the random x words       evaluation on word set
 random model   this random model serves as a baseline in our although we compared our model against a random word aggrework 
gate model  which served as a baseline in our work   we could
evaluate our model in different ways  to evaluate whether we selected a good subset of words  we can look at the actual weights
 or  values in our equations  in aggregate model and see how
these weights vary  intuitively  if our word set is bad or random 
these weights should fluctuate a lot as we vary the size of aggregate model while keeping other parameters fixed  because in
such a set  the words are not meaningful features to predict the
trends  on the other hand  if our word set is a good predictor 
then we should see a rather stablized weights across the words 
such that some words are positively correlated to the trend while
others negatively 

figure    our model vs random model

figure   clearly shows that our model fits the test data very
well while the random model cannot even fit the training set
very well  the difference is more obvious in the test set  on the
right side of the vertical line   here  the parameters used are
      ts      te        x       this confirms our assumption
that filtering the features  words  by how well they fit the target
trend individually  via single word modes  is a good feature selection process  because we used the same aggregate linear models
just with different set of features  
figure   shows l  errors of our model and random model when we
increase the number of words used in aggregate model  the train
error continues to decrease as before  our error in red and random
model in blue   however  we see an increasing trend for the test
error of random model  in purple  bold   while our models test
error decreases at first and then stabilizes  in green   this clearly
shows that our model performs significantly better than the random model  in this plot  parameters were       ts      te       

figure    aggregate model size vs weights

figure   shows how the weights of the top    words change while
we change the aggregate model size from   to     the x axis represents the number of words used in aggregate model  and there
are    curves shown in each plot  for instance  in figure    the
top red curve is for the top word bump  whose weight is pretty
much stablized after about    words  while the navy curve at
 

fibottom for the   th word  wa starts at x      because it is
not included in smaller aggregate models  and its weight does not
fluctuate much either  figure   was generated with parameters
      ts      te       
another way to evaluate our word set is to try to fit the entire
time series  both train and test set altogether  usnig the word set
chosen from the training set  that is  we select top x words from
training set as usual  but we solve the normal equation for the
entire period  this implicitly assumes that the aggregate linear regression model is a good model  figure    shows that we can in
fact fit the entire time series very nicely  the blue curve  fitting
the whole time series  fits the red curve  the target trends  very
well  while the green curve  our usual aggregate model  can only
fit the training set nicely  this suggests that the way we select
the aggregate word set is good  but our model may overfit the
training data set which leads to a big error in training set  here 
the paremters were ts      te        x            

day to day trends  in some sense  as the authors in     claims in
their work  too 
there were of course issues with our models as well  as discussed
briefly earlier  our best performing words in single word models
include junk words that are irrelevant to the topic  to remedy
this  our on going work and future work would be discussed next 

 

future work

there are a number of ideas that could improve the performance
of our model 

   

natural language procesing

in nlp  it is often helpful to use bi grams  two adjacent words 
instead of uni grams  and our models could easily adopt the bigram features as our models do not depend on uni grams  we are
half way through trying bi grams  we chose bi grams such that
each word must come from our vocabulary v for the uni gram
model  this ensures that any bi gram consists of two important
uni gram words   and the most frequent bi grams are all relevant
to our topic  which is different from our uni gram case  some
of such bi grams are president obama  barack obama  white
house  etc  recall that in uni gram model  the most frequent
words are rt  i  the  to  you  etc  in order  hence  bi gram
model seems to filter our meaningless words automatically  and
thus can possibly improve our models significantly  stemming of
words could also help  but we did not seek into this direction 

   

different trends

a different direction is to predict different trends  such as the
popularity vote of american idol contestants  this is a different
problem because it is now a binary classification  i e  eliminated
or stayed   hence  for the final version  we attempt to implement
better learning models and to apply them on at least two different time series data  obama job approval rate and american idol
contestant popularity  

   

more features

one very useful piece of informatino we explicitly did not use in
our model was the user information  for each tweet  we can
figure     fitting training set vs entire set
identify the user who posted the tweet  and we could try to add
  summary and conclusion
features about the user to improve our model  for instance  if the
in previous sections  we learned our aggregate models benefit from user mentions obama in his user profile  we would expect the
a larger data set  larger training set  and a larger set of features user to post politically related tweets more than average users 
 more words in model   this is expected and reasonable because we believe such extra features could help  but did not pursue this
our models should collect enough data in order to learn the target direction in this work 
trends  and larger set of features helps fitting the target trends 
  acknowledgements
section       and section       confirmed an important fact that
this project was jointly used for cs   w  and professor leskovec
our feature selection process via single word model is a promisprovided guidance on understanding and analyzing experiment reing process as our aggregate model makes a significantly better
sults 
prediction that any aggregate model with randomly picked words 
we were able to evaluate our model againt the random aggregate references
models  and confirmed that our model outperforms 
    bryan r  routledge noah a  smith brendan oconnor  ramalthough we only showed a handful number of plots in this
nath balasubramanyan  from tweets to polls  linking text
paper  the results were consistent across the parameter space 
sentiment to public opinion time series  aaai conference on
                ts                 te   ts                  x                
weblogs and social media       
however  for large values of   our aggregate models started predicting less accurately  and this indicates that predicting the far      rajan s  patel lynnette brammer mark s  smolinski
future is more difficult than the near future using twitter data 
larry brilliant jeremy ginsberg  matthew h  mohebbi 
which confirms that twitter data is volatile  which counts for
google flu trends  nature       

 

fi