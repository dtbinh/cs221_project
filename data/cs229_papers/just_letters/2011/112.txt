improving tictoc with machine learning
david cordoba

introduction
over the summer  a friend and i created an event discovery application after we were in san francisco
one night and found it was too difficult to discover cool local events happening around us at that
specific time  the kinds of events we wanted were things like live music at a coffee shop  mariachi
night at a mexican restaurant  or a happy hour at a bar  these are events that are not easy to find
because there is no one centralized source that compiles all of them  you d have to go to the websites of
each business near you to find out if any of them have cool events happening at the moment  we
scrapedevents sfgate com since they had pretty good events for the bay area and we scraped a happy
hours website  they both had well structured data  so we were able to place each event into one of the
following categories  sports  culture  music  food  and happy hours 

categorizing events with poor metadata
a problem we have run into while working on tictoc is that a lot of events out there dont have great
metainfo  our events have very few fields  a title  description  time range  location  and category  most
new sources we find have all but the category  so we need something to assign to the proper category 
fortunately  we have many events with good category info  so we have a pretty good training set to use
for classifying unknown events  the first challenge to get this to work is to find out which features to
use for classification 

preparing the data
the data is a bit awkward to use because it is currently stored in a google app engine project online 
because of processing constraints on app engine  bulk downloading of the data is a bottleneck for my
progress  what i am downloading  though  is a csv file that contains the text for a description  the text
for the title  and then the category  i will need to take a look at the words that are found and extract any
information that is not valid  much like how the data was preprocessed in problem set   for the naive
bayes excercise  i will need to select some set of distinct tokens  then create a file that could be loaded
into matlab to create a matrix of token appearance counts  where the i jth element is the number of
appearances of token j in the training example j 
there are some events that are counted in multiple categories  for our purposes  we don t want to
determine which set of categories it could fit in  just the best one  for that reason  i will randomly
select a category for events with multiple categories to be placed in  this prevents us from being
penalized when two versions of the same event with different categories end up in the testing set 

feature selection
my first attempt at classification is to use text classification on the title and description of the event  ill
try adding in the location afterward to see if that has a positive impact on the classification success rate 
my very first basic attempt will be to treat the title and description as a bag of words that i will then use
for classification 

fiof course  the list of tokens being used could be modified to be a bit more descriptive of the categories
themselves  one method for doing this is selecting k words from each category that have the highest
mutual information score  mutual information determines which tokens give the most information
about a particular category  for example  after calculating the mutual information scores for each of the
categories  i found that the most relevant words in the happy hours category included drink and
appetizer  while the most relevant words in the sports category included football and basketball 

training and classification
no feature selection
the data set includes over        event items  before implementing feature selection i decided to test
out my nave bayes classifier to get an estimation of how much feature selection really helps  in this
training example  we are only tokenizing by spaces  and including each word available  testing is done
by holding out a percentage of the total data set for classification  then comparing the predicted value to
the actual value 

classification error as a function of training set size ratio of data set used for training 

the results i got were not great  but they are a good start  this tells me that this training is a lot better
than it would be if it were to trivially select the most likely category based on the training set  about
    of the events belong to the culture category  so we are getting a better error rate than the    
rate we would average if we just guessed 
with feature selection
now we need to select the best features  based on the mutual information score  and truncate the
dictionary of features so that we only use those in the top k for each category  there are well over      
unique words  on average  in each category  in order for this to work we need to train on only a subset
of those words  there are two approaches  we could take a lump sum from each category and add that
to our dictionary  or we could take a percentage  testing is done just as before  except this time we are
holding the training set size constant at      and testing on the remaining     of the examples  we
will vary the feature set size instead 

fitraining error as a function of number of features taken from each category

training error as a function of percentage of category vocab used as features

without feature selection  the average error rate for training on     of the examples was about      
we only eclipsed that when we selected about      words from each category  or about     of the
vocab of a category  this is not great performance  and it shows that for this particular data set  feature
selection by mutual information does not do much to improve performance 

recommending events
one of the major problems that we encountered while developing this application was that there were
simply too many events out there  and we had no way of displaying relevant ones only  our best bet
was to hand pick a few of them and display those  we had created the ability for users to bookmark
certain events that they like  but we didn t know how we could use that information to curate the events
for us 

fiacquiring the data
unlike in category classification  this time i did not have the luxury of too many data points 
bookmarking  as it turns out  is not something people generally like to do  additionally  we only allow
you to bookmark  and there is no opposite action to show your distaste for a certain event  i set up a
website where people could go vote on random events in order to gather more data  and used that
information for the recommendations 

different recommendation engines
there are two general approaches that i could have used to make recommendations  content based
filtering and collaborative filtering  content based filtering requires knowledge of how to model the
different events  whereas collaborative filtering figures it out based on the similarity of ratings from
users  for this project i decided to use collaborative filtering  but since i already had this nave bayes
classifier for events  i decided to try that out on the recommendations to see how well it did 

collaborative filtering
i implemented this portion using mahout s collaborative filtering library  after formatting the data in a
way that the library would accept it  i fed it in and used the built in slope one recommender  this
provided me with the top recommendations for each user  and the projected rating for the events that a
user had not yet rated 
evaluating the results
my first idea for evaluating the results was to hold out a certain amount of the ratings data for testing 
while training on the rest  i would use the predicted value and use it to compare to the user s   or  
rating  with this evaluation  the correct rating was given        of the time  the average rating for all
of the data points was about       so this was only slightly better than guessing would have been 
collaborative filtering is not great for projecting scores  but excels when you are asking for the top
recommendations  i took the top    recommendations given by the algorithm and asked users to rate
those events  among other filler events   of the events that were given as recommendations  almost
    received favorable ratings from the users  collaborative filtering is more useful when there are a
lot of users rating a lot of different items  unfortunately  i was not able to collect enough data to
provide extremely relevant recommendations  but the modest results i got give me hope that we might
some day be able to implement this a lot better 

just for kicks  nave bayes recommendations
since nave bayes performed decently well when choosing the right category for an event  i decided to
see how well it would do when i ask it to choose if an event would be something i would like or
something i wouldn t like  i had rated     events  i gave    likes and    dislikes  by purely guessing
that i would like an event  i should expect about a     error rate  after running nave bayes  that is
exactly what i got  i decided to check out the most informative words for each category  based on the
mutual event score  to see if there are some key words that determine whether or not i like an event 
turns out there aren t  the words were mostly random  and didn t really make sense  maybe it s just
that there wasn t enough data  but the text content of the event does not necessarily tell us much about
whether an event would seem good to me or not  in order to make the recommendations better  we just
need a lot more user ratings  so we need to emphasize that in our application 

fi