hessian free optimization methods for machine learning
problems

aj friend  ed schmerling  akshay mittal
cs    class project

in this article  we describe the algorithm and study the performance of a hessian free optimization technique applied to machine learning problems  we implement the commonly used black box
model for optimization and solve a particular challenging recursive neural network learning problem  which
exhibits a non convex and non dierentiable function output  in order to adapt the method to machine
learning objective functions  we explore a few modications to the optimization routine such as damping 
mini batching  modifying termination criteria  etc  the convergence properties are compared against the
quasi newton l bfgs method 
abstract 

introduction
in most cases  machine learning models essentially solve a single or series of optimization  minimization 
steps to nd the optimum set of parameters and quantities of interest  the focus of this project is on an
ecient iterative routine to use in these optimization steps  the occurrence of large training sets and number
of parameters is common in machine learning models  as seen in learning problems such at those described
in      where recursive neural networks are implemented  the following sections describe a popular hessian
free algorithm      which was implemented and tested for a complex recursive autoencoder model 

we

model the objective as an unconditional black box  that is  we only get information about the function value
and the gradient  or subgradient  at each iteration step 

we also describe the modications done to the

standard hessian free algorithm to adapt it to machine learning problems  which can exhibit non smooth
and non convex response behavior  in order to ensure the good convergence properties of the newton like
steps 

hessian free optimization

black box model  our aim is to provide an optimization framework that is applicable to a wide range
of problems  in most machine learning problems however  we often run into large data sets and complex
code steps to evaluate the objective function and gradient  and it is often impractical to develop intrusive
optimization models for these problems  therefore  the framework we will use requires minimal information
about the objective function  i e  we only require the function value and a derivative at any query point 
thus  we have an unconditional oracle that takes as input a vector

g  x    f  x 

x

and outputs the function value

f

and

 or subgradient  

hessian free minimization  we implement the hessian free optimization method advanced by martens
in      we are interested in designing a newton or netwon like step at each iteration in the minimization
algorithm  which requires curvature i e second derivative information about the objective function  for large
problems  evaluating  storing and inverting the hessian

h  x      f  x 

can be prohibitively expensive due

to limited computational resources  quasi netwon methods like l bfgs tackle some of these challenges by
eciently storing a hessian approximation  a hessian free optimization algorithm  as the name suggests 
does not explicitly calculate the hessian at any point  the algorithm is based on the quadratic minimization
model i e  newton s method that is described as follows 

date

  dec          
 

fiat each iteration

k 

we seek a search direction

pk

that minimizes the quadratic approximation to the

objective function

 
qk  p    f  xk     pt g  xk     pt h  xk   p
 
 f  xk   p   

   

for reasons noted in the damping section below  we actually consider
term  instead of

h  xk   

b  xk     h  xk     i

as our quadratic

ideally  we would like to compute

pk   arg min qk  p   b  xk   pk   g  xk    
p

the above problem is a linear system  hessian free optimization attempts to solve this system using the
conjugate gradient  cg  iterative method  as it only requires matrix vector products involving

b  xk   

the

advantage of this idea is that we can form these matrix vector products without ever explicitly forming the
hessian matrix  evaluating

b  xk  

times a vector

d

is essentially a directional derivative of the gradient 

g  xk   d   g  xk  
  d


the limit can be calculated using nite dierences  taking   
machine precision  
should note that cg is designed to solve positive denite systems  and that b xk  
b  xk   d    h  xk     i  d   lim

 

where
we

will often be

indenite  we can terminate early or use directions of negative curvature when they are detected to account
for this  also  the system need not be solved exactly  often we can terminate the algorithm early with an
approximate solution to get a good search direction 
the basic outline of the algorithm is as follows 

algorithm   hessian free optimization
   
   
   
   
   
   
   

for k               do
gk  f  xk  
compute update 
dene the function bk  d    h  xk   d   d
pk  cg solve  bk   gk  
xk    xk   pk
end for

recursive autoencoders   test problem
the primary test problem for our implementation of hessian free optimization comes from the semi 

supervised recursive autoencoders for predicting sentiment distributions paper of socher et al       this
paper describes a model where  in a very rough sense  a neural network is used to recursively combine
word phrase vectors of a sentence in order to arrive at a single vector representing the whole sentence  this
vector might then in turn be used to infer a sentiment for the sentence  we wish to choose the neural network
parameters so that the error both in constructing this recursive combination and in deriving sentiments from
the output vectors is as small as possible  the objective function to be minimized is a sum of these errors plus
a regularization term  from this we may see that evaluating the rae objective function is approximately
linear in the size of its data set  assuming a xed distribution on the length of training sentences   a fact we
use to tune hf optimization for maximum eciency  this is the only fact specic to the test problem that
we use  everything else is consistent with a black box model for optimization 

adapting hf to machine learning problems
martens describes a number of modications to the basic algorithm   that he claims work particularly
well for neural network training      we describe our experience with implementing his enhancements for
training the rae model 

 

fidamping  the issue of damping is important to consider in the hf approach because we run cg with
implicit access to the true hessian at each iteration  thus  unlike with l bfgs where we maintain a strictly
positive denite approximation to the curvature matrix  we may run across directions of extremely low or
even negative curvature in hf  if we are not careful  moving along such a direction could result in a very large
cg step  possibly taking the algorithm outside of the region where the newton approximation is even valid 
one solution is to add a damping parameter to the hessian and consider

h

b   h   i  

constantly changing as the hf optimization algorithm searches  we start with

this reconditions

b is
     and update  according

and controls the length of each cg step  mimicking a trust region method 

as the condition of

to a levenberg marquardt style heuristic 

 
  then    
 
 
elseif k     then      
 
if k  

where

k

is the reduction ratio measuring the accuracy of the newton approximation and is dened as

follows 

k  

f  xk   pk    f  xk  
 
qk  pk    qk    

 
 
  but we nd that this value of  is too large for the rae objective  when     
 
and the quadratic model is judged to be inaccurate  increasing  by a factor of   
  is too extreme and
martens suggests

 

forces the steplength per iteration to

  

shutting down any optimization  on the other hand when we deem

 
 
     taking      leads to underdamping and the steplengths
  
grow out of control  we nd that a slower ratio of   
  works well for keeping damping reasonable for

the quadratic model as trustworthy  

 

the rae objective 

mini batching  the cg minimization problem solved in each step of the hf algorithm is only an approximation to the true rae objective  thus it seems reasonable to also approximate the directional derivatives

hd

required by cg if doing so is computationally advantageous  to this end we x mini batches of the

training dataset per hfo iteration to which we restrict our computation of the objective function  in the
rae example of      the training dataset has      sentences sentiments  if we choose a mini batch of only



    elements  each cg step eectively requires only

 
    th of a function evaluation  this is the speed

improvement that really makes hfo worth considering 
martens recommends growing the size of the mini batch as the hf algorithm progresses  but we actually
nd this not to be the case for the rae problem  after trying varying schemes of increasing minibatch size
we observe that keeping a xed minibatch size of

   

not only saves on computation but results in a better

objective function minimum in the end  more discussion of this phenomenon may be found in the results
section 

directions of negative curvature  even with damping  cg sometimes computes directions of negative
curvature for

b 

this is an unavoidable consequence of the rae objective being nonconvex  the standard

approaches are to exit cg and to either step along the last non negative curvature direction or to step along
the new direction of negative curvature  our experiments lead us to conclude that there is no clear dierence
between these two approaches 
in      martens recommends using the gauss newton matrix instead of the actual hessian matrix  as it
is a positive semidenite approximation to the hessian  thus  problems with negative curvature in cg are
avoided 

we did not use the gauss newton matrix  as our black box abstraction only gave us access to

the hessian  this may have been an important dierence between our implementation and      and better
performance might have been achieved with this approach 

mini batched linesearch  martens does not mention this addition  but we nd that hf performance is
improved by using backtracking linesearch on the mini batched function approximation to take a variable
steplength along the direction that cg returns  this is simply another way to take advantage of the relative
cheapness of mini batched function evaluations 

 

fi   

   
hessian free
lbfgs

   

   

   

   
objective function value

objective function value

hessian free
lbfgs

   

   

   

   

   

   

 

 

   

   

   

 

  

  

  
  
iteration

  

  

   

  

 

  

  

  
  
  
function evaluations

  

  

  

figure      comparing convergence properties  hf v s  l bfgs  the left plot gives the
number of hf iterations  the right plot gives the number of function evaluations  where
a function evaluation can be fractional if it only uses a minibatch instead of the whole data
set 

warm starting cg  it is not unreasonable to assume that each cg subproblem is somehow related or
similar  thus  the solution of two successive search direction problems can be similar  as cg is iterative 
starting the algorithm at a point which may already be close to the solution can reduce the number of
iterations needed to converge  martens notes  and we agree  that initializing cg at each hf iteration with
the direction output by the previous iteration s cg step improves the convergence rate 

results and conclusions
we compared our hf implementation to    iterations of the l bfgs algorithm used in      plots comparing the convergence behavior of the two algorithms are given in the gure 
we observed that hessian free optimization performs well against l bfgs at early iterations  but performs
worse later on  we expect this is due to the fact that l bfgs accumulates curvature information at each
iteration  improving the model of its problem  these early approximations are based on only a few gradient
evaluations  and are thus less accurate 
the hessian free approach uses only curvature information at the current point  but is potentially advantageous because it essentially has access to the actual hessian  the relative performance of hessian free
optimization appears to decrease as the l bfgs model  catches up to nding the true curvature information  however  we should note that there are many parameters to hessian free optimization  including
minibatch sizing  damping parameter  back tracking  and the treatment of direction of negative curvature 
tuning these parameters may provide better results on a per application basis and gives the user added
exibility in adapting the optimization method to his needs that l bfgs cannot provide to the same extent
as hessian free optimization does 
we were unable to match the performance of the l bfgs method 

this may be because we did not

suciently tune our implementation  however  it is also very likely that this is due to our use of the hessian
instead of the gauss newton matrix  which martens claims often provides better search directions 
another possibility is that l bfgs methods may simply be better suited to these types of problems  lbfgs makes very good use of the gradient information supplied at each step by incorporating approximate
curvature information into a hessian approximation  in this way  the information from each gradient is used
many times  in contrast  hf uses a gradient s information just once in each cg iteration  we believe that
hf is most promising when minibatching can be employed to make this drawback less severe 
in conclusion  we think that further experimentation with these methods should be done  exploring the
trade os between hf and l bfgs  to determine for which problem domains these methods are best suited 

 

fiacknowledgements
we would like to thank professor andrew ng and ta richard socher for their guidance and assistance
during the course of this project  we also acknowledge the other cs    teaching sta for their support and
help during the quarter towards the completion of this project and its key milestones 

references

   socher  r  et al  semi supervised recursive autoencoders for predicting sentiment distributions  conference on
empirical methods in natural language processing  emnlp       
   martens  j  deep learning via hessian free optimization  proceedings of the   th international conference on
machine learning        

 

fi