cs    final project  predicting news preferences
rory macqueen  deniz kaharamaner  gil shotan
december   th     

abstract

azines combined     this digitization of
news consumption opens up an opportuour goal was to predict news stories that nity for machine learning algorithms to
a user would read based on his or her improve users reading experience 
reading history  we came up with a set
of features for each story  including tf idf
values of the words in the title  the feed
from which the story originated  and a
measure of how many similar users read
that story  finally we weighted training
stories more heavily if a user both read
and clicked through that story  we fed
this matrix of features into a linear kernel svm    and achieved an f  score    of
      while we consider the results to be
satisfactory  we suspect the main obstacle
preventing us from achieving higher accuracy was our inability to accurately determine which stories a user was exposed
to  and  therefore  to determine whether a
non read was because she was not interested in the story or simply because she
did not see it 

introduction
figure    screenshot of the pulse application
this project applies a machine learning algorithm to predict which stories
users will read based on each users reading history  the ultimate goal is to to
tailor a news feed to a particular users

mobile devices have revolutionized the
way people consume news  over the past
three years  the proportion of web time
spent reading news has doubled     moreover  as of       people spend the same
amount of time on their mobile devices
as they do reading newspapers and mag 

fics    final project
interest based on his or her past reading
habits 

method
our data set was three fold  first  we
had a file consisting of all stories available
for the month of august       second 
we had a file containing reads  showing
which stories were read by each of our
     users  third  we had a file showing
which stories were clicked through by each
of our      users  one of our biggest challenges was taking this large data set  and
determining which stories each user was
actually exposed to  the two factors determining exposure were timestamps and
feeds  we decided that it is reasonable
to assume that any day on which a user
had no time stamped read stories indicated a day on which a user did not even
log in  hence that user was not exposed to
any of that days stories  we therefore did
not include those stories in that particular users training matrix  furthermore 
we used one scan through the whole data
set to determine which feed  e g  wall
street journal  techcrunch etc   each
user was subscribed to  this allowed us
to exclude from the users matrix any stories from other feeds  we used a set of
     stories and     users as a working
development set  we evaluated our performance iteratively   by increasing our
training set one day at a time  at each
iteration we attempted to predict the stories read by each user on the following day 
concretely  we started by training on day
   and tested on day    then trained on
days      and tested on day   etc  eventually  we tested our final system on the
other set of     users and        stories 
 we did not include all stories due to long
duration of training  

predicting news preferences

algorithm
we used a linear kernel svm algorithm   
to classify our data into reads and nonreads for a given user  to account for
outliers and the problem of non linearly
separable data  we use l  regularization
on the algorithm  we ran the algorithm
several times on development data  varying the parameter c  which controls the
cost to be incurred for having examples
with functional margin less than    we
found that value of     for c was optimal  we also experimented with different weights for the categories  read and
non read  represented in the algorithm as
  and   respectively  a higher weight for a
given category tells the algorithm that it
is more important that we get the classification of this category correct  even if it
might mean classifying some of the other
categorys points incorrectly  we assigned
a higher weight      to the reads category 
since  intuitively in this scenario  it seems
to be more important to have a higher recall at the expense of precision 

nlp
we started by representing a story only
using the term frequency and inverse document frequency of the words comprising
its title 
tf idf   tf t d   log

 d 
  d   t  d  

where  d  is the number of documents in
our corpus we chose a set of        tokens
to comprise our initial feature space based
on their frequency in more than        
stories  as with most natural language
corpi  the words in a data set of news stories are distributed approximately according to zipfs law    that is  the frequency
macqueen  kaharamaner  shotan

fics    final project

of any word is inversely proportional to
its rank in the frequency table  in other
words  the most frequent word will occur
approximately twice as often as the second most frequent word  three times as
often as the third most frequent word  and
so on  furthermore  we excluded stop
words from our vocabulary since they
have little contextual significance  we
also applied porters stemmer to reduce
similar words to their common morphological root by applying linguistic rules 
hence the words invade  invading and
invaded will all be mapped to a common
root  namely invade  which is what we
eventually stored in our training matrix 
after this initial phase of pre processing 
we chose the most frequent roots as our
set of tokens  we ended up with a very
sparse training matrix  as each story contains only several words  and only a fraction of which appeared in our dictionary 
our preliminary results revealed a high
bias in our hypotheses  which motivated
us to seek other features with higher predictive value 

incorporating feeds
we hypothesized that not all feeds are
treated equally by a given user  most
users are subscribed to over    feeds 
whereas an individual user can only view
  feeds at any given moment on his iphone
screen  an inverse correlation likely exists between the probability that a story
will be viewed and the amount of scrolling
necessary to reach it  furthermore  we
hypothesized that the ability of a particular feed to match a users preference
may vary significantly  some feeds may
produce stories that match a users preference more frequently than others  we
therefore decided to include the feeds of a

predicting news preferences

story as a feature  a scan of the data confirmed our hypothesis that indeed  some
feeds are more popular than others for a
given user 

clustering
another method we employed to gather
insight into a users preferences was to
view the reading patterns of similar users 
our conjecture was that if the reading
patterns of a group of users has been similar in the past  they are likely to be similar in the future as well  intuitively  it
is easy to believe that a certain group of
users is more interested in sports  others
in politics  etc  to test our conjecture we
use principal component analysis     we
defined the preferences matrix p  rmn  
where m is the number users and n is the
number of stories we used for this purpose  where each component of p contained one of   values 


  if user i did not read story j
pij     if user i read story j


  if user i clicked on story j
we implemented pca on the matrix p to
map the high dimensional user vectors to
a three dimensional space that we could
visualize  see figure     this visualization
suggested that users fall into   groups  or
clusters  we implemented kmeans    clustering on our development users  each
cluster was represented as a vector in rn  
which was computed as
x
c q j  
pij i user i belongs to cluster q 
i

and where c q j is the number of users in
cluster q who read story j  during testing  we had to assign each of our new test
users to one of these three pre computed
macqueen  kaharamaner  shotan

fics    final project

predicting news preferences

clusters  each user was represented by
a vector pi  rn   which is a row in the
matrix p   we assigned each user to the
cluster which had the smallest euclidean
distance from its center to the users vector 
ki   arg min   pi  c q   
q

where ki is the cluster assignment for
the user i  euclidean distance ended
up being a more accurate measure of
similarity between a user and a cluster
than cosine distance  cross validation on
the training data showed that euclidean
distance method correctly assigned users
with more than     accuracy  having assigned a test user to a cluster  we were
now able to add to each story of this
users feature matrix an additional feature  whose value was the number of users
in the same cluster who read that story 
the addition of this clustering feature improved results by approximately      
  
  
  
  
  
 
 
 
 

article in its original website  thereby indicating that she has an even greater interest in this story  to account for this
information  we want to tell the learning
algorithm that the words in this title have
even greater predictive value of a users
interest  i e we want to amplify the tf idf
values of these words in all future stories 
to accomplish this  we take each word in
the title and multiply its corresponding
feature column by a given click factor 
this gives more weight to future stories
that contain these words in their title 
experimenting with different values  we
found that a click factor of     was optimal  while incorporating the click factor did help  the gain was modest        
likely  this is because of the fact that a
click through must occur after a read  and
therefore the decision to click through is
probably motivated less by the words in
the title  and more by the content of the
full story  while it is true that the content itself is dependent on the title  and 
hence  there is some back propagation effect from the click through to the title  the
fact still remains that the relationship between the tf idf values of the title and the
click through event is at best an indirect
correlation 

 
 
  
  

  
  
 

  

results

 

 
 

 

figure    principal component analysis
showing   dimensional representation of
users preferences  colored coded by cluster assignments

incorporating clicks
a click through event means that a user
not only read but also opened up the full

due to the fact that we are dealing with
skewed classes  resulting from the relatively few articles read by each user  we
used the f  score    as an indicator of accuracy  our final results yielded an average accuracy of      per user per day 
however  our performance did not improve as we increased the size of our training set  and while the average user accuracy fluctuated around the value of      
the performance of the system diminished
macqueen  kaharamaner  shotan

fics    final project

predicting news preferences
  

for most users 

  

 

  

   

  

   

  

   

  

   

  

   

  

   

 

   

 
 

   

   

   

   

   

   

   

   

   

 

   

figure    histogram showing the distribution of daily f  scores across users for
the first day  in blue   and the last day
figure    accuracy as measured by daily
 in red 
f  score for several users  the red solid
line shows the average f  score of all users
and is hovering around      throughout conclusion and future
our testing period
   

 

 

  

  

  

  

  

work

we consider these results to be highly encouraging  we believe users will be satisfied with a system that could recommend
articles with a     chance that they will
opt to read them  however  we do believe
we could have achieved significantly better results if were better able to filter out
the set of stories a user was not exposed
to  furthermore  due to the limitations of
current natural language processing technology to extract meaning from a single
sentence  we believe better results could
be achieved by training our algorithms on
the entire text of an article  as opposed to
its title alone  in addition  if we were to
implement our system on a real live data
set  our vocabulary would have to be dynamic  incorporating new terms that are
introduced into the media  such a system
would also need to take into considerafigure    histogram showing the distri  tion the dimension of time by placing less
bution of average daily f  scores across weight on stories that appeared in the distant past  as opposed to stories that apusers
peared in the recent past 
our basic tf idf contributed to      of
the observed accuracy  incorporating feed
information boosted our accuracy by      
clustering yielded another      and incorporating click information increased our
accuracy by     
  

  

  

  

  

  

  

  

 
   

   

   

   

   

   

   

   

 

macqueen  kaharamaner  shotan

fics    final project

predicting news preferences

references
    andrew ng  cs      class lecture topic  support vector machines  nvidia
auditorium  stanford university  october   st      
    andrew ng  cs      class lecture topic  advice on applying machine learning
algorithms  nvidia auditorium  stanford university  november  th      
    r  e  fan  k  w  chang  c  j  hsieh  x  r  wang  and c  j  lin   liblinear 
a library for large linear classification  journal of machine learning research
                    software available at http   www csie ntu edu tw  cjlin liblinear
    emarketer         december      mobile passes print in time spent
among us adults  online   available  http   www emarketer com  pressrelease aspx r        
    christopher d  manning  prabhakar raghavan  hinrich schtze  introduction to
information retrieval   online  available  http   informationretrieval org
    andrew ng  cs      class lecture topic  unsupervised learning algorithms 
nvidia auditorium  stanford university  november   th      
    andrew ng  cs      class lecture topic  unsupervised learning  clustering 
nvidia auditorium  stanford university  november  th      

macqueen  kaharamaner  shotan

fi