indoor scene recognition
cs    autumn      final project writeup
lu li  lululi stanford edu   siripat sumanaphan  siripat stanford edu 
december         

 

introduction

while scene recognition problem is not new  it is still a challenging  open ended problem on which a
lot of further work can be done to improve feature selection and applicable learning algorithms  several
computer vision research groups at mit have worked on scene recognition  with many classes of scenery
images ranging from outdoor landscapes to indoor rooms or areas         while the accuracy rate is high
for some outdoor landscapes      classifying the indoor specific areas still posts a difficult task due to
similar image features across categories and huge variation within category 
our primary goal was to be able to identify the bathroom images given a set of images of all areas in
a home  we first selected bathroom simply because of its unique visual features  later on we extended
our work to also include binary classification of other classes as well as multi class classification  our
dataset was downloaded from      which ariadna quattoni et al  also used on for their work on indoor
scene recognition  with a larger set of scene categories       for our work  all algorithms were run on
sift feature space  we chose to work with sift feature as it was reported by jianxiong xiao et al  to
be one of the few top choices yielding high recognition accuracy      in addition  most images of indoor
home areas share a lot of common edge   curve  and corner related characteristics  which sift features
can well capture 

 

methods

we derived the outline for our methods from the paper
linear spatial pyramid matching using sparse coding for
image classification by jianchao yang et al      the
general flow of algorithms is shown in figure    we will
explain our working steps in the subsequent sections 

   

sift feature extraction

we used an online open c   sift extractor library
written by zhenhui xu      the library was written
based on the work done on sift features by david lowe
     the extractor converts an image into gray scale 
and doubles the size  when necessary   for effective local
extrema extraction  before extracting the sift vectors 
for each image  the extractor output a set of keypoints 
 i 
x    xj  rn   n        j           n i      where n i 
represents the number of keypoints for image i  it is often the case that n i     n k  for i    k  even though the
original image sizes might be the same  from experiments  we found that n i  ranges from approximately
    to      
 

figure    algorithm flow  figure from     

fi   

vector quantization using k means

to begin our classification  we first would like to learn for a codebook or a dictionary  the basis with
which to quantize our sift feature vectors  our simple starting step was to learn for this basis using
k means 
we selected the number of centroids  k  to be      below the minimum number of keypoints per
image we have ever encountered  the training for the codebook v  rn k was done on        sift
feature vectors  or keypoints  taken from    random images of seven different areas in a home  i e     
images per category  these areas include bathroom  bedroom  dining room  garage  kitchen  living room 
and children room  the codebook then consists of the k centroids we get from k means 

   

vector quantization using sparse coding

 a  codebook  k      

 b  obj val convergence

 c  codebook  k      

 d  obj val convergence

figure    results of sparse coding  codebooks and convergence of objective values
sparse coding is our alternative to the simple k means algorithm in finding the codebook  it was
reported to be an effective algorithm for this application because image patches are sparse in nature     
here  for a set of random sift features s    x i    i           m    we solve for a codebook v with k basis
vectors by solving    
m
x
min
kx i   v u i  k     ku i  k 
   
u v

i  

subject to kvk k      k              k
where u i  is column i of the matrix u  rkm   and vk is column k of the matrix v  rn k  

 

fito solve for codebook using sparse coding  we used matlab sparse coding software package provided
by honglak lee et al   which can be found at      the work done based on the paper efficient sparse coding
algorithms by the same authors      due to frequent warnings of internal maximum number of iterations
reached  either by matlab fmincon or    s featuresign m in the software package  we experimented on
several different numbers of basis vectors  tolerances  and  values  and performed some evaluations on
the resulting codebooks  see section       we can see from figure   that the objective values converged
only after about    iterations  but the values were still large  although we have no cost function from
k means to compare with  by observation  we found that the final objective value was larger for a
codebook with larger number of basis vectors 

   

histogram as feature for k means based codebook

after training for the centroids  we simply used the histogram
 i 

z
 i 

as our feature for image i  here uj

 i 

n
  x  i 
u
   i 
n j   j

  ek i   rn   where ek is the k th basis vector of the form
j

 i 

 i 

                          with   occuring at the k th index  kj represents the cluster xj belongs to 
the training set   z  i    y  i     i           m    m the number of training images  was fed into liblinear      here  y  i          for binary classification  and y  i                  n  for multi class
classification  where n is the number of classes 

   

maximum of absolute weights as feature for sc based codebook
 i 

for a codebook v derived from sparse coding  we solved for the weight vector uj

associated with a

 i 

keypoint xj by solving equation      but minimizing only with respect to u instead of u and v   in this
step  we again utilized    s featuresign m from honglak lees matlab sparse coding software package     
 i 
after we got uj   we calculated the feature z  i    associated with image i  from
 i 

zk   max  u i     k    u i     k         u i   n i   k    k           n 
the z  i  s were normalized  and fed into liblinear to train for an svm model  the same procedure
as described in section     

   

spatial pyramid matching

in addition to classification based only on the histogram  or
max pooling  of quantized sift features extracted from the
entire image  we also extended our feature vectors to capture
more spatial information of an image by concatenating with
location specific histograms 
in our case  we partitioned the image into  l   l sections using l     and    for a partition  we extracted sift
features and quantized them to get a histogram  or weights 
specific to that partition and scale  all histograms were concatenated together to form a higher dimensional feature representation of the image  these longer features vectors were
used to train a linear svm in the same manner as mentioned
before  we expected the trained model to be more effective for classification of classes with large enough dataset to
match the dimension of the feature vectors  now         
e g   living room  kitchen  and bedroom  each having more figure    spatial pyramid matching  figthan     images in our dataset 
ure from     
 

fi 
   

results
sparse coding  codebook evaluation

the evaluation was done by binary classification of bathroom images vs all other classes  the features
used were from l     only  z  i   r       the   train and   test are based on the number of bathroom
images available in the dataset        images from other six classes were randomly selected so that the
numbers of positive and negative samples are equal in both the train and test sets 
table    sparse coding  codebook evaluation
sc parameters
 k             
tol          s 
 k             
tol         s 
 k              
tol        s 

    train 
    test
          

    train 
    test
          

    train 
    test
         

    train 
    test
          

    train 
    test
          

          

           

          

          

          

          

          

          

          

          

here we show the mean accuracy  one standard deviation  s is the set of sift features used  s 
includes        sift features extracted from the total of    images     randomly selected images per
class  s  includes        sift features         randomly extracted features per class  codebook   with
k        seems to perform best for binary classification of bathroom 

   

binary classification

one specific class vs all other classes      train and     test
table    binary classification results
algorithms
k means  l    
k means  l       
sc   l    
sc   l    
sc   l    
sc   l       

bathroom
      
      
          
          
          
          

bedroom
      
      
          
          
          
          

kitchen
      
      
          
          
          
          

dining room
      
      
          
          
          
          

living room
      
      
          
          
          
          

for results of k means  the negative examples for each class were selected evenly among the other
four classes  and all the data available were used  i e       images for bathroom      images for bedroom 
    images for dining room      images for kitchen  and     images for living room 
for the results of sparse coding with l     the negative examples were selected evenly among the
other six classes  and the number of positive images for each class was fixed to be          train     test  
for the results of sparse coding with l         the negative examples were selected from the other four
classes only  the number of positive images was still fixed to be      hence  we actually had the number
of training data points much less than the dimension of the feature vectors          

   

multi class classification

we ran multi class classification on five classes   bathroom  bedroom  kitchen  dining room and living
room  with again     train and     test  the baseline accuracy for random guess is     
for codebook generated by k means  l     and    all images used  
using crammer and singer algorithm  accuracy         
using one vs all algorithm  accuracy         
 

fifor codebook generated by sparse coding  l          images per class  
using crammer and singer algorithm  accuracy                                          
using one vs all algorithm  accuracy                                          

 

conclusion

from our results for binary classification for each category  we can see that bathroom has higher accuracy
compared to others  it makes sense since bathroom is the most distinguishable among the five classes
and has very different objects and layout  sparse coding slightly outperforms k means on bathroom and
living room classification while the opposite case for the other three classes  the results from sparse
coding for l        did not improve the accuracy possibly due to small number of data points available
for training  much less than the dimension of the feature vectors  we could not generate z  i  s for the
entire dataset for l        using sc based codebook due to time limitation  the computation was fairly
expensive for the steps involved  i e   generating all the keypoints  doing l  feature sign  and max pooling 
from the results for multi class classification  although the accuracy is not high  our results are better
than the baseline random guess  sparse coding with l     and one vs all algorithm did particularly well
compared to other combinations of algorithms  especially with codebook  where we had k        these
results seem to suggest the underlying structural difference among the five categories that we explored 

 

acknowledgement

we would like to thank will zhou for providing us with ideas  advices  and recommending us the paper
linear spatial pyramid matching using sparse coding for image classification      from which this project
was originated 

references
    machine learning group at national taiwan university  liblinear  a library for large linear classification  http   www csie ntu edu tw  cjlin liblinear  
    honglak lee  alexis battle  rajat raina  and andrew y  ng  efficient sparse coding algorithms 
http   ai stanford edu  hllee softwares nips   sparsecoding htm 
    honglak lee  alexis battle  rajat raina  and andrew y  ng  efficient sparse coding algorithms  in
in nips  pages              
    david g  lowe  distinctive image features from scale invariant keypoints  international journal of
computer vision                    
    ariadna quattoni and antoni torralba  indoor scene recognition database  http   web mit edu 
torralba www indoor html 
    ariadna quattoni and antonio torralba  recognizing indoor scenes  in cvpr  pages         ieee 
     
    jianxiong xiao  james hays  krista a  ehinger  aude oliva  and antonio torralba  sun database 
large scale scene recognition from abbey to zoo  in cvpr  pages           ieee       
    zhenhui xu  sift extractor from open pattern recognition project  http   www openpr org cn 
index php code of individual algorithms    siftextractor view details html 
    jianchao yang  kai yu  yihong gong  and t  huang  linear spatial pyramid matching using sparse
coding for image classification  in computer vision and pattern recognition        cvpr      
ieee conference on  pages            june      

 

fi