accent recognition with neural network
matthew seal  matthew murray  ziyad khaleq
training data
our primary goal for our training data was to obtain audio clips of individuals
with a variety of accents saying the same phrase  while we also hoped to obtain a large
number of training samples  we were focused on obtaining training samples containing
the same words for our initial sample so that our features would be the same for each
training sample 
after some research  we found a university sponsored site that contained exactly
the type of data we were looking for  http   accent gmu edu   this site had clips of
numerous different accents saying the following phrase 
please call stella  ask her to bring these things with her from the store  six
spoons of fresh snow peas  five thick slabs of blue cheese  and maybe a snack for her
brother bob  we also need a small plastic snake and a big toy frog for the kids  she can
scoop these things into three red bags  and we will go meet her wednesday at the train
station 
due to the academic nature of this site  all of the audio clips contained essentially
no background noise  making our preprocessing simpler  furthermore  since the phrase
is contains a variety of words  we had a large number of features to use in our algorithm 
the next step was to preprocess this data into useful training samples 
preprocessing
there were two steps to our preprocessing  the first step was to convert the audio
clips from george masons website from  mov files into something more easily
manipulated in matlab  we chose to convert them to  wav files  and we used the audio
converter tool from iorgsoft to easily convert the  mov files into  wav format 
the second step was to split the  wav files into clips of individual syllables  we
planned to use each syllable as an input feature to the algorithm  in order to determine
the syllables  we used a magnitude window filter to select samples from the original clips 
in speech  there is important information at the beginning and the end of each syllable
that can be used to distinguish between accents  consequently  we desired to produce
samples that captured the entire length of the syllable  after experimenting with a
number of different sample lengths  we ultimately decided  based on performance results 
to produce samples clips that were a minimum of    ms and a maximum of    ms in
length  samples in this range of lengths seem to fully encapsulate most syllables 
once we have split original sound clips into samples  we normalize the
magnitudes of each sample to account for any differences in natural speech volume 
also  we shift each sample by the center frequency of the original speakers voice
 determined by examining the frequency content of the original clip   the sample is
shifted so that it is centered around the average speaking frequency of humans  the
combination of normalization and shifting is designed to eliminate any extraneous noise
not related to accent that might arise from a speakers gender or natural speaking volume 

fiaccent recognition with neural network
matthew seal  matthew murray  ziyad khaleq
the shifted and normalized sample is then divided into buckets according to the
frequency content of the sample  experimental results indicate that approximately  
buckets produce optimal results  these buckets containing information about the
frequency content of the sample are then the inputs to the neural network 
neural network design
our neural network design was built from scratch in python using no external
libraries  except numpy for preprocessing the wav files  to get all of the elements of our
final network complete we had a large coding task ahead of us  we started with a simple
three layer feed forward network that could take any set of inputs to the first layer and
give any output we desired in the final layer  we defined links between nodes and
between input output and nodes to have an explicit transfer function which defaulted to a
sigmoid function  the network thereby defaulted to taking in arbitrary data and returning
a confidence value from    to   for each discrete output 
we then analyzed the various options we had for choosing our learning algorithm 
the traditional method of training a neural network is to implement back propagation 
back propagation however is prone to local minima problems  which we believed accent
recognition has in abundance  we looked at several approaches and eventually decided to
try and implement a genetic algorithm to perform our learning based initially on training
feedforward neural networks using genetic algorithms  david j  montana and lawrence
davis         there is statistical and empirical evidence that this approach avoids many of
the localization problems found in high dimensionality state spaces  though convergence
can sometimes be slower than back propagation 
to produce our genetic algorithm we had to first create a way of obtaining a
neuron fitness indicator  the indicator needed to provide information about how much
the given neuron helped or hurt our output results  a simple approach would be to run the
network once for each neuron without the contribution of the given neuron and compare
the distance from training results against the original forward propagation  this however
makes learning an o n   operation 
to avoid this problem we made a neuron method which recursively removes its
effective input from the network without disrupting neurons that arent forward attached
to the original neuron  the results of running this fitness algorithm allow us to compare
the change in distance from training values with and without a neuron present in o   
time for a fixed number of output neurons  the overall algorithm then becomes o n  time
to determine the fitness of every hidden neuron on a network  the same method can be
applied to input neurons on trained networks to perform principle component analysis on
our input data and shrink our overall network size 
genetic algorithm
in implementing our genetic algorithm we had many choices in what elements we
could include in order to enhance performance  the primary feature of genetic updates
comes from a cross over technique where new neurons are made by combining parent
links and weights in interesting ways  much like chromosomes in cell meiosis 

fiaccent recognition with neural network
matthew seal  matthew murray  ziyad khaleq
however  there are a wide range of other techniques employed  including elitism 
immigration  mutation  colony separation  multiple parent cross over which can improve
performance in difficult problems  we chose to implement all of the aforementioned
techniques except colony separation 
the choices for our genetic algorithm resulted in a network where evolving the
network caused the following set of operations to occur  first we gathered the fitness of
each neuron in our hidden layer  then we randomly picked by weighted fitness a
percentage of our neurons to continue onto the next iteration as elite nodes  and finally
for the remaining neurons  up to the number of neurons that were present in our network
before  we either introduced an entirely new immigrant neuron with new weights and
links or performed cross over to generate a neuron from our existing network  this
produced an entirely new hidden layer for our network which was based on the prior
hidden network and the fitness of our neurons 
some of the process of evolving our network was enhanced to improve
convergence and local minima avoidance  for example  when we performed cross over
we used four parents rather than biologically common two parents to generate a new
neuron  additionally we added mutations to each cross over event to allow weights to
change from their original parental values  these mutations were allowed in rare cases to
change the entire transfer function between nodes from a linear relationship to a more
exotic function such as an exponential or other non linear operator  variations on a
simple genetic algorithm  douglas        provided a good set of initial parameter
values for these additional algorithms and gave us an indication of which parameters
could help with which type of problems as they appeared in our training process 
complete network topology

fiaccent recognition with neural network
matthew seal  matthew murray  ziyad khaleq
training and initial results
once we had our network build  our training data categorized and preprocessed 
and debugged our codebase we implemented a variety of training processes to attempt
accent recognition on a set of foreign english speakers  we started with stochastic
updates to our network  where we learned on a single speaker for an extended learning
period before attempted to introduce a new speaker  this process resulted in the network
recognizing a single speaker with      accuracy but which was unable to effectively
learn to differentiate two out of three speakers from each other  the network would overfit the particular training samples it had last received and forget how to recognize
speakers from earlier training  the process did show that our evolver was very effective
at matching a single speaker at a time though and could reach near      identification
with just    syllable training inputs 

given that our network had difficulty with stochastic updates  we decided to
apply batch updates to allow the network to see many speakers in short succession  this
resulted in our network never converging and eventually  once it had seen many samples
 to select a state which miscategorized all speakers  but with less total error than
correctly categorizing a single speaker  once the network was in this fixed state its
accuracy was close to random guesses  the rate at which the network reached a steady
state was heavily influenced by which data we used for hold out and what data we used
for training  certain samples seemed much more influential than others 

fiaccent recognition with neural network
matthew seal  matthew murray  ziyad khaleq
input changes and conclusions
with the knowledge that our network seemed unable to find a solid differentiation
between speakers and that specific samples had large affects on our network
performance  we re approached our input methodology  our original input was         
ms samples of syllables which were slit into sets of   fft buckets from time samples of
the beginning  middle and end of the sample  we tried increasing the number of buckets
and changing the size of our samples to include more phonemes per sample  changing
the number of buckets did not help the network  but using larger samples seemed to
provide some minor improvements in some cases  our best results were around    
accuracy in identification of various speaker accents on our hold out data 
given the difficulty of the task and the fact that we made certain assumptions
about the information needed to resolve accent differences it makes sense to us that our
network would have difficulty identifying the subtle differences in accents  we think
from our data that there isnt sufficient information in the sample formats we were
providing to conclusively distinguish various accents  the neural network was learning
more to recognize speech than to recognize a style of speech  to resolve such problems
we believe that a more detailed analysis of speech patterns is required before our network
could better resolve the input data  detailing the difficulties which foreign english
speakers have with pronouncing certain words could lead to a more structured input of
data into our network 
overall we were glad to be able to create a machine learning algorithm which was
outside of the scope of this course and apply the theory taught during the quarter to better
understand where our algorithm would do well and why it was not working as well as we
would have liked  we didnt want to just use a prebuilt library to analyze a task we knew
we could already solve  but instead see what limitations are present in applying new
techniques to hard problems and to learn how to build and alter machine learning
algorithms to our focus problem  we succeeded in finding where the algorithm was
limited and what we needed to focus on if we were continuing this work into another
quarter  with better pre processed data and variations on the network parameters we
think that our network could be trained to accurately predict a speakers accent  but that
with the data we gathered the problem space was not separable 
references
douglas  scott h   variations on a simple genetic algorithm   unpublished ai work 
web     nov         http   www scottdouglas net projects ga paper  pdf  
montana  david j   and lawrence davis  training feedforward neural networks using
genetic algorithms  international joint conferences on artificial intelligence 
      web     nov         http   ijcai org   
shibata  katsunari  and koji ito   gauss sigmoid neural network   neural networks 
      ijcnn      international joint conference     july       web    dec 
       http   ieeexplore ieee org xpls abs all jsp arnumber        tag    

fi