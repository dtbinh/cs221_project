reddit recommendation system
daniel poon  yu wu  david  qifan  zhang
cs     stanford university
december   th      

   introduction
reddit is one of the most popular online social news websites with millions of registered
users  a user can submit content  identified by links   and each of these can be upvoted or down voted by others  reddit does not yet have a recommendation system so the
goal of this project is to develop one using machine learning concepts  our model can
also be used to rank multiple recommendations since each recommendation has a realvalue score 
   data and statistics
the initial data set  obtained from the reddit team  had            votes         users
and           links  within        sub reddits   this data set represents     of all votes
in      
we reduced the amount of data used to improve efficiency  for instance  the number of
links was too large so we eliminated all links which had less than     votes  the reduced
data set has           votes         users and       links  within    sub reddits   some
additional statistics follow 
statistic
links with more than     votes
links with more than       votes
average value of votes
 sum of all vote values   number of votes 
number of down  votes
number of up votes
number of votes per link
number of votes per user
number of votes per sub reddit
number of users per sub reddit

value
   
   
      
       
         
   
  
      
   

   model evaluation  rmse
in order to evaluate our predictions  we use the widely acknowledged root mean square
error  rmse  defined as 




at test phase  we evaluate on     of the data which had been held out during training 

fi   baseline  naive predictor
since about     of votes are up votes  value     our first attempt is to try a naive
predictor that guesses    for all votes  by doing so  we obtain an rmse of         note
that this is a decent result because the statistics show that average value of votes is        
which is close to   
   k nearest neighbors  knn 
in the knn model  each user is represented as a vector of votes         or     and each
vote is for a particular link  we find the nearest neighbor users based on euclidean
distance  the number of nearest neighbors  k  was adjusted manually 
in the first variant of the knn model  we predict an unknown user vote as follows  if the
link has been seen by at least one neighbor  we predict the average of his neighbors
votes for the link  otherwise the link is unseen and we predict the users own vote average 
unfortunately  the best performance was seen at k    rmse            which suggests
that  in general  user average vote on unseen links is a better predictor than neighbor
average on seen links 
the model above was improved by predicting a weighted average of neighbor votes
 wneighbor        and user average  wuser        for seen links to achieve rmse           
at k      
one variant that did not lead to improvement was using link average rating for all unseen
links  the best performance with this approach yields rmse            at k      
    incorporating sub reddit data
since the data provided has sub reddit information  where each link belongs to one subreddit  it was important to incorporate this natural clustering in our model 
we were able to further improve the knn model by predicting each unseen link as a
weighted average of average sub reddit votes  wsub reddit        and average user vote
 wuser         this gives us rmse            at k       the improvement we obtained
confirms that the clustering inherent in sub reddit data is useful for the recommender 
   singular value decomposition  svd 
svd is one of most widely used models for collaborative filtering problems  data in
recommendation systems usually comes with very high dimensionality  the algorithm
assumes that the data actually lies in a lower dimensional subspace  therefore  by
compressing and reducing dimensionality  we try to discover hidden correlations and
patterns in the data while reducing noisy and redundant features  in essence  the
algorithm tries to decompose the original user link matrix a into the product of three
matrices 

fifrom this  we can construct diagonal matrix s so that
we have 

  now

here  a  is the best rank k approximation for the original matrix a  and we use the entries
of a  as predictions 
since users only provide ratings for a small set of links  we need to fill the missing values
in a  we have tested two approaches  filling with the average vote for a particular link 
and filling with average user vote  empirically  we have determined that the first
approach generates a better result so that is what we used in the final model  for the
decomposition algorithm  we used a randomized algorithm implemented in the redsvd
library  this algorithm very efficiently solves the decomposition with high accuracy 
svd by itself obtains an rmse of          with r       and k      
   bayesian probabilistic matrix factorization using markov chain monte carlo
 bpmf mcmc 
this model tries to tackle the matrix factorization problem through a bayesian approach 
the prior distributions over user and link vectors are assumed to be gaussian 



in addition  the model also places gaussian wishart priors on the user and link hyper
parameters
and
 

here w is the wishart distribution with

degrees of freedom  and

is a dd matrix 

the model is then trained through gibbs sampling  bpmf mcmc by itself achieves an
rmse of         which is a very decent performance 
   stochastic gradient descent  sgd 
we have also tested a simpler model based on stochastic gradient descent  let be the
link vector 
be the user vector  and
be the rating from user u for link l  the model
tries to minimize the prediction error 

fi
during the training process  we first calculate the error
update the parameters in the following way 

  then we can

sgd is able to attain an rmse of           although sgd does not perform as well as
bpmf mcmc  we determined that the performance was still decent enough that sgd
should be included in our final combined model 
   k means
another model we tried involved using k means  as with knn  each user is represented
as a vector of votes  one vote for each link  we used k means to cluster similar users and
make predictions for a test user link pair by taking the average of votes made by other
cluster members for that particular link  in the case where no cluster member has seen the
link  we predict user average  this algorithm gives us rmse of       for both    and
    clusters  however  upon closer examination  it turns out that the majority of users
       were assigned to one cluster  clustering had very little effect  therefore  we
decided to exclude k means from the final model 
    linear combination of models
at this point  we had four models  knn with sub reddit data  svd  bpmf mcmc  and
sgd  that we wanted to use in a linear combination for the final model  these will be
indexed in order    through    given a test user link pair  a model will make the
prediction of
  where u is the user and l is the link  we also added the   
and    constant terms to the linear combination  equivalently 

our linear combination is 

we implemented an automatic parameter tuner to efficiently adjust the weights for each
model  each weight was initialized to   at each iteration  the algorithm randomly
chooses a weight and assigns it a random real value in the range         if the combined
model performs better than the current best rmse  the assignment is kept  if
performance is the same as the best  the assignment is kept with     probability 
otherwise  the assignment is rejected 

fiseveral variations of this tuning process were attempted  we tried to normalize the
weights so that they sum to   after a weight assignment at every iteration but the
algorithm converged much slower than without normalization  we also tried the uniform
distribution u       and gaussian distribution n wx      wx   when generating a real
number at each iteration  the uniform distribution was much faster at zoning in on a set
of near optimal weights  while the normal distribution was much more useful to fine tune
weights  note that for the normal distribution  since variance approaches zero as wx
approaches zero  the weights can get stuck near zero  to address this  we ensure that
variance          our final parameter tuner runs about    iterations with uniform
distribution  followed by about     iterations with the normal distribution  the resulting
weights were  w               w               w           w               w   
           and w              this gives us a combined model rmse of          
which is better than any model individually  and the best performance overall  the
following graph summarizes the performances of all models 

    conclusion and future work
using data provided by the reddit team  we were able to build a recommendation system
which achieves an rmse of          using a linear combination of knn with sub reddit
data  svd  bpmf mcmc  and sgd  however  the recommender can certainly be
improved further 
the provided data has limited features  there are many powerful models that could be
used if we were given more detailed data  for example  if we had time stamps associated
with each vote  we would be able to test a few popular temporal models  intuitively  more
recent votes should be valued higher than older ones  furthermore  if we were given the
actual urls and titles of the links rather than encrypted ids  we may use nlp techniques
to add more features to the data  another promising group of models that we should test
are ones based on weighted ratings  although the original data does not have any weight
or confidence information  we can try to infer the weights based on the popularity of the
links and the activeness of the users  these additional features all have the potential to
decrease our rmse score and significantly improve the recommendation system 

fi