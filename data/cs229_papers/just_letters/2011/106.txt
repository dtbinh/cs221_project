yelp        times more information per view
sean choi  ernest ryu  yuekai sun
december         

abstract

in this project  we take a dierent approach and use
whats called the max norm 

in this project we investigate two machine learning
methods  one supervised and one unsupervised  that
will allow the information content of yelp data to
be eciently conveyed to the users  the rst is matrix completion via the novel max norm constraint
which out results show to be more powerful than the
traditional nuclear norm minimization  the second
is text summary via sparse pca which can provide
a concise summary of the available immense text reviews  we implement and run these algorithms on
actual yelp data and provide results 

     

our data matrix x  rnm is the restaurant star
ratings matrix  i e  xij is the i th users raring of
the j th restaurant  it is incomplete so we only
know a subset of the entries with indices  i  j  
        n         m   we normalize the rating by subtracting   to every entry so that xij 
                 this makes x uniformly bounded
in the sense that  xij      for all i  j 
     

 

data

introduction

matrix completion using max norm
regularization

the max norm of x is dened as
modern consumers are inundated with information
 
 
and choices  countless services now provide a dauntxmax   inf
max ui   max vi  
   
i     m
x u v t i     m
ing amount of information to a casual consumer  this
plethora of information brings the need for a recomand ui   vi denote the i th row of u and v respectively 
mender system provide individually customized lists
we consider the max norm constrained version of
of relevant products and a concise summary of what
matrix completion 
the product is 

in this project we investigate two methods to improve
min
 xij   u v t  ij   
yelps service  a better recommendation system via
u rnk  v rmk
 i j 
matrix completion with the max norm and a text
subject to  u v t max  
summary system via sparse pca 
where k is the rank of the prediction matrix which
can be interpreted as the number of latent variables 
  theory
our heuristic to solve this non convex constrained op    collaborative filtering with uniformly timization problem is alternating minimization over
u and v   fixing v   we obtain the following n embarbounded data
rassingly parallel convex optimization subproblems 
collaborative ltering  cf  has been popularized in
min xit  vj ui   
the past few years by the netix challenge  the
ui rn
mathematical statement of the problem is as follows 

subject to  ui   
can one reconstruct a matrix when only a subset of
max jm vj  
the entires have been observed 
one popular method is nuclear norm minimization which is a k  k qp and in particular is the wellsupported by the theory of compressed sensing      studied trust region subproblem     which can be
 

fisolved very eciently for small k by explicitly com   is    in particular the choice      will ensure that
puting the eigenvalue decomposition   a similar re  all our predictions will lie in the interval        
sult holds for xing u and we obtain the following
matrix completion algorithm  we again emphasize
the second iterpretation is the maximum margin
classier       consider the max norm variant of the
algorithm   max norm matrix completion
matrix completion problem
while not converged do
for i        n do
min y max
y
minui xi  ui v t  
subject
to 
x
subject to  ui       max jm vj  
ij yij      i  j   
end for
y can be decomposed into y   u v t hence every
for j        n do
entry of y can be expressed yij   uti vj where ui
minvj xj  u vj  
and vj are the rows of u and v respectively  we
subject to  vj       max in ui  
can interpret ui as a feature vector for user i and
end for
vj as a classier that classies users into users that
end while
like and dislike movie j  the features are the anity
that the for loop operation of algorithm   is embar  of user i to the k th latent variable  the constraint
xij yij    ensures the classier correctly classies
rassingly parallel 
users who have rated movie j 
on the other hand the well known svm optimization
      bias correction
problem is
nothing in algorithm   constrains the resulting prediction matrix y   u v y to have the same mean as
 
min w 
the data matrix x  in fact the discrepancy of the
w b  
means of y and x is not negligible by correcting this
t
subject to  yi  w xi   b      i           m
bias by letting
since xmax is dened as      each subproblem is
essentially seeking for a maximum margin classier

y   y   y  x  
where
y  

  
yij
nm
ij

x  

  
xij
  

 
min vj  
w  
subject to  xij  uti vj           i  j   

 i j 

provides an improved rmse empirically for us and
this interpretation suggests that the max norm regothers      
ularization is a generalization of the approach to nd
      interpretation of max norm regulariza  the maximum margin classier for a dataset that only
contains a boolean anity of the users for each movie 
tion
there are two interpretation that justies the use of
max norm regularization  the rst is that the max      using spca to summarize large text
corpora
norm regularization implies that the entries of y are
uniformly bounded by    by the cauchy schwartz yelp has a large database of text data that would
inequality 
make sense to a human reader  however many popt
ular restaurants have more than     revies and it
y max     yij      ui vj    ui   vj    
is unreasonable to expect users to make a judgement
for the application in consideration  valid predictions about the restaurant after digesting this large amount
are integers between   and   so a sensible choice of of data  in a sense  yelp is presenting these reviews to
users very ineciently  our goal is to implement an
 
the qp matrix is symmetric so for small k the eigenvalue
decomposition does not pose a significant numerical challenge  algorithm that would provide a concise summary that
would represent the key features of the restaurant 
however  the complexity grows at a rate of o k    
 

fi     

data

where st is the soft thresholding function
 
our data matrix x consists of tf idf scores  term
sgn x   x     if  x   
st  x     
frequency inverse document frequency  tf idf  score 
 
otherwise
commonly used in natural language processing  is dened as
this yields the following spca algorithm where the
application of st is embarrassingly parallel 
 terms  d
 d 
tf idf t  d   
log
 words  d
  d   t  d  
algorithm   spca for text summary
x    x
where t is a specic term and d is a particular documfor i        k do
net  each column of x corresponds to a document
while not converged do
and each row to a term 
ui   st  xi vi  vi      u  vi     
vi   st  xi ui  ui      v  ui     
      sparse principal component analysis
end while
 spca 
xi     xi  ui vit
principal conponent analysis  pca  is a widely used
end for
tool and is applicable to this application of text summary  however  the principal components generated by pca are hard to interpret as they are not
sparse  el ghaoui et al      suggested that sparse       varying the sparsity parameter u   v
pca  spca  can be eectively used to extract con  although this aspect was concealed in the previous
cise pricipal components which can be presented into algorithm discussion for the sake of conciseness  it is
a concise list of words 
important to impose a dierent spasity parameter for
the mathematical statement of spca is    
each principal components 
when the sparsity parameters u   v are xed
throughout the iterations  eventually the residual matrix xi     xi  ui vit becomes small  and the rewhere   f ro is the standard frobenius norm and sulting pc and loadings become identically    if
    is the element wise   norm  i e 
u   v are chosen to be small enough to avoid this
phenomenon then the initial pcs will be too dense 
k
n 

a solution to this problem is to decrease the value
 uij  
u    
of u   v throughout the iterations  in particular  we
i   j  
let u   u u and v   v v when the resulting pc
u   v are parameters that control the sparsity  this became identically zero  empirically        worked
approach is consistent with the standard method of well 
imposing an   penalization term for sparsity 
this optimization problem  however  is non convex
and non dierentiable  the standard heuristic that   methods
will nd a local optimum is block coordinate descent 
    data processing
to be precise  the i th principal component and its
corresponding loadings are found sequentially with to test our algorithms we have collected     million
the i th residual matrix xi by iteratively minimizing restaurant reviews for about        restaurants and
over u and v  the sub problem becomes
about   million users  ranging across the entire continental usa  we specically collected four features
 
min x  uv t  f ro   u u  
of each reviews  restaurant name  user name  user
u  
ratings and user text reviews  the raw data was rst
and with the use of sub gradients one can nd the inserted into a database and auto increment primary
analytical solution
key of the mysql database     was used to generate
 
 
 
 
 
an example of a quantitative measure of this could be the
x
v
 

u   st
i
i
u
frobenius norm 
v  
v  
 
min x  u v t  f ro   u u     v v  
u v  

 

ficompetition       which of course is done on an entirely dierent dataset 
we can see that the max norm matrix completion

restaurant ids and user ids   the nal data was compressed into restaurant id  user id  user rating and
user ratings  and exported into a xml format 
to convert the xml format into a format that resembles a sparse matrix representation we utilized the
amazon ec  cluster and hadoop 
extracting the restaurant ids  user ids and user ratings was straightforward  as for processing the text 
we decided to use stemming and to remove stop words
as it is arguably practice in text data mining     and
as it empirically gave better results  for stemming
we utilized the apache lucene     library and for stop
word removal the porter stemmer     algorithm  after this pre processing  the text data was formed into
a tf idf score vector as mentioned before 

   

rmse

   

nuc n
    

mn
    

mnbc
    

netix
    

table    comparison of matrix completion results
outperforms the other two standard algorithms  we
have included the netix result as merely a reference
to suggest that the performance of mn is not outrageously sub optimal  the rmse value of      is
not comparable to our result as the netix dataset
has quite dierent statistical properties compared to
out yelp dataset and the dierence in performance
should not be understood as a defeat 

sparse linear algebra
   

because of the data size  traditional dense linear algebra becomes infeasible and therefore we utilized
matlabs sparse matrix functionality for our algorithms  however  there were some diculties  especially in the spca algorithm  where we would encounter x  uv t where x is sparse and u  v are vectors  when this expression is evaluated the resulting
matrix loses its sparsity entirely  our solution was
to utilize stanford icmes shared memory machine
wich oers    gb of ram 
for future work  one could consider a sparse linear
algebra implementation that can retain the above
expression  which is very sparse  without explicitly
evaluating the expression  we did not take this path
due to time constraints 

 

ar
    

spca text summary

many of the sparse principal components from the
spca algorithm had a clear interpretation  restaurants with loadings of such pcs usually belonged to
the categories consistent with the pcs interpretation 
table   shows some example principal components
 rd pc
italian
pepper
crust
italian

 th pc
taco
burrito
maxica
salsa

  th pc
naan
indian
buet
masala

  th pc
dimsum
dumplg
chinese
noodle

  th pc
falafel
gyro
pita
sandwi

table    example principal components with their
most signicant entries 
from the spca algorithm with the   most signicant
 largest value  entries  the meaning of these pcs are
unambiguous  table   shows some example loadings

results
max norm matrix completion

the rank parameter k was set to k       as men  rest  name
tioned before there is a signicant cost in increasing el gran amigo
the value of k but emprically there was diminishing tagueria
returns in increasing k beyond    and in particular la mediterrak      did not yield a signicant improvement 
nee
the standard assessment of a matrix completion algo  beijing
rithm is the root mean square error  rmse  of cross  restaurant
validation  to this end  we held out     of the data tommaso
for cross validation 
ristorante
the max norm  mn  and max norm with bias cor  italiano
rection  mnbc  algorithms are benchmarked against
the average rating  ar   nuclear norm minimization table    example
 nuc n   and a result from a candidate in the netix pal components 
 

pc 
   

pc 
    

pc  
    

pc  
   

pc  
    

    

    

   

   

   

    

    

   

   

    

    

   

     

   

   

loadings of the   presented princi 

fifrom the spca algorithm  the loadings provide a
clear indication on what type of restaurant each one
is  

 

    l  el ghaoui  g  c  li  v  a  duong  v  pham 
a  srivastava  and k  bhaduri  sparse machine
learning methods for understanding large text
corpora  in proc  conference on intelligent data
understanding  october       accepted for publication  july      

conclusion

we investigated   learning algorithms that are ap      yehuda koren  factorization meets the neighplicable to recommender systems and we tested the
borhood  a multifaceted collaborative ltering
feasibility on a specic real world dataset  although
model  in proceedings of the   th acm sigkdd
this report did not detail on the complexity analyinternational conference on knowledge discovery
sis of the optimization algorithms and sparse numerand data mining  kdd     pages         new
ical linear algebra  the algorithms are quite ecient
york  ny  usa        acm 
and scalable as they reduce to embarrassingly parallel subproblems 
    jorge nocedal and stephen j  wright  numerical
matrix completion using max norm regularization
optimization  springer  august      
showed promise as it was able to outperform the
standard nuclear norm minimization algorithm  ul      h shen and j huang  sparse principal component analysis via regularized low rank matrix aptimately  it is unlikely that the max norm matrix
proximation  journal of multivariate analysis 
completion algorithm alone will fare well against its
                     
competitors as the best matrix completion algorithms
utilize blending  or mixing of dierent such algo      jerey l  solka  text data mining  theory and
rithms      however  the max norm matrix complemethods  statistics surveys                
tion algorithm could contribute to this blending as a
powerful ingredient 
     nathan srebro  learning with matrix factorizatext summary via spca also showed considerable
tions  phd thesis  mit  cambridge  ma  usa 
promise as it was successful in detecting the most im      aai        
portant features of restaurants  however  it did lack
the ability to extract ner features such as whether      yunhong zhou  dennis wilkinson  robert
schreiber  and rong pan  large scale parallel
the service of the restaurant is good or whether the
collaborative ltering for the netix prize  alrestaurant is crowded  in that regard  there is room
gorithmic aspects in information and managefor future work with text summary via spca  in
ment                     
a practical standpoint  automatic text summary of
yelp reviews are not very powerful if it cannot extract
any information beyond the genre of the restaurant 

references
    http   dev mysql com doc refman     en 
example auto increment html 
    http   lucene apache org java docs index html 
    http   snowball tartarus org algorithms english 
stemmer html 
    emmanuel j  candes and benjamin recht  exact matrix completion via convex optimization 
found  comput  math             december
     
 

the restaurant names  which are tremendously descriptive
for the above   examples  were of course not used in the spca
algorithm 

 

fi