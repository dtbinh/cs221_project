automatically identifying valid web form inputs
patrick mutchler
pcm d stanford edu

abstractin this paper we provide a classifier that can
accurately predict if a set of inputs is valid for a given web form 
our classifier is based on the assumption that text found near an
input field gives information about what kind of input that field
expects  we use a naive bayes classifier to determine if form
inputs are valid  we collected forms from    of the most visited
websites in america and hundreds of inputs to those forms  we
found that our classifier had a high success rate for determining
the validity of an input field pair as well as determining if a field is
a confirmation field  using our estimates from the naive bayes
classifier  we classified whole form inputs with high accuracy 

i  i ntroduction
achieving complete or near complete site coverage is an
essential feature of a web crawler  this task is made all the
more difficult by the complexity of modern web application
design  crawlers can no longer blindly follow hyperlinks and
hope to achieve any success  instead  they must be able to
interact with a variety of technologies  ajax  flash  etc   
monitor application state  and traverse behind web forms    
     the latter challenge is know as deep crawling  in order
to deep crawl effectively  a crawler must be able to submit
form data that the application will accept  unfortunately 
applications often expect specific input formats for individual
form fields  valid addresses  emails  phone numbers  etc   
top of the line crawlers make an attempt to solve this
problem by including a dictionary of common words that
identify specific kinds of inputs  when the crawler must fill
out an input field  it checks the id or name fields of the
input tag against its dictionary  if there is a match  then
the crawler inserts the appropriate input  this appraoch is
somewhat successful  but suffers from being very brittle  if
the name or id fields are not in precisely the expected format 
then the system fails  for example  the ids passwd and
user password  dont trigger the system to input a proper
password 
to help address this problem we propose a classifier that
can distinguish valid and invalid web form data  by testing
inputs with our classifier before submitting them to the web
application  a crawler can traverse an application with fewer
requests to the application  this makes the crawling process
faster and places a smaller burden on the application server 
our classifier takes advantage of two observations about
web site design  the first is that web applications are designed
to be immediately usable by anybody with basic web browsing
experience  and therefore must follow general patterns that are
common to most web applications  the pattern that we utilize
is form fields must have nearby text that tell the user what sort
of data is expected  and that the format of this text is relatively
consistent across web applications  the second observation is

that web applications are designed so that the code is as simple
and readable as possible  this means that the names used to
reference page elements reflect their functionality  concretely 
the reference names of form fields often contain information
about what sort of data the field expects 
using these observations  we developed a naive bayes
classifier that can accurrately predict if a set of inputs is valid
for a given web form  we collected data from    of the most
popular websites  based on alexa   we chose to only collect
forms to create accounts  since these forms tended to have a
large number of of input fields that expect particular kinds of
inputs  in all  we collected     complete form inputs  which
amounted to approximately      total input field pairs  we
found that our classifier had a high success rate for determining
the validity of an input field pair as well as determining if a
field is a confirmation field  using our estimates from the
naive bayes classifier  we were able to classify whole form
inputs with high accuracy 
ii  f ormulating the p roblem
before we can model an entire web form  we must model
individual form fields  we model each field fk as a set of
strings  fk    fk       fknk   corresponding to the id and name
attributes of the html tag as well as any relevant strings
found near the field on the rendered page  the id and name
attributes capture how the input tag is referenced by the
application while the nearby strings capture information that
a human user is given before being asked to fill out the form 
precise definitions of nearness and relevance are discussed
in section   
next we must model the input to the form field  it is not
enough to model the input  ik   directly as a string since it
is not likely that a particular input string will appear in our
training set  in addition to the input string  we also model the
input as a regular expression that matches the input string 
in particular  we choose the most exclusive regular expression
from a dictionary of regular expressions that matches the input
string  this is an appropriate choice because all of the common
input filters found on websites are regular expressions  we let
the term rk be the regular expression for input ik  
we can now model a form f as follows  f  
  i    r    f      i    r    f           in   rn   fn     an obvious approach would be to say that a form is valid if and only if
each of its input field pairs are valid  this  however  does not
capture the requirement that the inputs to different fields be
identical e g  password and confirm password fields  to
account for this  we include the term di for each i           n  
if di      then for f to be valid  ii must be identical to

fiii    note that this only allows for adjacent fields to expect
identical inputs  in practice  it is extremely unusual for two
non adjacenet fields to expect identical input strings 
finally  we have a complete model of a form f  
f     i    r    f    d      i    r    f    d           in   rn   fn   dn   
and we have an expression for valid f  
valid f     i          n  valid ri   fi        di  ii   ii 
iii  data c ollection
as far as we are aware  no public data sets exist for valid
and invalid form inputs  this meant that we had to collect our
own data specifically for this project  we built two tools for
automated data collection  the first tool extracts and parses
forms from raw html files while the second allows for
quickly creating valid and invalid inputs to a particular form 
in all  we collected     manual inputs to    forms from some
of the most visited web sites in america  based on alexa  
this is around      total input field pairs 
a  extracting forms from html
the two challenges in extracting forms from raw html
are determining what strings are relevant and determining
what strings are near to a given input field 
in order to decide what makes a particular string relevant 
we have a dictionary of strings that commonly appear in web
forms and provide identifying information about form inputs 
for example  the string zip appears in our dictionary because
it is common for address forms to include the word zip and
the word is a strong indication that a field expects a zip code 
our dictionary was collected manually and then trimmed by
removing strings and seeing if they affected our experimental
results  we also perform some normalization during this step 
for example  if a web form contains the word zipcode  we
normalize it to zip  this gives us greater uniformity between
different web forms and leads to better results 
computing nearness on a page directly is extremely difficult
since it requires us to correctly render the page and then
determine where each element is positioned on screen  instead 
we make the assumption that elements which are close to
eachother in the html parse tree will be close to eachother
on the rendered page  we say that text is near an input field
if their least upper bound has exactly one input tag and zero
form tags below it  see figure   for a visual explanation 
we built our tool using htmlcleaner  a java library that
provides methods for parsing raw html 
b  collecting valid and invalid inputs
in order to collect inputs  we built a simple web application
that allows us to upload a form and then record inputs to that
form  the application displays the form as it appears on the
original page except it includes checkboxes for the user to
assert that an input is valid or invalid  we assume that all of
the assertions about the validity of inputs are accurate 

fig     html parse tree demonstrating nearness  the string first name
is near the input with id firstname and the string last name is near the
input with id lastname but the string a simple form is not near to either
of the inputs 

c  dictionaries
our dictionaries of relevant strings  for modeling fields 
and relevant regular expressions  for modeling inputs  were
created manually based on what seemed appropriate  we then
adjusted our dictionaries  adding or removing elements   reran our experiments  and kept changes that led to better results 
this means that our dictionaries are fitted to the specific forms
we collected  this is perhaps an area of experimental bias  but
we feel that since forms are so uniform across web sites that
this approach was not inappropriate 
iv  i mplementation
a  computing validity for a single input field pair
first we show how to compute the probability that a single
input field pair is valid  for now we can ignore our form
based model and treat our training data as a set of input field
pairs  this means that in the following section  m refers to
the number of input field pairs in the entire training set 
let r correspond to a regular expression matching some
input and f correspond to a set of strings near some field 
this means that r is a signle integer matching an index
in our dictionary of regular expressions and f is a set of
integers  f    f        fn   where each fi matches an index in
our normalized string dictionary  finally let v be whether or
not an input field pair is valid 
we use bayes rule and the naive bayes assumption to
compute p  v r  f       
p  v r  f    p  v p  f  v p  r f  v 
where
p  f  v   

n
y

p  fk  v  and p  r f  v   

k  

n
y

p  r fk   v 

k  

by solving for the maximum likelihood  and using laplace
smoothing  we find that the ml estimates are 
m

p  v   

  x
  v  i    v 
m i  

pm pni
p  fk  v   

 i 

  fj   fk  v  i    v     
pmj  
 i    v     d
string  
i   ni  v

i  

fip  r fk   v   
pm pni
i  

j  

  r

 i 

i  

 r

 i 
j     fj

pm pni

 i 
fj

  fk  v

 i 

  v     

  fk  v  i    v     dregex  

   

since our string and regular expression dictionaries are
relatively small  fewer than     values each   it is possible
to precompute p  v   p  fk   v   and p  r fk   v  for each r  fk  
and v  this makes computing p  v r  f   extremely fast since
we just need to look up   values and do some additions  this
speed is essential since we need our classification time to be
much faster than actually sending an input to a website and
interpreting the results 

data except the inputs to the page that the input we are testing
belongs to  when we hold out an entire web page we see a
significant drop in accuracy  this is typically due to a page
having an unusual field  for example  our error rate when
testing on ebays signup form is high because it asks for a
phone number in an unusual manner  if we train on even a
few inputs to a web page  then our success rate is very close
to our success rate when training on the entire data set 

b  computing whether a field is a confirmation field
we compute whether a field is a confirmation field
using a similar method  again let f be the set of integers
 f    f        fn   where each fi matches an index in our normalized string dictionary and d be whether or not the field is
a confirmation field  note that in this section m refers to
the number of unique fields in the entire training set 
we use bayes rule and the naive bayes assumption to
compute p  d  f       
p  d f    p  d p  f  d 
where
p  f  d   

n
y

fig    

p  fk  d 

k  

again  we solve for the maximum likelihood to find the ml
estimates 
m
  x
  d i    d 
p  d   
m i  
 i 
 i 
  d 
j     fj   fk  d
pm
 i 
  d     dstring  
i   ni  d

pm pni
p  fk  d   

i  

  

percent successful classifications for input field pairs

we performed similar tests to see how succesful our system
is at determining if a field is a confirmation field  we either
train on the entire data set  the entire data set except one field 
or the entire data set except one web page and get similar
results to our previous experiement  we were surprised by the
significant decline in accuracy when holding out an entire web
page  since it would seem that each of the fields on a page
are somewhat independent  interestingly  all of our errors were
estimating that a field was a confirmation field when it wasnt 
this suggests that there is something wrong with the model
we are using 

just like in the previous section  we precompute p  d  and
p  fk  d  for all d and fk in order to speed up classification
time 
c  computing validity for an entire form
rather than trying to compute the probabilities that a form
input is valid or invalid  we instead take our estimates for
valid ri   fi   and di and compute valid f   using the expression given in section    this gave us very good results  which
are discussed in section   
v  r esults
to test our system  we collected three kinds of results  how
successful it is at determining if an input field pair is valid
 figure     how successful it is at determining if a field is
a confirmation field  figure     and how successful it is at
determining if an entire form input is valid  figure    
to test how successful our system is at determining if an
input field pair is valid  we computed the percentage of correct
classifications when training on the entire dataset  training on
all the data except the particular input  and training on all the

fig    

percent successful classifications of confirmation fields

finally  we tested how successful our system is at determining the validity of an entire form input  we used our simple
method of computing the validity of a form given our estimates
for the validity of each input field pair and our estimates for

fiwhether each field is a confirmation field  we got excellent
results  being able to correctly classify form inputs     of
the time even if we have not trained on the form at all 
these results are a little misleading  however  because the vast
majority of our errors come from misclassifying a valid form
input as invalid  this is because it only takes a single invalid
input field pair to make the entire form invalid  a single error
on a valid form input leads to a misclassification  for an invalid
form input to be misclassified  the system must make an error
for each invalid field input pair and no errors for each valid
field input pair  in particular  this means that it is possible to
make many errors and still correctly classify every form input 

fig    

percent successful classifications of entire forms

vi  c onclusion
in this paper we presented a classifier that can determine
whether or not an input to a web form is valid  we collected
forms from    of the most visited sites in america and    
inputs to these forms  using a naive bayes classifier  we were
able to identify valid input field pairs and confirmation fields
with high accuracy  using our estimates from the naive bayes
classifier  we classified whole form inputs with high accuracy 
r eferences
    doupe  a  and cova  m  and vigna  g  why johnny cant pentest  an
analysis of black box web vulnerability scanners  in  detection of intrusions and malware  and vulnerability assessment  pp                
    bau  j  and bursztein  e  and gupta  d  and michell  j  state of the
art  automated black box web application vulnerability testing  in  ieee
symposium on security and privacy  pp                
    ng  a  lecture notes     generative learning algorithms  available at
http   cs    stanford edu notes cs    notes  pdf

fi