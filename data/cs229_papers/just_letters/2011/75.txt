predicting the immediate future with recurrent
neural networks  pre training and applications
brandon richardson
december         
introduction
research preformed from the last   years has shown that the process of training
deep networks can be improved by carrying out the learning process in smaller phases
called pre training  rather than using back propagation to train the whole network  we
could just train several small modules separately and then when assembled fine tune the
entire network  this idea has only been applied to rnn in recent stanford research  the
current research done at stanford has divided the training into three phases  first  the
input to hidden unit layer connections is trained using an auto encoding objective 
second  the input to the hidden unit layer connections are held fixed and the temporal
connections are trained with short term memory  and third  the whole network is finetuned using the main objective function  the previous research shows that this new pretraining gives a better reconstruction error then previous methods  in some cases even
out performing the best hand engineered feature selection  this project is being done in
conjunction with quoc les research here at stanford  the main objective of this project
is to apply the pre training rnn algorithm to audio prediction 

the algorithm architecture 
the rnn algorithm is best described by referring to the figure below  this figure
shows a   layer network  with                   nodes per layer where the input and
output are composed of      values  the vertical connections are autoencoders and the
horizontal connections are the temporal connections  the nodes shown in green are only
used during pre training of the autoencoders 

fiinitial testing 
to verify the functionality of the rnn  and reinforce my understanding of the
algorithms architecture i began by training an rnn using the       video training
examples quoc had previously used to obtain results for his current research  this
previous research was attempting to predict the last   frames in a video from the previous
   frames 
the images shown below were generated using a   layer rnn with each interior
layer containing     nodes  these parameters were chosen to closely match the research
that had been previous completed using this frame prediction algorithm  and training
data  this model took about a week and half to process it was stopped before it fully
converged  the images shown below are the predictions made by this rnn on a test set i
designed  the test set is a simple ball of light moving from the bottom left corner of the
image to the top right hand corner of the image  top left  shows the rnns prediction of
the   th frame given the previous    frames  top right  shows the actual   th frame of the
video  bottom left  shows the actual   th frame  the last frame given as an input to the
rnn  bottom middle  shows the auto encoder output of the   th frame  from these
images its easy to see that the rnns prediction of the   th frame closely resembles   th
frame  the last frame given as an input to the rnn  therefore  the model appears to
poorly capture the temporal effect  also interesting to note is that the auto encoder output
for the   th frame scarcely resembles the actual   th frame  this leads me to believe that
the final objective function optimization is drastically changing the pre trained autoencoder parameters  such that the decode parameters properly output a prediction only
when the temporal connections are used 

fiapplication of rnn to audio 
after achieving reasonable results from the video data i applied the algorithm to
audio data  my goal was to predict the last second of a    second clip of music  the
training and test data is made up of      audio loops obtained from the apple program
soundtrack  the data clips from soundtrack where chosen since each loop in soundtrack
is on average    seconds in length and repeats a particular beat  conveniently the clips
are also made to be added together to create a full recording  this enabled me to created
       unique training examples by adding the tracks together in random combinations  i
held      of the original      examples aside to be used as the test set  unfortunately    
seconds of data at       samples second is too much data to process efficiently with the
algorithm  therefore  i compressed all the data to      samples second  this compression
reduces the complexity of the data while keeping the overall beat  the difference is hardly
noticeable when played back on normal speakers  each audio loop is sampled at   bit
resolution which is then normalize for use with the algorithm 
the primary difference between the audio data and the video data is that each
temporal step of the video data contains   frames of data  where each frame of the data
contains   x   samples from the same instance in time  each temporal step of the audio
data contains   second  or      samples of audio  where each sample has been obtained
at a different instance in time 
initially i attempted to fit the rnn model using only the      training tracks
supplied by soundtrack  the plot below shows the average reconstruction error for two
different networks fit to the training data  oddly the   layer      nodes per layer  rnn
achieved a lower reconstruction error then the   layer rnn 

when applied to the      example test set the average reconstruction error was
nearly identical to the training set  implying a good fit  unfortunately  when listening to
the estimated data it is clear the model has converged to a locally optimal solution rather
then the global solution causing the model to poorly reproduce the final second of audio 
this occurs for both the examples in the test set and the examples in the training set  the
plots below show the estimated data and the actual data for one training set example 

fiprobably the best one out of the many examples i viewed by hand 

rerunning the   layer      nodes per layer  rnn model with the        training
examples created by combining random combinations of the original      training tracks
supplied by soundtrack resulted in a considerably better average reconstruction error as
seen in the plot below 

some of the audio samples the model fails to predict entirely  however  after
listening to a few of the samples that the algorithm best fits and then listening to a few of
the ones that the algorithm fails to predict  it appears that the more times the beat repeats
in the first   seconds the better it can predict the last second  the samples where the
algorithm completely fails seem to correspond to samples where the beat never fully
repeats in the    second interval  the two graphs below shows the actual last second of
data  and the predicted last second of data for a test set sample where the beat repeated
a total of   times in the    second interval  the estimated last second is rather noisy  but
appears to accurately reproduce the subtly in the beat  filtering out the high frequency
noise  and playing the clip back with the estimated last second produces a near match to
the actual data 

fishown below is this models fit to the training data example that the previous model failed
to fit 

conclusion 
this project demonstrated how an rnn using pre training could be effective in
predicting audio  these results show that an rnn can successfully predict the last second
of audio data from the previous   seconds  however  it was also discovered that a large
number of training examples are necessary to prevent the algorithm from converging to
local minima  further research needs to be completed testing different numbers of layers
and different node sizes to find an optimal rnn for this type of audio prediction  the
loops from soundtrack only contained audio from around     different instruments 
therefore  it would be interesting to try predicting vocal audio clips  or even to test the
algorithms sensitivity to the different types of instruments 

references 
quoc  le  predicting the immediate future with recurrent neural networks  pretraining
and applications  nips       

fi