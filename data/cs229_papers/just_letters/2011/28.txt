nyc condo price estimation using nyc open data
hari arul
andres morales

introduction
this project explores the structure of the new york city housing market by predicting the price of
condominiums in new york city using the publicly available nyc open data dataset  estimating property
prices is a well known problem in statistics and machine learning  many different parametric and nonparametric models have been shown to achieve good levels of accuracy on certain datasets  we attempt
to use this novel dataset to tailor brand new features and gain a high accuracy in prediction  furthermore 
by harnessing the wide variety of features given by the dataset  we hope to gain insight into what groups of
features have the highest impact on generalization error 
this paper explores the procedure we took in learning about a new dataset by using clustering 
normalization  and pca  we then discuss the impact of using different learning algorithms on the accuracy
of prediction  to do so we used multinomial nave bayes  svm  svr  and locally weighted linear
regression on our new features  we saw that our features were not as predictive as the uci boston
housing dataset features given our results when we ran these same algorithms on that dataset  this
demonstrated that we could improve upon our features  and so we moved on to using feature selection and
adding location based features to better our dataset and our prediction accuracy  given these new
techniques our results improved considerably and did a good job of predicting housing prices 

nyc open data
nyc open data is a      project by new york city that has made available troves of data generated by
public agencies about the city  the data is updated as new information comes in  and is provided in a
machine readable format for people  like us  to examine  this data includes information as varied as
federal stimulus data in nyc regions  housing data split by boroughs  and wifi hotspot locations in the city 
nyc bigapps is a competition for developers sponsored by new york city that intends to find the best
applications that utilize nyc open data      we are applying to this competition with a housing price
prediction app that utilizes the machine learning described in this paper for the backend price calculations 
for the year            there were data points given for      condominiums in the manhattan area 

previous research
previous research in this problem area has focused on examining housing data  and trying to
predict prices using machine learning techniques  almost all of the articles use the uci machine learning
repository housing dataset comprising housing data from boston  the features include crime data by
town  house specific information  and socioeconomic and education data  and the predicted value was the
value of homes in thousands of dollars 
typically  these papers try to combine qualities about different learning models to come up with an
optimal model to train the housing data  and then show how it does a good job of prediction  for example 
a yann lecuns      paper combines both a parametric and non parametric model to train the final model
with a form of em      specifically  the authors make the assumption that house prices are a function of
both the features of the house  the parameterized model  and the suitability of the neighborhood  nonparametrically modeled   this model ended up being better at predicting housing prices than pure
parametric or non parametric algorithms  showing that new models can better represent housing prices 
the same nyu professors wrote another paper that added geographical information to help reduce
housing price prediction error  an approach which we eventually take with our nyc data       lastly  svms
and svrs have been used to predict housing prices        using the boston data  it was shown that as
long as information was known about the surrounding boston cities and towns  the value of housing could
be fairly accurately determined  from this previous research  we know there are multiple ways we can
train our data  and are hoping to find the best prediction scenario 

fiinitial features and dimensionality
after gathering housing market information from the nyc big data database  we decided to begin
solely with features directly related to the price of the house  our initial feature vector is in    dimensions 
and includes the borough  block  neighborhood  building classification  total units  year built  estimated
gross income  gross income per square foot  estimated expense  expense per square foot  and
operating income of the property and or property owner  we used    fold cross validation to generate our
training and testing data 
initially  we did not normalize our feature data  and when we attempted to use pca to find the
principal components of the data  we found that the first component explained      of our data  and
basically was comprised of one feature  after normalizing the data  pca gave   principal components
which explained       of the data  see figure     the first principal component placed high coefficients on
the features corresponding to building classification  year built  gross income  and expense per square
foot  this makes sense because each of these features provide different types of data  and there is a lot of
overlap between these features and the ones which were not weighted as heavily  such as gross income
per square foot and expense per square foot  this does not explain which features will help predict
housing prices  but it did help give us a sense of how representative our features were 

figure      of data explained by principal components

multinomial nave bayes
in previous research  multinomial nave bayes was used in a feature selection paper on the uci
boston housing dataset  where the problem became a binary classification problem with the y values were
separated according to the mean value of the target      on our dataset  we ended up classifying our
prices per square foot into the proper bucket         of the time  we used k means to find our two
buckets  and they roughly split the training into low priced houses        per square foot  and high priced
houses         
however  we found binary classification of housing prices an inaccurate way of actually predicting
what the price of a house should be  given it only really selects whether our house is cheap or expensive 
so  as our baseline algorithmic test  we decided to run nave bayes across n buckets  where each bucket
represents a price differential of      e g  if the min price of a bucket is       the max price will be       
we did the bucketing this way to ensure that whenever we correctly identified a bucket  our estimated price
would be at most     away from the median price of that bucket  all the previous research that we looked
at attempted to specify the percentage of houses that were correctly predicted within     of the actual
price  and when we implemented classification  this was the best way of adhering to these error
predictions  since we used nave bayes  we also had to discretize our input feature space 
with n buckets  for a given feature j  we bucketed our feature as follows 

fiwhere sj is the size of a bucket and bj is the bucket for a training example j  given a different number of
input buckets  and our output buckets as specified above  table   illustrates the classification performance
of this baseline test 
number of input buckets
accuracy
 
      
  
      
  
      
  
      
   
      
table    nave bayes results
when we run the same algorithm on the uci boston housing data that was used in research papers with
   input feature buckets  our accuracy was         as our future algorithms show  we can greatly
improve performance from nave bayes 

support vectors
svm
given we were already working on classification with nave bayes  our next step was using
support vector machines to try and better classify our test data into proper buckets  svm is a learning
algorithm developed by vapnik in order to obtain an optimal margin classifier in a high dimensional feature
space      svms have been used in the past for predicting housing prices  so we thought it would be useful
to try on our dataset      we looked at both linear kernels as well as gaussian radial basis kernels  whose
value depends on the distances between input vectors 
bucket size 

accuracy
radial basis
kernel
   
       
       
   
       
       
   
       
       
table    svm accuracy  no location data nor feature selection 
linear kernel

as we can see  increasing our bucket size naturally helps improve our classification ability 
however  compared to the results in lecun et al  our svm is not giving nearly as good performance  as
the rest of this paper demonstrates  we can still improve on this svm classification prediction through a
combination of feature selection and location data 

svr
support vector regression is a regression technique developed in      by drucker and vapnik et
al which extends upon vapniks svm concept to do regression analysis      similar to svms for
classification  svr determines the support vectors from the lagrangian of the loss function described in the
paper  which is slightly different from the original svm function  we wanted to use svr because not only
has it been used for predicting boston housing prices  but using regression can give us a better indication

fiof what our overall generalization error is  because we do not have to deal with bucketing  our svr
results  without including any location data  nor doing any feature selection  looks as follows 
mean squared error
        
average error    per square foot 
     
average housing price    per square foot 
      
  error
     
table    svr results  no location data nor feature selection 
we saw that using svr  our results averaged about     per square foot off from the actual price of
the house  given the average housing price of         per square foot for the condominiums given  this
lead to a       average error for the condominiums tested  the percentage of homes tested where our
predicted price was within     of the actual price was         we will be able to improve on this data
when we do feature selection  but our results will still not be as good as running svr on a dataset such as
the uci housing set 

location data and feature selection
one of the raw features provided by nyc open data that we did not initially include was the
address of the condominiums  however  after reading a set of papers highlighting the importance of
location in predicting housing prices  either as a separate model  or as an added feature  we decided to find
a way to add it in          using gps data for each of the addresses provided by yahoo   we used
longitude and latitude as additional features  we condensed both these features into one by calculating
their distance to the origin 
not only did we incorporate location data to obtain better results  but we used feature selection in
order to build up our best set of features using forward search without including extraneous ones that only
worsened our predictive ability  as a result  we ended up getting the minimal percent error by ignoring the
features block  neighborhood  and total units  see figure     it makes sense that we dont want to add
block and neighborhood because we are already adding in longitude and latitude information for our
location data  and this would only serve to potentially increase overfitting  total units ends up being
extraneous as well because we decided to predict over price per square foot  which makes the number of
units for the condominium not as worthwhile as if it was total price 
with our optimal seven features  including longitude latitude   running svr gives us   see table    

mean squared error
        
average error    per square foot 
     
average housing price    per
      
square foot 
  error
     
table    svr results with feature selection

figure    number of features vs  mse
these results are better than what we previously had  showing the importance of having the right features
when training data        of our predicted prices were within     of the actual price per square foot 
when we ran svr on our boston dataset with feature selection  we saw our results improve  on
the basis that the uci dataset has better and more predictive features for housing prices   specifically 
our average percent error is        and the predicted mse is         in thousands   which is in line with
the original svr paper that tested on the boston data        fo our predicted prices were within     of
the actual price of the data  and     of our data was within       which is in line but slightly worse than
yann lecuns paper which used a specific los angeles housing data      since our algorithms did better

fion those datasets  it made us realize that even though our predictions were ok  having better and more
predictive features for nyc open data would yield much better results 

locally weighted linear regression
now that we had location data as part of our feature set  we thought it would be interesting to run
locally weighted linear regression  where local is used in a literal and figurative sense  we weight each
of our test inputs by how close its gps location  i e  the location feature   giving closer training points a
higher weight  so we just run locally weighted regression using one feature for our weighting  and then
using our full set of features for our predicted price estimate  a scatterplot of our predicted and actual
results against the gps distance from the origin is plotted here 

figure    location vs  predicted   actual housing prices
our average error using lwlr was         so it is fairly reasonable to assume that location
 even as granular as our data  plays a big part in determining housing prices  given these results are in
line with svm and svr on the dataset 

conclusion
the results from our data did reasonably well on our training set  however  we dont think our
results are good enough to report on or to fully accurately make predictions in an application  given that
our more complicated tests ran well on the more fine tuned boston dataset  we are fairly confident that an
added feature set than the one we have  along with feature selection  would lead to significantly better
prediction results  data we could include  along with our current house specific data and location data 
are crime data  school district data  and socioeconomic data  all of which was used in the other datasets
but was not readily available  we believe thus that our algorithms will scale well with better data and will
allow us to confidently make predictions in the market 

references
    http        nycbigapps com 
    chopra  sumit  et al  discovering the hidden structure of house prices with a non parametric latent manifold model  acm        
    sumit  chopra  yann lecun  and andrew caplin  machine learning and the spatial structure of house prices and housing returns 
working paper   ssrn        
    wang  chaoyang  yanfeng sun  and yanchun liang  an improved svm based on similarity metric  journal of universal
computer science              
    drucker  harris  and vladimir vapnik  support vector regression machines  advances in neural information processing systems        
    radivojac  predrag  zoran obradovic  and a  keith dunker  feature selection filters based on the permutation test  ecml        

fi