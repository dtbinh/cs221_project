building a better tour experience with machine learning
alan guo  chanh nguyen  and taesung park

   introduction
the motivation of this project is to solve a problem that we
currently face working on a project called   bards  which
seeks to revolutionize the tour industry by constructing a
database of currently existing tour operators with the
airbnb like twist of allowing the average person to make
money as a part time tour guide  we wanted to apply
machine learning techniques to make sense of the largely
unanalyzed collection of tour operator websites and to
ultimately create a rich tour browsing experience for users 
one of the first challenges we faced was to build a classifier
for detecting tour operator websites across the web  this was
accomplished with relatively high accuracy using a bag of
words model and an svm classifier  we will not go into
much detail of this task and instead discuss the more difficult
problem of classifying the tour websites into multiple classes
such as cruise or hiking  the problem was not that the
classes were too similar  but rather due to the nature of tour
operator websites  which tend to mention affiliated tours on
their websites  making it difficult to simply find a specific
type of tour by simply looking for a keyword such as hiking
since many tour websites will mention this term  tours com 
one of the most comprehensive tour databases we found 
lists a collection of tours across the world broken down by
type  however  clicking on the first page of the culinary
category yields a mixture of resultsnone of which are
culinary tours  we wanted to build a system that can perform
better than whats available on tours com 
since the supervised data on tours com were unreliable  we
turned to unsupervised clustering  we performed bisecting
k means clustering  and it worked reasonably well  for
example  given three sets of handpicked websites on golf 
culinary  and cruise  bisecting clustering gave       accuracy 
which was better than the categorization by tours com 
lastly  we tried classifying websites based on its geographical
location  there have been attempts to associate documents
to geolocations  we devised a supervised classifier based on
wing et al        the nice thing about finding geolocation of
a tour website is that we can recommend other tours in
nearby regions  or show typical tour styles in a given region 
to summarize  we    apply nave bayesian and svm
approach to simply build a binary classifier for tours and

activities websites     refine our training set and build both
supervised unsupervised multiclass classification algorithm
for different type of tours  so we can tag categorize them  
and    predict geolocation of a tour website using supervised
learning 

   binary classification
our first task was to build a system that allows for the
collection of tour operator websites while rejecting the class
of all other websites on the internet  we scraped     known
tour and activity sites and about       non tour websites
from alexa and other sources 
we train both a multinomial naive bayesian  mnb 
classifier as well as svm binary classifier  with regards to
feature selection  we processed the data with stopwords and
other techniques discussed in the following section on multiclass classification  we achieved fairly accurate results with
the simple bag of words model  table    

training
size

features

test
size

nb
error

svm
error

description

   

    

   

     

    

test on training set

   

    

   

     

   

test on     samples

    

     

   

     

     

increased training
set size

    

     

   

     

     

added more test
data

    

    

   

     

     

reduced number of
features

table    results of binary classification show near equal
performance between mnb and svm 

fi   multi class classification
     supervised classification
data and model
in order to build our training data to categorize tour websites
by type  we scraped all       websites from tours com 
however  when we trained on this data and evaluated using a
hold out cross validation set of      the error was
somewhere near      which we quickly found out was due
to the mis categorization of many sites on tours com  other
tour directories were too region specific and so we had no
ground truth to work with  in order to reach our goal of
building and evaluating a system with good performance  we
chose to manually build a set of ground truth samples for
supervised learning  we collected     ground truth samples
in   categories by crawling through various tour directories
on the web  when looking for tour websites of a certain
category  our main criteria were the following 
   tour website must only offer a specific type of tour
for a particular region 
   tour website must not be generic tour search
service or directory  and must offer tours directly 
   if a tour website met the above   criteria but did not
fall into any of the   specific categories  it would be
placed in the  th category labeled other 
processing  sometimes the main page of a tour website does
not say much  whereas all the subpages may contain the
majority of information  we followed the links to extract text
from up to    pages on the same domain  we discard all
mark up and store the raw text only 
in order to improve the performance of all our learning
systems  we stemmed the words using the snowball library
which runs the porter stemming algorithm  this reduces
related words such as cruise  cruises  cruised  into
cruis so that all three forms of the same word get
recognized as the same feature 
we also noticed that many numbers and strange characters
were slipping through as common features  and found a
performance increase by restricting our word features to only
alphabetical characters 
feature selection

stopwords  we found a significant performance increase by
removing common words such as i or with from the
dictionary  the presence of these words  or lack thereof 

usually have little to no correlation with the categories we
are trying to distinguish and can only confuse the
classifiers more 
thresholding  in order to prevent rare terms from
influencing the classifiers  we used a lower threshold and
varied this number to determine the number of features 
we also enforced an upper threshold on terms that
appeared too often  which were bland terms such as tour
and travel  thresholding also reduces the dimensionality
of our feature vectors  reducing the average number of
unseen features and thus the effects of smoothing 
a drawback of thresholding is that some features
representing a certain category are more likely to fall under
a threshold if that category is underrepresented in the
training data  to counteract this  we gave documents with
lower representation a boost directly proportional to
categorys training size 
bigrams  realizing that some of the most distinguishing
features are phrases  we extracted bigrams and tuned their
weighting such that they would be able to compete with
single word features  which have the advantage of
appearing much more often  while increasing the number
of classes to   brought the accuracy of our system down
to about      using bigrams helped bring it back up to
about     
multinomial nave bayes
in order to get a binary classifier to classify amongst several
classes  we took an all vs one approach  for each class  we
marked all samples in that class as positive  and samples in all
other classes as negative  we then trained a multinomial
nave bayes classifier whose job was to predict the
probability of a sample belonging to that class  then  for any
given sample  we run it against each classifier and take the
classifier with the highest confidence 
multi class decision trees
in order to discover possible improvements  we wanted to
compare the performance of our all vs one classifier to that
of a multi class binary decision tree  as shown in figure   
we learned that while both classifiers struggled with the
addition of classes  the decision tree performs better when
there are fewer classes  but the multinomial nave bayes is
preferred when there are more than five 

fitable   our system detects tour websites against other
websites well in binary classification  in telling tour websites
against each other  our manually labeled ground truth was
critical for decent performance 
description
binary classification

training size features test size error
    
    
   
    

  class  tours com 
    
  class  manually labeled     

    
    

   
   

   
   

too common to have discriminating power  and also rare
words such as uvaggio  a type of wine  that does not
appear in most tour websites  next was to sort the terms
according to its variance on frequencies on documents 
the idea is that the terms that appear equally frequently in
all websites are not good classifiers  let
be the
frequency of term in document   the variance of term
is defined as
 

figure    a  classifier response to number of classes  b 
performance of final system with bigrams and feature
threshold tuning 

we sorted the terms by variance in descending order  and
took the first one thousand  lastly  we manually throw
away the terms that were meaningless or related to
geographical locations  for example  the words we threw
away include sans serif  a font type   buy  and     caused by
text encoding error   we also filtered out geographical
proper nouns because we did not want the clustering to be
performed by regions  but tour types  in the end  we had
    feature terms 

evaluation

feature vector representation

we tested all of our classifiers using     hold out cross
validation and found that the system was able to perform
reasonably well when trained on reliable ground truth data
with good feature processing and selection  understandably 
accuracy begins to degrade sharply with the use of more than
  classes due to higher chances of overlap 

each website is represented as a bag of words model on
the feature dictionary we created  then each entry is
weighted by tf idf to weight frequent words with less
discriminating power  finally  in order to account for
documents of different lengths  each document vector is
normalized so that it is of unit length  the distance
measure in this space is calculated using cosine distance 
not euclidean 

     unsupervised clustering

since the labeled training set from tous com were not
reliable  we also tried unsupervised clustering for type
classification  we initially started with basic k means
clustering on simple bag of words feature vectors with
euclidean distance measure  however  this approach did
not yield high accuracy  because the euclidean distances
become indiscernible as dimensions increase  beyer et al 
       realizing that  our final implantation uses bisecting
k means clustering on refined feature vectors with cosine
distance measure 
feature selection
first  we start with all words that appear in the websites
listed on tours com  next  the words are stemmed  then
we chose      words in the mid frequency range  for
example  we filtered away words like the  or tour that are

clustering algorithm
we followed the suggestion by steinbach et al      on
comparison of document clustering techniques  which
recommends bisecting k means clustering  the algorithm
is very simple once we have the regular k means clustering
algorithm 
   pick a cluster with largest size 
   find   sub clusters using the basic k means algorithm 
 bisecting step 
   repeat steps   and   until the desired number of
clusters is reached 
the number of clusters was chosen empirically    to  
clusters produced the most consistent websites collections 

firesult
using a hand picked set of websites on three categories 
golf  culinary  and cruise  we ran bisecting k means algorithm
into three clusters 

predicted
golf culinary cruise
golf
  
 
 
actual
culinary
 
  
 
cruise
 
 
  
accuracy                  
table    confusion matrix of bisecting k means

furthermore  we collected a few terms that characterize each
cluster  they were gathered by choosing the terms with
highest values of  frequency of the term in the centroid    maximum
frequency of the term in all other centroids   these terms can be
used as an alternative to categories in supervised learning 
suppose you query the database of tour websites on
tours com  with the keyword mediterranean      websites
are returned in our training set  clustering those into  
categories yields characterization terms in table    as you
can see  the terms reveal the nature of the clusters  thus  the
characterization terms can be used as an alternative to the
fixed categories in supervised learning  moreover  they are
more flexible than the fixed set of types in supervised
learning 
table   characterization terms of clustered websites searched by
 mediterranean   the terms were stemmed 

cluster  
cluster  
cluster  
cluster  
cluster  

yacht  charter  sail  gullet  cabin  motor  boat
bike  wine  cycl  cook  hike  empir  ride
cruis  wildlife  dive  costa  masai  mara  pacif
overnight  dinner  templ  church  christian 
hotel  jewish
thank  testimony  axum  empir  camel  desert 
women

   geolocation identification
another kind of categorization widely used in tour industry
is classifying by location rather than types  most of tour
portals such as orbitz com have that  unfortunately 
geographical categorization has been done manually by tour
operators  recently  there have been some attempts using
machine learning that tried to classify documents by

corresponding regions  we took the approach by wing et al 
     

supervised data and model
for the training set  we used the destination tagging on
tours com  tours com lists what regions the tour is offered
for each tour website  we fetched the lists and used them as
ground truth geolocations for the websites  the labeling on
tours com is not accurate  but manually identifying the
geolocations of tour websites enough for training in all
geolocations in the world was next to impossible  this list
gives a mapping from website urls to its geographical
names  then we fetched the list of countries and cities with
their latitude and longitude on various internet sources  now
we can construct a mapping from website urls to
latitudes longitudes  note that this is a one to many
relationship because a tour website usually offers tours in
more than one geolocation 

feature selection and feature vector representation
we took the same set of feature terms we used in
bisecting k means clustering  the only difference is that
we did not filter out the terms referring to geolocations 
because they are good indicators of identifying tour
regions 
method
first  we create a grid of latitude longitude  we used a
square grid of size one degree  for each grid  we can find
the websites associated with that grid in the training set 
we concatenate all website contents for that grid so that
we can create a frequency based feature vector for the grid 
the probability of a word
in document is simply
the count ratio 

similarly for a grid cell
distribution 

  we can compute the word

the distributions were smoothed by laplace smoothing 
now that we have word distribution of documents and
cells  identifying the geolocation of a given website is just
searching for the cell with most similar word distribution

fito that of the website  therefore  we need to establish a
similarity measure between two probability distributions 
we used kullback leibler divergence for that 

the cell
with the smallest kl divergence is the
predicted geolocation of the website 
result and applications

figure   the word distribution of yacht  dive  and cruise  the
distribution of yacht is in red  you can observe that these regions
are gathered along popular beach destinations 

we tested on    handpicked websites that refer to no
more than two countries  if the predicted geolocation is
either one of the two countries  that prediction is marked
as correct     of the prediction were correct  which gives
the accuracy of      errors were largely due to the fact
that our training set was not perfect  as many labeling on
tours com were wrong  also  some errors were caused by
the websites that refer to regions our latitude longitude
collection did not cover  such as cuba 
figure   hot keywords in different regions 

   references

figure   distribution of likelihood of a website
tunisiadiscoveries com  lighter red squares mean higher likelihood 
note that because of the term desert  other desert countries have
high likelihood as well 

the word distribution has other applications  for example 
you can visualize what the hot keywords for a tour in a
specific region are by showing the terms with high
probabilities  as in figure    if you were wondering where
to go for ultimate cruise yacht tour  you can superpose
the word distributions of cruise  yacht  and dive  as in figure
   moreover  by storing the geographical location of
websites by latitude longitude rather than place names  we
can recommend other popular tours in nearby regions 

    qi   davidson  web page classication  features and
algorithms 
http   www cse lehigh edu  xiq    pubs classificationsurvey lu cse        pdf
    shen et al  web page classification through summarization 
microsoft research in asia 
http   research microsoft com pubs          pdf
    b  wing  j  baldridge  simple supervised document
geolocation with geodesic grids  proceedings of the   th
annual meeting of the association for computational
linguistics  pages         portland  oregon  june       
     
    i  dhillon   j  kogan   c  nicholas    feature selection and
document clustering 
http   citeseerx ist psu edu viewdoc summary doi         
      
    m  steinbach  g  karypis  v  kumar  a comparison of
document clustering technique  university of minnesota 
    k beyer  j goldstein  r ramakrishnan  u shaft  when is
 nearest neighbor  meaningful       

fi