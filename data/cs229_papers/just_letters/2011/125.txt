detection of whistler waves
evan jeng
december         

 

introduction

a whistler wave is a very low frequency electromagnetic wave generated by lightning discharges  as the
electromagnetic wave travels through the plasma environments of the ionosphere and magnetosphere  it
undergoes dispersion of a several thousand kilohertz  because the propagation velocity decreases with
frequency  the lower frequency components of the wave arrive later than the higher frequency
components  thus  a radio station picking up a whistler wave would detect a signal that has decreasing
frequency in time  since the frequency range of the whistler waves lies within the human audible range  it
would be perceived as a descending tone if converted into audio  hence its name  a whistler wave
bounces back and forth across opposite sides of the planet  and the particular hop number and direction is
indicated by a number and a sign  a    whistler indicates a wave on its first upward path  whereas a  whistler indicates a wave on its downward path after its first reflection  in this project  we are interested in
the automatic detection of the presence of    whistler waves from radio data collected by the demeter
 detection of electro magnetic emissions transmitted from earthquake regions  satellite 

 

satellite data

the raw data gathered by demeter consists of time series of scalar valued electric field measurements
taken from various time spans during the satellites orbit  the sampling frequency is   khz  which sets
the nyquist frequency just above the maximum whistler frequency  a one second sample of the timedomain electric field signal is shown in figure  a  the time domain signal gives us amplitude as a
function of time  which is not too helpful  instead  we take a spectrogram of the data  which reveals the
frequency content of the signal as a function of time  an fft length of     with an overlap of    points

fibetween frames was used to generate the spectrogram shown in figure  b  as we can see  the whistler
waves are now easily identified as the white l shaped curves in the image 
while identifying whistler waves in the spectrogram is a rather straightforward task for a human  how we
would teach a machine to automatically detect whistlers is not immediately obvious  if we take an image
recognition approach  major challenges to overcome seem to be the lack of a smooth gradient field that
occur in normal images that would facilitate edge detection  poor signal to noise ratio  blurred boundaries
for closely spaced whistlers  disappearance and reappearance of curve segments  as well as interference
by extraneous signals 

 

previous attempts

prior to this work  an attempt at automatic whistler detection by stanfords vlf lab was done
under the strong assumption that the background noises  i e  non signal regions  are independently and
identically distributed  given that the shape and duration of the whistlers dont vary much from whistler
to whistler  they created a  d filter shown in figure   with
for the white grids and  for the black
grids  such that the sum of the values in the filter equals zero  the idea is that the expected filter output is
zero when applied to a frame consisting of only noise  for a frame containing a whistler wave  the filter
output would have a large positive value since it would lie in the white region of the frame  classification
would then involve either a constant thresholding or adaptive thresholding 
in practice  this approach does not work very well for a
number of reasons  first  the assumption that the background noise
has zero mean is largely invalid  if we look at the spectrogram in
figure  b  we see plenty of stray signals and other whistlers in the
black non signal region of the filter  unless a whistler exists in
isolation  the presence of extraneous signals will result in destructive
interference  leading to false negatives  on the other hand  strong
localized non whistlerlike pulse passing through the white grids in the
filter can lead to large outputs values and false positives  making this
approach a poor selector for shape 
convolution with spectrogram 

 

our approach

in this work  we tackle the problem using support vector machines  our goal was to improve the accuracy
of classification by focusing on shape that characterizes the whistlers  which is not well captured by the
work described in the previous section 

   

training data

we were able to hand label around     whistlers in the available data  we observed that the whistlers
have approximately the same duration of     seconds  a spectrogram frame containing a whistler would
then be around    x    points  spectrogram frequency bins x time steps   which corresponds to     
points per frame  given that we only have around     whistlers in the data  we will run into serious issues

fiwith high variance if we use every point in the frame as a feature  thus  some preprocessing of the data is
necessary before we start training our support vector machine 

   

preprocessing filtering

our first step in reducing the number of features is discarding the data in the non signal region of the
frame  this is done simply by applying a binary mask with   for points in the signal region and   for
those in the non signal regions to each frame  we will discuss how we determine signal and non signal
regions in the next section  but for now suppose the allowed signal region at each time step spans   
frequency bins  this would already take us to    x          features  down from       while this is a
significant improvement  we can do even better if we only kept the peak frequency bin within the signal
region at each time step  which ideally yields the trajectory of the whistler curve  this way  we have only
one value per time step  yielding a basic feature set of around    
figure    a spectrogram frame
containing a whistler  a  raw
spectrogram image  b  keeping only
the peak intensity frequency bin in
each column  c  applying binary
mask  d  keeping the maximum
frequency in each column after
applying binary mask 

as can be seen in figure    this preprocessing and feature reduction approach has the potential to work
very well  by using the index of the frequency bin as a feature  we have the added benefit of invariance to
intensity scaling  since the signal strength of whistlers vary  we want the support vector machine to
discriminate more on the structural characteristics of the trajectory  and not the absolute intensities of
individual points  while this idea is good in principle  our filter does not always yield perfectly nice
curves like the example in figure    sometimes extraneous signals within the defined signal region
dominate resulting in sharp discontinuities in the curve  however  we press forward with this approach 
hoping that given enough good data points within each training sample  the support vector machine will
be able to make the correct decision 

fi   

defining the signal region

let
and
be the number of frequency bins and time steps  respectively  in a frame  let
be a vector containing the indices of peak frequency bins within the chosen signal region for a given
frame   we want to find
such that
set a good upper and lower frequency bound on
the signal region at each time step  let
be the intensity of frequency bin at time offset   our
algorithm is as follows 
initialize

arbitrarily with rough approximation

repeat until convergence 

  where m is the number of whistler containing samples  and the constant in the last line of the
algorithm was a parameter to be optimized over in an outer loop given a particular feature set 

   

feature selection  training  and testing

we experimented with many combinations of features  including taking the first order and second order
differences to capture the slope and curvature information  while each whistler may last around    time
steps  it may not be optimal to include all    as features  for example  the tail end of the curve tends to
have lower signal strength and is often dominated by noise or extraneous signals  we used matlab and
liblinear to iterate over many combinations of features  using cross validation to empirically determine a
good feature set for our support vector machine 

 

results

we settled on using    time steps and the absolute values of the first and second order differences of the
peak indices as our feature set  table   compares the performance of different features sets 
feature set
stanford vlf

 
 

  of features
 
  
  
  
  
  
  

false positives
false negatives
overall error
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     
     

table    performance comparison of different feature sets  d   d   and d  refer to  th   st  and  nd
order differences of peak frequency bin indices 

fias we can see  the best feature set used the absolute value of the first order and second order differences
for a total of    features  achieving an overall error rate of      

 

conclusion

our support vector machine approach performs much better than the approach previously taken by the
stanfords vlf lab  in fact  on this data set  the previous approach does not do much better than random
guessing  yielding a       error using the best thresholding value  although we have achieved a pretty
good error rate of        we believe more training data is necessary  since using     training samples for
   features is not a very good ratio  however  these initial trials results are promising  with more training
data  future work could involve experimenting with more complicated features  such as local gradients
around the signal region 

fi