cs   a project report  flame war detection using
nave bayes classication techniques
louis boval  boval stanford edu   jimmy tobin  jtobin  stanford edu 

useful corpus of data from the notoriously ame ridden
youtube com comment sections  our implementation of a
classifying text using multinomial nave bayes is now a common technique  notably for spam e mail ltering  our goal nave bayes classier and the paths we followed to adapt
with this project was to attempt to adapt the same technique it to the problem at hand 
abstract

to classifying a harder class of documents  ame wars  or very
heated discussions in public internet forums  to do so we implemented a custom web scraper to build a corpus of data 
accompanied with our implementation of a nave bayes classier which we then augmented with several tweaks to attempt
to gain a better recall measure on large training sets  including a technique described in     to leverage the large amount
of unlabeled data we could gather 

 

 

corpus creation

the rst step of the development process for the project
once we ascertained a global outline of the techniques we
wanted to deploy was to nd a corpus of data on which to
train the eventual engine  most such sets of documents
used for text classication engines did not have heated
speech as a class  we therefore had to build the corpus
ourselves  which given the abundance of examples on the
web was clearly feasible as long as websites structures
lent themselves to our research and properly reected
in code the structure of discussions  we chose to focus
our eorts on youtube  which provides a stripped down
version of its video pages to view comments in bulk  eg 

introduction

internet forums  albeit very engaging and useful  are sadly
a magnet for all kinds of abuse  provocations  hate speech
or spam  various techniques are commonly used to detect
a subset of these superuous postings  including preventing automated scripts from posting using captchas 
or applying spam detection to lter out unwarranted promotional messages  one type of posts which still require
human intervention  however  are incendiary comments
and the resulting string of heated messages  which we
dub as  flame   they detract greatly from the atmosphere on public forums  and given the ease with which
they can be introduced without administrators noticing
means many willingly initiate them for the fun of it 
we wanted to see if spam ltering techniques could be
applied on discussion threads which display those issues 
and generate a model to detect future ames  given that
the contents of messages which initiate those can be fairly
well worded in spite of their meaning  we chose to focus
our detection attempts on the thread of messages which
follow a ame initiator which would display more telltale
signs  rather the just the initiator itself  in the following sections  we will describe our process to generate a

http   www youtube com all comments v myq upzjdjc  

we wrote a python program to accomplish this
task  using the freely available beautiful soup
 http   www crummy com software beautifulsoup  
module to parse html code into meaningful units 
and writing a simple command line interface to provide
access to the functionalities of the scraper  additionally 
the program was structured to allow us to quickly switch
the focus website to guard against possible issues during
the two month period of the project  we luckily didn t
have to  but a redesign of youtube com very late in the
project  although its changes to our pages of interest
were minor  proved the precaution to be justied   we
eventually were able to gather thread url from the
youtube frontpage  and use those to generate a corpus
of unlabeled documents  given this  we could launch an
interactive training session in the command line where
the user could classify a subset of his choice  and output
 

fi 

  nave bayes concepts and implementation

the corresponding training sets  after applying the
trained model to the rest of the data  the program could
also generate an html representation of each discussion
thread  for each video  with styling rules to dierentiate
between dierent classes of messages  to allow us to view
in the browser the results of the classication  with a
little eort the program could have been converted to a
cgi application to perform the required tasks entirely
controlled from a browser  excluding perhaps the initial
data gathering step which could run a little slow due to
understandable throttling on you tube s part 
after one hour of running the scraping portion of
the program  we obtained roughly         individual
messages from about    popular video threads  which
amounted to about        documents from our classifying perspective  once messages were organized in threads
of responses  out of this unlabeled corpus  we manually
labeled      documents on a ame not ame basis  and
built a set of training les of increasing length  chosen
at random in the total set of labeled documents to ensure fairness  to test the classifying engine s sensitivity
to training set size  the training les took the shape of
a series of documents  headed by their classication and
followed by a sequence of word word frequency pairs obtained by splitting the messages texts in words  ltering
those to group numbers and exclude one letter or twoletter words as well a non alphanumerical data  the les
were encoded in unicode to avoid issues with the original
website encoding  utf     which was tested by running
most of our early runs on a thread mixing japanese and
western characters 
once the frontend portion for manual classication and
corpus creation was built  we were able to focus on implementing a nave bayes classier  which we ll discuss in
the next section 

ument belonging to a certain class through a fairly fast
multiplication 
without delving into too much detail  which can be
reviewed on both bibliography references  let s establish
the basic classication model  a labeled document d is a
set of word frequencies fi for each word wi in the vocabulary  and a class label c  d       wi        c   combining
the document properties for all documents in the training
set  we emerge with a collection of word frequencies per
class label  which we can use to compute class probability for each unlabeled document during the classication
phase  more specically  for each unlabeled document we
compute the maximum likelihood of classes to choose the
best one given the model  as follows 
argmaxc p  c   c d   

p  c   c  

qn

p  wi  c   c fi
p  d 
i  

   

to simplify computations in the actual implementation  we ignore the p d  denominator which is a common
factor across the values we wish to maximize 
the value of p  wi  c   c  can be approximated using
the training data  as follows 
nic
p  wi  c   c    p v  
njc
j  

   

where nic is the count of occurrences of word wi in
training documents of class c   and  v  is the size of the
vocabulary  the occurrence count is the sum of all fi for
all documents in the training set 
p c c  is simply computed using the count of documents of class c in the training set relative to the total
count 
implementation

 

nave bayes concepts and implementation

multinomial nave bayes classier description

as presented in      a multinomial nave bayes classier
attempts to use the frequency of words in a document
relative to all the classes which we intend to represent  to
determine the class of an unlabeled document  an essential assumption  and reason for the name of the technique 
is to consider words to be independent of each other  thus
eectively allowing us to build our probabilistic model in
parallel across all words in the training documents  and
use baye s theorem to compute the probability of a doc 

the algorithm was entirely implemented in python  as a
script runnable both as standalone taking training les as
input  or directly callable from our scraper instance  this
aorded us great exibility when running tests over variations on our algorithm  and shielded us from potential
issues with the frontend program which had to contend
with network input and large amounts of data to save
between runs  using an intermediary data format helped
us evolve the main data structures without risking invalidating existing data 
the training portion of the script was fed labeled documents as an input and produced a model object containing the frequency vocabulary and various other measures

fi 

  improvements upon original algorithm

 class probability  to facilitate its use in a classication
run  equation     was computed in log format to oset potential over underow issues  and converted back
to exponential form if the precise values were needed for
comparison 
to evaluate the soundness of the algorithm  a subset
    to      of the training portion was reserved during
development to be used as a test set and not integrated in
the trained model  we will describe in more detail the use
of this set in the fth section of this report  one important note at this point is to remark that the ame class
is greatly outweighed by non ames in the documents we
trained with  and consequently we had to evaluate our
results in the context of skewed classes  as such test accuracy was not as meaningful as precision and recall  and
we will use both f  scores and accuracy to evaluate the
performance of our models 

 

other alterations to the algorithm

improvements upon original algorithm

in its original form  on our initial data sets  the algorithm produced accuracy levels of roughly      and an
f  score of      which required signicant improvement 
although increasing the corpus size eventually brought
these values up  we worked on improving the results by
implementing as well improvements to the model  at various steps in the creation process 
our main track of investigation in that view was the
semi supervised techniques described in         expectation maximization and frequency estimate from unlabeled
data  given the relative complexities of the techniques we
decided to focus on the latter one  seen in      reserving
expectation maximization to an eventual comparison if
the extension to frequency estimate yielded a signicant
improvement 
the main idea behind extending frequency estimate
was to use the frequency of instances of a word found in
the unlabeled data  and couple it with the class specic
conditional probabilities obtained from the training set 
this led to changing the form of equation     into 
p  c wi  l
p  wi  u
p  wi  l
p  c wj  l
p  wj  u
j   p  wj  l

p  wi  c   c    p v  

without discussing the theoretical soundness of the
technique  we note that our implementation yielded underwhelming results  as the precomputing step on the unlabeled data set we produced  given its sheer size  was
particularly lengthy  and the improvement to the f  score
was in the single digits  we could not compute the variance of this improvement given the running time of the
algorithm   for lack of time we decided not to attempt
to x the algorithm  as we obtained good results without
it  but a probable improvement would be to generate the
vocabulary at the scraping step  and update it to separate labeled from unlabeled at every step of the manual
classication and later training process  the refactoring
work implied seemed too great in comparison with the
apparent yield of the technique  but should be explored
if the system is intended to be used in a real context 

   

where subscript u indicates values obtained from the
unlabeled data set  and l values obtained from the labeled
data set  equation     is then fed into equation     to
produce the classication prediction 

to augment the algorithm  we decided to try other lighter
alterations based on intuitions about our target class of
documents 
firstly  we imported an implementation of the porter
word stemming algorithm from the python natural language toolkit set of modules  http   www nltk org    to
lter our vocabulary at the training stage  earlier would
have led us to a loss of data from the total scraped set
which we deemed unreasonable   and condense the size of
the vocabulary by removing synonyms  lightly misspelled
words and naturally merging words sharing the same root
under the assumption that meaning or use would be similar in the context of ame wars  the next section will
describe the results we obtained from this addition  as it
proved to be an interesting improvement on small data
sets 
in addition to stemming we also added call to a linux
utility  hunspell  http   hunspell sourceforge net    to
perform spell checking on the intake  in the hopes that
it would help us oset the general disregard for proper
spelling  syntax notwithstanding  displayed in youtube
comments  it also yielded a small improvement to the
model  although when combined with stemming did not
prove eective  most likely because  if used before stemming  it risked separating words which would otherwise
have been condensed by the stemming step  and when
used after would be mostly lost cpu time given that
stemming results are orthographically correct   and increased the running time by a large factor  we used a
direct call to the shell utility  which is particularly slow

fi  experiments

when executed in fast succession   despite the addition of
a cache to minimize calls to the spellchecker 
finally  we remarked that ame threads would be characterized by a longer than usual length  and so decided
to attempt using it as a dierentiating factor beyond the
bag of words  using it directly in the vocabulary was
deemed too tricky  as we would have to reconstruct a discrete probability function on all natural numbers based
on the length measurements from the training set  which
in itself added a supplementary machine learning problem  having to perform a linear regression on a model to
t to the collection of lengths  for lack of time we decided instead to predict the form of this model and test a
few values to see if it yielded a signicant improvement 
we asserted that the length probability could be modeled by a continuous distribution function shaped after
the arc tan function  which presents the particularity of
quickly rising from  p  to p  around    by shifting the
origin set of the function to cover document lengths and
changing its arrival set to        we obtain a cdf that
quickly moves to high probabilities at a certain threshold
arctan ll   
l    the format is roughly f  l         
     
p
 
f ld   was used to alter the output probabilities used
 
in equation      as follows p  c   f lame d    p  c  
f lame d    f  ld    p  c   n otf lame d  and

 
the following graph presents the values for accuracy
and f  when the trained model is reapplied to the training set  we notice that accuracy and f  both peak at
    and     respectively early on  while their variance decreases  the values were obtained by averaging over   
runs of the algorithm  with a test set being extracted thus
inducing a random factor and indicating the uniformity
of the training set   this indicates that the model is stabilizing 

the following two graphs present f  and accuracy values on the test set with and without stemming  two
remarks of note here  on the one hand stemming loses
in eectiveness when the training set size increases  being overtaken by the vanilla implementation  and on the
 
p  c   n otf lame d        f  ld     p  c   other hand both score continue rising after the previous
t values for the training set stabilize  indicating that
n otf lame d 
we tested a set of values for l  to measure its impact overtting is progressively reduced 
on the classier  we gained better results on small data
sets  but this ceased to be true on large ones  and our
intuition here is that we need to conduct a regression on
both l  and an additional a factor applied to  l   l    to
stretch the transition area 

 

experiments

as explained further up  we evaluated the eectiveness
of our algorithm by reserving a fraction of the training
sets for testing  we did not perform cross validation   we
noted that both accuracy and f  score improved significantly as the training set size increased  whereas improvement techniques became less eective  possibly due
to poor parametrization  the following graphs present
a few of the results obtained  focusing on the t of the
trained model to the training data  which only indicated
either good performance or massive overtting   and the
performance of the model on the test set with and without the inclusion of stemming  to provide an indication of
whether overtting is occurring in reality 

these results indicate that the classier is fairly eec 

fi  experiments

tive on the training set we obtained by manually labeling
documents retrieved from youtube comment threads  and
lead us to believe the algorithm can be used in some capacity as a ame war detector with the proper source data
set  also  as training examples become more numerous 
improvement techniques lose their eectiveness  due probably to a mist  taking into account document length  in
particular  might be a worthwhile improvement to extract
supplementary features from existing data  that would
only require implementing a regression technique on the
training set  without having to alter the original content
gathering 
conclusion

through the use of a nave bayes classier  and after training roughly      documents obtained by scraping comment threads from youtube com  we managed to
reach a fairly good classication performance for ame
war detection  while issues remain with overall performance  scalability and safety against overtting such a
diverse set of text  we believe a nave bayes based technique can be investigated further to automate the detection of these discussions which claim a signicant amount
of time for website administrators  crowd sourced ame
detection already exists by implementing simple voting
measures  karma on reddit   and these could be used to
augment the model presented here to improve accuracy
or possibly better adapt to context that display more
complexity than the admittedly limited community on
youtube 

references
    su  sayyad shirabad   matwin  large scale text
classication using semi supervised multinomial
naive bayes      
    nigam  mccallum  thrun   mitchell  learning to
classify text from labeled and unlabeled documents      

 

fi