automated patent classification
cs    cs   a  machine learning
ian christopher  sydney lin  sigurd spieckermann
december         

abstract
the goal of this project is to automate the process of classifying patents under the hierarchical
international patent classification system  ipc   in this age of innovation  the war over intellectual
property  ip  becomes common between companies and even individuals  although a patent
protects the ownership of ip  its application process is costly and slow  moreover  most patent
lawyers nowadays manually classify patent applications based on their knowledge  experience and
individual research  therefore  automation on patent classification not only helps to reduce human
error that might lead to expensive cost  but also accelerate the application process 

 

introduction

knowledge and research on the world intellectual
property  organization website with key words 
our system is aiming to categorize a query this increases our motivation on applying mapatent under a five leveled hierarchical classifica  chine learning to solve the multi class classification structure of different sections  classes  sub  tion problem 
classes  groups and main groups or subgroups  in
the training step  our classification algorithm analyzes the abstract  title  author  plus company    background
if applicable  and citations of a given patent and
text classification is a well known area of pattern
learns patterns between these features and its
recognition and information retrieval  though
assigned class es   in a fair amount of cases 
there has been a sufficient research in the area 
patents can fall under different categories and it
we tried to keep our effort largely original  that
may be difficult for a human expert to accurately
being said we did use a number of publications
classify a patent 
to guide us when results started to stall or there
due to our inexperience with patent patent
were too many possible next steps to choose
law  we met with a patent lawyer in the area
from 
in order to get a better understanding of comthere were a number of basic text classificamon practice and computer aided tools in this
tion papers that we read to better understand
business  after the conversation with him  we
the space   sl    was a well cited survey of
were surprised that for new patents applica 
tions  most patent lawyers mostly rely on their
http   www wipo int
 

figranted patents

sorts into hierarchical text classification  their
metrics for classification performance was especially interesting   rs    used neural networks
to address the problem   wl    used support
vector machines and produced competitive results   qhlz    helped introduce the use of latent features and their importance to the problem   xxyy    had very good results using a
two step classification scheme for each patent
that involved pruning the hierarchy before actually classifying a document  we even found
a paper that was also trying to classify patents
using text  ciw  but it ignored the hierarchical
nature of the problem 
in addition to browsing text classification papers  we considered other types of hierarchical
classification problems  in particular  ima  provided background of a classification competition on the imagenet data set and algorithms
that seemed more successful on image data 
google researchs  szyw    provided information about classifying videos 

utility
design
plant
reissue
sir

  patents

   
   
   
   
   

    

    

    

    

    

year

figure    granted patents  frequency of types

mately    mbytes uncompressed in size  consequently  a single year of patents takes up about
    gbytes of space  relatively speaking this is
not a massive amount of data but on commodity hardware it is a nontrivial amount as we are
intending to use many years of data  it is also
worth noting that the number of patents each
year roughly increases as time moves on so earlier data sets are smaller  another issue with
googles data turns out to be the inconsistency
  patent data
in its structural representation across different
our data source is bulk downloads of patent years  more specifically  the data over the last
data from google patents  which originates from ten years is provided according to two different
the united states patent and trademark of  xml schemata and one prefix based format 
fice  uspto   google hosts a number of different data sets within the patent legal space in      content
cluding patent applications  patent grants  and
there exist different types of patentsutility
maintenance events related to the sets  we use
patents  design patents  reissue patents  statuweekly bibliographic data on granted patents 
tory invention registrations  and plant  like
which google provides for all patents since      
corn beans trees  patentswith different properties regarding their information schemata 
    structure
utility patents are the most common types of
one of the challenges of dealing with this data patents and the ones we are focusing on  but
is the sheer size of the data sets  a typical the data set contains patents of all types  they
week in      comprises several thousand granted include the information that we anticipate to
patents  each of these weekly sets takes typi  be meaningful features as opposed to other
cally under    mbytes compressed and approxi  typesin particular design patents that make
 

fiin python that contains a complete set of
punctuation characters and replace all occurrences of those in our text data  stopwords are identified and removed by means
of the natural language toolkit development  nltk  library in python using the
english corpus 

the largest fraction of the non utility patents 
but merely consist of very few key words and
no classes  we wonder about their usefulness 
but do not dive deeper into this question 
further  there exist various classification system in different parts of the world besides the international patent classification system  ipc  
in general  there is no one to one mapping between the different systems  but united states
patents are classified by a national classification
system as well as by the ipc  this has lasting effects on the rest of our work as we decide to use
the ipc as the basis of our classification effort
due to its deeper hierarchy 

   

 stemming in order to achieve invariance
with respect to inflected forms  we reduce
words to their stem using the porter stemmer provided in the nltk library 
 mutual information and frequency
count only a subset of words in text corpora are often indicative of the content of a
text  in order to retain the most relevant
words and thereby limit the dimensionality of our dictionary  we combine the mutual information  mi  metric and the frequency counts words by intersecting both
sets  which are each sorted in descending order of their values  although mi is
only defined to relate words with one particular labelin our case patent classes
  we obtain an overall scalar metric that
attempts to generalize the relevance of a
word to all classes by summing the quantities for a word with respect to a particular class over all classes  however  it is
prone to give high weight to rare words that
are greatly nonuniformly distributed across
classes while they are in fact not particularly
indicative in general 

preprocessing

the first step in handling the data is to parse
it into a more appropriate format  the data
set provided by google contains a significant
amount of redundant information represented in
the notoriously verbose xml format  as a result  the data set is largely space inefficient for
our purposes and hence wastes storage capacity
as well as computation time in subsequent processing steps  we decided to convert the data
into a comma separated file format that only
contained information relevant to our problem 
this step resulted in a compression ratio of the
order ten to one making storage of patent data
across many years much more feasible 
in the second step  we preprocess the patent
text data in multiple ways to improve the performance of our learning algorithms 

 latent semantic analysis our abovedescribed steps only address cosmetic
changes and statistical filtering  by applying latent semantic analysis  lsa  to our
bag of words matrix  we attempt to identify similar semantic meanings of words and
project onto a lower dimensional subspace
of abstracted semantics  this step will

 stop word and punctuation removal
stop words do not contribute to the richness
of information of a text and punctuation is
difficult to handle properly by machines  by
removing both  the next is normalized and
better suited for our purposes  we remove
punctuation by means of the string library
 

fiprove vastly beneficial in later sections of optimization algorithm  the gpu implementation of the neural network uses a gradient dethis report 
scent optimization method 
in the third step  we generate additional features by computing the joint probabilities be    algorithms
tween an assignee as well as the assignees country
of origin and each class we are considering in our all algorithms except for the neural network are
classification task  finally  we construct the the binary classifiers  in order to achieve classificabinary valued matrix of classes  that a patent is tion with multiple simultaneous activations  we
categorized by and export all data to a mat  follow the one vs all methodology and learn a
lab data file using the scipy python library 
model for each class separately  neural networks
are naturally capable of performing a multi class
multiple activations classification task  in con  classification
sequence of only few patents being assigned to a
certain class our data quite skewed which espeour first goal is to accurately classify patents
cially diminishes the performance of the binary
into the first level of the classification hierarclassifiers  in order to account for the data skew 
chy  in the second step  we consider two levels of
we up sample the patents with positive class lathe hierarchy and flatten out the tree structure 
bel giving a noteworthy improvement  in order
hence  we attempt to classify to approximately
to comprehensively assess the performance of our
    different subclasses  third  we perform a hialgorithms  we utilize four common metrics  acerarchical classification task by training a model
curacy  precision  recall and f score  the acfor each level and passing patents with active
curacy alone may be misleading in some cases 
predictions on to the next level  in these steps 
e g  when the data we train on is skewed  the
we compare a number of different learning algoother three metrics give a better insight into the
rithms 
actual performance and are in fact still relevant
after up sampling the data because this step only
 logistic regression
introduces balance artificially 
during early stages of testing our various al linear l  regularized l  loss soft margin
gorithms on a    weeks data set of patents from
support vector machine
      we observe that the bag of words feature
 multi layer feedforward neural network    matrix with raw frequency counts is suboptimal because documents with a larger text corpus
gpu implementation 
cause larger values in their corresponding row 
we implemented all algorithms  except for the in order to account for this issue  we normalize
svm  ourselves in matlab and the gpu neu  the document word frequencies and see a signifral network in c   cuda by means of the icant improvement in the classification quality 
nvidia linear algebra library cublas and the
open source library thrust which is the cuda      logistic regression
equivalent of the stl in c    the cost function of the logistic regression algorithm and the in a first step  we start with a one vs all regularneural network are optimized using the l bfgs ized logistic regression classifier as a baseline
 

fian f score of approximately     on the test
set and slightly above that for the training
set after tuning the network parameters 

because it is simple to implement and because it
provides a good basis for the later evaluation of
our more advanced learning algorithms  logistic
regression performs well for its implementation
complexity and performs best for a bag of words
feature matrix reduced to     dimensions using
lsa  the result of this setting yields an f score
of approximately     on the first level of our test
set after training the classifier with l bfgs  in
the following steps  we use the logistic regression implementation to verify the performance of
the other learning algorithms 
because the regularization term does not improve the f score  we suspect logistic regression
might under fit the data set  consequently  we
implement a weighted logistic regression to improve the f score  at early stage of developing
algorithm  we defined accuracy with the assumption that there is only one active label  however 
the accuracy was below what we had hoped for 
we stopped further work on this direction because the low accuracy was very likely to be a
result of overfitting the data set 
     

 latent semantic analysis a further
approach to improve on the classification
quality drives us towards applying lsa to
our bag of words in order to capture semantic similarity between words  however 
our scores for both  training and test set 
drop by about     which is indicative of
a bias problem  different configurations of
the single hidden layer neural network do
not seem to improve results 
 latent semantic analysis     hidden
layers we extend our implementation to
handle multiple hidden layers in order to realize a more complex model  the number
and sizes of the hidden layers are specified
by a vector whose entries denote the number of hidden units for each layer  a    dimensional feature space using     hidden
units for the first     units for the second
hidden layer and a regularization parameter
of     give best results across all our tests 

backpropagation neural network

the convergence plot shown in figure   gives a
better understanding of the relationship of the
number of iterations and the performance of the
algorithm  overall  we notice that the cost functions of our networks are difficult to optimize and
even though we use the l bfgs algorithm  we
often terminate early in local minima and have
to repeat the optimization process with a new
set of random initial values or slightly modified
network parameters 

neural networks are capable of classify patents
into multiple active classes simultaneously which
makes them an attractive algorithm to use  in a
first implementation in matlab  we implement
a fully vectorized backpropagation neural network with one hidden layer and choose the number of hidden units close to the number of output units  coincidentally this architecture gives
us some of our best results compared to others
architectures with a single hidden layer  beyond
adjusting the number of hidden units in a single hidden layer  we take the following steps to
arrive at our final version of this algorithm 

     

backpropagation neural network
 gpu 

heavy training of the neural network with two
 normalized bag of words the nor  hidden layers requires several thousand iteramalized bag of words feature matrix yields tions of the l bfgs algorithm  unfortunately
 

fithrust library  we also implement a few specialized kernels in order to combine smaller operations in one kernel launch 
 
our gpu implementation of the backpropa 
gation neural network approximately yields a
factor    speed up over the vectorized matlab
 
implementation that we modify to use gradient descent as well for fair comparison  how 
ever  we notice that the gradient descent opti 
mization algorithm is vastly inferior to the lbfgs algorithm and is in fact unable to opti 
   
     
     
mize the cost function enough to train the neu  iterations
ral network  we make this observation for both
implementationsmatlab and c   cuda 
figure    neural network convergence plot    nevertheless  we appreciate the speed up and relevel 
fer to future work to implement an advanced optimization algorithm on the gpu 
value of cost function

convergence plot

this takes hours to run on the cluster we are using  luckily our group has some experience with
general purpose gpu programming  so we decide to port the neural network to cuda  for
simplicity and feasibility in the context of this
project  we implement the gradient descent optimization algorithm to minimize the cost function
instead of l bfgs 

     

linear l  regularized l  loss softmargin support vector machine

support vector machines are a typical learning
algorithm for text classification in many of the
papers we looked at so we decide to apply them
to our problem  conveniently linear support vector machines are available through the popular
library liblinear so testing them is a relatively
quick and easy task 
though plugging the svm library into our
code is simple and compare to the neural network there are fewer parameters to tweak  it
turns out that we get best results for a normalized bag of words matrix reduced through lsa
to a     dimensional feature space plus the assignee and assignees country of origin features as
described above  the performance of the svm
is very good and compares to our optimal results
using the neural network  in terms of execution
time  it is noticeably faster although we train one
model per class in consequence of the one vs all
method we use  we also tried to use nonlinear svms through the library but unfortunately

one of our tools for this implementation is
the thrust library  which provides a high level
interface for gpu programming  high level
here is relative term as the code is still low level 
regardless it helps speed up development with
matrix multiplication methods and a number of
other high level utilities 
in our implementation  matrices are represented using thrust vectors and managed by a
custom matrix class which handles the dimensions and wraps required linear algebra operations  matrix multiplications are generally executed using cublas whereas element wise matrix multiplications and reduction operations
e g  required to compute the cost function
utilize optimized cuda implementations of the
 

fithey do not seem to perform well and training is ers  nonetheless we try to develop methods that
slow 
would scale to more levels if we choose to do so 
we approach the hierarchy problem in two distinct ways  because we are trying to classify each
patent to at least one leaf node  we flatten the
hierarchy to a single level that just consists of
leaf nodes  the other approach is to classify
recursively on each level of the hierarchy until
we reach the bottom of the tree  ultimately we
are left with comparable results between the two
which actually seem to agree with a few results
we found in literature before implementing 

learning curve
training set
test set

error      f score 

    

   

    

   
 

 

 

 

the flat approach is the easiest to implement
because it only means changing the labels matrix in our preprocessing  after this we can just
run algorithms for dealing with just the first level
on the data set  as such  this approach is often
called for training for each of the one hundred
plus labels on the second level  consequently
this approach becomes much slower than the first
level code so we are unable to optimize parameters as well 

 
   

  training examples

 a  logistic regression
learning curve

error      f score 

training set
test set
   

the standard recursive approach is slightly
more difficult to implement but does not take too
long  we actually have a number of different versions of this approach depending on classification
parameters and how often we want to run latent
semantic analysis  top level  every level  etc   
because many of the classification algorithms are
one vs all  we actually have more train runs here
than in the flat approach  but here the training
sets are smaller as we only train on patents that
can be classified by the current node 

   

 
 

 

 

 

  training examples

 
   

 b  linear svm

figure    learning curves

     

hierarchy discussion

 

results

after getting sufficient results on the first layer
of the hierarchy  we move on to tackle the hier    future work
archy  taking baby steps  we decide to concentrate on just the first two levels until we have there are a number of ways that we could move
good enough accuracy to move on to the oth  forward with our work in the future 
 

fi a  classification on   level

logistic regression
neural network
linear svm

accuracy
                   
                   
                   

precision
                   
                   
                   

recall
                   
                   
                   

f score
                   
                   
                   

recall
                   
                   
      

f score
                   
                   
      

 b  classification on   levels

neural network
linear svm  flat 
linear svm

accuracy
                   
                   

precision
                   
                   
      

table    results for training test set
 classify further down the hierarchy
much of our time has been spent on classifying on just the first level  we held off
on classifying on the second level until we
had sufficiently accurate results because we
were worried that otherwise we would get
bad results 

lationships  titles  etc   
 hierarchy pruning a number of the
more successful papers we read  used a two
step classification approach  during the
first step a lightweight similarity metric was
used to prune the tree  leaving only plausible categories remaining  after it would
classify in this pruned hierarchy  of course
this would mean more training  but the results might be worth it 

 larger datasets the size of the raw
xml data is one the order of several gigabytes per year  though we can effectively
compress these files by picking out pertinent
metadata from xml  this is still a large set
without the help of a database if we want
to hold decades worth of data without a
database  a larger dataset would help us
with the uncommon classes and dive deeper
in to the classification hierarchy  which has
over sixty thousand leaf nodes  

 gpu optimization techniques at the
moment  we use gradient descent in our
gpu neural network propagation algorithm  we use this algorithm due to its
implementation simplicity but we might be
able to use a more powerful optimization algorithm to speed up our results there  in
particular  bfgs seems like a prime candidate to implement 

 additional features there were a number of fields in the raw xml data that we
ignored  though most of them do not seem
useful  one in particular could be very helpful  patent citations  unfortunately only
using only a year of data makes it hard
to resolve these citations  but if we had a
database we could construct numerous features from them  citation classes  graph re 

fireferences
 ciw 
 ima 

ioana costantea  radu ioan  and bot gert wanka  patent document classification
based on mutual information feature selection 
large scale visual recognition challenge      

http   www image net org 

challenges lsvrc      pascal ilsvrc pdf 
 qhlz   

x  qiu  x  huang  z  liu  and j  zhou  hierarchical text classification with latent concepts 
      http   www aclweb org anthology p p   p        pdf 

 rs   

miguel e  ruiz and padmini srinivasan  hierarchical text categorization using neural networks 
information retrieval                         a               

 sl   

aixin sun and ee peng lim  hierarchical text classification and evaluation  in data mining 
      icdm       proceedings ieee international conference on  pages               

 szyw   

y  song  m  zhao  j  yagnik  and x  wu  taxonomic classication for web based videos       

 wl   

x l wang and b l lu  improved hierarchical svms for large scale hierarchical text classification
challenge  large scale hierarchical text classification                   

 xxyy    g r  xue  d  xing  q  yang  and y  yu  deep classification in large scale text hierarchies 
in proceedings of the sigir conference on research and development in information retrieval 
acm press       

 

fi