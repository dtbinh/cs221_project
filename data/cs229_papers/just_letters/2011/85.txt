predicting indicators of social inequality with highway data
lynne burks mahalia miller daniel wiesenthal
stanford university  stanford  ca       usa

 lynne burks  mahalia  stanford edu

dw cs stanford edu

abstract
social inequality is an important contemporary issue  and
there is high demand for better information about its governing dynamics  indicators of social inequality may include unemployment  crime rates  number of welfare recipients  and
number of homeless people receiving assistance  in this paper  we apply supervised learning methods to predict such
socio economic metrics  we train on data from a large network of highway sensors throughout the san francisco bay
area       sensors gathering car speed and flow in each
highway lane every    seconds for    years  to make use of
this large amount of data  we develop a novel machine learning system capable of efficiently working with large datasets
using commodity hardware by leveraging a database backend
and parallel cloud computing services  we use the machine
learning package  weka  to train classification models  ultimately  we are able to predict with high accuracy several
socio economic indicators of social inequality using highway
traffic data  percent of californians on welfare  number of
californians on food stamps  unemployment rates  and crime
in oakland 

introduction
an increasingly relevant and popular metric for the health of
a culture or community is the degree to which the population
exhibits social inequality  while concept of social inequality is not well defined  more familiar and concrete measurements of the populationsuch as unemployment and crime
rates  or welfare and food stamp usageare metrics which 
in aggregate  can present a reasonably clear picture of the
disparities between different socio economic strata  jenkins
and micklewright        unfortunately  these metrics are
sometimes available only after a significant delay  at which
point the information is informative but not actionable  if it
were possible to access these metrics in real time  it might
be feasible to proactively address problems before they fully
formfor example  by deploying more patrol cars to locations that are likely to see an increase in crime rates 
other metrics of a population are much more readily
available  for example  the state of california has a network
of over        highway traffic sensors  which gather high
volumes of data automatically and have been doing so for
the past    years  these data  which may seem far removed


the first three authors contributed equally to this work 

from issues such as social inequality  may nonetheless capture subtle trends in a community  for example  researchers
such as hadayeghi et al         have noted correlation of
traffic and unemployment 
in this paper  we propose to use machine learning on automatically gathered and readily available data such as highway traffic to predict vital population metricssuch as crime
rate and unemploymentin real time  in order to both better
understand the health of a population and to provide actionable information about current concerns 

machine learning system design
we built a custom machine learning system from scratch
in python  leveraging mongodb as a database backend and
amazon web services  aws  for parallel cloud computing 
our python package offers submodules to create  manage 
and evaluate datasets  featuresets  sets of feature extracting
functions   and classifiers  see figure   for a flow chart  
we take an object oriented approach  and strive to provide a
simple high level interface 
ds   dataset  our dataset  
fs   featureset  our featureset  
p   projection ds  fs   our projection  
c   classifier  naivebayes  
c train p projected datums   
e   evaluator c  p projected datums   
e split dataset dev percent    
cv percent within dev    
e report cv performance  
e learning curve intervals     folds    
we found that the most constraining step in our workflow
was the creation of feature vectors from our data  hence 
the primary goal of our system was to make this process fast
and easy  we use a metaphor of projecting a dataset through
a featureset to create feature vectors  this can all be done in
one line of code  the feature projection process takes place
in parallel across either all local cores  or  optionally  aws
machines  this is made possible by our database backend
 and friendly abstraction layer   which offers simultaneous
and indexed access to our dataset  made efficient by carefully tuning our mongodb indexes  using this system  we
are able to easily and efficiently create feature vectors from
our raw data 

firaw data

  txt
  txt

all time periods for
which there exists a y

  txt

  txt
  txt
  txt

pems traffic
y  y 

   

dataset

yn

feature functions

sensor id
date

avg occupancy
time
delay
total flow
avg speed

hourly sensor
readings

target variables

y

x  x        xm

all sensor data recorded
within given time period

yn

datum list

database

classifier
weka

our implementation

naive bayes

naive bayes

logistic regression

figure    flow chart of our machine learning python package
our package also offers a creative approach to a nave
bayes classifier that is particularly well suited for environments in which new datums are frequently added to the training set  our approach has three interesting characteristics 
instant training  parallel classification  and real valued feature support 
the classifier stores all datums in its training set in a separate database collection and  when asked to classify an unknown datum  it queries the database holding these training
datums to gather the information needed   for example  to
calculate the likelihood that a particular feature has a particular value given some class c  one step is to query for all
datums in the training set that have class c   this means that
rather than have a load  train  classify cycle in which
the training phase loops over all datums gathering counts 
we have a load  classify cycle  in which the counting work
is done for us by the database  the result is that we have essentially instantaneous training since the real work is done
during classification  and yet the classification step is still
relatively efficient  because by leveraging database indexes 
we essentially have the database do the bookkeeping for us 
in addition to instant training  we can instantly re train our
classifier by simply associating it with a new set of training datums  which may be a super  or sub set of the original
training set   new classifications will immediately take advantage of this additional datum 
because we do lose some speed when classifying unseen
datums by not counting and caching in a training phase and
then simply applying the cached counts  we try to make up
for it in other ways  one way is through nested parallelization  when given a list of datums to classify  we parallelize
at no fewer than three levels  first  we classify each datum
in parallel  second  when classifying each datum  we evaluate the prior  likelihood for each class in parallel  third 
we evaluate the likelihood that a feature has a certain value
given the class for each feature in parallel  this high degree of parallelism means that individual datum classifica 

tions are relatively slow due to the overhead of spawning
so many parallel calls  but when classifying a large number of datums  we are able to use practically limitless cpu
power  via aws   so speed becomes directly dependent on
our database performance  we found this to be an interesting characteristic of our classifier  rather than being cpubound  our system is io bound 
vanilla nave bayes classifiers do not support real valued
features  which we found to be less than ideal  we therefore
built real valued feature support into our classifier  when
calculating the likelihood that a feature has a particular value
given some class  we model all observed values with these
characteristics as a gaussian distribution  and calculate the
likelihood that a value would fall into a small interval surrounding the particular value of interest 
because we wished to smooth our data  we extended this
gaussian approach in a way inspired by laplace smoothing
we increased the  used in our model by multiplying it by a
fixed constant  since adding some k would not respect the
differing variations of our feature values   this constant
is roughly equivalent to an add k approach in a traditional
nave bayes classifier  as it takes probability mass from the
most likely values and spreads it out across less likely values  such that no value gets a probability of  because  of
course  its a bad idea to estimate the probability of some
event to be   just because we havent seen it in our finite
training set 
we found it to not be immediately clear theoretically what
should happen if in the training set  we have never seen the
feature take on any value  given the class   empirically 
what we settled on was just offering a hand coded very small
likelihood  this corresponds theoretically to arguing that if
given some class we have never seen this feature take on any
value  perhaps due to sensor malfunction  as happened in
our data   then the likelihood that we will see any value is
very low 

fi      n

       n

       n

       n

       n

 
        w
 
 
 
 
 
 
 
 
  
 
  
 
 
 
 
 
  
 
 
 
 
 
 
 
 

        w

 
 
  

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

        w

       w

 

    
 
 
 
 
 
 
 
 
  
 
 
      
          
  
   
 
      
   
   
  
      n
       
 
           
    
  
   
  
 
  
 
 
 
 
 
  
   
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
    
  
 
 
     
 
  
  
  
 
 
 
 
 
 
      
 
 
     
  
 
            
 
 
 
  
  
  
  
 
  
 
  
 
 
  
 
   
 
    
  
 
 
 
 
 
 
     
 
 
 
 
   
    
 
 
 
       n
 
 
 
  
 
    
    
 
 
   
   
 
 
 
     
 
 
 
 
   
  
 
  
 
 
 
 
 
 
    
 
  
  
   
 
  
 
 
 
  
   
 
 
 
 
 
 
    
    
 
  
   
 
 
  
 
 
 
 
 
  
 
 
 
 
  
 
 
         
 
  
  
 
  
 
 
 
 
 
  
   
  
  
 
 
        
 
 
                           
 
          
 
      
 
 
   
  
  
 
 
       n
 
  
 
 
  
 
  
 
 
 
 
 
 
 
 
  
 
 
  
 
 
  
 
  
  
  
 
  
 
 
 
  
    
  
 
 
 
 
  
 
   
  
 
  
 
     
 
 
      
 
  
  
 
 
 
 
 
 
  
  
  
  
 
  
 
  
  
  
 
  
  
  
       n
  
   
     
 
       
 
    
 
 
     
 
  
  
     
 
 
 
  
 
 
  
 
 
 
 
 
  
 
 
 
 
  
      
 
   
  
 
        
 
  
      
 
 
  
 
  
 
   
 
 
 
    
      
 
 
  
 
   
     
 
 
 
       n
     
 
  
 
 
         
 
         



     

  
 

  

        w

  
        w

table    size of socio economic datasets
economic factor
dataset size
bart ridership 
  
food stamps 
   
gas price 
   
homeless 
   
oakland crime 
  
welfare 
   
san francisco crime 
  
unemployment 
   

 
 

  
miles
        w

 
  

 
 
   
 
          
 
 

       w

figure    map of all road sensors in the san francisco bay
area  with key locations of interest to this project shown as
red dots

data
dataset description
for this project  we used two types of data  highway traffic
data as predictor variables and socio economic data as target variables  the highway data comes from the caltrans
performance measurement system  pems   which is publically available at pems dot ca gov  choe  skabardonis  and
varaiya        pems consists of over        road sensors
located on major highways throughout the state of california that have recorded the speed and flow of passing cars in
all lanes every    seconds for the past decade  we used a
version of this data aggregated by the hour in order to make
the data more manageable without losing too much granularity  for this project  we focused on sensors located around
   key locations in the san francisco bay area  including
bridges  airports  and metropolitan areas  figure   shows a
map of all road sensors in the bay area  with chosen key
locations marked by red dots 
choosing measures of inequality is a difficult problem 
which has been a topic of open discussion in the field of
econometrics and economic theory over the last few decades
 atkinson        motivated by interest from economics to
answer questions regarding poverty and wealth distribution 
we selected relevant features related to unemployment and
public assistance from different public data sources  specifically  we examined eight socio economic factors      bart
ridership      number of californians on food stamps  governmental assistance to buy food       gas prices in california      number of homeless families that are receiving
assistance from the state of california      crime rates in
oakland      percent of the california population that is on
welfare      crime rates in san francisco  and     unemployment rates in california  each of these statistics are reported
monthly  and table   shows the number of data points in
each set with the data source as a footnote 

dataset preparation
defining data points for the purpose of classification in this
application is a non trivial task  our dataset has multiple
dimensions  namely space  the sensor recordings were geographically distributed across the san francisco bay area  
time  and recording dimensions  flow  occupancy  and velocity for each lane   in addition  the number of data points of
the target variables are limited to once per month for the last
ten years  so we expect limitations on how many features
will be optimal to train our model  thus  we investigated
different choices for defining predictor variables 
we define a data point as corresponding to selected measurements from a given month as well as the corresponding target variable  since we have nearly a decade of data 
we have     data points  as previously described  we focus on data from    key locations across the san francisco
bay area and we aggregate data from about    relevant sensors  which is a subsample of our entire dataset  as shown in
figure    for each month long time period  we separately
average the flow  speed  and delay recordings during morning and afternoon rush hour for each day of the week at each
chosen location  we could easily increase the number of features by including data from more sensors or using flow from
every day of the month rather than averaged across days of
the week  we could also reduce the number of features by
aggregating over an entire month  but we notice that the rush
hour time periods and the weekly variation offer more predictive power  perhaps because rush hour dynamics are intuitively more related to socio economic factors 
the schema for our data points is shown here 
 loc  avgflow am monday 
loc  avgspeed am monday 
loc  delay am monday 
   loc   delay pm sunday  y 
 

http   www bart gov about reports ridership aspx
http   www dss cahwnet gov research pg    htm
 
http   www eia gov oil gas petroleum data publications wrgp 
mogas history html
 
http   www cdss ca gov research pg    htm
 
http   www  oaklandnet com government o opd s 
crimereportarchives dowd      
 
http   www cdss ca gov research pg    htm
 
http   sf police org index aspx page    
 
http   www labormarketinfo edd ca gov cgi dataanalysis 
labforcereport asp menuchoice labforce
 

fi  
  
  classification error

of features  see table     classification of bart ridership
actually has an error of    probably because this dataset only
contains    data points and is being overfit 
see table   for a list of the most important features for
some socio economic factors  half of the features that are
important for predicting crime in oakland come directly
from sensors located in oakland or around the oakland
aiport  which makes intuitive sense  for unemployment
rates  most of the important features are related to delay over
bridges 

bart ridership
food stamps
gas price
oakland crime
unemployment
welfare
homeless
sf crime

  
  
  
  
  
 

   

   
   
   
training set size    of total data 

 

figure    learning curves of all socio economic factors 
computed using logistic regression in weka
our real valued target variables are each mapped to two
classes in order to use common machine learning classification algorithms  we initially tried two different mappings
to classes  one corresponding to whether the given point
in time had values higher or lower than the average over
our whole dataset  and another corresponding to whether the
value increased or decreased relative to the previous month 
unfortunately  the first of these mappings gave us unreasonably high predictive performance  and the second gave
us very low performance  see discussion in the results section  

results
preliminary results
as a first pass analysis of our data  we took the feature vectors computed using our machine learning system and analyzed them in weka with nave bayes and logistic regression classifiers  hall et al         the resulting classification
error for each socio economic factor calculated using both
learning algorithms and    fold cross validation is shown in
table    based on these results  it appears that highway traffic data is a relatively good predictor of how much the population of california is a  using welfare  b  on food stamps 
and c  receiving unemployment assistance 
we also used weka to compute a learning curve  which
shows how the classification error changes as the training
size increases  see figure     for most socio economic factors  the testing error does not change with increased training
set size  indicating that obtaining more data will not increase
accuracy  based on this observation  we decided to try improving our models using feature selection 

feature selection
we performed feature selection for each socio economic
factor in weka by computing the information gain for all
    features  which is a measure of the reduction in uncertainty of the target variable given an observation of the feature of interest  then for each socio economic factor  we
chose only the features with the largest information gains
and retrained our model  for most socio economic factors 
the error is significantly reduced by using this optimized set

table    classification error using logistic regression and
nave bayes with    fold cross validation

indicator
bart ridership
food stamps
gas price
homeless
oakland crime
welfare
sf crime
unemployment

all features
logistic nave
reg 
bayes
   
    
   
   
    
    
    
    
    
    
   
   
    
    
   
   

feature subset
logistic nave
reg 
bayes
   
   
   
   
    
    
    
    
    
    
    
   
    
    
   
   

interpretation
given the incredibly high accuracy in our classification
models  we re examined our choice of target variables  we
noted the common dependence on time of some of our predictor and target variables  for example  in the first implementation we separated percent of the population on welfare into two classes  one being below average over the last
decade and one being above  however  welfare was above
average for   years  then below for   years  then above until
present  showing very limited variability with time  see figure     similarly  traffic flow in some locations followed a
general increase with time  thus  the high accuracy in classification can be argued to be a trivial result for some socioeconomic factors  but we were able to predict crime rates in
oakland with only       error  even though this data shows
a large variability with time as shown in figure   
because of this common dependence on time  we investigated other choices of determining classes  such as using a
moving average with a bin width of   months or two years 
the resulting model had high classification error near     
indicating the model was not better than randomly predicting a class  future work could investigate the proper choice
of classes so as to make the moving average concept meaningful 
another consideration was to use traffic data from a given
month to predict the class of the social indicator for the upcoming month  for target variables that rarely changed from
month to month  like welfare   the predictive ability of our
models was about the same as classifying social indicators
using traffic data from the same month  however  for crime

fiinfo gain
     
     
     
     
     
     
     
     
     
     

table    features with highest information gain for crime rates in oakland and unemployment
oakland crime
unemployment
feature name
info gain feature name
oak airport avgspeed am fri
     
bridges avgdelay pm sat
paloalto avgflow am thu
     
bridges avgdelay am tue
oak avgflow pm wed
     
bridges avgdelay pm thu
sanjose avgspeed pm wed
     
sf avgdelay am thu
oak airport avgdelay am wed
     
oak avgspeed pm sat
santarosa avgspeed pm tue
     
bridges avgdelay pm sun
oak avgspeed pm sat
     
oak airport avgspeed pm sat
sanjose avgdelay am sun
     
oak airport avgspeed pm mon
oak airport avgspeed pm tue
     
fremont avgdelay pm mon
paloalto avgspeed am thu
     
sanjose avgflow am mon

  population on welfare

 

class
data
average

   

 

acknowledgments
   
                                                 

crimes reported in oakland

of californians on food stamps  and percent of the population on welfare  thus  we discuss appropriate choices for
binning target variables 
we show that recordings from highway sensors are good
predictors of monthly reports of crime in oakland  ca and
warrant further investigation 

year

    

class
data
average

    
    
    
    
    
   
   

we would like to thank andrew ng for insightful discussions and stanford university for resources 

    

year

    

figure    inequality indicator vs  time  shown as both actual
continuous data and our classification of above or below average where the indicator is percentage of californians on
welfare and crime in oakland 
in oakland  for example  we had     classification error
predicting the future month as compared to     when predicting within the same month 

conclusions
in this paper  we train classification models to predict social
inequality like the percent of californians on welfare  the
number of people in ca receiving food stamps  and crime
reports per month  we build the set of features from a large
dataset of highway sensor recordings using our custom machine learning system  we apply this framework to historical
data from highway sensors in the san francisco bay area
and indicators of local social inequality over the last decade 
common dependence of our predictor and target variables  such as on time  poses challenges for gaining nontrivial results  especially for unemployment rates  number

references
atkinson  a        on the measurement of inequality  journal of economic theory          
choe  t   skabardonis  a   and varaiya  p        freeway
performance measurement system  pems   an operational
analyis tool  transportation research board annual meeting 
hadayeghi  a   shalaby  a   and persaud  b       
macrolevel accident prediction models for evaluating safety
of urban transportation systems  transportation research
record              
hall  m   frank  e   holmes  g   pfahringer  b   reutemann 
p   and witten  i  h        the weka data mining software 
acm sigkdd explorations newsletter          
helpman  e   itskhoki  o   and redding  s        inequality and unemployment in a global economy  econometrica
               
jenkins  s   and micklewright  j        inequality and
poverty re examined  oxford  uk  oxford university
press 
khattak  a  j   wang  x   and zhang  h        spatial
analysis and modeling of traffic incidents for proactive incident management and strategic planning  transportation
research record  journal of the transportation research
board                 

fi