nonlinear extensions of reconstruction ica
apaar sadhwani and apoorv gupta
cs    project report  fall     

abstract in a recent paper     it was observed that unsupervised feature learning with overcomplete features could
be achieved using linear autoencoders  named reconstruction
independent component analysis   this algorithm has been
shown to outperform other well known algorithms by penalizing the lack of diversity  or orthogonality  amongst features 
in our project  we wish to extend and improve this algorithm
to include other non linearities 
in this project we have considered three unsupervised learning algorithms   a  sparse autoencoder  b  reconstruction ica
 rica   a linear autoencoder proposed in      and  c  nonlinear rica  a proposed extension of rica for capturing nonlinearities in feature detection 
our research indicates that exploring non linear extensions
of rica holds good promise  preliminary results with hyperbolic tangent function on the mnist dataset showed impressive
accuracy  comparable with sparse autoencoders   robustness 
and required a fraction of the computational effort 

i  m otivation
in many problems of interest   text  video  imaging  or
speech   a fundamental problem is that of feature recognition  hard coded  or  hand engineered  features have long
been used  and continue to provide us with great accuracy 
however  hard coding features requires much experience and
the performance with unexplored domains remains uncertain 
to overcome these obstacles  we use unsupervised learning 
wherein we try to identify features that occur in a sparse
fashion across our data  mathematically  this amounts to
trying to re construct a high dimensional vector by passing
it through a map with sparsity constraints  the sparsity
constraints force retention of only important details  that is 
the required features 
given our high level strategy for feature extraction  it is
important to note that while data contains only a sparse set of
features in a particular instance  the number of such features
can be potentially many  in line with this observation  the
restriction that the number of features be less than the
dimensionality of data seems unnatural  this occurs in many
algorithms  and rica provides a way of circumventing
this by penalizing degeneracy  instead of imposing it as a
hard constraint  however  with its current formulation  rica
captures only linear features  through our study  we explore
the benefits of extending it to non linear features 
ii  i ntroduction
in this paper  we compare the performance of three unsupervised learning algorithms on accuracy and time  the first
of these algorithms is sparse autoencoder  which is a feedforward neural network  here we consider the special case of
a neural network with three layers   an input layer  a hidden

layer  and an output layer  the hidden layer automatically
enforces sparsity constraints since we have a small number
of these  in addition  we enforce penalty on the average
activations of the hidden layer  which is especially useful for
the case of overcomplete features  this is further explained
in section iii  the next algorithm we consider is rica  as
explained in iv  with its soft orthonormality constraint 
overcompleteness is accommodated with no extra effort 
sparsity is however enforced as a separate penalty term in
the objective function  the performance obtained depends
considerably on the nature of the penalty term  and we
consider both l  and l  penalties  this is further explained
in section iv  and one may refer to the original paper    
for the underlying theory and details 
in the end  we propose a non linear extension to rica 
the non linear extension we test with is only for encoding
the features  while the decoding function remains linear  this
function is hyperbolic tangent  and our choice was based on
its ubiquitous use as a basis function throughout the machine
learning literature  we explain this procedure in greater detail
in section v 
it is important to note that all three algorithms can be
viewed under the same umbrella   sparse autoencoders use
sigmoid function for encoding decoding  rica uses linear
function for encoding decoding  and non linear rica uses
tanh for encoding and linear for decoding  we test the
three algorithms for different number of hidden features 
both above and below the overcompleteness threshold  the
training and testing is performed on the mnist dataset of
handwritten digits  which is obtained from ix  the results
and conclusions are discussed in section vi 
iii  s parse autoencoders
a  introduction
suppose we have unlabeled training examples set
 x      x              an autoencoder neural network is an unsupervised learning algorithm that applies backpropagation 
setting the target values to be equal to the inputs  i e   it
uses y i    x i    the autoencoder tries to learn a function
hw b  x   x 
b  model formation and framework
we use the following neural network defined by the following equations and framework  let nl denote the number
of layers in our network  our neural network has parameters
 w  b     w      b    w      b      where we write wi j  l  to
denote the parameter  or weight  associated with the connection between unit j in layer l  and unit i in layer l      also 

fi   perform a feedforward pass  computing the activations  ai  l  for
layers l    l    up to the output layer lnl   using equations       
   for the output layer  layer nl    set
  nl      x  a nl      f    z n   

   

where  denotes the element wise product operator 
   for l   nl     nl     nl               set
  l      w l  t   l       f    z l   

   

   compute the desired partial derivatives 
w  l  j w  b  x  y      l     a l   t
b l  j w  b  x  y      l   

   

   calculate jsparse  w  b  using 
fig    

s 

autoencoder

jsparse  w  b    j w  b    

 kl k j  

   

j  

bi  l  is the bias associated with unit i in layer l      we will
write ai  l  to denote the activation  or output value  of unit
i in layer l  given a fixed setting of the parameters  w  b  
our neural network defines a hypothesis hw b  x  that tries to
reconstruct the original input  specifically  the computation
that this neural network represents is given by 

where    j  sparsity constraint  and kl k j   is derived in     
here we update the parameters as follows 
h  
i
w  l     w  l      w  l      w  l 
m
h 
i
b l     b l   
b l 
m

a        f  w       x   w       x   w       x    b       
a        f  w       x   w       x   w       x    b       
 hw b  x  i   ai       f  wi      a       wi      a       wi      a        bi      

so  we observe the sparse auto encoders have the following unconstrained optimization problem 

   

in the sequel  we also let zi       nj   wi j     x j   bi denote
the total weighted sum of inputs to unit i in layer l  so that
 l 
 l 
ai   f  zi    also 
z

 l   l 

 l 

 w a

 b

 l   

 l   

a

  f  z

   

 

   

c  backpropagation algorithm
given a training set of m examples  we then define the
overall cost function to be 
h 

m

i 
 i 
j w  b   
j w 
b 
x
 
 

m i  
 
fifi  i 
h   m   fifi
fifi
fifi
 
  fifihw b  x i     x i  fifi    

m i    
 

nl   sl sl  

 l   

    w ji

l   i   j  
nl   sl sl  

 

 l   

    w ji

   

where  is the learning rate 
   train neural network using unconstrained optimizer l bfgs to find
w    argminw j w  b  
   use   w x   x   training data  test data  to find the training   test
features 
   calculate the test and training error using softmax regression 

a        f  w       x   w       x   w       x    b       

 l   

   

 

l   i   j  

   
the second term is a regularization term  also called a
weight decay term  that tends to decrease the magnitude of
the weights  and helps prevent overfitting the weight decay
parameter  controls the relative importance of the two
terms  the goal is to minimize j w  b  as a function of w
and b using the following back propagation algorithm 

minw

h

m

 k  w t   w x i    b    x i  k     

m i  

s 

 kl k j  

j  

    
 
where   x      exp x 
  as  we will explain later in section
v  this forms our basis for motivation for modifying the
optimization function of rica 

d  results
by forming the image formed by the pixel intensity values 
we can understand what feature a hidden unit is looking for 
each square in the figure below shows the input image x
that maximally activates one of hidden units  observation is
that different hidden units have learned to detect edges at
different positions and orientations in the image  we run the
proposed algorithm of sparse auto encoder on our dataset
and get the following features 
this demostrates that the sparse auto encoder algorithm
learns a set of edge detectors  which are like pen strokes
for mnist dataset   in the figure below  we show that as
we increase the number of hidden units in the sparse autoencoder  the test accuracy starts increasing  at the same
time  the train accuracy reaches      which might be an
indication of overfitting 

fiica  in rica  the orthonormality constraint is replaced
with a linear reconstruction penalty jut like the one we
defined above for sparse auto encoders in section iii   this
also makes it possible to use unconstrained solver l bfgs
for the optimization of the cost function  the reconstruction
penalty can also be applied across all receptive fields and
hence prevents degenerate features  also  as rica is based
on foundations similar to auto encoders  it is less sensitive
to whitening 
b  model formation
m

given the unlabeled data  x i   i     x i   rn   regular ica
is defined as the following optimization problem 
fig    

m

output visualization      hidden units 

minimizew

k

  g w j x i     subject to ww t   i

    

i   j  

rica on the other hand produces the following unconstrained optimization problem 
i
h m
m k
kw t w x i   x i  k       g w j x i   
minimizew

m i  
i   j  
    

fig     test and training accuracy for sparse auto encoder with different
hidden units

iv  r econstruction ica
a  motivation
sparsity has been shown to work well for learning feature
representations that are robust for object recognition  as
discussed in section iii  sparse autocoders is one of the many
ways  other methods include ica nad isa  independent
subspace analysis  refer paper  but the standard ica has
two major drawbacks 
 overcomplete features  the ica is not easy to train in
presence of overcomplete features  when the number of
features  dimensionality of input data   autoencoders
on the other hand work well in case of overcomplete
features 
 whitening  standard ica is sensitive to whitening  a
preprocessing step that decorrelated the input data  and
can not always be computed exactly for high dimensional data 
both constraints in the standard ica arise due to the
hard orthonormality constraints requiring the features to
be orthogonal i e  ww t   i  which is used to prevent
degenerate solution in the feature matrix w   however 
this condition of orthonormality cannot be met if we have
overcompleteness 
in      the authors have proposed reconstruction
ica rica   which is a modification to the standard

where g is a nonlinear convex function  e g   smooth l 
penalty  g      log  cosh      as it mimics the l  norm
k k    we also construct and analyze l  penalty in later
section  w is the weight matrix w  rkn where k is the
number of components  features  and w j is one row  feature 
in w  observe that rica is tied weighted  i e  w is the
encoding weights matrix and w t is the decoding weights
matrix  i e  the encoding step is w x i  and the decoding step
is w t  w x i    the aim is to minimize the following cost
function 
h m
i
m k
t
 i  
 i   
 i 
j w    
kw
f
 w
x
 

x
k
 
g 
f
 w
x
  
j


 
m i  
i   j  
j w    


h w     g w  
m

    

where 
h w      w t w x  x t  w t w x  x 

    

g w     g w j x 

    

the activation function considered in case of rica is
proposed to be a linear   w x i     in the next section  we
propose a non linear extension of rica which consider a
range of non linear functions f  w x i    instead of  w x i    and
try to compare its performance with our implementation of
rica in terms of time and accuracy 
v  n on l inear e xtension of rica
a  model formation
in this section  we propose our formulation of extending
rica to incorporate non linear functions  hereafter  we
refer to our algorithm as nle rica  the main motivation
behind nle rica is to form an intermediate function that

fitable i
t he following algorithm is used for l inear rica 

where 
h w      w t f  w x   x t  w t f  w x   x 

  initialize              
g w     g  f  w j x  
  using back propagation algorithm  calculate  a      a                          
  calculate w h w       ww t w xx t  w xx t w t w   w xx t  
and g      log cosh     and f                     
  calculate w g w     tanh w x x t

                    hence 
  calculate w j w     m
 w h w      w g w    and j w  
  use the unconstrained optimizer l bfgs to find w    argminw j w  

  use f  w x   x   training data  test data  to find the training   test features 
w j w     w h w     w g w  
  calculate the test and training error using softmax regression 
m

mimics linear function in case of rica and the non linear
function sigmoid  in case of sparse auto encoders  for the
same purpose  we consider the following functional form
for f  w x i    
 f  x    x  linear rica 
 f  x       x       tanh x   mixed nle rica i 
 f  x       x       tanh x   mixed nle rica ii 
 f  x    tanh x   pure nle rica 
the reason for including tanh x  in the nle rica is its
property of mimicking the shape of sigmoid functions 
thus  while making a transition from linear rica model

    
    
where

    

where we calculate w h w   using the backpropagation algorithm defined in section iii and
w g w     tanh  f  w x                tanh w x      x t

 w h w     w t h w      w g w  
m
t
t

         a      a             w g w  
m

w j w    

    
    

table ii
t he following algorithm is used for nle rica 
  initialize               and                
  using back propagation algorithm  calculate  a      a                          
  calculate  w h w    w t h w    w g w   

  calculate w j w     m
 w h w     w t h w      w g w    and j w  
  use the unconstrained optimizer l bfgs to find w    argminw j w  
  use f  w x   x   training data  test data  to find the training   test features 
  calculate the test and training error using softmax regression 

vi  r esults
fig    

a  effect of 

graph for tanh x  and    x      tanh x 

to pure nle rica model we incorporate some linearity
as well  these models are referred to as mixed nle rica
i and mixed nle rica ii models  hence  nle rica is
defined as the following optimization problem 

minw

h

m

i
m k
t
 i  
 i   
 i 
kw
f
 w
x
 

x
k
 
g 
f
 w
x
  
j


 

m i  

i   j  

    
we observe that in nle rica  the encoding is done
through f  w x i     but the decoding is done through
w t f  w x i     this is different from sparse auto encoder
where you had sigmoid function being used for encoding
 from input layer to hidden layer  and decoding  from hidden
layer to output layer   this also differs from rica in the
sense that the encoding is carried out by f  w x i    instead of
 w x i     also  observe the change in the l  penalty term in
the optimization problem  lets define the conventions before
discussing the algorithm 
j w    

h

m

m

k

i
 kw t f  w x i      x i  k       g  f  w j x i    

m i  

i   j  

j w    


h w     g w  
m

    

 seems to play an important role in the calculate of cost
for both linear rica and nle rica  very small values of
  like those in auto encoders  provide inadequate weight
to the error term of h w    for our study  we chose   
            the results for the two values are summarized in
the graph below  figure   
it is clear from the graph that we cannot treat a value of
           to be superior than the other  but a small value
of  can clearly be ignored 
b  effect of number of hidden units
choosing an optimal number of hidden units  k  is specifically important for the rica algorithms to work well  a low
number of hidden units make poor prediction giving high
test error  whereas  high number of units leads to may be
an indication of over fitting  making    training error   we
run our algorithms for k                             and get
the following results figure    
c  change to l  norm for reconstruction penalty function
instead of g      log cosh      the usual l  norm   we now
take g      k k   l  norm  as the reconstruction penalty  we
apply this reconstruction on the four rica functions and
obtain the following errors k              table iii  
as we can see  the l  penalty also provides good test and
training accuracy  but  the tradeoff present is slower running
time 

fifig    

test accuracy for different         k        iterations 

vii  i mplementation
fig     training top  and test accuracy bottom  for different values of
   k          iterations

in the project  we run our algorithms on        training          test samples from the mnist dataset  the
simulations were effected in the stanford corn computing
environment with   core     ghz amd opteron       
processor    gb ram    gb swap      gb temp disk 
running ubuntu gnu linux operating system     
viii  c onclusions and f uture w ork

fig     training top  and test accuracy bottom  for different number of
hidden units             iterations 
table iii
t est and t raining e rror for l  reconstruction penalty
model
linear rica
mixel nle rica i
mixed nle rica ii
pure nle rica

training accuracy
      
      
      
    

test accuracy
      
     
      
      

d  effect of 
we check the test accuracy by varying the parameter
 of our non linear model f                   tanh    
here we use w g w        f  w x                
 tanh w x      x t in the nle rica algorithm 
as is evident from figure    the test accuracy is
decreasing as we move from       pure nle rica 
to       linear rica   thus  nle rica can be more
accurate than linear rica 

the class of rica algorithms  both linear and nonlinear  provide a worthy alternative to sparse autoencoders 
especially for scenarios where it becomes important to allow
for overcomplete features  they compete well on accuracy of
classification and consume much less computational effort 
they are easy to code  seem quite robust in obtaining a
consistent minima  and give very similar in sampe and outof sample performance 
the non linear extension seems a candidate worthy of
exploring further   it beats the linear counterpart consistently 
future research should be directed towards experimenting
with other functions  and also with deriving a more sound
theoretical basis for their better performance 
amongst the drawbacks  we found that the values of the
hyperparameters   and the number of hidden units  in rica
play a significant role in obtaining a reasonable solution 
investing time in developing a rule of thumb to guide a first
time user would be very useful to more widespread use of
this method 
time permitting  we would have liked to test our results
on other standard databases for our non linear extension 
also  it was hard to compare the performance of sparse
autoencoders with that of rica  and its variants   as yet
another step  we are quite curious and look forward to
developing a theoretical framework to provide rules of thumb
to guide the trade offs between number of hidden units  linear
or non linear features  lambda  number of iterations  and
lastly  the number of layers of encoding to obtain a sparse
representation 

fiix  acknowledgements
we would like to acknowledge and thank mr  quoc le 
phd student at stanford cs department  for helping us with
the project and providing us the mnist dataset 
r eferences
    q v  le  a  karpenko  j  ngiam  a y  ng        ica with reconstuction cost for efficient overcomplete feature learning  nips 
    q v  le  j  ngiam  a  coates  a  lahiri  b  prochnow  a y  ng       
on optimization methods for deep learning  icml
    ng  a  cs   a lecture notes   stanford university  
    unix computing environments  stanford university 

fi