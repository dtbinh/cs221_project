cs     final project
reduced rank regression

name   ka wai tsang
sid            
   introduction

given m observations of the predictors xi  rp and the corresponding responses yi  rn  
let y    y    y            ym  t and x    x    x            xm  t   suppose x and y are related by
     

y   xa   e

where a is an unknown p  n matrix of coefficients and e is an unobserved m  n random
noise matrix with independent mean zero and variance      we want to find an estimate a
such that   y  x a   is small  if we use standard least square estimation directly to estimate
a in       without adding any constraints  then it is just the same as regressing each response
on the predictors separately  in this way  we actually ignore the possibility that the responses
may be correlated among themselves  besides  when there are many attributes  p is large 
and many different kinds of responses  n is large   the number of unknowns can be larger
than the sample size m  we may then need much more effort to collect more samples to
increase m or the least square method simply cannot be applied 
to address this problem  one popular way to handle it is reduced rank regression  let r m  
be the rank of a matrix m   if we expect r a    r   min  p  n  or a can be well approximated
by a low rank matrix  we can write a as a product of two matrices with rank r  see      that
is a   br cr   br  rpr and cr  rrn which have total r   n   p  unknowns needed to be
estimated  it can be much less than m if r a  is very small  the model       then become
y    xbr  cr   e
it can be interpreted as instead of p attributes  y actually only depends on r factors  each
factor is a linear combination of the attributes  in another words  this model says that the
attributes are correlated which is often the case in many real situations 
it is quite often that too many attributes are considered when we build a model  if we believe
that some attributes are actually not important to determine a response or we are only
interested to those really important attributes  then we would like a to be row sparse  let
j m   be the index set of the non zero rows of a matrix m and  j m    is the corresponding
cardinality  we would like to have  j a    or  j a    small so that we can only consider
those attributes in j a  in future prediction  a common way to make the estimate a sparse
is adding appropriate norm penalty  such as zero norm or   norm 
in this project  i am going to introduce three reduced rank regression methods  see      which
can give an estimate a with r a  and  j a   small 
   method  
step    we first find an estimate a  such that
     

a    arg min
   y  xb   f   r b  
pn
br



with          n   p     see      in this step  we add a penalty term r b   so we
would expect a  is a low rank matrix  let k   r a    
 

fi 

step    use the k computed before  find a such that
 
     
a   arg
min
    y  xb   f     b       
pn
br
 r b k  
p
p t
where   b       
  bi       bti is the i th row of b  here k   k   is a regularization
i  

parameter that can be estimated by cross validation method  notice that if the
constraint r b   k is removed  then a is a group lasso estimator  lasso method
will output sparsity in the estimation 
   method  
step    for each pair  k     f or   k  p and a range of values   we find an estimate ak 
such that it is the minimizer of       
step    among the ak  computed above  choose the one a such that
     

a   arg min   y  x ak     f         n    j ak     r ak      
ak 

this method is called selective reduced rank regression introduced in      we observe
that the penalty term here penalizes both r ak    and  j ak      it is an penalty
designed for selecting estimators with the best bias variance trade off relative to a
list of possible candidates 
   method  
step    we first find an estimate a  such that
 
a    arg min
    y  xb   f     b       
     
pn
br
 
the result is an group lasso estimator and so will be a sparse matrix  let x be the
predictor matrix that only contains the attribute columns selected here  that is the
columns that the corresponding rows in a  are non zero 
step    find a such that
     

a   arg min
   y  xb   f   r b  
qn
br



with q    j a              n   q   as before 
   simulation
we first make up some examples to see if these methods work  the setting i use here
is the similar as that in     so that i can compare the results  we construct the matrix of
dependent variables x with rows i i d  from a multivariate normal distribution mvn     
with jk    jk             j  k  p  set the coefficient matrix


bb  b 
a 
 
where b      b  is a j  r matrix and b  is a r  n matrix  all entries in b  and b  are
i i d  n         therefore  almost for sure j   j a  and r   r a   generate e  rmn such
that eij  n         then set y   xa   e 

fi 

to evaluate how good an approximation a is  for the same setting  m   j   p  n  r 
rho and b  we repeat the methods    times and then we look at     mean square error  mse 
 and
  xa  x a   f   mn  using test data at each run     the mean number of predictors   j  
mean rank estimate  r   and maybe the most important    how many correct or wrong 
predictors were selected in comparison to the correct coefficient matrix a  it is measured by

the missing rate  m    j  j   j  
and false chosen  f c    j  j   p   j    rate  a good
approximation should have low m and f a 
for the setting  m        j        p        n    r             and b       the simulation
results showed that the performances of these methods are close to each other  it is not
surprising because as the main components of these methods actually are similar  all these
methods successfully estimated the rank of the correct coefficient matrix and selected most
 around      of actual related features
mse
j
r
m
fc
method  
    
    
 
    
    
method  
    
    
 
    
    
method  
    
    
 
    
    
   vector autoregressive model
to apply these methods in real applications  i have collected the interest rate swaps for    
                            year from jan      to sep           my problem here is to estimate
the future values of the swaps using the past values  consider a vector autoregressive  v ar 
model
yt       yt  a         ytd ad   t


 
 a  


a  
     yt  yt        ytd   
   
    
ad
where yt  rn contains n swaps value at day t  ai  rnn are coefficient matrices and
t  rn are i i d  with mean   and covariance matrix   suppose we use m   d days to
approximate ai   then we have y   xa   e  y  rmn   x  rmp and a  rpn where
p     dn  the i th row of y is yti and the corresponding row of x is    yti        ytid   
notice that we have the term    in the above methods  for the case the covariance matrix
   i      suggests an unbiased estimator
     

s       y  p y    f   mn  pn 

where p is the orthogonal projection matrix on the column space of x 
in order to satisfy the assumptions of the v ar model  i first differentiate the times series in
the time dimension  by augmented dickey fuller test  no unit root is present in each time
series with     confidence level 
however  the performance of the methods in this application is not good  it may be because
 
    the above methods tend to generate an estimate with more zero rows which means
the corresponding columns should not be selected in the model  in autoregressive

fi 

model  people expect the values today depend more on the values right before today
than the days further before  before the experiment  i thought most of the zero rows
would appear at the bottom of the estimate matrix a which are corresponding to the
dates farthest away from the current day t so that the methods can tell me how many
days  d  should be used to predict the future values  however  the result showed that
the number of zero rows at the top of a is roughly the same as that at the bottom 
i think the problem is that if we want to find out how many days should be used in
the v ar model  then all the swaps value at the same day should be consider as one
group  either all the swaps in this day should be chosen or none of them 
    obviously the values of the interest rate swaps with different maturities are correlated  the assumption that the variance matrix e   i may be too strong for this
application and so       is not a good estimate 
to make the methods gave me something make sense  i only considered ar    model 
that is d      therefore  selecting variables is now nothing about selecting approximate
number of regression days in the model  but selecting the interest rate swaps that contain
most of the information about the swaps values tomorrow 
the procedure of this experiment is as follow 
    divide the data set into one training set  jan      to dec       and one test set
 jan      to sep       
    use the training set to compute a by the above methods 
 i 
 i 
    for t in test set  compute prediction yt   yt  a and error term  yt  yt    where
       
 n 
yt    yt   yt           yt   is a vector of actual differentiated interest rate swaps at day t 
the table below shows the results of those three methods  mean  yt   i  is the mean of
the estimated differentiated interest rates swaps using method i  from the table  we can see
that the results are very poor  i have modified the methods such as fixing the matrix to be
full rank and increasing d to see if these help improve the results  it helps a little bit but
still cannot give reasonable estimates 

mean  yt   
mean  yt    
mean  yt    
mean  yt    

  yr
     
     
     
     

  yr
     
     
     
     

  yr
     
     
     
     

  yr
     
     
     
     

  yr
     
     
     
     

  yr
     
     
     
     

   yr
     
     
     
     

   yr
     
     
     
     

the reason may be the methods are based on the assumption that the actual coefficient
matrix is of low rank and sparse  clearly  as swaps are correlated  the matrix should be of
low rank  however the matrix probably not sparse in this case 
   future works
to improve the methods so that they can be applied on more application  i think there are
at least   things we can do  first  we can introduce grouping into the algorithm  sometimes

fi 

the features are naturally grouped  like in the v ar model the features are group by dates 
if our question is which group should be chosen in the regression model  then treating each
feature individually should not be a good approach  to achieve this  instead of adding
p
p
p
penalty   b       
  bti       maybe we can use
  bit   f   where bit is a sub matrix of b
i  

i

formed by the row vector in a group with indices in i 
besides  we may also try to improve the estimate       or use another variance estimate
when the assumption the variance matrix e   i is not likely to be true 
references
    g  c  reinsel and r  p  velu  multivariate reduced rank regression  theory and applications 
lecture notes in statistics      springer  new york       
    f  bunea  y  she and m  wegkamp  joint variable and rank selection for parsimonious estimation of
high dimensional matrices       
    f  bunea  y  she and m  wegkamp  optimal selection of reduced rank estimators of high dimensional
matrices  annals of statistics                      
    http   www gsb stanford edu jacksonlibrary 

fi