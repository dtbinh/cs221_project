detecting audio video asynchrony
alex perelygin and michael r  jones
december         

  introduction

  features

asynchrony between the audio and video track in
streaming video can cause user frustration with
the service but often goes unreported  providers
of on demand internet streaming media such
as netflix reach millions of users and thus
are concerned with supplying correctly aligned
audio video content  our project will attempt
to detect a v misalignment in standard  nonstreaming  movie clips  we expect that if certain
time dependent video and audio features are
correlated  we should be able to predict the
misalignment of a video and audio track by
comparing the progression of these audio and
video features over a segment of the movie clip 

we selected audio and video features based on
their ability to reflect continuous states over
time  each feature was computed over a   second
segment  consisting of    frames  the value and
first derivative value of each feature was used 
since in many cases the change from the last
value of the feature is the most important point 
a total of      features per    second window
were collected 

    video features
we used a    bin image intensity histogram and
image intensity statistics  mean and standard
deviation   since the change of these over
time corresponds to how much the image is
changing  we reasoned this could correspond to
audio shifts  and is used in other research as a
synchrony measure which is the video equivalent
of audio energy      the correlation coefficient
between video frames was picked for a similar
reason  as low correlations would imply major
changes in the scene  the binary image area and
binary image centroid were used to detect the
proportion of the scene that was bright versus
dark  and where the center of the bright area
was  it was done by converting the frame image
into a binary image using a threshold calculated
by the graythresh function in matlab  this
was reasoned to be a more precise formulation
of a specific scene characteristic  brightness 
the change in this value was reasoned to also
correspond to audio shifts over time 

  assumptions and restrictions
misalignment in audio is perceivable when audio
leads video by more than    milliseconds and
lags video by more than    milliseconds     
based on this find we formulate the duration
of our time window during data collection to
capture the variation in a v features over time 
we will restrict misalignment of the audio
track to constant values over the duration of the
movie clip  i e  not time varying of progressive 
we will also limit our dataset to movie clips in
which the audio track has been delayed 

 

fi  method and implementation

also  face detection was performed using
the opencv viola jones algorithm     on each
frame  the feature value was   if any faces were
detected in the image  and   otherwise  this
was reasoned to correspond with the speech
track 

our approach to this project consisted of using
the feature set we developed to train a supervised
learning algorithm to predict misalignment  the
basic premise was that the    second window
used would contain a correlation of the changing
video features over time to the changing audio
features  and this would constitute a detectable
pattern  the goal of training would then be
for the learning algorithm to find this pattern 
match it with a set of known alignment delays 
and learn a prediction of the correct classification
based on this 

    audio features
the root mean square and the max min range of
the audio signal over a frame of video was used
as a general audio feature  which we reasoned
would correspond to a rough approximation of
the audio track for each second segment 
the standard deviation of the waveform 
the zero crossing rate  and the mel frequency
cepstral coefficients  mfccs  were all used to
identify similar segments of audio  as described
by gillet  essid  et al      the mfccs were
calculated using matlab code by daniel ellis     
the standard deviation and zero crossing rate
should correspond to periodic audio elements
and noise  respectively  the mfccs provide a
parameterization of the human speech signal
     so they are able to provide segmentation
of speech in an audio track  since human
detection of audio video synchrony is often
based on the alignment of speech with visual
cues such as lip movement  we reasoned these
coefficients could provide a rough indicator of
certain common video contexts  typical speechrecognition research combines the standard   
mfccs with    delta  first derivative  and   
delta delta  second derivative  mfccs       
these correspond to dynamic features of the
speech signal  used both for speech recognition
and speaker verification  the mean  standard
deviation  and max min range of these   
coefficients were used 

    feature extraction
we implemented the data set generation
 extraction of the features to be used  using the
videofilereader class of the vision toolkit in
matlab  this was done by stepping through each
frame in the videos used  using the frame rate to
collect the per second feature values  and storing
every    second window as a column in a features
matrix  misalignment was artificially introduced
by stepping through a fixed number of frames in
the audio track prior to beginning the feature
collection  introducing a delay equal to the time
spanned by the number of frames skipped 

    training and testing
we trained and tested using features extracted
from   movies  pirates of the caribbean
and imagine me   you  standard movies
with regular characteristics  these movies both
involved significant amounts of dialogue  leading
us to believe our speech recognition features
could come into play  pirates of the caribbean
involved significantly more action sequences than
the other movie  so we wished to compare the
effects of this on our learning  we trained and
tested on both movies separately and together  in
order to compare the performance of our learning
in both a specific and a more general setting 

a   bin average power of the audio waveform
was computed from its periodogram  the power
of audio signal and the change in power should
similarly correspond to shifts in the video scene 
as is noted by chollet et al     
 

fi    linear classifier

was still slow on our audio video feature set 
however  so we decided to use minfunc with the
initially  we attempted to use linear regression l bfgs method     to find the weight values
to create a linear prediction of alignment delay rather than using gradient descent 
based on the feature values  we implemented
it using stochastic gradient descent  since the
large data set would have made batch gradient     whitening and pca
descent very slow 
our first attempts to
this algorithm was still not able to learn from
run the algorithm resulted in exploding weight
the data very well  as it would not be able to
values that clearly never reached the costdecrease the generalization error on the training
minimizing value  we were able to fix this
set  we deduced that this was due to the feature
by using a much lower learning rate  around
values being of differing signs and magnitudes 
       instead of     as we picked originally  
which resulted in neural network weights that
and by dividing the feature values by their
were also very skewed  therefore  we applied
maximum values  thereby standardizing them 
principal components analysis with whitening to
with these practical considerations  we were
our data prior to training the neural network
able to minimize the cost function and converge
on it  the main idea here was to reshape the
to the optimal set of weights  however  the
data into a fairly uniformly scaled hypersphere
alignment delay values predicted by the classifier
so that it would be simpler to learn from  we
were extremely inaccurate  returning results that
initially used     whitened principle components
appeared random and were orders of magnitude
with a neural network consisting of    hidden
away from the correct value 
neurons to train  as we found this could find
the lowest generalization error with appropriate
regularization factors  but we varied this to
    neural network
test different possibilities  finally  we used the
based on the poor results offered by the linear liblinear svm with l  regularization l  loss     
classifier  we deduced that there was no linear to classify the data as a comparison to the neural
relationship between the feature values and the network that was our main learning algorithm 
true alignment delay in the    second windows
we were testing over  since we did not have
any deeper knowledge about what form of   results
relationship there could be  we decided to use
a neural network to automatically find and learn the first result that we noticed was the large
from the correlation  thus  we implemented a effect that the regularization factor had on the
recursive backpropagation neural network using generalization error  as the parameter value
a single hidden layer  which predicted a single approached    or indeed reached   the accuracy
output    for misaligned    for aligned  we of the trained neural network predictions on the
used a sigmoid activation function at all layers  training set approached       however  with
to create a clear boundary between aligned and larger values  the accuracy of the predictions
misaligned activations  we initially did this on the training set decreases before reaching a
using batch gradient descent  with a learning steady level around      as the chart below
rate of     and a regularization factor  lambda  shows  however  the accuracy on the test set
of      however  testing on the iris flower data did not change significantly as the generalization
set     revealed this was underfitting the data  so error changed  the testing accuracy remained
we reduced the regularization factor to       and around      or equivalent to random guessing 
were able to classify the data well  convergence this implied that  for a training set size of
 

fimaking predictions on unseen examples  we
also noticed that the generalization error of the
neural network on the training set increased as
the number of principal components increased 
meanwhile  the generalization error of svm
increased as the number of principal components
increased  we believe this can be attributed to
the regularization factor having a greater weight
on cost function as the size of the weight matrix
increases  this conclusion is reinforced by the
fact that for lambda equal to zero  the accuracy
of the neural network remains near      for any
number of principal components  conversely 
increasing the input dimension provides the
svm with greater facility to linearly separate the
training examples 

figure    regularization factor vs accuracy
      the neural network could determine an
appropriate pattern in the data  but this was not
useful to predicting previously unseen examples 
next  we chose a specific regularization factor
value          and varied the training set size
to determine if the poor performance was due
to overfitting  as the chart below shows  the
generalization error does increase as training set
size increases  but there is no corresponding
change in testing accuracy 

figure    svm vs neural network

  conclusion and future work
figure    training size vs accuracy
we were not able to find any useful correlation
between the features used and the alignment of
the video and audio tracks  the neural network
was able to find a pattern  but this pattern was
not useful for future prediction  we believe this
was due to the inapplicability of general audiovideo correlation measures to alignment  due to
the extremely wide range of scenarios in videos 
a way to mitigate this inapplicability would
be to introduce specific features for specific
scenarios  an example would be a feature to
detect an action such as a door slamming or an
explosion  paired with an audio feature to detect

this led to the same conclusion as from
the first result  the neural network was able
to do an excellent job of finding a non linear
pattern in the data and match it to the output 
but this was not useful for making future
predictions  finally  we compared our neural
networks performance to that of an svm  using
several different principle component numbers
                    and       our neural
network was better at finding patterns in the
data and classifying the training set than the
svm was  but they were equally poor at
 

fithe expected audio response  the data set would     hossan  m a   memon  s   gregory 
m a     a novel approach for mfcc
need to be restricted to include these scenarios 
feature extraction  signal processing and
then  a bank of specific feature scenario pairs
communication systems  icspcs       
could be constructed to create more specific cases
 th international conference on   vol   no  
where a clear expected video audio pair occurs 
pp            dec      

  acknowledgments

    frank  a    asuncion  a         
uci
machine
learning
repository
 http   archive ics uci edu ml  
irvine 
we would like to thank andrew maas for
ca  university of california  school of
generously offering advice and for providing the
information and computer science
matlab code for feature whitening and pca 
    mark schmidt  retrieved from  http 
  www di ens fr  mschmidt software 
minfunc html 

references

    relative timing of sound and vision for
     machine learning group at national
broadcast operations  advanced television
taiwan
university 
retrieved
from
systems committee report  is           
 http   www csie ntu edu tw  cjlin 
liblinear  
    herve bredin and gerard chollet       
audiovisual speech synchrony measure 
application to biometrics  eurasip j 
appl  signal process           january
               
    dirk jan
kroon 
retrieved
from
 http   www mathworks com 
matlabcentral fileexchange 
      viola jones object detection 
    gillet  o   essid  s   richard  g     on the
correlation of automatic audio and visual
segmentations of music videos  circuits
and systems for video technology  ieee
transactions on   vol     no    pp         
march     
    ellis 
d  plp and rasta  and
mfcc  and inversion  in matlab 
 http   www ee columbia edu  dpwe 
resources matlab rastamat  
    h  bredin and g  chollet  audiovisual
speech synchrony measure  application to
biometrics  eurasip journal on advances
in signal processing       
 

fi