cs    cs   a  final  project  writeup   
supervised  learning     stock  trend  classifier  
  
submitted               
chihchi  kao  
ckao stanford edu  
  

  
    note  for  teaching  staff  

  
unfortunately  my  project  partner   brain  von  
osdol   has  decided  to  withdraw  from  cs     
this  quarter  after  the  week  of  cs     midterm  
exam   this  rather  late  notice  limit  my  options  
completing  the  final  project  on  my  own  effort   
  
this  said   i  am  very  passionate  about  what  we  
were  trying  to  achieve  and  am  willing  to  
complete  the  work  by  myself   because  it  was  
rather  late  into  the  quarter  when  my  partner  
withdrew  from  class   therefore  i  dont  think  it  
is  a  good  idea  to  join  a  team   it  would  not  be  a  
fair  game  for  both  the  team  members  from  
other  cs     project  team  and  myself   
  
i  assume  that  i  have  the  explicit  approval  form  
cs    cs   a  staff  that  it  is  okay  to  
complete  this  final  project  on  my  own  since  i  
did  not  hear  back  after  submitted  project  
milestone   also   i  am  hoping  when  it  comes  to  
grading   the  grade  can  be  evaluated  at  the  
quality  of  my  work   instead  of  the  quantity   i  
may  not  be  able  to  implement  all  the  
features classifiers  that  we  originally  wanted  
to  build   but  every  step  into  the  project  will  
still  demonstrate  my  solid  understanding  to  
supervised  machine  learning  concepts  and  
model  diagnostic  skills   
  

    project  setup  
  

in  this  project  i  am  going  to  train  a  handful  of  
classifiers  to  predict  the  stock  trend  on  the  
next  day  in  terms  of  the  closing  price     in  other  
words   the  class  label  of  example  at  day  d  is     
if  the  close  price   d        close  price   d   and     
otherwise   
  
i  will  design  a  numbers  of  features  that  reflect  
the  embedded  information  that  ultimately  

affect  the  stock  closing  price  of  the  next  
trading  day   will  analyze  non linear  features  
along  side  with  linear  features  in  this  project   
  
at  each  step   i  will  perform  error  analysis  and  
diagnose  the  learning  algorithm   feature   and  
learning  objective  to  improve  the  overall  
prediction  accuracy   i  will  use  the  analysis  
mentioned  in  class  to  address  the  biggest  
error contributing  factor  in  the  next  step   
  
finally  a  performance  metric  will  be  designed  
to  measure  the  performance  of  my  classifier   i  
will  also  provide  a  plan  for  future  
improvement   
  

    stock  data  preprocess  
  
in  this  step  i  transform  the  raw  stock  data  
that  is  stored  in  csv  format  file  to  the  
conventional  data  format  adopted  in  
cs    cs   a   
  
     
  
the  first  thing  i  do  in  this  section  is  to  
generate  the  class  label   y i    for  a  given  
example   since  x i s  are  examples  each  
representing  a  feature  vector  corresponds  to  
date  i   i  define  the  class  label  y i          if  the  
stock  closing  price  at  day  i  is  lower  than  it  at  
day   i      in  other  word   class  label  is     if  the  
stock  closing  price  is  going  up  the  next  trading  
day   and     otherwise   
  
     
  
in  this  section   i  introduce  new  features  that  
are  likely  to  contain  information  about  the  
stock pricing  trend   
  
the  starter  feature  set  i  used  is  rather  nave  

fiand  will  be  improved  between  now  and  the  
project  deadline     
  
concretely   the  feature  set  is    open  price   
closing  price   day  high   day  low   adjusted  
closing  price   volume   
  
     
  
here  i  split  the  data  into  three  sets   training   
cross validation   and  test  set   the  weight  
factor  used  is   training         cv         and  
test         
  

  
    training  with  linear  regression  
classifier  
  
in  this  step  a  handful  of  popular  classifiers  are  
implemented  and  their  performance  
examined  in  multiple  steps   features  are  first  
normalized  according  to  their  mean  and  
variance   
  
     the  quick n dirty  first  attempt  
  
the  logistic  regression  with  regularization  
classifier  is  used  as  the  first  quick n dirty  
classifier  using  only  the  nave  feature  set  in  
hope  to  obtain  insight  about  the  underlying  
problem   
  
the  cost  function  j  here  is  the  standard  cost  
function  used  in  logistic  regression  and  has  
the  following  form   
  
j     



  m
 n
  y  i  log h  x  i         y  i  log    h  x  i        m   j
m i  
j   

  
i  use  the  matlab  function  fmincg m  given  from  
cs   a  programming  exercise  to  search  the  
theta  vector  that  minimizes  the  above  cost  
function   
  
a  learning  curve  is  generated  with      
different  sized  training  examples  drawn  from  
the  training  set  pool  to  help  decide  the  next  
step  of  project   the  matlab  output  as  well  as  
the  learning  curve  plot  are  listed  below   
  
  training examples
  
    
    
    
     
  

  

it  is  clear  that  more  samples  are  not  beneficial  
with  respect  to  the  stock  pricing  prediction   
especially  from  the  cv  set  error   it  suggests  
that  the  parameter   theta   learned  from  
training  data  is  worth  no  more  than  a  random  
guess  and  therefore  has  no  value  in  reality   
  
as  a  result   i  plan  to  significantly  improve  the  
feature  set  so  more  information  is  captured  in  
the  feature  vector   this  leads  to  the  next  step  
of  this  project   introduce  a  set  of  stock  
technical  indices  into  the  feature  set   
hopefully  including  all  the  tech  indices  that  
can  be  derived  from  the  known  stock  pricing  
data     
  
secondly   it  is  still  undetermined  whether  the  
learning  algorithm   i e   logistic  regression  with  
regularization   or  the  objective  function  of  the  
maximization  problem  introduces  more  
classification  error   in  order  to  answer  this  
question   i  will   after  improving  the  nave  
feature  set   implement  a  different  classifier   
eg   svm   and  compare  the  classification  
error accuracy  with  both  classifiers  to  decide  
the  next  step  of  this  project   
  
  
figure     

train error
         
         
         
         
         

cross validation error
         
         
         
         
         

  
training  curve  of  quick n dirty  classifier  

  

fi    improving  feature  set  
  
     rich  feature  set  
  
the  basic  feature  set  contains  only  the  price  
and  volume  information   now   i  add      the  
derivative  of  both  price  and  volume  and      the  
moving  average  to  the  feature  set   concretely   
i  use  finite  difference  equation  to  approximate  
the  first  and  second  order  derivatives  and  the  
different  statistic  moving  average  formulas   
such  as  simple   weighted   and  exponential  
moving  average   
  
now   train  the  logistic  regression  classifier  
 lr  in  this  report   w o  regularization   the  
matlab  output  as  well  as  the  learning  curve  
plot  are  listed  below   
  

      obtained  by       feature  set     
  
however   when  taking  a  closer  look  at  the  
value  of  cost  function  at  each  iteration   i  found  
out  the  cost  with  this  training  set  the  
underlying  minimizer   fminunc   often  cant  
improve  the  cost  after  a  short  amount  of  
iterations   usually         iterations    this  
observation  leads  to  the  next  round  of  the  
model  improvement   concretely   the  
optimization  objective  improvement   
  
  
  
figure     

logistic regression classifier
  training examples
train error cross validation error
  
        
         
    
        
        
    
         
         
    
         
         
     
         
           

  
  
note  that  the  performance  is  dramatically  
improved  with  this  updated  feature  set   
suggesting  that  it  captures  the  time  
dependent  behavior  of  the  stock  price   
  
diagnostic   
  
from  figure      it  is  clear  that  as  number  of  
training  example  increases   the  lr  classifier  
still  underfits  the  problem  and  results  in  
increasing  train cv  set  error   in  other  word   
the  model  has  a  high  bias  as  number  of  
training  sample  increases  and  therefore  we  
need  more  features  to  better  describe  the  
intrinsic  stock  pricing  information   
  
     adding  more  features  
  
   adding   rd  order  derivatives  to  various  price  
features  

  

  
training  curve  of  lr  classifier  with  rich  
feature  set  

  

  
figure      

logistic regression classifier
  training examples
train error cross validation error
  
        
         
    
         
         
    
         
         
    
         
         
     
         
         

  
from  figure      we  observe  the  improvements  
of  adding  higher  order  derivative  feature  with  
respect  to  time  because  the  train  and  test  
error  converges  at  around          when  
training  example  increases   comparing  to  

  
training  curve  of  lr  classifier  with  
 improved   rich  feature  set  

  

fi    improving  optimization  
objective  

  
  
figure     shows  the  cost  at  each  iteration  when  
using  the  feature  set  obtained  in       and  
fminunc  as  the  function  minimizer   
  
it  is  obvious  that  iterations  after  the  third  are  
not  improving  much  on  finding  the  minimum  
of  cost   
i  then  tried  to  use  a  different  function  
minimizer   fmincg   but  obtained  a  set  of  result  
doesnt  not  make  sense   
  
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration

 
 
 
 
 
 
 
 
 
  

 
 
 
 
 
 
 
 
 
 

cost          e   
cost          e   
cost  nan
cost  nan
cost  nan
cost  nan
cost  nan
cost  nan
cost  nan
cost  nan

  
clearly  something  went  wrong  when  updating  
the  cost  function  after  the  first  couple  
iterations   i  then  examined  the  matlab  code  to  
compute  the  logistic  regression  cost  function   
below  is  the  snippet  of  function  
lrcostfunction  i  have   
  
function  j  grad   
lrcostfunction theta  x  y  lambda 
m   length y     number of training
examples
hx   sigmoid x theta  
theta nointercept   theta 
theta nointercept        
  compute the regularized cost function
 use innerproduct to avoid   log   
j     lambda  
 theta nointercept  theta nointercept   
      y    log hx       y     log   hx   
  m 
  compute the gradient of the regularized
cost function
grad   x   hx   y  
grad    grad   lambda   
theta nointercept     m 
end

  
note  that  the  underlined  statement   i  found  
that  although  it  computes  the  cost  leveraging  
the  vectorized  computation   it  failed  to  
calculation        log     as  we  need  to  defined  
this  term  to  be      without  special  handling  of  
above  case   matlab  generates  a  nan  object  
and  it  eventually  propagates  out  to  convolute  

the  cost   
  
this  below  statement   combined  with  a  helper  
function   successfully  address  the  above  nan  
issue   
  
j     lambda  
 theta nointercept  theta nointercept   
      innerproduct y log hx    
innerproduct   y log   hx       m 
function value   innerproduct x y 
  calculate the vector inner product and
check for   log   
  x need to be a indicator vector ie x i 
     or   only
d    length x   d    length y  
if d     d 
fprintf  vector dimension does not
match  n   
error 
else
idx    x     
value   sum y idx   
end
end

  
note  that  function  innerproduct  makes  the  
assumption  that  input  vector  x  is  a  vector    
where  its  elements  can  only  be     or      we  
then  calculates  the  inner  product  of  x  and  y  by  
summing  over  y  indexed  by  x   implemented  
the  above    log     handling   the  classifier  
performance  dramatically  improves  to       
accuracy  evaluated  on  random  drawn  test  set  
 using  either  function  minimizers    
  
logistic regression classifier
  training examples
train error cross validation error
  
        
         
    
        
        
    
        
        
    
        
        
     
        
        

  
  
note  that  the  performance  is  again  
dramatically  improved  with  this  fix   
suggesting  that  now  the  lrcostfunction  
correctly  handles  the  case  where  y log hx         
   log     case   i e  the  prediction  is  identical  to  
the  class  label   
  
diagnostic   
  
from  figure      it  is  clear  that  as  number  of  
training  example  increases   the  lr  classifier  
successfully  converges  to  a  very  high  level  of  
accuracy   from  the  learning  curve   we  can  
conclude  that  this  model  suffers  from  either  
high bias  or  high variance  issue   
  

fisince  we  have  achieved  the  desired  level  of  
accuracy   it  justifies  that  it  is  not  needed  to  
future  investigate  those  twitter  related  
features  because  if  they  do  help  on  represent  
the  intrinsic  information  that  alters  the  stock  
trend   the  benefit  of  researching  on  it  is  
minimum    at  least  when  we  try  to  predict  the  
trend  of  dow  jones  index   
  
also   implementing using  a  svm  classifier  at  
this  stage  is  also  removed  from  the  to  do  list   
same  as  lr  with  normalization  because  the  
current  logistic  regression  classifier  works  
just  fine   

  

    summary     future  work  

  
a  quick n dirty  logistic  regression  classifier  is  
first  implemented  to  predict  the  stock  close  
price trend   of  the  next  day  and  a  multi round  
error  analysis  and  model  diagnostic  were  
performed  at  each  phase   finally   with  a  rich  
feature  set  and  a  correct  implemented  
optimization  algorithm  the  classifier  is  able  to  
achieve  the  desire  level  of  accuracy     
  
the  work  completed  in  this  project   in  my  
honest  opinion   has  successfully  
demonstrated  my  thorough  understanding  on  
designing  and  optimizing diagnosing  an  
sophisticated  supervised  machine  learning  
model  from  end  to  end   
  
my  current  plan  for  future  work  steers  toward  
to  learning  a  policy   possibly  using  the  
reinforcement learning  paradigm   to  
maximize  reward   capital  gain   on  investing  
solely  on  dow  jones  index  having  now  a  
model  that  predicts  the  trend  of  next  trading  
day   eventually  id  like  to  build  an  
autonomous  machine  learning  trader 

  

figure      

  
optimization  objective   j   v s   number  of  
iterations  
  
  
figure      

  
training  curve  of  lr  classifier  with  rich  
feature  set  after  fixing  nan  issue   figure      

  

  

fi