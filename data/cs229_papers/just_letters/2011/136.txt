forecasting video analytics
sami abu el haija  ooyala inc  haija stanford edu  sami ooyala com 

   introduction
ooyala inc provides video publishers an endto end functionality for transcoding  storing 
and delivering videos to a wide set of devices 
publishers can easily upload their video
content and make it available on websites 
smart phones  tablets  facebook applications 
and google tvs  in addition  publishers can
access a wealth of analytics data on how their
video content is being watched 
ooyala offers daily aggregated analytics  for
example  the number of plays a video received
on a given day  the aim of this research is to
analyze historical video analytics  model the
time series of video plays  and predict the
number of plays that a video is going to
receive over the next two days 
we begin this paper by describing the source
of our data  and our proposal for reducing the
inherit noise in the data set  sections   and    
afterwards  we describe the various models
we explored  and reason our choices  section
    then  we compare the accuracy achieved
by the models  section     lastly  we suggest
future work based on our insights and
conclude our findings  sections   and    

figure    video a  real plays    

to reduce noise  we apply a gaussian filter 
    

          
 

where    is the observed  real  number of
plays at time s     is the smoothed number of
plays  and 
     

 

exp   

  
 
  

   
for computational efficiency  we set w x  to
approximate a discrete gaussian distribution
with mean    that hard drops to   for  x      
and sums up to    in particular 

w                w       w             
w       w                w     elsewhere  
our models performed significantly better
when trained with smoothed data

   data sets
the goal of this research is to predict analytics
at a daily granularity  analytics granularity
currently offered to our publishers   however 
we construct our models on data more granular
than a day  our intuition tells us that sub day
patterns will allow us to better predict
analytics for day granularities  therefore we
ran mapreduce jobs to accumulate data at  hour granularity  hence  one day consists of
  buckets 

   gaussian smoothing
with   hour buckets  the number of plays for a
video across time is noisy 

figure    video a  smoothed plays  p 

   models
it is observed from the dataset that different
videos whiteness different viewership patterns 
for example  videos published by news
publishers receive a spike of interest in their
early lifetime  but the number of plays quickly
drops over time  on the other hand  longcontent videos observe more of a uniform
pattern of plays over time  in addition  it is
observed that most videos whiteness time
cycles  some videos are more popular at night 
others are more popular during the day 

fiin the remainder of this section  we describe
the different models we explored  we split the
models into two categories  per video models
and per publisher models 

    per video models
to overcome trend differences between
videos  these models look at each video in
isolation  fitting the model parameters per
video  where we 
   leave out the last two days of the data set
   train the model on the rest of the data
   compute the error between the predicted
and the actual number of plays for the last
two days 
      linear model
here  we model pt as a linear combination of
the historical plays  pt    pt     

      

     

     

     

   

figure    time series

                                           
where t  is a configurable constant  and     s
are the model parameters  written compactly 
  

   

    

finally  the above formula relates pt  with the
set of directly preceding ps       through
         in a predictive setting  the previous ps
will be given  and if we have       we can
estimate pt  however  in addition to estimating
pt  we would like to estimate pt    pt     
pt     such that we can estimate the number of
plays a video will receive over the next  
days  therefore  we construct    models  one
model to predict each future point  in
particular  we estimate     s satisfying 
  
 

      

                       
   

      autoregressive moving average model
here  we explore the classic autoregressive
moving average  arma  time series model
 box  jenkins  reinsel        on our dataset 
the arma p  q  has two components  the
autoregressive  ar  part  parameterized by p 
is similar to the linear model described in the
previous subsection  which models future
values in terms of previous values  the
moving average  ma  part  parameterized by
q  models latent noise in the time series  we
define arma p  q  for our data set as 
 

    

          
   

     

   

to fit the model parameter     for a video  we
generate training examples by traversing
over the historical timeline of the video  as we
want to relate plays       to t  historical values 
we start this traversal at t   t   and for each
increment of t  we extract one training
example with features      through       and
value      we leave out the last two days of the
dataset for prediction 
then  we compute     from the learning
examples using closed form linear regression 

 

       
   

where  s are white noise  here  we used the
arima   function in r to estimate the
parameters of the model     
note  we dropped the intercept term from the
arma model by setting include mean false 
its presence degraded our prediction quality 

    per publisher models
in these models  we train on each publisher in
isolation  our motivation is to capture general
trends on a publishers videos  if we can
accurately model the general trends of a
publisher  we could predict plays on recently
uploaded videos using the publishers general
trends 

fi      k means clustering
our driving intuition behind this model is to
find averages of shapes of curves that welldescribe a video publisher 
we would like to encode a segment of a timeseries as a vector  such that two vectors have a
low euclidean distance if the shapes of their
respective segments were similar  moreover 
we want the vector representation to be scaleinvariant  some videos are inherently more
popular than others  nonetheless  two news
videos  for instance  are likely to take a similar
path  e g  the number of plays during daytime
is roughly double than during nighttime  

   

   

produce thousands of shape vectors  then  we
run the k means clustering algorithm on the
vectors to find k vectors  centroids of clusters 
that well describe the publisher  below are
two cluster means computed for two
publishers  here  we construct a time series
from shape vectors by reversing the
computation  setting p       

figure    centroid for a news publisher  many of its
other centroids looked similar with varying the peak
positions and varying width of curve 

   
  

   

        
        

   

                
                 

figure    depicting vectorization

in the vectorization process  depicted above 
we compute the vector for the segment  p   p  
   where each component i of the vector is
the ratio of       pi          p    where p  is
the number of plays preceding the segment 
the choice of     the normalization constant 
is to remove noise coming from dead videos 
for instance  if a video received   play in a  hour window  followed by   plays in the next
window  followed by   play  could greatly
skew the general publishers curve shapes if
we indicated a      increase followed by a
     decrease  we feel that    would absorbaway vibrations when the number of plays is
less than     and the constant will have little
effect on the ratio of  pi   p   when the number
of plays is averaging well over one hundred 
in this model  we fixed the width of the
segment to be six days 
to extracting learning examples  given a
publisher  we traverse its videos timeseries to

figure    centroid for a long content publisher  which
shows more or less a consistent  cyclical pattern 

in a predictive setting  we have computed the k
clusters for a publisher  and we are given a
video curve with two segments  a known 
filled segment  and an unfilled  to be predicted
segment  we fix the width of the filled
segment   days  and the unfilled segment to  
days  such that the total width of the curve is  
days  matching the clusters widths  next  we
vectorize the known segment  just like before  
then  we choose the cluster that minimizes the
euclidean distance between the first   days of
the cluster vector and the known vector 
finally  we fill the unfilled segments according to the cluster vector 
this model can be summarized by the pseudocode  applied separately to each publisher  
train timeseries training set  
   vectors    vectorize timeseries training set  
   clusters    k means vectors  k  
 
predictplays video  
   s    vectorize timeseries video  last   days   
      
   cluster    argmin 
       
   ts  days    forecastusingshapevector s  cluster  

fi      principal component analysis
the intuition behind applying principal
component analysis  pca  comes from
visual inspection that features of a shape
vector are highly correlated  for example  if
the number of plays is increasing between     
am and       am  then after going through a
daily cycle  the number of plays likely to be
also increasing the next day  between      am
and       am 
we used pca to reduce the dimension of the
shape vector from       days  to     then  we
ran k means on the reduced dimensions to find
k averages  finally  mapped the k clusters back
to    features for the purpose of visualization
and inspection  and they looked like 

let     denote the predicted plays at time t  in
addition  for notational convenience  let   
denote the number of plays over the first
unseen day  and and    denote the number of
plays over the first and the second unseen
days 
    

               
       

  
   
        

similarly  let    and    be the
summations over predicted plays       

same

we define the prediction errors as 
      

 
error   day          


        

      

 
error   days          


        

moreover  we will use a consistent legend in
plotting time series  we will plot the portion
seen by our algorithms in blue  the predicted
line in orange  and the true  actual  line in red 

    linear model
we set tjs     days  yielding predictions 

figures        centroids after applying pca and
mapping back to original dimensions 

it is apparent from the centroids figures above
that our application of pca poorly modeled
our data  most centroids looked very similar to
the ones above  noisy  with lots of negative
numbers   after this visual inspection  we
stopped our exploration of pca to model our
data set 

figures         two typical prediction curves 

   results
to measure the accuracy of the models  we 
   quantitatively measure the prediction
error over two  unseen  days that follow
the training set
   qualitatively  visually inspect the shape
of the predicted curve  and see the extent
it follows the true  actual  curve 

figure     rare  but visible  case where the model
predicts negative numbers for a time period 

over a set of randomly chosen videos  the
linear model produced       and      
errors on one and two day predictions 
respectively 

fi    arma model
the arma model had the best performance
on our data set  some prediction graphs 

figure     prediction curves for arma       

varying p and q  average errors were 
p
  
  
  
  
  
  

q
 
 
 
 
 
 

error   day 
      
      
      
      
      
      

error    days 
      
      
      
      
      
      

table    error averaging over a number of randomly
chosen videos

it is worth noting that some videos achieve
better predictions with smaller p and q  while
others achieve better predictions with larger p
and q  in practice  there are algorithms that can
be used to identify a good choice of p and q
 such as  box jenkins method      

    k means clustering
for k      the prediction curves look like 

although in most cases  the predicted curve
and the true curve have similar shapes  the
accuracy of this model is much worse than the
per video models  we ran k means twice  for
k    ad k      then  for a randomly selected
set of curve shapes  we computed the average
error when predicting for   day and   days 
k
  
   

error   day 
      
      

error    days 
      
      

   future work
 invest more in per publisher models  try
mixture of gaussians model instead of kmeans clustering 
 model time of day and day of week
 look at different sources of data  such as
the countries that plays are coming from 
or the viewer ids that are watching the
modeled videos 

   conclusion
our tests have shown that our per video
models perform better than our per publisher
predictions  however  we strongly feel that it
is possible to extract relationships between
videos within the same publisher  nonetheless 
the described k means and pca approaches
were unable to capture such relationships 

   references
    george box  gwilym m  jenkins  and
gregory c  reinsel         time series
analysis  forecasting and control  third
edition  prentice hall
    james d  hamilton         time series
analysis  princeton university press
    ripley  b  d         time series
in r        r news           http   www rproject org doc rnews rnews        pdf

figures        and     the first two are common cases 
predicted and actual curves have similar directions  the
last is a rare case  where the known  blue  portion
matches the cluster but cluster spikes on unseen section

fi