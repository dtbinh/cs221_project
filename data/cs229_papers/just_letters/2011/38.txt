building fast proxies for optimization
of thermal reservoir simulation
matthieu rousset
december         

abstract

or flow that is imposed at the producing and injecting wells  an other important input feature is the
placement of the wells  in this project  we focus on
thermal applications  which require the injection of
heat to achieve oil production  specifically  we consider the steam assisted gravity drainage  sagd 
process  which involves injecting steam into the
subsurface in order to lower the oil viscosity  and
allow production by gravity  simulation of oil production from sagd is a very difficult task  because
it involves reproducing complex physical phenomena and strong nonlinearities  e g  large variation
of fluid properties with temperature   as a result 
the simulation of these processes is very time cpu
intensive  another aspect of this process  is its
higher economical cost compared to traditional oil
production  this makes it an ideal candidate for
optimization  which aims at maximizing the predicted profits   e g  net present value  npv  by finding the optimal design parameters and or
sequence of input controls  unfortunately  optimization algorithms require running many simulations  which is often impossible with cpu intensive thermal simulations  in this work  we investigate on the creation of fast proxies for thermal reservoir simulations  using an artificial neural network  ann  and a support vector regression
 svr  model  the proxies are then used to perform optimization  which is of high practical interest 
to the authors knowledge  there have been only
a few attempts at the creation of fast proxies for
thermal applications  queipo et al      have reported the use of neural networks to build proxies of the sagd process  and have used them to
perform global optimization of net present value 
although they seem to achieve good optimization
results  it is unclear how accurate the proxy models are  vanegas et al      have developed a semianalytical proxy model for sagd  based on the
widely used analytical model from butler  it allows the use of heterogeneous fields of rock permeability but it is still based on a simplified physical
description 

with decreasing amounts of conventional
energy resources available  production of
unconventional oil and gas has become an
important new source of energy  steamassisted gravity drainage  sagd  is a
widely used process  allowing the production of very heavy oils  the numerical
simulation of sagd  however  is complex
and highly cpu time intensive  moreover 
the economical cost of this process is high
and production design optimization could
be very beneficial  the high cpu time requirements  however  render optimization
impractical  in this paper  we aim at the
creation of fast proxies for sagd reservoir
simulation  with the objective of maximizing
net present value  npv  with respect to selected design parameters  for two practical
examples  we use the commercial reservoir
simulator stars to run a number of training simulations  these results are then processed for the creation of an artificial neural
network  ann  and a suport vector regression  svr  model  sensitivity analysis is
then performed on some of the proxies parameters in order to assess the quality of
the models  the key question of how many
training runs are required in order to obtain robust proxies for optimization  is also
answered with sensitivity analysis  finally 
optimization is performed using the ann
proxy model 

 

introduction

when producing oil from the subsurface  engineers
often build a detailed geological model of the oil
reservoir  this numerical representation of the underlying rocks and fluids is then used to predict the
flow behavior  under a given set of controls  the
controls usually represent the amount of pressure
 

fibuilding fast proxies for optimization of thermal reservoir simulation

 
   

machine learning methods

   the activation function that converts a neurons weighted input to its output activation 

artificial neural network  ann 

   the learning process for updating the weights
of the interconnections

ann is a combination of artificial neurons that
tries to mimic the structure of biological neural
networks observed in nervous systems  it is composed of input nodes  output nodes and a number
of hidden layers  containing artificial neurons  the
first layer is called the input layer  containing all
the input nodes  and the last layer is called the
output layer containing all the output nodes  the
number of hidden layers and the artificial neurons
in each of these layers is a design parameter that
can be varied  fig    shows an example of ann
with only one hidden layer of   artificial neurons 

the first two points are defined by the weight
matrix and the bias vector  the last point is
achieve by minimizing some kind of error  in this
work  we use matlab neural network toolbox to
build an ann with one hidden layer of sigmoid
neurons  error minimization is achieved by using the levenberg marquart backpropagation algorithm  starting with a set of randomly generated initial guess for the weights and biases  the
algorithm finds the parameters that minimize the
error between the desired and the actual output 
note that the number of input nodes is equal to the
number of input features and there is only one output node since we are using only one target variable
 the npv  

   

we let uji denote the input of the j th node of
the ith layer and vij denote its output  the scalar
ni represents the number of nodes in layer i and
k the total number of layers  for the input layer 
we have uj    v j and for the output layer  we have
ujk   vkj   for each of the hidden layers  we have
ni
x

wij  ui   bji

   

  

regression

m

where wi is the weight matrix and bi is the bias
vector between the ith and the i    th layers 
the neurons themselves represent activation
functions which can be any differentiable function  in our implementation  we choose the sigmoid function  defined as follows 
 
    ex

vector

first proposed by v  vapnik et al       svr is a
regression model that aims at predicting the output of a nonlinear function for some given values
of the input features  the general idea of svr is
to perform linear regression in a high dimensional
feature space where the input data are mapped via
a nonlinear function  similarly to support vector
machine  which is used for classification  svr only
consider a small number of training points to perform the prediction because it ignores any training data that is close  within a threshold   to the
model prediction  we include here a bried description of svr and more details can be found in     
in svr  the goal is to find a function f  x  that
has at most  deviation from the actual targets y  i 
for all the training data  and at the same time is
as flat as possible  in the case where f  x  is a
linear function of the form f  x       x    b  the
resulting primal optimization problem is given by 

figure    example of an artificial neural network
 source  wikipedia 

j
vi  
 

support
 svr 

minimize

subject to

x
 
 
 i   i  
kk   c
 
i  

 i 

y

 
 
x i    b     i

    x i    y  i    b     i


i   i   

   
here       represents the inner product of two vec 
tors  the norm kk measures the flatness of the
the neural network is fully defined by 
proxy  and the constraints force the model to ap   the interconnection pattern between different proximate all training points within  precision 
i and i are slacks variables introduced to allow
layers of neurons
g x   

   

 

fibuilding fast proxies for optimization of thermal reservoir simulation

    possible well placements  the net present
value  npv  is then computed for each configuration from the formula 

some trade off between the flatness of the function and the compliance with the  deviation constraints  c represents a weight given to the penalty
for violating the constraints 
the corresponding dual problem is given by 

max



m
x

 i  

 p
m

 i 
   i   i  c
i  

i  



m
x

i  

y  i   i  i  

   
the optimization problem in eq    is convex and
can be solved using a readily available optimization
algorithm  it is also clear from eq    that svr
can be kernelized by replacing terms of the form
  x  z   with a kernel function k x  z   in this
work  we use libsvm    for our implementation of
svr with a gaussian kernel function 

 
   

   

where po   pg   pw and ps represent the price of
oil  price of gas  cost of producing water and the
cost of injecting steam respectively  they are set
to      bbl      mmscf       bbl and     bbl 
oil and gas production are denoted with qo and
qg   water production with qw   and steam injection with qs   among the     sample points    
are picked randomly and labeled as testing samples  the remaining     sample points are used
for training the models  we first create a number of artificial neural network proxies using subsets of the training samples set of increasing size 
we use one hidden layers containing    artificial
neurons  for each model  we compute the error
between the ann response and the true response
using the testing set  results are displayed in fig 
   and show that the error decreases sharply with
increasing size of the training set until     sample points  then  it remains close to about     
for training sets containing more sample points 
we also checked the sensitivity of an ann model

i  

i  

s t 

npv   qo po   qg pg  qw pw  qs ps  

m

  x
 i  i   j  j   x i    x j   

  i j  

numerical examples
well placement optimization

an oil reservoir with a net thickness of    meters 
a width of about     meters and a length of    
meters is represented with a  d model containing
       grid blocks  each grid block is rectangular
with dimensions     m by   m and the model represents half of the entire reservoir  a pair of sagd
horizontal wells is placed on the symmetry plane 
our goal is to find the best position for the producer and injector wells on this symmetry plane 
the two design variables are the distances of the
injector and producer wells from the bottom of the
reservoir 

figure    ann testing error sensitivity to number
of training samples
trained with a set of     training samples  to the
number of hiden layers and number of artificial
neurons  for this example  we found that ann
was not very sensitive to these parameters 
we follow the same procedure to build a number
figure    schematic illustration of reservoir model of svr proxies with increasing number of training
  with the wells represented in black
samples  results are plotted in fig     svr provides a relatively lower testing error with a smaller
we use a matab script to run all possible combi  number of training samples than ann  the error
nations of input parameters  using the commercial does not decrease as sharply however  with an insimulator stars  since the injector well must al  creasing number of training points  but it reaches
ways lie above the producer well  this represents lower values 
 

fibuilding fast proxies for optimization of thermal reservoir simulation

figure    svr testing error sensitivity to number figure    svr testing error sensitivity to the value
of training points
of penalty function weight

we also plot in fig    and fig     the sensitivity
of an svr models trained with     training points
to the value of  and c respectively  as defined in
eq     it is clear from fig    that epsilon must be
chosen to be as small as possible  indeed  a smaller
value of epsilon leads to a smaller testing error  on
the other hand we see from fig     that the penalty
function weight c should be chosen greater than  
in order to insure minimal testing error 
figure    optimization using generalized pattern
search with an ann proxy

   

production control

for the same sagd model as presented in the previous section  we now seek to create a proxy model
allowing the optimization of the well controls  for
a given position of the wells  we set the pressure
for both the producer and injector wells at   intervals during the entire production time  so our
goal is now to create proxy models that take these
   parameters as input  and return the net present
value as an output  we run     simulations using the commercial simulator stars and label   
randomly chosen cases as testing samples 
we create a number of artificial neural networks
with an increasing number of training samples and
plot the testing error for each of these models in
fig     we see that ann can yield a very low
testing error for this problem with less than    
training samples  we also performed sensitivity
analysis for the number of layers and the number
of artificial neurons but we didnt find these parameters to be significant to the testing error 
we also build svr models with increasing numbers of training samples  results are displayed
in fig     they show that the testing error de 

figure    svr testing error sensitivity to the value
of 

finally we illustrate the use of such proxy models
for well placement optimization  we apply a generalized pattern search algorithm where the objective function is the npv  computed from an ann
proxy model trained with     training samples 
we show the evolution of the objective function
with the number of iterations during the optimization in fig     the optimal placement found from
the optimization is to have the producer well in
grid block    and the injector well in block    
this matches exactly the best positions found by
exhaustive search 
 

fibuilding fast proxies for optimization of thermal reservoir simulation

figure    ann testing error sensitivity to number figure     svr testing error sensitivity to number
of training samples
of training samples
     we have the used the ann proxy to perform optimization using the generalized pattern
search algorithm  this requires in the order of
    to     simulation runs  in order to obtain
a significant runtime speedup using these proxies 
the training runs would need to be run in parallel  in concusions  ann and svr are promising
black box approaches to create fast proxies for
optimization when massive parallel computing capability is available 

creases very sharply and then becomes stable for
more than    training samples  this surprisingly
sharp decrease of the testing error is probably due
to a low sensitivity of the target value  npv  to
many combinations of the input parameters 

references
    nestor v  queipo  javier v  goicochea p   and
salvador pintos  surrogate modeling based
optimization of sagd processes  spe       
    j w  vanegas  p  clayton  v  deutsch  and
l  b  cunha  uncertainty assessment of sagd
performances using a proxy model based on
butlers theory  spe       

figure    svr testing error sensitivity to number
of training samples

finally  we illustrate the use of these proxy mod    a  j  smolatand bernhard scholkof  a tutorial
els by using the generalized pattern search algoon support vector regression       
rithm to perform the optimization of npv  we plot
in fig     the evolution of the objective value  we
verified that the optimized npv is higher than any
npv found in the     simulations performed previously 

 

conclusions

we used both ann and svr to create fast proxy
models for complex thermal reservoir simulations 
we verified that ann and svr can provide robust proxies for the optimization of production
controls and well placements  for the two simple cases considered in this work  we found that
the number of training simulations required to obtain a small testing error is in the order of     to
 

fi