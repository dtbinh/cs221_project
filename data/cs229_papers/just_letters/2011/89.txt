dataset to train and test the predictor  all the
attributes were discretized and are listed below with their corresponding number of classes    

occupation classifier

   occupation    levels 
   annual income of household    levels 
   sex    levels 
   marital status    levels 
   age    levels 
   education    levels 
   how long have you lived in the san francisco  oakland san jose area     levels 
   dual incomes  if married    levels 
   persons in your household    levels 
    persons in household under       levels 
    householder status    levels 
    type of home    levels 
    ethnic classification    leveles 
    what language is spoken most often in your
home     levels 

sebastian jimenez bonnet           
december         

 

introduction

in this project i explored how accurately could
the occupation of a person be guesses based on
other demographic variables  to this end  several algorithms were attempted  such as multinomial logistic regression  svm and classification and regression trees  cart   i present
here the results from the most successful algorithm  cart  and discuss some of the reasons
that made it perform better 

many of the variables have many levels  including the response variable occupation which
has   levels  this  summed to the fact that al  the data
most           observations  of the data set
the data set used is an extract form a bigger have at least one missing value  makes this data
marketing database    originally intended to set hard to work with  the treatment of these
classify people by income  the original data missing values played a central role the project 
set consists of      questionnaires filled up by
the dataset was randomly split into a training
shopping mall customers in the san francisco
bay area  the questionnaires contained     set with      entries       and a learning set
questions  and the extract used consists on with      entries        the former was used
   demographic attributes  this dataset is a to fit our model and the latter was kept separate
mixed set in which we can find many cate  until the very end where it was used to estimate
gorical and numerical variables with a lot of the prediction error of our final model 
missing values  after those entries for which the
response variable was missing were removed 
a total of      observations served as our
 

source 
       

 

impact resources  inc   columbus  oh

for a complete listing of the levels visit http   wwwstat stanford edu  tibs elemstatlearn 

 

fi   

   

   

   

cv estimate
training set

   

proportion of root node error

   

classification relative error

 

  

   

   

   

   

number of splits

figure    a plot of the cv error and the training set error for each of the fitted trees  as a fraction of
the error achieved by the single node tree  on the x axis we have the number of nodes of the corresponding
trees

 

classification and regression
trees

ing observations  which is really important for
our dataset 

the cart algorithm  roughly speaking  generates a sequence of binary splits on one of the input variables at a time  so that the resulting measure of impurity is minimized each time  the
tree is grown until a stopping criteria is satisfied
and finally it is pruned to avoid overfitting  in
order to make a prediction  it assigns the class
with the most votes to each of the final nodes 
there are several reasons why trees are specially appropriate for our particular problem 
first  trees work well with discrete input and
response variables that have many levels  second  they are fairly easy to interpreted which
may lead to a deeper understanding of the relation of the input with the response variable 
lastly  trees offer several ways to deal with miss 

   

missing values

binary trees are specially well suited to deal with
missing values  there are several different approaches to this matter  the simplest one is to
forget about them and build a model based only
on observations that have all their attributes
observed  this approach wastes lots of information   we would dismiss almost     of our
dataset   and is not able to offer a prediction for
entries that have missing values themselves 
another approach is to add a dummy level to
each of our attributes  corresponding to a missing value  this approach is useful in that it can
help uncover a relation between the missing values and the response  but if the missing values
 

fiage          

 

age      

income                

age  

education    

householder    
student

dual income    

retired

professional

income        

sex  

sex  
professional retired

student

professional

dual income    

sex  
professional

age  

education    

dual income  

sales

time bay area      

professionalhomemaker

education    

education  

education  
homemaker

clerical

militar

income          
professionalfactory worker

factory worker

professional clerical

professional student

figure    our final binary tree 

considered to stop branching 

appear randomly or are the product of data processing rather that data collection  this approach
is also inefficient 
both of these approaches can be implemented
by many algorithms  but trees offer an additional
one  surrogate splits  it consists in choosing several substitute splits for each of the original splits
elected for the model  in choosing these surrogate splits  only variables different from the original one are considered  the split is then chosen
so that the resulting partition of the data has
the biggest overlap with the partition the original split achieved  where only data that has both
variables observed is considered   the second
surrogate excludes the original variables and the
variable for the first surrogate  and so on 

   the nodes considered for splitting were
required to have at least   observations 
   the end nodes were required to have at least
two observations 
   the split to be made had to decrease our
measure of impurity by at least cp         

these stopping rules aim to avoid huge trees
that are computationally expensive and which
would be trivially pruned at a later stage 
to fit the model i used r as the software platform  specifically i used the package rpart  the
algorithm calculates a whole path of nested trees 
from the trivial root node  to the full tree reached
with our stopping criteria  it indexes each tree
by the value of the parameter cp that would have
    fitting the predictor
taken to stop branching exactly at that tree 
in this implementation of the cart algorithm 
to choose the appropriate tree and avoid
the gini index is used as the measure of overfitting i calculated a    fold cross validation
impurity to grow the tree  three criteria were  cv  error for each of the trees  in figure   we
 

fi   

prediction accuracy
     

     

   

   

   

     

     
     
   

     
     

     
   

     
retired

student

professional

homemaker

clerical

factory

militar

sales

unemployed

figure    accuracy for class of the response variable occupation 

   

can see a plot of the training error and the cv error for each of these trees as a percentage of the
trivial root node classification  the root node
trivial error is        and is achieved by the classifier which assigns the most frequent class  in
our case professional managerial  to any input 
i used the two standard deviation parsimonious
approach in which the selected model is the simplest model  least number of final nodes  which
achieves a cv error within to standard deviations of the minimum cv error 

   

prediction

testing our final model on the test set  it has
a resulting test error of      at first sight it
may seem a bit high  but considering we have
  classes for the response variable and a sparse
training set  it is actually a quite remarkable performance  for a frame of reference  it can be
compared to the root node error  which is     
in figure   we can see the classification accuracy for each class  we can see that  excluding
the retired  which is easily identifiable by age  
there is a clear bias in favor of the more frequent
classes which are professional and student  in
fact the least frequent class  unemployed is never
assigned 

final model

 

our final model corresponds to a cp value of
         which translates into a binary tree with
   nodes  in figure   we can see the structure
of the model  note that the model only uses  
of the    possible variables  the variables that
seem to be the most important in our tree are
age  annual income and education 

conclusions

our tree predictor had a good performance in
classifying our test set  one key feature of our
model was its treatment of the missing values 
for comparison  i fitted another cart classifier
omitting the missing values and got a test error
 

fiof      which suggests that there is a consider   
able loss of information doing this 
there are some questions that still need to  i 
be answered in order to asses our results  how
general is our model  can we extend it to the
general population of california or the united
 ii 
states  one obstacle i can see  which figure  
supports  is that our predictions are heavily affected by the frequency of each class  probably   iii 
in order to have more generally applicable results  we could weight our observations in order
to balance the frequency of each class with the
frequency of the actual population we want to
predict 
regarding the importance of the variables  it
is important to keep in mind that binary trees
have a really high variance  little variation on
the data can mean many different combinations
on the selected variables for each split  in order
to better asses the importance of the variables 
at least when there is a lack of context to aid us 
random forests has a more robust importance
measure for the predictor variables  and may be
a good continuation of this work 

 

references
hastie t   r  tibshirani and j  friedman j
the elements of statistical learning stanford  california       
bishop  christofer m  pattern recognition
and machine learning cambridge       
michel  tom m  machine learning      

fi