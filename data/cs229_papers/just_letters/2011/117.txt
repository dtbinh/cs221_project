 

pulse news preference prediction
jing ma
chi zhang
cs    machine learning course project  stanford university
abstractthis paper is a summary of all the work
our group has done for the final project this quarter 
the topic is using algorithms taught in class to
develop a way for news prediction with a relatively
high accuracy  the dataset is provided by pulse
application  containing      users read and click
record  to make the result better  we tried a lot of
algorithms  such as simple data filtering  logistic
regression  feature deduction  and so on 

i 

introduction

language processing  nlp  is an
natural
important application area of machine learning
and already widely used today  a large portion of
text that people consume every day is news  the
news we read play an important role in how we form
opinions and beliefs about the world and its actors 
understanding peoples news preferences is
therefore an important task  if we can predict the
news stories that users are most interested in we
could first improve their reading experience  it could
further alter how well informed we are about many
issues going on in the world  in this project  we
investigate the algorithms to predict users news
preferences based on their previous reading
activities  we work on a large real world dataset of
news consumption which comes from a local startup
that allows people to read news on their phones 
the goal is to predict the stories people are most
likely to read  more specifically  we need to classify
the pieces of news to two categories  one that the user
is most likely to read  and one that the user most
probably would not read  this problem falls into the
area of document classification  some commonly
used algorithms for document classification includes

expectation maximization  nave bayes classifier 
term frequency  inverse document frequency  tf idf 
vector  latent semantic indexing  support vector
machines  artificial neural network and k nearest
neighbor algorithm  in our design  we mainly used
tf idf vector and logistic regression to classify the
news and predict users reading preference 
the remaining content of the report is organized as
following  first  we will briefly introduce the data set
and preprocessing we have done  in methodology
section  we will discuss about our feature selection
and logistic regression technique  a brief description
of the key concept tf idf vector is also included 
after that  we will discuss on our simulation and
results  then  we will conclude the report with a
summary and outlook 

ii 

dataset

we work on a large real world dataset of news
consumption in a certain time period which comes
from a local startup that allows people to read news
on their phones  there are three data files for
reference  stories log lists all the stories available for
people to read  the information for each story
includes the story url  story title  feed url  feed
title and time stamp 
user story reads log contains all stories read for a
sample of      pulse user during the time period  a
read event is defined as clicking a story title in pulse
and viewing the rss feed content in text mode  for
each story  there is information on user id  story
url  story title  feed url  feed title and timestamp 
user story clickthroughs log comprises similar
information as the user story reads log  a click
through event is defined as clicking a link to view a

fi 

story in web mode  the dataset covers reading
history of about one month 
we have done a lot of data processing work to ease
the simulation work  first  we removed duplicated
stories from the three files  besides  we divided the
user story reads log into smaller files  the original
file is over  gb  which will occupy a lot of memory
space during simulation  we divided the file into   
smaller files  each contains the reading history of   
users  moreover  we sorted the dataset  for
stories log  we rearranged the data such that they are
ordered by date  the user story reads log and
user story clickthroughs log are sorted by user first
and they by date  after that  we extracted the features
such as story title  feed title and url element from
each story 

iii 

methodology

in this section  we mainly describe the features we
have selected and the algorithm we are using for the
news classification and prediction
a  feature selection
we consider each piece of news in three major
components  story title tf idf vector  story url ip 
and feed title  the feed url are mostly pulse feeds 
which is relatively common  so we excluded it in
feature consideration 
the story tf idf vector is the primary feature we are
considering  first  we construct a key word set based
on the story titles a user has read and clicked  say we
have a list of document titles  the item in the tf idf
vector corresponds to the tf idf weight of the key
word at the same index  a keywords term frequency
is the number of times the word appears in the title 
its document frequency is the number of titles the
word appears over the total number of titles  the
keywords tf idf weight is its term frequency divided
by its document frequency  then  the tf idf vector is
normalized  the tf idf vector represents the
occurrence of the important keywords a user is

interested in  in this model  each document is first
represented as a term frequency vector in the
term space 

where

is the frequency of the ith term in

document   n is the total number of the selected
vocabulary  and d is the total number of documents
in the collection 
next  we weight each term based on its inverse
document frequency  idf  and obtain a tf  idf vector
for each document 

story url ip is another important feature to
consider  by observing the story url of specific
users reads and clicks  we found that the websites a
user is interested in are a limited number of webs and
they do not vary a lot within a certain period of time 
this is also true in real time experience  for
example  user a may usually check cnn com and
newyorktimes com for news while user b usually
prefers msn com and googlenews as his or her news
source  by story url ip we mean the main website
ip  for example  the story url ip is
www youtube com for the story with story url of
http   www  youtube com watch v  ee vqj d cw 
based on the users previous reads and clicks  we
established a url ip pool that the user is generally
interested  for each available story  if its url ip is
in the users url ip set  then this feature is   
otherwise  this feature is marked as   
besides  we also consider the feed titles  the feed
title can be regarded as a general category label of the
detailed stories  the feed titles of all the stories the
user reads can be consolidated as certain areas that
the user is interested in  each user usually has a
limited number of areas that they are interested in
and would like to read the news on them  thus 
similar to the story url ip  we constructed a feed
title set including the feed titles that the user has
previously read from  in prediction  if an available

fi 

storys feed title is in the users feed title pool  then
this feature is labeled as    otherwise  it is marked as
  
b  news prediction algorithm
there is      sample users information  for each
user  there is a two step classifier and evaluate it and
update it over the time series   say there are n total
number of days we have for the user 
train on day   and test on day         t
train on day      and test on day      t
train on day         and test on day    t
the news classification and prediction algorithm
comprises two steps  feed title filtering and logistic
regression based on story title tf idf vectors and story
url ip 
first  a story will pass through a feed title filter  for
each user  a feed title set is constructed and updated
during training  the feed title of the stories that the
user has read will be added to the set everyday during
training  in classification  if a storys feed title can be
found in the feed title set  this story will be passed to
the next step  otherwise  this story will be classified
as negative  meaning that the user would probably
not read it  this step is to see whether a story falls
into a users interested categories of reading 
next  if a story is within a users interested area  it
will be classified based on the story title and story
url ip using logistic regression  for each story  we
obtain a story url ip value   or   as described in
previous session  this value together with the story
titles tf idf vector forms the features for logistic
regression  the logistic regression classifier is
updated everyday based on all reading activities
before that day  this step further checks whether the
story is from the users preferred web list for news
and whether the user is interested in the keywords of
the story titles 
c  feature deduction
we noticed that the dimension of feature vector is

growing as we process more days data  so we
believe it is a good idea to add feature deduction into
this algorithm  however  due to the size of the tf idf
vector  the pca didnt work for the whole dataset on
our computers 
iv 

simulation and discussio

we define positive set as the set of stories the user
read and negative set as the set of stories the user did
not read  the flow of our simulation is as following 
on day i  we predict the users reading preference
using the classifier obtained from previous day and
evaluate this classifier in terms of positive set error
rate and negative set error rate  next  we update the
training set  we add the stories the user read on day i
to positive training set and add a portion of the
stories the user did not read to negative training set
such as the positive and negative training set have the
same size  then we update the feed title set and train
the new logistic regression classifier  after the
training  we update the negative training set  we use
the classifier to test the current negative training set 
if a story is predicted correctly as negative  remove
this story from the negative training set  this step
allows new negative stories be added to the negative
training set 
one key point of our implementation is that the
positive and negative training set are balanced to
train the logistic regression classifier fairer  also  we
used gradient ascent for the logistic regression
training  we attempted to use newtons method at
the beginning but found that a lot of matrices are
singular and not able to get an inverse  the
simulation is implemented in python  we used
sklearn and numpy modules in the implementation
of tf idf vector 
we obtained the learning curve as shown in figure
  and    for positive test set  the average error rate is
very high in the first few days  this is because the
training size is very small for positive training set  as
the training size increases day by day  the error rate
drops very quickly in first seven days 

fi 
classification for positive test set

our simulation focused on the news prediction of
user story reads  we also ran the algorithm over
user story clicks  however  the positive set average
error rate is relatively high  around      this is
because the dataset for each user is very small
compared to the huge data set of user story reads 
on average  a user only reads a few stories by click
through everyday  and less days are recorded  we
also tried to correlate the clickthroughs into
prediction of reads  but it did not help improve the
prediction accuracy 

   
   

average error rate

   
   
   
   
   
   
 

 

 

  

  
  
learning stage  day 

  

  

  

figure    two step classifier learning curve
for positive test set

v 

classification for negative test set
   

average error rate

   

   

   

   

 

 

 

  

  
  
learning stage

  

  

figure    two step classifier learning curve
for negative test set

  

conclusion and outlook

the tf idf vector provides a very good indication of
keyword weight  and this model is very useful to
classifier the users reading preference in term of
keywords  the two step classification algorithm is
relatively effective  the first step feed title filtering
greatly reduces the complexity of the classification
problem  it does not affect the positive examples
much  but it filters out the majority of the vast
stories available everyday  the second step of
logistic regression focus on the story title keyword
tf idf vector and also takes story url into
consideration  the algorithm is able to predict the
users reading preference with more than    
accuracy with the increase of learning stages and
growing of training examples  one drawback of the
algorithm is the long running time  the primary
work of future work of the project is to reduce the
vocabulary set and keyword features to improve the
running time without scarification of much
accuracy  moreover  we shall give more thoughts
on the user story clickthoughs data file to make
use of this relatively small dataset  we can also
collect more data on the clickthroughs to improve
the prediction accuracy 

after about ten days  the training size is relatively
adequate and the average error rate for positive set
fluctuates within      in contract  the average error
rate for the negative set is much smaller  it is around
   in the first    days and drops to within    in
later days  note that in the starting   days  the error
rate is very small  this is because the logistic
regression classifier has not been properly trained yet 
it tends to predict a story as negative at the beginning
references
due to the very small size of positive training
example  also  the feed title filter plays a very        manning  christopher d   foundations of statistical
natural language processing      
important role in the classification of negative
       christian s  perone  machine learning  text feature
examples  it rules out around     of the negative
extraction  tf idf  
stories and speed up the algorithm a lot 
http   pyevolve sourceforge net wordpress  p     

fi