david vetrano

cs   
learning unsupervised features from objects

unsupervised feature learning can be a powerful technique for some machine learning
problems  this project explores the use of a convolution deep belief network  cdbn  to learn
unsupervised features from images  deep belief networks  dbn  and cdbns have previously been
applied to many problems  from learning to generate facial expressions  susskind et al         to
unsupervised feature learning for audio classification  lee et al       b  
here  a cdbn is used to learn unsupervised features from two categories in the caltech    
dataset  griffin et al          after learning features  a simple linear svm is used to predict object
category using supervised information  the results are compared against tiny versions of the images as
a baseline and histogram of oriented gradients  hog  features  which have successful for instance in
person detection  dalal and triggs        
dbns share many commonalities with the cdbn  both algorithms have a hierarchical structure
where each layer learns to model statistical dependencies between variables  in a dbn  this  layer  is
called a restricted boltzmann machine  rbm   an rbm is a graphical model with two disjoint sets of
units  called visible and hidden units  hidden units model unobserved causes of the visible unit
activations  explicitly computing the gradient of its parameters is extremely expensive and infeasible in
practice  however  the rbm can be trained efficiently because the layers are conditionally independent
on one another  using gibbs sampling we can alternatingly sample from the conditional distributions of
one layer given the other  particularly  using contrastive divergence  cd  we sample very few times 
in practice  even one up down up pass can be quite effective  carreira perpinan and hinton        
cdbns  as proposed by lee et al   introduce two significant modifications to the dbn      a  
first  instead of learning weights for all positions in the image  we instead learn a bank of filters that
are convolved with a given image  taking advantage of the implicit structure of an image  second  to
encourage additional invariance  a pooling layer is added above each crbm  to allow for a fully
generative model  lee et al  propose a new method of pooling  which they call probabilistic maxpooling      a  
method
matlab code for a simple crbm  provided by honglak lee at  http   ai stanford edu  hllee 
softwares icml   htm  was used as a starting point  several important changes were implemented 
first  code was adapted to aid in exploring the effects of different parameters  code was written to walk

fithrough parameter space and save results at each step  in practice  running several instances
simultaneously took around     hours before a suitable set of parameters for the first layer were
learned  second  following lee et al          a sparsity penalty was added to encourage the learned
bases to be more sparse as

using cd  the crbm already approximates derivatives for its parameters  however  we can easily
compute the exact gradient of the sparsity term  during one epoch  several mini batches were first run 
afterward  one step of gradient descent was taken on the sparsity penalty term  following lee et al 
        only the partial derivatives of the bias parameters were computed and used for gradient descent 
two other interesting options were implemented in the crbm  first  an option to add a zeroborder of a certain size to the image was added  following a suggestion by krizkevsky  this feature was
found to be necessary to learn first layer bases  second  a debug mode was implemented to watch the
visible data  reconstructed visible data  hidden unit activations  and weight changes to the crbm
during training  this was found to be a useful tool when exploring different parameter choices
manually 
after crbm code was written  bases were learned  following lee et al       a   the first layer
bases were learned from the kyoto natural images dataset  two forms of preprocessing were used 
first  images were whitened to flatten the power spectrum  afterward  image data was transformed to
have mean   and variance     
to explore the effectiveness of a cdbn in learning unsupervised features  the caltech    
dataset was used to learn second layer bases from the tshirt and billiards categories  due to time
restrictions  the parameter space could not be adequately explored and these results are tentative  the
results indicate that these bases were moderately effective in classification 
results
first layer bases were difficult to learn using only the crbm with sparsity penalty  typically 
the learned bases focused on the corners and did a poor job of modeling the visible data  see figure    
following krizhevsky  a zero border half the width of the bases was added to the image after rescaling 
this modification was quite helpful  see figure     these features are generally centered

fiand are closer to the expected
result  however  they fail to
capture orientation information
and mostly exhibit radial
symmetry  in addition  the crbm
did not take advantage of all of
figure       first layer bases learned
without a zero border

   first layer bases after a zero border
was added

the bases effectively 

to address this issue  whitening was performed as a preprocessing step  whitening significantly
improved the results  as can be seen in figure    the bases learned capture much more interesting
information  for example  many bases are orientationdependent 
finally  layer two bases are presented in figure    these
bases were learned using a pooling factor of c   and were
trained on both categories  the first row contains hand selected
interesting features and the rest were selected randomly from
the     layer two bases  although some bases are quite noisy 

figure    first layer bases after whitening

the cdbn does learn gabor like features as well as interesting contours 
after learning features from the data  the
features were evaluated by their performance in object
classification using an svm  libsvm   features were
constructed by convolving the learned bases over each
input image and then scaling the result  except where
noted  all results are cross validated  these results use
only a linear kernel  although other kernels could be
used  the linear kernel appears to work fairly well  in
addition  since the feature space is highly dimensional
for certain evaluations and we have few examples 
using for instance an rbf kernel may not be a
figure       second layer bases learned from objects

panacea  finally  for simplicity  the dataset contains exactly     images  the minimum  from each of
the categories tshirt and billiards  thus  chance performance is     

fias a baseline  images from each category were simply scaled and used as input to the svm  we
can imagine  for instance  that billiards images tend to be more illuminated on the bottom half while
tshirt images are illuminated more uniformly  in this case  the raw image data could be quite
successful at classifying the images  images of size  x  were found to offer the best performance of
tested sizes  achieving        accuracy  see figure   for all results  
next  the first layer features were used as input to the svm as described  rescaling the result to
 x  again produced the best result  achieving        accuracy  this gives interesting insight  first  the
features learned on natural images do perform fairly well on classification of objects  second  since we
find significantly better performance than using the raw image data  these features capture more
valuable local information about the images than simply the means for image regions  see figure    
as a comparison  simple hog features were used as input to an svm  in this experiment 
histogram features were computed globally for the entire image  the best performance was achieved
using    hog features  correctly classifying        of the images  in a sense  these features are
extremely local as they operate on only several adjacent pixels  however  the relative success of this
method shows that this information is quite useful at classifying the images 
finally  the learned layer   features were evaluated  one problem in testing these features is that
the resulting input to the svm is high dimensional  for example  using all bases with even  x  scaled
feature data results in       dimensional data  as a simple experiment  the data for each feature was
scaled to  x          accuracy  and  x          accuracy   compressing down the response from
each filter performs a bit worse than the first layer features  larger data sizes were also tried  with
limited effectiveness  with only     total examples      of which are in the training set  the svm
could not effectively learn to classify the images 
to address this issue  two tricks were tested  first  only a small subset of randomly selected
bases were used  using only    bases of the total     and crushing down the responses to  x  resulted
in        accuracy  which is worse than achieved with the first layer bases and close to raw image
performance  other selections for the number of bases and scaled response size were approximately
equivalent and did not outperform even the  x  scaled responses  pca was also performed to find
linear combinations of features that had the greatest variance over all images  however  the results are
underwhelming  resulting in comparable to worse performance  not included  

finame

cross validated 

total dimensionality

performance

plain images   x 

y

 

      

plain images   x 

y

  

      

plain images   x 

y

  

      

plain images   x 

y

  

      

plain images    x  

y

   

      

plain images    x  

y

    

      

first layer bases   x 

y

   

      

first layer bases   x 

y

   

      

first layer bases   x 

y

    

      

hog    features

y

 

      

hog     features

y

  

      

hog     features

y

  

      

hog     features

n

  

      

hog     features

n

  

      

second layer bases   x 

y

   

      

second layer bases   x 

y

    

      

random    second layer bases   x 

y

   

      

figure    results of all tests
discussion
while deep belief networks can be quite powerful  getting all of the parameters right in a system
can be quite difficult  for this problem  features from the first layer significantly outperformed raw
image data  the second layer bases performed poorly  although it is suspected that this performance is
mostly due to the difficulty for the linear svm to separate the examples  a simple implementation of
hog outperformed all other features  i would like to continue exploring this type of problem more
thoroughly and using a gpu to make the computational times more tolerable 
works cited
carreira perpignan  m  and hinton  g  on contrastive divergence learning  artificial intelligence and statistics  pages       fort lauderdale        society for artificial intelligence and statistics 
dalal  n  and triggs  w  histograms of oriented gradients for human detection       ieee cvpr    vol       p              
griffin  gregory and holub  alex and perona  pietro  caltech     object category dataset  california institute of
technology       
krizhevsky  a  convolutional deep belief networks on cifar     unpublished manuscript       
lee  h   grosse  r   ranganath  r   ng  a y  convolutional deep belief networks for scalable unsupervised learning of
hierarchial representations  proceedings of the   th international conference on machine learning  montreal 
canada      a 
lee  h   largman  y   pham  p   ng  a  unsupervised feature learning for audio classification using convolutional deep
belief networks  advances in neural information processing systems                 b vol    pg      
susskind  j m   hinton  g e   movellan  j r   anderson  a k  generating facial expressions with deep belief nets 
affective computing  emotional modelling  synthesis and recognition  ars publishers  pp               

fi