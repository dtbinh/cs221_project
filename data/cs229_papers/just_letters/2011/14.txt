cs    project final report
sign language gesture recognition with unsupervised feature learning
justin k  chen  debabrata sengupta  rukmani ravi sundaram

  

introduction

the problem we are investigating is sign language recognition through unsupervised feature learning  being able
to recognize sign language is an interesting machine learning problem while simultaneously being extremely
useful for deaf people to interact with people who dont know how to understand american sign language
 asl  
our approach was to first create a data set showing the different hand gestures of the asl alphabet that we wish
to classify  the next step was to segment out only the hand region from each image and then use this data for
unsupervised feature learning using an autoencoder  followed by training a softmax classifier for making a
decision about which letter is being displayed  our final dataset consists of ten letters of the asl alphabet 
namely a  b  c  d  e  f  g  h  i and l  the following block diagram  figure    summarizes our approach 

fig     block diagram summarizing our approach for sign language recognition

fi  

methodology

   

data collection and hand segmentation

the data that we used was collected off of a microsoft kinect  d depth camera  videos were taken of the test
subjects hands while forming sign language letters  frames showing individual letters were extracted  for
segmenting out only the hand  we tried several approaches as described below 
the first approach involved modeling the skin color by a  d gaussian curve and then using this fitted gaussian to
estimate the likelihood of a given color pixel being skin  we first collected skin patches from    random images
from the internet  each skin patch was a contiguous rectangular skin area  the patches were collected from people
belonging to different ethnicities so that our model is able to correctly predict skin areas for a wide variation of
skin color  we first normalized each color as follows   r r  r g b   b b  r g b   we ignored the g
component as it is linearly dependant on the other two  we then estimated the mean and covariance matrix of the
 d gaussian  with r  b as the axes  as mean   e x    covariance c   e x    x   t  where x is the matrix with
each row being the r and b values of a pixel 

fig    histogram of the color distribution for skin patches and the corresponding gaussian model that was fit to it 

with this gaussian fitted skin color model  we computed the likelihood of skin for any pixel of a given test image 
finally  we thresholded the likelihood to classify it as skin or non skin  however  this approach did not give
significantly good results and failed to detect dimly illuminated parts of skin  the results of using this algorithm
for skin segmentation are shown in fig    b  
the second approach which we used is motivated by the paper      in which the authors first transform the image
from the rgb space to the yiq and yuq color spaces  then the compute the parameter    tan   v u  and
combine it with the parameter i to define the region to which skin pixels belong  specifically  the authors called
all pixels with      i      and    o         o as skin  for our experiments  we tweaked these thresholds a bit 
and found that the results were significantly better than our gaussian model in the previous approach  this might
have been because of two reasons   i  our gaussian model was trained using data samples of insufficient variety
and hence was inadequate to correctly detect skin pixels of darker shades  ii  fitting the model in the rgb space
performs poorly as rgb doesnt capture the hue and saturation information of each pixel separately  the results
of this second approach is shown in fig   c  
after having detected the skin regions  the next task was to filter out only the hand region and eliminate the face
and other background pixels that might have been detected  for this  we used the depth information that we

fiobtained from the kinect  which returns a grayscale depth image with objects having lower intensity values being
closer to the camera  we assumed that in any frame  the hand was the object closest to the camera  and used this
assumption to segment out only the hand  our final dataset consists of hand gestures for ten letters of the asl
alphabet bounded in a   x   bounding box  as shown in fig    we have      samples for each letter  giving us a
total size of        images for our entire dataset of ten letters 

fig   a  original image

a

b

c

fig   b  skin detected using the
gaussian model

d

e

f

fig   c  skin detected using yiq
and yuv color spaces

g

h

i

l

fig    samples of each letter from our final dataset

   

unsupervised feature learning and classification

the extracted data of hand images was fed into an autoencoder in order to learn a set of unsupervised features 
we used     images of each letter  so  a total of      images  as training data samples and fed them into the
sparse autoencoder  each input image from the segmentation block are images of size   x   pixels  a sparse
autoencoder is chosen with an input layer with   x   nodes and one hidden layer of     nodes  we used l bfgs
to optimize the cost function  this was run for about     iterations to obtain estimates of the weights 
visualization of the learned weights of the autoencoder is shown in fig    from this figure  it becomes evident
that the autoencoder learns a set of features similar to edges 

fig     visualization of the learned weights of the autoencoder

fithe next step was to train a softmax classifier in order to classify the    different letters based on the features
learnt by the autoencoder  the activations of the hidden layer of the autoencoder was fed into a softmax classifier 
the softmax classifier again learns using the l bfgs optimization function  this algorithm converges after about
   iterations  we tested the system classification accuracy by using the remaining     images per letter  so a
total of      images  as our test set 
  

results and discussions

in the previous sections  we have mentioned details of our implementation of the hand segmentation 
unsupervised feature learning and classification sub blocks  in this section  we report the performance of our
system through tables and figures 
as a preliminary diagnostic  we plotted a learning curve showing the percentage training error and test error in
classification as a function of the size of the training set  the following plot shows the learning curve 

fig     learning curve
in our milestone report  we had used    hidden units for our autoencoder  analyzing our learning curve  we
observed that the training error and the test error are close to each other  except for one aberration at training set
size        and even the training error is more than       which is somewhat high  hence suspecting that we
might be in the high bias region  we decided to increase the size of our features by increasing the number of
hidden units of our autoencoder to      our final classification accuracy achieved using this     length feature
vector was       the following table summarizes all our implementation details and reports the accuracy
obtained  
size of training set
    
    
    
    
    

size of feature
vector
   
   
   
   
   

number of classes
 letters 
  
  
  
  
  

accuracy of
classification    
     
     
     
     
     

fias a finishing step to our project  we have successfully created a real time implementation of our entire system  so
that hand gestures made in front of the kinect connected to our computer directly displayed the image captured by
the kinect  the segmented hand gesture and the output of our classifier  which is one of the ten letters in our
dataset  the evaluation process takes less than   seconds per frame  the following figures show examples of our
real time implementation and the results obtained 

fig      examples of prediction from our live demo
  

concluding remarks and future work

in this project  we have implemented an automatic sign language gesture recognition system in real time  using
tools learnt in cs     although our system works quite well as has been demonstrated through tables and images 
theres still a lot of scope for possible future work 
possible extensions to this project would be extending the gesture recognition system to all alphabets of the asl
and other non alphabet gestures as well  having used matlab as the platform for implementation  we feel that
we can improve upon the speed of our real time system by coding in c  the framework of this project can also be
extended to several other applications like controlling robot navigation using hand gestures and the like 
  

acknowledgements

we would like to thank prof  andrew ng and the tas of cs    for the valuable advice they provided from time
to time 
references
   

xiaolong teng  biani wu  weiwei yu and chongqing liu  a hand gesture recognition system based on local
linear embedding  april      

   

d  metaxas  sign language and human activity recognition  june       cvpr workshop on gesture recognition 

   

s  sarkar  segmentation robust representations  matching  and modeling for sign language recognition  june      
cvpr workshop on gesture recognition  co authors  barbara loeding  ruiduo yang  sunita nayak  ayush
parashar

appendix
this project was being done in combination with justin chens cs   a computer vision project 

fi