predicting the outcomes of baseball games
richard harris  allan joshua  justin sirignano
stanford university

abstract  in this paper we investigate using machine learning algorithms to predict the
outcomes of baseball games  baseball has a large amount of raw data  including
pitching  batting  and defensive statistics for each game  in addition  baseball has the
largest total number of games per season of any sport  this combination of readily
available data and the large number of games make baseball a great prospect for
machine learning  we use past data to predict the outcomes of baseball games with the
goal of discerning any patterns or shedding light on what characteristics produce a
winning baseball team  we use logistic boost and svms 

    introduction
inherently  certain statistics a baseball team produces represent its capability to win  for
example  a teams record gives good indication of how well a team will do in a game 
more times than not  the team with the better record wins  another example is head to
head record  usually  the team that has won most of the previous matchups wins  either
because the team is better as its record might indicate  or because the team matches up
well against the opposing team 
in addition to including features that account for the entire teams performance  it is also
plausible that certain players account for a large amount of the teams success  whether it
be power hitters on offense  or pitchers on defense  to represent this  we included
features that reflect how good home team hitters are vs  away team hitters  and how well
hitters on each team have done against their opponents starting pitcher  we also created
similar features for pitchers 
it is also common knowledge that both teams and players go on streaks and slumps  to
provide an accurate account of existing knowledge of these streaks and slumps to the
learners we used  data reflecting past performances in recent games is also included in the
feature set  given features summarizing the opposing teams past and recent
performances  opposing hitters past and recent performances  and opposing pitchers
past and recent performances  we felt our learners were ready to give us killer results 

    data and choice of features
     

fiwe created a large database detailing pitch by pitch events in each game for the last
thirty years  with this raw data  we wrote a series of queries to calculate the features we
were interested in  the features we looked at are 






team statistics head to head win differential
win count differential in last x amount of games
error count differential
win count differential of home team wins at home in last x games vs 
away team wins away in last x games
comparatively how well each team does against a starting pitcher of a
certain hand  left vs  right 



offensive statistics  recent performances   performance by year to date   and
career performance against opposing starting pitcher  
 player wise on base percentage
 player wise slugging percentage
 total number of bases



pitching statistics  recent performances and performance by year to date 
 at bats
 strike outs
 walks allowed
 singles allowed
 doubles allowed
 triples allowed
 homeruns allowed

we believe that recent performances of players are essential because they reveal
important information about the momentum of those players entering the game  all
players go through streaks and slumps  therefore  just using statistics for their careers
would be wildly inaccurate  it is better to take into account recent performance levels 

    methodology
      features
we are dealing with an inherently noisy data set  i e   the best team may not win all the
time   therefore  we desired an algorithm that is robust to noise  that is  it is less likely
to overfit noise  we choose to use logistic boost for this reason  the weak classifiers are
trees  we also use svm and compare the results 
                                                                                                                
   recent performances refer to player performance in the past                and    games  
   performance by year to date refers to player performance in the past             and   years  
   career  performance  against  the  opposing  starting  pitcher  refers  to  a  hitters  past  performance  against  the  
opposing  starting  pitcher  in  the  past                  and     years   
     

fiwe investigate several ways in which to improve our feature set  first  we tried
normalizing the features by their mean and variance  in addition  we attempted to use
polynomial features with a hope that the svm would provide a superior fit given the
nonlinear data  then  we tried to reduce the feature set by taking the features most
correlated with the outcome of the game  finally  we also did a principal component
analysis to reduce the feature set into its most important principal components  we found
that these methods helped marginally  if at all 
to determine the optimal number of iterations for the logistic boost  we did a k fold
cross validation with k       to gauge the out of sample accuracy of the svm and
logistic boost algorithms  we tested their fitted models on a separate validation set 
      choosing the optimal number of weak classifiers for boost
boost uses a number of weak classifiers to make predictions  the optimal number of
weak classifiers  i e   the number of iterations boost takes  must be determined  too
many iterations is prone to overfit the data while too few may underfit the data  we find
the optimal number of iterations by dividing the training set into a smaller training set
and a cross validation set  below we show the cross validated accuracy for logistic
boost for different numbers of iterations  we use these results to pick the number of
iterations for each boost algorithm which gives the highest cross validated accuracy as
the optimal number of iterations  given the optimal number of iterations  we retrain
the boost algorithm on the entire training set and then test it on a separate test set 

      choosing the optimal c for svm
we fully expect our data from baseball games to not be linearly separable  to
compensate for this  we used included a c parameter for the svm  the final value of

     

fic was chosen by empirically running through values of c from     through    
ultimately  we chose the value that produced the best results from these empirical tests 

    results and discussion
first  we will provide a brief summary of our best results  in the paragraphs following
the summary  we outline some steps we took to attempt to improve our learners
performance   best results here 
our immediate thoughts were to try three learners  logistic boost  adaboost  and an
svm  the results for each of these three learners is given below 
learner
description accuracy
logit
  
      
boost
iterations
adaboost   
      
iterations
svm
c   
        

true pos
      

false pos
      

false neg
      

true neg
      

      

      

      

      

      

      

      

      

both of the implementations of the learners we used provide a probability associated with
each prediction  in thresholding the probability of the result  we can filter out games that
are difficult for the learner to decide  this trick usually provides about   to     better
results  but at the cost of making three quarters of the data unusable  of course  this
translates to not being able to bet on a quarter of all games 
learner
logit boost
svm  c   

accuracy
      
      

  examples used
      
      

we next tried to reduce the dimensionality of our feature vector by using the features
most correlated with the result  this produced mixed results  but overall  the
performance of the learner did not improve  we also tried using principal components
analysis where we took the first    principal components  this also yielded mixed
results 
learner
logit
boost
svm

  princ comp accuracy
  
      
  

      

true pos
      

false pos
      

false neg true neg
      
      

      

      

      

      

while we understand sports are not deterministic  we were hoping to obtain better results 
to try to improve our results  we tried higher order polynomial features  this did not
help  however 

     

fion analyzing the learning curve and plotting jtrain and jcv as part of the model selection
process  we realized that we were under fitting the data and the training error was high as
well which indicated a high bias problem  we then decided to use a neural network and
learn the parameters using back propagation  we used one hidden layer with     logistic
units on it  using an advanced optimization algorithm  fmincg  we were able to train the
network within a reasonable timeframe  on increasing the number of iterations close to
     the network started over fitting the data and our training accuracy went up to      
on the training set  the network however  did poorly on the test set and the accuracy was
only        
we then regularized the cost function and minimized that and by trying various values of
lambda  the network started generalizing well and gave good results on the test set  with
   iterations we were able to get to an accuracy of    
we also tried svm with an rbf kernel and tried to pick gamma and c  we did not have
time to implement this but we are planning to continue working on it  our feature set size
and the number of training data points make an rbf kernel a prime candidate to pursue 
the cost function of the neural network was still decreasing on every iteration which
indicated that we would still be able to get better results  this is an avenue which we plan
to pursue as well 
  

    conclusion and future work
in conclusion  we tried several machine learning algorithms to predict the outcomes of
baseball games  the results were not spectacular  although we must recognize that the
outcomes of baseball games are very noisy and therefore difficult to predict  predictions
made when the algorithm is confident  i e   thresholding  are much more accurate  future
work might attempt to develop a better feature set or a better algorithm  something that
might be interesting to try would be logistic boost with logistic regression or svms as
weak classifiers 

     

fi