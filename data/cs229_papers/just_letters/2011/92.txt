predicting negative couchsurfing experiences using local information
jan overgoor
stanford university
overgoor stanford edu

abstract
we attempt to predict negative references
in the couchsurfing  cs  social network
using an svm classifier  the low frequency of negative references  however 
makes prediction very difficult  we attempt to achieve a classifier with a high f 
score by training on data sets with varying
distributions of positive and negative examples and by manipulating the decision
boundary  we also explore how trust propagates through the cs network in order to
design a set of features that represent personalized evaluations of other users  our
model of these local evaluations increases
classification performance 

 

introduction

couchsurfing   cs   is a social networking site
that allows travelers to find temporary hosts on
their journeys  users of the site maintain a public
profile that contains certain personal information
and a history of references  after a hospitality exchange users leave each other textual references 
which are tagged as either positive  neutral or negative  this reputation system allows people that
have never met before to evaluate each other  and
revealing misbehavior  the reference system is 
however  not a fully reliable representation of the
sentiment between users  of the    m reference
in the system  only   k are non positive    k are
neutral and   k are negative    lauterbach et al  
      showed that these numbers give a more positive image than what actually happens  not every
one leaves a reference after an experience and if
they do  they are strongly biassed towards a positive one in fear of reciprocity 
this also means that a negative reference  if it
occurs  carries a stronger weight and probably rep 

http   www couchsurfing org 

ellery wulczyn
stanford university
ewulczyn stanford edu

resents a particularly negative experience  we attempt to train an svm to predict negative experiences based on each users personal information
and position in the cs social graph  we first describe the data set and the features we used  after discussing the results  we present an alternative
method that favors information that comes from
trusted sources 

 

data   features

the complete dataset consists of    m user profiles and    m connections between users  a connection consists of a friendship link     m  and or
a reference     m   friendship links are accompanied by a value between    havent met  and  
 best friend   as well as a confidential assessment of how much the users trust each other  a
legend of these values is displayed in table   
value
 
 
 
 
 
 

semantics
i dont know them well enough to decide
i dont trust this person
i trust this person somewhat
i generally trust this person
i highly trust this person
i would trust this person with my life
table    legend of trust values

in our classification task  a training example
 x i    y  i    represents a directed experience between two users  as evaluated by one of the users 
y  i  is the evaluation of user t by user s  where
a positive example      refers to a negative rating and a negative example      refers to a nonnegative rating  either positive or neutral   we applied this inversion because we want to spot negative ratings  the feature vector x i    consists of
three parts  one set of features for both users and
a short amount of comparison features 
a single users feature vector consists of per 

fimodel selection n       k  

  

  
  

  

  
  
  
accuracy

test accuracy

  

  

  
  

  

  
  

  

    
  

lin
poly
rbf
sigmoid

  

 

  

 

 

  

  

 

  

 

  

c

  

 

   

 

   
 
   
number of training examples

 

   

 
 

x   

figure    model selection on different kernels and
c           

figure    quantity analysis with rbf kernel and
c      

sonal attributes  e g  age  sex and location   evaluation by the cs community  represented by distribution of friend scores  trust levels and number of
references received  and an evaluation of the cs
community  the same attributes  but given instead
of received   the comparison feature set contains
the difference between users in sex  age  location
of origin and experience on the site 
in constructing our training examples we discard any information from after the reference date
so as to simulate real prediction conditions  where
there is no post facto data available   

we also looked the learning rate as a function on
the number of training examples  because the size
of the training set has a big effect on our running
time  figure   shows that from about   k training
examples the difference in accuracy is relatively
small  so we decided on using that number for
the rest of our queries  interestingly  it seems that
learning has not stopped yet with the   k training
examples that we gave it  but the number of training examples is bound by the number of negative
references in the data set 
a third preliminary test we did was ablative
analysis  because of our large amount of features  n        and their conceptual proximity  we
chose to do ablative analysis on feature sets as opposed to individual features  the results are displayed in table    attributes u includes both the
personal attributes and the evaluation behavior of
u  the most striking result its that the   comparison features have the biggest impact on the performance  but in general the relative impact of the
feature sets is not significant  we considered using feature selection to reduce noise  but the run
time of backward search proved prohibitive  our
ablative analysis does not hint at feature selection
making a big difference  so we decided against it 
the overall performance of our system  an accuracy of above      seems like a great result 
but in reality  the ratio of positive and negative examples is heavily biased  only      or instances
in the total data set are negative  so positive examples   employing our classifier to detect negative
references would behave gravely trigger happy
 precision           in order to improve perfor 

 

method   results

we based our experimental set up on the libsvm
svm library  chang and lin         which we
accessed with our own python bindings  we implemented a caching system for the dot product
computation and made the decision boundary adjustable  our initial investigations concerned balanced training and test sets  with an equal number
of positive and negative examples  we used   fold
cross validation for every result in this paper 
we first performed model selection to select the
best kernel and error cost parameter c  the results
 displayed in figure    show that  above c       
there is little difference between the polynomial
 with d      and rbf kernels  a comparison in
computation time made us choose the rbf kernel
and c       
 
without this precaution  it is quite easy to produce very
accurate classifiers  which take advantage of information that
accumulated in reaction to the negative reference and  of
course  the negative reference itself 

fiaccuracy
      
      
      
      
      

   
   
   
   
     

    

   

    
f  score

feature set
attributes s
evaluation s
attributes t
evaluation t
comparison

table    ablative analysis  accuracy when leaving
a feature set out

   

    

   

 
    

   
   
   
     

   

 
 

   

precision

   

   

 

   

 

   
t

 

   

 

   

 

figure    f  performance for different training
sets

   

   

   

   

   

   

 

   

   

   

   
recall

   

   

   

   

alternative to such a global evaluation would be
to preferentially consider information from close 
highly trusted friends  maybe the wisdom of the
crowds is not as good as a piece of sound advice
from a trusted friend 

figure    precision recall curves for different
training sets
mance on a skewed test set     positive examples
and      negative examples  we tried variations
on two aspects  the decision boundary  t  and the
skewness of the training set  figure   shows how
the skewness of the training set affects the precision recall curve  made by varying the prediction
threshold  and figure   shows the performance as
expressed in f  scores  the different training sets
are expressed by the percentage of negative training examples  so       refers to using a training
set with the same skewness as the test set and    
is the balanced training set that we used before 
the overall best performance is achieved by the
    set  which has a f  score of      with the decision boundary of t      if we  however  care
more about recall  which we might in the extract
the negative references scenario  then the best we
can do is the       training set with t      which
retrieves     of the positive examples but with
only      precision 

 

local trust

in the model described above  we use the evaluation of a user by the entire cs community as a
feature set for predicting negative references  an

we hypothesize that if a user  the source  wants
to evaluate another user  the target  he would get
better information by considering the evaluation of
the target by his own friend network than considering the global evaluation by the entire cs network  the information that comprises any evaluation stems from the set of informants  users evaluated the target  we model an evaluation of the target that is personalized to the source by weighing
the information about the target by some function
of the trust the source has in the informants  the
underlying assumption is that information coming
from an informant we highly trust is worth more
than information from a person we dont know 
the three key components of this model are
estimating the trust the source has in each informant  mapping those trust values to information
weights and aggregating the weighted information
from each informant to form a local evaluation 

fi   

estimating trust

if there is a friend connection between the source
and an informant  the trust score is given explicitly
in the friendship connection and their is no need
to estimate trust  the cs graph  however  is very
sparse  the source is friends with an informant in
only    of the cases  to make a local evaluation richer in information  we can take another step
and include friends of friends as informants  here
the problem is that we dont know how much the
source trusts the friends of his friends  we only
know how much the source trusts his immediate
friend  t    and how much his immediate friend
trusts the informant  t     in this second part of our
project  we investigated how trust propagates in
the cs network by designing functions that given
t  and t  estimate the trust from the source to the
informant  t     in order to fit and test our models  we extracted all    m trust triads of the form
 t    t    t    from the cs database 
the trust propagation functions are of the form
  t    t      t    where t    t                      and
t            see table   for the semantics of
the trust values   we first created a simple probabilistic model  we treat trust t as a multinomial
random variable  if we assume t    t    t  to be independent  then it follows 
 

  t i 

p  t   t     i
i  
   

 

  t  i t   j t   k 

p  t    t    t         ijk  
i   j   k  
 

 

  t   i t   j 

p  t    t        ij
i   j  

using maximum likelihood estimation  we get 
  t    i  t    j 
ij   
m
l  
m

 l 

  t    i  t    j  t    k 
 
m
l  
m

ijk

 l 

 l 

 l 

p  t   t    t    

p  t    t    t   
p  t    t   
t   t   t 
t   t 

 avg  t    t     e t   t    t   
 max  t    t     arg maxt  p  t   t    t   
we also tried three linear models 
 lin  t    t     t    t     t   t      
 hlinsp  t    t     hierarchical linear model
with  t    t    as random effect
 hlinu  t    t     hierarchical linear model
with source user as random effect
all of our functional models except for max
rely on a sensible ordering and scaling of the input variables  one issue with the way cs encodes
trust is that the value   corresponds to i dont
know this person well enough to decide  which is
less than   which corresponds to i dont trust this
person  a value of   denotes a lack information
about trust rather than a lack of trust itself  in our
models  instead of treating   as strong distrust we
treat   as signaling an unknown trust value  which
we estimate as the average trust in the whole system  map     e t           
in order to establish a performance baseline for
each model  we generated a random data set with
the same distribution of trust scores as in the real
data set  as done in  leskovec et al           this
allows us to disentangle the influence of the distribution of trust scores from the structure of trust
triads on model performance  the results are displayed in table    we provide the mean squared
error for each model  mse  as well as the difference between the mse on the shuffled data set    
overall  it appears that the structure in the trust triads does allow a better prediction  mapping the
  values to e t   improves performance and avg
performs best 

 l 

using the definition of conditional probability 

p  t   t    t     

based on this model  we propose two methods
for predicting t   

trustfunction
avg
max
lin
hlinsp
hlinu

no mapping
mse   
             
             
             
             
             

   e t  
mse   
             
             
             
             
             

table    performance of different trust propagation functions

fianother concern  especially for the linear models  is that the difference in trust from one cs trust
level to another may not be constant  in the attempt to find an ideal scaling we tried to design an
invertible function  that would allow us to rescale
the cs trust values before learning our trust propagation functions and then map the outputs of the
trust propagation function back into cs trust values in order to test the accuracy of our model and
compare it to the baseline cs scaling 
as our rescaling function  we tried using the
logit function and piecewise quadratic function because they are invertible and have the ability to
make high trust levels even higher while making
low trust values even lower  to learn the parameters of our mapping function f we attempted to
minimize the following objective using gradient
descent 
m

 f

 

   f  t     f  t      t    

i  

this rescaling endeavor was rather unsuccessful
as mse under rescaling was never lower 

 

implementing and testing local
evaluation

as described earlier  we can think of a trust propagation function as completing a trust triad  which
is missing a single link  trust propagation can then
be applied n times to estimate the trust that s has
in an informant who is n     steps away 

this would in theory allow us to get a fully local evaluation  in practice  we only consider informants that are up to three steps away from the
source  we make this  rd order approximation
due to the computational difficulty of computing
every path from the source to every informant 
after computing the trust the source has in each
informant it is necessary to first weigh and then aggregate the information from each informant to arrive at a local evaluation  as information weights 
t   
we tried w   t    t 
  and w   t          log    t  
     which both increase in trust  w  weighs high

trust higher and low trust lower than w    information is aggregated  by taking the weighted average
of the information over each path from the source
to an informant 
to evaluate these local trust methods  we extended our training examples with a local evaluation of t  feature set  the performance of the
four methods  as well as a global trust baseline 
are displayed in table    unfortunately  we had to
train on a relatively small training set  m        
due time constraints and the computational cost of
finding all local informants 
case
acc

w o local evaluation
     

w 
     

w 
     

table    accuracy when training on a balanced
training set m        and a balanced test set
for both weighting schemes  adding local evaluation features increases accuracy by approximately     a problem is that only a third of
the cases have at least one informant within three
steps  in a real prediction system  however  one
could simply use the model trained with local evaluation features if they are present and otherwise
use the model trained without local evaluation features  we were unable to test the merit of local
trust with more robust amounts of training data 

references
chang  c  c  and lin  c  j         
libsvm 
a library for support vector machines 
acm
transactions on intelligent systems and technology               software available at http   
www csie ntu edu tw cjlin libsvm 
lauterbach  d   truong  h   shah  t   and adamic  l 
        surfing a web of trust  reputation and reciprocity on couchsurfing  com  in computational
science and engineering        cse    international conference on  volume    pages        
ieee 
leskovec  j   huttenlocher  d   and kleinberg  j 
        signed networks in social media  in proceedings of the   th international conference on
human factors in computing systems  pages     
      acm 

fi