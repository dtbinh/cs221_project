s tanford u niveristy
m achine l earning cs   

early defect identification of semiconductor processes
using machine learning

friday  december         

authors 
saul rosa
anton v ladimirov

professor 
dr  andrew n g  

friday  december         

fiearly defect identification of semiconductor processes using machine learning  december     

 

early defect identification of semiconductor
processes using machine learning
saul rosa  anton vladimirov

abstractearly wafer defect identification can account for
significant savings in test time and assist in improving the fabrication process  defect measurements with exhaustive parametric
structures can be extremely costly and prohibitive  embedded
rapid parametric data collection  using vcos  has been added
to the chip design  at the cost of semiconductor real estate  so
far the data has been considered too noisy and unreliable to
draw conclusions off of  using machine learning  models can be
generated  which allow for proper classification of chip quality 
this paper presents a method of applied machine learning to
take advantage of this data 

i  i ntroduction

i

n chip manufacturing processes and tools  generally have
a large impact on the final quality of chips  due to the
time intensive nature of measuring parameter structures that
indicate the quality of the tools and processes at each stage
only a sample of wafers are measured  as the production
line matures  less sample wafers are selected  this creates
an issue of being unable to track production process quality
over the lifespan of chips  manufacturers have attempted to
solve this issue by embedding dummy measurement structures
that have no functional purpose other than to measure these
process changes  due again to test time restrictions these
structures have limitations on the methods used to measure
device effects 
most of these structures rely on voltage controlled oscillator
 vco  methods of measuring singular effects  since each
value is only directly related to one device effect the values
often indicate little about the chip quality  values that show
significant shift are often readily caught by the fab without
further testing  however  combination of parameter values
may give us more insight into overall chip quality  there
is no known algorithm to determine which combinations of
parameters are indicative of overall quality  instead we use
machine learning strategy to discover them 
in the design selected for this study there are    selected
parameters and each has dependencies on the other  some of
the parameters may have little effect while others have a strong
correlation to the overall quality of a chip  given a detailed
knowledge of the chip design  some of these dependencies can
be understood  however  most others are unknown  we wish to
develop an algorithm that will accurately predict chip quality
based on a given set of measurements and in turn provide
the quality of the wafer  this will allow early classification
of failing chips that normally wouldnt be caught until much
later in the testing process  this would account for significant
savings in test time on poor quality wafers  as well as provide
early feedback to the fabrication plant on the quality of their
process 

fig     principal component analysis  pca  reduction of the data space
to a   dimensional view

ii  m otivation

t

he goal of the model is to create a method by which
wafer quality prediction can be made and fed back to
the fab  since these structures are measured so early in the
testing process and require very little overhead they are ideal
for fab feedback  with this added classification mode  wafers
with high defect rate can be isolated and understood using the
more complex  and slower to measure  parametric structures 
placing this model in line during a device bring up can help
to isolate issues in the fab process and improve yield  and in
mature processes act as an indicator of fab issues 
in some cases defects arent detected until well after chips
have been taken off the wafer and are being tested in system 
at this point the process of removing the chip and isolating the
fail is very costly  with the added layer of a learning algorithm
selecting out wafers  the number of defects in systems should
reduce 
iii  background

r

oughly        chips were selected for this study 
their respective vco data and defect classifications
were collected 
the process selected contains five groupings of ten values 
each at various locations on the chip  the goal is wafer
identification  but quality is measured by chip  so individual
chips are separated out in the data sets we collect 
in figure   we show a subset of the data set  each is
normally distributed  some outliers exist  but they are already
easily identified  and most represent issues with the measurement system  which are filtered out in advance 

fiearly defect identification of semiconductor processes using machine learning  december     

 

no added value  they are easily identified as process issues 
they were removed from out base set 
vi  s caling
everal data scaling strategies were tried to achieve
optimal results in chips quality classification  based on
prior knowledge  taking the proportion of parameters against
each other was attempted  this yielded minor improvements
in linear and logistic regression models but provided little
improvement when using a support vector machine 
various vco parameters represent different types of measurements  such as trace length or delay in gate response 
and have widely varying ranges  scaling parameters using
a unified scheme across all dimensions therfore results in
reduced accuracy  instead we scale each feature individually 
we found that scaling values to the range        worked to
optimize performance while keeping the range wide enough
so that rounding errors did not affect our model  other ranges
where considered but all performed worse or the same  a
linear scaling method from the original values to a set      
inclusive was chosen 

s
fig    

sample parameter distributions 

a  definitions
for the purpose of this paper we define a feature vector
x i   r   denoting a specific chip and a corresponding value
of y  i          denoting quality of the chip  where   means
 i 
the chip is determined to be defective  we define output y   
 i 
       as the predicted label of the sample x   additionally 
we define per chip wafer assignment as a column vector w 
where w i               n   is the wafer number for chip i 
iv  p rocess overview
the overall process of developing and applying the learning
algorithm can be represented in a series of successive steps that
normalize and prepare the data  find an appropriate training set 
and fit the data  figure   represents this process schematically
and subsequent sections provide more insight into details of
each step of the process 
the final outcome of the process is a model suitable for
detecting low yield wafers  where the yield threshold  can
be varied at the expense of accuracy 

fig    

data modeling process 

 i 

 i 
xj

xj  min xj  
  

j

max xj    min xj  
j

j

vii  s upport v ector m achine u tilization
various machine learning algorithms were attempted to
model the data  including linear regression  logistic regression 
and support vector machines  utilizing principal component
analysis  the dimensionality of the problem was reduced to  
and   dimensions  and the visual representation of this data
set  see figure    figure    provided compelling evidence that
the data under consideration was non linearly separable even
in fairly high dimensional spaces  therefore  svm with nonlinear kernels tended to perform better and was chosen as a
modeling algorithm for this application 

v  data f iltering

t

est measurement errors cause some vco registers to
report back erroneous values  in some cases the patterns
fail to initialize the test correctly  which results in a register
value of    in other cases the register fails to clear  resulting
in a value well outside of the expected range  since our data
set is so large          test cases  and failing measurements
do not provide any information about the quality of the chips
themselves  we removed chips with values of any individual
feature being equal to   or outside the range of     of the
distribution of measured values 
x     x  x   i           m  xi        xi  i     i  
some wafers had systemic problems that required no additional screening  in these cases some fab process caused all but
a handful of parts to fail gross functional testing  continuity or
power shorts  and had no vco values to report  in these cases
the remaining chips  usually less than     of the wafer  all
failed  since these skew our per wafer classification and have

fig    

 d representation of chip data  red represent failing chips 

a  kernel selection
a number of kernel functions were attempted to model the
data with following conclusions 
 linear kernel suffered from large bias and tended to
underfit the data regardless of the training set size 
causing both training set error and generalization error
to be large

fiearly defect identification of semiconductor processes using machine learning  december     





polynomial kernel suffered from large variance  which
caused it to overfit the training set data and perform very
poorly  worse than guessing  on a larger test data set
radial family of kernel functions  such as gaussian 
exponential  laplacian  tended to perform better overall
due to the nature of the data  the gaussian rbf kernel
was chosen because it provided an optimal balance of
sensitivity to function parameters and runtime performance
k x i    x j      exp   x i   x j       

the gaussian kernel maps the input space to an infinitedimensional feature space on which the svm algorithm is
used to perform a search for a separating hyperplane      this
allows us to deal with the non linearly separable data in the
original feature space  the tightness of the fit of the hyperplane
is controlled by the parameter  as well as the standard svm
mechanism c  then  the classification function becomes 
n
x
f  x    sgn 
yi i exp   x  xi         b 
i  

b  parameter selection
to deal with the non linearly separable data we introduce
a cost factor c that allows the algorithm to deliberately mis
classify examples while paying a premium for doing so  this
allows the maximization of the margin while at the same
time ensuring that as many examples as possible are classified
correctly 
in most cases the test data will be disproportionately split
between good samples and bad samples  with bad
samples accounting for roughly        of the data  the initial
assumption was that utilizing a split cost factor would be
beneficial in improving accuracy of our overall algorithm  we
defined c    c   such that our original svm problem was
modified 
n 

n

x
x
 
  w      c 
i   c
i
y w b  
i
i
min

so that the positive and negative examples are separated for
classification  we saw an increase in precision recall values of
the algorithm both on the training set  via cross validation 
and the test set  however  overall accuracy of the algorithm
suffered  negating any improvements that may have been
achieved 
an iterative approach was used for selecting the best values
of parameters  and c  figure   shows a plot of achieved
accuracy as a function of the svm parameters 
a smaller training set of      data points  with k fold cross
validation utilizing   disjoint subsets  was used to determine
projected algorithm accuracy  the maximum accuracy was
achieved at the values shown in figure   
c  feature selection
our original data set includes readings from    different
vcos on the chip  which implies that each element of training
and test set is a vector in r     two approaches to reducing

fig    

algorithm accuracy as a function of parameters   c


      
fig    

 

c
  

reported accuracy
      

maximizing parameters for gaussian kernel svm algorithm

the dimensionality of the problem were considered for the
purposes of speeding up the computational runtime as well as
improving overall accuracy of the algorithm  we know that the
input vectors actually represent data about various subsections
of a chip  with distinct sub groupings of    elements repeated
  times  such that elements can be broken up into the following
set of sets 
xg     x        x    
 x         x    
 x         x    
 x         x    
 x         x     
since the groupings of elements describe specific areas of
the chip in a repetitive manner  it seemed reasonable to utilize
principal component analysis to reduce the dimensionality of
the problem to   dimensions  corresponding to each area of
the chip   the pca reduction was performed by first finding
a unit length vector to satisfy the following condition for each
subset 
 
m
  x  i   j  t
t
usn   arg max u
x x
u
 
u
m i   sn sn
where xsn is a    dimensional subset of each element x
starting at element index s  then the reduction was repeatedly
performed on each subset 
x i    usn   xsn  
the resulting   dimensional subset was used in the same
svm training strategy as described earlier  the expectation
was that using pca in such a way would naturally decrease the
amount of noise generated by discrepancies in measurements
across chip subsections and improve accuracy as well as
improve runtime by reducing the size of the classification
problem  however  in practice  reducing dimensionality in
such a way actually hurt overall accuracy of the algorithm 
leading us to believe that discrepancies between measurements
within one subsection of a chip are indicative of the overall
quality of the chip 

fiearly defect identification of semiconductor processes using machine learning  december     

 

having determined the independence of data points within
physical chip groups we have attempted to perform independent feature selection via a forward search algorithm  a k fold
cross validation on a sample size of       chips was used
with optimal parameters found earlier in the process to select
a subset of features that would eliminate noise and provide
better accuracy  the parameters were also varied slightly for a
number of independent runs of f search algorithm to account
for the potential need for adjustment  we have discovered that
although the nature of the features is repetitive  each   th
vco repeats the same measurement on a different subsection
of the chip  there is no optimal subset of features f   smaller
than the original set that produces better accuracy 

arg max
 

f f

m
x

 
  svm xf    

 i 

 i 

 y  

 f

i  

where xf   denotes an input sample x with only features
f   selected and svm x  denotes a prediction made by our
algorithm 

d  training set size selection
our final consideration for the model is the optimal size of
the training set  in the context of this study the impact of the
training set size on algorithm performance is a very important
consideration because it determines how soon into production
of the new generation of chips we can achieve acceptable
accuracy on waver yield predictions  our intuition is that the
training error and generalization error should converge at some
optimal training set size  to verify our assumption we can
iteratively determine training error and generalization error for
various sizes of training sets using the optimal parameters and
features  figure   is a graphical representation of the training
set and test set error as a function of the training set size 

fig    

wafer yield distrobution

viii  p er wafer c lassification
ur original goal was a per wafer classification model
to identify wafers to be tested with the more intensive
screening process  with this in mind we now classify the chips
by wafer and look for yields that fall below some set threshold
   looking at the distribution of yield per wafer figure    we
see a bimodal nature in the distribution  the mean     of the
lower distribution is the set were interested in identifying 
these are wafers where yield fell outside the normal distribution and beyond an acceptable production level  from this
we can chose a boundary point for  to be            
 j 
defining an equation for wafer yield on of the test set wr
 j 
and wafer yield as predicted from our svm model wp  

o

wr j 

wp j 
fig    

  

  

  pk

  y  i       w i    j 
 
pk
 i    j 
y i     w

  pk

  y  
pk

 

y i  

 i 

y i  

y i  

training set and test set error by training set size

     w i    j 
  w i    j 

 
 

we can then define an equation for accuracy of our model
by wafer as 
it is evident from the graph that increasing the size of
the training set past        elements does not generate
significant change in accuracy  it is clear that this training
set size provides optimal balance between accuracy of the
algorithm and the computational resources needed to run it 
another important implication of the figure above is that
there is little benefit to implementing this system using online
learning  since large amounts of extra training data does not
provide additional accuracy for algorithmic predictions 

accuracy  

n
o
  x n  j 
  wr   wp j 
n w j  

using this a plot of the accuracy as the split point classification  is changed can be generated  figure    which shows
our best accuracy with the highest split point to be       
this matches out desired and expected split point based on
the bimodal nature of the data 

fiearly defect identification of semiconductor processes using machine learning  december     

fig    

accuracy by the split point   

ix  c onclusions
n a per chip basis our model does not perform well
enough to be effectively used in production  this result
is not surprising  considering previous attempts to use the
data to identify individual defective chips by searching for
distribution based indicators  however  using the svm model
in conjunction with wafer level classification  the accuracy
of prediction of low yield wafers can be as high as     
additionally we can develop a method of scaling projected
yields at the expense of accuracy  so higher yield wafers can
be identified as well  given this algorithm  wafers with high
rate of defects can be isolated and screened for fabrication
defects resulting from machine calibration issues  this level
of early classification assists in identification of process defects
and possible machinery issues  which in the past could only
be identified after additional screening  which results in cost
savings of several days of production per identified wafer 

o

r eferences
    boser  b e   guyon  i m   and vapnik  v       a training algorithm
for otimal margin classifiers  fifth annual workshop on computational
learning theory  pittsburgh  acm        
    chih chung chang and chih jen lin  libsvm   a library for
support vector machines  acm transactions on intelligent systems and technology                     software available at
http   www csie ntu edu tw  cjlin libsvm
    scholkopf b   sung k   burges c   girosi f   niyogi p   poggio t   vapnik
v   comparing support vector machines with gaussian kernels to radial
basis function classifiers  mit      
    t  joachims  making large scale svm learning practical  advances in
kernel methods   support vector learning  b  schlkopf and c  burges
and a  smola  ed    mit press       
    k  morik  p  brockhausen  and t  joachims combining statistical learning
with a knowledge based approach   a case study in intensive care
monitoring  proc    th intl conf  on machine learning  icml           

 

fi