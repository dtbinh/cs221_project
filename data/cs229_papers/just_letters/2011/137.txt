predicting edit locations on wikipedia using revision history
caitlin colgrove  julie tibshirani  remington wong
december         

 

introduction

 

contributions

there has been little to no published work on the
task of recommending sections in a wikipedia article  moreover suggestbot did not leverage machine learning in making its predictions  therefore our main contributions lie in formulating
the problem  preparing the relevant data  and
exploring a plausible machine learning approach
based on the users edit history  our final contribution is perhaps negative  the hypothesis that
a users edit history will help us recommend sections does not seem to hold  we ultimately find
that while revision history may help in predicting
what article a user chooses to edit  once a user
has settled on an article  the features of the section itself are the biggest predictors of whether
he will edit that section 

there has been increasing interest in the machine learning community in automatic task
design 
in a collaborative problem solving
setting  how can we best break up and assign tasks so as to optimize output  huang
et al   for example  considered the problem
of effectively assigning image labeling tasks
to amazon mechanical turkers      in the
realm of wikipedia prediction  cosley et al 
created a successful system for recommending
articles a user might be interested in editing
     nicknamed suggestbot  the system makes
use of co editing patterns and articles the user
has previously edited to predict new articles of
interest  this system represents a success in
automatic task design  deploying suggestbot
increased a users edit rate an average of fourfold 

 

we consider a complementary problem in
wikipedia task design  given that a user has
chosen an article to view  what sections would
he be most interested in editing  our hypothesis
is that  similarly to article recommendations  a
users personal edit history can help determine
which sections he will be most drawn to in
the future  when viewing an article about the
united states  for example  a politically oriented
user might hone in on sections about the government as opposed to cultural or geographic
information  wikipedia edit history has already
shown to be useful in other contexts such as
predicting the semantic stability of articles    
    and in the task at hand  it seems to provide
a rich source of personalized training data 

data

using wikipedias api  we downloaded the
entire revision histories of five wikipedia users 
hammersoft  jerryorr  sf     tangledorange 
we chose these
and the egyptian liberal 
users because they had substantial edit histories
and because they have been editing articles
recently 
these revision histories contained
the diff between the edit and the previous
content of the article  as well as information
about the text immediately surrounding the
revision  we also downloaded the entire text of
the    most recently edited articles for each user 
from the revision information  we needed
to determine the section that the user had
 

fiedited in each testing article  first  we parsed
the articles into sections using the    separator  we also had a special regular expression
designed to capture the infobox as a single
section  we then tokenized the text of the
revision using the stanford nlp parser     and
compared it to each section using the jaccard
similarity coefficient to pick the best matching
section  because wikipedia articles can change
a great deal over time  we used only very
recently edited articles in an attempt to ensure
that there would be a well matching section 

mean of the product of probabilities  we also
experimented with changing the prior  instead
of assuming a uniform distribution over sections 
we tried modeling edits as being uniform over
words  the new prior is then given by the
number of words in the section divided by
the total words in the document  concretely 
let ni denote the number of words in the ith
section and t be the total number of words in
the document  then our prediction for each
document is given by
c   arg max
i

 

ni
   
ni
log p word j    log
ni
t
j  

methodology

these probabilities are calculated using the usual
maximum likelihood estimates  letting m represent the number of revisions  ri be the words
in revision i  and v be the vocabulary 
 m
i   iword j ri   
p word j     
m
i    ri      v  

multinomial naive bayes our fundamental
model was a multinomial naive bayes model
over a single users entire revision history 
using individual tokens as features and different
sections of a test article as prediction classes 
we chose to only use text that the user explicitly
added or deleted for reasons explained below 
we also needed to make some modifications
specific to our data  first  since revisions are
typically very short  we could not obtain a
reasonable vocabulary only from training on
revisions  this sparseness of data made smoothing difficult  to remedy this  we chose the
vocabulary to be all words present in the most
recent ten documents  excluding the one in the
test set   along with an additional unknown
token for words that did not appear  we then
applied add alpha smoothing of     to the entire
vocabulary 

  our baseline is based on this new prior  and
simply predicts the longest section in the article 
mutual information related to our preference for short sections  we hypothesized that the
likelihoods for long sections were being swamped
by uninformative words  so to improve accuracy
and correct for length of sections  we introduced
a mutual information metric to select features 
more formally  the mutual information for word
j is given by

mi word j   

to make a prediction  our initial classifier
simply calculated the likelihood of each section
given the probabilities of the words it contained 
we then ranked sections by likelihood and
output the ranks 

   

ww cc

 

   

ww cc

p w  c  log

p w  c 
p w p c 

p w c p c  log

p w c 
p w 

where w    present  not present  and c  
 revised  present in a document but not revised  
we chose the top n tokens most predictive of
whether or not a section would be edited  as determined by the mutual information calculation

our initial results showed that our classifier overwhelmingly favored short paragraphs 
we therefore introduced length normalization
to all of our models  taking the geometric
 

fi we found n        to be reasonable   using
considering a success to be ranking the corthis feature selection  we recalculated the naive rect section in the top three  our algorithms perbayes scores  but this time only considering the formed very well  see table   however  the inmost predictive tokens 
formed baseline  just predicting sections based
on their length  was the largest contributor to
context and document weighting be  this success  in most cases  just this baseline was
cause revisions are so sparse  we experimented able to successfully predict the section edited in
with including two other types of text  the revi  eight out of the ten test documents  in two out
sion information we downloaded contained both of our five users  this baseline gives arguably the
the lines actually edited and a few lines sur  best results of any method we tried  see figure
rounding the change  we experimented with in      naive bayes did improve somewhat on this
cluding this edit context to increase the amount baseline for two of the other three users  see figof training data  we also experimented with ure    and matched it on the third 
using the entire article to calculate probabilities  introducing a discount factor  for words
that appear in the article but not in the revised
paragraph  after some experimentation we set
         letting di represent the document associated with revision i  our new maximum likelihood estimates become
 m
iword j ri   iword j di   
p word j    i   m
i    ri      di      v  

testing we trained separate models for each
of our five users  to test our results on a particular user  we performed leave one out cross validation on the users ten most recently edited documents  we then outputted the predicted ranking
of the section of the left out article  to evaluate
our ranking  we looked at the rank of the highest ranked section that was actually edited by
the user  the great majority of the time there
was only one  but occasionally there were many
edited sections  

 

results

while we ran many experiments on our data  we
can represent the important information through
the results of five different classifiers  our longest
section baseline  pure naive bayes  naive bayes
feature selection  naive bayes using context and
downweighted document text  and finally the
best out of the previous three with our new prior 

after noticing that some documents were quite
short  we ran several trials in which we chose the
ten most recent documents with ten or more sections  and we saw that the results were significantly worse than the results on the long documents in our original training set  however  because finding these ten longer documents meant
 

fiuser
jerryorr
hammersoft
sf   
the egyptian liberal
tangledorange

informed
baseline
   
   
   
   
   

nb
   
   
   
   
   

nb
with mi
   
   
   
   
   

nb with
full context
   
   
   
   
   

best with
prior
   
   
   
   
   

max
   
   
   
   
   

average  
of sections
    
   
    
    
    

table    fraction of test documents in which the edited section was ranked in the top three 
using much older revisions  the accuracy of our self have the most predictive power  indeed  we
matching was worse  and thus likely negatively achieved a startlingly good performance with our
impacted the overall prediction 
baseline using a single  simple feature of each section  these observations also help explain why
adding the context around an edit and using the
  error analysis and discus  full text of the revised document did not significantly improve our results 
sion
while most attempts to deal with length probour classifier performed well on all users  but lems had some beneficial effect  using mutual indifferent versions performed better for differ  formation did not seem to produce the same iment users  for example  naive bayes worked provements  using only a few features seemed
quite well on hammersoft because his edits fol  like a natural way to make sure that all the
lowed a distinct trend  he often focused on words being counted were of high quality  howcorrecting factual information in infoboxes and ever  this approach suffered from the fact that
taking down images that did not comply with our training data was already sparse  and so diswikipedia standards 
this meant that to  carding terms meant losing some of the little inkens common in infoboxes and image links were formation we had  as a result of this sparsity 
highly predictive  indeed  the top features se  if a paragraph had none of the words we deterlected by mi for hammersoft included the in  mined to be important  it would be automatifobox markup tokens       plus  png  
cally ranked very low even if it had a significant
on the other hand  while sf    frequently number of moderately informative words 
edited articles about software  he did not display
a preference as to which sections he edited  for
users like these  it seems as though modeling the   future work
overlap in content between a users previous edits and the test section is not the best approach  our error analysis strongly suggests that many
while using this overlap to predict what arti  users do not edit for content  and accounting
cles a user will edit may be effective  the sec  for this difference in editing patterns could
tions within an article are already semantically significantly improve accuracy  one plausible
similar  it is very difficult to capture content in interpretation is that some users are copy
such a fine grained way so as to make accurate editors while others are content editors 
distinctions between sections  moreover  our ex  it seemed that most of our users  especially
perience with the data suggests that most users sf     were copy editors and simply focused
might not even edit based on their interest in the on paragraphs that were poorly written or not
content of the particular section 
well cited  while a few users like hammersoft
rather it seems that features of the section it  had a distinct preference for editing tabular and
 

fivisual content  adding features of the section
itself such as the previous number of additions 
deletions  and revision reverts  the number of
distinct users that have edited the section  and
perhaps some sort of controversy rating would
greatly improve accuracy on our copy editors 
 indeed  the logistic regression classifier we built
in the milestone used only the revision histories
of particular articles  and demonstrated that
these features can be highly predictive of the
attention a piece of text will receive  

computation  new york  ny  usa       
    d  cosley  d  frankowski  l  terveen 
and j  ridel  suggestbot  using intelligent
task routing to help people find work in
wikipedia  in proceedings of the   th international conference in intelligent user interfaces 
honolulu  hi  usa       

    c  thomas and a p  sheth 
semantic convergence of wikipedia articles 
in
ieee wic acm international conference on
a more sophisticated model would posit a web intelligence  fremont  ca  usa       
latent binary variable that represents the users
editing style  based on the value of this variable      e  yamangil and r  nelken 
mining
certain features would be weighted differently  wikipedia revision histories for improving
for content editors  the users revision history sentence compression  in proceedings of the
would play a larger role as opposed to copy   th annual meeting of the association for
editors  for whom features of the section itself computation linguistics on human language
technologies  short papers  stroudsburg  pa 
would become more important 
usa       
elaborating on this idea of user profiling 
we could first use clustering to group users by     d  klein and c  manning 
accurate
similar editing behaviors  and use the revision unlexicalized parsing  in proceedings of the
histories of similar users to help predict other   st meeting of the association for computausers in the cluster  such clustering would also tional linguistics       
help solve the problem of sparse revision text 
    m  pennacchiotti and a  popescu 
finally  we spent a good deal of time on democrats 
republicans 
and starbucks
data collection and there are a number of logis  afficionados  user classification in twitter  in
tical issues that could be resolved  in particular  proceedings of the   th acm sigkdd internain our project we matched each revision to a tional conference on knowledge discovery and
corresponding section in the latest version of the data mining  new york  ny  usa       
article  which forced us to consider only recent
revisions in the testing set  for future work  we
would expand our testing set by downloading
the actual version of the article that the user
saw when making the particular edit 

 

references

    e  huang  h  zhang  d c  parkes  k z 
gajos  and y  chen  toward automatic task
design  a progress report  in proceedings
of the acm sigkdd workshop on human
 

fi