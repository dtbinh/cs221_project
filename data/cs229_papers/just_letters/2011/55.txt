 

sentiment analysis of occupy wall street tweets
robert chang  sam pimentel  alexandr svistunov
acknowledgements
richard socher  andrew maas  and maren pearson 
i  introduction

t

he rise of social media has changed political discourse
around the world by extending the potential reach
of otherwise marginal voices and creating opportunities
for massive group coordination and action  the arab
spring protest movements of the past year demonstrate
the political power of these tools  a survey by a news organization in the region reported that the vast majority
of      people surveyed over three weeks in march said they
were getting their information from social media sites    
per cent in egypt and    per cent in tunisia      
as the twitter stream becomes richer in information and
more significant in political impact  the value of monitoring and understanding the stream increases  the united
states department of homeland security  for example 
recently announced its intention to develop a system for
collecting intelligence information from twitter and facebook      the immense quantity of the data available
makes identifying key trends non trivial  however  as department of homeland security undersecretary carolyn
wagner comments  were still trying to figure out how
you use things like twitter as a source      how do you
establish trends and how do you then capture that in an intelligence product  human observers  inundated by thousands of tweets  need computational tools to aid them in
analysis of social media data  while application programming interfaces allow easy automated collection of large
social media datasets  analysis of these datasets remains
difficult  simple keyword searches can help identify topics
discussed in social media posts  but much of the value of
these data lies in the posters opinions about these topics 
encoded in their messages but difficult to extract computationally 
fortunately  machine learning offers a potential solution
through the field of sentiment analysis  a sentiment analysis algorithm seeks to identify the viewpoint s  underlying a text span     by extracting descriptive features
from text fragments and using them as inputs to a learned
hypothesis function  such algorithms have already been
used to classify opinions on current events as expressed in
news sources      asur and huberman applied sentiment
analysis to twitter data to forecast box office revenue for
movies with competitive accuracy      by training such an
algorithm to recognize specific political sentiments of interest rather than opinions about movies  an observer could
predict the future of relevant political movements much
as asur and huberman predicted market behavior     
in this investigation we apply machine learning methods to analysis of political sentiment in social media  we

concentrate on a specific political phenomenon  the occupy wall street movement  and a particular social media
platform  twitter com  furthermore  we restrict ourselves
to classifying postings into three categories  pro occupywall street  anti occupy wall street  and neutral or unrelated  we apply a series of machine learning techniques
to these data 
ii  methodology
a  data collection
to gain access to tweets associated with the ows movement  we leveraged the python scripting language and its
existing python twitter api library  from early november to early december  we collected the daily most recent
     tweets  which is roughly the rate limit per call  on
search twitter com  we stored information such as screen
names  geo locations  and texts were stored on the redis
server  before any pre cleaning  our sample size was a little
over         tweets 
b  assigning responses to observations
given the difficulty of assigning sentiment values to text
computationally  we relied on a consensus vote by several
human judges to determine response values for the observations  we used the amazon mechanical turk platform
to collect these votes from users over the internet  who
completed questionnaires requiring them to classify our
observed tweets as pro occupy wall street  anti occupywall street  or neutral irrelevant in exchange for small
monetary rewards  to help control for random guessing 
we randomized the order of the response buttons on the
online forms  each tweet was offered to five independent
judges  and the mode of the resulting votes was taken as
the consensus value  when a set of votes had multiple
modes  i e  when a       split occurred among the votes 
we concluded that the tweet was likely to have ambiguous
sentiment and classified it as neutral 
unfortunately we obtained low quality results from
amazon mechanical turk  even with a five vote consensus
system  we discovered many egregious misclassifications
while manually checking a small sample of our results  in
addition  we discovered by examining the vote breakdown
that few of the tweets had unanimous or even strong majority voting results  figure   shows a histogram of the
percent of voters agreeing with the assigned consensus label for each tweet  e g  a     figure means   of the   voters
agreed on the label  for a batch of      tweets  clearly only
a tiny fraction of the labels received a unanimous vote  and
less than     received four or more agreeing votes out of
five  in contrast      of the tweets received only one vote
for the top label  a case that occurred only when the vote
distribution had multiple modes  although some of the

fi 

tweets in our data set were genuinely ambiguous  this excessive level of disagreement suggested to us that an influential subset of the amazon mechanical turk workers who
had voted had done so either negligently or with insufficient basic background knowledge of occupy wall street
and or of english to distinguish sentiment on the issue 
in response to these results  we chose to discard the
amazon mechanical turk labels for all our tweets and assign new  accurate ones ourselves  although this approach
limited the size of our dataset to the number of tweets we
could assign labels to  we already faced similar limits when
using amazon mechanical turk due to constraints on the
financial resources needed to pay workers  in addition  our
self labeled dataset contained much more trustworthy labels  the resulting set of labels for      tweets was    
positive      negative  and     neutral or irrelevant 

d  feature creation
in the next step  we used regular expression to remove
non expressive characters but kept emoticons such as      
       and twitter special characters such as hashtag   and
direct messaging    furthermore  we removed all urls in
the tweets  although the presence of urls could be later
encoded as an indicator variable  we then tokenized the
string data  removed the stop words  and applied porter
stemming using pythons n lt k library 
in the next step  we performed frequency analysis on
single tokens  bi grams  and ngrams to identify popular
words and remove rare or short tokens      appearances
and     characters long   beyond simple freqency analysis  we also computed common subsets of tokens across
sentiment category groups  this information was useful
since if a high frequency token only appeared in one sentiment category  it indicated that this token was important
for identifying sentiments  on the other hand  if a frequent
token appeared across all different sentiment group  then
its widespread appearances were less helpful and could be
discounted  this intuition is related to the rationale behind  tf idf  which tries to identify important tokens
based on information on the response labels 
e  term document matrix

fig     histogram showing percent agreement with consensus label
for each tweet among amazon mechanical turk voters 

finally  we created the term document matrix and applied the tf idf transformation  in particular  we created
two versions of the document matrix  one that took frequency into account while the other kept track of presence
only  pang et al      found out that  unlike topic categorization  repeated use of certain keywords usually does
not indicate or help to identify strong sentiments  finally 
some tweets contained no features at all after parsing  e g 
tweets such as occupy wall street  that only contained
search query   and these were also removed 

c  data cleaning
as we scrutinized the raw data more closely  we realized how diverse and noisy the data were  which meant
additional data filtering was required  this was particularly true because we wished to focus on tweets expressing a strong opinion about ows and not just containing the search query occupy wall street  even longer
tweets expressing opinions that mentioned ows were not
all relevant  for example  during final exam week in december  we came across many tweets in which students
complained about having difficulties in finishing papers on
ows  rather than having direct opinions on the movement
itself 
in addition to removing non english and redundant
retweets  we made the major decision to remove ows related news tweets  arguably  news organizations are sometimes biased when reporting  e g  fox news    but many
news tweets simply update current events rather than reflecting opinions of its own or the general public  with this
rather aggresive filtering  we are left with only          
tweets  of the original tweet  but they tend to be relevant
and carry high sentiments 

iii  static classifier
the static classifier was implemented to serve as a benchmark for other classifiers  since it was expected that this
type of algorithm would perform badly for the type of data
each tweet contains 
the static classifier was implemented using the afinn    dictionary of valence rated english words developed
specifically for microblogs        the dictionary contains
     words  some of the words are grammatically different versions of the same word  for instance favorite and
favorites are two different words   the valence is an integer value from    to    with   being the most positive
connotation     the most negative one  
each tweet was then assigned a score as follows  for each
tweet in the sample  its score was the sum of the scores of
the words in it  if a word did not appear in the dictionary 
its score was taken to be   
the next part was to classify tweets according to the
score they obtained  while classification into two groups
would be straightforward  for instance  all non zero scores
would go into the appropriate category  with zero rated

fisentiment analysis of occupy wall street tweets

tweets being assigned randomly to one of the categories  
expansion to three categories requires a choice of bounds
 u  l  such that a tweet is assigned to a positive category
if score  u  to the neutral if l  score   u  and to the
negative category if score   l  the bounds were chosen to
maximize total accuracy on the largest balanced sample 
the bounds came out to be u      l      the overall accuracy of the algorithm for the largest sample size came
out to be          the sensitivity for the groups was     
for positive tweets       for negative ones  and      for neutral ones  as we can see  not only was the overall accuracy
small  but it was also achieved by very good performance
on one particular group of tweets  neutral  at the expense
of the others 
there are several possible reasons for poor performance
of the static classifier  first  the dictionary is fixed and limited in size  any word  no matter how emotionally charged 
will go undetected if it does not appear in the dictionary 
a related issue is that the dictionary does not pick up
twitter specific features like hash tags  which are picked
up by other algorithms 
second  the classifier is unable to pick up subtle linguistic constructs such as sarcasm  euphemisms  double entendre  etc 
iv  nave bayes algorithm
the model we implemented after the data gathering step
is a slightly modified version of the nave bayes model
with laplace smoothing  the modification is due to the
fact that we are using three categories instead of two in
the response variables 
let yi     for a positive tweet  yi     for a neutral tweet 
and yi     for a negative sentiment  and let j y k be the
probability of a jth token appearing in a tweet given that
the sentiment is k  k             let ni be the number
of words in a given tweet  and xji the number of times
token j appears in tweet i  then  the maximum likelihood
estimates of the probabilities become 
pm j
x   xji      yi   k 
pmi
j y k   i  
i   ni   yi   k 
pm
  yi   k 
y k   i  
m
whenever we want to make a prediction on a new tweet 
the probability of the new tweet having sentiment k is as
follows 
qn
j
j   p x  y   k p y   k 
p y   k x    p 
qn
j
k     j   p x  y   k p y   k  
the overall leave one out cross validation accuracy of
this algorithm when applied to the full data was       
v  support vector machine
in addition to nave bayes  we fit a support vector machine with a linear kernel to our data using the liblinear package      in order to handle   way classification 

 

liblinear fits   separate support vector machines  one to
classify tweets as being in or out of each of our possible
label categories  the leave one out cross validation accuracy achieved by this model on the full data set was       
slightly worse than random guessing 
vi  stanford classifier
the stanford classifier is a particular implementation
of a maximum entropy classifier for text classification  a
maximum entropy classifier is equivalent to a multiclass
logistic regression model      this implementation  written
in java  takes as inputs the response variables  as well as
the tweets themselves  in the trainng set  creates a set of
features according to the input parameters  and then tests
the performance of the model on the test set 
the stanford classifier model which used n grams consisting of letters showed very good performance  the overall accuracy for the largest possible training size was     
with the sensitivities for the three groups being       for
positive tweets        for negative tweets  and      for neutral tweets 
the stanford classifier achieves its high level of accuracy
by using n grams of letters along with individual words 
this expansion of the feature space has drawbacks  while
it improves performance  the high influence features it selects for a large role in prediction are no longer emotionally charged words but mostly meaningless text fragments 
nevertheless  for classification purposes  the stanford classifier is the best algorithm among those we tried  judging by overall accuracy and the sensitivities for different
groups 
vii  results
the two plots show the results of the four classification
methods tested  figure   shows test error plotted against
training sample size for each of the four algorithms  note
that leave one out cross validation error was used for the
static  nave bayes  and support vector machine models
but that the stanford classifier  which does not have a
built in cross validation method or an interface conducive
to performing cross validation  was tested against a heldout test  composed in large part of tweets discarded while
balancing the data  figure   contains four subplots  each
of which shows conditional accuracy in each label category
for one of our algorithms  plotted as a function of training
sample size  conditional accuracy is defined as the percentage of the tweets sharing a certain true label assigned
to that label by the classification algorithm  as such  it is
an adaptation of the concept of sensitivity that is applicable to three way classification 
contrary to expectation  few of the plots show a strong
decrease in test error with increasing training sample size 
support vector machine and stanford classifier performance do appear to improve slightly with the largest training samples  however  and we conclude that our small sample size prevents us from seeing the larger trend  we note
that although support vector machines can often give excellent performance on classification problems  our small

fi 

fig     test error for the algorithms for different training set sizes

sample size greatly reduces the accuracy of the linear kernel svm  which has the highest cross validation error of
any algorithm we tested  including the static classifier 
the sensitivity plots demonstrate some of the subtleties
of the different models  in particular  the static classifier achieves excellent performance on neutral tweets by
classifying almost every tweet as neutral  however  this destroys its conditional accuracy in predicting positive and
negative tweets  assuming we are interested in identifying
and distinguishing positive and negative tweets  the static
classifier is a terrible choice  worse in fact than the svm
although its cross validation error is lower  thus all our
machine learning algorithms outperform a static classifier
in areas of interest 
static classifier sensitivity results

naive bayes sensitivity results
   

   

conditional accuracy

conditional accuracy

   

   

   

   

   

   
   

   

   

    

    

    

    

    

   

   

    

    

    

data size

data size

svm sensitivity results

stanford classifier sensitivity results

   

    

in addition to performance measurements  we obtained
lists of high influence tokens from our nave bayes and
support vector machine models  for nave bayes  we selected the tokens with the highest conditional probabilities
of appearing given a tweet was positive and given a tweet
was negative  respectively  and for the svm we extracted
the tokens with the highest positive weights for each of the
two categories  the results are shown in the table  in the
column headings  nb indicates nave bayes    indicates
these tweets are associated with pro ows tweets and  
indicates that they are associated with anti ows tweets 
nb  
nb svm  
svm movement
dont
banker
nba
 occupywallstreet movement demand
strike
video
like
corrupt
union
music
get
greed
riot
tweet
f   
show
move
 xfgk k
would
cool
clue
support
im
 gt
miley
dont
street 
interview opinion
get
think
might
camp
looking at the tabular results  we notice that some of
the tokens the nave bayes finds likely to occur in positive
tweets are also likely to occur in negative tweets  dont
and movement  for example    this suggests these tokens
are simply very frequent and are not necessarily indicators
of positive or negative sentiment  in contrast  the tokens
with high positive weights for a given category in the svm
do tend to be good discriminators  while the lists contain
some tokens that are likely mere artifacts of small sample size  its unlikely that the token miley  taken from
twitter references to miley cyrus  would be a strong indicator of anti ows sentiment in general   others show us
interesting things about sentiment expression in twitter 
for example  one of the tokens with the highest pro ows
association in the nave bayes was the hashtag  occupywallstreet  suggesting  as we might expect  that hashtags can have strong sentiment value separate from that
of the words themselves  in addition  the negative association of the token street  points to the importance of
punctuation marks in sentiment analysis  while the token
street occurred in every tweet in our data set as part of
the phrase occupy wall street  the addition of a question mark gives it a strong negative slant 

    

viii  discussion
in this section  we will briefly discuss ideas that we think
are worth pursuing to improve model performance 

   

   

   

conditional accuracy

conditional accuracy

   

   

   

   

a  data collection
   

   

   

   

   

    

    

data size

    

    

    

   

   

    

    

    

    

    

data size

fig     test error curves for each of the four algorithms tested  plotted against training sample size  all test errors were computed using
leave one out cross validation except for the stanford classifiers 

leveraging urls of news tweets  as mentioned
earlier  we filtered out news tweets  which are estimated to
be about     of the tweets we collected  aggresively  one
idea to leverage these news tweets is to crawl the actual
news article when their urls are available  this would
give us textual information beyond the     characters restriction on twitter  although we did not implement this
idea  we did use similar techniques to scrape the titles of

fisentiment analysis of occupy wall street tweets

news articles when urls are available  but we found out
that urls tend to expire quickly  and many titles were
highly correlated with the tweet contents themselves 
target specific accounts  an alternative approach
to obtain more data set without labeling the sentiments
ourselves is to target specific twitter accounts that are
mostly likely to have pro or anti sentiments  accounts
such as  occupytogether   ows contains tweets that
mostly supports ows  while others such as  anti occupier   antioccupyws   antioccupier are certainly to be
against ows movement  the caveat  of course  is to be
careful when selecting these accounts  since voices of these
individuals might not be generalizable 
b  feature creation   selection
feature creation  based on the bag of words technique  our features were generated directly from the training set  while this allowed us to build a more domain
specific dictionary  the feature set was often sensitive to
current events  for example  during the brief time surrounding miley cyrus appearance at the ows concert 
the token miley came as one of the top tokens  but it is
unlikely to be a significant sentiment indicator in the long
term  one way to produce a more robust dictionary would
be to keep track a frequency table for existing tokens  and
update the ranking as new tweets arrives 
feature selection  with the benefit of a largre data
set  we could enlarge the feature space using different
strategies and perform regularizations on the model  this
is in fact what the stanford classifer did  max entropy
classification with regularization on a large feature space  
techniques such as negation  attaching not to tokens
after negation terms   part of speech  considering only adjectives and adverbs   and co occurence constrastive
distance analysis  measuring how likely two tokens are
to co appear  could be helpful to replace independence assumptions like those made by the nave bayes algorithm 
c  re evaluation of sentiment definition
even among directly relevant tweets expressing strong
sentiment  classifying that sentiments can be difficult for
humans  for example  at the time when nba announced
the quarter of a billion salary for star players  many commented that occupy protesters were focusing on the wrong
target  and that they should be occupying nba instead of
wall street  are such tweets pro  neutral  or anit ows 
perhaps a re evaluation and clarification of the sentiment
definition is needed to provide a more consistent approach
in labeling such tweets 
ix  conclusion
in this current project  we have shown that even a basic model such as nave bayes trained on a small data
set outperform a non machine learning model  although
our model performance did not achieve the kind of performance that would be desirable in an industrial application  we have shown that with increasing sample size 
the accuracy would reach much better results  we also

 

propose methods for improving data size  quality  and introduce strategies for constructing better feature selection 
in addition  machine learning concepts such as similarity
measures  clustering  and cross validations were applied in
helpful ways throughout the project 
the explosion of social networks allows researchers to
gain great insights to social interactions through text based
conversation and discourse  in our project we learned to
deal with the noisy nature of social data and recognized
that natural language processing is an iterative process
that requires careful fine tuning  achieving high predictive accuracy ultimately depends on the design decisions
in data collection  data parsing  and feature selection 
references
    sitaram asur  bernardo a  huberman  predicting the future
with social media ieee wic acm international conference
on web intelligence and intelligent agent technology       
    p  solomon banda  homeland security reviews social media
guidelines  associated press  oct           
    r  e  fan  k  w  chang  c  j  hsieh  x  r  wang  c  j  lin 
liblinear  a library for large linear classification  journal of machine learning research                     software
available at http   www csie ntu edu tw  cjlin liblinear
    namrata godbole  manjunath srinivasaiah  steven skiena 
large scale sentiment analysis for news and blogs international
conference on weblogs and social media  boulder  co       
    carol huang  facebook and twitter key to arab spring uprisings  report  the national  abu dhabi  june          retrieved
november          from http   www thenational ae
    christopher manning  daniel klein  kristina toutanova  jenny
finkel  galen andrew  joseph smarr  chris cox  roger levy 
rajat raina  pi chuan chang  marie catherine de marneffe  eric yeh  anna rafferty  and john bauer 
stanford
classifier 
the board of trustees of leland stanford junior university            
distributed under gnu general public license 
retrieved december          from
http   nlp stanford edu software classifier shtml
    f  a nielsen 
afinn      list of valence rated english
words   informatics and mathematical modelling  technical
university of denmark  retrieved december          from
http   www  imm dtu dk pubdb p php     
    bo pang  lillian lee  a sentimental education  sentiment analysis using subjectivity summarization based on minimum cuts 
acl    proceedings of the   nd annual meeting on association
for computational linguistics  association for computational
linguistics       

fi