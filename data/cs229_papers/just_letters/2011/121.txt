cs      machine learning project december     

 

predicting news reader preferences
vinay raj hampapur  helen lu

index termsmachine learning  natural language processing 
svm  community detection  matrix completion  clustering 

i  i ntroduction

w

e we explore several techniques for predicting user
reads on pulse  a newsreader app for mobile devices 
accurately predicting reads could allow the application to
personalize newsfeeds for each user by presenting stories most
likely to be of interest 
our approaches to building a classifier for predicting user
reads include supervised learning with svms and several
methods of clustering 
a  dataset
pulse provided a month of data for      users  the data
contains all stories published during august      and all the
user reads and clickthroughs in the same month  a read is
defined as the users first click on a story  depending on the
feed  this leads to a page showing anything from a short blurb
to the full text of the story  the user then has the option for
a clickthrough  which shows the original story in a browser 
the dataset contained the following information on each
story  in a tab delimited text file with one story per line 
 url 
 title 
 feed name and feed url 
 timestamp of the storys first read 
the stories were keyed on  story url  feed url  
the files of user reads and clickthroughs contains of the
above  plus the id of the user doing the read or clickthrough 
the timestamp refers to the date and time of the users read 
ii  s upervised l earning
in our supervised learning approach  we train on stories and
reads in the first i    days  and use the model to predict for
day i  where i                each users prediction model is
built only on the users past reads  and does not incorporate
other users preferences 
the dataset contains about         stories in     feeds 
but each user subscribes to a small number of feeds  since the
users have no way to read stories from other feeds  we remove
those from consideration 
the simple separation of stories into    days is problematic 
since users  a  do not open the pulse app every day   b 
sometimes read stories from previous days  and  c  cannot
v  hampapur is with the department of electrical engineering  stanford
university  stanford  ca        usa e mail  vinayraj stanford edu 
h  lu is with the department of computer science  stanford university 
stanford  ca        usa e mail  helenlu stanford edu 
report submitted december          

see stories published between their last look at pulse and the
end of the day  we could simply not test on the days when a
user did not open pulse  but problems  b  and  c  still stand 
precision would be low  from including stories the user had no
chance to read  recall would also be low  from passing over
older stories that the user does read 
our solution defines day i as the period of time between
the users last read on day i    and the users last read on day
i  if the user does not read any stories on day i     the start
time for day i reverts to the users most recent read time on
an earlier day  and day i    is skipped in testing  while this
strategy might still omit the users reads of even earlier stories 
we do capture the stories that are new to the reader  in our
results  recall is much better than precision  which suggests
that including even earlier stories in the search space would
hurt precision more than it would improve recall 

a  features
the features for a given story are the tf idf of the words in
the title  the feed the story comes from  and time between the
storys publication and the users last read of the day 
each row in matrix x represents a story available to the
user  and each element in matrix y represents whether the
user read the corresponding story  since each user has different
feeds and different read times  x is unique to each user 
   story title  we expect that each user is interested in
particular topics  and the title is the most easily accessible
information about the storys topic  also  the title  or the
beginning of the title  is the only information visible to the user
before the user clicks on the story   note  perhaps we should
truncate long titles for this reason  but titles are truncated inconsistently  depending on whether the feed contains pictures 
our dataset does not include information on the length of title
visible  or whether pictures are included  
we use tf idf  term frequency   inverse document frequency 
to weight the words depending on their importance within
the title  tf idf is actually not very good at emphasizing the
keywords in most sentences  since popular keywords appear
in many titles  however  tf idf is very easy to implement  and
is useful for de emphasizing words such as articles  pronouns 
and common verbs 
we separated the titles into words and reduced the dimensionality of the data 
  
  
  
  
  

delimit by space and hyphen 
remove numbers and punctuation 
change all letters to lowercase 
apply porters stemmer 
replace words with a total word count of   or less
 across all titles  with the token unk 

fics      machine learning project december     

after the first   steps  there are over        unique words 
porters algorithm and the elimination of rare words reduces
the number of unique words to        
when computing tf idf and eliminating rare words  we use
the entire month of stories as the corpus  rather than restricting
to earlier stories  this choice makes implementation simpler 
and does not give an unrealistic performance result  in a realworld application  there would be sufficient past data to have
a meaningfully large corpus 
   story feed  the next section of the x matrix consisted of
    columns  each representing a feed  an element   in row i
and column j indicates that story i came from feed j  including
feeds as a feature significantly improves performance  because
users favor some of their feeds over others 
   story timing  the last column of x contains the length
of time in minutes between the storys timestamp and the
users last read of the day  we concatenate multiple days
of stories together when the user does not access pulse for
multiple consecutive days  so we also need a way to discount
earlier stories  pulse groups stories by feed  and orders stories
within a feed chronologically  with most recent first  the
number of displayed stories per feed is limited  so the user
does not see old stories in prolific feeds 
we used time difference to favor newer stories because it
was easier to implement correctly  since the real determining
factor of a storys visibility is the number of newer stories in
the same feed  that would be a better feature 
also  a user might access pulse multiple times a day  and
we use time difference to the last read  a more advanced
implementation could instead calculate the time difference to
the next read 
   other  we explored incorporating the clickthrough data
by increasing the tf idf of the words in the clicked upon
stories  the hope was to weight those stories more heavily 
because the user showed greater interest in them  after testing
on a small group of users  we saw little effect  so we did not
include the clickthrough feature when running on the large
dataset 
b  svm learning algorithm
our machine learning algorithm was the default svm
in liblinear  l  regularized  l  loss  dual problem   logistic
regression would also be suitable  but we found the svm to
be faster  and speed was a high priority  the test set contains
    users and    days  to report results on every user and
every day  after the first   we would need to train and test
       models  even after reducing the dimensionality of the
data  the x matrix usually contains tens of thousands of rows
and thousands of columns 
first  we dropped all zero columns of x  these columns
corresponded to words that did not appear in any stories in
the users feeds  and feeds that the user does not subscribe to 
we then scaled the elements to fall in the range         
since the relative tf idf values within one row are important 
we scaled the entire block of tf idf columns with the same
value  and scaled the time column with another value 
there are generally far more negative examples than positive examples in the training set  so we include all the positive

 

examples and a random selection of the negative examples 
however  oftentimes there are so few negative examples that
they do not provide a good representation of all negative
examples  therefore  we then test the model on the full  all
negative examples included  training set  and add the higherdecision value incorrectly classified examples to the smaller
training set  to keep the training set balanced  we duplicate
an appropriate number of the positive training examples  we
retrain on this more representative training set 
in a real world application that extends over months and
years  it becomes impractical to use all previous days as the
training set  furthermore  users news preferences change over
time  we compared using all training days with using up to
   training days  but a more advanced system could learn the
optimal number of training days for each user 
iii  c lustering
the main idea that we pursue in this section is that if
we can cluster either or both the stories and users into well
defined groups  then we can leverage that information to make
predictions 
a  naive clustering
under this method  we group users who have accessed
stories originating from the same base url into the same cluster
of users we build this set of clusters by using the data for
a fixed number of days and then make predictions on the
remaining days  this model allows for users to be grouped
into multiple clusters  as the results will show  the key drawback in building this model stems from the over simplifying
assumptions made  hence  naive clustering  in building the
clusters 
consequently  we tried other techniques to build our clustering model 
b  community detection
our technique described here has been adapted from      we
begin by building our network where each vertex represents
a story and any two stories are connected if the sum
of the overlapping tf idfs of said stories is greater than a
heuristically determined value  this determines the adjacency
matrix a  intuitively  we would expect a good demarcation
of communities when the number of edges across different
communities is significantly less than the average number
of such edges for a randomly connected graph of similar
dimensions  this metric is formalized by a quantity called
the modularity  the modularity can either be positive or
negative but positive values indicate the possible presence of
community structures     we model our network for all the
stories  say n  for any one particular day  if we were to split
the graph into only two communities  we denote si     if
vertex i belongs to one community and si     if vertex i
belongs to a different community  also  the expected number
ki kj
where ki denotes the
of edges between vertices i and j is  m
degree of vertex i and m denotes the total number of edges
in the graph by noting that     si sj      is   if they are in the

fics      machine learning project december     

same community and   otherwise  modularity can be written
as 


  x
ki kj
q 
aij 
 si sj     
   
 m i j
 m


ki kj
  t
  x
aij 
 si sj    
s bs    
  q  
 m i j
 m
 m
ki kj
bij   aij 
 m
note that if wep
let ui denote the normalized eigen vectors
n
of b  then s   i    st ui  ui the goal now is to maximize
the modularity by performing an eigen decomposition of the
modularity matrix b and picking s which maximizes the
modularity
n

  x
q 
 uti s   i
 m i  
where i it is to be noted that since si is constrained to be   
in order to maximize the modularity  from the above equation
it is clear that if we pick si     when corresponding eigen
component is positive and si     when the corresponding
eigen component is negative will result in the highest inner
product  this is the core idea in the algorithm  we put all the
vertices with si     into one community and the remaining
vertices into the other community 
in      the author provides additional details as to extend
this idea to forming multiple communities  though  it should
be quite believable that the above technique can be applied
in a recursive fashion for completeness  provided below are
the set of equations for the change in modularity that  should 
occur when breaking a group g of size ng into  


x
x
   
q  
bij  si sj      
bij 
 m   i jg
i j


x
    x
bij si sj 
bij 
 m   i jg
i j
iv  m atrix completion
in this technique  we explored other ways in which to make
predictions on news reader preferences  our motivation to
begin this technique was to note that users  usually  have only
a few common factors driving their tastes or preferences    
to this end  we model our matrix a to be completed in
which the rows represent the users and columns represent
the stories  an entry aij     if user i has read story j and
  otherwise of course  for this technique to at least have a
chance at success  we must at least have one entry from each
row and column  then  the problem is reduced to finding a
low rank approximation to the matrix a since the data entries
far outnumber the degrees of freedom of said matrix 
initially  modeling our prediction problem seemed akin to
the netflix problem where users  rows of the data matrix 
are given the opportunity to rate movies  columns of the
data matrix   but users typically rate only very few movies
so that there are very few scattered observed entries of this

 

prediction results on     users
   
   
   
   
   
   
   
precision
recall
f 

   
 

 

  

  
  
days from start of data

  

  

fig     recall is consistently high  and precision is lower  we argue that
recall is more important than precision in a news prediction app  because the
newsreader would like to present all the stories that are interesting to the user 

data matrix  a completed matrix of this sort would enable
netflix to recommend titles that any user might be willing to
order  in order to test this idea  we built a sample matrix
a  r      and picked only the eigen vectors corresponding
to eigen values such that  i        to approximate this matrix 
say g  we ended up with a rank   matrix corresponding to
using the   largest eigen values  for all the entries in g  we
set gij     if gij       and we set gij     if gij      
it turned out that for the stories within the users feed  our
predictions were right roughly half the time  also  g now
invalidated some of the original data already used to build
the matrix  thus a major draw back in this technique is the
requirement that all the stories be known ahead of time and
that at least every user make his preference known for at least
  story  in other words  it is impossible to make a prediction on
a completely new brand of stories without first having at least
some preferences for some users  thus  we did not pursue this
idea to the fullest extent possible 

v  r esults   d iscussion
a  svm
we tested over     users and every third day of the month 
the average f score is defined as the average of f scores for
each user 
when using all available training data  average f  was
        when using the latest    days of training data 
average f  was         this result indicates that eliminating
older training data can be done without significantly hurting
prediction accuracy 
the following plots fig     fig    are for the algorithm
using all available data 

fics      machine learning project december     

 

relationship between prediction success and user habits

vi  c onclusion

   
   
   

fscore

   
   
   
   
   
   
  

 

 

 

  
  
avg readstosubscribedstories ratio

  

fig     average f  for a user is correlated with the proportion of stories the
user reads  it is much harder to predict for a user who subscribes to many
prolific feeds and yet reads few stories 
table i
c ommunity structures for stories across   days  f rom this we
can infer that it possible that a natural division into
community clusters exists

metrics
avg  size
max  members
  of communities

day  
  
  
 

day  
    
  
 

day  
    
  
 

day  
   
   
 

our supervised learning approach yielded an average fscore of         which seems low  but is actually reasonable
given that less than    of stories are read  furthermore 
average recall is         which means that our algorithm
suggests most of the stories that the user wants to read  perhaps
many of our incorrect guesses would also be interesting to the
user  we could test that hypothesis by looking at accessibility
of incorrect guesses  if these stories mostly occurred a long
time before the user opened the app  it could be that they were
buried far back in the feed  or  if the story was in a feed far
down the users list  perhaps the user did not have time to
scroll down and see it 
the next step would be to merge the supervised learning and
clustering algorithms  the clusters would be a valuable set of
features to use in supervised learning  in addition to having
a group of columns in x representing the feed of a given
story  there would be another group of columns representing
the cluster containing the story 
the clustering of stories via maximizing the spectral modularity turned out to be very effective in finding a network
structure  future work would be to make that algorithm
work successfully in realizable run times for the entire dataset  eventually  we would like to maintain two groups of
clusters  one cluster for similar stories which would lend itself
to predict a particular users other story preference and another
cluster for similar users wherein one users reading of a story
would lend itself to suggestion to other users in the same
cluster 
acknowledgment

b  community detection
we implemented this algorithm by placing a few constraints
on the input to facilitate time  memory and processing constraints  firstly  we capped the number of stories that went into
the algorithm to      we then placed an edge between any
two stories if they had at least   words in common and if their
tf idf was at least      the following table captures some of
key metrics that resulted from this algorithm first row gives us
the average number of stories per community  the second row
gives us the maximum number of stories any one community
had and the last row gives us the number of communities
into which the stories were divided into   similar results
follow for the remaining days  ideally  we would have run
this algorithm on the total set of stories and then  if any user
accessed any particular story in one cluster  we would then
predict that it would be very likely that the user would be
interested in reading any of the remaining stories in the same
cluster  in summary  community detection via maximization of
spectral modularity is a solid technique for grouping stories
into clusters 
a major draw back with this technique though is the
necessity to calculate eigen vectors at each iteration of the
process and consequently the inability to process more than
     connected stories at a time  another draw back of this
process it the requirement of knowing all the stories before
hand which cannot be the case for real time news prediction 

the authors would like to thank richard socher for highlevel guidance we would also like to thank the other pulse
project groups for sharing experiences and tips 
r eferences
    m  e  j  newman  modularity and community structure in networks 
national academy of sciences     
    e  j  candes and b  rechtexact matrix completion via convex optimization  applied and computational mathematics  caltech  pasadena
    
   
 liblinear  http   www csie ntu edu tw cjlin liblinear 
 porter
stemming
algorithm 
http   tartarus org martin 
porterstemmer 

fi