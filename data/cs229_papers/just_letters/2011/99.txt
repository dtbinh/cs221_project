detecting emotion in human speech
alex mordkovich  kelly veit  daniel zilber
 amordkov  kjveit  dzilber  stanford edu
december   th      

 

introduction

periods  divided by the average amplitude  there
are five different measurements of jitter taken  all
a variant of local jitter  local jitter is the average
absolute difference between consecutive periods 
divided by the average period 
a common set of features used in voice processing algorithms is the set of mel frequency cepstral coefficients  mfccs   cepstral features are
those represented on a nonlinear spectrum of
a spectrum  i e  derived by taking the fourier
transform of the logarithm of a spectrum  these
features are then represented on the mel scale 
which is designed to approximate the response
of human hearing by emphasizing frequencies to
which humans are sensitive  using praat  we extract    commonly used mfccs that model the
waveform  and use statistics on these coefficients
as additional feature 

detection of emotion in speech can be applied in
a variety of situations to allocate limited human
resources to clients with the highest levels of distress or need  such as in automated call centers
or in a nursing home  while a human may not
be available to assist everyone  automated emotion detection can be used to triage a customer
and  if they are growing angry or impatient  transfer them to speak with a human  it may also
be useful for helping autistic children and adults
learn to recognize more subtle social cues  or help
people refine their speaking skills so that they
have better control over the message they send 
other applications include improving personal assistants  such as siri  to help convey emotion in
text based communication  the applications for
emotion detection in speech are varied and practical  thus  we undertook the task of building a
machine learning system to detect emotion in utterances of human speech 

 

 

data

for our training  cross validation  and testing  we
used the emotional prosody speech and transcripts obtained from the linguistic data consortium      this data consists of recordings of professional actors reciting dates and numbers with
various emotional intonations  the semantic content of the utterances is intended to be emotionally neutral  as a form of psychological control
in the samples  each of the    audio recordings
    mb in size each  is annotated with a transcript file describing the time ranges of the utterances and the corresponding emotion label on
each utterance  a sample transcript line looks
like this 

features

we used the scripting capabilities of the freelyavailable praat     software to process the audio data and extract various statistics  including a standard voice report  the voice report
includes statistics for pitch  pulses  voicing  jitter  and harmonicity  we use the median  mean 
standard deviation  maximum  and minimum of
the above characteristics as some of our features 
pulse data includes the number of pulses  number
and mean of the periods  and the standard deviation of the periods  voicing looks at unvoiced
frames and calculates the number and percentage
of those  local shimmer is the average absolute
difference between the amplitudes of consecutive

                a  elation seventy one
this line indicates that the utterance from
      seconds into the recording until       sec 

fionds into the recording is an elated utterance of
the phrase two thousand six  the audio recordings and transcript data is preprocessed in a custom four stage pipeline to generate data file in
which each utterance is a data sample represented
as a single line  this resulting data file is loaded
into matlab as a stylized design matrix 
    of all the samples are used for training
and cross validation  the remaining     are set
aside as test data that is unseen until we are ready
to test our learning algorithms hypotheses 

 

cipal component  also we do not see any clear
clustering of data points 

figure    principal component space

pca

to get a sense of the training data  and to determine whether there are correlations among the
features  we ran principal component analysis
on it  note that since the various features are on
different scales  some are percentages  some are
hertz  some are counts  etc    from here on we
work with the zscores  normalized data  rather
than the raw values 

 

supervised k means

we chose to use k means clustering for our first
classification algorithm due to its simplicity and
applicability to a multilabel problem  however 
because the training data set is labeled  we implemented a supervised version of k means in which
the starting centroids are calculated by grouping
the data points by label and then calculating the
mean for each label 
without any further restrictions  this algorithm obtained an error of approximately    
when attempting to classify between    emotions 
when trained on the complete training data set 
this implies that the data is not clearly separated
into clusters in the given feature space  this is
also a consequence of using    input labels in the
training set  to get around this problem  additional different features need to be carefully selected or a different metric should be used to predict emotions correctly  we explored these options to improve the k means predictions by implementing custom feature selection  label grouping  and fitting normal distributions akin to gaussian descriminant analysis 

figure    explaining the variance
in figure    we see that the first two principal components explain only about     of the
variance  in general  we see that the variance is
distributed thinly across most of the features 
we also noted that the first two principal
components gave the largest  absolute  weight to
the features describing the median pitch and the
mean of the  th mfcc  respectively  indicating
that these are potentially the more useful features out of the lot 
we can also project the  normalized  data
onto the first two principal components  in figure    we see that almost all the mfccs are correlated positively with the second principal component  whereas some mfccs correlate negatively
and some correlate positively with the first prin 

   

feature selection

feature selection was implemented in two ways 
the first way was to find the combination of    
features that minimizes training error  the second approach was to use a forward or backward
search heuristic  neither approach improved results significantly  these search algorithms for
new features proved to be slow  below are some
 

fitest error
   
   
   
   
   
   
   

train
error
   
   
   
   
   
   
   

anxiety
boredom
cold anger
contempt
despair
disgust
elation
happy
hot anger
interest
neutral
panic
pride
sadness
shame
  error

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
    

feat   

feat   

feat  

 
  
  
 
 
  
 

  
  
  
 
 
  
 

  
  
  
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
    

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
    

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
    

was reevaluated on different samples  resulting in
      and       testing error  using two clusters yielded a     error  while four yielded a    
error 

   

in addition to the label grouping and feature
selection attempts to reduce error  one last approach we tried was to use gaussian distributions
for each label  in theory  this would help classify some of the outliers that were closer to incorrect centroids when using the l  norm  the
means were calculated the same way as centroids
but now a covariance matrix was calculated for
the features of samples with a specified label  to
avoid singular matrices  the data was normalized
using zscore  the results were on par with the
regular k means algorithm 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
    

 

svm

we experimented with the svm implementations
in liblinear  libsvm  and the matlab builtin
svmclassify for our classification tasks  the linear  polynomial  and rbf kernels were used with
libsvm  all data was normalized before processing  the results of these methods are shown in
the results matrix tables   and    backward
feature search was used to reduce error  as speed
was not a limiting factor  as seen in the result
matrix  this reduced error by approximately   
on average 
while others        have used svm to classify
emotions to distinguished between two or three
emotions that known a priori  classifying a wider
spectrum of emotions is a more pragmatic endeavor  the application of an algorithm that can
choose only between two known emotions is quite
limited  thus  we decided to train and test our algorithms on utterances ranging the full spectrum
of the    available emotions  the largest number of emotion labels that previous studies have
worked with is only    the best generalization
error we achieved for exact emotion classification
is        

of the results of the first approach on small sample sets containing    emotions  small data sets
were used to limit run time 

   

gaussian fitting

label grouping

to reduce the generalization error  the emotions
were categorized into several small groups instead
of    distinct labels  as shown in the table above 
after running hundreds of randomized labelings 
the best groupings were recorded with error being
significantly decreased as expected  in the range
of        test error  there were some trivial
relabelings in which most of the emotions were
grouped together  recreating the enviroment of
previous research in which one or two emotions
are classified against all the remaining emotions 
these labelings naturally offered the lowest error at around    to      when checking each
grouping  feature selection was also applied but
set to decrease test error rather than training error  making the results optmistic 
the grouping corresponding to        error
 

filibrary
kernel
error
liblinear
linear
      
libsvm
linear
      
libsvm
polynomial       
libsvm
rbf
      
table    exact emotion classification results

   

library
kernel
error
libsvm   bi  rbf
      
nary arousal
libsvm   bi  rbf
      
nary valence
libsvm
  rbf
      
high arousal
only
table    clustered emotion classification results

exact emotion classification

using the multiclass labeling support in libsvm 
we trained a model to distinguish between    emotional states  we used figure   

low
neutral high
valence
valence valence
high arousal hot anger 
elation
panic 
anxiety
neutral
disgust 
pride 
arousal
sadness 
interest 
contempt 
happy
cold
anger 
shame
low arousal
despair
boredom
table    emotion classification by arousal valence
arousal  excitement  of the emotion  the error is
reduced  here  the trade off is specific knowledge of the emotion  for modern applications 
this may be sufficient   the valence arousal space
provides valuable information about the emotion 
allowing a system such as automated call centers
 an application that does not need to distinguish
contempt from sadness  but needs to be sensitive
to low valence  to make an appropriate reaction 
using a multiclass svm to group emotions into
discrete groups  table   is the classification of
emotions with the lowest error  it is clear that
this classification is useful for applications   it is
similar to classifications in psychology  such as
the feeltrace    emotion classification  exploring
this concept deeper by distinguishing emotions on
one dimension only  the trade off becomes more
apparent  high arousal emotions  for example 
can be distinguished from all other emotions with
a        generalization error 

figure    error vs  training set size
as expected  the testing error falls as the
training set size increases  and it appears to be
flattening out  we also note that training error
remains unacceptably high and does not change in
any significant way as training set size increases 
however  there is still a large gap between the
testing error and the training error  as such  it
is difficult to say whether our algorithm is suffereing from high variance or high bias  adding
more data samples will not significantly improve
our results  one way to interpret this result is
that set of features we are extracting from the
voice data does not necessarily describe or correlate well with the emotional labels  if this is
the case  the acoustic and prosodic characterstics
of human utterances need to be studied better to
understand which features are indeed relevant 

   

 

clustered emotion classification

conclusion

the purpose of this project was to expand upon
current research in which speech is classified by

by grouping emotions according to a   dimensional space of level of valence  positivity  and
 

fiemotion  we looked at data that contained samples spanning    emotions and implemented kmeans clustering and svm algorithms to create
predictions  initially  both algorithms performed
poorly  feature selection and label grouping improved results at the price of specific emotion classification 

references
    shami  m    verhelst  w          an evaluation of the robustness of existing supervised
machine learning approaches to the classification of emotions in speech  speech communication           
    casale  s   russo  a   scebba  g   serrano 
s          speech emotion classification using machine learning algorithms  semantic
computing  ieee internation converence on
     
    http   www ldc upenn edu catalog catalogentry jsp 
catalogid ldc    s  
    http   www fon hum uva nl praat 
    http  emotion research net
    laukka  neiberg  forsell  karlsson  elenius
       expression of aect in spontaneous
speech  acoustic correlates and automatic detection of irritation and resignation computer
speech and language          
    nwe  foo  de silva        speech emotion
recognition using hidden markov models
speech communication           

 

fi