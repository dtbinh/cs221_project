movies genres classification by synopsis
ka wing ho

abstract

mapped to the same index in the dictionary as they usually dont provide much genre related information  the
remaining words were filtered through a python stemmer library     so words with the same base but different forms mapped to the same index in the dictionary 
for the        titles used in this project  a dictionary
with        words was generated  out of the       
words  only those which have occurred more than   
times in all the training samples were used in bow representation so only         denoted as v   words were
left  the term frequency inverse document frequency
 tfidf   of the words were then computed 

this paper investigated four different approaches for
automated movies genres classification based on their
synopsis 

  introduction

public movies database such as imdb provides genre
information to assist searching  the tagging of movies
genres is still a manual process which involves the collection of users suggestions sent to known email adm
dresses of imdb  movies are often registered with intf idf  wk   di     wki  log pm
accurate genres  automatic genres classification of a
  w
kd     
d  
movie based on its synopsis not only speeds up the classification process by providing a list of suggestion but where wki is the frequncy of the k th word in the i th
the result may potentially be more accurate than an un  movies synopsis  di    and m is the number of training test sets 
trained human 
after the preprocessing above  each movies synopsis
was represented by its tfidf vector x  r         the
  data set
vector was also normalized to make it a unit vector  this
was done to account for the variation in length between
the data used in this project is a set of text files from the synopsis of different movies  similar treatment of
imdb      they contain         synopsis     and tfidf and normalization were also done for the test set
        pieces of genre information     of movies and using the same        words  there is also a bit vector
tv shows  for this project         unique titles at y  r   associated with each movie where yl         
which both the synopsis and genre information were to indicate whether it belongs to genre l 
available were chosen and randomly split into     and
    sets  the former was used for training while the
latter for testing  note that a movie can be  and of    classification methods
ten so  associated with more than one genres  there
are    listed genres in the imdb data set and only the a movie can be associated with multiple genres so the
    denoted as l  most common genres were used in task in this project is a mult label classification problem 
this project  the genres names and percentages of we evaluated four different approaches  some of which
movies in them are  action         adventure        were published in previous literature 
comedy         crime         documentary        
 one vs all approach with svm
drama         family         romance          short
films        and thriller        respectively 
 multi label k nearest neighbor  knn     
the synopsis text file was processed to generate the
bag of words  bow  representation  the nltk stop parametric mixture model  pmm        
words list     was first used to remove all stopwords
 neural network
from the synopsis  all numerical words were also
 

fi   

svm
p  elj  hlp   is estimated from the training set by
updating counters d l  j  and d l  j  as follows 

while a movie can belong to mutliple genres  whether
tagging it wih a paritcular genre is just a binary classification problem  specifically  one can group all movies
of a particular genre together as the positive samples and
the rest as negative samples and train a binary classifier with these two disjoint sets  this approach is generally known as one vs all  however  if the sizes of the
two classes differ a lot  the classifier resulted is likely to
bias towards the more populous class  while the precision may be good  it would result in a rather low recall
for less common genres  e g  adventure   so  we also
tried two different approaches      increase the weight
 option  w in libsvm      of the penalty for misclassifying the smaller class  one simple way to determine the
weight is based on the ratio between the sizes of the two
classes      reduce the training set of the more populous
class to match the size of the less populous class  we
used libsvm     in this project  the input is the normalized tfidf matrix and the corresponding output labels for
each genres  we tried different solvers and misclassification penalty and found that the combination of l regularized l  loss support vector classification  dual 
with default penalty of   yielded the best result 

d l  j        d l  j         j           k 
for i           m 
for l   drama  comedy  thriller      
 i 
if  yl      
 i 
d l  cl     
else
 i 
d l  cl     
end
end
end
with laplace smoothing 
p  elj  hlp     pk

dpl  j     

j    

dpl  j      k    

   l  j  p

no smoothing is needed for the prior as there is at least
a movie in each genre 
p  hlp  

 

pm

 i 
i     yl

m

  p 

   l  p

alternately  one can also assume that the prior
follows a uniform distribution  in this case  its the same
intuitively  movies belonging to the same genre should as computing the maximum likelihood estimate mle  
share more common keywords in their synopsis  if one
 t 
c
 t 
yl    arg maxp      p  el l  hlp  
were to consider a movie synopsis x as a point in the
hyperspace  movies with similar genres combination of
x should be close to it  this is similar to the idea behind     parametric mixture model
multi label knn alogrithm in     which utitlizes the
statisical information of a documents k nearest neigh  in         the authors argued that the binary classification approach taken in section     doesnt represent the
bor to infer its genres 
the multi faceted negative samples well  they proposed
for a training sample  x i    y  i     its k nearest neighp
a generative model which conjectured that a document
 a 
 i 
bors are denoted as n  i    let cl    an  i  yl debelonging to muliple categories can be regarded as a
note the number of its k nearest neighbors which belong
mixture of words related to those categories  specif i 
to the genre l  let elj be the event that cl   j  ically  for a word w  suppose its probability of show            k   i e  j out of k neighbors are in genre l  and ing up in the synopsis of a movie in genre l is the pa i 
hlp be the event that yl   p          the task of clas  rameter l w   p  w l  where pv l w      for
w  
sifying whether a test sample t belongs to genre l is by a given movie belonging to multiple genres  the probcomputing the maximum a posteriori map  estimate 
ability of the word w showing up in its synopsis is a
 t 
cl
 t 
p
weighted sum of the word distribution over each genyl    arg maxp      p  hl  el  
p
pl
 i 
 i 
 t 
res  p  w y  i      l
c
l   l  l w where
l   l    
p
p
l
p  hl  p  el
 hl  
 i 
 i 
  arg maxp     
 t 
and l     if yl      we implemented the
c
p  el l  
pmm  model in     which made a simplifying assump t 
cl
 i 
p
p
tion that l has a uniform distribution over all the
  arg maxp      p  hl  p  el  hl  

   

multi label k nearest neighbor

 

filayer size of l        different hidden layer sizes  s 
were tried      and      the activation function was
the sigmoid function and the cost function with regularization was 

genres to which a movie belongs  in other words 
 i 
 i  p
 i 
l    yl   l
l   yl   for example  if a movie is an
action drama  its y  i                                    then
p  w y  i             w          w  
with the naive bayes assumption  the probability of a
synopsis x i  given genres y  i  is 
p  x i   y  i       

  xx
 i 
 i 
 yl log h  x i   l    yl  log  h  x i   l   
m
m

v pl
y
 i 
l   yl l w nw
  p
 
l
l    yl
i  

j    

l

i   l  

 x x       x x      
 j k    
 j k    
 
 m
s

 

 i 
nw

where
is the frequency of word w in x i  and  is
the set of all l w    is estimated by maximizing the
posterior p     where      x i    y  i    m
i     assume y is independent of  
taking
log
on
both
sides
p
 i   y  i       
gives map    arg max m
 logp
 x
i  
logp     where the prior
over
q qv the parameters is assumed to be p     l
l  
w   l w in      so  the
objective function to maximize is 

l

v

s

j   k  

j   k  

where h  x i   l is the output of l th output unit      
rsxv and      rlxs are the weights for the two connecting layers 
we also attempted to utilize pca to reduce the dimension of the input data  we used the princomp command
in matlab to generate the principal components and the
mappings of the tfidf vectors to the space of the principal
v
l x
l
v
m x
x
x
x
 i 
logl w components and trained neural networks with different
l l w  
n i 
j    
w log
number of principal components  the test set was also
i   w  
l   w  
l  
mapped into the space of the principal components bethe authors in     proved that the objective function can fore being fed into the trained neural network to make
be maximized by iteratively updating the followings un  prediction 
til convergence 
 i 
gl w   

 t   
l w

  

pm

 i 

  results

l l w

   p
l

 i 
l   l l w

 i   i 
 t 
i   nw gl w        
pv pm  i   i   t 
w  
i   nw gl w      

v

svm default 
svm weighted 
svm     
knn k     mle 
knn k     map 
pmm mle 
pmm map 
nn       p c        
nn       p c        
nn       p c        
nn       no pca 
nn       no pca 

  l  w

for a given test sample t  prediction of its genres y  t  is 
y  t     arg maxy p  x t   y    map  p  y   

precision
       
       
       
       
       
       
       
       
       
       
       
       

recall
       
       
       
       
       
       
       
       
       
       
       
       

f measure
       
       
       
       
       
       
       
       
       
       
       
       

where prior p  y    can be estimated from the training
set  this is essentially computing the maximum a posteriori  map  estimate  note there are        possible
values which y  can take on  each corresponding to
different combination of the    genres  alternately  one
can once again assume a uniform class prior as in      table    average precision  average recall and average
this is essentially the mle  we experimented with f measures over all genres 
both in this project 
the evaluation is based on the following metrics 

   

neural network
p recision  

lastly  we also trained a neural network using error
backpropagation  the neural net consisted of one hidden layer  an input layer of size v          and output

recall  
 

tp
tp   fp

tp
tp   fn

fivarious configurations of a neural network with     hidden units   is the weight of the regularization term and
p c is the number of principal components used if pca
was applied to the inputs 
as shown in table    svm default  had a low recall
rate for the less common genres in the data set as the
imbalance in class sizes resulted in a decision boundary
which favored the negative class  there is also a notable
difference in the recall between mle and map for the
knn model and  to a lesser extent  the pmm model 
for less common genres  its prior p  hl    is much less

    
   
avg precision
avg recall
avg fmeasure

    
   
    
   
    
   
    

c

 

  

  

  

  
  
  
number of neighbors k 

  

  

  

   

figure    knn mle  over different k
   
avg precision
avg recall
avg fmeasure

   

   

   

   

   

   

   

 

  

  

  

  
  
  
number of neighbors k 

  

  

  

   

figure    knn map  over different k

f m easure  

 t 

than p  hl    so the posterior probability p  hl   el l  
was dragged down by it  resulting in lower recall  but
higher precision  than knn mle   similar effect was
observed in the pmm map  model but the difference
with pmm mle  was not as much  there were       
possible priors to consider so the skewness among them
was more evenly spread out 
we experimented with values of k from   to    for
the knn models  figure   and figure   show the average precision  recall and f measure over different numbers of k  interestingly  for knn mle   both the precision and recall kept rising as the number of neighbors
increased  this may suggest that movies with the same
genres werent as closely clustered as initially thought 
it may also be the case that different genres perform best
at different values of k 
as mentioned in section      we tried neural networks
with hidden layer sizes of     and     respectively 
the latter actually had a slightly inferior performance to
the former  the high dimensionality of the input layer
 v            led to huge number of parameters in the
weight matrix     relative to the number of training
samples            a neural network with fewer numbers of hidden units has fewer parameters and it may
be less prone to overfitting in this case  we attempted
to reduce the the input dimension with pca but it appears that pca may not be as effective on this data set as
one would have desired  the maximum amount of variance captured by a single principal component was only
       the first                  and       principal components accounted for about              
and     of the variance respectively  its interesting to
note a neural network trained with only the first     
principal components  i e      of the size of the original input layer  still managed to achieve similar level of
precision and recall as one trained with all components
 albeit without regularization  

   p recision  recall
p recision   recall

where t p   f p and f n are the numbers of true positive  false positive and false negative results respectively 
table   shows the average precision  average recall and
average f measures over all genres by different algorithms  table   shows the precision and recall information for each genres 
svm default  and svm weighted  were svm classifiers trained with the entire training set but the latter
assigned more weight to the misclassification penalty
of the smaller class  svm      was a svm classifier trained with a subset of the training set so that
sizes of both classes were similar  knn mle map 
and pmm mle map  were the k nearest neighbor and
parametric mixture model respectively  nn    stand for
 

fisvm default 
svm weighted 
svm     
knn k     mle 
knn k     map 
pmm mle 
pmm map 
nn       p c        
nn       p c        
nn       p c        
nn       no pca 
nn       no pca 

svm default 
svm weighted 
svm     
knn k     mle 
knn k     map 
pmm mle 
pmm map 
nn       p c        
nn       p c        
nn       p c        
nn       no pca 
nn       no pca 

action
p
r
               
               
               
               
               
               
               
               
               
               
               
               
drama
p
r
               
               
               
               
               
               
               
               
               
               
               
               

adventure
p
r
               
               
               
               
               
               
               
               
               
               
               
               
family
p
r
               
               
               
               
               
               
               
               
               
               
               
               

comedy
p
r
               
               
               
               
               
               
               
               
               
               
               
               
romance
p
r
               
               
               
               
               
               
               
               
               
               
               
               

crime
p
r
               
               
               
               
               
               
               
               
               
               
               
               
short films
p
r
               
               
               
               
               
               
               
               
               
               
               
               

documentary
p
r
               
               
               
               
               
               
               
               
               
               
               
               
thriller
p
r
               
               
               
               
               
               
               
               
               
               
               
               

table    precision  p  and recall  r  of each genres by different algorithms 

references

    m  ling zhang and z  hua zhou  ml knn  a lazy
learning approach to multi label learning  pattern
    imdb alternative interfaces  url http   www 
recognition                    
imdb com interfaces 
    a  k  mccallum  multi label text classification with
a mixture model trained by em  in aaai    work    natural language toolkit  url http   www 
shop on text learning       
nltk org 
    imdb genres database 
     
url     n  ueda and k  saito  parametric mixture models
for multi labeled text  in in advances in neural inftp   ftp fu berlin de pub misc 
formation processing systems     pages        
movies database genres list gz 
mit press       
    imdb plots database       
url ftp 
  ftp fu berlin de pub misc 
movies database plot list gz 
    r  boulton 
pystemmer       
http   pypi python org pypi 
pystemmer       

url

    c  c  chang and c  j  lin  libsvm  a library for
support vector machines       
 

fi