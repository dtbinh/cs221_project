complex sentiment analysis using recursive autoencoders
shashank sashihithlu  siddhi s  soman
stanford university
 sashihit  ssoman   stanford edu
advised by richard socher

abstract
we use a machine learning framework based on
recursive autoencoders to make sentence level
predictions regarding the subject  object and
sentiment of the sentence  our model learns
vector representations of multi word phrases 
the syntactic structure of the sentence is
incorporated into our model by using the
stanford parser tree thereby enabling it to make
sentiment predictions with respect to subject 
object  etc 
 

introduction

deep learning involves learning a hierarchy
of internal representations using a stack of
multiple modules  each of which is trainable and
can implement complex non linear functions 
deep learning allows us to go from low level
representations to high level representations and
is more efficient and accurate than shallow
architectures such as kernel machines  a deep
architecture achieves this efficiency by trading
space for time and using sparse feature
encoding  we build a hierarchy of modules 
where stage k measures the compatibility
between features at level k   and level k  where
the compatibility may be measured as the
negative of log likelihood or energy  the
objective at each stage is to find a set of features
that would minimize the energy  the input to
stage k   is the feature vector of stage k  deep
belief networks have been shown to perform
significantly better than other techniques for
problems such as sentiment analysis  image
classification and handwriting analysis 

sentiment analysis  together with opinion
mining  is becoming a promising topic and has
important applications in the fields of social
media  customer relationship management etc 
our project aims at performing complex
sentiment analysis using deep learning
techniques  previous work in     looked at the
problem of extracting sentiments using
recursive autoencoders  we tried to extend this
work by performing more complex sentiment
analysis which includes extracting the subject 
object and the sentiment  we plan to use data
comprising of  opinionated text   the recursive
neural network architecture described in    
jointly learns how to parse and how to represent
phrases in a continuous vector space of features 
this allows us to capture syntactic and
compositional semantic information from
natural language sentences  we use this
architecture combined with the auto encoder
scheme to perform complex sentiment analysis 
 

dataset

we use the mpqa dataset which consists of
opinionated text  the mpqa opinion corpus
contains news articles from a wide variety of
news sources manually annotated for opinions
and other private states  i e   beliefs  emotions 
sentiments  speculations  etc    the corpus
contains     documents  a total of       
sentences  the articles in the corpus are from
    different foreign and u s  news sources
which date from june      to may      
the dataset contains annotations of different
types like express subjectivity  direct subjective 
objective speech event etc  these also have

finested annotations like polarity  attribute for
marking the polarity of the expression  in
context  according to the nested source  
intensity etc  we try to use the annotations
within this dataset to extract the different parts
of the sentence like subject  object along with
the sentiment 
 

essentially these same steps are repeated for all
triplets and the reconstruction error at nodes of
the tree is found 

recursive autoencoders

we represent words as continuous vectors of
parameters  we represent a sentence  or any ngram  as an ordered list of these vectors
  now  assume that we are given
a list of word vectors
as
well as a binary tree structure for this input in
the form of branching triplets of parents with
children 
  each child can be an
input word vector or a non terminal node in the
tree  for the example in fig    we have the
following
triplets 
in order to be able to
apply the same neural network to each pair of
children  the hidden representations
have to
have the same dimensionality as the s  given
this tree structure  we can now compute the
parent representations  the first parent vector
is computed from the children  
 

where we multiplied a matrix of parameters
by the concatenation of the two
children  one way of assessing how well this ndimensional vector represents its children is to
try to reconstruct the children in a
reconstruction layer 

during training  the goal is to minimize the
reconstruction errors of this input pair  for each
pair  we compute the euclidean distance
between the original input and its
reconstruction 

fig    rae on a binary tree
so far  the rae was completely unsupervised
and induced general representations that capture
the semantics of multi word phrases  we extend
raes to a semi supervised setting in order to
predict a sentence or phrase level target
distribution t  this is done by adding on top of
each parent node a simple softmax layer to
predict class distributions 

assuming there are k labels 
is a kdimensional multinomial distribution and
  let
be the kth element of the
multinomial target label distribution t for one
entry  the softmax layers outputs are
interpreted as conditional probabilities
  hence the cross entropy error is  

using this cross entropy error for the label as
well as the reconstruction error  we get the total
error for sentence  label pairs
in the corpus
as 

fiwhere e is summed over all nodes for a
particular sentence and is a weighted average of
the reconstruction error and the cross entropy
error  this error function is minimized with
respect to all the parameters by using l bfgs 
 

model

we use recursive autoencoders to model our
data and for making predictions  the previous
work included a greedy approach for structure
prediction using unsupervised rae  the
resulting tree structure captures as much of the
single word information as possible  in order to
allow reconstructing the word vectors  but does
not necessarily follow standard syntactic
constraints  hence though this approach might
be faster  it does not necessarily capture the
entire syntactic structure 
owing to this we chose not to use the greedy
approach of tree construction  instead we use
the stanford parser tree output as our tree
representation  this representation captures the
syntactic structure of the sentence  this helps us
correlate the syntactic and semantic information
of the sentence and helps us make better
predictions 
e g 
sentence 
the kimberley provincial hospital said it would
probably know by tuesday whether one of its
patients had congo fever 
parsed output 
 root
 s
 np  dt the   nnp kimberley   nnp
provincial   nnp hospital  
 vp  vbd said 
 sbar
 s
 np  prp it  
 vp  md would 

 advp  rb probably  
 vp  vb know 
 pp  in by 
 np  nnp tuesday   
 sbar  in whether 
 s
 np
 np  cd one  
 pp  in of 
 np  prp its   nns patients    
 vp  vbd had 
 np  nnp congo   nn
fever          
       
in our model  words are represented as a
continuous vector of parameters  our model
learns a vector representation of phrases and
sentences using rae as shown in the figure 
our model is also extended to learn a sentence
or phrase level target distribution in a semisupervised setting using an additional softmax
regression layer 
 

methodology

we used the annotations and information
provided by the mpqa dataset for training our
model  the first step was extracting the
sentences and corresponding annotations from
the mpqa dataset  the data was then preprocessed to the format required by our model 
     data pre processing 
the mpqa dataset required substantial preprocessing as we had to extract out the labels we
needed for all the words  phrases and sentences
within the dataset  this helped us in designing
our model to predict sentiments with respect to
the various parts of the sentence like subject and
object 
e g 
sentence  the kimberley provincial hospital
said it would probably know by tuesday
whether one of its patients had congo fever 

fipre processed data 

however since this does not incorporate the
syntactic structure of the sentence in making
predictions we choose not to utilize this
approach  instead we parse each sentence and
using the stanford parser convert each sentence
into its parsed form  we then convert this parsed
tree to a binary form to determine the
hierarchical structure of the sentence  we then
use this in our recursive autoencoder model for
making predictions 

element label word

basic word

subj

the

the

subj

kimberley

kimberley

subj

provincial

provincial

subj

hospital

hospital

   

said

say

subj

it

it

obj

would

would

obj

probably

probably

obj

know

know

   

by

by

   

tuesday

tuesday

obj

whether

whether

subj

one

one

   

of

of

   

its

its

   

patients

patient

     training and results 

   

had

have

obj

congo

congo

let
be the set of our model parameters  then the
gradient becomes 

obj

fever

fever

punc

 

 

     labelling 

similarly we processed the dataset to assign
subject  object and sentiment labels to each
word in each sentence for the dataset 
     tree representation of the sentence 
as mentioned before  the previous approach
utilized a greedy algorithm to find the best
possible tree representation for a sentence 

using the above tree structure we calculate the
multi label distribution and assign the different
labels  specifically for this problem we assign
two kinds of labels to the different elements 
the first one is the sos  subject  object 
sentiment classes  label which holds the
probability of a particular phrase or word
consisting of subject  object and sentiment
clause  the second label gives us more
information about the sentiment if the element
has one  currently we give it a simple labelling
indicating whether the element has a positive 
negative or neutral sentiment 

to compute this gradient  we first find
reconstruction error at all nodes and then
derivatives for these trees are computed
efficiently via backpropogation through
structure  because the derivatives of the
supervised cross entropy error also modify the
matrix
  this objective is not necessarily
continuous and a step in the gradient descent
direction may not necessarily decrease the
objective  however  we intend to use l bfgs

firun over the complete training data to minimize
the objective  it should be noted that the
expected objective of the project is not fully met
yet  however we believe that given more time 
we can tweak the parameters to get good results
as the model and algorithm we use might prove
to significantly outperform previous techniques
used for complex sentiment analysis 

 

references

  http   nlp stanford edu pubs socherlinngmanning ic
ml     pdf

  http   www socher org uploads main socherpennington
huangngmanning emnlp     pdf

  http   www socher org uploads main     sochermanni
ngng pdf

 

conclusion and future work

we tried to use recursive autoencoder
techniques to make predictions about the
sentence structure as well as the sentiment of the
sentence with respect to the subject and object 
training the data using deep learning techniques
to make predictions is a hard problem and we
believe that given more time we will really be
able to make a significant contribution towards
solving this problem  hopefully we should be
able to continue with this work and achieve
good results as we keep on conducting more
experiments  as an example of an additional
factor that could be taken into consideration  we
could take into account the form of the verb
 e g  active passive voice  tense  while making
predictions  the current model may be easily
extended to include this due to the use of the
stanford parser tree output in the model which
incorporates the entire syntactic structure of the
sentence  further experiments could make use
of these additional factors to make better
predictions 

  http   www cs pitt edu mpqa annotation html
  http   cogcomp cs illinois edu page resources data

fi