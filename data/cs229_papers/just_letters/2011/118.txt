classification of malicious network activity
w  nicholas greene
nathan newsom
due           

 

introduction

as more and more vital services today  e g  email  facebook  quantitative trading  depend on machine
learning algorithms  there is a greater incentive than ever for adversaries to manipulate these algorithms for
malicious ends  e g  spam  identity theft  cyberattacks   the field of adversarial learning has arisen out of
a need to design learning algorithms that are robust to these sort of disruptive manipulations 
spam filtering has often been cited as the classic adversarial machine learning problem      as filtering
methods grow more accurate  spammers employ more and more sophisticated methods to work around
them  e g  deliberate misspellings of keywords  etc    resulting in an ongoing arms race  a more grave
example  however  that illustrates the need for robust learning methods is the network intrusion problem 
where potentially sensitive  confidential  or damaging information is compromised through sophisticated
cyberattacks on computer networks  cybercrime and cyberwarfare are real problems        that advances in
adversarial machine learning may mitigate 

 

project outline

our project made use of an online to batch  o b  method based on the perceptron algorithm from     to
train a linear classifier to identify malicious network activity under adversarial conditions  including feature
deletion and feature corruption of the data  we tested the algorithm against standard  non adversarial
classifiers using a subset of the kdd cup      data set  available through the uci machine learning
repository      the data set was generated by mit lincoln laboratory for the      darpa intrusion
detection evaluation program and subsequently used for the      kdd cup competition  the full dataset
consists of nine weeks of raw tcp dump data for a local area network  lan  simulating a typical u s 
air force lan subject to several types of attacks including buffer overflows and ip sweeps  a total of   
features  connection duration  protocol  number of transmitted bytes  etc   describe the roughly   million
tcp connections 

 

adversarial models

following      we must impose some limits on our adversary to make the classification problem tractable 
while it is assumed that our adversary has full knowledge of our classification scheme and learned classifier 
we limit her influence to the testing phase  since it will be assumed that a relatively clean training set can
be obtained  although we may model the adversarys actions in the training phase   we allow the adversary
to perform two kinds of manipulations      the adversary can delete features entirely  i e  replace them with
   or     the adversary can replace feature data with white gaussian noise of the same mean and variance
of the original feature 
in addition we choose a noise tolerance parameter n and assign each feature j a cost vj  we p
will refer to v
as the cost vector   we then allow our adversary to corrupt any subset of the features such that j j
  vj  n
where j is the set of features that are uncorrupted  our adversary will choose to manipulate a subset of
features that causes the most damage to our prediction  subject to this noise constraint 

 

ficoncretely given a test example  x  y   rn         and the parameters to the learned  linear decision
boundary  w  b   our adversary will first rank each feature according to the following metric  ywj xj  vj   it
will then greedily manipulate
features  deletion or noise replacement  as long as ywj xj      i e  the feature
p
is informative  and j j
v

n   the ranking metric balances the damage the adversary can inflict  the
j
 
ywj xj term  with how costly that feature is to manipulate  the vj term   p
n
we choose two such cost vectors for our tests  normalizing both so that j   vj   n  first we weight the
features uniformly such that vi   vj      in this case the adversary simply chooses to corrupt the n features
that will cause the most damage  in the other case we choose vj to correspond to the mutual information of
the optimal one feature linear classifier using the following formula 
vj  

 
max i   xj   c   y  
z cr

where  xj   y   are random variables jointly distributed according to the uniform distribution over the training
data and z is our normalizing constant 

 

o b perceptron algorithm

as its name implies  the o b perceptron algorithm from     converts an online perceptron   modified for
adversarial robustness   to a batch learning algorithm using a simple averaging procedure  in the online
training phase  corrupted training examples are iteratively passed to the algorithm to be classified using the
current parameter settings  if the algorithm classifies poorly  the parameters are updated  in addition  the
parameter settings at each iteration are saved  after the classifier has seen all the training data  these saved
parameter settings from each iteration are averaged to give the final parameter settings 
more formally  given a training dataset s     x i    y  i    m
i     we initialize our parameters w and b to zeros 
at the i th iteration  we pass example  x i    y  i    to the algorithm to classify using the current parameters
w i  and b i    our adversary allows a set ji of features  via one of our models  to pass through to the
classifier intact  which results in the following prediction of y  i   


x  i   i 
h x i      sign b i   
wj xj   
jji

this hypothesis is compared to the true y  i    producing an empirical hinge loss 


  i    b i    x i    y  i      v  j   y  i  h x i     
 w
p
 
where       max      
  i    b i    x i    y  i         then the algorithm has classified  x i    y  i    correctly and we set
if  w
w i     w i 
b i     b i   
otherwise  we perform the following update 
 h
i
 w i    y  i   x i 
 i   
j
j
c
wj

 w i 
j
h
i
b i     b i    y  i  
 

j  ji
o w 

c

p

where    c n      m and   c   max min   c   c   this update rule is a modified version of the
standard perceptron update rule  where we factor in knowledge of the adverarys manipulations  learn at a
rate    and constrain our parameters to the hypercube of radius c 
 

fiafter the m training examples have been passed to the algorithm we compute our final parameters w
and b by averaging the previous m parameter settings 
m

w  

  x  i 
w
m i  

b  

  x  i 
b  
m i  

m

note that aforementioned c is an l regularization parameter and effectively keeps w and b small
and dense in order to build robustness into the classifier  other regularization schemes typically use
l  or l  norms  which can promote sparse solutions   the opposite of what is needed for this problem 
sparse parameters suggest that the algorithm has singled out a few powerful features that dominate the
classification decision  which makes them susceptible to corruption  i e  the adversary can cause more
damage by manipulating fewer features   with dense parameters  the algorithm has essentially hedged its
bets on which features to rely on since it knows that some features cannot be trusted 

 

experiments

we tested the o b perceptron on a subset of the kdd cup      dataset with     normal examples and
    attack samples that represent ip sweeps  while the entire kdd cup      dataset contains nearly  
million examples  we were limited by our available resources to processing this     example subset  each
example containes    features  including connection duration  continuous   protocol type  symbolic   number
of transmitted bytes  continuous   etc  the symbolic features were discretized using a one to one integer
mapping 
we compared the performance of the o b perceptron against two conventional binary  linear classification
algorithms  logistic regression  lr  and a support vector machine  svm  with a linear kernel  for our lr
implementation  we used a learning rate        and we declared the algorithm had converged when the
absolute difference between iterations for each parameter was less than tol          other than specifying
the linear kernel  we used the default parameters from the libsvm package      the regularization parameter
c for the o b set to   after cross validation with a       split of the data 
the classifiers were trained on a random    percent of the     examples and were tested on the remaining
   percent  this process was repeated    times and the accuracy results averaged  for each classifier  we
simulated our adversary using one of the aforementioned models  for the lr and svm only the testing
data was corrupted by the adversary  while both the training and testing data was corrupted for the o b
percetron  this follows the approach of       the results are presented in figure   

 

discussion

for the deletion case it is clear that the o b perceptron algorithm performs significantly better than the
other classifiers  for both choices of cost vectors  for example  with the deletion uniform cost pairing  when
n     the o b algorithm outperforms the svm by nearly    percent and the lr by    percent  with
the deletion mi pairing  when n     the o b algorithm again outperforms the svm by about    percent
and outperforms the lr by about    percent  in addition  the svm algorithm performs better than linear
regression  this is to be expected since svm seeks to maximize the geometrical margin  intuitively since
the prediction will generally be farther from the p
decision boundary  our adversary will have to cause more
damage to affect the prediction  as measured by jj wj xj  
similarly  the gaussian noise uniform cost pairing shows that   up to about n       the o b perceptron
algorithm outperforms the other classifiers  when n      the o b algorithm outperforms the other classifiers
by roughly    percent 
however  in the gaussian noise mi case the adversary is much less effective at influencing the classifiers 
demonstrated by the fact that all three perform roughly at the same level   from
p       percent  this should
be the case in general  since the expected damage an adversary can inflict is jj wj  xj  j   where j is
 

fi a  du

 b  cu

 c  dmi

 d  cmi

figure    classification accuracy for the three linear classifiers under different adversarial models   a  and
 b  show the results using uniform feature weights and the feature deletion model  respectively   c  and  d  
in turn  show results using mi selected feature weights and the gaussian noise feature corruption model 
the progression from  a  to  d  represent a decrease in classification difficulty  note the scales on the y axes 
p
the mean of the feature j as opposed to jj wj xj   in addition the data for our problem is very sparse
 calculated to be     percent sparse  with each feature having a small variance  so for most features xj
will be close to j   thus it is very difficult for our adversary to affect the data  especially when we limit her
ability to change those features which do not differ greatly from their mean 
the results also highlight the importance of the cost vector  allowing the adversary to select the n
most important features is much worse than when we weight the feature cost according to their mutual
information  even though the adversary can choose to corrupt more features overall  the uniform cost tests
also show that each classification is dependent on only a handful of features since deleting even one causes
a significant drop in accuracy  this  again  is expected due to the sparsity of the data 
overall we have demonstrated the sensitivity of these algorithms to such feature manipulation  we have
also shown that significant improvements can be made by choosing an algorithm tailored to the problem and
by selecting an appropriate cost vector for the training data 

 

fi 

future work

to apply the results in     as we did here there were a number of problems we faced  first  as was demonstrated  the performance gain of the o b model is not as drastic for the case of feature corruption and does
not address uncertainty in the classifications  secondly the adversarial model used has certain problems
when ranking features with uncertainty  finally it is difficult to extend the o b algorithm to problems using kernelization and to respect the condition that the adversary can only modify the original input features 
we hope however to address these difficulties adequately and to develop useful diagnostic tools from such
experiments 
in particular  we are both interested in applying these results to the diagnosis of certain psychological
disorders  this problem is particularly difficult due to uncertainty in both the input features and classifications  as opposed to other diagnostic problems  there are rarely tests which can provide objective data
and the underlying mechanics are often poorly understood  many of the tests that attempt to provide such
objective data are often prohibitively expensive  such as mris  and are often not covered under health
insurance plans 

references
    c  c  chang and c  j  lin  libsvm  a library for support vector machines  acm transactions on
intelligent systems and technology                    
    m  clayton  cyberattack on illinois water utility may confirm stuxnet warnings  the christian science
monitor          
    o  dekel and l  xiao  learning to classify with missing and corrupted features  machine learning 
                   
    a  frank and a  asuncion  uci machine learning repository       
    p  laskov and r  lippmann  machine learning in adversarial environments  machine learning           
          
    t  shanker  u s  weighs its strategy on warfare in cyberspace  the new york times          

 

fi