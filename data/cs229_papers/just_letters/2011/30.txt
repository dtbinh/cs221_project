accelerating checkers ai evolution
karl cobbe

paul lee

stanford university

stanford university

stanford university

kcobbe stanford edu

pslee cs stanford edu

nc  agom stanford edu

abstract
this project aims to determine the extent to which evolutionary algorithms can be enhanced by extrapolating local
evolutionary trends in the state space of possible agents 
by evaluating the fitness of many clustered agents  evolutionary algorithms generate a significant quantity of information about the local structure of the state space  basic implementations of evolutionary algorithms  however  do
not fully utilize this information  in particular  mutations
for each generation are random  they ignore any structure
in the state space  conventional evolution advances solely
based on selecting the fittest agents from generation to generation  we propose to learn structure in the state space 
through svm regression  and to favor those mutations which
perform well  as estimated by the learned model  we call
this method svm enhanced evolution  we show that when
used to evolve ai agents to play checkers  svm enhanced
evolution consistently out performs basic implementations
of evolutionary algorithms in both noisy and non noisy conditions  by extrapolating local evolutionary trends  svm
enhanced evolution achieves locally optimal performance in
approximately a third as many generations as conventional
evolution 

  

introduction

to provide the framework for testing svm enhanced evolution  we chose to evolve neural networks to play checkers 
we repeatedly conducted evolution on a fixed set of agents 
measuring the rates at which performance improves  we
computed both the average rate of performance improvement in conventional evolution and the average rate of performance improvement in svm enhanced evolution  this
was the metric used to compare the two algorithms 
in the case of playing checkers  as well as many other evolutionary problems  the fitness evaluation function is costly 
to get a reliable measure of the performance of any given
agent  hundreds of games of checkers must be played  applying svm regression to estimate local fitness levels in the
agent state space is orders of magnitude less computationally intensive than conventional evaluations  since playing
through an entire game of checkers is a relatively complex
computation  bypassing actual game play can significantly
improve the rate of evolution  in this way  svm enhanced
evolution benefits from the randomness of conventional evo 

andres gomez emilsson

lution  as well as the intelligence and guidance of machine
learning 

  

prior work

the idea of evolving neural networks to play checkers is
based on the success chellapilla and fogel had in evolving
their own checkers neural networks  in       on far less sophisticated hardware   they did not attempt to use machine
learning to speed up the evolution 

  

method

we will begin by describing the conventional evolutionary
algorithm  and then we will describe how we implemented
svm regression to improve conventional evolution 

   

implementation

each neural network has    inputs  for each playable square
on the checkers board   the inputs are   for a red piece 
   for a black piece      for a red king       for a black king 
and   for empty  valuing kings and checkers in a     ratio is known to be a decent approximation  a high positive
value indicates a board position favorable to red  and a high
negative value indicates a board position favorable to black 
aside from the second hidden layer  there is a single additional input to the output neuron  namely  the piece count
difference of the inputted board  number of black pieces subtracted from the number of red pieces  with kings appropriately weighted   the weight of this piece difference input 
like all other weights in the neural network  is an evolvable
parameter 
assigning an estimated value to board positions is the
bulk of our ai strategy  to actually select a move  the ai
performs a standard minimax search of the game tree down
to a ply depth of    since such a limited depth does not
capture much of the game space  it is clear that the ability of the neural network to value board positions becomes
important for our ai to succeed 

   

conventional evolution

to test the effectiveness of conventional evolution  we began by evolving a neural network with   hidden layers of   
neurons and    neurons  this was following the procedure
used by chellapilla and fogel  using these relatively large

fifigure    the figure on the left depicts the exploration of the fitness landscape performed by a single
evolutionary iteration  it highlights and makes use of only a fraction of the information available  namely 
the sections that score higher than a set threshold in the evaluation function for a particular region of the
state space  the figure on the right depicts the approach explored in this project  where the historical
evaluations are taken into account to infer the underlying structure of the fitness landscape 
neural networks  we were able to obtain similar results  as
coarsely evaluated against online checkers programs 
to implement svm enhanced evolution  we chose to significantly reduce the number of dimensions in the agent state
space  by making each agent a single layer neural network 
in which each of the    inputs directly feeds into the output 
we can uniquely describe each agent in    dimensions     dimensions for the input weights  plus a weight for the piece
differential   all comparisons between conventional evolution and svm enhanced evolution were made using these
single layer neural networks 
each generation is seeded by    agents  each agent produces a single child  so there are     agents in all  per generation  each agent then plays numgames games  half
as red  half as black  against a standard minimax strategy 
for reference  a standard minimax strategy can play competent  mediocre checkers  each neural net receives      
or    points for a win  draw  or loss  respectively  the   
most successful agents become the center of the subsequent
generation  by way of mutation 
evaluating an agent can be expensive  as each game played
is comparatively computationally expensive  we must therefore tradeoff between speed and confidence in our evaluations  in this way  numgames is an important parameter
to select  averaging over relatively few games yields evaluations with high variance  averaging over many games yields
evaluations with low variance but at the cost of computational resources 
in addition to numgames  many other important evolutionary parameters must be selected to govern the mechanics of each generation  in particular  the following questions
must be answered  how large should each generation be 
how many survivors should pass their genes onto the next
generation  these questions are difficult to answer  and different answers often yield markedly different results  if we
allow too few survivors or perform too coarse evaluations  in 

figure    the figure above depicts our checkers
graphical user interface 

formation is easily lost through generations  if we allow too
many survivors or perform too thorough evaluations  computational resources are needlessly wasted  we hardcoded
these parameters based on empirically observing which parameters led to high performing evolutionary runs  determining a generalized optimal answer is beyond the scope of
this paper  but it is nevertheless an important consideration 

   

svm enhanced evolution

svm enhanced evolution requires the following modifications to conventional evolution  the performance of each
agent in a given generation is recorded  and this information is stored even beyond the lifetime of the agent  at the
start of each generation  svm regression is performed on the
dataset of past agent performance  this generates a model
of the agent state space  a large number of children are
then generated through mutation  the     best children  as
judged by the learned model  are selected  and conventional
evolution resumes 

fisimply put  svm enhanced evolution provides a layer of
filtering on the mutations of each generation  only promising mutations are evaluated  the rest are discarded  because evaluation by the learned model requires negligible
computation  as compared to conventional evaluation  svm
enhanced evolution can consider a much larger number of
children during each generation 

   

parameters

with the use of cross validation we found that the radial
basis function was the most successful kernel type  in every
run  cross validation over several different parameter sets
determined the appropriate cost and gamma parameters for
this kernelization 
there are two important parameters unique to svm enhanced evolution  the first is taillength  and the second is maxjump 
taillength determines how many data points are considered when performing svm regression  in particular  the
most recent taillength data points are used  as more
data points are gathered  the oldest data points are discarded  this process tracks the path evolution has traversed
through the state space  called the tail 
the second important parameter unique to svm enhanced
evolution is maxjump  maxjump determines the maximum euclidean distance permitted between a parent and a
child  when selecting that child based on the results of svm
regression 
we will discuss the nature of these parameters  how we
chose to select their values  and how poor parameter selection impacts evolution 

  

figure    regular evolution vs  svm enhanced
evolution on a non noisy data set

results and discussion

we tested svm enhanced evolution against conventional
evolution on several different parameter sets  two of these
sets are shown in figures   and    these graphs each contrast   generation runs    of which use svm enhanced evolution and   of which use conventional evolution  svm enhanced evolution is colored orange  and conventional evolution is colored purple  the generation runs in figure   use a
relatively conservative parameter sets  the generations are
large  there are a relatively high number of survivors from
generation to generation  and  each agent is thoroughly evaluated before any comparisons are made  in contrast  figure
  is a much noisier data set  generations are smaller  few
agents survive between generations  and  only a handful of
games are played between agents before selecting the winners 
naturally  the generations in figure   can be evaluated
faster than the generations in figure    however  improving
performance level takes more generations in the noisier runs 
this is the essence of the tradeoff between thorough and
coarse evaluation 
one way of quantitatively assessing the relative speed at
which the strategies perform is to compare the average number of generations required to achieve a certain performance
level  table   summarizes the number of generations re 

figure    regular evolution vs 
evolution on a noisy data set

svm enhanced

fiquired to achieve approximately     of asymptotic performance  the parameter numgames in the table corresponds to the number of games that are used to asses the
performance of the agents  when this number is small  the
evaluation function is less precise and the data points are
therefore more noisy  in both conditions  svm enhanced
evolution outperforms regular evolution by an average factor of    even in noisy conditions svm regression was able
to illuminate underlying structure of the local fitness landscape 
it is interesting to examine the cause for the performance
differential between svm enhanced evolution and conventional evolution in the noisy runs in figure    in these runs 
the evaluation of any given generation contains relatively unreliable data  in conventional evolution  the algorithms only
memory lies in the genes of the latest generation  in contrast  svm enhanced evolution maintains memory not only
in this generation  but also in the dataset of the performance
of past generations  it is on this dataset that svm regression is performed to obtain an estimated model of the state
space  this increase in relevant memory enables svm evolution to learn from generations that conventional evolution
has long forgotten  against coarse generation evaluations 
this proves to be a significant advantage 
table    average number of generations to score
of    by type of evolution
noise level svm enhanced regular
 
  
   
 
  
   
  
  
  
  
   
    
   
   
    
either overvaluing or undervaluing taillength negatively impacts svm enhanced evolution  if too few data
points are in the tail  then svm regression is unable to estimate an accurate model of the state space  in this case  any
extrapolation done based on the results of svm regression is
only likely to hinder evolution  similarly  if too many data
points are used  then svm regression might be operating on
a largely stale dataset  long obsolete genotypes will inhibit
svm regressions ability to analyze more recent trends  as
before  extrapolation based on such results will likely hinder
evolution 
regarding the parameters  we chose to set taillength
to be      we are working in a    dimensional space  and we
figured that    data points per dimension was a reasonable
starting point  after conducting several trials with taillength ranging from     to       we empirically observed
    to be a reasonable choice 
we found that if the parameter  maxjump  is too low 
then svm regression will have a negligible impact on conventional evolution  if maxjump is too high  then svm
regression will extrapolate too far  overestimating the performance of many child agents  thereby steering evolution
off course 

these results were again confirmed by empirical observation  excessively low or high values of maxjump yielded
poor results  we selected an intermediate value for maxjump 
which we observed performed well 

  

topics for further research

further research can focus on two general directions  optimizing the current method  and exploring further applications of svm enhanced evolution 
for the purpose of improving our current method  the
parameter selection of the evolutionary algorithm can be
optimized  the process of parameter selection could be automated so that parameters are updated from generation to
generation  adapting as needed  based on feedback from generation performance  this could significantly improve the
overall performance of svm enhanced evolution  furthermore  it might be worth exploring different kernelizations 
the current approach uses a radial basis kernel  however 
a comprehensive study of the structure of the state space
might show that other kernel types are more appropriate 
a possible way to improve the precision of the evaluation
function would be to use a tournament rating system such
as the elo algorithm  by doing this  the necessary number
of games played to assess the fitness of the different agents
within a generation could be reduced without compromising
the accuracy of the evaluation  the elo rating system could
cope with the statistical variability in game outcomes much
better than simple averaging  in this way  a reliable rating
for an untested agent could be obtained with fewer games 
with regards to the applicability and generality of svm
enhanced evolution  it is worth investigating how well this
method scales to higher dimensions  it is possible to argue
that most state spaces have an underlying structure  but
high dimensionality might make the task of detecting that
structure intractable  we showed that the radial basis kernelization can detect some useful underlying structure of the
fitness of the state space of neural networks  but does this
generalize to the state spaces of other types of strategies 
possibly represented in higher dimensions 

  

bibliography

fogel  david b  blondie    playing at the edge of ai   san
francisco  morgan kaufmann        print 
fogel  david b  evolving a checkers player without relying on human experience  intelligence                    
web 

fi