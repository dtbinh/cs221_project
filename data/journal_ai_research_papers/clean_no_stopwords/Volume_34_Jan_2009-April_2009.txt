Journal Artificial Intelligence Research 34 (2009) 521567

Submitted 09/08; published 04/09

Anytime Algorithm Optimal Coalition Structure Generation
Talal Rahwan
Sarvapali D. Ramchurn
Nicholas R. Jennings

TR @ ECS . SOTON . AC . UK
SDR @ ECS . SOTON . AC . UK
NRJ @ ECS . SOTON . AC . UK

School Electronics Computer Science,
University Southampton, Southampton, SO17 1BJ, U.K.

Andrea Giovannucci

AGIOVANNUCCI @ IUA . UPF. EDU

SPECS Laboratory, Pompeu Fabra University, Barcelona, Spain.

Abstract
Coalition formation fundamental type interaction involves creation coherent
groupings distinct, autonomous, agents order efficiently achieve individual collective goals. Forming effective coalitions major research challenge field multi-agent
systems. Central endeavour problem determining many possible
coalitions form order achieve goal. usually requires calculating value every possible coalition, known coalition value, indicates beneficial coalition
would formed. values calculated, agents usually need find
combination coalitions, every agent belongs exactly one coalition,
overall outcome system maximized. However, coalition structure generation problem
extremely challenging due number possible solutions need examined,
grows exponentially number agents involved. date, therefore, many algorithms
proposed solve problem using different techniques ranging dynamic programming, integer programming, stochastic search suffer major limitations
relating execution time, solution quality, memory requirements.
mind, develop anytime algorithm solve coalition structure generation problem. Specifically, algorithm uses novel representation search space,
partitions space possible solutions sub-spaces possible compute upper
lower bounds values best coalition structures them. bounds
used identify sub-spaces potential containing optimal solution
pruned. algorithm, then, searches remaining sub-spaces efficiently
using branch-and-bound technique avoid examining solutions within searched subspace(s). setting, prove algorithm enumerates coalition structures efficiently
avoiding redundant invalid solutions automatically. Moreover, order effectively test
algorithm develop new type input distribution allows us generate reliable benchmarks compared input distributions previously used field. Given new
distribution, show 27 agents algorithm able find solutions optimal
0.175% time required fastest available algorithm literature. algorithm
anytime, interrupted would normally terminated, still provide solution
guaranteed within bound optimal one. Moreover, guarantees provide
quality solution significantly better provided previous state
art algorithms designed purpose. example, worst case distribution given 25
agents, algorithm able find 90% efficient solution around 10% time takes find
optimal solution.
c
2009
AI Access Foundation. rights reserved.

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

1. Introduction
Multi-agent systems considered important rapidly expanding area research artificial
intelligence. due natural fit many real-world scenarios, wide variety applications (Jennings, 2001). Now, typically, agents multi-agent system need organized
roles, relationships, authority structures govern behaviour clearly
defined (Horling & Lesser, 2005). Different organizational paradigms include hierarchies, teams,
federations, many others, strengths weaknesses, making suitable problems, less suitable others. Among organizational paradigms
becoming increasingly important coalitions. Coalitions distinguished
organizations goal-directed short-lived; i.e. coalitions formed purpose
mind, dissolved purpose longer exists, cease suit
designed purpose, profitability lost agents depart (Horling & Lesser, 2005). Another defining feature within coalition, agents coordinate activities order
achieve coalitions goal(s), coordination takes place among agents belonging different
coalitions (except coalitions goals interact). Moreover, organizational structure within
coalition usually flat (although could coalition leader acting representative
group whole).
area coalition formation received considerable attention recent research,
proved useful number real-world scenarios multi-agent systems. example,
e-commerce, buyers form coalitions purchase product bulk take advantage price
discounts (Tsvetovat, Sycara, Chen, & Ying, 2000). e-business, groups agents formed
order satisfy particular market niches (Norman, Preece, Chalmers, Jennings, Luck, Dang,
Nguyen, V. Deora, Gray, & Fiddian, 2004). distributed sensor networks, coalitions sensors
work together track targets interest (Dang, Dash, Rogers, & Jennings, 2006). distributed
vehicle routing, coalitions delivery companies formed reduce transportation costs
sharing deliveries (Sandholm & Lesser, 1997). Coalition formation also used information
gathering, several information servers form coalitions answer queries (Klusch & Shehory,
1996).
Generally speaking, coalition formation process viewed composed
three main activities outlined (Sandholm, Larson, Andersson, Shehory, & Tohme,
1999):
1. Coalition Value Calculation: context, number coalition formation algorithms
developed determine potential coalitions actually formed.
so, typically calculate value coalition, known coalition value,
provides indication expected outcome could derived coalition
formed. Then, computed coalition values, decision optimal
coalition(s) form taken. way value calculated depends problem
investigation.
electronic marketplace, example, value coalition buyers calculated
difference sum reservation costs coalition members
minimum cost needed satisfy requests members (Li & Sycara, 2002).
information gathering, coalition value designed represent measure
closely information agents domains related (Klusch & Shehory, 1996). cases
agents rationality bounded due computational complexity, value coalition
522

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

may represent best outcome achieve given limited computational resources
solving problem (Sandholm & Lesser, 1997).
2. Coalition Structure Generation: computed coalition values, coalition structure generation (CSG) problem involves partitioning set agents exhaustive
disjoint coalitions maximize social welfare. partition called coalition
structure. example, given set agents = {a1 , a2 , a3 }, exist five possible coalition structures: {{a1 } , {a2 } , {a3 }}, {{a1 } , {a2 , a3 }}, {{a2 } , {a1 , a3 }}, {{a3 } , {a1 , a2 }},
{{a1 , a2 , a3 }}.
usually assumed every coalition performs equally well, given coalition structure
containing (i.e. value coalition depend actions non-members).
settings known characteristic function games (CFGs), value coalition given characteristic function. Many, clearly all, real-world multi-agent
problems happen CFGs (Sandholm et al., 1999).
Note optimal solution CSG problem one maximizes social welfare.
Now, unlike cooperative environment agents mainly concerned maximizing social welfare, agents selfish environment concerned maximizing
utility. This, however, mean CSG algorithm cannot applied
selfish multi-agent systems. designer systems usually concerned raising overall efficiency system and, many cases, corresponds
maximizing social welfare. end, designer needs design enforcement
mechanism motivates agents join optimal coalition structure and, order
so, first needs know structure is. Moreover, knowing value optimal
coalition structure, knowing value within bound optimal, allows
designer evaluate relative effectiveness coalition structure currently formed
system.
3. Pay-off Distribution: determined coalitions formed, important
determine rewards agent get order coalitions stable.
Here, stability refers state agents incentive deviate coalitions belong (or little incentive weaker types stability). desirable
ensures agents devote resources chosen coalition rather
negotiating with, moving to, coalitions. ensures coalitions
last long enough actually achieve goals. analysis incentives long
studied within realm cooperative game theory. context, many solutions
proposed based different stability concepts. include Core, Shapley value, Kernel (more details found paper Osborne & Rubinstein,
1994). Moreover, schemes developed transfer non-stable pay-off distributions
stable ones keeping coalition structure unchanged (Kahan & Rapoport, 1984,
provide comprehensive review stability concepts transfer schemes game theory).
Note, however, agents cooperative environment incentive dissolve
coalition improves performance system whole. Therefore, pay-off distribution less important, main concern generating coalition structure
maximize social welfare.
523

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

One challenging activities coalition structure generation,
n
due number possible solutions grows exponentially (in O(nn ) (n 2 )
number agents involved (n)). specifically, proved finding optimal
coalition structure NP-complete (Sandholm et al., 1999). combat complexity, number
algorithms developed past years, using different search techniques (e.g.
dynamic programming, integer programming, stochastic search). algorithms, however,
suffer major limitations make either inefficient inapplicable, particularly given
larger numbers agents (see Section 2 details).
motivates aim develop efficient algorithm searching space possible
coalition structures. detail, given CFG setting, wish develop algorithm
satisfies following properties:
1. Optimality: run completion, algorithm must always able return solution
maximizes social welfare.
2. Ability prune: algorithm must able identify sub-spaces potential
containing optimal solution pruned search space.
property critical given exponential nature problem (e.g. given 20 agents,
number possible coalition structures 51,724,158,235,372).
3. Discrimination: algorithm must able verify, search, found
optimal solution, instead proceeding search hope better solution
found.
4. Anytime: algorithm able quickly return initial solution, improve
quality solution searches space, finds optimal one.
particularly important since agents might always sufficient time run
algorithm completion, especially given exponential size search space. Moreover,
anytime makes algorithm robust failure; execution stopped
algorithm would normally terminated, would still provide agents
solution better initial solution, intermediate one.
5. Worst Case Guarantees: algorithm able provide worst-case guarantees
quality solution. Otherwise, generated solution could always arbitrarily worse
optimal one. guarantees important trading solution
quality search time. example, quality current solution known
worse than, say, 95% optimal one, still significant portion
space left searched, agents might decide worthwhile carry
search. Obviously, better guarantees, likely agents
decide stop searching better solution.
research aims outlined above, paper makes following contributions state
art coalition structure generation:
1. provide new representation space possible coalition structures. representation partitions space much smaller, disjoint sub-spaces explored
524

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

independently find optimal solution. opposed widely-used representation (Sandholm et al., 1999; Dang & Jennings, 2004), coalition structures
categorized based number coalitions contain, representation categorizes
coalition structures sub-spaces based sizes coalitions contain. key
advantage representation that, immediately scanning input algorithm
(i.e. coalition values), compute average value coalition structures within
sub-space. Moreover, scanning input, also compute upper lower
bound value best coalition structure could found subspaces. Then, comparing bounds, possible identify sub-spaces
potential containing optimal solution pruned. second major
advantage representation allows agents analyse trade-off
size (i.e. number coalition structures within) sub-space improvement
may bring current solution virtue bounds. Hence, rather constraining
solution fixed sizes, Shehory Kraus (1998) do, agents using representation
make informed decision sizes coalitions choose (since
sub-spaces defined sizes coalitions within coalition structures).
2. develop novel, anytime, integer-partition based algorithm (called IP) coalitions structure generation uses representation discussed above, provides high guarantees quality solutions quickly. Moreover, IP guaranteed return
optimal solution run completion.
3. prove algorithm able enumerate coalition structures efficiently avoiding
redundant invalid solutions. enumeration technique also allows us apply branchand-bound reduce amount search needed.
4. many CSG algorithms literature evaluated using input distributions
defined Larson Sandholm (2000), prove distributions biased
far CSG problem concerned. Moreover, propose new distribution prove
tackles problem, making much suitable evaluating CSG algorithms
general.
5. evaluating time required return optimal solution, compare IP
fastest algorithm guaranteed return optimal solution (i.e. Improved Dynamic Programming (IDP) algorithm Rahwan & Jennings, 2008b). comparison shows IP
significantly faster. detail, IP empirically shown find optimal solution
0.175% time taken IDP given 27 agents.
6. benchmark IP previous anytime algorithms (Sandholm et al., 1999; Dang & Jennings, 2004), show provides significantly better guarantees quality
solutions generates time. detail, empirically show that, various numbers agents, quality initial solution (i.e. solution found scanning
input) usually guaranteed least 40% optimal, opposed n2 (which means
example, 10% 20 agents 8% 25 agents) Sandholm et al.s algorithm
Dang Jenningss algorithm. standard distributions evaluate
algorithm, also find usually terminates searching minute portions
525

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

search space generates near-optimal solutions (i.e. > 90% optimal) searching
even smaller portions search space (i.e. average around 0.0000002% search
space). tremendous improvement aforementioned algorithms could
guarantee solutions higher 50% optimal searching whole space.
Note significantly revised extended version previous papers (Rahwan, Ramchurn, Dang, & Jennings, 2007a; Rahwan, Ramchurn, Giovannucci, Dang, & Jennings, 2007b).
Specifically, provide paper comprehensive review available algorithms
CSG literature. also provide detailed analysis IP algorithm, describe pseudo code
functions used IP, prove correctness function searches different
sub-spaces. mathematical proof also provided regarding way size sub-space computed. Moreover, question validity standard value distributions used
literature, propose new value distribution (called NDCS) suitable evaluating
CSG algorithms. Finally, benchmark algorithm improved dynamic programming
algorithm (IDP) Rahwan Jennings (2008b) (instead standard DP algorithm).
remainder paper organized follows. Section 2, describe algorithms
currently available solving coalition structure generation problem, discuss
relative advantages limitations. Section 3, present novel representation search
space and, Section 4, present integer-partition based algorithm (IP), showing identifies sub-spaces pruned, searches remaining ones without
going invalid redundant coalition structures, using branch-and-bound technique. Section 5 provides empirical evaluation algorithm, benchmarks current state
art CSG literature. Section 6 concludes paper outlines future work. also
provide, appendices, summary main notations employed, well detailed proofs
theorems provided paper.

2. Related Work
Previous algorithms designed coalition structure generation problem
classified two main categories:
Exact algorithms1 using heuristics, integer programming, dynamic programming.
Non-exact algorithms using genetic algorithms, limiting search space way.
Next, discuss advantages limitations algorithms fall within
classes. Throughout paper, denote n number agents, =
{a1 , a2 , , } set agents. Moreover, define order agents follows:
ai , aj A, ai < aj iff < j, ai = aj iff = j. words, have: a1 < a2 < < .
Finally, denote v(C) value coalition C, V (CS) value coalition structure CS.
2.1 Exact Algorithms Coalition Structure Generation
exact algorithms coalition structure generation. developed distinguished based whether use dynamic programming heuristics.
1. Recall exact algorithm one always returns optimal solution exists (Evans & Minieka, 1992).

526

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

follows, outline features discuss relate ultimate goal developing
efficient, anytime, optimal coalition structure generation algorithm.
2.1.1 DYNAMIC P ROGRAMMING
consider computationally efficient algorithms designed return optimal solution. Note
emphasis, here, providing guarantee performance algorithm worstcase scenarios. context, Yeh (1986) developed dynamic programming algorithm solve
complete set partitioning problem. similar algorithm later developed Rothkopf,
Pekec, Harstad (1995) solve winner determination problem combinatorial auctions.
algorithms directly applied find optimal coalition structures, since problems
originally designed solve similar CSG problem.2 Also note
algorithms use basically technique and, therefore, computational
complexity. Thus, throughout paper, distinguish them, refer
dynamic programming (DP) algorithm. biggest advantage algorithm runs
O(3n ) time (Rothkopf et al., 1995). significantly less exhaustive enumeration
coalition structures (which O(nn )). fact, DP polynomial size input.
input includes 2n 1 values, following holds:
O(3n ) = O(2(log2 3)n ) = O((2n )log2 3 )
Therefore, computational complexity algorithm O(y log2 3 ), number
values input. While, one hand, algorithm literature guaranteed
find optimal coalition structure polynomial time (in size input), hand,
main limitation DP generate solutions anytime, large memory
requirement. Specifically, requires maintaining three tables memory containing 2n entries
each.
recently, Rahwan Jennings (2008b) developed Improved Dynamic Programming
algorithm (called IDP) performs fewer operations requires less memory DP (e.g. given
25 agents, performs 38.7% operations, requires 66.6% memory worst
case, 33.3% best). However, IDP return solutions anytime. mentioned earlier,
undesirable, especially given large numbers agents, time required return
optimal solution might longer time available agents.
2.1.2 NYTIME LGORITHMS W ITH W ORST C ASE G UARANTEES
Sandholm et al. (1999) first introduce anytime algorithm coalition structure generation establishes bounds quality solution found far. view coalition
structure generation process search call coalition structure graph (see Figure
1). undirected graph, every node represents possible coalition structure. nodes
categorized n levels, noted LV1 , , LVn level LVi contains coalition structures
contain coalitions. arcs represent mergers two coalitions followed upwards,
splits coalition two coalitions followed downwards.
2. involve partitioning set elements subsets based weights associated
every possible subset.

527

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

Figure 1: coalition structure graph 4 agents.
Sandholm et al. (1999) proved that, order establish bound quality coalition structure, sufficient search first two levels coalition structure graph.
case, bound would equal n, number searched coalition structures would
2n1 . also proved bound tight; meaning better bound exists
search. Moreover, proved search algorithm (other one
searches first two levels) establish bound searching 2n1 coalition structures
fewer. because, order establish bound, one needs go subset coalition structures every coalition appears least once.3 implies smallest subset
coalition structures searched bound established one every
coalition appears exactly once, subset occurs one containing
coalition structures belong first two levels graph.
first two levels searched, additional time remains, would desirable
lower bound search. Sandholm et al. (1999) developed algorithm
purpose. Basically, algorithm searches remaining levels one one, starting bottom
level, moving upwards graph. Moreover, Sandholm et al. also proved bound
improved whenever algorithm finishes searching particular level. interesting
that, searching bottom level (which contains one coalition structure) bound drops
half (i.e. = n2 ). Then, roughly speaking, divisor bound increases one every time
3. Otherwise, coalition appear coalition structures, value coalition happened arbitrarily better value coalitions, every coalition structure containing would
arbitrarily better not.

528

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

two levels searched, seeing one level helps little (Sandholm et al.,
1999).4
algorithm advantage anytime, able provide worst case guarantees quality solution found far. However, algorithm two major limitations:
algorithm needs search entire search space order bound
become 1. words, return solution guaranteed optimal, algorithm
simply performs brute-force search. discussed Section 1, intractable even
small numbers agents.
bounds provided algorithm might large practical use. example,
given n = 24, given algorithm finished searching levels LV1 , LV2 , LV24
(which contain 8,388,609 coalition structures) bound would = n/2 = 12.
means that, worst case, optimal solution 12 times better current
solution. words, value current solution guaranteed worse
8.33% value optimal solution. that, order reduce bound
= n/4, four levels need searched, namely LV23 , LV22 , LV21 , LV20 .
words, searching additional 119,461,563 coalition structures, value
solution guaranteed worse 16.66% optimal value. Similarly,
reduce bound = n/6, algorithm needs search additional 22,384,498,067,085
coalition structures guarantee value solution worse 25%
optimal value. Moreover, guarantee go beyond 50% entire space
searched.
Given limitations Sandholm et al.s (1999) algorithm, Dang Jennings (2004) developed
anytime algorithm also establish bound quality solution found far,
uses different search method. detail, algorithm starts searching top two
levels, well bottom one (as Sandholm et al.s algorithm does). that, however, instead
searching remaining levels one one (as Sandholm et al. do), algorithm searches
specific subsets remaining levels. Figure 2 compares performance algorithms, and, looking figure, see neither two algorithms significantly
outperforms other.
Note, however, algorithms meant case entire space
eventually searched. enough time perform search,
would used dynamic programming algorithm, performs search much quicker.
Instead, algorithms mainly developed cases space large
fully searched, even dynamic programming algorithm used.
discussed two algorithms use similar techniques (i.e. Sandholm et al., 1999
Dang & Jennings, 2004), discuss different approach also provide solutions anytime, establish worst-case guarantees quality solution. involves use
standard problem solving techniques rely general purpose solvers. detail, coalition structure generation problem formulated binary integer programming problem (or
n n
4. precise, depending number agents level searched, bound either

= 2, 3, , n. However, ease discussion without loss generality, assume throughout
n
paper bound simply
.

529

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

Figure 2: Given 24 agents, figure shows, log scale, comparison bound provided Sandholm et al. (1999) provided Dang Jennings (2004), given
different numbers searched coalition structures.

0-1 integer programming problem), since variable representing possible coalition either
take value 1 (indicating belongs formed coalition structure) 0 (indicating
doesnt). Specifically, given n agents, integer model CSG problem formulated
follows:
Maximize

2n
P

v(Ci ) xi

i=1

subject Z X = eT
X {1, 0}n
Z n 2n matrix zeros ones, X vector containing 2n binary variables,
eT vector n ones. detail, every line Z represents agent, every column
represents possible coalition. X, element xi = 1 corresponds coalition Ci
selected coalition structure. first constraint ensures selected coalitions
disjoint exhaustive.
integer programming problem typically solved applying linear relaxation coupled
branch-and-bound (Hillier & Lieberman, 2005). However, main disadvantage approach huge memory requirement, make applicable small numbers agents
(see Section 5 details).
530

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

2.2 Non-Exact Algorithms Coalition Structure Generation
algorithms provide guarantees finding optimal solution, provide
worst-case guarantees quality solutions. Instead, simply return good solutions.
However, fact return solution quickly, compared algorithms,
often makes class algorithms applicable, particularly given larger numbers agents.
Generally speaking, long regularity search space (i.e., evaluation
function arbitrary), genetic algorithms potential detect regularity hence
find coalition structures perform relatively effectively. end, Sen Dutta (2000)
developed genetic algorithm coalition structure generation. algorithm starts initial
set candidate solutions (i.e. set coalition structures) called population, gradually evolves towards better solutions. done three main steps: evaluation, selection,
re-combination. detail, algorithm evaluates every member current population,
selects members based evaluation, constructs new members selected ones
exchanging modifying contents. details implementation found
paper Sen Dutta (2000). main advantage algorithm return solutions
anytime, scales well increase number agents. However, main
limitation solutions provides guaranteed optimal, even guaranteed
within finite bound optimal. Moreover, even algorithm happens find optimal
solution, possible verify fact.
Another algorithm belongs class algorithms one developed Shehory
Kraus (1998). algorithm greedy operates decentralized manner. heuristics
propose (in order reduce complexity finding optimal coalition structure) involve
adding constraints size coalitions allowed formed. Specifically,
coalitions size q < n taken consideration. main advantage algorithm
take consideration overlapping coalitions.5 Moreover, Shehory Kraus prove
solution provide guaranteed within bound optimal solution. However,
optimal, mean best possible combination permitted coalitions. hand,
algorithm provides guarantees quality solutions compared actual optimal
could found coalitions taken consideration.
summarize, discussed earlier, main limitation algorithms provide
guarantees solutions generate search terminate. However,
algorithms scale well increase number agents, making particularly
suitable cases number agents large algorithm exponential
complexity executed time.
discussing different approaches coalition structure generation problem,
see approaches suffers major limitations, making either inefficient
inapplicable. motivates aim develop efficient CSG algorithms applied
wider range problems, taking consideration objectives outlined Section
1. mind, first present Section 3 novel representation search space,
present Section 4 novel algorithm belongs first class aforementioned
classification. show, algorithm avoids limitations exist state-of-the-art
5. solution containing overlapping coalitions means agents may participate one coalition
time.

531

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

algorithms belonging class, meets design objectives placed Section 1 CSG
algorithms.

3. Search Space Representation
section, describe novel representation search space (i.e. space possible
coalition structures). Recall space representation employed existing anytime algorithms undirected graph (see Figure 1 example), vertices represent coalition
structures (Sandholm et al., 1999; Dang & Jennings, 2004). representation, however, forces
possible solutions explored order guarantee optimal one found. Given
this, believe ideal representation search space allow computation solutions anytime, establishing bounds quality, allow pruning space
speed search. objective mind, section describe representation. particular, supports efficient search following reasons. First, partitions
space smaller, independent, sub-spaces identify upper lower bounds,
thus, compute bound solutions found search. Second, prune
sub-spaces since identify ones cannot contain solution better
best one found far. Third, since representation pre-determines size coalitions present
sub-space, agents balance preference certain coalition sizes cost
computing solution sub-spaces. Next, formally define representation
search space, describe algebraic properties, describe compute worst case bounds
quality solution representation allows us generate.
3.1 Partitioning Search Space
partition search space P defining sub-spaces contain coalition structures
similar according criterion. particular criterion specify based integer
partitions number agents.6 Recall integer partition n multiset positive
integers add exactly n (Andrews & Eriksson, 2004). example, given n = 4,
five distinct partitions are: [4], [3, 1], [2, 2], [2, 1, 1], [1, 1, 1, 1].7 easily shown
different ways partition set n elements directly mapped integer partitions
n, parts integer partition correspond cardinalities subsets (i.e.
sizes coalitions) within set partition (i.e. coalition structure). instance, coalition
structures {{a1 , a2 }, {a3 }, {a4 }} {{a4 , a1 }, {a2 }, {a3 }} mapped integer partition
[2, 1, 1] since contain one coalition size 2, two coalitions size 1. define
aforementioned mapping function F : P G, G set integer partitions n.
Thus, F defines equivalence relation P CS CS 00 iff F (CS) = F (CS 00 ) (i.e.
sizes coalitions CS CS 00 ). Given this, pre-image8
integer partition G, noted PG = F 1 [{G}], contains coalition structures correspond
6. criteria could developed partition space smaller sub-spaces, one develop
allows us choose coalition structures certain properties show later.
7. presentation clarity, square brackets used throughout paper (instead curly ones) distinguish
multisets sets.
8. Recall pre-image inverse image G G F : P G subset P defined F 1 [{G}] =
{CS P|F (CS) = G}.

532

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

integer partition G. Every pre-image represents sub-space representation.
implies number sub-spaces representation number
possible integer partitions, grows exponentially n. number, however, remains
insignificant compared number possible coalitions coalition structures (e.g., given
24 agents, number possible integer partitions 1575, number possible
coalitions 16777215, number possible coalition structures nearly 4.4 1017 ).
categorize sub-spaces levels based number parts within integer partitions. Specifically, level Pi = {PG : |G| = i} contains sub-spaces correspond
integer partition parts (see Figure 3 example 4 agents).9 follows, show
compute bounds sub-spaces (PG : G G) representation.

Figure 3: example representation search space given 4 agents.

3.2 Computing Bounds Sub-spaces
sub-space PG , possible compute upper lower bound value
best10 coalition structure could found it. end, let Ls list coalitions
size s, let maxs , mins , avgs , maximum, minimum, average value
coalitions Ls respectively. Moreover, given
Q integer partition G, let TG Cartesian product
lists Ls : G. is, TG = sG (Ls )G(s) , G(s) multiplicity
9. Note levels representation basically appear coalition structure graph,
except coalition structures within level categorized sub-spaces. words, coalition
structures belong sub-spaces Pi belong LVi .
10. Throughout paper, coalition structure described best highest value.

533

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

G. example, given G = [5, 4, 4, 4, 1, 1], TG = (L5 )1 (L4 )3 (L1 )2 . Note
TG contains many combinations coalitions considered invalid coalition structures.
coalitions within combinations may overlap. example, T[2,1,1]
contains following combination, {{a1 , a2 }, {a1 },{a3 }}, valid coalition structure
agent a1 appears two coalitions. Now, consider value element (i.e.
combination coalitions) TG sum values coalitions element,
maximum
value element TG take, denoted AXG , computed follows:
P
AXG = sG maxs G(s). Based this, easy demonstrate AXG upper
bound value best coalition structure PG (since PG subset TG ).
Similarly, minimum
value element TG take, denoted ING , computed
P
follows: ING =
sG mins G(s). Although could intuitively considered lower
bound value best coalition structure (i.e. solution) PG , show actually
possible compute higher (i.e. better) lower bound ING .
detail, let AV GG average value coalition structures PG . Then,
AV GG would lower bound value best coalition structure PG (since average
always greater than, equal to, minimum). key point note, here, compute
AV GG without go coalition structures PG . Instead, compute
simply summing averages coalition lists (see Theorem 1), averages
computed immediately scanning input, significantly smaller space
possible coalition structures.
Theorem 1. Let G = [g1 , , gi , , g|G| ] integer partition, let AV GG average
values coalition structures PG . Also, let avggi average values
coalitions Lgi . Then, following holds:
AV GG =

|G|
X

avggi

i=1

Proof. See Appendix B.
described novel representation search space, present (in following section)
anytime algorithm uses representation search possible coalitions structures
eventually find optimal one.

4. Solving Coalition Structure Generation Problem
Assuming value every coalition C given characteristic function
v(C) R,
P
value every coalition structure given function V (CS) = CCS v(C), goal
search set possible coalition structures, noted P, order find optimal
coalition structure computed as:
CS = arg max V (CS)
CSP

(1)

given v(C) C 2A \{}. Note that, section, terms coalition structure
solution used interchangeably.
Basically, novel anytime Integer-Partition based algorithm (which call IP) consists
following two main steps:
534

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

1. Scanning input order compute bounds (i.e. AXG AV GG ) every subspace PG so, (at small cost):
(a) find best coalition structures within particular sub-spaces.
(b) prune sub-spaces based upper-bounds.
(c) establish worst-case bound quality best solution found far.
2. Searching within remaining sub-spaces techniques use allow us to:
(a) avoid making unnecessary comparisons coalitions generate valid coalition
structures (i.e. contain disjoint coalitions).
(b) avoid computing coalition structure once.
(c) apply branch-and-bound reduce amount search done.
following sub-sections describe aforementioned steps detail. end,
use CS 0 denote best coalition structure found far, G 0 G denote integer
partitions represent sub-spaces searched.
4.1 Scanning Input
input coalition structure generation problem value associated coalition,
i.e. v(C) C 2A \{}. One way representing input use table containing
every coalition along value. Another way agree ordering coalitions,
use list containing values ordered coalitions (i.e. first value list
corresponds first coalition, second value corresponds second coalition, on).
use latter representation since require maintaining coalitions
memory. detail, assume input given follows: v(Ls ) {1, 2, . . . , n},
v(Ls ) list containing values coalitions size s. Moreover, assume
coalitions Ls ordered lexicographically. example, coalition {a1 , a2 , a4 }
elements ordered according indices, coalition found {a1 , a2 , a3 }
{a1 , a3 , a4 } list L3 (this depicted Figure 4). ordering easily generated
using techniques used Rahwan Jennings (2007). Next, describe individual
steps algorithm depicts scanning process (see Algorithm 1).
first, scan value one coalition size n (i.e. grand coalition). would
value coalition structure P[n] (which sub-space P1 ). that,
scan values coalitions size 1 (i.e. singleton coalitions), summing
values, get value coalition structure P[1,1,...,1] (which sub-space
Pn ). point (step 1), possible compute best coalition structure found far (i.e.
CS 0 ).
searched levels P1 Pn , show search level P2
low cost scanning input. end, let G 2 = {G G : |G| = 2} set
integer partitions contain two parts each. Then, result assumed ordering
b coalition structure CS = {C, C}
b always
input, two complementary coalitions C C
diametrically positioned coalition lists L|C| L|Cb| , happens even |C| = |C 0 |.
example, given 6 agents, coalitions {a1 } {a2 , a3 , a4 , a5 , a6 } diametrically positioned
535

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

Algorithm 1 : scanAndSearch() scan input, generate initial solutions bounds.
Require: n, {v(Ls )}s{1,2,...,n}
1: CS 0 arg maxCS{ {a1 ,...,an }, {{a1 },...,{an }} } V (CS)
2: = 1 b n
2 c
3:
sb n
4:
= sb {if cycling list.}
5:
end = b|v(Ls )|/2c
6:
else
7:
end = |v(Ls )|
8:
end
9:
Set maxs , maxsb, vmax , set sums , sumsb 0
10:
x = 1 end {cycle lists v(Ls ) v(Lsb).}
11:
x
b |v(Ls )| x + 1
12:
v v(Ls )x , vb v(Lsb)xb {extract element x, xb v(Ls ), v(Lsb).}
13:
vmax < v + vb
14:
vmax v + vb
15:
xmax = x {record index v(Ls ) v located.}
16:
end
17:
maxs < v
18:
maxs v {record maximum value v(Ls ).}
19:
end
20:
maxsb < vb
21:
maxsb vb {record maximum value v(Lsb).}
22:
end
23:
sums sums + v , sumsb sumsb + vb
24:
end
25:
x
bmax |v(Ls )| xmax + 1
26:
V (CS 0 ) < V ({Lxs max , Lxsbbmax })
27:
CS 0 {Lxs max , Lxsbbmax } {update best coalition structure found far.}
28:
end
29:
avgs sums /|v(Ls )| , avgsb sumsb/|v(Lsb)| {compute averages.}
30: end
31: G 0 G \ G 2
32: G G 0 {compute upper lower bounds sub-space G 0 .}
P
33:
AXG P sG maxs G(s)
34:
AV GG sG avgs G(s)
35: end
36: U B max[ V (CS 0 ), maxGG 0 [M AXG ] ]
37: LB max[ V (CS 0 ), maxGG 0 [AV GG ] ]
38:

G 0 prune(G 0 , {M AXG }GG 0 , LB )

{prune sub-spaces upper



bound lower LB .}
39:

min[ n/2 , U B /V (CS 0 ) ] {compute worst-case bound V (CS 0 ).}

40:

return CS 0 , , {maxs }s{1,...,n} , G 0 , {M AXG }GG 0 , {AV GG }GG 0

536

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

lists L1 L5 respectively, coalitions {a1 , a2 , a3 } {a4 , a5 , a6 } diametrically
positioned list L3 (see Figure 4 example 6 agents).

Figure 4: example assumed ordering coalition lists.
Based this, every integer partition G = [g1 , g2 ] G 2 , compute values
coalition structures PG simply summing values coalitions scan lists v(Lg1 )
v(Lg2 ), starting different extremities list. lists scanned (steps
10 24), possible obtain two values sum maximized. Moreover,
possible obtain indices lists values located (see xmax x
bmax
computed steps 15 25 respectively). Then, obtaining indices, know
Lg1 Lg2 find two coalitions belong best coalition structure P[g1 ,g2 ] (this
comes fact position value v(Ls ) : {1, ..., n} exactly position
corresponding coalition Ls ).
Note, however, input includes v(Lg1 ) v(Lg2 ) (i.e. include Lg1
Lg2 ). reason, algorithm required return coalition C given position
ordered list L|C| . Rahwan Jennings (2007) developed polynomial-time algorithm
exactly that. Therefore, use find required coalitions compose best coalition
structure P{g1 ,g2 } (see steps 26 27).11
scanning v(Lg1 ) v(Lg2 ), also compute maxg1 maxg2 (steps 17 22),
well avgg1 avgg2 (step 29). Note that, Algorithm 1, scan v(Ls ) v(Lns )
{1, . . . , b n2 c} implies maxs avgs computed {1, . . . , n}. Also
note whole process linear size input (i.e. O(y) = 2n 1 size
input).
11. Lxs mean extract element position x Ls .

537

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

computed maxs avgs every size s, compute upper lower bounds
every sub-space (as steps 32 34). using bounds, possible compute upper
bound U B lower bound LB value optimal coalition structure (see steps 36
37). Hence, every sub-space PG upper bound AXG < LB pruned straight
away. prune function (used step 38) implemented Algorithm 2.
Algorithm 2 :prune(G 0 , {M AXG }GG 0 , ) prune sub-spaces.
1: G G 0
2:
AXG {if upper bound PG lower .}
3:
G 0 G 0 \ G {remove G.}
4:
end
5: end
6: return G 0



Another advantage scanning procedure allows us compute worst-case bound
B
value CS 0 follows: = min( n2 , V U(CS
0 ) ) (see step 39). comes fact
Sandholm et al. (1999) proved value best coalition structure levels LV1 , LV2
LVn (corresponding P1 , P2 , Pn respectively) within bound n2 optimal.
far, scanning input, calculated maxs avgs {1, . . . , n},
searched levels P1 , P2 , Pn , calculated AXG AV GG sub-spaces
within remaining levels (i.e. P3 , ..., Pn1 ), pruned sub-spaces,
established worst-case bound quality best solution found far. Moreover,
possible specify bound 1 within solution acceptable. detail,
best solution found far fits within specified bound (i.e. ) search
required. Otherwise, sub-spaces pruned (if any) must searched.
Next, specify search done.
4.2 Selecting Searching Sub-Space
Given set sub-spaces left scanning input, select sub-space searched,
find best coalition structure it. that, prune remaining sub-spaces
upper bound lower best value found far. process selecting, searching,
pruning, repeated either following termination conditions reached:
best coalition structure found far fits within specified bound .
remaining sub-spaces either searched pruned.
seen Algorithm 3. Basically, algorithm works follows. sub-space PG00
selected searched (step 2).12 PG00 searched (step 3), removed set
remaining sub-spaces (step 4). that, check whether CS 0 modified
search (step 5), and, case, every sub-space upper bound lower V (CS 0 )
pruned (step 6).13 U B updated steps 8 9 respectively, current
12. step 2, actually select integer partition, implies corresponding sub-section searched.
13. Checking whether CS 0 belongs PG00 easily done checking whether sizes coalitions CS 0
match parts G00 .

538

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

best solution fits within specified bound returned (steps 10 11). Otherwise,
whole process repeated given remaining sub-spaces (if any). follows,
elaborate sub-space selection strategy sub-space search algorithm since
key parts algorithm.
Algorithm 3 :searchSpace() search, prune, remaining sub-spaces.
Require: G 0 , {M AXG }GG 0 , A,
1: G 0 6=
2:
Select G00 {select integer partition represents next sub-space
searched.}
3:
4:


CS 0 searchList(G00 , 1, 1, A, CS 0 , CS) {search within PG00 update CS 0 .}
G 0 G 0 \ G00
{remove PG00 list sub-spaces yet
searched.}

5:
6:

CS 0 PG00 {If CS 0 modified searching PG00 .}
G 0 prune(G 0 , {M AXG }GG 0 , V (CS 0 ))
{prune sub-spaces
upper bounds lower V (CS 0 ).}

7:
8:

end
U B max[ V (CS 0 ), maxGG 0 [M AXG ] ] {update upper bound value
optimal coalition structure(s).}

9:
10:
11:
12:
13:
14:



B
0
min[ V U(CS
0 ) , ] {update worst-case bound V (CS ).}

{if CS 0 within specified bound optimal.}
return CS 0
end
end
return CS 0

4.2.1 ELECTING UB -S PACE
easily seen that, unless search sub-spaces upper bound greater
V (CS 0 ), cannot verify CS 0 optimal solution. implies remains greater
1 following sub-spaces searched: {PG : AXG V (CS )}. done
selecting next sub-space searched using following selection rule:
Select G = arg max(M AXG )
GG 0

result selection strategy, sub-spaces upper bound lower V (CS )
searched constitute significant portion search space (see Section 5.3
details). Another result always beneficial search sub-space, even
sub-space contain better solution one found far.
selection strategy ensures U B reduced whenever sub-space searched, improves
worst-case guarantee quality current best solution.
Note selection rule mainly cases optimal solution sought. case
near-optimal solution bound > 1 specified (e.g., = 1.05 means
solution sought needs value least 95% optimal one),
539

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

selection rules may used. example, one could select search smallest sub-space

could, potentially, give value greater equal UB (hoping find acceptable solution
least amount search). expressed as:
Select G =

arg min


(|PG |)

GG 0 :U BG UB

|PG | size (i.e. number coalition structures in) PG . specifically, |PG |
computed follows:
Theorem 2. Let G = [g1 , . . . , g|G| ] integer partition, let |PG | number coalition
structures PG . Moreover, let Csn binomial coefficient,14 let E(G) underlying
set G.15 Then, following holds:
n(g1 +...+g|G|1 )

1
Cgn Cgng
. . . Cg|G|
2
Q
|PG | = 1
sE(G) G(s)!

Proof. See Appendix C.
key point note that, given representation, specify cases computing optimal solution would costly and, given this, modify selection rule
accordingly speed search.
Another advantage able control sub-spaces searched agents
choose types coalition structures build according computational resources
private preferences. example, argued computation time could reduced
limit size coalitions formed (Shehory & Kraus, 1998). However,
costly, self-imposed constraint since possibly means neglecting number highly efficient
solutions. Instead, using IP, possible determine, ex-ante (i.e. performing
search), sub-spaces promising according upper lower bounds. Therefore
computation time focused sub-spaces gains traded-off
computation time.
cases, agents may need form q coalitions (Shehory & Kraus, 1995).
example, may need perform q tasks therefore need divide q teams perform
tasks separately. Moreover, may wish coalitions maximum size z
may certain constraints amount resources available coalition. using
representation, preferences naturally expressed search directed fit
preferences transparently. Formally, search space easily redefined follows:
G 00 = {G G : |G| = g G : |g| z}
cases agents express preferences coalition structures certain
sizes, now, priori, balance preferences quality solutions
14. Recall binomial coefficient represents number possible combinations size taken n elements,
n!
computed follows: Csn = k!(nk)!
, n! factorial n.
15. Recall underlying set E(G) multiset G subset G element G appears
E(G). example, {1, 2} underlying set [1, 1, 2].

540

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

obtained. able determine worst-case bound optimal
U B
search given sub-space generate (i.e. AV
GG ). next describe search
chosen sub-space.
4.2.2 EARCHING UB -S PACE
Given integer partition G = [g1 , g2 , , g|G| ] G, need cycle coalition
structures belong PG order find best one. Here, without loss generality,
assume g1 g2 , g|G| . Perhaps obvious way performing cyclation


ff

process shown Figure 5. Here, variable CS = C1 , C2 , , C|G| used cycle though
coalition structures PG follows. First, C1 assigned one coalitions Lg1 .
that, C2 used cycle Lg2 coalition overlap C1 found.
that, C3 used cycle Lg3 coalition overlap {C1 , C2 } found.


repeated every Ck CS assigned coalition Lgk . case, CS would
valid coalition structure belonging PG . value coalition structure calculated

compared maximum value found far. that, coalitions CS updated

set CS another coalition structure PG . Here, coalition Ck updated
examined possible instances Ck+1 , . . . , , C|G| overlap {C1 , . . . , Ck }.
example, Figure 5, update C2 (step 5 figure) examined
possible instances C3 overlap {C1 , C2 } (steps 2, 3, 4 figure). ensures

CS assigned different coalition structures, that, eventually, every possible coalition
structure PG examined.
Next, show process done without storing lists Lg1 , Lg2 , , Lg|G|
memory. end, let LCgnk : 1 gk n list combinations size gk
taken set {1, 2, , n}, combinations ordered lexicographically list.
Given this, LCgnk Lgk contain subsets size gk taken set size n.
difference LCgnk list combinations numbers Lgk list coalitions
agents. Now, Rahwan Jennings (2007) shown cycle combinations
LCgnk without storing entire list memory. Instead, one combination stored time.
based assumed ordering implies last combination LCgnk always:
{1, 2, , gk }.
ordering
also implies that, given combination located index x list,
fi
fi
1 < x fiLCgnk fi, possible compute combination located index x 1 (for
details, see paper Rahwan & Jennings, 2007). Hence, order go coalitions
Lgk , use variable Mk cycle16 combinations LCgnk and, every instance
Mk , extract corresponding coalition Ck Lgk using following operation:
Ck = {ai | Mk }

(2)

example, given Mk = {2, 4, 5}, corresponding coalition would {a2 , a4 , a5 }. Since
direct mapping (as defined equation 2) every combination LCgnk coalition
Lgk , then, Mk cycle every combination LCgnk , cycle
coalitions Lgk .
16. done initializing Mk last combination LCgnk (i.e. {1, 2, , gk }), iteratively
shifting Mk list paper Rahwan Jennings (2007), every combination LCgnk
examined.

541

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

Figure 5: nave cyclation process cycling coalition structures sub-space.
542

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

Intuitively, nave cyclation process, call NCP, viewed efficient.
all, need find coalition structure PG maximum value,
NCP guarantees find coalition structure. However, suffers following major
limitations:
1. NCP works searching ordered sets TG Cartesian product lists
Ls : G order find belong PG (i.e. contain disjoint
coalitions). major limitation since space coalition structures already exponentially large, would counter-intuitive search even bigger space.
example, given 28 agents, number coalition structures P[1,2,3,4,5,6,7]
7.8 109 % number ordered sets T[1,2,3,4,5,6,7] . Note difference size
two spaces grows exponentially number agents involved.
2. Although NCP generate ordered set twice, generates multiple ordered sets
containing coalitions, ordered differently. example, given = {a1 , , a7 }
G = [2, 2, 3], NCP generates following ordered sets, h{a1 , a2 }, {a3 , a4 }, {a5 , a6 , a7 }i
h{a3 , a4 }, {a1 , a2 }, {a5 , a6 , a7 }i, correspond coalition structure. Note
need find best coalition structure and, order so, sufficient examine
value every coalition structure once. words, operation results
coalition structure generated considered redundant.
would desirable, then, find way cycle lists Lg1 , . . . , Lg|G|
valid combinations generated. words, would desirable Ck cycles
valid coalitions Lgk , rather going every coalition Lgk verifying
whether overlaps {C1 , . . . , Ck1 }. Moreover, order avoid performing redundant
operations, would desirable cyclation process guaranteed go
coalition structure once. Algorithm 4 describes novel cyclation process meets
requirements.
basic idea use searchList function cycle coalitions Lg1 .
coalitions, searchList called recursively17 cycle coalitions Lg2
overlap first coalition (i.e. one taken Lg1 ). Similarly, cycling
Lg2 , searchList called recursively cycle coalitions Lg3
overlap first two coalitions, on. repeated searchList called

cycle coalitions Lg|G| , case valid coalition structure (denoted CS

Algorithm 4) belongs PG . Then, CS value greater V (CS 0 ) CS 0
updated accordingly. remainder section describes Algorithm 4 avoids generating
invalid redundant coalition structures without making comparison coalitions. also
describes algorithm applies branch-and-bound technique speed search.
Avoiding invalid coalition structures: Given G = [g1 , . . . , g|G| ], define following ordered
sets agents: A1 , A2 , , A|G| , A1 contains n agents, Ak : 2 k |G| contains
Pk1
n i=1
gi agents. Moreover, assume agents Ak : 1 k |G| ordered
17. searchList actually implemented code recursive function (due inefficiency recursive
functions general). However, make Algorithm 4 easier understand, recursive form algorithm
presented paper.

543

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS


Algorithm 4 :searchList(G, k, , Ak , CS 0 , CS) search sub-space.
Require: {maxs }s{1, ,n} , {v(Ls )}s{1, ,n} , U B ,
1: k > 1 gk 6= gk1 {if size repeated.}
2:
1 {reset .}
3: end
Pk
|A |
4: Mk LCgk k Mk,1 n + 1
i=1 (gk G(gk ))
5:
6:
7:
8:
9:

Ck {Ak,i | Mk } {extract Ck given Mk Ak .}

k = |G| V (CS 0 ) < V (CS)

CS 0 CS {update
current best.}
P
P
else V (CS 0 ) < s{g1 , ,gk } v(Cs ) + s{gk+1 , ,gn } maxs {branch
potential finding coalition structure better CS 0 .}

CS 0 searchList(G, k + 1, Mk,1 , \ C, CS 0 , CS)
{branch next
coalition.}

10:
11:

end
B
V U(CS
V (CS 0 ) = AXG
0)

{stop required solution

found current best equal upper bound
sub-space.}

return
end
14: end
15: return CS 0

12:



CS 0

13:

ascendingly based indices (e.g. Ak contains agents a5 , a7 , a2 , order
would Ak = ha2 , a5 , a7 i). words, assume that: Ak,1 < Ak,2 < < Ak,|Ak | ,
Ak,i ith agent Ak .18 Now, given number coalitions C1 , C2 , , Cgk1 taken
lists Lg1 , Lg2 , , Lgk1 respectively, show cycle coalitions Lgk
overlap aforementioned ones, without storing Lgk memory.
detail, done using following modifications NCP:
Instead using Mk cycle combinations LCgnk (as NCP), use
|A |

cycle combinations LCgk k .
given instance Mk , extract corresponding coalition Ck Lgk using
following operation: Ck = {Ak,i | Mk } (see step 5 Algorithm 4). example, given
Mk = {1, 3, 5}, corresponding coalition contain agents a1 , a3 , a5 (as
NCP). Instead, contains 1st , 3rd , 5th element Ak .
differences ensure Mk cycles possible coalitions size gk taken
Ak (instead taken A). Based this, set Ak = A\{C1 , , Ck1 },
ensure every instance Ck overlap coalitions C1 , , Ck1 .
Figure 6 shows example given = A1 = {a1 , a2 , a3 , a4 , a5 , a6 , a7 } G = [2, 2, 3].
seen, M1 = {1, 6} implies C1 contains 1st 6th agents A1 (i.e. implies
18. Recall define order agents that, two agents ai , aj A, ai < aj iff
< j. details, see Section 2.

544

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

C1 = {a1 , a6 }). knowing agents belong C1 , assign A2
belong C1 , i.e. A2 = {a2 , a3 , a4 , a5 , a7 } (see agents A2 ordered based
indices A). mentioned earlier, M2 would cycle possible coalitions
size 2 A2 , none coalitions would overlap C1 . Similarly, M2 = {3, 5}
implies C2 contains 3rd 5th elements A2 (i.e. implies C2 = {a4 , a7 }),
knowing agents belong C2 , assign A3 belong C1
C2 (i.e. A3 = {a2 , a3 , a5 }), on.

Figure 6: Example novel cyclation process, given = {a1 , a2 , a3 , a4 , a5 , a6 , a7 } G =
[2, 2, 3].

modified cyclation process (MCP), describe above, generates coalition
structures PG (see Theorem 3), without performing comparison
coalitions.
Theorem 3. Given integer partition G G, every coalition structure PG generated
MCP.
Proof. See Appendix D.
Note, however, MCP suffers limitation NCP could generate
coalition structure (e.g. given G = [2, 2, 3], h{a1 , a2 }, {a3 , a4 }, {a5 , a6 , a7 }i
h{a3 , a4 }, {a1 , a2 }, {a5 , a6 , a7 }i generated MCP). Next, show
avoided.

545

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

Avoiding redundant coalition structures: note that, using MCP, coalition structure generated twice repeated parts integer partition
G (e.g. ff G =


[1, 2, 2, 3] G = [1, 4, 4, 4, 6]). MCP generates ordered sets C1 , , C|G| containing disjoint coalitions sizes match parts G (i.e. |Ck | = gk {1, , |G|}).


Based this, ordered set CS generated MCP, then, ordered set CS 0

contains coalitions different order (compared CS) also generated
MCP long sizes coalitions match parts G. This, course, happen
gk = gj : k 6= j. Based this, MCP needs modified cases
repeated parts G.19 modification done follows:
|A |

cycling combinations LCgk k , ensure
Pk first (i.e. smallest) element
Mk (denoted Mk,1 ) satisfies: Mk,1 n + 1 i=1 (gk G(gk )), = Mk1,1
gk = gk1 , = 1 otherwise (see step 4 Algorithm 4).
illustrated Figure 6 using connected boxes. detail, M1 cycles
combinations LC27 contained boxes (e.g. cycle combinations {5, 6},
{5, 7}, {6, 7}). Moreover, M2 cycles combinations LC25 contained
boxes connected one M1 currently cycling. modification ensures
Mk+1,1 Mk,1 gk+1 = gk . example, M1 cycling box LC27
containing combinations smallest element 3, M1,1 = 3. case, M2
cycles boxes LC25 containing combinations smallest element
3 4 (see boxes connected Figure 6), ensures M2,1 M1,1 .
final cyclation process (FCP), describe above, generates every coalition structure
PG exactly once.
Theorem 4. Given integer partition G G, every coalition structure PG generated exactly
FCP.
Proof. See Appendix E.
Note, however, given exponential size PG , would desirable avoid
generating coalition structure potential value greater maximum one
found far. Next, show done using branch-and-bound technique.
Applying Branch-and-Bound: mentioned earlier, cycling coalition structures
PG , update Ck examined possible instances {Ck+1 , . . . , C|G| }
overlap {C1 , . . . , Ck }. words, update Ck examined
possible coalition structures start {C1 , . . . , Ck }. However, knew none
coalition structures could value greater maximum value found far,
could update Ck straight away (i.e. without go possible instances
{Ck+1 , . . . , C|G| }). order so, calculate upper bound values coalitions
added {C1 , . . . , Ck }. Specifically, computed maxs every possible coalition
19. Note coalition structures usually contain repeated coalition sizes (e.g. 99.6% given 20
agents).

546

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

size {1, 2, . . . , n}, calculate upper bound, denoted AX[gk+1 ,...,g|G| ] ,
follows:
AX[gk+1 ,...,g|G| ] =

|G|
X

maxgi

i=k+1

Now, define VP({C1 , , Ck }) sum values coalitions C1 , , Ck (that is,
k
V (C1 , . . . , Ck ) =
i=1 v(Ci )), V ({C1 , ..., Ck }) + AX[gk+1 ,...,g|G| ] represents upper
bound value coalition structure could obtained coalition structure starting
{C1 , , Ck } ending coalition sizes [gk+1 , . . . , g|G| ].
Hence, V (CS 0 ) V ({C1 , ..., Ck }) + AX[gk+1 ,...,g|G| ] implies none coalition structures start {C1 , ..., Ck } end coalitions sizes: gk+1 , ..., g|G|
value greater V (CS 0 ) (this checked step 8 Algorithm 4). hand,
V (CS 0 ) < V ({C1 , . . . , Ck }) + AX[gk+1 ,...,g|G| ] implies could coalition structure
starts {C1 , , Ck } better current best coalition. However, still
necessarily imply coalition structures need examined. because,
algorithm moves next list, may find certain coalition structures
better current best. Formally, every coalition Cj : k < j < |G|, still have:
V (CS 0 ) > V ({C1 , ..., Cj }) + AX[gj+1 ,...,g|G| ] . Figure 7 illustrates branch-and-bound
technique applied searching sub-space.

Figure 7: Applying branch-and-bound searching coalition structures subspace.

547

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

5. Performance Evaluation
section, empirically evaluate IP algorithm, benchmark state
art literature. Since IPs ability prune space depends closeness upper
lower bounds actual optimal value, since closeness determined spread
distribution coalition values, crucial IP tested different value distributions.
Moreover, aim evaluate ability algorithm generate solutions anytime zoom
high quality solutions rapidly.
follows, first discuss validity properties different value distributions
use test algorithm (Section 5.1). Then, benchmark algorithm fastest
available algorithm literature (i.e. IDP) using aforementioned distributions (Section 5.2).
Finally, empirically evaluate efficiency effectiveness algorithm generating
solutions anytime (Section 5.3).
5.1 Benchmarking
common practice benchmarking search heuristics choose standard instances
problem compare various algorithms exist without giving priori knowledge
type input presented with. standard instances coalition structure
generation problems defined used Larson Sandholm (2000) namely:20
1. Normal: v(C) |C| N (, 2 ) = 1 = 0.1.
2. Uniform: v(C) |C| U (a, b) = 0 b = 1.
use distributions benchmark algorithm, also question validity
distributions. because, previous work (Rahwan et al., 2007b), noted
normal uniform distributions tend generate solutions small numbers coalitions. However, show that, coalition values picked Normal Uniform distributions
(scaled size coalition), resulting distribution coalition structure values
biased (see Theorem 5). Given this, experiments defined according Normal Uniform
distributions could favour algorithms others.
Theorem 5. coalition values taken normal distribution follows: C
A, v(C) |C| N (, 2 ), taken uniform distribution follows: C
A, v(C) |C| U (a, b), then, given coalition structure CS 0 : |CS 0 | > 1, exists another
coalition structure CS 00 : |CS 00 | < |CS 0 | that:


P V (CS 00 ) = V (CS ) > P V (CS 0 ) = V (CS )
is, probability CS 00 optimal coalition structure greater CS 0 .
Proof. See Appendix F.
20. sub super-additive distributions also studied literature, cases usually known
priori distribution coalition values actually types (in case known priori
optimal coalition structure is). Moreover, previous results distributions produced interesting
insights (Rahwan et al., 2007b) experiment these.

548

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

remedy this, propose new input distribution tailored specifically CSG problem. distribution, define NDCS (Normally Distributed Coalition Structures),
constructed generating coalition values following way:
NDCS: v(C) N (, 2 ), = |C| =

p
|C|.

case, turns value every possible coalition structure independently
drawn normal distribution leads us following theorem:
p
Theorem 6. Iff have: C A, v(C) N (, 2 ), = |C| = |C|,
following holds:
CS P, V (CS) N (|A| , |A|)
Proof. See Appendix G.
Since NDCS distribution ensures every coalition structure value drawn
distribution, ensures search space biased. Thus, efficiency search algorithms
finding optimal coalition structure strongly tested cases.
Using input distributions, benchmark algorithm state-of-theart algorithm, namely IDP (see Section 2). Note experiment anytime
algorithms since need search whole space find optimal value generally
feasible within reasonable time, even small numbers agents. Also, shown
Rahwan et al. (2007b) industrial strength software CPLEX cannot handle inputs
18 agents since runs memory therefore run experiments
here. graphs plot 95% confidence interval every point (given 800 runs 15
20 agents 100 runs 21 25 agents).21
5.2 Experiment 1: Optimality
experiment, compare algorithms performances given different numbers agents
(from 15 27). time find optimal coalition structure measured terms clock time
(in milliseconds) Intel 2.6GHz Quad Core PC 3Gigabytes RAM. algorithms
coded using JAVA 1.6. running times plotted log scale Figure 8.22 note IP-X
application IP distribution X, X NDCS, Normal, Uniform (as described
above). seen, IP finds optimal coalition structure significantly faster IDP
distributions. best case (Uniform 27 agents) IP 570 times better IDP (i.e. takes
0.175% time taken IDP) worst case (NDCS 16 agents) 1.7 times faster
IDP. also seen performance IP slowest given NDCS distribution
(compared IP-Normal IP-Uniform). determine cause this, first discuss two
main problems affect performance IP:
21. plotting 95% confidence interval, aim check statistical significance difference means
taken point across different series. Thus, two points two different series overlapping confidence
intervals, equivalent saying null hypothesis validated (i.e. means significantly different)
t-test = 0.05. confidence intervals overlap, means significantly different.
22. running time IDP deterministic since runs O(3n ). Hence, recorded running time 25
agents extrapolated results 27 agents.

549

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

Figure 8: Time find optimal solution IDP, IP applied NDCS, Normal, Uniform
distributions.

1. Pruning sub-spaces: higher upper bounds sub-spaces lower value
optimal coalition structure, harder prune sub-spaces. deduced
pruning function use Algorithm 2. Moreover, bigger sub-spaces higher
upper bounds, longer algorithm take find optimal solution.
algorithm always search sub-space highest upper bound check
solution found optimal.
2. Branch-and-bound: higher upper bounds sub-spaces lower optimal
coalition structure value, harder prune branch-and-bound. deduced
pruning applied step 8 Algorithm 4. because, applying branchand-bound within sub-space P{g1 ,g2 ,...,gn } , current best solution CS 0 compared
sum coalition values maximum value coalitions remaining coalition
sizes follows:
V (CS 0 ) >

X
C{Lg1 , ,Lgk }

v(C)+

X

maxg move next coalition structure

g{gk+1 , ,gn }

Now, best solution low compared upper bound, is:
X
V (CS 0 ) << U BG =
maxg
g{g1 , ,gn }

then, branch-and-bound applied deeper (i.e. increasing variable k condition
above) sub-space order make sure coalition structure evaluated
optimal. Hence, worst case would search whole sub-space (i.e. apply
step 9 Algorithm 4 increasing values k n).
550

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

order see different issues affect performance algorithm respect
different distributions, recorded value optimal coalition structure upper bounds
sub-spaces (given 21 agents) averaged 20 runs.23 also exactly recorded
size sub-space (i.e. number coalition structures per sub-space). results
plotted Figure 9. note following distribution:

Figure 9: Top: upper bounds optimal coalition structure value, bottom: size sub-spaces.
Note values bottom graph plotted log scale. Points
abscissa two graphs correspond sub-space. arrows show
direction search distribution.

NDCS: biggest sub-spaces ones highest upper bounds. Hence,
much harder prune large portions search space. Moreover, average optimal
23. values upper bounds average optimal coalition structure rounded scaled ease
explanation clearer plot.

551

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

coalition structure value relatively low compared upper bounds bigger subspaces. Hence, applying branch-and-bound distribution hard.
Normal: smaller sub-spaces ones highest upper bounds. Hence, pruning
large portions space easily done searching smaller sub-spaces good
solutions are. Moreover, value optimal coalition structure tends higher
upper bounds large sub-spaces, relatively close highest upper bounds.
Hence, easier branch-and-bound prune large portions sub-spaces.
Uniform: upper bounds sub-spaces relatively high compared
distributions (i.e. close highest upper bound). fact, upper bounds
actually nearly equal average optimal solution allows algorithm prune
sub-spaces soon found optimal solution, happens almost
immediately scanning input.
Finally, note Figure 9 shows portion space avoided given selection
strategy described earlier Section 4.2. detail, recall strategy guaranteed
avoid searching sub-spaces upper bound lower V (CS ). seen
figure, many sub-spaces (in case NDCS Uniform distributions) upper
bound lower V (CS ), although sub-spaces relatively small. Moreover,
case Normal distribution, almost sub-spaces upper bound lower V (CS ),
among largest ones!
studied performance IP terms completion time, next focus studying
ability generate solutions anytime.
5.3 Experiment 2: Anytime Quality
experiment, evaluate anytime property algorithm, recording value solutions generated returning guaranteed optimal one.
particular, recorded two indicative measures quality solutions. First, computed
ratio value current best solution optimal solution (obtained end
(CS 0 )
run). ratio noted ropt = VV (CS
) . measure shows effective algorithm
zooming good solutions. Second, recorded ratio rbound value cur0)
rent best solution upper bound optimal value (i.e. rbound = V U(CS
B ). measure
theoretical guarantee algorithm places quality solution (see Section 4.1).
Ideally, algorithm able minimise difference ropt rbound minimal
time.
results plotted Figure 10 distributions: NDCS, Normal, Uniform.24
discuss results distributions turn.
NDCS: seen, algorithm high quality guarantees (i.e. rbound > 90%)
less half time required find optimal solution. also produces high
quality solutions (i.e. ropt > 90%) within less 10% time required terminate.
24. points plotted averages computed 500 runs 19 agents 22 agents, 100 runs 25 agents.
error bars depict 95% confidence interval intervals results recorded.

552

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

Figure 10: Quality (ropt ) bound (rbound ) generated solution. cases, x-axis
represents time (in milliseconds) y-axis represents ratio solution
optimal.
553

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

Normal: case, algorithm able come guaranteed high quality solutions
much faster NDCS distribution. Moreover, case, high quality solutions
(i.e. ropt > 90%) guaranteed (i.e. rbound > 90%) less 10% time
find optimal value. results fact upper bounds far
optimal value NDCS case.
Uniform: expected earlier results presented Section 5.2, algorithm generates
high quality solutions (i.e. ropt 100%) faster distributions (shortly
scanning input). Moreover, solutions guaranteed near-optimal (i.e.
rbound > 99%) within 15% time find optimal.
Next, compare worst-case guarantees provided IP provided Sandholm et al.s (1999) Dang Jenningss (2004) algorithms (see Figure 11). seen,
algorithm significantly outperforms Dang Jenningss Sandholm et al.s distributions. particular, scanning input, IP able guarantee solution nearly
40% (in worst case) optimal compared 10% algorithms. Moreover,
guarantee usually reaches 100% searching minute portions search space (on average
around 0.0000019% hardest distribution), guarantees provided algorithms
go beyond 50% whole space searched. Also note generate
high quality solutions (i.e. > 90%) searching even smaller portions search space
(on average around 0.0000002% hardest distribution). Thus, actual computational time,
25 agents example, able return solution guaranteed higher 90%
optimal around 250 seconds worst case 300 milliseconds best case.

6. Conclusions Future Work
Coalition formation, process group software agents come together agree
coordinate cooperate performance set tasks, important form interaction
multi-agent systems. coalitions improve performance individual agents and/or
system whole, especially tasks cannot performed single agent, group
agents performs tasks efficiently. One challenging problems arise
coalition formation process coalition structure generation, involves partitioning
set agents exhaustive disjoint coalitions social welfare maximized.
paper, developed evaluated anytime integer-partition based algorithm (called
IP) finds optimal solutions much faster previous algorithm designed purpose.
strength approach founded upon two main components:
use novel representation search space partitions smaller, disjoint
sub-spaces explored independently find optimal solutions. representation,
based integer partitions number agents involved, allows agents
balance trade-offs preferences certain coalition sizes computation required find solution. Moreover, trade-offs made informed
manner since compute bounds sub-spaces search space. bounds allow us prune search space guarantee quality solution found
search. may also, depending distribution input values, allow us obtain
optimal solution almost immediately scanning input.
554

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

Figure 11: Worst case bounds generated IP using Normal NDCS distributions compared
Sandholm et al.s (1999) Dang Jenningss (2004) algorithms 25 agents.
results Uniform distribution trivial since IP average finds optimal
almost immediately scanning input. Note error bars omitted
IP results reasons clarity.

555

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

devise technique allows us cycle coalition structures within given
sub-space. Unlike nave cyclation technique generates combinations coalitions,
verifies whether combinations valid coalition structure, cyclation technique generates valid ones (thus, avoiding search space possible combinations coalitions, exponentially larger space coalition structures).
addition, cyclation technique perform redundant operations since avoids
generating coalition structure once. Finally, applying branch-andbound technique, able identify coalition structures cannot improve
quality solution found far, thus, avoid generating them.
Altogether, components allow us make significant performance gains existing
approaches. detail, experiments show IP avoids searching search space,
therefore, requires significantly less time, compared algorithms, order return
optimal solution. example, IP outperforms IDP orders magnitude (0.175%
time taken IDP 27 agents best case). Moreover, IP interrupted optimal
value found, still return solutions close optimal (usually 95%
optimal), high worst-case guarantees (usually 90%). solutions
always better (above 40% optimal right scanning input) returned
Sandholm et al.s (1999) Dang Jenningss (2004) algorithms (i.e. less 10%
optimal). algorithms also search large portion search space
able get better guarantees algorithm able prune find near-optimal solutions
relatively quickly (above 90% optimal within 10% time find optimal solution
25 agents).
number important extensions IP could envisaged. example, recently
combined IDP algorithm IP (IP-IDP) (Rahwan & Jennings, 2008a) explore
approaches including linear programming techniques improve bounds used IP. However,
extensions deal exponential input (i.e. 2n memory locations least n
agents) IP. Therefore, important develop techniques extend approach order minimise cycling coalition values number agents increases.
require adapting cyclation technique bound computation. Hence, future
work, need devise representations sub-spaces allow us cycle intelligently
larger inputs develop new techniques compute bounds used branch-andbound algorithm. trying adapt approach problems, also aim determine
degree IP used solve common incomplete set partitioning problems
occur combinatorial auctions (Rothkopf et al., 1995) crew scheduling (Hoffman & Padberg,
1993). Finally, aim see whether patterns exploit algorithm also arise
combinatorial optimisation problems studied area combinatorics (e.g.,
Kreher & Stinson, 1998; Papadimitriou & Steiglitz, 1998).

7. Acknowledgments
research paper undertaken part ALADDIN (Autonomous Learning Agents
Decentralised Data Information Systems) project jointly funded BAE Systems
EPSRC (Engineering Physical Research Council) strategic partnership (EP/C548051/1).
Andrea Giovannucci funded Juan de la Cierva programme (JCI-2008-03006)
EU funded Synthetic Forager project (ICT-217148-SF). also wish thank Professor Tuomas
556

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

Sandholm comments, well anonymous reviewers valuable comments
previous versions paper. also grateful Dr. Viet Dung Dang contributions
earlier versions paper. Finally, wish thank Dr. W. T. Luke Teacy help
proofs anonymous reviewers constructive comments.

557

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

Appendix A. Summary Notation


ai
n
C
|C|
v(C)
CS
V (CS)
CS
U B
LB
CS 0


LCsi
Ls
v(Ls )
maxs
mins
avgs
P
Pi
LVi
G
G(s)
E(G)
G
G2
TG
PG
AXG
ING
AV GG
F

CS
Mk
Ak
Csn
P (x)
IDP
N (, 2 )
U (a, b)
ropt
rbound

set agents.
agent A.
number agents A.
coalition.
cardinality C.
value C.
coalition structure.
value CS.
optimal coalition structure.
upper bound V (CS ).
lower bound V (CS ).
best coalition structure found far.
bound quality best solution found far.
bound within solution acceptable.
list possible combinations size taken set {1, 2, . . . , i}.
list coalitions size ordered lexicographically.
list containing values coalitions Ls .
maximum value coalitions Ls .
minimum value coalitions Ls .
average value coalitions Ls .
set possible coalition structures.
ith level representation space possible coalition structures.
ith level coalition structure graph.
integer partition n.
multiplicity G.
underlying set G.
set possible integer partitions n.
set possible integer partitions n contain two parts each.
Cartesian product lists Ls : G.
sub-space (in space representation) corresponds G (i.e. pre-image G F ).
maximum value elements TG .
minimum value elements TG .
average value elements TG .
function maps coalition structure CS integer partition G that: C CS, g G : |C| = g.
variable used cycle coalition structures PG .
variable used cycle list combinations size gk .
ordered set containing agents members C1 , . . . , Ck1 .
binomial coefficient (i.e. number possible combinations size taken n elements).
probability x.
improved dynamic programming algorithm.
Normal distribution mean variance 2 .
Continuous Uniform distribution interval [a, b].
ratio value current best solution value optimal solution.
ratio value current best solution upper bound value optimal
solution.

558

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

Appendix B. Proof Theorem 1.
Let G = [g1 , g2 , . . . , g|G| ] contain
elements G
ff natural ordering them, let PG
return ordered coalition structures C1 , C2 , . . . , C|G| : Ci Lgi , order coalitions within coalition structure taken consideration. example, given n = 4 G =
[1, 1, 2], two ordered coalition structures: h{a1 }, {a2 }, {a3 , a4 }i h{a2 }, {a1 }, {a3 , a4 }i
PG correspond one coalition structure: {{a1 }, {a2 }, {a3 , a4 }} PG . Now, since number repetitions coalition structure PG same25 (e.g., example
G = [1, 1, 2], coalition structures PG appear twice PG ), have:
AV GG = AV GG

(3)

AV GG average value coalition structures PG . Now, define Nn (g1 , g2 , . . . , g|G| )
number ordered coalition structures PG , have:
AV GG =

X
1
V (CS)
Nn (g1 , g2 , . . . , g|G| )
CSPG

=

X
1
Nn (g1 , g2 , . . . , g|G| )

X

v(C)

CSPG CCS

Moreover, every coalition C Lgi , are: Nngi (g1 , g2 , . . . , gi1 , gi+1 , . . . , g|G| ) ordered
coalition structures C happens ith coalition. Based this, have:
Nn (g1 , g2 , . . . , g|G| ) = |Lgi | Nngi (g1 , . . . , gi1 , gi+1 , . . . , g|G| )

(4)

Similarly, number times v(C) occurs ith position sum coalition values
PG Nngi (g1 , . . . , gi1 , gi+1 , . . . , g|G| ). Given this, next compute AV GG follows:

AV GG =

|G|
X
X
1
Nngi (g1 , . . . , gi1 , gi+1 , . . . , g|G| ) v(C)
Nn (g1 , g2 , . . . , g|G| )
i=1 CLgi

|G|

=

X X Nngi (g1 , . . . , gi1 , gi+1 , . . . , g|G| )
v(C)
Nn (g1 , g2 , . . . , g|G| )
i=1 CLgi

|G|
X
X

1
v(C) (following equation (4))
|Lgi |
i=1 CLgi


|G|
X
X
1
=
v(C)
|Lgi |
=

i=1

=

|G|
X

CLgi

avggi

i=1

25. Specifically, coalition structure repeated x! times contains x coalitions size.

559

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

Based this, well (3), find that:
AV GG =

|G|
X

avggi

i=1



Appendix C. Proof Theorem 2.
Generally speaking, given set B A, number possible combinations size
|A||B|
set B = A\B equal Cs
. Based this, every coalition C size g1 ,
ng1
Cg2
coalitions size g2 overlap it. Similarly, every disjoint coalitions
n(g +g +...+gi )
(C1 , C2 , . . . , Ci ) sizes g1 , g2 , . . . , gi respectively, Cgi+1 1 2
coalitions size
gi+1 overlap union C1 C2 . . . Ci .
Based this, TG cartesian product lists Lgi : gi G, TG subset TG
contains elements (i.e. combinations coalitions) coalitions overlap,
number elements TG computed follows:
fi fi
n(g1 +...+g|G|1 )
fi fi
1
. . . Cg|G|
(5)
fiTG fi = Cgn1 Cgng
2
Moreover, note combination coalitions {C1 , C2 , . . . , C|G| }, {1, 2, . . . , |G|} :
|Ci | = gi , appears exactly PG (since considered unique coalition structure) could
appear TG (since ordering coalitions matters elements TG ).
particular, gi appears x times G, every coalition structure PG corresponds x! elements
TG , coalitions size gi ordered differently elements. example,
given G = [1, 2, 2, 2, 5], size 2 appears 3 times G means every coalition structure
{C1 , C2 , C3 , C4 , C5 } PG corresponds 3! elements TG (since 3! number possible
permutations C2 , C3 , C4 ). generalized follows:
fi fi
fi fi
fiTG fi
G = [g1 , g2 , . . . , g|G| ] G, |PG | =
(6)
G(g1 )! G(g2 )! . . . G(g|G| )!
G(gi ) denotes multiplicity gi G. Then, (5) (6), find that:
n(g1 +...+g|G|1 )

1
Cgn1 Cgng
. . . Cg|G|
2
Q
|PG | =
sE(G) G(s)!

E(G) underlying set G.


Appendix D. Proof Theorem 3.
Given integer partition G = [g1 , . . . , g|G| ] G, need prove coalition structures
PG generated MCP. Without loss generality, assume parts G
560

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

increasing order. is:
g1 g2 . . . g|G|

(7)

Now, way MCP works generating ordered sets coalitions that, every ordered set,
first coalition belongs Lg1 second belongs Lg2 on. Moreover, way
ordered sets generated ensures coalitions ordered sets overlap.
words, MCP generates subset TG , denoted TG , defined follows:26
TG =




ff

C1 , ..., C|G| | {1, ..., |G|}, Ci |Ci | = gi j {1, ..., |G|} : j 6= i, Ci Cj =

Then, given coalition structure CS PG , let TGCS subset TG containing ordered
sets correspond CS. is:
CS
TG
=

E

nD
C1 , ..., C|G| | {1, ..., |G|}, Ci CS |Ci | = gi j {1, ..., |G|} : j 6= i, Ci Cj =

(8)
{{a },{a2 },{a3 ,a4 }}

1
example, T[1,1,2]

= {h{a1 }, {a2 }, {a3 , a4 }i , h{a2 }, {a1 }, {a3 , a4 }i}. Next, given


ff
coalition structure CS PG , prove |TGCS | 1. end, let C1 , C2 , . . . , C|G|
ordering coalitions belong CS. Then, (7) (8), see that:



fi
fi
ff
C1 , C2 , . . . , C|G| TGCS iff |C1 | |C2 | . . . fiC|G| fi

(9)

fi
fi
since least one way ordering coalitions CS |C1 | ... fiC|G| fi,
least one ordered set TGCS . words, |TGCS | 1. This, turn, implies
every coalition structure PG generated MCP.


Appendix E. Proof Theorem 4.
Given integer partition G = [g1 , . . . , g|G| ] G, let TeG set ordered sets generated
FCP. Moreover, given coalition structure CS PG , let TeGCS subset TeG containing
ordered sets correspond CS. is:
n


ff
TeGCS = C1 , C2 , . . . , C|G| TeG | {1, 2, . . . , |G|}, Ci CS
Next, prove |TeGCS | = 1. define TG TGCS Appendix D. also assume,
without loss generality, order (7) holds. Note that, G(gi ) = 1 {1, . . . , |G|},
difference way FCP works way MCP works.27
hand, exists {1, ..., |G|} G(gi ) > 1, difference FCP
MCP FCP avoids coalition structures generated MCP. implies
26. Recall TG cartesian product lists: Ls : G.
27. Recall G(gi ) multiplicity gi G.

561

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

that:

G(gi ) = 1 {1, . . . , |G|} TeG = TG CS PG , TeGCS = TGCS
else TeG TG CS PG , TeGCS TGCS

(10)
(11)



ff
Now, given coalition structure CS PG , let C1 , C2 , . . . , C|G| defined Appendix (i.e.
ordering coalitions belong CS). Then, (9), find number
ordered sets TGCS equal
fi
fi number possible ways ordering coalitions CS
fi
that: |C1 | |C2 | ... C|G| fi. Based this, well (10) (11), distinguish
two cases:

G(gi ) = 1 {1, ..., |G|},
fi would
fi one possible way ordering
fi
coalitions CS |C1 | ... C|G| fi (because every coalition CS unique
size). implies |TGCS | = 1, (10), find |TeGCS | = 1.
{1, ..., |G|} : G(gi ) > 1,fi thenfithere would multiple ways ordering coalitions
CS |C1 | ... fiC|G| fi, implies |TGCS | > 1. However, (11),
know TeGCS subset TGCS . Then, proving TeGCS contains exactly one
ordered sets TGCS , prove |TeGCS | = 1. precise, case have:
|Cx | = |Cx+1 | = ... = |Cx+y |, every possible permutation coalitions
generated MCP, need prove one generated FCP.
Based this, denote ck smallest28 agent Ck , sufficient prove
FCP generates one permutation satisfies: cx < cx+1 < ... < cx+y . Note
agents Ak ordered Ak,1 < Ak,2 < < Ak,|Ak | . Based this, ck = Ak,i ,
1 agents Ak smaller ck , since Ak+1 = Ak \Ck ,
1 agents Ak+1 smaller ck . Therefore, ensure ck < ck+1 ,
sufficient generate Ck+1 contain first (i.e. smallest) 1 agents
Ak+1 . example, given Ak = ha1 , a4 , a5 , a7 , a8 , a9 Mk = {3, 5}, would
Ck = {a5 , a8 } ck = Ak,3 . implies Ak+1 contains two agents smaller
ck (namely, agents a1 a4 ). Therefore, ensure ck < ck+1 , sufficient
generate Ck+1 contain first (i.e. smallest) two agents Ak+1 .
done ensuring Mk+1 contain elements 1 2. words,
done ensuring Mk+1,1 Mk,1 , direct result way FCP modified.
proving |TeGCS | = 1 CS PG , prove FCP generates every coalition structure
PG exactly once.

28. Recall that, two agents ai , aj A, say ai smaller aj < j. comes
assumed ordering set agents (see Section 2 detail).

562

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

Appendix F. Proof Theorem 5
first prove Theorem 5 normal distribution case (i.e. case C A, v(C)
|C|N (, 2 )). Specifically, show coalition structures contain fewer coalitions
likely optimal. order prove this, first prove following lemma
deals properties normal distribution.
Lemma 1. given value r R, two random variables Xa N (, 2 )
Xb N (, b 2 ) < b , following holds:
P (Xa > r) < P (Xb > r)

(12)

Proof. Given r R, let ,a2 (r) ,2 (r) cumulative distribution functions
b
N (, 2 ) N (, b 2 ) respectively. is,



1
r

,a2 (r) =
1 + erf
2
2
1
,2 (r) =
b
2




r

1 + erf
b 2

RM
2
erf(M ) = 2 0 et error function. Then, order prove inequality
(12) holds, sufficient prove that:
,a2 (r) > ,2 (r)

(13)

b

end, given < b , following holds, abs(M ) absolute value :




r
r


abs
> abs
2
b 2
This, turn, implies that:

erf

r

2




> erf

r

b 2



Based this, well fact erf(M ) 0, deduce (13) holds.

Based lemma, given coalition structure CS 0 : |CS 0 | > 1, prove
exists another coalition structure CS 00 : |CS 00 | < |CS 0 | that:


P V (CS 00 ) = V (CS ) > P V (CS 0 ) = V (CS )
detail, let CS 0 = {Cx1 , , Cx , Cy1 , , Cy } CS 00 = {Cx , Cy1 , , Cy }
Cx = Cx1 Cx . Then, based properties normal distribution, have:


v(Cx ) N |Cx | , |Cx |2 2
(14)
563

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

and:


(15)
v(Cx1 ) + + v(Cx ) N (|Cx1 | + + |Cx |) , (|Cx1 |2 + + |Cx |2 ) 2
2 denote mean variance distriNow, given coalition structure CS, let CS CS
bution V (CS). Then, based (14) (15), have:

X

CS 00 = (|Cx | ) +

(|C| )

CCS 00 \{Cx }

X

2
2
2
CS
00 = (|Cx | ) +

(|C| )2

CCS 00 \{Cx }

have:
X

CS 0 = ((|Cx1 | + + |Cx |) ) +

CCS 0 \{C



2
CS
(|Cx1 |2 + + |Cx |2 ) 2 +
0 =

(|C| )

x1 , ,Cx }

X

(|C| )2

CCS 0 \{Cx1 , ,Cx }

Since |Cx | = |Cx1 | + + |Cx |, since CS 00 \ {Cx } = CS 0 \ {Cx1 , , Cx }, see
distribution V (CS 00 ) V (CS 0 ) differ way variances differ. Note that:
|Cx |2 = (|Cx1 | + + |Cx |)2 > |Cx1 |2 + + |Cx |2
2
2
implies CS
0 < CS 00 . Therefore, based Lemma 1, find value r R:

P (V (CS 0 ) > r) < P (V (CS 00 ) > r)
words, likely CS 00 value greater r, implies
likely CS 00 optimal coalition structure.
proved Theorem 5 normal distribution case, give intuition behind
proof uniform distribution case (i.e. case C A, v(C) |C|U (a, b)). Specifically, assuming CS 0 CS 00 defined above, would v(Cx ) |Cx | U (a, b)
and, coalition C {Cx1 , , Cx }, would v(C) |C| U (a, b). Then, easy
verify P (v(Cx ) r) less P (v(Cx1 ) + + v(Cx ) r) high values r.
intuition behind difference probabilities sum Uniformly distributed variables
(called Uniform Sum distribution) results distribution giving lower probability low
high values, higher probability middle ranged values. Instead, uniformly distributed
variable, values equally probable. Therefore, given Uniform Sum distribution Uniform distribution minimum maximum values, Uniform distribution give
higher probability higher values. Hence, proof holds Uniform distribution
well.

564

fiAppendix G. Proof Theorem 6.
Given following:
C A, v(C) N (|C| , |C|)

(16)

need prove value every coalition structure independently drawn
normal distribution. Specifically, prove following holds:
CS P, V (CS) N (|A| , |A|)

(17)

properties normal distribution, know that, two independent random
variables, x x N (x , x 2 ) N (y , 2 ), have:
(x + y) N (x + , x 2 + 2 )

(18)

Then, based (16) (18), two coalition values, v(C1 ) v(C2 ), satisfy following
(since independent random variables):
(v(C1 ) + v(C2 )) N (|C1 | + |C2 | , |C1 | + |C2 |)
implies following true:
!
CS P,

X

v(C)

!
N

CCS

X

|C| ,

CCS

X

|C|

(19)

CCS

Finally, note assume following:
CS P, V (CS) =

X

v(C)

(20)

CCS

CS P, C, C 0 CS, C C 0 =

(21)

Then, (19), (20), (21), find that:
CS P, V (CS) N (|CCS | , |CCS |)
implies (17) holds since CCS = A.


References
Andrews, G., & Eriksson, K. (2004). Integer Partitions. Cambridge University Press, Cambridge,
UK.
Dang, V. D., & Jennings, N. R. (2004). Generating coalition structures finite bound optimal guarantees. Proceedings Third International Joint Conference Autonomous
Agents Multi-Agent Systems (AAMAS-04), pp. 564571.

fiR AHWAN , R AMCHURN , G IOVANNUCCI , & J ENNINGS

Dang, V. D., Dash, R. K., Rogers, A., & Jennings, N. R. (2006). Overlapping coalition formation
efficient data fusion multi-sensor networks. Proceedings Twenty First National
Conference Artificial Intelligence (AAAI-06), pp. 635640.
Evans, J., & Minieka, E. (1992). Optimization Algorithms Networks Graphs, 2nd edition.
Marcel Dekker, New York, USA.
Hillier, F. S., & Lieberman, G. J. (2005). Introduction operations research. McGraw-Hill, New
York, USA.
Hoffman, K. L., & Padberg, M. (1993). Solving airline crew scheduling problems branch-andcut. Management Science, 39(6), 657682.
Horling, B., & Lesser, V. (2005). survey multi-agent organizational paradigms. Knowledge
Engineering Review, 19(4), 281316.
Jennings, N. R. (2001). agent-based approach building complex software systems. Communications ACM, 44(4), 3541.
Kahan, J., & Rapoport, A. (1984). Theories Coalition Formation. Lawrence Erlbaum Associates
Publishers, New Jersey, USA.
Klusch, M., & Shehory, O. (1996). polynomial kernel-oriented coalition formation algorithm
rational information agents. Proceedings Second International Conference MultiAgent Systems (ICMAS-96), pp. 157164.
Kreher, D. L., & Stinson, D. R. (1998). Combinatorial Algorithms: Generation, Enumeration,
Search (Discrete Mathematics applications). CRC Press.
Larson, K., & Sandholm, T. (2000). Anytime coalition structure generation: average case study.
Journal Experimental Theoretical Artificial Intelligence, 12(1), 2342.
Li, C., & Sycara, K. P. (2002). Algorithm combinatorial coalition formation payoff division
electronic marketplace. Proceedings First International Joint Conference
Autonomous Agents Multiagent Systems (AAMAS-02), pp. 120127.
Norman, T. J., Preece, A. D., Chalmers, S., Jennings, N. R., Luck, M., Dang, V. D., Nguyen, T. D.,
V. Deora, J. S., Gray, W. A., & Fiddian, N. J. (2004). Agent-based formation virtual
organisations. International Journal Knowledge Based Systems, 17(24), 103111.
Osborne, M. J., & Rubinstein, A. (1994). Course Game Theory. MIT Press, Cambridge MA,
USA.
Papadimitriou, C. H., & Steiglitz, K. (1998). Combinatorial Optimization: Algorithms Complexity. Dover Publications.
Rahwan, T., & Jennings, N. R. (2007). algorithm distributing coalitional value calculations
among cooperative agents. Artificial Intelligence, 171(89), 535567.
Rahwan, T., & Jennings, N. R. (2008a). Coalition structure generation: dynamic programming
meets anytime optimisation. Proceedings Twenty Third Conference Artificial
Intelligence (AAAI-08), pp. 156161.
Rahwan, T., & Jennings, N. R. (2008b). improved dynamic programming algorithm coalition
structure generation. Proceedings Seventh International Conference Autonomous
Agents Multi-Agent Systems (AAMAS-08), pp. 14171420.
566

fiA N NYTIME LGORITHM PTIMAL C OALITION TRUCTURE G ENERATION

Rahwan, T., Ramchurn, S. D., Dang, V. D., & Jennings, N. R. (2007a). Near-optimal anytime
coalition structure generation. Proceedings Twentieth International Joint Conference
Artificial Intelligence (IJCAI-07), pp. 23652371.
Rahwan, T., Ramchurn, S. D., Giovannucci, A., Dang, V. D., & Jennings, N. R. (2007b). Anytime
optimal coalition structure generation. Proceedings Twenty Second Conference
Artificial Intelligence (AAAI-07), pp. 11841190.
Rothkopf, M. H., Pekec, A., & Harstad, R. M. (1995). Computationally manageable combinatorial
auctions. Management Science, 44(8), 11311147.
Sandholm, T. W., Larson, K., Andersson, M., Shehory, O., & Tohme, F. (1999). Coalition structure
generation worst case guarantees. Artificial Intelligence, 111(12), 209238.
Sandholm, T. W., & Lesser, V. R. (1997). Coalitions among computationally bounded agents. Artificial Intelligence, 94(1), 99137.
Sen, S., & Dutta, P. (2000). Searching optimal coalition structures. Proceedings Sixth
International Conference Multi-Agent Systems (ICMAS-00), pp. 286292.
Shehory, O., & Kraus, S. (1995). Task allocation via coalition formation among autonomous agents.
Proceedings Fourteenth International Joint Conference Artificial Intelligence
(IJCAI-95), pp. 655661.
Shehory, O., & Kraus, S. (1998). Methods task allocation via agent coalition formation. Artificial
Intelligence, 101(12), 165200.
Tsvetovat, M., Sycara, K. P., Chen, Y., & Ying, J. (2000). Customer coalitions electronic
marketplace. Proceedings Fourth International Conference Autonomous Agents
(AA-01), pp. 263264.
Yeh, D. Y. (1986). dynamic programming approach complete set partitioning problem. BIT
Numerical Mathematics, 26(4), 467474.

567

fiJournal Artificial Intelligence Research 34 (2009) 637-674

Submitted 07/08; published 04/09

Sentence Compression Tree Transduction
Trevor Cohn

tcohn@inf.ed.ac.uk

Mirella Lapata

mlap@inf.ed.ac.uk

School Informatics
University Edinburgh
10 Crichton Street Edinburgh EH8 10AB, UK

Abstract
paper presents tree-to-tree transduction method sentence compression.
model based synchronous tree substitution grammar, formalism allows local
distortion tree topology thus naturally capture structural mismatches.
describe algorithm decoding framework show model
trained discriminatively within large margin framework. Experimental results sentence
compression bring significant improvements state-of-the-art model.

1. Introduction
Recent years witnessed increasing interest text-to-text generation methods many
natural language processing applications, ranging text summarisation question answering machine translation. heart methods lies ability perform
rewriting operations. instance, text simplification identifies phrases sentences
document pose reading difficulty given user substitutes simpler alternatives (Carroll, Minnen, Pearce, Canning, Devlin, & Tait, 1999; Chandrasekar &
Srinivas, 1996). question answering, questions often paraphrased order achieve
flexible matching potential answers (Lin & Pantel, 2001; Hermjakob, Echihabi,
& Marcu, 2002). Another example concerns reformulating written language render
natural sounding speech synthesis applications (Kaji, Okamoto, & Kurohashi,
2004).
Sentence compression perhaps one popular text-to-text rewriting methods.
aim produce summary single sentence retains important
information remaining grammatical (Jing, 2000). appeal sentence compression
lies potential summarization generally document compression, e.g.,
displaying text small screens mobile phones PDAs (Vandeghinste & Pan,
2004). Much current work literature focuses simplified formulation
compression task allow rewriting operations word deletion.
Given input source sentence words x = x1 , x2 , . . . , xn , target compression formed
removing subset words (Knight & Marcu, 2002).
Despite restricted word deletion, compression task remains challenging
modeling perspective. Figure 1 illustrates source sentence target compression
taken one compression corpora used experiments (see Section 5 details).
case, hypothetical compression system must apply series rewrite rules order
c
2009
AI Access Foundation. rights reserved.

fiCohn & Lapata














VP
WHNP
RB

WP

exactly

NP

VP
NP

NNS

WHNP

VBD PRP CC

records

WP

NP

WHNP

NP

VBN

WP

NNS

VBP

VBN

involved



records



involved

NNS VBP

made ones

VP
VP

(a) Source

VP

(b) Target

Figure 1: Example sentence compression showing source target trees. bold
source nodes show terminals need removed produce target
string.

WHNP
RB



WHNP

WP

WP

NP



NP
NP

VP



(1)



(2)

(3)




VP

WHNP

VP
VP


WHNP

CC




WHNP




NP

VP


(4)

(5)

Figure 2: Example transduction rules, displayed pair tree fragments. left
(source) fragment matched node source tree, matching
part replaced right (target) fragment. Dotted lines denote variable
correspondences, denotes node deletion.

obtain target, e.g., delete leaf nodes exactly and, delete subtrees made
ones, merge subtrees corresponding records involved.
concretely, system must access rules like shown Figure 2. rules
displayed pair tree fragments left fragment corresponds source
right target. instance, rule (1) states wh-noun phrase (WHNP)
consisting adverb (RB) wh-pronoun (WP) (e.g., exactly what) rewritten
wh-pronoun (without adverb). two things note here. First,
syntactic information plays important role, since deletion decisions limited
individual words often span larger constituents. Secondly, large number
compression rules varying granularity complexity (see rule (5) Figure 2).
Previous solutions compression problem cast mostly supervised
learning setting (for unsupervised methods see Clarke & Lapata, 2008; Hori & Furui, 2004;
Turner & Charniak, 2005). Sentence compression often modeled generative framework
638

fiSentence Compression Tree Transduction

aim estimate joint probability P (x, y) source sentence x
target compression (Knight & Marcu, 2002; Turner & Charniak, 2005; Galley &
McKeown, 2007). approaches essentially learn rewrite rules similar shown
Figure 4 parsed parallel corpus subsequently use find best
compression set possible compressions given sentence. approaches
model compression discriminatively subtree deletion (Riezler, King, Crouch, & Zaenen,
2003; Nguyen, Horiguchi, Shimazu, & Ho, 2004; McDonald, 2006).
Despite differences formulation, existing models specifically designed sentence compression mind generally applicable tasks requiring
complex rewrite operations substitutions, insertions, reordering. common
assumption underlying previous work tree structures representing source
sentences target compressions isomorphic, i.e., exists edge-preserving
bijection nodes two trees. assumption valid sentence compression hold rewriting tasks. Consequently, sentence compression
models restrictive; cannot readily adapted generation problems
since able handle structural lexical divergences. related issue concerns deletion operations often take place without considering
structure target compression (the goal generate compressed string rather
tree representing it). Without syntax-based language model (Turner & Charniak,
2005) explicit generation mechanism licenses tree transformations
guarantee compressions well-formed syntactic structures.
straightforward process subsequent generation analysis tasks.
paper present sentence compression model deletion-specific
account ample rewrite operations scales rewriting tasks. formulate
compression problem tree-to-tree rewriting using synchronous grammar (with rules
like shown Figure 2). Specifically, adopt synchronous tree substitution
grammar (STSG) formalism (Eisner, 2003) model non-isomorphic tree structures
efficient inference algorithms. show grammar induced
parallel corpus propose discriminative model rewriting task
viewed weighted tree-to-tree transducer. learning framework makes use
large margin algorithm put forward Tsochantaridis, Joachims, Hofmann, Altun
(2005) efficiently learns prediction function minimize given loss function.
also develop appropriate algorithm used training (i.e., learning
model weights) decoding (i.e., finding plausible compression model).
Beyond sentence compression, hope work described might
relevance tasks involving structural matching (see discussion Section 8).
remainder paper structured follows. Section 2 provides overview
related work. Section 3 presents STSG framework compression model
employ experiments. Section 5 discusses experimental set-up Section 6
presents results. Discussion future work concludes paper.

2. Related Work
Synchronous context-free grammars (SCFGs, Aho & Ullman, 1969) generalization
context-free grammar (CFG) formalism simultaneously produce strings two
639

fiCohn & Lapata

languages. used extensively syntax-based statistical MT. Examples
include inversion transduction grammar (Wu, 1997), head transducers (Alshawi, Bangalore,
& Douglas, 2000), hierarchical phrase-based translation (Chiang, 2007), several variants
tree transducers (Yamada & Knight, 2001; Grael & Knight, 2004).
Sentence compression bears resemblance machine translation. Instead translating one language another, translating long sentences shorter ones
within language. therefore surprising previous work also adopted
SCFGs compression task. Specifically, Knight Marcu (2002) proposed noisychannel formulation sentence compression. model consists two components:
language model P (y) whose role guarantee compression output grammatical channel model P (x|y) capturing probability source sentence x
expansion target compression y. decoding algorithm searches compression maximizes P (y)P (x|y). channel model stochastic SCFG,
rules extracted parsed parallel corpus weights estimated using
maximum likelihood. Galley McKeown (2007) show obtain improved SCFG
probability estimates Markovization. Turner Charniak (2005) note SCFG
rules expressive enough model structurally complicated compressions
restricted trees depth 1. remedy supplying synchronous grammar set general special rules. example, allow rules form
hNP,NPi h[NP NP 1 CC NP 2 ], NP 1 (boxed subscripts added distinguish
two NPs).
work formulates sentence compression framework synchronous treesubstitution grammar (STSG, Eisner, 2003). STSG allows describe non-isomorphic tree
pairs (the grammar rules comprise trees arbitrary depth) thus suited textrewriting tasks typically involve number local modifications input text.
Especially modification described succinctly terms syntactic transformations, dropping adjectival phrase converting passive verb phrase active
form. STSG restricted version synchronous tree adjoining grammar (STAG, Shieber
& Schabes, 1990) without adjunction operation. STAG affords mild context sensitivity,
however increased cost inference. SCFG STSG weakly equivalent, is,
string languages identical produce equivalent tree pairs. example,
Figure 2, rules (1)(4) expressed SCFG rules, rule (5) cannot
source target fragments two level trees. fact would impossible
describe trees Figure 1 using SCFG. grammar rules therefore general
obtained Knight Marcu (2002) account elaborate tree
divergences. Moreover, adopting expressive grammar formalism, naturally model syntactically complex compressions without specify additional rules
(as Turner & Charniak, 2005).
synchronous grammar license large number compressions given source
tree. grammar rule typically score overall score compression sentence x derived. Previous work estimates scores generatively
discussed above. opt discriminative training procedure allows incorporation manner powerful features. use large margin technique proposed
Tsochantaridis et al. (2005). framework attractive supports configurable loss function, describes extent predicted target tree differs
640

fiSentence Compression Tree Transduction

reference tree. devising suitable loss functions model straightforwardly
adapted text rewriting tasks besides sentence compression.
McDonald (2006) also presents sentence compression model uses discriminative
large margin algorithm. model rich feature set defined compression bigrams
including parts speech, parse trees, dependency information, without however making explicit use synchronous grammar. Decoding model amounts finding
combination bigrams maximize scoring function defined adjacent words
compression intervening words dropped. model differs
McDonalds two important respects. First, capture complex tree transformations go beyond bigram deletion. tree-based, decoding algorithm
better able preserve grammaticality compressed output. Second, treebased representation allows greater modeling flexibility, e.g., defining wide range
loss functions tree string yield. contrast, McDonald define loss
functions final compression.
Although bulk research sentence compression relies parallel corpora
modeling purposes, approaches use training data small amount.
example work Hori Furui (2004), propose model automatically
transcribed spoken text. method scores candidate compressions using language
model combined significance score (indicating whether word topical not),
score representing speech recognizers confidence transcribing given word
correctly. Despite conceptually simple knowledge lean, model operates
word level. Since take syntax account, means deleting
constituents spanning several subtrees (e.g., relative clauses). Clarke Lapata (2008)
show unsupervised models greatly improved linguistically motivated
constraints used decoding.

3. Problem Formulation
mentioned earlier, formulate sentence compression tree-to-tree rewriting problem
using weighted synchronous grammar coupled large margin training process.
model learns parallel corpus input (uncompressed) output (compressed) pairs
(x1 , y1 ), . . . , (xn , yn ) predict target labeled tree source labeled tree x.
capture dependency x weighted STSG define
following section. Section 3.2 discusses extract grammar parallel
corpus. rule score, ngram output tree,
overall score compression sentence x derived. introduce scoring
function Section 3.3 explain training algorithm Section 3.5. framework
decoding amounts finding best target tree licensed grammar given source
tree. present chart-based decoding algorithm Section 3.4.
3.1 Synchronous Grammar
synchronous grammar defines space valid source target tree pairs, much
regular grammar defines space valid trees. Synchronous grammars treated tree
transducers reasoning space possible sister trees given tree, is,
trees produced alongside given tree. essentially transducer
641

fiCohn & Lapata

Algorithm 1 Generative process creating pair trees.
initialize source tree, x = RS
initialize target tree, = RT
initialize stack frontier nodes, F = [(RS , RT )]
node pairs, (vS , vT ) F
choose rule hvS , vT h, ,
rewrite node vS x
rewrite node vT
variables, u
find aligned child nodes, (cS , cT ), vS vT corresponding u
push (cS , cT ) F
end
end
x complete

takes tree input produces tree output. grammar rules specify
steps taken transducer recursively mapping tree fragments input tree
fragments target tree. many families synchronous grammars (see
Section 2), elect use synchronous tree-substitution grammar (STSG). one
simpler formalisms, consequently efficient inference algorithms, still
complex enough model rich suite tree edit operations.
STSG 7-tuple, G = (NS , NT , , , P, RS , RT ) N non-terminals
terminals, subscripts indicating source target respectively, P productions RS NS RT NT distinguished root symbols.
production rewrite rule two aligned non-terminals X NS NT
source target:
hX, h, ,
(1)
elementary trees rooted symbols X respectively. Note
synchronous context free grammar (SCFG) limits one level elementary
trees, otherwise identical STSG, imposes limits. Non-terminal
leaves elementary trees referred frontier nodes variables.
points recursion transductive process. one-to-one alignment frontier
nodes specified . alignment represent deletion (or insertion)
aligning node special symbol, indicates node present
tree. nodes aligned , allows subtrees deleted
transduction. disallow converse, -aligned nodes , would license
unlimited insertion target tree, independently source tree. capability
would limited use sentence compression, also increasing complexity
inference.
grammar productions used generative setting produce pairs trees,
transductive setting produce target tree given source tree. Algorithms 1
2 present pseudo-code processes. generative process (Algorithm 1) starts
two root symbols applies production rewrites symbols
productions elementary trees. elementary trees might contain frontier nodes,
642

fiSentence Compression Tree Transduction

Algorithm 2 transduction source tree target tree.
Require: complete source tree, x, root node labeled RS
initialize target tree, = RT
initialize stack frontier nodes, F = [(root(x), RT )]
node pairs, (vS , vT ) F
choose rule hvS , vT h, , matches sub-tree rooted vS x
rewrite vT
variables, u
find aligned child nodes, (cS , cT ), vS vT corresponding u
push (cS , cT ) F
end
end
complete

case aligned pairs frontier nodes pushed stack, later rewritten
using another production. process continues recursive fashion stack
empty frontier nodes remaining , point two trees complete.
sequence rewrite rules referred derivation, source
target tree recovered deterministically.
model uses STSG transductive setting, source tree given
target tree generated. necessitates different rewriting process,
shown Algorithm 2. start source tree, RT , target root symbol,
aligned root node source, denoted root(x). choose production
rewrite pair aligned non-terminals productions source side, , matches
source tree. target symbol rewritten using . variable
matching node source corresponding leaf node target tree pushed
stack later processing.1 process repeats stack empty,
therefore source tree covered. complete target tree.
use term derivation refer sequence production applications. target
string yield target tree, given reading non-terminals tree
left right manner.
Let us consider compression example Figure 1. tree editing rules
Figure 2 encoded STSG productions Figure 3 (see rules (1)(5)). Production (1),
reproduces tree pair (1) Figure 2, production (2) tree pair (2), on. notation
Figure 3 (primarily space reasons) uses brackets ([]) indicate constituent boundaries.
Brackets surround constituents non-terminal child nodes,
terminals, non-terminals bracketed subtrees. boxed indices short-hand notation
alignment, . example, rule (1) specify two WP non-terminals
aligned RB node occurs source tree (i.e., heads deleted subtree). grammar rules allow differences non-terminal category source
target, seen rules (2)(4). also allow arbitrarily deep elementary trees,
1. Special care must taken aligned variables. Nodes -aligned signify source
sub-tree point deleted without affecting target tree. reason safely
ignore source nodes deleted manner.

643

fiCohn & Lapata

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
(11)
(12)

hWHNP, WHNPi
hS, NPi
hS, VPi
hS, VPi
hS, Si
hWP, WPi
hNP, NPi
hNNS, NNSi
hVP, VPi
hVBP, VBPi
hVP, VPi
hVBN, VBNi

Rules perform major tree edits
h[WHNP RB WP 1 ], [WHNP WP 1 ]i
h[S NP 1 VP ], NP 1
h[S NP VP 1 ], VP 1
h[S WHNP 1 ], VP 1
h[S [S WHNP 1 2 ] [CC and] 3 ], [S WHNP 1 [S NP 2 VP 3 ]]i
Rules preserve tree structure
h[WP what], [WP what]i
h[NP NNS 1 ], [NP NNS 1 ]i
h[NNS records], [NNS records]i
h[VP VBP 1 VP 2 ], [VP VBP 1 VP 2 ]i
h[VBP are], [VBP are]i
h[VP VBN 1 ], [VP VBN 1 ]i
h[VBN involved], [VBN involved]i

Figure 3: rules Synchronous Tree Substitution Grammar (STSG) capable generating sentence pair Figure 1. Equivalently, grammar defines
transducer convert source tree (Figure 1(a)) target tree
(Figure 1(b)). rule rewrites pair non-terminals pair subtrees,
shown bracketed notation.

evidenced rule (5) trees depth two. Rules (6)(12) complete
toy grammar describes tree pair Figure 1. rules copy parts
source tree target, terminals (e.g., rule (6)) internal nodes children
(e.g., rule (9)).
Figure 4 shows grammar used transduce source tree
target tree Figure 1. first steps derivation also shown graphically Figure 5. start source tree, seek transduce root symbol
target root symbol, denoted S/S. first rule applied rule (5) Figure 3; source side, = [S [S WHNP S] [CC and] S], matches root source tree
requisite target category, = S. matching part source tree
rewritten using rules target elementary tree, = [S WHNP [S NP VP]]. three
three variables annotated reflect category transformations required
node, WHNP/WHNP, S/NP S/VP. process continues leftmost
nodes, labeled WHNP/WHNP. Rule (1) (from Figure 3) applied, deletes
nodes left child, shown RB/, retains right child. subsequent rule completes
transduction WHNP node matching string exactly. algorithm continues visit variable node finishes variable nodes remaining,
resulting desired target tree.
3.2 Grammar
previous section outlined STSG formalism employ sentence compression
model, save one important detail: grammar itself. example, could obtain
644

fiSentence Compression Tree Transduction

[S/S [S [WHNP exactly what] [S [NP records] [VP made it]]]
[CC and] [S [WHNP which] [S [NP ones] [VP involved]]]]
5
[S [WHNP/WHNP [RB exactly] [WP what]] [S [S/NP [NP records] [VP made it]]
[S/VP [WHNP which] [S [NP ones] [VP involved]]]]]
1
[S [WHNP [WP/WP what]] [S [S/NP [NP records] [VP made it]]
[S/VP [WHNP which] [S [NP ones] [VP involved]]]]]
6
[S [WHNP [WP what]] [S [S/NP [NP records] [VP [VBD made] [NP [PRP it]]]]]
[S/VP [WHNP which] [S [NP ones] [VP [VBP are] [VP [VBN involved]]]]]]]
2
[S [WHNP [WP what]] [S [NP [NNS/NNS records]]
[S/VP [WHNP which] [S [NP ones] [VP involved]]]]]
8
[S [WHNP [WP what]] [S [NP [NNS records]]]
[S/VP [WHNP which] [S [NP ones] [VP involved]]]]
4
[S [WHNP what] [S [NP records] [S/VP [NP ones] [VP involved]]]]
3
[S [WHNP what] [S [NP records] [VP/VP [VP [VBP are] [VP [VBN involved]]]]]]
9
[S [WHNP what] [S [NP records] [VP [VBP/VBP are] [VP/VP [VBN involved]]]]]
10 [S [WHNP what] [S [NP records] [VP [VBP are] [VP/VP [VBN involved]]]]]
11 [S [WHNP what] [S [NP records] [VP [VBP are] [VP [VBN/VBN involved]]]]]
12 [S [WHNP [WP what]] [S [NP [NNS records]] [VP [VBP are] [VP [VBN involved]]]]]

Figure 4: Derivation example sentence pair Figure 1. line shows rewrite step,
denoted subscript identifies rule used. frontier
nodes shown bold X/Y indicating symbol X must transduced
subsequent steps. sake clarity, internal nodes
omitted.

synchronous grammar hand, automatically corpus, combination.
requirement grammar allows source trees training set
transduced corresponding target trees. maximum generality, devised
automatic method extract grammar parsed, word-aligned parallel compression
corpus. method maps word alignment constituent level alignment
nodes source target trees. Pairs aligned subtrees next generalized create
tree fragments (elementary trees) form rules grammar.
first step algorithm find constituent alignment, define
set source target constituent pairs whose yields aligned one another
word alignment. base approach alignment template method (Och & Ney,
2004), uses word alignments define alignments ngrams (called phrases
SMT literature). method finds pairs ngrams least one word one
ngrams aligned word other, word either ngram aligned
word outside ngram. addition, require ngrams syntactic
constituents. formally, define constituent alignment as:
C = {(vS , vT ), ((s, t) (vS ) (vT ))

(2)

(@(s, t) (s (vS ) (vT )))}
vS vT source target tree nodes (subtrees), = {(s, t)} set word
alignments (pairs word-indices), () returns yield span subtree (the minimum
maximum word index yield) exclusive-or operator. Figure 6 shows
645

fiCohn & Lapata











VP

WHNP

NP

RB

WP

NNS

exactly



records

VP
NP

WHNP

VBD PRP CC
made





WP


NP

VP

NNS VBP
ones



VBN
involved









VP

WHNP

NP

RB

WP

NNS

exactly



records

WHNP


NP

made

NP

VP
WHNP

VBD PRP CC




WP


NP


VBN
involved










NP

RB

WP

NNS

exactly



records

WHNP


VP

WHNP

WHNP

VBD PRP CC
made





WP


NP

VP

NNS VBP
ones



WP NP VP

VP
NP

VP

VP

NNS VBP
ones





VBN
involved

Figure 5: Graphical depiction first two steps derivation Figure 4. source
tree shown left partial target tree right. Variable nodes
shown bold face dotted lines show alignment.

word alignment constituent alignments licensed sentence pair
Figure 1.
next step generalize aligned subtree pairs replacing aligned child subtrees
variable nodes. example, Figure 6 consider pair aligned subtrees
[S ones involved] [VP involved], could extract rule:
hS,VPi h[S [WHNP [WP which]] [S [NP [NNS ones] [VP [VBP are] [VP [VBN involved]]]]]],
[VP [VBP are] [VP [VBN involved]]]i

(3)

However, rule specific consequently useful transduction
model. order applied, must see full subtree, highly unlikely
occur another sentence. Ideally, generalize rule match many
source trees, thereby allow transduction previously unseen structures.
example, node pairs labeled (VP1 , VP1 ), (VBP, VBP), (VP2 , VP2 ) (VBN, VBN)
generalized nodes aligned constituents (subscripts added distinguish
646

fiSentence Compression Tree Transduction








VP

WHNP

NP

NP

VP
WHNP NP

VP

RB
WP NNS VBD PRP CC WP NNS VBP VBN
exactly records made
ones involved


VP
WHNP NP
VP
WP NNS VBP VBN
records involved

Figure 6: Tree pair word alignments shown binary matrix. dark square indicates
alignment words row column. overlaid rectangles
show constituent alignments inferred word alignment.

two VP nodes). addition, nodes WHNP, WP, NP NNS source
unaligned, therefore generalized using -alignment signify deletion.
perform possible generalizations example,2 would produce
rule:
hS,VPi h[S WHNP 1 ], VP 1
(4)
many possible rules extracted applying different legal
combinations generalizations (there 45 total example).
Algorithm 3 shows minimial (most general) rules extracted.3 results
minimal set synchronous rules describe tree pair.4 rules
minimal sense cannot made smaller (e.g., replacing subtree
variable) still honoring word-alignment. Figure 7 shows resulting minimal
set synchronous rules example Figure 6. seen example,
many rules extracted overly general. Ideally, would extract every rule
every legal combination generalizations, however leads massive number rules
exponential size source tree. address problem allowing limited
number generalizations skipped extraction process. equivalent
altering lines 4 7 Algorithm 3 first make non-deterministic decision whether
match ignore match continue descending source tree. recursion depth
limits number matches ignored way. example, allow one
2. generalizations mutually exclusive, take highest match trees.
3. non-deterministic matching step line 8 allows matching options individually.
implemented mutually recursive function replicates algorithm state process
different match.
4. Algorithm 3 extension Galley, Hopkins, Knight, Marcus (2004) technique extracting
SCFG word-aligned corpus consisting (tree, string) pairs.

647

fiCohn & Lapata

Algorithm 3 extract(x, y, A): extracts minimal rules constituent-aligned trees
Require: source tree, x, target tree, y, constituent-alignment,
1: initialize source target sides rule, = x, =
2: initialize frontier alignment, =
3: nodes vS , top-down
4:
vS null-aligned
5:
(vS , )
6:
delete children
7:
else vS aligned target node(s)
8:
choose target node, vT
{non-deterministic choice}
9:
call extract(vS , vT , A)
10:
(vS , vT )
11:
delete children vS
12:
delete children vT
13:
end
14: end
15: emit rule hroot(), root()i h, ,
level recursion extracting rules (S, VP) pair Figure 6, get
additional rules:
hS,VPi h[S [WHNP WP ] 1 ], VP 1
hS,VPi h[S WHNP [S NP VP 1 ]], VP 1
two levels recursion, also get:
hS,VPi h[S [WHNP [WP which]] 1 ], VP 1
hS,VPi h[S [WHNP [WP which]] [S NP VP 1 ]], VP 1
hS,VPi h[S WHNP [S [NP NNS ] VP 1 ]], VP 1
hS,VPi h[S WHNP [S NP [VP VBD 1 VP 2 ]]], [VBD 1 VBD 2 ]i
Compared rule (4) see specialized rules add useful structure
lexicalisation, still sufficiently abstract generalize new sentences, unlike
rule (3). number rules exponential recursion depth, fixed depth
polynomial size source tree fragment. set recursion depth
small number (one two) experiments.
guarantee induced rules good coverage unseen trees.
Tree fragments containing previously unseen terminals non-terminals, even unseen
sequence children parent non-terminal, cannot matched grammar productions. case transduction algorithm (Algorithm 2) fail way
covering source tree. However, problem easily remedied adding new
rules grammar allow source tree fully covered.5 node
5. alternative, equally valid, techniques improving coverage simplify syntax trees.
example, done explicitly binarizing large productions (e.g., Petrov, Barrett, Thibaux,
& Klein, 2006) implicitly Markov grammar grammar productions (e.g., Collins, 1999).

648

fiSentence Compression Tree Transduction

hS,Si
hWHNP,WHNPi
hWP,WPi
hS,NPi
hNP,NPi
hNNS,NNSi
hS,VPi
hS,VPi
hVP,VPi
hVBP,VBPi
hVP,VPi
hVBN,VBNi














h[S [S WHNP 1 2 ] CC 3 ], [S WHNP 1 [S NP 2 VP 3 ]]i
h[WHNP RB WP 1 ], [WHNP WP 1 ]i
h[WP what], [WP what]i
h[S NP 1 VP ], NP 1
h[NP NNS 1 ], [NP NNS 1 ]i
h[NNS records], [NNS records]i
h[S WHNP 1 ], VP 1
h[S NP VP 1 ], VP 1
h[VP VBP 1 VP 2 ], [VP VBP 1 VP 2 ]i
h[VBP are], [VBP are]i
h[VP VBN 1 ], [VP VBN 1 ]i
h[VBN involved], [VBN involved]i

Figure 7: minimal set STSG rules extracted aligned trees Figure 6.

source tree, rule created copy node child nodes target tree.
example, see fragment [NP DT JJ NN] source tree, add rule:

hNP,NPi h[NP DT 1 JJ 2 NN 3 ], [NP DT 1 JJ 2 NN 3 ]i

rules, source node copied target tree, therefore transduction algorithm trivially recreate original tree. course, grammar
rules work conjunction copying rules produce target trees.
copy rules solve coverage problem unseen data, solve
related problem under-compression. occurs unseen CFG productions
source tree therefore applicable grammar rules copy rules,
copy child nodes target. None child subtrees deleted unless
parent node deleted higher-level rule, case children
deleted. Clearly, would add considerable modelling flexibility able delete some,
all, children. reason, add explicit deletion rules source
CFG production allow subsets child nodes deleted linguistically
plausible manner.
deletion rules attempt preserve important child nodes. measure
importance using head-finding heuristic Collins parser (Appendix A, Collins,
1999). Collins method finds single head child CFG production using hand-coded
tables non-terminal type. desire set child nodes, run algorithm
find matches rather stopping first match. order match
found used ranking importance child. ordered list child nodes
used create synchronous rules retain head 1, heads 12, . . . , heads.
649

fiCohn & Lapata

fragment [NP DT JJ NN], heads found following order (NN, DT,
JJ). Therefore create rules retain children (NN); (DT, NN) (DT, JJ, NN):
hNP,NPi h[NP DT JJ NN 1 ], [NP NN 1 ]i
hNP,NNi h[NP DT JJ NN 1 ], NN 1
hNP,NPi h[NP DT 1 JJ NN 2 ], [NP DT 1 NN 2 ]i
hNP,NPi h[NP DT 1 JJ 2 NN 3 ], [NP DT 1 JJ 2 NN 3 ]i
Note one child remains, rule also produced without parent node,
seen second rule above.
3.3 Linear Model
STSG defines transducer capable mapping source tree many possible
target trees, little use without kind weighting towards grammatical trees
constructed using sensible STSG productions yield fluent compressed target sentences. Ideally model would define scoring function target
trees strings, however instead operate derivations. general, may many
derivations produce target tree, situation referred spurious ambiguity. fully account spurious ambiguity would require aggregating derivations
produce target tree. would break polynomial-time dynamic program used inference, rendering inference problem NP-complete (Knight, 1999).
end, define scoring function derivations:
score(d; w) = h(d), wi

(5)

derivation6 consisting sequence rules, w model parameters,
vector-valued feature function operator h, inner product.
parameters, w, learned training, described Section 3.5.
feature function, , defined as:
X
X
(d) =
(r, source(d)) +
(m, source(d))
(6)
rd

mngrams(d)

r rules derivation, ngrams(d) ngrams yield target
tree feature function returning vector feature values rule. Note
feature function access rule, r, also source tree, source(d),
conditional model therefore overhead terms modeling
assumptions complexity inference.
second summand (6), ngrams yield target tree
feature function ngrams. Traditional (weighted) synchronous grammars
allow features decompose derivation (i.e., expressed using first
summand (6)). However, limiting requirement, ngram features
allow modeling local coherence commonly used sentence compression
literature (Knight & Marcu, 2002; Turner & Charniak, 2005; Galley & McKeown, 2007;
6. derivation, d, fully specifies source, x = source(d), target tree, = target(d).

650

fiSentence Compression Tree Transduction

Clarke & Lapata, 2008; Hori & Furui, 2004; McDonald, 2006). instance, deleting
sub-tree left right siblings, critical know new siblings
grammatical configuration, also yield still forms coherent string.
reason, allow ngram features, specifically conditional log-probability
ngram language model. Unfortunately, comes price ngram features
significantly increase complexity inference used training decoding.
3.4 Decoding
Decoding aims find best target tree licensed grammar given source tree.
mentioned above, deal derivations place target trees. Decoding finds
maximizing derivation, , of:
=

argmax

score(d; w)

(7)

d:source(d)=x

x (given) source tree, source(d) extracts source tree derivation
score defined (5). maximization performed space derivations
given source tree, defined transduction process shown Algorithm 2.
maximization problem (7) solved using chart-based dynamic program
shown Algorithm 4. extends earlier inference algorithms weighted STSGs (Eisner, 2003) assume scoring function must decompose derivation,
i.e., features apply rules terminal ngrams. Relaxing assumption leads
additional complications increased time space complexity. equivalent using grammar intersection original grammar ngram language
model, explained Chiang (2007) context string transduction SCFG.
algorithm defines chart, C, record best scoring (partial) target tree
source node vS root non-terminal t. back-pointers, B, record maximizing
rule store pointers child chart cells filling variable rule. chart
also indexed n 1 terminals left right edges target trees yield
allow scoring ngram features.7 terminal ngrams provide sufficient context evaluate
ngram features overlapping cells boundary chart cell combined another
rule application (this operation performed boundary-ngrams function line
15). best illustrated example. Using trigram features, n = 3, node
rewritten [NP fast car] must store ngram context (the fast, fast car)
chart entry. Similarly [VP skidded halt] would ngram context (skidded to,
halt). applying parent rule [S NP VP] rewrites two trees adjacent
siblings need find ngrams boundary NP VP.
easily retrieved two chart cells contexts. combine right edge NP
context, fast car, left edge VP context, skidded to, get two trigrams
fast car skidded car skidded to. trigrams fast car, skidded
halt already evaluated child chart cells. new
combined chart cell given context (the fast, halt) taking left right
7. Strictly speaking, terminals right edge required compression model would
create target string left-to-right manner. However, algorithm general
allows reordering rules hPP,PPi h[PP 1 NP 2 ], [PP NP 2 1 ]i. rules required
text-rewriting tasks besides sentence compression.

651

fiCohn & Lapata

Algorithm 4 Exact chart based decoding algorithm.
Require: complete source tree, x, root node labeled RS
1: let C[v, t, l] R chart representing score best derivation transducing
tree rooted v tree root category ngram context l
2: let B[v, t, l] (P, x NT L) corresponding back-pointers, consisting
production source node, target category ngram context
productions variables
3: initialize chart, C[, , ] =
4: initialize back-pointers, B[, , ] = none
5: source nodes, vS x, bottom-up
6:
rules, r = hvS , h, , matches sub-tree rooted vS
7:
let target ngrams wholly contained
8:
let features vector, (r, x) + (m, x)
9:
let l empty ngram context
10:
let score, q 0
11:
variables, u
12:
find source child node, cu , vS corresponding u
13:
let tu non-terminal target child node corresponding u
14:
choose child chart entry, qu = C[cu , tu , lu ]
{non-deterministic choice lu }
15:
let boundary-ngrams(r, lu )
16:
update features, + (m, x)
17:
update ngram context, l merge-ngram-context(l, lu )
18:
update score, q q + qu
19:
end
20:
update score, q q + h, wi
21:
q > C[vS , Y, l]
22:
update chart, C[vS , Y, l] q
23:
update back-pointers, B[vS , Y, l] (r, {(cu , tu , lu )u})
24:
end
25:
end
26: end
27: find best root chart entry, l argmaxl C[root(x), RT , l]
28: create derivation, d, traversing back-pointers B[root(x), RT , l ]

edges two child cells. merging process performed merge-ngram-context
function line 17. Finally add artificial root node target tree n 1 artificial
start terminals one end terminal. allows ngram features applied
boundary ngrams beginning end target string.
decoding algorithm processes source tree post-order traversal, finding
set possible trees ngram contexts source node inserting
chart. rules match node processed lines 624. feature vector,
, calculated rule ngrams therein (line 8), ngrams bordering child
cells filling rules variables (line 16). Note feature vector includes
features specific rule boundary ngrams, wholly contained
652

fiSentence Compression Tree Transduction

child cell. reason score sum scores child cell (line
18) feature vector model weights (line 20). new ngram context, l,
calculated combining rules frontier ngram contexts child cells (line
17). Finally chart entry node updated score betters previous value
(lines 2124).
choosing child chart cell entry line 14, many different entries
different ngram context, lu . affects ngram features, , consequently
ngram context, l, score, q, rule. non-determinism means every
combination child chart entries chosen variable, combinations
evaluated inserted chart. number combinations product
number child chart entries variable. bounded O(|TT |2(n1)V )
|TT | size target lexicon V number variables. Therefore
asymptotic time complexity decoding O(SR|TT |2(n1)V ) number
source nodes R number matching rules node. high complexity
clearly makes exact decoding infeasible, especially either n V large.
adopt popular approach syntax-inspired machine translation address
problem (Chiang, 2007). Firstly, use beam-search, limits number different
ngram contexts stored chart cell constant, W . changes base
complexity term, leading improved O(SRW V ) still exponential
number variables. addition, use Chiangs cube-pruning heuristic
limit number combinations. Cube-pruning uses heuristic scoring function
approximates conditional log-probability ngram language model logprobability unigram model.8 allows us visit combinations best-first
order heuristic scoring function beam filled.The beam rescored
using correct scoring function. done cheaply O(W V ) time, leading
overall time complexity decoding O(SRW V ). refer interested reader
work Chiang (2007) details.
3.5 Training
turn problem derivations scored model. given source
tree, space sister target trees implied synchronous grammar often large,
majority trees ungrammatical poor compressions. job
training algorithm find weights reference target trees high scores
many target trees licensed grammar given lower scores.
explained Section 3.3 define scoring function derivations. function
given (5) (7), reproduced below:
f (d; w) =

argmax hw, (d)i

(8)

d:source(d)=x

Equation (8) finds best scoring derivation, d, given source, x, linear model.
Recall derivation generates source tree x target tree. goal
8. use conditional log-probability ngram language model ngram feature. order
use ngram features, binary identity features specific ngrams, would first advisable
construct approximation decomposes derivation use cube-pruning heuristic.

653

fiCohn & Lapata

training procedure find parameter vector w satisfies condition:
i, : source(d) = xi 6= di : hw, (di ) (d)i 0

(9)

xi , di ith training source tree reference derivation. condition states
training instances reference derivation least high scoring
derivations. Ideally, would also like know extent predicted target
tree differs reference tree. example, compression differs gold
standard respect one two words treated differently compression
bears resemblance it. Another important factor length compression.
Compressions whose length similar gold standard preferable longer
shorter output. loss function (yi , y) quantifies accuracy prediction
respect true output value yi .
plethora different discriminative training frameworks optimize
linear model. Possibilities include perceptron training (Collins, 2002), log-linear optimisation conditional log-likelihood (Berger, Pietra, & Pietra, 1996) large margin
methods. base training Tsochantaridis et al.s (2005) framework learning
Support Vector Machines (SVMs) structured output spaces, using SVMstruct implementation.9 framework supports configurable loss function particularly
appealing context sentence compression generally text-to-text generation. also efficient training algorithm powerful regularization. latter
critical discriminative models large numbers features, would otherwise
over-fit training sample expense generalization accuracy. briefly summarize
approach below; detailed description refer interested reader
work Tsochantaridis et al. (2005).
Traditionally SVMs learn linear classifier separates two classes
largest possible margin. Analogously, structured SVMs attempt separate correct
structure structures large margin. learning objective
structured SVM uses soft-margin formulation allows errors training set
via slack variables, :
n

1
CX
min ||w||2 +
, 0
w, 2
n

(10)

i=1

i, : source(d) = xi 6= di : hw, (di ) (d)i (di , d)
slack variables, , introduced training example, xi C constant
controls trade-off training error minimization margin maximization.
Note slack variables combined loss incurred linear constraints. means high loss output must separated larger margin
low loss output, much larger slack variable satisfy constraint. Alternatively, loss function used rescale slack parameters, case
constraints (10) replaced hw, (di ) (d)i 1 (dii ,d) . Margin rescaling
theoretically less desirable scale invariant, therefore requires tuning
additional hyperparameter compared slack rescaling. However, empirical results show
9. http://svmlight.joachims.org/svm_struct.html

654

fiSentence Compression Tree Transduction

little difference two rescaling methods (Tsochantaridis et al., 2005). use
margin rescaling practical reason approximated accurately
slack rescaling chart based inference method.
optimization problem (10) approximated using algorithm proposed
Tsochantaridis et al. (2005). algorithm finds small set constraints fullsized optimization problem ensures sufficiently accurate solution. Specifically,
constructs nested sequence successively tighter relaxation original problem using
(polynomial time) cutting plane algorithm. training instance, algorithm
keeps track selected constraints defining current relaxation. Iterating
training examples, proceeds finding output radically violates
constraint. case, optimization crucially relies finding derivation
high scoring high loss compared gold standard. requires finding
maximizer of:
H(d) = (d , d) hw, (di ) (d)i

(11)

search maximizer H(d) (11) performed decoding algorithm presented Section 3.4 extensions. Firstly, expanding (11)
H(d) = (d , d) h(di ), wi + h(d), wi see second term constant
respect d, thus influence search. decoding algorithm maximizes
last term, remains include loss function search process.
Loss functions
decompose
rules target ngrams derivation,
P
P

, r) +

(d
(d , d) =
nngrams(d) N (d , n), easily integrated
rd R
decoding algorithm. done adding partial loss, R (d , r) + N (d , n)
rules score line 20 Algorithm 4 (the ngrams recovered ngram contexts
manner used evaluate ngram features).
However, many loss functions decompose rules ngrams.
order calculate losses chart must stratified loss functions arguments
(Joachims, 2005). example, unigram precision measures ratio correctly predicted
tokens total predicted tokens therefore loss arguments pair counts,
(T P, F P ), true false positives. initialized (0, 0) updated
rule used derivation. equates checking whether target terminal
reference string incrementing relevant value. chart extended (stratified)
store loss arguments way ngram contexts stored decoding.
means rule accessing child chart cell get multiple entries,
different loss argument values well multiple ngram contexts (line 14 Algorithm
4). loss argument rule application calculated rule loss
arguments children. stored chart back-pointer list (lines
2223 Algorithm 4). Although loss evaluated correctly complete
derivations, also evaluate loss partial derivations part cube-pruning
heuristic. Losses large space argument values coarsely approximated
beam search, prunes number chart entries constant size.
reason, focused mainly simple loss functions relatively small space
argument values, also use wide beam search (200 unique items 500
items, whichever comes first).
655

fiCohn & Lapata

Algorithm 5 Find gold standard derivation pair trees (i.e., alignment).
Require: source tree, x, target tree,
1: let C[vS , vT ] R chart representing maximum number rules used align
nodes vS x vT
2: let B[vS , vT ] (P, x y) corresponding back-pointers, consisting production
pair aligned nodes productions variables
3: initialize chart, C[, ] =
4: initialize back-pointers, B[, ] = none
5: source nodes, vS x, bottom-up
6:
rules, r = hvS , h, , matches sub-tree rooted vS
7:
target nodes, vT y, matching
8:
let rule count, j 1
9:
variables, u
10:
find aligned child nodes, (cS , cT ), vS vT corresponding u
11:
update rule count, j j + C[cS , cT ]
12:
end
13:
n greater previous value chart
14:
update chart, C[vS , vT ] j
15:
update back-pointers, B[vS , vT ] (r, {(cS , cT )u})
16:
end
17:
end
18:
end
19: end
20: C[root(x), root(y)] 6=
21:
success; create derivation traversing back-pointers B[root(x), root(y)]
22: end
discussion far assumed given gold standard derivation, yi
glossing issue find it. Spurious ambiguity grammar means
often many derivations linking source target, none clearly
correct. select derivation using maximum number rules,
small, therefore provide maximum generality.10 found using Algorithm 5,
chart-based dynamic program similar alignment algorithm inverse transduction
grammars (Wu, 1997). algorithm time complexity O(S 2 R) size
larger two trees R number rules match node.
3.6 Loss Functions
training algorithm described highly modular theory support wide
range loss functions. widely accepted evaluation metric text compression. zero-one loss would straightforward define inappropriate problem,
10. also experimented heuristics, including choosing derivation random selecting
derivation maximum minimum score model (all using search algorithm
different objective). these, maximum scoring derivation competitive
maximum rules heuristic.

656

fiSentence Compression Tree Transduction

would always penalize target derivations differ even slightly reference
derivation. Ideally, would like loss wider scoring range discriminate
derivations differ reference. may good compressions whereas others may entirely ungrammatical. reason developed
range loss functions draw inspiration various metrics used evaluating
text-to-text rewriting tasks summarization machine translation.
Loss functions defined derivations look item accessible including
tokens, ngrams CFG rules. first class loss functions calculates Hamming
distance unordered bags items. measures number predicted items
appear reference, along penalty short output:
hamming (d , d) = F P + max (l (T P + F P ), 0)

(12)

P F P number true false positives, respectively, comparing
predicted target, dT , reference, dT , l length reference.
include second term penalize overly short output otherwise predicting little
nothing would incur penalty.
created three instantiations loss function (12) over: 1) tokens,
2) ngrams (n 3), 3) CFG productions. case, loss argument space
quadratic size source tree. Hamming ngram loss attempt defining
loss function similar BLEU (Papineni, Roukos, Ward, & Zhu, 2002). latter
defined documents rather individual sentences, thus directly applicable
problem. Now, since losses operate unordered bags may reward
erroneous predictions, example, permutation reference tokens zero
token-loss. less problem CFG ngram losses whose items overlap,
thereby encoding partial order. Another problem loss functions described
penalize multiply predicting item occurred reference. could problem function words common sentences.
Therefore developed two additional loss functions take multiple predictions
account. first measures edit distance number insertions deletions
predicted reference compressions, bags-of-tokens. contrast
previous loss functions, requires true positive counts clipped
number occurrences type reference. edit distance given by:
X
edit (d , d) = p + r 2
min(pi , qi )
(13)


p q denote number target tokens predicted tree, target(d),
reference, = target(d ), respectively, pi qi counts type i. loss
arguments edit distance consist vector counts item type
reference, {pi , i}. space possible values exponential size source tree,
compared quadratic Hamming losses. Consequently, expect beam search
result many search errors using edit distance loss.
last loss function F1 measure, harmonic mean precision recall,
measured bags-of-tokens. edit distance, calculation requires counts
clipped number occurrences terminal type reference.
657

fiCohn & Lapata

Ref:
Pred:

[S [WHNP [WP what]] [S [NP [NNS records]] [VP [VBP are] [VP [VBN involved]]]]]
[S [WHNP [WP what]] [S [NP [NNS ones]] [VP [VBP are] [VBN involved]]]]
Loss
Token Hamming
3-gram Hamming
CFG Hamming
Edit distance
F1

Arguments
P = 3, F P = 1
P = 8, F P = 5
P = 8, F P = 1
p = (1, 0, 1, 1, 1)
p = (1, 0, 1, 1, 1)

Value
1/4
5/14
1/9
2
1/4

Table 1: Loss arguments values example predicted reference compressions.
Note loss values compared different loss functions;
values purely illustrative.

therefore use loss arguments calculation. F1 loss given by:
F1 (d , d) = 1
P

min(p ,q )

2 precision recall
precision + recall
P

(14)

min(p ,q )

precision = p recall = q . F1 shares arguments
edit distance loss, also exponential space loss argument values
consequently subject severe pruning beam search used training.
illustrate loss functions, present example Table 1. Here,
prediction (Pred) reference (Ref) length (4 tokens), identical syntactic
structure, differ one word (ones versus records). Correspondingly, three
correct tokens one incorrect, forms arguments token Hamming loss,
resulting loss 1/4. ngram loss measured n 3 start end
string padded special symbols allow evaluation boundary ngrams.
CFG loss records one incorrect CFG production (the preterminal [NNS ones])
total nine productions. last two losses use arguments: vector values
counts reference type. first four cells correspond what, records,
involved, last cell records types. example, edit distance two
(one deletion one insertion) F1 loss 1/4 (precision recall 3/4).

4. Features
feature space defined source trees, x, target derivations, d. devised two
broad classes features, applying grammar rules ngrams target terminals.
defined single ngram feature, conditional log-probability trigram language
model. trained BNC (100 million words) using SRI Language Modeling
toolkit (Stolcke, 2002), modified Kneser-Ney smoothing.
rule hX,Y h, , i, extract features according templates detailed
below. templates give rise binary indicator features, except explicitly stated.
features perform boolean test, returning value 1 test succeeds 0
otherwise. example rule corresponding features shown Table 2.
658

fiSentence Compression Tree Transduction

Type: Whether rule extracted training set, created copy rule and/or
created delete rule. allows model learn preference
three sources grammar rules (see row Type Table 2)
Root: root categories source, X, target, , conjunction, X
(see rows Root Table 2).
Identity: source side, , target side, , full rule, (, , ). allows
model learn weights individual rules sharing elementary tree. Another feature checks rules source target elementary trees identical, =
(see rows Identity Table 2).
Unlexicalised Identity: identity feature templates replicated unlexicalised elementary trees, i.e., terminals removed frontiers (see
rows UnlexId Table 2).
Rule count: feature always 1, allowing model count number rules
used derivation (see row Rule count Table 2).
Word count: Counts number terminals , allowing global preference
shorter longer output. Additionally, record number terminals
source tree, used target terminal count find number
deleted terminals (see rows Word count Table 2).
Yield: features compare terminal yield source, (), target, ().
first feature checks identity two sequences, () (). use identity
features terminal yields, terminal source (see
rows Yield Table 2). also replicate feature templates sequence
non-terminals frontier (pre-terminals variable non-terminals).
Length: Records difference lengths frontiers , whether
targets frontier shorter source (see rows Length Table 2).
features listed defined rules grammar. includes
copy delete rules, described Section 3.2, added address
problem unseen words productions source trees test time. Many
rules applied training set, receive weight share
features rules used training. However, training model learns
disprefer coverage rules unnecessary model training set,
described perfectly using extracted transduction rules. dual use training
set grammar extraction parameter estimation results bias coverage
rules. bias could addressed extracting grammar separate corpus,
case coverage rules would useful modeling training set
testing sets. However, solution problems, namely many target
trees training may longer reachable. bias possible solutions
interesting research problem deserves work.
659

fiCohn & Lapata

Rule: hNP,NNSi h[NP CD ADJP [NNS activists]], [NNS activists]i
Type
type = training set
1
Root
X = NP
1
Root
= NNS
1
Root
X = NP = NNS
1
Identity
= [NP CD ADJP [NNS activists]]
1
Identity
= [NNS activists]
1
Identity = [NP CD ADJP [NNS activists]] = [NNS activists]
1
UnlexId.
unlex. = [NP CD ADJP NNS]
1
UnlexId.
unlex. = NNS
1
UnlexId.
unlex. = [NP CD ADJP NNS] = NNS
1
Rule count

1
Word count
target terminals
1
Word count
source terminals 1
Yield
source = [activists] target = [activists]
1
Yield
terminal activists source target
1
Yield
non-terms. source = [CD, ADJP, NNS] target = [NNS]
1
Yield
non-terminal CD source target
1
Yield
non-terminal ADJP source target
1
Yield
non-terminal NNS source target
1
Length
difference length
2
Length
target shorter
1
Table 2: Features instantiated synchronous rule shown above. features
non-zero values displayed. number source terminals calculated using
source tree time rule applied.

5. Experimental Set-up
section present experimental set-up assessing performance
sentence compression model described above. give details corpora used, briefly
introduce McDonalds (2006) model used comparison approach, explain
system output evaluated.
5.1 Corpora
evaluated system three publicly available corpora. first Ziff-Davis
corpus, popular choice sentence compression literature. corpus originates
collection news articles computer products. created automatically
matching sentences occur article sentences occur abstract (Knight
& Marcu, 2002). two corpora11 created manually; annotators asked
produce target compressions deleting extraneous words source without changing
word order (Clarke & Lapata, 2008). One corpus sampled written sources,
11. Available http://homepages.inf.ed.ac.uk/s0460084/data/.

660

fiSentence Compression Tree Transduction

Corpus
CLspoken
CLwritten
Ziff-Davis

Articles
50
82


Sentences
1370
1433
1084

Training
882
908
1020

Development
78
63
32

Testing
410
462
32

Table 3: Sizes various corpora, measured articles sentence pairs. data split
training, development testing sets measured sentence pairs.

British National Corpus (BNC) American News Text corpus, whereas
created manually transcribed broadcast news stories. henceforth refer
two corpora CLwritten CLspoken, respectively. sizes three
corpora shown Table 3.
three corpora pose different challenges hypothetical sentence compression
system. Firstly, representative different domains text genres. Secondly,
different compression requirements. Ziff-Davis corpus aggressively
compressed comparison CLspoken CLwritten (Clarke & Lapata, 2008). CLspoken speech corpus, often contains incomplete ungrammatical utterances
speech artefacts disfluencies, false starts hesitations. utterances varying lengths, wordy whereas others cannot reduced further. means
compression system leave sentences uncompressed. Finally,
note CLwritten average longer sentences Ziff-Davis CLspoken. Parsers
likely make mistakes long sentences could potentially problematic
syntax-based systems like one presented here.
Although model capable performing editing operation, reordering
substitution, learn training corpora. corpora contain
deletions, therefore model learn transduction rules encoding, e.g.,
reordering. Instead rules encode deleting inserting terminals restructuring internal nodes syntax tree. However, model capable general text
rewriting, given appropriate training set learn perform additional
edits. demonstrated recent results adapting model abstractive
compression (Cohn & Lapata, 2008), edit permitted, deletion.
experiments CLspoken CLwritten followed Clarke Lapatas (2008) partition training, test, development sets. partition sizes shown Table 3.
case Ziff-Davis corpus, Knight Marcu (2002) defined development
set. Therefore randomly selected (and held-out) 32 sentence pairs training
set form development set.
5.2 Comparison State-of-the-Art
evaluated results McDonalds (2006) discriminative model. approach,
sentence compression formalized classification task: pairs words source
sentence classified adjacent target compression. Let x = x1 , . . . , xN
denote source sentence target compression = y1 , . . . , yM yi occurs
x. function L(yi ) {1 . . . N } maps word yi target index word
661

fiCohn & Lapata

source (subject constraint L(yi ) < L(yi+1 )). McDonald defines score
compression sentence x dot product high dimensional feature
representation, f , bigrams corresponding weight vector, w,
score(x, y; w) =


X

hw, f (x, L(yj1 ), L(yj ))i

(15)

i=2

Decoding framework amounts finding combination bigrams maximize
scoring function (15). maximization solved using semi-Markov Viterbi
algorithm (McDonald, 2006).
model parameters estimated using Margin Infused Relaxed Algorithm
(MIRA Crammer & Singer, 2003), discriminative large-margin online learning technique.
McDonald (2006) uses similar loss function Hamming loss (see (12)) without
explicit length penalty. loss function counts number words falsely retained
dropped predicted target relative reference. McDonald employs rich feature
set defined words, parts speech, phrase structure trees, dependencies.
gathered adjacent words compression words dropped.
Clarke Lapata (2008) reformulate McDonalds (2006) model context integer
linear programming (ILP) augment constraints ensuring compressed
output grammatically semantically well formed. example, target sentence
negation, must included compression; source verb subject,
must also retained compression. generate solve ILP every
source sentence using branch-and-bound algorithm. Since obtain performance
improvements McDonalds model several corpora, also use comparison
model.
summarize, believe McDonalds (2006) model good basis comparison
several reasons. First, good performance, treated state-of-theart model. Secondly, similar model many respects training algorithm
feature space differs one important respect: compression performed
strings trees. McDonalds system make use syntax trees,
peripherally via feature set. contrast, syntax tree integral part
model.
5.3 Evaluation
line previous work assessed models output eliciting human judgments.
Following Knight Marcu (2002), conducted two separate experiments. first
experiment participants presented source sentence target compression
asked rate well compression preserved important information
source sentence. second experiment, asked rate grammaticality
compressed outputs. cases used five point rating scale high
number indicates better performance. randomly selected 20 sentences test
portion corpus. sentences compressed automatically system
McDonalds (2006) system. also included gold standard compressions. materials
thus consisted 180 (20 3 3) source-target sentences. Latin square design ensured
subjects see two different compressions sentence. collected
662

fiSentence Compression Tree Transduction

ratings 30 unpaid volunteers, self reported native English speakers. studies
conducted Internet using WebExp,12 software package running Internetbased experiments.
also report results using F1 computed grammatical relations (Riezler et al.,
2003). Although F1 conflates grammaticality importance single score, nevertheless shown correlate reliably human judgments (Clarke & Lapata,
2006). Furthermore, usefully employed development feature engineering parameter optimization experiments. measured F1 directed labeled
dependency relations. models compressed output parsed using RASP
dependency parser (Briscoe & Carroll, 2002). Note could extract dependencies directly output model since generates trees addition strings. However,
refrained order compare models equal footing.

6. Results
framework presented Section 3 quite flexible. Depending grammar extraction strategy, choice features, loss function, different classes models derived.
presenting results test set discuss specific model employed
experiments explain parameters instantiated.
6.1 Model Selection
parameter tuning model selection experiments conducted development set CLspoken corpus. obtained syntactic analyses source target
sentences Bikels (2002) parser. corpus automatically aligned using algorithm finds set deletions transform source target.
equivalent minimum edit distance script deletion operations permitted.
expected, predicted parse trees contained number errors, although
gold standard trees quantify error effect prediction
output. notice, however, errors source trees test set always
negatively affect performance model. many instances model able
recover errors still produce good output compressions. recoveries,
cases involved either deleting erroneous structure entirely preserving it.
often resulted poor output tree, string yield acceptable cases. Less
commonly, model corrected errors source using tree transformation rules.
rules acquired training set errors source tree
test tree. example, one transformation allows prepositional phrase
moved high VP attachment object NP attachment.
obtained synchronous tree substitution grammar CLspoken corpus using
method described Section 3.2. extracted maximally general synchronous rules.
complemented specified rules allowing recursion one ancestor
given node.13 Grammar rules represented features described Section 4.
important parameter modeling framework choice loss function.
12. See http://www.webexp.info/.
13. Rules pruned 5 variables 15 nodes.

663

fiCohn & Lapata

Losses
Hamming (tokens)
Hamming (ngram)
Hamming (CFG)
Edit Distance
F1
Reference

Rating
3.38
3.28
3.22
3.30
3.15
4.28

Std. dev
1.05
1.13
0.91
1.20
1.13
0.70

Table 4: Mean ratings system output (CLspoken development set) using different
loss functions.

evaluated loss functions presented Section 3.6 follows. performed grid search
hyper-parameters (a regularization parameter feature scaling parameter,
balances magnitude feature vectors scale loss function)14
minimized relevant loss development set, used corresponding system
output. gold standard derivation selected using maximum number rules
heuristic, described Section 3.5. beam limited 100 unique items 200 items
total. grammar filtered allow 50 target elementary trees
every source elementary tree.
next asked two human judges rate scale 1 5 systems compressions
optimized different loss functions. get idea quality output
also included human-authored reference compressions. Sentences given high numbers
grammatical preserved important information. mean ratings
shown Table 4. seen differences among losses large,
standard deviation high. Hamming loss tokens performed best
mean rating 3.38, closely followed edit distance (3.30). chose former
latter less coarsely approximated search. subsequent experiments
report results using token-based Hamming loss.
also wanted investigate synchronous grammar influences performance.
default system described used general rules together specialized rules
recursion depth limited one. also experimented grammar uses
specialised rules maximum recursion depth two grammar uses solely
maximally general rules. Table 5 report average compression rate, relations-based
F1 Hamming loss tokens different grammars. see adding
specified rules allows better F1 (and loss) despite fact search space
remains same. observe slight degradation performance moving depth 2
rules. probably due increase spurious ambiguity affecting search quality,
also allowing greater overfitting training data. number transduction rules
grammar also grows substantially increased depth 20,764
maximally general extraction technique 33,430 62,116 specified rules depth
14. found setting regularization parameter C = 0.01 scaling parameter 1 generally
yields good performance across loss functions.

664

fiSentence Compression Tree Transduction

Model
max general rules
depth 1-specified rules
depth 2-specified rules
max rules
max scoring
unigram LM
bigram LM
trigram LM
features
rule features
token features

Compression rate
80.79
79.72
79.71
79.72
81.03
76.83
83.12
79.72
79.72
83.06
85.10

Relations F1
65.04
68.56
66.44
68.56
65.54
59.05
67.71
68.56
68.56
67.51
68.31

Loss
341
315
328
315
344
336
317
315
315
346
341

Table 5: Parameter exploration feature ablation studies (CLspoken development set).
default system shown asterisk.

1 2, respectively. growth grammar size exponential specification
depth therefore small values used.
also inspected rules obtained maximally general extraction technique
better assess rules differ obtained vanilla SCFG (see Knight &
Marcu, 2002). Many rules (12%) deeper structure therefore would
licensed SCFG. due structural divergences source target
syntax trees training set. 13% rules describe change syntactic
category (X 6= ), therefore remaining 76% rules would allowable
Knight Marcus transducer. proportion SCFG rules decreases substantially
rule specification depth increased.
Recall Section 3.3 scoring function defined derivations rather
target trees strings, treat derivation using maximum number rules
gold standard derivation. sanity check, also experimented selecting
derivation maximum score model. results Table 5 indicate
latter strategy effective selecting derivation maximum number
rules. conjecture due overfitting. training data used
extract grammar, derivations maximum score may consist rules
rare features model data well generalize unseen instances.
Finally, conducted feature ablation study assess features useful
task. particularly interested see ngram features would bring
benefit, especially since increase computational complexity decoding
training. experimented unigram, bigram, trigram language model. Note
unigram language model computationally expensive two models
need record ngram contexts chart. shown Table 5,
unigram language model substantially worse bigram trigram deliver
similar performances. also examined impact features grouping
two broad classes, defined rules defined tokens. aim
see whether underlying grammar (represented rule-based features) contributes
665

fiCohn & Lapata

better compression output. results Table 5 reveal two feature groups
perform comparably. However, model using token-based features tends compress
less. features highly lexicalized, model able generalize well
unseen data. conclusion, full feature set better counts two
ablation sets, better compression rate.
results reported measured string output. done first
stripping tree structure compression output, reparsing, extracting dependency
relations finally comparing dependency relations reference. However,
may wish measure quality trees themselves, string yield.
simple way measure this15 would extract dependency relations directly
phrase-structure tree output.16 Compared dependencies extracted predicted
parses using Bikels (2002) parser output string, observe relation F1
score increases uniformly tasks, 2.50% 4.15% absolute. Therefore
systems tree output better encodes syntactic dependencies tree resulting
re-parsing string output. system part NLP pipeline, output
destined down-stream processing, accurate syntax tree extremely
important. also true related tasks desired output tree, e.g.,
semantic parsing.

7. Model Comparison
section present results test set using best performing model
previous section. model uses grammar unlexicalized lexicalized rules
(recursion depth 1), Hamming loss based tokens, features Section 4.
model trained separately corpus (training portion). first discuss
results using relations F1 move human study.
Table 6 illustrates performance model (Transducer1) CLspoken, CLwritten, Ziff Davis. also report results corpora using McDonalds (2006)
model (McDonald) improved version (Clarke ILP) put forward Clarke
Lapata (2008). also present compression rate system reference gold
standard. cases tree transducer model outperforms McDonalds original model
improved ILP-based version.
Nevertheless, may argued model unfair advantage since
tends compress less models, therefore less likely make many
mistakes. ensure case, created version model
compression rate similar McDonald. done relatively straightforwardly
manipulating length penalty Hamming loss. smaller penalty
words model tend drop. Therefore, varied length penalty (and
hyper-parameters) development set order obtain compression rate similar
15. could alternatively measure tree metrics, tree edit distance. However, standard
measures used parser evaluation (e.g., EVALB) would suitable, assume parse
yield fixed. case reference target string often different systems output.
16. extract dependency relations conversion tool CoNLL 2007 shared task, available
http://nlp.cs.lth.se/pennconverter/.

666

fiSentence Compression Tree Transduction

Model
Transducer1
Transducer2
McDonald
Clarke ILP
Reference

CLspoken
Compression rate
82.30
69.89
68.56
77.70
76.11

Relations F1
66.63
59.58
47.48
54.12


Model
Transducer1
Transducer2
McDonald
Clarke ILP
Reference

CLwritten
Compression rate
76.52
61.09
60.12
71.99
70.24

Relations F1
58.02
49.48
48.39
54.84


Model
Transducer1
McDonald
Clarke ILP
Reference

Ziff Davis
Compression rate
67.45
66.26
48.67
56.61

Relations F1
56.55
54.12
46.77


Table 6: Results CLspoken, CLwritten, Ziff Davis corpus (testing set); compression
rate relations-based F1.

McDonald.17 model applied test set performance shown
Table 6 Transducer2. refrained Ziff-Davis, since original
transducer obtained compression rate comparable McDonald (67.45 vs. 66.26).
seen, Transducer2 yields better F1 CLspoken CLwritten. differences
F1 statistically significant using Wilcoxon test (p < 0.01). Transducer1
numerically outperforms McDonald Ziff-Davis, however difference significant
(the Ziff-Davis test set consists solely 32 sentences).
next consider results judgment elicitation study assesses
detail quality generated compressions. Recall participants judge compressed output two dimensions, grammaticality importance. compared
output system (Transducer2 CLspoken CLwritten Transducer1
Ziff-Davis) output McDonald (2006) reference gold standard. Table 7
illustrates examples compressions participants saw.
17. matched compression rate McDonald scaling length penalty 0.50 0.25
CLwritten CLspoken corpora, respectively. Another way control compression rate would
modify chart-based decoder fashion similar McDonald (2006). However, leave
future work.

667

fiCohn & Lapata

S: wish parents teachers could like teacher,
could communicate.
M: wish teachers could like teacher.
T: wish teachers could like this, could communicate.
R: wish parents teachers could like this, could
communicate.
S: Treasury refusing fund phase city technology
colleges.
M: Treasury refusing fund colleges.
T: Treasury refusing fund city technology colleges.
R: Treasury refusing fund city technology colleges.
S: Apparel makers use design clothes quickly produce
deliver best-selling garments.
M: Apparel makers use design clothes produce deliver
best-selling garments.
T: Apparel makers use design clothes.
R: Apparel makers use design clothes.
S: Earlier week, conference call analysts, bank said boosted
credit card reserves $350 million.
M: Earlier said credit card reserves $350 million.
T: conference call analysts, bank boosted card reserves $350
million.
R: conference call analysts bank said boosted credit card
reserves $350 million.
Table 7: Compression examples CLspoken, CLwritten, Ziff-Davis (S: source sentence, M: McDonald, 2006, T: transducer, R: reference gold standard)

Table 8 shows mean ratings18 system (and reference) CLspoken,
CLwritten, Ziff-Davis. carried Analysis Variance (Anova) examine
effect system type (McDonald, Transducer, Reference) compression ratings. Anova revealed reliable effect three corpora. used post-hoc Tukey
tests examine whether mean ratings system differed significantly (p < 0.01).
CLspoken corpus Transducer perceived significantly better McDonald, terms grammaticality importance. obtain result
CLwritten corpus. two systems achieve similar performances Ziff-Davis (the
grammaticality importance score differ significantly). Ziff-Davis seems
less challenging corpus CLspoken CLwritten less likely highlight differences among systems. example, Turner Charniak (2005) present several variants
noisy-channel model, achieve compressions similar quality Ziff-Davis
(grammaticality ratings varied 0.13 informativeness ratings 0.31
human evaluation). cases Transducer McDonald yield significantly
18. statistical tests reported subsequently done using mean ratings.

668

fiSentence Compression Tree Transduction

Model
Transducer
McDonald
Reference

CLspoken
Grammaticality
4.18
2.74
4.58

Importance
3.98
2.51
4.22

Model
Transducer
McDonald
Reference

CLwritten
Grammaticality
4.06
3.05
4.52

Importance
3.21
2.82
3.70

Model
Transducer
McDonald
Reference

Ziff-Davis
Grammaticality
4.07
3.98
4.65

Importance
3.23
3.22
4.12

Table 8: Mean ratings compression output elicited humans ( : sig. diff. McDonald ( < 0.01); : sig. diff. Reference ( < 0.01); using post-hoc Tukey
tests)

worse performance Reference, save one exception. CLspoken corpus,
significant difference Transducer gold standard.
results indicate highly expressive framework good model sentence compression. several experimental conditions, across different domains,
obtain better performance previous work. Importantly, model described
compression-specific, could easily adapted tasks, corpora languages (for
syntactic analysis tools available). supervised, model learns fit
compression rate training data. sense, somewhat inflexible cannot
easily adapt specific rate given user imposed application (e.g.,
displaying text small screens). Nevertheless, compression rate indirectly manipulated adopting loss functions encourage discourage compression directly
decoding stratifying chart length (McDonald, 2006).

8. Conclusions
paper formulated sentence compression tree-to-tree rewriting task.19
developed system licenses space possible rewrites using tree substitution grammar. grammar rule assigned weight learned discriminatively
within large margin model (Tsochantaridis et al., 2005). specialized algorithm used
learn model weights find best scoring compression model. argue
19. source code freely available http://homepages.inf.ed.ac.uk/tcohn/t3.

669

fiCohn & Lapata

proposed framework appealing several reasons. synchronous grammar
provides expressive power capture rewrite operations go beyond word deletion
reordering, changes non-terminal categories lexical substitution. Since
deletion-specific, model could ported rewriting tasks (see Cohn & Lapata,
2008, example) without overhead devising new algorithms decoding
training. Moreover, discriminative nature learning algorithm allows incorporation manner powerful features. rich feature space conjunction
choice appropriate loss function afford greater flexibility fitting empirical
data different domains tasks.
evaluated model three compression corpora (CLspoken, CLwritten, ZiffDavis) showed cases yields results superior state-of-the-art (McDonald, 2006). experiments also designed assess several aspects proposed
framework complexity synchronous grammar, choice loss function,
effect various features, quality generated tree output. observed
performance improvements allowing maximally general grammar rules specified
once, producing larger lexicalized rules. concurs Galley McKeown
(2007) also find lexicalization yields better compression output. choice
loss function appears less effect. devised three classes loss functions
based Hamming distance, Edit distance F1 score. Overall, simple token-based
Hamming loss achieved best results. conjecture due simplicity
evaluated precisely many loss functions isnt affected
poor parser output. feature ablation study revealed ngram features beneficial,
mirroring similar finding machine translation literature (Chiang, 2007). Finally,
found trees created generation algorithm accurate compared
output parser applied string output. augurs well use cascaded NLP
pipeline, systems use compression output input processing,
potentially make better use system output.
Future extensions many varied. obvious extension concerns porting
framework rewriting applications document summarization (Daume III &
Marcu, 2002) machine translation (Chiang, 2007). Initial work (Cohn & Lapata, 2008)
shows tree-to-tree transduction model presented easily adapted
sentence abstraction task compression takes place using rewrite operations
restricted word deletion. Examples include substitution, reordering, insertion.
future directions involve detailed feature engineering, including source conditioned features ngram features besides language model. research needed
establish suitable loss functions compression rewriting tasks. particular
interesting experiment loss functions incorporate wider range
linguistic features beyond parts speech. Examples include losses based parse trees
semantic similarity. Finally, experiments presented work use grammar
acquired training corpus. However, nothing inherent formalization
restricts us particular grammar. therefore plan investigate potential method unsupervised semi-supervised grammar induction techniques
rewriting tasks including paraphrase generation machine translation.
670

fiSentence Compression Tree Transduction

Acknowledgments
grateful Philip Blunsom insightful comments suggestions
anonymous referees whose feedback helped substantially improve present paper.
Special thanks James Clarke sharing implementations Clarke Lapatas
(2008) McDonalds (2006) models us. acknowledge support EPSRC
(grants GR/T04540/01 GR/T04557/01). work made use resources
provided Edinburgh Compute Data Facility (ECDF). ECDF partially
supported eDIKT initiative. preliminary version work published
proceedings EMNLP/CoNLL 2007.

References
Aho, A. V., & Ullman, J. D. (1969). Syntax directed translations pushdown assembler. Journal Computer System Sciences, 3, 3756.
Alshawi, H., Bangalore, S., & Douglas, S. (2000). Learning dependency translation models
collections finite state head transducers. Computational Linguistics, 26 (1), 45
60.
Berger, A. L., Pietra, S. A. D., & Pietra, V. J. D. (1996). maximum entropy approach
natural language processing. Computational Linguistics, 22 (1), 3971.
Bikel, D. (2002). Design multi-lingual, parallel-processing statistical parsing engine.
Proceedings 2nd International Conference Human Language Technology
Research, pp. 2427, San Diego, CA.
Briscoe, E. J., & Carroll, J. (2002). Robust accurate statistical annotation general text.
Proceedings Third International Conference Language Resources Evaluation, pp. 14991504, Las Palmas, Gran Canaria.
Carroll, J., Minnen, G., Pearce, D., Canning, Y., Devlin, S., & Tait, J. (1999). Simplifying
text language impaired readers. Proceedings 9th Conference European Chapter Association Computational Linguistics, pp. 269270, Bergen,
Norway.
Chandrasekar, R., & Srinivas, C. D. B. (1996). Motivations methods text simplification. Proceedings 16th International Conference Computational
Linguistics, pp. 10411044, Copenhagen, Danemark.
Chiang, D. (2007). Hierarchical phrase-based translation. Computational Linguistics, 33 (2),
201228.
Clarke, J., & Lapata, M. (2006). Models sentence compression: comparison across
domains, training requirements evaluation measures. Proceedings 21st
International Conference Computational Linguistics 44th Annual Meeting
Association Computational Linguistics, pp. 377384, Sydney, Australia.
Clarke, J., & Lapata, M. (2008). Global inference sentence compression: integer linear
programming approach. Journal Artificial Intelligence Research, 31, 399429.
671

fiCohn & Lapata

Cohn, T., & Lapata, M. (2008). Sentence compression beyond word deletion. Proceedings 22nd International Conference Computational Linguistics, pp. 137144,
Manchester, UK.
Collins, M. (2002). Discriminative training methods hidden Markov models: theory
experiments perceptron algorithms. Proceedings 2002 Conference
Empirical Methods Natural Language Processing, pp. 18, Morristown, NJ.
Collins, M. J. (1999). Head-driven statistical models natural language parsing. Ph.D.
thesis, University Pennsylvania, Philadelphia, PA.
Crammer, K., & Singer, Y. (2003). Ultraconservative online algorithms multiclass problems. Machine Learning, 3, 951999.
Daume III, H., & Marcu, D. (2002). noisy-channel model document compression.
Proceedings 40th Annual Meeting thev Association Computational
Linguistics, pp. 449456, Philadelphia, PA.
Eisner, J. (2003). Learning non-isomorphic tree mappings machine translation.
Companion Volume Proceedings 41st Annual Meeting Association
Computational Linguistics, pp. 205208, Sapporo, Japan.
Galley, M., Hopkins, M., Knight, K., & Marcu, D. (2004). Whats translation rule?.
Proceedings 2004 Human Language Technology Conference North
American Chapter Association Computational Linguistics, pp. 273280,
Boston, MA.
Galley, M., & McKeown, K. (2007). Lexicalized Markov grammars sentence compression.
Proceedings Human Language Technologies 2007: Conference North
American Chapter Association Computational Linguistics, pp. 180187,
Rochester, NY.
Grael, J., & Knight, K. (2004). Training tree transducers. Proceedings 2004 Human
Language Technology Conference North American Chapter Association
Computational Linguistics, pp. 105112, Boston, MA.
Hermjakob, U., Echihabi, A., & Marcu, D. (2002). Natural language based reformulation
resource wide exploitation question answering. Proceedings 11th Text
Retrieval Conference, Gaithersburg, MD.
Hori, C., & Furui, S. (2004). Speech summarization: approach word extraction
method evaluation. IEICE Transactions Information Systems, E87D(1), 1525.
Jing, H. (2000). Sentence reduction automatic text summarization. Proceedings
6th Applied Natural Language Processing Conference, pp. 310315, Seattle, WA.
Joachims, T. (2005). support vector method multivariate performance measures.
Proceedings 22nd International Conference Machine Learning, pp. 377384,
Bonn, Germany.
Kaji, N., Okamoto, M., & Kurohashi, S. (2004). Paraphrasing predicates written
language spoken language using web. Proceedings 2004 Human Language Technology Conference North American Chapter Association
Computational Linguistics, pp. 241248, Boston, MA.
672

fiSentence Compression Tree Transduction

Knight, K. (1999). Decoding complexity word-replacement translation models. Computational Linguistics, 25 (4), 607615.
Knight, K., & Marcu, D. (2002). Summarization beyond sentence extraction: probabilistic
approach sentence compression. Artificial Intelligence, 139 (1), 91107.
Lin, D., & Pantel, P. (2001). Discovery inference rules question answering. Natural
Language Engineering, 7 (4), 342360.
McDonald, R. (2006). Discriminative sentence compression soft syntactic constraints.
Proceedings 11th Conference European Chapter Association
Computational Linguistics, pp. 297304, Trento, Italy.
Nguyen, M. L., Horiguchi, S., Shimazu, A., & Ho, B. T. (2004). Example-based sentence
reduction using hidden markov model. ACM Transactions Asian Language
Information Processing, 3 (2), 146158.
Och, F. J., & Ney, H. (2004). alignment template approach statistical machine
translation. Computational Linguistics, 30 (4), 417449.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: method automatic
evaluation machine translation. Proceedings 40th Annual Meeting thev
Association Computational Linguistics, pp. 311318, Philadelphia, PA.
Petrov, S., Barrett, L., Thibaux, R., & Klein, D. (2006). Learning accurate, compact,
interpretable tree annotation. Proceedings 21st International Conference
Computational Linguistics 44th Annual Meeting Association Computational Linguistics, pp. 433440, Sydney, Australia.
Riezler, S., King, T. H., Crouch, R., & Zaenen, A. (2003). Statistical sentence condensation
using ambiguity packing stochastic disambiguation methods lexical-functional
grammar. Proceedings 2003 Human Language Technology Conference
North American Chapter Association Computational Linguistics, pp.
118125, Edmonton, Canada.
Shieber, S., & Schabes, Y. (1990). Synchronous tree-adjoining grammars. Proceedings 13th International Conference Computational Linguistics, pp. 253258,
Helsinki, Finland.
Stolcke, A. (2002). SRILM extensible language modeling toolkit. Proceedings
International Conference Spoken Language Processing, Denver, CO.
Tsochantaridis, I., Joachims, T., Hofmann, T., & Altun, Y. (2005). Large margin methods
structured interdependent output variables. Journal Machine Learning
Research, 6, 14531484.
Turner, J., & Charniak, E. (2005). Supervised unsupervised learning sentence
compression. Proceedings 43rd Annual Meeting Association Computational Linguistics, pp. 290297, Ann Arbor, MI.
Vandeghinste, V., & Pan, Y. (2004). Sentence compression automated subtitling:
hybrid approach. Text Summarization Branches Out: Proceedings ACL-04
Workshop, pp. 8995, Barcelona, Spain.
673

fiCohn & Lapata

Wu, D. (1997). Stochastic inversion transduction grammars bilingual parsing parallel
corpora. Computational Linguistics, 23 (3), 377404.
Yamada, K., & Knight, K. (2001). syntax-based statistical translation model. Proceedings 39th Annual Meeting Association Computational Linguistics,
pp. 523530, Toulouse, France.

674

fiJournal Artificial Intelligence Research 34 (2009) 133-164

Submitted 07/08; published 03/09

Generic Preferences Subsets Structured Objects
Maxim Binshtok
Ronen I. Brafman

MAXIMBI @ CS . BGU . AC . IL
BRAFMAN @ CS . BGU . AC . IL

Computer Science Department
Ben-Gurion University, Israel

Carmel Domshlak

DCARMEL @ IE . TECHNION . AC . IL

Faculty Industrial Engineering Management
Technion, Israel

Solomon E. Shimony

SHIMONY @ CS . BGU . AC . IL

Computer Science Department
Ben-Gurion University, Israel

Abstract
Various tasks decision making decision support systems require selecting preferred
subset given set items. focus problems individual items described
using set characterizing attributes, generic preference specification required, is,
specification work arbitrary set items. example, preferences
content online newspaper form: viewing, newspaper contains
subset set articles currently available. preference specification subset
provided offline, able use select subset currently available
set articles, e.g., based tags. present general approach lifting formalisms
specifying preferences objects multiple attributes ones specify preferences
subsets objects. also show compute optimal subset given
specification relatively efficient manner. provide empirical evaluation approach
well worst-case complexity results.

1. Introduction
Work reasoning preferences focuses mostly task recognizing preferred elements
within given set. However, another problem interest selecting optimal subset
elements. Optimal subset selection important problem many applications: choice
feature subsets machine learning, selection preferred bundle goods (as in, e.g., home
entertainment system), finding best set items display users screen, selecting best
set articles newspaper best members committee, etc.
Earlier work problem mostly focused question one construct
ordering subsets elements given ordering elements set (Barbera, Bossert,
& Pattanaik, 2004). main distinction made sets items mutually
exclusive, sense one eventually materialize, sets items
jointly materialize. formalism agnostic issue, although clearly motivated
latter case. Barbera et al. note, past work focused case mutually exclusive
elements. This, example, would case selecting set alternatives
decision-maker (or nature) ultimately choose one (e.g., courses action). However,

2009 AI Access Foundation. rights reserved.

fiB INSHTOK , B RAFMAN , OMSHLAK , & HIMONY

substantial body work latter setting items might materialize jointly,
individual items preferentially comparable.
paper focuses somewhat different context set-preference specification. First,
assume items subsets composed structured, sense set
attributes associated them. example, items movies, attributes could
genre, language, year, director; items politicians, attributes could political
views politicians various topics, party affiliation, level experience. Second,
require generic preference specification, sense used diverse collections
items. example, specifying guidelines composition committee,
guidelines generic, used induce preference relation subsets given set
politicians, provided set attributes fixed. Third, assume preferential
ordering individual items, although certainly captured one attributes
describing items.
instructive example type domain mind personalized online
newspapers. First, problem selection newspaper one subset selection
select subset set available articles place newspaper. Second, database
articles constantly changing. Therefore, approach requires explicitly specifying preferences inclusion specific item inappropriate, number
items large, would require us constantly change preference specification set items changes. Finally, would want base approach method
transforming ordering items ordering subsets items, want
rank item, obvious instances complementarity substitutability. instance, even prefer articles Britney Spears articles topic, two
similar articles may less interesting set comprising one one
Spice Girls.1
One recent work considers similar setting desJardins Wagstaff (2005),
works specifying preferences abstract properties sets. particular, desJardins
Wagstaff offer formalism preference specification users specify preferences
set values attribute attains within selected set items. One could assert
whether values attained attribute selected subset diverse concentrated
around specific value. addition, desJardins Wagstaff also suggest heuristic search
algorithm finding good, though necessarily optimal, sets items.
work, present general, two-tiered approach dealing set preferences
setting. approach combines language specifying certain types set properties,
arbitrary preference specification language expressing preferences single, attributed
items. basic idea first specify set properties care about, specify preferences
values properties. specification induces preference ordering sets
based values sets provide properties interest. believe suggested
approach intuitive powerful. Although paper focus particular set
properties devised relatively efficient optimization algorithm, general
form, two-tiered approach generalizes approach desJardins Wagstaff (2005)
diversity specificity two set properties. principle, one express general
1. realize common rules rationality may apply users preferences.

134

fiG ENERIC P REFERENCES UBSETS TRUCTURED BJECTS

properties referring multiple attributes, well general conditional preferences
values properties.
Essentially, approach re-states problem specifying preferences sets terms used
specify preferences single items. formulation, items stand possible sets,
attributes items (user-defined) set-property values. Thus, principle,
approach allows us re-use formalism specifying preferences single items. paper consider two specific instantiations formalism: qualitative preferences based
CP TCP-nets (Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004; Brafman, Domshlak, &
Shimony, 2006a), quantitative preferences represented generalized additively independent
(GAI) value functions (Bacchus & Grove, 1995; Fishburn, 1969). algorithm suggest
computing optimal subset given qualitative preferences based similar optimization algorithm TCP-nets. number items case large, algorithm
modified substantially exploit special structure items. modifications enable
us compute optimal subset faster.

2. Specifying Set Preferences
formalism use set-preference specification makes one fundamental assumption:
items sets interest built described terms attributes, values
attributes distinguishes different items. shall use denote set individual items, X denote set attributes describing items. example, imagine
items question US senate members, attributes values are: Party affiliation (Republican, Democrat), Views (liberal, conservative, ultra conservative), Experience
(experienced, inexperienced).
2.1 Properties Items Properties Item Sets
Given set X item-describing attributes, first, already talk complex item
properties, e.g., senate members liberal views, inexperienced, conservative senate members. formally, let X union attribute domains, is,
X = {X = x | X X , x Dom(X)} ,
let LX propositional language defined X usual logical operators. LX
provides us language describing complex properties individual items. Since items
viewed models LX , write |= whenever item satisfies
property LX .
Given language LX , specify arbitrary properties item sets based
attribute values items set, property least two Democrats,
Democrats Republicans. generally, given item property LX ,
talk number items set property , denote ||(S),
is, ||(S) = |{o S|o |= }|. Often set implicitly defined, simply write ||.
Thus, |Experience=experienced|(S) number experienced members S. Often, simply
abbreviate |experienced|.
||() integer-valued property sets, also specify boolean set properties
follows: h|| REL ki, LX , REL relational operator integers, k Z
135

fiB INSHTOK , B RAFMAN , OMSHLAK , & HIMONY

non-negative integer. property satisfied set |{o S|o |= }| REL k. running
example use following three set properties:
P1 : h|Party affiliation = Republican Political view = conservative| 2i
P2 : h|Experience = experienced| 2i
P3 : h|Political view = liberal| 1i
P1 satisfied (only) sets least two members either Republican conservative.
P2 satisfied sets least 2 experienced members. P3 satisfied sets least one
liberal.
also write h|| REL ||i, similar interpretation. example, h|Republican| >
|Democrat|i holds sets containing Republicans Democrats. even general
language could include arithmetic operators (e.g., require twice many Republicans Democrats)
aggregate functions (e.g., average number years job). instances
general notion specifying properties sets function attribute values sets
members. paper, focus language relational operators restricted
equalities inequalities. clear, concrete setting eases presentation,
restricting language allows us provide efficient subset-selection algorithms.
Indeed, many ideas present apply general languages. particular,
generality holds overall preference-specification methodology, search-overCSPs technique computing optimal subsets introduced later paper. However,
specific techniques use implement ideas, bounds generation, specific
translation properties CSPs, rely heavily use specific, restrictive languages.
Finally, note important property preference specification approach independent actual set items available moment. generality important many
applications reasoning set preferences must performed different,
often initially unknown sets items. example, case specifying guidelines
selecting articles online newspaper, selecting set k results information query.
2.2 Reasoning Set Preferences
specified set properties interest, define preferences values
properties using preference specification formalism. discuss two specific formalisms, namely TCP-nets (Brafman et al., 2006a), extension CP-nets (Boutilier et al., 2004),
Generalized Additively Independent (GAI)-value functions (Bacchus & Grove, 1995; Fishburn,
1969). former formalism purely qualitative preference specification, yielding partial
preference order objects interest. latter quantitative specification formalism
represent value function.
Let P = {P1 , . . . , Pk } collection set properties. TCP-net P captures statements following two types:
(1) Conditional Value Preference Statements. Pi1 = pi1 Pij = pij Pl = pl
preferred Pl = p0l . is, Pi1 , . . . , Pij certain value, prefer one value
Pl another value Pl .

136

fiG ENERIC P REFERENCES UBSETS TRUCTURED BJECTS

(2) Relative Importance Statements. Pi1 = pi1 Pij = pij Pl important
Pm . is, Pi1 , . . . , Pij certain value, prefer better value Pl even
compromise value Pm .
statement allows us compare certain pairs item sets follows:
- statement Pi1 = pi1 Pij = pij Pl = pl preferred Pl = p0l implies
given two sets S, 0 (1) Pi1 = pi1 Pij = pij holds, (2) satisfies
Pl = pl 0 satisfies Pl = p0l , (3) 0 identical values properties
except Pl , preferred 0 .
- statement Pi1 = pi1 Pij = pij Pl important Pm implies
given two sets S, 0 (1) Pi1 = pi1 Pij = pij holds, (2)
preferred value Pl , (3) 0 identical values attributes except Pl
Pm , preferred 0 . (Notice care value Pm Pl
improved.)
refer reader work Brafman et al. (2006a) details TCP-nets,
graphical structure, consistency, etc. algorithms paper, used TCP-nets,
assume acyclic TCP-net Brafman et al.. latter property ensures consistency
provided preferences, well existence certain good orderings P respect
TCP-net.
example, consider following preferences president forming committee.
prefers least two members either Republican conservative, is, prefers P1
P1 unconditionally. (Depending context, use P denote property P
value P = true. use P denote P = false.) P1 holds, prefers P2 P2 (that is, least
two experienced members), committee recommendations carry weight. P1 holds,
prefers P2 P2 (that is, one inexperienced) would easier influence
decision. president unconditionally prefers least one liberal, is, prefers
P3 P3 , give appearance balance. However, P3 less important P1
P2 . additional external constraint (or possibly preference) total number
members three.2
GAI value functions map elements interest (item sets case) real values quantifying theP
relative desirability elements. Structure-wise, GAI value functions form
U (S) = i=1,...,n Ui (Pi (S)), Pi P subset properties. example, Presidents preferences imply following GAI structure: U (S) = U1 (P1 (S), P2 (S)) + U2 (P3 (S))
Presidents conditional preferences P2 value tie P1 P2 together, independent P3 value. U1 would capture weight conditional preference, combined
absolute preference P1 value. U2 would represent value property P3 .
might quantify preferences follows: U1 (P1 , P2 ) = 10, U1 (P1 , P2 ) = 8, U1 (P1 , P2 ) = 2,
U1 (P1 , P2 ) = 5; U2 (P3 ) = 1, U2 (P3 ) = 0. course, infinitely many quantifications
possible.
2. external constraints, cardinality constraint, modeled preference high
value/importance. fact, model cardinality constraints implementation.

137

fiB INSHTOK , B RAFMAN , OMSHLAK , & HIMONY

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

Q {}
Sopt
Q contains set UB(S) > Value(Sopt )
argmaxS 0 Q UB(S 0 )
Q QS
\ {S 0 | LB(Sopt ) UB(S 0 )}
Q Q {S {o} | \ S}
argmaxS 0 Q Value(S 0 )
Value(S) > Value(Sopt )
Sopt
end
end
return Sopt
Figure 1: Subset-space branch-and-bound search optimal subset available items S.

3. Finding Optimal Subset
general, given preference specification set available items, goal find
optimal subset Sopt respect preference specification. is, set
0 S, properties Sopt satisfies less desirable properties 0 satisfies.
consider two classes algorithms finding optimal subset. two classes
algorithm differ space search. next section, describe comparative
empirical evaluation algorithms. running example use following set
available items S:
o1
o2
o3
o4

Republican
Republican
Democrat
Democrat

conservative
ultra conservative
conservative
liberal

inexperienced
experienced
experienced
experienced

3.1 Searching Sets Space
obvious approach generating optimal subset search directly space
subsets. priori approach attractive, indeed, shall see later implementation approach scale up. However, given often interested sets
small size heuristics used enhance search quality, thought worth exploring
approach.
branch-and-bound (B&B) algorithm space sets depicted Figure 1. set
S, algorithm assumes access upper bound UB(S) lower bound LB(S) estimates
maximal value superset S. algorithm maintains queue Q sets, queue
initialized contain empty set. step, algorithm selects highest upper-bound
set queue. Next, algorithm removes Q sets 0 upper bound UB(S 0 )
good lower bound LB(S) selected set S, adds Q minimal
(that is, one-item) extensions S. latter sets correspond successors search
space. Different implementations algorithm differ sort queue. best-first
version depicted pseudo-code sorts queue according heuristic value set,
138

fiG ENERIC P REFERENCES UBSETS TRUCTURED BJECTS

case heuristic upper bound value sets supersets. contrast, depthfirst version always positions children newly expanded node front queue.
implemented tested versions.
method used generate bounds set must depend actual preference representation formalism, well type set properties used, idea natural
given quantitative value function. lower bound LB(S) use actual value Value(S)
S. Note possible descendants lower values because, general,
set-properties may monotonic (e.g., average value higher 5.) However, since
possible solution, valid lower bound.
upper bound, proceed follows: First, consider set-property values
consistent S. is, set property, examine values supersets
potentially provide property. example, consider P2 suppose contains single
experienced member. currently, P2 holds. However, satisfy P2 add one
experienced member. Thus, values P2 consistent S. contrast, two
experienced members S, P2 inconsistent matter add S,
never satisfy P2 . Next, given sets possible set-properties values respect
set S, bound value supersets maximizing values locally.
Specifically, GAI value function, look local function Ui , consider
assignment it, among consistent values, would maximize Ui . Clearly, may result
overall value overestimation, since know whether locally optimizing joint
assignments consistent. Similar ideas used quantitative representations,
various soft-constraint formalisms (Bistarelli, Fargier, Montanari, Rossi, Schiex, & Verfaillie,
1999).
Consider running example GAI value function end Section 2,
consider searching optimal subset = {o1 , o2 , o3 , o4 } using depth-first version B&B.
start empty set, property values provided empty set P1 , P2 , P3 . Thus,
lower bound LB(), value empty-set, 5. upper bound UB(),
consider best property values individually consistent extensions ,
P1 , P2 , P3 , accumulative value 11. Sopt also initialized empty set,
next generate children (only possible) selected set , singleton
sets: {o1 }, {o2 }, {o3 }, {o4 }. Except {o4 }, lower upper bounds identical
empty set, inserted queue. {o4 } lower bound 6 upper
bound 11. Suppose {o1 } first queue element, select expansion. results
adding {o1 , o2 }, {o1 , o3 }, {o1 , o4 } queue, lower upper bounds sets
(8, 11), (8, 11), (6, 11), respectively. Next, set {o1 , o2 } examined respect current
Sopt = , Sopt assigned {o1 , o2 }. Since assumed depth-first version B&B
proceed expanding {o1 , o2 }, obtaining {o1 , o2 , o3 }, {o1 , o2 , o4 } lower upper bounds
being, respectively, (10, 11) (11, 11). lower bound 11 {o1 , o2 , o4 } prune
away rest nodes queue, done.
important issue depth-first B&B order sets generated. implementation, node search space, items ordered according sum
value properties help satisfy. example, initially, conservative member
o1 could help us satisfy P1 .
contrast quantitative preference representation formalisms, qualitative preferences typically induce partial ordering property collections. case, harder generate strict
139

fiB INSHTOK , B RAFMAN , OMSHLAK , & HIMONY

upper lower bounds must comparable possible solution. One way handle
linearize ordering require stronger property optimality respect
resulting total order. Here, TCP-nets present good choice efficient simple way generating value function consistent acyclic TCP-net (Brafman
& Domshlak, 2008). value function retains structure original network important make bounds computation efficient (notably, Ui depends small number
property values).
3.2 Searching CSPs
attractiveness item subsets evaluated terms fixed collection set-properties P,
thus different sets provide identical property values equivalent perspective.
immediate conclusion considering separately preferentially equivalent subsets
available items redundant. remove redundancy, suggest alternative method
search directly set-property value combinations. course, problem given
set-property value combination, obvious whether find actual subset
combination properties. answer question, generate CSP satisfiable
exists subset considered set-property values. overall search
procedure schematically works follows.
1. Systematically generate combinations set-property values.
2. combination, search subset providing combination setproperty values.
3. Output subset satisfying optimal (achievable) combination set-property values.
make approach efficient possible, two things, namely:
(1) Find way prune sub-optimal set-property value combinations early possible.
(2) Given set-property value combination, quickly determine whether subset satisfies
combination.
Considering first task, let P1 , . . . , Pk ordering set-properties P.3 Given
ordering P, incrementally generate tree property combinations. root tree
corresponds empty assignment P. node n corresponding partial assignment
P1 = p1 , . . . , Pj = pj , every possible value pj+1 property Pj+1 , tree contains
child n corresponding partial assignment P1 = p1 , . . . , Pj = pj , Pj+1 = pj+1 .
tree leaves correspond (all) complete assignments P. tree running example
depicted Figure 2. Note that, implicitly, node tree associated (possibly
empty) set subsets S, notably, subsets provide set-property value combination
associated node.
search optimal set, expand tree set-property value combinations
trying expand tree nodes possible pruning certain value combinations P either
3. Throughout paper, assume preference specifications using TCP nets, conditional
preference (CP) arcs, importance arcs, conditional importance (CI) arcs. scheme implementations allow arcs, CI arcs force ordering set properties dynamic, may depend value
assignments previous properties. clarity exposition, thus preferred present technical details.

140

fiG ENERIC P REFERENCES UBSETS TRUCTURED BJECTS



P1

P1

P1 P2 P3

P1 P2

P1 P2

P1 P2
P1 P2 P3

P1 P2 P3 P1 P2 P3

P1 P2 P3

P1 P2

P1 P2 P3 P1 P2 P3

P1 P2 P3

Figure 2: Illustration search tree running example.
sub-optimal respect set preferences, unsatisfiable respect S. standard way
is, again, using branch-and-bound search procedure, requires us
derive effective upper lower bounds value best subset satisfying partial value
combination P. addition, order associate properties values affects
pruning ability throughout search process. get leverage bounds,
would like explore children node decreasing order purported attractiveness.
Moreover, fixing ordering set-properties themselves, would like properties
potentially contribute appear earlier ordering. instance, P1 value
running example greater influence overall attractiveness subset value
P2 , thus P1 better branched first. addition, P1 preferred true, thus
subtree corresponding P1 = true better explored first. Similarly, P2 preferred
true P1 = true, preferred false, otherwise. ordering reflected tree
Figure 2, left right pre-order traversal tree.
Now, let us consider second task determining whether subset satisfies given setproperty value combination. Given partial assignment P, set following CSP.
First, CSP boolean variable xi every available item oi S. example, CSP
contains variables x1 , . . . , x4 items o1 , . . . , o4 respectively. Intuitively, xi = 1 encodes oi
part (searched for) subset S, whereas xi = 0 means oi subset.
Next, translate every set-property value certain constraint variables.
instance, [P1 ] = true, constraint C1 : x1 + x2 + x3 2 added CSP. Note
C1 explicitly encodes requirement (of P1 = true) subset least two
elements satisfy Republican conservative. {o1 , o2 , o3 } candidates
either Republican conservative. Alternately, [P1 ] = f alse, constraint
C1 : x1 + x2 + x3 < 2 added CSP. Finally, specify value P1 ,
constraints related P1 added all. Likewise, [P2 ] = true [P3 ] = true
would add constraints C2 : x2 + x3 + x4 2 C3 : x4 1, respectively. general,
hard verify CSP constructed way concrete item set set-property
value combination solvable subset satisfying . Moreover, CSP
solvable, solutions explicitly provides us subset S.
worth briefly pointing difference CSPs generate
typical CSPs usually discussed literature. work general CSPs deals constraints
141

fiB INSHTOK , B RAFMAN , OMSHLAK , & HIMONY

small, typically two-variable, subsets problem variables. contrast, constraints
CSPs generated optimization process global, constraint possibly defined CSP variables. Yet another special property CSPs constructed
purposes sense meaningful talk partial assignments
contextunassigned variables always regarded de facto assigned value 0 since
corresponding items, default, belong subset search for.
partial assignments set-properties P map CSPs, node tree setproperty value combinations maps CSP, entire tree viewed tree CSPs.
important property tree-of-CSPs children CSP node CSPs obtained
adding one additional constraint parent CSP, notably constraint corresponding
additional property value want set satisfy. implies CSP node
tree unsatisfiable, descendants unsatisfiable well. fact, make
stronger use nature search tree, recognizing reuse work done
parent node speed solution children. see latter, consider CSP C
tree-of-CSPs, child CSP C 0 C , let solution C . C 0 extends C
constraint C, subset 0 ruled C also ruled C 0 . Hence, solving
C C 0 considers subsets order (that is, using ordering set
elements), solving C 0 start leaf node corresponding S, solution generated
C . Moreover, constraint C represents boolean set property, solution
C 0 = C {C}, solution C {C}, sibling C 0 . Using
ideas, share work done different CSP nodes tree-of-CSPs. fact, set
properties boolean, approach needs backtrack property (we call
property limited backtracking), thereby considerably improving empirical performance
algorithm.
overall branch-and-bound algorithm space CSPs depicted Figure 3. is,
algorithm formulated case quantitative preference formalisms. formulation
algorithm qualitative case essentially same, minor technical differences
important computational property. CP/TCP-nets, guarantee limited
backtracking required follow following guidelines. First, must order variables
(line 1) order consistent topology network. Note TCP-nets, ordering
may conditional, is, order two variables may vary depending value
earlier variables. Second, line 2, property values must (possibly partially) ordered
best worst, given values parent properties (which must instantiated
earlier). case, first satisfiable set properties constitutes optimal choice (Brafman
et al., 2006a). Assuming solve intermediate nodes tree-of-CSPs, know
backtrack level assuming boolean set-properties, but, again, backtracks
may occur integer-valued properties.
node data structure used algorithm two attributes. search node n,
n. captures partial assignment set-properties P associated node n,
n.S captures subset satisfying n. exists, otherwise value false.
functions Value, LB, UB semantics subset-space search algorithm
Figure 1. pseudocode assume fixed ordering set-property values (line 2),
one vary depending earlier values (and exploit implementation). Finally,
142

fiG ENERIC P REFERENCES UBSETS TRUCTURED BJECTS

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:

Fix ordering set-properties P
Fix ordering values set property P P
Fix ordering available items
Q {n[; ]}
Sopt
Q empty
n pop(Q)
construct-and-solve-csp(n)
n.S 6= f alse UB(n.S) > Value(Sopt )
Value(n.S) > Value(Sopt )
Sopt n.S
end
Let P highest-ordered set property unassigned n.
possible value p P
n0 [n. {P = p}; n.S]
Q Q {n0 }
. position n0 Q depends search strategy
end
end
end
return Sopt
Figure 3: CSP-space branch-and-bound search optimal subset available items S.

pseudo-code leaves open choice search strategy used branch-and-bound,
choice fully captured queue insertion strategy line 16.
illustrate flow algorithm, let us consider running example. Recall
example already requirement discovered subset size 3, translates
constraint C : x1 + x2 + x3 + x4 = 3. first CSP consider {C, C1 }
constraints. Assume CSP variables ordered {x1 , x2 , x3 , x4 }, value 1 preceding value
0 xi . case, first solution find S1 : x1 = 1, x2 = 1, x3 = 1, x4 = 0.
next CSP adds constraint C2 . solving CSP, continue search (using
order xi values) current solution S1 , turns satisfy C2
well. Thus, virtually effort required solve CSP. Next, want also satisfy C3 .
set constraints corresponds leaf node tree-of-CSPs corresponds complete
assignment P1 P2 P3 set-properties. current item set Sopt = S1 liberal,
continue assignment S2 : x1 = 1, x2 = 1, x3 = 0, x4 = 1 (requiring us
backtrack CSP-solution space assignments x4 x3 ). set
satisfies properties leftmost leaf node tree-of-CSPs. prove setproperty value combination optimal using upper/lower bounds, done. Otherwise,
need explore additional nodes tree-of-CSPs. latter case, next CSP correspond
P1 , P2 , P3 , constraints {C, C1 , C2 , C3 }. However, already solution node,
exactly S1 . see that, note S1 solution parent current CSP,
solution sibling {C, C1 , C2 , C3 }. Hence, since P3 boolean property, S1 must
satisfy {C, C1 , C2 , C3 }.

143

fiB INSHTOK , B RAFMAN , OMSHLAK , & HIMONY

3.3 Solving underlying CSPs
algorithm solving intermediate CSPs based well known backtrack-search algorithm, first presented Prosser (1993) simple iterative form. time,
adapted algorithm well known enhancements CSP solving (such NoGood
recording forward checking (FC)) specifics CSPs setting.
Initially, variables values statically ordered least constrained
(although also discuss experiments performed dynamic variable/value ordering).
motivation static ordering two-fold. First, constraints much global,
ordering preprocessing stage. Second, discussed previous section, static
ordering allows us better utilize solutions CSPs solving descendent CSPs.
basic backtrack algorithm, own, unsurpisingly performs quite poorly
setting, refined utilizing following observations techniques.
Monotonicity improving constraints. operator constraint =
items constrained property already current partial solution,
one cannot satisfy constraint making additional assignments. property holds
constraint operators < . Using observation, possible detect
need backtrack early search.
Forward Checking. certain type forward checking performed constraints. Clearly, satisfying constraint requires least k items added
subset, number remaining items satisfy desired property less k,
search algorithm must backtrack.
Can/Must strategy. can/must strategy corresponds advanced check
interactions constraints. idea quite simple: (i) least p items must
added constructed subset satisfy constraint Ci , (ii) q items added
constructed subset without violating another constraint Cj , (iii) items
added property constrained Ci also property constrained Cj ,
and, finally, (iv) p > q, Ci Cj cannot satisfied simultaneously. Moreover,
assignments yet unassigned variables resolve conflict, thus situation
dead end. kind reasoning allows discovery barren nodes quite early
search, pruning large portions search tree. reason correctly can/must
strategy, maintain data structure unique items pair constraints,
well keep track number remaining items influence property constrained
Ci influence properties constrained Cj .
example, assume middle search two set properties:
SP1 : |A1 = a| 5 SP2 : |A2 = b| 3. Suppose already picked 3 items
influence SP1 2 items influence SP2 . result, satisfy SP1 , must add
least another two items influence satisfy SP2 add one item
influences SP2 . items choose {ok ...on } value
attribute A1 value b attribute A2 , obviously cannot satisfy SP1
SP2 within setting, thus backtrack.
Finally, discuss recording NoGoods, improvement basic backtracking algorithm
proved impact setting.
144

fiG ENERIC P REFERENCES UBSETS TRUCTURED BJECTS

3.3.1 N G OOD R ECORDING
standard definition NoGood CSP literature partial assignment cannot
extended full solution problem. learn NoGood, use prune
certain paths search tree. smaller NoGood, occasions use it,
greater pruning power. Thus, interest recognize minimal NoGoods, different
techniques developed perform NoGood resolution order produce best
general NoGoods possible (see, e.g., Dechter, 1990; Schiex & Verfaillie, 1993; Dago & Verfaillie,
1996).
noted earlier, CSPs generate differ significantly typical binary CSPs.
Consequently, NoGood recording algorithm adapted accordingly. particular, constraints global, makes sense try generating NoGoods global, too.
Thus, instead recording assignments variables, record influence current assignment constraints. Every variable influences set constraints.4 Thus, NoGood,
store influence set selected far constraints. Specifically, suppose
generated set S1 , recognized extensible set satisfying constraints.
(This immediately follows fact backtracked set.) generate NoGood N records property associated constraint, many items satisfying
property occur S1 . Now, suppose encounter different set S2 effect
N constraints. fewer options extend S2 extend S1 , know
S2 , well, cannot extended solution. However, options extend
S2 S1 , cannot conclude S2 NoGood point. order better quantify
options available extend S1 record, beyond actual NoGood N , level (depth)
assignment tree generated. Given CSP solver uses static variable
ordering, know encounter set generates properties NoGood
N , level higher S1 , safely prune extensions. reason is,
additional extension options available S1 .
correctness NoGood recording mechanism proposed depends static
variable ordering, well specific value ordering variables CSP, namely,
h1, 0i. show correctness, note NoGood used recorded.
Consequently, node using NoGood would right search tree node
NoGood recorded at. would like stress that, since constraints global,
matter items added subset, rather influence items
constraints. two sets exactly influence constraints identical
respect optimization process.
3.3.2 EARCH LGORITHM
procedure depicted Figure 4 extends basic backtrack algorithm subroutine C PROVE altered include combination in-depth checks discussed earlier,
utilize early conflict detection techniques, including NoGoods check. Also added call
DD N G OOD subroutine recording NoGoods backtracking. P n, generated
instance CSP problem variables indexed 1 |S| node tree-space search
4. assume without loss generality every item set available items influences least one constraint
constraint set C , since items influence constraint safely eliminated.

145

fiB INSHTOK , B RAFMAN , OMSHLAK , & HIMONY

respectively, inputs procedure. algorithm systematically tries assign values
problem variables, backtracking recording NoGoods facing dead end.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

consistent n.S satisfies n.
not(consistent)


H VALUES(P .vars[i]) C MPROVE(P )
P.i L ABEL(P.i, consistent) . current CSP variable available values, try set, update consistency
else


DD N G OOD (P ,i)
. Record NoGood


P.i U NLABEL(P.i)
. Backtrack
end
P.i = 0
. backtracked first indexed variable solution available
return false
end
end
return true
Figure 4: Conflict backtrack algorithm NoGood recording

4. Experimental Results
evaluate different algorithms using subset movie database publicly available
imdb.com. simulated scenario selecting movies three-day film festival according
organizers preferences. Three models growing complexity engineered reflect
preferences organizers; models defined terms 5, 9, 14 set-properties, respectively. addition, total number films constrained 5 (which actually modeled
using strong preference). Figure 5 depicts list P14 14 properties alterations; P5 P9 consist corresponding prefixes (SP1 SP5 , SP1 SP9 ,
respectively) P14 . produce even complex problem instances cause many backtracks
space set-property assignments slightly altered 14-properties model, creating two
0 P 00 .
additional models denoted henceforth P14
14
4.1 Preference Specification
Figure 6 provides verbal description qualitative preferences film festival program
used experiments. Figure 7 depicts TCP-net encodes preferences terms
concrete set-properties listed Figure 5. experiments GAI value functions,
preferences quantified compiling TCP-net GAI value function orders
items consistently TCP-net (Brafman & Domshlak, 2008). task empirical
evaluation find optimal subset set available movies {S400 , S1000 , S1600 , S3089 },
Si corresponds set movies, respect five models
preferences sets. experiments conducted using Pentium 3.4 GHz processor
2GB memory running Java 1.5 Windows XP Professional. runtimes reported tables
seconds, indicating process incompletion four hours.

146

fiG ENERIC P REFERENCES UBSETS TRUCTURED BJECTS

SP1 = h|Year 2002| = 5i
SP2 = h|Genre = Comedy| 2i
SP3 = h|Genre = Thriller| 3i
SP4 = h|Genre = Family| > 1i
SP5 = h|Color = B&W| > 1i
SP6 = h|Director = Spielberg| 1i
SP6 = h|Director = Spielberg| 1i
SP7 = h|Sound = Mono| 2i
SP8 = h|Genre = War Genre = Film-noir| = 0i

SP8 = h|Genre = War Genre = Film-noir| 4i
SP8 = h|Genre = Film-noir| 4i
SP9 = h|Location = North America| > 1i
SP10 = h|Actor = Famous Actress = Famous| = 5i
SP11 = h|Actress = Famous| 2i
SP12 = h|Genre = Drama| 2i
SP13 = h|Release Date < 1970| 1i
SP14 = h|Net Profit 1000000| 2i
SP14 = h|Net Profit 1000000| 5i

Figure 5: Set-properties used modeling user preferences movies selection domain.


0
Alteration P14 , achieve backtracking - denoted P14



0
00
alteration P14
achieve even backtracking - denoted P14

1. prefer new movies old movies, therefore prefer movies 2002 later, important
me.
2. love comedies, thrillers family movies.
3. prefer many movies black white (not one movie).
4. movies new (after 2002) would prefer least 2 comedies.
5. find least 2 comedies also prefer 1 family movie, less 3 thrillers.
However right number family movies important right number
thrillers.
6. movies new, prefer least 2 movies black white vintage touch.
7. movies new, prefer least one movie directed Steven Spielberg, otherwise, dont
like newer films
8. previous condition holds, number movies mono sound may greater 2.
9. prefer war films film-noir festival. However condition satisfied,
prefer films filmed North America important
preferences movie color B&W.
10. draw attention, prefer 5 movies famous actors actresses.
11. highlight female roles, prefer least 2 movies famous actress.
12. prefer least 2 dramas people tend think dramas sophisticated movies
genre.
13. prefer least one classical movie.
14. prefer least one commercially successful movie, i.e. movie whose net profit one
million dollars.

Figure 6: Informal description assumed preferences selecting set movies film
festival program.

First, initial experiments quickly showed search space subsets (Table 1)
scale up. 20 elements, converge optimal solution within
hour, even preference specification involved 5 set-properties. outcome holds
combinations qualitative quantitative preference specifications, depth-first best-first
schemes branch-and-bound, queue ordering based sets upper bound, lower bound,

147

fiB INSHTOK , B RAFMAN , OMSHLAK , & HIMONY

weighted combinations both. Table 1 provides snapshot corresponding results TCPnet specified nine set properties. table describes total number subsets generated
optimal subset found (see column Subset Sopt ), total number subsets
generated optimal subset recognized optimal (under Subsets generated). DFS appears much effective BFS, branching factor larger databases overwhelms
approach. Also, may thought larger databases easier quickly
generate good sets, found moderately larger (e.g,. 25+) much larger (e.g., 3000)
datasets, approach slow. Various improvements may possible, given much
better performance approach discussed later, unlikely make difference.

SP2 : SP4 SP4
SP2 : SP4 SP4

SP2 : SP3 SP3
SP2 : SP3 SP3

SP1 : SP2 SP2
SP1 : SP2 SP2

SP1 SP1

SP3

SP2

SP1

SP4

SP5

SP6

SP8

SP1 SP6
SP1 SP6
SP1 SP6
SP1 SP6

:
:
:
:

SP7
SP7
SP7
SP7

SP9

SP14

SP7 SP9
SP7 SP9
SP7 SP9
SP7 SP9

:
:
:
:

SP14
SP14
SP14
SP14

SP8 : SP9 SP9
SP8 : SP9 SP9

SP8 SP9
SP8 SP9
SP8 SP9
SP8 SP9

:
:
:
:

SP12
SP12
SP12
SP12

SP12
SP12
SP12
SP12

SP12

SP7
SP7
SP7
SP7

SP1 : SP6 SP6
SP1 : SP6 SP6

SP1 : SP5 SP5
SP1 : SP5 SP5

SP8 SP8

SP7

SP13

SP11

SP9 : SP13 SP13
SP9 : SP13 SP13

SP10 : SP11 SP11
SP10 : SP11 SP11

SP14
SP14
SP14
SP14

SP10
SP14 : SP10 SP10
SP14 : SP10 SP10

Figure 7: TCP-net model preference sets movies film festival program.
Next, consider CSP-space branch-and-bound search. particular, compared
two variants approach use dynamic static variable value orderings.
follows, two variants denoted BB-D BB-S, respectively. static
variable/value orderings usually considered weaker approach CSP solving, earlier
shown that, domain, static ordering allows certain optimizations
potential improve efficiency overall problem solving. particular, static variable
ordering allows record global NoGoods described Section 3.3.1; results algorithms
record NoGoods denoted name suffix +ng. addition, tried share

148

fiG ENERIC P REFERENCES UBSETS TRUCTURED BJECTS

S8
S8
S10
S10
S15
S15
S20
S20

Method
BFS
DFS
BFS
DFS
BFS
DFS
BFS
DFS

Subsets Sopt
18
83
40
672
7879
11434
28407
28407

Subsets generated
4075
630
15048
2935
104504
30547
486079
231616

Time (sec)
0.56
0.19
2.34
0.47
68.23
3.13
1584.67
28.578

Table 1: snapshot results subsets-space search. preferences specified
TCP-net nine set properties.
Method

S400

S1000

S1600

S3089

P5
P5
P5
P5
P5

BB-D
BB-S
BB-S+inc
BB-S+ng
BB-S+ng+inc

0.3
0.14
0.05
0.17
0.05

0.77
0.14
0.1
0.1
0.11

1.30
0.17
0.12
0.15
0.13

4.02
0.25
0.18
0.21
0.19

P9
P9
P9
P9
P9

BB-D
BB-S
BB-S+inc
BB-S+ng
BB-S+ng+inc

0.43
0.14
0.06
0.17
0.06

1.42
0.24
0.14
0.25
0.14

2.42
0.26
0.17
0.34
0.18

6.58
0.34
0.15
0.35
0.17

P14
P14
P14
P14
P14

BB-D
BB-S
BB-S+inc
BB-S+ng
BB-S+ng+inc

0.66
0.17
0.06
0.3
0.1

2.03
0.43
0.15
0.57
0.19

4.69
1.09
0.43
1.06
0.38

14.92
0.78
0.5
0.95
0.54

0
P14
0
P14
0
P14
0
P14

BB-S
BB-S+inc
BB-S+ng
BB-S+ng+inc

6.5
2.1
16.1
4.68

27.1
27
19.4
18.4

278
259
54.8
76.3



230.2
210.8

00
P14
00
P14
00
P14
00
P14
00
P14

BB-D
BB-S
BB-S+inc
BB-S+ng
BB-S+ng+inc

4113.48
101.4
81.03
110
107.9


5370
5523
269.9
266.8


16306
16643
646.1
646.8




3335
3013

Set-properties

Table 2: Empirical results evaluating CSP-space search procedures qualitative preference specification using TCP-nets.

information consecutive CSP problem instances search tree CSPs;
algorithms adopting technique denoted name suffix +inc.
Table 2 depicts results evaluation variants CSP-space branch-and-bound
search algorithm (Figure 3). First, table shows overhead maintaining NoGoods
pay simple preference specifications. However, complex problems requiring intense CSP solving, use NoGood recording proved useful, letting us

149

fiB INSHTOK , B RAFMAN , OMSHLAK , & HIMONY

Method

S400

S1000

S1600

S3089

P5
P9

BB-S
BB-S

0.24
0.16

0.14
0.25

0.17
0.28

0.26
0.41

P14
P14

BB-S+inc
BB-S+ng+inc

38.91
19

2376.40
160.53


494.98


1349.9

Set-properties

Table 3: Results CSP-space search quantitative preference specification using GAI
value functions.

solve previously unsolvable instances. Next, reader may notice table that, least
problems used tests, contribution incremental approach substantial.
instance, NoGood recording seems contribute much efficiency optimization process. Moreover, complex problems, switching incremental version
sometimes even leads performance degradation. appears overhead maintaining
copying partial solution cases pay off.
next set experiments mirrored first one, GAI value functions instead
purely qualitative TCP-nets. GAI functions obtained properly quantifying
qualitative preferences used first tests. Table 3 provides representative snapshots
results. value functions set-properties P5 P9 basic branch-and-bound algorithm
static variable/value orderings performs scales (with growing set alternatives S) quite
well. complex value functions larger set properties P14 performance
significantly degrades, even incrementality-enhanced algorithm cannot solve problem instances 1000 CSP variables. hand, adding NoGoods recording proves
dramatically improve performance, leading solving even largest problem instances.
Tables 2 3 suggest qualitative difference performance CSP-space search
quantitative qualitative preference representation models. good reasons expect
behavior. First, compact qualitative models preference may (and typically do) admit
one optimal (that is, non-dominated) solution. That, principle, makes finding one
optimal solution easier. Second, preferences captured TCP-net, variable
orderings ensuring first solution found optimal one. contrast, GAI value
functions, generate optimal solution, typically still explore search tree
prove better solution exists. worst case, explore entire tree CSPs,
forcing us explore number CSPs exponential |P|.
summary, first conclusion taken experiments subsets-space search
fails escape trap large branching factor, stratified procedures CSP-space
search show much higher potential. problems require little backtracking space
CSPs, latter procedures actually effective TCP-net GAI function preference
specification. Obviously, procedure forced explore many different CSPs, performance
unavoidably degrades. note that, larger databases, backtracks often indicate inherent
conflict desirable set-properties, conflicts might possibly recognized resolved off-line. work investigate issue, leaving optional direction
future improvement.
rather non-trivial example used section provides reader also opportunity
assess suitability different preference specification languages. example, although

150

fiG ENERIC P REFERENCES UBSETS TRUCTURED BJECTS

used boolean-valued set properties, may argued natural-language preference
statements would better expressed using integer-valued set properties. Similarly, users may find
preference specification formalism, soft-constraints (Bistarelli et al., 1999),
naturally capture natural language preferences. opportunity us
reemphasize while, obvious reasons, focus concrete choice language,
believe two-tiered approach suggested far general.

5. Complexity Analysis
Though reasonable runtimes obtained us empirically search CSPs,
algorithm classes described worst-case exponential running time. begs question whether problem computationally hard. Obviously, external constraints,
subset optimization NP-hard. show even without external constraints, problem
typically remains NP-hard, even significant restrictions problem.
Naturally, complexity subset selection depends precise nature preference
specification formalism used. results presented assume TCP-net-based specification. Hardness results model immediately apply GAI model, based existing
reduction (Brafman & Domshlak, 2008). cases, problems tractable TCPnet model become NP-hard GAI model used, instead. Thus, unless stated otherwise,
assume henceforth preferences properties specified TCP-net.
analyzing complexity problem consider following problem parameters:
n, overall number items data set.
a, number attributes items.
m, number set properties, i.e. number nodes TCP-net.
k, maximal property formula size, defined number logical connectives (and, or, not)
formula.
maximum attribute domain size, i.e. maximum number distinct values
attribute.
, number times attribute value appear dataset.
5.1 NP-Hard Classes
Theorem 1. using TCP-based preferences set properties, finding optimal subset
given set items (POS) NP-hard even items described terms binary-valued
attributes, set properties atomic (that is, = 2 k = 0).
Proof. proof polynomial reduction well-known NP-hard Vertex Cover (VC)
problem. Given graph G = (V, E), vertex cover G vertex subset V 0 V covering
edges graph, is, every edge e E, vertex v V 0 e incident
v. optimization version VC corresponds finding minimal size vertex cover G.
Given VC problem instance G = (V, E), construct POS problem instance specifying TCP-net N item set follows. vertex v V create item (denoted
151

fiB INSHTOK , B RAFMAN , OMSHLAK , & HIMONY

Pe1 Pe1

Pe2 Pe2

Pe3 Pe3

Pe1

Pe2

Pe3

Pek Pek
...

Pek

SUM
SUM = 0 SUM=1 SUM=2 . . . SUM=n

Figure 8: TCP-net reduction VC POS proof Theorem 1.
ov ), thus |S| = |V | = n items. edge e E define attribute X
(denoted Xe ), thus |X | = |E| = attributes. attributes X defined
binary, {0, 1}, domain. item ov , value attribute Xe ov [Xe ] = 1
e incident v G. Next, edge e E, define binary set property
Pe = h|Xe | > 0i takes value true least one item selected subset provides value 1 attribute Xe . addition, define single multi-valued empty set property
SUM h||i5 . domain SUM property defined integer-value range [0..n].
Note that, construction, properties utilize one attribute per property, thus logical
connectives, providing us k = 0. preferences set properties
1. binary property Pe , preference value true, is, Pe Pe .
2. empty property SUM simply prefer smaller values,
(SUM = 0) (SUM=1) (SUM=2) . . . (SUM=n)
edges TCP-net N , depicted Figure 8, importance arcs Pe
SUM, meaning would rather temporize value SUM property
Pe f alse.
Proposition 1 ensures optimal subset POS problem constructed always
corresponds proper vertex cover G.
Proposition 1. subset undominated respect constructed TCP-net
N , every edge e E, Pe (S) = true.
Proof. Given undominated (with respect N ) subset S, let Pe set property
Pe (S) = f alse. construction, exists item o[Xe ] = 1. Considering
0 = {o}, 0 preferred respect N (i) 0 provide
exactly values set properties except Pe SUM, (ii) provides preferred
5. Since formula inside set property degenerate, fact equivalent h|true|i, every item selection
set comply it. set property simplest implementation counter

152

fiG ENERIC P REFERENCES UBSETS TRUCTURED BJECTS

value SUM 0 provides preferred value Pe , (iii) preferential improvement Pe
dominates SUM. Thus 0 dominates S, contradicting assumption undominated.
Lemma 1. subset undominated respect constructed TCP-net N ,
exists vertex cover VS G |VS | = |S|.
Proof. proof straightforward. Let VS = {v | ov S}. undominated
respect N , Proposition 1 Pe (S) = true binary edge-related properties
Pe . turn, Pe (S) = true implies o[Xe ] = 1 least one item S. construction,
o[Xe ] = 1 vertex v covers edge e. Together mapping vertices V
items bijective, latter implies |VS | = |S|.
Lemma 2. exists minimal vertex cover G size exists subset
undominated respect N SUM(S) = s.
Proof. Let undominated subset |S| = s. construction, Pe (S) = true
binary set properties Pe , SUM(S 0 ) = s. Lemma 1, exists vertex cover VS
G |VS | = s. Suppose contrary VS minimal, is, exists vertex
cover V 0 G |V 0 | < s. Now, construct subset 0 = {ov | v V 0 }. Since mapping
V bijective, |S 0 | = |V 0 | < s, thus SUM(S 0 ) < s. Likewise,
construction set properties V 0 vertex cover, Pe (S) = true Pe .
This, however, implies 0 preferred respect N , contradicting statement
undominated.
Theorem 1 follows immediately Lemma 2 fact reduction clearly
polynomial.
Theorem 2. Given TCP-based preferences set properties, finding optimal subset given
set items (POS) NP-hard even items described terms single attribute,
set properties binary-valued, containing 2 logical connectives (that is,
= 1 k = 2).
Proof. proof polynomial reduction k-SAT, k 3. Given k-SAT problem
instance propositional variables V logical formula , construct POS problem instance specifying TCP-net N item set follows. variable v V , construct
item ov item ov , thus contains item every possible literal formula.
value attribute X defined follows: item ol , A(ol ) = l (where l
literal, either v v, v V ). binary set properties P TCP-net N defined
follows.
Properties ensuring variable assignment legitimate. variable v V ,

Pv =h|X = v X = v| = 1i,
is, S, Pv (S) = true contains exactly one items
{ov , ov }.
153

fiB INSHTOK , B RAFMAN , OMSHLAK , & HIMONY

Properties ensuring satisfied. clause C = (l1 l2 l3 ...) :
PC =h|X = l1 X = l2 X = l3 ...| 1i
is, S, PC (S) = true contains least one item corresponding
literal C.
Finally, complete preference specification, make properties independent (that is,
TCP-net edges), properties prefer value true value false.
illustrate construction, consider 3-SAT formula = (x z) (y) (x z).
formula, construction leads
item
ox
ox
oy
oy
oz
oz

X
x
x


z
z

Set properties:
Px =h|X = x X = x| = 1i
Py =h|X = X = y| = 1i
Pz =h|X = z X = z| = 1i
PC1 =h|X = x X = X = z| 1i
PC2 =h|X = y| 1i
PC3 =h|X = x X = z| 1i

show finding undominated subset respect N equivalent
finding satisfying assignment . Let undominated respect N . show
provides value true set propositions Pv PC (in case call ultimately
preferred subset) satisfiable.
First, let ultimately preferred subset S. Given S, construct mapping
: V 7 {true, f alse} A(v) = true ov S, A(v) = f alse ov S. Note
well-defined because, ultimately preferred subset S, Pv (S) = true, thus,
v V , exactly one item {ov , ov } present S. Clearly, legal assignment
. addition, PC (S) = true. Thus, clause C , least one item
X = li C belongs S. construction, implies satisfies clauses ,
thus satisfiable.
Converesly, suppose preferentially undominated respect N ,
ultimately preferred. POS problem undominated subset S, show
unsatisfiable. Assuming contrary, let satisfying assignment . Given A, construct
subset SA SA = {ol | literal l A}, show SA dominates respect N
(contradicting assumed undominance S, finalizing proof Theorem 2).
construction, since legal assignment V , Pv (SA ) = true set properties Pv . Also, since satisfying assignment , PC (SA ) = true set
properties PC . Therefore, SA actually ultimately preferred subset S. Finally, since
set properties P preferentially independent N , value true always preferred value
f alse set properties, SA dominates respect N .
Notice Theorems 1 2 subsume other. Theorem 1 poses restriction
number item attributes problem instance, restrict domain attributes.
Theorem 2 restricts number attributes 1, restriction domain size
attribute, restriction property size looser imposed Theorem 1.
154

fiG ENERIC P REFERENCES UBSETS TRUCTURED BJECTS

Finally, note tightening condition Theorem 2, allowing 1 connective set-property definition prevents us using reduction proof
Theorem 2 respectibe satisfiability problems would polynomial-time solvable
2-SAT problems. conjecture, however, fragment POS still NP-hard. fact,
Section 5.3 show corresponding fragment POS GAI preference specification
(instead TCP-nets) indeed NP-hard.
5.2 Tractable Classes
Several tractable classes POS, obtained restricting problem class discussed
Theorem 2, characterized single-attribute item description (that is, = 1), discussed
below. trivially tractable (Section 5.2.1) non-trivially tractable (Section 5.2.2) cases,
assume relational symbols either equalities inequalities, specification
property equalities (attribute = value) used, addition allow empty
set property specified. latter restriction due fact empty set property
somewhat special, enriches descriptive power allowing one simulate additional
attribute certain cases, single-attribute restriction crucial tractability result.
proceed actual results, note that, single-attribute item description,
two set properties conflict demands backtracking choosing items (i.e.
CSP solution). illustrate conflicts, consider following examples.
1.

1.a h|A = ai | 5i
1.b h|A = ai | 3i

Set property 1.a redundant, subsumed 1.b

2.

2.a h|A = | = 5i
2.b h|A = | > 6i

One set properties must false.

3.

3.a h|A = al | < 7i
3.b h|A = al | 9i

One set properties must false.

conflicts set-properties resolved offline, prior actual process subset
selection, totally disregarding available items. Hence, within process subset selection,
assume conflicts set properties. Consequently, subset selection
done greedy manner.
5.2.1 RIVIALLY RACTABLE C LASS
Theorem 3. Finding optimal subset given set items (POS) respect TCP-net
preference specification P items described terms single attribute,
set properties atomic (that is, = 1 k = 0).
algorithm problem class Theorem 3 depicted Figure 9. algorithm runs
time O(m2 n), number set properties n number available items S.
loop line 4 algorithm iterates set properties, time checking compatibility previously considered properties, requires (m2 ) time. procedures
G ET ATISFYING ET () H ATISFYING ET () process item once.
Hence, total running time algorithm O(m2 n).6
6. runtime analysis include ordering TCP-net variables assumed given. One way
would topological sort net, obviously done polynomial time.

155

fiB INSHTOK , B RAFMAN , OMSHLAK , & HIMONY

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

Sopt
Fix preference ordering set properties P
Pass
property P P
(not (P .isSatisfied))
P conflict Pass
Set next value P w.r.t. Pass
else
H ATISFYING ET(P )
Sopt Sopt G ET ATISFYING ET(P )
P .isSatisfied true
Pass Pass {P }
end
end
end
end
return Sopt
procedure G ET ATISFYING ET(P )

item
property value defined P
{o}
end
|S| P .op P .cardinality
return
end
end
end procedure

. Offline conflict resolution

. cardinality satisfies P

Figure 9: polynomial-time algorithm POS problems TCP-net preference specification, single-attribute item description, set properties atomic (that is,
= 1 k = 0).

5.2.2 N -T RIVIALLY RACTABLE C LASS
end Section 5.1 mentioned complexity POS limiting setproperty description one logical connective still open problem. If, however,
impose limitations summarized Table 4, show problem becomes tractable.
Theorem 4. Finding optimal subset given set items (POS) respect TCP-net
preference specification P restricted Table 4.
First discuss implicit limitations (or special problem properties) imposed
explicit limitations listed Table 4.
156

fiG ENERIC P REFERENCES UBSETS TRUCTURED BJECTS

1. items one attribute (a = 1)
2. property formulas 1 connective (k = 1), positive (that is,
disallow negation)
3. empty property disallowed
4. number attribute value appearances limited = 1 (that is, values
attribute domain cannot repeated)
Table 4: Characteristics tractable subclass POS presented Section 5.2.2.
1. restriction one attribute-value appearance data set provides one-toone correspondence attribute values items S. means item
uniquely represent specific attribute-value combination, vice versa.
2. restriction single-attribute item description renders connective redundant.
properties using logical connective form:
X = xi X = xj .
(Without loss generality assume 6= j, otherwise simply drop one
terms.) properties obviously cannot satisfied item two different
values attribute X. fact, set properties defined way equivalent
property always f alse.
3. relevant cardinalities set properties [0..2]. property defined using
one connective restriction number repetitions expressive enough
state set property involving 2 items. value set property:
h|A = ai = aj |

op

valuei

greater 2, op {, >}, cannot satisfied. op property
<, value greater 2, substituted effectively equivalent
set property op value = 2 .
algorithm problem class Theorem 4 depicted Figure 10. algorithm bears
similarity algorithm Figure 9, except procedures G ET ATISFYING ET
H ATISFYING ET reason simultaneously satisfaction collections set-property
values, utilizing 2-SAT solving. Specifically, Table 5 show valid
property POS problem translated 2-SAT CNF formula. Lemma 3
prove correctness translation. note using 2-SAT
answer question subset items satisfying already evaluated set-property
values. procedures G ET ATISFYING ET H ATISFYING ET use aforementioned
reduction 2-SAT provide answer polynomial time.
Lemma 3. subset satisfying property-values Pass
satisfying assignment 2-SAT formula constructed Pass .
157

fiB INSHTOK , B RAFMAN , OMSHLAK , & HIMONY

h|X
h|X
h|X
h|X
h|X
h|X

= xi | > 2i infeasible
= xi | 2i infeasible
= xi | 1i substituted
= xi | = 1i translated (vi ) clause
= xi | 0i translated (vi vi ) clause
= xi | = 0i translated (vi ) clause
Properties 0 logical connectives

h|X = xi X = xj | > 2i infeasible
h|X = xi X = xj | 2i substituted
h|X = xi X = xj | = 2i translated (vi )
(vj ) clauses
h|X = xi X = xj | 1i translated
(vi vj ) clause
h|X = xi X = xj | = 1i translated
(vi vj ) (vi vj ) clauses
h|X = xi X = xj | 0i translated
(vi vj ) clause
h|X = xi X = xj | = 0i translated (vi )
(vj ) clauses
Properties 1 logical connective

Table 5: Translation set properties POS subclass Section 5.2.2 2-SAT.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

Fix preference ordering set properties P
Sopt
Pass
property P P
(not (P .isSatisfied))
Set next value P w.r.t. Pass
H ATISFYING ET(Pass )
Sopt G ET ATISFYING ET(Pass )
P .isSatisfied true
Pass Pass {P }
end
end
end
return Sopt

. Use reduction 2-SAT
. Use reduction 2-SAT

Figure 10: poly-time algorithm POS problems TCP-net preference specification,
characteristics Table 4.

Proof. construction, injective correspondence properties POS
problem clauses 2-SAT problem. Every property P P injectively corresponds
certain clause P . Every item injectively corresponds propositional variable vi V .
Thus, correspondence selected subset assignment simply
vi = true S.

158

(1)

fiG ENERIC P REFERENCES UBSETS TRUCTURED BJECTS

translation injective rather straightforward (without introducing auxiliary
clauses properties), trivial subset satisfies properties Pass
assignment satisfies clauses corresponding 2-SAT formula.
shows correctness algorithm Figure 10, finalizes proof Theorem 4.
5.3 Complexity POS: TCP-nets vs. GAI Preference Specification
restrictions Table 4 able show POS problem TCP-net
preference specification tractable reduction 2-SAT, need backtrack
searching attribute value space. interesting question is, specification
done using GAI functions?
Theorem 5. Finding optimal subset given set items (POS) respect GAI preference specification NP-hard even items described terms single attribute, set
properties binary-valued, containing 1 logical connective (that is, = 1
k = 1).
Proof. proof polynomial reduction MAX-2SAT. far item definitions
properties concerned, reduction essentially reduction k-SAT
proof Theorem 2. is, variable v V , construct item ov item ov .
value attribute X defined follows: item ol , A(ol ) = l (where l
literal, either v v, v V ). Set properties also proof Theorem 2,
limited 2 variables per clauses (re-stated convenience below):
variable v V :

Pv =h|X = v X = v| = 1i,
is, properties ensuring variable assignment legitimate.
clause C = (l1 l2 ) :

PC =h|X = l1 X = l2 | 1i,
is, properties ensuring satisfied.
value function specification legitimate variable assignments enforced,
larger number clauses satisfied preferred. achieved using additively independent
value function (i.e., factor contains single variable), values follows.
clause-satisfying property value 1 true, 0 f alse. literalsatisfying property value 0 true, negative value 2m f alse,
number clauses.
Lemma 4. Given GAI value function item set constructed 2-CNF formula
, exists subset value U (S) = p exists assignment
satisfying p clauses .
159

fiB INSHTOK , B RAFMAN , OMSHLAK , & HIMONY

Proof. Let subset non-negative value. implies construction (since
clause-satisfying properties PC ) literal-satisfying properties must true
S, respective assignment constructed Equation 1. Conversely, let
legitimate assignment variables V . One define corresponding set SA ,
(by construction) properties Pv true. Also, observe construction number PC
set properties true SA number clauses satisfied assignment
A.
theorem follows immediately properties construction set properties
preferences.
end Section 5.1 noted restrictions problem parameters
severe Theorem 2, limiting number logical connectives per set property
1, longer show whether problem tractable NP-hard TCPnet preference specification. However, Theorem 5 shows that, preferences specified using
GAI value function, problem fact NP-hard. Moreover, problem class Theorem
5 subsumes class Theorem 4, thus provides additional result showing even
though TCP-net specification respective problem tractable, GAI preference
specification becomes NP-hard.

6. Related Work
introduction, mentioned closely related work desJardins Wagstaff (2005).
approach, motivation provide user diverse collection values either
reflect set possible choices better applications user must eventually select
single item, diversity selected set objective own. work Price
Messinger (2005) explicitly concerned problem. Specifically, consider
problem recommending items user, view type subset selection problem.
example, suppose want recommend digital camera user. large set available
cameras, able recommend k cameras. Price Messinger consider question
select set, proposing candidate set maximize expected value
users choice set. suggest concrete algorithmic approach handling problem.
input problem form partial representation users preferences (which
diverse, work) naturally, concrete techniques different ours.
papers share assumption ranking sets, common previous work discussed
Barbera et al. (2004), ultimately one item selected set. However,
necessarily start initial ranking single items, case, work
desJardins Wagstaff utilizes attribute value items selection process.
Earlier work ranking subsets motivated problems college admissions
problem (Gale & Shapley, 1962), need select best set fixed cardinality among
pool college candidates. admissions officer various criteria good class students
wishes come optimal choice. key questions concerned line
work good properties set rankings whether simple
representation. example property set ranking may desirable following:
given set S, replace member c member c0 obtain set 0 ,
c0 preferred c, 0 preferred S. example representation ranking
160

fiG ENERIC P REFERENCES UBSETS TRUCTURED BJECTS

additive representation items associated real values one set preferred
another sum elements values larger. would interesting study similar question
context structured objects.
question ranking sets appears areas, logics preference likelihood.
example, main question considered Halpern (1997) construct ordering
formulas based ordering truth assignments. Formulas associated set
worlds satisfied, hence, question comparing likelihood formulas
corresponds ranking respective set models given initial ranking
single models. Much work non-monotonic logics uses Shohams preference semantics (Shoham,
1987), semantically, work (see, e.g., Kraus, Lehmann, & Magidor, 1990) viewed
attempting answer opposite question define ranking single truth assignments given
some, possibly partial, ordering formulas, i.e., sets models.
number lines work related specification solution methods. first
work Russian Doll Search (RDS), well known algorithm combinatorial optimization,
originally presented Verfaillie, Lematre, Schiex (1996) efficient algorithm Constraint Optimization Problems (COP). idea behind approach solve consecutively harder
problems. Initially, problem solved considering one variable. optimal result
provides lower bound. iteration, additional variables considered, eventually original problem solved. using lower bound obtained previous iteration (and
optimizations) technique often able solve original problem efficiently. Recently
Rollon Larrosa (2007) extended Russian Doll Search support multi-objective optimization
problems. multi-objective optimization problem goal optimize several parameters
(attributes) variables problem. Usually parameters cannot simultaneously
optimized. technique Rollon Larrosa involves incremental solution
objectives included, and, sense, related search CSPs approach
incrementally consider set properties. Indeed, different desirable set properties
viewed different objectives.
Another related area Pseudo-Boolean Constraint (PBC) Satisfaction Problems (Sheini
& Sakallah, 2005). PBC form:
X
wi li k.


li literals interpret values either 0 (false) 1 (true); wi
real-valued coefficients; k integer. Thus Pseudo-Boolean CSPs special form integer programs, nicely represent cardinality constraints generate. Thus, one option
solving type CSPs generated would using dedicated PBC solver. run several popular PBC solvers satisfiability instances generated optimization: Pueblo (Sheini &
Sakallah, 2005), MiniSat (Een & Sorensson, 2005), Galena (Dixon & Ginsberg, 2002).
solvers showed comparable results satisfiable cases, unsatisfiable cases, PBC
solvers showed better performance. appears due use linear programming
preliminary test satisfiability.
Another line work bears important connection winner determination
combinatorial auctions. regular auctions, bidders bid single item. combinatorial auctions, bidders bid bundles items. Thus, bidders must provide preferences different
subsets set auctioned items. goal combinatorial auctions allocate set
161

fiB INSHTOK , B RAFMAN , OMSHLAK , & HIMONY

goods different bidders best manner (e.g., maximizing payment seller maximizing total welfare). differs problem selecting single optimal subset
concerned. However, cases, preferences subsets must provided optimization algorithm. number subsets exponential number items, researchers
combinatorial auctions sought bidding languages succinctly describe preferences
interest (Boutilier & Hoos, 2001; Nisan, 2006). distinguishes specification approach
reliance existence item features desire provide generic specification
depend concrete set items. Work combinatorial auctions also attempts break
specification way. typically done specifying values small bundles
providing rules deriving value larger sets values smaller sets.

7. Conclusion
suggested simple, yet general approach lifting attribute-based preference specification
formalism one specifying preferences sets. focused one instantiation idea
via concrete language specifying set properties, suggested two methods computing
optimal subset given specification. One method based searching space explicit
subsets, searches implicit subsets represented CSPs. search spaces
meaningful regardless specific underlying preference specification algorithm although
precise search bounds generation method vary. focused two concrete popular
specification formalisms, one qualitative one quantitative, experiment provide complexity results. Although problem generally NP-hard, expected, experimental
results quite encouraging.
wish reemphasize choices, set property language preference specification formalism possible, may appropriate various cases. Indeed,
interesting topic future research would see choices fit best natural application areas; whether algorithm presented paper modified handle
languages; complexity optimal subset selection problem affected
choices.
Though incremental search CSPs appears better method optimal subset selection, leaves questions open. First, interesting question whether efficient NoGood
recording scheme rely static variable value orderings exists. Intuitively,
scheme exist since CSPs generated efficiently encoded SAT boolean
CNF formula (Bailleux & Boufkhad, 2004; Een & Sorensson, 2005), clause learning well
known technique SAT solving. Second, seen incremental approach usually
improves overall performance, contribution substantial really improves
performance better individual CSP solving. begs two questions: (1) better utilize solutions across CSPs, (2) Would representing solving CSPs generated pseudo-boolean
CSPs (Manquinho & Roussel, 2006) SAT instances lead faster solution times? Naturally,
alternative approaches also feasible.
Finally, various applications, set elements gradually changes, need adapt
selected subset changes. example use approach choose
interesting current articles, new articles constantly appear. likely case
preferred set similar current set, would like formulate incremental approach
adapts changes quickly.

162

fiG ENERIC P REFERENCES UBSETS TRUCTURED BJECTS

Acknowledgments
Preliminary versions work appeared (Brafman, Domshlak, Shimony, & Silver, 2006b;
Binshtok, Brafman, Shimony, Mani, & Boutilier, 2007). authors wish thank anonymous
reviewers useful comments suggestions. Brafman supported part NSF grant
IIS-0534662, Brafman Domshlak supported COST action IC0602, Binshtok, Brafman Shimony supported Deutsche Telekom Laboratories Ben-Gurion University,
Paul Ivanier Center Robotics Research Production Management, Lynn
William Frankel Center Computer Science.

References
Bacchus, F., & Grove, A. (1995). Graphical models preference utility. Proceedings
11th Annual Conference Uncertainty Artificial Intelligence (UAI), pp. 310, San
Francisco, CA.
Bailleux, O., & Boufkhad, Y. (2004). Full CNF encoding: counting constraints case. 7th
International Conference Theory Applications Satisfiability Testing (SAT), Vancouver, BC, Canada.
Barbera, S., Bossert, W., & Pattanaik, P. K. (2004). Handbook Utility Theory. Volume II: Extensions, chap. Ranking Sets Objects, pp. 893977. Kluwer Academic Publishers.
Binshtok, M., Brafman, R. I., Shimony, S. E., Mani, A., & Boutilier, C. (2007). Computing optimal
subsets. Proceedings 22nd National Conference Artificial Intelligence (AAAI),
pp. 12311236, Vancouver, BC, Canada.
Bistarelli, S., Fargier, H., Montanari, U., Rossi, F., Schiex, T., & Verfaillie, G. (1999). Semiringbased CSPs valued CSPs: Frameworks, properties, comparison. Constraints, 4(3),
275316.
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004). CP-nets: tool representing reasoning conditional ceteris paribus preference statements. Journal
Artificial Intelligence Research, 21, 135191.
Boutilier, C., & Hoos, H. H. (2001). Bidding languages combinatorial auctions. Proceedings
17th International Joint Conference Artificial Intelligence (IJCAI), pp. 12111217,
Seattle, WS.
Brafman, R. I., Domshlak, C., & Shimony, S. E. (2006a). graphical modeling preference
importance. Journal Artificial Intelligence Research, 25, 389424.
Brafman, R. I., Domshlak, C., Shimony, S. E., & Silver, Y. (2006b). Preferences sets.
Proceedings 21st National Conference Artificial Intelligence (AAAI).
Brafman, R. I., & Domshlak, C. (2008). Graphically structured value-function compilation. Artificial Intelligence, 172, 325349.
Dago, P., & Verfaillie, G. (1996). Nogood recording valued constraint satisfaction problems.
ICTAI, pp. 132139.
Dechter, R. (1990). Enhancement schemes constraint processing: Backjumping, learning,
cutset decomposition. Artif. Intell., 41(3), 273312.
163

fiB INSHTOK , B RAFMAN , OMSHLAK , & HIMONY

desJardins, M., & Wagstaff, K. (2005). DD-PREF: language expressing preferences
sets. Proceedings 20th National Conference Artificial Intelligence (AAAI), pp.
620626, Pittsburgh, PA, USA.
Dixon, H. E., & Ginsberg, M. L. (2002). Inference methods pseudo-Boolean satisfiability
solver. Proceedings 18th National Conference Artificial Intelligence (AAAI), pp.
635640, Edmonton, Canada.
Een, N., & Sorensson, N. (2005). Translating pseudo-boolean constraints SAT. Journal
Satisability, Boolean Modeling Computation (JSAT), 2, 126.
Fishburn, P. C. (1969). Utility Theory Decision Making. John Wiley & Sons.
Gale, D., & Shapley, L. S. (1962). College admissions stability marriage. American
Mathematical Monthly, 69, 915.
Halpern, J. (1997). Defining relative likelihood partially-ordered preferential structures. Journal
Artificial Intelligence Research, 7, 124.
Kraus, S., Lehmann, D., & Magidor, M. (1990). Nonmonotonic reasoning, preferential models
cumulative logics. Artificial Intelligence, 44, 167207.
Manquinho, V. M., & Roussel, O. (2006). first evaluation pseudo-boolean solvers. Journal
Satisability, Boolean Modeling Computation (JSAT), 2, 103143.
Nisan, N. (2006). Bidding languages combinatorial auctions. Cramton, P., Shoham, Y., &
Steinberg, R. (Eds.), Combinatorial Auctions, chap. 2. MIT Press.
Price, B., & Messinger, P. (2005). Optimal recommendation sets: Covering uncertainty user
preferences. Proceedings 20th National Conference Artificial Intelligence (AAAI),
pp. 541548, Pittsburgh, PA.
Prosser, P. (1993). Hybrid algorithms constraint satisfaction problem. Computational Intelligence, 9, 268299.
Rollon, E., & Larrosa, J. (2007). Multi-objective Russian Doll Search. Proceedings 22nd
National Conference Artificial Intelligence (AAAI), pp. 249254, Vancouver, BS, Canada.
Schiex, T., & Verfaillie, G. (1993). Nogood recording static dynamic constraint satisfaction
problems. ICTAI, pp. 4855.
Sheini, H. M., & Sakallah, K. A. (2005). Pueblo: modern pseudo-boolean SAT solver. Proceedings Conference Design, Automation Test Europe (DATE), pp. 684685.
Shoham, Y. (1987). semantics approach non-monotonic logics. Proceedings 10th
International Joint Conference Artificial Intelligence (IJCAI), pp. 388392, Milan, Italy.
Verfaillie, G., Lematre, M., & Schiex, T. (1996). Russian Doll Search solving constraint optimization problems. Proceedings 13th National Conference Artificial Intelligence
(AAAI), pp. 181187, Portland, OR.

164

fiJournal Artificial Intelligence Research 34 (2009) 391442

Submitted 07/08; published 03/09

Solving #S Bayesian Inference Backtracking Search
Fahiem Bacchus
Shannon Dalmao
Toniann Pitassi

FBACCHUS @ CS . TORONTO . EDU
TONI @ CS . TORONTO . EDU

Department Computer Science
University Toronto
Toronto, Ontario
Canada, M5S 3G4

Abstract
Inference Bayes Nets (BAYES) important problem numerous applications probabilistic reasoning. Counting number satisfying assignments propositional formula
(#S AT) closely related problem fundamental theoretical importance. problems,
others, members class sum-of-products (S UM P ROD) problems. paper
show standard backtracking search augmented simple memoization scheme
(caching) solve sum-of-products problem time complexity least good
state-of-the-art exact algorithm, also achieve best known time-space
tradeoff. Furthermore, backtrackings ability utilize flexible variable orderings allows us
prove achieve exponential speedup standard algorithms UM P ROD
instances.
ideas presented utilized number solvers applied
various types sum-of-product problems. systems exploited fact backtracking
naturally exploit problems structure achieve improved performance range
problem instances. Empirical evidence performance gain appeared published works
describing solvers, provide references works.

1. Introduction
Probabilistic inference Bayesian Networks (BAYES) important well-studied problem
numerous practical applications probabilistic reasoning (Pearl, 1988). Counting number
satisfying assignments propositional formula (#S AT) also well-studied problem
fundamental theoretical importance. two problems known closely related.
particular, decision versions #S BAYES #P-complete (Valiant, 1979b, 1979a;
Roth, 1996), natural polynomial-time reductions problem
(Darwiche, 2002; Sang, Beame, & Kautz, 2005b; Chavira, Darwiche, & Jaeger, 2006).
direct relationship two problems arises observation
instances general sum products problem (S UM P ROD). Perhaps
fundamental algorithm UM P ROD (developed general way Dechter 1999) based
idea eliminating variables problem one one following fixed order.
algorithm called variable elimination (VE), core notion many state-of-the-art exact
algorithms UM P ROD (and BAYES).
SAT, problem determining whether propositional formula satisfying
assignments, also instance UM P ROD, original Davis-Putnam algorithm (DP)
determining satisfiability (Davis & Putnam, 1960) uses ordered resolution version
c
2009
AI Access Foundation. rights reserved.

fiBACCHUS , DALMAO , & P ITASSI

variable elimination. However, DP never used practice performance far inferior
modern versions backtracking search based DPLL algorithm (Davis, Logemann, & Loveland,
1962). fact DP provably less powerful modern versions DPLL equipped clause
learning (Hertel, Bacchus, Pitassi, & van Gelder, 2008).
performance gap naturally raises question whether backtracking search could
used solve types UM P ROD problems efficiently variable elimination.
paper, present general algorithmic framework using backtrack search methods (specifically DPLL) solve UM P ROD related problems.1 first show straightforward adaptation backtracking solving UM P ROD insufficient. However, examining sources
inefficiency able develop simple caching schemes allow backtracking
algorithm, #DPLL-Cache, achieve performance guarantees state-of-the-art exact algorithms UM P ROD, terms time space. Furthermore, prove backtrackings
natural additional flexibility allows sometimes achieve exponential speedup existing algorithms. Specifically, present family UM P ROD instances #DPLL-Cache
achieves exponential speedup original versions three prominent algorithms UM P ROD.
Besides theoretical results, also good reasons believe backtracking based
algorithms potential perform much better worst case guarantees problems
arise real domains. fact, subsequent work investigated practical application
ideas presented problem counting satisfying assignments, BAYES, constraint
optimization successful results (Sang, Bacchus, Beame, Kautz, & Pitassi, 2004; Sang
et al., 2005b; Sang, Beame, & Kautz, 2005a, 2007; Davies & Bacchus, 2007; Kitching & Bacchus,
2008).
outline paper follows. Section 2, define UM P ROD ; demonstrate #S ,
BAYES, important problems instances class problems; discuss various graphtheoretic notions width used characterize complexity algorithms UM P ROD; review core state-of-the-art exact algorithms UM P ROD. Section 3,
discuss DPLL-based algorithms caching solving #S UM P ROD provide worst
case complexity bounds algorithms. bounds best time space
guarantees achieved currently known algorithms. Section 4, provide framework
comparing algorithms algorithms UM P ROD prove caching DPLL
efficiently simulate known exact algorithms sometimes achieving super-polynomially
superior performance. Section 5 discuss work used algorithmic
ideas build practical solvers various problems. Finally, provide closing remarks
Section 6.

2. Background
section, first define sum-of-products (S UM P ROD) class problems, illustrate BAYES, #S AT, important problems instances UM P ROD.
show rest paper, backtracking search equipped different caching schemes
1. notion backtracking previous set commitments utilized contexts, including
algorithms UM P ROD. However, referring standard algorithmic paradigm backtracking
search explores single tree partial variable assignments depth-first manner. algorithm
extensive history stretches back hundred years (Bitner & Reingold, 1975).

392

fiBACKTRACKING EARCH



#SAT BAYES

well suited solving UM P ROD. key computational structure exploited algorithms UM P ROD explained graph theoretic notion width captures
structure identified. Different notions width exist, present three different definitions
show yield essentially equivalent measures complexity. different definitions
however useful different algorithms easily analyzed using different definitions width. Finally, briefly review important exact algorithms solving
UM P ROD related problems.
2.1 Sum-of-Products
Dechter (1999) shown BAYES many problems instances
general problem called UM P ROD (sum-of-products). instance UM P ROD defined
tuple hV, F, , i, V set discrete valued variables {X1 , . . . , Xn }, F set
functions {f1 , . . . , fm } fi defined set variables Ei V, addition
operator, multiplication operator. range functions F depends problem,
operators range commutative, associative,
distributes . Typical examples involve functions range boolean domain,
disjunction conjunction , reals, ordinary
addition multiplication.
Definition 1 (S UM P ROD) Given hV, F, , UM P ROD problem compute
MM
X1 X2




MO

fi (Ei ),

Xn i=1

i.e., sum () values (assignments) variables V product () functions
F evaluated assignments.
number well known problems instances UM P ROD. describe
below.
2.1.1 BAYES :
BAYES problem computing probabilities Bayesian Network (BN). Developed Pearl
(1988), Bayesian network triple (V, E, P) (V, E) describes directed acyclic graph,
nodes V = {X1 , . . . , Xn } represent discrete random variables, edges represent direct
correlations variables, associated random variable Xi conditional
probability table CPT (or function), fi (Xi , (Xi )) P, specifies conditional distribution
Xi given assignments values parents (Xi ) (V, E). BN represents joint distribution
random variables V probability assignment (x1 , . . . , xn ) variables
Q
given equation Pr (x1 , . . . , xn ) = ni=1 fi (xi , (xi )), fi (xi , (xi )) fi evaluated
particular assignment.
generic BAYES problem compute posterior distribution variable Xi given
particular assignment variables : i.e., Pr (Xi |). Since Xi finite set
k values, problem reduced computing k values Pr (Xi = dj ),
j = 1, . . . , k normalizing sum 1. values Pr (Xi = dj )
computed making assignments well Xi = dj , summing
393

fiBACCHUS , DALMAO , & P ITASSI

variables joint distribution Pr (x1 , . . . , xn ). Given product decomposition
Pr (x1 , . . . , xn ), equivalent reducing functions fi P setting variables
assigned Xi = dj , summing product remaining variables; i.e.,
instance UM P ROD.
Computing Marginals common solving BAYES want compute marginals.
is, instead wanting compute marginal Pr(Xi |) one particular variable Xi ,
want compute marginal variables instantiated .
2.1.2 ARKOV R ANDOM F IELDS
Markov Random Fields Markov Networks (MN) (Preston, 1974; Spitzer, 1971) similar
Bayesian Networks also define joint probability distribution set discrete
random variables V = {X1 , . . . , Xn } using set functions fi , called potentials,
set variables Ei V. particular, probability assignment (x1 , . . . , xn ) variables
given normalized product fi evaluated values specified assignment:
Q
fi (Ei [x1 , . . . , xn ]). difficulty compute partition function, normalizing constant:
Z=

X




XY

fi (Ei ).

Xn i=1

X1

Computing partition function thus instance UM P ROD.
2.1.3 OST P ROBABLE E XPLANATION
Probable Explanation (MPE) problem finding probable complete assignment
variables Bayes net (or Markov net) agrees fixed assignment subset
variables (the evidence). evidence, , instantiation variables E V, MPE
problem computing
max
V E




fi | (Ei E),

i=1

fi | reduction function fi instantiations variables E (yielding
function variables Ei E).
2.1.4
Let V = {X1 , X2 , . . . , Xn } collection n Boolean variables, let (V) k-CNF
Boolean formula variables clauses {c1 , . . . , cm }. assignment Boolean
variables V satisfying makes formula True (i.e., () = 1). asks, given Boolean
formula (V) k-CNF, satisfying assignment? viewing clause ci
function variables Ei (i.e., maps assignment variables TRUE assignment
satisfies clause FALSE otherwise), see equivalent instance
UM P ROD hV, {c1 , . . . , cm }, , i:
_
X1




_^

Xn i=1

394

ci (Ei ).

fiBACKTRACKING EARCH



#SAT BAYES

2.1.5 #S
Given k-CNF formula (V) boolean variables V = {X1 , . . . , Xn }, above, #S
problem determining number satisfying assignments . viewing clause
ci function variables Ei {0, 1} (i.e., maps satisfying assignments 1
falsifying assignments 0), see #S equivalent instance UM P ROD
hV, {c1 , . . . , cm }, +, i:
X



X1

2.1.6 PTIMIZATION




XY

ci (Ei ).

Xn i=1

ECOMPOSED BJECTIVE F UNCTIONS

Let V = {X1 , . . . , Xn } collection finite valued variables, optimization task find
assignment values variables maximizes objective function O(V) (i.e.,
function maps every complete assignment variables real value). many problems
decomposed sum sub-objective functions {f1 , . . . , fm } fi
function subset variables Ei . problem cast UM P ROD
instance hV, {f1 , . . . , fm }, max, +i
max max
X1

Xn


X

fi (Ei ).

i=1

2.2 Computational Complexity UM P ROD
UM P ROD computationally difficult problem. example, #S known complete
complexity class #P (Valiant, 1979b, 1979a) BAYES (Roth, 1996). Many special cases
easy remain hard #S AT, e.g., Valiant showed decision version #S #P
hard even clause size, k, 2, Roth (1996) showed problem hard even
approximate many cases easy, e.g., (V) monotone, Horn, 2-CNF.
Despite worst case intractability, algorithms UM P ROD, e.g., variable elimination
algorithm presented Dechter (1999), successful practice. key structure exploited
algorithm, algorithms, functions fi many UM P ROD problems
often relatively local fairly independent. is, often case sets variables
Ei function fi depends small, function dependent small
local set variables, sets share variables other,
functions fi fairly independent other. graph theoretic notion Tree Width used
make intuitions precise.
2.3 Complexity Measures Tree width
natural hypergraph, H = (V, E), corresponding instance hV, F, , UM P ROD. hypergraph, V corresponds set V variables, every function fi
domain set Ei , corresponding hyperedge, Ei .
width hypergraph critical measure complexity essentially state-ofthe-art algorithms #S , BAYES, UM P ROD. three different (and well known)
notions width define section. also show different notions
width basically equivalent. equivalences known, although need state
395

fiBACCHUS , DALMAO , & P ITASSI

prove basic properties, order analyze new algorithms, relate standard
algorithms.
Definition 2 (Branch width) Let H = (V, E) hypergraph. branch decomposition H
binary tree node labelled subset V . |E| many leaves
, labels one-to-one correspondence hyperedges E. node
n , let denote union leaf labeling subtree rooted n, let B denote
union labelings rest leaves. label n set vertices v
intersection B. branch width branch decomposition H
maximum size labeling . branch width H minimum branch width
branch decompositions H.
Example 1 Figure 1 shows particular branch decomposition Tbd hypergraph H = (V, E)
V = {1, 2, 3, 4, 5} E = {{1, 2, 3}, {1, 4}, {2, 5}, {3, 5}, {4, 5}}. Tbd branch width
3.
{}
H
HH
HH



{3, 4, 5}

{3, 4, 5}

H
H

H

HH

{2, 3, 4}

{2, 5}

{3, 5}

{4, 5}

H

H

H

{1, 2, 3}

{1, 4}

Figure 1: branch decomposition branch width 3 H = {(1, 2, 3), (1, 4), (2, 5), (3, 5),
(4, 5)}.

Definition 3 (Elimination width) Let H = (V, E) hypergraph, let = v1 , . . . , vn
ordering vertices V , vi ith element ordering. induces sequence
hypergraphs Hn , Hn1 , . . . , H1 H = Hn Hi1 obtained Hi follows.
edges Hi containing vi merged one edge vi removed. Thus underlying
. induced width H size largest edge
vertices Hi v1 , . . . vi1
hypergraphs Hn , . . . , H1 . elimination width H minimum induced width
orderings .
Example 2 ordering = h1, 2, 3, 4, 5i hypergraph H Example 1 produces
following sequence hypergraphs:
H5 = {(1, 2, 3), (1, 4), (2, 5), (3, 5), (4, 5)}
H4 = {(2, 3, 4), (2, 5), (3, 5), (4, 5)}
H3 = {(3, 4, 5), (3, 5), (4, 5)}
H2 = {(4, 5), (4, 5)}
H1 = {(5)}
396

fiBACKTRACKING EARCH



#SAT BAYES

induced width H 3the edges (1, 2, 3) H1 , (2, 3, 4) H2 (3, 4, 5) H3
achieve size.
Tree width third notion width.
Definition 4 (Tree width) Let H = (V, E) hypergraph. tree decomposition H
binary tree node labelled subset V following way. First,
every hyperedge e E, leaf node must label contains e. Secondly, given
labels leaf nodes every internal node n contains v V label n path
two leaf nodes l1 l2 whose labels contain v.2 tree width tree decomposition
H maximum size labeling minus 1, tree width H minimum
tree width tree decompositions H.
Example 3 Figure 2 shows Ttd tree decomposition H Example 1. Ttd tree width 3.
{3, 4, 5}
H
HH
HH



{2, 3, 4, 5}
H
HH


{1, 2, 3, 4}

{2, 5}

{3, 4, 5}
HH

{3, 5}

{4, 5}

H

H

H

{1, 2, 3}

{1, 4}

Figure 2: Tree decomposition tree width 3 H Example 1.
next three lemmas show three notions basically equivalent. proofs
Lemmas 2 3 given appendix.
Lemma 1 (Robertson & Seymour, 1991) Let H hypergraph. branch width H
tree width H plus 1, tree width H 2 times branch width H.
Lemma 2 Let H = (V, E) hypergraph tree decomposition width w.
elimination ordering vertices V induced width H w.
Lemma 3 Let H hypergraph elimination width w. H tree decomposition tree width w.
Letting TW (H), BW (H), EW (H) represent tree width, branch width elimination
width hypergraph H, lemmas give following relationship three
notions width: hypergraphs H
BW (H) 1 TW (H) = EW (H) 2BW (H).
2. Since labels internal nodes determined labels leaf nodes way, seen
pair nodes n1 n2 tree decomposition every node lying path must contain v
label v appears n1 n2 labels. commonly known running intersection property tree
decompositions.

397

fiBACCHUS , DALMAO , & P ITASSI

{4, 5}
H
HH
H


{3, 4, 5}

{4, 5}

H
H

HH


{2, 3, 4, 5}

{3, 5}

H
H

H

{1, 2, 3, 4}

{2, 5}

H

H

H

{1, 2, 3}

{1, 4}

Figure 3: Tree decomposition hypergraph H Example 1 constructed
ordering = h1, 2, 3, 4, 5i.

Example 4 tree decomposition Ttd H = {(1, 2, 3), (1, 4), (2, 5), (3, 5), (4, 5)} given Figure 2 property tree width twice branch width branch
decomposition Tbd H given Figure 1. Ttd obtain ordering = h1, 2, 3, 4, 5i
used Example 2. (The proof Lemma 2, given appendix, shows elimination ordering constructed tree-decomposition.) shown Example 2,
induced width 3, equal tree width tree decomposition Ttd constructed.
Finally, ordering construct new tree decomposition H shown Figure 3.
(The proof Lemma 3 shows tree decomposition constructed elimination
ordering). induced width 3 and, indicated Lemma 3 tree decomposition constructed
equal tree width 3.
noted definition tree decompositions varies slightly definitions
appear literature, e.g., (Bodlaender, 1993). Following Robertson Seymour (1991)
defined tree decompositions hypergraphs, rather graphs, made
two extra restrictions simplify proofs results. First, restricted tree
decompositions binary trees, second required hyperedge contained
label leaf node tree decomposition. Usually tree decompositions
restricted binary trees, require hyperedge contained nodes label
(not necessarily leaf node).
difficult show tree decomposition fails satisfy two restrictions
converted tree decomposition satisfying restrictions without changing width.
However, straight forward observe without two restrictions tree width
equal elimination width. Hence, restrictions change tree width.
2.4 Exact Algorithms UM P ROD
Next briefly review three prominent exact algorithms BAYES. algorithms solve
general problem UM P ROD. algorithms fact nondeterministic algorithms
considered families procedures, member particular deterministic realization.
398

fiBACKTRACKING EARCH



#SAT BAYES

2.4.1 VARIABLE E LIMINATION :
Variable bucket elimination (VE) (Dechter, 1999) fundamental algorithm UM P ROD.
Variable elimination begins choosing elimination ordering, variables V = {X1 ,
. . ., Xn }: X(1) , . . ., X(n) . (This nondeterministic part computation). first
phase, functions involving X(1) , collected together set FX(1) , new function,
F1 computed summing X(1) . new function sums product functions
FX(1) X(1) values. Specifically, F1 function variables functions
FX(1) except X(1) , value assignment variables
F1 () =

X



f (, X(1) = d).

dvals(X(1) ) f FX(1)

Summing X(1) induces new hypergraph, H1 , hyperedges corresponding set
functions FX(1) replaced single hyperedge corresponding new function F1 .
process continues sum X(2) H1 n variables summed out.
Note sequence hypergraphs generated summing variables according
sequence hypergraphs defines induced width (Definition 3).
original Davis-Putnam algorithm (Davis & Putnam, 1960) based ordered resolution
instance variable elimination. Consider applying variable elimination formulation
given above. AT, new functions Fi computed stage need preserve whether
product functions FX(i) 0 1, exact number satisfying assignments need
remembered. accomplished representing Fi symbolically set clauses.
Furthermore, set clauses computed generating clauses obtained
resolving X(i) , discarding old clauses containing X(i) . resolution step
corresponds summing operation, yields precisely Davis-Putnam (DP) algorithm
satisfiability.3
2.4.2 R ECURSIVE C ONDITIONING :
Recursive conditioning (RC) (Darwiche, 2001) another type algorithm UM P ROD. Let
= hV, F, , instance UM P ROD H underlying hypergraph. RC
divide conquer algorithm instantiates variables V break problem
disjoint components. proceeds solve components independently. original spaceefficient version recursive conditioning, specified Darwiche (2001), begins branch
decomposition H width w depth d, initially empty set instantiated variables
. (Choosing nondeterministic part computation.) call algorithm RC-Space
show Algorithm 1.
branch decomposition specifies recursive decomposition problem used
RC-Space follows. Let label (n) label node , let ST UM P ROD problem
defined variables functions contained . (In initial call complete branch
decomposition containing variables functions S, initially ST = S). Starting r,
root , RC-Space solves reduced UM P ROD ST | assignments variables
3. Rish Dechter (2000) previously made connection DP variable elimination. thus
able show, DP runs time nO(1) 2O(w) , w branch width underlying hypergraph SAT
instance.

399

fiBACCHUS , DALMAO , & P ITASSI

label (left(r)) label (right(r)) yet instantiated , left(r) right (r) left
right children r. sum solution inputed instance ST | .
renders set functions subtree leftChild (r) (i.e., leaf labels) disjoint
functions rightChild (r). Thus , RC-Space independently solve
subproblems specified leftChild (r)| rightChild (r)| (i.e., sum products
functions left/right subtree conditioned instantiations )
multiply answers obtain solution ST | . leaf nodes, function fi associated
node variables instantiated, algorithm simply LOOKUP fi
current value.
Algorithm 1: RC-SpaceLinear Space Recursive Conditioning
1
2
3
4
5
6
7
8
9
10
11

RC-Space (T, )
begin
leaf node
return LOOKUP(value function labeling leaf node)
p = 0; r = root (T )
~x = variables label (left(r)) label (right(r)) uninstantiated
forall {instantiations ~x}
p = p + RC-Space (leftChild (T ), ) RC-Space (rightChild (T ), )
end
return p
end

less space-efficient time-efficient version recursive conditioning, called RCCache, caches intermediate values reused reduce computation. Algorithm 2
shows RC-Cache algorithm. Like RC-Space, invocation RC-Cache solves subproblem specified variables functions contained passed subtree . Since functions
share variables label (root(T )) variables outside , instantiations subset, y, intersecting label (root(T )) affect form subproblem.
Hence, RC-Cache return answer invoked y, even
assignments changed. RC-Cache, thus use index cache, storing computed result cache (line 13) returning immediately answer already cache
(line 7).
Propagation Since RC instantiates problems variables, propagation employed.
is, RC perform additional inference compute implicit effects assignment
remaining problem ST | . example, functions UM P ROD problem
clauses (e.g., solving #S AT) unit propagation performed. Propagation
make recursive conditioning effective. example, one remaining clauses becomes
falsified unit propagation, recursive conditioning immediately move next
instantiation variables ~x. Similarly, unit propagation force value variables
encountered subsequent recursive calls, thus reducing number different instantiations
must attempted recursive call. noted propagation reduce
worst case complexity algorithm, UM P ROD problems propagation ineffective.
however improve algorithms efficiency families problems.
400

fiBACKTRACKING EARCH



#SAT BAYES

Algorithm 2: RC-CacheRecursive Conditioning caching
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

RC-Cache (T, )
begin
leaf node
return LOOKUP(value function labeling leaf node)
= label (root(T ))
InCache(T, y)
return GetValue(T, y)
p = 0; r = root (T )
~x = variables label (left(r)) label (right(r)) uninstantiated
forall {instantiations ~x}
p = p + RC-Cache (leftChild (T ), ) RC-Cache (rightChild (T ), )
end
AddToCache((T ,y), p)
return p
end

RC-Cache+ simple extension RC used practice set variables ~x
label (left(r) label (right (r)) (line 10 Algorithm 2) iteratively rather once.
is, rather iterate complete assignments ~x instantiate variables one
time, performing propagation assignment. make propagation effective,
since, e.g., empty clause might detected instantiating subset variables ~x
thus number iterations loop might reduced.
variables ~x set iteratively order assigned vary.
Furthermore, order assignment vary dynamically. is, depending values assigned first k variables ~x, algorithm make different choices
unassigned variable ~x assign next.
call extension RC-Cache uses incremental assignments dynamic variable ordering within set ~x, RC-Cache+ . RC-Cache+ uses caching scheme RC-Cache,
flexibility variable ordering. noted however, RC-Cache+
complete freedom variable ordering. must still follow inputed branch decomposition . is, variable chosen must come set ~x label (left(r)) label (right (r)).
contrast DPLL based algorithms present next section, always
free choose remaining unassigned variable next variable assign.
Space-Time Tradeoff RC attractive feature achieve non-trivial space-time
tradeoff, taking less time caches recursively computed values (RC-Cache), taking less
space without caching (RC-Space). fact, Darwiche Allen (2002) show smooth
tradeoff achieved, RC-Space RC-Cache two extremes.
DPLL based algorithms presented share number features RC; also reduce
decompose input problem making instantiations, gain efficiency caching, achieve
similar space-time tradeoff. However, algorithms based paradigm backtracking,
rather divide conquer. particular, explore single backtracking tree
decomposed subproblems solved separately rather solved interleaved
401

fiBACCHUS , DALMAO , & P ITASSI

fashion. result, limited following decomposition scheme specified fixed
branch decomposition. see, limitation static decomposition scheme means
RC-Space RC-Cache must perform exponentially worse algorithms instances.
2.4.3 AND/OR EARCH :
recent work Dechter Mateescu (2007) shown notion AND/OR search
spaces (Nilsson, 1980) applied formalize divide conquer approach UM P ROD
problems utilized RC. formulation structure guides AND/OR search algorithm
pseudo tree. (Choosing pseudo tree nondeterministic part computation.)
Definition 5 (Primal Graph) primal graph hypergraph H undirected graph G
vertices H edge connecting two vertices two vertices
appear together hyperedge H.
Definition 6 (Pseudo Tree) Given undirected graph G vertices edges (V, EG ), pseudo
tree G directed rooted tree vertices edges (V, ET ) (i.e., set vertices
G), edge e G must connect vertex one ancestors.
is, e = (v1 , v2 ) e EG e 6 ET implies either v1 ancestor v2 v2
ancestor v1 .
implies edge G connecting vertices lying different subtrees .
Given UM P ROD problem = hV, F, , underlying hypergraph H, form G,
primal graph H. vertices G variables problem V pair variables
appear together function F connected edge G. pseudo tree
G property two vertices (variables S) appear functions
F ancestors descendants, cannot appear functions siblings
ancestors siblings descendants siblings.
implies variable v ancestors instantiated, variables contained children subtrees become disconnected. is, variables subtrees
longer appear functions together, resulting subproblems solved independently.
AND/OR search algorithm utilizes fact solve subproblems independently, like
recursive conditioning.
Example 5 Given hypergraph H = (V, E) V = {1, 2, 3, 4, 5} E = {{1, 2, 3},
{1, 4},{2, 5}, {3, 5}}, primal graph H G = (V, EG ) EG = {(1, 2), (1, 3), (2, 3),
(1, 4), (2, 5), (3, 5)}. H, primal graph G, pseudo tree G shown Figure 4.
dotted lines shown pseudo tree edges G pseudo tree.
seen diagram edges connect nodes ancestors.
space efficient version AND/OR-Space search algorithm (Dechter & Mateescu,
2007) shown Algorithm 3. solves UM P ROD instance = hV, F, , i, taking
input pseudo tree problem (i.e., hypergraph converted primal graph
G, pseudo tree G), initially empty set instantiated variables . algorithm solves sub-problem original instance reduced instantiations , S| .
sub-problem solved defined functions S| variables contained
passed sub-tree . Initially, empty original pseudo tree containing
variables, algorithm solves original problem S.
402

fiBACKTRACKING EARCH



#SAT BAYES

1
1

2

3

1

2

3
4

4

5

Hypergraph

4

5

2
3
5

Primal Graph
Pseudo Tree

Figure 4: hypergraph, primal graph, pseudo tree Example 5.

nodes pseudo tree variables problem S, also attach node
n set functions fns(n). function f F fns(n) (a) n scope
f (b) variables scope f ancestors n . means f
fully instantiated set arguments AND/OR search instantiates node (variable) n.
Algorithm 3: AND/OR-SpaceLinear Space AND/OR search
1
2
3
4
5
6
7
8
9
10

AND/OR-Space (T, )
begin
p = 0; r = root (T )
STr = set subtrees r
forall {instantiations r}
Q
= f fns(r) LOOKUP(value f {r = d})
Q
p = p + STr AND/OR-Space (T , {r = d})
end
return p
end

algorithm operates variable r root pseudo tree . instantiation r algorithm computes , product functions F become fully
instantiated assignment r, i.e., fns(r). invokes separate recursion
child r passing subtree rooted child recursive call. AND/OR search exploits
decomposition separate recursions. r one child, problem
decomposedthere single reduced subproblem resulted instantiating r.
Like RC, AND/OR search made time efficient expense using space.
Algorithm 4 shows caching version AND/OR-Cache (called AND/OR graph search Dechter
Mateescu (2007)). Let label (n) node n pseudo tree set ancestors
n appear function n descendant n . instantiations label (n) affect functions variables subtree rooted n.
Hence, label (n) plays role root label passed branch decomposition RCCache: instantiations variables affect subproblem currently computed.
403

fiBACCHUS , DALMAO , & P ITASSI

Hence, like RC-Cache, AND/OR-Cache use instantiations subset, y, intersecting
label (root(T )) along index cache.
Finally, RC-Cache+ , propagation used decrease number branches
AND/OR search needs explore. example, recursive calls children r
terminated one calls returns value zero.
Algorithm 4: AND/OR-CacheAND/OR search caching
1
2
3
4
5
6
7
8
9
10
11
12
13

AND/OR-Cache (T, )
begin
p = 0; r = root (T )
= label (root (T ))
InCache(T, y)
return GetValue(T, y)
STr = set subtrees r
forall {instantiations r}
Q
= f fns(r) LOOKUP(value f {r = d})
Q
p = p + STr AND/OR-Cache (T , {r = d})
end
return p
end

AND/OR-Cache+ variable order dynamism employed AND/OR search.
particular, variables along chain pseudo tree reordered without affecting
decompositions specified . chain sub-path none nodes, except
perhaps last, one child. Figure 4 nodes 2, 3, 5 form chain. resultant
extension, AND/OR-Cache+ , dynamically chose next instantiate variables
chain starts root passed pseudo tree . (Marinescu Dechter (2006) refer
AND/OR-Cache+ AND/OR partial variable ordering. However utilize
caching version algorithm.)
pass rest chain (and nodes below) next recursive call,
chosen variable last chain invoke separate recursive call child. Like
RC-Cache+ , AND/OR-Cache+ complete freedom choice variableit must
chose variable top chain. Furthermore, AND/OR-Cache+ use caching
scheme bottom chain (i.e., variables chain instantiated) since
cache requires set variables instantiated. makes AND/OR-Cache+
similar RC-Cache+ .
2.4.4 E XACT LGORITHMS
algorithm commonly used BAYES join tree algorithm (Lauritzen & Spiegelhalter, 1988), also adapted solve kinds UM P ROD problems. join-tree
algorithm first organizes primal graph UM P ROD problem tree clustering
variables, performs message passing tree messages computed
variable elimination process. context BAYES main advantage join-tree algorithms
404

fiBACKTRACKING EARCH



#SAT BAYES

compute marginals. compute posterior probability variables
given evidence.
contrast, default version variable elimination computes posterior distribution
single variable. However, Kask et al. (2005) show join-tree algorithm reduced
version remembers intermediate results runs time
space VE. Hence, results state comparing new backtracking based
algorithms also hold join tree algorithm.
Computing Marginals algorithms described above, i.e., VE, RC, AND/OR
search, modified compute marginals solving BAYES without change
worst case complexity. particular, besides results Kask et al. (2005), Darwiche (2001)
shown RC compute marginals BAYES problems extra bottom traversal
search treeat doubling run time. technique applied AND/OR
search algorithms. DPLL algorithms present here, Sang et al. (2005b) given
even simpler scheme modifying computing marginals. Sang et al.s
scheme involves maintaining extra information search require extra
traversal search tree.
Another algorithm mostly superseded cut-set conditioning (Pearl, 1988).
idea identify subset variables set reduce underlying hypergraph
UM P ROD tree. reduced UM P ROD easily solved. However,
approach requires trying possible instantiations cut-set yielding runtime usually
worse RC-Cache. Nevertheless, cutset conditioning potentially applied conjunction
exact algorithms (Mateescu & Dechter, 2005).
Finally, important early algorithm called DDP presented Bayardo Pehoushek
(2000). version DPLL utilized dynamic decomposition solving #S AT.
terms algorithms discussed above, AND/OR-Space viewed version
DDP utilizes pseudo tree guide variable ordering. original presentation DDP,
variable ordering could used including dynamic variable orderings. search continued
problem decomposed independent components (tested search)
point separate recursion used solve component. Hence, DDP explored AND/OR
search tree, however tree need correspond pseudo tree original problem.
(The DVO DSO AND/OR search schemes presented Mateescu Dechter (2005) also
versions DDP run particular variable ordering heuristics). comparison algorithms
present next section, Bayardo Pehoushek (2000) provide complexity analysis
DDP, DDP use caching enhance performance, DDP still less flexibility
variable ordering. particular, problem split independent components
search must solve components sequentially separate recursions. Inside recursion
search branch variables current component. is, DDP cannot interleave
solution components like DPLL algorithms present here.
2.5 Complexity Analysis
known algorithms BAYES, #S UM P ROD run exponential-time worst case.
However, branch width underlying hypergraph instance, w, small,
algorithms much efficient. shown algorithms VE, RCCache AND/OR-Cache discussed run time space nO(1) 2O(w) . note
405

fiBACCHUS , DALMAO , & P ITASSI

complexity algorithms usually given terms tree width elimination width,
branch width. However, Lemmas 1, 2, 3, concepts equivalent within factor
2, therefore asymptotic complexity equivalently stated terms three
notions width (tree width, branch width, elimination width). analyzing backtracking
algorithms, branch width somewhat natural, reason chosen state
complexity results terms branch width.
runtime variable elimination algorithm easily seen nO(1) 2O(w) .
see this, notice algorithm proceeds n stages, removing one variable stage. Suppose algorithm run variable ordering elimination width v. algorithm
removes ith variable ith stage. ith stage, functions involving variable
merged obtain new function. indicated Section 2.4.1, computing new function
involves iterating overall possible instantiations variables. runtime stage therefore exponential number underlying variables new function, bounded v.
Thus, runtime algorithm bounded nO(1) 2O(v) . Lemmas 1 2 3,
elimination width v, branch width v + 1, therefore overall runtime
claimed. also noted since new function must stored, space complexity
variable elimination time complexity, i.e., nO(1) 2O(w) .
also shown run times RC-Cache RC-Cache+ bounded nO(1) 2O(w)
(Darwiche, 2001). Further, nice time-space tradeoff. is, space-efficient implementation RC, RC-Space, runs time 2O(w log n) needs space linear size
input, RC-Cache space complexity equal time complexity, nO(1) 2O(w) .
present proofs showing DPLL based algorithms achieve time time/space
bounds; proofs give bounds RC-Space, RC-Cache, RC-Cache+ special cases.
Finally, shown AND/OR-Space runs time 2O(w log n) (Dechter & Mateescu,
2007). Specifically, Dechter Mateescu show AND/OR-Space runs time exponential
height inputed pseudo tree, Bayardo Miranker (1995) show height bounded
w log n. Lemma 1 shows bound also holds branch width. Similarly, Dechter
Mateescu (2007) show AND/OR-Cache runs time space bounded nO(1) 2O(w)
exploiting close relationship pseudo trees elimination orders.
Making algorithms deterministic. stated above, algorithms fact nondeterministic algorithms requiring different nondeterministically determined input. Hence,
stated complexity bounds mean exists choice nondeterministic input (i.e.,
variable ordering VE, branch decomposition RC, pseudo tree AND/OR
search) algorithm achieve stated complexity bound.
However, achieve runtime practice, need able find good branch
decomposition (variable ordering, pseudo tree) efficiently. Unfortunately, general problem
computing optimal branch decomposition (i.e., one width equal branch width
H) NP-complete. However, Robertson Seymour (1995) present algorithm computing
branch decomposition branch width within factor 2 optimal runs
time nO(1) 2O(w) , w branch width H. first running deterministic algorithm
compute good branch decomposition, one obtain deterministic versions RC-Cache
RC-Cache+ run time space nO(1) 2O(w) , well deterministic version RC-Space
runs linear space time 2O(w log n) . deterministic versions longer require access
nondeterministically determined choice achieve stated runtimes.
406

fiBACKTRACKING EARCH



#SAT BAYES

Algorithm 5: DPLL SAT
1
2
3
4
5
6
7
8
9
10

DPLL ()
begin
clauses
return TRUE
else contains empty clause
return FALSE
else
choose variable x appears
return (DPLL(|x=0 ) DPLL(|x=1 ))
end

Similarly nearly optimal branch decomposition, use Lemmas 1-3 find nearly
optimal elimination ordering, thus obtain deterministic version variable elimination
algorithm runs time space nO(1) 2O(w) . finally, nearly optimal elimination
ordering bucket-tree construction Dechter Mateescu (2007) used construct
nearly optimal pseudo tree, thus obtain deterministic version AND/OR-Space
runs linear space time 2O(w log n) , deterministic version AND/OR-Cache runs
time space nO(1) 2O(w) .

3. Using DPLL #S UM P ROD
present methods augmenting backtracking search different caching schemes
solve UM P ROD time space guarantees least good exact
algorithm UM P ROD. ease presentation present DPLL-based algorithms solving
#S AT, derive complexity results algorithms. Later discuss algorithms
complexity results applied instances UM P ROD (like BAYES).
3.1 DPLL #DPLL:
DPLL nondeterministic algorithm AT, also used solve various generalizations AT, including #S (Dubois, 1991; Zhang, 1996; Birnbaum & Lozinskii, 1999; Littman,
Majercik, & Pitassi, 2001). DPLL solves performing depth-first search space
partial instantiations (i.e., standard backtracking search algorithm). nondeterministic part
computation lies choice variable query (i.e., instantiate) next
search. operates problems encoded clause form (CNF).
standard DPLL algorithm solving given Algorithm 5. use notation
|x=0 |x=1 denote new CNF formula obtained reducing setting variable x
0 1. Reducing x = 1 (x = 0) involves removing clauses containing x (x)
removing falsified x (x) remaining clauses.
DPLL nondeterministic procedure generates decision tree representing underlying
CNF formula. solving AT, decision tree traversed depth-first manner either
satisfying path encountered, whole tree traversed (and paths falsify formula).
nondeterminism algorithm occurs choice variable line 8. practice
407

fiBACCHUS , DALMAO , & P ITASSI

Algorithm 6: #DPLL #S (no caching)
1
2
3
4
5
6
7
8
9
10

#DPLL ()
// Returns probability
begin
clauses
return 1
else contains empty clause
return 0
else
choose variable x appears
return ( 12 #DPLL(|x=0 ) + 21 #DPLL(|x=1 ))
end

nondeterminism typically resolved via heuristic choice. Also, algorithm utilizes early
termination disjunctive test line 9; i.e., first test returns TRUE second recursive
call made. Thus, algorithm stops finding first satisfying path.
Note require DPLL perform unit propagation. particular, unit propagation
always realized choice variable line 8. particular, force DPLL
always chose variable appears unit clause whenever one exists,
effect forcing DPLL perform unit propagation every variable instantiation. is,
variable chosen, instantiated one values, input CNF reduced.
reduced formula, |x=0 |x=1 , passed next recursive call may contain unit clauses.
unit propagation, variables clauses would instantiated satisfy unit clauses.
instead, force one variable chosen next, one instantiation would immediately
fail due generation empty clause, would instantiate variable
value unit propagation. Hence, since analyze DPLL nondeterministic algorithm,
includes deterministic realizations perform unit propagation.
simple modification DPLL allows count satisfying assignments. Algorithm 6 gives
#DPLL algorithm counting. algorithm actually computes probability set
satisfying assignments uniform distribution. Hence, number satisfying assignments
obtained multiplying probability 2n , n number variables .
alternative would return 2 raised number unset variables whenever clauses
(line 4) multiply recursively computed counts 21 (line 9).
Known exponential worst-case time bounds DPLL also apply #DPLL: unsatisfiable
formulas, algorithms traverse entire decision tree terminating. Although
decision tree small (e.g., immediate contradiction detected), families
formulas decision tree must large. particular, implicit results Haken (1985)
decision tree formulas encoding (negation the) propositional pigeonhole
principle exponential size, thus DPLL #DPLL must take exponential-time
examples. lower bound not, however, help us discriminate algorithms since
known algorithms #S BAYES take exponential-time worst-case. Nevertheless,
#DPLL requires exponential time even instances efficiently solved competing
algorithms UM P ROD. see this, consider 3CNF formula 3n variables consisting
408

fiBACKTRACKING EARCH



#SAT BAYES

n clauses share variables. complete decision tree exponential size, therefore
#DPLL require exponential time. contrast, since formula low tree width
solved polynomial time VE, RC, AND/OR search.
3.2 DPLL Caching:
Given obvious application DPLL solve UM P ROD give exponentially worse
performance standard algorithms, examine ways modifying DPLL
solve #S (and thus BAYES UM P ROD) efficiently. understand source
#DPLLs inefficiency consider following example.
Example 6 following diagram shows run #DPLL = {(w x)(y z)}. node
shows variable branched on, current formula #DPLL working on. left hand
branches correspond setting branch variable FALSE, right variable set
TRUE. empty formula indicated {}, formula containing empty clause
indicated {()}. diagram shows #DPLL encounters solves subproblem {(y z)}
twice: along path (w = 0, x = 1) along path (w = 1). Note
example unit propagation realized choice variable orderingafter w set FALSE,
#DPLL chooses instantiate variable x since variable appears unit clause.
w:{(x w))(y z)}
H
HH

HH

HH


H

x:{(x)(y z)}

y:{(y z)}

H
HH

HH


H
HH

0:{()}

y:{(y z)}
H

H

H

z:{(z)}

z:{(z)}

1:{}

HH

0:{()}

1:{}

1:{}

HH

0:{()}

1:{}

one considers example applying #DPLL disjoint sets clauses, becomes
clear formulas #DPLL encounter subproblem exponential number
times.
3.2.1 DPLL



IMPLE C ACHING (#DPLL-S IMPLE C ACHE )

One way prevent duplication apply memoization. indicated Example 6, associated
every node DPLL tree formula f subtree rooted node trying
compute number satisfying assignments f . performing depth-first search
tree, keep cache contains formulas f already solved, upon
reaching new node tree avoid traversing subtree value corresponding
formula already stored cache.
Example 6 would cache {(y z)}, solve along path (w = 0, x = 1)
thereby avoid traversing subtree (w = 1).
409

fiBACCHUS , DALMAO , & P ITASSI

Algorithm 7: #DPLL algorithm simple caching (#DPLL-SimpleCache)
1
2
3

4
5
6
7
8
9
10

#DPLL-SimpleCache ()
// Returns probability
begin
InCache()
// Also detects obvious formulas.
return GetValue()
else
choose variable x appears
val = 12 #DPLL-SimpleCache (|x=0 ) + 21 #DPLL-SimpleCache (|x=1 )
AddToCache(,val )
return val
end

form caching, call simple caching (#DPLL-SimpleCache)
easily implemented shown Algorithm 7.4 #DPLL, #DPLL-SimpleCache returns
probability input formula ; multiplying 2n gives number satisfying assignments.
addition formulas stored cache also following obvious formulas whose
value easy compute. (1) empty formula {} containing clauses value 1. (2)
formula containing empty clause value 0. Obvious formulas treated
implicitly stored cache (they need explicitly stored cache, rather values
computed required).
following (low complexity) subroutines used access cache. (1) AddToCache(, r):
adds cache fact formula value r. (2) InCache(): takes input formula
returns true cache. (3) GetValue(): takes input formula known
cache returns stored value. various ways computing cache key .
example, maintained sorted set sorted clauses, cached text
string. caching scheme nO(1) complexity.
Surprisingly, simple caching, reasonably well. following theorem shows simple
caching achieves runtime bounded 2O(w log n) , w underlying branch width.
complexity analysis earlier algorithms presented Section 2.5, simple caching algorithm
also made deterministic first computing branch decomposition within factor
2 optimal (using Robertson-Seymour algorithm), running #DPLL-SimpleCache
variable ordering determined branch decomposition.
Theorem 1 solving #S n variables, execution #DPLL-SimpleCache
runs time bounded 2O(w log n) w underlying branch width instance. Furthermore, algorithm made deterministic time guarantees.
Although theorem shows #DPLL-SimpleCache fairly well, performance
quite good best UM P ROD algorithms (which run time nO(1) 2O(w) ).
4. Simple caching utilized (Majercik & Littman, 1998), without theoretical analysis.

410

fiBACKTRACKING EARCH



#SAT BAYES

Algorithm 8: #DPLL algorithm component caching (#DPLL-Cache)
1
2
3

4
5
6
7
8
9
10
11
12
13
14
15
16

#DPLL-Cache ()
// Returns probability set disjoint formulas
begin
InCache()
// Also detects obvious formulas.
return GetValue()
else
= RemoveCachedComponents()
choose variable x appears component
= ToComponents(|v=0 )
#DPLL-Cache ( {} )
+ = ToComponents(|v=1 )
#DPLL-Cache ( {} + )
AddToCache(, 21 GetValue( ) + 12 GetValue(+ ))
#DPLL-Space
RemoveFromCache( + )
return GetValue()
end

3.2.2 DPLL



C OMPONENT C ACHING (#DPLL-C ACHE )

show sophisticated caching scheme allows #DPLL perform well
best known algorithms. call new algorithm #DPLL-Cache, implementation given
Algorithm 8.
algorithm generalize cache deal sets formulas. First, say
(single) formula known value stored cache obvious formula (and
value implicitly stored cache). Given set formulas say set known
either every known, whose value known zero. cases
say value equal product values .
generalize cache access subroutines. (1) InCache() generalized
take input set formulas . returns true known defined. (2) Similarly
GetValue() generalized take sets formulas input. returns product cached
values formulas .
intuition behind #DPLL-Cache recognize variables set input formula
may become broken disjoint components, i.e., sets clauses share variables
other. Since components share variables compute number solutions
component multiply answers obtain total solution count. Thus, intended
GetValue called set disjoint components . case correctly return
solution count i.e., product solution counts .
algorithm creates standard DPLL tree, however caches component formulas
values computed. keeps input decomposed form set disjoint components,
components already cache (and thus value known) remove
411

fiBACCHUS , DALMAO , & P ITASSI

parts inputreducing size problem still solve avoiding
resolve components.
new algorithm uses previously defined cache access subroutines along two additional (low complexity) subroutines. (1) ToComponents(): takes input formula , breaks
set minimal sized disjoint components, returns set. (2) RemoveCachedComponents(): returns input set formulas known formulas removed. input
#DPLL-Cache always set disjoint formulas. Hence, run #DPLL-Cache input formula
initially make call #DPLL-Cache (ToComponents()).
ToComponents simply computes connected components primal graph generated
. is, graph variables nodes, two nodes connected
corresponding variables appear together (in polarity) clause . connected
component primal graph (which computed simple depth-first traversal
graph Cormen, Leiserson, Rivest, & Stein, 2001), defines set variables whose clauses form
independent component .
call #DPLL-Cache completes solution unknown components
set inputed components . components known product values
components returned line 4. Otherwise input set components reduced removing known components (line 6), must leave least one unknown component
potentially reduces size remaining problem solved. variable unsolved component chosen branched on. Since variable appears component
assignment affect . particular, assignment might break smaller components (line 8 11). recursive call solve components passed, two
recursive calls value computed cached (line 12). Finally, since components
inputed set solved value retrieved cache returned.
Example 7 Figure 5 illustrates behavior #DPLL-Cache formula = {(a, b, c, x),
(a, b, c), (a, b, c), (d, e, f, x), (d, e, f ), (d, e, f )}. Although problem could solved
simpler search tree, use variable ordering generates interesting behavior.
node shows variable branched on, current set components #DPLLCache working on. known components (i.e., already cache) marked
asterisk ( ). branch variables set FALSE left branch TRUE right branch.
empty formula indicated {}, formula containing empty clause indicated
{()}. simply diagram use unit propagation simplify formula branch
variable set. avoids insertion diagram nodes unit clause variables
branched on. Finally, note known formulas removed recursive call made, per
line 6 Algorithm 8).
root, x set false, broken two components a,b,c = {(a, b, c),
(a, b, c), (a, b, c)}, d,e,f = {(d, e, f ), (d, e, f ), (d, e, f )}. search tree demonstrates
matter search interleaves branching variables different components,
components still solved independently. see leftmost node tree
branches f succeeds solving component {(e, f ), (e, f )}. component added
cache. Similarly, parent node branches b solves component {(b, c), (b, c)}.
(The subcomponents + generated setting b, lines 8 11 Algorithm 8, performing unit propagation equal empty formula, {}, thus known). backtrack
d, alternate value affect component {(b, c), (b, c)}, value
retrieved cache leaving component {(e, f )} solved. Branching e solves
412

fiBACKTRACKING EARCH



#SAT BAYES

x:{}
HH
HH

HH

H

..
a,b,c ,
a:
.
d,e,f
H
HH

HH


HH


HH



H


{(b, c)},
{(b, c), (b, c)},
b:
d:
d,e,f
d,e,f
H
H
n
nH
HH

H
{}
{}

H
HH

HH





{(b, c), (b, c)} ,
{(b, c), (b, c)},
e:
b:
{(e, f )}
{(e, f ), (e, f )}
HH
HH

H


H

{}
{}
f:
{(e, f ), (e, f )}
{(e, f ), (e, f )}
H

H
n nH
{}
{()}

H
n
nH
{}
{}

Figure 5: Search Space #DPLL-Cache

component. Backtracking {(e, f )} {(e, f ), (e, f )} solved,
d,e,f value computed placed cache. backtracking a, alternate value
affect component d,e,f , value retrieved cache leaving
component {(b, c)} solved. Branching b solves component,
{(b, c)} {(b, c), (b, c)} solved a,b,c value computed placed cache.
search backtrack try setting x TRUE.
obtain following upper bound runtime #DPLL-Cache.
Theorem 2 solving #S n variables, exists execution #DPLL-Cache runs
time bounded nO(1) 2O(w) w underlying branch width instance. Furthermore,
algorithm made deterministic time guarantees (as discussed Section 2.5).
see #DPLL-Cache achieve level performance best UM P ROD
algorithms.
Finally, third variant #DPLL caching, #DPLL-Space , achieves nontrivial time-space tradeoff. algorithm natural variant #DPLL-Cache, modified remove
cached values linear space consumed. algorithm utilizes one additional subroutine. (6) RemoveFromCache(): takes input set formulas (a set components) removes
cache. splitting component variable instantiation computing
value part, #DPLL-Space cleans cache removing sub-components,
value whole component retained. Specifically, #DPLL-Space exactly like
#DPLL-Cache, except calls RemoveFromCache( + ) returning (line 14).
413

fiBACCHUS , DALMAO , & P ITASSI

Theorem 3 solving #S n variables, execution #DPLL-Space uses
space linear instance size runs time bounded 2O(w log n) w underlying
branch width instance. Furthermore, algorithm made deterministic
time space guarantees.
proofs Theorems 13 given appendix.
3.3 Using DPLL Algorithms Instances UM P ROD:
DPLL algorithms described section easily modified solve instances
UM P ROD. However, since #S #P complete many instances UM P ROD also solved
simply encoding #S AT. example, approach readily applicable BAYES
proved empirically successful (Sang et al., 2005b). Furthermore, encoding provided
Sang et al. (2005b) achieves complexity guarantees standard algorithms BAYES.
(That is, CNF encoding tree width greater original Bayes Net). Note
encoding assigns non-uniform probabilities values variables. is, variable x
probability x = 0 might equal probability x = 1. easily accommodated
algorithms: instead multiplying value returned recursive call 21 simply
multiply probability corresponding variable value (i.e., Pr (x = 0) Pr (x = 1)).
hand, conversion #S inapplicable undesirable algorithms
modified solve instances UM P ROD directly. UM P ROD, want compute
L Nm
L
j=1 fj (Ej ). DPLL chooses variable, Xi , value Xi recursively
Xn
X1 . . .
solves reduced problem F|Xi =d . (Hence, instead binary decision tree builds k-ary tree).
reduced problem F|Xi =d compute

X1

...



Xi1 Xi+1

...


MO

fj (Ej )|Xi =d ,

Xm j=1

fj (Ej )|Xi =d fj reduced setting Xi = d. #DPLL-SimpleCache caches solution
reduced problem avoid recomputing it. example, remember reduced problem
remembering original functions F remain (i.e., reduced constant
value) set assignments reduced remaining functions. #DPLL-Cache caches
solution components reduced problem. example, remember component
remembering set original functions form component along set assignments
reduced functions. compute current components finding connected
components primal graph generated hypergraph UM P ROD instance
instantiated variables removed. straightforward adaptation show three
theorems continue hold #DPLL, #DPLL-Cache, #DPLL-Space modified solve UM P ROD.
Algorithm 9 shows #DPLL-Cache, example, modified solve general UM P ROD problems. algorithm takes input set components , like #DPLL-Cache,
initially containing components original problem. algorithm fns(x) denotes set
functions original problem (a) contain x scope, (b) fully instantiated
instantiation x.
414

fiBACKTRACKING EARCH



#SAT BAYES

Algorithm 9: UM P ROD-DPLL-Cache algorithm arbitrary UM P ROD problems
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

UM P ROD-DPLL-Cache ()
begin
InCache()
return GetValue()
else
= RemoveCachedComponents()
choose variable x appears component
p=0
foreach domain x
= ToComponents(|x=d )
Q
= f fns(x) LOOKUP(value f {x = d})
p = p + UM P ROD-DPLL-Cache( {} )
end
AddToCache(, p)
return GetValue()
end

4. Comparing Algorithms BAYES #S
section, prove DPLL based algorithms least powerful standard
complete algorithms solving #S AT, provable powerful many
instances. last feature important means solving UM P ROD using
DPLL augmented caching cases solve problems beyond reach many
standard complete algorithms.
mentioned earlier, algorithms UM P ROD well new DPLL-based algorithms,
actually nondeterministic algorithms require nondeterministically chosen input. (This
input viewed sequence bits). VE, nondeterministic bits encode elimination ordering; RC, nondeterministic bits encode branch decomposition; AND/OR
search nondeterministic bits encode pseudo tree; DPLL based algorithms,
nondeterministic bits encode underlying decision tree indicating variable queried
next backtracking process. Thus comparing power algorithms must
careful nondeterminism resolved. example, operating bad
elimination ordering cannot expected run efficiently #DPLL-Cache operating
good branching strategy. First present definitions allow us state results
precisely.
Definition 7 Let f CNF formula. Define Time[VE](f ) minimal runtime variable elimination algorithm solving #S f , choices elimination orderings f .
Similarly define Time[A](f ), equal RC-Cache, RC-Space, RC-Cache+ , AND/OR-Space,
AND/OR-Cache, AND/OR-Cache+ , #DPLL-Cache, #DPLL-Space. (For example, Time[RCCache](f ) minimal runtime RC-Cache algorithm solving #S f , possible
branch decompositions f .)

415

fiBACCHUS , DALMAO , & P ITASSI

Definition 8 Let B two nondeterministic algorithms #S AT. say
polynomial-time simulates B fixed polynomial p every CNF formula f
Time[A](f ) p(Time[B](f )).
following theorem shows RC-Cache RC-Cache+ polynomially simulate VE.
proof theorem implicit results Darwiche (2001).
Theorem 4 RC-Cache RC-Cache+ polynomially simulate VE.
prove DPLL caching powerful previous algorithms.
Theorem 5 #DPLL-Cache polynomially simulates RC-Cache, RC-Cache+ , AND/OR-Cache,
AND/OR-Cache+ , VE. #DPLL-Space polynomially simulates RC-Space, AND/OR-Space
DDP.5
proof theorem given appendix. noted proof also
implies deterministic version #DPLL-Cache time (and space) complexity least good deterministic realization RC-Cache, RC-Cache+ , AND/ORCache, AND/OR-Cache+ , VE. Similarly, deterministic version #DPLL-Space
time (and space) complexity least good deterministic realization RC-Space,
AND/OR-Space DDP.
prove DPLL caching cases run super-polynomially faster
previous algorithms. proof given appendix.
Theorem 6 None RC-Space, RC-Cache, AND/OR-Cache, AND/OR-Space polynomially simulate #DPLL-Cache, #DPLL-Space, #DPLL.
theorem shows #DPLL-Cache/Space basic advantage standard
algorithms UM P ROD. is, problems RC, AND/OR search, require
time super-polynomially greater #DPLL-Cache matter branch decomposition, pseudo
tree, variable ordering supplied with, even caching utilized. proof
theorem shows advantage #DPLL-Cache arises ability utilize dynamic variable orderings, branch order variables differently. flexibility dynamic
variable ordering instances gives rise increased opportunities contradictions thereby
significantly decreasing overall runtime.
note Theorem 6 cover algorithms flexibility
variable ordering, i.e., AND/OR-Cache+ , RC-Cache+ , DDP. open problem whether
#DPLL-Cache superpolynomially faster algorithms instances, although
conjecture Theorem 6 also true algorithms.
particular, note #DPLL-Cache still greater flexibility variable ordering
algorithms. None algorithms complete flexibility variable ordering.
AND/OR-Cache+ must select uninstantiated variable chain starts root
passed pseudo tree; RC-Cache+ must select uninstantiated variable intersection
labels left right children root passed branch decomposition; DDP must
select uninstantiated variable component currently solving. contrast #DPLLCache select uninstantiated variable.
5. DDP algorithm presented Bayardo Pehoushek (2000).

416

fiBACKTRACKING EARCH



#SAT BAYES

difficulty proving Theorem 6 algorithms tradeoff flexibility variable ordering ability decompose problem. clearest
example occurs AND/OR-Cache+ . AND/OR-Cache+ passed pseudo tree
simply single chain variables, complete flexibility variable ordering,
time never decompose problem. Similarly, RC-Cache+ provided branch
decomposition large labels flexibility variable ordering,
less effective decomposing problem. family problems used prove Theorem 6
flexibility variable ordering needed achieve superpolynomial speedup, thus
example AND/OR-Cache+ achieve speedup completely sacrificing decomposition.
#DPLL-Cache manage tradeoff flexibility variable ordering decomposing problem sophisticated ways. example, ability use variable
ordering encourages decomposition parts search tree using different variable orderings parts search tree. instance, Cachet system, based
#DPLL-Cache, employs heuristic dynamically trades variables ability decompose
problem ability refute current subtree (Sang et al., 2005a). (It employs weighted
average number clauses variable satisfy variables VSID score Moskewicz,
Madigan, Zhao, Zhang, & Malik, 2001). #DPLL-Cache also ability interleave solving
current set components successively choosing variables different components.
extend Theorem 6 cover AND/OR-Cache+ , RC-Cache+ DDP family problems exploiting features #DPLL-Cache would developed.

5. Impact Practice
results paper first presented conference paper (Bacchus, Dalmao, &
Pitassi, 2003), since time number works influenced algorithmic ideas
presented here.
Cachet system (Sang et al., 2004, 2005a) state art #S solver directly based
results presented here. Cachet like #DPLL-Cache algorithm, based ideas
dynamic decomposition components caching component solutions. advance
previous #S solvers use caching remember previously solved components
integration clause learning. previous best #S solver, DDP solver (Bayardo
& Pehoushek, 2000), also performed dynamic component detection neither component
caching clause learning. results highlighted importance component caching
possibility basing #S solver standard DPLL implementation thus making integration
clause learning feasible.
Cachet resolved number issues making algorithms presented practical.
included practical ways implementing caching components including method efficiently computing key could used cache lookup. (This method subsequently
improved Thurley, 2006). Cachet system also used solve BAYES, probable
explanation (MPE), weighted MAX-SAT problems encoding problems weighted
#S problems (Sang et al., 2005b, 2007). approach proved successful, especially BAYES often much superior standard BAYES algorithms. applications
#S Cachet system BAYES advanced Li et al. (2006, 2008).
also noted practical #S solving applications problems like
BAYES also advanced period work RC algorithm application
417

fiBACCHUS , DALMAO , & P ITASSI

compiling CNF representations model counting tractable, e.g., (Darwiche, 2004;
Chavira & Darwiche, 2006, 2008). work also illustrated value converting various
problems weighted #S instances, utilization techniques like clause learning (in
case integrated RC style algorithm). also considerable work advancing AND/OR search, e.g., (Dechter & Mateescu, 2004; Marinescu & Dechter, 2006; Dechter &
Mateescu, 2007).
One difference Cachet system RC AND/OR search based systems mentioned Cachet utilized dynamic decomposition scheme. particular, Cachet used
dynamic variable ordering heuristic attempts trade variables ability decompose
problem ability refute current subtree. variable ordering dynamically
determined search, Cachet cannot predict components generated search.
Hence examine current component (i.e., component containing variable
instantiated) discover new components generated. Thus Cachet utilized approach like
specified Algorithm 8 function like ToComponents invoked newly reduced component (see line 8). ToComponents must linear computation find new components (e.g.,
depth-first search union-find algorithm). addition, component must examine
clauses contained component compute cache key.
contrast, RC AND/OR search take input static precomputed decomposition
scheme (i.e., branch decomposition pseudo tree). Hence, able find components
without extra work search, able efficiently compute cache keys
components. example, AND/OR search, algorithm simply follows supplied
pseudo tree. variable V along variables path root V
instantiated, AND/OR search knows variables subtree rooted child V
forms independent component. Hence, detect components search constant time. Similarly, need examine clauses variables new components
compute cache key. Instead compute cache key node pseudo tree roots
component set instantiations parents root appear clauses
variables component. Note that, set parents whose instantiations relevant
computed search done search look current
values.
Thus, using static decomposition scheme RC AND/OR search gain efficiency
Cachet. However, statically computed decompositions always effective
dynamic scheme employed Cachet. First, useful override precomputed decomposition scheme drive search towards contradictions. gist Theorem 6
shows dynamic flexibility variable ordering provide superpolynomial reductions
size explored search tree better exploiting contradictions. Second, static decompositions cannot account different values variables. is, formula arises
instantiating variable V 0 quite different formula arises instantiating V 1. difference negatively affect performance RC AND/OR search
least couple ways: components might generated predicted static
decomposition scheme thus static scheme might fully exploit decomposition; due
specific changes formula generated particular instantiations, static decomposition
scheme might inappropriate much search space.
practice, Cachet displays performance least good systems built using
RC algorithm, cases performance superior (see empirical results presented
418

fiBACKTRACKING EARCH



#SAT BAYES

Sang et al., 2004, 2005a). also noted #DPLL-Cache easily utilize static
decomposition scheme gain efficiencies schemes. example, provided
pseudo tree #DPLL-Cache follow ordering variables pseudo tree
parents always instantiated children. Like AND/OR search know
children node pseudo tree root independent component, also
able detect components constant time. Furthermore, would able utilize
efficient caching scheme AND/OR search. case advantage AND/OR search would
would freedom interleave solving components.6
recent work algorithms also applied optimization problems (Kitching
& Bacchus, 2008). work involved adding branch bound techniques decomposition
component caching described #DPLL-Cache. branch bound dynamic variable
ordering effective. particular, one wants branch variables drive
value current path towards better value generate global bound
effective pruning rest search space. empirical results Kitching Bacchus
(2008) show added flexibility #DPLL-Cache sometimes yield significant performance improvements AND/OR search even extra flexibility AND/OR-Cache+
exploited.

6. Final Remarks
paper studied DPLL caching, analyzing performance various types
caching #S AT. results apply immediately number instances UM P ROD
problem including BAYES, since #S complete class #P. However, proofs also
modified without much difficulty complexity results apply directly problem
UM P ROD.
sophisticated caching methods also explored solving Beame et al.
(2003) showed methods considerably increase power DPLL.
However, sophisticated caching methods currently practical due large
overheads. related work, one results Aleknovich Razborov (2002) showed
SAT could solved time nO(1) 2O(w) . results extend problem UM P RODas
shown Section 2.1 SAT instance UM P ROD.
proved theoretical point view, #DPLL-Cache efficient terms
time space state-of-the-art exact algorithms UM P ROD. Moreover,
shown specific instances, #DPLL-Cache substantially outperforms basic versions
algorithms. empirical results presented works described Section 5 indicate
advantages often realized practice problems DPLL based
algorithms yield significant performance improvements.
number reasons DPLL based algorithms outperform traditional
algorithms UM P ROD. Algorithms like join tree algorithm (which used many
BAYES inference systems), take advantage global structure interconnections
functions characterized tree width branch width instance. DPLL algorithms
however, also naturally exploit internal local structure within functions.
accomplished instantiating variables reducing functions accordingly. lead
6. Marinescu Dechter (2007) present method searching AND/OR tree best-first manner. method
also interleave solving components, general best-first search exponential space overheads.

419

fiBACCHUS , DALMAO , & P ITASSI

improvements especially functions encoded way expose functions
internal structure, encoding sets clauses (e.g., see Li et al., 2008). two
prominent examples structure exploited DPLL.
First, subproblems might contain zero valued functions. case algorithms
need recurse furtherthe reduced subproblem must value 0.7 corresponding
situation occurs one intermediate functions, Fi , produced summing
variables, value 0 setting inputs. obvious way fully exploiting
situation. achieve gains ignoring parts Fi domain map 0
Fi appears product functions. However, still expend considerable effort
computing intermediate function Fj many whose non-zero values might fact
irrelevant eventually multiplied zero values Fi .
Second, input functions become constant prior variables
set (e.g., clause might become equivalent TRUE one literals become
true), might become independent remaining variables. means subproblems f |xi =1 f |xi =0 might quite different underlying hypergraphs. DPLL-based
algorithms take advantage fact, since work reduced problems separately.
example, algorithms free use dynamic variable orderings, different variable
ordering used solving subproblem. VE, hand, decompose problem
way, hence cannot take advantage structure.
BAYES situation corresponds context-specific independence random variable X might dependent set variables W, Y, Z considering possible assignments variables (so f (X, W, Y, Z) one input functions), W = True
might X becomes independent (i.e., f (X, W, Y, Z)|W =1 might function F (X, Z)
rather F (X, Y, Z)). Previously ad-hoc methods proposed (Boutilier, Friedman,
Goldszmidt, & Koller, 1996) take advantage kind structure.
noted however, problems functions little internal structure
significantly efficient algorithms (RC, AND/OR search
DPLL algorithms). uses simple multiplication summation operations
overheads involved instantiating variables exploring AND/OR search
tree backtracking tree.
RC AND/OR search share advantages VE. However,
much flexibility DPLL algorithms. shown Theorem 6 fully exploiting
zero valued functions instances require dynamic variable orderings lie outside
range basic versions RC AND/OR search. Although proof cover
enhanced versions RC AND/OR (RC-Cache+ AND/OR-Cache+ ), pointed
even versions flexibility DPLL algorithms. practice,
empirical evidence provided Cachet system (Sang et al., 2004, 2005a) branch
bound system described Kitching Bacchus (2008) support belief added
flexibility important practice.
exploitation context-specific independence also poses problems RC AND/OR
search algorithms. particular, static decomposition schemes employ incapable
fully exploiting structureas pointed underlying hypergraphs subproblems arising different instantiations radically different. However, although DPLL
7. #S corresponds situation clause becomes empty.

420

fiBACKTRACKING EARCH



#SAT BAYES

algorithms principle able exploit structure, remains open problem find practical
ways accomplishing this. Specifically, decomposition scheme computed prior search
sophisticated (and computationally complex) algorithms utilized. difficult overcome
overhead methods used dynamically search (although see Li
van Beek 2004 work direction). development methods light weight
enough use search still effective selecting decomposition promoting variables
remains open problem.
Finally, shown proof Theorem 5, RC AND/OR search possess intrinsic
advantages DPLL algorithms except perhaps conceptually simplicity. proof shows
DPLL algorithms simulate RC AND/OR search way additional
computation required. Furthermore, pointed Section 5 algorithms also able
utilize static decomposition schemes obtaining efficiency gains RC AND/OR
search.
Recently, several papers (Sanner & McAllester, 2005; Mateescu & Dechter, 2007) made
significant progress developing compact representations functions (rather tabular
form), thereby potentially enhancing algorithms discussed paper (VE, RC, etc.)
allowing exploit additional local structure within functions. interesting future step
would combine unique dynamic features #DPLL-Cache one promising
compact function representations try improve UM P ROD algorithms.
Acknowledgments research funded governments Ontario Canada
NSERC PREA programs. results paper presented earlier conference paper (Bacchus et al., 2003). thank Michael Littman valuable conversations.

Appendix A. Proofs
A.1 Lemmas Relating Branch Width, Tree Width, Elimination Width
Lemma 2 Let H = (V, E) hypergraph tree-decomposition width w.
ordering vertices V induced width H w.
Proof: Let H = (V, E) hypergraph tree width w let Ttd tree decomposition
achieves width w. is, maximum sized label Ttd size w + 1. assume
without loss generality labels leaves Ttd one-to-one correspondence
edges H. arbitrary node Ttd , let label (m) set vertices label
m, tree rooted m, vertices(m) union labels leaf nodes
(i.e., hyperedges H appearing ), depth(m) distance root.
Let x vertex H, let leaves(x) set leaves Ttd contain x
label. define node(x) deepest common ancestor Ttd nodes leaves(x),
depth vertex, depth(x), depth(node(x)). Note x label (node(x)), since
path left-most leaf leaves(x) right-most leaf must pass node(x);
x appear label node outside subtree rooted node(x), since
leaf outside subtree contains x.
Finally let = x1 , . . . , xn ordering vertices depth(y) < depth(x),
must precede x ordering. use notation < x indicate precedes x
421

fiBACCHUS , DALMAO , & P ITASSI

ordering (and thus eliminated x). claim induced width
width Ttd , i.e., w.
Consider Anode(x) , subtree rooted node(x), vertices(node (x)), union labels
leaves Anode (x) . make following observations vertices.
1. vertices(node (x)) < x, labels node(x) node(y) must ancestor
node(x) (or equal). < x implies depth(y) depth(x). must path
leaf Anode(x) containing node(y), since node (y) least high node(x)
path must go node(x) (or must node(x) = node(y)). either case
label (node(x)).
2. vertices(node(x)) > x node(y) must lie inside Anode(x) node(y)
must descendant node(x) (or equal). > x implies depth(y) depth(x).
must path leaf containing node(y), since node(y)
least deep node(x) must either path node(y) node(x),
node(y) = node(x).
Note condition 2 implies > x appears subtree node(x),
hyperedges original hypergraph H containing must also subtree
node(x).
claim hyperedge produced stage elimination process xi eliminated
contained label (node(xi )). Since size set bounded w + 1, thus verify
induced width bounded w (note hyperedge produced elimination
contain xi label (node(xi )) does).
base case x1 eliminated. hyperedges containing x1 contained subtree node(x1 ), thus hyperedge created x1 eliminated contained vertices(node (x1 )).
vertices vertices(node (x1 )) follow x1 ordering must label
node(x1 ) vertices(node(x1 )) label (node(x1 )).
xi eliminated two types hyperedges might unioned together: (a)
hyperedges containing xi part original hypergraph H, (b) hyperedges containing xi produced x1 , . . . , xi1 eliminated. original hyperedges, among leaves node(xi ), thus contained vertices(node(xi )).
new hyperedge produced eliminating one previous variables, say variable y,
hyperedge produced contained label (node (y)) induction, turn contained
vertices(node(y)). subtree node(x) get hyperedge contained
vertices(node(x)) since superset vertices(node (y)). Otherwise, node(y) lies another
part tree, label cannot contain x (no node outside subtree node(x) x
label). Thus hyperedge created eliminated also cannot contain xi .
sum hyperedge created xi eliminated contained vertices(node(xi )), since
hyperedges containing xi stage set. Furthermore, vertices x1 , . . . , xi1
removed hyperedge, thus contains variables following xi ordering. Hence,
(1) hyperedge contained label (node(xi )). 2
Lemma 3 Let H hypergraph elimination width w. H tree-decomposition
tree width w.
422

fiBACKTRACKING EARCH



#SAT BAYES

Proof: (Proof Lemma 3) Let = x1 , . . . xn elimination ordering H.
construct tree decomposition H using follows. Initially, |E| trees, size
1, one corresponding edge e E. first merge trees containing xn bigger tree,
Tn , leaving us new, smaller set trees. merge trees containing xn1
bigger tree, Tn1 . continue way formed single tree, . fill
labels intermediate vertices tree tree-decomposition. is,
n two leaves contain vertex v, every node along path
n must also contain v label. hard see xi , tree Ti (created
merging trees containing xi ) property label root (which connects
rest ) contained ei xi , ei hyperedge created xi eliminated.
Basically, nodes xj , j > i, already contained Ti xi need label Ti root.
Furthermore, xj j < contained Ti root label, xj must original
hyperedge variable xk k i: thus xj would appeared hyperedge ei generated
xi eliminated.
Hence tree width final tree larger induced width . 2
A.2 Complexity Results Caching Versions DPLL
proof theorems 1 2 need common notation definitions. Let f
k-CNF formula n variables clauses, let H underlying hypergraph associated
f branch width w. results Darwiche (2001), branch decomposition
H depth O(log m) width O(w). Also results Robertson Seymour (1995),
possible find branch decomposition, Tbd , Tbd branch width O(w) depth
O(log m), time nO(1) 2O(w) . Thus main goal three theorems prove
stated time space bounds DPLL-based procedures, run static
ordering easily obtainable Tbd .
Recall leaves Tbd one-to-one correspondence clauses f .
number vertices Tbd according depth-first preorder traversal Tbd . vertex numbered
i, let fi denote subformula f consisting conjunction clauses corresponding
leaves tree rooted i. Let Vars(fi ) set variables (sub)formula fi . Recall
branch decomposition label vertex i, label (i), set variables
intersection Vars(fi ) Vars(f fi ). node Tbd partitions clauses f three
sets clauses: fi , fiL , fiR , fiL conjunction clauses leaves Tbd
left fi , fiR conjunction clauses leaves right fi .
DPLL caching algorithms achieve stated run time bounds querying variables specific, static variable ordering. is, branch DPLL decision tree,
DT , variables instantiated order. (In contrast dynamic variable ordering allows DPLL decide variable query next based assignments
made before. Thus different branches query variables different order.). variable
ordering used DT determined depth-first pre-ordering vertices branch decomposition Tbd labeling vertices. Let (i, 1), . . . , (i, ji ) denote variables
label (i) appear label earlier vertex Tbd . Note since width Tbd
w, ji w i. Let 1, . . . , z sequence vertex numbers Tbd . DPLL algorithm query variables underlying f following static order: = h(i1 , 1), (i1 , 2), . . . ,
(i1 , j1 ), (i2 , 1), . . . , (i2 , j2 ), . . . , (is , 1), . . . , (is , js )i i1 < i2 < . . . < z, j1 , . . . , js w.
423

fiBACCHUS , DALMAO , & P ITASSI

Note vertices Tbd , nothing queried since variables label may
occurred labels earlier vertices. notation allows vertices skipped.
underlying complete decision tree, DT , created DPLL algorithms input f thus
tree j1 + j2 + . . . + js = n levels. levels grouped layers, ith layer
consisting ji levels. Note 2l nodes level l DT , identify particular
node level l (l, ) particular assignment first l variables ordering,
((q, r), ), (q, r) lth pair ordering , before.
DPLL algorithms carry depth-first traversal DT , keeping formulas cache
already solved along way. (For #DPLL-SimpleCache, formulas stored
cache form f | , #DPLL-Cache #DPLL-Space, formulas stored
various components ToComponents(f | ).) algorithm ever hits node formula
computed already solved, avoid computation, thus
complete depth-first search DT rather depth-first search pruned version DT .
theorems, want get upper bound size pruned tree actually searched
algorithm.
Theorem 1 solving #S n variables, execution #DPLL-SimpleCache
runs time bounded 2O(w log n) w underlying branch width instance. Furthermore, algorithm made deterministic time guarantees.
Proof: want show size subtree DT searched #DPLL-SimpleCache
2O(w log n) . backtracking particular node (l, ) = ((q, r), ) level l DT ,
formula put cache, already known, form f | . (Recall setting
first l variables.) However, see although 2l different ways set , number
distinct formulas form actually much smaller 2l . Consider partial assignment, ,
set variables including (q, r), q r jq .
number variables set (the length ) j1 + j2 + . . . + jq1 + r.
Let denote partial assignment consistent variables
came labels vertices path root Tbd including vertex q
set. idea reduction , removed assignments
irrelevant fq fqR .
Consider happens DPLL algorithm reaches particular node ((q, r), ) level
l DT . point algorithm solving subproblem f | , thus, backtrack
node, f | = fqL | fq | fqR | placed cache, already known. Note
variables subformula fqL set , thus either fqL | = 0, case nothing new
put cache, fqL | = 1 case f | = fq | fqR | = fq | fqR | put
cache. Thus, set distinct subformulas placed cache level l = (q, r)
set subformulas form fq | fqR | , setting variables labels
root vertex q, plus variables (q, 1), ..., (q, r). w variables,
q depth Tbd (each label w variables since width Tbd ). Hence
total number 2(wd) . implies number subtrees DT
level l + 1 actually traversed #DPLL-SimpleCache 2 2wd = 2O(wd) ,
depth node q Tbd . Let number nodes DT actually traversed
#DPLL-SimpleCache. Then, n2O(wlog n) , since sum number nodes
visited every level DT node q Tbd O(log m) = O(log n).
424

fiBACKTRACKING EARCH



#SAT BAYES

Accounting time search cache, overall runtime #DPLL-SimpleCache
t2 , number nodes DT traversed algorithm. Thus,
#DPLL-SimpleCache runs time (n2O(wlog n) )2 = 2O(wlog n) . 2
Theorem 2 solving #S n variables, exists execution #DPLL-Cache runs
time bounded nO(1) 2O(w) w underlying branch width instance. Furthermore,
algorithm made deterministic time guarantees.
Proof: prove theorem placing bound number times #DPLL-Cache
branch variable xl . Using notation specified above, xl corresponds pair (q, r)
ordering used #DPLL-Cache. is, xl rth new variable label vertex q
branch decomposition Tbd .
#DPLL-Cache utilizes static ordering , branches on, queries, variables
according order, always reducing component containing variable xi currently due queried. However, since previously cached components always removed (by
RemoveCachedComponents algorithm), variable xi turn
queried, component among active components contains xi . case, #DPLLCache simply moves next variable ordering, continuing advance finds
first variable appear active component. branch variable
reducing component appears in, leaving components unaltered.
implies time #DPLL-Cache selects xl variable next branch
must case (1) xl appears active component. particular value component
already cache. (2) variable prior xl ordering appears active
component. variables either assigned particular value previous recursive
invocations, component appeared removed value already
cache.
branch decomposition Tbd let p qs parent (q must parent since root
empty label). claim whenever #DPLL-Cache selects xl next variable branch
on, active component containing xl must component reduction fp whose form
determined solely settings variables p r variables q already
set. case, 2(w+r) = 2O(w) different components xl
appear in, hence #DPLL-Cache branch xl 2O(w) times time one
components gets stored cache.
prove claim. label q consists variables appearing ps label variables
appearing label qs sibling. Since variables label (p) set, q
sibling must identical set unqueried variables labels. Hence, q must
left child p time right child visited ordering, xl already
queried. Thus, time xl queried, fp affected current setting
label (p) (as variables shares rest formula) first r queried
variables label (q). is, fp 2(w+r) different configurations, thus
component containing xl also many different configurations.
Thus n variables obtain bound number branches decision tree explored
#DPLL-Cache n2O(w) . proof previous theorem, overall runtime
quadratic number branches traversed, give claimed bound nO(1) 2O(w) . 2

425

fiBACCHUS , DALMAO , & P ITASSI

Theorem 3 solving #S n variables, execution #DPLL-Space uses
space linear instance size runs time bounded 2O(w log n) w underlying
branch width instance. Furthermore, algorithm made deterministic
time space guarantees.
Proof: proof, natural work tree decomposition rather
branch decomposition.
Let f k-CNF formula n variables clauses let H underlying hypergraph associated f . begin tree decomposition Ttd depth O(log m) width
O(w) (computable time nO(1) 2O(w) ). assume without loss generality leaves
Ttd one-to-one correspondence clauses f . node Ttd partitions f
three disjoint sets clauses: fi , conjunction clauses leaves subtree Ttd rooted
i, fiL , conjunction clauses leaves Ttd left fi , fiR , conjunction
clauses leaves Ttd right fi . #DPLL-Space query variables associated
labels Ttd according depth-first preorder traversal. Let variables label (i) appearing earlier label path root node denoted S(i) = (i, 1), . . . , (i, ji ).
non-leaf node j k left right children, variables S(i)
exactly variables occur fj fk occur outside fi . let c
total number nodes Ttd , #DPLL-Space query variables underlying f
following static order: S(1), S(2), . . . , S(c), S(i) may empty. underlying
decision tree, DT , created #DPLL-Space complete tree n levels.
identify particular node level l DT = (l, ) particular assignment
first l variables ordering, = ((q, r), ) (the r th variable S(q)).
#DPLL-Space carries depth-first traversal DT , storing components formulas
cache solved. However, components formulas also popped cache
total space ever utilized linear. algorithm hits node components
formula computed known, avoid traversing subtree rooted node.
Thus searches pruned version DT .
(pruned) depth-first traversal DT , edge traversed traversed twice,
direction. given time traversal, let E = E1 E2 set edges
traversed, E1 edges traversed forward direction,
E2 edges traversed directions. edges E1 constitute partial
path p starting root DT . edge p labeled either 0 1. Let p1 , . . . , pk
set subpaths p (beginning root) end 1-edge. Let 1 , . . . , k subrestrictions
corresponding p1 , . . . , pk except last variable originally assigned 1
assigned 0. example, p (x1 = 0, x3 = 1, x4 = 0, x5 = 1, x6 = 0, x2 = 0),
1 = (x1 = 0, x3 = 0), 2 = (x1 = 0, x3 = 1, x4 = 0, x5 = 0). information
cache time contains ToComponents(f |i ), k.
node q Ttd corresponding subformula fq , context fq set variables
defined follows. Let (q1 , . . . , qd ) denote vertices Ttd path root q (excluding q itself). context fq set Context (fq ) = S(q1 ) S(q2 ) . . . S(qd ).
Intuitively, context fq set variables queried nodes lie along path
q. Note reach level l = (q, 1) DT , first variable S(q) queried,
already queried many variables, including variables Context(fq ). Thus set
variables queried level l = (q, 1) partitioned two groups relative fq :
426

fiBACKTRACKING EARCH



#SAT BAYES

irrelevant variables, set Context(fq ) relevant variables. claim arbitrary
level l = (q, r) DT , nodes level l actually traversed nodes ((q, r), )
irrelevant variables (with respect fq ) set 0. total number nodes
level l = (q, r) 2|Context(fq )|+r 2w log n . Since true
levels, total number nodes DT traversed bounded n2w log n . Thus,
remains prove claim.
Consider node = ((q, r), ) DT . is, = 1 2 . . . q1 b1 . . . br1 ,
i, assignment variables S(i), b1 . . . br1 assignment first r 1
variables S(q). Let context fq S(q1 ) . . . S(qd ), log n. suppose
assigns 1 non-context (irrelevant) variable, say first assignment occurs ut ,
tth variable u , u q 1. want show algorithm never traverses s.
Associated partial path DT ; also call partial path . Consider
subpath/subassignment p including ut = 1. traversed, start
traversing p. Since last bit p 1 (i.e., ut = 1) get point, stored
cache ToComponents(f | ) exactly like p except last bit, ut , zero. Let
j first node q1 , q2 , . . . qd property set variables S(j) queried
p. (On path q Ttd , j first node along path variables S(j)
queried p.) ToComponents(f | ) consists three parts: (a) ToComponents(fjL | ), (b)
ToComponents(fj | ), (c) ToComponents(fjR | ).
consider path p extends p way DT , p shortest subpath
variables S(i) < j queried. restriction corresponding p
refinement p variables S(1)S(2). . . S(j1) set. Since already set everything occurs j, go beyond p component ToComponents(f |p )
already cache. ToComponents(f |p ) consists three parts: (a) ToComponents(fjL |p ),
(b) ToComponents(fj |p ), (c) ToComponents(fjR |p ). set everything occurs j, formulas (a) known. Since p agree variables relevant
fj , ToComponents(fj |p ) = ToComponents(fj | ) hence formulas (b) cache.
Similarly formulas (c) cache since ToComponents(fjR |p ) = ToComponents(fjR | ).
Thus components ToComponents(f |p ) cache, hence shown
never traverse beyond p hence never traverse s. Therefore total number nodes traversed
level l = (q, r) 2wd , depth q Ttd , desired. yields
overall runtime 2O(w log n) .
left argue space used linear instance size. total number formulas
ever stored cache simultaneously linear depth tree decomposition,
O(log m). Since store restricted formula f | storing associated restriction
, total space ever used O(n log m), linear input size. 2
A.3 Comparing Algorithms BAYES #S
proving next theorem, first discuss detail structure search space
explored various versions RC, AND/OR search DDP. algorithms operate
way. instantiate variables problem decomposes independent
components solve components separate recursions. Hence, solving CNF
formula f generate AND/OR search tree (Dechter & Mateescu, 2007).
427

fiBACCHUS , DALMAO , & P ITASSI

AND/OR search tree AO generated one algorithms solves #S
instance f (a CNF formula), rooted tree. node n AO labeled formula n.f
subtree n generated solving n.f . root A0 labeled original formula
f . four different types nodes AO:
Query nodes. query node q associated variable q.var two children corresponding
two possible instantiations q.var . is, children labeled formulas
q.f |q.var =0 q.f |q.var =1 . query node q generated search algorithm whenever
chooses instantiate q.var executes recursive calls two resultant reduced
formulas.
nodes. node, a, query node parent, one children
query nodes. node generated search algorithm
decomposes current formula two independent components following instantiation parent query nodes variable. components solved
one subtrees rooted nodes children. a.f splits components
V
fi , = 1, . . . , k, a.f = fi , ith child labeled fi . Note fi
share variables. Hence, set query node variables appear subtree
i-th child disjoint set query node variables appearing j-th
child j 6= i.
Failure nodes. leaf nodes tree labeled formula containing empty
clause. caching used, failure nodes might also labeled formula cache
already shown unsatisfiable.
Satisfying nodes. leaf nodes tree labeled formula containing
clauses. caching used, satisfying nodes might also labeled satisfiable
formula cache whose model count already know.
Figure 6 shows example AND/OR search tree.
node n AO also value, n.value, computed algorithm generates it.
need distinguish zero values n.value = 0, non-zero values denoted
n.value = 1. Every satisfying node value 1, every failure node value 0. query
node value 1 least one children value 1, node value
1 children value 1. example, Figure 6 node value
0, query node 2 value 1. Note children node AO must
value 1 except possibly right child. algorithms generating AO terminate search
node soon discover value 0 childthis implies node
value 0. seen n.value = 0 n.f unsatisfiable n.value = 1 n.f satisfiable.
Given node n AO, let AO(n) AND/OR subtree AO rooted n. satisfying
assignment ns formula n.f defines solution subtree S(n) AO(n). particular, S(n)
connected subtree AO(n) rooted n (1) q query node S(n) S(n)
also contains child q corresponding assignment made (i.e., [q.var ] value
assigned q.var , S(n) contain child labeled formula q.f |q.var =[q.var] ),
(2) node S(n) S(n) contains children a, (3) contains failure
nodes. example, solution subtree AND/OR tree shown Figure 6 (i.e., solution
subtree root node) formed leaf nodes b, c, f, l; query nodes 1, 2, 3, 4, 5, 6,
428

fiBACKTRACKING EARCH



#SAT BAYES

1

11

2



16



n
3

8

6

12

13

!

E

u
7

B

!

C

e
4

5

c





17

!

!



j

19

p
q

h

15

18

g

!

b



10

9

f


14

l

r



!



x



z

!

v

k

w

F il
Failure
node

!

Satisfying node
Query node
node


Figure 6: example AND/OR search tree query nodes numbered 119, leaf nodes (both
failure satisfying) numbered az, nodes labeled AE.

7, 8; nodes A, B. particular, left value query nodes 1, 2, 3, 5, 6, 7
8, along right value query node 4 satisfy clauses formula 1.f . solution
subtree AO(n) exists n.value = 1.
Finally, AND/OR search tree say query node whose parent node
component root. also classify root node component root. Figure 6 query nodes 1
(the root node), 3, 6, 8, 4, 5, 9, 10, 12, 13, 17, 19 component roots.
Theorem 5 #DPLL-Cache polynomially simulates RC-Cache, RC-Cache+ , AND/OR-Cache,
AND/OR-Cache+ , VE. #DPLL-Space polynomially simulates RC-Space, AND/OR-Space
DDP.
Proof: Since RC-Cache polynomially simulates ignore proof: showing
#DPLL-Cache polynomially simulates RC-Cache also shows polynomially simulates VE.
Also assume proof algorithms use unit propagation,
#DPLL-Cache/Space. explained Section 3.1, #DPLL-Cache/Space without unit propagation
polynomially simulate versions #DPLL-Cache/Space using unit propagation.
429

fiBACCHUS , DALMAO , & P ITASSI

stated algorithms generate AND/OR search tree solving CNF formula
f . prove theorem first show AND/OR search tree solving f converted
partial DPLL decision tree, DT , bigger. show DPLL algorithms
solve f using DT guide variable ordering. Thus, obtain result minimal
runtime stated algorithms, must result generation AND/OR
search tree AOmin , also achieved DPLL algorithms. particular, run
partial decision tree constructed AOmin , DPLL algorithms achieve polynomially
similar runtime. (This suffices prove theorem, need show existence
execution DPLL algorithms achieving run time.)
make distinction AND/OR search tree constructed partial decision
tree clear, use suffixes ao dt indicate elements AND/OR tree decision
tree respectively.
DPLL decision trees contain query variables, satisfying nodes, failure nodes,
satisfying failure nodes leaf nodes. construct partial decision tree DT
AND/OR tree AO expanding left solution subtree S(nao ) every node nao AO
nao .value = 1 linear sequence query variables DT using depth-first ordering
query variables S(nao ). nodes nao AO nao .value = 0 expansion
attempted, case result sequence query nodes terminate failure nodes.
Every node qdt DT pointer, dtao(qdt ) node qao AO, end construction pointers establish map nodes DT nodes AO. Initially, root
DT pointer root AO. Then, node qdt DT :
1. dtao(qdt ) query node qao AO, make qdt query node create left
right child, ldt rdt , qdt DT . make qdt query variable qao (i.e.,
qdt .var = qao .var ), set children point children qao (i.e., dtao(ldt )
dtao(rdt ) set left right children qao AO).
2. dtao(qdt ) node aao AO, reset dtao(qdt ) left child
aao AO. apply first rule above, continue.
3. dtao(qdt ) failure node AO set qdt failure node. case qdt
children.
4. dtao(qdt ) satisfying node AO examine path ao AO root
dtao(qdt ). Let rao last component root ao right sibling.
(a) rao exists, node path rao dtao(qdt ) AO right
child query node whose left child value 1, reset dtao(qdt )
leftmost right sibling rao . node also component root, hence query
node AO. apply first rule above, continue.
(b) Otherwise (either rao exist node path rao
right child query node whose left child value 1), make qdt satisfying
node. case qdt children.
Rule 4 construction convert leftmost solution subtree node nao
AO sequence query nodes DT performing depth-first traversal solution
subtree. particular, solution subtree leftmost right sibling deepest component
430

fiBACKTRACKING EARCH



#SAT BAYES

1

11

2

3

16

12

n
4

!

13o

17

p

e
14

5b

u

15

18



x
6c


7

!

!

q

r

19v





w

9


!

8f

g

z

10i

h
!

l



j

k
Failure node
!

Satisfying node
Query node

Figure 7: partial DPLL decision tree constructed AND/OR search tree Figure 6.
query leaf nodes n numbered number corresponding node,
dtao(n), AND/OR search tree.

root depth-first successor satisfying leaf node. condition node route
sibling right child query node whose left child value 1 ensures
perform depth-first traversal along leftmost solution subtree along subsequent solution
subtrees. Figure 7 shows partial decision tree would constructed AND/OR
search tree Figure 6.
diagram, satisfying nodes whose pointers reset next component root using
rule 4a, numbered corresponding query node AND/OR tree followed
leaf label corresponding satisfying node. example, node 5b Figure 7 represents
satisfying child b node 4 Figure 6 redirected depth-first successor node 5
(the leftmost right sibling deepest component root 4).
another example, AND/OR tree, right child node 6 node C. Hence,
decision tree, right child corresponding query node 6, becomes query node 9
leftmost child node C (rule 2). Furthermore, reach satisfying node j AND/OR
tree, proceed hence left child query node 10 decision tree becomes
terminal satisfying node (rule 3). particular, although path root node 10
AND/OR tree contains component root right sibling, namely node 6, path also contains
node C right child query node (node 6) whose left child (node 7) value 1.
431

fiBACCHUS , DALMAO , & P ITASSI

two things note. First, node ndt DT variables instantiated path
ao A0 root dtao(n) instantiated values path dt
DT root ndt . Since Rules 3 4b terminate paths, nodes dt inserted
Rules 1, 2, 4b. Rules 1 2 insert nodes dt whose parents already dt ,
Rule 1 ensures values assigned AO. Finally, Rule 4a inserts
node adt dt one dtao(adt )s siblings already dt , hence siblings (and as)
parent must already dt .
Second, variable queried twice along path DT . is, node ndt DT
ancestor ndt ndt .var = ndt .var . path dt DT grown applications
Rules 1, 2, 4a. Since path AO queries variable twice, Rules 1 2 must
preserve condition. Similarly Rule 4a moves new component root aao , set query
variables aao AO disjoint set query variables already appearing dt .
Using above, AND/OR search tree AO generated algorithms RCSpace, AND/OR-Space DDP solving formula f , construct corresponding
partial decision tree DT . show #DPLL-Space solve f exploring search tree
larger DT . Note DT larger AO, hence show
#DPLL-Space solve f polynomially similar run time, proving polynomially
simulate RC-Space, AND/OR-Space DDP. (Note run time algorithms
polynomially related size search trees explore.)
execute #DPLL-Space using variable ordering specified DT . is, starting
root rdt DT , #DPLL-Space always query variable current node DT , ndt .var ,
descend ndt left child. backtracks ndt descend right child.
Hence, need show #DPLL-Space must backtrack reaches leaf DT . is,
explores search tree larger DT .
First, #DPLL-Space reaches failure node DT must detect empty clause backtrack. Rule 3 construction failure node fdt DT must correspond failure node
dtao(fdt ) AO. Since variables instantiated path AO root dtao(fdt )
instantiated values path DT root fdt , see empty
clause detected AO dtao(fdt ) #DPLL-Space must also detect empty clause
fdt . (Note algorithm generated AO used unit propagation, assume
#DPLL-Space well).
Second, #DPLL-Space reaches satisfying node sdt DT must detect current
set components solved backtrack (line 4 Algorithm 8). Let dt path DT
root sdt , ao path AO dtao(sdt ) root, crdt node dt
dtao(crdt ) component root AO (we say crdt component root dt ).
claim (a) lao left sibling dtao(crdt ) AO, exists node ldt dt
dtao(ldt ) = lao , lao .f satisfied dt ; (b) rao right sibling dtao(crdt )
AO rao .f #DPLL-Spaces cache.
Given claim (a) clauses original formula yet satisfied dt clauses
rao .f nodes rao AO right siblings component root crdt dt (i.e.,
rao right sibling component root dtao(crdt ) AO). #DPLL-Space arrived crdt ,
prior reaching sdt , variables AO path root dtao(crdt ) already
instantiated values dt . Thus, pao dtao(crdt )s parent AO, #DPLLSpace would recognized rao .f separate component instantiated pao .var ,
would added rao .f list components (at line 8 11 Algorithm 8). Note that,
432

fiBACKTRACKING EARCH



#SAT BAYES

solved rao .f would removed #DPLL-Spaces cache backtracks undo
instantiation pao .var . (At point solution pao children would combined
yield solution pao .f ).
Furthermore, following variable ordering specified DT , #DPLL-Space would instantiate variables r.f along path dt . Hence, component #DPLL-Spaces
list components reaches ns must equal rao .f right sibling rao component root dt , claim (b) removed call RemoveCachedComponents()
(line 6). leave #DPLL-Space empty list components solve, hence must
backtrack sdt .
prove claims. (a) see DT always visit children node
AO left right order. is, inserting component root crdt path, must
first visit left siblings lao dtao(crdt ). inserting ldt path (with dtao(ldt ) = lao ),
instantiate ldt start query nodes lao searching alternate instantiations
variables able traverse leftmost solution subtree AO(lao ). traversal
results insertion path solution lao .f , DT inserts crdt path
using Rule 4a.
(b) observe sdt satisfying node DT application Rule 4b.
Hence two possible cases. First, none component roots dt
right sibling. case every clause original formula satisfied #DPLL-Space must
backtrack. example, Figure 7 occurs leaf nodes l y.
Otherwise, let crdt component root dt dtao(crdt ) right sibling AO,
let ndt first node dt following crdt (i) ndt successor dt right
child, (ii) dtao(ndt ) left child AO value 1. node ndt must exist, else sdt
would leaf node DT Rule 4a. #DPLL-Space arrived node crdt
would rao .f list components right siblings rao dtao(crdt ). might
also unsolved components list. components, however, must equal
rao .f right sibling rao component root dt preceding crdt , must
placed list components prior #DPLL-Space reaching crdt . Then, #DPLL-Space
arrived ndt would taken left branch first. Thus would previously invoked
right sibling components component list.
#DPLL-Space invoked list components either solves every component,
placing cache keeping backtracks node
first placed list, discovers one components unsatisfiable. one
components unsatisfiable, immediately backtrack point component
first placed list. particular, recursive calls list components contains known
unsatisfiable component return immediately since call InCache() detect
list components product equal zero.
Hence, taking left branch ndt , #DPLL-Space, list components,
components form rao .f right siblings component roots ndt dt , also lao .f
lao left child dtao(ndt ) AO. Since lao value 1, lao .f satisfiable, either
#DPLL-Space solve components, placing value cache, discover
one components rao .f unsatisfiable backtrack without visiting sdt . Therefore,
visit sdt would solved components could potentially list components,
components would still cache since placed list arriving
sdt .
433

fiBACCHUS , DALMAO , & P ITASSI

shows #DPLL-Space polynomially simulates RC-Space, AND/OR-Space DDP.
RC-Cache AND/OR-Cache gain RC-Space AND/OR-Space solve
components once. is, arrive node nao generated
AND/OR tree AO, nao .f solved immediately backtrack.
#DPLL-Cache gains efficiency #DPLL-Space. particular, need never solve
component once. Using caching removing previously solved components
list components gives rise savings realized adding caching
AND/OR RC. Formally, construction partial decision tree DT used.
AO mark nodes search terminated cache hit satisfying node (if cached
formula satisfiable) failure node (if cached formula unsatisfiable). Now, example,
nodes satisfying failure nodes children components
solved before. Applying construction AO gives rise partial decision tree DT ,
shown #DPLL-Cache using DT guide variable choices explore search
tree size DT . proves #DPLL-Cache polynomially simulates
RC-Cache AND/OR-Cache.
subtle point #DPLL-Cache might solve component point
search. particular, component first appears #DPLL-Caches list components
previously added unsatisfiable component, #DPLL-Cache backtrack without solving .
Following DT , #DPLL-Cache enough work find first solution,
proceed components list. search first solution, cache
unsatisfiable reductions found search. Thus, next time encounters
follow variable ordering extra work: cached unsatisfiable reductions
immediately prune paths leading failure proceed directly first solution
. components list satisfiable, eventually backtrack first
solution continue solve . Hence, although #DPLL-Cache might encounter many
times solving it, encounter, except first, require adding search tree
number nodes linear number variables . number nodes added
first encounter, first solution found, encounter finally solves
, together equal number nodes required AO solve . Hence, encounters without
solving increase size #DPLL-Caches search tree polynomial.
Finally, note construction given accommodates use dynamic variable orderings
order variables varies branch branch AND/OR search tree. (Varying
value assigned along left right branch query variable also accommodated).
is, proof also shows #DPLL-Cache polynomially simulates AND/OR-Cache+
RC-Cache+ . 2
Theorem 6 None RC-Space, RC-Cache, AND/OR-Cache, AND/OR-Space polynomially simulate #DPLL-Cache, #DPLL-Space, #DPLL.
prove theorem first observe result Johannsen (Johannsen, 2001),
#DPLL-Cache, #DPLL-Space, #DPLL solve negation propositional stringof-pearls principle (Bonet, Esteban, Galesi, & Johannsen, 1998) time nO(log n) , run
dynamic variable ordering. prove (in Theorem 7) algorithms require
time exponential n problem. Hence, none algorithms polynomially simulate
#DPLL (or stronger #DPLL-Space #DPLL-Cache).
434

fiBACKTRACKING EARCH



#SAT BAYES

string-of-pearls principle, introduced different form Clote Setzer (1998)
explicitly Bonet et al. (1998) follows. bag pearls, colored red
blue, n pearls chosen placed string. string-of-pearls principle says first
pearl string red last one blue, must red-blue blue-red pair
pearls side-by-side somewhere string. negation principle, Sm,n , expressed
variables pi,j pj [n] j [m] pi,j represents whether pearl j mapped
vertex string, pj represents whether pearl j colored blue (pj = 0) red (pj = 1).
clauses SPm,n follows.
(1) hole gets least one pearl:
j=1 pi,j , [n].
(2) hole gets one pearl: (pi,j pi,j ), [n] j [m] ,j [m], j 6= j .
(3) pearl goes one hole: (pi,j pi ,j ), [n], [n], 6= , j [m].
(4) leftmost hole gets assigned red pearl rightmost hole gets assigned blue pearl:
(p1,j pj ) (pn,j pj ), j [m].
(5) two adjacent holes get assigned pearls color: (pi,j pi+1,j pj pj ),
1 < n, j [m], j [m], j 6= j , (pi,j pi+1,j pj pj ), 1 < n, j [m],
j [m], j 6= j .
Johannsen (Johannsen, 2001) shows SPn,n quasipolynomial size tree resolution proofs.
follows #DPLL, #DPLL-Space #DPLL-Cache solve SPn,n quasipolynomial time.
Lemma 4 (Johannsen, 2001) SPn,n solved time nO(log n) #DPLL, #DPLL-Space,
#DPLL-Cache.
Theorem 7 Let = 1/5. algorithms RC-Space, RC-Cache, AND/OR-Cache, AND/OR
Space, VE, #DPLL-Cache using static variable ordering, require time 2n solve SPn,n .
Proof: seen proof Theorem 5 #DPLL-Cache using static variable
ordering polynomially simulate stated algorithms.

Hence, suffices prove #DPLL-Cache static ordering requires time 2n
SPm,n , = n. static ordering, mean variables queried according
ordering long mentioned current formula. is, allow variable
skipped irrelevant formula currently consideration. visualize SPn,n
bipartite graph, n vertices left, n pearls right. pearl variable
pj corresponding n pearls, edge variable pi,j every vertex-pearl pair. (Note
variables corresponding vertices still refer them.)
Fix particular total ordering underlying n2 + n variables, 1 , 2 , . . . , l . pearl j,
let fanin (j) equal number edge variables pk,j incident pearl j one first
variables queried. Similarly, vertex i, let fanin (i) equal number edge variables pi,k
incident vertex one first variables queried. set pearls S, let fanin (S)
equal number edge variables pk,j incident pearl j one first
variables queried. Similarly set vertices S, fanin (S) equals number edge variables
pi,k incident vertex one first variables queried. Let edgest (j)
435

fiBACCHUS , DALMAO , & P ITASSI

edgest (S) defined similarly although set edges rather number
edges. clear context whether domain objects pearls vertices.
use simple procedure, based particular ordering variables, marking
pearl either C F follows. procedure, pearl may point marked
C later overwritten F; however, pearl marked F, remains
F duration procedure. pearl j marked C particular point
time, t, means point, color pearl already queried, fanin (j)
less n , = 2/5. pearl j marked F particular point time t, means
point fanin (j) least n . (The color j may may queried.)
pearl j unmarked time t, means color yet queried, fanin (j)
less n .
l 1 n2 +n, following. lth variable queried pearl variable (l = pj
j), less n edges pi,j incident j queried far, mark pj
C. Otherwise, lth variable queried edge variable (l = pi,j ) fanin l (j) n ,
mark pearl j F (if already marked F). Otherwise, leave pearl j unmarked.
Eventually every pearl become marked F. Consider first time either
lot Cs, lot Fs. precisely, let first time either exactly
n Cs (and less many Fs) exactly n Fs (and less many
Cs.) exactly n Cs occurs first, call case (a). Extend ta follows.
Let +1 , . . . , +c largest segment variables pearl variables pj j
already marked F. ta = + c. Notice query immediately following ta
either pearl variable pj currently unmarked, edge variable. hand,
exactly n Fs occurs first, call case (b). Again, extend tb ensure
query immediately following tb either pearl variable pj currently unmarked,
edge variable.
intuition case (a) (a lot Cs), lot pearls colored prematurelythat is,
know position mapped toand hence lot queries must asked.
case (b) (a lot Fs), lot edge variables queried thus lot queries asked.
proceed prove formally.
begin notation definitions. Let f = SPn,n , let Vars(f ) denote
set variables underlying f . restriction partial assignment variables
underlying f either 0 1. variable x unassigned , denote (x) = . Let
DPLL tree based variable ordering . is, decision tree variable
queried level . Recall corresponding node v formula f |
restriction corresponding partial path root v. tree traversed
depth-first search. vertex v corresponding path p traversed, check see
f |p already cache. is, need traverse subtree rooted v.
yet cache, traverse left subtree v, followed right subtree v.
subtrees traversed, pop back v, store f |p cache.
induces ordering vertices (and corresponding paths) traversedwhenever
pop back vertex v (and thus, store value cache), put v (p) end
current order.
Lemma 5 Let f SPn,n let static ordering variables. Let partial restriction
variables. runtime #DPLL-Cache (f, ) less runtime #DPLLCache (f | , ), ordering unassigned variables consistent .
436

fiBACKTRACKING EARCH



#SAT BAYES

Lemma 6 restriction , f | 6= 0 (pi,j ) = , pi,j occurs f | .
Proof: Consider clause Ci = (pi,1 . . . pi,m ) f . Since pi,j clause, pi,j
occur f | , Ci | must equal 1. Thus exists j 6= j (pi,j ) = 1.
clause (pi,j pi,j )| = pi,j thus pi,j disappear f | . 2
Corollary 1 Let total ordering Vars(f ). Let , partial restrictions sets
exactly 1 , . . . , q sets exactly 1 , . . . , q , q < q. Suppose exists k = pi,j
sets k (k ) = . either f | = 0 f | = 0 f | 6= f | .
Case (a). Let total ordering Vars(f ) case (a) holds. Let P C denote set
exactly n pearls marked C let P F denote set less n pearls (disjoint
P C ) marked F. Note (the color of) pearls P C queried time ta ;
color pearls P F may queried time ta , color pearls P P C P F
queried time ta . Note total number edges pi,j
queried n+ + n1+ 2n1+ .

define partial restriction, , 2n variables 1 , . . . , ta follows.
j P F , fix one-to-one mapping P F [n] range(j) edgesta (j)
j. j P C , variable pi,j queried 1 , . . . ta , set pi,j 0. vertex
variables pi,j queried 1 , . . . , ta , map exactly one pearl j
pj P P C P F . 2n i. (This arbitrary long consistent
one-to-one mapping already defined P F .) remaining pj P P C P F
yet mapped to, set queried variables pi,j 0. pearls pj P F
queried 1 , . . . , ta , assign fixed color pearl (all Red Blue)
smallest Red/Blue gap large possible. Note gap size least n1 .
sets variables 1 , . . . ta except variables pj , j P C . Since n variables,

number restrictions 1 , . . . , ta consistent exactly 2n . Let denote set
restrictions.
Let f = f |Ma let ordering unassigned variables consistent . (The
set unassigned variables is: pj , j P C , plus variables k , k > ta .) Let DPLL
tree corresponding solving f . Lemma 5, suffices show #DPLL-Cache

run inputs f , takes time least 2n .
Note first n variables queried pearl variables P C , thus set

n
2 paths height exactly n correspond set possible settings variables.

want show vertex v height n (corresponding 2n settings
variables P C ), v must traversed #DPLL-Cache, thus runtime least

2n .
Fix vertex v, corresponding path S. v traversed,
occurs ordering, f | = f | . want
show cannot happen. several cases consider.
1a. Suppose || n 6= . partial assignments
variables P C inconsistent one another. easy check case,
f | 6= f | .
2a. Suppose || > n , (n + 1)st variable set edge variable pi,j .
| | n , (pi,j ) = . Corollary 1, follows f | 6= f | .
437

fiBACCHUS , DALMAO , & P ITASSI

3a. Suppose || > n (n + 1)st variable set pearl variable pj . (Again,
know pj unset .) Since case (a), assume pj P P C P F .
Call vertex bad P P F P C edgesta (i). bad, fanin ta (i) greater
n 2n n/2. Since total number edges queried 2n1+ , follows
number bad vertices 4n . implies find pair i, + 1
vertices pearl j that: (1) pi,j queried 1 , . . . , ta ; (2) pi+1,j queried
1 , . . . , ta ; (3) pj P P C P F thus pj also queried. Thus clause
(pi,j pj pi+1,j pj )| disappear shrink f | , thus f | 6= f | .
Case (b). Let total ordering Vars(f ) case (b) holds. let P C denote set
less n pearls marked C let P F denote set exactly n pearls marked F.

define partial restriction Mb 2n variables 1 , . . . , follows. Call
vertex full variables pi,j queried 1 , . . . , tb . n full vertices.
j P F , fix pair vertices Fj = (ij , ij ) [n]. Let union n sets Fj
denoted F . F following properties. (1) j, element Fj full; (2)
j P F , Fj edgestb (j); (3) every two distinct elements F least distance 4 apart.
Since f anintb (j) n , = 2/5 > , possible find sets Fj satisfying criteria.
pi,j queried 1 , . . . tb , j P F 6 Fj , Mb set pi,j 0.
j P C , variable pi,j queried 1 , . . . tb , set pi,j 0. full vertex , map
exactly one pearl j pj P P C P F . (Again arbitrary long
consistent one-to-one mapping.) remaining pj P P C P F yet
mapped to, set queried variables pi,j 0. pearls pj P C , color Red.
pearls pj P F queried, assign fixed color pearl.
variables queried 1 , . . . tb set Mb edge

variables, pi,j , j P F , Fj . Let denote set 2n settings edge
variables j P F mapped exactly one element Fj . Let f = f |Mb let
DPLL tree corresponding solving f , ordering unassigned
variables consistent . Lemma 5, suffices show #DPLL-Cache f takes

time least 2n .
Note first 2n variables queried variables Pij ,j , Pij ,j , j P F .
nontrivial paths height 2n j P F mapped exactly one
vertex Fj , since otherwise formula f set 0. Thus, nontrivial paths height
2n correspond S. want show nontrivial vertex v height 2n
(corresponding restrictions S), v must traversed #DPLL-Cache, thus

runtime least 2n .
Fix vertex v corresponding path S. want show ,
occurs ordering, f | 6= f | . three cases consider.
1b. Suppose || 2n . nontrivial, partial mappings pearls
j P F Fj , inconsistent one another. easy check case
f | 6= f | .
2b. Suppose || > 2n (2n + 1)st variable set edge variable pi,j .
| | 2n , (pi,j ) = . Corollary 1, follows f | 6= f | .
438

fiBACKTRACKING EARCH



#SAT BAYES

3b. Suppose || > 2n (2n + 1)st variable set pearl variable pj .
definition tb , assume pj P P C P F . reasoning similar case 3a,
find vertices i, i+1, pearl j P P C P F none variable pi,j , pi+1,j , pj
queried 1 , . . . , tb . Thus clause (pi,j pj pi+1,j pj )| disappear
shrink f | 1, therefore f | 6= f | .


Thus two cases, #DPLL-Cache f takes time least 2n thus

#DPLL-Cache f takes time least 2n . 2

References
Aleknovich, A., & Razborov, A. (2002). Satisfiability, Branch-width Tseitin Tautologies.
Annual IEEE Symposium Foundations Computer Science (FOCS), pp. 593603.
Bacchus, F., Dalmao, S., & Pitassi, T. (2003). Algorithms Complexity Results #SAT
Bayesian Inference. Annual IEEE Symposium Foundations Computer Science
(FOCS), pp. 340351.
Bayardo, R. J., & Pehoushek, J. D. (2000). Counting Models using Connected Components.
Proceedings AAAI National Conference (AAAI), pp. 157162.
Bayardo, R. J., & Miranker, D. P. (1995). space-time trade-off solving Constraint Satisfaction Problems. Proceedings International Joint Conference Artificial Intelligence
(IJCAI), pp. 558562.
Beame, P., Impagliazzo, R., Pitassi, T., & Segerlind, N. (2003). Memoization DPLL: Formula
Caching Proof Systems. IEEE Conference Computational Complexity, pp. 248264.
Birnbaum, E., & Lozinskii, E. L. (1999). good old Davis Putnam procedure helps counting
models. J. Artif. Intell. Research (JAIR), 10, 457477.
Bitner, J. R., & Reingold, E. (1975). Backtracking programming techniques. Communications
ACM, 18(11), 651656.
Bodlaender, H. L. (1993). tourist guide Treewidth. Acta Cybernetica, 11(12), 121.
Bonet, M., Esteban, J. L., Galesi, N., & Johannsen, J. (1998). Exponential separations
restricted resolution cutting planes proof systems. Annual IEEE Symposium Foundations Computer Science (FOCS), pp. 638647.
Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-specific independence
Bayesian Networks. Uncertainty Artificial Intelligence, Proceedings Annual Conference (UAI), pp. 115123.
Chavira, M., & Darwiche, A. (2006). Encoding CNFs empower component analysis. Theory
Applications Satisfiability Testing (SAT), pp. 6174.
Chavira, M., & Darwiche, A. (2008). probabilistic inference weighted model counting.
Artificial Intelligence, 172(6-7), 772799.
Chavira, M., Darwiche, A., & Jaeger, M. (2006). Compiling relational bayesian networks exact
inference. Int. J. Approx. Reasoning, 42(1-2), 420.
Clote, P., & Setzer, A. (1998). PHP, st-connectivity odd charged graphs. Proof Complexity
Feasible Arithmetics, Vol. 39 DIMACS Series, pp. 93117. AMS.
439

fiBACCHUS , DALMAO , & P ITASSI

Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001). Introduction Algorithms. 2nd
Edition. McGraw Hill.
Darwiche, A., & Allen, D. (2002). Optimal time-space tradeoff probabilistic inference. European Workshop Probabilistic Graphical Models. Available www.cs.ucla.edu/darwiche.
Darwiche, A. (2001). Recursive conditioning. Artificial Intelligence, 126, 541.
Darwiche, A. (2002). logical approach factoring belief networks. Proceedings International Conference Principles Knowledge Representation Reasoning, pp. 409420.
Darwiche, A. (2004). New advances compiling CNF decomposable negation normal form.
Proceedings European Conference Artificial Intelligence (ECAI), pp. 328332.
Davies, J., & Bacchus, F. (2007). Using reasoning improve #SAT solving. Proceedings
AAAI National Conference (AAAI), pp. 185190.
Davis, M., Logemann, G., & Loveland, D. (1962). machine program theorem-proving. Communications ACM, 4, 394397.
Davis, M., & Putnam, H. (1960). computing procedure quantification theory. Journal
ACM, 7, 201215.
Dechter, R. (1999). Bucket elimination: unifying framework reasoning. Artificial Intelligence,
113, 4185.
Dechter, R., & Mateescu, R. (2004). Mixtures deterministic-probabilistic networks
AND/OR search space. Uncertainty Artificial Intelligence, Proceedings Annual Conference (UAI), pp. 120129.
Dechter, R., & Mateescu, R. (2007). AND/OR search spaces graphical models. Artificial Intelligence, 171(2-3), 73106.
Dubois, O. (1991). Counting number solutions instances satisfiability. Theoretical
Computer Science, 81, 4964.
Haken, A. (1985). intractability resolution. Theoretical Computer Science, 39, 297305.
Hertel, P., Bacchus, F., Pitassi, T., & van Gelder, A. (2008). Clause learning effectively psimulate general propositional resolution. Proceedings AAAI National Conference
(AAAI).
Johannsen, J. (2001). Exponential incomparability tree-like ordered resolution. Unpublished manuscript, available http://www.tcs.informatik.uni-muenchen.
de/jjohanns/notes.html.
Kask, K., Dechter, R., Larrosa, J., & Dechter, A. (2005). Unifying tree decompositions reasoning
graphical models. Artificial Intelligence, 166(1-2), 165193.
Kitching, M., & Bacchus, F. (2008). Exploiting decomposition constraint optimization problems.
Proceedings Principles Practice Constraint Programming (CP), pp. 478492.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computation probabilities graphical structures application expert systems. Journal Royal Statistical Society Series
B, 50(2), 157224.
440

fiBACKTRACKING EARCH



#SAT BAYES

Li, W., & van Beek, P. (2004). Guiding real-world sat solving dynamic hypergraph separator decomposition. Proceedings International Conference Tools Artificial
Intelligence (ICTAI), pp. 542548.
Li, W., van Beek, P., & Poupart, P. (2006). Performing incremental Bayesian Inference dynamic
model counting. Proceedings AAAI National Conference (AAAI), pp. 11731179.
Li, W., van Beek, P., & Poupart, P. (2008). Exploiting causal independence using weighted model
counting. Proceedings AAAI National Conference (AAAI).
Littman, M. L., Majercik, S. M., & Pitassi, T. (2001). Stochastic boolean satisfiability. J. Automated
Reasoning, 27(3), 251296.
Majercik, S. M., & Littman, M. L. (1998). Maxplan: new approach probabilistic planning.
Proceedings International Conference Artificial Intelligence Planning Scheduling (AIPS), pp. 8693.
Marinescu, R., & Dechter, R. (2006). Dynamic orderings AND/OR branch-and-bound search
graphical models. Proceedings European Conference Artificial Intelligence
(ECAI), pp. 138142.
Marinescu, R., & Dechter, R. (2007). Best-first AND/OR search graphical models. Proceedings AAAI National Conference (AAAI), pp. 11711176.
Mateescu, R., & Dechter, R. (2007). AND/OR multi-valued decision diagrams weighted graphical models. Uncertainty Artificial Intelligence, Proceedings Annual Conference
(UAI).
Mateescu, R., & Dechter, R. (2005). AND/OR cutset conditioning. Proceedings International Joint Conference Artificial Intelligence (IJCAI), pp. 230235.
Moskewicz, E., Madigan, C., Zhao, M., Zhang, L., & Malik, S. (2001). Chaff: Engineering
efficient sat solver. Proc. Design Automation Conference (DAC).
Nilsson, N. J. (1980). Principles Artificial Intelligence. Tioga.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems (2nd edition). Morgan Kaufmann,
San Mateo, CA.
Preston, C. (1974). Gibbs States Countable Sets. Cambridge University Press.
Rish, I., & Dechter, R. (2000). Resolution versus search: Two strategies SAT. Journal
Automated Reasoning, 24(1), 225275.
Robertson, N., & Seymour, P. (1991). Graph minors X. obstructions tree-decomposition. Journal
Combinatorial Theory, Series B, 52, 153190.
Robertson, N., & Seymour, P. (1995). Graph minors XIII. disjoint paths problem. Journal
Combinatorial Theory, Series B, 63, 65110.
Roth, D. (1996). hardness approximate reasoning. Artificial Intelligence, 82(12), 273
302.
Sang, T., Bacchus, F., Beame, P., Kautz, H. A., & Pitassi, T. (2004). Combining component caching
clause learning effective model counting. Theory Applications Satisfiability
Testing (SAT).
441

fiBACCHUS , DALMAO , & P ITASSI

Sang, T., Beame, P., & Kautz, H. A. (2005a). Heuristics fast exact model counting. Theory
Applications Satisfiability Testing (SAT), pp. 226240.
Sang, T., Beame, P., & Kautz, H. A. (2005b). Performing Bayesian Inference weighted model
counting. Proceedings AAAI National Conference (AAAI), pp. 475482.
Sang, T., Beame, P., & Kautz, H. A. (2007). dynamic approach MPE weighted MAXSAT. Proceedings International Joint Conference Artificial Intelligence (IJCAI),
pp. 173179.
Sanner, P., & McAllester, D. (2005). Affine algebraic decision diagrams (aadds) applications structured probabilistic inference. Proceedings International Joint Conference Artificial Intelligence (IJCAI), pp. 13841390.
Spitzer, F. L. (1971). Markov random fields Gibbs ensembles. American Mathematical Monthly,
78, 14254.
Thurley, M. (2006). sharpSATCounting models advanced component caching implicit
BCP. Theory Applications Satisfiability Testing (SAT), pp. 424429.
Valiant, L. G. (1979a). complexity enumeration reliability problems. SIAM Journal
Computing, 9, 410421.
Valiant, L. G. (1979b). Complexity Computing Permanent. Theoretical Computer Science, 8, 189201.
Zhang, W. (1996). Number models satisfiability sets clauses. Theoretical Computer
Science, 155, 277288.

442

fiJournal Artificial Intelligence Research 34 (2009) 569-603

Submitted 07/08; published 04/09

Learning Document-Level Semantic Properties
Free-Text Annotations
S.R.K. Branavan
Harr Chen
Jacob Eisenstein
Regina Barzilay

BRANAVAN @ CSAIL . MIT. EDU
HARR @ CSAIL . MIT. EDU
JACOBE @ CSAIL . MIT. EDU
REGINA @ CSAIL . MIT. EDU

Computer Science Artificial Intelligence Laboratory
Massachusetts Institute Technology
77 Massachusetts Avenue, Cambridge 02139

Abstract
paper presents new method inferring semantic properties documents leveraging free-text keyphrase annotations. annotations becoming increasingly abundant due
recent dramatic growth semi-structured, user-generated online content. One especially
relevant domain product reviews, often annotated authors pros/cons
keyphrases real bargain good value. annotations representative
underlying semantic properties; however, unlike expert annotations, noisy: lay authors
may use different labels denote property, labels may missing. learn
using noisy annotations, find hidden paraphrase structure clusters keyphrases.
paraphrase structure linked latent topic model review texts, enabling system predict properties unannotated documents effectively aggregate semantic
properties multiple reviews. approach implemented hierarchical Bayesian model
joint inference. find joint inference increases robustness keyphrase clustering
encourages latent topics correlate semantically meaningful properties. Multiple evaluations demonstrate model substantially outperforms alternative approaches summarizing
single multiple documents set semantically salient keyphrases.

1. Introduction
Identifying document-level semantic properties implied text core problem natural
language understanding. example, given text restaurant review, would useful
extract semantic-level characterization authors reaction specific aspects restaurant, food service quality (see Figure 1). Learning-based approaches dramatically
increased scope robustness semantic processing, typically dependent
large expert-annotated datasets, costly produce (Zaenen, 2006).
propose use alternative source annotations learning: free-text keyphrases produced novice users. example, consider lists pros cons often accompany
reviews products services. end-user annotations increasingly prevalent online,
grow organically keep pace subjects interest socio-cultural trends. Beyond
pragmatic considerations, free-text annotations appealing linguistic standpoint
capture intuitive semantic judgments non-specialist language users. many real-world
datasets, annotations created documents original author, providing direct window
semantic judgments motivated document text.

c
2009
AI Access Foundation. rights reserved.

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

pros/cons: great nutritional value
... combines all: amazing product, quick friendly service, cleanliness, great nutrition ...
pros/cons: bit pricey, healthy
... awesome place go health conscious. really great low calorie dishes
publish calories fat grams per serving.

Figure 1: Excerpts online restaurant reviews pros/cons phrase lists. reviews assert
restaurant serves healthy food, use different keyphrases. Additionally,
first review discusses restaurants good service, annotated
keyphrases.

major obstacle computational use free-text annotations inherently noisy fixed vocabulary, explicit relationship annotation keyphrases,
guarantee relevant semantic properties document annotated. example,
pros/cons annotations accompanying restaurant reviews Figure 1, underlying
semantic idea expressed different ways keyphrases great nutritional value
healthy. Additionally, first review discusses quality service, annotated such.
contrast, expert annotations would replace synonymous keyphrases single canonical label, would fully label semantic properties described text. expert annotations
typically used supervised learning methods. demonstrate paper, traditional
supervised approaches perform poorly free-text annotations used instead clean, expert
annotations.
paper demonstrates new approach handling free-text annotation context
hidden-topic analysis document text. show regularities text clarify noise
annotations example, although great nutritional value healthy different
surface forms, text documents annotated two keyphrases likely
similar. modeling relationship document text annotations large dataset,
possible induce clustering annotation keyphrases help overcome
problem inconsistency. model also addresses problem incompleteness novice
annotators fail label relevant semantic topics estimating topics predicted
document text alone.
Central approach idea document text associated annotations reflect
single underlying set semantic properties. text, semantic properties correspond
induced hidden topics similar growing body work latent topic models,
latent Dirichlet allocation (LDA; Blei, Ng, & Jordan, 2003). However, unlike existing work topic
modeling, tie hidden topics text clusters observed keyphrases. connection
motivated idea text associated annotations grounded shared set
semantic properties. modeling properties directly, ensure inferred hidden
topics semantically meaningful, clustering free-text annotations robust
noise.
approach takes form hierarchical Bayesian framework, includes LDA-style
component word text generated mixture multinomials. addition, also incorporate similarity matrix across universe annotation keyphrases,

570

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

constructed based orthographic distributional features keyphrases. model
matrix generated underlying clustering keyphrases, keyphrases
clustered together likely produce high similarity scores. generate words
document, model two distributions semantic properties one governed annotation
keyphrases clusters, background distribution cover properties mentioned
annotations. latent topic word drawn mixture two distributions.
learning model parameters noisily-labeled training set, apply model unlabeled
data.
build system extracts semantic properties reviews products services.
system uses training corpus includes user-created free-text annotations pros cons
review. Training yields two outputs: clustering keyphrases semantic properties,
topic model capable inducing semantic properties unlabeled text. clustering
annotation keyphrases relevant applications content-based information retrieval,
allowing users retrieve documents semantically relevant annotations even surface
forms differ query term. topic model used infer semantic properties
unlabeled text.
topic model also used perform multi-document summarization, capturing key
semantic properties multiple reviews. Unlike traditional extraction-based approaches multidocument summarization, induced topic model abstracts text review representation capturing relevant semantic properties. enables comparison reviews even
use superficially different terminology describe set semantic properties.
idea implemented review aggregation system extracts majority sentiment
multiple reviewers product service. example output produced system
shown Figure 6. system applied reviews 480 product categories, allowing users
navigate semantic properties 49,490 products based total 522,879 reviews.
effectiveness approach confirmed several evaluations.
summarization single multiple documents, compare properties inferred model expert annotations. approach yields substantially better results
alternatives research literature; particular, find learning clustering free-text
annotation keyphrases essential extracting meaningful semantic properties dataset.
addition, compare induced clustering gold standard clustering produced expert
annotators. comparison shows tying clustering hidden topic model substantially
improves quality, clustering induced system coheres well clustering
produced expert annotators.
remainder paper structured follows. Section 2 compares approach previous work topic modeling, semantic property extraction, multi-document summarization.
Section 3 describes properties free-text annotations motivate approach. model
described Section 4, method parameter estimation presented Section 5.
Section 6 describes implementation evaluation single-document multi-document
summarization systems using techniques. summarize contributions consider directions future work Section 7. code, datasets expert annotations used paper
available online http://groups.csail.mit.edu/rbg/code/precis/.

571

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

2. Related Work
material presented section covers three lines related work. First, discuss work
Bayesian topic modeling related technique learning free-text annotations.
Next, discuss state-of-the-art methods identifying analyzing product properties
review text. Finally, situate summarization work landscape prior research
multi-document summarization.
2.1 Bayesian Topic Modeling
Recent work topic modeling literature demonstrated semantically salient topics
inferred unsupervised fashion constructing generative Bayesian model document text. One notable example line research Latent Dirichlet Allocation (LDA; Blei
et al., 2003). LDA framework, semantic topics equated latent distributions words
text; thus, document modeled mixture topics. class models
used variety language processing tasks including topic segmentation (Purver, Kording,
Griffiths, & Tenenbaum, 2006), named-entity resolution (Bhattacharya & Getoor, 2006), sentiment
ranking (Titov & McDonald, 2008b), word sense disambiguation (Boyd-Graber, Blei, & Zhu,
2007).
method similar LDA assigns latent topic indicators word
dataset, models documents mixtures topics. However, LDA model unsupervised,
provide method linking latent topics external observed representations
properties interest. contrast, model exploits free-text annotations dataset
ensure induced topics correspond semantically meaningful properties.
Combining topics induced LDA external supervision first considered Blei
McAuliffe (2008) supervised Latent Dirichlet Allocation (sLDA) model. induction
hidden topics driven annotated examples provided training stage. perspective supervised learning, approach succeeds hidden topics mediate
document annotations lexical features. Blei McAuliffe describe variational expectationmaximization procedure approximate maximum-likelihood estimation models parameters. tested two polarity assessment tasks, sLDA shows improvement model
topics induced unsupervised model added features supervised
model.
key difference model sLDA assume access clean
supervision data training. Since annotations provided algorithm free-text
nature, incomplete fraught inconsistency. substantial difference input
structure motivates need model simultaneously induces hidden structure freetext annotations learns predict properties text.
2.2 Property Assessment Review Analysis
model applied task review analysis. Traditionally, task identifying properties product review texts cast extraction problem (Hu & Liu, 2004; Liu,
Hu, & Cheng, 2005; Popescu, Nguyen, & Etzioni, 2005). example, Hu Liu (2004) employ
association mining identify noun phrases express key portions product reviews. polarity extracted phrases determined using seed set adjectives expanded via WordNet

572

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

relations. summary review produced extracting property phrases present verbatim
document.
Property extraction refined PINE (Popescu et al., 2005), another system
review analysis. PINE employs novel information extraction method identify noun phrases
could potentially express salient properties reviewed products; candidates
pruned using WordNet morphological cues. Opinion phrases identified using set handcrafted rules applied syntactic dependencies extracted input document. semantic
orientation properties computed using relaxation labeling method finds optimal assignment polarity labels given set local constraints. Empirical results demonstrate PINE
outperforms Hu Lius system opinion extraction identifying polarity opinion words.
two feature extraction methods informed human knowledge way opinions
typically expressed reviews: Hu Liu (2004), human knowledge encoded using
WordNet seed adjectives; Popescu et al. (2005), opinion phrases extracted via handcrafted rules. alternative approach learn rules feature extraction annotated
data. end, property identification modeled classification framework (Kim &
Hovy, 2006). classifier trained using corpus free-text pro con keyphrases
specified review authors. keyphrases compared sentences review
text; sentences exhibit high word overlap previously identified phrases marked pros
cons according phrase polarity. rest sentences marked negative examples.
Clearly, accuracy resulting classifier depends quality automatically induced annotations. analysis free-text annotations several domains shows automatically mapping even manually-extracted annotation keyphrases document text difficult
task, due variability keyphrase surface realizations (see Section 3). argue rest
paper, beneficial explicitly address difficulties inherent free-text annotations.
end, work distinguished two significant ways property extraction methods described above. First, able predict properties beyond appear verbatim text.
Second, approach also learns semantic relationships different keyphrases, allowing
us draw direct comparisons reviews even semantic ideas expressed using
different surface forms.
Working related domain web opinion mining, Lu Zhai (2008) describe system
generates integrated opinion summaries, incorporate expert-written articles (e.g., review online magazine) user-generated ordinary opinion snippets (e.g., mentions
blogs). Specifically, expert article assumed structured segments, collection
representative ordinary opinions aligned segment. Probabilistic Latent Semantic Analysis
(PLSA) used induce clustering opinion snippets, cluster attached one
expert article segments. clusters may also unaligned segment, indicating
opinions entirely unexpressed expert article. Ultimately, integrated opinion summary combination single expert article multiple user-generated opinion snippets
confirm supplement specific segments review.
works final goal different aim provide highly compact summary multitude user opinions identifying underlying semantic properties, rather supplementing
single expert article user opinions. specifically leverage annotations users already
provide reviews, thus obviating need expert article template opinion inte-

573

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

gration. Consequently, approach suitable goal producing concise keyphrase
summarizations user reviews, particularly review taken authoritative.
work closest methodology approach review summarizer developed Titov
McDonald (2008a). method summarizes review selecting list phrases
express writers opinions set predefined properties (e.g.,, food ambiance restaurant
reviews). system access numerical ratings set properties,
training set providing examples appropriate keyphrases extract. Similar sLDA, method
uses numerical ratings bias hidden topics towards desired semantic properties. Phrases
strongly associated properties via hidden topics extracted part summary.
several important differences work summarization method
Titov McDonald. method assumes predefined set properties thus cannot capture
properties outside set. Moreover, consistent numerical annotations required training,
method emphasizes use free-text annotations. Finally, since Titov McDonalds
algorithm extractive, facilitate property comparison across multiple reviews.
2.3 Multidocument Summarization
paper also relates large body work multi-document summarization. Researchers
long noted central challenge multi-document summarization identifying redundant
information input documents (Radev & McKeown, 1998; Carbonell & Goldstein, 1998; Mani
& Bloedorn, 1997; Barzilay, McKeown, & Elhadad, 1999). task crucial significance
multi-document summarizers operate related documents describe facts
multiple times. fact, common assume repetition information among related sources
indicator importance (Barzilay et al., 1999; Radev, Jing, & Budzikowska, 2000; Nenkova,
Vanderwende, & McKeown, 2006). Many algorithms first cluster sentences together,
extract generate sentence representatives clusters.
Identification repeated information equally central approach multi-document
summarization method selects properties stated plurality users, thereby eliminating rare and/or erroneous opinions. key difference algorithm existing summarization systems method identifying repeated expressions single semantic property.
Since existing work multi-document summarization focuses topic-independent
newspaper articles, redundancy identified via sentence comparison. instance, Radev et al.
(2000) compare sentences using cosine similarity corresponding word vectors. Alternatively, methods compare sentences via alignment syntactic trees (Barzilay et al., 1999;
Marsi & Krahmer, 2005). string- tree-based comparison algorithms augmented
lexico-semantic knowledge using resources WordNet.
approach described paper perform comparisons sentence level. Instead, first abstract reviews set properties compare property overlap across
different documents. approach relates domain-dependent approaches text summarization (Radev & McKeown, 1998; White, Korelsky, Cardie, Ng, Pierce, & Wagstaff, 2001; Elhadad
& McKeown, 2001). methods identify relations documents comparing
abstract representations. cases, abstract representation constructed using off-the-shelf
information extraction tools. template specifying types information select crafted
manually domain interest. Moreover, training information extraction systems requires
corpus manually annotated relations interest. contrast, method require

574

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

Incompleteness
Property
Good food
Good service
Good price
Bad food
Bad service
Bad price
Average

Recall

Precision

F-score

0.736
0.329
0.500
0.516
0.475
0.690
0.578

0.968
0.821
0.707
0.762
0.633
0.645
0.849

0.836
0.469
0.586
0.615
0.543
0.667
0.688

Inconsistency
Keyphrase Top Keyphrase
Count
Coverage %
23
38.3
27
28.9
20
41.8
16
23.7
20
22.0
15
30.6
22.6
33.6

Table 1: Incompleteness inconsistency restaurant domain, six major properties prevalent reviews. incompleteness figures recall, precision, F-score
author annotations (manually clustered properties) gold standard property
annotations. Inconsistency measured number different keyphrase realizations
least five occurrences associated property, percentage frequency
commonly occurring keyphrases used annotate property.
averages bottom row weighted according frequency property occurrence.

manual template specification corpora annotated experts. abstract representations
induce linguistically rich extraction templates, nevertheless enable us
perform in-depth comparisons across different reviews.

3. Analysis Free-Text Keyphrase Annotations
section, explore characteristics free-text annotations, aiming quantify degree
noise observed data. results analysis motivate development learning
algorithm described Section 4.
perform investigation domain online restaurant reviews using documents downloaded popular Epinions1 website. Users website evaluate products providing
textual description opinion, well concise lists keyphrases (pros cons)
summarizing review. Pros/cons keyphrases appealing source annotations online
review texts. However, contributed independently multiple users thus unlikely
clean expert annotations. analysis, focus two features free-text annotations: incompleteness inconsistency. measure incompleteness quantifies degree
label omission free-text annotations, inconsistency reflects variance keyphrase
vocabulary used various annotators.
test quality user-generated annotations, compare expert annotations produced systematic fashion. annotation effort focused six properties
commonly mentioned review authors, specifically shown Table 1. Given
review property, task assess whether reviews text supports property.
annotations produced two judges guided standardized set instructions. contrast
author annotations website, judges conferred training session ensure consistency completeness. two judges collectively annotated 170 reviews, 30 annotated
1. http://www.epinions.com/

575

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

Property: good price
relatively inexpensive, dirt cheap, relatively cheap, great price, fairly priced, well priced, reasonable
prices, cheap prices, affordable prices, reasonable cost

Figure 2: Examples many different paraphrases related property good price appear
pros/cons keyphrases reviews used inconsistency analysis.

both. Cohens Kappa, measure inter-annotator agreement ranges zero one,
0.78 joint set, indicating high agreement (Cohen, 1960). average, review text
annotated 2.56 properties.
Separately, one judges also standardized free-text pros/cons annotations
170 reviews. reviews keyphrases matched six properties. standardization allows direct comparison properties judged supported reviews
text properties described reviews free-text annotations. find many semantic properties judged present text user annotated average,
keyphrases expressed 1.66 relevant semantic properties per document, text expressed
2.56 properties. gap demonstrates frequency authors omitted relevant semantic
properties review annotations.
3.1 Incompleteness
measure incompleteness, compare properties stated review authors form
pros cons stated review text, judged expert annotators.
comparison performed using precision, recall F-score. setting, recall proportion
semantic properties text review author also provided least one annotation
keyphrase; precision proportion keyphrases conveyed properties judged supported
text; F-score harmonic mean. results comparison summarized
left half Table 1.
incompleteness results demonstrate significant discrepancy user expert
annotations. expected, recall quite low; 40% property occurrences stated
review text without explicitly mentioned annotations. precision scores indicate
converse also true, though lesser extent keyphrases express properties
mentioned text.
Interestingly, precision recall vary greatly depending specific property.
highest good food, matching intuitive notion high food quality would key salient
property restaurant, thus likely mentioned text annotations. Conversely, recall good service lower users, high quality service apparently
key point summarizing review keyphrases.
3.2 Inconsistency
lack unified annotation scheme restaurant review dataset apparent across
reviewers, annotations feature 26,801 unique keyphrase surface forms set 49,310 total
keyphrase occurrences. Clearly, many unique keyphrases express semantic property
Figure 2, good price expressed ten different ways. quantify phenomenon, judges
576

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

Figure 3: Cumulative occurrence counts top ten keyphrases associated good service
property. percentages total 1,210 separate keyphrase occurrences
property.

manually clustered subset keyphrases associated six previously mentioned properties. Specifically, 121 keyphrases associated six major properties chosen, accounting
10.8% keyphrase occurrences.
use manually clustered annotations examine distributional pattern keyphrases
describe underlying property, using two different statistics. First, number
different keyphrases property gives lower bound number possible paraphrases.
Second, measure often common keyphrase used annotate property,
i.e., coverage keyphrase. metric gives sense diffuse keyphrases within
property are, specifically whether one single keyphrase dominates occurrences property.
Note value overestimate true coverage, since considering tenth
keyphrase occurrences.
right half Table 1 summarizes variability property paraphrases. Observe
property associated numerous paraphrases, found multiple times
actual keyphrase set. importantly, frequent keyphrase accounted third
property occurrences, strongly suggesting targeting labels learning
limited approach. illustrate last point, consider property good service, whose
keyphrase realizations distributional histogram appears Figure 3. cumulative percentage
frequencies frequent keyphrases associated property plotted. top four
keyphrases account three quarters property occurrences, even within limited
set keyphrases consider analysis, motivating need aggregate consideration
keyphrases.
next section, introduce model induces clustering among keyphrases
relating keyphrase clusters text, directly addressing characteristics data.

577

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY


x

h


c

z

w













keyphrase cluster model
keyphrase cluster assignment
keyphrase similarity values
document keyphrases
document keyphrase topics
probability selecting instead
selects word topics
background word topic model
word topic assignment
language models topic
document words

Dirichlet(0 )
x` Multinomial()
(
Beta(= ) x` = x`0
s`,`0
Beta(6= ) otherwise


= [d,1 . . . d,K ]

(
d,k

1


x` = k l hd
otherwise

Beta(0 )
cd,n Bernoulli(d )
Dirichlet(0 )
(
Multinomial(d ) cd,n = 1
zd,n
Multinomial(d ) otherwise
k Dirichlet(0 )
wd,n Multinomial(zd,n )

Figure 4: plate diagram model. Shaded circles denote observed variables, squares
denote hyperparameters. dotted arrows indicate constructed deterministically x h. use refer small constant probability mass.

578

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

4. Model Description
present generative Bayesian model documents annotated free-text keyphrases.
model assumes annotated document generated set underlying semantic topics.
Semantic topics generate document text indexing language model; approach,
also associated clusters keyphrases. way, model viewed extension
Latent Dirichlet Allocation (Blei et al., 2003), latent topics additionally biased
toward keyphrases appear training data. However, coupling flexible,
words permitted drawn topics represented keyphrase annotations.
permits model learn effectively presence incomplete annotations, still
encouraging keyphrase clustering cohere topics supported document text.
Another critical aspect model desire ability use arbitrary comparisons
keyphrases, addition information surface forms. accommodate
goal, treat keyphrase surface forms generated model. Rather, acquire
real-valued similarity matrix across universe possible keyphrases, treat matrix
generated keyphrase clustering. representation permits use surface
distributional features keyphrase similarity, described Section 4.1.
advantage hierarchical Bayesian models easy change parts
model observed parts hidden. training, keyphrase annotations
observed, hidden semantic topics coupled clusters keyphrases. account
words related semantic topics, topics may associated keyphrases. test
time, model presented documents keyphrase annotations hidden.
model evaluated ability determine keyphrases applicable, based hidden
topics present document text.
judgment whether topic applies given unannotated document based probability mass assigned topic documents background topic distribution.
annotations, background topic distribution capture entirety documents
topics. task involving reviews products services, multiple topics may accompany
document. case, topic whose probability threshold (tuned development
set) predicted supported.
4.1 Keyphrase Clustering
handle hidden paraphrase structure keyphrases, one component model estimates
clustering keyphrases. goal obtain clusters cluster correspond welldefined semantic topic e.g., healthy good nutrition grouped single
cluster. overall joint model generative, generative model clustering could easily
integrated larger framework. approach would treat keyphrases
cluster generated parametric distribution. However, representation would
permit many powerful features assessing similarity pairs keyphrases, string
overlap keyphrase co-occurrence corpus (McCallum, Bellare, & Pereira, 2005).
reason, represent keyphrase real-valued vector rather surface
form. vector given keyphrase includes similarity scores respect every observed keyphrase (the similarity scores represented Figure 4). model similarity
scores generated cluster memberships (represented x Figure 4). two keyphrases

579

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

Lexical

cosine similarity surface forms two keyphrases, represented word frequency vectors.

Co-occurrence

keyphrase represented vector co-occurrence values.
vector counts many times keyphrases appear documents
annotated keyphrase. example, similarity vector
good food may include entry tasty food, value
would number documents annotated good food
contain tasty food text. similarity two
keyphrases cosine similarity co-occurrence vectors.

Table 2: two sources information used compute similarity matrix experiments.
final similarity scores linear combinations two values. Note cooccurrence similarity contains second-order co-occurrence information.

Figure 5: surface plot keyphrase similarity matrix set restaurant reviews, computed according Table 2. Red indicates high similarity, whereas blue indicates low
similarity. diagram, keyphrases grouped according expertcreated clustering, keyphrases similar meaning close together. strong series
similarity blocks along diagonal hint information could induce
reasonable clustering.

580

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

clustered together, similarity score generated distribution encouraging high similarity; otherwise, distribution encouraging low similarity used.2
features used producing similarity matrix given Table 2, encompassing lexical
distributional similarity measures. implemented system takes linear combination
two data sources, weighting sources equally. resulting similarity matrix keyphrases
restaurant domain shown Figure 5.
described next section, clustering keyphrases, model takes advantage
topic structure documents annotated keyphrases, addition information
individual keyphrases themselves. sense, differs traditional approaches paraphrase
identification (Barzilay & McKeown, 2001; Lin & Pantel, 2001).
4.2 Document Topic Modeling
analysis document text based probabilistic topic models LDA (Blei et al.,
2003). LDA framework, word generated language model indexed
words topic assignment. Thus, rather identifying single topic document, LDA identifies
distribution topics. High probability topic assignments identify compact, low-entropy
language models, probability mass language model topic divided among
relatively small vocabulary.
model operates similar manner, identifying topic word, denoted z
Figure 4. However, LDA learns distribution topics document, deterministically construct document-specific topic distribution clusters represented
documents keyphrases figure. assigns equal probability topics
represented keyphrase annotations, small probability topics. Generating
word topics way ties together clustering language models.
noted above, sometimes keyphrase annotation represent semantic
topics expressed text. reason, also construct another background distribution topics. auxiliary variable c indicates whether given words topic drawn
distribution derived annotations, background model. Representing c
hidden variable allows us stochastically interpolate two language models
. addition, given document likely also discuss topics covered
keyphrase. account this, model allowed leave clusters empty, thus
leaving topics independent keyphrases.
4.3 Generative Process
model assumes observed data generated stochastic process involving hidden
parameters. section, formally specify generative process. specification guides
inference hidden parameters based observed data, following:
L keyphrases, vector s` length L denoting pairwise similarity score
interval [0, 1] every keyphrase.
document d, bag words wd length Nd . nth word wd,n .
2. Note model similarity score independent draw; clearly assumption strong, due
symmetry transitivity. Models making similar assumptions independence related hidden variables
previously shown successful (for example, Toutanova & Johnson, 2008).

581

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

document d, set keyphrase annotations hd , includes index ` document annotated keyphrase `.
number clusters K, large enough encompass topics actual
clusters keyphrases, well word-only topics.
observed variables generated according following process:
1. Draw multinomial distribution K keyphrase clusters symmetric Dirichlet
prior parameter 0 .3
2. ` = 1 . . . L:
(a) Draw `th keyphrases cluster assignment x` Multinomial().
3. (`, `0 ) = (1 . . . L, 1 . . . L):
(a) x` = x`0 , draw s`,`0 Beta(= ) Beta(2, 1), encouraging scores biased
toward values close one.
(b) x` 6= x`0 , draw s`,`0 Beta(6= ) Beta(1, 2), encouraging scores biased
toward values close zero.
4. k = 1 . . . K:
(a) Draw language model k symmetric Dirichlet prior parameter 0 .
5. = 1 . . . D:
(a) Draw background topic model symmetric Dirichlet prior parameter 0 .
(b) Deterministically construct annotation topic model , based keyphrase cluster
assignments x observed document annotations hd . Specifically, let H set
topics represented phrases hd . Distribution assigns equal probability
element H, small probability mass topics.4
(c) Draw weighted coin Beta(0 ), determine balance
annotation background topic models .
(d) n = 1 . . . Nd :
i. Draw binary auxiliary variable cd,n Bernoulli(d ), determines whether
topic word wd,n drawn annotation topic model background model .
ii. Draw topic assignment zd,n appropriate multinomial indicated
cd,n .
iii. Draw word wd,n Multinomial(zd,n ), is, language model indexed
words topic.
3. Variables subscripted zero fixed hyperparameters.
4. Making hard assignment zero probability topics creates problems parameter estimation.
probability 104 assigned topics represented keyphrase cluster memberships.

582

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

5. Parameter Estimation
make predictions unseen data, need estimate parameters model. Bayesian
inference, estimate distribution parameter, conditioned observed data
hyperparameters. inference intractable general case, sampling approaches allow
us approximately construct distributions parameter interest.
Gibbs sampling perhaps generic straightforward sampling technique. Conditional distributions computed hidden variable, given variables model.
repeatedly sampling distributions turn, possible construct Markov chain
whose stationary distribution posterior model parameters (Gelman, Carlin, Stern, &
Rubin, 2004). use sampling techniques natural language processing previously
investigated many researchers, including Finkel, Grenager, Manning (2005) Goldwater,
Griffiths, Johnson (2006).
present sampling equations hidden variables Figure 4. prior
keyphrase clusters sampled based hyperprior 0 keyphrase cluster assignments
x. write p( | . . .) mean probability conditioned variables.
p( | . . .) p( | 0 )p(x | ),

= p( | 0 )
p(x` | )
`

= Dirichlet(; 0 )



Multinomial(x` ; )

`

= Dirichlet(; 0 ),
i0 0 + count(x` = i). conditional distribution derived based conjugacy
multinomial Dirichlet distribution. first line follows Bayes rule, second
line conditional independence cluster assignments x given keyphrase distribution .
Resampling equations k derived similar manner:
p(d | . . .) Dirichlet(d ; 0d ),
p(k | . . .) Dirichlet(k ; k0 ),
P
0 = +
0d,i = 0 + count(zn,d = cn,d = 0) k,i
0
count(wn,d = zn,d = k).
0
building counts , consider cases cn,d = 0, indicating topic zn,d
indeed drawn background topic model . Similarly, building counts k0 ,
consider cases word wd,n drawn topic k.
resample , employ conjugacy Beta prior Bernoulli observation likelihoods, adding counts c prior 0 .
p(d | . . .) Beta(d ; 0d ),

P
count(c
=
1)
d,n
0
n
.
= 0 + P
n count(cd,n = 0)

583

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

keyphrase cluster assignments represented x, whose sampling distribution depends
, s, z, via :
p(x` | . . .) p(x` | )p(s | x` , x` , )p(z | , , c)






p(x` | )
p(s`,`0 | x` , x`0 , )
p(zd,n | )
`0 6=`

cd,n =1


= Multinomial(x` ; )







Beta(s`,`0 ; x` ,x`0 )
Multinomial(zd,n ; ) .

`0 6=`

cd,n =1

leftmost term equation prior x` . next term encodes dependence
similarity matrix cluster assignments; slight abuse notation, write x` ,x`0
denote = x` = x`0 , 6= otherwise. third term dependence word topics
zd,n topic distribution . compute final result probability expression
possible setting x` , sample normalized multinomial.
word topics z sampled according topic distribution , background distribution
, observed words w, auxiliary variable c:
p(zd,n | . . .) p(zd,n | , , cd,n )p(wd,n | zd,n , )
(
Multinomial(zd,n ; )Multinomial(wd,n ; zd,n )
=
Multinomial(zd,n ; )Multinomial(wd,n ; zd,n )

cd,n = 1,
otherwise.

x, zd,n sampled computing conditional likelihood possible setting
within constant proportionality, sampling normalized multinomial.
Finally, sample auxiliary variable cd,n , indicates whether hidden topic zd,n
drawn . c depends prior hidden topic assignments z:
p(cd,n | . . .) p(cd,n | )p(zd,n | , , cd,n )
(
Bernoulli(cd,n ; )Multinomial(zd,n ; )
=
Bernoulli(cd,n ; )Multinomial(zd,n ; )

cd,n = 1,
otherwise.

Again, compute likelihood cd,n = 0 cd,n = 1 within constant proportionality,
sample normalized Bernoulli distribution.
Finally, model requires values fixed hyperparameters 0 , 0 , 0 , 0 , tuned
standard way based development set performance. Appendix C lists hyperparameters
values used domain experiments.
One main applications model predict properties supported documents
annotated keyphrases. test time, would like compute posterior estimate
unannotated test document d. Since annotations present, property prediction
based text component model. estimate, use Gibbs sampling
procedure, restricted zd,n , stipulation cd,n fixed zero zd,n
always drawn . particular, treat language models known; accurately
integrate possible language models, use final 1000 samples language models
training opposed using point estimate. topic, probability exceeds
certain threshold, topic predicted. threshold tuned independently topic
development set. empirical results present Section 6 obtained manner.
584

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

Figure 6: Summary reviews movie Pirates Caribbean: Worlds End P R ECIS.
summary based 27 documents. list pros cons generated automatically using system described paper. generation numerical ratings
based algorithm described Snyder Barzilay (2007).

6. Evaluation Summarization Quality
model document analysis implemented P R ECIS,5 system performs single-
multi-document review summarization. goal P R ECIS provide users effective access
review data via mobile devices. P R ECIS contains information 49,490 products services
ranging childcare products restaurants movies. products, system
contains collection reviews downloaded consumer websites Epinions, CNET,
Amazon. P R ECIS compresses data product short list pros cons
supported majority reviews. example summary 27 reviews movie
Pirates Caribbean: Worlds End shown Figure 6. contrast traditional multidocument summarizers, output system sequence sentences, rather list
phrases indicative product properties. summarization format follows format pros/cons
summaries individual reviewers provide multiple consumer websites. Moreover, brevity
summary particularly suitable presenting small screens mobile
devices.
automatically generate combined pros/cons list product service, first apply
model review. model trained independently product domain (e.g., movies)
using corresponding subset reviews free-text annotations. annotations also provide
set keyphrases contribute clusters associated product properties.
5. P R ECIS accessible http://groups.csail.mit.edu/rbg/projects/precis/.

585

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

model trained, labels review set properties. Since set possible properties
reviews product, comparison among reviews straightforward
property, count number reviews support it, select property part
summary supported majority reviews. set semantic properties converted
pros/cons list presenting common keyphrase property.
aggregation technology applicable two scenarios. system applied unannotated reviews, inducing semantic properties document text; conforms traditional way learning-based systems applied unlabeled data. However, model
valuable even individual reviews include pros/cons keyphrase annotations. Due
high degree paraphrasing, direct comparison keyphrases challenging (see Section 3).
inferring clustering keyphrases, model permits comparison keyphrase annotations
semantic level.
remainder section provides set evaluations models ability capture
semantic content document text keyphrase annotations. Section 6.1 describes evaluation
systems ability extract meaningful semantic summaries individual documents,
also assesses quality paraphrase structure induced model. Section 6.2 extends
evaluation systems ability summarize multiple review documents.
6.1 Single-Document Evaluation
First, evaluate model respect ability reproduce annotations present individual documents, based document text. compare wide variety baselines
variations model, demonstrating appropriateness approach task. addition,
explicitly evaluate quality paraphrase structure induced model comparing
gold standard clustering keyphrases provided expert annotators.
6.1.1 E XPERIMENTAL ETUP
section, describe datasets evaluation techniques used experiments
system automatic methods. also comment hyperparameters tuned
model, sampling initialized.
Statistic
# reviews
avg. review length
avg. keyphrases / review

Restaurants
5735
786.3
3.42

Cell Phones
1112
1056.9
4.91

Digital Cameras
3971
1014.2
4.84

Table 3: Statistics datasets used evaluations
Data Sets evaluate system reviews three domains: restaurants, cell phones,
digital cameras. reviews downloaded Epinions website; used user-authored
pros cons associated reviews keyphrases (see Section 3). Statistics datasets
provided Table 3. domains, selected 50% documents training.
consider two strategies constructing test data. First, consider evaluating semantic
properties inferred system expert annotations semantic properties present
document. end, use expert annotations originally described Section 3 test

586

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

set;6 reiterate, annotations 170 reviews restaurant domain,
hold 50 development set. review texts annotated six properties according
standardized guidelines. strategy enforces consistency completeness ground truth
annotations, differentiating free-text annotations.
Unfortunately, ability evaluate expert annotations limited cost producing annotations. expand evaluation domains, use author-written keyphrase
annotations present original reviews. annotations noisy presence
property annotation document strong evidence document supports property,
inverse necessarily true. is, lack annotation necessarily imply
respective property hold e.g., review good service-related keyphrase may
still praise service body document.
experiments using free-text annotations, overcome pitfall restricting evaluation predictions individual properties documents annotated
property antonym. instance, evaluating prediction good service property,
select documents either annotated good service bad service-related
keyphrases.7 reason, semantic property evaluated unique subset documents. details development test sets presented Appendix A.
ensure free-text annotations reliably used evaluation, compare
results produced expert annotations whenever possible. shown Section 6.1.2, free-text
evaluations produce results cohere well obtained expert annotations, suggesting
labels used reasonable proxy expert annotation evaluations.
Evaluation Methods first evaluation leverages expert annotations described Section 3.
One complication expert annotations marked level semantic properties,
model makes predictions appropriateness individual keyphrases. address
representing expert annotation commonly-observed keyphrase
manually-annotated cluster keyphrases associated semantic property. example,
annotation semantic property good food represented common keyphrase realization, great food. evaluation checks whether keyphrase within clusters
keyphrases predicted model.
evaluation author free-text annotations similar evaluation expert
annotations. case, annotation takes form individual keyphrases rather semantic
properties. noted, author-generated keyphrases suffer inconsistency. obtain consistent
evaluation mapping author-generated keyphrase cluster keyphrases determined
expert annotator, selecting common keyphrase realization
cluster. example, author may use keyphrase tasty, maps semantic cluster
good food; select common keyphrase realization, great food. expert
evaluation, check whether keyphrase within clusters predicted model.
Model performance quantified using recall, precision, F-score. computed
standard manner, based models representative keyphrase predictions compared
corresponding references. Approximate randomization (Yeh, 2000; Noreen, 1989) used
statistical significance testing. test repeatedly performs random swaps individual results
6. expert annotations available http://groups.csail.mit.edu/rbg/code/precis/.
7. determination made mapping author keyphrases properties using expert-generated gold standard
clustering keyphrases. much cheaper produce expert clustering keyphrases obtain expert
annotations semantic properties every document.

587

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

candidate system, checks whether resulting performance gap remains least
large. use test valid comparing nonlinear functions random variables, F-scores, unlike common methods sign test. Previous work
used test include evaluations Message Understanding Conference (Chinchor, Lewis, &
Hirschman, 1993; Chinchor, 1995); recently, Riezler Maxwell (2005) advocated
use evaluating machine translation systems.
Parameter Tuning Initialization improve models convergence rate, perform two
initialization steps Gibbs sampler. First, sampling done keyphrase clustering
component model, ignoring document text. Second, fix clustering sample
remaining model parameters. two steps run 5,000 iterations each. full joint model
sampled 100,000 iterations. Inspection parameter estimates confirms model convergence. 2GHz dual-core desktop machine, multithreaded C++ implementation model
training takes two hours dataset.
model needs provided number clusters K.8 set K large enough
model learn effectively development set. restaurant data set K 20. cell
phones digital cameras, K set 30 40, respectively. values tuned using
development set. However, found long K large enough accommodate significant number keyphrase clusters, additional account topics keyphrases,
specific value K affect models performance. hyperparameters
adjusted based development set performance, though tuning extensive.
previously mentioned, obtain document properties examining probability mass
topic distribution assigned property. probability threshold set property via
development set, optimizing maximum F-score.
6.1.2 R ESULTS
section, report performance model, comparing array increasingly
sophisticated baselines model variations. first demonstrate learning clustering annotation keyphrases crucial accurate semantic prediction. Next, investigate impact
paraphrasing quality model accuracy considering expert-generated gold standard clustering keyphrases another comparison point; also consider alternative automatically computed
sources paraphrase information.
ease comparison, results experiments shown Table 5 Table 6,
summary baselines model variations Table 4.
Comparison Simple Baselines first evaluation compares model four nave
baselines. four treat keyphrases independent, ignoring latent paraphrase structure.
Random: keyphrase supported document probability one half.
results baseline computed expectation, rather actually run. baseline
expected recall 0.5, expectation select half correct
keyphrases. precision average proportion annotations test set
number possible annotations. is, test set size n properties, property
8. requirement could conceivably removed modeling cluster indices drawn Dirichlet
process prior.

588

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

Random

keyphrase supported document probability one half.

Keyphrase text

keyphrase supported document appears verbatim text.

Keyphrase classifier

separate support vector machine classifier trained keyphrase.
Positive examples documents labeled author
keyphrase; documents considered negative examples.
keyphrase supported document keyphrases classifier returns
positive prediction.

Heuristic keyphrase
classifier

Similar keyphrase classifier, except heuristic methods used attempt reduce noise training documents. Specifically wish
remove sentences discuss keyphrases positive examples.
heuristic removes positive examples sentences
word overlap given keyphrase.

Model cluster text

keyphrase supported document paraphrases appear
text. Paraphrasing based models keyphrase clusters.

Model cluster classifier

separate classifier trained cluster keyphrases. Positive examples documents labeled author keyphrase
cluster; documents negative examples. keyphrases
cluster supported document clusters classifier returns
positive prediction. Keyphrase clustering based model.

Heuristic model cluster
classifier

Similar model cluster classifier, except heuristic methods used reduce noise training documents. Specifically wish remove
positive examples sentences discuss keyphrases
clusters. heuristic removes positive examples sentences
word overlap keyphrases given cluster.
Keyphrase clustering based model.

Gold cluster model

variation model clustering keyphrases fixed
expert-created gold standard. text modeling parameters learned.

Gold cluster text

Similar model cluster text, except clustering keyphrases according expert-produced gold standard.

Gold cluster classifier

Similar model cluster classifier, except clustering keyphrases
according expert-produced gold standard.

Heuristic gold cluster
classifier

Similar heuristic model cluster classifier, except clustering
keyphrases according expert-produced gold standard.

Independent cluster model

variation model clustering keyphrases first learned
keyphrase similarity information only, separately text.
resulting independent clustering fixed text modeling parameters learned. variations key distinction full model
lack joint learning keyphrase clustering text topics.

Independent cluster text

Similar model cluster text, except clustering keyphrases
according independent clustering.

Independent cluster
classifier

Similar model cluster classifier, except clustering keyphrases
according independent clustering.

Heuristic independent
cluster classifier

Similar heuristic model cluster classifier, except clustering
keyphrases according independent clustering.

Table 4: summary baselines variations model compared.
589

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

Method
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

model
Random
Keyphrase text
Keyphrase classifier
Heuristic keyphrase classifier
Model cluster text
Model cluster classifier
Heuristic model cluster classifier
Gold cluster model
Gold cluster text
Gold cluster classifier
Heuristic gold cluster classifier
Independent cluster model
Independent cluster text
Independent cluster classifier
Heuristic independent cluster classifier

Recall
0.920
0.500
0.048
0.769
0.839
0.227
0.721
0.731
0.936
0.339
0.693
1.000
0.745
0.220
0.586
0.592

Restaurants
Prec. F-score
0.353 0.510
0.346 0.409
0.500 0.087
0.353 0.484
0.340 0.484
0.385 0.286
0.402 0.516
0.366 0.488
0.344 0.502
0.360 0.349
0.366 0.479
0.326 0.492
0.363 0.488
0.340 0.266
0.384 0.464
0.386 0.468

Table 5: Comparison property predictions made model series baselines
model variations restaurant domain, evaluated expert semantic annotations.
results divided according experiment. methods model
significantly better results using approximate randomization indicated
p 0.05, p 0.1.

590

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

Method
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

model
Random
Keyphrase text
Keyphrase classif.
Heur. keyphr. classif.
Model cluster text
Model cluster classif.
Heur. model classif.
Gold cluster model
Gold cluster text
Gold cluster classif.
Heur. gold classif.
Indep. cluster model
Indep. cluster text
Indep. cluster classif.
Heur. indep. classif.

Recall
0.923
0.500
0.077
0.905
0.997
0.416
0.859
0.910
0.992
0.541
0.865
0.997
0.984
0.382
0.753
0.881

Restaurants
Prec. F-score
0.623 0.744
0.500 0.500
0.906 0.142
0.527 0.666
0.497 0.664
0.613 0.496
0.711 0.778
0.567 0.698
0.500 0.665
0.604 0.571
0.720 0.786
0.499 0.665
0.528 0.687
0.569 0.457
0.696 0.724
0.478 0.619

Recall
0.971
0.500
0.171
1.000
0.845
0.829
0.876
1.000
0.924
0.914
0.810
0.969
0.838
0.724
0.638
1.000

Cell Phones
Prec. F-score
0.537 0.692
0.489 0.494
0.529 0.259
0.500 0.667
0.474 0.607
0.547 0.659
0.561 0.684
0.464 0.634
0.561 0.698
0.497 0.644
0.559 0.661
0.468 0.631
0.564 0.674
0.481 0.578
0.472 0.543
0.464 0.634

Digital Cameras
Recall Prec. F-score
0.905 0.586 0.711
0.500 0.501 0.500
0.715 0.642 0.676
0.942 0.540 0.687
0.845 0.531 0.652
0.812 0.596 0.687
0.927 0.568 0.704
0.942 0.568 0.709
0.962 0.510 0.667
0.903 0.522 0.661
0.874 0.674 0.761
0.971 0.508 0.667
0.945 0.519 0.670
0.469 0.476 0.473
0.496 0.588 0.538
0.969 0.501 0.660

Table 6: Comparison property predictions made model series baselines
model variations three product domains, evaluated author free-text annotations. results divided according experiment. methods
model significantly better results using approximate randomization indicated
p 0.05, p 0.1. Methods perform significantly better
model p 0.05 indicated .

591

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

P
ni
appears ni times, expected precision
i=1 mn . instance, restaurants
gold standard evaluation, six tested properties appeared total 249 times 120
documents, yielding expected precision 0.346.
Keyphrase text: keyphrase supported document appears verbatim
text. Precision high recall low, model unable detect
paraphrases keyphrase text. instance, first review Figure 1,
cleanliness would supported appears text; however, healthy would
supported, even though synonymous great nutrition appear.
Keyphrase classifier:9 separate discriminative classifier trained keyphrase. Positive examples documents labeled author keyphrase; documents considered negative examples. Consequently, particular keyphrase,
documents labeled synonymous keyphrases would among negative examples.
keyphrase supported document keyphrases classifier returns positive prediction.
use support vector machines, built using SVMlight (Joachims, 1999) features
model, i.e.,word counts.10 partially circumvent imbalanced positive/negative
data problem, tuned prediction thresholds development set maximize F-score,
manner tuned thresholds model.
Heuristic keyphrase classifier: baseline similar keyphrase classifier above, attempts mitigate noise inherent training data. Specifically, given
positive example document may contain text unrelated given keyphrase. attempt
reduce noise removing positive examples sentences word
overlap given keyphrase. keyphrase supported document keyphrases
classifier returns positive prediction.11
Lines 2-5 Tables 5 6 present results, using gold annotations original
authors annotations testing. model outperforms three baselines evaluations
strong statistical significance.
keyphrase text baseline fares poorly: F-score random baseline three
four evaluations. expected, recall baseline usually low requires
keyphrases appear verbatim text. precision somewhat better, presence
significant number false positives indicates presence keyphrase text
necessarily reliable indicator associated semantic property.
Interestingly, one domain keyphrase text perform well digital cameras.
believe prevalence specific technical terms keyphrases used
domain, zoom battery life. technical terms also frequently used
review text, making recall keyphrase text substantially higher domain
evaluations.
9. Note classifier results reported initial publication (Branavan, Chen, Eisenstein, & Barzilay, 2008)
obtained using default parameters maximum entropy classifier. Tuning classifiers parameters allowed us
significantly improve performance classifier baselines.
10. general, SVMs additional advantage able incorporate arbitrary features, sake
comparison restrict using features across methods.
11. thank reviewer suggesting baseline.

592

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

keyphrase classifier baseline outperforms random keyphrase text baselines,
still achieves consistently lower performance model four evaluations. Notably,
performance heuristic keyphrase classifier worse keyphrase classifier except one case.
alludes difficulty removing noise inherent document text.
Overall, results indicate methods learn predict keyphrases without accounting intrinsic hidden structure insufficient optimal property prediction. leads us
toward extending present baselines clustering information.
important assess consistency evaluation based free-text annotations (Table 6) evaluation uses expert annotations (Table 5). absolute scores
expert annotations dataset lower scores free-text annotations, ordering performance various automatic methods across two evaluation scenarios.
consistency maintained rest experiments well, indicating purpose
relative comparison different automatic methods, method evaluating
free-text annotations reasonable proxy evaluation expert-generated annotations.
Comparison Clustering-based Approaches previous section demonstrates
model outperforms baselines account paraphrase structure keyphrases.
ask whether possible enhance baselines performance augmenting
keyphrase clustering induced model. Specifically, introduce three systems, none
true baselines, since use information inferred model.
Model cluster text: keyphrase supported document paraphrases
appears text. Paraphrasing based models clustering keyphrases.
use paraphrasing information enhances recall potential cost precision, depending
quality clustering. example, assuming healthy great nutrition
clustered together, presence healthy text would also indicate support great
nutrition, vice versa.
Model cluster classifier: separate discriminative classifier trained cluster
keyphrases. Positive examples documents labeled author keyphrase
cluster; documents negative examples. keyphrases cluster
supported document clusters classifier returns positive prediction. Keyphrase
clustering based model. keyphrase classifier, use support vector machines trained word count features, tune prediction thresholds individual cluster development set.
Another perspective model cluster classifier augments simplistic text modeling
portion model discriminative classifier. Discriminative training often considered powerful equivalent generative approaches (McCallum et al., 2005),
leading us expect high level performance system.
Heuristic model cluster classifier: method similar model cluster classifier above,
additional heuristics used reduce noise inherent training data. Positive
example documents may contain text unrelated given cluster. reduce noise,
sentences word overlap clusters keyphrases removed.
keyphrases cluster supported document clusters classifier returns positive prediction. Keyphrase clustering based model.
593

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

Lines 6-8 Tables 5 6 present results methods. expected, using clustering
keyphrases baseline methods substantially improves recall, low impact
precision. Model cluster text invariably outperforms keyphrase text recall keyphrase
text improved addition clustering information, though precision worse cases.
phenomenon holds even cameras domain, keyphrase text already performs well.
However, model still significantly outperforms model cluster text evaluations.
Adding clustering information classifier baseline results performance sometimes
better models. result surprising, model cluster classifier gains
benefit models robust clustering learning sophisticated classifier assigning
properties texts. resulting combined system complex model itself,
potential yield better performance. hand, using simple heuristic reduce
noise present training data consistently hurts performance classifier, possibly
due reduction amount training data.
Overall, enhanced performance methods, contrast keyphrase baselines,
aligned previous observations entailment research (Dagan, Glickman, & Magnini, 2006),
confirming paraphrasing information contributes greatly improved performance semantic
inference tasks.
Impact Paraphrasing Quality previous section demonstrates one central
claims paper: accounting paraphrase structure yields substantial improvements semantic inference using noisy keyphrase annotations. second key aspect research
idea clustering quality benefits tying clusters hidden topics document
text. evaluate claim comparing models clustering independent clustering
baseline. also compare gold standard clustering produced expert human annotators. test impact clustering methods, substitute models inferred clustering
alternative examine resulting semantic inferences change. comparison
performed semantic inference mechanism model, well model cluster
text, model cluster classifier heuristic model cluster classifier baselines.
add gold standard clustering model, replace hidden variables correspond keyphrase clusters observed values set according gold standard clustering.12 parameters trained modeling text. model variation, gold
cluster model, predicts properties using inference mechanism original model.
baseline variations gold cluster text, gold cluster classifier heuristic gold cluster classifier
likewise derived substituting automatically computed clustering gold standard clusters.
additional clustering obtained using keyphrase similarity information. Specifically, modify original model learns keyphrase clustering isolation
text, learns property language models. framework, keyphrase clustering
entirely independent review text, text modeling learned keyphrase
clustering fixed. refer modification model independent cluster model.
model treats document text mixture latent topics, reminiscent models
supervised latent Dirichlet allocation (sLDA; Blei & McAuliffe, 2008), labels acquired
performing clustering across keyphrases preprocessing step. previous experiment, introduce three new baseline variations independent cluster text, independent cluster
classifier heuristic independent cluster classifier.
12. gold standard clustering created part evaluation procedure described Section 6.1.1.

594

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

Lines 9-16 Tables 5 6 present results experiments. gold cluster model
produces F-scores comparable original model, providing strong evidence clustering
induced model sufficient quality semantic inference. application expertgenerated clustering baselines (lines 10, 11 12) yields less consistent results, overall
evaluation provides little reason believe performance would substantially improved
obtaining clustering closer gold standard.
independent cluster model consistently reduces performance respect full joint
model, supporting hypothesis joint learning gives rise better prediction. independent
clustering baselines, independent cluster text, independent cluster classifier heuristic independent cluster classifier (lines 14 16), also worse counterparts use model
clustering (lines 6 8). observation leads us conclude expert-annotated
clustering always improve results, independent clustering always degrades them.
supports view joint learning clustering text models important prerequisite
better property prediction.
Clustering
Model clusters
Independent clusters

Restaurants
0.914
0.892

Cell Phones
0.876
0.759

Digital Cameras
0.945
0.921

Table 7: Rand Index scores models clusters, learned keyphrases text jointly, compared clusters learned keyphrase similarity. Evaluation cluster quality
based gold standard clustering.

Another way assessing quality automatically-obtained keyphrase clustering
quantify similarity clustering produced expert annotators. purpose
use Rand Index (Rand, 1971), measure cluster similarity. measure varies zero
one, higher scores indicating greater similarity. Table 7 shows Rand Index scores
models full joint clustering, well clustering obtained independent cluster model.
every domain, joint inference produces overall clustering improves upon keyphrasesimilarity-only approach. scores confirm joint inference across keyphrases
document text produces better clustering considering features keyphrases alone.
6.2 Summarizing Multiple Reviews
last experiment examines multi-document summarization capability system.
study models ability aggregate properties across set reviews, compared baselines
aggregate directly using free-text annotations.
6.2.1 DATA E VALUATION
selected 50 restaurants, five user-written reviews restaurant. Ten annotators
asked annotate reviews five restaurants each, comprising 25 reviews per annotator.
used six salient properties annotation guidelines previous restaurant
annotation experiment (see Section 3). constructing ground truth, label properties
supported least three five reviews.

595

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

Method
model
Keyphrase aggregation
Model cluster aggregation
Gold cluster aggregation
Indep. cluster aggregation

Recall
0.905
0.036
0.238
0.226
0.214

Prec.
0.325
0.750
0.870
0.826
0.720

F-score
0.478
0.068
0.374
0.355
0.330

Table 8: Comparison aggregated property predictions made model series
baselines use free-text annotations. methods model significantly better results using approximate randomization indicated p 0.05.

make property predictions set reviews model baselines
presented below. automatic methods, register prediction system judges
property supported least two five reviews.13 recall, precision, F-score
computed aggregate predictions, six salient properties marked annotators.
6.2.2 AGGREGATION PPROACHES
evaluation, run trained version model described Section 6.1.1. Note
keyphrases provided model, though provided baselines.
obvious baseline summarizing multiple reviews would directly aggregate
free-text keyphrases. annotations presumably representative reviews semantic
properties, unlike review text, keyphrases matched directly other. first
baseline applies notion directly:
Keyphrase aggregation: keyphrase supported restaurant least two five
reviews annotated verbatim keyphrase.
simple aggregation approach obvious downside requiring strict matching independently authored reviews. reason, consider extensions aggregation
approach allow annotation paraphrasing:
Model cluster aggregation: keyphrase supported restaurant least two
five reviews annotated keyphrase one paraphrases. Paraphrasing
according models inferred clustering.
Gold cluster aggregation: model cluster aggregation, using expert-generated
clustering paraphrasing.
Independent cluster aggregation: model cluster aggregation, using clustering
learned keyphrase similarity paraphrasing.
13. three corroborating reviews required, baseline systems produce positive predictions, leading
poor recall. Results setting presented Appendix B.

596

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

6.2.3 R ESULTS
Table 8 compares baselines model. model outperforms annotationbased baselines, despite access keyphrase annotations. Notably, keyphrase aggregation performs poorly, makes predictions, result requirement
exact keyphrase string match. before, inclusion keyphrase clusters improves performance baseline models. However, incompleteness keyphrase annotations (see
Section 3) explains recall scores still low compared model. incorporating
document text, model obtains dramatically improved recall, cost reduced precision,
ultimately yielding significantly improved F-score.
results demonstrate review summarization benefits greatly joint model
review text keyphrases. Nave approaches consider keyphrases yield inferior results,
even augmented paraphrase information.

7. Conclusions Future Work
paper, shown free-text keyphrase annotations provided novice users
leveraged training set document-level semantic inference. Free-text annotations
potential vastly expand set training data available developers semantic inference
systems; however, shown, suffer lack consistency completeness.
overcome problems inducing hidden structure semantic properties, correspond
clusters keyphrases hidden topics text. approach takes form
hierarchical Bayesian model, addresses text keyphrases jointly.
model implemented system successfully extracts semantic properties unannotated restaurant, cell phone, camera reviews, empirically validating approach. experiments demonstrate necessity handling paraphrase structure free-text keyphrase
annotations; moreover, show better paraphrase structure learned joint framework
also models document text. approach outperforms competitive baselines semantic
property extraction single multiple documents. also permits aggregation across
multiple keyphrases different surface forms multi-document summarization.
work extends actively growing literature document topic modeling. topic modeling paraphrasing posit hidden layer captures relationship disparate surface
forms: topic modeling, set latent distributions lexical items, paraphrasing
represented latent clustering phrases. show two latent structures linked,
resulting increased robustness semantic coherence.
see several avenues future work. First, model draws substantial power features measure keyphrase similarity. ability use arbitrary similarity metrics desirable;
however, representing individual similarity scores random variables compromise,
clearly independent. believe problem could avoided modeling generation
entire similarity matrix jointly.
related approach would treat similarity matrix across keyphrases indicator
covariance structure. model, would learn separate language models keyphrase,
keyphrases rated highly similar would constrained induce similar language
models. approach might possible Gaussian process framework (Rasmussen &
Williams, 2006).

597

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

Currently focus model identify semantic properties expressed given
document, allows us produce summary properties. However, mentioned
Section 3, human authors give equal importance properties producing summary
pros cons. One possible extension work would explicitly model likelihood
topic annotated document. might avoid current post-processing step
uses property-specific thresholds compute final predictions model output.
Finally, assumed semantic properties unstructured. reality,
properties related interesting ways. Trivially, domain reviews would desirable
model antonyms explicitly, e.g., restaurant review simultaneously labeled
good bad food. relationships properties, hierarchical structures, could
also considered. suggests possible connections correlated topic model Blei
Lafferty (2006).

Bibliographic Note
Portions work previously presented conference publication (Branavan et al., 2008).
current article extends work several ways, notably: development evaluation
multi-document review summarization system uses semantic properties induced
method (Section 6.2); detailed analysis distributional properties free-text annotations
(Section 3); expansion evaluation include additional domain sets baselines
considered original paper (Section 6.1.1).

Acknowledgments
authors acknowledge support National Science Foundation (NSF) CAREER grant IIS0448168, Microsoft Research New Faculty Fellowship, U.S. Office Naval Research
(ONR), Quanta Computer, Nokia Corporation. Harr Chen supported National Defense Science Engineering NSF Graduate Fellowships. Thanks Michael Collins, Zoran
Dzunic, Amir Globerson, Aria Haghighi, Dina Katabi, Kristian Kersting, Terry Koo, Yoong Keok
Lee, Brian Milch, Tahira Naseem, Dan Roy, Christina Sauper, Benjamin Snyder, Luke Zettlemoyer,
journal reviewers helpful comments suggestions. also thank Marcia Davidson
members NLP group MIT help expert annotations. opinions, findings,
conclusions recommendations expressed article authors, necessarily reflect views NSF, Microsoft, ONR, Quanta, Nokia.

598

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

Appendix A. Development Test Set Statistics
Table 9 lists semantic properties domain number documents used
evaluating properties. noted Section 6.1.1, gold standard evaluation
complete, testing every property document. Conversely, free-text evaluations
property use documents annotated property antonym
number documents differs semantic property.
Domain
Restaurants (gold)
Restaurants

Cell Phones

Cameras

Property
properties
Good food
Bad food
Good price
Bad price
Good service
Bad service
Good reception
Bad reception
Good battery life
Poor battery life
Good price
Bad price
Small
Large
Good price
Bad price
Good battery life
Poor battery life
Great zoom
Limited zoom

Development documents
50

Test Documents
120

88

179

31

66

69

140

33

67

59

120

28

57

84

168

56

113

51

102

34

69

Table 9: Breakdown property development test sets used evaluations section 6.1.2.

599

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

Appendix B. Additional Multiple Review Summarization Results
Table 10 lists results multi-document experiment, variation aggregation
require automatic method predict property three five reviews predict
property product, rather two presented Section 6.2. baseline systems,
change causes precipitous drop recall, leading F-score results substantially worse
presented Section 6.2.3. contrast, F-score model consistent across
evaluations.
Method
model
Keyphrase aggregation
Model cluster aggregation
Gold cluster aggregation
Indep. cluster aggregation

Recall
0.726
0.000
0.024
0.036
0.036

Prec.
0.365
0.000
1.000
1.000
1.000

F-score
0.486
0.000
0.047
0.068
0.068

Table 10: Comparison aggregated property predictions made model series
baselines use free-text annotations. Aggregation requires three five reviews
predict property, rather two Section 6.2. methods
model significantly better results using approximate randomization indicated
p 0.05.

Appendix C. Hyperparameter Settings
Table 11 lists values hyperparameters 0 , 0 , 0 used experiments domain.
values arrived tuning development set. cases, 0 set
(1, 1), making Beta(0 ) uniform distribution.
Hyperparameters
0
0
0

Restaurants
0.0001
0.001
0.001

Cell Phones
0.0001
0.0001
0.0001

Cameras
0.0001
0.1
0.001

Table 11: Values hyperparameters used domain across experiments.

600

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

References
Barzilay, R., McKeown, K., & Elhadad, M. (1999). Information fusion context multidocument summarization. Proceedings ACL, pp. 550557.
Barzilay, R., & McKeown, K. R. (2001). Extracting paraphrases parallel corpus. Proceedings ACL, pp. 5057.
Bhattacharya, I., & Getoor, L. (2006). latent Dirichlet model unsupervised entity resolution.
Proceedings SIAM International Conference Data Mining.
Blei, D. M., & Lafferty, J. D. (2006). Correlated Topic Models. Advances NIPS, pp. 147154.
Blei, D. M., & McAuliffe, J. (2008). Supervised topic models. Advances NIPS, pp. 121128.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal Machine
Learning Research, 3, 9931022.
Boyd-Graber, J., Blei, D., & Zhu, X. (2007). topic model word sense disambiguation.
Proceedings EMNLP, pp. 10241033.
Branavan, S. R. K., Chen, H., Eisenstein, J., & Barzilay, R. (2008). Learning document-level semantic properties free-text annotations. Proceedings ACL, pp. 263271.
Carbonell, J., & Goldstein, J. (1998). use MMR, diversity-based reranking reordering
documents producing summaries. Proceedings ACM SIGIR, pp. 335336.
Chinchor, N. (1995). Statistical significance MUC-6 results. Proceedings 6th Conference
Message Understanding, pp. 3943.
Chinchor, N., Lewis, D. D., & Hirschman, L. (1993). Evaluating message understanding systems:
analysis third message understanding conference (MUC-3). Computational Linguistics, 19(3), 409449.
Cohen, J. (1960). coefficient agreement nominal scales. Educational Psychological
Measurement, 20(1), 3746.
Dagan, I., Glickman, O., & Magnini, B. (2006). PASCAL recognising textual entailment challenge. Lecture Notes Computer Science, 3944, 177190.
Elhadad, N., & McKeown, K. R. (2001). Towards generating patient specific summaries medical
articles. Proceedings NAACL Workshop Automatic Summarization, pp. 3240.
Finkel, J. R., Grenager, T., & Manning, C. (2005). Incorporating non-local information information extraction systems Gibbs sampling. Proceedings ACL, pp. 363370.
Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2004). Bayesian Data Analysis (2nd edition).
Texts Statistical Science. Chapman & Hall/CRC.
Goldwater, S., Griffiths, T. L., & Johnson, M. (2006). Contextual dependencies unsupervised
word segmentation. Proceedings ACL, pp. 673680.
Hu, M., & Liu, B. (2004). Mining summarizing customer reviews. Proceedings SIGKDD,
pp. 168177.
Joachims, T. (1999). Making Large-Scale Support Vector Machine Learning Practical, pp. 169184.
MIT Press.

601

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

Kim, S.-M., & Hovy, E. (2006). Automatic identification pro con reasons online reviews.
Proceedings COLING/ACL, pp. 483490.
Lin, D., & Pantel, P. (2001). Discovery inference rules question-answering. Natural Language
Engineering, 7(4), 343360.
Liu, B., Hu, M., & Cheng, J. (2005). Opinion observer: Analyzing comparing opinions
web. Proceedings WWW, pp. 342351.
Lu, Y., & Zhai, C. (2008). Opinion integration semi-supervised topic modeling. Proceedings WWW, pp. 121130.
Mani, I., & Bloedorn, E. (1997). Multi-document summarization graph search matching.
Proceedings AAAI, pp. 622628.
Marsi, E., & Krahmer, E. (2005). Explorations sentence fusion. Proceedings European
Workshop Natural Language Generation, pp. 109117.
McCallum, A., Bellare, K., & Pereira, F. (2005). conditional random field discriminativelytrained finite-state string edit distance. Proceedings UAI, pp. 388395.
Nenkova, A., Vanderwende, L., & McKeown, K. (2006). compositional context sensitive multidocument summarizer: exploring factors influence summarization. Proceedings
SIGIR, pp. 573580.
Noreen, E. (1989). Computer-Intensive Methods Testing Hypotheses: Introduction. John
Wiley Sons.
Popescu, A.-M., Nguyen, B., & Etzioni, O. (2005). OPINE: Extracting product features opinions reviews. Proceedings HLT/EMNLP, pp. 339346.
Purver, M., Kording, K. P., Griffiths, T. L., & Tenenbaum, J. B. (2006). Unsupervised topic modelling multi-party spoken discourse. Proceedings COLING/ACL, pp. 1724.
Radev, D., Jing, H., & Budzikowska, M. (2000). Centroid-based summarization multiple documents: Sentence extraction, utility-based evaluation user studies. Proceedings
ANLP/NAACL Summarization Workshop.
Radev, D., & McKeown, K. (1998). Generating natural language summaries multiple on-line
sources. Computational Linguistics, 24(3), 469500.
Rand, W. M. (1971). Objective criteria evaluation clustering methods. Journal
American Statistical Association, 66(336), 846850.
Rasmussen, C. E., & Williams, C. K. I. (2006). Gaussian Processes Machine Learning. MIT
Press.
Riezler, S., & Maxwell, J. T. (2005). pitfalls automatic evaluation significance
testing MT. Proceedings ACL Workshop Intrinsic Extrinsic Evaluation
Measures Machine Translation and/or Summarization, pp. 5764.
Snyder, B., & Barzilay, R. (2007). Multiple aspect ranking using good grief algorithm.
Proceedings NAACL/HLT, pp. 300307.
Titov, I., & McDonald, R. (2008a). joint model text aspect ratings sentiment summarization. Proceedings ACL, pp. 308316.

602

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

Titov, I., & McDonald, R. (2008b). Modeling online reviews multi-grain topic models.
Proceedings WWW, pp. 111120.
Toutanova, K., & Johnson, M. (2008). Bayesian LDA-based model semi-supervised part-ofspeech tagging. Advances NIPS, pp. 15211528.
White, M., Korelsky, T., Cardie, C., Ng, V., Pierce, D., & Wagstaff, K. (2001). Multi-document
summarization via information extraction. Proceedings HLT, pp. 17.
Yeh, A. (2000). accurate tests statistical significance result differences. Proceedings COLING, pp. 947953.
Zaenen, A. (2006). Mark-up barking wrong tree. Computational Linguistics, 32(4), 577580.

603

fiJournal Artificial Intelligence Research 34 (2009) 339-389

Submitted 06/08; published 03/09

Identification Pleonastic Using Web
Yifan Li
Petr Musilek
Marek Reformat
Loren Wyard-Scott

yifan@ece.ualberta.ca
musilek@ece.ualberta.ca
reform@ece.ualberta.ca
wyard@ece.ualberta.ca

Department Electrical Computer Engineering
University Alberta
Edmonton, AB T6G 2V4 Canada

Abstract
significant minority cases, certain pronouns, especially pronoun it,
used without referring specific entity. phenomenon pleonastic pronoun
usage poses serious problems systems aiming even shallow understanding natural
language texts. paper, novel approach proposed identify uses it:
extrapositional cases identified using series queries web, cleft
cases identified using simple set syntactic rules. system evaluated four
sets news articles containing 679 extrapositional cases well 78 cleft constructs.
identification results comparable obtained human efforts.

1. Introduction
Anaphora resolution, associates word phrase (the anaphor) previously
mentioned entity (the antecedent), active field Natural Language Processing (NLP)
research. important role many applications non-trivial level understanding natural language texts desired, notably information extraction
machine translation. illustrate, information extraction system trying keep track
corporate activities may find dealing news Microsoft today announced
adopting XML default file format next major version Microsoft
Office software . . . would impossible provide insight Microsofts
intention without associating pronominal anaphors antecedent,
Microsoft.
Adding already complex problem finding correct antecedent, pronouns
always used fashion shown earlier example. well-known
pronouns, especially it, occur without referring nominal antecedent,
antecedent all. Pronouns used without antecedent, often referred
pleonastic structural, pose serious problem anaphora resolution systems. Many
anaphora resolution systems underestimate issue choose implement specific
module handle pleonastic pronouns instead input sanitized manually
exclude cases. However, high frequency pronoun usage general pleonastic
cases particular warrants phenomenon deserves serious treatment.
pronoun it, accounts pleonastic pronoun usages, far
frequently used pronouns British National Corpus (BNC). Wall Street
Journal Corpus (WSJ; Marcus, Marcinkiewicz, & Santorini, 1993), upon study
2009 AI Access Foundation. rights reserved.

fiLi, Musilek, Reformat, & Wyard-Scott

based, accounts 30% personal pronoun usage. percentage
cases lacks nominal antecedent also significant: previous studies reported
figures 16% 50% (Gundel, Hedberg, & Zacharski, 2005) analysis
based upon WSJ corpus results value around 25%, half
pleonastic cases.
Applying criteria similar established Gundel et al. (2005), usage
generally categorized follows. instances analyzed shown
italics; corresponding antecedents, extraposed clauses, clefted constituents
marked underlining.
1. Referential nominal antecedent
[0006:002]1 thrift holding company said expects obtain regulatory approval
complete transaction year-end.
refers thrift holding company.
2. Referential clause antecedent
[0041:029] board insurance company financial problems,
insists made secret it.
refers fact person board insurance company.
[0102:002-003] Everyone agrees nations old bridges need
repaired replaced. theres disagreement it.
it, together do, refers action repairing replacing bridge.
3. antecedent Pleonastic
(a) Extraposition
[0034:020] doesnt take much get burned.
infinitive clause get burned extraposed original position
filled expletive it. equivalent non-extraposed sentence get
burned doesnt take much.
[0037:034] shame meeting never took place.
equivalent non-extraposed sentence meeting never took place
shame.
(b) Cleft2
[0044:026] disturbing, educators, students, blamed
much wrongdoing.
equivalent non-cleft version disturbing, educators, students,
blamed much wrongdoing.
1

example sentences selected WSJ corpus, locations encoded format [article:sentence].
2
claim cleft pronouns classified expletive (Gundel, 1977; Hedberg, 2000).
Nevertheless, change fact pronouns nominal antecedents; hence clefts
included analysis.

340

fiIdentification Pleonastic Using Web

[0591:021] partly reason exchange last week began trading
stock basket product . . .
equivalent non-cleft version exchange last week began trading
stock basket product partly reason.
(c) Local Situation
[0207:037] unpleasant evening . . .
category consists instances related weather, time, distance,
information local situation. Since texts reviewed study lack
instances subtypes, weather time cases discussed.
4. Idiomatic
[0010:010] governor couldnt make it, lieutenant governor welcomed
special guests.
paper focuses pleonastic cases (the third category), subclass carries
unique syntactic and/or semantic signatures. idiomatic category, consisting
non-anaphoric cases well, less coherent identification much subjective
nature, making less attractive target.
paper organized follows: Section 2 provides brief survey related work
toward classification identification pleonastic it; Section 3 proposes
web-based approach identification pleonastic it; Section 4 demonstrates proposed
method case study; Section 5 follows evaluation; finally, Section 6 discusses
findings presents ideas future work.

2. Previous Work
Evans (2001) pointed out, usage covered serious surveys English
grammar, (e.g. Sinclair, 1995) also provide classifications based semantic
categories. recent study, Gundel et al. (2005) classify third-person personal pronouns
following comprehensive hierarchy:
Noun phrase (NP) antecedent
Inferrable
Non-NP antecedent
Fact
Situation

Proposition
Reason

Activity

Event

Full cleft
Atmospheric

Truncated cleft
pleonastic

Pleonastic
Full extraposition
Truncated extraposition
Idiom
Exophoric
Indeterminate
341

fiLi, Musilek, Reformat, & Wyard-Scott

Without going details category, apparent length
list phenomenon pleonastic it, generally pronouns without explicit
nominal antecedents, painstakingly studied linguists. However, despite
identified one open issues anaphora resolution (Mitkov, 2001), work automatic
identification pleonastic relatively scarce. date, existing studies area fall
one two categories: one wherein rule-based approach used, using
machine-learning approach.

2.1 Rule-based Approaches
Paice Husk (1987) together Lappin Leass (1994) provide examples rulebased systems make use predefined syntactic patterns word lists. Paice
Husk approach employs bracketing patterns . . . . . . meet
syntactic restrictions extraposition cleft. matched portions sentences
evaluated rules represented word lists. example, . . . rule
prescribes one task status words, good bad, must present amid
construct. order reduce false positives, general restrictions applied sentence
features construct length intervening punctuation.
Lappin Leasss (1994) approach employs set detailed rules
Modaladj Cogv-ed S, Modaladj Cogv predefined
lists modal adjectives (e.g. good useful ) cognitive verbs (e.g. think believe),
respectively. Compared Paice Husks (1987) approach, method much
restrictive, especially rigidly-specified grammatical constraints. example,
clear original Lappin Leass paper whether system would able
recognize sentences [0146:014] isnt clear, however, whether . . . despite claim
system takes syntactic variants consideration.
Lappin Leasss (1994) approach part larger system, evaluation
provided. Paice Husk (1987) approach, hand, evaluates impressively.
accuracy 93.9% determining pleonastic constructs data used
rule development, without using part-of-speech tagging parsing.
rule-based systems rely patterns represent syntactic constraints word
lists represent semantic constraints. makes relatively easy implement
maintain. However, features also make less scalable challenged
large unfamiliar corpora, accuracies deteriorate. example, Paice Husk
(1987) noticed nearly 10% decrease accuracy rules developed using one subset
corpus applied another subset without modifications. Boyd, Gegg-Harrison,
Byron (2005) also observed significant performance penalty approach
applied different corpus. words, rule-based systems good
designed be. Denber (1998) suggested using WordNet (Fellbaum, 1998) extend
word lists, doubtful helpful would considering enormous number
possible words included existing lists number inapplicable words
identified approach.
342

fiIdentification Pleonastic Using Web

2.2 Machine-learning Approaches
Recent years seen shift toward machine-learning approaches, shed new light
issue. Studies Evans (2001, 2000) Boyd et al. (2005) examples class.
systems employ memory-based learning grammatical feature vectors; Boyd et al.s
approach also includes decision tree algorithm produces less ideal results.
attempt place uses seven categories, including pleonastic nominal anaphoric
among others, Evans uses 35 features encode information position/proximity,
lemmas, part-of-speech, related pronoun components interest,
words noun phrases, sentence. Evans reported 73.38% precision
69.25% recall binary classification pleonastic cases, overall binary classification
accuracy 71.48%. later study featuring MARS3 , fully automatic pronoun resolution
system employs approach, Mitkov, Evans, Orasan (2002) reported
significantly higher binary classification accuracy 85.54% approach applied
technical manuals.
Boyd et al.s (2005) approach targets pleonastic alone. uses 25 features,
concern lengths specific syntactic structures; also included part-of-speech
information lemmas verbs. study reports overall precision 82% recall
71%, and, specifically, recalls extrapositional cleft constructs 81%
45%, respectively.
addition, Clemente, Torisawa, Satou (2004) used support vector machines
feature-set similar proposed Evans (2001) analyze biological medical texts,
reported overall accuracy 92.7% higher memory-based
learning implementation. Ng Cardie (2002) built decision tree binary anaphoricity
classification types noun phrases (including pronouns) using C4.5 induction
algorithm. Ng Cardie reported overall accuracies 86.1% 84.0% MUC-6
MUC-7 data sets. Categorical results, however, reported possible
determine systems performance pronouns. Using automatically induced rules,
Muller (2006) reported overall accuracy 79.6% detecting non-referential
spoken dialogs. inter-annotator agreement study conducted paper indicates
difficult even humans classify instances spoken dialogs. finding
supported experiences.
Machine-learning approaches able partly circumvent restrictions imposed
fixed word lists rigid grammatical patterns learning. However, advantage
also comes price training required initial development phase
different corpora re-training preferable since lemmas part feature sets. Since
existing approaches fall within area supervised learning (i.e. training data need
manually classified), limited number lemmas gather training may
lead degraded performance unfamiliar circumstances. Moreover, features used
learning unable reliably capture subtleties original sentences, especially considering non-technical documents. example, quantitative features
frequently used machine-learning approaches, position distance, become less
reliable sentences contain large number adjuncts. Additionally, meanings
lemmas often domain-dependent vary local structural lexical
3

Available online http://clg.wlv.ac.uk/demos/MARS/

343

fiLi, Musilek, Reformat, & Wyard-Scott

environment nuances cannot captured lemma features alone. short,
machine-learning approaches generally deliver better performance classifying
rule-based counterparts do, inherent problems.

3. Web Based Approach
syntactic patterns semantics various clause constituents play important roles
determining third-person personal pronoun pleonastic. role grammar quite
obvious since extrapositions clefts must follow grammatical patterns
defined. example, commonly seen type it-extraposition follows
pattern:
+ copula + status + subordinate clause
[0089:017]

easy
see ancient art ropes.
contrast, role semantics plays little obscure one sits starts
dream exceptions (Paice & Husk, 1987) analogous [0074:005] . . . taken
measures continue shipments work stoppage. vis-a-vis [0367:044] . . . didnt
take rocket scientist change road bike mountain bike . . . , referential
pleonastic cases share syntactic structure. Despite less overt role, failure
process semantic information result severe degradation performance.
observation supported word-list-based systems dramatic decay accuracy
confronted text obtained word lists from.
Like every classification system, proposed system strives cover many cases
possible time perform classification accurately possible. achieve
this, attempts make good use syntactic semantic information embedded
sentences. set relaxed yet highly relevant syntactic patterns first applied
input text filter syntactically inviable cases. Unlike matching routines
previous approaches, process avoids detailed specification syntactic patterns. Instead,
tries include every piece text containing construct possible interest. Different
levels semantic examinations performed subtype pleonastic constructs.
reasons discussed later Section 3.2.2, semantic analysis performed clefts.
WordNet-based analysis used identify weather/time cases among
samples examined systems development stage, cases pertaining class
relatively uniform manner expression. complex populous class,
extrapositions, candidates subjected series tests performed queries
web. Results queries provide direct evidence specific configuration
clause constituents generally used.
reason corpus-based approach chosen versus applying manually constructed knowledge sources, word list WordNet, fourfold:
1. Manually constructed knowledge sources, regardless comprehensive are,
contain small portion general world knowledge. particular settings
study, general world knowledge used making judgements
words allowed serve matrix verb extraposition, even
subtle, specific sense word permitted.
344

fiIdentification Pleonastic Using Web

2. Manually compiled knowledge sources subject specific manners organization
may satisfy systems needs. Taking WordNet example, identifies
large number various relationships among entities, information mainly
organized along axes synonyms, hypernyms (kind-of relationship), holonyms
(part-of relationship) etc., surroundings particular word
interest study.
3. Natural languages evolving quickly. Taking English example, year new
words incorporated language4 rules grammar
immune changes either. Using large frequently-updated corpus
web allows system automatically adapt changes language.
4. importantly, corpora collect empirical evidence language usage.
sample size large enough, case web, statistics specific
construct generally used corpora employed indicator speakers
intention.
proposed approach also inspired Hearsts (1992) work mining semantic
relationships using text patterns, many quests followed direction (Berland & Charniak, 1999; Poesio, Ishikawa, im Walde, & Vieira, 2002; Markert,
Nissim, & Modjeska, 2003; Cimiano, Schmidt-Thieme, Pivk, & Staab, 2005). Unlike
investigations focus semantic relationship among noun phrases, pleonastic
pronoun identification problem mandates complex queries built according
original sentences. However, binary nature problem also makes simpler
apply comparative analysis results multiple queries, which, turn, leads better
immunity noise.
Figure 1 illustrates general work flow proposed system. sentence first
preprocessed obtain dependency tree part-of-speech tags, passed
syntactic filtering component determine whether minimum grammatical requirements pleonastic constructs met. also syntactic filtering
process clefts weather/time expressions identified using syntactic cues
WordNet respectively. candidate extrapositions thereafter used instantiate
various queries search engines; results returned queries serve parameters
final decision-making mechanism.
3.1 Preprocessing
preprocessing component transforms syntactic information embedded natural
language texts machine-understandable structures. preprocessing stage,
word assigned part-of-speech tag, whole sentence parsed using dependency grammar (DG) parser. simplicitys sake, current system designed use
WSJ corpus, already tagged parsed context-free grammar (CFG). head
percolation table similar proposed Collins (1999) used obtain head component phrase. rest phrase constituents rearranged
4
Metcalf Barnhart (1999) compiled chronicle many important additions vocabulary
American English.

345

fiLi, Musilek, Reformat, & Wyard-Scott

Figure 1: Illustration system work flow broken three processing stages preprocessing, syntactic filtering, web-based analysis.

head component form dependency tree using procedure detailed Xia Palmer
(2001). Figure 2 illustrates syntactic structure sentence WSJ corpus.
original CFG parse tree derived dependency structure shown side-by-side.
Head entities underlined CFG diagram circled DG diagram.

Figure 2: Illustration sentences syntactic structure, annotated WSJ
corpus (left) head percolation (right).

346

fiIdentification Pleonastic Using Web

shown Figure 2, function tags (e.g. SBJ, TMP, CLR) tracing information
present context-free parse tree ported dependency tree.
real-world parsers usually produce tags. Except deliberate omission,
parse trees contain essentially information, presented different manners.
study, dependency structure preferred popular phrase structure
mainly explicit marking head components complementing/modifying relationships among various components. feature helpful
instantiating search-engine queries.
3.2 Syntactic Filtering
syntactic filtering process determines whether clause meets grammatical requirements extraposition cleft construct matching clause respective
syntactic patterns.
3.2.1 Extrapositions
It-extrapositions occur clause dislocated ordinary position replaced
it. it-extraposition usually follows pattern:
matrix clause
z

}|



noun phrase




adjective phrase

+



prepositional phrase
itsubject +







verb phrase

|

{z

matrix verb phrase

{









+ extraposed clause






}

(1)

pattern summarizes general characteristics subject it-extrapositions,
pronoun assumes subject position. matrix verb (the verb following it)
main copula be, serves equate associate subject ensuing logical
predicate, must followed either noun phrase, adjective phrase, prepositional
phrase.5 special requirement matrix verb phrase otherwise. Similarly,
almost restriction placed upon extraposed clause except full clause
either introduced without complementizer (e.g. [0037:034] shame
meeting never took place.) led that, whether, if, one wh-adverbs (e.g.
how, why, when, etc.). constraints developed generalizing small portion
WSJ corpus largely accordance patterns identified Kaltenbock
(2005). Compared patterns proposed Paice Husk (1987), also cover
cases . . . , . . . . . . whether , allow broader range
candidates considering sentences explicitly marked (such [0037:034]).
configuration covers sentences as:
5
copula verbs receive treatment. arrangement made accommodate
cases verbs seem appear immediately followed extraposed clause.

347

fiLi, Musilek, Reformat, & Wyard-Scott

[0529:009] Since cost transporting gas important producers ability sell it, helps input access transportation
companies.
[0037:034] shame meeting never took place.
[0360:036] insulting demeaning say scientists needed new crises
generate new grants contracts . . .6
[0336:019] wont clear months whether price increase stick.
Except case last sentence, constructs generally overlooked
previous rule-based approaches identified Section 2.1. last sample sentence
illustrates, plus sign (+) pattern serves indicate forthcoming component
rather suggest two immediately adjacent components.
common grammatical variants pattern also recognized system,
including questions (both direct indirect), inverted sentences, parenthetical expressions (Paice & Husk, 1987). expands patterns coverage sentences
as:
[0772:006] remembered hard outsider become accepted . . .
[0562:015] sooner vans hit road morning, easier
us fulfill obligation.
[0239:009] Americans seems followed Malcolm Forbess hot-air lead
taken ballooning heady way.
Aside subject matrix clause, extrapositional also appear
object position. system described captures three flavors object extraposition.
first type consists instances followed object complement:
[0044:014] Mrs. Yeargin fired prosecuted unusual South
Carolina law makes crime breach test security.
case system inserts virtual copula object object
complement (a crime), making construct applicable pattern subject extraposition. example, underlined part prior example translates crime
breach test security.
two kinds object extraposition relatively rare:
Object verb (without object complement)
[0114:007] Speculation company asking $100 million operation said losing $20 million year . . .
Object preposition
[1286:054] see kids dont play truant . . .
cases cannot analyzed within framework subject extraposition thus
must approached different pattern:
verb + [preposition] object + full clause

(2)

6
Neither insulting demeaning Paice Husks (1987) list task status words therefore
cannot activate . . . pattern.

348

fiIdentification Pleonastic Using Web

current system requires full clauses start complementizer that.
restriction, however, included simplify implementation. Although object expositions common clauses led that, full clauses without leading
complementizer also acceptable.
According Kaltenbocks (2005) analysis special cases noun phrases
appear extraposed component, amazing number theologians
sided Hitler. noted noun phrases semantically close subordinate
interrogative clauses therefore considered marginal case extraposition. However, cases found corpus annotation process
consequently excluded study.
3.2.2 Cleft
It-clefts governed slightly restricted grammatical pattern. Following Hedberg
(1990), it-clefts expressed follows:
subject + copula + clefted constituent + cleft clause

(3)

cleft clause must finite (i.e. full clause relative clause); clefted constituents restricted either noun phrases, clauses, prepositional phrases.7 Examples
sentences meeting constraints include:
[0296:029] total relationship important.
[0267:030] also law school Mr. OKicki first wife
first seven daughters.
[0121:048] market goes down, figure paper profits Im losing.
addition, another non-canonical probably even marginal case also identified
cleft:
[0296:037] really understand Filipinos feel passionately
involved father figure want dispose yet
need.
Text following structure sample, wh-adverb immediately precedes it,
captured using syntactic pattern appending virtual prepositional phrase
matrix copula (e.g. reason), missing information already
given.
examples represents possible syntactic construct it-clefts.
difficult tell second third cases apart respective extrapositional
counterparts, even difficult differentiate first case ordinary copula
sentence restrictive relative clause (RRC). example, following sentence,
[0062:012] precisely kind product thats created municipal landfill
monster, editors wrote.
slightly modified version,
[0062:012] kind product thats created municipal landfill monster, editors wrote.
7
Adjective adverb phrases also possible relatively less frequent excluded
analysis.

349

fiLi, Musilek, Reformat, & Wyard-Scott

similar construction. However, latter considered cleft construct
first RRC construct. make things worse, pointed many (e.g. Boyd
et al., 2005, p.3, example 5), sometimes impossible make distinction without
resorting context sentence.
Fortunately, majority cases syntactic features, especially clefted
constituent, provide useful cues. it-cleft construct, cleft clause constitute head-modifier relationship clefted constituent, instead forms existential exhaustive presupposition8 (Davidse, 2000; Hedberg, 2000; Lambrecht, 2001).
example, figure paper profits Im losing. implies context something (and one thing) speaker going lose, associates paper
profits it. significant difference semantics often leaves visible traces syntactic layer, which, applicability proper nouns clefted constituents,
obvious. Others less obvious. system utilizes following grammatical cues
deciding construct it-cleft9 :
clefted constituent:
Proper nouns10 pronouns, cannot modified RRC;
Common nouns without determiner, generally refer kinds11 ;
Plurals, violate number agreement;
Noun phrases grounded demonstratives possessives,
modified RRCs, unambiguously identify instances, making unnecessary cases employ RRC;
Noun phrases grounded definite determiner the, modified
-preposition whose object also noun phrase grounded
plural. constructs usually sufficient introducing uniquely identifiable entities (through association), thus precluding need additional RRC
modifiers. words kind, sort, likes considered exceptions
rule;
Adverbial constructs usually appear complements. example,
phrases denoting location (here, etc.) specific time (today, yesterday
etc.), clause led when;
Full clauses, gerunds, infinitives.
subordinate clause:
constructs appear awkward used RRC. example, one would
generally avoid using sentences place dirty,
8

applies canonical clefts, include class represented [0267:030].
construct considered it-cleft conditions met.
10
exceptional cases proper names used additional determiners RRC modifiers, John TV last night, c.f. Sloats (1969) account.
11
validity assertion debate (Krifka, 2003). Nevertheless, considering particular
syntactic setting discussion, highly unlikely bare noun phrases used denote specific
instances.
9

350

fiIdentification Pleonastic Using Web

better alternatives. current implementation two patterns considered
inappropriate RRCs, especially syntactic settings described Equation 3: A) subordinate verb phrase consists copula verb
adjective; B) subordinate verb phrase consists element
verb itself.
Combined:
clefted constituent prepositional phrase subordinate clause
full clause, case [0267:030], construct classified
cleft12 .
rules based heuristics may exceptions, making less
ideal guidelines. Moreover, mentioned earlier, cleft cases cannot told
apart RRCs grammatical means. However, experiments show rules
relatively accurate provide appropriate coverage, least WSJ corpus.
3.2.3 Additional Filters
Aside patterns described earlier sections, additional filters installed
eliminate semantically unfit constructs therefore reducing number trips
search engines. filtering rules follows:
clause identified subordinate clause subsequently processed
extraposition cleft, number commas, dashes colons clause
either zero one, rule adopted Paice Husks
(1987) proposal.
Except copula be, sentences matrix verbs appearing perfect tense
considered either extraposition cleft.
subject multiple verb phrases, sentence considered
either extraposition cleft.
Sentences noun phrase matrix logical predicate together subordinate
relative clause considered extraposition.
Sentences matrix verb preceded modal auxiliaries could would
subordinate clause led wh-adverb considered extraposition.
example, [0013:017] . . . could complete purchase next summer bid
one approved . . . considered extraposition.
Except first, rules optional deactivated case introduce
false-negatives.
3.3 Using Web Corpus
first question regarding using web corpus whether regarded
corpus all. Kilgarriff Grefenstette (2003) pointed out, following definition
12
case cleft, chances extraposition. assumption, therefore,
affect overall binary classification.

351

fiLi, Musilek, Reformat, & Wyard-Scott

corpus-hood corpus collection texts considered object language
literary study, answer yes. fundamental problem resolved, remains
find whether web effective tool NLP tasks.
corpus, web far well-balanced error-free. However,
one feature corpus even remotely comparable size. one
knows exactly big is, major search engines already indexes billions
pages. Indeed, web large sometimes misspelled word yield tens
thousands results (try word neglectible). sends mixed signal using
web corpus: good side, even relatively infrequent terms yield sizable results;
bad side, web introduces much noise manually-compiled corpora do.
Markert Nissims (2005) recent study evaluating different knowledge sources
anaphora resolution, web-based method achieves far higher recall ratio
BNC- WordNet-based, time yielding slightly lower precision.
Similar things said webs diverse unbalanced composition,
means used universal knowledge source one manage
get overwhelmed non-domain-specific information.
said, still hard overstate benefits web offers.
largest collection electronic texts natural language, hosts good portion
general world knowledge, also stores information using syntax
defines language. addition, devoid systematic noise introduced
manually-constructed knowledge sources compilation process (e.g. failure
include less frequent items inflexible ways information organization). Overall, web
statistically reliable instrument analyzing various semantic relationships stored
natural languages means examples.
also suggested Kilgarriff (2007) many others, technically difficult
exploit web use local corpus often dangerous rely solely
statistics provided commercial search engines. mainly due fact
commercial search engines designed corpus research. Worse, design
goals even impede uses. example, search engines skew order results using
number different factors order provide users best results. Combined
fact return results certain thresholds, making essentially
impossible get unbiased results. annoyances include unreliable result counts, lack
advanced search features13 , unwillingness provide unrestricted access
APIs. new search engine specifically designed corpus research available,
seems work around restrictions live rest.
3.4 Design Search Engine Queries
discussed previous sections, it-extrapositions cannot reliably identified using syntactic signatures alone combination synthetic knowledge bases. overcome
artificial limitations imposed knowledge sources, proposed system resorts web
necessary semantic information.
13
example, wildcard () feature Google, could immensely useful query construction, longer restricts results single words since 2003; Yahoos ability support alternate words
within quoted texts limited, MSN offer feature all.

352

fiIdentification Pleonastic Using Web

system employs three sets query patterns: what-cleft, comparative expletive test, missing-object construction. set provides unique perspective
sentence question. what-cleft pattern designed find sentence investigation valid what-cleft counterpart. Since it-extrapositions what-clefts
syntactically compatible (as shown Section 3.4.1) valid readings usually
obtained transformations one construct other, validity what-cleft
indicative whether original sentence extrapositional. comparative
expletive test patterns straightforward directly check whether instance
replaced entities cannot used expletively context
extrapositional it. alternate construct invalid, original sentence
determined expletive. third set patterns supplemental. intended
identifying relatively rare phenomenon missing-object construction,
may reliably handled previous pattern sets.
Designing appropriate query patterns important step efforts exploit
large corpora knowledge sources. complex queries web, especially
important suppress unwanted uses certain components, could result different word senses, different sentence configuration, speakers imperfect command
language. example, query shame could return valid extrapositional construct RRC shame perpetuated life;
query right could return valid what-clefts sentences
ought right . . . study employs three different approaches
curb unwanted results:
first important measure comparative analysis pairs similarlyconstructed queries sent search engine ratios result counts
used decision. method effective problems caused different
sentence configuration bad language usage, since generally neither contribute
fraction results large enough significantly affect ratio. method also
provides normalized view web interest study
exactly frequently specific construct used, whether likely
carry specific semantic meaning used.
second measure use stubs query patterns, detailed following
sections. Stubs help ensure outcomes queries syntactically semantically similar original sentences partly resolve problems caused
word sense difference.
Finally, infeasible use comparative analysis, part query results
validated obtain estimated number valid results.
3.4.1 Query Pattern I: What-cleft
first query pattern,
+ verb phrase + copula + stub

(4)

what-(pseudo-)cleft construct encompasses matrix-level information found
it-extraposition. pattern obtained using three-step transformation illustrated
353

fiLi, Musilek, Reformat, & Wyard-Scott

below:

1)

2)

3)

+ verb phrase + clause

easy
see ancient art ropes. [0089:017]

clause
+
verb phrase
see ancient art ropes easy.

+ verb phrase + copula + clause
easy

see ancient art ropes.

+ verb phrase + copula + stub


easy

(5)

Step 1 transforms original sentence (or clause) corresponding non-extraposition
form removing pronoun restoring information canonical subjectverb-complement order. example, clause see . . . considered real
subject moved back canonical position. non-extraposition form subsequently converted step 2 what-cleft highlights verb phrase. Finally,
step 3, subordinate clause reduced stub enhance patterns coverage.
choice stub depends structure original subordinate clause: used
original subordinate clause infinitive, gerund, . . . infinitive construct14 .
rest cases, original complementizer, that, case
complementizer, used stub. use stub pattern imposes syntactic
constraint, addition ones prescribed pronoun copula is,
demands subordinate clause present query results. choice stubs also reflects,
certain degree, semantics original texts therefore seen weak
semantic constraint.
examples what-cleft transformation:
[0059:014] remains unclear whether bond issue rolled over.
remains unclear whether
[0037:034] shame meeting never took place.
shame
what-cleft pattern identifies whether matrix verb phrase capable
functioning constituent it-extraposition. Information subordinate clauses
discarded construct used relatively infrequently adding extra restrictions
query prohibit yielding results many cases.
it-extraposition constructs appears . . . said . . .
valid non-extraposition counterpart, what-cleft versions often bear
certain degrees validity queries instantiated pattern often yield results
(albeit many) reputable sources. also worth noting although input
output constructs transformation syntactically compatible,
necessarily equivalent terms givenness (whether information one sentence
14
According Hamawand (2003), . . . infinitive construct carries distinct semantics; reducing
infinitive alone changes function. However, exceptional cases, find reduction
generally acceptable. i.e. lost semantics affect judgment expletiveness.

354

fiIdentification Pleonastic Using Web

entailed previous discourse). Kaltenbock (2005) noted percentage
extrapositional constructs carrying new information varies greatly depending
category text. contrast, what-cleft generally expresses new information
subordinate clause. presupposed contents two constructs different, too.
What-clefts, according Gundel (1977), it-clefts derived,
existential exhaustive presuppositions carried it-cleft counterparts.
hand, it-extrapositions, semantically identical corresponding
non-extrapositions, lack presuppositions or, most, imply weaker strength
(Geurts & van der Sandt, 2004). discrepancies hint derived what-cleft
stronger expression original extraposition, may queries
instantiated pattern tend yield considerably less results.
Another potential problem pattern omission subordinate verb,
occasionally leads false positives. example, differentiate
helps input access transportation companies helps expand
horizon. deficiency accommodated additional query patterns.
3.4.2 Query Pattern II: Comparative Expletiveness Test
second group patterns provides simplified account original text
different flavors. execution, results individual queries compared assess
expletiveness subject pronoun. set patterns takes following general
form:
pronoun + verb phrase + simplified extraposed clause
(6)
difference among individual patterns lies choice matrix clause subject
pronoun: it, which, who, this, he. patterns instantiated submitted
search engine, number hits obtained version far outnumber
versions combined original text it-extraposition; otherwise
number hits least comparable. behavior reflects expletive nature
pronoun it-extraposition, renders sentence invalid replaced
pronouns pleonastic use.
simplified extraposed clause take different forms depending original
structure:
Original Structure
infinitive (to meet you)
. . . infinitive15 (for see document)
gerund (meeting you)
full clause led complementizer
(it shame meeting never took place)
full clause without complementizer
(it shame meeting never took place)

Simplified
infinitive + stub
infinitive + stub
gerund + stub
complementizer + stub
+ stub

Table 1: Simplification extraposed clause
15
. . . passive-infinitive transformed active voice (e.g. products sold sell
products).

355

fiLi, Musilek, Reformat, & Wyard-Scott

Similar case Pattern I, stub used syntactic constraint
semantic cue. Depending type search engine, stub either the,
widely used determiner, combination various determiners, personal
pronouns possessive pronouns, indicate subsequent noun phrase.
case infinitive construct involves subordinate clause led wh-adverb that,
complementizer used stub. arrangement guarantees results returned
query conform original text syntactically semantically. null value
used stubs object position original text lacks nominal object.
illustrate rules transformation, consider following sentence:
[0044:010] teacher said OK use notes test,
said.
relevant part sentence is:
+ verb phrase + clause

OK
use notes test
Applying clause simplification rules, first query obtained:
+ verb phrase + simplified clause

OK
use
second query generated simply replacing pronoun alternative
pronoun:
alternative pronoun + verb phrase + simplified clause

OK
use
Google reports 94,200 hits query, one page found using alternative
query. Since pronoun used much broader context, replacing
alone hardly makes balanced comparison. Instead, combination which, who, this,
used, illustrated following examples:
[0044:010] teacher said OK use notes test,
said.

)
(

ok use
which/who/this/he
[0089:017]
( easy see
) ancient art ropes.

easy see
which/who/this/he
special set patterns used object extrapositions16 accommodate unique
syntactic construct:
verb + [preposition] pronoun + + stub

(7)

Stubs chosen according rules main pattern set, however one
alternative pronoun used.
16
Instances containing object complements treated framework subject extraposition
included here.

356

fiIdentification Pleonastic Using Web

[0114:007] Speculation company asking $100 million
operation
said)to losing $20 million year . . .
(




3.4.3 Query Pattern III: Missing-object Construction
One search engine annoyance ignore punctuation marks. means one
search text matches specific pattern string, sentences
end pattern string. stubs used Pattern II generally helpful excluding sentences semantically incompatible original search
results. However, circumstances stub attached queries (where
query results ideally consist sentences end query string),
search engine may produce results needed. Sentences conforming pattern
+ copula + missing-object construction, (referring book) easy
read, present one situation. unique construction special
treatment needed missing-object construction usually it-extraposition
counterpart object present, example easy read book . Since
missing-object constructions virtually (only shorter) extrapositional counterparts, good chance identified extrapositions.
following additional examples missing-object construction:
[0290:025] non-violent civil disobedience centerpiece, rather
lawful demonstration may attract crime, difficult
justify.
[0018:024-025] price new shares set. Instead, companies
leave marketplace decide.
[0111:005] declined elaborate, say, seemed right
thing minute.
Two sets patterns proposed17 identify likes foregoing examples.
first pattern, compound adjective test, inspired Nannis (1980) study considering
easy-type adjective followed infinitive (also commonly termed tough construction)
single complex adjective. pattern takes form
stub + adjectivebase -to -verb

(8)

stub, serving limit outcome query noun phrases, takes combination determiners a/an alone; original adjective also converted base form
adjectivebase comparative superlative form. Expanding Nannis original
claims, pattern used evaluate adjectives18 well constructs furnished
. . . infinitive complements. following example demonstrates patterns
usage:
17

Preliminary experiments confirmed effectiveness patterns. However, due sparseness
samples belonging class, included reported evaluation.
18
based observation compounds ready-to-fly (referring model aircrafts)
exist, hard obtain complete enumeration easy-type adjectives.

357

fiLi, Musilek, Reformat, & Wyard-Scott

[0258:024] machine uses single processor, makes easier program
competing machines using several processors.
easy-to-program
second set consists two patterns used comparative analysis
general profile:
+ verbgerund + stub
(9)
verbgerund gerund form original infinitive. complementizer
used sole purpose ensuring verbgerund appears subject subordinate
clause sentences returned queries. words, phrases computer
programming pattern matching excluded. first pattern, stub
combination prepositions (currently chosen); second one,
combination determiners alone used. example:
[0258:024] machine uses single processor, makes easier program
competing machines
using)several processors.
(
in|from
programming

set patterns tests transitivity verb semantic environment similar
original sentence. verb used transitively often, pattern
determiners yield results, vice versa. supported preceding
sample sentences, usually-transitive verb used without object19 good indicator
missing-object construction sentence diagnosed referential.
3.4.4 Query Instantiation
Patterns must instantiated information found original sentences
submitted search engine. Considering general design principles system,
advisable instantiate patterns original texts significantly reduces
queries coverage. Instead, object matrix verb phrase truncated
matrix verb expanded order obtain desired level coverage.
truncation process provides different renditions based structure original
object:
Adjective phrases:
head word used. head word modified too,
modifier also retained order better support . . . construct
maintain compatibility semantics original text.
Common noun phrases:
possessive ending/pronoun, -preposition:
phrase replaced $PRPS$ plus head word. $PRPS$ either list
possessive pronouns one widely used, depending caliber
search engine used. example, location expanded |
| | | | | location.
19
omitted object preposition (e.g. difficult account for.) effect,
identifiable syntactic means alone.

358

fiIdentification Pleonastic Using Web

determiners:
phrase replaced choice $DTA$, $DTTS$, $DTTP$, combination
$DTA$ $DTTS$, plus head word. $DTA$ list (or one the)
general determiners (i.e. a, an, etc.). $DTTS$ refers combination
definite article singular demonstratives that. $DTTP$
plural counterpart $DTTS$. choice based configuration
original text maintain semantic compatibility.
without determiner:
head word used.
Proper nouns pronouns:
phrase replaced $PRP$, list (or one the) personal pronouns.
Prepositional phrases:
object preposition truncated recursive operation.
Numeric values:
phrase lot used instead.
Matrix verbs expanded include simple past tense third person singular present form aid WordNet generic patterns.
applicable, particles also remain attached verb.
Generally speaking, truncation expansion good ways boosting patterns
coverage. However, current procedures truncation still crude, especially
handling complex phrases. example, phrase reckless course action
([0198:011]) yields $PRPS$ course, results total loss original semantics.
enhancements truncation process may improve performance
improvement likely limited due endless possibilities language usage
constraints imposed search engines.
Aside truncating expanding original texts, stepped-down version
Pattern II, denoted Pattern II0 , also provided enhance systems coverage.
current scheme simply replace extraposed clause new stub
original extraposed clause infinitive, . . . infinitive, gerund construct.
example,
[0089:017]
( easy see
) ancient art ropes.

easy
which/who/this/he
situations, downgraded version applied.
3.5 Binary Classification It-extraposition
Five factors taken consideration determining whether sentence question
it-extraposition:
Estimated popularity what-cleft construct (query Pattern I)
denoted
W = nw vw
359

fiLi, Musilek, Reformat, & Wyard-Scott

nw number results reported search engine, vw
percentage valid instances within first batch snippets (usually 10, depending
search engine service) returned query. Validation performed
case-sensitive regular expression derived original query. Since whatcleft pattern capitalized beginning, regular expression looks
instances appearing beginning sentence. particularly important
validate results what-cleft queries search engines produce
results based interpretation original query. example, Google
returns pages containing Whats found query found that,
might helpful counterproductive purpose
study.
Result comparative expletiveness test (query Pattern II)
denoted
nX
r=
nit
nit number results obtained original version query,
nX total number results produced replacing pronouns
who. smaller ratio r is, likely sentence
investigated extraposition. Extrapositional sentences usually produce
r value 0.1 less. versions query yield insufficient results
(max(nit , nX ) < Nmin ), r takes value Rscarce = 1000. Since it-extrapositions
relatively rare, better assume sentence extrapositional
insufficient data judge otherwise. case nX sufficient
version query produces result (nX >= Nmin nit = 0), r takes
value Rzero = 100. Values Rzero Rscarce large numbers chosen arbitrarily,
mainly visualization purposes. words Rzero Rscarce hint
sentence probably extrapositional, however neither indicates degree
likelihood.
Result stepped-down
comparative expletiveness test
n0
0
0
denoted r0 = nX
0 , nit nX number results returned

version alternate version stepped-down queries (c.f. Section 3.4.4,
Page 359). stepped-down queries simplified versions queries used
calculate r. Due simplification, r0 usually sensitive extrapositions.
However queries stepped-down versions, case original queries
reused, causing r0 = r. Similar way r defined, r0 also takes values
Rscarce Rzero special situations.
Synthesized expletiveness
new variable R defined based values r, nit , nX , r0 :
(

R=

r, max(nit , nX ) Nmin ,
r0 , max(nit , nX ) < Nmin .

original queries yield enough results, R takes value r since original
queries better preserve sentence context generally accurate. However,
360

fiIdentification Pleonastic Using Web

original queries fail, system resorts back-up method using
stepped-down queries bases judgement results instead. Overall, R
seen synthesized indicator subject pronoun generally used
similar syntactic semantic setting original sentence.
Syntactic structure sentence
denoted S, binary variable indicating sentence investigation belongs
syntactic construct prone generating false-positives. average
what-cleft queries yield fewer results less reliable since cannot
used provide comparative ratios. However, still useful last line
defence curb impacts certain syntactic constructs repeatedly cause
comparative expletive tests produce false-positives. Currently one construct
identified verb infinitive construct, helps input
everyone expects post results tomorrow . Therefore,
(

S=

TRUE, sentence matches verb infinitive,
FALSE, otherwise.

final binary classification it-extraposition, E, defined follows:
(

E=

((R < Rexp ) (W > Nmin )), = TRUE,
(R < Rexp ),
= FALSE.

(10)

Nmin Rexp , set 10 0.15 respectively study, threshold constants
chosen based upon empirical observations. words, system recognizes instance extrapositional unlikely (by comparing R Rexp ) alternative
pronoun used place syntactic semantic settings. verb
infinitive constructs, also required sentence viable what-cleft variant
(by comparing W Nmin ).
worth noting todays major commercial search engines return exact
number results query rather estimates. negative effect
somewhat mitigated basing final decision ratios instead absolute numbers.

4. Case Study
better illustrate system work flow, two sample sentences selected WSJ
corpus taken whole process. first sample, [0231:015], classified
it-extraposition; other, [0331:033] (with preceding sentence providing context),
referential case nominal antecedent. particulars implementation
also discussed here.
[0231:015] fund manager life-insurance company said three factors make
difficult read market direction.
[0331:032-033] recent report classifies stock hold. appears
sort hold one makes heading door.
361

fiLi, Musilek, Reformat, & Wyard-Scott

4.1 Syntactic Filtering
First, syntactic structures sentence identified dependencies among
constituents established, shown Figures 3 4.

Figure 3: Syntactic structure [0231:015] (fragment)

Figure 4: Syntactic structure [0331:033] (fragment). Readings B, indicated
DG parse tree, discussed text.

362

fiIdentification Pleonastic Using Web

sample sentence [0231:015], expletive appears object verb makes
followed object complement difficult, therefore virtual copula (tagged VBX)
created dependency tree order treat framework subject
it-extrapositions. [0331:033], two different readings produced one assuming
appears matrix verb (reading A, c.f. Figure 4), taking (reading
B). accomplished drilling chain verbs beginning parent
verb node. top chain, system starts recursive process
find verbs infinitives directly attached current node moves
newly found node. process interrupted current verb node furnished
elements verbal adverbial complements/modifiers.
filtering process, various components sentences identified, listed
Table 2.

Sentence
0231:015
0331:033
0331:033

Reading

B

Matrix
Verb
Object

difficult
appears

sort

Conjunction



Subordinate
Subject Verb
Object
read direction

sort
One

Table 2: Component breakdown case study samples

4.2 Pattern Instantiation
Using components identified Table 2, five queries generated reading,
listed Tables 3-5. Patterns II0 -it II0 -others refer stepped-down versions
(c.f. Section 3.4.4, Page 359) II-it II-others respectively. queries shown
generated specifically Google take advantage features available Google.
use alternative search engine Yahoo, component expansions determiner
lists turned off, separate queries need prepared individual pronouns.
order get accurate results, queries must enclosed double quotes
sent search engines.

Pattern

II-it
II-others
II0 -it
II0 -others

Query
is|was|s difficult is|was
is|was|s difficult read the|a|an|no|this|these|their|his|our
which|this|who|he is|was|s difficult read the|a|an|no|this|these|
their|his|our
is|was|s difficult
which|this|who|he is|was|s difficult
Table 3: Queries [0231:015]
363

Results
1060
3960
153
6.3 106
1.5 105

fiLi, Musilek, Reformat, & Wyard-Scott

Pattern

II-it
II-others
II0 -it
II0 -others

Query
appears|appeared is|was
appears|appeared the|a|an|no|this|these|their|his|our
which|this|who|he appears|appeared the|a|an|no|this|these|
their|his|our
appears|appeared
which|this|who|he appears|appeared

Results
44
7.5 104
3.2 105
2.2 106
2.6 106

Table 4: Queries [0331:033], Reading

Pattern

II-it
II-others
II0 -it
II0 -others

Query
is|was|s its|my|our|his|her|their|your sort is|was
is|was|s its|my|our|his|her|their|your sort the|a|an|no|
this|these|they|we|he|their|his|our
which|this|who|he is|was|s its|my|our|his|her|their|your sort
the|a|an|no|this|these|they|we|he|their|his|our
II-it
II-others

Results
0
0
0
0
0

Table 5: Queries [0331:033], Reading B
4.3 Query Results Classification
every reading, number results five queries (nw Pattern I;
nit II-it; nX II-others; n0it II0 -it; n0X II0 -others) obtained
search engine; first 10 results what-cleft query also validated obtain
estimated percentage (vw ) valid constructs. W (= nw vw ), r(= nX /nit ), r0 (= n0X /n0it ),
R (choosing either r r0 depending whether max(nit , nX ) 10)
calculated accordingly, recorded Table 6.
Query
[0231:015]
[0331:033].A
[0331:033].B

nw
1060
44
0

vw
70%
0%
-

nit
3960
7.5E4
0

nX
153
3.2E5
0

n0it
6.3E6
2.2E6
0

n0X
1.5E5
2.6E6
0

W
742
0
0

r
0.04
4.3
1000

r0
0.02
1.2
1000

R
0.04
4.3
1000

Table 6: Query results case study sample sentences
appears suspicious vw set 0 reading [0331:033].A, means
valid instances found. quick look returned snippets reveals that, indeed, none
10 snippets queried contents beginning sentence. Also note
reading [0331:033].B, r r0 , consequently R set Rscarce = 1000
since query produced enough results.
decided Table 2 readings [0231:015] [0331:033].B bear
verb infinitive construct, hence = FALSE; [0331:033].A = TRUE.
Applying Equation 10 Section 3.5, [0231:015] [0331:033].B, final classification
364

fiIdentification Pleonastic Using Web

E based whether R sufficiently small (R < 0.15). [0331:033].A, system
also needs check whether what-cleft query returned sufficient valid results (W > 10).
final classifications listed Table 7.
Sentence
[0231:015]
[0331:033]
[0331:033]

Reading

B

W
742
0
0


FALSE
TRUE
FALSE

R
0.04
4.3
1000

Ereading
YES



E
YES


Table 7: Final binary classification case study sample sentences
Since neither readings [0331:033] classified such, sentence it-extraposition
construct.

5. Evaluation
order provide comprehensive picture systems performance, twofold assessment used. first evaluation, system exposed sentence collection
assisted development. Accordingly, results obtained evaluation reflect,
certain degree, systems optimal performance. second evaluation aims revealing systems performance unfamiliar texts running developed system
random dataset drawn rest corpus. Two additional experiments also
conducted provide estimation systems performance whole corpus.
Three performance measures used throughout section: precision, recall,
balanced F-measure (van Rijsbergen, 1979). Precision defined ratio correctly
classified instances specific category (or collection categories) number
instances identified system belonging category (categories). words,
P
precision calculated P = PT+F
P , P F P number true positives
false positives respectively. Recall defined ratio correctly classified instances
specific category (or collection categories) total number instances
P
category (categories), R = PT+F
N , F N denotes number false negatives.
Finally, F-measure weighted harmonic mean precision recall used indicate
systems overall performance. precision recall weighted equally, used
R
study, balanced F-measure defined F = P2P+R
.
Following Efron Tibshiranis (1993) Bootstrap method, 95% confidence intervals
obtained using 2.5th 97.5th percentiles bootstrap replicates
provided alongside system performance figures indicate reliability. number
replicates arbitrarily set B = 9999, much greater commonly
suggested value 1000 (e.g., see Davison & Hinkley, 1997; Efron & Tibshirani, 1993)
pleonastic instances sparse. case precision recall value 100%,
bootstrap percentile method reports interval 100%-100%, makes little sense.
Therefore, situation adjusted Wald interval (Agresti & Coull, 1998) presented
instead. two systems compared, approximate randomization test (Noreen,
1989) similar used Chinchor (1992) performed determine difference
statistical significance. significance level = 0.05 number shuffles R = 9999,
chosen arbitrarily, used significance tests performed.
365

fiLi, Musilek, Reformat, & Wyard-Scott

5.1 Development Dataset
purpose study, first 1000 occurrences WSJ corpus
manually annotated authors20 . part set also inspected order
determine values constants specified Section 3.5, develop surface
structure processor. annotation process facilitated custom-designed utility
displays sentence within context represented nine-sentence window containing
six immediately preceding sentences, original, two sentences follow.
Post-annotation review indicates presentation corpus sentences worked well.
Except (less 0.5%) cases, authors found need resort broader
contexts understand sentence; circumstances valid antecedents
located outside context window antecedent found within it.
Category
Nominal Antecedent
Clause Antecedent
Extraposition
Cleft
Weather/Time
Idiom

Grand Total

Instances
756
60
118
13
9
18
26
1000

Percentage
75.60%
6.00%
11.80%
1.30%
0.90%
1.80%
2.60%
100.00%

Table 8: Profile development dataset according authors annotation
Table 8 summarizes distribution instances dataset according authors
consensus. category labeled consists mostly instances fit well
categories, e.g. identified nominal antecedent plural
antecedent inferred, well certain confusing instances. twenty-six instances,
two might remotely recognized one types interests study:
[0101:007] though size loan guarantees approved yesterday significant, recent experience similar program Central America
indicates could take several years new Polish government fully use aid effectively.
[0296:048] comic try pretend theyre still master race.
Neither instance identified anaphoric. However, first construct neither
valid non-extraposition version valid what-cleft version, making difficult justify
extraposition, second case considered refer atmosphere
aroused action detailed when-clause.
order assess whether pleonastic categories well-defined ability
ordinary language users identify pleonastic instances, two volunteers, native English
speakers, invited classify instances development dataset. help
concentrate pleonastic categories, volunteers required assign
instance one following categories: referential, extraposition, cleft, weather/time,
20

Annotations published online appendix http://www.ece.ualberta.ca/~musilek/pleo.

zip.

366

fiIdentification Pleonastic Using Web

idiom. referential category covers instances nominal antecedents
clause antecedents, well instances inferrable antecedents. Table 9 outlines
annotators performance reference authors consensus. degree agreement
annotators, measured kappa coefficient (; Cohen, 1960), also given
table.
Category

Precision

Referential
99.38%
Extraposition
82.54%
Cleft
38.46%
Weather/Time 66.67%
Idiom
39.39%
Overall Accuracy/

Volunteer 1
Recall F-measure

95.49%
88.14%
76.92%
44.44%
72.22%
93.50%

97.40%
85.25%
51.28%
53.33%
50.98%

Precision

96.38%
88.68%
72.73%
75.00%
50.00%

Volunteer 2
Recall F-measure

98.10%
79.66%
61.54%
33.33%
61.11%
94.20%

97.23%
83.93%
66.67%
46.15%
55.00%


.749
.795
.369
-.005
.458
.702



Except Weather/Time category (p = 0.5619), values statistically significant p <
0.0001.

Table 9: Performance volunteer annotators development dataset (evaluated
using authors annotation reference) degree inter-annotator agreement measured Cohens kappa (). authors annotations refitted
simplified annotation scheme used volunteers.
many factors contributing apparently low values Table 9,
notably skewed distribution categories inappropriate communication
classification rules. Di Eugenio Glass (2004) others pointed out, skewed distribution categories negative effect value. Since distribution
instances dataset fairly unbalanced, commonly-accepted guideline
interpreting values ( > 0.67 > 0.8 thresholds tentative definite conclusions respectively; Krippendorff, 1980) may directly applicable case.
addition, classification rules communicated annotators orally examples not-so-common cases, object it-extrapositions, might
well understood annotators. Another interesting note results
strong tendency annotators (albeit different cases) classify
it-clefts it-extrapositions. Rather taking sign cleft category
well-defined, believe reflects inherent difficulties identifying instances pertaining
category.
5.2 Baselines
Two baselines available comparison WSJ annotation, done manually
provided corpus; results replication Paice Husks (1987)
algorithm (PHA). cautioned that, given subjectivity issues discussed
paper lack consensus certain topics field linguistics, recall ratios
presented baseline results forthcoming results proposed system
compared quantitatively. example, original Paice Husk algorithm
recognize certain types object extrapositions always distinguish
367

fiLi, Musilek, Reformat, & Wyard-Scott

individual types pleonastic it; WSJ corpus neither special annotation
parenthetical (c.f. Section 3.2.1, Page 348, [0239:009]) established annotation
policy certain types object extrapositions (Bies, Ferguson, Katz, & MacIntyre, 1995).
attempts made correct issues.
Table 10 summarizes performance baselines development dataset.
expected, Paice Husks (1987) algorithm perform well since WSJ
articles different from, tend sophisticated than, technical
essays algorithm designed for. Compared originally reported precision
93% recall 96%, replicated PHA yields 54% 75% respectively
development dataset. performance replica largely line Boyd et al.
(2005) obtained implementation algorithm different dataset.

Measurement
Reference
Identified Baseline
Baseline True Positives
Precision
Recall
F-measure

WSJ Annotation
Extraposition
Cleft
118
13
88
12
87b
12
98.86%
100%
73.73% 92.31%
84.47% 96.00%

Replicated PHA
Overalla
140
194
105
54.12%
75.00%
62.87%



Includes clefts, extrapositions, time/weather cases.
Based manual inspection, two cases originally annotated extrapositional WSJ
determined inappropriate. See discussions below.
b

Table 10: Performance baselines development dataset, evaluated
authors annotation.
31 (118 87) extrapositional cases annotated WSJ broken
following categories followed respective number instances:
Category
Unrecognized
Object without complement
Parenthetical
Inappropriate non-extraposition
Agentless passive
seems/appears . . .
worth . . .
Others
Valid non-extraposition
. . .
Others
Total

Items
3
1
2
18
9
4
2
3
10
2
8
31

Table 11: Profile false negatives WSJ annotation reference authors
annotation
368

fiIdentification Pleonastic Using Web

stating Characteristic extraposition final clause replace
it, Bies et al. (1995) define class narrowest sense. Since interpretation
definition entirely subjective matter, way determining real coverage
annotations. However, portions corpus reviewed,
practice annotation entirely consistent.
Two sentences marked extraposition corpus annotators consensus
indicates otherwise. Considering golden standard status WSJ corpus,
also listed here:
[0277:040] Moreover, member Mitsubishi group, headed
one Japans largest banks, sure win favorable loan.
[0303:006] compromises convince Washingtons liberals
simply stay course, administration stray
course issues.
first sentence considered dubious likely referring company
member Mitsubishi group. second one considered cleft actually also
marked cleft corpus. Since case corpus annotations,
extraposition marking considered mistake manually removed.
Paice Husk (1987) algorithm suffers false-positive . . . . . .
construct detection, may fixed incorporating part-of-speech phrase structure information together additional rules. However, fixes greatly complicate
original system.
5.3 Results
development dataset, results produced proposed system follows:
Extraposition
118
116
113

Cleft
13
13
13

Weather/Time
9
10
9

Overalla
140
139
136

Precision
95% C.I.b

97.41%
94.07-100.00%

100.00%
79.74-100.00%

90.00%
66.67-100.00%

97.84%
95.21-100.00%

Recall
95% C.I.b

95.76%
91.79-99.12%

100.00%
79.74-100.00%

100.00%
73.07-100.00%

97.14%
93.98-99.34%

F-measure
95% C.I.

96.58%
93.98-98.72%

100.00%
-

94.74%
80.00-100.00%

97.49%
95.45-99.21%

Measurement
Reference
Identified
True Positives


b

Combining extraposition, cleft, weather/time one category.
Adjusted Wald intervals reported extreme measurements.

Table 12: Performance system development dataset, evaluated using authors annotation reference.

369

fiLi, Musilek, Reformat, & Wyard-Scott

statistical significance tests reveal information regarding systems performance comparison two volunteers baselines:
Compared volunteer annotators, systems better performance three
pleonastic categories statistically significant.
extraposition category, difference WSJ annotations (higher)
precision system statistically significant.
Compared Paice Husks (1987) algorithm, systems higher precision
statistically significant.

Target System
Volunteer 1
Volunteer 2
WSJ Annotation
Replicated PHA

Extraposition

Cleft

Weather/Time

F-measure+ /p < .001 F-measure+ /p < .001 F-measure+ /p = .033
F-measure+ /p < .001 F-measure+ /p = .007 F-measure+ /p = .025
Precision /p = .630 F-measure+ /p = 1.00
(All Categories) Precision+ /p < .001

Table 13: Results statistical significance tests presented format
Test Statisticsign /p-value. plus sign (+ ) indicates system performs
better reported measurement; otherwise minus sign ( ) used. fair
comparisons made precision recall, F-measure used
test statistic; otherwise applicable measurement reported.

Using authors annotation reference, system outperforms human volunteers. higher performance usually desirable, particular case, could
indicate possible problems design experiment. Since English language
used speakers also shaped group people, impractical system speaks better English human counterparts do. One
plausible clue paradox analytic approach needed gain insight
issue pronoun classification, casual English speakers see
perspective. Green Hecht (1992) many others indicated, capable users
language necessarily ability formulate linguistic rules. However,
kinds analytic skills prerequisite order explicitly classify pronoun one
many categories. Thus, true performance casual speakers measured
ability comprehend produce various pleonastic constructs. addition,
factors, time constraints imperfections category definitions
conveyed, may also play role limiting volunteers performance. authors
annotation, hand, much less influenced issues therefore considered expert opinion experiment. shown Section 5.2, WSJ annotation
extrapositions clefts, also considered expert opinion, highly compatible
authors. differences two annotations mostly attributed
narrower definition extraposition adopted WSJ annotators. Therefore,
WSJ annotations precision 98.86% extrapositions (when verified authors
370

fiIdentification Pleonastic Using Web

annotation) probably appropriate hint upper-limit practically important
system performance.
extraposition category, 279 individual cases passed syntactic filters
evaluated search engine queries. Results queries obtained Google
web service, Google SOAP21 Search API. three (116 113) cases false-positives
caused missing-object constructions corrected using patterns detailed
Section 3.4.3.
five (118 113) false-negative cases listed below:
[0283:013] newspaper said past time Soviet Union create
unemployment insurance retraining programs like
West.
[0209:040] one thing say sterilize, another successfully pollinate plant, said.
[0198:011] Sen. Kennedy said . . . would reckless course
action President Bush claim authority without congressional approval.
[0290:049] Worse, remained well-meaning naive president
United States administer final infamy upon fought
died Vietnam.
[0085:047] easy roll something comprehensive, make
pay, Mr. Jacob says.
Sentence [0283:013] misplaced weather/time. Sentence [0209:040] properly handled syntactic processing subcomponent. Sentences [0198:011] [0290:049] involve
complex noun phrases (underlined) object position matrix verbs
difficult reduce something generic, head noun pronoun, still remain confident original semantics maintained. last case,
sentence [0085:047], fails full queries (containing part subordinate clause)
failed yield enough results stepped-down versions overwhelmed noise.
last four false-negatives annotated correctly WSJ corpus. systems
recall ratio 87 verified WSJ extraposition annotations therefore 95.40%, comparable
overall recall.
5.4 System Performance Parser Output
Thus far, system evaluated based assumption underlying
sentences tagged parsed (almost) perfect accuracy. Much effort
made reduce dependency. example, tracing information function tags
original phrase structures deliberately discarded; system also tries search
possible extraposed cleft clauses marked complements matrix object.
However, deficiencies tagging parsing may still impact systems performance.
Occasionally, even golden standard manual markups appear problematic happen
get way task.
21

Simple Object Access Protocol XML-based message protocol web services.

371

fiLi, Musilek, Reformat, & Wyard-Scott

therefore necessary evaluate system sentences automatically
tagged parsed order answer question well would perform
real world. Two state-of-the-art parsers employed study: reranking parser
Charniak Johnson (2005), Berkeley parser Petrov, Barrett, Thibaux,
Klein (2006). systems performance respective interpretations
development dataset sentences reported Tables 14 15. Table 16 compares
systems real-world performance various baselines.
Measurement
Reference
Identified
True Positives
Precision
95% C.I.b
Recall
95% C.I.b
F-measure
95% C.I.

b

Extraposition
118
114
110
96.49%
92.68-99.20%
93.22%
88.43-97.41%
94.83%
91.60-97.49%

Cleft
13
12
12
100.00%
78.40-100.00%
92.31%
73.33-100.00%
96.00%
84.62-100.00%

Weather/Time
9
10
9
90.00%
66.67-100.00%
100.00%
73.07-100.00%
94.74%
80.00-100.00%

Overalla
140
136
132
97.06%
93.92-99.32%
94.29%
90.18-97.81%
95.65%
93.08-97.90%

Combining extraposition, cleft, weather/time one category.
Adjusted Wald intervals reported extreme measurements.

Table 14: Performance system development dataset parsed Charniak
parser, using authors annotation reference.

Extraposition
118
114
111

Cleft
13
11
10

Weather/Time
9
9
8

Overalla
140
134
130

Precision
95% C.I.

97.37%
94.07-100.00%

90.91%
70.00-100.00%

88.89%
62.50-100.00%

97.01%
93.81-99.32%

Recall
95% C.I.

94.07%
89.47-98.18%

76.92%
50.00-100.00%

88.89%
62.50-100.00%

92.86%
88.44-96.91%

F-measure
95% C.I.

95.69%
92.75-98.17%

83.33%
62.50-96.55%

88.89%
66.67-100.00%

94.89%
92.02-97.35%

Measurement
Reference
Identified
True Positives



Combining extraposition, cleft, weather/time one category.

Table 15: Performance system development dataset parsed Berkeley
parser, using authors annotation reference.

372

fiIdentification Pleonastic Using Web

Comparing System Performance Charniak Parser Output to:
Target System
Extraposition
Cleft
Weather/Time


System w/o Parser F-measure /p = .131 F-measure /p = 1.00 F-measure= /p = 1.00
Volunteer 1
F-measure+ /p = .001 F-measure+ /p < .001 F-measure+ /p = .030
Volunteer 2
F-measure+ /p < .001 F-measure+ /p = .041 F-measure+ /p = .021
WSJ Annotation
Precision /p = .368 F-measure= /p = 1.00
Replicated PHA
(All Categories) Precision+ /p < .001
Comparing System Performance Berkeley Parser Output to:
Target System
Extraposition
Cleft
Weather/Time


System w/o Parser F-measure /p = .380 F-measure /p = .128 F-measure /p = 1.00
Volunteer 1
F-measure+ /p < .001 F-measure+ /p = .014 F-measure+ /p = .061
Volunteer 2
F-measure+ /p < .001 F-measure+ /p = .314 F-measure+ /p = .046
WSJ Annotation
Precision /p = .627 F-measure /p = .374
Replicated PHA
(All Categories) Precision+ /p < .001
Table 16: Results statistical significance tests comparing systems performance
parser output various systems, presented format
Test Statisticsign /p-value. plus sign (+ ) indicates proposed system
performs better target system reported measurement; equal
sign (= ) indicates tie; otherwise minus sign ( ) used. fair comparisons
made precision recall, F-measure used test
statistic; otherwise applicable measurement reported.

significance tests reveal that:
using parser statistically significant influence systems performance;
system outperforms volunteer annotators identifying it-extrapositions;
regardless parser used, difference systems performance
WSJ annotation statistically significant;
regardless parser used, system outperforms Paice Husk (1987)
algorithm.
5.5 Correlation Analysis Extrapositions
Figures 5 8 illustrate correlation decision factors true
expletiveness pronoun question. 279 items passed initial syntactic
filtering process included dataset first 116 extrapositional
rest separated break X-axis. arrangement made order better
visualize contrast positive group negative group. Figures 6
8, different grey levels used indicate number results returned
queries darker shade, popular construct question web.
constant Rexp = 0.15 also indicated break Y-axis.
373

fiLi, Musilek, Reformat, & Wyard-Scott

illustrated, factors identified Section 3.5 good indicators expletiveness. W
(Figure 5) weakest four factors due number false positives produced
incorrect language usage. clear evidence web noisier ordinary corpora
results counts web may appropriate sole decision-making
factor. comparison, r (Figure 6) almost perfect correlation expletiveness
instances. However, full versions queries usually return fewer results many
cases yield results expletive cases (unfilled items plotted top graph
indicate cases enough results, c.f. Section 3.5). stepped-down versions
queries (Figure 7), less accurate themselves, serve well used
back up, illustrated R plot (Figure 8). Part false-positive outliers
R plot produced full queries expressions habitually associated it,
[0135:002] . . . said expects post sales current fiscal year . . . .
used pronoun, expressions usually describe information quoted person
organization already named earlier sentence, making natural
choice subject pronoun. Normally problematic expressions take form verb
infinitive-complement, i.e. S=TRUE. According decision process described
Section 3.5, W also considered situation, effectively eliminates noise.

374

fiIdentification Pleonastic Using Web

Figure 5: scatter plot illustrating correlation W (the estimated number
valid results returned what-cleft queries) expletiveness
instance. extrapositional instances arranged left side plot
rest cases right. query returns valid results,
corresponding item shown hollow circle bottom plot.

375

fiLi, Musilek, Reformat, & Wyard-Scott

Number

Results

Figure 6: scatter plot illustrating correlation r (the ratio hit count
produced expression substitute pronouns original expression) expletiveness instance. extrapositional instances
arranged left side plot rest cases right.
items shaded according hit counts produced corresponding
original expressions. query returns insufficient results, corresponding item
shown hollow unshaded circle top plot.

376

fiIdentification Pleonastic Using Web

Number

Results

Figure 7: scatter plot illustrating correlation r0 (similar r
stepped-down queries) expletiveness instance. extrapositional instances arranged left side plot rest cases
right. items shaded according hit counts produced
corresponding original expressions. query returns insufficient results,
corresponding item shown hollow unshaded circle top plot.

377

fiLi, Musilek, Reformat, & Wyard-Scott

Number

Results

Figure 8: scatter plot illustrating correlation R (synthesized expletiveness;
takes value r complex queries produce enough results,
takes value r0 fail so) expletiveness
instance. extrapositional instances arranged left side plot
rest cases right. items shaded according
hit counts produced corresponding original expressions. query returns
insufficient results, corresponding item shown hollow unshaded circle
top plot.

5.6 Generalization Study
order evaluate well system generalizes, 500 additional sample sentences
randomly selected rest WSJ corpus test dataset. distribution
instances comparable development dataset, shown Table 17.
378

fiIdentification Pleonastic Using Web

Category
Nominal Antecedent
Clause Antecedent
Extraposition
Cleft
Weather/Time
Idiom

Grand Total

Instances
375
24
63
8
6
11
13
500

Percentage
75.00%
4.80%
12.60%
1.60%
1.20%
2.20%
2.60%
100.00%

Table 17: Profile test dataset according authors annotation
shown Table 18, overall level inter-annotator agreement slightly higher
development dataset. Except idiom category, categorical values
also higher counterparts development dataset. discrepancy
likely due chance, since two volunteers worked independently started
different datasets (Volunteer 1 started development dataset Volunteer 2 started
test dataset).
Category

Precision

Referential
98.48%
Extraposition
87.10%
Cleft
29.41%
Weather/Time 100.00%
Idiom
31.82%
Overall Accuracy/


Volunteer 1
Recall F-measure

95.12%
85.71%
62.50%
50.00%
53.85%
91.80%

96.77%
86.40%
40.00%
66.67%
40.00%

Precision

97.30%
80.00%
57.14%
100.00%
47.06%

Volunteer 2
Recall F-measure

96.83%
82.54%
50.00%
50.00%
61.54%
92.80%

97.07%
81.25%
53.33%
66.67%
53.33%


.797
.811
.490
.665
.280
.720

values statistically significant p < 0.0001.

Table 18: Performance volunteer annotators test dataset (evaluated using
authors annotation reference) degree inter-annotator agreement
measured Cohens kappa (). authors annotations refitted
simplified annotation scheme used volunteers.

Measurement
Reference
Identified Baseline
Baseline True Positives
Precision
Recall
F-measure


WSJ Annotation
Extraposition
Cleft
63
8
54
6
52
6
96.30% 100.00%
82.54%
75.00%
88.89%
85.71%

Replicated PHA
Overalla
77
97
55
56.70%
71.43%
63.22%

Includes clefts, extrapositions, time/weather cases.

Table 19: Performance baselines test dataset, evaluated authors
annotation.
379

fiLi, Musilek, Reformat, & Wyard-Scott

Table 19 summarizes performance baselines test dataset. two
(54 52) false-positive extrapositions WSJ annotation listed together
respective context:
[1450:054-055] Another solution cities might consider giving special priority
police patrols small-business areas. cities losing business
suburban shopping centers, may wise business investment
help keep jobs sales taxes within city limits.
[1996:061-062] think go turn things around. tough
thing cant.
first case considered referential, second case believed refer
hypothetical situation introduced when-clause.
5.6.1 Performance Analysis
test dataset, system able maintain precision; exhibits slight deterioration recall overall performance still within expectations. findings
summarized Table 20.
Extraposition
63
60
58

Cleft
8
6
6

Weather/Time
6
7
6

Overalla
77
73
70

Precision
95% C.I.b

96.67%
91.38-100.00%

100.00%
64.26-100%

85.71%
50.00-100.00%

95.89%
90.77-100.00%

Recall
95% C.I.b

92.06%
84.85-98.25%

75.00%
40.00-100.00%

100.00%
64.26-100.00%

90.91%
84.15-97.01%

F-measure
95% C.I.

94.31%
89.60-98.11%

85.71%
57.14-100.00%

92.31%
66.67-100.00%

93.33%
88.75-97.10%

Measurement
Reference
Identified
True Positives


b

Combining extraposition, cleft, weather/time one category.
Adjusted Wald intervals reported extreme measurements.

Table 20: Performance system test dataset, evaluated using authors
annotation reference.

149 instances evaluated extraposition using queries, covering 62 63 extrapositions. excluded case introduced form direct question, whose particulars
syntactic processing subsystem prepared for. four false negatives,
three involve noun phrases matrix object position. One two clefts
recognized arises imperfect processing corpus. addition, false positive
weather/time category caused verb hail , treated noun
system.
five (63 58) false-negative extraposition cases annotated corpus
WSJ annotation agrees six clefts identified proposed system. Thus
380

fiIdentification Pleonastic Using Web

systems recall ratio verified WSJ annotations 90.38% extraposition 100%
cleft.
Target System
Volunteer 1
Volunteer 2
WSJ Annotation
Replicated PHA

Extraposition

Cleft

Weather/Time

F-measure+ /p = .041 F-measure+ /p = .005 F-measure+ /p = .248
F-measure+ /p = .002 F-measure+ /p = .119 F-measure+ /p = .254
Precision /p = .697 F-measure= /p = 1.00
(All Categories) Precision+ /p < .001

Table 21: Results statistical significance tests, presented format
Test Statisticsign /p-value. plus sign (+ ) indicates system performs
better reported measurement; equal sign (= ) indicates tie; otherwise
minus sign ( ) used. fair comparisons made precision
recall, F-measure used test statistic; otherwise applicable
measurement reported.

Measurement
Reference
Identified
True Positives

Performance Charniak Parser Output
Extraposition
Cleft Weather/Time
63
8
6
58
7
7
55
6
6

Overalla
77
72
67

Precision
95% C.I.

94.83%
88.24-100.00%

85.71%
50.00-100.00%

85.71%
50.00-100.00%

93.06%
86.36-98.51%

Recall
95% C.I.b

87.30%
78.26-95.08%

75.00%
37.50-100.00%

100.00%
64.26-100.00%

87.01%
78.95-94.12%

F-measure
95% C.I.

90.91%
84.75-95.77%

80.00%
50.00-100.00%

92.31%
66.67-100.00%

89.93%
84.30-94.57%

Performance Berkeley Parser Output
Extraposition
Cleft Weather/Time
63
8
6
58
5
7
56
5
6

Overalla
77
70
67

Measurement
Reference
Identified
True Positives
Precision
95% C.I.b

96.55%
91.11-100.00%

100.00%
59.90-100.00%

85.71%
50.00-100.00%

95.71%
90.28-100.00%

Recall
95% C.I.b

88.89%
80.60-96.23%

62.50%
25.00-100.00%

100.00%
64.26-100.00%

87.01%
79.22-93.90%

F-measure
95% C.I.

92.56%
87.14-96.97%

76.92%
40.00-100.00%

92.31%
66.67-100.00%

91.16%
85.94-95.52%


b

Combining extraposition, cleft, weather/time one category.
Adjusted Wald intervals reported extreme measurements.

Table 22: Performance system test dataset using parser-generated output,
evaluated using authors annotation reference.
381

fiLi, Musilek, Reformat, & Wyard-Scott

Results significance tests, summarized Table 21, reveal following additional
information systems performance test dataset:
systems higher performance recognizing it-extrapositions volunteers
statistically significant;
extraposition category, difference WSJ annotations (higher) precision system statistically significant;
system outperforms Paice Husk (1987) algorithm, difference
statistically significant.
Tables 22 23 outline systems performance test dataset parsers
used. Again, parsers cause slight deteriorations system performance. However,
changes statistically significant. either parser used, system able
perform well WSJ annotations.
Comparing System Performance Charniak Parser Output to:
Target System
Extraposition
Cleft
Weather/Time


System w/o Parser F-measure /p = .125 F-measure /p = 1.00 F-measure= /p = 1.00
Volunteer 1
F-measure+ /p = .298 F-measure+ /p = .013 F-measure+ /p = .247
Volunteer 2
F-measure+ /p = .022 F-measure+ /p = .269 F-measure+ /p = .246
WSJ Annotation
Precision /p = .886 F-measure /p = 1.00
Replicated PHA
(All Categories) Precision+ /p < .001
Comparing System Performance Berkeley Parser Output to:
Target System
Extraposition
Cleft
Weather/Time
System w/o Parser F-measure /p = .501 F-measure /p = 1.00 F-measure= /p = 1.00
Volunteer 1
F-measure+ /p = .131 F-measure+ /p = .035 F-measure+ /p = .256
Volunteer 2
F-measure+ /p = .009 F-measure+ /p = .308
F-measure+ /p = .27


WSJ Annotation
Precision /p = .809 F-measure /p = 1.00
Replicated PHA
(All Categories) Precision+ /p < .001
Table 23: Results statistical significance tests comparing systems performance
parser output various systems, presented format
Test Statisticsign /p-value. plus sign (+ ) indicates source system performs better reported measurement; equal sign (= ) indicates tie;
otherwise minus sign ( ) used. fair comparisons made
precision recall, F-measure used test statistic; otherwise
applicable measurement reported.

5.6.2 Estimated System Performance Whole Corpus
relative sparseness clefts makes hard assess real effectiveness proposed
approach. compensate this, approximate study conducted. First, instances
whole corpus processed automatically using proposed approach. identified
382

fiIdentification Pleonastic Using Web

cleft instances merged already annotated corpus
form evaluation dataset 84 sentences, subsequently verified manually. 76
instances 84 considered valid cleft constructs authors. Respective
performances proposed approach WSJ annotation reported Table 24;
differences statistically significant.
System
WSJ

Proposed
Approach

Total
76

Identified
66

Common
Precision
63
95.45%
95% C.I.: 89.55-100.00%

Recalla
82.94%
74.32-90.79%

F-measurea
88.73%
82.86-93.79%

76

75

70
93.33%
95% C.I.: 87.50-98.65%

92.11%
85.53-97.40%

92.72%
87.84-96.65%



reported recall ratios F-measures synthetic dataset cannot extended
whole corpus.

Table 24: Estimated system performance it-cleft identification entire corpus
Three false positives produced proposed approach actually extrapositions22 , expected (c.f. Footnote 12, Page 351). Thus, binary classification
pleonastic it, items cleft category higher contributions overall
precision category. whole corpus annotated,
impossible obtain precise recall figures either WSJ annotations proposed
approach. However, since rest corpus (other synthetic dataset)
contain true positives either system contains number false-negatives
systems, proposed system maintain higher recall ratio
WSJ annotations whole corpus.
similar experiment conducted extrapositions using sentences already
annotated corpus. 656 annotated extrapositional instances manually verified
637 (97.10%) turn valid cases. system produced queries 623
instances consequently recognized 575 them, translating 90.27% (95% C.I. 89.0193.56%) recall ratio verified annotations. Given fact development
dataset test dataset proposed system yields slightly higher recall whole
dataset subsets identified WSJ annotations, performance
extrapositions whole WSJ corpus likely remain 90% recall.
Similar situation test based random cases, large portion falsepositives contributed imperfect handling surface structures noun phrases
matrix object position, particularly form takes/took . . . . . .
additional experiments, seems particular construct addressed
different pattern, what/whatever takes verb, eliminates noun phrase.
Alternatively, construct could possibly assumed extrapositional without issuing
queries all.
22
kind cleft separated extrapositions using additional pattern attaches
prepositional phrase subordinate verb. However, number samples justify
inclusion study.

383

fiLi, Musilek, Reformat, & Wyard-Scott

6. Discussion
paper novel pleonastic-it identification system proposed. Unlike precursors,
system classifies extrapositions submitting queries web analyzing returned
results. set rules also proposed classification clefts, whose particular manner
composition makes difficult apply web-based approach. Components
proposed system simple effectiveness independent type text
processed. shown generalization tests, system maintains precision
recall degrades small margin confronted unfamiliar texts.
indication general principles behind system over-fitted text
derived. Overall, evaluated WSJ news articles
considered difficult type nonfiction system capable producing results
par slightly inferior casually trained humans.
systems success important implications beyond particular problem
pleonastic-it identification. First, shows web used answer linguistic questions based upon simplistic semantic relationships. Second,
comparative study effective means get highly accurate results web despite fact noisier manually compiled corpora. addition, success
simple guidelines used identifying clefts may serve evidence speakers
intention heavily reflected surface structures utterance, bid
make distinguishable similarly constructed sentences.
problems left unaddressed current study, notably handling
complex noun phrases prepositional phrases. Generally speaking, approach
query instantiation somewhat crude. solve noun-phrase issue, finer-grained query
downgrading proposed, viz. first supply query original noun phrase,
head noun, finally adjective modifies head noun, one.
effectiveness approach determined. discussed Section 5.6.2, special
rule used verb take. This, however, may open door exception-based
processing, contradicts principle system provide unified approach
pleonastic pronoun identification. Overall, much data experiments
needed query instantiation procedures finalized.
Aside two sets patterns currently use, information
used assess validity possible extraposition. example, extrapositions
matrix verbs much likely remain present tense past tense,
noun phrases (if any) matrix object position likely indefinite,
extraposed clauses generally longer matrix verb phrases. fuzzy-based
decision system multiple input variables could possibly provide significant performance
gains.
Although system able yield reasonable performances output either
parser tested, introduce additional errors final results. combined
dataset development test items, parsers cause statistically significant deteriorations performance significance level 0.1 (Charniak parser: p=0.008 F-measure
extrapositions; p=0.071 F-measure clefts). possible incorporating
pattern-based method compensate problems caused imperfect parsing
improve recall ratios; however, data needed confirm this.
384

fiIdentification Pleonastic Using Web

Another concern syntactic processing component used system limited.
limitation, caused designers lack exposure large variety different
constructs, essentially different problem imposed limited number
patterns previous systems. Eventually, proposed system, limitation
eliminated. illustrate, current design able correctly process sentences like
difference make buy; however, takes minor effort correct
upgrading subsystem recognizes pre-posed objects. upgrade,
may performed manually even automatically machine-learning
approaches, solves one syntactic problems moves system closer able
recognize grammatically valid constructs. contrast, take considerably
effort patch rigidly defined rules upgrade word lists rule-based
systems achieve comparable performances.
writing article, Google deprecated SOAP-based search API.
move makes technically difficult precisely replicate results reported study
since search engines lack ability process alternate expressions (i.e. WordA
WordB ) embedded within quoted query. use different search engine, matrix verbs
expanded instead converted respective third-person
singular present form only. Stubs also simplest form only, described
earlier sections. preliminary experiments also seems possible replace
combination which/who/this/he alone, plus necessary changes maintain
number agreement among constituents queries. changes may
negative effects final outcome system, unlikely severe.
Like NLP tasks, classifying usage inherently difficult, even
human annotators already knowledge problem one thing
speak language, another clearly explain rationale behind specific
construct. Although widely accepted extrapositional expletive, line
extrapositional cases referential ones sometimes thin.
clearly manifested existence truncated extrapositions (Gundel et al., 2005),
obviously valid referential readings. Similar things said relationship
among three pleonastic categories well idioms. example, Paice Husk classify
remains . . . idiom construct classified extraposition
evaluations. Aside applying syntactic guidelines proposed study,
assumed annotation process extraposition either valid
non-extraposed reading valid what-cleft reading. also assumed cleft
generate valid non-clefted reading joining clefted constituent directly cleft
clause without leading relative pronoun adverb. light subjective nature
problem, annotations published web online appendix better
serve readers.

References
Agresti, A., & Coull, B. A. (1998). Approximate better exact interval estimation binomial proportions. American Statistician, 52 (2), 119126.
Berland, M., & Charniak, E. (1999). Finding parts large corpora. Proceedings 37th annual meeting Association Computational Linguistics
385

fiLi, Musilek, Reformat, & Wyard-Scott

Computational Linguistics, pp. 5764.
Bies, A., Ferguson, M., Katz, K., & MacIntyre, R. (1995). Bracketing guidelines Treebank II style. Tech. rep. MS-CIS-95-06, Department Computer Information
Science, University Pennsylvania.
Boyd, A., Gegg-Harrison, W., & Byron, D. (2005). Identifying non-referential it: machine
learning approach incorporating linguistically motivated patterns. Proceedings
ACL Workshop Feature Engineering Machine Learning Natural Language
Processing, pp. 4047. Association Computational Linguistics.
Charniak, E., & Johnson, M. (2005). Coarse-to-fine n-best parsing maxent discriminative reranking. Proceedings 43rd Annual Meeting Association
Computational Linguistics (ACL05), pp. 173180, Morristown, NJ, USA. Association
Computational Linguistics.
Chinchor, N. (1992). statistical significance MUC-4 results. Proceedings
4th conference Message understanding (MUC4), pp. 3050, San Mateo, CA.
Morgan Kaufmann.
Cimiano, P., Schmidt-Thieme, L., Pivk, A., & Staab, S. (2005). Learning taxonomic relations heterogeneous evidence. Buitelaar, P., Cimiano, P., & Magnini, B.
(Eds.), Ontology Learning Text: Methods, Applications Evaluation, Frontiers Artificial Intelligence Applications, pp. 5973. IOS Press, Amsterdam.
Clemente, J. C., Torisawa, K., & Satou, K. (2004). Improving identification nonanaphoric using support vector machines. Proceedings International
Joint Workshop Natural Language Processing Biomedicine Applications
(NLPBA/BioNLP04).
Cohen, J. (1960). coefficient agreement nominal scales. Educational Psychological Measurement, 20 (1), 3746.
Collins, M. (1999). Head-Driven Statistical Models Natural Language Parsing. Ph.D.
thesis, University Pennsylvania.
Davidse, K. (2000). constructional approach clefts. Linguistics, 38 (6), 11011131.
Davison, A. C., & Hinkley, D. V. (1997). Bootstrap Methods Application. Cambridge series statistical probabilistic mathematics. Cambridge University Press,
Cambridge, UK.
Denber, M. (1998). Automatic resolution anaphora English. Tech. rep., Eastman
Kodak Co.
Di Eugenio, B., & Glass, M. (2004). kappa statistic: second look. Computational
Linguistics, 30 (1), 95101.
Efron, B., & Tibshirani, R. (1993). Introduction Bootstrap. Chapman Hall,
New York, USA.
Evans, R. (2000). comparison rule-based machine learning methods identifying
non-nominal it. Christodoulakis, D. (Ed.), Proceedings 2nd International
Conference Natural Language Processing (NLP00), Vol. 1835 Lecture Notes
Computer Science, pp. 233241, Berlin. Springer.
386

fiIdentification Pleonastic Using Web

Evans, R. (2001). Applying machine learning toward automatic classification it.
Literary Linguistic Computing, 16 (1), 4557.
Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press,
Cambridge, Mass., USA.
Geurts, B., & van der Sandt, R. (2004). Interpreting focus. Theoretical Linguistics, 30 (1),
144.
Green, P. S., & Hecht, K. (1992). Implicit explicit grammar: empirical study. Applied
Linguistics, 13 (2), 168184.
Gundel, J., Hedberg, N., & Zacharski, R. (2005). Pronouns without NP antecedents:
know pronoun referential?. Branco, A., McEnery, T., & Mitkov,
R. (Eds.), Anaphora Processing: Linguistic, Cognitive Computational Modelling,
pp. 351364. John Benjamins, Amsterdam, Netherlands.
Gundel, J. K. (1977). cleft sentences come from?. Language, 53 (3), 543559.
Hamawand, Z. (2003). For-to complement clauses English: cognitive grammar analysis.
Studia Linguistica, 57 (3), 171192.
Hearst, M. A. (1992). Automatic acquisition hyponyms large text corpora.
Proceedings 14th international conference Computational Linguistics, pp.
539545.
Hedberg, N. (1990). Discourse Function Cleft Sentences English. Ph.D. thesis,
University Minnesota.
Hedberg, N. (2000). referential status clefts. Language, 76 (4), 891920.
Kaltenbock, G. (2005). It-extraposition English: functional view. International Journal
Corpus Linguistics, 10 (2), 119159.
Kilgarriff, A. (2007). Googleology bad science. Computational Linguistics, 33 (1), 147151.
Kilgarriff, A., & Grefenstette, G. (2003). Introduction special issue Web
corpus. Computational Linguistics, 29 (3), 333347.
Krifka, M. (2003). Bare NPs: Kind-referring, indefinites, both, neither?. Proceedings
Semantics Linguistic Theory (SALT) XIII, New York, USA. CLC Publications.
Krippendorff, K. (1980). Content Analysis: Introduction Methodology. Sage Publications, Inc., Beverly Hills, USA.
Lambrecht, K. (2001). framework analysis cleft constructions. Linguistics,
39 (3), 463516.
Lappin, S., & Leass, H. J. (1994). algorithm pronominal anaphora resolution. Computational Linguistics, 20 (4), 535561.
Marcus, M. P., Marcinkiewicz, M. A., & Santorini, B. (1993). Building large annotated
corpus English: Penn Treebank. Computational Linguistics, 19 (2), 313330.
Markert, K., & Nissim, M. (2005). Comparing knowledge sources nominal anaphora
resolution. Computational Linguistics, 31 (3), 367402.
387

fiLi, Musilek, Reformat, & Wyard-Scott

Markert, K., Nissim, M., & Modjeska, N. N. (2003). Using web nominal anaphora
resolution. Dale, R., van Deemter, K., & Mitkov, R. (Eds.), Proceedings
EACL Workshop Computational Treatment Anaphora, pp. 3946.
Metcalf, A., & Barnhart, D. K. (1999). America Many Words: Words
Shaped America. Houghton Mifflin, Boston, USA.
Mitkov, R. (2001). Outstanding issues anaphora resolution. Gelbukh, A. (Ed.),
Proceedings 2nd International Conference Computational Linguistics
Intelligent Text Processing (CICLing01), Vol. 2004 Lecture Notes Computer
Science, pp. 110125, Berlin. Springer.
Mitkov, R., Evans, R., & Orasan, C. (2002). new, fully automatic version Mitkovs
knowledge-poor pronoun resolution method. Gelbukh, A. F. (Ed.), Proceedings
3rd International Conference Computational Linguistics Intelligent Text
Processing (CICLing02), Vol. 2276 Lecture Notes Computer Science, pp. 168186,
London, UK. Springer-Verlag.
Muller, C. (2006). Automatic detection nonreferential spoken multi-party dialog.
Proceedings 11th Conference European Chapter Association
Computational Linguistics (EACL06), pp. 4956.
Nanni, D. L. (1980). surface syntax constructions easy-type adjectives.
Language, 56 (3), 568581.
Ng, V., & Cardie, C. (2002). Identifying anaphoric non-anaphoric noun phrases
improve coreference resolution. Proceedings 19th international conference
Computational linguistics (COLING02), pp. 17, Morristown, NJ, USA. Association
Computational Linguistics.
Noreen, E. W. (1989). Computer-Intensive Methods Testing Hypotheses : Introduction. Wiley-Interscience, New York, USA.
Paice, C. D., & Husk, G. D. (1987). Towards automatic recognition anaphoric
features english text: impersonal pronoun it. Computer Speech & Language,
2 (2), 109132.
Petrov, S., Barrett, L., Thibaux, R., & Klein, D. (2006). Learning accurate, compact,
interpretable tree annotation. Proceedings 21st International Conference
Computational Linguistics 44th annual meeting ACL (ACL06), pp.
433440, Morristown, NJ, USA. Association Computational Linguistics.
Poesio, M., Ishikawa, T., im Walde, S. S., & Vieira, R. (2002). Acquiring lexical knowledge
anaphora resolution. Proceedings Third International Conference
Language Resources Evaluation, pp. 12201224.
Sinclair, J. (Ed.). (1995). Collins COBUILD English Grammar. Harper Collins, London,
U.K.
Sloat, C. (1969). Proper nouns English. Language, 45 (1), 2630.
van Rijsbergen, C. J. (1979). Information Retrieval (2nd edition). Butterworth-Heinemann,
Newton, MA, USA.
388

fiIdentification Pleonastic Using Web

Xia, F., & Palmer, M. (2001). Converting dependency structures phrase structures.
Proceedings first international conference Human language technology
research (HLT01), pp. 15, Morristown, NJ, USA. Association Computational
Linguistics.

389

fiJournal Artificial Intelligence Research 34 (2009) 209-253

Submitted 06/2008; published 03/2009

Mechanisms Making Crowds Truthful
Radu Jurca

radu.jurca@gmail.com

Google Inc., Switzerland

Boi Faltings

boi.faltings@epfl.ch

Ecole Polytechnique Federale de Lausanne (EPFL)
Artificial Intelligence Laboratory (LIA)
CH-1015 Lausanne, Switzerland

Abstract
consider schemes obtaining truthful reports common hidden signal
large groups rational, self-interested agents. One example online feedback
mechanisms, users provide observations quality product service
users accurate idea quality expect. However,
(i) providing feedback costly, (ii) many motivations providing
incorrect feedback.
problems addressed reward schemes (i) cover cost obtaining
reporting feedback, (ii) maximize expected reward rational agent
reports truthfully. address design incentive-compatible rewards feedback
generated environments pure adverse selection. Here, correlation
true knowledge agent beliefs regarding likelihoods reports
agents exploited make honest reporting Nash equilibrium.
paper extend existing methods designing incentive-compatible rewards
also considering collusion. analyze different scenarios, where, example,
agents collude. scenario investigate whether collusion-resistant,
incentive-compatible reward scheme exists, use automated mechanism design specify
algorithm deriving efficient reward mechanism.

1. Introduction
increasing number applications artificial intelligence extract knowledge large
groups agents, also termed wisdom crowds. One example online feedback forums (also known reputation mechanisms) obtaining information
products services. testimonies previous buyers disclose hidden, experience-related
(Parasuraman, Zeithaml, & Berry, 1985), product attributes quality, reliability,
ease use, etc., observed purchase. previously unavailable
information allows buyers make better, efficient decisions, eliminates
problems would otherwise lead collapse online markets1 .
Recent studies, however, raise important questions regarding ability existing reputation mechanisms reflect real quality product. First, absence clear
incentives drives users voice opinions. example, Hu, Pavlou,
1. Akerlof (1970) warns Market Lemons, asymmetric information drives all, except
worst quality sellers market.
c
2009
AI Access Foundation. rights reserved.

fiJurca & Faltings

Zhang (2006) Admati Pfleiderer (2000) show Amazon2 ratings books
CDs follow great probability bi-modal, U-shaped distributions
ratings either good, bad. controlled experiments items
reveal normally distributed opinions, authors conclude users moderate outlook unlikely report. Talwar, Jurca, Faltings (2007) identify another factor
promotes rating, namely desire contribute something new previously
submitted reports. cases, reputation mechanism collects unrepresentative
sample reviews necessarily informative average user.
Second, even distressful, users intentionally lie order gain external
benefits distorted reputation. Harmon (2004) reports authors write fake
reviews Amazon order boost sale books, trash reputation
competing titles. White (1999) describes manipulation techniques pushing songs
charts, Elliott (2006) Keates (2007) identify problems associated fake hotel
reviews travel reputation site TripAdvisor.com. Although still see high levels
altruistic (i.e., honest) reporting, increasing awareness gains made
manipulating online reputation likely attract dishonest reporting future.
problems solved reputation mechanism rewards users reporting
feedback. First, reward cover cost reporting, users leave
feedback allow reputation mechanism estimate precisely quality
products services. Second, honest feedback yield higher rewards lying,
rational agents find best interest truthful. technique limited
reputation mechanisms, applies generally setting private signal
inferred reports crowd self-interested rational agents.
reader might already ask whether reasonable assume explicit
payments 3 incentivise users change reporting behavior. Although humans
known sometimes act irrationally, many examples online systems rewards successfully promoted elicitation private information. Prediction markets,
example, consistently outperform traditional prediction tools (Figlewski, 1979; Pennock,
Debnath, Glover, & Giles, 2002) users seem equally motivated fake real
money (Servan-Schreiber, Wolfers, Pennock, & Galebach, 2004). Another example
ESP Game4 extracted impressive volume image tags rewarding players
virtual points. Moreover, future online economy likely contain increasing numbers automated agents, design, programmed behave rationally
maximize utility.
Fundamental results mechanism design literature (dAspremont & Grard-Varet,
1979; Cremer & McLean, 1985) show side payments designed create
incentive agents reveal private opinions truthfully. best payment
schemes constructed based proper scoring rules (Kandori & Matsushima, 1998;
Johnson, Pratt, & Zeckhauser, 1990; Clemen, 2002), exploit correlation
observations different buyers good.
2. http://www.amazon.com
3. term payments general includes non-monetary rewards preferential access resources,
social status bonus points.
4. http://www.espgame.org/

210

fiMechanisms Making Crowds Truthful

Miller, Resnick, Zeckhauser (2005) adapt results online feedback forums
characterized pure adverse selection. environments, buyers observe
innate quality attributes products service providers, possibly noise.
role reputation mechanism signaling, i.e. aggregate reports buyers
accurate estimates attributes. Examples situation product rating
forums Amazon, ePinions Bizrate, services provided
machines networks anonymous fashion.
contrast reputation mechanisms also used sanctioning role counter moral
hazard. exists environments provider vary quality attributes
particular buyer strategic manner. role reputation mechanism
spread information seller misbehavior increase cost make unattractive.
example environment seller ratings online marketplaces.
two roles reputation complementary, solve two important problems associated online markets (Dellarocas, 2006). signaling role acts
information asymmetries, allows agents accurately identify capable partners.
sanctioning role, hand, acts cheating incentives encourages honest
behavior.
Like Miller et al. (2005), concentrate paper pure signaling mechanisms
ignore effects associated moral hazard. set users assumed experience
product service, possibly noise, later report privately perceived
quality signal central reputation mechanism. product service assumed
consistent quality time, quality observed different users modeled
randomly drawn distribution. assumptions quite common
services delivered automated systems like web services intelligent agents, failures
degradation performance due random events planned
strategic operator.
reputation mechanism scores every submitted feedback comparing another report (called reference report) submitted different user
good. Miller et al. (2005) prove existence general incentive-compatible payments
honest reporting Nash equilibrium, expected reward large enough
cover effort reporting.
Intuitively, incentive-compatible payments exploit correlation private
signal observed agent, agents beliefs regarding reference report. Different
quality signals trigger different updates agents private beliefs (by Bayes Law),
thus modify agents expectations regarding value reference report. paying
reporters according well submitted feedback improves public predictor
reference report (tested actual reference report, assumed honest), agents
incentive align public predictor private beliefs, thus report
truth. Honest reporting becomes Nash equilibrium.
Unfortunately, honest reporting Nash Equilibrium (NE) mechanism. Jurca Faltings (2005) show binary incentive-compatible payment mechanisms using single reference report several equilibria; moreover, least one lying
equilibrium gives agents higher expected payoffs truthful NE. brings
forth problem collusion, rational agents could potentially coordinate lying
equilibrium gives higher payoff honest equilibrium.
211

fiJurca & Faltings

simplest lying equilibrium agents always report same, thus
leading perfect prediction reference reports. product service real
world occasional defects, truthful reporting always noisy predictor
reference report, thus able match payoff lying strategy.
overcome problem using single several reference reports. start
observation real world, even perfect products services
occasionally defective. Thus, reward reports predict slightly
imperfect situation. key idea score report set least 4 reference
reports, reward report according distribution reference reports,
without considering order. giving higher reward matching one
reference reports, possible give higher expected payoff truthful reporting
equilibrium.
difficult see scale rewards obtain characteristic, use
automated mechanism design (Conitzer & Sandholm, 2002) compute rewards
satisfy criteria. technique first applied problem Jurca
Faltings (2006) compute minimal payments required ensure honest reporting
reputation information. Jurca Faltings (2007a) augment technique formulating
requirement collusion safety additional constraints desired mechanism.
paper extends previous work presenting unified framework designing incentivecompatible, collusion resistant rewards broader set collusion scenarios.
concretely, vary complexity collusion scenario along three dimensions.
First, consider size coalition, study happens
agents become part lying coalition. complexity coordination second
dimension, consider cases colluders necessary sophistication
coordinate different reporting strategies. Finally, third dimension addresses
transfer utilities includes settings colluders make side-payments
colluders.
However, combinations formally treated; contain contradictory assumptions (e.g., colluders assumed capable side-payments, also assumed
capable coordinate different strategies) lead trivial impossibility results (e.g., collusion resistance clearly impossible one strategic agent controls online identities,
exactly scenario agents collude may transfer payments among themselves).
paper proceeds follows. Section 2 formally introduces model, Section 3
introduces incentive-compatible payment mechanisms presents properties. Section 4 addresses design collusion-resistant reward mechanisms different
scenarios. Finally discuss related work future directions improve results.

2. Model
consider online market number rational buyers (or agents) experience
product (or service). quality product remains fixed, defines
products (unknown) type. finite set possible types, denotes member
set. assume buyers share
P common belief regarding prior probability
P r[], product type . P r[] = 1.
212

fiMechanisms Making Crowds Truthful

purchase, every buyer perceives binary signal quality (i.e., true
type) product. 1 denotes high quality signal captures satisfaction
buyer product. 0, hand, denotes low quality signal,
buyers dissatisfaction product. Every product type characterized
different probability distribution signals perceived buyers. Let P r[1|]
probability buyer product type satisfied (i.e., observes quality
signal 1). P r[1|1 ] 6= P r[1|2 ] 1 6= 2 , P r[1|] assumed common knowledge.
make simpler reader follow formal notation, present
numerical example. example extended introduce new notation,
serve subsequent sections illustrate results.
Example. Alice, owner old house, needs plumbing work done. knows
good (type G ) bad (type B ) plumbers, good plumbers provide high
quality service much higher probability: e.g., P r[1|G ] = 0.9 P r[1|B ] = 0.15. Alice
picks plumber Yellow Pages, given reputation source, believes
plumber, Bob, likely good: e.g., P r[G ] = 0.8 P r[B ] = 0.2. Therefore,
Alice expects get good service probability P r[G ]P r[1|G ] + P r[B ]P r[1|B ] = 0.75.
central reputation mechanism asks every buyer submit feedback. Buyers assumed rational,
report
truth. set pure reporting strategies
constrained

buyer = s(0), s(1)
|s(0),
s(1)

Q
2 , Q2 = {0, 1} set quality sig
nals, = s(0), s(1) denotes strategy according buyer announces
s(0) Q2 observes low quality, s(1) Q2 observes high quality.
often call reports 0 1 negative, respectively positive report.
ease notation, name four members set honest strategy (s),
lying strategy (slie ), always reporting one strategy (spos ) always reporting
0 strategy (sneg ):
= (0, 1) buyer reports 0 observes low quality 1 observes
high quality;
slie = (1, 0) buyer reports 1 observes low quality 0 observes
high quality;
spos = (1, 1) buyer reports 1 regardless observation;
sneg = (0, 0) buyer reports 0 regardless observation;
reputation mechanism rewards buyers submitted reports. payment
received buyer depend information available reputation mechanism:
namely, reports submitted buyers, common knowledge regarding
environment (probability distribution types, conditional probability distributions
quality signals). assume reputation mechanism updates public reputation
information batches N reports. agents submitted N reports
batch assumed access public information, motivates
common priors assumption beginning section. rest paper
analyze reward mechanisms work static sets N reports; real settings,
213

fiJurca & Faltings

however, mechanisms designed batches size
N.
Note reputation mechanism (i) know true type product,
(ii) cannot purchase product order get first-hand experience regarding
quality.
Discarding notation dependence common knowledge, payment
mechanism (employed reputation mechanism) function : Q2 (Q2 )N 1 R+ ,
(ri , ri ) 0 amount paid buyer reports ri Q2
N 1 buyers report ri (Q2 )N 1 . reports ri also called reference reports
agent i, since constitute reference computing payment agent i.
constrain payments non-negative online forums cannot impose punishments
reporters.
order reports important, therefore simplify payment mechanism
) r

assuming (ri , ri ) = (ri , ri
ri contain number
positive reports. compact description payment mechanism thus given
amounts (r, n) n {0, 1, . . . , N 1} number positive reports submitted
reference reporters.
payoff expected agent depends distribution reference reports.
agents report honestly, distribution reference reports computed
prior beliefs, true observation, oi Q2 agent i. probability
exactly n positive reports submitted N 1 agents is:
P r[n|oi ] =

X

P r[n|]P r[|oi ];

(1)



P r[n|] binomial probability distribution function, P r[|oi ] computed Bayes Law:
N 1

N 1n
P r[1|]n 1 P r[1|]
;
n
X
P r[oi |]P r[]
P [|oi ] =
; P r[oi ] =
P r[oi |]P r[];
P r[oi ]
P r[n|] =



Example. Bob plumber gets work done, Alice observes result learns
something new Bobs type. Alice sees good work, posterior belief regarding
type Bob P r[G |1] = 1P r[B |1] = 0.96 (computed Bayes Law), therefore,
Alice believe client get good service Bob probability:
P r[1|1] = P r[1|G ]P r[G |1] + P r[1|B ]P r[B |1] = 0.87. hand, Alice
happy work done Bob, posterior belief be: P r[G |0] = 1 P r[B |0] =
0.32, expect another client receive good service Bob probability:
P r[1|0] = P r[1|G ]P r[G |0] + P r[1|B ]P r[B |0] = 0.39.
reputation mechanism offers Alice following reward scheme: report paid
matches reference report. negative report paid $2.62, positive report
paid $1.54. Consequently, reward scheme formally described (0, 0) = 2.62,
(1, 1) = 1.54, (1, 0) = (0, 1) = 0. Assuming reference report truthful,
one easily verify Alice maximizes expected payment reporting truth:
Alice experiences good service plumber, expects client also gets
214

fiMechanisms Making Crowds Truthful

good service probability 87%. Assuming client reports truthfully, Alices
expected payment is: .871.54+.130 = 1.34 reports good service, .870+.132.62 =
0.34 reports bad service; Likewise, Alice experiences bad service, expects
reference report negative probability 1 .39 = .61. case, expected
payment is: .39 1.54 + .61 0 = 0.6 reports good service, .39 0 + .61 2.62 = 1.6
reports bad service. cases, honest reporting better lying $1.
numerical example specifies payments dollars mention
value service object reputation report. specifically avoid dependence reward mechanism value goods traded market. Instead,
relate rewards marginal gain telling truth, monetary unit
defined minimum expected loss agent miss-reports instead telling
truth.
strategy profile vector (si )i=1,...,N , prescribing reporting strategy si
agent i. sometimes use notation = (si , si ), si strategy
profile agents except i; i.e., si = (sj ), j = 1, . . . , 1, + 1, . . . , N . Given
profile reporting strategies (si , si ), let [n, si ] describe belief agent regarding
distribution reference reports, when:
n N 1 agents observe high quality signal, 1
N 1 agents reporting according strategy profile si ;
Given n si , agent believes probability [n, si ](x) x reference reports
positive. si (oi ) Q2 value report prescribed strategy si given true
observation oi , expected payoff agent is:
V (si , si |oi ) =

N
1
X

P r[n|oi ]

n=0

N
1
X



[n, si ](x) si (oi ), x ;

(2)

x=0

Throughout paper restrict attention pure reporting strategies
pure strategy equilibria. reason behind choice grounded practical considerations: mixed strategies mixed strategy equilibria complex difficult
understand, therefore unlikely observed practical applications. Acknowledging
limitations brought assumption, still believe results valuable
number practical scenarios.

3. Incentive-Compatible Payment Mechanisms
section study general payment mechanisms incentive-compatible, without
worrying collusion resistance. payment mechanism incentive-compatible
honest reporting Nash Equilibrium (NE): i.e., agent gain lying
agents report honestly. Formally, let (si , si ) strategy profile agents report
honestly. optimal agent report truth if, observation oi ,
honest report maximizes agents expected payoff:
V (si , si |oi ) > V (si , si |oi ); si \ {s}, oi Q2 ;

Since reference reports truthful, expected payoff agent is:
V (si , si |oi ) =

N
1
X
n=0

215

P r[n|oi ] (oi , n);

fiJurca & Faltings

incentive-compatibility constraints become:
N
1
X

P r[n|oi ] (oi , n) >

n=0

N
1
X

P r[n|oi ] (1 oi , n); oi Q2 ;

(3)

n=0

Practical mechanisms, however, need offset lying incentives offering certain margins
truth-telling. Honest reporting must better lying least margin ,
chosen mechanism designer offset external benefits agent might obtain
lying. Rewriting (3) account margin , incentive-compatible payment
mechanism satisfies constraints:
N
1
X



P r[n|1] (1, n) (0, n) ;

n=0
N
1
X



P r[n|0] (0, n) (1, n) ;

(4)

n=0

formalizing intuition profitable report positively (respectively negatively) observing high (respectively low) quality.
Kandori Matsushima (1998), Miller et al. (2005) show possible
construct payment mechanisms satisfy constraints (4), based scoring rules.
Jurca Faltings (2006) build existence result describe algorithm
computes optimal (i.e., budget minimizing) payment mechanism. use
latter approach paper, obvious practical advantages designing incentive
compatible reputation mechanism cheaply possible.
expected payment honest reporter (in truthful NE) weighted sum
expected payment agent truthfully reports 1, expected
payment agent truthfully reports 0:
N
1
N
1
h

X
X
E V (si , si ) = P r[1]
P r[n|1] (1, n) + P r[0]
P r[n|0] (0, n);
n=0

(5)

n=0

P r[1] (respectively P r[0]) prior probabilities
P agent perceive high
(respectively low) quality, defined as: P r[oi ] = P r[oi |]P r[].
payment scheme minimizes budget required pay one honest report
therefore solves linear optimization problem:
LP 3.1.
N
1
N
1
h

X
X
min E V (si , si ) = P r[1]
P r[n|1] (1, n) + P r[0]
P r[n|0] (0, n);
n=0

s.t.

N
1
X

n=0




P r[n|1] (1, n) (0, n) ;

n=0
N
1
X



P r[n|0] (0, n) (1, n) ;

n=0

(0, n), (1, n) 0; n = {0, 1, . . . , N 1};

216

fiMechanisms Making Crowds Truthful

Although numerical algorithms efficiently solve LP 3.1, analytical solution helps
us gain additional insights structure incentive-compatible payment mechanisms.
turns LP 3.1 simple solution:
Proposition 3.1. incentive-compatible payment scheme minimizes expected
payment honest reporter (defined LP 3.1) is:
(0, n) = 0, n 6= 0;

(1, n) = 0, n 6= N 1

P r[N 1|0] + P r[N 1|1]
;
P r[N 1|1]P r[0|0] P r[N 1|0]P r[0|1]
P r[0|0] + P r[0|1]
(1, N 1) =
;
P r[N 1|1]P r[0|0] P r[N 1|0]P r[0|1]

(0, 0) =

Proof.
optimal payment mechanism symmetric rewards perfect consensus among
reporters: i.e., agent gets rewarded report agrees report
agents. reason consensus rewards optimal comes structure
incentive compatible constraints. Clearly must least two positive payments: one
rewarding negative report configuration reference reports, rewarding
positive report configuration reference reports. proof (the full
details available Appendix A) shows enough two positive payments, corresponding configurations reference reports must reflect
consensus.
first part intuitively simpler motivate. properties Bayesian updates
makes always exist n1 n2 P r[n1 |0] > P r[n1 |1]
P r[n2 |1] > P r[n2 |0] (e.g., configuration n1 agents report 1 becomes
probable negative experience, configuration n2 agents report
1 becomes likely positive experience). potentially infinite payments,
fact n1 n2 exist makes possible satisfy incentive compatible constraints;
therefore payment mechanism two positive payments (0, n1 ) (1, n2 )
incentive compatible. formal proof Appendix uses dual formulation show
thing.
second part proof shows expected payment minimized
scheme rewards consensus (i.e., n1 = 0 n2 = N 1). dual LP 3.1 reveals
expected payment agent proportional ratios P r[n1 |1]/P r[n1 |0]
P r[n2 |0]/P r[n2 |1]. ratios reflect relative change agents beliefs following
subjective private experience. e.g., P r[n1 |1]/P r[n1 |0] reflects relative change
belief n1 agents report 1, given positive opposed negative experience.
Likewise, P r[n2 |0]/P r[n2 |1] relative change belief n2 agents report
1, given negative opposed positive experience. following lemma shows
ratios (and therefore expected payment agent) minimized n1 = 0
n2 = N 1.
Lemma 3.1. Given set types , probability distributions P r[1|], prior belief
P r[n+1|1]
types P r[] number agents N , PP r[n|1]
r[n|0] < P r[n+1|0] n = 0 . . . N 1.
217

fiJurca & Faltings

full proof Lemma also provided Appendix A.



mechanisms5

payment
satisfy incentive compatibility constraints
similar property: must least two values reference reports, n1 < n2 ,
that:
(0, n1 ) > (1, n2 )
(1, n2 ) > (0, n2 );

requirement n1 < n2 direct consequence Lemma 3.1. (0, n1 )
(1, n2 ) scaled appropriately6 , rational agent prefers bet n1 observes
low quality, bet n2 observes high quality.
exactly property makes impossible design incentive-compatible
mechanism honest reporting unique NE one reference report
(Jurca & Faltings, 2005). n1 n2 constrained take values 0, respectively 1,
(0, 0) > (0, 1), (1, 1) > (1, 0), illustrated example Section 2. Therefore,
constant reporting strategies always reporting 0 1 also Nash Equilibria. Moreover,
since expected payment honest reporter linear combination (0, 0)
(1, 1), least one constant reporting equilibrium generates higher payoff
reporters honest equilibrium. Hence vulnerability payment mechanism
lying colluders.
Using several reference reports not, default, eliminate problem. result
Proposition 3.1 shows incentive-compatible constraints alone, also generate reward
schemes vulnerable conformity rating (i.e, everybody reports thing).
cases, nevertheless, payment schemes based several reference reports
constrained reward agreement, one could specify conditions, added
design problem generate collusion-resistant mechanisms.
next section. assume N > 2 agents system analyze
supplementary constraints added design problem order deter collusion.
consider several collusion scenarios, whenever possible present algorithm
outputs reward mechanism incentive-compatible collusion-resistant.

4. Collusion-resistant, Incentive-compatible Rewards
ideal reward mechanism deters coalition, matter big, even every colluder may use different strategy side-payments possible. mechanism, unfortunately, trivially impossible: given agents may collude use side-payments
subsidize agents might otherwise quit coalition, payment mechanism
doesnt leverage encourage honest reporting. Whatever payment scheme,
coalition adopt strategy maximizes total revenue, regardless
truth.
Positive results may obtained imposing restrictions possible lying
coalitions. first restriction agents collude. agents altruistic
nature report honestly moral social reasons. agents aware
5. One might wish, example, design mechanism minimizes expected
budget
paid N
PN
buyers. case,

objective
function


problem
LP
3.1
is:
B
=
P
r[n]
n (1, n 1) +
n=0

(N n) (0, n) , P r[n] prior probability n N buyers observe high quality;
6. (1, n1 ) (0, n2 ) typically 0

218

fiMechanisms Making Crowds Truthful

collusion opportunities, cannot contacted forming coalition. Social legal norms
collusion may furthermore create prejudices deter agents entering
coalition.
second restriction addresses complexity coordination among colluders.
Symmetric collusion strategies prescribe colluders reporting according
strategy. coordination symmetric strategies simple, requires one
anonymous access publicly available source information specifies colluding
strategy. Intuitively, role coordination device may played public blog
analyzes mechanisms informs potential colluders profitable
symmetric colluding strategy. Asymmetric collusion strategies, hand, require
significantly complex coordination. Since every colluder may use different reporting
strategy, coordination device must know identity colluder instructing
collusion strategy. often unfeasible, either colluders might want
reveal identity thus create trace misbehavior, identity
colluders cannot known actual reporting takes place.
third restriction addresses availability side-payments colluders (or
transferable utilities). Even rewards offered reputation mechanism
monetary, kind micro-payments would required among colluders
difficult expensive implement. Side-payments even less feasible rewards
offered reputation mechanism kind, currency control
reputation mechanism (e.g., Yahoo points Slashdot karma cannot transferred
even users wanted to). conversion subjective resources real money
afterwards transferred even difficult transfer itself.
One notable exception side-payments feasible strategic entity
controls number online identities, sybils (Cheng & Friedman, 2005). Here,
controlling agent interested maximizing overall revenue (i.e., sum revenues
obtained sybils), side-payments physically occur7 .
summarize, address collusion scenarios where:
agents become part lying coalition,
colluders coordinate using different strategies,
colluders make side-payments colluders.
remaining seven restricted collusion scenarios (see Table 1) addressing five. exclude settings utilities transferred coalition
restricted symmetric strategies. discussed previous paragraph, transferable
utilities mostly characteristic sybil attacks, strategic agent controls
several online identities. believe unreasonable assume strategic agent
cannot coordinate online identities controls asymmetric strategy profiles.
scenarios involving non-transferable utilities, collusion resistance emerge
consequence honest reporting (or attractive enough) equilibrium.
7. Whenever rewards non-monetary, overall utility controlling agent usually less
sum utilities sybils. Slashdot, example, ten users bad karma worth one
user good karma. Nevertheless, keep simplicity assumption additive utilities
controlling agent.

219

fiJurca & Faltings

agents
collude
agents
collude

Non-Transferable Utilities
symmetric
asymmetric
strategies
strategies
Section 4.1

Section 4.2

Section 4.3

Section 4.4

Transferable Utilities
symmetric
asymmetric
strategies
strategies
unreasonable impossible

assumption
prevent collusion
unreasonable
Section 4.5
assumption

Table 1: Different collusion scenarios.

agents may collude, honest reporting dominant equilibrium impossible.
Therefore, resort designing reward schemes honest reporting unique
Nash equilibrium, Pareto-optimal Nash equilibrium. fraction agents
may collude (non-colluders assumed report honestly) also consider designing
rewards make honest reporting dominant strategy colluders. following
subsections address one collusion scenario, describe possible methods designing
collusion-resistant, incentive-compatible reward mechanisms.
4.1 Full Coalitions Symmetric Strategies, Non-Transferable Utilities
assume agents (i) coordinate (before purchases
product) (pure) reporting strategy, (ii) cannot make side-payments
one another. simple form coordination colluders considerably simplifies
problem mechanism designer; supplementary constraint incentivecompatible payment mechanism ensure none pure symmetric strategy
profiles NE.
4.1.1 Unique Nash equilibrium.
set pure strategies finite (and contains 3 lying strategies) therefore exhaustively enumerate constraints prevent corresponding symmetric lying strategy
profiles NE:
spos (always reporting 1) NE rational agent would rather report 0 instead
1 given agents follow spos :
(0, N 1) > (1, N 1);

(6)

sneg (always reporting 0) NE rational agent would rather report 1
instead 0 given agents follow sneg ;
(1, 0) > (0, 0);

(7)

slie NE least one agent (either observing 1 0) would rather report
truth. Given agents always lie, N 1 n reference reports
positive whenever n high quality signals actually observed:
220

fiMechanisms Making Crowds Truthful

either

N
1
X



P r[n|0] (0, N 1 n) (1, N 1 n) > 0;

n=0



N
1
X



P r[n|1] (1, N 1 n) (0, N 1 n) > 0;

(8)

n=0

objective function (5), constraints (4), (6), (7) (8) define optimal
incentive-compatible payment mechanism also collusion-resistant sense explained beginning section (i.e., honest reporting unique pure-strategy
symmetric NE). compute payments, mechanism designer must solve two linear
optimization problems, one corresponding branch constraint (8).
Proposition 4.1. Collusion-resistant, incentive-compatible rewards require minimum N =
4 agents.
Proof. Proposition direct consequence Proposition 3.1, consider
supplementary constraints (7) (6) prevent high rewards unanimous agreement.
discussed proof Proposition 3.1, incentive compatible reward mechanism
requires two distinct configuration reference reports denoted n1 n2 that:
configuration n1 (i.e., n1 reference reports positive) agent reporting
0 rewarded agent reporting 1: e.g., (0, n1 ) > (1, n1 )
configuration n2 (i.e., n2 reference reports positive) agent reporting
1 rewarded agent reporting 0: e.g., (1, n2 ) > (0, n2 )
n1 < n2 (proven Lemma 3.1)
collusion positive report (all agents report 1) prevented n2 6= N 1,
otherwise (1, N 1) would greater (1, N 1), spos would NE.
Likewise, collusion negative report (all agents report 0) prevented
n1 6= 0 otherwise (0, 0) > (1, 0), sneg would NE. Unless N 4
constraints n1 6= 0; n1 < n2 n2 6= N 1 cannot simultaneously satisfied.
words, must exist rich enough set possible configuration reference reports

order satisfy collusion resistance incentive compatible constraints.
Taking plumber example described Section 2 N = 4 agents, conditional
distribution reference reports computed according Eq. (1):
0
1

P r[0|]
0.4179
0.0255

P r[1|]
0.2297
0.0389

P r[2|]
0.1168
0.2356

P r[3|]
0.2356
0.7

experience negative, two types (G B ) become almost equally likely
agents private belief. mix distribution reference reports induced
two types generates U-shaped distribution described first row
table. However, experience positive, good type becomes dominant,
distribution reference reports almost entirely dictated P r[n|G ] (the second row
table). optimal collusion-resistant, incentive-compatible payment mechanism
following:
221

fiJurca & Faltings

(, )
0
1

0
0


1
12.37
0

2
0
6.29

3

0

small positive value, guaranteed margin truth-telling = 1.
N > 4 payment mechanism looks rewards report one
agents agree submitted report. time, opposing consensus
rewarded small amount .
Payments exactly structure represent general solution design
problem context. Moreover, payments always exist:
Proposition 4.2. Given set types , probability distributions P r[1|], prior belief
types P r[], number agents N 4, following payment system honest
reporting unique symmetric NE:
(0, n) = 0, n 6= 1, N 1; (1, n) = 0, n 6= 0, N 2; (0, N 1) = (1, 0) =

P r[1|1]

P r[1|0]P r[1|1]P r[N 2|0]P r[N 2|1] condition
P r[1|0]
P r[N 2|0]P r[N 2|1]P r[1|0]P r[1|1] condition B
(0, 1) =


P r[N 2|1]+P r[N 2|0]
P r[1|0]P
otherwise
r[N 2|1]P r[N 2|0]P r[1|1]

P r[N 2|1]

P r[1|0]P r[1|1]P r[N 2|0]P r[N 2|1] condition
P r[N 2|0]
P r[N 2|0]P r[N
condition B
(1, N 2) =
2|1]P r[1|0]P r[1|1]


P r[1|1]+P r[1|0]
P r[1|0]P r[N 2|1]P r[N 2|0]P r[1|1] otherwise

P r[1|0]P r[1|1] > P r[N 2|0]P r[N 2|1]


P r[N 2|1] > P r[1|1]
A=
P r[N 2|1]2 P r[1|1]2 > P r[1|0]P r[1|1] P r[N 2|0]P r[N 2|1]


P r[N 2|0]P r[N 2|1] > P r[1|0]P r[1|1]
^


P r[1|0] > P r[N 2|0]
B=
P r[1|0]2 P r[N 2|0]2 > P r[N 2|0]P r[N 2|1] P r[1|0]P r[1|1]


^

Proof. straight-forward check small enough, payments described
proposition verify constraints (4), (6) (7). Moreover, payments minimize
expected payment honest reporter.

4.1.2 Pareto-optimal Nash equilibrium.
less strict notion collusion resistance requires honest reporting Pareto-optimal
NE. intuition stable (i.e., equilibrium) coalition necessarily make
colluders worse honest equilibrium. Assuming non-transferable utilities,
colluders benefit coalition cannot subsidize ones make loss,
hopefully, latter refuse join coalition first place. see towards
end subsection, payment mechanisms honesty Pareto-optimal NE
significantly cheaper payments designed unique honest NE.
payment mechanism honest reporting Pareto-optimal equilibrium
solves following optimization problem:
222

fiMechanisms Making Crowds Truthful

LP 4.1.
N
1
N
1
h

X
X
min E V (si , si ) = P r[1]
P r[n|1] (1, n) + P r[0]
P r[n|0] (0, n);
n=0

s.t.

N
1
X

n=0




P r[n|1] (1, n) (0, n) ;

n=0
N
1
X



P r[n|0] (0, n) (1, n) ;

n=0



(1, N 1) < E V (si , si ) ;


(0, 0) < E V (si , si ) ;



PN 1
_
n=0 P r[n|0] (0, N 1 n) (1, N 1 n) > 0
P
N 1


P r[n|1] (1, N
n=0 lie
1 n) (0, N 1 n) > 0
lie
E V (si , si ) < E V (si , si ) ;
(0, n), (1, n) 0; n = {0, 1, . . . , N 1};

first two constraints make honest reporting Nash equilibrium. next two
constraints prevent lying colluders spos sneg get higher rewards honest
equilibrium. constraints always easier satisfy constraints (6), (7)
prevent equilibria spos sneg . last constraint requires symmetric
profile every agent lies either NE, generates expected payoff
lower honest equilibrium. expected payoff agent reporting according
slie everybody else reports according slie is:
1


NX
lie
E V (slie
P r[n|0]P r[0] (1, N 1 n) + P r[n|1]P r[1] (0, N 1 n) ;
, si ) =
n=0

One remark LP 4.1 always optimal consider constraint
limits expected payoff colluder slie expected payoff obtained
honest equilibrium (i.e., third inequality disjunctive constraint LP 4.1).
Numerical simulations performed random problems show 40- 50% problems
collusion-resistant payments cheaper eliminating altogether symmetric lying
equilibrium slie : i.e., either first, second inequality last constraint
LP 4.1 easier satisfy third inequality.
either case, resulting optimal payments following structure:


(0, 0) = (1, N 1) = E V (si , si ) . values prevent lying coalitions
spos sneg Pareto-dominate honest reporting equilibrium;
(0, 1) > 0 (1, N 2) > 0 scaled satisfy incentive-compatibility
constraints, easiest three inequalities prevent coalition slie ;
payments 0.
plumber example Section 2 payments following:
(, )
0
1

0
1.30
0

1
4.52
0
223

2
0
1.26

3
0
1.30

fiJurca & Faltings

payments much smaller ones generated mechanism honest
reporting unique equilibrium. therefore observe fundamental tradeoff
strongness collusion resistance guarantees offered mechanism, price
mechanism pay enforce guarantees. collusion scenario described
section, Proposition 4.2 shows always possible design mechanism
honest reporting unique equilibrium. Nevertheless, mechanism honest
reporting Pareto-optimal, unique, significantly cheaper. see
tradeoff subsequent scenarios.
move next collusion scenario, let us briefly analyze influence N
(i.e., number reports available mechanism) properties mechanism.
Jurca Faltings (2006) show incentive-compatibility constraints easier
satisfy N becomes larger. Intuitively, property consequence structure
design problem: number constraints LP 3.1 independent N , however,
number variables increases N . Therefore, dual LP 3.1 constant number
variables, increasing number constraints. Moreover, constraints dual
also harder satisfy, means maximization objective dual
decrease N . Hence objective primal, i.e., cost mechanism, also
decreases N . Without going technical details, true collusionresistant mechanisms: larger values N , collusion-resistance constraints become
easier satisfy mechanism lower cost. subsequent sections maintain
property: information helps mechanism designer specify better targeted
rewards, turn decreases total cost mechanism.
4.2 Full Coalitions Asymmetric Strategies, Non-Transferable Utilities
next collusion scenario considering N agents coordinate
asymmetric collusion strategies, without able make side-payments one
another. N agents different reporting strategy, collusion
strategy profile denoted = (si ), = 1, . . . , N , si reporting strategy
agent i.
distinguish two cases, communication (and therefore coordination collusion strategy profile) happens agents perceive
quality signals product purchase. latter case, payment scheme
satisfy incentive-compatibility constraints. former case, honest reporting
never unique Nash equilibrium mechanism; however, honest reporting
Pareto-optimal Nash equilibrium.
Proposition 4.3. agents communicate coordinate reports perceiving
quality signals, strict incentive-compatible payment mechanisms exist.
Proof. Consider two settings, identical except observation agent i.
setting I, agent observes oi = 0, setting II, agent observes oi = 1; settings
agents observe n high quality signals. incentive-compatible mechanism requires
report 0 setting I, 1 setting II. Assume agents report truthfully;
communication phase (happening signals perceived) agent learns
settings reference reports contain n positive reports. incentive-compatible
payment mechanism requires that:
224

fiMechanisms Making Crowds Truthful

(0, n) > (1, n) - honest reporting strictly better setting ;
(1, n) > (0, n) - honest reporting strictly better setting II;
Clearly impossible.



previous proposition formalizes intuition truth-telling may exante Nash equilibrium. reference reports must unknown agent order
allow design incentive-compatible payments.
4.2.1 Unique Nash equilibrium.
communication takes place agents observe signals, incentivecompatible payments exist, always accept Nash equilibria agents lie:
Proposition 4.4. agents communicate coordinate reports perceiving
quality signals, payment mechanism unique honest reporting Nash equilibrium.
Proof. proof shows full coalition always find profile constant reporting
strategies, = (si ), = 1, . . . , N , si {sneg , spos } NE.
define family reporting strategy profiles s(n) = (si ) n N agents
always report 1, N n agents always report 0: i.e.,
si = spos , A1 ; si = sneg , A0 ;
|A1 | = n, |A2 | = N n;
A1 A0 = ; A1 A0 = {1, 2, . . . , N };

(9)

Assume payment mechanism defined (, ) accepts honest reporting
unique NE. seen Section 3 incentive-compatible constraints (4)
imply existence n1 < n2 {0, 1, . . . , N 1} (0, n1 ) > (1, n1 ),
(1, n2 ) > (0, n2 ).
non-transferable utilities, strategy profile s(n2 + 1) NE
one n2 + 1 agents report 1 would rather report 0:
(0, n2 ) > (1, n2 );

one N n2 1 agents report 0 would rather report 1:
(1, n2 + 1) > (0, n2 + 1);

first inequality cannot true choice n2 ; therefore, must
(1, n2 + 1) > (0, n2 + 1).
Similarly, s(n2 + 2) NE iff either (0, n2 + 1) > (1, n2 + 1) (impossible),
(1, n2 + 2) > (0, n2 + 2). Continuing argument find (1, N 1) > (0, N 1)
makes s(N ) (i.e., agents report 1) Nash equilibrium. Hence result
proposition.

Proposition 4.4 holds regardless number reports, N , available reputation
mechanism. proof shows incentive-compatible reward schemes property n {0, . . . , N 1}, either (1, n) > 0 (1, n + 1) < (0, n + 1),
225

fiJurca & Faltings

(1, N 1) > 0. first case, coalition adopt lying strategy n + 1
agents always report 1, N n1 agents always report 0. structure payments
makes coalition stable, agent finds profitable deviate coalition.
second case payment scheme vulnerable everybody always reporting 1.
4.2.2 Pareto-optimal Nash equilibrium.
lying equilibria always exist scenario, necessarily Pareto-dominate
honest reporting NE. Take example incentive-compatible payments solve
LP 3.1, additional constraints (0, 0) = 0 (1, N 1) = 0. stable
coalition form strategy profiles s(n2 + 1) s(n1 ), n2 + 1 (respectively n1 )
agents always report 1 others always report 0 regardless observation.
equilibrium, however, Pareto-dominate truthful one: agents report 0
get reward, whereas get rewarded honest equilibrium.
payment mechanism improved setting (0, n1 1) = (1, n2 +1) =
, small positive value. modification eliminates equilibria s(n2 + 1)
s(n1 ) instead introduces equilibria s(n2 +2) s(n1 1). equilibria
extremely unattractive (some agents get paid , others dont get paid all)
dominated honest equilibrium.
Proposition 4.5. Given set types , conditional probabilities P r[1|], prior
belief types P r[], N = 4 agents, following payment scheme honest reporting
Pareto-optimal Nash equilibrium:
(, )
0
1

0

0

1
x>0
0

2
0
y>0

3
0


values x depend probabilities P r[1|] P r[], small positive
value.
Proof. payments similar Proposition 4.2 except consensus
rewarded small amount instead discouraged. way mechanism
three NE: honest reporting, always reporting 1 always reporting 0. lying
equilibria, however, generate much lower revenues (assuming, course, small
enough); therefore, honest reporting Pareto-optimal equilibrium. proof
mechanism 3 NE based brute force: x taking values specified
Proposition 4.2, verify strategy profile NE. details presented
Appendix B.

general reward mechanisms based N > 4 reports, honest reporting become
Pareto-optimal NE considering lying strategy profiles, s, adding design
problem either following linear constraints:
V (si , si |oi ) < V (si , si |oi ) i, oi si ;




E V (si , si ) < E V (si , si ) i;

(10)

first constraint ensures strategy profile NE, consists disjunction 8 linear inequalities: reporting strategy si = (si (0), si (1)) S,
226

fiMechanisms Making Crowds Truthful

agent reporting according si incentive deviate either observing 0,
observing 1. four strategies one possible deviation
observed signal, hence 8 inequalities. second constraint ensures
Pareto-dominate honest equilibrium, consists similar disjunction 4
inequalities. Note two strategy profiles represent different permutations
set N reporting strategies generate constraints. Therefore,
N +3
different constraints imposing honesty Pareto-optimal NE, consisting
3
disjunction 12 linear inequations. resulting optimization problem
disjunctive linear program transformed mixed integer linear program
(Sherali & Shetty, 1980).
Unfortunately, complexity resulting optimization problem exponential
number N reporters considered payment mechanism. Since payment
mechanism depends current belief types , reputation mechanism might
required frequently update payments order reflect changing beliefs.
large values N clearly infeasible.
therefore consider special family payment mechanisms designed
efficiently make honest reporting Pareto-optimal NE. basic idea consider
payments similar Proposition 4.5, reward report
one reference reports agree. Consensus positive negative feedback also
rewarded small amount , payments zero:
(, )
0
1

0

0

1
x>0
0

2. . . N-3
0. . . 0
0. . . 0

N-2
0
y>0

N-1
0


Figure 1: Payment mechanism N > 4 agents.
payment mechanism depends 2 parameters, x must scaled
prevent lying strategy profile become NE Pareto-dominating honest
equilibrium. Note strategy profile one agent reports according
spos sneg become successful collusion strategy. least two agents always
report 1, none agents ever want report 0 (as (0, n) = 0 n 2).
Similarly least two agents always report 0, none agents ever want
report 1. Nevertheless, consensus equilibria yield small payoffs, significantly
lower payoff honest reporting equilibrium.
Following intuition proof Proposition 4.5, many remaining lying
strategy profiles cannot NE regardless values x y. Let us consider set
potential lying equilibrium strategy profiles:
= {(n0 sneg , n1 spos , n s, nl slie )| n0 + n1 + n + nl = N };

(11)

n0 {0, 1} agents always report 0, n1 {0, 1} agents always report 1, n
/ {N 1, N }
agents report honestly nl agents always lie. cardinality 4(N 1). profile
NE strategy si s, agent reporting according si
incentive deviate another reporting strategy given agents
keep reporting according si . Let oi Q2 signal observed agent i. report
227

fiJurca & Faltings

prescribed strategy si ri = si (oi ) Q2 , given small enough ignored,
expected payoff agent is:
P r[1|oi , si ] x ri = 0
P r[N 2|oi , si ] ri = 1

P r[1|oi , si ] P r[N 2|oi , si ] probabilities exactly 1, respectively
N 2 N 1 agents report positively given observation oi
strategy profile si .
deviation reporting 1 ri profitable observation oi if:
P r[1|oi , si ] x P r[N 2|oi , si ] > 0 ri = 0
P r[N 2|oi , si ] P r[1|oi , si ] x > 0 ri = 1

conditions make NE therefore expressed set 8
inequalities following structure:
aj x bj > 0; aj , bj > 0
ak x + bk > 0; ak , bk > 0
b

maxj ajj > mink abkk system inequations infeasible, positive
values x corresponding strategy profile NE. However,
b
maxj ajj < mink abkk , values x make NE, therefore,
design problem must specify constraint prevents Pareto-dominating
honest NE. corresponding constraint disjunction inequalities: 2
restricting x values make NE, 3 limit expected
payments colluders expected payment honest equilibrium.
Since 4(N 1) potential lying strategy profiles, optimization problem
defining x 4N 2 constraints: 2 linear incentive-compatible constraints
4(N 1) disjunctive linear constraints. transformation mixed integer
linear program involves adding 4(N 1) integer variables, worst case,
result exponential-time (in N ) complexity design problem.
Fortunately, eliminate strategy profiles analytically. turns
payment mechanism Figure 1 accept Nash Equilibrium
strategy profile lest one agent reports truthfully another agent reports
according slie :
Proposition 4.6. Let = (n0 sneg , n1 spos , n s, nl slie ) strategy profile
n0 agents always report 0, n1 agents always report 1, n agents report honestly nl
agents always lie. (n 6= 0 nl 6= 0) (n0 = 1 n1 = 1), cannot Nash equilibrium
payment mechanism described Figure 1.
Proof. reasons explained above, restricted number agents always
reporting 0 1 following cases: (i) n0 = 0, n1 = 0, (ii) n0 = 1, n1 = 0, (iii)
n0 = 0, n1 = 1 (iv) n0 = 1, n1 = 1. cases, consider strategy profiles
n 1 agents honest, remaining agents lie according slie . profile
show values x simultaneously satisfy equilibrium constraints
honest lying agent. Moreover, n0 = n1 = 1 show strategy
228

fiMechanisms Making Crowds Truthful

profiles agents honest agents lie, cannot Nash equilibria.
technical details proof given Appendix C.

remaining lying strategy profiles considered computing values x
following:

s1 = (N slie ) agents lie; constraints prevent equilibrium also
considered LP 4.1;


s2 = sneg , (N 1) slie one agent always reports 0, agents lie;


s3 = spos , (N 1) slie one agent always reports 1, agents lie;

solution x therefore found constant time.
4.3 Partial Coalitions Symmetric Strategies, Non-Transferable Utilities
section move attention full partial coalitions, agents
become part lying coalition. non-colluders assumed report honestly,
reports used mechanism deter partial lying coalition. Jurca
Faltings (2005) show trusted reports (reports trusted true) useful
preventing lying coalitions; nevertheless, important difference previous work,
here, honest reports cannot identified selectively used reputation
mechanism.
4.3.1 Unique Pareto-otpimal Nash equilibrium.
start Section 4.1, assuming symmetric collusion strategies sidepayments available among colluders. number colluders Ncol < N , remaining N = N Ncol report honestly. 3 symmetric pure lying strategies,
appropriate constraints ensure none becomes Nash equilibrium,
Pareto-dominates honest equilibrium.
Concretely, let Pr[|] probability distribution reports submitted noncolluders, Pr[n|oi ] probability n N agents report positively given
observation oi Q2 . Likewise, let Pr[|] probability distribution reports
submitted colluders: i.e., Pr[n|oi ] probability n Ncol 1
colluders report positively.
payment scheme makes honest reporting unique Nash equilibrium
colluders minimizes expected payment honest reporter solves following
optimization problem:
229

fiJurca & Faltings

min
s.t.



E V (si , si ) ;
N
X



P r[n|0] (0, n) (1, n) ;

n=0
N
X



P r[n|1] (1, n) (0, n) ;

n=0
N
_ X
oi Q2

n=0

N
_ X
oi Q2




Pr[n|oi ] (0, n) (1, n) < 0

n=0

N
_ X
oi Q2




Pr[n|oi ] (1, n + Ncol 1) (0, n + Ncol 1) < 0

Pr[n|oi ]

n=0

Ncol 1

X




Pr[Ncol 1 x|oi ] (1 oi , n + x) (oi , n + x) < 0

x=0

(0, n), (1, n) 0; n = {0, 1, . . . , N 1};

besides first two incentive-compatibility constraints, third, forth fifth
constraints encourage deviations symmetric collusion spos , sneg slie respectively. resulting optimization problem disjunctive linear program.
Finally, honest reporting made Pareto-optimal equilibrium modifying
optimization problem disjunctive constraints preventing equilibrium
lying symmetric strategies, also specify inequalities limiting payoff received
colluder expected payment honest reporter:
colluders spos gain less honest equilibrium:
N
X



Pr[n|oi ] (1, n + Ncol 1) < E V (si , si ) ;

n=0

colluders spos gain less honest equilibrium:
N
X



Pr[n|oi ] (0, n) < E V (si , si ) ;

n=0

colluders slie expect gain less honest equilibrium:
X
oi =0,1

P r[oi ]

N
X
n=0

Pr[n|oi ]

NX
col 1



Pr[Ncol 1 x|oi ] (1 oi , n + x) < E V (si , si ) ;

x=0

numerical simulation show optimization problem defined always
accepts solution. therefore conjecture always possible design incentivecompatible, collusion resistant rewards restrictions discussed section.
formal proof result remains future work.
4.4 Partial Coalitions Asymmetric Strategies, Non-Transferable Utilities
opposed Section 4.3, section consider practical scenario
colluders also employ asymmetric lying strategies: i.e., strategy profiles = (si ),
230

fiMechanisms Making Crowds Truthful

= 1, . . . Ncol . Side payments allowed among colluders, concerned
equilibrium Ncol agents become part coalition; remaining
N = N Ncol agents assumed report honestly.
4.4.1 Unique Pareto-optimal Nash equilibrium.
mechanism makes honest reporting Nash equilibrium follows guidelines derived Proposition 4.5 scenario agents collude: namely
reputation mechanism must consider N = 4 reports, mechanism rewards consensus
small positive payment , otherwise pays report three
four reports agree. proof Proposition 4.5 shows payment scheme accepts
three Nash equilibria:
agents report honestly,
agents always reports 0,
agents always report 1.
.
restriction made section least one become part
coalition (and thus reports truth) restricts set equilibria one,
agents report truthfully. Even remaining three agents collude, NE
payment mechanism described Proposition 4.5 report truth.
General payment mechanisms based N > 4 agents designed following
methodology Section 4.2: consider strategy profiles colluders
use, add constraints design problem (i) lying strategy profile
NE, (ii) NE lying strategy profile Pareto-dominates honest reporting NE.
Concretely, let SNcol set strategy profiles colluders use:
SNcol = {(n0 sneg , n1 spos , n s, nl slie )|n0 + n1 + n + nl = Ncol }

= (n0 sneg , n1 spos , n s, nl slie ) SNcol strategy profile n0
Ncol colluders always report 0, n1 colluders always report 1, n colluders report honestly,
nl colluders always lie. colluders report according strategy profile s, let
(s, s) = (n0 sneg , n1 spos , (n + N ) s, nl slie ) strategy profile used N
agents, N = N Ncol non-colluders report honestly.
Honest reporting unique Nash equilibrium colluders if:




V si , (si , s)|oi < V si , (si , s)|oi

(12)

colluder observation oi . Similarly, Pareto-dominate honest
reporting equilibrium if:
h



E V si , (si , s) < E V (si , si )

(13)

colluder i.
Lets compare constraints expressed similar constraints Section
4.2 described Eq. (10). note two important differences. First, inequalities apply
231

fiJurca & Faltings

Ncol colluders, entire set agents, strategy profile
NE N agents, might still Nash equilibrium colluders. Take,
example, case colluders lie: i.e., = (Ncol slie ). strategy profile
agents therefore (s, s) = (Ncol slie , N s). may well possible that:
lying colluder finds optimal report according slie given N agents report
honestly Ncol 1 agents report lies;
honest reporter would rather file negative report positive experience, given
N 1 agents report honestly Ncol agents lie.
(s, s) NE considering agents, equilibrium subset
colluders. Similarly, possible colluders gain better (s, s) honest reporters
gain less, (s, s) Pareto-dominates honest equilibrium colluders,
agents. constraints (12) (13) therefore stricter counterparts
(10), non-colluders assumed unconditionally report truth without taking
account actions lying coalition.
Second, separately consider constraint (12), honest reporting
enforced unique NE colluders, disjunction constraints (12) (13),
honest reporting enforced Pareto-optimal NE colluders. presence
honest reports makes possible design payment mechanisms honesty
unique NE, alternative available assumptions Section 4.2.
cases, constraints preventing lying equilibria (or preventing lying equilibria
dominating honest equilibrium) represented disjunction linear inequalities,
consequently, conjunction mixed integer linear constraints. resulting design
problem MILP, and, discussed Section 4.2, worst-time complexity grows
exponentially number N agents.
Section 4.2, use payment mechanism Figure 1 reduce
complexity design problem honest reporting unique Pareto-optimal
NE. Proposition 4.6, know that:
NE, one colluder reports according sneg spos ;
honest reporter liar cannot regard strategies optimal given
one agents reports according sneg spos
Therefore, remaining colluding strategy profiles must considered designing payments x following:
(Ncol slie ) colluders lie;
(sneg , (Ncol 1) slie ) one colluder always reports 0 others always lie;
(spos , (Ncol 1) slie ) one colluder always reports 1 others always lie;

232

fiMechanisms Making Crowds Truthful

4.4.2 Stronger equilibrium notion.
evaluating reward mechanisms accepts unique Pareto-optimal honest
reporting Nash equilibrium, let us note subgame restricted strategies
Ncol colluders accepts stronger equilibrium concept Nash equilibrium.
fraction colluders small enough, available honest reports make
colluder incentive report honestly matter colluders
reporting. abuse notation slightly call equilibrium dominant equilibrium.
Nevertheless, honest reporting dominant strategy given N honest reporters.
Pr[|] describes probability distribution N honest reports, c
number positive reports submitted Ncol 1 colluders, payments (, )
make honest reporting dominant strategy, minimize payment honest
reporter, defined following optimization problem:
LP 4.2.
N
1
N
1
X
X


min E V (si , si ) = P r[1]
P r[n|1] (1, n) + P r[0]
P r[n|0] (0, n);
n=0

s.t.

N
X

n=0



Pr[n|0] (0, n + c) (1, n + c) ;

n=0
N
X



Pr[n|1] (1, n + c) (0, n + c) ;

n=0

c {0, . . . Ncol 1},
(0, n), (1, n) 0; n = {0, 1, . . . , N 1};

remaining question large may colluding fraction be, collusionresistant, incentive-compatible mechanisms exist.

Proposition 4.7. half agents collude, (i.e., Ncol > N/2),
incentive-compatible payment mechanism make truth-telling dominant strategy
colluders.

Proof. intuition behind proof following: Ncol > N/2, Ncol 1
colluders submit least many reports remaining N Ncol honest reporters.
Therefore, sequence honest reports, corrected carefully chosen sequence
colluding reports, lying profitable.
Formally, let us extract system inequalities defined LP 4.2, subset
corresponding c = {0, . . . , N }. subset exists since N < Ncol 1. Let us form
233

fiJurca & Faltings

following optimization problem:
min

P r[1]

N
1
X

P r[n|1] (1, n) + P r[0]

n=0
N
X

s.t.

N
1
X

P r[n|0] (0, n);

n=0



Pr[n|0] (0, n + c) (1, n + c) ; c = 0 . . . N

n=0
N
X



Pr[n|1] (1, n + c) (0, n + c) ; c = 0 . . . N

n=0

(0, n), (1, n) 0; n = {0, 1, . . . , N 1};

Let yc0 yc1 dual variables corresponding constraints colluding
agents report c positive signals, agent observes 0, respectively 1. dual problem
becomes:
Ncol 1

max



X

(yc0 + yc1 );

c=0

s.t.

n
X
c=0
n
X

Pr[n c|0]yc0 Pr[n c|1]yc1 P r[0]P r[n|0]; n = 0 . . . N
Pr[n c|1]yc1 Pr[n c|0]yc0 P r[1]P r[n|1]; n = 0 . . . N

c=0
N
X

Pr[N + 1 c|0]yc0 Pr[N + 1 c|1]yc1 P r[0]P r[N + n + 1|0]; n = 0 . . . N

c=n+1
N
X

Pr[N + 1 c|1]yc1 Pr[N + 1 c|0]yc0 P r[1]P r[N + n + 1|1]; n = 0 . . . N

c=n+1

One easily verify dual problem accepts solutions:
yc1 = Pr[c|0] const,
yc0 = Pr[c|1] const;

(14)

positive constants. dual problem therefore unbounded, makes
primal infeasible.

bound Proposition 4.7 also tight. Consider numerical example Section 2, assume reputation mechanism N = 4 reports. following payments
resistant collusion Ncol = 2 agents:
(, )
0
1

0
1.575
0

1
3.575
0

2
0
2.203

3
0
0.943

example, Alice observes 1, reporting 1 better reporting 0 report
colluder:
Pr[0|1] (1, 0) + Pr[1|1] (1, 1) + Pr[2|1] (1, 2) = 1.715;
Pr[0|1] (0, 0) + Pr[1|1] (0, 1) + Pr[2|1] (0, 2) = 0.715;
Pr[0|1] (1, 1) + Pr[1|1] (1, 2) + Pr[2|1] (1, 3) = 1.138;
Pr[0|1] (0, 1) + Pr[1|1] (0, 2) + Pr[2|1] (0, 3) = 0.138;
234

fiMechanisms Making Crowds Truthful

Pr[0|1] = 0.0385, Pr[1|1] = 0.1830 Pr[2|1] = 0.7785 probabilities
0, 1, 2 N = 2 non-colluders report positively, given Alice observed high
quality.
general case, design problem 2Ncol different constraints, therefore,
expect budget required reputation mechanism grow Ncol .
resort numerical simulations study average cost incentive-compatible,
collusion-resistant reputation mechanism fraction colluders increases. randomly
generated 5000 problems follows:
set possible types randomly chosen 2 20;
type, , probability, P r[1|], buyers observe high quality
randomly chosen 0 1;
consider reward mechanisms 5, 10, 15, 20 25 agents.
problem every number agents varied number colluders 1
N/2. Figure 2 plots average normalized cost collusion-resistant mechanism
function colluding fraction, Ncol /N . One see collusion resistance comes
almost free long less one third population colludes. bound
cost increases exponentially, makes mechanisms impractical.

average normalized cost

15
N=10
N=15
N=20
N=25
10

5

0
0

0.1

0.2
0.3
colluding fraction

0.4

0.5

Figure 2: average cost mechanism increase colluding fraction. cost
normalized cost corresponding incentive-compatible mechanism
collusion-resistant.

Figure 2 seems contradict observation made Section 4.1 mechanism
lower cost higher values N . However, costs plotted
235

fiJurca & Faltings

horizontal axis describing absolute (not relative) number colluders, order
lines reversed: red line showing cost mechanism smallest number
reference reports (N = 10) highest values.
also used numerical simulations investigate tightness bound set
Proposition 4.7. Table 2 presents distribution maximum collusion threshold
randomly generated problems. 95% problems actually able
max = bN/2c
compute payment mechanisms resist maximum coalition size Ncol
described Proposition 4.7. settings, however, mechanism
vulnerable coalition fractions significantly smaller one half. sufficient
conditions characterize settings accept robust mechanisms exactly one
half colluders subject future research.

max = bN/2c.
Table 2: Distribution maximum coalition bound. Ncol
max = 2
N = 5, Ncol
max = 5
N = 10, Ncol
max = 7
N = 15, Ncol
max = 10
N = 20, Ncol
max = 12
N = 25, Ncol

Distribution max coalition size (in %)
max , N max 1, . . . , 1]
[Ncol
col
[99.98, 0.02]
[99.5, 0.36, 0.1, 0.04, 0]
[98.88, 0.54, 0.38, 0.08, 0.1, 0.02, 0]
[97.1, 0.86, 0.78, 0.56, 0.34, 0.2, 0.1, 0.04, 0.02, 0]
[96.3, 0.98, 0.76, 0.58, 0.48, 0.4, 0.24, 0.1, 0.1, 0.04, 0.02, 0]

also compared performance reward mechanisms employ different
equilibrium concepts. Figure 3 compares average normalized cost collusion-resistant
payment mechanism honest reporting is: (i) dominant strategy, (ii) unique
NE, (iii) Pareto-optimal NE. plots generated solving 100 randomly
generated problems, N = 10 N = 15 agents. Computing payment mechanism
satisfies constraints (12) (13) requires significantly time, hence
lower number generated problems. Moreover, capabilities solver exceeded
payments using 15 agents. Nevertheless, loss computational efficiency
clearly rewarded lower cost mechanism, coverage greater coalitions.
4.5 Partial Coalitions Asymmetric Strategies, Transferable Utilities
last scenario assume one strategic agent controls number fake online
identities, sybils. agents perspective, individual revenues obtained
sybil irrelevant; objective agent maximize cumulated revenue obtained
sybils.
fact utilities transferable makes problem mechanism designer
significantly harder. previous scenarios, constraints made incentivecompatible mechanism collusion-resistant ensured lying coalitions either unstable
unprofitable. However, transferable utilities allow colluders subsidize others,
non-equilibrium colluding strategies still exist. Therefore, necessary (and
sufficient) condition collusion resistance context requires cumulated
revenue coalition maximized reporting truth.
236

fiMechanisms Making Crowds Truthful

2.5

2.2
Dominant EQ
Unique NEQ
Paretooptimal NEQ

2

average normalized cost

average normalized cost

2

Dominant EQ
Unique NEQ
Paretooptimal NEQ
1.5

1

1.8
1.6
1.4
1.2
1
0.8
0.6

0.5
1

2

3

4
5
6
number colluders

7

8

9

2

(a) N=10 agents

4

6
8
10
number colluders

12

14

(b) N=15 agents

Figure 3: Average normalized cost collusion-resistant payment mechanism. Different
equilibrium concepts.

Another difference settings Sections 4.2 4.4 colluders coordinate
reporting strategy observing quality signals. assumption supported
interpretation one strategic entity controls several fake online identities.
Concretely, looking payment mechanism following property: whenever Ncol colluding agents observe c high quality signals, cumulated revenue maximized reporting c positive reports. underlying assumption non-colluders
(the N = N Ncol agents) reporting honestly. revenue coalition
reports r (out Ncol ) computed follows. r colluders report positively
rewarded (1, r 1 + n), Ncol r colluders report negatively rewarded
(0, r + n); n number positive reports submitted (honest) non-colluders.
expected revenue coalition therefore:
V (r|c) =

N
X



Pr[n|c] r (1, r 1 + n) + (Ncol r) (0, r + n) ;

(15)

n=0

Pr[n|c] probability n N honest agents report positively, given
c Ncol colluders observed high quality signals.
Honest reporting best strategy coalition, c {0, . . . Ncol },
arg maxr V (r|c) = c:
N
X
n=0


Pr[n|c] c (1, c 1 + n) + (Ncol c) (0, c + n) r (1, r 1 + n)


(16)

(Ncol r) (0, r + n) ; r 6= c {0, . . . Ncol }

cheapest incentive-compatible, collusion-resistant payment mechanism minimizes
objective function (5) linear constraints (16):
237

fiJurca & Faltings

LP 4.3.
N
1
N
1
h

X
X
min E V (si , si ) = P r[1]
P r[n|1] (1, n) + P r[0]
P r[n|0] (0, n);
n=0

s.t.

n=0

(16) true, c, r {0, . . . Ncol }, c 6= r
(0, n), (1, n) 0; n = {0, 1, . . . , N 1};

example described Section 2, assuming Alice controls Ncol = 3 different
online identities may submit feedback Bob, following payments based
N = 6 reports deter Alice lying:
(, )
0
1

0
20.85
45.54

1
0
28.78

2
0
0

3
0
0

4
4.40
0

5
9.98
4.31

Even Alice controlled Ncol = 5 N = 6 reports, still find payments
make honest reporting rational. payments, however, significantly higher:
(, )
0
1

0
3455
1530

1
0
5569

2
1378
4674

3
615
3736

4
0
0

5
1125
2585

turns general case, one honest report enough allow design
incentive-compatible payments also deter sybil attacks size N 1. example
payments presented proposition below:
Proposition 4.8. Given set types , conditional probabilities P r[1|], prior
belief types P r[] number N reports, following payments encourage honest
reporting strategic agent controls N 1 different reports:
SR(0, 0)
SR(1, 0)
SR(1, N 1)
; (0, 1) =
; (1, N ) =
;
Ncol
Ncol
Ncol
(x + 1)SR(1, x) xSR(0, x + 1)
(0, x + 1) =
; x = 1...N 1
Ncol
(N 1 x)SR(0, x + 1) (N 2 x)SR(1, x)
(1, x) =
; x = 1...N 1
Ncol
(0, 0) =

SR(i, j), {0, 1}, j = {0, . . . , N 1} proper scoring rule: e.g., SR(i, j) =
log(Pr[i|j]).
Proof. expected payment agent controls N 1 different identities, observes
c N 1 positive signals reports r positive reports reputation mechanism
computed Eq. (15):


V (r|c) = P r[0|c] r (1, r 1) + (N 1 r) (0, r) +


P r[1|c] r (1, r) + (N 1 r) (0, r + 1)
= . . . = P r[0|c]SR(0, r) + P r[1|c]SR(1, r);
238

fiMechanisms Making Crowds Truthful

definition proper scoring rule strictly maximized r = c: i.e.
V (c|c) V (r|c) > 0 r 6= c. scaling scoring rule appropriately (i.e., multiplication addition constant), honest reporting made better lying
least margin .

Proposition 4.8 grantees existence incentive-compatible collusion resistant rewards
one report controlled strategic agent. However, seen
example Section 2, payments expensive, hence unpractical.
therefore used numerical simulations evaluate marginal cost increasing collusion
resistance, increase number colluders (i.e., reports controlled agent).
Section 4.4, generated 5000 random problems computed optimal payments
N = 5,10,15,20 25 reports. case, gradually increased coalition size
(i.e., Ncol ) 1 N 1.
Figure 4 plots average normalized cost collusion-resistant mechanism
function coalition fraction. cost grows exponentially coalition fraction
covers one half entire population. behavior also observed Section 4.4 evaluated cost incentive-compatible, collusion-resistant mechanisms
absence transferable utilities (Figure 2). However, payments required
assumption non-transferable utilities significantly smaller payments derived
section settings transferable utilities.

average normalized cost

15
N=10
N=15
N=20
N=25
10

5

0
0

0.2

0.4
0.6
colluding fraction

0.8

1

Figure 4: average cost mechanism increase colluding fraction (setting
transferable utilities). cost normalized cost corresponding
incentive-compatible mechanism collusion resistant.

239

fiJurca & Faltings

mechanisms defined present previous sections assume mechanism designer knows total number colluders. Formally, mechanism designed
robust collusion Ncol agents necessarily robust coalition
sizes smaller Ncol . Take example mechanism defined Proposition 4.8:
strategic agent controls less N 1 identities observes less N 1 quality
signals, therefore less precise beliefs signals received (and reported)
non-colluders. noisy beliefs make cases, smaller coalition
may regard certain lying strategy profitable honest reporting. one
caveat, however: colluders know lying strategy provably inefficient
access information (i.e., remaining N 1 reports). therefore believe practical purposes, mechanism designed robust Ncol
colluders effectively deter coalitions smaller Ncol .

5. Related Work
One interesting alternative payment schemes encourage honest feedback develop
mechanisms make best interest providers truthfully reveal hidden
quality attributes. truthful declaration quality eliminates need reputation
mechanisms significantly reduces cost trust management.
Braynov Sandholm (2002), example, consider exchanges goods money
prove market agents trusted degree deserve trusted
equally efficient market complete trustworthiness. scaling amount
traded product, authors prove possible make rational sellers
truthfully declare trustworthiness. However, assumptions made trading
environment (i.e. form cost function selling price supposed
smaller marginal cost) common electronic markets.
Another interesting work addresses trustworthiness reputation information
Goodwill Hunting mechanism Dellarocas (2002). mechanism works eBay-like
markets provides way make sellers indifferent lying truthfully declaring
quality good offered sale. particularity work goods
advertised buyers reputation mechanism, modify asking
price initially set seller. reputation mechanism thus compensates momentary
gains losses made seller misstating quality good, creates
equilibrium sellers find rational truthfully announce quality. major
advantage mechanism works even sellers offer various goods
different values.
Mechanisms encouraging honest reporting also present number commercial
applications. famous perhaps ESP Game (von Ahn & Dabbish, 2004),
designed encourage human users label web images. game8 pairs two users
random, shows image. player must individually write tags
image, without able see tags written partner. soon two
player write tag, gain points pass next picture. goal
get many points possible fixed amount time. Intuitively, game
simple strategy: players must write many correct tags possible, since image
8. http://www.espgame.org

240

fiMechanisms Making Crowds Truthful

see synchronization device allows reach agreement tag.
game successful, authors claim way, images web
tagged several months.
incentive mechanism behind ESP game has, however, several problems. First,
vulnerable cheating strategies group players agree reach agreement
simple tag like the. strategy could posted popular blog
exposed rapidly ESP players. simple collusion strategies give colluders
significant competitive advantage, detriment game designers collect
garbage tags. problem partly addressed taboo lists containing
confirmed tags already submitted picture.
second problem rewards equal possible tags. Picasso,
players match tag painting equally rewarded players correctly
identify painting Picasso. gives incentives players concentrate
simplest possible tags like person, man, woman, etc, without spending effort
provide informative tags. problem partly corrected Google
Image Labeler9 , franchise ESP Game, rewards players inversely proportional
frequency tag agree on. However, exact algorithm computing
rewards public. Yahoo! also known use version ESP Game tag
collection images.
Another example commercial application using payment mechanisms encourage
honest reporting Amazons Mechanical Turk10 . role system provide
marketplace human users solve tasks difficult machines,
easy people (i.e., short translations, tagging, face recognition, natural language search,
etc). Task owners pay workers answering tasks, also specify
payment rules: e.g., worker gets paid (or receives bonus) answer confirmed
different worker solving task.
number feedback forums reward raters independently based impact
reviews users. ePinion.com, example, professional reviewers get
paid depending votes expressed normal users, purchases made
reading reviews. Another example startup Friend2Friend.com11 allows users
gain commissions recommending products friends.
Central results paper principle automated mechanism design
(AMD). mechanism created automatically (using optimization algorithms)
specific problem instance, given specific information available mechanism designer. idea important advantages since (a) used address classes
problems known manually designed mechanisms, (b) circumvent impossibility results restricting mechanism one particular setting, (c)
generate better mechanisms capitalizing specific information available
present setting, (d) shifts effort mechanism design machine.
Since first introduced Conitzer Sandholm (2002), AMD used generate
several impressive results. Conitzer Sandholm (2003a) (a) reinvented Mayerson
auction maximizes sellers expected revenue single-object auction, (b) created
9. http://images.google.com/imagelabeler/
10. http://www.mturk.com/mturk/welcome
11. http://www.friend2friend.com/

241

fiJurca & Faltings

expected revenue maximizing combinatorial auctions, (c) created optimal mechanisms
public good problem. Guo Conitzer (2007) use AMD optimally redistribute
payments generated VCG mechanism, Conitzer Sandholm (2007) incrementally
design incentive compatible mechanisms, Hajiaghayi, Kleinberg, Sandholm (2007)
focus AMD online settings. Conitzer Sandholm (2003b) show AMD
potentially exponentially faster settings structured preferences allow
concise representation input. Conitzer Sandholm (2004) describe efficient
algorithm AMD mechanism deterministic, allow payments
one type-reporting agent. AMD also used design multi-stage mechanisms
reduce burden information elicitation querying agents relevant
information (Sandholm, Conitzer, & Boutilier, 2007). results paper add
already long list results obtained AMD.
results Section 4 mostly related literature implementation theory
incentive contracts principle-(multi)agent settings. main goal implementation theory characterize space social choice rules implementable
mechanism given game-theoretic equilibrium concept. complete information settings, well established results characterize necessary sufficient conditions social
choice rule (SCR) implementable dominant strategy Nash equilibrium.
example, SCRs implemented dominant strategies strategy-proof
(Gibbard, 1973), SCRs Nash-implemented must satisfy property
monotonicity veto power (Maskin, 1999). Unfortunately, SCRs practical interest satisfy monotonicity requirement. Fortunately, non-monotonic SCRs
implemented undominated Nash equilibria (Palfrey & Srivastava, 1991), subgame
perfect equilibria multi-stage mechanisms. Another relaxation extends set
implementable SCRs consider virtual implementation, socially optimal outcome required occur probability close one (Matsushima, 1988; Abreu &
Sen, 1991).
environments incomplete information agents private information
shared agents. truthful revelation private information
ensured social choice rules Bayesian incentive-compatible. Moreover, Bayesian
monotonicity condition necessary Bayesian implementation (Jackson, 1991). Moore
Repullo (2005) characterize SCRs virtually Bayesian implemented pure
strategies, derive necessary sufficient conditions incentive compatibility
virtual monotonicity.
However, applying implementation theory feedback reporting setting (an
environment incomplete information) provides nothing constraints
payment function honest reporting unique Bayesian Nash equilibrium.
implementation theory terms, set possible world states consists combinations
N privately perceived quality signal (one signal agent). outcome space
contains possible sets N feedback reports possible combinations N positive
payments made N agents. desirable SCR contains social choice functions
map possible states world (i.e., set privately perceived signals)
outcomes reported feedback correspond privately perceived signals).
Implementation theory tells SCR must incentive compatible (i.e., social
choice functions prescribe outcomes payments agents make truthfully
242

fiMechanisms Making Crowds Truthful

reveal private information) Bayesian monotone (i.e., social choice functions
prescribe outcomes payments received agents make honest reporting
unique equilibrium). results Section 4 translate requirements practical
constraints allow computation payment functions (and therefore social choice
functions) Bayesian Nash implementable.
number papers discuss incentive contracts principal offer several
agents whose effort levels private. reward received agent depends
output observed principal, declarations agents. Holmstrom
(1982), (1988), Li Balachandran (2000) show efficient contracts exist
also incentive-compatible collusion-proof. feedback reporting problem
similar, differs one major aspect: reputation mechanism designer (i.e., principal)
observe direct signal correlated reporters (i.e., agents) private
information.

6. Discussion Future Work
Throughout paper considered pure reporting strategies. Extending results
mixed -strategy equilibria remains open question, poses non-trivial computational
problems: constraints required prevent mixed equilibria longer linear,
significantly increases complexity design problem.
paper limit investigation binary settings quality signal
observed agents feedback reported reputation mechanism either 0
1. extension n-ary feedback conceptually straight-forward (see Appendix
D), resulting mechanism design problem becomes exponentially complex
increase number possible feedback values. remains challenge future work
design efficient algorithms able quickly compute incentive-compatible
collusion-resistant reward mechanisms non-binary feedback.
Another challenge relax requirement common prior information. model
introduce Section 2 assumes agents share prior beliefs regarding
probability positive negative feedback. Jurca Faltings (2007b) show incentive
compatible reward schemes still designed agents small amounts private
information. However, methodology cannot easily extended also address
collusion.
Yet another direction future research design payment mechanisms
resistant complex collusion scenarios, where, example, several strategic
agents, controlling several fake identities, try manipulate reporting mechanism.
believe, however, many practical scenarios techniques
presented paper successfully used ensure safety collusion. One
example monitoring service quality. Jurca, Binder, Faltings (2007) describe
framework monitoring quality web service based client feedback. idea
estimate quality delivered service provider directly reports
clients. approach much cheaper precise compared traditional
monitoring devices proxy analyze communication client
server. Nevertheless, mechanism must provide incentives honest reporting
also discourage lying coalitions. feedback settings often binary, specifies
243

fiJurca & Faltings

whether certain service level agreements (SLA) met provider.
quality provider defined capacity fulfill SLA, usually becomes
public information. quality time defines priors designing mechanism
time + 1, quite natural majority clients consider priors
public knowledge.
Many tasks Amazons Mechanical Turk also fit model used paper.
example, certain tasks ask human raters specify whether two descriptions refer
item not. tasks, raters must vote thumbs piece news,
must tag photographs contain certain visual clue, human face. answer
tasks modeled binary feedback signal (e.g., items
not, positive negative vote, photograph contains human face) answers
different users may considered conditionally independent given description
task. Moreover, conditional probabilities may quite often assumed common
knowledge: example, assumption 98% raters correctly identify
human face decent quality photograph, natural, also likely
belief internet-savvy humans.
generally, believe techniques useful providing incentives human
raters label training data supervised machine learning algorithms. Many practical
classifiers binary (e.g., photograph contains certain feature, word misspelled not) composed binary classifiers (e.g., recognizing hand-written
digit). framework similar Mechanical Turk harvest power crowds
produce extensive training sets different algorithms.
Another potential application collaborative question answering forums like
Yahoo! Answers. forums users may post questions remain open predefined
period time. period, users write new answers, vote existing
answers. voting mechanism essential differentiating good bad answers,
proper incentives may ensure higher participation accurate results.
forums especially vulnerable collusion, since author providing best answer
often rewarded forum points, public recognition, benefits. biggest
challenge contexts obtain accurate estimates different probabilities
enter design problem. example, prior probability high quality answer
estimated history site. hand, conditional probability
user find given answer high quality complicated estimate. example,
mechanism designer might use natural language processing algorithm figure
degree matching question answer. designer could also search
documents contain keywords related question answer, analyze
documents refine matching degree answer given question. General
user statistics also factored estimate likelihood random user find
given answer useful. Although estimates inevitably noisy, might work
well enough average user.
results fully extended feedback sets arbitrary size, techniques
paper relevant feedback reporting scenarios. mention one
supplementary examples, ESP game, every photo tag assumed drawn
finite set concepts, conditional probabilities seeing certain tag
244

fiMechanisms Making Crowds Truthful

Non-Transferable Utilities
symmetric
asymmetric
strategies
strategies

agents
collude
agents
collude

-unique honest NE;
-Pareto-optimal
honest NE
-unique honest NE;
-Pareto-optimal
honest NE

-Pareto-optimal
honest NE
-unique honest NE;
-Pareto-optimal
honest NE;
-sometimes honest
dominant strategy
);
(not Ncol N
2

Transferable Utilities
symmetric
asymmetric
strategies
strategies
unreasonable
assumption

impossible prevent collusion

unreasonable
assumption

-(sybil
attack),

coalition
maximizes revenue reporting
honestly;

Table 3: Summary results.

estimated using word frequencies different languages, image recognition techniques,
historical data regarding distribution tags similar photos.

7. Conclusion
online feedback reputation become increasingly important sources information,
explicit measures must guarantee honest reporting best interest participants. Previous work shows possible construct payment mechanisms
make honest reporting Nash equilibrium, agents expect get rewarded
truthful report lie. Unfortunately, mechanisms also equilibria
reporters lie. creates collusion opportunities, since several agents coordinate
lies order improve revenues.
paper addressed design incentive-compatible payments also
resistant collusion. consider different collusion scenarios (i)
agents collude, (ii) colluders coordinate symmetric asymmetric strategy
profiles, (iii) colluders transfer payments other. Table 3 summarizes
results obtained scenario.
Section 4.1 assume agents may collude cannot make side-payments
other. showed incentive-compatible payments efficiently constructed
honest reporting unique pure strategy symmetric NE, Pareto-optimal
pure symmetric NE. Section 4.2 keep assumptions, investigate asymmetric collusion strategies. find incentive-compatible payment mechanism also
accepts asymmetric lying equilibria. Nevertheless, payment mechanisms
honest reporting Pareto-optimal NE.
Sections 4.3 4.4 assume fraction agents may collude, noncolluders report honestly. colluders coordinate symmetric strategy profiles
cannot make side-payments other, payments always exist honest
reporting made unique NE (with lower payments) Pareto-optimal NE.
colluders coordinate asymmetric strategies (Section 4.4), payments still
devised make honest reporting unique Pareto-optimal NE. less one half
population collude, payments sometimes devised make honest
reporting dominant strategy. Numerical simulations, however, show payments
245

fiJurca & Faltings

required deter coalition fractions greater one third become exponentially expensive.
Finally, Section 4.5 describes incentive-compatible payments resistant sybil
attacks: i.e., strategic agents creates several fake identities order manipulate
payment mechanism. designer ensure set reports submitted
coalition reflects aggregated experience coalitions. Individual colluders
necessarily report truth, overall, reputation mechanism obtains correct
information.

Acknowledgments
authors wish thank anonymous reviewers helpful comments suggestions.

Appendix A. Proof Proposition 3.1
solving LP 3.1, let us write corresponding dual problem:
max y0 + y1 ;
s.t.
P r[n|0]y0 P r[n|1]y1 P r[0]P r[n|0]
P r[n|1]y1 P r[n|0]y0 P r[1]P r[n|1]
n {0, . . . , N 1};

y0 (respectively y1 ) dual variable corresponding constraint
agent observes 0 (respectively 1). dividing first set constraints P r[n|0]
second set constraints P r[n|1], have:
y0 y1 P r[n|1]/P r[n|0] P r[0], n {0, . . . , N 1};
y1 y0 P r[n|0]/P r[n|1] P r[1], n {0, . . . , N 1};

Clearly, among 2(N 1) constraints dual problem, two active, correP r[n|0]
sponding to: n1 = arg minn PP r[n|1]
r[n|0] , n2 = arg minn P r[n|1] . follows two
variables LP 3.1 non-zero values (i.e., (0, n1 ) 6= 0 (1, n2 ) 6= 0),
satisfy linear equations:
P r[n1 |0] (0, n1 ) P r[n2 |0] (1, n2 ) = ;
P r[n1 |1] (0, n1 ) + P r[n2 |1] (1, n2 ) = ;

remaining part proof show n1 = 0 n2 = N 1. that,
P r[n+1|1]
prove PP r[n|1]
r[n|0] < P r[n+1|0] n = 0, 1, . . . , N 2.
246

fiMechanisms Making Crowds Truthful

P r[n|1]P r[n + 1|0] P r[n|0]P r[n + 1|1] =

X

P r[]



X

P r[1|]
P r[0|]
P r[n|]
P r[]
P r[n + 1|]
P r[1]
P r[0]


X

X

P r[0|]
P r[1|]
P r[]
P r[]
P r[n|]
P r[n + 1|]
P
r[0]
P
r[1]



=

X

P r[]



X

X
P r[1|]
P r[0|]
(N 1 n)P r[1|]
P r[n|]
P r[]
P r[n|]

P r[1]
P r[0]
(n + 1)P r[0|]


X
P r[0|]
P r[1|]
(N 1 n)P r[1|]
P r[]
P r[n|]
P r[n|]
P r[0]
P r[1]
(n + 1)P r[0|]





X
2
(N 1 n)
=
P r[]P r[1|]P r[n|]
(n + 1)P r[1]P r[0]

!
X
X

P r[1|]2
P r[]P r[0|]P r[n|]
P r[]
P r[n|]
< 0;
P r[0|]


P r[]

p
theCauchy-Schwartz inequality applied vectors ( P r[]P r[0|]P r[n|])
P r[1|] P r[]P r[n|]

(
) .
P r[0|]

Appendix B. Proof Proposition 4.5
idea proof show find positive values x
payment scheme defined Proposition 4.5 three NE: honest reporting, everybody
reporting 0 everybody reporting 1.
NE n agents report according slie 4 n agents report honestly.
Proposition 4.2 know x found prevent equilibrium
agents lie. Similarly, incentive-compatible constraints ensure strategy profile
one agent always lies three agents always report truth cannot NE. Let
us show profile = (3 slie , s) three agents lie one agent reports
truth. honest reporter observing low quality signal report honestly
if:
P r[2|0]x P r[1|0]y > 0;

honest agent reports positive report observing high quality if:
P r[2|1]x + P r[1|1]y > 0;
P r[1|1]
However, Lemma 3.1 PP r[1|0]
r[2|0] > P r[2|1] , two inequalities never
simultaneously satisfied.
Consider profile = (2 slie , 2 s) two agents lie two agents report
truth NE. One honest reporter reports truth if:

(3P r[3|0] + 2P r[1|0])x (3P r[0|0] + 2P r[2|0])y > 0;
(3P r[3|1] + 2P r[1|1])x + (3P r[0|1] + 2P r[2|1])y > 0;

liar, hand, reports according slie if:
(3P r[0|1] + 2P r[2|1])x (3P r[3|1] + 2P r[1|1])y > 0;
(3P r[0|0] + 2P r[2|0])x + (3P r[3|0] + 2P r[1|0])y > 0;
247

fiJurca & Faltings

4 inequalities satisfied
3P r[3|1] + 2P r[1|1] < 3P r[0|1] + 2P r[2|1];
3P r[0|0] + 2P r[2|0] < 3P r[3|0] + 2P r[1|0];

impossible.
NE one agent always reports 1, n agents report according slie 3 n
agents report honestly. Clearly, 3 agents report honestly, agent always reporting
1 incentive deviate report 0 observing low quality. Consider strategy
profile = (spos , 2 s, slie ) one agent reports according spos , two agents report
honestly one agent reports according slie . liar, slie equilibrium iff:
Pr[0|0]x + Pr[1|0]y > 0;
Pr[0|1]x Pr[1|1]y > 0;

Pr[n|oi ] probability n 2 honest reporters observe 1, given




observation oi Q2 . Lemma 3.1, Pr[1|1] > Pr[1|0] , inequations
P r[0|1]
P r[0|0]
cannot hold simultaneously.
Consider strategy profile = (spos , s, 2 slie ) one agent reports according
spos , one agent reports honestly two agents report according slie . agent
reporting honestly, iff:
Pr[2|0]x Pr[0|0]y > 0;
Pr[2|1]x + Pr[0|1]y > 0;

Pr[n|oi ] probability n 2 liars observe 1, given observation
oi Q2 . impossible since Lemma 3.1

Pr[0|0]
Pr[2|0]

>

Pr[0|1]
.
Pr[2|1]

neg

lie , (3n)
Similar
techniques


used

prove


strategy
profile

,
ns


sneg , spos , n slie , (2 n) NE. Therefore, constraint (besides
incentive-compatibility constraints) acting payments x intended prevent
lying equilibrium. x take exactly values described Proposition 4.2.

Appendix C. Proof Proposition 4.6


Consider strategy profile = n s, (N n) slie n 1 agents report
honestly, others always lie. NE, honest reporter must expect
higher payment reporting truth, liar must expect higher payment lying.
Consider honest reporter observing 0. report negative signal
P r[ri = 1|0]x > P r[ri = N 2|0]y, P r[ri = 1|0] P r[ri = N 2|0]
probabilities exactly 1, respectively N 2 remaining N 1 agents report
positive signals. Exactly one agents reports positive signal when:
one honest reporters observes low quality, liars observe
high quality,
honest reporters observe low quality, one liars observe high
quality.
248

fiMechanisms Making Crowds Truthful

P r[ri = 1|0] =

X

P r[|0]



X


n 1
N n
P r[1|]P r[0|]n2
P r[1|]N n +
1
N n

n 1


N n
P r[|0]
P r[0|]n1
P r[1|]N n1 P r[0|]
0
N n 1

(n 1)!(N n + 1)!
(n)!(N n)!
P r[N n + 1|0] +
P r[N n 1|0]
(N 1)!
(N 1)!


(n 1)!(N n)!
=
(N n + 1)P r[N n + 1|0] + nP r[N n 1|0] ;
(N 1)!
=

Similarly,
P r[ri = N 2|0] =


(n 1)!(N n)!
(N n + 1)P r[n 2|0] + nP r[n|0] ;
(N 1)!

Hence honest reporter incentive truthfully submit negative report
if:




(N n + 1) P r[N n + 1|0]x P r[n 2|0]y + n P r[N n 1|0]x P r[n|0]y > 0;

hand, honest reporter submit positive report observing high
quality signal if:




(N n + 1) P r[n 2|1]y P r[N n + 1|1]x + n P r[n|1]y P r[N n 1|1]x > 0;

Exactly reasoning leads following two inequations liar:




(N n) P r[n 1|0]y P r[N n|0]x + (n + 1) P r[n + 1|0]y P r[N n 2|0]x > 0;




(N n) P r[N n|1]x P r[n 1|1]y + (n + 1) P r[N n 2|1]x P r[n + 1|1]y > 0;

exist x four inequalities satisfied time if:
(N n + 1)P r[n 2|0] + nP r[n|0]
(N n + 1)P r[n 2|1] + nP r[n|1]
<
(N n + 1)P r[N n + 1|0] + nP r[N n 1|0]
(N n + 1)P r[N n + 1|1] + nP r[N n 1|1]
(N n)P r[n 1|0] + (n + 1)P r[n + 1|0]
(N n)P r[n 1|1] + (n + 1)P r[n + 1|1]
<
(N n)P r[N n|1] + (n + 1)P r[N n 2|1]
(N n)P r[N n|0] + (n + 1)P r[N n 2|0]

equivalently:
(N n + 1)P r[n 2|0] + nP r[n|0]
(N n + 1)P r[N n + 1|0] + nP r[N n 1|0]
<
(N n + 1)P r[n 2|1] + nP r[n|1]
(N n + 1)P r[N n + 1|1] + nP r[N n 1|1]
(N n)P r[n 1|1] + (n + 1)P r[n + 1|1]
(N n)P r[N n|1] + (n + 1)P r[N n 2|1]
<
(N n)P r[n 1|0] + (n + 1)P r[n + 1|0]
(N n)P r[N n|0] + (n + 1)P r[N n 2|0]

However, one show that:
(N n + 1)P r[n 2|0] + nP r[n|0]
(N n)P r[N n|1] + (n + 1)P r[N n 2|1]
<
(N n)P r[N n|0] + (n + 1)P r[N n 2|0]
(N n + 1)P r[n 2|1] + nP r[n|1]


(N n + 1)P r[N n + 1|0] + nP r[N n 1|0]
(N n)P r[n 1|1] + (n + 1)P r[n + 1|1]
<
(N n + 1)P r[N n + 1|1] + nP r[N n 1|1]
(N n)P r[n 1|0] + (n + 1)P r[n + 1|0]

249

fiJurca & Faltings

means honest reporter liar cannot believe strategies
optimal (given strategies agents).
Consider strategy profile = (sneg , n s, N n 1 slie ) one agent
always reports 0, n 1 agents report honestly, N n 1 1 agents always lie.
honest reporter liar believe NE if:



nPr[N n 2|0] + (N n)Pr[N n|0] x Pr[n 1|0]y


nPr[N n 2|1] + (N n)Pr[N n|1] x + Pr[n 1|1]y


(n + 1)Pr[N n 3|0] + (N n 1)Pr[N n 1|0] x + Pr[n|0]y


(n + 1)Pr[N n 3|1] + (N n 1)Pr[N n 1|1] x Pr[n|1]y

>0
>0
(17)
>0
>0

Pr[j|oi ] probability j N 2 agents observe high quality signals,
given observation oi .
Nevertheless,
Pr[n 1|0]
Pr[n|0]
<
Pr[n|1]
P r[n 1|1]


nPr[N n 2|0] + (N n)Pr[N n|0]
(n + 1)Pr[N n 3|0] + (N n 1)Pr[N n 1|0]
<
nPr[N n 2|1] + (N n)Pr[N n|1]
(n + 1)Pr[N n 3|1] + (N n 1)Pr[N n 1|1]

means inequalities (17) never simultaneously satisfied.
Using exactly technique one show that:


= spos , n s, (N n 1) slie one agent always reports 0, n 1 agents
report honestly, N n 1 1 agents always lie NE;


= sneg , spos , n s, (N n 2) slie one agent always reports 0, one
agent always reports 1, n 0 agents report honestly, N n 1 0 agents always
lie NE.

Appendix D. Extending results n-ary feedback
assumed far agent observe report two signals: 0 1.
framework extended n-ary feedback imposing supplementary constraints
design problems. example, lets assume set quality signals (and feedback
reports) contains elements: Q = {q1 , q2 , . . . qM }. incentive compatibility constraints
equivalent (4) become:


X
P r[ri |qj ] (qj , ri ) (qk , ri ) ; qj , qk Q;
ri Q(N 1)

qj Q signal actually observed agent i, qk Q every lie agent
could report, ri Q(N 1) configuration N 1 reference reports.
compared (4) three observations become apparent. First, 2 constraints
binary feedback case must replaced (M 1) constraints n-ary feedback
250

fiMechanisms Making Crowds Truthful

setting. Second, size constraint grows sum N terms sum
1
CN
+M 2 terms (all possible unordered sequences N 1 signals drawn Q). Finally,
1
reward mechanism defined CN
+M 2 payments instead 2N .
constraints suffer similar blowup, makes design problem
general n-ary feedback case significantly harder solve.

References
Abreu, D., & Sen, A. (1991). Virtual Implementation Nash Equilibria. Econometrica,
59, 9971022.
Admati, A., & Pfleiderer, P. (2000). Noisytalk.com: Broadcasting opinions noisy environment. Working Paper 1670R, Stanford University.
Akerlof, G. A. (1970). market lemons: Quality uncertainty market mechanism. Quarterly Journal Economics, 84 (3), 488500.
Braynov, S., & Sandholm, T. (2002). Incentive Compatible Mechanism Trust Revelation.
Proceedings AAMAS, Bologna, Italy.
Cheng, A., & Friedman, E. (2005). Sybilproof reputation mechanisms. Proceeding
Workshop Economics Peer-to-Peer Systems (P2PECON), pp. 128132.
Clemen, R. T. (2002). Incentive contracts strictly proper scoring rules. Test, 11,
167189.
Conitzer, V., & Sandholm, T. (2002). Complexity mechanism design. Proceedings
Uncertainty Artificial Intelligence Conference (UAI).
Conitzer, V., & Sandholm, T. (2003a). Applications Automated Mechanism Design.
Proceedings UAI-03 Bayesian Modeling Applications Workshop.
Conitzer, V., & Sandholm, T. (2003b). Automated Mechanism Design Structured
Outcome Space..
Conitzer, V., & Sandholm, T. (2004). Algorithm Automatically Designing Deterministic Mechanisms without Payments. Proceedings AAMAS-04.
Conitzer, V., & Sandholm, T. (2007). Incremental Mechanism Design. Proceedings
IJCAI.
Cremer, J., & McLean, R. P. (1985). Optimal Selling Strategies Uncertainty
Discriminating Monopolist Demands Interdependent. Econometrica, 53 (2),
34561.
dAspremont, C., & Grard-Varet, L.-A. (1979). Incentives Incomplete Information.
Journal Public Economics, 11, 2545.
Dellarocas, C. (2002). Goodwill Hunting: Economically Efficient Online Feedback.
Padget, J., & et al. (Eds.), Agent-Mediated Electronic Commerce IV. Designing Mechanisms Systems, Vol. LNCS 2531, pp. 238252. Springer Verlag.
Dellarocas, C. (2006). Strategic Manipulation Internet Opinion Forums: Implications
Consumers Firms. Management Science, 52 (10), 15771593.
251

fiJurca & Faltings

Elliott, C. (2006). Hotel Reviews Online: Bed Hope, Half-Truths Hype.
New York Times.
Figlewski, S. (1979). Subjective information market efficiency betting market.
Journal Political Economy, 87 (1), 7588.
Gibbard, A. (1973). Manipulation Voting Schemes: General Result. Econometrica, 41,
587601.
Guo, M., & Conitzer, V. (2007). Worst-Case Optimal Redistribution VCG Payments.
Proceedings EC07, pp. 3039.
Hajiaghayi, M., Kleinberg, R., & Sandholm, T. (2007). Automated Online Mechanism
Design Prophet Inequalities. Proceedings AAAI07.
Harmon, A. (2004). Amazon Glitch Unmasks War Reviewers. New York Times.
Holmstrom, B. (1982). Moral Hazard Teams. Bell Journall Economics, 13, 324340.
Hu, N., Pavlou, P., & Zhang, J. (2006). Online Reviews Reveal Products True
Quality?. Proceedings ACM Conference Electronic Commerce (EC 06).
Jackson, M. O. (1991). Bayesian Implementation. Econometrica, 59, 461477.
Johnson, S., Pratt, J., & Zeckhauser, R. (1990). Efficiency Despite Mutually Payoff-Relevant
Private Information: Finite Case. Econometrica, 58, 873900.
Jurca, R., Binder, W., & Faltings, B. (2007). Reliable QoS Monitoring Based Client
Feedback. Proceedings 16th International World Wide Web Conference
(WWW07), pp. 10031011, Banff, Canada.
Jurca, R., & Faltings, B. (2005). Enforcing Truthful Strategies Incentive Compatible
Reputation Mechanisms. Internet Network Economics (WINE05), Vol. 3828
LNCS, pp. 268277. Springer-Verlag.
Jurca, R., & Faltings, B. (2006). Minimum Payments Reward Honest Reputation
Feedback. Proceedings ACM Conference Electronic Commerce (EC06),
pp. 190199, Ann Arbor, Michigan, USA.
Jurca, R., & Faltings, B. (2007a). Collusion Resistant, Incentive Compatible Feedback
Payments. Proceedings ACM Conference Electronic Commerce (EC07),
pp. 200209, San Diego, USA.
Jurca, R., & Faltings, B. (2007b). Robust Incentive-Compatible Feedback Payments.
Fasli, M., & Shehory, O. (Eds.), Trust, Reputation Security: Theories Practice,
Vol. LNAI 4452, pp. 204218. Springer-Verlag, Berlin Heidelberg.
Kandori, M., & Matsushima, H. (1998). Private observation, communication collusion.
Econometrica, 66 (3), 627652.
Keates, N. (2007). Deconstructing TripAdvisor. Wall Street Journal, page W1.
Li, S., & Balachandran, K. (2000). Collusion proof transfer payment schemes multiple
agents. Review Quantitative Finance Accounting, 15, 217233.
Ma, C. (1988). Unique implementation incentive contracts many agents. Review
Economic Studies, 555572.
252

fiMechanisms Making Crowds Truthful

Maskin, E. (1999). Nash Equilibrium Welfare Optimality. Review Economic Studies,
66, 2328.
Matsushima, H. (1988). New Approach Implementation Problem. Journal
Economic Theory, 45, 128144.
Miller, N., Resnick, P., & Zeckhauser, R. (2005). Eliciting Informative Feedback: PeerPrediction Method. Management Science, 51, 1359 1373.
Moore, J., & Repullo, R. (2005). characterization virtual Bayesian implementation.
Games Economic Behavior, 50, 312331.
Palfrey, T., & Srivastava, S. (1991). Nash-implementation using Undominated Strategies.
Econometrica, 59, 479501.
Parasuraman, A., Zeithaml, V., & Berry, L. (1985). Conceptual Model Service Quality
Implications Future Research. Journal Marketing, 49, 4150.
Pennock, D., Debnath, S., Glover, E., & Giles, C. (2002). Modeling information incorporation markets application detecting explaining events. Proc.
18th Conf. Uncertainty Artifcial Intelligence, pp. 405411.
Sandholm, T., Conitzer, V., & Boutilier, C. (2007). Automated Design Multistage Mechanisms. Proceedings IJCAI07.
Servan-Schreiber, E., Wolfers, J., Pennock, D., & Galebach, B. (2004). Prediction markets:
money matter. Electronic Markets, 14 (3).
Sherali, H., & Shetty, C. (1980). Optimization Disjunctive Constraints. SpringerVerlag.
Talwar, A., Jurca, R., & Faltings, B. (2007). Understanding User Behavior Online Feedback Reporting. Proceedings ACM Conference Electronic Commerce
(EC07), pp. 134142, San Diego, USA.
von Ahn, L., & Dabbish, L. (2004). Labeling Images Computer Game. Proceedings
ACM CHI.
White, E. (1999). Chatting Singer Pop Charts. Wall Street Journal.

253

fiJournal Artificial Intelligence Research 34 (2009) 499-520

Submitted 10/08; published 3/09

Exploiting Single-Cycle Symmetries
Continuous Constraint Problems
Vicente Ruiz de Angulo
Carme Torras

ruiz@iri.upc.edu
torras@iri.upc.edu

Institut de Robotica Informatica Industrial (CSIC-UPC)
Llorens Artigas 4-6, 08028-Barcelona, Spain.
WWW home page: www.iri.upc.edu

Abstract
Symmetries discrete constraint satisfaction problems explored exploited last years, symmetries continuous constraint problems received attention. focus permutations variables consisting
one single cycle. propose procedure takes advantage symmetries
interacting continuous constraint solver without interfering it. key concept
procedure classes symmetric boxes formed bisecting n-dimensional
cube point dimensions time. analyze classes
quantify function cube dimensionality. Moreover, propose simple
algorithm generate representatives classes number variables
high rates. problem example chemical field cyclic n-roots problem
used show performance approach practice.

1. Introduction
Symmetry exploitation discrete constraint satisfaction problems (CSPs) received
great deal attention lately. Since CSPs usually solved using AI search algorithms,
approaches dealing symmetries fall two groups: entail reformulating
problem adding constraints search (Flener, Frisch, Hnich, Kiziltan, & Miguel, 2002;
Puget, 2005), break symmetries along search (Meseguer & Torras, 2001;
Gent, 2002). Permutations variables, interchangeability values commonly
addressed symmetries repertoire techniques developed,
relying computational group theory.
contrary, symmetries largely disregarded continuous constraint
solving, despite important growth theory applications field
recently experienced (Sam-haroud & Faltings, 1996; Benhamou & Goualard, 2000; Jermann
& Trombettoni, 2003; Porta, Ros, Thomas, & Torras, 2005). Continuous (or numerical)
constraint solving often tackled using Branch-and-Prune (B&P) algorithms (Hentenryck,
Mcallester, & Kapur, 1997; Vu, Silaghi, Sam-Haroud, & Faltings, 2005), iteratively
locate solutions inside initial domain box, alternating box subdivision (branching)
box reduction (pruning) steps.
Motivated molecular conformation problem, paper deal
simple type box symmetry, namely domain variables (i.e., box dimensions) undergo single-cycle permutation leaving constraints invariant. clear,
cycle involves n variables, algorithm handles n 1 symmetries (excluding
c
2009
AI Access Foundation. rights reserved.

fiRuiz de Angulo & Torras

identity) generated cycle composition. Since computational gain
shown roughly proportional n, longest cycle appearing problem formulation chosen input algorithm.
single-cycle permutation leaves constraints unchanged form constraint symmetry terminology introduced Cohen, Jeavons, Jefferson, Petrie,
Smith (2006). Note constraint symmetry also solution symmetry,
way around. Thus, symmetries deal subset possible solution
symmetries; advantage assessed (although perhaps difficult
find) problem formulation, therefore operative.
approach exploit symmetries continuous constraint problems requires
initial domain symmetric variables n-cube, starts subdividing
cube point along dimensions once. Since box symmetry transitive
relation, subboxes resulting subdivision fall equivalence classes. Then,
B&P algorithm (or similar continuous constraint solver) called subboxes
representatives symmetry equivalence class. Finally, solution found,
symmetric ones generated. Note symmetry handling doesnt interfere
inside workings constraint solver.

2. Symmetry Continuous Constraint Problems
interested solving following general continuous Constraint Satisfaction Problem (continuous CSP): Find points x = (x1 , . . . , xn ) lying initial box Rn satisfying
constraints f1 (x) C1 , . . . , fm (x) Cm , fi function fi : Rn R, Ci
interval R.
particular feature require Continuous Constraint Solver (CCS)
work axis-aligned box Rn input. Also, assume
CCS returns solution boxes. Note CCS returning solution points limit case still
contained framework.
say function : Rn Rn point symmetry problem exists
associated permutation fi (x) = f(i) (s(x)) Ci = C(i) , = 1, . . . , m.
consider symmetry property relates points equivalent regards
continuous CSP. Concretely, definition one conclude
x solution problem iff s(x) solution problem.
Let two symmetries continuous CSP associated permutations
. easy see composition symmetries s(t()) also symmetry
associated permutation (t ()).
interesting type symmetries permutations (bijective functions set onto
itself) components x. Let finite set. cycle length k permutation
exist distinct elements a1 , . . . ak (ai ) = (a(i+1)mod k )
(z) = z element z D. cycle represented (a1 , . . . ak ).
Every permutation expressed composition disjoint cycles (i.e, cycles without
common elements), unique order factors. Composition cycles
represented concatenation, example (a1 , . . . ak )(b1 , . . . bl ). paper focus
particular type permutations, namely constituted single cycle.
500

fiExploiting Single-Cycle Symmetries

simplest form1 , s(x1 , x2 , . . . xn ) = (x(1) , x(2) , . . . x(n) ) = (x2 , x3 ...xn , x1 ),
(i) = (i + 1) mod n.
Example: n = 3, = 4, x = (x1 , x2 , x3 ) [1, 1] [1, 1] [1, 1],
f1 (x) :

x21 + x22 + x23 [5, 5] x21 + x22 + x23 = 5

f2 (x) :

2x1 x2 [0, ] 2x1 x2 > 0

f3 (x) :

2x2 x3 [0, ] 2x2 x3 > 0

f4 (x) :

2x3 x1 [0, ] 2x3 x1 > 0

exists symmetry s(x1 , x2 , x3 ) = (x2 , x3 , x1 ), need reordering variables. constraint permutation associated (1) = 1, (2) = 3,
(3) = 4, (4) = 2.
Generally unique symmetry given problem. exists symmetry
s, example s2 (x) = s(s(x)) another symmetry. general, using convention
denoting s0 (x) identity mapping, {si (x), = 0 . . . n1} set different symmetries
obtained composing s(x) itself, > n si (x) =
si mod n (x). Thus, single-cycle symmetry generates composition n 1 symmetries,
excluding trivial identity mapping. may different numbers cycles.
Imagine example continuous CSP n = 4 permutation variables (1
2 3 4) symmetry. Then, permutation obtained composing twice, (1 3)(2 4),
also symmetry problem, different number cycles, longest
cycle length two instead four. Besides, former permutation cannot generated
latter. algorithm presented paper deals compositions
single-cycle symmetry, even single-cycle symmetries.
gain obtained proposed algorithm shown roughly proportional
number different compositions selected symmetry. Therefore, several
single-cycle symmetries exist continuous CSP problem, algorithm used
generating symmetries composition, i.e., longest
cycle. Note single-cycle permutations dealing need encompass
problem variables, since remaining ones considered fixed (unitary cycles).

3. Box Symmetry
Since continuous constraint solvers work boxes, turn attention set
points symmetric belonging box B Rn . 2
Let single-cycle symmetry corresponding circular variable shifting introduced preceding section, B = [x1 , x1 ]. . .[xn , xn ] box Rn . box symme1. general, variables must arranged suitable order one apply circular shifting. Thus, general form single-cycle symmetry s(x) = h1 (g(h(x))),
h(x1 , . . . xn ) = (x(1) , . . . , x(n) ), n general permutation orders variables,
g(x1 , . . . , xn ) = (x(1) , . . . x(n) ) circular shifting above. Thus, cycle defining symmetry expressed = 1 ((())). Since reordering change substantially
presented concepts algorithms, simplified notation paper assuming order
component variables appropriate one, i.e., = .
2. set {s(x) s.t. x B} also box s(x) = (s1 (x), . . . , sn (x)) = (g1 (x(1) ), . . . , gn (x(n) )),
si i-th component s, arbitrary permutation, gi : R R function
interval R {gi (x) s.t. x I} also interval R.

501

fiRuiz de Angulo & Torras

try function defined S(B) = {s(x) s.t. x B} = [x(1) , x(1) ] . . . [x(n) , x(n) ] =
[x2 , x2 ] . . . [xn , xn ] [x1 , x1 ]. box symmetry function also associated constraint permutation , associated s. denote composed
times. say, then, B1 B2 symmetric boxes exists s.t. (B1 ) = B2 .
Box symmetry equivalence relation defining symmetry equivalence classes. Let
R(B) set different boxes symmetry class B, R(B) = {S (B), {0, . . . , n
1}}. instance, box B 0 = [0, 4] [2, 5] [2, 5] [0, 4] [2, 5] [2, 5], R(B 0 ) composed
0 (B 0 ) = B 0 , 1 (B 0 ) = [2, 5][2, 5][0, 4][2, 5][2, 5][0, 4] 2 (B 0 ) = [2, 5][0, 4]
[2, 5][2, 5][0, 4][2, 5]. Note 3 (B 0 ) B 0 subsequent applications
box symmetry would repeat sequence boxes. define period P (B)
box B P (B) = |R(B)|. easily shown R(B) = {S (B), {0, . . . , P (B) 1}}.
example, box B 0 , R(B 0 ) = {S 0 (B 0 ), 1 (B 0 ), 2 (B 0 )} P (B 0 ) = 3.
Box symmetry implications continuous CSP, direct consequence
point symmetry case:
solution inside box B, solution inside symmetric
boxes either.
box B B solution iff (B ) (B) solution box {1 . . . P (B)
1}.
Sketch proof first statement: Assume solution inside B
solution xsol inside (B). definition box symmetry exists point x0sol B
xsol = si (x0sol ). Using property highlighted Section 2 deduce x0sol
must also solution, contradicts hypothesis.
Sketch proof second statement: solution box box least solution
point inside. Assume B B solution box containing solution point xsol . Inside
(B ) point si (xsol ) that, property highlighted Section 2, must
also solution. Conversely, assume (B ) B solution box. Thus contains
least solution point, xsol . definition symmetric box, point symmetric
point x0sol B xsol = si (x0sol ). Using property Section 2 conclude
x0sol must also solution and, thus, B solution box.
statements rephrased follows :
set solution boxes contained box B SolSet, set solution boxes
contained symmetric box (B) {S (B ) s.t. B SolSet}
means solutions inside B found, solutions inside
symmetric boxes (B), {1 . . . P (B) 1} available without hard calculations.
following sections show exploit property save much computing time
meta-algorithm uses CCS tool without interfering it.
3.1 Box Symmetry Classes Obtained Bisecting n-cube
algorithm propose exploit box symmetry makes use symmetry classes
formed bisecting n-dimensional cube n (i.e., period 1) dimensions
time point, resulting 2n boxes. denote L H
two subintervals original range divided. example, n = 2,
502

fiExploiting Single-Cycle Symmetries

following set boxes {L L, L H, H L, H H} whose periods 1, 2,
2 1, respectively. symmetry classes are: {L L}, {L H, H L},
{H H}. Representing two intervals L H 0 1, respectively, dropping
symbol, sub-boxes coded binary numbers. Let SRn set
representatives, formed choosing smallest box binary order class.
example, SR2 = {00, 01, 11}. Note cube n partitioned thought
set binary numbers length n, SRn nothing subset
whose elements different circular shift.
algorithm exploiting symmetries way uses SRn explained
next section. Afterwards, Sections 6 7, study many components SRn has,
distributed and, importantly, generated.

4. Algorithm Exploit Box Symmetry
Algorithm 1: CSym1 algorithm.
Input: n-cube, [xl , xh ] [xl , xh ].
single-cycle box symmetry, S.
Continuous Constraint Solver, CCS.
Output: set boxes covering solutions.

5

SolutionBoxSet EmptySet
x SelectBisectionPoint(xl , xh )
foreach b SRn
B GenerateSubBox(b, xl , xh , x )
SolutionBoxSet SolutionBoxSet ProcessRepresentative(B)

6

return SolutionBoxSet

1
2
3
4

symmetry exploitation algorithm propose uses CCS external routine.
internals CCS must modified known.
idea first divide initial box number symmetry classes. Next, one
needs process representative class CCS. end, applying
box symmetries solution boxes obtained way, one would get solutions
lying space covered whole classes, i.e., initial box. advantage
procedure CCS would process fraction initial box. Assuming
initial box n-cube covering interval [xl , xh ] dimensions,
directly apply classes associated SRn . procedure exploit single-cycle symmetries
way presented Algorithm 1.
Since SRn set codes real boxes need translation codes
boxes given initial box. operator GenerateSubBox(b, xl , xh , x ) returns
box V = V1 Vn corresponding code b = b1 . . . bn [xl , xh ] range
initial box dimensions x point interval bisected:
(
[xl , x ] bi = 0,
Vi =
(1)
[x , xh ] bi = 1.
503

fiRuiz de Angulo & Torras

point x calculated SelectBisectionPoint(xl , xh ) xl <
< xh , reasonable one (xl + xh )/2. iterations line 4 generate set
representative boxes that, together symmetries, cover initial n-cube.
ProcessRepresentative(B) returns solution boxes associated B, is,
solutions inside R(B), still words, solutions inside B inside symmetric
boxes. ProcessRepresentative(B) based property stated end Section
3, allows obtain solutions class B processing B
CCS. SolSet set solutions found inside representative box class, B.
ApplySymmetry(SolSet, ) calculates set solutions box (B) applying
boxes SolSet. Since number symmetries B P (B), benefits
exploiting symmetries class representative proportional period.
x

Algorithm 2: ProcessRepresentative function.
Input: box, B.
single-cycle box symmetry, S.
Continuous Constraint Solver, CCS.
Output: set solution boxes contained B symmetric boxes.

4

SolSet CCS(B)
otalSolSet SolSet
i=1: P (B) 1
otalSolSet otalSolSet ApplySymmetry(SolSet, )

5

return otalSolSet

1
2
3

correctness algorithm easy check. set boxes searches explicitly implicitly (by means symmetry) solutions U = {R(B) s.t. B representative}.
fact, U set boxes formed bisecting initial box dimensions
time point. U covers whole initial box and, thus, algorithm finds
solutions problem. Moreover, finds solution box once,
boxes U volume common (they share wall).
4.1 Discussion Efficiency CSYM1
CSym1 algorithm launches CCS algorithm |SRn | small boxes instead
original large one. Three factors affect efficiency compared standard
approach:
1. Fraction domain processed. fraction original domain directly
dealt CCS. fraction function periods SRn components. One element period p represents class formed p boxes, one
processed CCS. Since boxes classes equal size,
fraction calculated dividing number representatives
n|
P |SRn |
total number boxes classes, |SR
2n =
P (B) . expected time gain
P

BSRn

P (B)

n
inverse quantity, BSR
denoted IFDP (Inverse Fraction
|SRn |
Domain Processed). n grows (see Section 6), majority elements

504

fiExploiting Single-Cycle Symmetries

SRn period n, thus IFDP tends n. However, low n, IFDP
significantly smaller n. main factor determining efficiency
CSym1.
2. Smaller processed boxes. Since CCS initial boxes using CSym1 2n times
smaller original initial box, average size boxes processed
CCS also smaller standard case. Prune (box reduction contraction)
step carried quickly smaller boxes Branch-and-Prune algorithms.
fact, best Branch-and-Prune algorithms box contraction operators exhibiting
second-order convergence, contraction rate requires small enough boxes
hold practice.
3. Number representatives. disadvantage fractioning excessively
initial domain. see noting that, using original large initial
box, contraction operator lowers upper bound symmetric variable,
information could used lower upper bound variable many
representative boxes SRn . commented above, contraction operator would
act strongly representatives themselves, loss parallelization
effect anyway present. factor irrelevant small-length cycle symmetries,
say n = 6, |SRn | small (see Section 6 again) compared
number boxes CCS must process general. However, n approaches
20, number representatives begins become overwhelming.

5. Two Illustrative Examples
two problems solved Branch-and-Prune CCS presented
Porta, Ros, Thomas, Corcho, Canto, Perez (2008). polytope-based method
similar Sherbrooke E. C. (1993) global consistency, exhibits quadratic
convergence. machine used carry experiments paper 2.5 Ghz
G5 Apple computer.
5.1 Cycloheptane
Molecules modeled mechanical chains making reasonable approximations.
two atoms joined chemical bond, one assume rigid link
them. Thus, first approximation bond lengths constant. second one
angles two consecutive bonds also constant. words,
distances atoms subchain three atoms assumed constant.
configurations atoms molecule satisfy distance constraints, sometimes
denoted rigid-geometry hypothesis, valid conformations molecule kinematic
sense. constraints induced rigid-geometry hypothesis particularly strong
molecule topology forms loops, cycloalkanes. problem finding
valid conformations molecule formulated distance-geometry (Blumenthal,
1953) problem distances points (atoms) fixed known,
one must find set values unknown (variable) distances compatible
embedding points R3 . unknown distances found solving set
505

fiRuiz de Angulo & Torras

constraints consisting equalities inequalities determinants formed subsets
fixed variable distances (Blumenthal, 1953).

d2
d3

d1
d7

d5

d6

d4

Figure 1: Cycloheptane. Disks represent carbon atoms. Constant variable distances
atoms represented continuous dashed lines, respectively.

Figure 2: Three-dimensional projection cycloheptane solutions. lightest (yellow)
boxes solutions found inside representatives using CCS (line 1
Algorithm 2). colored boxes solutions obtained applying
symmetries yellow boxes (line 4 Algorithm 2).

Figure 1 displays known unknown distances cycloheptane, molecule basically composed ring seven carbon atoms. distance two consecutive atoms
ring constant equal everywhere. distance two atoms connected
506

fiExploiting Single-Cycle Symmetries

atom also known constant matter atoms. problem underconstrained, infinite number solutions dimensionality 1. problem several
symmetries. use one them, s(d1 , . . . , d7 ) = (d(1) , d(2) , . . . , d(7) ) = (d2 , d3 . . . , d7 , d1 ).
length cycle symmetry n = 7, IFDP 6.4.
number boxes processed using raw CCS without symmetry handling 1269,
using CSym1 total number 196, giving ratio 6.47 IFDP. problem
solved 4.64 minutes using CSym1, compares favorably 31.6 minutes
spent using algorithm Porta et al. (2008) alone, reduction factor 6.81,
slightly greater IFDP. means that, although number representatives begins
relevant (|SR7 | = 20), factor 2 Section 4.1 determining factor 3
section, since (small) time overhead introduced handling box symmetries
also included reported time. Figure 2 shows projection d1 , d2 d3
solutions obtained using CSym1. solutions found inside five representative boxes
period seven, containing 16, 1, 4, 64 1 solution boxes, respectively, chosen level
resolution. total number solutions boxes therefore 7(16+1+4+64+1)= 602.
5.2 Cyclic n-roots Problem
following polynomial equation system n = 5 instance so-called cyclic
n-roots problem described Bjorck Froberg (1991).

x1 + x2 + x3 + x4 + x5 = 0
x1 x2 + x2 x3 + x3 x4 + x4 x5 + x5 x1 = 0
x1 x2 x3 + x2 x3 x4 + x3 x4 x5 + x4 x5 x1 + x5 x1 x2 = 0
x1 x2 x3 x4 + x2 x3 x4 x5 + x3 x4 x5 x1 + x4 x5 x1 x2 + x5 x1 x2 x3 = 0

(2)

x1 x2 x3 x4 x5 1 = 0

ten real solutions problem. system single-cycle symmetry:
s(x1 , . . . , x5 ) = (x2 , x3 , x4 , x5 , x1 ), well multiple-cycle symmetry considered
paper. Thus, cycle length n = 5, |SR5 | = 8, IFDP 4. running
CCS alone using initial box [10, 10]5 , number processed boxes 399,
exploiting aforementioned symmetry CSym1 algorithm number reduces
66. last case, two solutions found representative box period 5,
symmetry led ten solutions. Running times 16.86 seconds (CCS alone)
2.08 seconds (CSym1) giving gain eight. double IFDP,
highlights benefits factor 2 Section 4.1 bring efficacy
approach. number representatives small compared number boxes
processed CCS alone, making factor 3 Section 4.1 irrelevant case.
Table 1 contains results n=4 n=8 cyclic n-roots problem [10, 10]n
domain, except n=8 domain [5, 5]8 . n=4 n=8
continuum solutions which, chosen resolution, produces 992 2435 solution
boxes, respectively. this, number processed boxes n=5 smaller
n=4, logically smaller also n=6 n=8. Two observations
507

fiRuiz de Angulo & Torras

IFDP
number processed boxes CCS alone
number processed boxes CSym1
rate processed boxes
time CCS alone
time CSym1
time gain CSym1

n=4

n=5

n= 6

n=7

2.7
1855
500
3.7
12.0
3.0
4.0

4.0
399
66
6.0
16.9
2.1
8.1

4.5
3343
510
6.6
642.0
95.8
6.7

6.4
38991
5070
7.7
20442.0
2689.7
7.6

n=8
(reduced domain)
7.1
108647
13304
8.2
227355.2
27296.5
8.3

Table 1: Results n-cyclic roots problem. Times given seconds.

made. First, time gains always higher corresponding IFDPs, implying
preponderance factor 2 Section 4.1 factor 3. Second, time gain follows rather
accurately rate number processed boxes using CCS alone using
CSym1.
Tests cyclic n-roots problem using classical CCSP solver, RealPaver (Granvilliers & Benhamou, 2006), carried (Jermann, 2008). results preliminary difficult expose concisely, since great variability depending issues
pruning method used (RealPaver offers several options) problem
coded (factorized not). every case, however, observed time gains greater
expected IFDP.

6. Analysis SRn : Counting Number Classes
Let us define quantities interest:
-Nn : Number elements SRn .
-FP n : Number elements SRn correspond full-period boxes, i.e., boxes period
n.
-Nnm : Number elements SRn 1s.
-FP nm : Number elements SRn correspond full-period boxes 1s.
Polyas theorem (Polya & Read, 1987) could used determine quantities given n building possibly huge polynomial elucidating
coefficients. present simpler way calculating and, time, make
reader familiar concepts used algorithm generate SRn .
begin looking expression FP n . number 1s allowed,
total number binary numbers 2n . periods exist binary
numbers divisors n. Thus, following equation holds:
X

p FP p = 2n .

pdiv(n)

Segregating p = n,
508

(3)

fiExploiting Single-Cycle Symmetries

X

n FP n +

p FP p = 2n ,

(4)

p
FP p .
n

(5)

pdiv(n), p<n

solving FP n :
FP n =

2n

n

X
pdiv(n), p<n

recurrence simple baseline condition: FP 1 = 2.
Then, Nn follows easily
Nn =

X

FP p .

(6)

pdiv(n)

Segregating p = n, efficient formula obtained:
Nn =

2n
+
n

X
pdiv(n), p<n

np
FP p .
n

(7)

formula valid n > 1. remaining case N1 = 2.

n
use similar techniques obtain FP nm Nnm .
binary numbers
1s n 0s. binary numbers circular shifts others
(like 011010 110100). number shifted versions binary number period
box represented binary number. example, 1010, period 2,
another shifted version, 0101. binary number representing box period p
n
seen concatenation n/p numbers length n/p
= p period p. means

concatenated numbers full-period, n/p
1s. Thus, number
binary numbers period p shifted numbers counted (i.e., number
n
. common divisors n m, denote
classes period p) FP n/p
n/p
div(n, m), periods. Since p shifted versions binary number
period p, write

X
n
n
=
p FP n/p
.
(8)
n/p

pdiv(n,m)

change variable f = n/p get
X
f div(n,m)

n
FP nf mf =
f


n
.


(9)

Note index summation goes values before.
segregate case f = 1 summand,

X
n
n
n FP nm +
FP nf mf =
,
(10)
f

f div(n,m), f >1

and, finally, obtain
509

fiRuiz de Angulo & Torras

FP nm =

n




n

FP nf mf

X



f

f div(n,m), f >1

.

(11)

recurrence relation FP nm computed using following
baseline conditions:
(
0
=
1

FP nn , FP n0

n > 1
n = 1

(12)

Nnm obtained adding number classes period:
Nnm =

X

FP nf mf .

(13)

f div(n,m)

Segregating f = 1, efficient formula obtained:
Nnm =


n
+


X

n
)FP nf mf ,
f

(14)

(1 p)FP p mp ,

(15)

(1

f div(n,m), f >1

carrying change variable p = n/f :
Nnm =


n
+


X

n

pdiv(n,m), p<n

Note change summation range. equation valid whenever > 0
< n. Otherwise, Nnm = 1.
possible extend concept FP n (and FP nm ) reflect number members
p
):
SRn period p (and 1s), denote Nnp (Nnm
Nnp

p
Nnm

(
0
=
FP p

p
/ div(n)
otherwise

(
0
=
FP p, mp
n

p
/ div(n, m)
otherwise

(16)

(17)

Figure 3(a) displays number classes (Nn ) function n. curve indicates
exponential-like behavior. confirmed Figure 3(b) using larger logarithmic
scale, curve appears almost perfectly linear. Figure 4 example
distribution classes period n = 12. Figure 5 shows percentage full-period
classes SRn (100 Nnn /Nn ). One see percentage classes period different
n significant low n, approaches quickly 0 n grows. Finally, Figures 6(a)
6(b) display distribution classes SRn number 1s n = 12
n = 100, respectively. majority classes concentrates interval middle
graphic, around n/2. interval becomes relatively smaller n grows.
510

finumber classes symmetric boxes

350
300
250
200
150
100
50
0

2

4

6

8

10

12

1x1011
1x1010
1x109
1x108
1x107
1x106
1x105
1x104
1x103
1x102
1x101
1

0

5

box dimensionality (i.e., number variables)

10

15

20

25

30

(b)

Figure 3: Number elements SRn function n.

100

10

1

1

2

3

4

35

box dimensionality (i.e., number variables)

(a)

number classes symmetric boxes

number classes symmetric boxes

Exploiting Single-Cycle Symmetries

5

6

7

8

9

10

11

12

box period

Figure 4: Number elements SR12 distributed period.

511

40

fiRuiz de Angulo & Torras

7. Generating SRn , Classes Symmetric Boxes
naive procedure obtain SRn would initially generate boxes originated bisecting
n-dimensional cube point dimensions time. Then, one
check boxes set detect whether circular shift
others. complete process generating SRn way involves huge number
operations even rather small dimensions. Although SRn ns could precomputed stored database, suggest algorithm capable calculating
SRn fly without significant computational overhead.

percentage full-period classes

100

80

60

40

20

0
2

4

6

8

10

12

14

16

18

20

box dimensionality (i.e., number variables)

Figure 5: Percentage full-period elements SRn function n.

number classes symmetric boxes

number classes symmetric boxes

made counting, distinguish different subsets SRn basis number
1s period:
-SRnm : Subset elements SRn 1s.
-SRpnm : Subset elements SRn 1s period p.
SRpn : Subset elements SRn period p.
global point view, generation SRn carried follows. First,
SRn0 generated, constituted always unique member. Afterwards, SRnm

70
60
50
40
30
20
10
0

0

2

4

6

8

10

12

1x1027
8x1026
6x1026
4x1026
2x1026

0

0

20

40

60

80

100

number 1s code

number 1s code

(a)

(b)

Figure 6: Number elements SRn distributed number 1s. (a) n =12. (b) n=100.

512

fiExploiting Single-Cycle Symmetries

= 1 . . . n generated. generation SRnm divided SRpnm ,
p div(n, m), compose it. algorithm ClassGen described generates
full-period representatives given number variables n > 1 number ones
> 0, i.e., generates SRnnm . representatives lower period p div(n, m)
obtained concatenating one block n/p = f times. Therefore, order obtain
SRpnm , generate SRpp algorithm, concatenate elements
f

f times. Thus, without loss generality, follows describe workings
algorithm ClassGen computes codes full period, namely n.
use compact coding binary numbers representing boxes consisting
ordered lists chains numbers. first number code number 0s
appearing first 1 binary number. i-th number code > 1
number 0s (i1)-th i-th 1s binary number. example,
number 0100010111 codified 13100. length numerical codification
number 1s codified binary number, denoted m.
binary numbers cannot codified way, last digit
0. But, except zeros case, always element class
codified correctly (for example 0011 element class 0110). objective
representative class, rather advantage, half
boxes already eliminated beginning. zeros box, SRn0 , common
every n, generated separately, already mentioned.
codification allows determine box full-period way
binary representation: box period n iff number circular shifts lower
length numerical chain result never equal original. instance,
example full-period, 22, corresponding 001001, not. difference
that, new representation, shifts must compared.
code box seen number base n m. full-period box,
circular shifts code different numbers, arranged strictly increasing
numerical order. take representative box class largest element
class expressed code (which smallest expressed binary number).
example, class 130 two elements represented coding,
013 301, latter chosen representative class.
Note box belonging SRnm n 0s or, equivalently, sum
components code n m.
output algorithm codes length m, whose sum components
n m, representatives class full-period. Codes length
whose components sum desired number rather easy generate systematically.
representativeness full-period conditions difficult guarantee efficiently.
handle exploiting properties codes stated below, make
use definition i-compability.
say code i-compatible compatible position sub-chain
beginning position > 1 ending last position (thus length + 1)
strictly smaller numerical terms sub-chain length beginning
first position. example, 423423 compatible positions 2 3,
4-compatible.
513

fiRuiz de Angulo & Torras

Property 1 code class representative full-period iff i-compatible
s.t. 1 < 6 m.
Thus, instead comparing chains length (i.e., code shifted versions),
determine code validity comparing shorter sub-chains. second property helps
us devise still faster simpler algorithm:
Property 2 code i-compatible sub-chain position + l equal
sub-chain position 1 1 + l code also compatible positions + 1
+ l.

Algorithm 3: CodeValidity algorithm.
Input: code length expressed array, A.
Output: boolean value indicating whether code valid, i.e., whether
full-period class representative.
1
2
3
4
5
6
7
8
9
10

i2
ctrol 1
V alidCode True
V alidCode & <
A[i] > A[ctrol] V alidCode F alse
else A[i] < A[ctrol] ctrol 1
else ctrol ctrol + 1;
ii+1

/* A[i] = A[ctrol] */

A[m] A[ctrol] V alidCode F alse
return ValidCode

property interesting permits checking validity code
travelling along once, shown Algorithm 3. trick decision
i-compatibility delayed position following numbers
beginning string, finally resolves positively, compatibility
intermediate numbers also guaranteed. Hence, i-compatibility either resolved
simple comparison requires l comparisons. latter case, either compatibility
l positions also resolved (if outcome positive) compatibility intermediate
positions doesnt matter (because outcome negative and, thus, code labelled
non valid without checks). ctrol variable charge maintaining last index
head sub-chain compared current compatibility check.
examining compatibility current position i, value lower
ctrol position, code sure i-compatible therefore must worry
(i + 1)-compatibility back-warding ctrol first position. value ctrol
position equal current position i, compatibility position still
ascertained, continue advancing current ctrol positions equality
disappears. words, condition must fulfilled non rejecting
invalid code position
514

fiExploiting Single-Cycle Symmetries

Algorithm 4: ClassGen algorithm.
Input: sum numbers remain written right (from
position pos m), sum.
index next position written, pos.
index current control element, whose value cannot surpassed

next position, ctrol.
length code, m.
Array class codes generated, A.
Output: set codes representing classes, SR.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

SR EmptySet
pos =
sum < A[ctrol]
A[m] sum
SR {A};

/* otherwise, SR remain EmptySet */

else
pos = 1
LowerLimit = dsum/me
U pperLimit sum
else
LowerLimit = 0
U pperLimit Minimum(A[ctrol], sum)
= U pperLimit LowerLimit
A[pos]
= A[ctrol]Sand pos 6= 1
/* = A[ctrol] = U pperLimit */
SR SR ClassGen(sum i, pos + 1, ctrol + 1, m, A)
else
/* < A[ctrol] pos = 1 */

SR SR ClassGen(sum i, pos + 1, 1, m, A)

19
20

return SR

A[i] 6 A[ctrol],

(18)

condition transformed A[i] < A[ctrol] = resolve last
pending compatibility checks. aside, note codes general
raw binary numbers, representativeness full-periodness defined
way both. Therefore, three properties CodeValidity algorithm apply also
raw binary numbers.
rather direct way generate SRnnm would generate codes length
whose sum components n (the number zeros expressed binary
number) filter CodeValidity. Instead, taken
515

fiRuiz de Angulo & Torras

efficient approach, generating codes satisfy conditions need
checked explicitly CodeValidity. Therefore, Algorithm 3 (presented clarity
purposes) used.
main procedure obtain full-period representatives 1s, i.e., SRnnm ,
recursive program presented Algorithm 4. ClassGen(n m, 1, 1, m, A),
array length m, must called obtain SRnnm , given n > 1, > 0.
call procedure writes single component code position indicated
parameter pos, beginning pos = 1, subsequently incremented
recursive call. recursion finishes rightmost end code, pos = m.
first parameter, sum, sum components code remain written.
range values written position pos limited LowerLimit U pperLimit,
except last position m. following show correctness algorithm
verifying limits chosen satisfy two requirements code:
sum numbers code completed algorithm must nm. First,
recall initial call algorithm done using parameter sum = n m.
position 1 pos < number written must greater
equal sum numbers still written, quantity represented sum,
subsequent positions possible write positive integers, least
zeros. condition imposed U pperLimit line 9 pos = 1 line 12
(juxtaposed code validity conditions) 1 < pos < m. number written
pos substracted sum parameter next recursive call. Finally,
pos = m, possibility satisfy sum condition assign value
sum last element code.
code validity conditions, CodeValidity, number
written position pos must smaller equal A[ctrol] 1 < pos < m,
strictly lower A[ctrol] pos = m. conditions reflected
U pperLimit assignments made lines 12 3, respectively. LowerLimit usually
(pos < 1, line 4) set smallest possible element codes, 0.
beginning code (pos = 1, line 8) tight value chosen since,
value lower upper rounded value dsum/me, way distribute
remains sum among positions code without putting value
greater initial one, would make code non-representative.
maintenance ctrol variable similar within CodeValidity
algorithm: write pos something strictly minor A[ctrol], ctrol back-warded
first position. Otherwise, ctrol incremented 1 next recursive call write
pos + 1.
output algorithm list valid codes decreasing numerical order.
instance, output obtained requesting SR993 ClassGen(6, 1, 1, 3, A) is: {600,
510, 501, 420, 411, 402, 330, 321, 312}. example, case recursion
arrives pos = without returning valid code frustrated code 222, whose last
number written code full-period.
Figure 7 displays quantitative results reflect efficiency ClassGen.
dashed line accounts complete times required generate class representatives
516

fiExploiting Single-Cycle Symmetries

total time
millions representatives per second

1.4

20

1.2

1
15
0.8
10

0.6

total time (seconds)

number representatives (millions/second)

25

0.4

5

0.2
0
5

10

15

20

25

30

0

dimensionality (number variables)

Figure 7: Total time (dashed line) generate SRn , rates generation (continuous
line) class representatives function n.

SRn n = 2 n = 30. worth noting SR30 requires second
entirely generated. continuous line encodes division |SRn | time required
generate SRn , measured millions class representatives generated second.
evident efficiency ClassGen high even grows slightly
n. behavior shows dead-ends recursion statistically insignificant,
proves tightness bounds used enforce values code numbers.

8. Conclusions
approached problem exploiting symmetries continuous constraint satisfaction problems using continuous constraint solvers. approach general make
use box-oriented CCS black-box procedure. particular symmetries
tackled single-cycle permutations problem variables.
suggested strategy bisect domain, n-cube initial box, simultaneously
dimensions point. forms set boxes grouped box
symmetry classes. representative class selected processed CCS
symmetries representative applied resulting solutions.
way, solutions within whole initial domain found, processed fraction set representatives CCS. time savings
obtained processing representative applying symmetries solutions tend
proportional number symmetric boxes representative. Therefore, symmetry exploitation complete full-period representatives, since maximum
number symmetric boxes. Another factor improves efficiency could
517

fiRuiz de Angulo & Torras

expected considerations smaller average size boxes processed
CCS approach.
also studied automatic generation classes resulting bisecting
n-cube analyzed numerical properties. algorithm generating classes
powerful, eliminating convenience pre-calculated table. numerical
analysis classes revealed average number symmetries class representatives tends quickly n number variables, n, grows. good news,
since n maximum number symmetries attainable single-cycle symmetries
n variables, leading time reductions factor close n. Nevertheless, small n
still significant fraction representatives maximum number
symmetries. Another weakness proposed strategy exponential growth
number classes function n.
problems small large n tackled refined subdivision
initial domain box symmetry classes, left near future work.
also currently approaching extension work deal permutations
problem variables composed several cycles. Another complementary research line
addition constraints search CCS. constraints specific
symmetry class. Finally, extension Branch-and-Bound algorithms nonlinear
optimization could envisaged.

Acknowledgments
extended version work presented CP 2007 (Ruiz de Angulo & Torras, 2007).
authors acknowledge support Generalitat de Catalunya consolidated
Robotics group, Spanish Ministry Science Education, project DPI200760858, Comunitat de Treball dels Pirineus project 2006ITT-10004.

References
Benhamou, F., & Goualard, F. (2000). Universally quantified interval constraints.
Springer-Verlag (Ed.), CP 02: Proceedings 6th International Conference
Principles Practice Constraint Programming, pp. 6782.
Bjorck, G., & Froberg, R. (1991). faster way count solutions inhomogeneous
systems algebraic equations, applications cyclic n-roots. J. Symb. Comput.,
12 (3), 329336.
Blumenthal, L. (1953). Theory aplications distance geometry. Oxford University
Press.
Cohen, D., Jeavons, P., Jefferson, C., Petrie, K. E., & Smith, B. M. (2006). Symmetry
definitions constraint satisfaction problems. Constraints, 11 (2-3), 115137.
Flener, P., Frisch, A., Hnich, B., Kiziltan, Z., & Miguel, I. (2002). Breaking row column
symmetries matrix models. CP 02: Proceedings 8th International Conference Principles Practice Constraint Programming, pp. 462476. Springer.
518

fiExploiting Single-Cycle Symmetries

Gent, I. P. (2002). Groups constraints: Symmetry breaking search.
Proceedings CP-02, LNCS 2470, pp. 415430. Springer.
Granvilliers, L., & Benhamou, F. (2006). Realpaver: interval solver using constraint
satisfaction techniques. ACM Trans. Mathematical Software, 32, 138156.
Hentenryck, P. V., Mcallester, D., & Kapur, D. (1997). Solving polynomial systems using
branch prune approach. SIAM Journal Numerical Analysis, 34, 797827.
Jermann, C., & Trombettoni, G. (2003). Inter-block backtracking : Exploiting structure continuous csps. In: 2nd International Workshop Global Constrained
Optimization Constraint Satisfaction, pp. 1530. Springer.
Jermann, C. (2008). Personal communication..
Meseguer, P., & Torras, C. (2001). Exploiting symmetries within constraint satisfaction
search. Artif. Intell., 129 (1-2), 133163.
Polya, G., & Read, R. (1987). Combinatorial enumeration groups, graphs chemical
compounds. Springer-Verlag.
Porta, J. M., Ros, L., Thomas, F., Corcho, F., Canto, J., & Perez, J. (2008). Complete
maps molecular loop conformational spaces. Journal Computational Chemistry,
29 (1), 144155.
Porta, J. M., Ros, L., Thomas, F., & Torras, C. (2005). branch-and-prune solver
distance constraints. IEEE Trans. Robotics, 21, 176187.
Puget, J.-F. (2005). Symmetry breaking revisited. Constraints, 10 (1), 2346.
Ruiz de Angulo, V., & Torras, C. (2007). Exploiting single-cycle symmetries branchand-prune algorithms. CP 07: Proceedings 13th International Conference
Principles Practice Constraint Programming, pp. 864871.
Sam-haroud, D., & Faltings, B. (1996). Consistency techniques continuous constraints.
Constraints, 1, 85118.
Sherbrooke E. C., P. N. M. (1993). Computation solution nonlinear polynomial
systems. Computer Aided Geometric Design, 10, 379405.
Vu, X.-H., Silaghi, M., Sam-Haroud, D., & Faltings, B. (2005). Branch-and-prune search
strategies numerical constraint solving. Tech. rep. LIA-Report 7, Swiss Federal
Institute Technology (EPFL).

519

fiJournal Artificial Intelligence Research 34 (2009) 1-25

Submitted 04/08; published 1/09

Interactive Policy Learning
Confidence-Based Autonomy
Sonia Chernova
Manuela Veloso

soniac@cs.cmu.edu
veloso@cs.cmu.edu

Computer Science Dept.
Carnegie Mellon University
Pittsburgh, PA USA

Abstract
present Confidence-Based Autonomy (CBA), interactive algorithm policy
learning demonstration. CBA algorithm consists two components take
advantage complimentary abilities humans computer agents. first component, Confident Execution, enables agent identify states demonstration
required, request demonstration human teacher learn policy based
acquired data. algorithm selects demonstrations based measure action
selection confidence, results show using Confident Execution agent requires fewer demonstrations learn policy demonstrations selected
human teacher. second algorithmic component, Corrective Demonstration, enables
teacher correct mistakes made agent additional demonstrations
order improve policy future task performance. CBA individual components compared evaluated complex simulated driving domain. complete
CBA algorithm results best overall learning performance, successfully reproducing
behavior teacher balancing tradeoff number demonstrations
number incorrect actions learning.

1. Introduction
Learning demonstration growing area artificial intelligence research explores
techniques programming autonomous agents demonstrating desired behavior
task. demonstration-based approaches, teacher, typically human, shows agent
perform task. agent records demonstrations sequences stateaction pairs, learns policy reproduces observed behavior.
Many learning demonstration approaches inspired way humans animals
teach other, aiming provide intuitive method transfer human task knowledge
autonomous systems. Compared exploration-based methods, demonstration learning
often reduces learning time eliminates frequently difficult task defining
detailed reward function (Smart, 2002; Schaal, 1997).
article, present interactive demonstration learning algorithm, ConfidenceBased Autonomy (CBA), enables agent learn policy interaction
human teacher. learning approach, agent begins initial knowledge
learns policy incrementally demonstrations acquired practices task.
demonstration consists training point representing correct action performed
particular state. agents state represented using n-dimensional feature vector
c
2009
AI Access Foundation. rights reserved.

fiChernova & Veloso

composed continuous discrete values. agents actions bound
finite set action primitives, basic actions combined together perform
overall task. Given sequence demonstrations (si , ai ), state si teacherselected action ai A, goal agent learn imitate teachers behavior
generalizing demonstrations learning policy mapping possible
states actions A.
method gathering demonstrations heart demonstration learning
algorithms. CBA performs function two algorithmic components: Confident
Execution, enables agent select demonstrations real time interacts
environment using automatically calculated confidence thresholds, Corrective Demonstration, enables teacher improve learned policy correct
mistakes additional demonstrations. complete Confidence-Based Autonomy
algorithm provides fast intuitive method policy learning, incorporating shared
decision making learner teacher. experimental evaluation,
highlight strengths learning components compare learning performance
five different demonstration selection techniques. results indicate complex
domain, Confident Execution algorithm reduces number demonstrations required
learn task compared demonstration selection performed human teacher.
Additionally, find teachers ability correct mistakes performed agent
critical optimizing policy performance.
Section 2, discuss related work learning demonstration. present
overview complete Confidence-Based Autonomy learning algorithm Section 3,
followed detailed descriptions Confident Execution Corrective Demonstration
components Sections 4 5, respectively. Section 6, present experimental
evaluation complete algorithm components complex simulated driving
domain. Section 7 presents summary discussion possible extensions work.

2. Related Work
wide variety algorithms policy learning demonstration proposed
within machine learning robotics communities. Within context reinforcement
learning (Sutton & Barto, 1998), demonstration viewed source reliable
information used accelerate learning process. number approaches
taking advantage information developed, deriving modifying
reward function based demonstrations (Thomaz & Breazeal, 2006; Abbeel & Ng,
2004; Papudesi, 2002; Atkeson & Schaal, 1997), using demonstration experiences
prime agents value function model (Takahashi, Hikita, & Asada, 2004; Price &
Boutilier, 2003; Smart, 2002; Schaal, 1997).
Demonstration also coupled supervised learning algorithms policy
learning, including Locally Weighted Regression low level skill acquisition (Grollman &
Jenkins, 2007; Browning, Xu, & Veloso, 2004; Smart, 2002), Bayesian networks high level
behaviors (Lockerd & Breazeal, 2004; Inamura, Inaba, & Inoue, 1999), k-nearest
neighbors algorithm fast-paced games robot navigation tasks (Saunders, Nehaniv,
& Dautenhahn, 2006; Bentivegna, Ude, Atkeson, & Cheng, 2004). recent survey covers
2

fiInteractive Policy Learning Confidence-Based Autonomy

demonstration learning algorithms detail (Argall, Chernova, Browning,
& Veloso, 2009).
addition policy learning demonstration, several areas research also
explored algorithms demonstration selection. Within machine learning research, active
learning (Blum & Langley, 1997; Cohn, Atlas, & Ladner, 1994) enables learner query
expert obtain labels unlabeled training examples. Aimed domains
large quantity data available labeling expensive, active learning directs
expert label informative examples goal minimizing number
queries. context reinforcement learning, Ask Help framework enables
agent request advice agents confused action take,
event characterized relatively equal quality estimates possible actions given
state (Clouse, 1996). Similarly motivated techniques used robotics identify
situations robot request demonstration teacher (Grollman &
Jenkins, 2007; Lockerd & Breazeal, 2004; Nicolescu, 2003; Inamura et al., 1999).
closely related work Dogged Learning algorithm (Grollman & Jenkins, 2007),
confidence-based learning approach teaching low-level robotic skills. algorithm,
robot indicates teacher certainty performing various elements task.
teacher may choose provide additional demonstrations based feedback.
similarly motivated, work differs Dogged Learning algorithm number
ways, important use classification instead regression policy
learning, algorithms ability adjust confidence threshold data instead
using fixed value.

3. Confidence-Based Autonomy Overview
Confence-Based Autonomy algorithm enables human user train task policy
demonstration. algorithm consists two components:
Confident Execution (CE): algorithm enables agent learn policy based
demonstrations obtained regulating autonomy requesting help
teacher. Demonstrations selected based automatically calculated classification
confidence thresholds.
Corrective Demonstration (CD): algorithm enables teacher improve
learned policy correcting mistakes made agent supplementary
demonstrations.
Figure 1 shows interaction components. Using Confident Execution algorithm, agent selects states demonstration real time interacts
environment, targeting states unfamiliar current policy action
uncertain. timestep, algorithm evaluates agents current state actively
decides autonomously executing action selected policy requesting
additional demonstration human teacher.
assume underlying model agents task MDP. agents policy
represented learned using supervised learning based training data acquired
demonstrations. Confidence-Based Autonomy combined supervised
3

fiChernova & Veloso

Figure 1: Confidence-Based Autonomy learning process.
learning algorithm provides measure confidence classification. policy
represented classifier C : (a, c, db), trained using state vectors si inputs,
actions ai labels. classification query, model returns model-selected
action A, action selection confidence c, decision boundary db highest
confidence query (e.g. Gaussian component GMMs).
effectively select demonstrations, learner must able autonomously identify
situations demonstration provide useful information improve policy.
Confident Execution selects agent autonomy request demonstration based
measure action-selection confidence c returned classifier. Given current
state learner, algorithm queries policy obtain confidence selecting
action state, regulates autonomy based confidence. learner
executes returned action ap confidence c threshold , determined
decision boundary classifier, db. Confidence threshold indicates
agent uncertain action take, seeks help teacher
form demonstration. Receiving additional demonstration, ad , low confidence
situation improves policy, leading increased confidence, therefore autonomy,
future similar states. training data becomes available, quality policy
improves autonomy agent increases entire task performed
without help teacher. Section 4 compare two methods using classification
confidence select states demonstration.
Using Confident Execution algorithm, agent incrementally acquires demonstrations explores environment. practices task, agent uses policy
learned point make decisions demonstration autonomous execution. However, relying policy learning complete, algorithm likely
4

fiInteractive Policy Learning Confidence-Based Autonomy

make mistakes due factors overgeneralization classifier incomplete
data area state space. address problem article introduces
second algorithmic component, Corrective Demonstration, allows teacher provide corrections agents mistakes. Using method, incorrect action
observed, teacher provides additional demonstration agent indicating
action executed place. addition indicating wrong action selected, method also provides algorithm correct action perform
place, ac . correction therefore informative negative reinforcement
punishment techniques common algorithms, leading agent learn quickly
mistakes.
Together, Confident Execution Corrective Demonstration form interactive learning algorithm learner human teacher play complimentary roles. learner
able identify states demonstration required; fact, results show
algorithm able better human teacher due differences perception
representation abilities. teacher, hand, possesses expert knowledge
overall task, applied performing demonstrations spotting execution
mistakes. function agent cannot perform yet learned
desired behavior. way, Confidence-Based Autonomy takes advantage
complimentary abilities human agent. Sections 4 5 present Confident
Execution Corrective Demonstration components detail.

4. Confident Execution Algorithm
Confident Execution policy learning algorithm agent must select demonstration examples, real time, interacts environment. timestep,
algorithm uses thresholds determine whether demonstration correct action
agents current state provide useful information improve agents policy.
demonstration required, agent requests help teacher, updates policy based resulting action label. Otherwise agent continues perform task
autonomously based policy.
two distinct situations agent requires help teacher,
unfamiliar states ambiguous states. unfamiliar state occurs agent encounters situation significantly different previously demonstrated state,
represented outlying points Figure 2. want demonstrate
every possible state, therefore need model generalize, would like prevent
over-generalization truly different states.
Ambiguous states occur agent unable select multiple actions
certainty. situation result demonstrations different actions similar
states make accurate classification impossible, region overlapping data classes
Figure 2. cases, additional demonstrations may help disambiguate situation.
goal Confident Execution algorithm divide state space regions
high confidence (autonomous execution) low confidence (demonstration)
unfamiliar ambiguous regions fall low confidence areas. Given world state,
two evaluation criteria used select demonstration autonomy:
5

fiChernova & Veloso

Nearest Neighbor distance: Given = N earestN eighbor(s), distance
current state nearest (most similar) training datapoint, agent may act
autonomously distance threshold dist .
Classification confidence: Given c, classification confidence current state,
agent may act autonomously value c confidence threshold
conf .
methods calculating thresholds dist conf presented Sections 4.1 4.2.
section, continue discussion Confident Execution algorithm assuming
values given.
Algorithm 1 presents details Confident Execution algorithm. assume
preexisting knowledge task, initialize algorithm empty set
training points . Since classifier initially available, threshold conf initialized
infinity ensure agent controlled demonstration initial
learning stage. Distance threshold dist initialized 0.
main learning algorithm consists loop (lines 4-20), iteration
represents single timestep. behavior algorithm determined whether
agent currently executing action. action progress, algorithm performs
additional computation timestep (line 20). action complete,
algorithm evaluates state determine next action perform (lines 6-18).
Evaluation begins obtaining agents current state environment (line 6).
information used calculate nearest neighbor distance query
learned classifier C obtain policy action ap confidence c. values
compared confidence distance thresholds decide demonstration
autonomy (line 9). similar states previously observed, learned model
confident selection, algorithm finishes timestep initiating autonomous

Figure 2: Outlying points regions overlapping data classes represent unfamiliar
ambiguous state regions, respectively.

6

fiInteractive Policy Learning Confidence-Based Autonomy

Algorithm 1 Confident Execution Algorithm
1: {}
2: conf inf
3: dist 0
4: true
5:
actionComplete
6:
GetSensorData()
7:
= NearestNeighbor(s)
8:
(ap , c, db) C(s)
9:
c > conf < dist
10:
ExecuteAction(ap )
11:
else
12:
RequestDemonstration()
13:
ad GetTeacherAction()
14:
ad 6= N U
15:
{(s, ad )}
16:
C UpdateClassifier(T )
17:
(conf , dist ) UpdateThresholds()
18:
ExecuteAction(ad )
19:
else
20:
//do nothing

execution policy selected action ap (line 10). Otherwise initiates request
teacher demonstration (lines 12-18).
agent requests demonstration pausing indicating teacher
demonstration required. Note assume domain allows agent pause
execution. Following demonstration request, algorithm checks whether demonstration performed (lines 13-14). teachers response available, new training
datapoint consisting current state corresponding demonstrated action ad
added training set (line 15). model classifier retrained, threshold
values updated, executing teacher selected action (lines 16-18).
teachers response immediately available, timestep terminates
whole process repeated next iteration. agent senses state, performs
threshold comparison checks demonstration. non-blocking mechanism
enables agent wait demonstration teacher without losing awareness
surroundings. cases agents environment dynamic, maintaining
date information important state may change time initial
request demonstration. Associating action label agents recent
state, one teacher likely responding to, therefore critical learning
accurate model. Additionally, changes environment result agent attaining
high confidence state without actions own. cases, autonomous execution
task automatically resumed. summary, demonstration request made,
actions taken agent either demonstration received
teacher, changes environment result high confidence state.
7

fiChernova & Veloso

Using approach, Confident Execution enables agent incrementally acquire
demonstrations representing desired behavior. datapoints acquired, fewer
states distant training data encountered, performance classification
confidence improve, autonomy agent increases. Task learning complete
agent able repeatedly perform desired behavior without requesting demonstrations. following sections present methods calculating distance
confidence thresholds.
4.1 Distance Threshold
purpose distance threshold evaluate similarity agents
current state previous demonstrations. evaluation metric uses nearest neighbor
distance, defined Euclidian distance query closest point
dataset. agent state query, obtain nearest neighbor distance representing
similar previously demonstrated state. value compared distance
threshold dist .
value distance threshold dist calculated function average nearest
neighbor distance across dataset demonstrations. Evaluating average similarity
states provides algorithm domain-independent method detecting
outliers, points unusually far previously encountered states. trials article,
value dist set three times average nearest neighbor distance across
dataset.
alternate method detecting outliers would use classification confidence
request demonstrations low confidence states. However, situations arise
confidence directly correlated state similarity. example, many classifiers
set datapoints encircling empty region, similar shape donut, would result
highest classification confidence associated empty center region far
previous demonstrations. Distance provides reliable prediction similarity, even
cases.
4.2 Confidence Threshold
confidence threshold used select regions uncertainty points
multiple classes overlap. agents perspective, points regions represent
demonstrations two distinct actions states appear similar, difficult
distinguish based sensor data. problem frequently arises demonstration
learning number reasons, teachers inability demonstrate task
consistently, noise sensor readings, inconsistency agents
teachers sensing abilities. would like set confidence threshold value
prevents either model classifying overlapping region high confidence1 .
following section discuss use limitations single fixed threshold value.
present algorithm using multiple adjustable thresholds Section 4.2.2.
1. See Section 7.2 discussion data regions.

8

fiInteractive Policy Learning Confidence-Based Autonomy

(a)

(b)

(c)

Figure 3: Examples fixed threshold failure cases: (a) Fully separable data classes
overly conservative threshold value (b) Overlapping data classes overly
general threshold value (c) Data classes different distributions common
threshold value

4.2.1 Single Fixed Threshold
single, fixed confidence threshold value provides simple mechanism approximate
high confidence regions state space. Previous algorithms utilizing classification confidence threshold behavior arbitration used manually-selected single threshold
value (Inamura et al., 1999; Lockerd & Breazeal, 2004; Grollman & Jenkins, 2007). However, choosing appropriate value difficult constantly changing dataset
model. Figure 3 presents examples three frequently encountered problems.
Figure 3(a) presents case two action classes distinct fully separable.
model trained dataset able classify points complete accuracy, without
misclassifications. However, current threshold value classifies 72% points
high confidence, marking remaining 28% points uncertain. case,
lower threshold value would preferred would allow model generalize
freely. resulting larger high confidence region would reduce number redundant
demonstrations without increasing classification error rate either data class.
Figure 3(b) presents example opposite case, stricter threshold value
would preferred. example data classes overlap, resulting middle region
points cannot classified high accuracy. higher threshold value would
prevent classification points region either data class, initiating instead
request demonstration would allow teacher disambiguate situation.
Figure 3(c) presents case datapoints two data classes
different distributions. fixed threshold value appropriate left class, 42%
points right class labeled low confidence.
Classification complex multi-class data depends upon multiple decision boundaries.
Using value decision boundaries exacerbate problems highlighted
above, single value often cannot found constrains model classification
areas allowing generalization others. resulting effect agent requests
many demonstrations things already knows, demonstrations
unlearned behavior. address problem, present algorithm calculating
unique threshold value decision boundary.
9

fiChernova & Veloso

(a)

(b)

(c)

Figure 4: Autonomy threshold calculation: (a) Example dataset, highlighted overlapping region (b) Learned decision boundary, misclassified points marked
confidence values (c) Learned threshold values data class, low confidence region containing overlapping points remains center.

4.2.2 Multiple Adjustable Thresholds
section, contribute algorithm calculating confidence threshold
decision boundary, customized unique distribution points. analysis,
assume able query classifier obtain confidence score representing
likelihood particular input belongs within specified decision boundary.
algorithm begins dividing dataset training test set training
classifier C. resulting learned model used classify withheld test set,
correct action labels known. algorithm calculates unique confidence
threshold decision boundary based confidence scores misclassified points.
Given confidence scores set points mistakenly classified decision boundary,
assume future classifications confidences values likely
misclassifications well. threshold therefore calculated function
confidence scores.
Specifically, define classified point tuple (o, a, , c), original
observation, demonstrated action label, model-selected action, c
model action confidence. Let Mi = {(o, ai , , c)|am 6= ai } set points
mistakenly classified decision boundary i. confidence threshold
Pvalue set
Mi

c

average classification confidence misclassified points: conf = |Mi | . take
average avoid overfitting noisy data. values, based maximum standard
deviation, used conservative estimate required. threshold value 0
indicates misclassifications occurred model able generalize freely.
Figure 4 presents example threshold calculation process. Figure 4(a) presents
small sample dataset, rectangular box figure highlights region state
space points classes overlap. Figure 4(b) shows learned decision
boundary (in case SVM) separating two data classes. Six misclassified points
marked (mis-)classification confidences returned model. Misclassified points
side decision boundary used calculate respective confidence
thresholds. Figure 4(c) shows confidence threshold lines values based
10

fiInteractive Policy Learning Confidence-Based Autonomy

(a)

(b)

(c)

Figure 5: Multiple adjustable thresholds applied failure cases shown Figure 3.

calculations. resulting low confidence region middle image captures
noisy datapoints.
Given multi-threshold approach, classification new points performed first
selecting action class highest confidence query. comparison
line 9 Algorithm 1 performed using threshold decision boundary
highest confidence query. Using method, threshold value
likely decision boundary represent point used decide demonstration
autonomy.
Figure 5 shows example failure cases discussed Section 4.2.1 addressed
multi-thresholded approach. Customizing threshold value unique data
distribution enables algorithm correctly classify 100% points Figures 5(a)
(c). Since misclassifications, model generalizes freely examples.
dataset Figure 5(b), perfect classification possible, confidence
thresholds set overlapping region falls low confidence area.
example uses Gaussian mixture model, elliptical confidence gradient around
mean results large low confidence area even far overlapping region.
classification methods, Support Vector Machines, drawback.
presented multi-threshold approach algorithm independent, Figure 6 presents
classification results four different classification methods: Gaussian mixture models, random forests (RF), Support Vector Machine quadratic kernel, SVM radial
basis function (RBF) kernel. table summarizes classification performance
algorithm lists threshold values models.
Algorithm
GMM
RF
SVM quad.
SVM RBF

Correct-Misclas.-Unclass.
98.6% 0.4% 1.0%
99.1% 0.1% 0.8%
98.5% 0.1% 1.4%
98.9% 0.1% 1.0%

Thresholds
(0, 0, 0.012)
(0.14, -0.355)
(335.33, -68.77)
(0.825, -0.268)

Table 1: Classifier comparison.

11

fiChernova & Veloso

(a) Gaussian mixture model

(b) Random Forest

(c) SVM (quadratic)

(d) SVM (RBF)

Figure 6: Classification dataset high low confidence regions using different classification methods.

5. Corrective Teacher Demonstration
presented Confident Execution algorithm enables agent identify unfamiliar
ambiguous states prevents autonomous execution situations. However, states
incorrect action selected high confidence autonomous execution
still occur, typically due over-generalization classifier. article present
Corrective Demonstration algorithm which, coupled Confident Execution, enables
teacher correct mistakes made agent. Algorithm 2 combines Corrective
Demonstration (lines denoted ) Confident Execution presents complete
Confidence-Based Autonomy algorithm.
Corrective Demonstration technique comes play time agent executes
autonomous action. action selected autonomous execution, algorithm
records agents state led decision saves value within variable sc
(line 11). execution autonomously selected action, algorithm checks
teacher demonstration every timestep (lines 22-23). corrective demonstration
made, new training datapoint consisting recorded demonstration state sc
corrective action ac added training set (line 24). classifier thresholds
retrained using new information.
12

fiInteractive Policy Learning Confidence-Based Autonomy

Algorithm 2 Confidence-Based Autonomy algorithm: Confident Execution Corrective
Demonstration
1: {}
2: conf inf
3: dist 0
4: true
5:
GetSensorData()
6:
actionComplete
7:
(ap , c, db) C(s)
8:
= NearestNeighbor(s)
9:
c > conf < dist
10:
ExecuteAction(ap )
11:
sc

12:
else
13:
RequestDemonstration()
14:
ad GetTeacherAction()
15:
ad 6= N U
16:
{(s, ad )}
17:
C UpdateClassifier(T )
18:
(conf , dist ) UpdateThresholds()
19:
ExecuteAction(ad )
20:
else
21:
autonomousAction

22:
ac GetTeacherAction()

23:
ac 6= N U

24:
{(sc , ac )}

25:
C UpdateClassifier(T )

26:
(conf , dist ) UpdateThresholds()


Using algorithm, teacher observes autonomous execution agent
corrects incorrect actions. Unlike previous demonstration technique
agent given next action perform, correction performed relation
agents previous state mistake made. example, observing
driving agent approaching close behind another car, teacher able indicate
instead continuing drive forward, agent merging
passing lane. way, addition indicating wrong action performed,
Corrective Demonstration also provides algorithm action
performed place. technique effective negative reinforcement,
punishment, techniques common algorithms, leading agent learn quickly
mistakes.
13

fiChernova & Veloso

Figure 7: Screenshot driving simulator. agent, black car currently
center lane, drives fixed speed must navigate around cars avoid
collisions. road consists five lanes: three traffic lanes two shoulder
lanes.

6. Evaluation Comparison
section present evaluation comparison complete Confidence-Based
Autonomy algorithm components simulated car driving domain (Abbeel & Ng,
2004), shown Figure 7.
6.1 Domain Description
driving domain, agent represents car driving busy highway.
learners car travels fixed speed 60 mph, cars move lanes
predetermined speeds 20 40 mph. road three normal lanes
shoulder lane sides; agent allowed drive shoulder pass
cars, cannot go off-road. Since learner cannot change speed, must
navigate cars use shoulder lanes avoid collision. agent
limited three actions: remaining current lane, shifting one lane left
right current position (A = {forward,left,right}). teacher demonstrates task
keyboard interface. simulator framerate 5 fps paused
demonstration requests.
agents state represented by: = {l, dl , dc , dr }. State feature l discrete value
symbolizing agents current lane number. remaining three features, denoted
letter d, represent distance nearest car three driving lanes
(left, center right). distance features continuously valued [-25,25] range;
note nearest car lane behind agent. Distance measurements
corrupted noise create complex testing environment. agents policy
relearned time 10 new demonstrations acquired.
driving domain presents varied challenging environment; car distances
discretized rounding nearest integer value, domain would contain
600,000 possible states. Due complexity domain, agent requires large
14

fiInteractive Policy Learning Confidence-Based Autonomy

number demonstrations initialize classifier, resulting nearly constant demonstration requests early training process. simplify task teacher, add
short 300 datapoint, approximately 60 second, non-interactive driving demonstration
session initialize learning process. learning stage required, simplifies task teacher continuous demonstration preferred frequent
pauses demonstration requests.
performance learning algorithm evaluated time 100 new demonstrations acquired. evaluation, agent drove 1000 timesteps road
segment fixed consistent traffic pattern. road segment used
training, instead algorithm trained using randomly generated car traffic pattern.
Since algorithm aims imitate behavior expert, true reward function
exists evaluate performance given policy. present two domain-specific evaluation metrics capture key characteristics driving task. first evaluation
metric agents lane preference, proportion time agent spends
lane course trial. metric provides estimate similarity driving
styles. Since demonstrated behavior attempts navigate domain without collisions,
second evaluation metric number collisions caused agent. Collisions
measured percentage total timesteps agent spends contact
another car. Always driving straight colliding every car middle lane results
30% collision rate.
6.2 Experimental Results
present performance evaluation comparison following demonstration
selection techniques:
G Teacher-guided, demonstrations selected teacher without confidence feedback algorithm without ability perform retroactive
corrections
CES Confident Execution, demonstrations selected agent using single
fixed confidence threshold
CEM Confident Execution, demonstrations selected agent using multiple
adjustable confidence thresholds
CD Corrective Demonstration, demonstrations selected teacher performed corrections response mistakes made agent
CBA complete Confidence-Based Autonomy algorithm combining Confident
Execution using multiple adjustable confidence thresholds Corrective Demonstration
demonstration selection method, underlying policy agent learned
using multiple Gaussian mixture models, one action class (Chernova & Veloso,
2007). Videos driving task available www.cs.cmu.edu/soniac.
Figure 8 presents performance results five algorithms respect
defined lane preference collision metrics. describe discuss elements
15

fiChernova & Veloso

Figure 8: Evaluation agents driving performance 100-demonstration intervals
five demonstration selection methods. bar graphs indicate
percentage time agent spent road lane. Values bar
indicate percentage collision timesteps accrued evaluation trial.
teacher performance bar right figure shows teachers driving
lane preference collision rate evaluation road segment. goal
algorithm achieve performance similar teacher.

16

fiInteractive Policy Learning Confidence-Based Autonomy

figure detail following sections. evaluation, figure presents bar
representing composite graph showing percentage time spent agent
lane. value bar indicates number demonstrations upon
evaluated policy based. value bar indicates percentage incurred
collisions evaluation.
bar right figure shows performance teacher evaluation road segment. evaluation indicates teacher prefers drive center
left lanes, followed preference left shoulder, right shoulder right lane.
teacher also successfully avoids collisions, resulting collision rate 0%. goal
learning algorithm achieve driving lane pattern similar teacher
also without collisions. Note that, described previous section, policy learning
initialized 300-demonstration dataset algorithms. initialization
results identical performance across algorithms initial learning segment.
6.2.1 G Demonstration Selection
top row Figure 8 summarizes performance teacher-guided demonstration
selection approach. approach, teacher performed training alternating
observing performance agent selecting demonstrations that, opinion,
would improve driving performance. teacher selected training examples without
receiving feedback action selection confidence, without ability provide
corrective demonstrations incorrect actions already executed agent.
Instead, teacher required anticipate data would improve policy.
training process terminated teacher saw improvement agent
performance.
Figure 8 shows results agents performance evaluations 100-demonstration
intervals throughout learning process. similarity driving lane preference
agent improves slowly course learning, significant fluctuations.
example, 500 demonstrations, agents preference drive empty left
shoulder, thereby incurring collisions. One hundred demonstrations later, policy
shifted prefer center lane. However, agent yet learned avoid
cars, resulting 38.8% collision rate. policy stabilizes approximately 1100
demonstrations, representing driving style similar teacher, small
number collisions. Without confidence feedback agent, difficult
teacher select exact termination point learning. Training continued until,
1300 demonstrations, learners policy showed little improvement. final policy
resulted lane preference similar expert, 2.7% collision
rate.
6.2.2 CES Demonstration Selection
second row Figure 8 presents results Confident Execution algorithm
single autonomy threshold. demonstration selection approach, demonstrations
selected agent learning terminated agent stopped requesting
demonstrations performed actions autonomously. autonomy threshold value
17

fiChernova & Veloso

selected hand evaluated multiple performance trials. Results best fixed
threshold presented.
Compared teacher-guided approach, policy learned using CES algorithm
stabilizes quickly, achieving performance similar teachers 700 demonstrations. number collisions low persistent, even agent gains full
confidence stops requesting demonstrations 1008 demonstrations. final lane
preference similar expert, collision rate 3.8%.
6.2.3 CEM Demonstration Selection
third row Figure 8 presents results Confident Execution algorithm
multiple autonomy thresholds, calculated using algorithm presented Section 4.2.2. demonstration selection methods, CEM required fewest number
demonstrations learn task, completing learning 504 demonstrations.
result indicates use multiple adjustable thresholds successfully focuses demonstration selection informative areas state space greatly reducing number
redundant demonstrations. Throughout learning process, number Gaussian
components within model varied 9 41. large variation highlights
importance automating threshold calculation process, since hand-selecting individual
thresholds component would impractical. lane preference final policy
similar expert. However, agent still maintained small collision
rate 1.9%.
6.2.4 CD Demonstration Selection
evaluation first three algorithms highlights difficulty driving problem.
approaches able select demonstrations resulted policy
mimics overall driving style teacher. However, policies resulted
small number collisions, typically occurred agent merged close
another vehicle touched bumper. mistakes difficult correct using
techniques evaluated far. Even within teacher guided demonstration selection
method, human teacher full control demonstration training data,
time collision observed incorrect decision already made
algorithm. Instead, retroactive demonstration required correct already made
mistakes, Corrective Demonstration algorithm.
fourth row Figure 8 present evaluation demonstration selection
using Corrective Demonstration algorithm. approach, demonstrations
selected teacher corrections response mistakes made agent.
Behavior corrected teacher included collisions, well incorrect lane preference
(e.g. always driving shoulder) rapid oscillations lanes. enable
teacher accurately perform corrections, simulation slowed 5 2 frames
per second. Learning terminated agent required corrections.
shown Figure 8, complete training process using Corrective Demonstration took 547
demonstrations, achieving final policy correctly imitates teachers driving style
0% collision rate. following section, discuss performance compares
complete CBA algorithm.
18

fiInteractive Policy Learning Confidence-Based Autonomy

6.2.5 CBA Demonstration Selection
final row Figure 8 presents evaluation complete Confidence-Based Autonomy algorithm, combines CEM CD. Using approach, learning complete
agent longer requests demonstrations able perform driving task
without collisions. Using CBA agent required total 703 demonstrations learn
task, successfully learning navigate highway without collisions.
analyze impact two CBA learning components comparing number
distribution demonstrations acquired algorithm learning process.
section refer learning components CBA CBA-CE CBA-CD
differentiate algorithm evaluations presented previous sections. Note
behavior Confident Execution component dependent upon method used
set autonomy thresholds. evaluation use multiple adjustable thresholds
calculated average value misclassified points.
Figure 9(a), datapoint along x-axis represents number demonstrations
requested using CBA-CE (top) initiated teacher using CBA-CD (bottom)
100-timestep interval, approximately 40 seconds simulator runtime (excluding pauses
demonstration requests). Since first three 100-demonstration timesteps consist entirely non-interactive demonstration, values timesteps 100 and, due
scaling, exceed bounds graph. Figure 9(b) shows cumulative number
demonstrations component, total, grows respect training time.
complete training process lasts approximately hour half.
Analysis graphs shows demonstrations occur early training
process. Importantly, Confident Execution accounts 83% total number demon-

(a)

(b)

Figure 9: (a) Timeline showing number demonstrations initiated agent
Confident Execution (top) initiated teacher Corrective Demonstrations (bottom) changes course training. (b)
cumulative number demonstrations acquired component, total,
time.

19

fiChernova & Veloso

strations, indicating agent guides learning. demonstration requests occur first minutes training agent encounters
many novel states classification confidence remains low. agent requires
corrections stage many mistakes prevented requesting demonstration instead performing low confidence action. Corrective Demonstration plays
greatest role towards end training process, accounts 73% final
100 demonstrations. stage learning agents action selection confidence
high enough rarely asks demonstrations. policy already closely imitates
teachers driving style small number collisions remain. Corrective Demonstration
enables teacher fine-tune policy eliminate collisions. result highlights
importance Corrective Demonstration, whether alone conjunction another
selection technique, optimizing policy performance.
CBA achieves similar final performance compared CD algorithm evaluated
previous section, requires approximately 150 additional demonstrations learn
policy. additional demonstrations attributed Confident Execution demonstration requests served increase classification confidence change
outcome agents action. Viewed another way, datapoints correspond states
agent would performed correct action even asked
demonstration. result appears allowing agent make mistakes
correcting fact, done CD evaluation, may best demonstration
selection approach respect performance metrics defined overall
number demonstrations.
However, eliminating ability request demonstrations utilizing retroactive correction several drawbacks, namely requiring constant full attention
teacher, and, importantly, requiring agent make many mistakes learns
correct policy. comparison, CBA algorithm enables agent request demonstrations low confidence states, thereby avoiding many incorrect actions. original
lane preference collision metrics take difference account focus
final policy performance agent.
evaluate difference algorithms, additionally examine number
collisions agent incurs course learning. Using CD algorithm,
agent incurs 48% collisions (278 vs. 188) training using CBA.
Therefore, allowing agent request demonstrations low-confidence states,
CBA algorithm requires slightly greater number demonstrations greatly reducing
number incorrect actions performed learning. reduction number
action errors significant due importance many learning domains, especially
robotic applications errors may pose dangers system.
summary, evaluation shown ability retroactively correct mistakes
crucial optimizing policy eliminating collisions. best performance
achieved Corrective Demonstration Confidence-Based Autonomy methods,
CD requiring fewer demonstrations incurring greater number collisions
training. choice CD CBA therefore viewed tradeoff
number demonstrations frequency undesired actions training.
fact, CD special case CBA autonomy threshold set classify
points high confidence. Adjusting selectiveness CBA autonomy thresholds
20

fiInteractive Policy Learning Confidence-Based Autonomy

could, therefore, provide user sliding control mechanism effects agents
tendency perform autonomous actions versus demonstration requests. Importantly,
note overall number demonstrations required either approach less
teacher-guided method tiny fraction overall state space.

7. Discussion
section, discuss several promising directions future work, well number
existing extensions presented learning methods.
7.1 Evaluation Non-Technical Users
presented demonstration learning algorithm provides fast intuitive method
programming adapting behavior autonomous agents. believe general
representation classifier-independent approach makes CBA usable wide range
applications. One particular application interest use demonstration learning
enable non-technical users program autonomous agents. believe CBA would
highly suitable application assume teacher technical
knowledge policy learning, requiring teacher expert task.
results presented article obtained using single teacher, one
authors. Additional studies could evaluate algorithm usability performance wider
user base, non-programmers particular.
7.2 Representation Action Choices
Demonstration-based learning provides natural intuitive interface transferring human task knowledge autonomous agents. However, operating rich environments,
agents inevitably face situations multiple actions equivalently applicable.
example, agent encounters obstacle directly path option moving
left right avoid it. surrounding space empty, directions equally valid
performing desired task. Human demonstrators faced choice equivalent
actions typically perform demonstrations consistently, instead selecting among
applicable actions arbitrarily time choice encountered. result, training
data obtained agent lacks consistency, identical, nearly identical, states
associated different actions. presented CBA algorithm, inconsistent
demonstrations would result persistent region low confidence, leading agent
repeatedly request demonstrations within inconsistent domain region. successfully extended CBA identify regions state space conflicting demonstrations
represent choice multiple actions explicitly within agents policy (Chernova & Veloso, 2008a).
7.3 Improvement Beyond Teacher Performance
policy learned Confidence-Based Autonomy algorithm inherently limited
quality demonstrations provided human teacher. Assuming
teacher expert task, approach aims imitate behavior teacher.
However, many domains teacher demonstrations may suboptimal limited
21

fiChernova & Veloso

human ability. Several demonstration learning approaches developed enable
agent learn experiences addition demonstrations, thereby improving
performance beyond abilities teacher (Stolle & Atkeson, 2007; Smart, 2002).
Extending CBA algorithm include similar capability remains promising direction
future work. Possible approaches include incorporating high-level feedback (Argall,
Browning, & Veloso, 2007) reward signal (Thomaz & Breazeal, 2006) teacher,
well filtering noisy inaccurate demonstrations.
7.4 Policy Use Learning
CBA algorithm considers learning complete agent able perform
required behavior, repeatedly correctly, without requesting demonstrations
requiring corrections. policy learning complete, standard procedure
vast majority policy learning algorithms turn learning process freeze
policy. approach also used algorithm, propose
continuing use Confident Execution component may long-term benefits
beyond policy learning. particular, algorithms ability identify anomalous states
may enable agent detect notify user system errors unexpected input.
studies needed evaluate use algorithm, believe
mechanism would provide useful safety feature long-term autonomous operation
negligible cost performing threshold comparison timestep.
7.5 Richer Interaction
presented demonstration learning approach relies limited form interaction agent teacher. agent requests demonstrations teacher,
teacher responds single recommended action. level interaction
typical traditional active learning approaches, fails take full advantage
vast task knowledge teacher possesses. believe extending algorithm
include richer interaction abilities could provide faster intuitive training
method. Many promising directions future research exist area. example,
developing domain-independent dialog exchange agent teacher incorporates clarification questions high level advice could speed learning enable
agent represent high level goals task. ability play back rewind
demonstration sequences would additionally enable teacher agent reexamine
reevaluate past learning experiences.
7.6 Application Single-Robot Multi-Robot Systems
Learning demonstration techniques extensively studied within robotics
community due interactive nature fast learning times. work,
shown CBA algorithm highly effective learning variety single-robot tasks
(Chernova & Veloso, 2007, 2008a).
Furthermore, many complex tasks require collaboration multiple robots.
now, one greatest challenges preventing demonstration learning algorithms
generalizing multi-robot domains problem limited human attention,
22

fiInteractive Policy Learning Confidence-Based Autonomy

fact teacher able pay attention to, interact with, robots
time. Based CBA algorithm, developed first multi-robot
demonstration learning system addresses limited human attention problem
taking advantage fact Confident Execution component CBA prevents
autonomous execution actions low-confidence states (Chernova & Veloso, 2008b).
flexMLfD system utilizes individual instances CBA robot, learner
acquires unique set demonstrations learns individual task policy. preventing
autonomous execution low-confidence states, CBA makes learner robust periods
teacher neglect, allowing multiple robots taught time.

8. Conclusion
article presented Confidence-Based Autonomy, interactive algorithm policy
learning demonstration. Using algorithm, agent incrementally learns
action policy demonstrations acquired practices task. CBA algorithm
contains two methods obtaining demonstrations. Confident Execution component
enables agent select demonstrations real time interacts environment,
using confidence distance thresholds target states unfamiliar
current policy action uncertain. Corrective Demonstration component allows
teacher additionally perform corrective demonstrations incorrect action
selected agent. teacher retroactively provides demonstrations specific error
cases instead attempting anticipate errors ahead time. Combined, techniques
provide fast intuitive approach policy learning, incorporating shared decision
making learner teacher.
Experimentally, used complex simulated driving domain compare five methods
selecting demonstration training data: manual data selection teacher, confidencebased selection using single fixed threshold, confidence-based selection using multiple
automatically calculated thresholds, corrective demonstration, confidence-based selection combined corrective demonstration. Based evaluation, conclude
confidence-based methods able select informative demonstrations
human teacher. single multiple threshold approaches, multiple adjustable
threshold technique required significantly fewer demonstrations focusing onto regions
uncertainty reducing number redundant datapoints. best final policy performance, however, achieved Corrective Demonstration complete ConfidenceBased Autonomy algorithms, achieved lane preference similar
teacher without collisions. Together, demonstration selection algorithms represent
tradeoff number demonstrations frequency undesired actions
training. Corrective Demonstration required slightly fewer demonstrations
learn final policy, compared CBA resulted significant increase number
errors made agent course learning process. CBA algorithm,
therefore, provides best demonstration selection method domains incorrect
actions desirable training process.
23

fiChernova & Veloso

Acknowledgments
research partially sponsored Department Interior, National Business
Center contract no. NBCHD030010 SRI International subcontract no.
03-000211, BBNT Solutions subcontract no. 950008572, via prime Air Force
contract no. SA-8650-06-C-7606. views conclusions contained document
authors interpreted representing official policies,
either expressed implied, sponsoring institution, U.S. government
entity. Additional thanks Paul Rybski making simulation package available.

References
Abbeel, P., & Ng, A. (2004). Apprenticeship learning via inverse reinforcement learning.
Proceedings International Conference Machine Learning, New York, NY,
USA. ACM Press.
Argall, B., Chernova, S., Browning, B., & Veloso, M. (2009). survey robot learning
demonstration. Robotics Autonomous Systems, appear.
Argall, B., Browning, B., & Veloso, M. (2007). Learning demonstration critique human teacher. Second Annual Conference Human-Robot Interactions
(HRI 07), Arlington, Virginia.
Atkeson, C. G., & Schaal, S. (1997). Robot learning demonstration. Proceedings
International Conference Machine Learning, pp. 1220, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Bentivegna, D. C., Ude, A., Atkeson, C. G., & Cheng, G. (2004). Learning act
observation practice. International Journal Humanoid Robotics, 1 (4).
Blum, A. L., & Langley, P. (1997). Selection relevant features examples machine
learning. Artificial Intelligence, 97 (1-2), 245271.
Browning, B., Xu, L., & Veloso, M. (2004). Skill acquisition use dynamicallybalancing soccer robot. Proceedings Nineteenth National Conference Artificial
Intelligence, pp. 599604.
Chernova, S., & Veloso, M. (2007). Confidence-based policy learning demonstration
using gaussian mixture models. Proceedings International Conference
Autonomous Agents Multiagent Systems, pp. 18.
Chernova, S., & Veloso, M. (2008a). Learning equivalent action choices demonstration.
Proceedings International Conference Intelligent Robots Systems,
pp. 12161221.
Chernova, S., & Veloso, M. (2008b). Teaching collaborative multi-robot tasks
demonstration. Proceedings IEEE-RAS International Conference Humanoid Robots.
Clouse, J. A. (1996). integrating apprentice learning reinforcement learning. Ph.D.
thesis, University Massachisetts, Department Computer Science.
Cohn, D., Atlas, L., & Ladner, R. (1994). Improving generalization active learning.
Machine Learning, 15 (2), 201221.
24

fiInteractive Policy Learning Confidence-Based Autonomy

Grollman, D., & Jenkins, O. (2007). Dogged learning robots. IEEE International
Conference Robotics Automation, pp. 24832488.
Inamura, T., Inaba, M., & Inoue, H. (1999). Acquisition probabilistic behavior decision model based interactive teaching method. Proceedings Ninth
International Conference Advanced Robotics, pp. 523528.
Lockerd, A., & Breazeal, C. (2004). Tutelage socially guided robot learning. Proceedings IEEE/RSJ International Conference Intelligent Robots Systems,
pp. 34753480.
Nicolescu, M. N. (2003). framework learning demonstration, generalization
practice human-robot domains. Ph.D. thesis, University Southern California.
Papudesi, V. (2002). Integrating advice reinforcement learning. Masters thesis, University Texas Arlington.
Price, B., & Boutilier, C. (2003). Accelerating reinforcement learning implicit
imitation.. Journal Artificial Intelligence Research, 19, 569629.
Saunders, J., Nehaniv, C. L., & Dautenhahn, K. (2006). Teaching robots moulding behavior scaffolding environment. Proceeding 1st ACM SIGCHI/SIGART
conference Human-robot interaction, pp. 118125, New York, NY, USA. ACM
Press.
Schaal, S. (1997). Learning demonstration. Advances Neural Information Processing Systems, pp. 10401046. MIT press.
Smart, W. D. (2002). Making Reinforcement Learning Work Real Robots. Ph.D. thesis,
Department Computer Science, Brown University, Providence, RI.
Stolle, M., & Atkeson, C. G. (2007). Knowledge transfer using local features. Proceedings IEEE International Symposium Approximate Dynamic Programming
Reinforcement Learning, pp. 2631.
Sutton, R., & Barto, A. (1998). Reinforcement Learning: Introduction. MIT Press,
Cambridge, MA.
Takahashi, Y., Hikita, K., & Asada, M. (2004). hierarchical multi-module learning system
based self-interpretation instructions coach. Proceedings RoboCup 2003:
Robot Soccer World Cup VII, pp. 576 583.
Thomaz, A. L., & Breazeal, C. (2006). Reinforcement learning human teachers: Evidence feedback guidance implications learning performance. Proceedings Twenty-First Conference Artificial Intelligence, pp. 10001005.

25

fiJournal Artificial Intelligence Research 34 (2009) 707-755

Submitted 08/08; published 04/09

Efficient Informative Sensing using Multiple Robots
Amarjeet Singh
Andreas Krause
Carlos Guestrin
William J. Kaiser

AMARJEET @ EE . UCLA . EDU
KRAUSEA @ CALTECH . EDU
GUESTRIN @ CS . CMU . EDU
KAISER @ EE . UCLA . EDU

Abstract
need efficient monitoring spatio-temporal dynamics large environmental applications, water quality monitoring rivers lakes, motivates use robotic sensors
order achieve sufficient spatial coverage. Typically, robots bounded resources,
limited battery limited amounts time obtain measurements. Thus, careful coordination
paths required order maximize amount information collected, respecting
resource constraints. paper, present efficient approach near-optimally solving NP-hard optimization problem planning informative paths. particular, first
develop eSIP (efficient Single-robot Informative Path planning), approximation algorithm
optimizing path single robot. Hereby, use Gaussian Process model underlying phenomenon, use mutual information visited locations remainder
space quantify amount information collected. prove mutual information
collected using paths obtained using eSIP close information obtained optimal
solution. provide general technique, sequential allocation, used extend
single robot planning algorithm, eSIP, multi-robot problem. procedure
approximately generalizes guarantees single-robot problem multi-robot case.
extensively evaluate effectiveness approach several experiments performed in-field
two important environmental sensing applications, lake river monitoring, simulation
experiments performed using several real world sensor network data sets.

1. Introduction
Global climate change corresponding impetus sustainable practices environment-related
activities brought forth challenging task observing natural phenomena exhibiting dynamics space time. Observing characterizing dynamics high fidelity
critical answering several questions related policy issues monitoring control
understanding biological effects activity microbes organisms living (or dependent
on) environments. Monitoring algal bloom growth lakes salt concentration rivers,
illustrated Fig. 1, specific examples related phenomena interest biologists
environment scientists (MacIntyre, 1993; Ishikawa & Tanaka, 1993; MacIntyre, Romero, & Kling,
2002).
Monitoring environmental phenomena, algal bloom growth lake, requires measuring physical processes, nutrient concentration, wind effects solar radiation, among
others, across entire spatial domain. One option acquire data processes would
statically deploy set sensing buoys (Reynolds-Fleming, Fleming, & Luettich, 2004). Due
large spatial extent observed phenomena, approach would require large number
sensors order obtain high fidelity data. spatio-temporal dynamics environments

c
2009
AI Access Foundation. rights reserved.

fiS INGH , K RAUSE , G UESTRIN & K AISER

(a) Confluence San Joaquin Merced River

(b) Lake Fulmor, San Jacinto mountain reserve

Figure 1: Deployment sites used performing path planning in-field.
motivate use actuated sensors robots carrying sensors together efficient approach
planning paths actuated sensors. actuated sensors used past
(Dhariwal et al., 2006) measuring phenomena various locations hence providing
biologists critical information state lake.
Typically however, robots strict resource constraints, storage battery energy,
limits distance travel number measurements acquire
observed phenomena varies significantly. constraints necessitate careful motion planning
robots coordinating paths order maximize amount collected information,
satisfying given resource constraints. paper, tackle important problem
seeking informative paths collection robots, subject constraints cost incurred
robot, e.g. due limited battery capacity.
order optimize paths robots, first need quantify informativeness
particular chosen path. work, adopt approach spatial statistics employ
probabilistic models spatial phenomena. Using models, informativeness viewed
terms uncertainty prediction phenomena unobserved locations, given
observations made mobile robots subset locations (the selected path). particular, use rich class probabilistic models called Gaussian Processes (GPs) (Rasmussen &
Williams, 2006) shown accurately model many spatial phenomena (Cressie, 1991),
apply mutual information (MI) criterion (Caselton & Zidek, 1984) quantify reduction
uncertainty achieved selected robot paths.
Unfortunately, problem finding optimal collection paths, maximizing mutual
information criterion, NP-hard search problem, typically intractable even small
spatial phenomena. paper, develop approximation algorithm efficiently
finds provably near-optimal solution optimization problem. key insight
allow us obtain algorithm mutual information (and several notions
informativeness (as discussed Krause Guestrin, 2007) satisfies submodularity, intuitive
diminishing returns property - making new observation helps made
observations far, less already made many observations (Krause et al., 2008).
problem optimizing path single robot maximize submodular function
visited locations studied Chekuri Pal (2005), developed algorithm, recursivegreedy, strong theoretical approximation guarantees. Unfortunately, running time
708

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

approach quasi-polynomial: scales log , possible sensing locations. property
makes algorithm impractical environmental sensing applications, typical numbers
(M ) observation locations reaching several hundreds more. paper, present two
techniques spatial decomposition branch bound search overcoming limitations recursive-greedy approach Chekuri et al., making practical real world sensing
problems. call efficient approach single robot path planning eSIP (efficient Single-robot
Informative Path planning).
provide general approach, sequential-allocation, used extend
single robot algorithm, eSIP, multi-robot setting. furthermore prove
generalization leads minimal reduction (independent number mobile robots)
approximation guarantee provided single robot algorithm. combine eSIP sequentialallocation develop first efficient path planning algorithm (eMIP) coordinates multiple
robots, resource constraint, order obtain highly informative paths, i.e. paths
maximize given submodular function, mutual information. exploiting submodularity, prove strong theoretical approximation guarantees algorithm.
extensively evaluate effectiveness approach several experiments performed
in-field two important environmental sensing applications, lake river monitoring. river
campaign executed confluence two rivers, Merced river San Joaquin river, California August 7-11, 2007. Fig. 1a displays aerial view San Joaquin deployment site.
lake campaign executed lake located University California, Merced campus
August 10-11, 2007. Fig. 1b displays aerial view lake Fulmor. campaigns,
Networked Info Mechanical System (NIMS) (Jordan et al., 2007), cable based robotic system,
used perform path planning observing two dimensional vertical plane (cross-section).
addition analyzing data deployments, provide extensive experimental analysis
algorithm several real world sensor network data sets, including data collected using
robotic boat lake Fulmor (Dhariwal et al., 2006).
manuscript organized follows. formally introduce Multi-robot Informative
Path Planning (MIPP) problem Section 2. Section 3, discuss sequential-allocation
approach extending single robot path planning algorithm multi-robot setting
preserving approximation guarantees. review recursive-greedy algorithm proposed
Chekuri et al. (Section 5), example single-robot algorithm. Subsequently, present
spatial decomposition (Section 6) branch bound techniques (Section 7) drastically improve running time recursive-greedy make practical real world sensing
applications. Section 8, evaluate approach in-field experiments well simulations real world sensing datasets. Section 9, review related work, present
conclusions Section 10. proofs results presented Appendix.

2. Multi-robot Informative Path Planning Problem
formally define Multi-robot Informative Path Planning (MIPP) problem. assume
spatial domain phenomenon discretized finitely many sensing locations V.
subset V, let I(A) denote sensing quality, i.e. informativeness, observing
phenomenon locations A. Details appropriate choices sensing quality given below.
also associate location v V, sensing cost C(v) > 0, quantifying expenses
obtaining measurement location v. traveling two locations, u v, robot in-

709

fiS INGH , K RAUSE , G UESTRIN & K AISER

curs traveling cost C(u, v) > 0. robot traverses path space: st-path P sequence
l locations starting node s, finishing t. cost C(P) path P = (s = vP
1 , v 2 , . . . , vl =
l1
t) sum sensing costs traveling costs along path, i.e. C(P) =
i=2 C(vi ) +
Pl
C(v
,
v
).


case
l
=
2,
cost


path
P


involve
traveling
cost

i1
i=2
starting finishing locations C(s, t). use notation P refer sequence
nodes path, subset sensing locations P V (ignoring sequence). collection k paths P = {P1 , . . . , Pk }, one robot, I(P) = I(P1 Pk ) denotes sensing quality paths, quantifies amount information collected k paths.
goal MIPP problem find collection P k paths, specified starting finishing
location si ti (not necessarily different), path bounded cost C(Pi ) B
specified budget B, paths informative, i.e. I(P) large possible.
Formally, problem defined as:
max I(ki=1 Pi ); subject C(Pi ) B, {1, . . . , k}.

Pi V

(1)

lake monitoring example goal performing surface monitoring using boats,
first discretized two-dimensional surface lake finitely many sensing locations (as
depicted Fig. 1b). single robot scenario, seek find informative path
P1 (in terms predicting algal bloom content) starting location finishing location
t. experiment cost C(vi ) corresponds energy required making chlorophyll related
measurements (indicators amount algal bloom). traveling cost C(vi1 , vi ) corresponds
energy consumption traveling location vi1 vi . budget B quantifies total
energy stored boats battery.
2.1 Quantifying Informativeness:
quantify sensing quality I? model spatial phenomena, common approach
spatial statistics use rich class probabilistic models called Gaussian Processes (GPs, c.f.,
Rasmussen Williams, 2006). models associate random variable Xv location
v V. joint distribution P (XV ) used quantify uncertainty prediction
P (XV\A | XA = xA ) phenomena unobserved locations XV\A , making observations
XA = xA small subset locations. quantify uncertainty use, example,
mutual information (MI) criterion (as discussed Caselton Zidek, 1984). set
locations, P, MI criterion defined as:
MI(A) H(XV\A ) H(XV\A | XA )

(2)

H(XV\A ) entropy unobserved locations V \ A, H(XV\A | XA )
conditional entropy locations V \ sensing locations A. Hence mutual information
measures reduction uncertainty unobserved locations. Therefore, lake monitoring
example, would like select locations reduce uncertainty algal bloom
content prediction lake environment. Conveniently, GP, mutual information criterion
computed efficiently analytically (Caselton & Zidek, 1984). effectiveness mutual
information select informative sensing locations studied Krause et al. (2008). Several
alternative information criteria entropy (Ko et al., 1995), information disk model (Bai et al.,
2006) alphabetical optimality criterion A-, D- E-optimal also used
associate sensing quality observation locations related problem domain.
710

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

2.2 Submodularity:
Even consider constraints length paths robots, problem
selecting locations maximize mutual information NP-hard (Krause et al., 2008). Hence,
general, likely cannot expect able efficiently find optimal set locations.
Instead, goal efficiently find near-optimal solutions, sensing quality
(e.g. mutual information), provably close optimal sensing quality.
key observation, allow us obtain strong approximation guarantees,
mutual information satisfies following diminishing returns property (Krause et al., 2008):
locations already sensed, less information gain sensing new
location. intuition formalized concept submodularity: function f submodular
(Nemhauser et al., 1978) if:
B V V \ B; f (A s) f (A) f (B s) f (B).

(3)

Another intuitive property sensing quality monotonic1 , means I(A) I(B)
B V. Hence, select sensing locations, collect
information. Lastly, mutual information normalized, i.e. I() = 0.
thus define MIPP problem problem optimizing paths length B
k robots, selected sensing locations maximize normalized, monotonic submodular function I(). definition MIPP problem allows approach applied
monotonic submodular objective function, mutual information. generalization
useful, several notions informativeness shown satisfy submodularity (Krause
& Guestrin, 2007).
2.3 Online vs Offline Path Planning:
Many robotic path planning applications, search rescue, involve uncertain environments
complex dynamics partially observed. Informative path planning selecting
best locations observe subject given sensing constraints, uncertain environments
necessitates trade exploration (gathering information environment)
exploitation (using current belief state environment effectively). distinguish two different classes algorithms: nonadaptive (offline) algorithms, plan commit
paths observations made, adaptive (online) algorithms, update
replan new information collected. online offline settings NP-hard optimization problems. paper, discuss approximation algorithms offline setting
exploit known belief environment efficient path planning. plan work towards extending approach exploration-exploitation trade-off incorporate online model
adaptation future.

3. Approximation Algorithm MIPP
problem optimizing path single robot (i.e. k = 1) maximize submodular function visited locations, constrained upper bound (B) path cost, first studied
Chekuri Pal (2005). review recursive-greedy algorithm detail Section 5.
1. monotonicity holds approximately mutual information (Krause et al., 2008), however sufficient
purposes paper.

711

fiS INGH , K RAUSE , G UESTRIN & K AISER

1

Algorithm:sequential-allocation

Input: B, k, starting / finishing locations s1 , . . . , sk , t1 , . . . , tk , V
Output: set informative paths P1 , . . . , Pn
2 begin
3
A0 ;
4
1 k
// Performing path planning ith robot
5
Pi SP P (si , ti , B, Ai1 , V);
// Committing previously selected locations
6
Ai Ai1 Pi ;
7
return P1 , . . . , Pk ;
8 end
Algorithm 1: Sequential allocation algorithm multi robot path planning using single robot path planning algorithm SPP. Output set paths P1 , . . . , Pk provides approximation guarantee 1 +
approximation guarantee single robot path planning algorithm SP P .

lake monitoring problem, seek plan multiple paths, one robot. One possibility apply single-path algorithm product graph, i.e. plan path tuples
locations simultaneously representing locations robots. However, straightforward
application single-robot planning algorithm would lead increase running time
exponential number robots, therefore intractable practice. aware
sub-exponential approximation algorithm challenging multiple-robot path planning
problem. paper, present simple algorithm multi-robot scenario exploit
approximation algorithm single robot case, recursive-greedy algorithm,
discussed Chekuri Pal (2005), (almost) preserve approximation guarantee,
avoiding exponential increase running time.
algorithm, sequential-allocation, successively applies single robot path planning algorithm k times get paths k robots. Hereby, planning jth path, approach takes
account locations already selected previous j 1 paths. Committing (approximately) best possible path stage moving next stage makes approach
greedy terms paths.
pseudocode algorithm presented Algorithm 1 Fig. 2 illustrates approach
three robots. algorithm takes input budget constraint B, number available robots
k, starting finishing location available robot s1 , . . . , sk , t1 , . . . , tk complete set
discrete observation locations V select from. Let us assume single robot path
planning algorithm, SP P , takes input starting location si , finishing location ti , budget
constraint B, set locations already selected observation set possible observation
locations visited. Fig. 2, three robots starting finishing location.
planning path first robot (i = 1), input set already selected observation
locations empty. subsequent stage, commit locations selected previous
stages pass already observed locations input next call SP P . Let Ai1 locations already visited paths P1 , . . . , Pi1 , A0 = . residual information, IAi1
path P unvisited locations defined IAi1 (P) = I(Ai1 P)I(Ai1 ). verified
normalized, monotonic submodular function, residual information
712

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

Figure 2: Illustration sequential allocation algorithm three robots, starting finishing
location.

IAi1 . Thus, stage use SP P find informative path respect modified
residual sensing quality function. Fig. 2, planning P2 , locations selected P1 considered sensing quality function used IP1 . Similarly, evaluating path P3 , locations
selected P1 P2 taken account sensing quality function used IP1 P2 .
Perhaps surprisingly, straight-forward greedy sequential allocation approach guaranteed perform almost well black box algorithm used path planning. formally,
assume -approximate algorithm single robot problem, i.e. algorithm which,
starting budget B monotonic submodular function f , guaranteed find path recovering least fraction 1/ optimal information achievable budget.
case, following theorem proves sequential allocation procedure approximation
guarantee close well:
Theorem 1. Let approximation guarantee single path instance informative
path planning problem. sequential-allocation algorithm achieves approximation guarantee (1 + ) MIPP problem. special case, robots starting
(si = sj , i, j) finishing locations (ti = tj , i, j), approximation guarantee improves
1/(1 exp (1/)) 1 + .
work Blum et al. (2003) proved Theorem 1 special case additive (modular)
sensing quality functions. paper, extend result general submodular functions.
example -approximate algorithm single robot problem, next section,
review recursive-greedy algorithm proposed Chekuri Pal (2005). algorithm
approximation guarantee O(log2 |P |), |P | number locations visited
optimal solution P . Hence, algorithm, performance guarantee obtained
MIPP problem sequential allocation O(log2 |P |) well2 .
2. order apply sequential allocation recursive-greedy algorithm, can, planning ith path, simply
pass set nodes visited previous 1 paths input parameter R, illustrated Algorithm 2.

713

fiS INGH , K RAUSE , G UESTRIN & K AISER

(a)

(b)

(c)

(d)

Figure 3: Illustration performance simple greedy approaches compared optimal approach.

4. Note Greedy Path Planning
work Krause et al. (2008) considered sensor placement problem, subset V
k locations selected order maximize mutual information, without considering path
costs. exploiting submodularity property MI, proved discretization V
fine enough GP satisfies mild regularity conditions, greedily selecting locations based
criterion near-optimal. specifically, greedy algorithm (which call GreedySubset
following), selecting first locations Ai , picks location maximum residual
information i.e. vi+1 = argmaxv IAi ({v}) sets Ai+1 = Ai {vi+1 }. GreedySubset hence
iteratively adds locations increase mutual information most. Using result proposed
Nemhauser et al. (1978) performance greedy algorithm submodular functions,
work Krause et al. (2008) showed GreedySubset selects sets achieve mutual
information least (1 1/e) OPT , OPT optimal mutual information among
sets size, small error incurred due discretization.
strong performance greedy algorithm unconstrained (no traveling costs locations) case motivates question whether simple greedy approach could perform
well complex path planning setting considered paper. difficult
give general impossibility statement question, several natural extensions greedy
algorithm shown perform arbitrarily badly.
example, consider setting define cost C(A) set nodes cost
cheapest path connecting nodes A. Assuming locations Ai already picked,
natural extension greedy algorithm add location v improves
benefit-cost ratio
IA (v)
v = argmax
,
vV\A CAi (v)
714

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

CAi (v) = C(Ai {v}) C(Ai ) increase cost adding v already selected
locations Ai .
Fig. 3 shows small example illustrating intuitive greedy procedure perform arbitrarily poorly compared optimal approach. example illustrated Fig. 3a,
starting finishing location 2B total available budget. reward associated observation location displayed parenthesis corresponding locations.
ease illustration, assume reward associated observation location
modular function (instead submodular function). Traveling cost associated
corresponding edges example. Starting location s, possible options first observation
location select either o1 , g1 t. Observation location o1 lead cluster n (=
B/) locations separated traveling cost associated reward 1 (except o1
associated reward ). o1 separated g1 traveling cost B/2
rest locations cluster assumed unreachable location outside
cluster. Observation location g1 lead series (= B/) locations, separated
previous one traveling cost associated reward 2.
illustrated Fig. 3b, optimal approach would select o1 first location, paying
traveling cost B/2 earning small reward . robot observes o1 ,
observe rest (B/ 1) locations cluster, providing reward 1 return back
spending total 2B traveling cost. Thus, total reward collected optimal
approach, example, 1(B/ 1) + .
illustrated Fig. 3c, greedy approach based reward-cost ratio select g1
first observation location (with highest reward cost ratio 2). Since o1 distance B/2
away g1 provides reward , approach continue along series, observing locations till gm returning back s. Total reward collected approach
2B. hand, simple greedy approach based reward (as illustrated Fig. 3d)
simply select first observation location return back s, collecting total reward
1. Since ratio B/ arbitrarily large 0, reward collected simple intuitive
greedy approaches (2B 1) arbitrarily poor compared reward collected
optimal approach (1(B/ 1) + ).
Although, reward function considered example assumed modular function, submodular optimal reward also arbitrarily large, compared submodular reward
collected simple greedy approaches (the difference submodular modular reward
depend correlation selected observation locations). insight necessitates
development complex algorithms path planning considered paper.

5. Recursive-greedy Algorithm
review recursive-greedy algorithm proposed Chekuri Pal, since forms
basis efficient single robot path planning approach. basic strategy algorithm
divide-and-conquer approach. path starting location (s) finishing location (t)
middle location (vm ) number locations (or different 1)
either side vm path. Thus, problem finding path divided
two smaller subproblems finding smaller subpaths (s vm vm t) concatenating
small subpaths. number locations, subpaths either side
middle node different costs, i.e. budget total path split two smaller

715

fiS INGH , K RAUSE , G UESTRIN & K AISER

1

Algorithm:recursive-greedy (RG)

Input: s,t,B,R,iter
Output: informative path P
2 begin
3
c(s, t) > B
4
return Infeasible;
5
P s, t;
6
Base case: iter=0 return P;
7
fR (P);
// Trying location middle node
8
foreach vm V
// Trying possible budget splits
9
1 B1 B
// Planning subpath one side middle node
10
P1 RG(s, vm , B1 , R, iter 1);
// Planning subpath side middle node,
committing nodes selected first subpath
11
P2 RG(vm , t, B B1 , R P1 , iter 1);
12
fR (P1 P2 ) >
13
P P1 P2 ;
14
fR (P);
15
return P;
16 end
Algorithm 2: Recursive greedy algorithm single robot instance MIPP proposed Chekuri Pal
(2005). Output path P provides approximation guarantee IX (P) IX (P )/ d1 + log ke, represent
submodular reward function, P represent optimal path k represent number nodes optimal
path.

budgets (not necessarily equal), one subpath. Searching best middle location
trying possible budget splits either side middle location, optimizing complete
path, would result exhaustive search optimal solution therefore prohibitively expensive. Instead performing exhaustive search, recursive-greedy algorithm
follows simple greedy strategy, wherein possible budget splits possible
middle nodes considered, one first plan optimal subpath one side middle location,
commit planned subpath optimize subpath side. path,
consisting independently optimized subpath svm subpath vm optimized subject observation locations already selected vm , may result suboptimal path. Nonetheless,
Chekuri Pal proved path approximation guarantee O(log2 |P |),
|P | number locations visited optimal solution P .
order implement greedy approach, recursive calls planning second subpath similarly done sequential allocation optimize residual reward function
measures incremental gain taking account information already obtained locations selected first subpath. formally, let set P1 refer locations selected
first subpath, consider residual submodular function fP1 set locations
716

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

fP1 (A) = f (A P1 ) f (P1 ). P2 set locations second subpath, holds
f (P1 ) + fP1 (P2 ) = f (P1 P2 ). Hence, first recursive call (with submodular function f )
returns path P1 , second recursive call (with submodular function fP1 ) returns path P2 ,
sum scores subproblems exactly equals score concatenated path.
Let us formalize intuitive description recursive-greedy algorithm. pseudocode algorithm presented Algorithm 2. inputs algorithm starting
location s, finishing location t, upper bound path cost B, parameter R defines
residual submodular function function needs maximized defined
fR (P) = f (P R) f (R), parameter represents recursion depth. maximum
number locations selected stage calculated using recursion depth 2i .
base case (recursion depth = 0), algorithm simply returns path P = (s, t) (if
cost c(s, t) B).
recursive case, algorithm searches path maximum reward iterating
possible locations (that reached given budget constraint) middle locations
(Line 8), i.e. locations could possibly split required path two subpaths equal number locations either side. middle location, algorithm explores possible
splits available budget (Line 9) across two subpaths either side middle location.
Reducing recursion depth 1, subpath, ensures number locations
selected either side middle location. However, exploring second subpath,
algorithm commits locations selected first subpath passing input
residual parameter (Line 11). two subpaths found way concatenated
provide complete path. algorithm stores best possible path already
searched problem space, replacing better path whenever path found.
5.1 Structure Search Problem
instructive consider recursive structure generated recursive-greedy algorithm.
Fig. 4 illustrates example structure running recursive-greedy lake sensing
application given starting (s) finishing (t) location upper bound path cost
(B). search using recursive-greedy represented graphically sum-max tree.
root max node representing objective finding path maximum possible reward,
cost path bounded budget B. max node, children
search tree represent sum nodes corresponding sum rewards collected two subpaths
either side middle location. Therefore, end first iteration, graphical representation max node root several sum nodes children, feasible middle
location possible budget splits around middle location. partial tree end first
iteration shown Fig. 4a.
sum node, formed end first iteration, algorithm applied recursively left subpath. Thus first step second iteration seeks find vm path
maximum possible reward budget constraint corresponding respective budget split
sum node. Then, approach commits selected locations left side, recurses right subpath (to search vm path), given selected locations. result,
sum node two max nodes children, representing objective find subpath
maximum reward either side selected middle location. algorithm greedy
commits locations selected first subpath optimizing second subpath.

717

fiS INGH , K RAUSE , G UESTRIN & K AISER

(a) recursive-greedy first iteration

(b) recursive-greedy second iteration

Figure 4: Illustration recursive greedy algorithm, proposed Chekuri Pal, lake sensing application.
Sum-max tree presents graphical representation problem space.

partial tree end second iteration shown Fig. 4b. Despite greedy nature,
recursive-greedy approach provides following approximation guarantee:
Theorem 2. (Chekuri & Pal, 2005) Let P = (s = v0 , v1 , . . . , vk = t) optimal s-t-path
solution. Let P path returned RG(s, t, B, R, i). d1 + log ke, IX (P)
IX (P )/ d1 + log ke.
1
Hence, recursive-greedy solution P obtains least fraction d1+log
optimal
2 ke
information, k n, i.e. total number locations traversed optimal path
smaller total number locations discretized spatial domain. Referring back Theorem 1, MIPP problem using recursive-greedy single robot path planning approach,
= d1 + log ke.

5.2 Running Time
inspecting recursive structure, running time recursive-greedy algorithm seen
quasi-polynomial. specifically, running time algorithm O((M B)O(log2 ) ),
B budget constraint = |V| total number possible observation locations.
So, even small problem = 64 locations, exponent 6, resulting
large computation time, making algorithm impractical observing several real world physical
processes.
large computational effort required recursive-greedy attributed two issues: 1)
large branching factor max nodes recursion tree (sum nodes possible
middle node possible budget split across middle node) 2) (possibly) unnecessary
recursion exploring subtrees problem space provide us improved reward compared current best solution. following sections, propose two complementary
approaches (can used independently others) intended ameliorate concerns: spatial decomposition technique, branch bound approach. Spatial decomposition
718

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

Starting node

Starting cell Cs

Ending node

Ending cell Ct

Cs

Middle cell Cm
Ct
P1, budget = B
P2, budget = BeB

Incoming
path P1



b

(a) Spatial decomposition phenomenon



c

Smoothed path

Exiting path
P2

Cell center

(b) Cell paths travel within cells

(c) Cell paths path smoothing

Figure 5: Illustration spatial decomposition recursive-eSIP using surface sensing lake environment
example. sensing domain ((a), top) decomposed grid cells ((a), bottom). recursive-eSIP jointly optimizes
cell-paths ((b), top) allocations experiments cells ((b), bottom). Within cells, locations connected
cell center. recursive-eSIP concatenates paths between-cell within cell paths ((c), top) finally heuristics
applied eMIP smooth path ((c), bottom).
(discussed Section 6) seeks reduce high branching factor (i.e. number sum nodes
search tree) clustering sensing locations running recursive-greedy
clusters instead actual sensing locations. Branch bound (discussed Section 7) seeks
avoid unnecessary recursion maintaining lower upper bound possible reward
subtree search tree pruning tree accordingly. two approaches, together
sequential-allocation (discussed Section 3) provide efficient algorithm multi robot
informative path planning.

6. Spatial Decomposition Approximating MIPP SD-MIPP
section, explain detail process spatial decomposition corresponding improvements running time achieved process. approach assumes traveling
cost arbitrary locations given euclidean distance.
intuitive approach improving running time spatially decompose sensing
region smaller sub-regions, containing cluster sensing locations. thus think
planning informative paths deciding sub-regions explore, deciding
locations sense within sub-regions. idea exploring sub-regions motivates
decomposition sensing domain smaller regions (cells). run recursivegreedy algorithm cells instead actual sensing locations. Since size cellular
region small, traveling cost within cell ignored3 . ignore traveling cost
within cells, sensing locations inside selected cells chosen using GreedySubset
approach (as proposed Krause et al., 2008), taking advantage strong approximation guar3. may robotic platforms non-holonomic motion constraints make small motions much challenging thus traveling cost smaller distances within cell may become non-negligible. systems,
large traveling cost smaller motions, system specific constraints may possible account
performing cellular decomposition greedy algorithm may constrained select locations
close).

719

fiS INGH , K RAUSE , G UESTRIN & K AISER

antee unconstrained setting discussed Section 4. Fig. 5 presents illustration
approach explained follows:
1. decompose sensing region, containing finitely many discrete sensing locations (c.f.,
e = {C1 , C2 , . . . , CN } (c.f., Fig. 5a,
Fig. 5a, top), collection non-overlapping cells V
bottom). distance two cells defined distance centroids
cells. cell Ci contains set locations vi V, representing sensing locations, coordinates locations, euclidean metric space, lie within
boundary containing cell.
2. approximate original MIPP problem spatially decomposed MIPP problem,
e SD-MIPP, jointly optimize cell-paths V
e (c.f., Fig. 5b,
SD-MIPP problem V.
top) using recursive-greedy algorithm, allocation observations within
cells visited paths using GreedySubset algorithm. Thus, allocating measurements cell, ignore traveling cost within cell (c.f., Fig. 5b, bottom). Since
cells large, simplification leads small additional cost
SD-MIPP solution transformed back original MIPP problem.
3. transfer (approximate) SD-MIPP solution, consisting cell-path allocation
observations cells (c.f., Fig. 5c, top), back original MIPP problem. smooth
path (c.f., Fig. 5c, bottom) using heuristics, e.g. tour-opt heuristics discussed
Lin (1965).
Dual optimization cell paths budget allocation observations within visited cell
motivated splitting available budget budget Bt traveling cells budget making experiments sensing locations within visited cells. split easily
incorporated recursive-greedy algorithm well required paths recursivegreedy optimized observation locations cells containing locations. Formally,
SD-MIPP problem following: want find path PC = (Cs = Ci1 , . . . , Cil = Ct ),
robot starting cell Cs containing starting node finishing cell Ct containing
finishing node t, travel cost Bt . travel budget measured terms
distances centers visited cells, cost traveling within cells defined 0.
addition, visited cell Cij PC , want select set sensing locations Aij ,
total experimental cost (for making observations within visited cells) upper bounded
, i.e. C(Ai1 Ail ) , information I(Ai1 Ail ) large possible.
optimal SD-MIPP solution uses optimal split budget Bt . simplify
presentation, rescale costs cells form uniform grid quadratic cells
width L, assume sensing cost Cexp constant locations. assumptions
easily relaxed, allow us relate path costs number cells traversed,
simplify discussion.
following lemma states exists SD-MIPP version (PC ) MIPP-optimal
path (P ), (almost) cost, information.
Lemma 3. Let P = (s = v0 , v1 , . . . , vl = t) optimal s-t-path solution MIPP, constrained
budget B. exists corresponding SD-MIPP path PC = (Cs = Ci1 , . . . , Cil = Ct ),

traversing locations Ai1 Ail , budget 2 2B + 4L collecting
information.

720

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

Algorithm: eMIP
e k, starting / finishing locations s1 , . . . , sk , t1 , . . . , tk
Input: B,
Output: collection informative paths P1 , . . . , Pk
2 begin
3
Perform spatial decomposition cells;
4
Find starting ending cells Csi Cti ;
5
R ;
// Path planning robot
6
= 1 k
// Trying different combination traveling
experimental budget
e
iter = 0 blog2 Bc
7
iter
e
8
B 2 ;
0
9
Piter
recursive-eSIP (Csi , Cti ,Be ,R,iter);
0
10
Smooth Piter
using tour-opt heuristics;
0 );
11
Pi argmaxiter I(Piter
12
R R Pi ;
13
return P1 , . . . , Pk ;
14 end
Algorithm 3: eMIP algorithm informative multi robot path planning. Procedure Line 7 Line 11
1

effectively implements eSIP algorithm. eSIP repeated (Line 6) using sequential allocation described
Section 3 (Line 6) get paths robot i.

present algorithm finding approximately optimal solution SD-MIPP,
show solution
gives us approximate solution original MIPP problem,
slightly increased cost 2 2B + 4L, ensuring optimal solution MIPP exists
corresponding SD-MIPP setting.
6.1 Algorithm SD-MIPP
e smooths paths
eMIP algorithm solves SD-MIPP problem V
selected observation locations provide solution MIPP. Let us first clarify algorithmic
nomenclature specifically:
recursive-eSIP: implements approach similar recursive-greedy selecting path
e greedily selects observation locations within visited cell using GreedySubset;
V
eSIP: iterates different values traveling budget calling recursive-eSIP corresponding values input smoothing output path recursive-eSIP using
tour-opt heuristics;
eMIP: effectively implements sequential-allocation eSIP single robot path planning algorithm
complete algorithm works follows: outer loop (Line 6 Algorithm 3) implements
sequential allocation algorithm performing path planning multiple robots. procedure
721

fiS INGH , K RAUSE , G UESTRIN & K AISER

inside outer loop (Line 7 Line 11 Algorithm 3) implements eSIP algorithm. procedure iterates different combination traveling experimental budget, allocating Bt
(= 2iter ) total budget traveling cells, (= Bt ) making experiments within visited cells. Stepping Bt powers 2 results faster performance
(log2 instead iterations). increase input budget factor 2, exponential
increase traveling budget guaranteed try traveling budget, Bt (= 2iter BtApp ) BtApp
traveling budget best approximation path. Since overall budget increased
factor 2, remaining experimental budget also guaranteed experimental
budget corresponding best approximation path. Therefore, exponential increase traveling
budget increase required budget factor 2. eSIP procedure
calls recursive-eSIP (explained Algorithm 4), selecting cells visit, greedily allocating
observations visited cells. Finally, eSIP procedure calls tour-opt heuristics smooth
output path recursive-eSIP.
recursive-eSIP procedure takes input starting cell Cs , finishing cell Ct , experimental budget , residual R indicating locations visited thus far (initially passed empty
eMIP), maximum recursion depth, iter (initially passed log2 Bt eMIP). then:
1. Iterate possible choices middle cells Cm (such are, almost, equal
fe (of available experimental
number cells either side Cm ) budget splits B
budget ) spend making experiments subpaths Cs Cm Cm Ct
fe either linearly (more accurate) exponentially
(c.f., Fig. 5b). budget splits B
(faster) spaced, described below.
2. Recursively find subpath P1 Cs Cm , constrained budget B 0 , leaving remaining
budget (Be B 0 ) subpath P2 . Reducing recursion depth (iter) 1,
subpaths P1 P2 , ensures equal number cells visited either side Cm .
lowest level recursion depth 0 signifies cell selected corresponding path.
lowest recursion level, use GreedySubset algorithm (c.f., Section 4)
select sensing locations based residual information function IR constrained
budget B 0 . illustration, black locations middle cell Cm Fig. 5b bottom,
selected GreedySubset algorithm budget B 0 = 4 provide
maximum improvement mutual information.
3. commit locations selected P1 , recursively find subpath P2
Cm Ct , experimental budget B 0 . Committing locations selected P1
requires greedily select sensing locations lowest recursion level based
residual information function IRP1 .
4. Finally, concatenate locations obtained P1 P2 output best path
algorithm (c.f., Fig. 5c, top).

6.2 Linear vs. Exponential Budget Splits
Step 1 recursive-eSIP procedure (as explained Section 6.1) considers different budget splits
fe left right subpaths. Similar recursive greedy algorithm, one choose
B0 B
fe = {0, 1, 2, 3, . . . , 1, } linearly spaced. Since branching factor proportional
B
number considered splits, linear budget splits leads large amount computation effort.
722

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

1

2
3
4
5
6
7

8
9

10
11
12
13
14
15

Algorithm: recursive-eSIP
Input: Cs , Ct , , R, iter
Output: informative path P Cs Ct
begin
(d(Cs , Ct ) > 2iter L) return Infeasible;
// Greedy node selection within starting finishing cell
P GreedySubsetBe ,R (vi : vi Cs Ct );
(iter = 0) return P;
reward IR (P);
// Trying cell middle cell
foreach Cm C
// Trying possible budget split
fe
B 0 B
// Planning subpath one side middle cell
P1 recursive-eSIP (Cs , Cm , B 0 , R, iter 1);
// Planning subpath side middle cell
committing nodes selected first subpath
P2 recursive-eSIP (Cm , Ct , B 0 , R P1 , iter 1);
(IR (P1 .P2 ) > reward)
P P1 .P2 ;
reward IR (P);
return P;
end
Algorithm 4: recursive-eSIP procedure path planning.

fe = {0, 20 , 21 , 22 , . . . , 2log2 } {Be ,
alternative consider exponential splits: B
0
1
2
2 , Be2 , Be2 , . . . , 0}. case, branching factor logarithmic experimental
budget. Even though guaranteed find solutions linear budget splits,
theoretically (as given Lemmas 4 7) empirically (as illustrated Fig. 14c
14d) show performance gets slightly worse case, compared significant
improvement running time. addition two ways splitting budget, also confe = {0, 20 , 21 , 22 , . . . , 2log2 }),
sidered one-sided exponential budget splits (i.e. B
reduces branching factor factor 2 compared exponential splits defined above. Although provide theoretical guarantees third possibility, experimentally found
perform well (c.f., Section 8).
6.3 Algorithmic Guarantees
algorithm greedy two ways:
recursion depth 0, sensing locations selected greedily based mutual information criterion.
exploring subpath P2 , recursive-eSIP procedure commits locations selected
subpath P1 .
723

fiS INGH , K RAUSE , G UESTRIN & K AISER

Due greedy steps, recursive-eSIP approximation algorithm necessarily find optimal solution. following lemma, however, guarantees performance bound
path output eSIP procedure:
Lemma 4. Let PC = (Cs = C1 , . . . , Ck = Ct ) optimal solution single robot instance
e optimal set locations selected within
SD-MIPP, constrained budget B,
b solution returned eSIP. I(P)
b 11/e I(P ).
visited cell Cj . Let P
C
1+log k
2

6.4 Solving MIPP Problem
Now, need transfer approximately optimal solution obtained SD-MIPP back MIPP.
path cells, observation locations selected greedily within visited cell, transformed path observation locations connecting locations selected cell Cij
cells center, (as indicated Fig. 5b bottom), connecting selected centers path (Fig. 5c
top), finally expanding resulting tree tour traversing tree twice (by traversing
edge tree direction, set nodes connected tree converted
set nodes connected path). traversal results tour twice
long shortest tour connecting selected vertices. (Of course, even better solution
obtained applying improved approximation algorithm TSP, algorithm proposed
Christofides, 1976). following Theorem completes analysis algorithm:
Theorem 5. Let P optimal solution single robot instance MIPP problem
b achieving information
budget constraint B. Then, eSIP algorithm find solution P


11/e
), whose cost 2(2 2B + 4L)(1 + L 2 )
b
value least I(P)
I(P
1+log2 N
Cexp


fe 2(2 2B + 4L)(1 + L 2 )N log2 32
case linear budget split B
Cexp

fe .
case exponential budget split B
performance guarantee w.r.t. number cells N instead number sensing
locations, case work Chekuri Pal (2005). However, input budget
constraint violated amount based size cells spatial decomposition.
violation input budget constraint leads tradeoff computation effort additional
cost incurred tuned based specific application requirements. size cell
small (in limit reducing cell observation location), number cells large
result higher computation time reduced additional cost. hand,
size cell large, computation time small algorithm needs pay higher
additional traveling cost.
Running time analysis eSIP straightforward. algorithm calls routine recursive-eSIP
log2 B times. TI time evaluate mutual information I, time computing
greedy subset Tgs (Line 4, Algorithm 4) O(NC2 TI ), NC maximum number
locations per cell. recursion step try cells reached available traveling budget (Line 7, Algorithm 4). possible experimental budget split, try
fe among two subpaths P1 P2 (Line 8,
(linearly exponentially spaced) splits B
e following proposition states
Algorithm 4). recursion depth would log2 (min(N, B)).
running time eSIP:

724

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

Proposition 6. worst case running
time eSIP linearly spaced splits experimental
budget Tgs log2 B(N B)log2 N , exponentially spaced splits experimental
budget Tgs log2 B(2N log2 B)log2 N
Comparing running time recursive-greedy algorithm (O((M B)O(log2 ) )), note
reduction B log2 B base, log number locations (log2 ) log
number cells (log2 N ) exponent. two improvements turn impractical recursivegreedy approach much viable algorithm.
Varying number cells (and correspondingly size cell) results trade-off
computation effort traveling cost within cell ignored eSIP
algorithm. Proposition 6 states computation effort directly proportional number
cells N. Therefore increase number cells, corresponding computation effort
eSIP algorithm also increase. hand, reducing number cells result
increasing size cell. Since eSIP algorithm ignores traveling cost within
cell, larger cell size imply larger traveling cost ignored eSIP algorithm hence
larger overshoot cost resultant output path input budget B. Lemma 3 states
corresponding additional cost incurred output path calculated using eSIP algorithm terms
cell size L. Based specific application requirements, one decide appropriate number cells fine tune trade-off computation effort additional path cost
incurred. Fig. 14f shows corresponding collected reward vary significantly
varied number cells application observing temperature lake environment.

7. Branch Bound
spatial decomposition technique effectively enables trade-off running time complexity achieved approximation guarantee. However, eSIP algorithm still solve
super-polynomial, albeit sub-exponential, search problem. following, describe several
branch bound techniques allow reduction computation effort making
approach tractable real world sensing experiments.
7.1 Problem Representation
specific structure search space representation motivated many proposed branch
bound approaches. Similarly recursive structure recursive-greedy algorithm (discussed
Section 5), recursive-eSIP problem structure also represented sum-max tree,
shown Fig. 6a. small difference exists selection observation locations along
solution path. case recursive-greedy, sum nodes traversed selected
path represents physical observation location. However, case recursive-eSIP, sum
node selected path represents cell corresponding traversed path. observation
locations sum node selected greedily, within corresponding cell, based available
experimental budget. Using sum-max tree problem structure, explain proposed
branch bound approaches prune parts tree provide improvement
currently known best solution path. proposed branch bound techniques
outlined recursive-eSIP procedure presented Algorithm 5.

725

fiS INGH , K RAUSE , G UESTRIN & K AISER

1

Algorithm: recursive-eSIP branch bound
Input: Cs , Ct , , R, iter, rewardLB,
Output: informative path P Cs Ct

2
3
4
5
6
7
8
9
10

begin
(d(Cs , Ct ) > 2iter L)
return Infeasible
P GreedySubsetBe ,R (vi : vi Cs Ct );
(iter = 0)
return P
f ilterCells Ci Ci s.t. d(Cs , Ci ) 2iter L/2 d(Ci , Ct ) 2iter L/2 ;
foreach Cm f ilterCells
fe
B 0 B

12

// Calculating upper bound using GreedySubset
U BP1 calculateU B(Cs , Cm , B 0 , iter 1, R);
U BP2 calculateU B(Cs , Cm , B 0 , iter 1, R);

13

((U BP1 + U BP2 ) > rewardLB)

11

15

// Calculating lower bound P1
heurP1 heuristicOP(Cs , Cm , B 0 , R, iter 1);
LBP1 max(IR (heurP1 ), rewardLB U BP2 );

16

// Recursive search P1
P1 recursive-eSIP (Cs , Cm , B 0 , R, iter 1, LBP1 , );

14

18

// Calculating lower bound P2
heurP2 heuristicOP(Cm , Ct , B 0 , R P1 , iter 1);
LBP2 max(IRP1 (heurP2 ), rewardLB IR (P1 ));

19

// Recursive search P2
P2 recursive-eSIP (Cm , Ct , B 0 , R P1 , iter 1, LBP2 , );

17

(Iresid (P1 .P2 ) > rewardLB)
P P1 .P2 ;
rewardLB Iresid (P1 .P2 );

20
21
22
23
24

return P;
end

Algorithm 5: recursive-eSIP procedure branch bound approaches efficient path planning.
procedure corresponds max node search space input rewardLB representing calculated lower
bound. sum node search space effectively combines recursive calls subpaths (implemented Line 16 Line 19). Since recursion reduces traveling budget (2iter L) half, initial pruning
Line 8 removes cells reached next recursion step. Line 15 Line 18 calculate lower
bound subpaths either side selected middle cell. Input represents scaling factor one
b 11/e I(P )
sub-approximation heuristics. Approximation guarantee output path P given I(P)
1+log2 N

submodular reward function P optimal path.

726

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

(a) sum-max tree

(b) Pruning sum nodes

(c) Tighter lower bounds

Figure 6: Illustration branch & bound approach. (a) shows sum-max tree representing search space.
max node selects middle cell budget allocation, sum node combines two subpaths either side
selected middle cell. (b) shows upper bound sum node (e.g. value 18 Sum2 ), smaller
lower bound parent max node (e.g. value 20 Max1 ) used prune branches search tree. (c)
shows lower bound max nodes tightened (e.g. value 7 Max6 improved 9 using upper bound 11
sibling axn7 lower bound 20 grandparent Max1 ) allow pruning otherwise may
possible (e.g. pruning Sum4 upper bound value 8).

7.2 Efficient Search Problem Space
naive implementation recursive-eSIP, entire recursion tree would eventually traversed.
However, many considered subpaths may highly suboptimal. Several heuristics
proposed past similar path planning problem empirical efficiency claims, without
approximation guarantee. use one heuristic (c.f., Chao et al., 1996, hereafter referred
heuristicOP) calculate solution path satisfying budget constraints, trying maximize collected reward. Since path efficiently calculated small computation
effort, use path initial known solution. total reward collected path used
input lower bound (input variable rewardLB Algorithm 5) root max node. Since
computation effort associated heuristicOP small, also used rest max nodes
search tree calculate lower bound nodes (discussed detail Section 7.2.2).
child sum nodes, upper bound collected reward calculated exploiting submodularity reward function (procedure calculateU B called Line 11 12
727

fiS INGH , K RAUSE , G UESTRIN & K AISER

1

Algorithm:calculateUB

Input: Cs , Ct , , iter, R
Output: upper bound UB information gain
2 begin
// Selecting set reachable cells
3
possibleCells Ci Ci s.t. d(Cs , Ci ) + d(Ci , Ct ) 2iter L ;
// Greedy node selection within reachable cells
4
P GreedySubsetBe ,R (vi : vi possibleCells);
5
UB Iresid (P);
6
return U B;
7 end
Algorithm 6: Procedure calculating upper bound max nodes. Upper bound child max nodes added
obtain upper bound parent sum node.

Algorithm 5 explained detail Algorithm 6). need process sum node
children upper bounds greater current best solution (Line 13 Algorithm 5).
current best solution parent max node updated collected reward
child sum nodes greater previously known best solution reward (Line 20 Algorithm 5).
Fig. 6b presents graphical illustration concept. completely exploring branch
Sum1 , current best solution value 20 updated lower bound Max1 . smaller lower
bound (18) Sum2 results pruning sub-branch rooted Sum2 . However, nodes Sum3
upper bound (24) higher current best solution (20), need explored
potentially provide solution path better reward current best solution.
7.2.1 U PPER B OUND Sum N ODES
Algorithm 6 presents calculateUB procedure obtaining upper bound collected
reward max node used recursive-eSIP (Line 11, 12 Algorithm 5) pruning
search space. upper bound sum node calculated adding upper bound
child max nodes. calculate upper bounds relaxing path constraints, finding
optimal set reachable locations path (P1 P2 ). Since problem NPhard, exploit submodularity reward function approximate using GreedySubset
algorithm. Fig. 7 illustrates example calculating upper bound. first calculate set
reachable locations w.r.t. remaining traveling budget. locations contained within
cells Ci reachable cells Cs Ct (Line 3 Algorithm 6). boundary reachable
locations illustrated ellipse Fig. 7.
Then, run GreedySubset algorithm greedily select best possible locations
possible reachable locations (Line 4 Algorithm 6). example, Vi Vj selected
using GreedySubset Fig. 7. Since GreedySubset guarantees constant factor (11/e) approximation (Nemhauser et al., 1978), multiplying resulting information value (1 1/e)1 provides
upper bound information achievable path (and hence corresponding max child
node). Therefore, Fig. 7 reward collected locations Vi (MI(Vi )) Vj (MI(Vj ))
multiplied factor (1 1/e)1 provides upper bound collected reward. However, since
path cost constraint relaxed, total cost observing Vi Vj (dsi + dij + djt ) may

728

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

Figure 7: Illustration calculating upper bound using GreedySubset.
available budget B. Fig. 6c, example, use calculateUB get upper bounds
13 Max6 11 Max7 , resulting upper bound 13 + 11 = 24 Sum3 4 .
7.2.2 L OWER B OUND Max N ODES :
Effective pruning subtree rooted sum nodes would require calculating lower bounds
parent max node efficiently. One way calculate lower bounds exploring one branch
completely (as explained Section 7.2). procedure computationally expensive. Instead, implement two ways acquiring lower bounds faster: Using heuristicOP 5
(as explained obtaining initial best solution), based current best solution
grandparent max node. use larger two different lower bounds.
Fig. 6c illustrates graphical presentation procedure calculating lower bounds
using current best solution grandparent max node. call procedure altLB. calculate upper bound (exploiting submodularity) 11 Max7 node. node Max6 , since
grandparent node Max1 lower bound 20, subtree rooted Max6 provide
reward least 9 (20 - 11) explored further. lower bound value 9 calculated using
altLB tighter lower bound provided heuristic (7), enabled pruning branch
Sum4 (with upper bound 8).
Lines 15 18 Algorithm 5 illustrate altLB procedure. using altLB, lower
bound subpath P1 (in Line 15), calculated using upper bound subpath P2 .
hand, calculating lower bound using altLB subpath P2 (in Line 18), exact reward
P1 (IR (P1 )) used instead upper bound. Since actual reward always tighter
calculated upper bound, lower bound calculated subpath P2 (using altLB) tighter
lower bound calculated subpath P1 . motivates exploring subpath higher
experimental budget first upper bound unexplored subpath (with lower experimental budget) tighter making lower bound first subpath tighter6 . heuristic
4. even compute tighter online bounds maximizing monotonic submodular functions, discussed
Nemhauser et al. (1978).
5. heuristicOP proposed modular functions found provide good solution paths even
submodular setting.
6. note higher experimental budget, GreedySubset (used calculate upper bound) potentially
select locations far apart (since path cost constraint ignored). path cost constraint
incorporated, locations become infeasible make upper bound loose.

729

fiS INGH , K RAUSE , G UESTRIN & K AISER

exploring subpath higher experimental budget first also exploited improve
computation effort.
Maintaining lower bound node search tree also makes approach anytime,
i.e. search terminated point even completed. current best solution
graph already searched available early termination. Early termination
particularly advantageous scenarios required obtain best possible path traversed
robot hard upper bound available time calculate path.
7.2.3 N ODE RDERING
illustration Fig. 6b demonstrates better currently known solution likely help
increased pruning search tree. order improve current best solution faster,
max node explore sum nodes decreasing order upper bounds. intuitive
idea higher upper bound likely indicator higher reward value. Thus upper bound
Line 11 12 Algorithm 5 calculated separately rest computation (in
loops implemented Line 9 10 Algorithm 5) executed decreasing order
upper bound. approach similar node ordering employed improve pruning
efficiency Depth First Branch Bound (DFBnB) (Zhang & Korf, 1995).
7.2.4 UB - APPROXIMATION
Upper lower bounds derived explained potentially loose. address
issue, trade collected information improved execution time, introducing
several sub-approximation heuristics. first heuristic, node ordering performed,
explore top K sum nodes. heuristic, termed sub-approximation (Ibaraki et al.,
1983), found effective practice.
second heuristic, instead comparing lower bound parent max node directly
upper bound child sum nodes (when deciding subproblems prune), scale
lower bound factor > 1 (Line 13 Algorithm 5). scaling often allows us
prune many branches would pruned otherwise. Unfortunately, optimistic
pruning also potentially cause us prune branches pruned, decrease information collected algorithm. practice, sufficiently small values,
procedure speed algorithm significantly, without much effect quality solution. performance comparison computation effort collected reward using several
real world sensing datasets discussed Section 8.2.

8. Experimental Results
performed several experiments in-field well simulation (using real world sensing
datasets) demonstrate usefulness proposed algorithm several diverse environmental
sensing applications. In-field experiments performed using Networked InfoMechanical
System (NIMS) (Jordan et al., 2007), tethered robotic system. Real world sensing datasets used
performing scaling multi robot experiments simulation collected using either
network static sensors robotic boat.

730

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

(a) Schematic representation system

(b) Image captured performing path planning

Figure 8: Aquatic based NIMS (NIMS-AQ)is platform NIMS family used performing path planning
lake environment.

8.1 In-field Experiments
Several experiments performed in-field demonstrate applicability modeling phenomenon Gaussian Process using eMIP perform path planning diverse aquatic sensing applications. include river monitoring application objective studying salt
concentration, lake monitoring several applications interest limnologists.
applications, NIMS used monitor cross-section (two dimensional vertical plane
environment) aquatic environment. phenomenon interest modeled Gaussian Process use mutual information criterion submodular reward function, quantifying informativeness observation locations. learned Gaussian Process model mutual
information objective provided input eMIP subset locations output
algorithm subsequently observed, using NIMS robotic platform. order quantify
efficiency approach, predict phenomenon unobserved locations compute
root mean square (RMS) error predicted phenomenon ground truth (calculated
observing uniformly spaced locations path planning experiment).
8.1.1 ROBOTIC P LATFORM :
Aquatic Networked InfoMechanical Systems platform (NIMS-AQ) latest family
NIMS systems (Jordan et al., 2007; Pon et al., 2005; Borgstrom et al., 2006), developed specifically
aquatic applications used lake deployment. family NIMS systems
successfully deployed several terrestrial aquatic sensing applications. 2006 alone,
NIMS used several successful campaigns forests (La Selva, Costa Rica James Reserve,
California), rivers (San Joaquin, California Medea Creek, California), lake (Lake Fulmor, California), mountain ecosystems (White Mountains, California),
Fig. 8a displays schematic view system. basic infrastructure system includes rigid sensing tower supported two Hobie FloatCat pontoons7 catamaran configuration. actuation module resides top sensing tower drives horizontal cable
vertical payload cable (horizontal vertical motion respectively) across cross-section
aquatic environment. Power system provided two deep cycle marine batteries housed
top pontoons. horizontal drive cable kept center-aligned craft using guide
7. Developed Hobie Cat Company.

731

fiS INGH , K RAUSE , G UESTRIN & K AISER

(a) Observed distribution raster scan August 11

(b) Predicted distribution observing locations
output eMIP

Figure 9: Distribution electrical conductivity (microSiemens per centimeter) observed confluence San
Joaquin river, California. Points represent observation locations corresponding experiment.

pulleys repositioned based type aquatic environment NIMS-AQ
sampling (flowing still water conditions). Fig. 8b shows NIMS-AQ performing path planning
lake environment.
8.1.2 ENSING R IVER E NVIRONMENT
first in-field application approach executed confluence two distinct rivers,
Merced river San Joaquin river, California August 7-11, 2007 (hereafter referred
San Joaquin deployment). Fig. 1a displays aerial view San Joaquin deployment site.
scientific objective confluence zone characterize transport mixing phenomena
confluence two distinct rivers Merced river (relatively low salinity) agricultural
drainage-impacted San Joaquin River (relatively high salinity) observing several parameters
may indicate mixing behavior two streams. river observations useful answering important questions pertaining spatio-temporal variability velocity water quality
dynamics resulting pollutant inputs, hydrodynamic mixing regimes, biogeochemical cycling processes distributed time space. Understanding mixing patterns important policy issues related water distribution river ecosystems (Brekke
et al., 2004).
total width observed cross-section 40 meters maximum depth 1.4 meters (closer middle cross-section). Several experiments executed past
characterize mixing phenomena confluence site (Singh et al., 2007a; Harmon et al.,
2007). Primary experimental design campaigns comprised making observations
uniformly spaced locations two dimensional cross-section (hereafter referred raster scan)
repeating experiments several times understand spatial temporal dynamics
environment. experiments took several hours, thus restricting experiments
small number cross-sections (one two) within limited deployment time. However,
detailed understanding confluence environment would require observing multiple crosssections, within limited time frame. necessitates use adaptive sampling approach
model observed phenomenon, make observations small number locations based
model effectively predict phenomenon unobserved locations.

732

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

Mixing patterns characterized confluence observing electrical conductivity
indicated amount salt concentration water. Fig. 9a displays typical distribution
cross-section confluence zone x-axis representing distance along cross-section
y-axis representing depth. Low concentration electrical conductivity towards lower
x values contributed clear water Merced river end displaying high
concentration salts carried San Joaquin river. first use data one raster
scan performed first day deployment (displaying similar characteristics) learn
non-stationary Gaussian Process model, using covariance function parameterization described
Krause Guestrin (2007). parameters chosen maximizing marginal likelihood (Rasmussen & Williams, 2006). non-stationary process learned dividing
complete region smaller sub-regions combining locally-stationary GPs
sub regions.
total 114 locations observed raster scan used learning GP
model. set 16 locations selected total 114 (14%) using eMIP algorithm
starting finishing location either end cross-section displayed Fig. 9a.
set 16 observation locations observed next days. required
dwelling time8 30 seconds observing electrical conductivity, large reduction number observation locations resulted significant reduction experimental time well (14% compared
raster scan).
Since environmental phenomena exhibit spatial temporal dynamics, performed raster
scans experiment get measure ground truth electrical conductivity.
predicted electrical conductivity, computed making observations subset 16
locations selected using eMIP, compared ground truth. Fig. 9b displays predicted distribution specific conductivity points representing observed locations output
eMIP. Fig. 9a displays distribution observed using raster scan performed
path planning experiment.
RMS error predicted distribution raster scan performed path
planning experiment 45.99 S/cm. hand, RMS error predicted
distribution raster scan performed path planning experiment 53.87 S/cm.
RMS error two raster scans performed path planning experiment, indicating temporal variation environment, 57.55 S/cm. Low RMS error
predicted distribution, compared RMS error raster scans performed
path planning experiment clearly indicates effectiveness approach
modeling path planning environments. Path planning experiments performed
days also demonstrated similar prediction accuracy, maintaining significant reduction total experimental time.
8.1.3 ENSING L AKE E NVIRONMENT
second set in-field experiments executed lake campus University California, Merced August 10-11, 2007 (hereafter referred lake deployment). site
chosen based convenience accessibly located university campus similarity several lakes interest diverse limnology applications, including
study growth patterns algal bloom. Nuisance algal bloom impair beneficial use
8. Time sensor kept static get accurate measurement.

733

fiS INGH , K RAUSE , G UESTRIN & K AISER

(a) Observed distribution raster scan August 11

(b) Predicted distribution observing locations
output eMIP

Figure 10: Distribution temperature (o C) little UC Merced campus. Points represent observation locations
corresponding experiment.

aquatic systems, blocking sunlight underwater vegetation, consuming oxygen water,
producing surface scum odors. growth pattern algal bloom lake dependent
spatial temporal dynamics temperature, dissolved nutrients light occurring different layers environment. Thus, temperature one critical parameter observe
lake environment controls several physical processes occurring low flow aquatic
environments (in contrast San Joaquin river environment considerable water
flow).
total width observed cross-section 70 meters, maximum depth
1.81 meters. Similarly San Joaquin deployment, first learned non-stationary GP model
using temperature data one raster scans performed August 10. Fig. 10a displays
typical surface distribution temperature observed raster scan lake. total
89 locations observed raster scan. set 15 locations selected 89
locations (17%) using eMIP algorithm starting ending location either end
cross-section displayed Fig. 10a. set 15 observation locations observed
next day using NIMS robotic platform. Similar San Joaquin deployment, performed
raster scans experiment get measure ground truth temperature
distribution. predicted temperature, computed making observations subset
locations selected using eMIP, compared ground truth. smaller dwelling time
10 seconds (required measuring temperature) cover entire length lake
cross-section, reduction experimental time 50% (when compared raster scan).
Fig. 10b displays predicted distribution temperature points representing observed
locations output eMIP. Fig. 10a displays distribution observed using raster scan performed path planning experiment. RMS error predicted distribution
raster scan performed path planning experiment 0.73 C. hand,
RMS error predicted distribution raster scan performed path planning
experiment 0.82 C. RMS error two raster scans performed
path planning experiment, indicating temporal variation environment, 1.25 C.
low RMS error predicted distribution raster scans, comparison
temporal variation exhibited lake environment, indicates effectiveness approach
low-flow lake environment well.
734

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

8.2 Experiments Sensing Datasets
Several experiments performed simulation using real world sensing datasets analyze
scaling algorithm different approaches varying experimental cost, exponential
increase budget split, varying size cells spatial decomposition comparison
several heuristics, among others. Three different datasets, collected real world sensing applications, used experiments. first dataset consists measurements temperature
Lake Fulmor, James Reserve (hereafter referred lake temperature dataset). Fig. 1b displays
aerial view Lake Fulmor. robotic boat, part Networked Aquatic Microbial Observing
System (NAMOS) (Dhariwal et al., 2006), used collect surface temperature data around
lake, width around 50 meters length around 250 meters. discussed earlier, understanding temperature distribution prime importance limnology since governs several physical
phenomena occurring lake environment, including growth algal bloom.
average speed boat approximately 0.4 m/s. Half total measurements (218
different sensing locations) used learn nonstationary Gaussian Process model maximizing marginal likelihood (Rasmussen & Williams, 2006), remaining measurements
used experimentation. divided lake 22 cells (except experiments
studying effect changing size cell spatial decomposition), distance
adjacent cell approximately 21 meters. Based average speed, motivated typical
measurement duration roughly 25 seconds, set experiment cost 10.5 meters (except
experiment understanding effect scaling experimental cost).
second dataset, used data existing deployment 52 wireless sensor motes
learn amount temperature variability Intel Research Laboratory, Berkeley (hereafter
referred Berkeley temperature dataset). sensing locations lie within bounding region
length 45 meters width 40 meters. divided complete region uniform grid containing 20 equal sized cells, determined experimental cost 9 meters (approximate distance
travel adjacent cells). learned GP model discussed Krause et al. (2006).
Finally, explored performance algorithm precipitation dataset collected
167 regions equal area, approximately 50 km apart, years 1949-1994. followed
preprocessing model learning described work Krause et al. (2008). large physical
spread sensing regions makes dataset unconventional mobile robot path planning
application. avoid unrealistic scenario, normalized coordinates regions lie
within bounding region length 7 meters width 9 meters, keeping actual sensing
data observed location. divided complete region uniform grid 20 cells
experimental cost 1.4 meters (approximate traveling distance adjacent cells).
plots comparing performance algorithm, x-axis represent total
cost path including traveling cost selected locations sensing
cost selected location (translated distance discussed above). comparing
computation effort measure performance, seconds, y-axis drawn logarithmic scale.
computation effort running code implemented Matlab 3.2 GHz dual processor
core 4 GB RAM. comparing collected reward measure performance, y-axis
represent mutual information (submodular reward function) collected making observations
selected locations.

735

fi5

10

10

Recursive
greedy

4

10

Collected Reward

Execution Time (seconds)

INGH , K RAUSE , G UESTRIN & K AISER

3

10

2

10

eMIP

1

10

0

10
60

Recursive
greedy

8

6

eMIP

4
60

80
100 120 140 160
Cost output path (meters)

80

100

120

140

160

Cost output path (meters)

(a) Comparison computation effort

(b) Comparison collected reward

Figure 11: Comparison eMIP recursive-greedy subset Berkeley temperature dataset 23 sensing

5

15

10

subapproximation
Subapprox: 10%

subapproximation
Collected Reward

Execution Time (seconds)

locations.

Subapprox: 10%

Subapprox: 20%
Best Possible 20
subproblems

0

10
200

250

300

350

400

Subapprox: 20%

10

5

Best possible 20
subproblems
0
200

450

250

300

Uniform density

350

400

450

Cost output path(meters)

Cost output path(meters)

(a) Comparison computation effort

(b) Comparison collected reward

Figure 12: Comparison computation effort collected reward several sub-approximation heuristics used
improve running time eMIP lake temperature dataset. Significant improvement execution time observed,
particularly longer paths, without significant reduction collected reward.

8.2.1 C OMPARISON R ECURSIVE - GREEDY LGORITHM :
compare performance approach recursive-greedy algorithm, proposed
Chekuri et al., selected subset 23 locations total 52 locations Berkeley
temperature dataset. small subset locations selected since running time recursivegreedy quasi-polynomial large complete dataset. Fig. 11a Fig. 11b
display comparison computation effort collected reward smaller dataset
two algorithms. evident plots, approach provides significant improvement
running time (of several orders magnitude higher budget values) (almost) collected reward. Since recursive greedy algorithm essentially search procedure greedily
restricted search space, result also indicates exhaustive search paths intractable
even small real world sensing problem. sudden jump execution time eMIP Fig. 11a
budget = 100 meters due additional iteration step (c.f., Line 7 Algorithm 3) added due
increase input budget constraint. Thereafter, additional increase budget results increase experimental budget. Since recursive-eSIP computes efficiently
small problem, additional increase experimental budget increase computation effort
significantly.

736

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

300

Distance (meters)

250

Lake Boundary
Starting
Location

200
Cells
150

eMIP Path

100
50
0
0

Possible observation
locations
50

100
150
200
Distance (meters)

250

300

Figure 13: Illustration path selected using eMIP lake temperature dataset.
8.2.2 C OMPARISON U NIFORM AMPLE PACING :
compared performance eMIP simple uniform sample spacing algorithm, referred
Uniform density. case Uniform density, starting finishing given locations,
greedily select two observation locations within nearest cells compute corresponding path cost path reward. Uniform density algorithm output best possible path
amongst possible simple uniform sample spacing algorithms due greedy observation selection
within cell. Fig. 12b, compares collected reward Uniform density eMIP
lake temperature dataset. Increased collected reward eMIP, compared Uniform density, empirically justifies complexity eMIP. Additionally, eMIP also provides strong approximation
guarantee possible uniform sample spacing algorithm. Fig. 13 illustrates
path selected eMIP lake temperature dataset, demonstrating eMIP tend
cause uniform sample spacing. traversed cells, location selected
observation, others many three observation locations selected within
cell.
8.2.3 C OMPARISON UB - APPROXIMATION H EURISTICS :
Various sub-approximation heuristics discussed Section 7 compared empirically analyze
utility improving execution time corresponding reduction collected reward,
any. displayed Fig. 12a compares heuristics computation effort,
sub-approximation heuristic provides improvement execution time scenario
branch bound heuristics sub-approximation heuristics used.
improvement higher values input budget observed lower bound increased
factor (= 1.2 20%). Fig. 12b displays corresponding comparison heuristics
collected reward. interesting observe none sub-approximation approaches
resulted considerable reduction collected reward.

737

fi5

10

8

Cost = 1.12

4

Cost = 0.56
Collected Reward

Execution Time (seconds)

INGH , K RAUSE , G UESTRIN & K AISER

Cost = 0.84

10

Cost = 0.56

3

10

Cost = 1.4
2

10
15

20
25
Cost output path (meters)

Cost = 0.84
6
Cost = 1.12
5

5

14

Collected Reward

Linear variation
4

Exponential variation
ends

3

10

30

Exponential variation
ends

12

Linear variation
10

8

Exponential increase 0
2

10
200

Exponential variation 0
250

300

350

400

6
200

450

250

300

350

400

450

Cost output path(meters)

Cost output path(meters)

(c) Computation effort variation experimental
budget split using lake temperature dataset

(d) Collected reward variation experimental budget split using lake temperature dataset

5

10

20

Grid: 20 cells
4

Collected Reward

Execution Time (seconds)

20
25
Cost output path (meters)

(b) Collected reward variation sensing cost using
precipitation dataset

10

10

Cost = 1.4

4
3
15

30

(a) Computation effort variation sensing cost using precipitation dataset

Execution Time (seconds)

7

Grid: 14 cells

10

3

10

2

10

Grid: 22 cells

0

Grid: 22 cells

10

Grid: 20 cells

5

Grid: 33 cells

1

10

Grid: 14 cells
15

200

400

600

0
0

800

Grid: 33 cells
200

400

600

800

Cost output path(meters)

Cost output path(meters)

(e) Computation effort variation grid size
spatial decomposition using lake temperature dataset

(f) Collected reward variation grid size spatial decomposition using lake temperature dataset

Figure 14: Comparison collected reward computation effort variation several approaches used eMIP.

738

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

8.2.4 VARIATION ENSING C OST:
Fig. 14a Fig. 14b compare computation effort collected reward sensing cost
varied precipitation dataset. reduction experimental cost, locations
observed total input budget resulting increased collected reward. However,
experiments, computation effort approximately same. Due diversity
environmental applications, sensing cost depend sensors (settling time) scale
dynamics occurring observed phenomena. experiment indicates eMIP used
diverse range sensing costs, per demands diverse environmental applications.
8.2.5 VARIATION E XPERIMENTAL B UDGET PLIT:
discussed Section 6, strategy exponentially increasing experimental budget split
results increased additional path length required guarantee approximation factor
collected reward. performed several experiments available datasets analyze
empirical performance increasing budget splits exponentially. Fig. 14c Fig. 14d compares
computation effort collected reward linear increase, one sided exponential variation
0 two-sided exponential variation 0 budget B lake temperature
dataset. Since smaller number budget splits considered recursive-eSIP case
exponential increase, computation effort smaller compared linear increase
budget splits. Interestingly, small reduction collected reward,
budget values, exponential increase employed. Hence, even though theoretical
approximation guarantee exponential increase experimental budget weaker, empirically
collected reward linear exponential increase budget splits found
comparable wide range input budgets.
8.2.6 NALYSIS PATIAL ECOMPOSITION :
discussed Section 6, conversion SD-MIPP solution (a cell path) solution
MIPP (a path observation locations) result additional path length exceeding input
budget B. additional path length depend size cell (or size grid covering
complete spatial domain) SD-MIPP problem result trade-off computation
effort. Variation grid-size result corresponding variation traveling cost
neighboring cells. result opportunity travel cells denser grid
input budget constraint. However, keep experimental cost constant across varying
grid size (since experiment cost depends observed phenomena independent
spatial decomposition), scaled accordingly, proportion traveling cost
neighboring cells. Fig. 14f compares collected reward varying grid sizes lake
temperature dataset, changing grid size 14 33 cells. interesting observe
change grid size (almost) negligible effect collected reward. hand,
increase grid density resulted larger number cells path planning
performed thus leading increased computation effort input budget. comparison
computation effort varying grid size displayed Fig. 14e. Note drastic increase
computation time grid discretization made finer.

739

fi16

20

3 Robots

Total RMS Error

Total Collected Reward

INGH , K RAUSE , G UESTRIN & K AISER

15

2 Robots
10

14
2 Robots

12
3 Robots

10

1 Robot
5
200

250

300

350

400

8
200
250
300
350
400
450
Average cost output path per robot (meters)

450

Average cost output path per robot (meters)

(a) Collected reward starting location

(b) RMS error starting location
15

25

3 Robots

Total RMS Error

Total Collected Reward

1 Robot

20

2 Robots
15

Single Robot
10
5
200

250

300

350

5

2 Robots

3 Robots

0
250
300
350
400
450
Average cost output path per robot (meters)

400

Average cost output path per robot (meters)

(c) Collected reward different starting location

Start 3

10

1 Robot

(d) RMS error different starting location

Boundary

Cells

1

33

Start 2

73

Start 1

Robot-2
Robot-1

Robot-3

(e) Paths selected using MIPP

Figure 15: Analysis experiments performed multiple robots different (optimized) starting location using
lake temperature dataset.

8.2.7 ULTI - ROBOT E XPERIMENTS
evaluated performance eMIP multi-robot algorithm simulation using several sensing datasets. Fig. 15 displays empirical analysis several experiments using lake temperature dataset. first experiment performed robot starting starting
location. Fig. 15a Fig. 15b display collected reward root mean square (RMS) error
number robots varied one three. Due sequential-allocation ap-

740

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

proach (wherein remove locations already selected selecting locations
next robot) information never hurts principle, collected reward increases number
robots increased hence corresponding root mean square error prediction
unobserved locations gets reduced. However, incremental change performance one
two robots larger incremental change two three robots, expected
submodularity (diminishing returns) property mutual information.
Fig. 15c Fig. 15d display collected reward RMS error different starting location chosen robot. scenario, set four starting locations pre-determined
location one end lake (see reference Fig. 15e three four starting
locations marked). starting location three robots selected greedily based
collected information. different starting location selected opposite end
lake second robot, incremental change collected reward (and corresponding decrease
root mean square error) number robots increased one two much higher
corresponding change starting location chosen second robot
well. However, similar scenario starting location, incremental change
number robots increased one two higher compared number
robots increased two three (due submodularity mutual information). Fig. 15e
illustrates selected paths three robots selected using eMIP.

9. Related Work
large body related work theory path planning applications. Approximation algorithms proposed several related problems. Variants path planning
studied field Operations Research Traveling Salesman Problem (TSP)
Vehicle Routing Problem (VRP). robotics, several path planning approaches
studied applications Simultaneous Localization Mapping (SLAM) search
exploration. sensor networks geostatistics, closely related work studies optimal placement
static sensors modeling phenomenon Gaussian Processes. Several adaptive sampling approaches studied decide subset locations observe order understand
phenomenon dynamics effectively. addition, similar approaches explored planning paths
mobile robots acting data mules, collecting data sampled network static sensors.
9.1 Operations Research
interesting special case MIPP problem given case node fixed
reward, goal find path maximizes sum rewards (Traveling Salesman
Problem Profits, TSPP, Feillet et al., 2005). sum rewards modular (additive)
function, special case submodular functions. subcategory TSPP optimization problem defined maximize collected reward keeping associated cost less
given budget B. studied Orienteering Problem (OP) selective TSP (Laporte &
Martello, 1990), Maximum Collection Problem (Kataoka & Morito, 1988) literature.
additivity assumption made orienteering problem unrealistic informative path
planning setting, assumes information provided adjacent locations independent,
whereas would typically expect strong amount correlation. fact, observations
independent, would point selecting observations spatial prediction.

741

fiS INGH , K RAUSE , G UESTRIN & K AISER

paper, hence study general orienteering problem submodular reward functions,
proposed earlier Submodular Orienteering Problem (Chekuri & Pal, 2005).
9.1.1 ULTIPLE - PATH E XTENSIONS :
extension TSPP multiple paths studied Vehicle Routing Problem Profits
(VRPP) literature. Like TSPP, several variants VRPP previously considered.
Prize Collecting VRP (PCVRP) (Tang & Wang, 2006) class VRPP objective determine subset customers visit minimize total distance traveled,
minimize vehicles used maximize collected reward. multi-robot version OP
(in case additive reward functions) studied Team Orienteering Problem I-Ming
et al. (1996) Multiple Tour Maximum Collection Problem Butt Ryan (1999).
9.1.2 K NOWN PPROXIMATIONS RIENTEERING P ROBLEM :
OP known NP-hard (Golden et al., 1987). Several versions OP studied
literature classified starting (and finishing) location (root)
pre-specified not. case unrooted OP (when starting location specified), approximation guarantees known Prize Collecting TSP k-TSP easily extended (Johnson
et al., 2000). several constant factor approximations known PC-TSP k-TSP
problems best one 2 approximation (Garg, 2005). However extension
apply rooted version problem best path unrooted version may
contain root may far away root thus leading violation budget constraint.
rooted OP, Arkin et al. (1998) gave (2 + ) approximation OP geometric
settings. Blum et al. (2003) gave first constant factor approximation rooted OP general
undirected graphs. also extended algorithm multi-path OP. running time
algorithm, though polynomial, large (more specifically, O(n5 log( 1 )) total
reward path). Recently Chekuri et al. (2008) gave polynomial time algorithm OP
undirected graphs improved approximation guarantee (2 + ). problem formulation
specified starting location (s) finishing location (t) falls category rooted OP
submodular (non-additive) reward function.
Another classification OP done based symmetry space possible
locations. approximation guarantees hold true symmetric spaces (undirected
graphs). Obtaining good approximation algorithm directed (asymmetric) orienteering problem stated open problem Blum et al. (2003). Chekuri Pal (2005) gave first
approximation algorithm O(log n) guarantee runs quasi-polynomial running time.
running time recently improved independently two different works (Chekuri et al., 2008;
Nagarajan & Ravi, 2007), proposing poly-time approximation algorithm providing approximation guarantee O(log2 n), though using different approaches. metric space conversion procedure used spatial decomposition approach limits eMIP symmetric spaces
only.
9.1.3 EQUENTIAL LLOCATION :
Blum et al. (2003) proposed sequential allocation approach extend algorithms single-robot
orienteering multiple robot setting, special case additive (modular) reward
functions. paper, generalize result submodular reward functions. initial
742

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

version paper published (Singh et al., 2007b), realized sequential-allocation
procedure instance maximizing submodular function subject matroid constraint (Calinescu et al., 2007). define partition matroid disjoint union = M1 Mk
k ground sets Mi , one robot. set Mi contains feasible paths robot i.
collection 2M subsets P |P Mi | 1 (i.e. P corresponds
collection paths, constraint pick one set Mi ) forms
independent sets partition matroid. Hence, problem finding collection maximally
informative paths problem finding independent set matroid maximizing submodular function. Current work progress Goundan Schulz (2008) provides general results
performance sequential allocation procedure setting, used prove
sequential allocation results originally presented Singh et al. (2007b).
9.2 Robotic Applications
considerable work path planning robotics community several applications,
including simultaneous localization mapping (SLAM) search exploration. Several
different approaches studied applications, including auction based algorithms, data-adaptive approaches information gain based algorithms.
9.2.1 IMULTANEOUS L OCALIZATION APPING :
goal Simultaneous Localization Mapping (SLAM) build maps environment
performing exploration environment objective estimate robot position
world features simultaneously. Several approaches optimizing different objective functions
proposed perform path planning SLAM. Bourgault et al. (2002) proposed exploration
framework using occupancy grid (OG) environment model (performing spatial decomposition
observed environment) objective maximize mutual information OG map.
Stachniss et al. (2005) developed greedy algorithm selecting next location visit
maximize information gain map.
contrast approaches, Sim Roy (2005) attempted optimize entire trajectory, next step, algorithm introduces approximations without theoretical bounds. Simmons et al. (2000) proposed distributed approach exploration mapping
multiple robots minimizing overlap information gain amongst multiple robots.
provided quantitative results simulation provide theoretical bounds
approach. little work SLAM setting upper bound total cost path.
addition, aware approaches SLAM carry approximation guarantees
either single multi-robot cases. interesting direction future work would analyze
applicability approach SLAM setting.
9.2.2 EARCH E XPLORATION :
search exploration application involves path planning robot goal searching
moving target(s) given environment, e.g. target surveillance security applications
patient tracking health care domain. Performing path planning using stochastic inference provides
advantage robustness sensing motion uncertainty though added complexity computational intractability. Roy Earnest (2006) proposed approach effectively compute
trajectories target tracking based maximizing mutual information (evaluated using change
743

fiS INGH , K RAUSE , G UESTRIN & K AISER

variance probability distribution). used particle filter approach, performing clustering particles followed path planning clusters. Lau et al. (2006) formulated
target tracking indoor environments generalization NP-complete optimal searcher path
(OSP) problem (Trummel & Weisinger, 1986). sought optimize probability detection
within given time horizon accounting undetected target probability function
previously visited locations search. used several branch bound approaches
speed search process. objective maximizing information gain subject budget
constraints path cost makes eMIP suitable candidate performing path planning
problems.
Ryan (2008) used approach partitioning search space subgraphs multi-robot
path planning. take conceptually similar approach, also reducing search space decomposing space regions performing path planning regions. However,
address complex utility functions, quantifying informativeness visited locations
limited specific graph structures stacks, halls, cliques, rings case
work Ryan (2008). Recently, Thompson Wettergreen (2008) used eMIP algorithm
near-term path planning performing autonomous exploration surficial units Amboy
Crater Mojave desert, California.
9.2.3 P LANNING YSTEMS PPLICATIONS :
Certain applications robotic path planning used plan graphs (Blum & Furst, 1997) compute
estimate resources time required achieve goals states encountered
search process. case over-subscription planning problem wherein subset goals
accomplished within limited time resources available planning system,
work Smith (2004) used orienteering heuristic provide ordered set goals
considered planner. Briel et al. (2004) proposed several heuristics efficiently solving
over-subscription planning problem. However, earlier proposed heuristics,
reward function considered modular (additive). eMIP used efficiently solve oversubscription planning problem submodular setting strong approximation guarantees.
9.3 Sensor Networks
Phenomenon modeling decide optimal placement set static sensors well studied
sensor networks geostatistics communities. Gaussian Process models spatial phenomena studied extensively (Cressie, 1991). Guestrin et al. (2005) proved that, case
phenomena governed Gaussian Process models, selecting placement sensors greedily
based mutual information near-optimal. Krause et al. (2006) extended work include
communication cost sensors optimizing sensor placement. communication constrained setting, similar path planning problem considered paper, greedy
algorithm performs badly, involved algorithms developed. Batalin et al. (2004)
showed combining static mobile sensing devices, even simple scenario, result
significant improvement sensing performance. scenario, combination
static mobile sensing devices available, several approaches optimal placement static
sensors combined eMIP observe given phenomenon efficiently.

744

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

9.3.1 DATA C OLLECTION ENSOR N ETWORK :
different scenario mobile robot combined network static sensors
improve lifetime sensor network performing tours collecting data sampled
static network. Somasundara et al. (2007) showed problem collecting data
environment shows spatial temporal dynamics NP-complete provided
integer linear programming formulation same. compared performance several
heuristics simulation single multi-robot scenario. Meliou et al. (2007) proposed
nonmyopic approach application data gathering tours using algorithm submodular
orienteering (SOP) black box. provided strong approximation guarantees extensive
empirical evaluation indicates applicability approach applications.
setting, eMIP used orienteering algorithm provide better approximation guarantee
addition improved running time.
9.3.2 DAPTIVE AMPLING E NVIRONMENTAL PPLICATIONS :
Recent advances robotics opened opportunities high fidelity monitoring dynamic
environmental sensing applications. Rahimi et al. (2004) explored several policies adaptively
sampling environment. Singh et al. (2006) proposed multiscale adaptive sampling approach
uniformly sampling environment first stage followed sampling locations order
minimize mean square error most. also extended approach multiple robots,
although without providing theoretical bounds. Using several in-field experiments well
simulations using real world sensing datasets, demonstrate several environmental
phenomenon effectively sampled adaptively using eMIP.

10. Conclusions Future Work
paper, presented eSIP, approximation algorithm efficient planning informative
paths. eSIP near-optimally solves NP-hard problem maximizing collected information
upper bound path-cost. eSIP algorithm builds recursive-greedy algorithm
Chekuri Pal (2005). eSIP preserves approximation guarantees recursive-greedy,
overcoming computational intractability spatial-decomposition several branch
bound approaches. also presented general approach, sequential-allocation, extends
single-robot algorithm, eSIP, multiple-robot setting providing provably strong
approximation guarantee.
also provide extensive empirical evaluation demonstrate effectiveness approach
real world sensing applications. performed several in-field experiments two important
environmental sensing applications lake monitoring (at small lake UC Merced campus)
river monitoring (at San Joaquin river, California). Networked Info Mechanical System (NIMS)
used robotic system performing path planning deployments
demonstrate practicality algorithm. also performed extensive simulation experiments
using several real world sensor network data sets. global climate change corresponding
impetus sustainable practices, expect efficient path planning approaches help
address challenge monitoring environment-related activities effectively.
future, plan explore applicability algorithm application domains
SLAM search rescue. plan work towards understanding limitations

745

fiS INGH , K RAUSE , G UESTRIN & K AISER

learning static GP model real world scenarios, extend approach online model adaptation.

Acknowledgments
would like thank Maxim Batalin helpful discussions, Bin Zhang providing lake
data set Michael Stealey, Henry Pai Victor Chen help river lake deployment. work partially supported NSF Grants No. CNS-0509383, CNS-0625518, CNS0331481, ANI-00331481, CCR-0120778, ECCS-0725441, ONR MURI W911NF0710287
gift Intel. Carlos Guestrin partly supported Alfred P. Sloan Fellowship IBM
Faculty Fellowship. Andreas Krause partially supported Microsoft Research Graduate
Fellowship.

APPENDIX
Theorem-1. Let approximation guarantee single path instance informative
path planning problem. sequential-allocation algorithm achieves approximation guarantee (1 + ) MIPP problem. special case, robots starting
(si = sj , i, j) finishing locations (ti = tj , i, j), approximation guarantee improves
1/(1 exp (1/)) 1 + .
Proof Theorem 1. case robots start finish location, let
total reward collected optimal solution. Additionally, define difference
reward collected optimal solution, approximation algorithm,
end stage i. Hence, 0 = .
Let Ai = P1 Pi nodes selected approximation algorithm stage
(A0 = ), let P = {P1 , . . . , Pk } denote collection paths chosen optimal solution.
) f (Ai ) =
Consider residual reward fAi . find fAi (P ) = f (Ai P ) f (Ai ) f (P P
due monotonicity f . path Pj fAi (Pj ) k1 , j fAi (Pj ) <
= fAi (P ), contradicting monotonic submodularity fAi . Hence path Pj
fAi (Pj ) k1 , thus approximation algorithm guaranteed find path Pi
1
fAi (Pi ) k
.
difference reward collected optimal solution reward collected
Algorithm 1 stage + 1 most:
i+1 (1 1/k)i ,
(1 1/k)i+1 .
Thus k stages, difference reward bounded k (11/k)k exp (1/).
Hence, reward collect Algorithm 1 least (1 exp (1/)) times optimal reward,
resulting approximation factor 1/(1 exp (1/)).
case robot different starting finishing location, let Pi set
nodes visited optimal path stage i. Let Oi set nodes visited optimal
path stage i, i.e., Oi = ij=1 Pj , O0 = O1 = P1 . reward collected
746

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

approximation algorithm stage bounded as:
fAi1 (Pi ) 1/(fAi1 (Pi )).
k stages, total collected reward given as:
k
X

k
X
fAi1 (Pi ) 1/(
fAi1 (Pi )).

i=1

(4)

i=1

Since left hand side telescopic sum, get:
k
X

fAi1 (Pi ) = f (ki=1 Pi ) = f (Ak ).

(5)

i=1

right hand side (RHS):
k
X
R.H.S. = 1/(
fAi1 (Pi )),
i=1
k
X
= 1/( (f (Pi Ai1 ) f (Ai1 ))).
i=1

Adding Oi1 terms using submodularity property, get
k
X
R.H.S. 1/( (f (Oi Ai1 ) f (Oi1 Ai1 ))),
i=1

= 1/ [f (O1 ) 0 + f (O2 A1 ) f (O1 A1 ) + + f (Ok Ak1 ) f (Ok1 Ak1 )] .
Rearranging terms, get:
"
R.H.S. 1/ f (Ok Ak1 )

k1
X

#
(f (Oi Ai ) f (Oi Ai1 )) .

i=1

Using monotonicity (f (Ok Ak1 ) f (Ok )) submodularity f ( f (Oi Ai ) f (Oi
Ai1 ) f (Ai ) f (Ai1 )), get
"
#
k1
X
R.H.S. 1/ f (Ok )
(f (Ai ) f (Ai1 )) ,
i=1

= 1/ [f (Ok ) f (Ak1 )] .
Using monotonicity (f (Ak ) f (Ak1 )), get
R.H.S. 1/ [f (Ok ) f (Ak )] .
Substituting Equation (5) (6) Equation (4), get:
f (Ak ) 1/ [f (Ok ) f (Ak )] ,
747

(6)

fiS INGH , K RAUSE , G UESTRIN & K AISER

thus:
f (Ak ) 1/( + 1)f (Ok ).
resulting approximation guarantee (1 + ).
theorem proof inspired proof multi-path orienteering provided
Blum et al. (2003).
Lemma 3. Let P = (s = v0 , v1 , . . . , vl = t) optimal s-t-path solution MIPP, constrained budget B. exists corresponding SD-MIPP path PC = (Cs = Ci1 , . . . , Cin =

Ct ), traversing locations Ai1 Ail , budget 2 2B + 4L collecting
information.
Proof Lemma 3. Let P optimal path MIPP, constrained budget B. need ensure MIPP transformed SD-MIPP, PC corresponding optimal solution,
enough budget PC feasible new problem domain. recall,
new problem domain, SD-MIPP, traveling new cell costs L (distance centroids
adjacent cells), irrespective sensing location within cell.

L

1

2

4

3

5
6

Figure 16: Illustration increased budget requirement SD-MIPP.
corresponding SD-MIPP, optimal path may make 4 experiments 4 different
cells (Cells 1,2,3 4 Fig. 16) sharing common vertex, sensing location different
cell close common vertex, requiring infinitesimally small traveling cost. Increasing budget 4L accounts case. Furthermore, paying additional cost
L traveling two corners edge cell, PC make experiments 2 new
cells (Cells 5,6 Fig. 16. Thus, total number cells visited PC upper bounded
2(B/L) + 4. Hence, budget 2B + 4L suffices render PC feasible SD-MIPP solution.
convert MIPP two-dimensional
Euclidean distance corresponding L1 distance,

budget needs increased 2B ensure P feasible L1 metric. Accounting
conversion Euclidean distance L1 , total budget B required
SD-MIPP,

ensure feasibility optimal solution MIPP, upper bounded 2 2B + 4L.
Lemma 4. Let PC = (Cs = C1 , . . . , Ck = Ct ) optimal solution single robot instance
e optimal set locations selected within
SD-MIPP, constrained budget B,
b
b 11/e I(P ).
visited cell Cj . Let P solution returned eSIP. I(P)
C
1+log k
2

Proof Lemma 4. prove induction length n optimal path. Let Fg (=
(1 1/e)) constant factor due greedy selection sensing locations within cell.
748

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

Also assume budget constraint SD-MIPP problem. case n = 1, iter = 0
Algorithm 4 select greedy subset nodes set Cs = Ct . give
approximation guarantee Fg (Krause et al., 2008) compared optimal set number
observations selected cell (and hence information obtained optimal SD-MIPP
path visiting cell).
Now, assuming induction hypothesis holds n = k/2, get:
Fg
IX (P ),
(1 + log(k/2))
Fg

IX (P ).
log k

IX (P)

hold true traveling budget Bek/2 experimental budget Bek/2 . Let us
analyze case n = k. Let P1 optimal path Cs Ck/2 constrained budget B 0 .
Since increase experimental budget split linearly, B 0 vary 0 Bek , Bek
traveling cost visiting k cells. Since cost less Bek/2 , using induction
hypothesis,
Fg
IX (P1 ).
(7)
IX (P1 )
log k
Similarly, X 0 = X P1 following approximation guarantee holds true P2 :
IX 0 (P2 )

Fg
IX 0 (P2 ).
log k

definition submodular function:
IX 0 (P2 ) = I(P2 P1 X) I(P1 X),
= IX (P1 P2 ) IX (P1 ).
Substituting (8), get
IX 0 (P2 )

Fg
(IX (P1 P2 ) IX (P1 )).
log k

Using monotonicity I,
IX 0 (P2 )

Fg
(IX (P2 ) IX (P)).
log k

Adding (7), finally get:
Fg
(IX (P1 ) + IX (P2 ) IX (P)),
log k
(Fg + log k) IX (P) Fg (IX (P1 ) + IX (P2 )),
IX (P)

(1 + log k) IX (P) Fg (IX (P1 ) + IX (P2 )).
Since IX submodular function,
(1 + log k) IX (P) Fg (IX (P )),
Fg
(IX (P )).
IX (P)
1 + log k

749

(8)

fiS INGH , K RAUSE , G UESTRIN & K AISER

proof inspired analysis recursive greedy algorithm submodular
orienteering proposed Chekuri Pal (2005).
case exponential budget splits, budget needs increased, albeit sub-linearly:
Lemma 7. Let PC = (Cs = Ci1 , . . . , CiN = Ct ) optimal SD-MIPP solution constrained
e Let P solution returned eMIP exponential splits experimental
budget B.
3
e I(P) 11/e I(P ).
budget, started increased budget N log2 2 B.
C
1+log N
Proof Lemma 7. set paths eMIP considers exponential splits let us call
exponential paths general strict subset linear paths considered linear
splits. proof Lemma 4 indeed shows path returned eMIP achieves factor
11/e
1+log N less information optimal exponential path. need show increasing
3
budget factor N log2 2 guarantees optimal linear path feasible exponential path.
Every exponential path represented complete binary tree, whereby every internal node
given level tree corresponds choice middle node experimental budget allocation
left right sub-path corresponding recursion level. Further, every leaf tree
corresponds set observations selected visited cell. Consider tree representing
e inner node, restriction exponential splits lead
optimal linear path budget B.
situation, either left right sub-path receives less experimental budget allocated
optimal path. proof strategy turn new tree 0 , selects
observations corresponds valid exponential path. order achieve this, annotate
inner node v, receives Bv experimental budget optimal linear allocation,
new feasible exponential budget Bv0 Bv . suffices show root R holds
0 (n)log2 3/2 B = (3/2)log2 n B . Label edges 0 1, sub-path
BR
R
R
corresponding edge labeled 1 receives smaller part linear budget split. Hence,
e
leaf v path k ones receives Bv (1/2)k total linear budget requirement B.

0
0
Let us derive bounds Bv bottom up. prove induction Bv (3/2) Bv
height v (distance leaves). suffice condition Br0 (3/2)log2 n Br ,
want prove. leaves v clearly Bv0 = Bv sufficient, since split done hence
reward collected linear exponential split same. Let v inner node
children l r, w.l.o.g., left child l annotated 0. construction, Br Bv /2.
induction hypothesis, Bl0 (3/2)m1 Bl , Br0 (3/2)m1 Br . choose Bv0 = Bl0 + 2Br0 ,
find feasible exponential budget split allocating least Bl0 l Br0 r.
split require increasing budget exponentially till suffice r allocating rest l.
ensure always budget split suffice r exponential budget irrespective
whether represents P1 P2 , need exponential splits sides, trying
exponential increase 0 (Bexp ) Bv Bexp cases r represents P1 P2
respectively. Bv0 (3/2)m1 Bl + 2(3/2)m1 Br = (3/2)m1 Bv + (3/2)m1 Br
(3/2)m Bv .
Theorem 5. Let P optimal solution single robot instance MIPP problem
b achieving information
budget constraint B. Then, eSIP algorithm find solution P


11/e
2

b
)
value least I(P) 1+log N I(P ), whose cost 2(2 2B + 4L)(1 + L Cexp
2


3
fe 2(2 2B + 4L)(1 + L 2 )N log2 2
case linear budget split B
Cexp

fe .
case exponential budget split B
750

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

Proof Theorem 5. Let B budget requirement SD-MIPP according Lemma 4 (or
Lemma 7 case exponential splits) P corresponding solution returned eMIP.
Let Cexp cost making observation sensing location. Maximum number
sensing locations visited P CB
. Since account traveling sensing
exp
locations, additional cost equivalent traveling centroid visited cells
corresponding sensing location paid solution SD-MIPP transformed
back

get solution MIPP. sensing location, maximum additional cost L 2 incurred
traveling sensing location returning back centroid, L length
cell. Thus additional cost
solution path MIPP problem, transformed SD-MIPP

2
problem upper bounded BL
Cexp . Since eMIP considers exponential budget splits
traveling experimental budget, increase budget another factor 2 guarantees
split defined optimal MIPP solution feasible. Combining analysis Lemma 3
Lemma 4 completes proof.

References
Arkin, E. M., Mitchell, J. S. B., & Narasimhan, G. (1998). Resource-constrained geometric network
optimization. Symposium Computational Geometry, pp. 307316.
Bai, X., Kumar, S., Xua, D., Yun, Z., & Lai, T. H. (2006). Deploying wireless sensors achieve
coverage connectivity. Proceedings 7th ACM international symposium
Mobile ad hoc networking computing, pp. 131142.
Batalin, M. A., Rahimi, M., Yu, Y., Liu, D., Kansal, A., Sukhatme, G. S., Kaiser, W. J., Hansen, M.,
Pottie, G. J., Srivastava, M., & Estrin, D. (2004). Call response: experiments sampling
environment. Proceedings 2nd international conference Embedded networked
sensor systems, pp. 2538.
Blum, A., Chawla, S., Karger, D. R., Lane, T., Meyerson, A., & Minkoff, M. (2003). Approximation
algorithms orienteering discounted-reward tsp. Annual Symposium Foundation
Computer Science (FOCS), p. 46.
Blum, A. L., & Furst, M. L. (1997). Fast planning planning graph analysis. Artificial
Intelligence, 90, 16361642.
Borgstrom, P. H., Stealey, M. J., Batalin, M. A., & Kaiser, W. J. (2006). NIMS3D: novel rapidly
deployable robot 3-dimensional applications. IEEE/RSJ International Conference
Intelligent Robots Systems, Beijing, China.
Bourgault, F., Makarenko, A., Williams, S., Grocholsky, B., & Durrant-Whyte, H. (2002). Information based adaptive robotic exploration. IEEE/RSJ International Conference Intelligent
Robots Systems (IROS), pp. 540545.
Brekke, L. D., Miller, N. L., Bashford, K. E., Quinn, N. W., & Dracup, J. A. (2004). Climate change
impacts uncertainty water resources san joaquin river basin, california. Journal
American water resource association, 40, 149164.
Briel, M. V. D., Sanchez, R., Do, M. B., & Kambhampati, S. (2004). Effective approaches partial
satisfaction (over-subscription) planning. AAAI, pp. 562569. AAAI Press.

751

fiS INGH , K RAUSE , G UESTRIN & K AISER

Butt, S. E., & Ryan, D. M. (1999). optimal solution procedure multiple tour maximum
collection problem using column generation. Computers Operations Research, 26, 427
441.
Calinescu, G., Chekuri, C., Pl, M., & Vondrk, J. (2007). Maximizing submodular set function subject matroid constraint (extended abstract). Integer Programming Combinatorial
Optimization (IPCO), Vol. 4513 Lecture Notes Computer Science, pp. 182196.
Caselton, W., & Zidek, J. (1984). Optimal monitoring network design. Statistics Probability
Letters.
Chao, I.-M., Golden, B. L., & Wasil, E. A. (1996). fast effective heuristic orienteering
problem. European Journal Operations Research, 88, 475489.
Chekuri, C., Korula, N., & Pal, M. (2008). Improved algorithms orienteering related problems. Proc. 19th Annual ACM-SIAM Symposium Discrete Algorithms (SODA08).
SIAM. appear.
Chekuri, C., & Pal, M. (2005). recursive greedy algorithm walks directed graphs. Annual
Symposium Foundation Computer Science (FOCS), pp. 245253.
Christofides, N. (1976). Worst-case analysis new heuristic traveling salesman problem.
Tech report,CMU.
Cressie, N. A. C. (1991). Statistics Spatial Data. Wiley.
Dhariwal, A., Zhang, B., Stauffer, B., Oberg, C., Sukhatme, G. S., Caron, D. A., & Requicha, A. A.
(2006). Networked aquatic microbial observing system. IEEE International Conference
Robotics Automation (ICRA).
Feillet, D., Dejax, P., & Gendreau, M. (2005). Traveling salesman problem profits. Transportation Science, 39(2), 188205.
Garg, N. (2005). Saving epsilon: 2-approximation k-mst problem graphs. ACM
Symposium Theory Computing (STOC), pp. 396402.
Golden, B., Levy, L., & Vohra, R. (1987). orienteering problem. Naval Research Logistics, 34,
307318.
Goundan, P. R., & Schulz, A. S. (2008). Revisiting greedy approach submodular set function
maximization.. Working paper, MIT.
Guestrin, C., Krause, A., & Singh, A. P. (2005). Near-optimal sensor placements gaussian processes. International Conference Machine Learning (ICML).
Harmon, T. C., Ambrose, R. F., Gilbert, R. M., Fisher, J. C., Stealey, M., & Kaiser, W. J. (2007).
High-resolution river hydraulic water quality characterization using rapidly deployable
networked infomechanical systems (NIMS RD). Environmental Engineering Science, 24(2),
151159.
I-Ming, C., Golden, B., & Wasil, E. (1996). team orienteering problem. European Journal
Operation Research, 88, 464474.
Ibaraki, T., Muro, S., Murakami, T., & Hasegawa, T. (1983). Using branch-and-bound algorithms
obtain suboptimal solutions. Mathematical Methods Operations Research, 27(1), 177202.

752

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

Ishikawa, T., & Tanaka, M. (1993). Diurnal stratification effects wind-induced currents
water qualities lake kasumigaura, japan. Journal Hydraulic Research, 31(3), 307
322.
Johnson, D. S., Minkoff, M., & Phillips, S. (2000). prize collecting steiner tree problem: theory
practice. Symposium Discrete Algorithms (SODA), pp. 760769.
Jordan, B. L., Batalin, M. A., & Kaiser, W. J. (2007). NIMS RD: rapidly deployable cable based
robot. IEEE International Conference Robotics Automation (ICRA), Rome, Italy.
Kataoka, S., & Morito, S. (1988). algorithm single constraint maximum collection
problem. Journal Operational Research Society Japan, 31, 515530.
Ko, C.-W., Lee, J., & Queyranne, M. (1995). exact algorithm maximum entropy sampling.
Operations Research, 43(4), 684691.
Krause, A., & Guestrin, C. (2007). Near-optimal observation selection using submodular functions.
AAAI Nectar track.
Krause, A., Singh, A., & Guestrin, C. (2008). Near-optimal sensor placements Gaussian processes: Theory, efficient algorithms empirical studies. Journal Machine Learning
Research (JMLR), Vol. 9, pp. 235284.
Krause, A., & Guestrin, C. (2007). Nonmyopic active learning gaussian processes:
exploration-exploitation approach. International Conference Machine Learning
(ICML), pp. 449456.
Krause, A., Guestrin, C., Gupta, A., & Kleinberg, J. (2006). Near-optimal sensor placements: Maximizing information minimizing communication cost. Proceedings fifth international conference Information processing sensor networks (IPSN), pp. 210.
Laporte, G., & Martello, S. (1990). selective travelling salesman problem. Discrete Applied
Mathematics, 26, 193207.
Lau, H., Huang, S., & Dissanayake, G. (2006). Probabilistic search moving target
indoor environment. IEEE/RSJ International Conference Intelligent Robots Systems
(IROS), pp. 33933398.
Lin, S. (1965). Computer solutions traveling salesman problem. Bell System Technical
Journal, 44, 22452269.
MacIntyre, S. (1993). Vertical mixing shallow, eutrophic lake: Possible consequences
light climate phytoplankton. Limnology Oceanography, 38(4), 798817.
MacIntyre, S., Romero, J. R., & Kling, G. W. (2002). Spatial-temporal variability surface layer
deepening lateral advection embayment lake victoria, east africa. Limnology
Oceanography, 47(3), 656671.
Meliou, A., Krause, A., Guestrin, C., & Hellerstein, J. M. (2007). Nonmyopic informative path
planning spatio-temporal models. Association Advancement Artificial Intelligence
(AAAI), pp. 602607.
Nagarajan, V., & Ravi, R. (2007). Poly-logarithmic approximation algorithms directed vehicle
routing problems. Proc. 10th Internat. Workshop Approximation Algorithms Combinatorial Optimization Problems (APPROX07), Vol. 4627 LNCS, pp. 257270. Springer.
753

fiS INGH , K RAUSE , G UESTRIN & K AISER

Nemhauser, G., Wolsey, L., & Fisher, M. (1978). analysis approximations maximizing
submodular set functions. Mathematical Programming, 14, 265294.
Pon, R., Batalin, M., Gordon, J., Rahimi, M., Kaiser, W., Sukhatme, G., Srivastava, M., & Estrin,
D. (2005). Networked infomechanical systems: mobile wireless sensor network platform.
Proceedings fifth international conference Information processing sensor networks (IPSN), pp. 376381.
Rahimi, M., Pon, R., Kaiser, W., Sukhatme, G., Estrin, D., & Srivastava, M. (2004). Adaptive
sampling environmental robotics. IEEE International Conference Robotics
Automation (ICRA).
Rasmussen, C. E., & Williams, C. K. (2006). Gaussian Process Machine Learning. Adaptive
Computation Machine Learning. MIT Press.
Reynolds-Fleming, J. V., Fleming, J. G., & Luettich, R. A. (2004). Portable autonomous vertical
profiler estuarine applications. Estuaries, 25, 142147.
Roy, N., & Earnest, C. (2006). Dynamic action spaces information gain maximization search
exploration. American Control Conference.
Ryan, M. R. K. (2008). Exploiting subgraph structure multi-robot path planning. Journal
Artificial Intelligence Research (JAIR), Vol. 31, pp. 497542.
Sim, R., & Roy, N. (2005). Global a-optimal robot exploration slam. IEEE International
Conference Robotics Automation (ICRA).
Simmons, R. G., Apfelbaum, D., Burgard, W., Fox, D., Moors, M., Thrun, S., & Younes, H. (2000).
Coordination multi-robot exploration mapping. Association Advancement
Artificial Intelligence (AAAI), pp. 852858.
Singh, A., Nowak, R., & Ramanathan, P. (2006). Active learning adaptive mobile sensing networks. Proceedings fifth international conference Information processing
sensor networks (IPSN), pp. 6068.
Singh, A., Batalin, M. A., Chen, V., Stealey, M. J., Jordan, B., Fisher, J., Harmon, T., Hansen, M., &
Kaiser, W. J. (2007a). Autonomous robotic sensing experiments san joaquin river. IEEE
International Conference Robotics Automation (ICRA), pp. 49874993, Rome, Italy.
Singh, A., Krause, A., Guestrin, C., Kaiser, W. J., & Batalin, M. A. (2007b). Efficient planning
informative paths multiple robots. International Joint Conference Artificial Intelligence (IJCAI), pp. 22042211, Hyderabad, India.
Smith, D. E. (2004). Choosing objectives over-subscription planning. International Conference
Automated Planning Scheduling (ICAPS).
Somasundara, A. A., Ramamoorthy, A., & Srivastava, M. B. (2007). Mobile element scheduling
dynamic deadlines. IEEE Transactions Mobile Computing, Vol. 6, pp. 395410.
Stachniss, C., Grisetti, G., & Burgard, W. (2005). Information gain-based exploration using raoblackwellized particle filters. Robotics Science Systems (RSS).
Tang, L., & Wang, X. (2006). Iterated local search algorithm based large-scale neighborhood prize-collecting vehicle routing problem. International Journal Advanced
Manufacturing Technology, 113.
754

fiE FFICIENT NFORMATIVE ENSING USING ULTIPLE ROBOTS

Thompson, D. R., & Wettergreen, D. (2008). Intelligent maps autonomous kilometer-scale science survey. International Symposium Artificial Intelligence, Robotics Automation
Space (iSAIRAS).
Trummel, K. E., & Weisinger, J. R. (1986). complexity optimal searcher path problem.
Operations Research, 34(2), 324327.
Zhang, W., & Korf, R. E. (1995). Performance linear-space search algorithms. Artificial Intelligence, 79(2), 241292.

755

fiJournal Artificial Intelligence Research 34 (2009) 605635

Submitted 11/08; published 04/09

Inferring Shallow-Transfer Machine Translation Rules
Small Parallel Corpora
Felipe Sanchez-Martnez
Mikel L. Forcada

fsanchez@dlsi.ua.es
mlf@dlsi.ua.es

Departament de Llenguatges Sistemes Informatics
Universitat dAlacant, E-03071 Alacant (Spain)

Abstract
paper describes method automatic inference structural transfer rules
used shallow-transfer machine translation (MT) system small parallel
corpora. structural transfer rules based alignment templates, like used
statistical MT. Alignment templates extracted sentence-aligned parallel corpora
extended set restrictions derived bilingual dictionary
MT system control application transfer rules. experiments conducted
using three different language pairs free/open-source MT platform Apertium show
translation quality improved compared word-for-word translation (when
transfer rules used), resulting translation quality close obtained
using hand-coded transfer rules. method present entirely unsupervised
benefits information rest modules MT system inferred
rules applied.

1. Introduction
Machine translation (MT) may defined use computer translate text
one natural language, source language (SL), another, target language (TL). MT
difficult mainly natural languages highly ambiguous also two
languages always express content way (Arnold, 2003).
different ways MT problem approached may classified
according nature knowledge used development MT system.
point view, one distinguish corpus-based rule-based approaches;
although, hybrid approaches also possible.
Corpus-based approaches MT, example-based MT (EBMT; Nagao, 1984;
Carl & Way, 2003) statistical MT (SMT; Brown et al., 1993; Knight, 1999), use large
collections parallel texts source knowledge engine learns
perform translations. parallel text text one language together translation
another language; large collection parallel texts usually referred parallel
corpus. Although corpus-based approaches MT grown interest last
years, require large amounts, order tens millions words, parallel
text achieve reasonable translation quality (Och, 2005). vast amount parallel
corpora available many under-resourced language pairs demanding MT services.
Rule-based MT (RBMT) systems use knowledge form rules explicitly coded
human experts attempt codify translation process. RBMT systems heavily depend linguistic knowledge, morphological bilingual dictionaries (containing
c
2009
AI Access Foundation. rights reserved.

fiSanchez-Martnez & Forcada

SL
text

Analysis

SL IR

Transfer

TL IR

Generation

TL
text

Figure 1: Scheme general transfer-based MT system.

lexical, syntactic even semantic information), part-of-speech disambiguation rules
manually disambiguated corpora, large set rules. process building RBMT
system involves considerable human effort order develop necessary linguistic resources (Arnold, 2003).
Generally, RBMT systems work parsing (or analyzing) SL text, usually creating
intermediate (symbolic) representation (IR), text TL generated (Hutchins & Somers, 1992). According nature IR used, RBMT system
may said either interlingua transfer-based. interlingua MT system uses
single IR independent languages involved translation; advantage
using language-independent IR transfer module needs developed
new language pair; disadvantage IR used difficult design hard
implement, even so, open-domain tasks. contrast, transfer-based MT system
uses two IRs, one languages involved; advantage easing
design development IRs used, cost develop transfer
module new language pair.
Transfer-based MT systems usually work applying, addition lexical transfer
mappings, set structural transfer rules SL IR created analysis,
order transform TL IR TL text finally generated (see
Figure 1). level analysis, therefore degree abstraction provided IR,
varies depending related languages involved are. Translating distant
languages (such English Japanese) requires deep analysis (syntactic semantic),
translation related languages (for example Romance languages)
achieved shallow parsing. call last type transfer-based systems
shallow-transfer MT systems.
1.1 Overview
paper focuses automatic inference small parallel corpora set structural (shallow-)transfer rules used shallow-transfer RBMT systems convert
SL IR TL IR TL text generated. development
transfer rules requires qualified people code manually; therefore, automatic
inference may save part human effort. method present entirely unsupervised benefits information rest modules MT system
inferred rules applied, line method proposed Sanchez-Martnez et al.
(2008) train part-of-speech taggers unsupervised way use MT.
approach existing bilingual dictionary used guide inference structural transfer rules (see below), bilingual entries dictionary learned.
approach aimed inference transfer rules small parallel
corpora1 application open-domain tasks. Note small parallel corpora may
1. Small compared size corpora commonly used build corpus-based MT systems (Och, 2005).

606

fiInferring Shallow-Transfer MT Rules Small Parallel Corpora

insufficient obtain wide-coverage bilingual dictionaries, demonstrated results
obtained translating state-of-the-art SMT system trained small
parallel corpora (see section 5). Notice manually building bilingual dictionary
language pair usually much easier developing shallow structural transfer rules it,
moreover, former task partially automated.
method propose automatic inference shallow-transfer rules parallel corpora based alignment template (AT) approach initially proposed
use SMT framework (Och, 2002; Och & Ney, 2004). defined
generalization performed aligned phrase2 pairs (or translation units) using word
classes.
adapt approach RBMT framework, ATs extended set
restrictions control application structural shallow-transfer rules. end:
bilingual dictionary RBMT system inferred rules
integrated used ensure lexical content bilingual phrase pair
extracted training corpus (see section 2.2) reproduced MT
system;
linguistically motivated word classes used generalize extracted bilingual
phrase pairs, deriving ATs them; and,
set restrictions, derived bilingual dictionary RBMT system,
attached control application part transfer rule; extension
definition called extended AT.
extended ATs extracted training corpora, transfer rules
generated them. experiments reported section 5, shallow-transfer rules
used Apertium MT engine (see appendix A) generated directly Apertiums
XML-based structural transfer language. interesting property inferred rules
human-readable may, therefore, edited human experts improve
performance supplemented new rules; MT developers use method
infer initial set rules improve focusing difficult issues.
method (Sanchez-Martnez & Forcada, 2007) predecessor (SanchezMartnez & Ney, 2006) already presented conferences; here, explain
method detail, test two additional language pairs use training corpora
different sizes evaluate impact size translation quality. Moreover,
paper perform detailed analysis inferred rules results obtained;
end, provide confidence intervals, allow better interpretation
results achieved. also discuss process followed build parallel corpora used
learn transfer rules.
2. purpose paper, stick terminology used Och Ney (2004)
definition SMT practitioners, phrase refer text segment, necessarily
well-formed syntactic constituent.

607

fiSanchez-Martnez & Forcada

1.2 Related Work
attempts learn automatically semi-automatically structural
transformations needed produce correct translations TL. approaches
classified according translation framework learned rules applied.
approaches learn transfer rules used RBMT. Probst et al. (2002)
Lavie et al. (2004) developed method learn transfer rules MT involving underresourced languages (such Quechua) limited resources. end, small
parallel corpus (of thousand sentences) built help small set bilingual
speakers two languages. parallel corpus obtained translating controlled
corpus language resources (English Spanish) under-resourced
language means elicitation tool. tool also used graphically annotate
word alignments two sentences. Finally, hierarchical syntactic rules,
seen constituting context-free transfer grammar, inferred aligned
parallel corpus.
Menezes Richardson (2001) propose method infer transfer mappings (rules)
source target languages. Prior acquisition transfer mappings,
align nodes source target parse trees using existing bilingual lexicon
look word correspondences. Then, following best-first strategy using
small alignment grammar method aligns remaining (not-aligned) nodes.
alignments nodes parse trees obtained, frequencies
computed sufficient context retained disambiguate competing mappings
translation time. approach greatly differs one Menezes Richardson:
(i) use syntactic parser, bilingual dictionary alignment grammar
obtain word alignments sentence-aligned parallel corpus, use
statistical methods; (ii) use bilingual dictionary, use discard
useless bilingual phrases derive restrictions control application ATs,
computation word alignments; (iii) approach
ambiguity solve translation time.
Caseli et al. (2006) propose method infer bilingual resources (structural transfer
rules bilingual dictionaries) used shallow-transfer MT aligned parallel
corpora. Previously generation transfer rules, alignment blocks (sequences
aligned words) built translation examples found parallel corpus
considering three different types word alignments according geometry (crossings,
unaligned words, etc.). Then, shallow-transfer rules built three-step procedure.
first step, identify patterns two phases, monolingual bilingual;
second step method generates shallow-transfer rules deriving monolingual
bilingual constraints, also seen rule itself; finally, third step rules
filtered order solve ambiguity caused rules matching SL sequence
words. inferred rules human-readable, inferred method
propose, may therefore also edited human experts. approach differs
Caseli et al. rules induced: approach uses bilingual phrase
pairs without concerned type alignments words, way
Caseli et al. induce rules depends type alignment blocks. addition,
approach ever produce one rule matching sequence
608

fiInferring Shallow-Transfer MT Rules Small Parallel Corpora

SL items, therefore ambiguity needs solved. Furthermore, infer
bilingual dictionary; instead, use existing bilingual dictionary guide inference
shallow-transfer rules, control application inferred rules.
EBMT framework, researchers dealt problem inferring
kind translation rules called translation templates (Kaji et al., 1992; Brown, 1999; Cicekli
& Guvenir, 2001). translation template defined bilingual pair sentences
corresponding units (words phrases) coupled replaced variables. Liu
Zong (2004) provide interesting review different research works dealing
translation templates. Brown (1999) uses parallel corpus linguistic knowledge
form equivalence classes (both syntactic semantic) perform generalization
bilingual examples collected. method works replacing word
corresponding equivalence class using set grammar rules replace patterns
words tokens general tokens. Cicekli Guvenir formulate acquisition
translation templates machine learning problem, translation templates
learned differences similarities observed set different translation
examples, using morphological information all. Kaji et al. use bilingual dictionary
syntactic parser determine correspondences translation units
learning translation templates. approach differs applied EBMT
framework because, one hand, transfer rules generated method
propose mainly based lexical forms (consisting lemma, lexical category
morphological inflection information) and, hand, flatter,
less structured non-hierarchical, makes suitable shallow-transfer MT.
Moreover, way translation rules chosen application greatly differs
chosen EBMT framework.
Finally, SMT framework use (Och & Ney, 2004) seen
integration translation rules statistical translation models, since generalization abstraction, transformations apply translating SL TL
using word classes.
rest paper organized follows: next section reviews alignment
template (AT) approach; section 3 explains ATs extended set restrictions
order use generate shallow-transfer rules used RBMT (section 4).
Section 5 describes experiments conducted results achieved. Finally, section 6
discusses method described outlines future research lines.

2. Alignment Template Approach
alignment template (AT) approach (Och, 2002; Och & Ney, 2004) introduced
SMT framework one feature functions maximum entropy model (Och
& Ney, 2002) try generalize knowledge learned specific phrase similar
phrases.
performs generalization bilingual phrase pairs using word classes instead
words. z = (Sm , Tn , A) consists sequence Sm SL word classes, sequence
Tn n TL word classes, set pairs = {(i, j) : [1, n] j [1, m]}
alignment information TL SL word classes two sequences.
609

fiSanchez-Martnez & Forcada

met



request
personal


mi nal ha ido ch
e
c
sf
ti ers
e
ti
p p
sa

Figure 2: Alignment words English sentence personal request
met Spanish sentence mi peticion personal ha sido satisfecha. alignment
information represented binary matrix.

Learning set ATs sentence-aligned parallel corpus consists of: (i) computation word alignments, (ii) extraction bilingual phrase pairs, (iii)
generalization bilingual phrase pairs using word classes instead words
themselves.
2.1 Word Alignments
variety methods, statistical (Och & Ney, 2003) hybrid (Caseli et al., 2005),3 may
used compute word alignments (sentence-aligned) parallel corpus. experiments reported section 5, word alignments obtained training classical statistical
translation models translate language L1 language L2 (and vice versa)
computing Viterbi alignments previously estimated translation models.
Viterbi alignment SL TL sentences defined alignment whose probability maximal translation models previously estimated. resulting Viterbi
alignments A1 A2 (one translation direction) symmetrized
refined intersection method proposed Och Ney (2003, p. 33). Symmetrization
needed order allow SL word aligned one TL word; otherwise,
wrong alignments obtained SL word actually corresponds one TL
word.
Figure 2 shows word alignment SpanishEnglish sentence pair. alignment
information represented binary matrix value 1 (large black squares)
means words corresponding positions aligned; analogously, value 0
(small black squares) means words aligned.

3. Caseli et al.s (2005) method hybrid prior application heuristics, uses statistical
tool (NATools) obtain probabilistic bilingual dictionary (Simoes & Almeida, 2003).

610

fiInferring Shallow-Transfer MT Rules Small Parallel Corpora

2.1.1 Training
order train translation models calculate Viterbi alignments pair
aligned sentences found training corpus free/open-source GIZA++ toolkit4 (Och
& Ney, 2003) used default parameters.
computation word alignments consists of:
1. training IBM model 1 (Brown et al., 1993) 5 iterations; model, word
order affect alignment probabilities;
2. training HMM alignment model (Vogel et al., 1996) 5 iterations; alignment
model property making alignment probabilities explicitly dependent
alignment position previous word;
3. training IBM model 3 (Brown et al., 1993) 5 iterations; model,
probability alignment depends positions aligned words
length SL TL sentences. addition, IBM model 3 also introduces fertilities;
fertility word defined number aligned words language.
finally,
4. training IBM model 4 (Brown et al., 1993) 5 iterations; model identical
IBM model 3 except fact models reordering phrases may
moved around units.
Note obtaining Viterbi alignments statistical translation models
longer used.
2.2 Extraction Bilingual Phrase Pairs
Bilingual phrase pairs automatically extracted word-aligned sentence pairs.
Usually, extraction bilingual phrase pairs (Zens et al., 2002) performed considering possible pairs certain length ensuring that: (i) words consecutive,
(ii) words within bilingual phrase pair aligned words outside.
set BP(wS J1 , wT I1 , A) bilingual phrases extracted word-aligned
sentence pair (wS1 , . . . , wSJ ), (wT1 , . . . , wTI ) may formally expressed follows:
, wT i+n
):
BP(wS J1 , wT I1 , A) = {(wS j+m
j

(i0 , j 0 ) : j j 0 j + i0 + n}.

However, approach bilingual phrase pairs also required first last
words sides (source target) aligned least one word side.5
Integrating additional constraints, previous equation may rewritten as:
, wT i+n
):
BP(wS J1 , wT I1 , A) = {(wS j+m
j

4. http://www.fjoch.com/GIZA++.html
5. Experiments conducted without requirement show significant degradation translation quality achieved inferred rules.

611

fiSanchez-Martnez & Forcada

((i0 , j 0 ) : j j 0 j + i0 + n)
(k [i, + n] : (wSj , wTk ) A)
(k 0 [i, + n] : (wSj+m , wTk0 ) A)
(l [j, j + m] : (wSl , wTi ) A)
(l0 [j, j + m] : (wSl0 , wTi+n ) A)}.
Figure 3 shows set bilingual phrase pairs one SL word extracted
word-aligned SpanishEnglish sentence pair shown Figure 2.
2.3 Generalization
generalization bilingual phrase pairs simply done using word classes instead
words themselves; end, function maps single words word classes
defined. use word classes allows description word reorderings, preposition
changes divergences SL TL. Och Ney (2004) use automatically
obtained (Och, 1999) word classes extract ATs SMT. However, RBMT, linguistically motivated word classes related used remaining modules MT
system must used (see section 3.1).

3. Alignment Templates Shallow-Transfer Machine Translation
apply approach shallow-transfer MT system, parallel corpus
ATs learned must intermediate representation (IR) used translation
engine. shallow-transfer MT transformations apply mainly related lexical
forms; therefore, IR used translation engine usually consists lemma, lexical
category morphological inflection information word.
order convert parallel corpus IR used engine, analysis
modules (morphological analyzers part-of-speech taggers) engine used
analyze sides parallel corpus computing word alignments.
analyzing sides parallel corpus have, word, lemma, lexical category
morphological inflection information. Note generalizations performed
word alignments bilingual phrase pair extraction using word classes based
morphological information (see next section).
3.1 Word-Class Definition
transformations apply mainly based lexical category inflection
information SL TL words, function maps words word classes map
word word class representing lexical category morphological inflection
information (such verb, preterite tense, third person, plural).
Using lexical category morphological inflection information define set
word classes allows method learn general syntactic rules reordering
agreement rules, verb tense changes, among others. However, order learn lexical
changes, preposition changes auxiliary verb usage, words assigned
single-word classes representing lexical form, discussed next.
612

fiInferring Shallow-Transfer MT Rules Small Parallel Corpora

request
personal
n l
ona
c
ti rs
pe pe








met



doech

sf
ti
sa

ha



request
ha ido personal

n l
ona n h
c
ti rs
pe pe



request
personal

mi nal ha
c
ti ers
e
p p



request
personal

mi nal ha ido
c

ti ers
e
p p

request
personal

mi nal
c
ti ers
e
p p
met




ha ido ch
e
sf
ti
sa




request
personal
n al ha
ci son
si

er
e
p p
met



request
personal

n nal ha ch


e
c
sf
ti ers
e
ti
p p
sa

Figure 3: Set bilingual phrase pairs (see section 2.2) extracted word-aligned Spanish
English sentence pair shown Figure 2. Note bilingual phrase pairs containing one word
whole word-aligned sentence pair omitted.

613

fiSanchez-Martnez & Forcada

Paulo

estar
van
n n
ro e ul
e
Pa
vi
u



es

(noun.loc)
a-(pr)
(verb.inf)
anar-(vaux.pres.3rd.pl)

)
pl r) c)
d. n -(p .lo
r
t.3 e oun

(n
p
.

(v

b
er

R = {w1 =verb.*, w3 =noun.*}
Figure 4: Example SpanishCatalan bilingual phrase (left), corresponding (right)
obtained word replaced corresponding word class, TL restrictions (see
section 3.2) Spanish-to-Catalan translation. Words boldface correspond lexicalized
categories (see section 3.1). Word classes horizontal axis correspond SL (Spanish)
vertical axis TL (Catalan).

3.1.1 Lexicalized Categories
set (lexicalized) categories usually involved lexical changes prepositions
auxiliary verbs may provided. words whose lexical category set
lexicalized categories (from on, lexicalized words) lemma also used defining
word class belong to. way, lexicalized words placed single-word
classes representing particular lexical form. example, prepositions considered
lexicalized categories, words would different word classes, even
lexical category morphological inflection information, whereas words book
house would word class (noun, singular).
Typically set lexicalized categories subset set closed categories,
is, grow addition new words lexicon: pronouns, auxiliary
verbs, prepositions, conjunctions, etc. typical lexicalized words prepositions,
usually many different translations depending SL context.
Figure 4 shows example SpanishCatalan bilingual phrase generalization performed word replaced corresponding word class; words
boldface correspond lexicalized categories. shown Figure 4 generalizes,
one hand, use auxiliary Catalan verb anar express past perfect
(preterite) tense and, hand, preposition change refers location name, name city country. Note lexicalized words (e.g.
anar-(vaux.pres.3rd.pl), en-(pr)) coexist non-lexicalized categories (e.g. (verb.inf), (noun.loc)) without distinction.
3.2 Extending Definition Alignment Template
section 2 defined tuple z = (Sm , Tn , A) alignment
SL TL word classes considered. definition extended
z = (Sm , Tn , A, R), set restrictions, R, TL inflection information
non-lexicalized categories, added control application part transfer rule.
614

fiInferring Shallow-Transfer MT Rules Small Parallel Corpora

(adj.m.sg)
(noun.m.sg)
el-(art.m.sg)
)
g) g )
.s .f.s .f.sg
f
.
rt un dj
-(a (a
el (

R = {w2 =noun.m.*, w3 =adj.*}
Figure 5: SpanishCatalan TL restrictions inflection information Spanishto-Catalan translation (see section 3.2).

3.2.1 TL Restrictions
translating (see section 4.3.1), is, applying inferred ATs, TL
inflection information non-lexicalized words taken corresponding (aligned)
TL word class applied, bilingual dictionary; this,
restrictions needed order prevent applied certain conditions
would produce incorrect translation.
illustrate need restrictions let us consider would happen
translating Spanish phrase la silla roja 6 Catalan applying extended
shown Figure 5, applied case. generalizes
propagation masculine gender article adjective translating SL
(Spanish) noun feminine singular SL (same article adjective)
masculine equivalent Catalan, case silla. applying
extended Figure 5, morphological generator (see Appendix A) inflect
lexical form cadira-(noun.m.sg), exist Catalan,7 cadira feminine.
taking account restrictions TL inflection information, one
referring w2 extended Figure 5, prevent application
application would produce incorrect lexical form inflect, running example.
TL restrictions obtained bilingual dictionary MT system
inferred transfer rules integrated. Bilingual dictionaries may explicitly code
inflection information translation SL lexical form, inflection
information changes one language other. TL restrictions could derived
kinds bilingual dictionaries; however, extraction easier second
case, is, changes inflection information explicitly coded.
experiments (see section 5) Apertium MT platform used; Apertium bilingual dictionaries, changes inflection information explicitly coded.
following two examples show, one hand, SpanishCatalan bilingual entry and,
hand, restriction TL inflection information Spanish-to-Catalan
translation derived bilingual entry:8
6. Translated English red chair.
7. Note lexical category morphological inflection information TL lexical form inflect
taken TL part AT.
8. Lemmas tags <l> </l> (left) correspond Spanish words; analogously, lemmas
tags <r> </r> (right) correspond Catalan words. Lexical category inflection information
coded tag <s> (symbol ), first one lexical category.

615

fiSanchez-Martnez & Forcada

Bilingual entry without change inflection information
<e><p>
<l>castigo<s n="noun"/></l>
<r>castig<s n="noun"/></r>
</p></e>
Restriction: w=noun.*
Bilingual entry gender changes feminine (Spanish) masculine
(Catalan)
<e><p>
<l>calle<s n="noun"/><s n="f"/></l>
<r>carrer<s n="noun"/><s n="m"/></r>
</p></e>
Restriction: w=noun.m.*
seen, restrictions provide lexical category morphological inflection information lexical form translation time looking
bilingual dictionary; star end restriction means rest inflection information restricted. second bilingual entry would responsible
restrictions attached w2 shown Figure 5. applied
noun (w2 ) masculine TL (see next section know ATs applied); note
inflection information w3 restricted all; w3 refers
adjective masculine feminine, gender depends gender
noun qualifies.

4. Generation Apertium Transfer Rules
section describes automatic generation Apertium structural shallow-transfer
rules; note, however, generation transfer rules shallow-transfer MT
systems would also feasible following approach presented here.
structural transfer Apertium (see appendix A) uses finite-state pattern matching
detect, usual left-to-right, longest-match way, fixed-length patterns lexical forms
process performs corresponding transformations. (generic) shallow-transfer
rule consists sequence lexical forms detect transformations need
applied them.
4.1 Discarding Useless Bilingual Phrase Pairs
bilingual phrase pairs useful inference transfer rules, since generalization would performed cannot used RBMT;
precisely, bilingual phrase pairs satisfying one following conditions useless,
therefore, discarded:
616

fiInferring Shallow-Transfer MT Rules Small Parallel Corpora

SL TL non-lexicalized words aligned. translating SL nonlexicalized word (see next section) inflection information taken aligned
TL word class, therefore corresponding alignment must exist.
bilingual phrase pair cannot reproduced MT system transfer rules used. happens translation equivalent bilingual
dictionary differs one observed bilingual phrase. Note TL restrictions extracted bilingual dictionary, translation equivalents
agree extracted could end set restrictions making sense
all.
4.2 Selecting Alignment Templates Use
decide ATs take account generation rules, method provided
frequency count threshold. ATs whose frequency count threshold
discarded. experiments, two different ways interpreting frequency count
tested:
use directly frequency count c,
use modified frequency count c0 = c(1 + log(l)), l stands length
SL part AT.
second approach aims solving problem caused fact longer ATs
lower frequency counts may accurate take context account.
similar approach used (Mikheev, 1996) work learning part-of-speech
guessing rules favor longer suffixes shorter ones.
4.3 Rule Generation
rule consists set U extended ATs sequence SL word classes,
different sequences TL word classes, different alignment information different set
TL restrictions. Formally may expressed follows:
U = {(Sm , Tn , A, R) Z : Sm = U },

(1)

Z refers whole set ATs U sequence SL word classes
ATs U common. Note rule matches different sequence U SL word
classes and, therefore, ambiguity application shallow-transfer rules
translation time.
rule U coded Apertiums XML-based transfer language. code generated
rule applies always frequent U satisfies TL restrictions R;
therefore, competing ATs selected according frequency. default AT,
translates word word, always added lowest frequency count.
TL restrictions one applied none remaining ATs applied
TL restrictions met.
check restrictions TL inflection information met,
translation non-lexicalized word retrieved bilingual dictionary; then,
617

fiSanchez-Martnez & Forcada

retrieved morphological attributes (lexical category inflection information) compared specified corresponding restriction; applicable
restrictions hold.
4.3.1 Application Alignment Template
code generated Apertiums XML-based transfer language applies
guided sequence Tn TL word classes. actions perform unit Tn
depend type word class:

word class corresponds non-lexicalized word, translation lemma
aligned SL (non-lexicalized) word retrieved looking bilingual dictionary; then, lexical category morphological inflection information
provided TL word class attached translated lemma;

word class corresponds lexicalized word, introduced is; remember
word classes belonging lexicalized words represent complete lexical forms consisting
lemma, lexical category morphological inflection information.

Note information SL lexicalized words taken account applying given (just detecting it).
following example illustrates shown Figure 4 would applied
order translate Spanish Catalan input text vivieron en Francia.9
text segment, morphological analysis part-of-speech tagging, transformed
MT engine SL IR vivir -(verb.pret.3rd.pl) en-(pr) Francia-(noun.loc),
becomes input structural transfer module. applied order
specified TL part. word classes corresponding non-lexicalized words,
aligned SL words translated TL (Catalan) looking bilingual
dictionary: vivir translated viure Francia translated Franca. Then,
inflection information provided TL part (see Figure 4) attached
translated lemma. Finally, word classes corresponding lexicalized words copied
output appear TL part AT. running example structural
transfer output would TL IR anar -(vaux.pres.3rd.pl) viure-(verb.inf) a-(pr)
Franca-(noun.loc), morphological generation module would transform
Catalan phrase van viure Franca.

9. Translated English lived France.

618

fiInferring Shallow-Transfer MT Rules Small Parallel Corpora

5. Experiments
approach presented paper tested translation directions
SpanishCatalan (es-ca) SpanishGalician (es-gl) language pairs,
Spanish-to-Portuguese (es-pt) translation.10,11
parallel corpora used training different sources. SpanishCatalan
parallel corpora come El Periodico de Catalunya,12 daily newspaper published
Catalan Spanish; SpanishGalician parallel corpora come Diario Oficial de
Galicia,13 official publication autonomous government Galicia published
Galician Spanish; SpanishPortuguese parallel corpora come JRCAcquis Multilingual Parallel Corpus (Steinberger et al., 2006)14 contains European
Union (EU) law applicable EU member states.
test importance amount parallel corpora available training
used corpora different sizes. precisely, used training corpora around
0.25, 0.5, 1.0, 1.5, 2.0 million words language. corpora built
way that, language pair, larger corpora include shorter ones. Note
word alignments computed different training corpus isolation
extraction extended ATs used inference shallow-transfer
rules.
explained section 3.1, set categories usually involved lexical changes
needs provided definition word classes learn syntactic
transformations, also lexical transformations. end, small set eight ten
lexicalized categories used language. common lexicalized categories
are: prepositions, pronouns, determiners, subordinate conjunctions, relatives, modal verbs
auxiliary verbs.
length bilingual phrase pairs extracted used obtain ATs
restricted maximum 7 SL words experiments. Remember section 2.2
extract bilingual phrases pair word-aligned sentences possible pairs
(within certain length) considered; restricting length making
problem computationally affordable.
respect frequency count threshold used select set ATs take
account (see section 4.2), tested frequency count thresholds 5 40
translation tasks selection criteria. frequency count used evaluation
one giving best translation edit rate (TER; Snover et al., 2006) translating
corpus, similar one used testing, 1 000 sentences (see Table 1); Table 5
(page 627) provide thresholds used rules inferred corpus
2.0 million words language.
10. linguistic data used freely downloaded http://sf.net/projects/apertium, packages
apertium-es-ca-1.0.2 (around 12 800 bilingual entries), apertium-es-gl-1.0.4 (around 10 800 bilingual entries) apertium-es-pt-0.9.2 (around 11 000 bilingual entries); number bilingual entries
reported correspond lemma-based entries.
11. possible criticism used standard translation task test approach;
done Apertium linguistic resources (morphological bilingual dictionaries)
necessary standard tasks available.
12. http://www.elperiodico.com
13. http://www.xunta.es/diario-oficial
14. http://wt.jrc.it/lt/Acquis/

619

fiSanchez-Martnez & Forcada

Language pair

sentences

es-ca

1 000

es-gl

1 000

es-pt

1 000

words
es: 22 583
ca: 22 451
es: 22 698
gl: 20 970
es: 23 561
pt: 22 941

Table 1: Number sentences number words language different corpora used
select frequency count threshold used evaluation. threshold finally used depends
translation task; see Table 5 page 627 know threshold used
translation task rules inferred parallel corpus 2.0 million words
language.

5.1 Evaluation
performance presented approach compared MT system
transfer rules used (word-for-word MT), MT system using hand-coded transfer rules,15 using state-of-the-art
SMT system trained using parallel corpora. latter used
free/open-source SMT toolkit Moses (Koehn et al., 2007) SRILM language modelling toolkit (Stolcke, 2002). training SMT system done follows:16 First,
translation model trained using 90% training corpus. Then, 5-gram
language model trained using SRILM toolkit whole training corpus. Finally, minimum error rate training algorithm (Och, 2003) used remaining 10%
training corpus adjust weight feature.17 features used
SMT system used Moses default: 5 phrase-table features (source-to-target
target-to-source phrase translation probabilities, source-to-target target-to-source
lexical weightings, phrase penalty), distance-based cost (total number word movements), sentence word count, TL model.
Translation performance evaluated using two different measures; one hand,
translation edit rate (TER; Snover et al., 2006), hand, bilingual
evaluation understudy (BLEU; Papineni et al., 2002); cases evaluation
corpora used confidence intervals measures reported
given (see below).
5.1.1 Confidence intervals
Confidence intervals MT quality measures calculated bootstrap resampling method described Koehn (2004). general, bootstrap resampling method
consists estimating precision sample statistics (in case, translation quality
measures) randomly resampling replacement (that is, allowing repetitions)
full set samples (Efron & Tibshirani, 1994); MT, sentences respective
15. corresponding Apertium language packages.
16. detailed training instructions visit http://www.statmt.org/wmt09/baseline.html.
17. minimum error rate training used BLEU evaluation measure.

620

fiInferring Shallow-Transfer MT Rules Small Parallel Corpora

Language pair

sentences

es-ca

2 400

es-gl

2 450

es-pt

2 000

words
es: 55 064
ca: 54 730
es: 55 826
gl: 51 603
es: 55 814
pt: 53 762

Table 2: Number sentences number words language different test corpora
used evaluation.

reference translations. method property assumptions made
underlying distribution variable, case, MT quality measure.
calculation confidence intervals consists following steps:
1. translation performance evaluated large number times, experiments
1 000 times, using randomly chosen sentences test corpus, counterpart sentences reference corpus;
2. calculated measures sorted ascending order;
3. top q% bottom q% elements removed list.
that, remaining values interval [a, b]. interval approximates
probability 1 2q/100 range values quality measure reported lies
test corpora number sentences equal used carry evaluation.
5.1.2 Evaluation corpora
Table 2 shows number sentences number SL TL words different
test corpora used evaluation inferred rules translation considered. test corpora come independent parallel corpora, different source,
relation used training. precisely, test corpora Spanish
Catalan SpanishGalician comes Revista Consumer Eroski (Alcazar, 2005),18
magazine addressed consumers published Spanish, Catalan, Galician Basque;
test corpora SpanishPortuguese comes shared evaluation task 2008
workshop SMT.19
5.2 Results
Figure 6 shows TER BLEU scores, together respective 95% confidence
intervals, achieved translation direction SpanishCatalan language pair
using training corpora different sizes. error rates reported are: (a) results
frequency count directly used select set ATs use rules generation,
(b) results achieved state-of-the-art SMT system trained corpora, (c)
18. http://revista.consumer.es
19. http://www.statmt.org/wmt08/wmt08-eval.tar.gz

621

fiSanchez-Martnez & Forcada

Catalan (SL)
... els gossos catalogats de perillosos han
de tenir una asseguranca ...
... es va descobrir en el cacauet ...
... va tenir un infart de miocardi ...
... els fonaments cientfics per considerar
funcionals diversos aliments son ...
... lenveja es manifesta ...
... cal preservar-lo de la llum ...

Spanish (TL)
... los perros catalogados de peligrosos
deben tener seguro ...
.. se descubrio en el cacahuete ...
... tuvo un infarto de miocardio ...
los fundamentos cientficos para considerar funcionales varios alimentos son ...
la envidia se manifiesta ...
... hay que preservarlos de la luz ...

Table 3: Translation examples Catalan-to-Spanish translation. translations reported
produced using automatically inferred rules; words boldface indicate changes
respect word-for-word translation; indicates word deleted respect word-forword translation.

results achieved using hand-coded transfer rules, (d) results wordfor-word translation (when structural transformations applied). results achieved
modified frequency count described section 4.2 used select set ATs
use reported since indistinguishable practice achieved
using directly frequency count; reason, considered rest
experiments. Notice cases, except SMT results, linguistic
data (morphological bilingual dictionaries) used. Catalan-to-Spanish
translations produced automatically inferred rules shown Table 3.
Results Figure 6 show that, expected, translation quality achieved
inferred transfer rules better word-for-word translation, even small
parallel corpus around 0.5 million words language used; note however,
case Spanish-to-Catalan translation confidence intervals overlap training
corpus 0.25, 0.5 1.0 million words, overlap smaller latter.
Results Figure 6 also show SMT system performs worse rules automatically inferred parallel corpus even worse word-for-word
translation. training corpora used large enough learn
wide-coverage bilingual lexicon and, consequently, words translate unknown SMT system. Remember approach learns transfer rules
parallel corpus, bilingual entries, bilingual dictionary used
hand-coded rules, automatically inferred rules word-for-word translation.
section 5.2.1 (page 626) discuss results achieved SMT system
bilingual dictionary corresponding Apertium package added SMT training
data.
Figure 7 shows, translation direction SpanishGalician language pair,
MT quality measures translation setups reported Spanish
Catalan Figure 6.
SpanishGalician language pair shows results agreement obtained
SpanishCatalan; however, improvement Galician-to-Spanish translation quality,
compared word-for-word translation, smaller. addition, improvement obtained
case SpanishCatalan increasing amount corpora used training greater
622

fiInferring Shallow-Transfer MT Rules Small Parallel Corpora

Spanish Catalan
28

72
70
BLEU (% words)

26
TER (% words)

74

AT-count
SMT
hand
w4w

24
22
20
18

68
66
64
62
60
AT-count
SMT
hand
w4w

58
16

56

14

54
0.25 0.5

1
1.5
Millions words

2

0.25 0.5

1
1.5
Millions words

2

Catalan Spanish
26

72
70
BLEU (% words)

24
TER (% words)

74

AT-count
SMT
hand
w4w

22
20
18

68
66
64
62
60
58

AT-count
SMT
hand
w4w

56

16

54
14

52
0.25 0.5

1
1.5
Millions words

2

0.25 0.5

1
1.5
Millions words

2

Figure 6: TER BLEU scores (vertical axis), respective 95% confidence intervals,
translation direction SpanishCatalan language pair using training corpora
different sizes (horizontal axis). AT-count refers result achieved count directly
used select set ATs use; SMT refers result achieved state-of-the-art SMT
system trained parallel corpora; hand refers results achieved hand-coded
transfer rules used; w4w (word word) refers result achieved transfer rules
used.

623

fiSanchez-Martnez & Forcada

Spanish Galician
26

74
72
BLEU (% words)

24
TER (% words)

76

AT-count
SMT
hand
w4w

22
20
18
16

70
68
66
64
62
60

AT-count
SMT
hand
w4w

58

14

56

12

54
0.25 0.5

1
1.5
Millions words

2

0.25 0.5

1
1.5
Millions words

2

Galician Spanish
24

76
74
BLEU (% words)

22
TER (% words)

78

AT-count
SMT
hand
w4w

20
18
16

72
70
68
66
64
62

AT-count
SMT
hand
w4w

60

14

58
12

56
0.25 0.5

1
1.5
Millions words

2

0.25 0.5

1
1.5
Millions words

2

Figure 7: TERs BLEU scores (vertical axis), respective 95% confidence interval,
translation direction SpanishGalician language pair using training corpora
different sizes (horizontal axis). measures reported correspond results achieved
using different MT setups (as described Figure 6).

624

fiInferring Shallow-Transfer MT Rules Small Parallel Corpora

Spanish Portuguese
70

66

26
BLEU (% words)

68
TER (% words)

28

AT-count
SMT
hand
w4w

64
62
60
58

24
22
20
18
AT-count
SMT
hand
w4w

56
16

54
52

14
0.25 0.5

1
1.5
Millions words

2

0.25 0.5

1
1.5
Millions words

2

Figure 8: TER BLEU scores (vertical axis), respective 95% confidence intervals,
Spanishto-Portuguese translation using training corpora different sizes (horizontal
axis). measures reported correspond results achieved using different MT setups
(see Figure 6).

SpanishGalician, shown slope curve.
significant frequent patterns learned training corpora selected
early. Note method unlikely perform worse word-for-word translation
(when rules used).
Concerning Spanish-to-Portuguese translation, Figure 8 shows TER BLEU
scores achieved different sizes training corpora used. Notice automatically
inferred rules perform better word-for-word translation, although confidence
intervals show large overlap. worth mentioning confidence intervals obtained
hand-coded transfer rules also overlap automatically inferred rules
word-for-word translation. rest experiments SMT system performs
worse training corpus large enough learn wide-coverage bilingual
lexicon.
difference results achieved using hand-coded transfer rules
using rules (word-for-word translation) small compared rest
translation tasks considered paper. Moreover, TER BLEU scores obtained
poor although Spanish Portuguese two related languages, and, therefore,
translating difficult task. Indeed, evaluation
hand-coded transfer rules performed using evaluation corpus reference
translation post-edited (corrected) version MT output produced
hand-coded rules shows TER 10%.
poor results obtained Spanish-to-Portuguese may explained fact
evaluation corpus, well training corpora used, may built
translating one language (say Spanish Portuguese) other, translating
625

fiSanchez-Martnez & Forcada

es-ca
ca-es
es-gl
gl-es
es-pt

AT-count
TER
OOV (%)
[15.8, 16.7]
4.3%
[15.0, 15.9]
4.9%
[14.7, 15.6]
9.3%
[13.9, 14.7]
10.2%
[54.2, 56.0]
3.8%

SMT+dictionary
TER
OOV (%)
[18.0, 19.0]
3.4%
[15.5, 16.4]
3.8%
[16.2, 17.1]
6.9%
[13.6, 14.4]
8.2%
[57.2, 59.0]
3.1%

SMT
TER
OOV (%)
[20.1, 21.2]
5.7%
[17.7, 18.7]
6.1%
[19.1, 20.0]
18.7%
[18.0, 18.8]
21.1%
[62.4, 64.1]
12.6%

Table 4: 95% confidence intervals TER ratio out-of-vocabulary (OOV) words
test corpus translated: rules automatically obtained parallel corpora (AT-count),
SMT system trained parallel corpora (SMT), SMT system trained
parallel corpora plus corresponding Apertium bilingual dictionary (SMT+dictionary).
data reported correspond case training corpus 2.0 million words
language.

third language (possibly English French).20 causes reference translation
different compared translations automatically performed, thus giving
high TERs. hand, may also cause alignments obtained
training corpora unreliable, shown percentage discarded bilingual phrase
pairs. percentage is, training corpora, around 54% Spanish-to-Portuguese
translation, 22% SpanishCatalan language pairs, around 20%
SpanishGalician language pair.
5.2.1 Adding bilingual dictionary SMT training data
aim testing whether difference translation performance
shallow-transfer rules SMT system due fact Apertium uses
wide-coverage, manually-built bilingual dictionary, added bilingual dictionary
corresponding Apertium package SMT training data (Tyers et al., 2009).21
worth noting adding bilingual dictionary training corpus
improve vocabulary coverage SMT systems inferred, also helps word
alignment process adding word-to-word alignment, gives additional advantage
SMT system respect systems; bilingual dictionary added
corpus used learn used automatic inference shallow-transfer rules.
Table 4 shows 95% confidence intervals TER ratio out-of-vocabulary
(OOV) words test corpus translated means Apertium shallowtransfer rules automatically obtained form parallel corpus 2.0 million words
language (AT-count); translated using SMT system trained
parallel corpus (SMT); and, translated SMT system trained par20. Remember training corpora contains European Union law evaluation corpus comes
European Parliament proceedings.
21. Apertium bilingual dictionaries contain lemma-based bilingual entries expanded
include possible inflected forms adding SMT training data. inflecting
lemma-based bilingual entries bilingual dictionary added SMT training data consists
(approximately) 1.8 million entries SpanishCatalan, 1.2 million entries SpanishGalician,
0.9 million entries SpanishPortuguese.

626

fiInferring Shallow-Transfer MT Rules Small Parallel Corpora

es-ca
ca-es
es-gl
gl-es
es-pt

freq.
count
8
14
13
6
25

number
rules
32 165
17 930
14 764
28 573
5 402

rules
used
8 133
6 785
3 777
4 898
2 636

% used
25.3%
37.8%
25.6%
17.1%
48.8%

% performing
word-for-word
2.77%
2.08%
1.16%
1.51%
1.18%

Table 5: translation task, following data shown: frequency count threshold
used, number rules generated, number (and percentage rules) used
translation corresponding evaluation corpus, percentage rule applications end
performing word-for-word translation. data reported correspond rules obtained
training corpora 2.0 million words language.

allel corpus containing original corpus 2.0 million words language plus
corresponding Apertium bilingual dictionary (SMT+dictionary).
results Table 4 show that, expected, SMT results improve
bilingual dictionary added training corpus; note however, results obtained
es-ca, ca-es, es-gl, es-pt still worse achieved automatically
inferred rules, although ca-es SMT+dictionary confidence interval shows large
overlap automatically inferred rules. translation task
SMT+dictionary system provides better results automatically inferred rules
gl-es task, although confidence interval overlaps automatically
inferred rules. cases ratio OOV words SMT+dictionary
automatically inferred rules words present bilingual dictionary
appear training corpus.
5.2.2 Analysis inferred rules
Table 5 shows, translation task, frequency count threshold used generation rules, number rules obtained number used
translation corresponding evaluation corpus; remember frequency count
threshold used translation task one minimizing TER translating
corpora described Table 1. data reported Table 5 correspond rules inferred
largest training corpora (2.0 million word language). Note number
inferred rules varies depending translation task; instance, number rules
es-ca around twice number rules ca-es, produce
minimum TER less rules happen needed case ca-es.
data Table 5 reveal, one hand, percentage rules finally used
translate corresponding evaluation corpus varies depending translation task,
and, hand, percentage rules end applying default
(which performs word-for-word translation, see section 4) depends translation
task, although always 3%.
Figure 9 shows Spanish-to-Catalan translation, top, number rules
obtained number rules used translation evaluation corpus,
627

fiSanchez-Martnez & Forcada

grouped rule length (number SL word classes); and, bottom, number
rule applications number rule applications end performing word-forword translation (apply default AT); generation rules frequency count
threshold 8 used. Notice rules unit length, i.e. rules process
single SL word class: needed bilingual dictionary leaves
translation decisions open, gender number words
masculine feminine, singular plural, TL. data figure
correspond rules inferred form largest training corpora used; case,
rest training corpora, similar behaviour obtained; happens
remaining translation tasks.
Figure 9 shows rules generated process SL patterns 3 4 word
classes; number rules processing 7 SL word classes low. Remember
extraction bilingual phrase pairs length restricted 7 SL words.
Finally, worth mentioning number inferred rules high compared
number hand-coded rules. Note, however, automatically inferred rules
specific lexicalized hand-coded ones. Hand-coded rules use macros complex
control flow statements allow treat phenomena rule.

6. Discussion
paper focused inference structural transfer rules used MT,
precisely inference shallow-transfer rules. describes extend
approach introduced SMT framework order use generate shallow-transfer
rules used RBMT. end, small amount linguistic information,
addition linguistic data used MT engine, used order learn
syntactic changes, also lexical changes apply translating SL texts TL.
linguistic information consists small set lexical categories involved lexical
changes (prepositions, pronouns, etc.) easily provided expert.
approach tested using data three existing language pairs
free/open-source shallow-transfer MT engine Apertium; precisely, presented approach tested translation directions SpanishCatalan Spanish
Galician languages pairs, Spanish-to-Portuguese translation. language
pair, training corpora different sizes used test importance
size training corpora available.
evaluation done, cases, using independent parallel corpora, coming
independent source, relation parallel corpora used training.
evaluation translation quality achieved automatically inferred rules
compared using hand-coded shallow-transfer rules, word-for-word
translation, using state-of-the-art SMT system trained parallel
corpora. cases automatically inferred rules perform better SMT system;
moreover, Apertium bilingual dictionary added SMT training data
one translation task performed slightly better automatically inferred rules. Notice
approach, unlike Caseli et al. (2006), aimed learning shallow-transfer
rules, bilingual entries, used bilingual dictionary provided
corresponding Apertium language-pair package.
628

fiInferring Shallow-Transfer MT Rules Small Parallel Corpora

Rules generated actually used test corpus
12000

rules generated
rules used

Number rules

10000

8000

6000

4000

2000

0
1

2
3
4
5
6
Rule length (number SL word classes)

7

Rule applications applications end word word test corpus
7000

rule applications
rules perform word-for-word

6000

Number rules

5000
4000
3000
2000
1000
0
1

2
3
4
5
6
Rule length (number SL word classes)

7

Figure 9: Spanish-to-Catalan translation, rules generated used translation
corresponding evaluation corpus (top), number rule applications number
applications end performing word-for-word translation (bottom). Reported data
grouped rule length (number SL word classes).

629

fiSanchez-Martnez & Forcada

evaluation inferred rules translation directions Spanish
Catalan SpanishGalician language pairs show improvement translation
quality compared word-for-word translation, even small parallel corpus
used. case Spanish-to-Portuguese translation, small improvement: confidence intervals show large overlap.
knowledge, first time approach extended use
RBMT; important property inferred rules edited human
experts improve them. means developers RBMT systems use
method obtain set initial transfer rules refined linguists;
proceeding way, human experts focus difficult issues writing
accurate transfer rules MT, required rules automatically obtained
parallel corpora. point view, great advantage corpusbased approaches MT, SMT, because, approach, automatically generated
rules coexist hand-coded ones.
respect parallel corpus used training, results achieved inferred
rules Spanish-to-Portuguese translation show procedure followed build
parallel corpus, is, way translation one language
one performed, deserves special attention. opinion, may concluded
parallel corpora built translating third language may
appropriate task inferring rules used RBMT, especially languages
involved closely related third language not.
must mentioned software implementing method described paper
released free/open-source software GNU GPL license22 freely
downloaded http://apertium.sf.net, package name apertium-transfer-tools.
public availability source code ensures reproducibility experiments
conducted allows researchers improve approach discussed here, saving
implement algorithms again. addition, method
implemented way integrates Apertium free/open-source
MT platform (see appendix A); benefits, one hand, research uses
Apertium research platform, hand, people developing new language
pairs Apertium.
plan improve generated rules using linguistic criteria extraction
bilingual phrase pairs generalized ATs. Note experiments
reported paper bilingual phrase pairs extracted training corpus without
worrying whether well-formed syntactic constituents not. also plan study
use lexicalized categories flexible way. would interest
context-dependent lexicalized categories, is, categories lexicalized
contexts, others; would improve generalization performed
extended ATs reduce number inferred rules.
Another improvement plan achieve extension present approach
rules translation less-related language pairs inferred. Recently,
transfer Apertium extended translate divergent languages
splitting structural transference phase 3 stages: first one detects word patterns
22. http://www.gnu.org/licenses/gpl-2.0.html

630

fiInferring Shallow-Transfer MT Rules Small Parallel Corpora

lexical
transfer
l
postmorph.
part-of-speech
morph.
struct.
SL
TL






generator
analyzer
tagger
generator
transfer
text
text
Figure 10: Main modules free/open-source shallow-transfer MT engine Apertium used
experiments (see appendix A).

called chunks; second one operates sequences chunks; finally, third one makes
finishing operations within chunks detected first stage. approach
could extended detecting chunks training parallel corpus using linguistic criteria
mentioned previous paragraph, using Marker Hypothesis (Green, 1979),
done Gough Way (2004), extracting ATs based chunk classes instead
word classes, done now. case, would worth testing method
present translation less-related languages using longer ATs larger
training corpora.

Acknowledgments
Work funded Spanish Ministry Education Science European Social Fund research grant BES-2004-4711, Spanish Ministry Industry,
Tourism Commerce projects TIC2003-08681-C02-01, FIT340101-2004-3
FIT-350401-2006-5, Spanish Ministry Education Science project
TIN2006-15071-C03-01. authors thank anonymous referees suggesting significant improvements paper Francis Tyers proof-reading it.

Appendix A. Apertium Machine Translation Platform
appendix briefly describes free/open-source shallow-transfer MT engine Apertium23 (Armentano-Oller et al., 2006) used experiments. Apertium follows
shallow-transfer approach shown Figure 10:
morphological analyzer tokenizes text surface forms delivers,
surface form, one lexical forms consisting lemma, lexical category
morphological inflection information.
part-of-speech tagger (categorial disambiguator) chooses, using first-order
hidden Markov model (Cutting et al., 1992; Baum & Petrie, 1966), one lexical
forms corresponding ambiguous surface form.
lexical transfer module reads SL lexical form delivers corresponding TL lexical form looking bilingual dictionary.
23. MT engine, documentation, linguistic data different language pairs downloaded
http://apertium.sf.net.

631

fiSanchez-Martnez & Forcada

structural transfer module (parallel lexical transfer) uses finite-state
chunker detect patterns, articlenounadjective, lexical forms
need processed word reorderings, agreement, etc., performs
operations. module applies structural transfer rules automatically
inferred parallel corpora using method paper.
morphological generator delivers TL surface form TL lexical form,
suitably inflecting it.
post-generator performs orthographic operations contractions (e.g.
Spanish de+el del ) apostrophations (e.g. Catalan el+institut linstitut).
Apertium MT engine completely independent linguistic data used
translate given language pair. Linguistic data coded using XML-based formats,24
allows easy data transformation maintenance.

References
Alcazar, A. (2005). Towards linguistically searchable text. Proceedings BIDE (BilbaoDeusto) Summer School Linguistics 2005, Bilbao. Universidad de Deusto.
Armentano-Oller, C., Carrasco, R. C., CorbA-Bellot, A. M., Forcada, M. L., Ginest-Rosell,
M., Ortiz-Rojas, S., Perez-Ortiz, J. A., Ramrez-Sanchez, G., Sanchez-Martnez, F., &
Scalco, M. A. (2006). Open-source Portuguese-Spanish machine translation. Computational Processing Portuguese Language, Proceedings 7th International
Workshop Computational Processing Written Spoken Portuguese, PROPOR
2006, Vol. 3960 Lecture Notes Computer Science, pp. 5059. Springer-Verlag.
Arnold, D. (2003). translation difficult computers. Computers Translation:
translators guide. Benjamins Translation Library.
Baum, L. E., & Petrie, T. (1966). Statistical inference probabilistic functions finite
state Markov chains. Annals Mathematical Statistics, 37 (6), 15541563.
Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., & Mercer, R. L. (1993). mathematics
statistical machine translation: Parameter estimation. Computational Linguistics,
19 (2), 263311.
Brown, R. D. (1999). Adding linguistic knowledge lexical example-based translation
system. Proceedings Eighth International Conference Theoretical
Methodological Issues Machine Translation (TMI-99), pp. 2232.
Carl, M., & Way, A. (Eds.). (2003). Recent Advances Example-Based Machine Translation, Vol. 21. Springer.
Caseli, H. M., Nunes, M. G. V., & Forcada, M. L. (2005). LIHLA: lexical aligner based
language-independent heuristics. Anais V Encontro Nacional de InteligAa ncia
Artificial (ENIA 2005), pp. 641650.
24. XML formats (http://www.w3.org/XML/) type linguistic data defined
conveniently-designed XML document-type definitions (DTDs) may found inside Apertium
package.

632

fiInferring Shallow-Transfer MT Rules Small Parallel Corpora

Caseli, H. M., Nunes, M. G. V., & Forcada, M. L. (2006). Automatic induction bilingual resources aligned parallel corpora: application shallow-transfer machine
translation. Machine Translation, 20 (4), 227245. Published 2008.
Cicekli, I., & Guvenir, H. A. (2001). Learning translation templates bilingual translation examples. Applied Intelligence, 15 (1), 5776.
Cutting, D., Kupiec, J., Pedersen, J., & Sibun, P. (1992). practical part-of-speech tagger. Proceedings Third Conference Applied Natural Language Processing.
Association Computational Linguistics, pp. 133140.
Efron, B., & Tibshirani, R. J. (1994). introduction Bootstrap. CRC Press.
Gough, N., & Way, A. (2004). Robust large-scale EBMT marker-based segmentation.
Proceedings 10th International Conference Theoretical Methodological
Issues Machine Translation, pp. 95104, Baltimore, MD.
Green, T. (1979). necessity syntax markers. Two experiments artificial languages. Journal Verbal Learning Behavior, 18, 481496.
Hutchins, W. J., & Somers, H. L. (1992). Introduction Machine Translation. Academic
Press.
Kaji, H., Kida, Y., & Morimoto, Y. (1992). Learning translation templates bilingual
text. Proceedings 14th Conference Computational Linguistics, pp. 672
678. Association Computational Linguistics.
Knight, K. (1999). statistical machine translation tutorial workbook. 35 pages. (http:
//www.isi.edu/natural-language/mt/wkbk.rtf).
Koehn, P. (2004). Statistical significance tests machine translation evaluation. Proceedings Conference Empirical Methods Natural Language Processing, pp.
388395.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B.,
Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., & Herbst, E.
(2007). Moses: Open source toolkit statistical machine translation. Proceedings Annual Meeting Association Computational Linguistics (ACL),
demonstration session.
Lavie, A., Probst, K., Peterson, E., Vogel, S., Levin, L., Font-Llitjos, A., & Carbonell,
J. (2004). trainable transfer-based machine translation approach languages
limited resources. Proceedings Workshop European Association
Machine Translation (EAMT-2004).
Liu, Y., & Zong, C. (2004). technical analysis translation templates. Proceedings
IEEE International Conference Systems, Man & Cybernetics (SMC), pp.
47994803. IEEE.
Menezes, A., & Richardson, S. D. (2001). best-first alignment algorithm automatic
extraction transfer mappings bilingual corpora. Proceedings ACL
Workshop data-driven machine translation, pp. 3946.
633

fiSanchez-Martnez & Forcada

Mikheev, A. (1996). Unsupervised learning word-category guessing rules. Proceedings
Thirty-Fourth Annual Meeting Association Computational Linguistics,
pp. 327333.
Nagao, M. (1984). framework mechanical translation English Japanese
analogy principle. Elithorn, A., & Banerji, R. (Eds.), Artifical Human
Intelligence, pp. 173180. North-Holland.
Och, F. J. (1999). efficient method determining bilingual word classes. EACL99:
Ninth Conference European Chapter Association Computational Lingustics, pp. 7176.
Och, F. J. (2002). Statistical machine translation: single-word models alignment
templates. Ph.D. thesis, RWTH Aachen University. (http://www-i6.informatik.
rwth-aachen.de/publications/download/520/Och--2002.pdf).
Och, F. J. (2003). Minimum error rate training statistical machine translation.
41st Annual Meeting Association Computational Linguistics, pp. 160167,
Sapporo, Japan.
Och, F. J. (2005). Statistical machine translation: Foundations recent advances. Tutorial
MT Summit X. (http://www.mt-archive.info/MTS-2005-Och.pdf).
Och, F. J., & Ney, H. (2002). Discriminative training maximum entropy models
statistical machine translation. Proceedings 40th Annual Meeting
Association Computational Lingustics (ACL), pp. 295302.
Och, F. J., & Ney, H. (2003). systematic comparison various statistical alignment
models. Computational Linguistics, 29 (1), 1951.
Och, F. J., & Ney, H. (2004). alignment template approach statistical machine
translation. Computational Linguistics, 30 (4), 417449.
Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002). BLEU: method automatic evaluation machine translation. Proceeding 40th Annual meeting
Association Computational Linguistics, pp. 311318.
Probst, K., Levin, L., Peterson, E., Lavie, A., & Carbonell, J. (2002). MT minority
languages using elicitation-based learning syntactic transfer rules. Machine Translation, 17 (4), 245270.
Sanchez-Martnez, F., & Forcada, M. L. (2007). Automatic induction shallow-transfer
rules open-source machine translation. Proceedings 11th Conference
Theoretical Methodological Issues Machine Translation (TMI 2007), pp. 181
190.
Sanchez-Martnez, F., & Ney, H. (2006). Using alignment templates infer shallow-transfer
machine translation rules. Lecture Notes Computer Science 4139, Proceedings
FinTAL, 5th International Conference Natural Language Processing, pp. 756767.
Sanchez-Martnez, F., Perez-Ortiz, J. A., & Forcada, M. L. (2008). Using target-language
information train part-of-speech taggers machine translation. Machine Translation, 22 (1-2), 2966.
634

fiInferring Shallow-Transfer MT Rules Small Parallel Corpora

Simoes, A., & Almeida, J. (2003). NATools - statistical word aligner workbench. Procesamiento del Lenguaje Natural, 31, 217224.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L., & Makhoul, J. (2006). study translation edit rate targeted human annotation. Proceedings 7th Conference
Association Machine Translation Americas, Visions Future
Machine Translation, pp. 223231, Cambridge, MA, USA.
Steinberger, R., Pouliquen, B., Widiger, A., Ignat, C., Erjavec, T., Tufis, D., & Varga, D.
(2006). JRC-Acquis: multilingual aligned parallel corpus 20+ languages.
Proceedings 5th International Conference Language Resources Evaluation (LREC).
Stolcke, A. (2002). SRILM extensible language modeling toolkit. Proceedings
International Conference Spoken Language Processing, pp. 901904, Denver, CO.
Tyers, F. M., Dugast, L., & Park, J. (2009). Rule-based augmentation training data
BretonFrench statistical machine translation. Proceedings 13th Annual
Conference European Associtation Machine Translation, Barcelona, Spain.
press.
Vogel, S., Ney, H., & Tillmann, C. (1996). HMM-based word alignment statistical translation. COLING 96: 16th International Conference Computational Linguistics, pp. 836841.
Zens, R., Och, F. J., & Ney, H. (2002). Phrase-based statistical machine translation. KI
2002: Advances Artificial Intelligence: Proceedings 25th Annual German Conference
AI, Vol. 2479 Lecture Notes Computer Science, pp. 1832. Springer-Verlag.

635

fiJournal Artificial Intelligence Research 34 (2009) 757821

Submitted 11/08; published 04/09

Conservative Inference Rule
Uncertain Reasoning Incompleteness
Marco Zaffalon

zaffalon@idsia.ch

Galleria 2
IDSIA
CH-6928 Manno (Lugano), Switzerland

Enrique Miranda

mirandaenrique@uniovi.es

Department Statistics Operations Research
University Oviedo
C-Calvo Sotelo, s/n
33007 Oviedo, Spain

Abstract
paper formulate problem inference incomplete information
general terms. includes modelling process responsible incompleteness,
call incompleteness process. allow process behaviour partly
unknown. use Walleys theory coherent lower previsions, generalisation
Bayesian theory imprecision, derive rule update beliefs incompleteness
logically follows assumptions, call conservative inference rule.
rule remarkable properties: abstract rule update beliefs
applied situation domain; gives us opportunity neither optimistic
pessimistic incompleteness process, necessary condition
draw reliable strong enough conclusions; coherent rule, sense
cannot lead inconsistencies. give examples show new rule applied
expert systems, parametric statistical inference, pattern classification,
discuss generally view incompleteness processes defended well
consequences.

1. Introduction
consider general inference problem: want draw conclusions Z
observation facts . Z variables related, sense
observing value may change beliefs value z target
variable Z assumes Z.1
Although apparently simple, setting already captures main features
many important problems, making inference expert systems, learning values
statistical parameters data, learning data classify new objects
one set preestablished categories (i.e., so-called pattern classification),
others.
1. Throughout paper, shall maintain convention using capital letters variables, corresponding calligraphic letters spaces possibilities, lower-case letters elements
spaces.
c
2009
AI Access Foundation. rights reserved.

fiZaffalon & Miranda

(V )isit Asia

Smo(K)ing

+

?

Tu(B)erculosis

Lung cance(R)



Bronc(H)itis

+

B (O)r R

+

s=

Abnorma(L) X-rays

Dyspne(A)

Figure 1: Bayesian network called Asia. letters parentheses denote variables
corresponding nodes. variable, say X, binary states x0
yes x00 no.

Let us make concrete help graphical language Bayesian
networks (Pearl, 1988):2 consider well-known Asia net displayed Figure 1,
intended model artificial medical problem. nodes Bayesian network
variables arcs model probabilistic dependencies them; node holds
probability distribution node given joint state parent nodes.
probabilities making distributions also called parameters network.
Now, assume network (both graph parameters) provided
domain expert, field expert systems. Say network
used diagnosing lung cancer; case R target node others used
predict value R. Therefore case Z corresponds R vector
(V, K, B, H, O, L, A). another situation, may want infer parameters
data. example, denote chance tuberculosis conditional
recent visit Asia, say sample joint values B V
wish infer value . problem so-called parametric inference,
Z corresponds sample D. Finally, say goal use Asia
net learn data diagnose lung cancer, i.e., predict state R
next patient see, characterise vector (V, K, B, H, O, L, A). case
2. results present paper restricted case Bayesian networks, since
applied Bayesian networks, often use convey intuition easily.

758

fiConservative Inference Rule

need collect, data set D, values variables network
past patients. data set exploited infer parameters Asia net,
used classification predicting value R case expert systems.
Therefore, focus pattern classification, Z corresponds so-called class
variable, namely R, tuple (D, V, K, B, H, O, L, A).
common feature previous examples observing useful
inferring Z, indeed reason introduced model.
subtle point observing important realise: often,
observation fact coincide fact itself. example, consider
Asia network, focusing problem parametric inference: case, may well
happen make mistakes collecting values B V sample; might
instance mix yes values values. useful regard situation
related two levels information: latent level actual sample records
right values variables, manifest level observed sample, related
to, necessarily coincide actual sample, going used
inference. exploit paradigm based latent manifest level
generally powerful conceptual tool. instance, use Asia network
expert system, may case certain patient characterised vector
values (v 0 , k 00 , b00 , h00 , o0 , l0 , a0 ), possible access values variables H
A. Therefore observation characteristics patient (?, ?, ?, h00 , ?, ?, a0 ),
denote symbol missing value question mark. Again, think
former vector latent latter manifest. generally speaking, idea
underlying present discussion devices use observe , whatever
are, may let us see exactly is.
order account problem, explicitly model observation
new variable W , taking values finite set W possible observations. call W
observation . previous terminology, W manifest variable latent
one. words, regard W output process observing facts,
called observational process paper (other authors call measurement
process). think many different types observational processes. paper
restrict attention special case observational processes called incompleteness
processes (IPs). IPs processes lead set-valued observations coarsening facts.
special case IPs missingness processes, i.e., turn facts
entire possibility space.
example, process prevented variables observed
expert system case coarsening process: turned fact (v 0 , k 00 , b00 , h00 , o0 , l0 , a0 )
observation (?, ?, ?, h00 , ?, ?, a0 ), regarded set 25 complete
vectors obtained replacing question marks values unobserved variables
possible ways. hand, take fact consideration
single variable, say V , process makes V observed missingness
process writing question mark equivalent writing entire possibility space
variable.3
3. IPs makes sense introduce additional variable W even incomplete
observations possibility space ; case Asia net, happens

759

fiZaffalon & Miranda

Missing coarsened data indeed commonplace expert systems,
evidence inference task based usually incomplete. arise
frequently also many fields; data mining, mention one, missing data
pervasive problem applications well important theoretical area research.
words, challenges posed IPs widespread. also means problem
incomplete information appears fundamental component general task
uncertain reasoning; conceptually deep leads complicated problems practice.
Moreover, powerful theoretical tools general task uncertain
reasoning, Bayes rule generalisations, tools uncertain
reasoning incompleteness general case. Currently, popular approach
based assumption IP produces incompleteness coarsening facts
random, means non-selectively. assumption models non-selectiveness
called coarsening random (or CAR, see Gill et al., 1997), missing random (or
MAR, see Little & Rubin, 1987) special case missingness processes (see also Jaeger,
2008). CAR implies incompleteness non-informative ignored; thus
creates formal basis applying methods developed complete information
incomplete case. example, assume vector (?, ?, ?, h00 , ?, ?, a0 )
created CAR IP, allowed infer value R sole basis subvector (h00 , a0 ); precisely, aim computing posterior probability R = r0 ,
CAR allows us write P (R = r0 |W = (?, ?, ?, h00 , ?, ?, a0 )) = P (R = r0 |H = h00 , = a0 ).
case general.
fact, incompleteness may well informative. example, Asia network,
may case information whether person Asia
provided frequency two groups. example incompleteness
generated within communication protocol, giving information key
part communication. somewhat selective way reporting information, albeit
frequent, compatible CAR assumption. pointed long ago
Shafer (1985), also outlined implications well complications
uncertain reasoning: among these, fact modelling IPs difficult task.
recently, used argue frequent use CAR (Grunwald &
Halpern, 2003); large agreement scientific community CAR
strong, hence inappropriate many situations (see Manski, 2003).
De Cooman Zaffalon (2004) tried remedy approach IPs alternative
CAR based coherent lower previsions (Walley, 1991), i.e., closed convex sets
probabilities also called credal sets Levi (1980). led rule updating
beliefs incomplete information expert systems called conservative updating rule
(CUR).
regard CAR optimistic approach incomplete information, CUR
regarded pessimistic: assume nearly anything
IP, practice leads inference based working set facts
consistent (i.e., completions of) incomplete information hand.
previous example wish compute posterior probability R = r0 ,
CUR leads us consider 25 completions vector (?, ?, ?, h00 , ?, ?, a0 ), i.e.,
symbol ? possible value variable; observations contain question marks must
necessarily possibility space W .

760

fiConservative Inference Rule

(v, k, b, h00 , o, l, a0 ), v V, k K, b B, O, l L, compute
P (r0 |v, k, b, h00 , o, l, a0 ). posterior inference summarised lower
upper probabilities P (R = r0 |W = (?, ?, ?, h00 , ?, ?, a0 )) := minv,k,b,o,l P (r0 |v, k, b, h00 , o, l, a0 )
P (R = r0 |W = (?, ?, ?, h00 , ?, ?, a0 )) := maxv,k,b,o,l P (r0 |v, k, b, h00 , o, l, a0 ). words,
inference imprecise, i.e., width interval determined lower upper probabilities need zero, logical consequence ignorance CUR
assumes IP. interesting side remark, CUR flexible CAR dealing
incomplete data: eventually lead conclusions depend
variable. W variable certain point derivation cancels out.
CUR drawbacks. forces us always assume ignorance IP, even
know process CAR, instance. could case previous
example might know variables B, O, L subject CAR IP;
using CUR obliged ignore information take completions,
above; would lead inference much weak. Furthermore, CUR
developed expert systems only. cannot applied parametric inference
parameter space infinite, limitation assumptions leading
CUR may sometimes prevent applied generally.
paper attempt get best CAR CUR. assume IP
actually made two parts, one acts CAR process another unknown
us. use theory coherent lower previsions derive corresponding rule,
call conservative inference rule (CIR).
CIR following properties:
Much like traditional, CAR-based, updating, unlike CUR, rule
abstract task updating beliefs basis observations. such,
applied every situation. CIR, different applications follow simply giving
different meanings facts , observations W , quantity Z
interested.
CIR allows variables involved analysis, except W , take values
infinite spaces (CUR allows finite spaces). allows us easily focus
statistical problems goal inference often value continuous
parameter, problems also use auxiliary continuous variables
later marginalised build model.
deal incomplete observations, missing ones CUR.
Finally, importantly, CIR shown lead self-consistent (or coherent) inference
strong sense Walley (1991, Section 7.1.4(b)).
CIR leads treat information made incomplete CAR IP similarly traditional updating, subject unknown one similarly CUR. example, consider vector (?, ?, ?, h00 , ?, ?, a0 ) assuming variables B, O, L subject
CAR IP V, K IP whose behaviour unknown us. CIR leads following
posterior lower upper probabilities R = r0 : P (R = r0 |W = (?, ?, ?, h00 , ?, ?, a0 )) :=
minv,k P (r0 |v, k, h00 , a0 ) P (R = r0 |W = (?, ?, ?, h00 , ?, ?, a0 )) := maxv,k P (r0 |v, k, h00 , a0 ).
If, addition, know two completions V, K make sense, say (v 0 , k 00 )
761

fiZaffalon & Miranda

(v 00 , k 0 ) (this means unknown IP coarsening rather missingness process),
ability CIR deal coarsening processes allows take advantage
information, leading informative lower upper probabilities obtained optimising two completions. Furthermore, CIR leads inference based,
predecessors, facts, W variable.
organisation material paper follows attept make paper accessible also readers prefer go formal proofs behind CIR.
particular, paper logically divided two parts. first part, including
Section 5, intended describe obtained CIR scope application. Therefore part briefly gives introductory material needed define
CIR, notions coherent lower previsions Section 2 incompleteness
processes Section 3. definition CIR given Section 4. discuss significance compare CUR Section 4.1. show CIR applied
probabilistic expert systems (Section 5.1), parametric inference (Section 5.2), pattern
classification (Section 5.3), time giving also example. first part also discuss number properties consequences wider view IPs defending
(e.g., Sections 5.2.25.2.3). (Section 5.3.2) particularly important
data mining; depends fact usually possible learn
IP data, yet inferences may critically depend it.
second part paper technical works foundations CIR.
aim, first give advanced notions coherent lower previsions Section 6,
updating, independence, coherence sense Walley (1991).
state number probabilistic assumptions Section 7.1, including CAR, discuss
Section 7.2 (CAR also discussed Appendix A) derive CIR Section 7.3.
parts even technical, needed show applying CIR
cannot lead inconsistencies, relegated Appendix B.

2. Coherent Lower Previsions
give short introduction concepts results behavioural theory
imprecise probabilities shall need introduce conservative inference rule.
refer reader work Walley (1991) in-depth study coherent lower
previsions, paper Miranda (2008) survey theory.
Consider possibility space . may represent instance set possible outcomes
experiment. theory coherent lower previsions, beliefs
likelihood outcomes represented means lower previsions gambles :
Definition 1 Given possibility space , gamble bounded real-valued function
. set gambles denoted L(). lower prevision P real functional
defined set gambles K L().
gamble f represents random reward, depends priori unknown value
. lower prevision P set gambles K represents subjects supremum acceptable
buying prices gambles, sense > 0 f K, subject
disposed accept uncertain reward f P (f ) + , P (f ) also seen
762

fiConservative Inference Rule

constant gambles identically equal real values P (f ) , respectively.4
Intuitively, lower previsions represent lower expectations gambles: subject
disposed buy gamble anything smaller expected reward; however, lack
knowledge probability different rewards may enable give
lower bound expectation, accepting buying price smaller bound.
bound lower prevision gamble.
shall use IA denote special type gamble: indicator function set A,
i.e., function whose value 1 elements 0 elsewhere. shall sometimes
use notation P (A) lower prevision P (IA ) confusion possible.
Consider variables X1 , . . . , Xn , taking values sets X1 , . . . , Xn , respectively.
subset J {1, . . . , n} shall denote XJ (new) variable
XJ := (Xj )jJ ,
takes values product space
XJ := jJ Xj .
shall also use notation X n X{1,...,n} . identify possibility space
X n.
Definition 2 Let J subset {1, . . . , n}, let J : X n XJ so-called
projection operator, i.e., operator drops elements vector X n
correspond indexes J. gamble f X n called XJ -measurable
x, X n , J (x) = J (y) implies f (x) = f (y). shall denote KJ set
XJ -measurable gambles.
notion means value f takes depends components x X n
belong set J.5
one-to-one correspondence XJ -measurable gambles X n
gambles XJ : given XJ -measurable gamble f X n , define f 0 XJ
f 0 (x) := f (x0 ), x0 element J1 (x); conversely, given gamble g XJ ,
gamble g 0 X n given g 0 (x) := g(J (x)) XJ -measurable.
Let subset {1, . . . , n}, let P (XO ) lower prevision set KO
XO -measurable gambles. say P coherent following three
conditions hold f, g KO , > 0:
(C1) P (f ) inf f .
(C2) P (f ) = P (f ).
4. say gamble f P (f ) + desirable subject, f P (f ) almostdesirable. follows interpretation set desirable (resp., almost-desirable) gambles
closed addition multiplication non-negative reals, gamble f
dominates desirable gamble g also desirable (resp., almost-desirable).
5. notion related common notion measurability implies gamble f : X n R
indeed measurable mapping consider -field {J1 (A) : XJ } initial space
-field final space.

763

fiZaffalon & Miranda

(C3) P (f + g) P (f ) + P (g).
Coherence means subject cannot raise lower prevision P (f ) gamble
considering acceptable buying transactions implied gambles
domain.
Remark 1 (Coherent upper previsions) Although paper shall work
coherent lower previsions, supremum acceptable buying prices, shall also use
times so-called coherent upper previsions. upper prevision gamble f , P (f ),
represents infimum acceptable selling price f subject, sense
> 0 transaction P (f ) f + desirable him.
Taking account interpretation, follows upper lower previsions
must conjugate functions, sense P (f ) = P (f ) gambles f .
shall work almost exclusively lower previsions, use upper previsions
refer conjugate functions helps simplify notation. Finally,
say upper prevision coherent conjugate lower prevision.
important point introduce particular case coherent lower previsions
special interest us: linear previsions.
Definition 3 lower prevision P (XO ) set KO linear coherent
P (f + g) = P (f ) + P (g) f, g KO .
Linear previsions correspond case subjects supremum acceptable buying
price (lower prevision) coincides infimum acceptable selling price (or upper prevision) every gamble domain. coherent lower prevision P (XO ) linear,
denote P (XO ). linear prevision corresponds expectation operator (with
respect Dunford integral, see book Bhaskara Rao & Bhaskara Rao, 1983)
respect finitely additive probability.
One interesting feature linear previsions allows us easily characterise coherence.
Definition 4 P (XO ) said dominate P (XO ) P (f ) P (f ) every XO -measurable
gamble f .
lower prevision P (XO ) coherent lower envelope closed6
convex7 set dominating linear previsions, denote M(P (XO )). follows
also P (XO ) lower envelope set extreme points M(P (XO )). denote
set extreme points M(P (XO )) ext(M(P (XO )).
Example 1 Assume subject information outcome variables
XO belongs finite subset XO , nothing more. model
beliefs so-called vacuous lower prevision P (XO ) given P (f ) := minA f ()
every f KO . set M(P (XO )) dominating linear previsions corresponds
6. weak* topology, smallest topology evaluation functionals given
f (P ) := P (f ), f L(), continuous.
7. is, linear previsions P1 , P2 set (0, 1), linear prevision P1 + (1 )P2
also belongs set.

764

fiConservative Inference Rule

finitely additive probabilities P (XO ) satisfying constraint P (A) = 1. Among these,
extreme points degenerate probability measures respect A,
follows linear prevision M(P (XO )) convex combination these.
Consider two disjoint subsets O, {1, . . . , n}, 6= . P (XO |XI ) represents
subjects behavioural dispositions gambles depend outcome
variables {Xk , k O}, coming know outcome variables {Xk , k I}.
such, defined set gambles depend values variables
only,8 i.e., set KOI XOI -measurable gambles X n . Given gamble f
x XI , P (f |XI = x) represents subjects supremum acceptable buying price
gamble f , came know variable XI took value x (and nothing else).
thus consider gamble P (f |XI ) XI , x XI takes value P (f |XI = x).
Definition 5 functional P (|XI ) maps gamble f domain KOI
gamble P (f |XI ) called conditional lower prevision.
definition well posed sets {I1 (x) : x XI } form partition X n .
possible confusion variables involved lower prevision, shall
use notation P (f |x) P (f |XI = x). particular, P (y|x) mean P (Iy |x)
pairs values x, y.
Walleys theory conditional lower prevision P (XO |XI ) defined KOI required
self-consistent, separately coherent. Separate coherence means one hand
subject knows variable XI taken value x, cannot raise (conditional)
lower prevision P (f |x) gamble considering acceptable buying transactions
implied gambles domain, hand bet
odds event XI = x observed it.
case, domain linear set gambles, definition following:
Definition 6 conditional lower prevision P (XO |XI ) separately coherent
x XI , f, g KOI , > 0:
(SC1) P (f |x) inf 1 (x) f ().


(SC2) P (f |x) = P (f |x).
(SC3) P (f + g|x) P (f |x) + P (g|x).
Using conditions, see clearly separately coherent conditional lower
prevision also regarded lower bound conditional expectation. also follows
= , separate coherence coincides notion coherence introduced above.
Given separately coherent conditional lower prevision P (XO |XI ) domain KOI ,
see P (XO |x) defined set XO -measurable gambles, f
KOI , P (f |x) = P (g|x), g XO -measurable gamble given g(w) = f (I c (w), x)
w X n .
8. refer work Miranda De Cooman (2005) Walley (1991) general definitions
following notions section terms partitions, domains necessarily
(these) linear sets gambles.

765

fiZaffalon & Miranda

unconditional case, also consider conditional upper previsions,
represent subjects infimum acceptable selling prices gambles f KOI ,
coming know value variables XI (and nothing else). P (f |x) =
P (f |x) gambles f . Similarly, conditional lower prevision P (XO |XI )
set KOI linear separately coherent P (f + g|x) = P (f |x) +
P (g|x) x XI f, g KOI . Conditional linear previsions correspond
case subjects supremum acceptable buying price (lower prevision) coincides
infimum acceptable selling price (or upper prevision) every gamble domain.
separately coherent conditional lower prevision P (XO |XI ) linear, denote
P (XO |XI ).
conditional lower prevision P (XO |XI ) dominates P (XO |XI ) P (f |x) P (f |x)
every XOI -measurable gamble f every x XI . conditional lower prevision
P (XO |XI ) separately coherent lower envelope closed
convex set dominating conditional linear previsions, denote M(P (XO |XI )).
bit abuse notation, since actually every x XI set M(P (XO |x))
set linear previsions. follows also P (XO |XI ) lower envelope set
extreme points M(P (XO |XI )), say P (XO |XI ) extreme point
M(P (XO |XI )) every x XI , P (XO |x) extreme point closed convex set
M(P (XO |x)). denote set extreme points M(P (XO |x)) ext(M(P (XO |x))).
Example 2 Consider following experiment: subject throws coin; lands heads,
selects ball urn red white balls unknown composition; lands
tails, selects ball urn red blue balls also unknown composition. Let
X1 result first experiment, values {heads, tails}, X2 color
ball drawn second experiment, values {red, white, blue}.
may model using conditional prevision P (X2 |X1 ) P (X2 |X1 = heads)
vacuous {red, white}, P (X2 |X1 = tails) vacuous {red, blue}. extreme
points M(P (X2 |X1 )) conditional previsions P (X2 |X1 ), P (X2 |heads)
degenerate either red white, P (X2 |tails) degenerate either red blue.

3. Basic Setting
section introduce basic assumptions spaces considered model
incompleteness process.
3.1 Domain
mentioned Introduction, paper consider problem drawing
conclusions value target variable Z takes Z information
value another variable takes set Y. paper shall assume
set finite, set Z possible values target variable Z infinite.
3.2 Incompleteness Process
uncommon devices use get information , whatever
are, may let us see exactly is. this, explicitly model observation
766

fiConservative Inference Rule

new variable W , taking values finite set W possible observations. call
W observation . W represents outcome observational process.
paper focus observational processes called incompleteness processes,
turn fact entire possibility space, also nonempty subsets
possibility space.
Remark 2 (Concerning W variable) remark necessary introduce
variable W even quite common habit applications deal
variables Z . possibility drop W depends assumptions done.
instance, assume CAR/MAR W variable certain point derivation
cancels out, leading formulae involve Z . one confuse
operational procedures theoretical representation needed derivation
rule. theoretical derivation take account latent level information,
represented , need coincide manifest level, namely W , hence
need model process leads W . also necessary
assumptions made paper lead eventually
drop W . Moreover, distinction latent manifest level new
proposal all: contrary, present, somewhat implicitly, even original works
MAR (Little & Rubin, 1987) fully explicitly recent works (Grunwald &
Halpern, 2003).
focus representing IP. start characterising IP so-called
multi-valued map (this idea modelling IP multi-valued map goes back
essentially Strassen, 1964). connects facts observations: Y,
gives us set (y) W observations IP may turn fact y. require
set nonempty:
(y) 6= .
(IP1)
Take example Asia network. fact consideration instance :=
(v 0 , k 00 , b00 , h00 , o0 , l0 , a0 ) vector (V, K, B, H, O, L, A), taken equal , (y)
represents set incomplete instances may generated IP starting
y. instance, IP may (y) equal set 27 incomplete
instances obtained (v 0 , k 00 , b00 , h00 , o0 , l0 , a0 ) giving possibility replace values
vector question marks possible ways.
makes possible associate, observation w W, set facts may
originate it: i.e.,
{w} := {y : w (y)}.
Asia network, observation might w = (?, ?, ?, h00 , ?, ?, a0 ). (y) defined
above, {w} set 25 completions (?, ?, ?, h00 , ?, ?, a0 ). (y) might also
defined differently, instance allowing replacements values
vector question marks (the possibility replace values question marks might
also depend values variables vector take jointly). case
{w} would subset 25 completions. important set {w}
allow us identify value uniquely, unless singleton. tells
us IPs observational processes produce W coarsening , i.e., yielding
observation w corresponding set {w} possible values expect
767

fiZaffalon & Miranda

encompass (this expectation formalised Section 7.1.2). follows IPs
generalisation the, perhaps popular, missingness processes, consider
two possibilities: either {w} singleton Y, case said
missing. case Asia net, could characterise missingness process writing
(y) = {(v, k, b, h, o, l, a), (?, ?, ?, ?, ?, ?, ?)} = (v, k, b, h, o, l, a) Y.9 {w} = ,
w cannot produced Y, therefore eliminated W without
consequences; shall henceforth assume
w W {w} 6= .

(IP2)

Finally, define w W set facts w compatible
observation:
{w} := {y : (y) = {w}}.
3.3 Refining Facts, Observations, Incompleteness Process
section, add structure incompleteness process representing
combination two different incompleteness processes, act different parts
fact. model two parts writing := (Y , ), two new
variables, values Y, respectively, = Y. analogous way,
regard observation W observation W jointly observation
W , hence write W := (W , W ). again, W W two new variables,
values W W, respectively, W = W W.
additional variables introduced indeed allow us think two new IPs: first acts
variable , leading observation W , thus characterised certain multi-valued
map ; second acts variable , leading observation W , characterised
another multi-valued map . call unknown IP CAR IP, respectively,
former aim modelling IP whose behaviour unknown us
latter CAR IP.
Assuming
(y) = (y) (y)
(IP3)
allows us regard two IPs, taken together, single incompleteness process introduced Section 3.2, i.e., one maps (Y , ) = (W , W ) = W . on,
call overall IP. Assumption (IP3) follows consequence intention
model problems two IPs observe different parts fact therefore
interact. discussed length end Section 7.2.
impose assumptions unknown CAR IP. Consider
sets {w} , {w} , {w} , {w} , defined obvious way basis .
following assumptions resemble Assumptions (IP1)(IP2) motivated
arguments:
(y) 6=
(IP4)
9. avoid confusion, may worth outlining whether process coarsening missingness
process depends focus on. case Asia network, instance, process
missingness process single variable net (i.e., yields either value variable
question mark) simultaneously coarsening process vector variables (in sense
yield vector values vector made entirely question marks).

768

fiConservative Inference Rule

(y) 6=

(IP5)

w W {w} 6=

(IP6)

w W {w} 6= .

(IP7)

also impose additional requirement unknown IP:
w W {w} = .

(IP8)

means start implementing idea ignorance
procedure used unknown IP select observation starting fact.
(y) = (y) (y) implies {w} = {w} {w} {w} =
{w} {w} w W. shows (IP4)(IP7) imply (IP1)(IP2).

4. Conservative Inference Rule
notations introduced far finally allow us write definition conservative
inference rule. extent, assume beliefs (Z, ) form
joint lower prevision P 1 .
Definition 7 (Conservative inference rule) Consider gamble g Z w W,
assume P 1 ({w} ) > 0. Let {w}1 := {y {w} : P 1 (y, {w} ) > 0}, define
R(g|y, {w} ) :=

inf

P P 1 :P (y,{w} )>0

P (g|y, {w} )

{w}1 . let
R(g|w) := min R(g|y, {w} ).

(CIR)

y{w}1

Later, Section 7.3, shall see definition actually follows theorem
certain number assumptions.
Let us clarify intuition behind definition R(g|w). goal update beliefs
certain function g Z make observation w . Remember
w regarded pair (w, w), w part observation originated
unknown IP w originated CAR IP. conservative inference
rule prescribes update beliefs adopting lower prevision R(g|w) computed
Formula (CIR). means (i) consider completions w property
conditioning event (Y = y, {w} ) positive upper probability P 1 ;
(ii) compute updated beliefs events: applying
Bayes rule linear prevision dominates P 1 event positive
probability, create set posterior linear previsions whose lower envelope
R(g|y, {w} ); finally, define lower prevision R(g|w) minimum lower
previsions obtained completions considered. minimum, particular,
meaning conservative possible respect data made
incomplete unknown IP: deal considering complete data
could IP started operating. hand, data subject
CAR IP treated usual, is, conditioning set completions {w} .
769

fiZaffalon & Miranda

4.1 Significance Conservative Inference Rule
think CIR generalisation two kinds updating rules. generalises
traditional updating rule, one that, instance, prescribes discarding missing observations. CIR coincides rule case IP made CAR component.
hand, CAR component, overall IP unknown, CIR
similar (but powerful than) so-called conservative updating rule (CUR) proposed De Cooman Zaffalon (2004). components present, CIR acts
mix traditional updating CUR.
CUR, CIR imprecise-probability rule: generally leads lower upper
expectations, partially determined decisions. follows, part, allows P 1
imprecise. Yet, even take linear prevision P1 , CIR turns
imprecise-probability rule: imprecision arises logical consequence ignorance
unknown IP.
4.1.1 Comparison CUR
instructive analyse deeply difference CIR CUR. extent,
let us first give definition CUR using notation introduced far:
Definition 8 (Conservative updating rule) Assume Z finite set, =
W = W , is, overall IP entirely unknown. Furthermore, assume
originated missing values: means regard vector-valued,
components either observed precisely missing. Assume also P 1 (y) > 0
{w} . every gamble g Z, define
R(g|y) :=

inf

P P 1 :P (y)>0

P (g|y)

{w} . let
R(g|w) := min R(g|y).
y{w}

(CUR)

major differences CIR CUR discussed below.
first difference CUR allows unknown IP present.
so, CUR model beliefs stronger ignorance IP, CURbased inferences conservative necessary. CIR tries remedy
allowing mixed states knowledge made ignorance CAR. makes CIR
flexible rule lead strong enough conclusions many applications.
far important difference CIR CUR generality application. theory used derive CUR restricted case P 1 directly
assessed rather obtained number conditional unconditional
lower previsions, possibly together notion independence.
case, instance, statistical inference involves using certain number
lower previsions model prior knowledge likelihood function notion independence (or exchangeability) build P 1 . Although one apply CUR
also general common conditions, theory guarantee
770

fiConservative Inference Rule

CUR leads self-consistent inference cases. CIR, hand,
shown Sections 7.3 lead coherent inference wide spectrum
conditions, cover nearly practical situations. means
applying CIR always leads probabilities self-consistent well consistent
original assessments. (Observe since CUR special case CIR,
proofs finally show CUR also leads self-consistent inference.) words,
CIR much CUR rule general task updating beliefs based
observation facts. this, similar spirit traditional updating,
typically use every situation: e.g., case expert systems well
update beliefs given data problems statistical inference. CIR, different
applications follow simply giving different meanings facts (Y , ), observations
(W , W ), quantity Z interested. Section 5 give examples
show done variety cases.
three characteristics make CIR flexible CUR.
first CIR allows target space infinite, CUR defined
case finite spaces.
second CIR deals general incompleteness processes, CUR
special case processes may originate missing information
variables consideration. related restriction mentioned
definition CUR. characteristic CIR important aiming
obtain strong conclusions possible: taking advantage partially observed
facts, CIR leads general stronger conclusions CUR.
third somewhat technical. CUR requires every element {w}
given positive upper probability. CIR requires entire set {w} .
difference important practice. see this, consider applications
quite common represent deterministic relations among variables degenerate
probabilities equal zero one. relations naturally give rise zero upper
probabilities: every joint state variables satisfy relation
zero upper probability definition. CUR cannot used applications. CIR
can, simply leads neglect states zero upper probabilities:
precisely, given observation (w, w), leads consider compatible facts
(y, {w} ) possible condition given positive upper
probability, or, words, {w}1 .
Section 5.1 show differences CIR CUR impact
example.
4.1.2 Originality CIR
Something interesting note CIR, traditional updating CUR, based
variables Z : i.e., applying CIR one need consider
W -variables. makes CIR particularly simple use. Another consideration concerns
originality: best knowledge, CIR appears first time.
771

fiZaffalon & Miranda

contributions literature similar CIR case statistical model learning
samples made incomplete unknown missingness process (e.g., Manski, 2003; Ramoni
& Sebastiani, 2001; Zaffalon, 2002). surprising, intuition take
completions incomplete sample actually natural. aware
work proposing, especially deriving, abstract general rule update beliefs
incomplete information CIR.

5. Applications
following sections shall show CIR leads easily several different rules according applications study, present number examples.
make things simpler go details multi-valued maps used; shall
take W set nonempty subsets Y, assume hypotheses
done throughout paper hold. adopt similar considerations CAR IP.
5.1 CIR Expert Systems
Probabilistic expert systems represent key quantities domain vector variables,
section assumed take values finite spaces. One
variables target, i.e., variables objective inference. remaining
ones introduced extent inferring value target.
Asia network Figure 1 represents well-known artificial example expert system
medical domain. supposed expert provided graph well
probabilities define Asia network, reported Table 1. want use
network make diagnosis, first choose target node, R B.
collect information patient, well results medical tests, X-rays,
make available network instantiating related nodes graph
observed values. Finally, query network update probability target node
given evidence inserted.
usually nodes, apart target node, instantiated
values missing, way network compute belief updating depends
assumptions them. traditional way entails marginalising
equivalent assuming subject MAR process. special
case CIR obtained dropping unknown IP. general version CIR offers
flexibility treatment, produce different solution, illustrate
example below. also use coarsened observations show set-valued
observations may enter picture probabilistic inference CIR
CAR case.
Example 3 Say senior doctor hospital find desk report
junior colleague certain patient visited. patient came hospital
state dyspnea (A = a0 ). colleague visited patient find
sign bronchitis (H = h00 ). also made questions patient collect
background information. remembered ask something smoking
well recent trip Asia; end clear determinate
enough got know patient concerned one two
772

fiConservative Inference Rule

V = v0

0.01

K = k0

0.5

B=

b0

v0
0.05

v 00
0.01

R=

r0

k0
0.1

k 00
0.01

H = h0

k0
0.6

k 00
0.3

= o0

b0 r0
1

b0 r00
1

L=

l0

o0
0.98

o00
0.05

A=

a0

o0 h0
0.9

o0 h00
0.7

b00 r0
1

b00 r00
0

o00 h0
0.8

o00 h00
0.1

Table 1: Asia example: probabilities variable (first column) graph conditional
values parent variables. state prime corresponds yes.

773

fiZaffalon & Miranda

things. unexperienced colleague thought situation serious sent
patient back home. instead bit cautious. fact suspect
patient might hidden information privacy reasons, although recognise
know reasons. Overall, implicitly assuming existence
unknown IP variables V K leads {w} = {(v 0 , k 00 ), (v 00 , k 0 )},
denote w value W corresponds observation (V, K). words
regard (v 0 , k 00 ) (v 00 , k 0 ) two possible completions information
lack (V, K).
Let us make two remarks proceeding: (i) unknown IP coarsening
process, missingness process, allows us restrict completions
(V, K) two elements; (ii) despite name unknown IP (also very)
informative use coarsened rather missing values.
finally judge remaining variables subject MAR IP. reasonable colleague simply decided patient test. case
probability missingness one independently actual values variables
consideration. instructive stress case MAR IP colleague.
point ready diagnosis. first run algorithm Bayesian
nets using completion (v 0 , k 00 ), obtaining posterior probability cancer 0.052
tuberculosis 0.258; run algorithm more, case using
completion (v 00 , k 0 ), leads 0.423 updated probability cancer 0.042
tuberculosis. question first run suggests
diagnose tuberculosis, second diagnose cancer. Since
idea one right completion (V, K), admit able
discriminate tuberculosis cancer time. words, model
telling information weak draw useful conclusion,
implicitly suggesting collect stronger information. order so, invite
patient next visit, explaining importance knowing whether
Asia, something actually confirms, thus letting know eventually V = v 0
K = k 00 . updated probabilities 0.052 cancer 0.258 tuberculosis.
basis, ask patient undergo X-rays, turn abnormal (L = l0 ),
leading 0.151 0.754, respectively, probabilities cancer tuberculosis,
diagnose tuberculosis.
Consider might happen assuming variables subject CAR/MAR
IP, common Bayesian nets. case, information initially
= a0 , H = h00 , (V, K) {(v 0 , k 00 ), (v 00 , k 0 )}. CAR allows us
forget W write (V, K) belongs {(v 0 , k 00 ), (v 00 , k 0 )}. event
easy incorporate Bayesian network: enough insert new node Asia
network child V K state yes probability one
(V, K) {(v 0 , k 00 ), (v 00 , k 0 )}. new node instantiated state yes.
point, running algorithm Bayesian nets yields probabilities cancer
tuberculosis conditional = a0 , H = h00 , (V, K) {(v 0 , k 00 ), (v 00 , k 0 )}, 0.418
0.045, respectively. would make suspect cancer; moreover, might
well induce take following erroneous course reasoning: since probability
cancer high even without knowing exact values V K, trying obtain
information waste time; must rather focus concrete evidence
774

fiConservative Inference Rule

X-rays make diagnosis. obtaining positive X-rays test (L = l0 ) would
enforce beliefs even raising probability cancer 0.859, leading
mistaken diagnosis.
considerations limited expert systems based precise probability; completely analogous considerations would done case expert systems model
knowledge using closed convex sets mass functions (or, equivalently, coherent lower
prevision lower envelope) CIR also suited. Credal networks,
example, provide modelling capabilities (Cozman, 2000, 2005).
easy rephrase expert system models setting paper. Say
expert system based vector variables (Z, Y1 , . . . , Ym , Y1 , . . . , Yn ), Z
target variable, others subject unknown CAR
IP, respectively. sufficient write := (Y1 , . . . , Ym ), := (Y1 , . . . , Yn ),
consider set joint mass functions (Z, ), equivalently lower
prevision P 1 made taking lower envelope, given. inference expert
system, quite general form, corresponds compute R(g|w, w), w
observation vector w vector.
consider cases make things clearer. extreme, already
mentioned, case = 0, means CAR IP.
updating rule follows CIR traditional updating: R(g|w) = R(g|{w} ).
Say that, even specific, g indicator function Iz z Z,
0
0
{w} = {(y1 , . . . , yj , yj+1
, . . . , yn0 ) : yj+1
Yj+1 , . . . , yn0 Yn )}, i.e., first
j variables observed precisely, others missing. updating rule
becomes:
R(g|{w} ) = R(Iz |y1 , . . . , yj , Yj+1 , . . . , Yn ) = R(Iz |y1 , . . . , yj ),
equal inf P P 1 :P (y1 ,...,yj )>0 P (z|y1 , . . . , yj ). latter updating rule implemented credal networks; P 1 linear prevision, rule used Bayesian
networks.
Consider extreme: n = 0, i.e., unknown IP.
Similarly previous case, say g indicator function z; {w} =
0 , . . . , 0 ) : 0
0
{(y1 , . . . , yi , yi+1

i+1 Yi+1 , . . . , ym Ym )}. CIR becomes R(g|w) =
minyi+1 Yi+1 ,...,ym Ym inf P P 1 :P (y1 ,...,ym )>0 P (z|y1 , . . . , ym ). case nearly coincides
conservative updating rule proposed De Cooman Zaffalon (2004),
differences already discussed Section 4.1. differences important: instance,
CUR cannot deal coarsened observations makes impossible use model
observation Example 3. would even prevent CUR applied
generally example: one hand, presence logical gate
Asia network creates states zero upper probability (e.g., B = b0 , R = r0 , = o00 )
incompatible assumptions underlying CUR; other, since domain
knowledge (i.e., Asia net) built number conditional mass functions,
cannot guarantee CUR leads self-consistent inference proof deal
extended case. know CIR instead lead self-consistent inference
consequence Corollary 3 Appendix B.
neither n equal zero, CIR becomes mix two extreme cases
illustrated, example: leads treat variables subject CAR IP
775

fiZaffalon & Miranda

traditional updating, treating subject unknown IP similarly
CUR does. Mixing two things advantage greater expressivity: allows us
represent beliefs overall IP two extremes.
conclude section briefly discussing problem computations
CIR special case CUR.
context classification Bayesian nets according CUR, Antonucci
Zaffalon (2007) proved NP-hardness result, well given exact algorithm.
algorithm variant variable elimination algorithm better complexity
traditional algorithms Bayesian nets (implicitly using CAR): works linear
time polytree networks (i.e., one path
two nodes graph dropping orientation arcs) also number
general nets; remaining ones takes exponential time. Moreover, another
paper, Antonucci Zaffalon (2006) shown missing observations
problem CIR-updating Bayesian nets traditional (i.e., MAR-based)
updating credal nets equivalent. exploited recent paper (Antonucci &
Zaffalon, 2008, Section 9) show NP-hardness CIR-updating Bayesian nets,
give procedure solve problem via existing algorithms credal nets. idea
behind procedure relatively straightforward, takes inspiration method
proposed Pearl (1988, Section 4.3) represent instantiated node Bayesian net
using equivalent formulation: formulation based removing instantiation
node adding dummy child actually instantiated certain value.
approach basically taken CIR-updating case node missing
unknown way; CIR imposes deal considering possible instantiations.
Considering dummy-child method shown equivalent using
single imprecise-probability dummy child instantiated unique value.
way node originally missing unknown way treated node
missing random, dummy child simply instantiated imprecise-probability
node. consequence, original Bayesian net becomes credal network task
becomes computation MAR-based updating net. Since already
many algorithms designed task, described transformation allows one solve
CIR-updating problem via known algorithms MAR-updating problem credal nets.
Yet, MAR-updating credal networks difficult problem MAR-updating
Bayesian nets, NP-hard also polytrees (De Campos & Cozman, 2005). Therefore
CIR-updating appears demanding MAR- CUR-updating.
necessarily going problem practice approximate algorithms credal nets
nowadays allow one solve updating problems large-scale networks (e.g., Antonucci
et al., 2008).
5.2 CIR Parametric Statistical Inference
statistical problem parametric inference, given sample use update
beliefs so-called parameter, say , values . admissible values
index family data generation models consider possible problem
consideration. exclude possibility set infinite,
often case statistics.
776

fiConservative Inference Rule

setting, fact thus true, hence complete, sample. represent
following vector variables:


D1
D2


(1)
:= . .
.
.
DN
elements sample also called units data. assume on,
variables Di space possibilities every i.
exemplify problem parametric statistical inference focusing
Asia Bayesian network Figure 1: frequent step construction Bayesian
nets inference network parameters data set. Say, instance,
interested evaluating chance person suffers tuberculosis know
person made recent visit Asia. extent might exploit
data set variables B V , one below:




:=




(b0 , v 0 )
(b0 , v 00 )
(b0 , v 00 )
(b00 , v 00 )
(b00 , v 0 )
(b00 , v 0 )





,




(2)

assume addition data generated according identical
independently distributed (IID) process.
parametric inference relying tools Bayesian statistics. means
compute posterior linear prevision (i.e., posterior expectation) P (g|y),
certain function g : R. obtained integrating posterior density function
, obtained turn Bayes rule applied prior density function, likelihood
function (or likelihood ) L() := P (y|).
case, common choice prior would Beta density : Bs,t0 ()
0
0
st 1 (1 )s(1t )1 , t0 positive real hyper-parameters respectively
overall strength prior knowledge prior expectation . case
0
data set (2), choice leads posterior expectation equal 1+st
3+s .
0
Setting := 1 t0 := 1/2, namely, Perks prior (see Perks, 1947), obtain 1+st
3+s = 0.375,
regarded approximation true parameter value.
Imprecise probability approaches parametric inference often extend Bayesian statistical methods working set prior density functions, updating
Bayes rule, whenever possible, using likelihood function, thus obtaining set
posteriors. example imprecise Dirichlet model proposed Walley (1996a).
notation terminology, means imprecise case parametric inference
based unconditional lower prevision P 1 (, ), obtained means prior P 1 ()
likelihood P (Y |) rule called marginal extension, updated
posterior lower prevision R(|Y ) procedure called regular extension. Marginal
regular extension introduced precisely Section 6.
777

fiZaffalon & Miranda

previous example related data set (2) variables binary,
imprecise Dirichlet model called imprecise Beta model (Bernard, 1996), easily
applied follows. idea consider set Beta densities fixed prior
strength, say = 1: i.e., set Bs,t0 = 1 t0 (0, 1).10 set
regarded single imprecise prior states prior probability category
b0 lies (0, 1), seems reasonable way model state prior ignorance
Bayesian prior, Perks above. imprecise prior leads imprecise
0
posterior expectation : compute it, enough reconsider expression 1+st
3+s
let t0 take values (0, 1); leads interval [0.25, 0.50] delimited
0
0
0
lower upper posterior probabilities obtained 1+st
3+s = 0 = 1,
respectively. Again, interval estimate11 appears much reasonable
reliable precise posterior expectation obtained Bayesian approach, taking
especially account sample available infer parameter consideration
size three!
real problems often face complication deal
incompleteness statistical data: rather complete sample y, may given
incomplete sample w, i.e., one corresponds set {w} complete samples.
set {w} arises consequence missingness partial observability values.
data set (2) might instance turned incompleteness process following
incomplete data set:
0 0
(b , v )
(b0 , v 00 )


(b0 , ?)

(3)
w :=
(b00 , v 00 ) .


(b00 , ?)
(?, v 0 )
example {w} set eight complete data sets obtained replacing
question marks possible ways.
question parametric inference incomplete sample.
represent unit Vector (1) regarding generic variable Di pair (Yi , Yi ),
let := (Y1 , . . . , YN ), := (Y1 , . . . , YN ), = (Y , ).
easy use CIR address question parametric inference incomplete
data, know subject CAR IP, know IP acts
. Observing plays role Z, obtain CIR prescribes using following
rule: R(g|w) = miny{w}1 R(g|y, {w} ), rewrites also
R(g|w, w) = min
y{w}1

inf

P P 1 :P (y,{w} )>0

P (g|y, {w} ),

(4)

10. reason exclude extreme points interval avoid creating conditioning events
zero lower probability discuss implications. present setup restrictive,
shown closed interval would eventually lead inferences.
11. important aware intervals conceptually different tools Bayesian
credible intervals, arise second-order information chance. present intervals
arise ignorance unknown IP, something makes set IPs consistent
assumptions second-order information. Stated differently,
counterpart intervals precise case point estimates, credible intervals,
instead compared imprecise credible intervals (e.g., see Walley, 1996a, Section 3).

778

fiConservative Inference Rule

imprecision (, ) originates via prior imprecision only. prove Corollary 4
Appendix B rule leads self-consistent inference.
regard lower expectation Equation (4) arising two kinds imprecise beliefs. first beliefs model lower prevision P 1 ().
remaining beliefs embodied likelihood function. imprecise knowledge, too,
since actually multiple likelihood functions, unknown IP,
model using different conditional previsions P (|y, {w} ) {w}1 .
words, working incomplete data according CIR equivalent working set
posteriors arise applying Bayes rule, whenever possible, prior-likelihood
pair model.
case data set (2), might know order create incomplete
sample (3) it, variable B subject MAR missingness process,
might know kind missingness process acted variable V ;
case, generic unit (2) written pair (yi , yi ). Say focus
computing posterior expectation , chosen person suffers
tuberculosis know person made recent visit Asia. Say
also use Perks prior Bayesian case, order get
distracted discussion prior imprecision. case, outer minimum (4)
corresponds minimising four completions variable V ; following infimum
corresponds take account Perks prior. four completions mentioned give
rise four different data sets contain single missing value last unit.
them, compute lower probability category b0 conditional v 0 , using
Expectation-Maximisation algorithm (Dempster, Laird, & Rubin, 1977) together Perks
prior, obtaining four values: 0.63, 0.87, 0.50, 0.83 (these values rounded second
digit). lower probability minimum: 0.5; analogously, maximum
upper probability: 0.87. think two values delimiting interval
probability consideration: [0.5, 0.87]. width interval reflects
ignorance missingness process V .
consider Bayesian case, using B V MAR assumption,
common learning parameters data sets. case, ExpectationMaximisation algorithm together Perks prior yields estimate probability
equal 0.86. Apart unrealistic precision estimate obtained
small data set, observe using MAR arbitrarily led us close upper
extreme interval [0.5, 0.87]. even questionable consider
Bayesian estimate complete data set (2) close lower extreme.
summary, using CIR able obtain interval estimates arguably
realistic point estimates provided traditional methods
MAR assumption justified. also follows exploiting option given CIR
use models prior knowledge carefully model available information,
absence information, imprecise Dirichlet model.
Finally, example illustrates, working multiple likelihoods consequence
taking possible completions incomplete part sample subject
unknown IP. intuitive procedure; therefore, surprising analogous
procedures already advocated missing data context robust statistical inference (see, e.g., Manski, 2003; Ramoni & Sebastiani, 2001; Zaffalon, 2002). Actually,
779

fiZaffalon & Miranda

discussion (and following) section regarded formal justification
cited approaches point view theory coherent lower previsions.
also regarded generalisation approaches considers
joint presence CAR unknown IP, allows one work
incomplete rather missing data.
5.2.1 IID+ID Case
previous section, made hypotheses generation complete
sample. However, practice frequent deal independent identically
distributed data (also called multinomial data), indeed already assumed IID
data (2). multinomial data, units identically distributed conditional
, helps simplifying developments. Call space possibilities common
units. assume finite (as shall derivation CIR
rule Section 7.3), parameter defined vector [d ]dD , generic
element represents aleatory probability = d. follows vector
[d ]dD , whose generic element P (d|); subset |D|-dimensional
unit simplex. Taking account
QNunits also independent conditional ,
follows likelihood factorises i=1 di .
complete data IID, may reasonable assume also overall IP
independently distributed (ID). assumption allows us represent observation
complete sample vector, i.e., incomplete sample:


W1
W2


W := . .
..
WN
generic unit Wi observation Di . Remember regard Di
pair (Yi , Yi ); consequence, also regard Wi pair variables (Wi , Wi ),
Wi Wi observations Yi Yi , respectively. Finally, let W := (W1 , W2 , . . . , WN )
W := (W1 , W2 , . . . , WN ). example sufficient consider data set (3),
instance W current language; generic unit instance
Wi = (Wi , Wi ), Wi corresponds variable V Wi B.
Using newly introduced notation, form CIR takes following (with
obvious meaning symbols):
R(g|w, w) =

min

(y1 ,...,yN ){w}1

R(g|y1 , . . . , yN , {w1 } , . . . , {wN } ),

(5)

R(g|y1 , . . . , yN , {w1 } , . . . , {wN } ) equal
inf

P P 1 ,P (y1 ,...,yN ,{w1 } ,...,{wN } )>0

P (g|y1 , . . . , yN , {w1 } , . . . , {wN } ).

rule leads self-consistent inference established Corollary 4 Appendix B.
Moreover, new formulation shows ID assumption incompleteness process leads practice inhibiting certain types coarsening: create logical
780

fiConservative Inference Rule

connections different units. one hand, confirms expressive power
coarsening. other, suggests applications might easier forget
ID assumption simply focus specification kind coarsening
behaviour. approach appears intuitive hence accessible especially
people little experience statistics might involved analysis.
5.2.2 Full IID Case?
reasonable assume IP identically distributed besides independently distributed? consequences assumption?
start addressing second question. IID complete data, generic
element fixed aleatory probability, denoted . Call W space
possibilities common variables Wi , = 1, . . . , N . IID IP, generic
element w W fixed aleatory probability produced conditional d, let us
. follows w fixed unconditional aleatory probability produced:
call P
w
w := dD dw , means process produces elements W also
multinomial. seems considerably simplify setup considered far, thus
regarded advantage full IID assumption (this advantage clear practical
consequences case pattern classification, illustrated Section 5.3.2.)
hand, turn address first question, possible advantages seem dwarfed strength assumption itself. indeed questionable
assumption may largely valid applications. IPs often generated
sophisticated behaviour patterns involve humans, complex agents,
regarded protocols communications. case giving giving
information fundamental intelligent part communication. Assuming
case IP IID, seems much strong assumption. fact,
fundamental difference data-generating processes (giving rise Z )
observational processes (giving rise W ). believe essence difference
number cases make much sense assume observational
process identically distributed, IID make sense, least reasonable
approximation many data-generating processes.
5.2.3 CAR Units
considered IID+ID setup Section 5.2.1 gives us opportunity slightly
extend analysis done far. idea since overall IP need
identically distributed, may happen act CAR process units. Say
certain unit 0 entirely coarsened random. question form takes CIR now.
easy see defining := (Y1 , . . . , YN ), := (D0 , Y1 , . . . , YN ),
corresponding observation variables W := (W1 , . . . , WN ), W := (W0 , W1 , . . . , WN ),
lead following rule:

R(g|w, w) =

min

(y1 ,...,yN ){w}1

R(g|y1 , . . . , yN , {w0 } , {w1 } , . . . , {wN } ).
781

fiZaffalon & Miranda

particular, Unit 0 missing, i.e., {w0 } = D, rule prescribes discard unit
consideration:
R(g|w, w) =

min

(y1 ,...,yN ){w}1

R(g|y1 , . . . , yN , {w1 } , . . . , {wN } ).

observation particularly important applications may well case
practice units entirely missing non-selective way. CIR tells us
units, Unit 0 above, going alter beliefs Z.
situation obviously different units turned missing units
unknown process; case justification discard them. correct way
proceed make missing values part observed data, apply
CIR again. would lead us consider completions missing units
account.
5.3 CIR Pattern Classification
section focuses problems pattern classification, special case so-called predictive
inference. Loosely speaking, kind inference concerned predicting future
elements sequence based available part sequence itself. classification
problem, available sequence represented following matrix:


C1 F1
C2 F2


(6)
..
.. .
.
.
CN

FN

generic line i, unit i, matrix represents object described pair
variables relabel Di := (Ci , Fi ) consistent notation used
parametric inference. Variable Ci represents characteristic object
interested, call class. Denote set classes C. definition pattern
classification, C finite set. Variable Fi represents features object
informative class.
try make clear example focused Asia network.
Say data set records state variables Asia network
number persons. Say also interested classifying people smokers
nonsmokers. case, value Ci , i.e., class variable i-th person
data set, state variable K Asia network person i. Variable Fi
vector remaining variables Asia network person; words,
Fi represents profile person data set.
consider next element sequence, represented unit
DN +1 := (CN +1 , FN +1 ), also called unit classify. previous example, DN +1
would next person see. goal classification predict value
CN +1 assumes given values variables; example amounts
predict whether next person smoker given profile person,
relationship profiles classes suggested historical data.
words, framework CN +1 Z (D1 , . . . , DN , FN +1 ) .
782

fiConservative Inference Rule

Predicting class c0 CN +1 regarded action uncertain reward gc0 ,
whose value gc0 (c) depends value c CN +1 actually assumes. case precise
probability, optimal prediction class copt maximises expected reward:
P (gcopt |d1 , . . . , dN , fN +1 ) P (gc0 |d1 , . . . , dN , fN +1 ),

c0 C.

previous expression equivalent next:
P (gcopt gc0 |d1 , . . . , dN , fN +1 ) 0,

c0 C.

similar expression used imprecise probability: case optimal
class one undominated, i.e., c0 C:12
P (gcopt gc0 |d1 , . . . , dN , fN +1 ) > 0.
Despite similarity notation, important realise fundamental
difference two cases: precise probability always leads determinate prediction,
imprecise probability not. imprecise probability progression according
stronger beliefs lead less indeterminate decisions, completely determined
ones (see paper De Cooman & Zaffalon, 2004, Section 2.10, wider discussion
decision making imprecise probability).
discussion point highlighted precise imprecise
probabilities, predictions based (upper, hence lower) previsions functions
g : C R. order address issue incompleteness pattern classification,
therefore, without loss generality, restrict attention task updating beliefs
CN +1 .
task usually regarded made two distinct steps. first concerned
learning sample, represented Matrix (6), probabilistic model
relationship features classes. second applying model observed
value fN +1 FN +1 order finally compute (lower) prevision gamble g.
illustrate two steps case IID data; general cases treated
analogously.
Bayesian framework, first step often done way similar parametric
inference. One defines family density functions may responsible producing
units data, indexes admissible values (usually continuous)
parameter , assesses prior density . prior likelihood lead
via Bayes rule posterior density conditional (d1 , . . . , dN ). second step
corresponds use posterior together density (or mass function)
DN +1 conditional , obtain (integrating out) so-called predictive posterior:
i.e., mass function CN +1 conditional (d1 , . . . , dN , fN +1 ), embodies
wanted probabilistic model relationship features classes. Computing
posterior prevision P (g|d1 , . . . , dN , fN +1 ) trivial point.
situation similar imprecise case, difference using set
prior densities, i.e., lower prior P 1 (), leads lower predictive prevision:
P (g|d1 , . . . , dN , fN +1 ) :=

inf

P P 1 :P (d1 ,...,dN ,fN +1 )>0

P (g|d1 , . . . , dN , fN +1 ).

12. Walley (1991) calls maximality related decision criterion. See work Troffaes (2007)
comparison criteria.

783

fiZaffalon & Miranda

ready address issue incomplete data using CIR. must first
define variables subject CAR unknown IP. respect
feature variables, assume generic variable Fi (i = 1, . . . , N + 1) equal
pair (Fi , Fi ), usual meaning symbols. Define vectors :=
(C1 , F1 , . . . , CN , FN , FN +1 ), := (F1 , . . . , FN , FN +1 ). make things readable,
also define Yi := (Ci , Fi ), Yi := Fi , = 1, . . . , N ; YN +1 := FN +1 , YN +1 := FN +1 .
obviously arbitrariness definitions vectors ,
might well case applications require different choices. present
definitions done illustrative purposes.
definitions, CIR leads following rule update beliefs g:
R(g|w, w) = min(y1 ,...,yN +1 ){w}1 inf P P 1 :P (y1 ,...,yN +1 ,{w1 } ,...,{wN +1 } )>0
P (g|y1 , . . . , yN +1 , {w1 } , . . . , {wN +1 } ).

(7)

Corollary 5 proves rule leads self-consistent inference. Equation (7) states
one consider (i) set possible completions part observed sample

originated unknown IP, N
i=1 {wi }1 ; (ii) set possible completions part
unit classify originated unknown IP, {wN +1 }1 ; (iii) set (possible)
precise priors, dominating prior lower prevision P 1 ().
choices, problem becomes computation posterior expectation g
single Bayesian (i.e., precise-probability) classifier. consider possible
choices (i)(iii) above, working (7) amounts work set Bayesian classifiers.
set posterior expectations originated Bayesian classifiers summarised (7)
lower expectation.
look problem also another point view. Imprecision produces
lower expectation (7) regarded originated three components. first
presence multiple priors. second presence multiple likelihoods,
consistent certain completion part sample originated
unknown IP. third component set-valued observation part FN +1
features unit classify, regard imprecise description object
classify.
5.3.1 Example
Let us focus initial example Section 5.3 interested
classifying people either smokers nonsmokers. make things easier, take gc0
0-1 loss function, i.e., compute posterior probabilities rather expectations
order issue classification. Moreover, take graph Asia net represent
probabilistic dependencies variables involved (yet, assume
net parameters given, learning parameters data part inferential
task classification solve). make example handy, assume
people want classify miss information variables V , B,
O, L Asia net, missingness MAR: is, variables
constitute FN +1 . implies, (7), actually discard variables V , B,
O, L consideration: equivalently work inference task relying
learning set variables K, R H alone. holds working
784

fiConservative Inference Rule

(7) amounts work set Bayesian classifiers, property holds
them.13 consequence, turns portion Asia net
relevant classification displayed Figure 2. structure,

Smo(K)ing

+



Lung cance(R)

Bronc(H)itis

Figure 2: naive sub-network Asia net related variables K, R H.
feature variables represented disconnected children class variable called
naive network. use precise-probability classifier, best known naive
Bayes classifier (Duda & Hart, 1973).
Say learning set, originated IID process, following:








:=








(r0 , h00 , k 0 )
(r0 , h00 , k 0 )
(r0 , h0 , k 0 )
(r0 , h0 , k 0 )
(r00 , h0 , k 0 )
(r00 , h0 , k 00 )
(r00 , h0 , k 00 )
(r00 , h00 , k 00 )
(r00 , h00 , k 00 )
(r0 , h00 , k 00 )









.








data set characterised strong positive relationship smoking cancer
weak positive relationship smoking bronchitis. Learning naive Bayes
applying model four joint instances feature variables,
obtain following predictions:


(r0 , h0 , k 0 )
(r0 , h00 , k 0 )


(8)
(r00 , h0 , k 00 ) ,
(r00 , h00 , k 00 )
used Perks prior naive Bayes14 experiments. replace
prior imprecise one, obtain extension naive Bayes imprecise
13. proof relationship specific classifier found work Corani Zaffalon (2008,
Section 3.1.2).
14. Modified described Zaffalon (2001, Section 5.2) (refer Perks prior defined analogy
IDM section) make comparison easier.

785

fiZaffalon & Miranda

probability. moreover define prior P 1 () model state prior
ignorance, similarly done case parametric inference, obtain
imprecise-probability classifier instance so-called naive credal classifier 2
(or NCC2, see Corani & Zaffalon, 2008). reinterpret NCC2 set naive Bayes
classifiers. present case, classifier per precise prior consistent
(i.e., dominating) imprecise one.
way NCC2 issues classification specific instance R H understood
easily terms set naive Bayes classifiers corresponds: classification
NCC2 union classes issued naive Bayes classifiers set.15
means naive Bayes classifiers issue class, classification
NCC2 determinate, i.e., made single class. Otherwise, disagreement
among naive Bayes classifiers, NCC2 issues entire set K, i.e., indeterminate
classification.16
obtain determinate predictions
infer NCC2 learning set d,
17
reported (8): means information data strong enough smooth
contribution precise prior used NCC2 favor single class.
move interesting examples introducing missing values. Consider
following two instances classify missing information cancer:


(?, h0 )
.
(9)
(?, h00 )
treatment instances different naive Bayes NCC2. NCC2
naturally embed assumption R H made missing unknown
process, deal considering two completions question mark
instance. words, classification NCC2 only, before, equal
union classes delivered equivalent set naive Bayes classifiers, union
also taken respect two completions instance classify.
hand, naive Bayes assume MAR deal incomplete instances,
lead discard R conditioning variables.
classifications issued naive Bayes NCC2 respectively:



(?, h0 , k 0 )
(?, h0 , K)
,
.
(?, h00 , k 00 )
(?, h00 , K)
words, naive Bayes knowing state bronchitis sufficient determine
whether person smoker not, despite weak relationship learning set
two characteristics; NCC2 cautious believes possible
determine smoking state missing, necessarily ignorable way,
value important predictor variable. hand, checked
missing variable H instead R, naive Bayes NCC2 agree
15. case class variable binary.
16. total indeterminacy consequence class variable binary; cases output set
classes subset classes, least informative one. cases, classification
partially indeterminate, carries therefore useful information.
17. predictions NCC2 computed open-source software freely available address
http://www.idsia.ch/giorgio/jncc2.html.

786

fiConservative Inference Rule

prediction determinate: knowing person cancer tells us person
smoker, vice versa.
situation become even critical naive Bayes use incomplete

learning set wd := (w1 , . . . , wN ) rather d,








wd :=








(r0 , h00 , k 0 )
(r0 , h00 , k 0 )
(r0 , ?, k 0 )
(r0 , ?, k 0 )
(r00 , ?, k 0 )
(r00 , h0 , k 00 )
(r00 , h0 , k 00 )
(r00 , ?, k 00 )
(r00 , ?, k 00 )
(r0 , ?, k 00 )









.








originated data set turning values H missing
complete portion data set suggests H good predictor K.
going problem naive Bayes, still deal incompleteness
assuming MAR, leads compute counts learning set discarding
missing values. NCC2, hand, implicitly considers complete data sets
consistent wd. Therefore case NCC2 regarded set naive
Bayes classifiers obtained considering precise priors previously described
relationship NCC2 likelihoods arise complete data sets
consistent wd.
Running classifiers predict class units (9), obtain naive
Bayes predicts first person smoker second is, cases
posterior probability equal 0.90 ! NCC2, reasonably, outputs K cases,
way say information enough make reliable prediction.
example also points NCC2 correctly suspends judgment even instances
precise models naive Bayes actually confident (for details
point, see Section 4.4 paper Corani & Zaffalon, 2008).
5.3.2 Failure Empirical Evaluations
Consider question IID vs. ID assumption IP, discussed Section 5.2.2, placed context classification. make things easier, focus
finite possibility spaces, assume addition CAR IP present
classes always observed precisely (i.e., singletons).
first interesting thing note following. Recall imposing full IID
assumption (i.e., IID data-generating process IP) implies
(set-valued) observations regarded outcomes multinomial process. This,
together fact classes always observed precisely, enables us discard
-variables consideration: necessary one learn relationship classes features directly using W -variables. make
clear, consider case incomplete samples originated missing values.
787

fiZaffalon & Miranda

full IID assumption implies case one allowed regard symbol ? missing value possible value, proceed traditional learning methods
case complete samples. shows full IID assumption classification
makes CIR collapse traditional updating rule, although applied W -variables.
makes things easier deal with, thus regarded advantage
full IID assumption classification. already argued IID assumption
IP strong, turn consider weaker ID assumption also pattern
classification, consider tenable.
Let us focus simple classification setting complete data generated
IID way, IP ID. problem predict class unit
classify given features previous units (1, . . . , N ). already mentioned
Section 5.3, usually done precise case following maximum expected utility
approach. Call model acts way precise classifier.
common practice precise classifiers measure accuracy empirically.
simplest form, obtained randomly splitting available data learning
test set, inferring classifier former testing latter. simple,
yet powerful, idea responsible much success popularity classification,
enables one relatively confident well classifier performs previously
unseen data. Unfortunately, key characteristic lost cannot assume
unknown IP IID.
see why, consider (for = 1, . . . , N + 1) Boolean class variable Ci two
Boolean feature variables Ai Bi , Fi = Fi = (Ai , Bi ). Assume class
result exclusive logical disjunction two attributes: i.e., class equals one
either first second attribute equals one, both. Assume
complete data eventually turned incomplete data (unknown) IP whose
action make Bi missing (Ai , Bi ) = (1, 0), i. Let WAi
WBi observation variables Ai Bi , respectively. IP characterised
P (WBi =?|Ai = 1, Bi = 0) = 1, observing pattern (WAi = 1, WBi =?)
implies Ci = 1 certainty. conditions, precise classifier clearly expected
learn BN +1 missing irrelevant predict CN +1 , whose value coincides
value +1 . Since true available data, partitioning
learning test set nothing confirm it: prediction accuracy pattern
(WAN +1 = 1, WBN +1 =?) perfect, i.e., 100%. IP identically distributed,
happens classifier put work operative environment, IP
changes, particular making Bi missing (Ai , Bi ) = (1, 1); or,
words: P (WBi =?|Ai = 1, Bi = 1) = 1. put work practice, classifier
always wrong pattern (WAN +1 = 1, WBN +1 =?), prediction accuracy dropping
0%.
course example designed illustrate extreme situation. However,
experiments real data sets done without using extreme unknown IP, reported
Corani Zaffalon (2008, Section 4.6), show phenomenon indeed severely
bias empirical measures performance. appears point fact: empirical
evaluations doomed failure general data made incomplete nonIID unknown process.
788

fiConservative Inference Rule

considerations may profound implications classification, generally data analysis. fields scientific research rest two fundamental pillars: (i)
assumptions made develop certain model (e.g., classifier) tenable; (ii)
empirical evaluations reliable. crucial point pillars may
fragile incomplete data, unable sustain credible models conclusions.
way left cope critical issues seems necessarily rely tenable
assumptions. involves recognising incompleteness process may IID.

6. Advanced Notions Coherent Lower Previsions
introduce advanced notions coherent lower previsions need
rest paper, particular derivation CIR Section 7.
6.1 Coherence Number Conditional Lower Previsions
Reconsider setup introduced Section 2 made variables X1 , . . . , Xn
subject expresses beliefs. practice, provide assessments disjoint subsets
O, {1, . . . , n}; thus uncommon model subjects beliefs using finite number
different conditional lower previsions. Formally, going consider shall
call collections conditional lower previsions.
Definition 9 Let P 1 (XO1 |XI1 ), . . . , P (XOm |XIm ) conditional lower previsions respective domains K1 , . . . , Km L(X n ), Kj set XOj Ij -measurable gambles,18
j = 1, . . . , m. called collection X n j1 6= j2 {1, . . . , m},
either Oj1 6= Oj2 Ij1 6= Ij2 .
means two different conditional lower previsions giving information
set variables XO , conditional set variables XI .
Even conditional lower previsions collection separately coherent,
need coherent one another, collection could still express inconsistent
beliefs. able model joint coherence, first introduce new concepts.
Remember use IA denote indicator function set A, i.e., function
whose value 1 elements 0 elsewhere.
Definition 10 gambles f domain KOI conditional lower prevision
P (XO |XI ), x XI , shall denote G(f |x) gamble takes
P value
n
I1 (x) (y)(f (y)P (f |x)) X , G(f |XI ) gamble equal xXI G(f |x).


(almost-)desirable gambles subject: behavioural interpretation P (f |x) supremum acceptable buying price f contingent x, gamble
G(f |x)+I1 (x) equivalent buying f price P (f |x), provided XI = x,

therefore desirable. Since happens arbitrarily
P small, deduce transaction G(f |x) almost-desirable. G(f |XI ) = xXI G(f |x) also almost-desirable
follows rationality principle sum gambles almost-desirable
subject also almost-desirable.19
18. use Kj instead KOj Ij order alleviate notation confusion possible
variables involved.
19. case infinite XI need add another rationality principle (Walley, 1991, Sect. 6.3.3).

789

fiZaffalon & Miranda

Definition 11 XI -support S(f ) gamble f KOI given
S(f ) := {I1 (x) : x XI , f I1 (x) 6= 0},

(10)



i.e., set conditioning events restriction f identically zero.
Definition 12 Let P 1 (XO1 |XI1 ), . . . , P (XOm |XIm ) separately coherent. coherent fj Kj , j = 1, . . . , m, j0 {1, . . . , m}, f0 Kj0 , z0 XIj0 ,
B {I1
(z0 )}
j=1 Sj (fj )
j0



X
sup
Gj (fj |XIj ) Gj0 (f0 |z0 ) (x) 0,
xB

j=1

Sj (fj ) XIj -support fj , defined Equation (10).
restrict supremum subset X n gamble f 0 f < 0
subset X n desirable subject.
notion Definition 12 sometimes also called joint (or strong) coherence.
strongest notion self-consistency Walleys theory, regarded unique
axiom theory. intuition behind notion able
raise conditional lower prevision gamble taking account acceptable
transactions implicit conditional previsions. Let us make clearer. Assume
Definition 12 fails, > 0



X

Gj (fj |XIj ) Gj0 (f0 |z0 ) (x) < 0,
(11)
j=1

(z0 )}
every x B {I1
j=1 Sj (fj ). consequence,
j
0

Gj0 (f0 |z0 ) Iz0 = Iz0 (f0 (P j0 (f0 |z0 ) + ))


X

Gj (fj |XIj ),

j=1

(z0 )}
taking account inequality holds trivially elements outside {I1
j0

j=1 Sj (fj ), class consequence Equation (11). since righthand side sum almost-desirable gambles, means left-hand side
also almost-desirable. means price strictly smaller P j0 (f0 |z0 ) +
acceptable buying price gamble f0 , conditional z0 . contradicts
interpretation P j0 (f0 |z0 ) supremum acceptable buying price.
Example 4 (Walley, 1991, Example 7.3.5) Let X1 , X2 two random variables taking
values X1 = X2 = {1, 2, 3}, assume make somewhat contradictory assessments X1 = 1 X2 = 1, X1 = 2 X2 = 2, X2 = 1 X1 = 2, X2 = 2 X1 = 1
X1 = 3 X2 = 3. assessments modelled conditional previsions
P (X1 |X2 ) P (X2 |X1 ) given
P (f |X2 = 1) = f (2, 1), P (f |X2 = 2) = f (1, 2), P (f |X2 = 3) = f (3, 3)
P (f |X1 = 1) = f (1, 1), P (f |X1 = 2) = f (2, 2), P (f |X1 = 3) = f (3, 3)
790

fiConservative Inference Rule

gamble f X1 X2 . see coherent, consider gambles
f1 = I{(1,2),(2,1)} , f2 = I{(1,1),(2,2)} , f3 = 0. follows Equation (10) S1 (f2 ) =
{{1} X2 , {2} X2 } S2 (f1 ) = {X1 {1}, X1 {2}}.
G(f1 |X2 ) + G(f2 |X1 ) G(f3 |X1 = 1) = G(f1 |X2 ) + G(f2 |X1 ) < 0
set {(x1 , x2 ) B B S1 (f2 ) S2 (f1 ) {11 (1)}} = X1 X2 \ {(3, 3)}.
6.2 Coherence Graphs
collection lower previsions given graphical representation call coherence graph (Miranda & Zaffalon, 2009). coherence graph directed graph made two
types nodes: actual dummy nodes. Dummy nodes one-to-one correspondence
lower previsions collection; actual nodes represent variables X1 , . . . , Xn .




?

K

?

V


?




R

B


U

H

w

? - sff

?
?





L

Figure 3: coherence graph originated A1+ -representable collection: P 1 (V ),
P 2 (K), P 3 (B|V ), P 4 (R|K), P 5 (H|K), P 6 (O|B, R), P 7 (L|O), P 8 (A|O, H).
Figure 3 shows coherence graph conditional distributions owned nodes
Asia network represent lower previsions, i.e., P 1 (V ), P 2 (K), P 3 (B|V ),
P 4 (R|K), P 5 (H|K), P 6 (O|B, R), P 7 (L|O), P 8 (A|O, H). order avoid confusion,
stress coherence graph built set lower previsions,
arising graphical models. use Asia network illustrative purposes.
Moreover, building coherence graph Asia net, completely disregard
independence information coded net, focus list conditional
distributions. Also, coherence graph irrespective whether conditional
assessments linear not.
adopt conventions display coherence graph: denote actual nodes
letter corresponding variable; dummy nodes instead denoted black
solid circles labelled. Finally, dummy node single parent
single child, show arrow entering node, make graph simpler
see.
collection lower previsions turned coherence graph turning
lower previsions subgraph called D-structure: directed graph consisting
dummy node, directed predecessors (i.e., parents) successors (i.e., children),
791

fiZaffalon & Miranda

arcs connecting dummy node parents children. parents
dummy node actual nodes corresponding variables right-hand side
conditioning bar prevision related dummy node. children
actual nodes corresponding variables left-hand side conditioning
bar. example, D-structure P 1 (V ) subgraph Figure 3 consisting V ,
dummy parent, arc connecting them; D-structure P 6 (O|B, R)
subgraph consisting actual nodes B, R, O, dummy child first two
parent last one, three arcs connecting them.
order distinguish coherence graph graphical models Bayesian
nets, note coherence graph cannot deduce independence nodes
graph: instance, Figure 3 cannot conclude variables L, independent
given O, H. reason instances conditional previsions
coherence graph variables L dependent.
Coherence graphs different forms, forms implication
coherence related collection lower previsions. paper focus
coherence graphs called type A1+ : coherence graphs acyclic
property actual node graph exactly one parent. indeed case
graph Figure 3. general class graphs A1 graphs, defined
similarly A1+ case difference actual node graph
required one parent.
coherence graph type A1+ (resp. A1), say related collection
lower prevision A1+ -representable (resp. A1-representable). A1-representable collections
interesting separate coherence lower previsions collection
equivalent joint coherence, shown Miranda Zaffalon (2009, Proposition 4).
means A1-representable collections known coherent irrespective
numbers make lower previsions, provided give rise separately
coherent lower previsions.
6.3 Updating Coherent Lower Previsions
One updating rule used lower previsions regular extension:
Definition 13 Let P coherent lower prevision, f KOI x XI element
P (x) > 0, P n
conjugate upper prevision derived
P . regular
P (f Ix )
extension R(f |x) R(f |x) := inf P (x) : P M(P ), P (x) > 0 .
Recall M(P ) set linear previsions P domain L(X n ) dominate P .
XI finite P (x) > 0 x XI , conditional lower prevision R(XO |XI )
defined regular extension coherent P . definition shows regular
extension also nice sensitivity analysis interpretation: i.e., applying Bayes rule
dominating linear previsions whenever possible. Perhaps reason, regular
extension proposed used number times literature updating
rule (e.g., De Campos, Lamata, & Moral, 1990; De Cooman & Zaffalon, 2004; Fagin &
Halpern, 1991; Jaffray, 1992; Walley, 1981, 1991, 1996b).
Example 5 Assume know coin loaded, sense either always lands
heads tails, know which. Let Xi outcome throw
792

fiConservative Inference Rule

= 1, 2. beliefs (X1 , X2 ) may modelled vacuous lower prevision P
{(heads, heads), (tails, tails)}. apply regular extension define R(X2 |X1 ), obtain
R(X2 = heads|X1 = heads) = 1, R(X2 = tails|X1 = tails) = 1,
taking account P (heads, heads) + P (tails, tails) = 1 linear prevision P
M(P ).
6.4 Products Conditional Lower Previsions
shall later use generalisation marginal extension theorem (Walley, 1991, Theorem 6.7.2) established Miranda De Cooman (2007).
Definition 14 Let P 1 (XO1 ), P 2 (XO2 |XI2 ), . . . , P (XOm |XIm ) separately coherent conditional lower previsions respective domains K1 , . . . , Km , I1 = Ij =
n
j1
i=1 (Ii Oi ) = Ij1 Oj1 j = 2, . . . , m. marginal extension L(X )
given
P (f ) = P 1 (P 2 (. . . (P (f |XIm )| . . . )|XI2 )),
smallest unconditional coherent lower prevision (jointly) coherent
P 1 (XO1 ), P 2 (XO2 |XI2 ), P 3 (XO3 |XI3 ), . . . , P (XOm |XIm ).
see definition makes sense, note gamble f X n ,
P (f |XIm ) gamble XIm , hence belongs domain P m1 (XOm1 |XIm1 );
P m1 (P (f |XIm )|XIm1 ) gamble XIm1 belongs therefore domain
P m2 (XOm2 |XIm2 ); repeating argument, P 2 (. . . (P (f |XIm )| . . . )|XI2 )
gamble XI2 = XO1 belongs domain unconditional prevision P 1 .
idea behind construction marginal extension that,
hierarchical information, way combine joint prevision follow order
information structured. reduces, instance, law total probability
case linear previsions finite spaces. applicable general
situations: combine finite number previsions, two them;
dealing infinite spaces; lower previsions instead linear
ones.
give next notion conditional independence coherent lower previsions
shall use following.
Definition 15 Consider three variables, Xi , Xj , Xk , coherent conditional lower prevision P (Xi , Xj |Xk ). Xi Xj strongly independent conditional Xk Xi Xj
stochastically independent given Xk extreme points M(P (Xi , Xj |xk ))
xk Xk .
previous definition allows joint beliefs created marginal ones, common independence precise probability: consider coherent conditional lower previsions P (Xi |Xk ) P j (Xj |Xk ). build so-called strong product P sp (Xi , Xj |Xk ),
793

fiZaffalon & Miranda

i.e., least-committal lower prevision follows assumption strong
independence, lower envelope following set:
Msp :=

{P (Xi , Xj |Xk ) := Pi (Pj (Xj |Xi , Xk )|Xk ) s.t.xk Xk ,
Pi (Xi |xk ) ext(M(P (Xi |xk ))), P (Xj |xk ) ext(M(P j (Xj |xk )),
xi Xi , Pj (Xj |xi , xk ) := P (Xj |xk )},

Pj (Xj |Xi , Xk ) defined using stochastic independence, linear prevision
P (Xi , Xj |Xk ) obtained applying marginal extension theorem.
Strong independence relatively straightforward generalisation stochastic independence imprecision. also notion immediately given sensitivity
analysis interpretation: since every xk Xk extreme point M(P sp (Xi , Xj |xk ))
satisfies stochastic independence, interpret P sp (Xi , Xj |Xk ) model arising
partial knowledge underlying linear prevision P (Xi , Xj |Xk ) known satisfy
stochastic independence. words, P sp (Xi , Xj |Xk ) could regarded obtained
listing number candidate linear previsions P (Xi , Xj |Xk ), satisfies
stochastic independence, taking lower envelope.
also remarked imprecise case, work sets probabilities previsions, unique extension notion stochastic independence
probability measures (see Campos & Moral, 1995; Couso et al., 2000; Miranda, 2008,
Section 4). notion considered strongest, therefore informative, possibilities considered references above.20
notion alternative strong independence instance epistemic irrelevance
proposed Walley (1991, Section 9.1.1).
Definition 16 Given coherent lower previsions P (Xi |Xj , Xk ) P (Xi |Xk ), say
Xj epistemically irrelevant Xi conditional Xk holds P (Xi |Xj , Xk ) =
P (Xi |Xk ).
Epistemic irrelevance naturally suited behavioural interpretation: Xj irrelevant
Xi given Xk context Xk beliefs subject Xi , expressed
coherent lower prevision P (Xi |Xk ), change getting know value
Xj . sensitivity analysis interpretation epistemic irrelevance possible general joint created marginal information epistemic irrelevance,
related extreme points necessarily satisfy stochastic independence. fact, strong
independence implies epistemic irrelevance converse implication hold.
Moreover, irrelevance asymmetric notion: knowing Xj irrelevant Xi
imply Xi irrelevant Xj . One create symmetric notion epistemic independence (see, e.g., Walley, 1991; Miranda, 2008) requiring Xj irrelevant
Xi Xi irrelevant Xj ; still, strong independence implies epistemic independence
way around hold.
Strong independence expressed means epistemic irrelevance together
requirement. Consider P (Xi |Xk ) P j (Xj |Xk ); want create strong
20. related fact never holds precise previsions convex set
one element satisfy stochastic independence; therefore requiring extreme points
credal set, ones keeping behavioural information, amounts go far possible
direction independence.

794

fiConservative Inference Rule

product using epistemic irrelevance. first write P j (Xj |Xi , Xk ) = P j (Xj |Xk ),
strong independence implies epistemic irrelevance. apply marginal extension
theorem create so-called irrelevant product P ip (Xi , Xj |Xk ) = P (P j (Xj |Xi , Xk )|Xk ),
least committal lower prevision follows marginals assumption epistemic irrelevance. terms sets dominating linear previsions,
marginal extension theorem states P ip (Xi , Xj |Xk ) lower envelope set
Mip :=

{P (Xi , Xj |Xk ) = Pi (Pj (Xj |Xi , Xk )|Xk ) : xi Xi , xk Xk ,
Pi (Xi |xk ) ext(M(P (Xi |xk ))), Pj (Xj |xi , xk ) ext(M(P j (Xj |xi , xk )))}.

Even xk Xk sets ext(M(P j (Xj |xi , xk ))), xi Xi , identical one another
irrelevance condition, building joint prevision P (Xi , Xj |Xk )
forced choose extreme point them. makes
difference Mip Msp , fact write Msp way similar
Mip follows:
Msp :=

{P (Xi , Xj |Xk ) = Pi (Pj (Xj |Xi , Xk )|Xk ) : xi Xi , xk Xk ,
Pi (Xi |xk ) ext(M(P (Xi |xk ))), Pj (Xj |xi , xk ) ext(M(P j (Xj |xi , xk )))
s.t. Pj (Xj |x0i , xk ) = Pj (Xj |x00i , xk ) x0i = x00i }.

words, think strong product obtained procedure
use irrelevant product, additional requirement xk Xk ,
extreme points chosen ext(M(P j (Xj |xi , xk ))), xi Xi , must coincide every time.
shall use observation Section 7.1.3.

7. Derivation CIR
next sections state number assumptions, discuss them, eventually use
derive CIR.
7.1 Modelling Beliefs
aim section represent beliefs vector (Z, Y, W ). intend
model beliefs using coherent lower previsions. refer Sections 2 6 and,
generally, book Walley (1991) concepts results shall need.
way represent beliefs (Z, Y, W ) coherent lower prevision L(Z
W), representing joint beliefs variables. But, precise probability,
often easier build joint models composition simpler conditional
unconditional models. shall therefore start focusing following lower previsions,
illustrate clarity coherence graph Figure 4:
(LP1) coherent lower prevision, denoted P 1 , set XZ,Y -measurable gambles;
(LP2) separately coherent conditional lower prevision P 2 (W |Z, ) set XZ,Y,W measurable gambles, modelling beliefs W given value (z, y) taken
(Z, ).
795

fiZaffalon & Miranda


)

Z

?



j?

?

W

q


q?
z

*
?

W

Figure 4: coherence graph initial conditional lower previsions express beliefs (Z, Y, W ).

(LP3) conditional lower prevision P 3 (W |Z, Y, W ) L(Z W), representing
beliefs W given value (Z, Y, W ) takes.
coherent lower prevision P 1 intended express beliefs domain
interest, modeled variables Z . words, P 1 express
beliefs facts. conditional lower prevision P 2 (W |Z, ) concerned
beliefs unknown incompleteness process. Finally, conditional lower
prevision P 3 (W |Z, Y, W ) express beliefs CAR incompleteness process.
exactly done detailed next sections. moment, going
build joint coherent lower prevision (Z, Y, W ) lower previsions listed
(LP1)(LP3). done marginal extension theorem introduced Section 6.4.
present case, generalised marginal extension theorem states lower
prevision P given
P (f ) = P 1 (P 2 (P 3 (f |Z, Y, W )|Z, ))
gambles f Z W smallest (using point-wise ordering lower
previsions) coherent lower prevision L(Z W) jointly coherent
lower previsions listed (LP1)(LP3). implies, example, P coincides
coherent lower prevision P 1 domain. smallest lower prevision
coherent assessments, P captures behavioural implications present
P 1 , P 2 (W |Z, ) P 3 (W |Z, Y, W ), without making additional assumptions; shall
therefore use model beliefs vector (Z, Y, W ).
P also lower envelope closed convex set M(P ) dominating linear
previsions. useful purposes paper construct set explicitly.
Consider set
(
)
X
M0 := P : P (g) = P1 (hg ), hg (z, y) :=
g(z, y, w)P2 (w|z, y)P3 (w|z, y, w) ,
(12)
w

g L(Z, Y, W), P1 ext(M(P 1 )), P2 (W |z, y) ext(M(P 2 (W |z, y)))
P3 (W |z, y, w) ext(M(P 3 (W |z, y, w))) (z, y, w) Z W. M(P ) =
CH(M0 ), operator CH() stands convex closure. consequence, P
calculated lower envelope class M0 .
796

fiConservative Inference Rule

7.1.1 Domain Beliefs
Consider variables Z , represent facts. Beliefs facts, form
coherent lower prevision P 1 , specific domain consideration; reason,
call domain beliefs. case Asia network, P 1 would correspond simply
joint mass function coded network itself.
shall impose minimal assumption domain beliefs, P 1 must
flexible tool possible order express beliefs wide range domains.
task postponed focusing different kind beliefs, call beliefs
incompleteness process.
7.1.2 Beliefs Incompleteness Process
naturally conditional type: formalise believe modus
operandi overall IP, i.e., procedure turns facts observations.
represent beliefs assumptions satisfied conditional lower
prevision P 2 (W |Z, ), related unknown IP, P 3 (W |Z, Y, W ), related CAR
IP. start unknown IP.
unknown IP introduced formalise idea incompleteness process
nearly ignorant. term nearly emphasise that, despite
deep kind ignorance, something assumed known unknown IP. one
thing, produces incompleteness basis . formally, assume
separately coherent conditional lower prevision P 2 (W |Y )
P 2 (f |z, y) = P 2 (f |y)

(BIP1)

XW ,Y,Z -measurable gambles f (z, y) (Z, Y).
(BIP1) states observing different values Z change beliefs
W , know value takes Y. assumption turns coherence graph
Figure 4 Figure 5. Assumption (BIP1) arises somewhat naturally within
interpretation IPs observational processes, regard unknown IP
process takes input outputs W . Still, important assumption
following developments, therefore discussed detail Section 7.2.

)

Z

?




?

W

q


q?
z

*
?

W

Figure 5: coherence graph Figure 4 application Assumption (BIP1).
797

fiZaffalon & Miranda

second thing assume unknown IP related perfection
multi-valued map :21
(BIP2)
P 2 ((y)c |y) = 0.
words, assumption practically exclude possibility
Y, unknown IP may lead observations outside (y). Note well
defined, important assume (y) nonempty required (IP4). Remember
that, elsewhere, use P refer conjugate upper prevision P ; see
Remark 1 details.
Nothing assumed unknown IP. led introduce
assumptions CAR IP, first two analogous (BIP1) (BIP2),
respectively. first one existence separately coherent conditional lower prevision P 3 (W |Y )
P 3 (f |z, y, w) = P 3 (f |y)
(BIP3)
XW,Y,Z -measurable gambles f (z, y, w) (Z, Y, W). assumption turns
coherence graph Figure 5 Figure 6.

)

Z

?




?

W

q



?

W

Figure 6: coherence graph Figure 5 application Assumption (BIP3).
second assumption perfection multi-valued map :
P 3 ((y)c |y) = 0.

(BIP4)

Again, assumption made possible (IP5).
assume something substantial CAR IP. Indeed, CAR IP
introduced model process produces incompleteness random fashion,
say, way related underlying value takes Y.
model belief follows:
w W, {w} P 3 (w|y) = P 3 (w|y) = w ,
(BIP5)
P
w positive constants satisfy w(y) w = 1, restriction
w (y) justified (BIP4).
assumption usually called coarsening random (or CAR, see Gill et al., 1997).
special case missingness processes, one usually refers CAR missing random
(or MAR, see Little & Rubin, 1987). CAR/MAR probably frequently imposed
21. could argued assumption corresponding one CAR IP, is, (BIP4), follow
definition multi-valued maps. nevertheless, state assumptions explicitly
clarity.

798

fiConservative Inference Rule

assumption IPs literature; embodies idea IP non-selective (or
non-malicious) producing observations. Together (BIP4), makes conditional
prevision P 3 (W |Y ) precise: i.e., beliefs CAR IP determinate (they
conditional expectation respect probability distribution). important
point must require explicitly compatible CAR assumption
(see Appendix details); leads assumption reported
Section 3.3: assume
leads (BIP5) admissible system linear constraints,

(IP9)

is, one solution.
characterised unknown CAR IPs, also determine way interact make overall IP; imposing
assumption strong independence, introduced Section 6.4:
W W strongly independent conditional Y.

(BIP6)

assumption discussed Section 7.2.22
7.1.3 Joint Beliefs
section aim constructing joint coherent lower prevision P mentioned
beginning Section 7.1. that, need first find explicit representations
assessments related unknown CAR IP, basis Assumptions (BIP1)
(BIP6). joint constructed using marginal extension theorem mentioned
beginning section, taking account assumption strong independence (BIP6).
Consider unknown IP. Assumption (BIP1) allows us focus attention
conditional lower prevision P 2 (W |Y ). Thanks Assumption (BIP2), know
Y, P 2 (W |y) gives lower probability one (y). Since know nothing else,
take P 2 (W |y) least-committal lower prevision, i.e., smallest separately
coherent conditional lower prevision assigns probability one (y). lower
prevision also called vacuous relative (y), given P 2 (f |y) = min{f (w, y) :
w (y)} XW ,Y -measurable gamble f .23
22. similarities model developed section traditional hidden
Markov models (see instance Rabiner, 1989). First, focus distinction two
levels information, latent manifest one, discussed Introduction. reason
concerned explicit representation observational process, relates
W variables, case coincides incompleteness process. refer
Figure 6, make things concrete, see W -variables manifest depend
directly related hidden -variables. common representation hidden Markov
models. Second, Assumption (BIP6) could interpreted kind Markovianity property. Yet,
also differences: major one two W -variables related order (such time order),
generally explicitly representing order model. Another
quantify probabilistic information observation conditional related hidden variable
set conditional mass functions rather single one. gives us expressivity key
model unknown IP.
23. follows implication (BIP2) actually equivalence.

799

fiZaffalon & Miranda

Using indicator functions, easily write extreme points M(P 2 (W |y)). Let
: W {0, 1} indicator function (y). write rather I{}
simplify notation. ext(M(P 2 (W |y))) = {I : (y)}. concludes definition
P 2 (W |y) also P 2 (W |z, y), given coincide. Yet, dealing
P 2 (W |z, y), shall use slightly different formulation: ext(M(P 2 (W |z, y)) = {I(z,y) :
(z, y) (y)}. reason extended notation (z, y) allows us know,
addition vertex, value (z, y) focus, and, consequently, focus
specific lower prevision P 2 (W |z, y). Hence, shall see mapping Z
W (z, y), (z, y) belongs (y).
situation somewhat easier CAR IP. Similarly previous case,
know restrict attention conditional lower prevision P 3 (W |Y ),
thanks (BIP3). also know conditional lower prevision P 3 (W |Y )
actually precise (we shall denote P3 (W |Y ) on), P3 (W |y) consists
single mass function determined Assumption (BIP5): i.e., mass function assigns
probability P (w|y) = w I{w} (y) generic element w W.
point define coherent lower prevision P models beliefs
value (Z, Y, W ) assume jointly. rewriting Expression (12) according
previous arguments using addition (BIP6):
M0 := {P : P (g) = P1 (hg ), hg (z, y) =

P

w

w g(z, y, w)I(z,y) (w)I{w} (y)I{w} (y),

P1 ext(M(P 1 )), (z, y) (y) s.t. (z, y) = (z 0 , 0 ) = 0 ,
(z, y, w) Z W},

(13)

introduced new term I{w} (y). term actually redundant,
introduced convenient next developments. see
redundant, note I{w} (y) = 0 implies w
/ (y) hence, since (z, y) (y)
definition, I(z,y) (w) = 0. usage (BIP6), hand, leads
substantial introduction requirement (z, y) = (z 0 , 0 ) = 0 .
needed represent strong independence means epistemic irrelevance, discussed
end Section 6.4. lower envelope P set M0 thus constructed therefore
called strong product assessments P 1 (Z, ), P 2 (W |Y ), P3 (W |Y ).
strong product coherent lower prevision, also coherent
P 1 (Z, ), P 2 (W |Y ), P3 (W |Y ), follows Corollary 1 Appendix B. Importantly,
also case coherent lower prevision P 1 (Z, ) constructed modular way
smaller pieces information (i.e., joint coherent number smaller
pieces information), provided coherence graph representing A1+ ;
situation often case. case, instance, examples
Sections 5.15.3, shown Corollaries 35 Appendix B.
7.1.4 Assumption Domain Beliefs
Recall introduced W way model observations, hence beliefs
consistent possibility observation. extent,
seems necessary least believe {w} produced:
w W P ({w} ) > 0,
800

(DB1)

fiConservative Inference Rule

taking account coherence implies P ({w} ) = P 1 ({w} ). Assumption (DB1)
equivalent existence extreme point P1 M(P 1 ) P1 ({w} ) > 0.
following proposition shows assumption sufficient make beliefs consistent
possibility observation w.
Proposition 1 Assumption (DB1) implies P (w) := P (Z, Y, w) > 0.
Proof. Let P1 extreme point M(P 1 ) satisfying P1 ({w} ) > 0, exists
Equation (DB1), take (z, y) = w {w} . joint P constructed
Equation (13) satisfies P (w) = w P1 ({w} ) > 0. Hence, P (w) > 0. 2
7.2 Assumptions Discussed
discuss (BIP1), (BIP5) (BIP6) detail. assumptions quite
weak relatively easy accept. Assumptions (BIP2) (BIP4) may exceptions
makes sense consider IPs imperfect (or lie),
scope present paper.
start Assumption (BIP5), i.e., CAR. CAR models process
coarsen facts specific purpose. CAR excludes way many common important processes. Consider medical domain, example, focusing diagnostic
application. fact case might describe information patient, gender,
age, lifestyle, also results medical tests. Conclusions would made
possible diseases. IP case (at least part it) often results interaction
doctor patient; indeed, usually systematic bias reporting,
asking for, symptoms present instead symptoms absent;
bias report, ask for, urgent symptoms others (Peot & Shachter, 1998).
Furthermore, doctor typically prescribes subset possible diagnostic tests,
according personal views cost/benefit criteria.24 Overall, process described
non-CAR definition, incompleteness arises following patterns depend
specific facts consideration.
Descriptions one support idea CAR strong means
informal arguments. also formal level, recent research suggested CAR
assumed hold less frequently appears practice (Grunwald &
Halpern, 2003); one also remember way test CAR statistically
(Manski, 2003), always degree arbitrariness assuming it.
taken account order put CAR balanced perspective.
say CAR rejected priori, situations CAR
completely justified. Consider one notable example: case know
missing probability equal one. case related IP clearly satisfies MAR:
probability missingness one irrespective value . broadly speaking, MAR
holds processes produce missingness unintentional way. cases,
24. seems support idea shall hardly get ever rid incompleteness: often, incompleteness
happen mistake, rather, generated deliberately. cases actually represents
patterns knowledge (indeed, one often tell disease patient was, not, suspected
looking medical tests doctor did, not, prescribe). sense,
seems incompleteness doomed deeply rooted many, most, real problems;
such, appears fundamental, indissoluble, component uncertain reasoning.

801

fiZaffalon & Miranda

assuming MAR would lead results far weak. CAR IP modelling
framework presented Section 7.1.2 designed account situations: i.e.,
provide one flexibility stating beliefs incompleteness process without
adopt necessarily worst-case approach (as opposed best case embodied
CAR/MAR), kind unknown IP.
unknown IP designed model ones ignorance incompleteness process.
makes sense adopt conservative approach model IPs practice, two specific
reasons: first, IPs may difficult processes model. special case
observational processes, often result human-to-human interaction,
complex factors; number cases actually regarded result
communication protocol. medical example intended illustrate this.
saying anything new: difficulty modelling IPs pointed already long
ago (Shafer, 1985; Grunwald & Halpern, 2003). IPs difficult objects handle also
second reason: IP models tightly dependent specific situations. Consider
medical example: different doctors typically ask different questions diagnose
disease, even hospital. changing hospital, one find entirely different
procedures diagnose disease, procedures depend local culture,
money available make tests, ones, local time constraints.
words, even one able model IP specific situation, model may
longer appropriate another doctor charge diagnosis, one
tries apply IP model another hospital, perhaps another country. summary,
modelling IP (especially precise way) may present serious practical difficulties,
as, contrast domain beliefs (e.g., medical knowledge), way information
accessed may well depend particular environment system used;
means models IP may easily reusable, may therefore costly.
arguments support considering conservative approach model IP
effectively implemented, reason introducing unknown IP
Section 7.1.2. Recall unknown IP actually nearly unknown,
require (BIP1) holds. hand, observe dropping (BIP1) could
draw vacuous conclusions Z. see this, suppose want predict
probability Z = z given certain observation w. Assume end
section CAR IP, order make things easier. without
assuming (BIP1), could exclude possibility IP produces w
Z 6= z, probability Z = z zero. way, could exclude
possibility IP produces w Z = z, probability Z = z
one. course, intermediate randomised cases would also possible,
probability would vacuous. emphasise, perhaps surprisingly, complete
ignorance IP consistent possibility drawing useful conclusions.
said this, still useful wonder whether (BIP1) reasonable present
setup. easier rewrite (BIP1) somewhat natural form. focus
special case spaces possibilities finite precise beliefs
unknown IP. multiply sides equation P (w|z, y) = P (w|y)
conditional probability z given y, obtaining P (z, w|y) = P (z|y)P (w|y). new
equation states variables Z W independent conditional . words,
original assumption equivalent saying already know fact y, making
802

fiConservative Inference Rule

observation w completely superfluous predicting target variable. appears
nothing else precise characterisation problems incomplete missing
information: problems characterised fact something
missing actually measured, problem missing information disappears.
case, observation w would carry information z via implications
fact y: would say something z also own. means
information useful predict target variable included definition
possible facts (see work De Cooman & Zaffalon, 2004 discussion
assumption called MDI related assumption consideration).
conclude section discussing Assumption (BIP6), namely strong independence
W W conditional . discuss assumption one could principle
consider weaker notions independence replace strong independence.
reason used strong independence kind general framework developed technically quite difficult embed judgments
independence W W conditional . hand, problem strong
independence easily lend full behavioural interpretation unlike notions, epistemic irrelevance independence. acknowledge
behavioural interpretation important way guide mathematical
developments one comes decision making.
Yet, one could argue inference rule, CIR, follows assumptions
weakest possible one (if exclude rule leads vacuous posterior expectations
lead informative conclusion) part related unknown
IP, leads consider replacements missing information
possible latent data. Therefore one could conjecture replacing strong independence
W W conditional weaker notion independence, inference rule
could either stay lead vacuous inferences. Whatever outcome, choice
strong independence would even reasonable.
7.3 Derivation CIR
order formulate basic problem paper sufficiently general way, focus
problem updating beliefs generic function g : Z R, posterior beliefs
conditional W = w. precise case, would done computing
P (g|w) =

P (gIw )
,
P (w)

provided P (w) > 0.
Something similar done imprecise case, case uncommon
P (w) zero. case, obtain infinite number conditional lower
previsions coherent P (W ). already discussed De Cooman
Zaffalon (2004), proposed regular extension effective updating rule.
natural choice also sensitivity analysis interpretation coherent
lower previsions, pointed Section 6.3, therefore also choice
pursue. Using regular extension entails additional rationality assumption
reported Walley (1991, Appendix J3); coherence unconditional lower prevision derived trivial present context, W finite (see Appendix B
803

fiZaffalon & Miranda

details). also recall P (w) > 0, regular extension provides
coherent updated lower prevision.
form CIR rule given Definition 7 Section 4 derived next theorem.
important remark theorem hold, need require space
unknown IP finite, necessary results establish
Appendix B. reason infinite class {w}1 define
theorem may empty w W, making rule inapplicable.
Theorem 1 (Conservative inference rule theorem) Consider gamble g Z
w W. Let {w}1 := {y {w} : P 1 (y, {w} ) > 0}, let us define regular extension
R(g|y, {w} ) =

inf

P P 1 :P (y,{w} )>0

P (g|y, {w} )

{w}1 .
R(g|w) = E 1 (g|w) := min R(g|y, {w} ).
y{w}1

Proof. First all, prevision P constructed using Equation (13),
X
X
P (w) = w
P (y, {w} ) = w
P (y, {w} ),

(14)

y{w} ,(z,y)=w

yY,(z,y)=w

w W, second equality follows Assumption (BIP2), also taking
account mapping depends value . Similarly, also
X
X
P (gIy,{w} ) = w
P (gIy,{w} )
P (gIw ) = w
y{w} ,(z,y)=w

yY,(z,y)=w

w W gamble g Z.
Fix w W, define := {P P 1 : P (w) > 0}, M1 := {P P 1 : P (y, {w} ) >
0 {w} }. gambles g Z, R(g|w) = inf{P (g|w) : P M}
E 1 (g|w) = inf{P (g|y, {w} ) : P M1 }.
start proving E 1 (g|w) R(g|w). this, going prove
{w} P M1 P (y, {w} ) > 0 P 0
P 0 (g|w) P (g|y, {w} ). Consider P , let P1 restriction L(Z Y).
P1 P 1 . Let mapping (z, y) = w (z 0 , 0 ) 6= w 0 6= y.
construct mapping Assumption (IP8) set {w} empty,
0
therefore every 0 {w} w 6= w 0 {w0 } . Let P 0 P
joint prevision constructed P1 using Equation (13). Taking account
Equation (14), see prevision satisfies P 0 (w) = w P 0 (y, {w} ) = w P (y, {w} ) >
0. consequence,
P 0 (g|w) =

w P 0 (gIy,{w} )
P (gIy,{w} )
P 0 (gIw )
=
=
= P (g|y, {w} ),
P 0 (w)
w P 0 (y, {w} )
P (y, {w} )

last equality holds P (y, {w} ) > 0. Hence, deduce E 1 (g|w)
R(g|w).
804

fiConservative Inference Rule

show converse inequality. going prove P
P 0 M1 P (g|w) P 0 (g|y, {w} ). Consider P M.
P
P
w y{w} ,(z,y)=w P (gIy,{w} )
P (gIw )
y{w} ,(z,y)=w P (gIy,{w} )
P
P
=
P (g|w) =
=
.

P (w)
w y{w} ,(z,y)=w P (y, {w} )
y{w} ,(z,y)=w P (y, {w} )
Define := {y {w} s.t. P (y, {w} ) > 0, (z, y) = w}. Since P (w) > 0, follows
Equation (14) set nonempty, equality expressed
P
yI P (gIy,{w} )
P (g|w) = P
.

yI P (y, {w} )
Applying Lemma 3 Appendix deduce existence y1
P
P (gIy1 ,{w} )
y{w} ,(z,y)=w P (gIy,{w} )
P
.


P (y1 , {w} )
y{w} ,(z,y)=w P (y, {w} )
Let P1 restriction P L(Z Y). Consider mapping (z, y) = w
= y1 (z 0 , 0 ) 6= w 0 . construct mapping
Assumption (IP8) set {w} empty, therefore every 0 {w}
0
w 6= w 0 {w0 } . Let P 0 P joint prevision constructed P1
using Equation (13). prevision satisfies P 0 (y1 , {w} ) = P (y1 , {w} ) > 0. Moreover,
P
P 0 (gIy1 ,{w} )
P (gIy1 ,{w} )
y{w} ,(z,y)=w P (gIy,{w} )
0

P
P (g|y1 , {w} ) = 0
= P (g|w),
=


P (y1 , {w} )
P (y1 , {w} )
y{w} ,(z,y)=w P (y, {w} )
second equality follows P 0 = P = P1 L(Z Y). deduce
E 1 (g|w) R(g|w) consequence equal. 2
important point whether lower prevision defined CIR rule
theorem going coherent initial assessments: P 1 (Z, ), P 2 (W |Y ), P3 (W |Y ).
want case want CIR lead self-consistent inference. question
given positive answer Theorem 3 Appendix B.
Moreover, theorem also deduce (in subsequent Corollary 2) coherence also maintained much general conditions: one hand,
examples application CIR rule discussed Sections 5.15.3,
P 1 (Z, ) obtained making strong product number conditional lower previsions P 1 (XO1 |XI1 ), . . . , P (XOm |XIm ). consider set variables interest
{X1 , . . . , Xn } contains {Z, } include W . assume moreover
coherence graph associated P 1 (XO1 |XI1 ), . . . , P (XOm |XIm ) A1+ ,

follows (see Appendix B)
i=1 (Ii Oi ) = i=1 Oi = {1, . . . , n}. instance
situation given Figure 7.
Corollary 2 Appendix B shows coherence also holds general situation.
hand, corollary also proves coherence maintained apply
regular extension finite number conditioning events, thus actually creating
additional finite number coherent lower previsions original assessments:
assessments going jointly coherent. another important result
practice one indeed often conditions number events. theoretical results give
us guarantees CIR rule well-behaved.
805

fiZaffalon & Miranda

R

B

H

w

? - sff

?
?





Z

Figure 7: general model.

8. Conclusions
paper introduced new rule update beliefs incompleteness
called conservative inference rule (CIR). CIR designed explicitly taking
consideration problem modelling process makes observations incomplete, called incompleteness process (or IP). represented
process made two sub-processes, one non-selective another
unknown us. consequence, CIR deals differently data known
coarsened random ones. first case CIR treats using
CAR/MAR assumption, taking completions incomplete data.
CIR regarded generalisation traditional, Bayesian rule update
beliefs recently proposed conservative updating rule (De Cooman & Zaffalon,
2004). CIR generalisation two rules enables one consider mixed situations. make CIR quite flexible rule applications. Moreover,
enables CIR avoid risk overconfident IP, problem
traditional updating, well avoid over-pessimistic may happen
conservative updating rule.
CIR also general rule. used coarsened missing data,
statistics well expert systems, applied predict state
target variable irrespective cardinality space possibilities. indeed
tried illustrate characteristics using number application domains. Moreover,
using examples shown CIR makes quite difference traditional
CAR/MAR-based way dealing incomplete data: examples traditional
updating shown lead conclusions hardly justifiable evidence
hand, and, even worse, way user may realise this.
cases CIR instead naturally cautious clearly communicates user
limits strength possible conclusions, logical consequence strength
(or weakness) available information.
Finally, CIR nice theoretical properties, notably coherent rule.
Loosely speaking, means using CIR possible give rise probabilistic
inconsistencies. seen Theorem 3 Appendix B, assumptions
require coherence CIR fairly general: initial assessments special
kind, characterised using graphical structure called coherence graph
type A1+ , require possibility spaces involved analysis
finite conditioning events spaces positive upper probability.
806

fiConservative Inference Rule

hypotheses guarantee use Walleys notion regular extension update
beliefs assessments thus obtained coherent initial assessments.
like conclude section commenting also bit assumptions
considered construction model open problems
derive work.
One point requirement spaces possibilities finite. could
relaxed taking account conditional prevision defined regular extension
coherent unconditional derived even infinite case. However,
cannot guarantee third point Lemma 2 Appendix B holds
general situations, point necessary proof Theorem 3. related
notion conglomerability discussed much detail Walley (1991). open problem
would extend results general situations addressing question
conglomerability. hand, said, results applicable even
target space Z variable interest, Z, infinite. allowed us, instance,
model parametric inference case parameter space infinite, discussed
Section 5.2. Moreover, assuming upper probability values Z
positive, allows us include case prior beliefs parameter
precise parameter space infinite, coincides traditional setup.
Another important point assumption domains lower previsions:
required instance P 1 (Z, ) defined set XZY -measurable
gambles. Similar considerations made P 2 (W |Y ) P 3 (W |Y ).
requirements met, still apply results extending assessments
using notion natural extension given Walley (1991). easy see
extensions satisfy hypotheses theorems. considerations allow us
cover particular case lower probabilities instead lower previsions.
interesting feature CIR rule derived Theorem 1 allows us
make passage updated information Z knowing observation W
information Z knowing value takes. think key
using vacuous linear previsions express information provides
W , i.e., components incompleteness process either unknown
random. open problem determine whether possibilities allowing
us analogous property. Similarly, would interesting study
assumptions incompleteness process weakened.
conclude, CIR new rule update beliefs incompleteness, general,
based solid theoretical foundations. gives us first time opportunity
avoid optimistic pessimistic incompleteness process, thus
creating basis draw credible strong enough conclusions number applications.
say regard CIR last word subject. contrary,
seems us research IPs far scratched surface uncertain reasoning
incompleteness. particular, probably number applications
rules stronger CIR (yet based tenable assumption traditional updating)
needed. Developing rules appears important research avenue.
investigation probably directed primarily creating new assumptions
IPs make resulting rule stronger still general enough. way could
using starting point framework developed here. particular, one could
807

fiZaffalon & Miranda

take Assumptions (IP1)(IP9), (BIP3)(BIP6) (DB1) are, strengthening
related unknown IP, is, (BIP1) (BIP2). machinery used
derive CIR could used derive new rule follows stronger
assumptions. new assumptions impose whether lead rule
useful domain-specific matter future investigation.

Acknowledgments
Preliminary work topic paper appeared proceedings ISIPTA 05:
fourth International Symposium Imprecise Probabilities Applications
(Zaffalon, 2005). like thank reviewers paper comments
helped improve clarity readability. work partially supported Swiss
NSF grants n. 200020-116674/1 200020-121785/1, projects TIN2008-06796C04-01, MTM2007-61193.

Appendix A. CAR Assumption (IP9)
important realise multi-valued maps consistent
CAR assumption. example: take := {1, 2, 3, 4}, W := {a, b, c}, define
multi-valued map (1) := {a, b}, (2) := {b, c}, (3) := {a, c}, (4) := {a, b, c}.
(BIP5), P3 (a|1) = P3 (a|3) = P3 (a|4) = , P3 (b|1) = P3 (b|2) =
P3 (b|4) = b , P3 (c|2) = P3 (c|3) = P3 (c|4) = c . Requiring addition nonnegativity probabilities conditional mass functions normalised, leads
following system linear constraints:
+ b = 1
b + c = 1
+ c = 1
+ b + c = 1
, b , c 0.
system solution, adding first three equations deduce +b +c =
1.5, incompatible fourth equation. Therefore CAR process
consistent defined above.
example shows order define properly, need add (IP9)
assumption (IP5) (IP7): system linear constraints originated
(BIP5) solution, namely, admissible. Checking admissibility
easy finite hence number constraints system; allows
one use standard techniques linear programming solve problem.
example also points another question, concerns relationship Gill
et al.s well-known CAR everything theorem (Gill et al., 1997, Section 2). Loosely
speaking, theorem states, precise probability context, always possible
define CAR process; example seems contradict theorem.
order show actually contradiction, need represent example
setup Gill et al.s paper, focused random sets. means
808

fiConservative Inference Rule

regard W variable takes values powerset Y, hence CAR
IP intended function maps elements subsets itself.
make transition, identify element w W corresponding
set {w} ; example, possible values W , interpreted random set,
{1, 3, 4}, {1, 2, 4}, {2, 3, 4}. intuition element 1
coarsened CAR IP set {1, 3, 4} {1, 2, 4}; element 2 {1, 2, 4}
{2, 3, 4}; element 3 {1, 3, 4} {2, 3, 4}; element 4 {1, 3, 4}, {1, 2, 4}
{2, 3, 4}. correspondence really consistent, also need make sure
zero probability assigned elements power set correspond
sets {w} , w W; necessary (BIP4) Gill et al.s
setup explicitly use notion multi-valued mapping. background
take closer look relationship example Gill et al.s theorem.
theorem states, language, matter W distributed,
one find unconditional mass function CAR process lead W
distributed way. select unconditional mass function random
set W respects constraint assigning zero probabilities above, actually
implementing multi-valued map example, know CAR
process consistent it.
key solve paradox Gill et al. allow CAR-IP probabilities P3 (Y 0 |y)
non-zero set 0 includes y, sets determined
multi-valued map, do. freedom makes possible always find CAR process.
Consider example above, assume W distributed uniformly
three sets {1, 3, 4}, {1, 2, 4}, {2, 3, 4}: i.e., assigned probability equal 13 .
make choice consistent CAR process choosing {1,3,4} := {1,2,4} :=
{2,3,4} := 13 (i.e., := b := c := 31 ) if, addition, set {1} := {2} := {3} := 13 ,
i.e., allow CAR process able potentially output three sets.
since know three sets cannot really returned, obliged also
set P (1) := P (2) := P (3) := 0, thus solving problem, although arguably somewhat
artificial way.
Indeed, since forced set zero probability elements Y, one
might wonder elements included set Y. Precisely observation
already used source criticism Gill et al.s CAR everything theorem
(Grunwald & Halpern, 2003, Section 4.4), leading authors latter paper say
cases like actually CAR process. formulation based
multi-valued map simply confirms statement alternative way.

Appendix B. Coherence CIR
appendix, going prove assessments construction
conservative inference rule satisfy notion coherence moreover also
coherent probabilistic assessment deduce using CIR.
so, want cover also case unconditional prevision P 1 (Z, ) constructed
number conditional unconditional previsions, typical
case practice. shall use notations established Section 2 Section 6.
809

fiZaffalon & Miranda

consider variables X1 , . . . , Xn taking values respective sets X1 , . . . , Xn , take
collection conditional previsions P 1 (XO1 |XI1 ), . . . , P (XOm |XIm ).
start lemma shows A1 graphs naturally entail notion order
corresponding lower previsions: particular, possible permute indexes
lower previsions way admissible paths two dummy
nodes index origin precedes destination.25
Lemma 1 coherence graph {P 1 (XO1 |XI1 ), . . . , P (XOm |XIm )} A1,
may assume without loss generality k = 1, . . . , m, Ok (k1
i=1 Ii ) = .
Proof. start proving Ok (m
i=1 Ii ) = k. Assume ex-absurdo
hold. k {1, . . . , m} f (k) 6= k Ok (k) 6= .
Define z0 := 1, zk := f (zk1 ) k 1. must {zk } {z0 , . . . , zk1 } = k 1,
establish cycle coherence graph, contradicting thus A1. Hence,
|{z0 , z1 , . . . , zk1 }| = k k 1, means zm exist, or, equivalently,
Ozm1 Ii = = 1, . . . , m. Hence, k Ok (m
i=1 Ii ) = .
assume without loss generality k = m.
prove k 6= Ok (m1
i=1 Ii ) = . Assume ex-absurdo
hold. k {1, . . . , 1} g(k) 6= k,
Ok Ig(k) 6= . Define z0 := 1, zk := g(zk1 ) k 1. must {zk } {z0 , . . . , zk1 } =
k 1, establish cycle coherence graph, contradicting thus
A1. Hence, |{z0 , z1 , . . . , zk1 }| = k k 1, means zm1
exist, or, equivalently, Ozm2 Ii = = 1, . . . , 1. Hence, k 6=
Ok (m1
i=1 Ii ) = . assume without loss generality k = 1.
similar reasoning allows us deduce existence order {1, . . . , m}
Ok (jk Ij ) = k = 1, . . . , m. Finally assume without loss generality
order coincides natural order. 2
restrict attention particular case A1+ graphs. Lemma 1,
collection P 1 (XO1 |XI1 ), . . . , P (XOm |XIm ) A1+ -representable, assume without
loss generality I1 = . Let A1 := , Aj := j1
i=1 (Ii Oi ) j = 2, . . . , + 1, let
P 0j (XOj |XAj Ij ) given set Hj XAj+1 -measurable gambles j = 1, . . . ,
P 0j (f |z) := P j (f (z, )|Ij (z))
z XAj Ij f Hj . Since P j (XOj |XIj ) separately coherent j =
1, . . . , m, P 0j (XOj |XAj Ij ). Moreover, thanks Lemma 1 {O1 , . . . , Om }
forms partition {1, . . . , n} focus A1+ graphs, sets indexes
conditional variables P 01 (XO1 ), . . . , P 0m (XOm |XAm Im ) form increasing sequence
hence satisfy hypotheses generalised marginal extension theorem.
consequence, P 01 (XO1 ), . . . , P 0m (XOm |XAm Im ) also coherent.
similar reasoning shows take j = 1, . . . , conditional linear prevision
0 (X
Pj0 (XOj |XAj Ij ) dominates P 0j (XOj |XAj Ij ), P10 (XO1 ), . . . , Pm
Om |XAm Im )
jointly coherent. Moreover, since {O1 , . . . , Om } partition {1, . . . , n}, Theorem 3
25. order notion similar graph-theoretic notion topological ordering, applied
dummy nodes.

810

fiConservative Inference Rule

Miranda De Cooman (2007) implies prevision P X n coherent
0 (X
assessments P10 (XO1 ), . . . , Pm
Om |XAm Im )
0
P (f ) = P10 (P20 (. . . (Pm
(f |XAm Im )| . . . )|XA2 I2 )).

(15)

0 (X
words, P10 (XO1 ), . . . , Pm
Om |XAm Im ) give rise unique joint lower prevision.

Definition 17 Assume P (XOj |XAj Ij ) dominates P 0j (XOj |XAj Ij )
j = 1, . . . ,
inf P (XOj |XAj Ij ) = P 0j (XOj |XAj Ij ).


coherent lower prevision P defined P := inf P , P coherent prevision
determined P (XO1 ), . . . , P (XOm |XAm Im ) Equation (15), called lower envelope
model.
Intuitively, lower envelope model joint lower prevision built number
conditional unconditional assessments. interest lower envelope models arises
common practice build joint models smaller conditional
unconditional ones, use joint model draw conclusions. Lower
envelope models abstract procedure general case coherent lower previsions.
particular cases lower envelope models, consider following:
1. j = 1, . . . , consider P (XOj |XAj Ij ) M(P 0j (XOj |XAj Ij )),
P marginal extension P 01 (XO1 ), . . . , P 0m (XOm |XAm Im ).
2. j = 1, . . . , take P (XOj |XAj Ij ) set extreme points
M(P 0j (XOj |XAj Ij )), additional requirement P (XOj |z) = P (XOj |z 0 )
Ij (z) = Ij (z 0 ), lower envelope model P called strong product
P 1 (XO1 ), . . . , P (XOm |XIm ).
paper, make inferences using strong product. results Miranda De Cooman (2007), let Pj (XOj |XIj ) extreme point M(P j (XOj |XIj ))
j = 1, . . . , build linear prevision P manner described above,
P, P1 (XO1 |XA1 I1 ), . . . , Pm (XOm |XAm Im ) coherent. Moreover, deduce following:
Theorem 2 Let P 1 (XO1 ), . . . , P (XOm |XIm ) A1+ -representable collection, let
P lower envelope model associated it. P , P 1 (XO1 ), . . . , P (XOm |XIm )
coherent.
Proof. consequence marginal extension theorem Miranda De Cooman
(2007) previsions P , P (XO1 |XA1 I1 ), . . . , P (XOm |XAm Im ) coherent. Applying Theorem 8.1.6 Walley (1991), deduce lower envelopes
families, lower previsions P , P 01 (XO1 |XA1 I1 ), . . . , P 0m (XOm |XAm Im ),
also coherent. result follows applying gamble f Kj belongs Hj ,
P 0j (f |XAj Ij )(x) = P 0j (f (Aj Ij (x), )|Aj Ij (x)) = P j (f (Ij (x), )|Ij (x))
x X n. 2
following useful corollary theorem:
811

fiZaffalon & Miranda

Corollary 1 Let P strong product P 1 (Z, ), P 2 (W |Y ), P3 (W |Y ), constructed
Section 7.1.3. P coherent also jointly coherent original assessments
P 1 (Z, ), P 2 (W |Y ), P3 (W |Y ). holds also P 1 (Z, ) constructed joint
coherent number smaller pieces information, provided coherence graph
representing A1+ .
Proof. P coherent lower prevision follows lower envelope
set linear previsions. coherence P 1 (Z, ), P 2 (W |Y ), P3 (W |Y ) follows
Theorem 2, coherence graph P 1 (Z, ), P 2 (W |Y ), P3 (W |Y ) A1+ ,
evident Figure 6. general situation P 1 (Z, ) constructed
modular way result follows similarly coherence graph representing
assessments together P 2 (W |Y ), P3 (W |Y ) also A1+ . 2
prove next use regular extension derive new conditional previsions
P m+1 (XOm+1 |XIm+1 ), . . . , P m+k (XOm+k |XIm+k ) strong product P , conditional previsions coherent initial assessments. this, need establish
first following:
Lemma 2 Let P , P (XO |XI ) coherent unconditional conditional previsions, XI
finite. Let R(XO |XI ) defined P using regular extension z XI P (z) > 0,
equal P (XO |z) P (z) = 0. Then:
1. P , R(XO |XI ) coherent.
2. R(XO |XI ) P (XO |XI ).
3. P P , P (XO |XI ) coherent P dominates
P (XO |XI ).
Proof. Since XI finite set, apply Theorem 6.5.4 book Walley
(1991) deduce coherence P , R(XO |XI ) equivalent P (Iz (f R(f |z))) = 0
z XI .26 P (z) = 0, trivial. P (z) > 0, follows Walley (1991,
Appendix J3).
second statement, consider z XI P (z) > 0, f KOI . Assume
ex-absurdo R(f |z) < P (f |z). follows definition regular extension
P P P (z) > 0 P (f |z) < P (f |z). Since P (z) > 0,
follows generalised Bayes rule P (f |z) unique value satisfying 0 =
P (Iz (f P (f |z))). consequence, given P (f |z) > P (f |z), Iz (f P (f |z))
Iz (f P (f |z)), whence
0 = P (Iz (f P (f |z))) P (Iz (f P (f |z))) P (Iz (f P (f |z)) = 0,
using since P , P (XO |XI ) coherent satisfy generalised Bayes rule.
implies P (Iz (f P (f |z))) = P (Iz (f P (f |z))) = 0, two different
values P (Iz (f )) = 0. contradiction.
26. called generalised Bayes rule. P (x) > 0, unique value P (G(f |x)) =
P (Ix (f P (f |x))) = 0 holds.

812

fiConservative Inference Rule

finally establish third statement. Consider P P , z XI . P (z) > 0,
f KOI P (f |z) uniquely determined generalised Bayes rule
dominates regular extension R(f |z). Hence, P (f |z) R(f |z) P (f |z),
last inequality follows second statement. Finally, P (z) = 0, taking element
P (XO |z) M(P (XO |z)) P (Iz (f P (f |z))) = 0 f KOI .
completes proof. 2
establish next result, need introduce consistency notion conditional lower previsions less restrictive coherence:
Definition 18 Let P 1 (XO1 |XI1 ), . . . , P (XOm |XIm ) separately coherent conditional previsions. weakly coherent fj Kj , j = 1, . . . , m, j0 {1, . . . , m},
f0 Kj0 , z0 XIj0 ,



X
sup
Gj (fj |XIj ) Gj0 (f0 |z0 ) (x) 0.

xX n

(16)

j=1

intuition behind notion subject raise conditional
lower prevision P j0 (f0 |z0 ), represents supremum acceptable buying price f0
contingent z0 , positive , using desirable gambles G1 (f1 |XI1 ),. . . ,Gm (fm |XIm ).
difference notion (strong) coherence supremum sum Equation (16) required non-negative whole space X n , necessarily
least one summands non-zero. implies condition holds trivially
gambles f0 , f1 , . . . , fm elements w X n belong
set {I1
(z0 )}
j=1 Sj (fj ).
j0
Miranda Zaffalon (2009, Theorem 1), P 1 (XO1 |XI1 ), . . . , P (XOm |XIm )
weakly coherent joint lower prevision P (X1 , . . . , Xn ) pairwise
coherent conditional lower prevision P j (XOj |XIj ) collection. Similar results
hold case focus collections made linear previsions, difference
joint whose existence equivalent weak coherence linear, too. However,
behavioural interpretation, number weakly coherent conditional lower previsions
still present forms inconsistency; see Walley (1991, Example 7.3.5) example
Walley (1991, Chapter 7), Walley, Pelessoni, Vicig (2004), Miranda (2009)
discussion.
Theorem 3 Let P 1 (XO1 |XI1 ), . . . , P (XOm |XIm ) separately coherent conditional lower
previsions whose associated coherence graph A1+ . Let P strong product. Consider
disjoint Om+j , Im+j j = 1, . . . , k. Assume XIm+j finite j = 1, . . . , k
P (z) > 0 z XIm+j , define P m+j (XOm+j |XIm+j ) using regular extension
j = 1, . . . , k.
P , P 1 (XO1 |XI1 ), . . . , P m+k (XOm+k |XIm+k ) coherent.
Proof. Let denote set linear previsions constructed combining extreme
points M(P j (XOj |XIj )), j = 1, . . . , m, manner described Equation (15).
strong product P lower envelope M.
813

fiZaffalon & Miranda

Since XIm+j finite j = 1, . . . , k, follows [Appendix J3] book Walley
(1991) P P m+j (XOm+j |XIm+j ) coherent. Applying Theorem 1 Miranda
Zaffalon (2009), deduce previsions P , P 1 (XO1 |XI1 ), . . . , P m+k (XOm+k |XIm+k )
weakly coherent. Hence, Theorem 7.1.5 Walley (1991) implies suffices
show P 1 (XO1 |XI1 ), . . . , P m+k (XOm+k |XIm+k ) coherent. Consider fi Ki =
1, . . . , + k, j0 {1, . . . , + k}, z0 XIj0 , f0 Kj0 , let us prove
sup

"m+k
X

B

#
[fi P (XOi |XIi )] Iz0 (f0 P j0 (f0 |z0 )) () 0

(17)

i=1

B {I1
(z0 )} m+k
i=1 Si (fi ).
j0
Assume first j0 {m + 1, . . . , + k}. Equation (17) hold,
> 0
#
"m+k
X
sup
[fi P (XOi |XIi )] Iz0 (f0 P j0 (f0 |z0 )) () = < 0.
I1 (z0 )

i=1

j0

Since P (z0 ) > 0 assumption, follows definition regular extension
P P (z0 ) > 0 Pj0 (f0 |z0 ) P j0 (f0 |z0 ) < 2 .
definition elements M, Pi (XOi |XIi ) M(P (XOi |XIi )), = 1, . . . , m,
P coherent Pi (XOi |XIi ) = 1, . . . , m. hand, applying
Lemma 2, j = 1, . . . , k, conditional prevision Pm+j (XOm+j |XIm+j )
dominates P m+j (XOm+j |XIm+j ) coherent P . Note Pj0 (XOj0 |z0 ) uniquely
determined P Bayess rule P (z0 ) > 0.
Using conditional previsions, deduce X n ,
"m+k
#
X
[fi Pi (XOi |XIi )] Iz0 (f0 Pj0 (f0 |z0 )) ()
i=1
"m+k
X



i=1

whence

"m+k
X

#


[fi P (XOi |XIi )] Iz0 (f0 P j0 (f0 |z0 )) () + ,
2
#

[fi Pi (XOi |XIi )] Iz0 (f0 Pj0 (f0 |z0 )) ()

i=1


2

I1
(z0 ).
j0

P

Let us denote g := m+k
i=1 [fi Pi (XOi |XIi )] Iz0 (f0 Pj0 (f0 |z0 )).
coherence P, Pj (XOj |XIj ) j = 1, . . . , + k implies P (g) = 0. also
P (g) P (gIz0 ) 2 P (z0 ) < 0, first inequality holds g 0 since
assuming Equation (17) hold. contradiction.
Assume next j0 {1, . . . , m}, let us prove existence linear prevision
Q conditional previsions Pj (XOj |XIj ) M(P j (XOj |XIj )) coherent Q
j = 1, . . . , + k Pj0 (f0 |z0 ) = P j0 (f0 |z0 ) Q(B) > 0 B {I1
(z0 )}
j
0

m+k
i=1 Si (fi ).
814

fiConservative Inference Rule

Assume first P (z0 ) > 0. P P (z0 ) > 0.
prevision determined (and therefore coherent with) conditional previsions
P1 (XO1 |XI1 ), . . . , Pm (XOm |XIm ), Pi (XOi |XIi ) belongs M(P (XOi |XIi ))
= 1, . . . , m.
Let Pj00 (XOj0 |XIj0 ) extreme point M(P j0 (XOj0 |XIj0 )) Pj00 (f0 |z0 ) =
P (f0 |z0 ), let Q element determined conditional previsions
P1 (XO1 |XI1 ), . . . , Pj00 (XOj0 |XIj0 ), . . . , Pm (XOm |XIm ). Since coherence graph
P 1 (XO1 |XI1 ), . . . , P (XOm |XIm ) A1, Lemma 1 implies assume without loss generality k = 1, . . . , m, Ok (k1
i=1 Ii ) = . Since moreover {O1 , . . . , Om } forms partition {1, . . . , n}, deduce Ii i1
j=1 Oj
0 1
Oj , whence value Q XIj0 -measurable
= 1, . . . , n. particular, Ij0 jj=1
gambles uniquely determined P1 (XO1 |XI1 ), . . . , Pj0 1 (XOj0 1 |XIj0 1 ). consequence, Q(z0 ) = P (z0 ) > 0.

Consider conditional previsions Pm+j (XOm+j |XIm+j ) pairwise coherent
Q dominate P m+j (XOm+j |XIm+j ) j = 1, . . . , k. previsions
Lemma 2. previsions
Q, P1 (XO1 |XI1 ), . . . , Pj00 (XOj0 |XIj0 ), . . . , Pm (XOm |XIm ),
Pm+1 (XOm+1 |XIm+1 ), . . . , Pm+k (XOm+k |XIm+k )
satisfy conditions stated above, B = I1
(z0 ).
j
0

P (z0 ) = 0, two possibilities: either fm+1 = = fm+k = 0,
Equation (17) holds P 1 (XO1 |XI1 ), . . . , P (XOm |XIm ) coherent;
j1 {1, . . . , k} fm+j1 6= 0. case, consider set B
Sm+j1 (fm+j1 ). set form B = I1
(z1 ) z1 XIm+j1 .
m+j
1

P (z1 ) > 0 assumption, Q Q(z1 ) > 0.
definition M, Pi (XOi |XIi ) M(P (XOi |XIi )), = 1, . . . , m,
Q coherent Pi (XOi |XIi ) = 1, . . . , m. Consider prevision Pj00 (XOj0 |z0 )
M(P j0 (XOj0 |z0 )) Pj00 (f0 |z0 ) = P j0 (f0 |z0 ), define conditional
prevision Pj00 (XOj0 |XIj0 )
Pj00 (f |z)

(
Pj00 (f |z0 ) z = z0
=
Pj0 (f |z0 ) otherwise.
0

Since Q(z0 ) = P (z0 ) = 0, given f Kj0 Q(Pj0 (f |XIj0 )) = Q(Pj0 (f |XIj0 )) =
Q(f ), last equality follows coherence Q Pj0 (XOj0 |XIj0 ).
Hence, Q Pj00 (XOj0 |XIj0 ) coherent. hand, applying Lemma 2,
j = 1, . . . , k, Pm+j (XOm+j |XIm+j ) P m+j (XOm+j |XIm+j )
coherent Q. deduce
Q, P1 (XO1 |XI1 ), . . . , Pj00 (XOj0 |XIj0 ), . . . , Pm+k (XOm+k |XIm+k )
satisfy stated conditions, B = I1
(z1 ).
m+j
1

815

fiZaffalon & Miranda

take previsions, deduce X n ,
#
"m+k
X
[fi Pi (XOi |XIi )] Iz0 (f0 Pj0 (f0 |z0 )) ()
i=1
"m+k
X



#
[fi P (XOi |XIi )] Iz0 (f0 P j0 (f0 |z0 )) ().

i=1

Pm+k
Let us denote g :=
i=1 [fi Pi (XOi |XIi )] Iz0 (f0 Pj0 (f0 |z0 )). coherence Q
Pj (XOj |XIj ) j = 1, . . . , + k implies Q(g) = 0. Consider set B
{I1
(z0 )} m+k
i=1 Si (fi ) Q(B) > 0. Equation (17) hold,
j0
must > 0 s.t. supB g() = < 0. Since also follows g() 0,
deduce Q(g) Q(gIB ) Q(B) < 0. contradiction.
conclude Equation (17) holds consequence previsions
P , P 1 (XO1 |XI1 ),. . . ,P m+k (XOm+k |XIm+k ) coherent. 2
Next use results apply CIR rule general frameworks.
necessary examples application CIR rule discussed Sections 5.1
5.3.
Corollary 2 Let {X1 , . . . , Xn } set variables contains {Z, }
include W . Assume coherence graph associated P 1 (XO1 |XI1 ),. . . , P (XOm |XIm )
A1+ . Let P strong product
P 1 (XO1 |XI1 ), . . . , P (XOm |XIm ), P (W |Y ), P (W |Y ).
Assume XIm+j finite j = 1, . . . , k P (z) > 0 z XIm+j , define
P m+j (XOm+j |XIm+j ) using regular extension j = 1, . . . , k. Then:
(i) P , P 1 (XO1 |XI1 ), . . . , P (XOm+k |XIm+k ), P (W |Y ), P (W |Y ) coherent.
(ii) P (Z|W ) one previsions derive strong product P using regular
extension, P (Z|W ) satisfies Equation (CIR).
Proof. First all, P 1 (XO1 |XI1 ), . . . , P (XOm |XIm ), P (W |Y ), P (W |Y ) satisfy hypotheses Theorem 2: associated coherence graph A1+ {W, XO1 , . . . , XOm }
set variables interest. Hence, strong product P coherent
assessments. moreover use regular extension build updated models
P m+j (XOm+j |XIm+j ) using regular extension j = 1, . . . , k, follows Theorem 3
conditional lower previsions P m+1 (XOm+j |XIm+1 ), . . . , P m+k (XOm+k |XIm+k )
coherent P , P 1 (XO1 |XI1 ), . . . , P (XOm |XIm ), P (W |Y ), P (W |Y ).
Secondly, strong product P coincides one obtain using unconditional lower prevision P (XO1 , . . . , XOm ) obtain making strong product P 1 (XO1 |XI1 ), . . . , P (XOm |XIm ) one hand conditional assessments
P (W |Y ), P (W |Y ) hand: results Miranda De Cooman (2007),
extreme points set M(P (XO1 , . . . , XOm )) obtained applying marginal
extension extreme points M(P 1 (XO1 |XI1 )),. . . ,M(P (XOm |XIm )). Hence,
updated prevision P (Z|W ) obtain regular extension also satisfies Equation (CIR)
case. 2
816

fiConservative Inference Rule

Corollary 3 Updating credal network finite number times CIR leads coherent
inference.
Proof. enough observe probabilistic assessments used build credal
networks lead A1+ coherence graph, detailed Theorem 8 Miranda Zaffalon
(2009), graph, supplemented parts P (W |Y ) P (W |Y ),
remains A1+ graph. result follows Corollary 2. 2
Corollary 4 parametric inference finite number times CIR according
Rules (4) (5) leads coherent inference.
Proof. coherence Rule (4) ensured Corollary 2 since coherence graph
overall assessments used A1+ , shown Figure 8. holds Rule (5)

?


/

R





W

W


?


?

Figure 8: coherence graph parametric statistical inference.
observe also case related coherence graph A1+ , illustrated
Figure 9. 2

?



/

Y1


?

W1

js

R

Y1

...

W1

...


?

/

YN

R

WN

WN


?

YN


?

Figure 9: coherence graph parametric statistical inference IID+ID case.
Corollary 5 Computing finite number posterior predictive lower previsions CIR
classification according Rule (7) leads coherent inference.
Proof. coherence Rule (7) ensured Corollary 2 since coherence graph
overall assessments used A1+ , shown Figure 10. 2
817

fiZaffalon & Miranda


?


s9
/

Y1


?

W1

zs

?


R

Y1

...

W1

...


?

/

/

R

YN

R

YN

YN +1

WN

WN

WN +1 WN +1


?


?


?

YN +1

q

Z


?

Figure 10: coherence graph pattern classification IID+ID case.
Lemma 3 Consider bj 0, cj > 0 j = 1, . . . , n. j1
!
Pn
bj
j=1 bj
Pn
1.
(18)
cj1
j=1 cj
PProof. Assume ex-absurdo
cjP n
k=1 bk
< bj , making
n
k=1 ck



Equation (18) hold. j = 1, . . . , m,
sum j sides inequality obtain

P
n
n
n
X
X
X
cj nk=1 bk
Pn
=
bk <
bj ,
k=1 ck
j=1

k=1

j=1

contradiction. 2

References
Antonucci, A., & Zaffalon, M. (2006). Equivalence Bayesian credal nets
updating problem. Lawry, J., Miranda, E., Bugarin, A., Li, S., Gil, M. A.,
Grzegorzewski, P., & Hryniewicz, O. (Eds.), Proceedings third international
conference Soft Methods Probability Statistics, pp. 223230, Netherlands.
Springer.
Antonucci, A., & Zaffalon, M. (2007). Fast algorithms robust classification Bayesian
nets. International Journal Approximate Reasoning, 44 (3), 200223.
Antonucci, A., & Zaffalon, M. (2008). Decision-theoretic specification credal networks:
unified language uncertain modeling sets Bayesian networks. International
Journal Approximate Reasoning, 49 (2), 345361.
Antonucci, A., Zaffalon, M., Sun, Y., & de Campos, C. P. (2008). Generalized loopy 2U:
new algorithm approximate inference credal networks. Jaeger, M., & Nielsen,
T. D. (Eds.), Proceedings fourth European Workshop Probabilistic Graphical
Models, pp. 1724.
Bernard, J.-M. (1996). Bayesian interpretation frequentist procedures Bernoulli
process. American Statistician, 50 (1), 713.
818

fiConservative Inference Rule

Bhaskara Rao, K. P. S., & Bhaskara Rao, M. (1983). Theory Charges. Academic Press,
London.
Campos, L., & Moral, S. (1995). Independence concepts convex sets probabilities.
Besnard, P., & Hanks, S. (Eds.), UAI-95, pp. 108115, San Mateo. Morgan Kaufmann.
Corani, G., & Zaffalon, M. (2008). Learning reliable classifiers small incomplete
data sets: naive credal classifier 2. Journal Machine Learning Research, 9,
581621.
Couso, I., Moral, S., & Walley, P. (2000). survey concepts independence imprecise
probability. Risk, Decision Policy, 5, 165181.
Cozman, F. G. (2000). Credal networks. Artificial Intelligence, 120 (2), 199233.
Cozman, F. G. (2005). Graphical models imprecise probabilities. International Journal
Approximate Reasoning, 39 (23), 167184.
De Campos, C. P., & Cozman, F. G. (2005). inferential complexity bayesian
credal networks. IJCAI-05, pp. 13131318. IJCAI.
De Campos, L. M., Lamata, M. T., & Moral, S. (1990). concept conditional fuzzy
measures. International Journal Intelligent Systems, 5, 237246.
De Cooman, G., & Zaffalon, M. (2004). Updating beliefs incomplete observations.
Artificial Intelligence, 159 (12), 75125.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood incomplete
data via EM algorithm. Journal Royal Statistical Society, 39 (1), 138.
Duda, R. O., & Hart, P. E. (1973). Pattern Classification Scene Analysis. Wiley, New
York.
Fagin, R., & Halpern, J. Y. (1991). new approach updating beliefs. Bonissone,
P. P., Henrion, M., Kanal, L. N., & Lemmer, J. F. (Eds.), Uncertainty Artificial
Intelligence, Vol. 6, pp. 347374. North-Holland, Amsterdam.
Gill, R., Van der Laan, M., & Robins, J. (1997). Coarsening random: characterisations,
conjectures counter-examples. Lin, D.-Y. (Ed.), Proceedings first Seattle
Conference Biostatistics, pp. 255294. Springer.
Grunwald, P., & Halpern, J. (2003). Updating probabilities. Journal Artificial Intelligence
Research, 19, 243278.
Jaeger, M. (2008). Ignorability statistical probabilistic inference. Journal Artificial
Intelligence Research, 24, 889917.
Jaffray, J.-Y. (1992). Bayesian updating belief functions. IEEE Transactions Systems, Man Cybernetics, 22, 11441152.
Levi, I. (1980). Enterprise Knowledge. MIT Press, London.
Little, R. J. A., & Rubin, D. B. (1987). Statistical Analysis Missing Data. Wiley, New
York.
Manski, C. F. (2003). Partial Identification Probability Distributions. Springer-Verlag,
New York.
819

fiZaffalon & Miranda

Miranda, E. (2008). survey theory coherent lower previsions. International
Journal Approximate Reasoning, 48 (2), 628658.
Miranda, E. (2009). Updating coherent previsions finite spaces. Fuzzy Sets Systems,
160 (9), 12861307.
Miranda, E., & De Cooman, G. (2005). Coherence independence non-linear spaces.
Tech. rep., Universidad Rey Juan Carlos, Spain. Downloadable address
http://bellman.ciencias.uniovi.es/emiranda/.
Miranda, E., & De Cooman, G. (2007). Marginal extension theory coherent lower
previsions. International Journal Approximate Reasoning, 46 (1), 188225.
Miranda, E., & Zaffalon, M. (2009). Coherence graphs. Artificial Intelligence, 137 (1),
104144.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann, San Mateo.
Peot, M. A., & Shachter, R. D. (1998). Learning dont observe. Cooper,
G. F., & Moral, S. (Eds.), Uncertainty Artificial Intelligence (Proceedings
Fourteenth Conference), pp. 439446. Morgan Kaufmann Publishers, San Francisco,
CA.
Perks, W. (1947). observations inverse probability including new indifference
rule. J. Inst. Actuar., 73, 285312.
Rabiner, L. (1989). tutorial hidden markov models selected applications speech
recognition. Proceedings IEEE, 77 (2), 257286.
Ramoni, M., & Sebastiani, P. (2001). Robust learning missing data. Machine Learning,
45 (2), 147170.
Shafer, G. (1985). Conditional probability. International Statistical Review, 53, 261277.
Strassen, V. (1964). Mefehler und Information. Zeitschrift fur Wahrscheinlichkeitstheorie
und Verwandte Gebiete, 2, 273305.
Troffaes, M. C. M. (2007). Decision making uncertainty using imprecise probabilities:
introductory overview. International Journal Approximate Reasoning, 45 (1),
1729.
Walley, P. (1981). Coherent lower (and upper) probabilities. Tech. rep. Statistics Research
Report 22, University Warwick, Coventry.
Walley, P. (1991). Statistical Reasoning Imprecise Probabilities. Chapman Hall,
New York.
Walley, P. (1996a). Inferences multinomial data: learning bag marbles. J.
R. Statist. Soc. B, 58 (1), 357.
Walley, P. (1996b). Measures uncertainty expert systems. Artificial Intelligence, 83 (1),
158.
Walley, P., Pelessoni, R., & Vicig, P. (2004). Direct algorithms checking consistecy
making inferences conditional probability assessments. Journal Statistical
Planning Inference, 126 (1), 119151.
820

fiConservative Inference Rule

Zaffalon, M. (2001). Statistical inference naive credal classifier. De Cooman, G.,
Fine, T. L., & Seidenfeld, T. (Eds.), ISIPTA 01: Proceedings Second International Symposium Imprecise Probabilities Applications, pp. 384393,
Netherlands. Shaker.
Zaffalon, M. (2002). Exact credal treatment missing data. Journal Statistical Planning
Inference, 105 (1), 105122.
Zaffalon, M. (2005). Conservative rules predictive inference incomplete data.
Cozman, F. G., Nau, R., & Seidenfeld, T. (Eds.), ISIPTA 05: Proceedings
Fourth International Symposium Imprecise Probabilities Applications,
pp. 406415, Manno, Switzerland. SIPTA.

821

fiJournal Artificial Intelligence Research 34 (2009) 675-706

Submitted 11/08; published 04/09

Planning Chain Causal Graphs Variables
Domains Size 5 NP-Hard
Omer Gimenez

omer.gimenez@upc.edu

Dept. de Llenguatges Sistemes Informatics
Universitat Politecnica de Catalunya
Jordi Girona, 1-3
08034 Barcelona, Spain

Anders Jonsson

anders.jonsson@upf.edu

Dept. Information Communication Technologies
Universitat Pompeu Fabra
Roc Boronat, 138
08018 Barcelona, Spain

Abstract
Recently, considerable focus given problem determining boundary
tractable intractable planning problems. paper, study complexity planning class Cn planning problems, characterized unary operators
directed path causal graphs. Although one simplest forms causal graphs
planning problem have, show planning intractable Cn (unless P = NP),
even domains state variables bounded size. particular, show plan
existence Ckn NP-hard k 5 reduction Cnf-Sat. Here, k denotes
upper bound size state variable domains. result reduces complexity
gap class Ckn cases k = 3 k = 4 only, since C2n known tractable.

1. Introduction
ongoing effort planning community determine complexity different classes planning problems. Known tractable classes usually characterized
simple causal graph structure accompanied additional restrictions variables
operators. However, boundary tractable intractable planning problems
still clearly established. present paper contributes novel complexity result
class planning problems simple causal graph structure literature,
effort reduce complexity gap.
problem determining tractable classes planning problems purely theoretical interest. instance, complex planning problems projected onto tractable
fragments planning problems generate heuristics used search (Katz &
Domshlak, 2008b). Also, causal graph heuristic (Helmert, 2006) exploits hierarchical structure planning problem transforming tractable form: first,
translates propositional variables multi-valued variables, process simplifies
causal graph problem; then, keeps relaxing problem causal graph
becomes acyclic.
present paper aims study complexity planning problems class Cn ,
defined Domshlak Dinitz (2001). class Cn contains planning problems
c
2009
AI Access Foundation. rights reserved.

fiGimenez & Jonsson

Ckn
k=2
k {3, 4}
k5

Plan generation
P
EXP
EXP

Macro plan generation
P
?
Intractable

Plan existence
P
?
NP-hard

Table 1: Overview complexity results class Ckn .

multi-valued variables chain causal graphs, i.e., causal graph directed path
(implying operators unary). notation n indicates number state
variables unbounded. particular, study complexity plan existence Cn ,
i.e., determining whether exists plan solves planning problem Cn .
Even though planning problems Cn exhibit extremely basic form causal structure, i.e., linear dependence state variables, solving planning problems Cn
necessarily tractable, even impose additional restrictions. Let Ckn subclass
Cn state variables domains size k. known class C2n
polynomial-time solvable (Brafman & Domshlak, 2003) plan existence class
Cn NP-hard (Gimenez & Jonsson, 2008a). aim study complexity plan
existence classes between, namely Ckn k 3.
Domshlak Dinitz (2001) showed solvable instances C3n require
exponentially long plans. means polynomial-time plan generation
algorithm Ckn k 3, case C2n . However, rule
existence polynomial-time algorithm determines plan existence class Ckn ,
even algorithm generates plans succinct form, like Jonsson (2007)
Gimenez Jonsson (2008a). incompatible Cn NP-hard.
paper, prove plan existence class Ckn NP-hard k 5.
words, even causal graph directed path domains state
variables restricted contain 5 values, deciding whether plan
exists solving corresponding planning problem NP-hard. result implies
sufficient planning problem exhibit linear variable dependence restricted
variable domain sizes; additional restrictions necessary make planning tractable.
Table 1 shows overview complexity results class Ckn date.
Macro plan generation mean algorithm generating compact representation
solution, work Jonsson (2007) Gimenez Jonsson (2008a).
Intractable result column means complexity yet unknown
cannot P unless P = NP (else plan existence would P). row k = 2 due
Brafman Domshlak (2003), column plan generation due Domshlak
Dinitz (2001), contributions present paper marked boldface. Note
novel result subsumes Gimenez Jonsson (2008a), showed NP-hardness
k = O(n).
paper organized follows. Section 2 relate results previous work,
Section 3 introduce notation used throughout. Section 4 give formal
proof reduction Cnf-Sat planning problems C11
n . main result,
5
reduction Cnf-Sat planning problems Cn , proved Section 5. Although
11
result C5n subsumes C11
n , believe intuitive idea behind Cn
676

fiChain Causal Graphs Domains Size 5

reduction easier understand, may interest anyone trying prove hardness
results similar circumstances. Section 6 discuss complexity remaining
classes C3n C4n .
also prove correctness third reduction, time Cnf-Sat C7n ,
7
Appendix A. reductions C11
n Cn previously appeared conference paper
(Gimenez & Jonsson, 2008b), present paper provides formal proof correctness.

2. Related Work
complexity planning studied extensively last twenty years (Bylander, 1994; Chapman, 1987; Erol, Nau, & Subrahmanian, 1995). Many tractable classes
planning problems exploit notion causal graph one way another. Knoblock
(1994) usually credited introducing causal graph work hierarchical
planning. Williams Nayak (1997) required planning problems acyclic causal
graphs effort ensure tractability. Jonsson Backstrom (1998) defined class
3S planning problems, also acyclic causal graphs, showed plan existence
tractable class.
Domshlak Dinitz (2001) introduced class Cn planning problems studied
paper, well several related classes, particular causal graph
structure. Brafman Domshlak (2003) designed polynomial-time algorithm solving planning problems binary state variables polytree causal graphs bounded
indegree, proving planning tractable class C2n . Brafman Domshlak
(2006) presented complexity results related tree-width causal graph. Katz
Domshlak (2008a) used causal graph structure prove several complexity results
optimal planning.
Jonsson (2007) Gimenez Jonsson (2008a) designed polynomial-time algorithms
solve planning problems restricted causal graphs generating hierarchy
macros. Recently, Chen Gimenez (2008) showed complexity planning
intractable unless size largest connected component causal graph bounded
constant. Consequently, causal graph structure alone enough guarantee
tractability, implying additional restrictions needed.

3. Notation
Throughout paper, use [i..n] denote set {i, . . . , n}.
Let V set state variables, let D(v) finite domain state variable
v V . define state function V maps state variable v V
value s(v) D(v) domain. partial state p function subset Vp V
state variables maps state variable v Vp p(v) D(v). frequently use
notation (v1 = x1 , . . . , vk = xk ) denote partial state p defined Vp = {v1 , . . . , vk }
p(vi ) = xi vi Vp .
planning problem tuple P = hV, init, goal, Ai, V set variables, init
initial state, goal partial goal state, set operators. operator
= hpre(a); post(a)i consists partial state pre(a) called pre-condition
677

fiGimenez & Jonsson

v1

v2

v3

v4

v5

Figure 1: Example causal graph planning problem class Ck5 .
partial state post(a) called post-condition. Operator applicable state
s(v) = pre(a)(v) v Vpre(a) , applying operator state results
new state (v) = post(a)(v) v Vpost(a) (v) = s(v) otherwise.
partial plan planning problem P sequence operators a1 , . . . , ak Ak ,
k 0, a1 applicable initial state init and, [2..k], ai
applicable following application a1 , . . . , ai1 starting init. Note partial plan
necessarily solve P . plan solving P partial plan goal
state goal satisfied following application a1 , . . . , ak . P solvable
exists plan .
causal graph planning problem P directed graph (V, E) state
variables nodes. edge (u, v) E u 6= v exists
operator u Vpre(a) Vpost(a) v Vpost(a) . Figure 1 shows example
causal graph form directed path. structure causal graph implies
operator unary, i.e., post-condition specified single variable
v, pre-condition specified (at most) v predecessor v causal
graph.
paper study class Ckn planning problems, defined follows:
Definition 3.1. planning problem P belongs class Ckn causal
graph P directed path and, v V , |D(v)| k.
planning problems Ckn , domain transition graph, DTG, state variable
v labelled, directed graph (D(v), E ) values domain v nodes.
edge (x, y) E label l D(v ) exists operator
hv = l, v = x; v = yi A, v predecessor v causal graph. edge
without label indicates pre-condition corresponding operator defined v
alone. edge one label indicates existence multiple operators
pre- post-condition v different pre-conditions v .

4. C11
n NP-hard
section prove C11
n NP-hard reduction Cnf-Sat. words,
every CNF formula F associate planning instance P11 (F ) C11
n P11 (F )
solvable F satisfiable. first describe planning problem P11 (F ),
explain intuitive idea behind reduction, finally provide formal proof
correctness.
Let F = C1 Ck CNF formula k clauses n variables x1 , . . . , xn .
define planning problem P11 (F ) = (V, init, goal, A) follows. variable set V
{si | [1..2n 1]} {vs } {vij | [1..k], j [1..n]} {ve } {ei | [1..2n 1]},
domains D(si ) = D(ei ) = D(ve ) = {0, 1} [1..2n 1], D(vs ) = {0, 1, x},
D(vij ) = {gx , g0 , g1 , ax , a0 , a1 , b0 , b1 , cx , c0 , c1 } [1..k], j [1..n]. initial state
defined init(si ) = init(ei ) = init(ve ) = 0, [1..2n 1], init(vs ) = x, init(vij ) = ax
678

fiChain Causal Graphs Domains Size 5

s1

s2n1

vs

v1n

v11

vk1

vkn



e1

e2n1

Figure 2: Causal graph planning problem P11 (F ).
0
1
1

0

0
0

1

0
1

1

x
1
0

0 0

1
1

Figure 3: DTGs variables s1 , s2 , . . . , s2n1 , vs .
[1..k], j [1..n], goal state partial state defined goal(vin ) = gx
[1..k], goal(ve ) = 0, goal(ei ) = (i mod 2) [1..2n 1].
providing formal definition operators A, give intuitive overview
planning problem P11 (F ). this, present causal graph P11 (F ) well
DTGs state variable. reader interested formal proof
correctness reduction may skip Section 4.2, introduce formal
definitions operators order prove several theoretical properties P11 (F ).
4.1 Intuition
planning problem P11 (F ) associated CNF formula F consists three parts,
clearly defined role. three parts illustrated Figure 2, showing causal
graph P11 (F ). first part P11 (F ) corresponds state variables s1 , . . . , s2n1 , vs ,
second part corresponds state variables v11 , . . . , v1n , . . . , vk1 , . . . , vkn , third
part corresponds state variables , e1 , . . . , e2n1 . role first part generate
message corresponding assignment variables CNF formula F .
role second part verify whether assignment satisfies clause Ci ,
remember fact (using value state variable vin ). Finally, role third part
make sure message propagated way end chain.
DTGs state variables s1 , . . . , s2n1 , vs appear Figure 3. state variables
used generate assignment variables x1 , . . . , xn CNF formula F .
this, operators P11 (F ) defined way value vs change
x either 0 1, 0 1 change back x. Thus, applying
operators P11 (F ) possible generate sequence x, m1 , x, . . . , x, mn , x values
vs , mj {0, 1} j [1..n].
define message sequence m1 , . . . , mn n symbols (either 0 1) corresponding sequence values vs . follows, refer symbols
bits message. value x used separator distinguish consecutive
bits message. Given message m, assignment defined (xj ) = mj
j [1..n]. Thus, assignment x1 determined first choice whether
change value vs x 0 1, on. purpose remaining state
variables si first part restrict message contain n bits.
679

fiGimenez & Jonsson

(a)

(b)
a0

g0

a0

g0

c0

b0

c0

b0
a0,b0,g0

0

0

x
0

gx
1

ax

1

x
cx

x

gx
a1,b1,g1

1

x

ax,cx,gx

a0,b0,g0

x

ax,cx,gx

ax
ax,cx,gx

a0,b0,g0

ax,cx,gx

a1,b1,g1

ax,cx,gx

cx

x
a1,b1,g1

g1

a1

c1

b1

g1

a1

b1

ax,cx,gx
c1

(c)
a0

g0
a0,b0
c0,g0

cx,gx g
0
gx

c1,g1

cx,gx g1

ax

a1,b1
g1

c0

b0
ax,cx

c0

c0

cx

c1

cx

ax,cx
a1

cx

cx

c1
b1

cx
c1

Figure 4: DTGs (a) v11 , (b) vi1 > 1, (c) vij j > 1. Dashed edges
explained text.

DTGs state variables vij , [1..k] j [1..n], appear Figure 4.
dashed edges DTGs indicate corresponding operators depend CNF
formula F . example, assignment (x1 ) = 1 satisfies clause C1 , edge
v11 = ax label 1 Figure 4(a) points g1 , else points b1 . Likewise, (x1 ) = 0
satisfies C1 , edge v11 = ax label 0 points g0 , else points b0 .
Recall role second part check whether assignment generated
first part satisfies CNF formula F . clause Ci variable xj
F , main function state variable vij check whether assignment (xj ) = mj
satisfies Ci . this, state variable vij acts finite state automaton propagates
bit message keeping track j-th bit message arrives.
Since domain size state variables restricted, way vij count
number bits received. Instead, fact j-th bit arrived indicated
vi(j1) . Moreover, last state variable vin clause Ci remember
whether Ci satisfied assignment variable xj .
summary, state variable vij second part performs following functions
values operators:
1. Propagate message generated vs .
2. Check whether assignment xj (the j-th bit m) satisfies clause Ci .
680

fiChain Causal Graphs Domains Size 5

0

0
a0,a1,
b0,b1, 0
g0,g1

ax
cx
gx
1

0
1

1

0

1
1

Figure 5: domain transition graph variables , e1 , . . . , e2n1 .
3. Remember whether Ci satisfied assignment xl , l j.
4. j < n Ci satisfied, propagate fact.
5. j < n, let vi(j+1) know (j + 1)-th bit message arrived.
Note third function strictly necessary j = n. However, including
state variables makes reduction compact symmetry.
Next, briefly describe vij implements functions. value
domain vij subscript 0, 1, x. propagate message, vij always moves
value whose subscript matches predecessor (in case v11 , subscript
match value vs ). Unless Ci satisfied assignment xl , l < j,
value vij remains subdomain {a0 , a1 , ax } prior arrival j-th bit.
clause Ci encoded dashed edges DTGs variables vij .
operators j-th bit mj arrives, vij moves ax gmj
assignment (xj ) = mj satisfies Ci , bmj otherwise. fact value vij
subdomain {g0 , g1 , gx } indicates Ci satisfied assignment
xl , l j. fact propagated way vin since subsequent state variable
Ci forced move value subdomain {g0 , g1 , gx } whenever value
predecessor {g0 , g1 , gx }. Whether clause Ci satisfied checked
defining goal state vin = gx .
Finally, j < n vij moves bmj , vi(j+1) moves amj . there, vij
choice move cx , causing vi(j+1) return ax . next bit arrives, vij
moves either c0 c1 , correctly indicating vi(j+1) (j + 1)-th bit arrived.
Consequently, vi(j+1) moves either g0 (g1 ) b0 (b1 ), depending whether
assignment xj+1 satisfies Ci . Hence, values type b used delay transition
vi(j+1) value type either b g. mechanism allows
variable vij react j-th bit. clause Ci , operators vi1 defined
vi1 always reacts first bit.
DTGs state variables , e1 , . . . , e2n1 appear Figure 5. function
state variables make sure n bits message propagated end
causal graph. state variable (strictly speaking, planner solving planning
problem) never forced select operator, choose propagate bit
message instead wait next bit arrive acting. turn, may cause
another state variable incorrectly conclude clause (not) satisfied.
variables third part prevent happening, since goal state defined
way cannot reached unless bits message arrive end
causal graph.
681

fiGimenez & Jonsson

Variable
s1
si ,
[2..2n 1]
vs

Operator
hs1 = 0; s1 = 1i
hsi1 = 0, si = 0; si = 1i
hsi1 = 1, si = 1; si = 0i
hs2n1 = 0, vs = x; vs = mi
hs2n1 = 1, vs = m; vs = xi

Qualifier

{0, 1}
{0, 1}

Table 2: Operators variables s1 , s2 , . . . , s2n1 , vs .

4.2 Formal Proof
section, prove C11
n NP-hard showing planning problem P11 (F )
solvable formula F satisfying assignment. start with, provide
formal definitions operators P11 (F ). operators s1 , . . . , s2n1 , vs appear
Table 2, corresponding DTGs appear Figure 3. operators variables vij ,
[1..k] j [1..n], appear Table 3, DTGs appear Figure 4. Finally,
operators , e1 , . . . , e2n1 appear Table 4, DTGs appear Figure 5.
reduce space requirement use shorthand definitions operators.
words, hv = m, v = c; v = mi, {a, b}, denotes existence two operators
hv = a, v = c; v = ai hv = b, v = c; v = bi. Similarly, hv {a, b}, v = c; v = di denotes
existence two operators hv = a, v = c; v = di hv = b, v = c; v = di. state
variables vij also introduce reference numbers allow us easily refer operators.
Furthermore, operators conditional properties CNF formula F ;
operator exists indicated property satisfied. example, operator
hv22 = c0 , v23 = ax ; v23 = g0 exists clause C2 satisfied x3 , operator
hv22 = c0 , v23 = ax ; v23 = b0 exists C2 satisfied x3 . use set notation
xj Ci denote literal xj appears clause Ci .
proof organized follows. begin series technical definitions
lemmas (4.14.6) related operators implications. Definition 4.7 introduces notion admissible plans, Lemma 4.8 states plan solving P11 (F )
admissible. Next, Lemma 4.10 establishes admissible plan corresponds
assignment variables CNF formula F , operator choices
plan forced given assignment. Finally, Lemma 4.13 determines exact sequence
values taken state variable execution admissible plan, making
possible check whether goal state reached end execution. Theorem
4.14 concludes admissible plans solving P11 (F ) corresponding
satisfying assignments F .
Definition 4.1. Given partial plan P11 (F ) variable v V , (v) number
times value v changed operators .
Lemma 4.2. partial plan P11 (F ), holds
(si ) [1..2n 1],
(vs ) 2n.
682

fiChain Causal Graphs Domains Size 5

Variable
v11

vi1 ,
[2..k]

vij ,
[1..k],
j [2..n]

Ref.
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
(11)
(12)
(13)
(14)
(15)
(16)
(17)
(18)
(19)
(20)
(21)

Operator
hvs = 1, v11 = ax ; v11 = g1
hvs = 1, v11 = ax ; v11 = b1
hvs = 0, v11 = ax ; v11 = g0
hvs = 0, v11 = ax ; v11 = b0
hvs = m, v11 = cx ; v11 = cm
hvs = m, v11 = gx ; v11 = gm
hvs = x, v11 = bm ; v11 = cx
hvs = x, v11 = cm ; v11 = cx
hvs = x, v11 = gm ; v11 = gx
hv(i1)n {a1 , b1 , g1 }, vi1 = ax ; vi1 = g1
hv(i1)n {a1 , b1 , g1 }, vi1 = ax ; vi1 = b1
hv(i1)n {a0 , b0 , g0 }, vi1 = ax ; vi1 = g0
hv(i1)n {a0 , b0 , g0 }, vi1 = ax ; vi1 = b0
hv(i1)n {am , bm , gm }, vi1 = cx ; vi1 = cm
hv(i1)n {am , bm , gm }, vi1 = gx ; vi1 = gm
hv(i1)n {ax , cx , gx }, vi1 = bm ; vi1 = cx
hv(i1)n {ax , cx , gx }, vi1 = cm ; vi1 = cx
hv(i1)n {ax , cx , gx }, vi1 = gm ; vi1 = gx
hvi(j1) = c1 , vij = ax ; vij = g1
hvi(j1) = c1 , vij = ax ; vij = b1
hvi(j1) = c0 , vij = ax ; vij = g0
hvi(j1) = c0 , vij = ax ; vij = b0
hvi(j1) {am , bm }, vij = ax ; vij =
hvi(j1) = gm , vij = ax ; vij = gm
hvi(j1) = cm , vij = cx ; vij = cm
hvi(j1) {cm , gm }, vij = gx ; vij = gm
hvi(j1) {ax , cx }, vij = ; vij = ax
hvi(j1) = cx , vij = bm ; vij = cx
hvi(j1) = cx , vij = cm ; vij = cx
hvi(j1) {cx , gx }, vij = gm ; vij = gx

Qualifier
x1 C1
x1
/ C1
x1 C1
x1
/ C1
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
x1 Ci
x1
/ Ci
x1 Ci
x1
/ Ci
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
xj Ci
xj
/ Ci
xj Ci
xj
/ Ci
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}

Table 3: Operators variables v11 , . . . , vkn .

Variable


hvkn

e1
ei , [2..2n 1]

Operator
{a0 , a1 , b0 , b1 , g0 , g1 },
hvkn {ax , cx , gx },
hve = 1, e1
hve = 0, e1
hei1 = 1, ei
hei1 = 0, ei

= 0; = 1i
= 1; = 0i
= 0; e1 = 1i
= 1; e1 = 0i
= 0; ei = 1i
= 1; ei = 0i

Table 4: Operators variables , e1 , . . . , e2n1 .

683

fiGimenez & Jonsson

Proof. induction i. = 1, variable s1 change once, (s1 ) 1.
[2..2n 1], follows inspection operators cannot change value
si twice without changing value si1 (the operator setting si
1 si1 = 0 pre-condition, operator resetting si 0 si1 = 1
pre-condition). Since change value si initial state without
first changing value si1 , follows (si ) (si1 ) + 1 (i 1) + 1 =
induction. argument holds variable vs predecessor s2n1 , (vs )
(s2n1 ) + 1 (2n 1) + 1 = 2n.
Lemma 4.3. partial plan P11 (F ) vij , [1..k] j [1..n],
holds (vij ) (v ), v predecessor vij causal graph.
Proof. before, follows inspection operators cannot change
value vij twice without changing value v between. see this, note
subscript value D(vij ) either x, 0, 1. operator vij either changes
value one subscript x one subscript 0 (1), v also value
subscript 0 (1), one subscript 0 (1) one subscript x, v also value
subscript x (the argument holds v11 , although values predecessor
vs x, 0, 1 without subscripts).
Note value vij cannot change initial state without first changing
value v , since v value subscript 0 1 value vij change
initial value ax . Consequently, value vij cannot change times
value v , (vij ) (v ) claimed.
Lemma 4.4. vij , [1..k] j [1..n], partial state (v = x, vij = y),
v predecessor vij causal graph, one applicable operator
changing value vij .
Proof. inspecting operators easy see pair operators vij
different pre-conditions. exception rule operators exist
simultaneously due properties CNF formula F (e.g. operators (1) (2)).
Lemma 4.5. partial plan P11 (F ), holds
(ve ) (vkn ),
(e1 ) (ve ),
(ei ) (ei1 ) [2..2n 1].
Proof. Let v variable among , e1 , . . . , e2n1 , let v predecessor causal
graph. before, cannot change value v twice without changing value v
between. v {e1 , . . . , e2n1 }, operator setting v 1 requires v = 1,
operator resetting v 0 requires v = 0. v = , operator setting v 1 requires
v value subscript 0 1, operator resetting v 0 requires v
value subscript x. Note that, either case, cannot change value v
initial state without first changing value v . Thus, (v) (v )
variables, claimed.
684

fiChain Causal Graphs Domains Size 5

turn problem finding plan solves P11 (F ).
Lemma 4.6. Let plan solves P11 (F ).
(ei ) 2n [1..2n 1],
(ve ) 2n.
Proof. descending induction i. = 2n 1, goal(e2n1 ) = 1, value
e2n1 change least initial value init(e2n1 ) = 0, implying (e2n1 )
1 = 2n (2n 1). [1..2n 2], assume (ei+1 ) 2n (i + 1) holds
induction. Lemma 4.5 follows (ei ) (ei+1 ) 2n (i + 1). However,
since goal(ei ) 6= goal(ei+1 ) since solves P11 (F ), follows (ei ) 6= (ei+1 ).
Hence (ei ) > (ei+1 ), follows (ei ) 2n i, claimed.
argument applies e1 predecessor , since goal(ve ) = 0 6= 1 = goal(e1 ), yielding
(ve ) 2n.
Definition 4.7. admissible plan planning problem P11 (F ) partial plan
(si ) = i, (vs ) = (v11 ) = . . . = (vkn ) = (ve ) = 2n, (ei ) = 2n i,
[1..2n 1].
Lemma 4.8. plan solves P11 (F ) admissible.
Proof. Lemmas 4.3 4.5 (vs ) (v11 ) (vkn ) (ve ). But,
Lemmas 4.2 4.6, values equal 2n, since 2n (vs ) (ve ) 2n.
proof Lemma 4.2 (si ) (si1 ) + 1, [2..2n 1],
(vs ) (s2n1 ) + 1, together Lemma 4.2 (vs ) = 2n implies (si ) = i,
[1..2n 1]. proof Lemma 4.6 (ve ) > (e1 ), (ei ) > (ei+1 ),
[1..2n 2], (e2n1 ) 1, together Lemma 4.6 (ve ) = 2n implies
(ei ) = 2n i, [1..2n 1].
Please note converse Lemma 4.8 true, is, admissible plans
solve planning problem P11 (F ).
consequence Lemma 4.8, find plan solves P11 (F ) need
consider admissible plans. particular, admissible plan changes value variable vs
exactly 2n times, generating sequence 2n + 1 values. Note value vs always
changes x either 0 1, back x.
Definition 4.9. Let admissible plan, let x, m1 , x, . . . , x, mn , x sequence
2n + 1 values variable vs takes execution , mj {0, 1}
j [1..n]. use denote message m1 , . . . , mn induced , use
denote formula assignment (xj ) = mj j [1..n].
turns out, operators part admissible plan completely
determined message induced .
Lemma 4.10. Let admissible plan P11 (F ) let induced message.
operators changing value variable vij , [1..k] j [1..n], well
sequence values variable vij takes execution , completely
determined .
685

fiGimenez & Jonsson

Proof. v {v11 , . . . , vkn }, let v causal graph predecessor. proof
Lemma 4.3 know cannot change value v twice without changing value
v between, initial state, change value v
change value v. definition admissible know (v ) = (v) = 2n.
way admissible plan change value v 2n times without changing
value v 2n times first change value v , v, v , on.
Now, Lemma 4.4 know that, given partial state (v = x, v = y),
one applicable operator changing value v. Thus, time admissible
plan changes value v value v , one operator so.
plan choice select operator since allowed change value
v changing value v. Consequently, sequence values taken
v completely determined, operators v, well sequence values
takes on, completely determined also. proof follows double induction
j, since sequence values taken vs (the predecessor v11 ) completely
determined message .
follows Lemma 4.10 relevant degree freedom admissible
plan selecting elements message , repeatedly deciding whether move
vs = 0 vs = 1 vs = x. selected, operator choices
forced, else plan admissible. particular, message unique
state executing admissible plan starting init results s. remains
determine whether unique state matches goal state.
Remark. Note Lemma 4.10 mention operator order admissible plan.
Indeed, change order operators admissible plan without making
plan inadmissible. example, let v1 , v2 , v3 three consecutive variables
causal graph, let ha11 , a12 , a13 , a21 , a22 , a23 subsequence operators changing
values, aji j-th operator changing value vi . subsequence

ha11 , a12 , a21 , a13 , a22 , a23 achieves result. long partial order haji , aji+1 , aj+1

respected j, change operator order please.
proceed determine sequence values variable vij , [1..k] j
[1..n], takes execution admissible plan induced message .
First, define satisficing index clauses, sequence values plan.
Definition 4.11. Let admissible plan induced message = m.
clause Ci , let satisficing index Ti [1..n+1] smallest number (xTi ) =
mTi satisfies Ci . number exists, Ti = n + 1.
Definition 4.12. Let admissible plan. clause Ci [1..2n + 1],
let sequence values Qti () vector n values representing, variable
vij , j [1..n], t-th value taken vij execution .
following lemma key understanding idea behind reduction C11
n , since
specifies sequences values admissible plan induces execution.
Lemma 4.13. Let assignment variables x1 , . . . , xn formula F .
686

fiChain Causal Graphs Domains Size 5

1) Existence. exists admissible plan planning problem P11 (F ) induced assignment = .
2) Claim. Let Qti sequences values described Part 3) lemma.
admissible plans = sequences values Qti () = Qti ,
[1..k] [1..2n + 1].
3) Sequence values. sequence values Qti , [1..k] [1..2n + 1],
follows.
a) j < Ti ,
nj

j1

z }| {
c x cx
Qi2j1 =
2j
Qi = cmj cmj
2j+1
=
c x cx
Qi

ax
bmj
cx

z }| {
ax ax
amj amj
ax ax

ax
gmj
gx

z }| {
ax ax
gmj gmj
gx gx

b) j = Ti ,
nj

j1

Qi2j1
Q2j

Q2j+1


z }| {
c x cx
=
= cmj cmj
=
c x cx

c) j > Ti ,
jTi

nj

z }| {
gx gx
gmj gmj
gx gx

z }| {
gx gx
gmj gmj
gx gx

Ti 1

Qi2j1
Q2j

Q2j+1


z }| {
=
c x cx
= cmj cmj
=
c x cx

gx
gmj
gx

Proof. proving lemma, must check definition Qti given Part 3
consistent. necessary due overlapping statements, namely, every
odd 1 2n + 1, sequence Qti defined twice, Qi2j1 j = 2t ,

another time Qi2j +1 j = 2t . However, sequences values well-defined
+1
definitions Qi2j1 Q2j
match combination j j = j 1,

shown following table.


j

Qi2j +1 = Qi2j1

Case (a)

z }| { z }| {
cx cx ax ax

Case (b)

z }| { z }| {
cx cx ax ax

Case (c)

z }| { z }| {
cx cx gx gx

Case (c)

z }| { z }| {
cx cx gx gx

j

j

1 < j < Ti :

Case (a)

j

1 < j = Ti :

Case (a)

Ti 1

j = Ti + 1 n:

Case (b)

Ti 1

Ti + 1 < j n:

Case (c)

687

nj

nj

nTi +1
nTi +1

fiGimenez & Jonsson

Now, prove Parts 2 3 lemma. Assume admissible plan induced
assignment = . proof proceeds double induction j. particular,
2j+1
prove validity three statements type Qi2j1 , Q2j
, assuming
, Qi




statements type Qi (for < t) statements type Qi2j 1 , Q2j


Qi2j +1 (for j < j) already hold. first prove validity Qi2j1 . j = 1,
Qi2j1 = Q1i = ax ax Cases (a) (b) corresponds initial state vi1 , . . . , vin
(note Case (c) cannot hold j = 1). j > 1 know that, since statements

consistent, Qi2j1 = Qi2j +1 j = j 1, hence correctness Qi2j1 follows
induction j.
2j+1
Next, prove statements relative Q2j
. Consider variable v
Qi
precedes vi1 causal graph, values number 2j 1, 2j, 2j + 1 takes
execution . = 1, v = vs values x, mj , x. > 1,
v = v(i1)n and, induction i, values ax , amj , ax j < Ti1 j < n;
ax , bmj , cx j = n < Ti1 ; ax , gmj , gx j = Ti1 ; gx , gmj , gx j > Ti1 .
proof divided 6 parts, depending values j Ti .
I) 1 = j < Ti . Consider following table, write instead mj = m1
simplify notation.
v
vi1
2j 1
{x, ax , gx }
ax
{m, , bm , gm }
2j
2j + 1 {x, ax , cx , gx }


vi2
ax








vin
ax



three rows table correspond values number 2j 1, 2j, 2j + 1
variables v , vi1 , . . . , vin . first column corresponds possible values
predecessor v vi1 take on. first row given Qi2j1 , second
2j+1
third rows, filled, correspond Q2j
.
Qi
Let A2j operator causing 2j-th value vi1 . According previous
table, pre-condition A2j must compatible
hv {m1 , am1 , bm1 , gm1 }, vi1 = ax
is, values variables v vi1 A2j applied. Since Ti > 1, (x1 ) =
m1 satisfy clause Ci , operator A2j must one labelled (2)
(4) Table 3. (Only one operators applicable, depending value
m1 whether v vs v(i1)n .) either case, application A2j causes
value vi1 become bm1 , fill blank previous table.
v
vi1
vi2
2j 1
{x, ax , gx }
ax
ax
2j
{m, , bm , gm } bm (2, 4)
2j + 1 {x, ax , cx , gx }








vin
ax



way, check A2j+1 , operator causing (2j + 1)-th value
vi1 , must one labelled (7) Table 3; new value vi1 cx .
688

fiChain Causal Graphs Domains Size 5

remaining variables, easy check variables vi2 , . . . , vin become am1 , due
operators type (14), become ax , due operators type (18).
table complete:
v
vi1
vi2 vin
2j 1
{x, ax , gx }
ax
ax ax
2j
{m, , bm , gm } bm (2, 4) (14)
2j + 1 {x, ax , cx , gx }
cx (7)
ax ax (18)
shows Case (a) Lemma 4.13 holds j = 1 Ti > 1.
II) 1 = j = Ti . proof similar Case (I). Since Ti = 1, (x1 ) = m1
satisfies clause Ci . result, admissible operators causing 2j-th value
vi1 labelled (1) (3). either case, value vi1 becomes gm1 .
Consequently, admissible operators vi2 , . . . , vin different before.
resulting table:
v
vi1
vi2 vin
2j 1
{x, ax , gx }
ax
ax ax
2j
{m, , bm , gm } gm (1, 3) gm gm (15)
2j + 1 {x, ax , cx , gx }
gx (9)
gx gx (21)
III) 1 < j < Ti . case, remaining ones, show resulting table.
always write = mj . follows, omit column v since possible
values always same.
vi1
vi2 vi(j1)
vij
vi(j+1) vin
2j 1 cx
c x cx
ax
ax ax
cm (5)
cm cm (16) bm (11, 13)
(14)
2j
2j + 1 cx (8)
cx cx (20) cx (19)
ax ax (18)
IV) 1 < j = Ti .
vi1
vi2 vi(j1)
vij
vi(j+1) vin
2j 1 cx
c x cx
ax
ax ax
cm (5)
cm cm (16) gm (10, 12)
gm gm (15)
2j
2j + 1 cx (8)
cx cx (20) gx (21)
gx gx (21)
V) 1 = Ti < j.
vi1
vi2 vin
2j 1 gx
gx gx
2j
gm (6) gm gm (17)
2j + 1 gx (9) gx gx (21)
VI) 1 < Ti < j.
vi1
vi2 vi(Ti 1)
viTi vin
2j 1 cx
c x cx
gx gx
2j
cm (5)
cm cm (16) gm gm (17)
cx cx (20) gx gx (21)
2j + 1 cx (8)
689

fiGimenez & Jonsson

remains check Case (a) Lemma 4.13 follows parts (I) (III),
Case (b) parts (II) (IV), Case (c) parts (V) (VI). proves Part
2 3 lemma.
Finally, note existence admissible plan directly follows previous
discussion, since always specified operators used every situation,
assumed existence. proves Part 1 lemma.
Theorem 4.14. exists plan solves planning problem P11 (F )
exists assignment satisfies CNF formula F .
Proof. : Given assignment satisfies F , construct admissible plan whose
induced formula assignment equals , choosing sequence values vs accordingly. follows Ti n clause Ci , since exists variable xj
(xj ) = mj satisfies Ci . Then, Q2n+1
form indicated Case (b) (c) Lemma

4.13. either case, (2n + 1)-th value variable vin gx , required goal state.
plan thus solves P11 (F ).
: Let plan solves planning problem P11 (F ). Lemma 4.8 plan
admissible. show contradiction = satisfies F . Assume not.
exists clause Ci satisfied , implying Ti = n + 1. Since n < Ti , (2n + 1)-th
value variable vin cx according Case (a) Lemma 4.13. contradicts solving
P11 (F ), since goal value vin cx gx .
Proposition 4.15. Plan existence C11
n NP-hard.
Proof. largest variable domains planning problem P11 (F ) variables
v11 , . . . , vkn , contain 11 values. proof follows immediately well-known
NP-hardness Cnf-Sat, Theorem 4.14, fact produce planning
problem P11 (F ) polynomial time given CNF formula F .
4.3 Example
illustrate reduction using small example CNF formula F = (x1 x2 ) one
clause two variables x1 x2 . variable set corresponding planning problem
P11 (F ) V = {s1 , s2 , s3 , vs , v11 , v12 , , e1 , e2 , e3 }. admissible plan induce
four different messages (0, 0), (0, 1), (1, 0), (1, 1). message (0, 0) corresponds
assignment satisfy F . plan solves P11 (F ) induced
message (0, 1) appears Table 5. Note that, following execution plan, goal state
goal = (v12 = gx , = 0, e1 = 1, e2 = 0, e3 = 1) satisfied desired; last value change
variable appearing goal state marked using boldface.

5. C5n NP-hard
section, describe reduction Cnf-Sat C5n . CNF formula F
associate planning problem P5 (F ). clause Ci variable xj F , P5 (F ) contains
1 , domain D(v 1 ) = {a , , , b }, v 2 , domain D(v 2 ) =
two state variables vij
x 0 1 x
ij
ij
ij
2 , D(v 2 ) = {a , b , b }.
{ax , a0 , a1 , b0 , b1 }. values a0 a1 omitted vin
x 0 1

690

fiChain Causal Graphs Domains Size 5

..
.
hs1 = 0, s2 = 0; s2 = 1i
hs2 = 1, s3 = 1; s3 = 0i
hs3 = 0, vs = x; vs = 1i
hvs = 1, v11 = cx ; v11 = c1
hv11 = c1 , v12 = ax ; v12 = g1
hv12 = g1 , = 0; = 1i
hve = 1, e1 = 0; e1 = 1i
hs1 = 0; s1 = 1i
hs1 = 1, s2 = 1; s2 = 0i
hs2 = 0, s3 = 0; s3 = 1i
hs3 = 1, vs = 1; vs = xi
hvs = x, v11 = c1 ; v11 = cx
hv11 = cx , v12 = g1 ; v12 = gx
hv12 = gx , = 1; = 0i

hs3 = 0, vs = x; vs = 0i
hvs = 0, v11 = ax ; v11 = b0
hv11 = b0 , v12 = ax ; v12 = a0
hv12 = a0 , = 0; = 1i
hve = 1, e1 = 0; e1 = 1i
he1 = 1, e2 = 0; e2 = 1i
he2 = 1, e3 = 0; e3 = 1i
hs2 = 0, s3 = 0; s3 = 1i
hs3 = 1, vs = 0; vs = xi
hvs = x, v11 = b0 ; v11 = cx
hv11 = cx , v12 = a0 ; v12 = ax
hv12 = ax , = 1; = 0i
hve = 0, e1 = 1; e1 = 0i
he1 = 0, e2 = 1; e2 = 0i
..
.

Table 5: plan solves planning problem P11 (F ) example formula F .
(a)

(b)

(c)

(d)

a0

a0

a0

a0
a0

x

0

b0

x

ax

bx

ax

x

x

b1

ax

ax b0

ax

ax b1

bx
ax

ax
a1

1

ax a0

ax

a1

ax bx

ax

ax
a1

a1

b0
a0

ax
a0

bx

ax

(e)
b0
ax bx

a1

ax a1
a1

ax bx
b1

ax bx

a1
b1

1 , (b) v 1 , > 1, (c) v 1 , j > 1, (d) v 2 , j < n, (e) v 2 .
Figure 6: DTGs (a) v11
i1
ij
ij


state variables s1 , . . . , s2n1 , vs , , e1 , . . . , e2n1 , well domains corresponding
2 .
operators, before, except predecessor vkn
1 ) = init(v 2 ) = , [1..k]
initial state new state variables init(vij
x
ij
1
j [1..n], goal state goal(vi1 ) = ax , [1..k]. Table 6 lists operators
1 v 2 , [1..k] j [1..n], Figure 6 shows corresponding DTGs.
variables vij
ij
Table 6 also lists new operators variable , different pre-conditions
2 .
predecessor vkn
5.1 Intuition
reduction C5n based following idea: instead using explicit value
remember clause satisfied, goal remain initial value ax .
way able reduce size variable domains needed reduction.
Somewhat surprisingly, new reduction uses fewer total operators C11
n .
691

fiGimenez & Jonsson

Variable
1
v11

1,
vi1
[2..k]
1,
vij
[1..k],
j [2..n]
2,
vij
[1..k],
j [1..n 1]

2 ,
vin
[1..k]



Ref.
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
(11)
(12)
(13)
(14)
(15)
(16)
(17)
(18)
(19)
(20)
(21)
(22)

Operator
1 = ; v1 =
hvs = m, v11
x 11

1 = ; v1 =
hvs = x, v11
11
x
1 = ; v1 = b
hvs = x, v11
11
x
1 = ; v1 =
2
= bm , vi1
hv(i1)n
x i1

2
1 = ; v1 =
hv(i1)n
= ax , vi1
x
i1
2
1 = ; v1 = b
hv(i1)n
= ax , vi1
i1
x
1 = ; v1 =
2
= , vij
hvi(j1)

x ij
2
1 = ; v1 =
hvi(j1)
= ax , vij
ij
x
1 = b ; v1 =
2
= bm , vij
hvi(j1)

x ij
1
1
2
hvi(j1) = ax , vij = ; vij = bx
1 = , v2 = ; v2 =
hvij

x ij
ij
1 = , v2 = ; v2 =
hvij
x
x ij
ij
2
2
1
hvij = , vij = ax ; vij = bm
1 = , v2 = b ; v2 =
hvij
x
x ij
1 ij
1 = b , v2 = b ; v2 =
hvij
x ij
1 ij
x
1 = , v2 = b ; v2 =
hvij
x ij
0 ij
x
1 = b , v2 = b ; v2 =
hvij
x ij
0 ij
x
1 = , v2 = ; v2 = b
hvin

x

1 = , v2 = b ; v2 =
hvin
x
1
x
1 = b , v2 = b ; v2 =
hvin
x
1
x
1 = , v2 = b ; v2 =
hvin
x
x
0
1 = b , v2 = b ; v2 =
hvin
x
0
x
2 = b , v = 0; v = 1i
hvkn
e
e
2 = , v = 1; v = 0i
hvkn
x e
e

Qualifier
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
xnj+1 Ci
xnj+1
/ Ci
xnj+1 Ci
xnj+1
/ Ci
{0, 1}
x1 Ci
x1
/ Ci
x1 Ci
x1
/ Ci
{0, 1}

1 , v 2 , v , [1..k] j [1..n].
Table 6: Operators variables vij
e
ij

692

fiChain Causal Graphs Domains Size 5

reduction C5n also uses another new idea. reduction C11
n , information
propagated forward, i.e., variable vij changed value according value
predecessor vi(j1) . reduction C5n , however, constructed information propagated forward (in particular, bits message) information
propagated backwards (the index bit currently checking). planning
problem arranged variable v may several applicable operators,
one satisfies pre-condition applicable action successor v . result
value v time + 1 depends value v time t.
explain planning problem P5 (F ) bit detail. Due backward
propagation mechanism, bits message checked reverse order.
words, vin checks first bit, vi(n1) checks second bit, vi1 checks n-th
2 check whether (n j + 1)-th bit satisfies clause C ,
bit. purpose vij

1 inform v 2
whereas purpose vij


(n

j
+
2)-th
bit

arrived.
i(j1)
1 also keeps track whether C satisfied first (n j + 1)
Implicitly, vij

bits.
Assume without loss generality message 0 0. Let us see happens
corresponding assignment satisfy clause Ci . Upon arrival first bit,
2 move b . requires v 1 = pre-condition,
state variable vin
0
0

l , j [1..n 1] l {1, 2}, . Next, v 2
turn requires state variables vij
0

1 = b . turn, requires state
move back ax , requires pre-condition vin
x
l , j [1..n 1] l {1, 2}, . v 1 moves b
variables vij
x
x

2
a0 , requiring vi(n1) = b0 pre-condition.
1 b following (nj +1)We see that, long clause remains unsatisfied, vij
x
1 b following last bit. Assume
th bit. particular, means vi1
x
2 moves b , requires v 1
(n j + 1)-th bit satisfies clause Ci . vij
0
x
ij
1
move ax instead bx . there, way vi(j1)
bx following
1 following last bit, satisfying goal
(n j + 2)-th bit. particular, vi1
x
state.
5.2 Formal Proof
proof C5n organized much way C11
n . Note variables
s1 , . . . , s2n1 , vs , , e1 , . . . , e2n1 before, Lemmas 4.2 4.6 still apply
P5 (F ). easy check Lemmas 4.3, 4.5 4.8 also hold P5 (F ). However,
Lemma 4.4 longer holds, since several operators share preconditions, namely
operators (2) (3), (5) (6), (8) (10), (11) (13). spite this,
operators sequences values admissible plan completely determined
induced message , P11 (F ) (as shown Lemma 4.10):
Lemma 5.1. Let admissible plan P5 (F ) let induced message.
l , [1..k], j [1..n], l {1, 2},
operators changing value variable vij
l takes execution ,
well sequence values variable vij
completely determined .
1 , assume without loss generality value .
Proof. First consider variable v11
0
1 , namely (2), changing value
Given (vs = x), two applicable operators v11

693

fiGimenez & Jonsson

ax , (3), changing value bx . first sight, admissible plan choose either.
2 pair
However, admissible, change value v11
1 . Note v 1 = , v 2 either two values, namely
value changes v11
0
11
11
2 , admissible operator v 2 (12),
a0 b0 . value v11
0
11
1 = . Thus, changes value v 1 b longer admissible,
pre-condition v11
x
x
11
2 b , correct choice depends
choose operator (2). value v11
0
2 (16)
CNF formula F . xn satisfies clause C1 , admissible operator v11
1 = , choose operator (2). Otherwise, admissible
pre-condition v11
x
2
1 = b , choose operator (3).
operator v11 (17) pre-condition v11
x
2 .
1
either case, operator choice v11 forced given value v11
1 , [2..k], v 1 , [1..k] j [2..k],
reasoning applies variables vi1
ij
2 , [1..k] j [1..n 1], corresponding operators share
vij
pre-conditions. degree freedom admissible plan selecting induced
message choosing operators vs accordingly. remaining operator choices
and, consequently, sequences values completely determined induced message
.
prove lemma similar Lemma 4.13, establishing sequence values taken
state variables P5 (F ) execution admissible plan.
Definition 5.2. Let admissible plan P5 (F ). clause Ci
[1..2n + 1], let sequence values Qti () vector 2n elements representing,
l , j [1..n] l {1, 2}, t-th value taken variable v l
variable vij
ij
l ]. define diagonal value
execution . Let us denote value Qt ()[vij
1
qji (), [1..k] j [1..n], value Q2j+1 ()[vi(nj+1)
].
Lemma 5.3. Let assignment variables x1 , . . . , xn formula F .
1) Existence. exists admissible plan planning problem P5 (F ) induced
assignment = .
2) Claim. Let qji described Part 3) lemma. admissible plans
= diagonal values qji () = qji [1..k] j [1..n].
3) Diagonal values. diagonal values qji , [1..k] j [1..n],
follows.
a) j < Ti , qji = bx .
b) j Ti , qji = ax .
Proof. Note that, according Lemma 5.1, diagonal values qji (), also
full sequences values Qti (), completely determined admissible plan .
prove, then, admissible plans exist assignment , claimed Part 1,
diagonal values match expression given Part 3. prove two facts
careful, general analysis planning problem P5 (F ), explaining
analysis implies lemma. Incidentally, sequences values Qti () also
694

fiChain Causal Graphs Domains Size 5

obtained analysis; study important
purposes.
l variable P (F ). Clearly,
Let admissible plan, let v = vij
5

subscript t-th value Q ()[v] v takes depends parity t, since
operators affecting v change subscript x = {0, 1} back x.
Namely, subscript Qt ()[v] x = 2p 1, = 2p, p-th
bit message .
1,
Now, j [2..n 1] [1..k], consider t-th values variables vij
2 , v1
vij
i(j+1) take on, = 2p 1, 2p, 2p + 1. previous observation subscripts
implies (trivially) know something values.
1 ] Qt ()[v 2 ] Qt ()[v 1
Qt ()[vij
ij
i(j+1) ]
= 2p 1 {ax , bx }
ax
{ax , bx }
= 2p

{am , bm }

= 2p + 1 {ax , bx }
ax
{ax , bx }
1
] affects values diagonal,
study value Q2p1 ()[vi(j+1)
2p1
1
2p+1
1
2p
2
()[vi(j+1)
] = ax , check
()[vij ]. Q
namely Q ()[vij ] Q
one possible outcome.
1
2]
1]
Rule
]
Qt ()[vi(j+1)
Qt ()[vij
Qt ()[vij
= 2p 1 {ax , bx }
ax
ax
= 2p


(11)

(7)
= 2p + 1
ax
(8)
ax
(12)
{ax , bx }

is, value type ax propagated along diagonal another value ax .
call Propagation Rule I.
1
] = bx .
study possible outcomes Q2p1 ()[vi(j+1)
2p
2
2p+1
1
case, values Q ()[vij ] Q
()[vij ] diagonal depend whether
p-th bit message clause Ci satisfied xnj+1 = (c.f.
operators (14)(17) (18)(22) Table 6). Ci satisfied xnj+1 = m, follows
values must bm ax . Propagation Rule II.
1]
2]
1
Rule II
Qt ()[vij
Qt ()[vij
Qt ()[vi(j+1)
]
= 2p 1 {ax , bx }
ax
bx

bm
(13)

(9)
= 2p
= 2p + 1
ax
(8)
ax
(14, 16)
{ax , bx }

contrary, clause Ci satisfied, values must bm bx .
call Propagation Rule III.
1
2]
1]
Rule III Qt ()[vij
Qt ()[vi(j+1)
]
Qt ()[vij
= 2p 1 {ax , bx }
ax
bx
= 2p

bm
(13)

(9)
= 2p + 1
bx
(10)
ax
(15, 17)
{ax , bx }

695

fiGimenez & Jonsson

Finally, let us consider cases j = 1 j = n, treated
2 values type . Also note
previous analysis. Note variables vin

1 cannot take value b time < 2n + 1, cannot change further,
variables vi1
x
since pre-conditions operators (1)(3), = 1, (4)(6), [2..k],
1 = b . Thus, possible outcome two variables
compatible vi1
x
p < n following.
1
2 ]
1 ]
]
Qt ()[v(i+1)1
Qt ()[vin
Qt ()[vin
= 2p 1 {ax , bx }
ax
ax
= 2p

bm
(18)

(4)
ax
(19, 21; 20, 22)
ax
(5)
= 2p + 1 {ax , bx } (8; 10)
1
] either ax bx , using operators
Note that, p = n, value Q2p+1 ()[v(i+1)1
1 ,
(5) (6). reader check similar analysis applies variable v11
operators (1)(3) take role operators (4)(6).
Let us summarize previous analysis following table.

t=1
t=2
t=3
t=4
..
.

1
2
1
vi1
vi1
vi2

ax ax ax

ax

..
.

= 2n 2
= 2n 1 ax
= 2n

= 2n + 1





1 v2
2
1
vin
vi(n1)
vi(n1)

ax
ax ax ax
bm
ax
bm
..
.



bm
ax
bm
ax

first row previous table contains initial state planning problem:
variables set ax . leftmost column rightmost column contain values
1 v 2 . Then, values b right column propagated
taken variables vi1


along diagonals using three propagation rules already discussed: value type
yields values type according Rule I; value type b yields value type
clause satisfied Rule II, type b satisfied, Rule III.
applies propagating values first row: since type a,
values top-left triangle type a, according Rule I. Note also longest
diagonal coincides diagonal values qji Definition 5.2.
discussion proceed prove lemma. Let assignment formula
F . existence plan = implied analysis already done
l ], since shown operators used case produce
values Qt [vij
actual changes value.
1 ],
Finally, consider diagonal values qji () j = 1, . . . , n, is, values Q3 ()[vin
5
1
2n+1
1
Q ()[vi(n1) ], . . ., Q
()[vi1 ]. Let j < Ti Case (a), is, first j bits
message , assigned variables x1 , . . . , xj , satisfy clause Ci . Consequently,

2j+1 ()[v 1
1 ], q = Q5 ()[v 1
diagonal values q1i = Q3 ()[vin
2
i(n+1j) ] must
i(n1) ], . . ., qj = Q
696

fiChain Causal Graphs Domains Size 5

bx , according Rule III. contrary, assume j Ti Case (b),
follows qpi = bx p < Ti due Rule III, qpi = ax p = Ti due Rule II,
qpi = ax j p > Ti due Rule I.
Theorem 5.4. exists valid plan solving planning problem P5 (F )
exists assignment satisfies CNF formula F .
Proof. : Lemma 5.3, existence assignment satisfies F implies
admissible plans = satisfy qji () = qji . Since Ti n [1..k], follows
qni = ax , required goal state P5 (F ). plan thus solves P5 (F ).
: Let plan solving planning problem P5 (F ). Since Lemma 4.8 holds
P5 (F ), plan admissible. show contradiction = satisfies F .
Assume not. exists clause Ci satisfied . Thus, Lemma 5.3 implies
1 following execution b .
qji () = bx j [1..n]. particular, value vi1
x
1)=a .
contradicts solving P5 (F ), since bx different goal state goal(vi1
x
Proposition 5.5. Plan existence C5n NP-hard.
Proof. largest variable domains planning problem P5 (F ) variables
2 , [1..k] j [1..n 1], contain 5 values. proof follows immediately
vij
NP-hardness Cnf-Sat, Theorem 5.4, fact produce
planning problem P5 (F ) polynomial time given CNF formula F .

6. Discussion
paper, shown problem determining whether solution plan exists
planning problems class Ckn NP-hard whenever k 5. contrast, Brafman
Domshlak (2003) developed polynomial-time algorithm generating plans solve
planning problems class C2n . said intermediate cases, namely
Ckn k {3, 4}? follows, sketch arguments tractability
cases. Although discussion mostly based intuition gained studying
classes, might prove helpful someone trying determine complexity.
one hand, seems likely us plan existence C4n also NP-hard.
reduction C5n uses one type state variable whose domain larger 4, namely
2 . Finding reduction C4 seems possible, although likely difficult since
vij
n
available options become increasingly restricted state variable domains get smaller.
particular, tried failed find reduction C4n .
Domshlak Dinitz (2001) showed exist planning problems C3n
exponential length minimal solutions. Although often indicates planning class
difficult, imply plan existence intractable. exemplified
Jonsson Backstrom (1998) define class planning problems exponential
length minimal solutions plan existence could checked polynomial time.
present authors (Gimenez & Jonsson, 2008a) showed even plan generation
particular class could done polynomial time, resulting plans given
compact format macros.
second argument favor hardness C3n may multiple ways
transition two values variable. example, consider planning problem
697

fiGimenez & Jonsson

two actions changing value variable v 0 1, namely
= hv = 0, v = 0; v = 1i = hv = 1, v = 0; v = 1i. Since variables 3 values,
possible neither v = 0 v = 1 hold current state. planner would
thus choose whether satisfy v = 0 v = 1. contrast, C2n two
actions could replaced single action hv = 0; v = 1i since one always
applicable. consequence, even minimal plan length bounded planning
problem C3n , may exponentially many plans length (in fact,
main idea behind reductions).
Another observation regards number possible domain transition graphs
state variable. k 2, possible show state variable Ckn may
2
2k (k1) distinct domain transition graphs. words, number graphs grows
exponentially k. particular, state variables C2n 24 = 16 distinct
graphs, number C3n 218 . Although large number possibilities
guarantee hardness, clear expressive power C3n much higher
C2n .
evidence provided suggests C3n significantly harder C2n . However,
sure C3n hard enough intractable. State variables three
values lend well type reduction presented, since
propagating message requires three values. reduction C3n , idea
underlying may message-passing mechanism exploited.
hand, maybe way determine plan existence C3n polynomial time.
algorithm would take consideration multiple (but finite) combinations domain
transition graphs three values, well inherent structure graphs. know
expressive power domain transition graphs 5 values large handle
polynomial time; maybe case using 3 values.

Acknowledgments
work partially funded APIDIS MEC grant TIN2006-15387-C03-03.

Appendix A. C7n NP-hard
appendix, describe modify reduction C11
n resulting
planning problem, call P7 (F ), needs variable domains size 7. reduction previously appeared conference paper (Gimenez & Jonsson, 2008b), without
proof. main idea reduction same, construction used check
assignment satisfies clause Ci involved. Previously, used n variables {vij }j[1 . . n] whose role was, essentially, check whether j-th bit (xj )
propagated message satisfies Ci . modified reduction, variable vij replaced
1 , v 2 , v 3 , collectively play role. variables
three variables vij
ij
ij
s1 , . . . , s2n1 , vs , , e1 , . . . , e2n1 , well domains corresponding operators,
3 .
before, except predecessor vkn
1 ) = D(v 3 ) = {a , , , b , b , b , g }
domains new variables D(vij
x 0 1 x 0 1 x
ij
2
D(vij ) = {gx , g0 , g1 , ax , a0 , a1 , bx } [1..k], j [1..n]. initial state
1 ) = init(v 2 ) = init(v 3 ) = , [1..k] j [1..n], goal
variables init(vij
x
ij
ij
698

fiChain Causal Graphs Domains Size 5

Variable
1
v11

1,
vi1
[2..k]

1,
vij
[1..k],
j [2..n]

Ref.
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
(11)
(12)
(13)
(14)
(15)
(16)
(17)

Operator
1 = ; v1 = g
hvs = 1, v11
x
x 11
1 = ; v1 = b
hvs = 1, v11
x 11
1
1 = ; v1 = g
hvs = 0, v11
x
x 11
1 = ; v1 = b
hvs = 0, v11
x 11
0
1 = g ; v1 = b
hvs = m, v11
x 11

1 = b ; v1 = b
hvs = m, v11
x 11

1 = b ; v1 = b
hvs = x, v11
11
x
3
1 = ; v1 = g
hv(i1)n
{a1 , b1 }, vi1
x
x i1
1 = ; v1 = b
3
{a1 , b1 }, vi1
hv(i1)n
x i1
1
1
1
3
hv(i1)n {a0 , b0 }, vi1 = ax ; vi1 = gx
3
1 = ; v1 = b
hv(i1)n
{a0 , b0 }, vi1
x i1
0
3
1 = g ; v1 = b
hv(i1)n
{am , bm }, vi1
x i1

3
1 = b ; v1 = b
hv(i1)n
{am , bm }, vi1
x i1

1
1
3
hv(i1)n {ax , bx }, vi1 = bm ; vi1 = bx
3
1 = ; v1 = g
hvi(j1)
= b1 , vij
x ij
x
1
1
3
hvi(j1) = b1 , vij = ax ; vij = b1
3
1 = ; v1 = g
hvi(j1)
= b0 , vij
x ij
x
1 = ; v1 = b
3
= b0 , vij
hvi(j1)
x ij
0
3
1 = ; v1 = g
hvi(j1)
= gx , vij
x ij
x
1
1
3
hvi(j1) = , vij = ax ; vij =
3
1 = g ; v1 = b
hvi(j1)
= bm , vij

x ij
3
1 = b ; v1 = b
hvi(j1) = bm , vij
x ij

3
1 = ; v1 =
hvi(j1)
{ax , bx }, vij
x
ij
1 = b ; v1 = b
3
= bx , vij
hvi(j1)
ij
x

Qualifier
x1 C1
x1
/ C1
x1 C1
x1
/ C1
{0, 1}
{0, 1}
{0, 1}
x1 Ci
x1
/ Ci
x1 Ci
x1
/ Ci
{0, 1}
{0, 1}
{0, 1}
xj Ci
xj
/ Ci
xj Ci
xj
/ Ci
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}

1 , [1..k] j [1..n].
Table 7: Operators variables vij

2 ) = g , [1..k]. Table 7 shows operators variables v 1 , [1..k]
state goal(vin
x
ij
2 v 3 , [1..k]
j [1..n], Table 8 shows operators variables vij
ij
j [1..n]. Figures 7 8 shows corresponding domain transition graphs. Table 8 also
shows new operators variable , different pre-conditions
3 .
predecessor vkn

A.1 Intuition
intuition behind reduction C7n largely C11
n . planning
problem P7 (F ) corresponding CNF formula F consists three parts, first
third identical P11 (F ). Thus, difference lies second part. Recall
reduction C11
n , clause Ci variable xj F , planning
problem P11 (F ) contains state variable vij performs following functions:
1. Propagate message generated vs .
699

fiGimenez & Jonsson

Variable
2,
vij
[1..k],
j [1..n]

Ref.
(18)
(19)
(20)
(21)
(22)
(23)
(24)
(25)
(26)
(27)
(28)
(29)
(30)
(31)

3,
vij
[1..k],
j [1..n]



Operator
1 {a , b }, v 2 = ; v 2 =
hvij

x ij

ij
1 = g , v2 = ; v2 = g
hvij
x
x ij
x ij
1 = b , v2 = g ; v2 = g
hvij
ij
x ij

1 = b , v2 = b ; v2 =
hvij

x ij
ij
1 = , v2 = ; v2 =
hvij
x ij
ij
x
1 = b , v2 = ; v2 = b
hvij
x
ij
x ij
1 = b , v2 = g ; v2 = g
hvij
x ij
ij
x
2 = , v3 = ; v3 =
hvij
ij
x ij

2 = g , v3 = ; v3 = g
hvij
x
x ij
x ij
2 = g , v3 = g ; v3 = b
hvij
ij
x ij

2 {a , g }, v 3 = b ; v 3 = b
hvij

x ij

ij
2 = , v3 = ; v3 =
hvij
x ij
ij
x
3
3
2
hvij = bx , vij = ; vij = bx
2 {b , g }, v 3 = b ; v 3 = b
hvij
x
x x
ij
ij
3
hvkn {a0 , a1 , b0 , b1 }, = 0; = 1i
3 {a , b }, v = 1; v = 0i
hvkn
x x
e
e

Qualifier
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}
{0, 1}

2 , v 3 , v , [1..k] j [1..n].
Table 8: Operators variables vij
e
ij

(a)

(b)

(c)

b0

b0

a0

0
ax

0

0
bx

gx
1

x

ax
1

1

a0
a0,b0 b0

a0,b0

x

gx

a1, b1

a1, b1

ax
bx

b0

bx

ax

gx

a1
b1

ax
bx

b1
ax,bx

b1

b0
ax,bx

a0

b1

b0

b0

bx
bx

gx
b1

bx

b1

a1
a1

b1

1 , (b) v 1 [2..k], (c) v 1 [1..k], j [2..n].
Figure 7: DTGs (a) v11
ij
i1

2. Check whether assignment xj (the j-th bit m) satisfies clause Ci .
3. Remember whether Ci satisfied assignment xl , l j.
4. j < n Ci satisfied, propagate fact.
5. j < n, let vi(j+1) know (j + 1)-th bit message arrived.
first fourth function propagate information thus performed
state variables information lost. However, functions
performed different state variables. idea behind reduction C7n split vij
1 , performs second function, v 2 , performs third,
three variables: vij
ij
3
vij , performs fifth.
700

fiChain Causal Graphs Domains Size 5

(a)

(b)
a0

g0
b0

a0
bx

bx
a0,b0
gx

gx

g1

b0

ax

bx

ax
a1,b1

b1

bx

ax

a1

a0

ax

b0

bx

g0
a0,g0

ax

bx

gx

bx,gx
bx

gx
bx,gx

ax

b1

a1
a1

bx

a1,g1

g1
b1

2 (b) v 3 [1..k], j [1..n].
Figure 8: DTGs (a) vij
ij

before, message propagated using subscripts values domains
1 moves
state variables. j-th bit mj message arrives, state variable vij
1 moves
ax gx assignment (xj ) = mj satisfies Ci , bmj otherwise. vij
gx , forced move bmj next, forgetting Ci satisfied. However,
1 g , subsequent state variables C also move g , propagating
value vij
x

x
2 able remember
fact Ci satisfied. Consequently, state variable vin
Ci satisfied remaining within subdomain {g0 , g1 , gx }.
1 moves b , causing v 2 v 3 move .
(xj ) = mj satisfy Ci , vij
mj
mj
ij
ij
1
3
2
1
there, vij , vij , vij move bx . next bit arrives, vij moves b0 (b1 ),
2 move (a ) v 3 b (b ). indicates v 1
causing vij
0
1
0
1
ij
i(j+1) (j + 1)-th
bit arrived, causing act accordingly. before, operators defined
1 always reacts first bit clause C .
vi1

A.2 Formal Proof
Since variables s1 , . . . , s2n1 , vs , , e1 , . . . , e2n1 before, Lemmas 4.2
4.6 apply P7 (F ). However, Lemma 4.3 violated since sometimes possible
change value variable twice without changing value predecessor (e.g.
using operators (1) (5)). Consequently, Lemma 4.8, states plans
solve P11 (F ) admissible, longer holds P7 (F ).
l ) variables middle
prove equivalent lemmas P7 (F ), redefine (vij
causal graph:
l , [1..k], j [1..n],
Definition A.1. Given partial plan variable vij
l ) number subscript changes v l execution .
l {1, 2, 3}, let (vij
ij
l , [1..k], j [1..n],
Lemma A.2. partial plan P7 (F ) vij
l ) (v ), v predecessor v l causal
l {1, 2, 3}, holds (vij
ij
graph.
l . operator
Proof. Follows immediately inspection operators vij
l
changes subscript vij z {0, 1, x} pre-condition v subscript z (or
1 predecessor v ). operators changing value
value z case v11

1 g pre-condition v subscript (or value) different x,
vij
x
1 since pre-condition v 1 .
operators change subscript vij
x
ij

701

fiGimenez & Jonsson

3 ).
Lemma A.3. partial plan P7 (F ), (ve ) (vkn
3 ) denotes
Proof. Note (ve ) still denotes number value changes , (vkn
3 . time change value v need
number subscript changes vkn
e
3
change subscript vkn between. addition, first value change requires
3 different initial state. Thus, (v ) (v 3 ).
subscript vkn
e
kn

Definition A.4. admissible plan planning problem P7 (F ) partial plan
1 ) = . . . = (v 3 ) = (v ) = 2n, (e ) = 2n i,
(si ) = i, (vs ) = (v11
e

kn
[1..2n 1].
Lemma A.5. plan solves planning problem P7 (F ) admissible.
1 ) (v 3 ) (v ).
Proof. Lemmas A.2 A.3 (vs ) (v11
e
kn
use Lemmas 4.2 4.6 apply reasoning proof Lemma
4.8.
l exactly 2n
words, admissible plan change subscript vij
l extra time moving g . However,
times, although change value vij
x
l ), cannot prove equivalent Lemma 4.10
even new definition (vij
l , l {1, 2}, choose follow predecessor g without
P7 (F ), since variable vij
x
making plan inadmissible. Consequently, sequences values Qti () admissible
plan longer completely determined induced message . Nevertheless,
still prove lemma similar Lemma 4.13.

Definition A.6. Let admissible plan. clause Ci [1..2n + 1],
let sequence values Qti () vector 3n elements representing, variable
l , j [1..n] l {1, 2, 3}, first value following (t 1)-th subscript change
vij
l execution .
vij
Lemma A.7. Let assignment variables x1 , . . . , xn formula F .
1) Existence. exists admissible plan planning problem P7 (F ) induced
assignment = .
2) Claim. Let Qti sequences values described Part 3) lemma.
satisfies F , exists admissible plan = Qti () = Qti ,
[1..2n+1] [1..k]. satisfy clause Ci , admissible
plans = Qti () = Qti , [1..2k + 1].
3) Sequence values. sequence values Qti , [1..k] [1..2n + 1],
follows.
a) j < Ti ,
j1

Qi2j1
Q2j

Q2j+1


nj
}|
{
z
z
}|
{
bx bx bx bx bx bx
ax ax ax
ax ax ax ax ax ax
=
= bm bm bm bm bm
=
bx bx bx bx bx bx
bx bx bx
ax ax ax ax ax ax

702

fiChain Causal Graphs Domains Size 5

b) j = Ti ,
j1

Qi2j1
Q2j

2j+1
Qi

nj
z
}|
{
z
}|
{
=
bx bx bx bx bx bx
ax ax ax
ax ax ax ax ax ax
= bm bm bm bm bm gm bm bm gm bm bm gm bm
=
bx bx bx bx bx bx
bx gx bx
bx gx bx bx gx bx

c) j > Ti ,
jTi

Ti 1

Qi2j1
Q2j

2j+1
Qi

nj

z
z
}|
{
}|
{
}|
{
z
bx gx bx bx gx bx bx gx bx bx gx bx bx gx bx
= bx bx bx bx bx bx
= bm bm bm bm bm gm bm bm gm bm bm gm bm bm gm bm bm gm bm
= bx bx bx bx bx bx
bx gx bx bx gx bx bx gx bx bx gx bx bx gx bx

Proof. Note similarity lemma Lemma 4.13. before, must show
operators, time Tables 7 8, whose post-conditions equal values
2j+1
given Qi2j1 , Q2j
. Again, must check consistency statements
Qi
2j +1
2j1

j = j 1. implies, Lemma 4.13, statements
Qi
Qi
Qi2j1 valid, due initial state ax ax induction j.
2j+1
remains show statements Q2j
also valid.
Qi
proof divided six parts Lemma 4.13. Note that,
contrast lemma, aim show that, satisfies F , exists
admissible plan given Qti , admissible plans form.
sometimes execution plan one operator could chosen,
resulting plan would still admissible. tables follow, alike
proof Lemma 4.13, indicate operator choice leads
desired Qti , use boldface remark operators forced. add
extra row tables indicate sometimes need apply two operators
variable changing subscript. disparities respect Lemma 4.13
occur parts II IV proof, require Ti n, is, satisfying clause
Ci , fixed i. Thus, satisfy clause Ci , admissible plans
sequences values Qti [1..2n + 1].
I) 1 = j < Ti .
1 v2 v3
1 v 2 v 3 |k [2..n]
vi1
vik
i1 i1
ik ik
2j 1 ax ax ax
ax ax ax
2j
bm (2, 4; 18; 25) (13; 18; 25)
ax ax ax (16; 22; 29)
2j + 1 bx bx bx (7; 23; 30)

II) 1 = j = Ti .
1 v2 v3
1 v 2 v 3 |k [2..n]
vi1
vik
i1 i1
ik ik
2j 1 ax ax ax
ax ax ax
gx gx gx (1, 3; 19; 26)
gx gx gx (12; 19; 26)
2j
bm gm bm (5; 20; 27)
bm gm bm (14; 20; 27)
bx gx bx (7; 24; 31)
bx gx bx (17; 24; 31)
2j + 1

703

fiGimenez & Jonsson

III) 1 < j < Ti .
1 v 2 v 3 |k [1..j 1] v 1 v 2 v 3
1 v 2 v 3 |k [j + 1..n]
vik
vik
ij ij ij
ik ik
ik ik
2j 1
b x bx bx
ax ax ax
ax ax ax
2j
bm bm (6, 15; 21; 28) bm (9, 11; 18; 25) (13; 18; 25)
bx bx bx (7, 17; 23; 31)
bx bx bx (17; 23; 30)
ax ax ax (16; 22; 29)
2j + 1

IV) 1 < j = Ti .
1 v 2 v 3 |k [1..j 1] v 1 v 2 v 3
1 v 2 v 3 |k [j + 1..n]
vik
vik
ij ij ij
ik ik
ik ik
2j 1
b x bx bx
ax ax ax
ax ax ax
bm bm (6, 15; 21; 28) gx gx gx (8, 10; 19; 26)
gx gx gx (12; 19; 26)
2j
bm bm
bm gm bm (14; 20; 27)
bm gm bm (14; 20; 27)
2j + 1
bx bx bx (7, 17; 23; 31)
bx gx bx (17; 24; 31)
bx gx bx (17; 24; 31)

V) 1 = Ti < j.
1 v 2 v 3 |k [2..n]
1 v2 v3
vik
vi1
i1 i1
ik ik
2j 1
bx gx bx
bx gx bx
2j
bm gm bm (6; 20; 28) bm gm bm (15; 20; 28)
2j + 1
bx gx bx (7; 24; 31)
bx gx bx (17; 24; 31)

VI) 1 < Ti < j.
1 v 2 v 3 |k [1..T 1] v 1 v 2 v 3 |k [T ..n]
vik


ik ik
ik ik ik
2j 1
bx bx bx
bx gx bx
2j
bm bm (6, 15; 21; 28)
bm gm bm (15; 20; 28)
2j + 1
bx bx bx (7, 17; 23; 31)
bx gx bx (17; 24; 31)

Theorem A.8. exists plan solves planning problem P7 (F )
exists assignment satisfies CNF formula F .
Proof. : Given assignment satisfies F , construct admissible plan whose
induced formula assignment equals , choosing sequence values vs accordingly. follows clause Ci , Ti n, since exists variable xj
(xj ) = mj satisfies Ci . Since n Ti , exists admissible plan Qi2n+1
form indicated Case (b) (c) Lemma A.7. either case, (2n + 1)-th
2 g , required goal state. plan thus solves P (F ).
value variable vin
x
7
: Let plan solves planning problem P7 (F ). Lemma A.5 plan
admissible. show contradiction = satisfies F . Assume not.
exists clause Ci satisfied . Thus, Lemma A.7 applies sequence
2 following execution
values Q2n+1
. particular, means value vin

bx according Case (a) lemma. contradicts solving P7 (F ), since bx
2 )=g .
different goal state goal(vin
x
Proposition A.9. Plan existence C7n NP-hard.
704

fiChain Causal Graphs Domains Size 5

Proof. largest variable domains planning problem P7 (F ) variables
1 , . . . , v 3 , contain 7 values. proof follows immediately NP-hardness
v11
kn
Cnf-Sat, Theorem A.8, fact produce planning problem P7 (F )
polynomial time given CNF formula F .

References
Brafman, R., & Domshlak, C. (2003). Structure Complexity Planning Unary
Operators. Journal Artificial Intelligence Research, 18, 315349.
Brafman, R., & Domshlak, C. (2006). Factored Planning: How, When, Not.
Proceedings 21st National Conference Artificial Intelligence, pp. 809814.
Bylander, T. (1994). computational complexity propositional STRIPS planning.
Artificial Intelligence, 69, 165204.
Chapman, D. (1987). Planning conjunctive goals. Artificial Intelligence, 32(3), 333377.
Chen, H., & Gimenez, O. (2008). Causal Graphs Structurally Restricted Planning.
Proceedings 18th International Conference Automated Planning Scheduling, pp. 3643.
Domshlak, C., & Dinitz, Y. (2001). Multi-Agent Off-line Coordination: Structure Complexity. Proceedings 6th European Conference Planning, pp. 277288.
Erol, K., Nau, D., & Subrahmanian, V. (1995). Complexity, Decidability Undecidability
Results Domain-Independent Planning. Artificial Intelligence, 76(1-2), 7588.
Gimenez, O., & Jonsson, A. (2008a). Complexity Planning Problems Simple
Causal Graphs. Journal Artificial Intelligence Research, 31, 319351.
Gimenez, O., & Jonsson, A. (2008b). Search Tractability Boundary Planning
Problems. Proceedings 18th International Conference Automated Planning
Scheduling, pp. 99106.
Helmert, M. (2006). Fast Downward Planning System. Journal Artificial Intelligence
Research, 26, 191246.
Jonsson, A. (2007). Role Macros Tractable Planning Causal Graphs.
Proceedings 20th International Joint Conference Artificial Intelligence, pp.
19361941.
Jonsson, P., & Backstrom, C. (1998). Tractable plan existence imply tractable
plan generation. Annals Mathematics Artificial Intelligence, 22(34), 281296.
Katz, M., & Domshlak, C. (2008a). New Islands Tractability Cost-Optimal Planning.
Journal Artificial Intelligence Research, 32, 203288.
Katz, M., & Domshlak, C. (2008b). Structural Patterns Heuristics via Fork Decompositions. Proceedings 18th International Conference Automated Planning
Scheduling, pp. 182189.
Knoblock, C. (1994). Automatically generating abstractions planning. Artificial Intelligence, 68(2), 243302.
705

fiGimenez & Jonsson

Williams, B., & Nayak, P. (1997). reactive planner model-based executive.
Proceedings 15th International Joint Conference Artificial Intelligence, pp.
11781185.

706

fiJournal Artificial Intelligence Research 34 (2009) 297-337

Submitted 06/08; published 03/09

Monte Carlo Sampling Methods Approximating
Interactive POMDPs
Prashant Doshi

PDOSHI @ CS . UGA . EDU

Department Computer Science
University Georgia
415 Boyd GSRC
Athens, GA 30602

Piotr J. Gmytrasiewicz

PIOTR @ CS . UIC . EDU

Department Computer Science
University Illinois Chicago
851 S. Morgan St
Chicago, IL 60607

Abstract
Partially observable Markov decision processes (POMDPs) provide principled framework
sequential planning uncertain single agent settings. extension POMDPs multiagent
settings, called interactive POMDPs (I-POMDPs), replaces POMDP belief spaces interactive
hierarchical belief systems represent agents belief physical world, beliefs
agents, beliefs others beliefs. modification makes difficulties obtaining solutions due complexity belief policy spaces even acute.
describe general method obtaining approximate solutions I-POMDPs based particle filtering (PF). introduce interactive PF, descends levels interactive belief
hierarchies samples propagates beliefs level. interactive PF able mitigate belief space complexity, address policy space complexity. mitigate
policy space complexity sometimes also called curse history utilize complementary method based sampling likely observations building look ahead reachability
tree. approach completely address curse history, beats back curses
impact substantially. provide experimental results chart future work.

1. Introduction
Interactive POMDPs (I-POMDPs) (Gmytrasiewicz & Doshi, 2005; Seuken & Zilberstein, 2008)
generalization POMDPs multiagent settings offer principled decision-theoretic
framework sequential decision making uncertain multiagent settings. I-POMDPs applicable autonomous self-interested agents locally compute actions execute
optimize preferences given believe interacting others possibly
conflicting objectives. Though POMDPs used multiagent settings,
strong assumption agents behavior adequately represented implicitly (say,
noise) within POMDP model (see Boutilier, Dean, & Hanks, 1999; Gmytrasiewicz & Doshi,
2005, examples). approach adopted I-POMDPs expand traditional state space
include models agents. models sophisticated intentional models,
ascribe beliefs, preferences, rationality others analogous notion agent
c
2009
AI Access Foundation. rights reserved.

fiD OSHI & G MYTRASIEWICZ

types Bayesian games (Harsanyi, 1967; Mertens & Zamir, 1985). models, finite
state machines, ascribe beliefs rationality agents call subintentional
models. agents beliefs within I-POMDPs called interactive beliefs, nested
analogously hierarchical belief systems considered game theory (Mertens & Zamir, 1985;
Brandenburger & Dekel, 1993; Heifetz & Samet, 1998; Aumann, 1999), theoretical computer
science (Fagin, Halpern, Moses, & Vardi, 1995) hyper-priors hierarchical Bayesian
models (Gelman, Carlin, Stern, & Rubin, 2004). Since interactive beliefs may infinitely
nested, Gmytrasiewicz Doshi (2005) defined finitely nested I-POMDPs computable specializations infinitely nested ones. Solutions finitely nested I-POMDPs map agents states
belief environment agents models policies. Consequently, I-POMDPs
find important applications agent, human, mixed agent-human environments. potential
applications include path planning multi-robot environments, coordinating troop movements
battlefields, planning course treatment multi-treatment therapy, explaining commonly observed social behaviors (Doshi, Zeng, & Chen, 2007).
However, optimal decision making uncertain multiagent settings computationally hard
requiring significant time memory resources. example, problem solving decentralized POMDPs shown lie NEXP-complete class (Bernstein, Givan, Immerman, &
Zilberstein, 2002). Expectedly, exact solutions finitely nested I-POMDPs difficult compute
well, due two primary sources intractability: (i) complexity belief representation proportional dimensions belief simplex, sometimes called curse
dimensionality. (ii) complexity space policies, proportional number
possible future beliefs, also called curse history.
sources intractability exist POMDPs also (see Pineau, Gordon, & Thrun, 2006;
Poupart & Boutilier, 2004) curse dimensionality especially acute I-POMDPs.
I-POMDPs complexity belief space even greater; beliefs may
include beliefs physical environment, possibly agents beliefs agents
beliefs, beliefs others, on. Thus, contributing factor curse
dimensionality level belief nesting considered. total number agent models
grows exponentially increase nesting level, solution complexity.
observe one approach solving finitely nested I-POMDP investigate collapsing model traditional POMDP, utilize available approximation methods apply
POMDPs. However, transformation POMDP straightforward. particular,
seem possible model update agents nested beliefs part transition function POMDP. transition function would include nested beliefs require solutions
others models defining it, thus quite different standard ones current
POMDP approaches apply.
article, present first set generally applicable methods computing approximately optimal policies finitely nested I-POMDP framework demonstrating computational savings. Since agents belief defined agents models, may complex
infinite space, sampling methods able approximate distributions large spaces arbitrary accuracy promising approach. adopt particle filter (Gordon, Salmond, & Smith,
1993; Doucet, Freitas, & Gordon, 2001) point departure. growing empirical evidence (Koller & Lerner, 2001; Daum & Huang, 2002) particle filters unable significantly
reduce adverse impact increasing state spaces. Specifically, number particles needed
maintain error exact state estimation increases number dimensions increase.
298

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

However, rate convergence approximate posterior true one independent
dimensions state space (Crisan & Doucet, 2002) weak assumptions. words,
may need particles maintain error state space increases, rate
error reduces remains unchanged, regardless state space. Furthermore, sampling approaches
allow us focus resources regions state space considered likely
uncertain environment, providing strong potential computational savings.
generalize particle filter, specifically bootstrap filter (Gordon et al., 1993),
multiagent setting, resulting interactive particle filter (I-PF). generalization
trivial: simply treat agent automaton whose actions follow fixed
known distribution. Rather, consider case agents intentional possess beliefs, capabilities preferences. Subsequently, propagation step I-PF becomes
complicated standard PF. projecting subject agents belief time,
must project agents belief, involves predicting action anticipating observations. Mirroring hierarchical character interactive beliefs, interactive particle filtering
involves sampling propagation hierarchical levels beliefs. empirically
demonstrate ability I-PF flexibly approximate state estimation I-POMDPs,
show computational savings obtained comparison regular grid based implementation.
However, sample identical number particles nesting level, total number
particles associated complexity, continues grow exponentially nesting level.
combine I-PF value iteration sample sets thereby providing general way
solve finitely nested I-POMDPs. approximation method anytime applicable agents
start prior belief optimize finite horizons. Consequently, method finds applications online plan computation. derive error bounds approach applicable
singly-nested I-POMDPs discuss difficulty generalizing bounds multiply nested
beliefs. empirically demonstrate performance computational savings obtained
method standard test problems well larger uninhabited aerial vehicle (UAV) reconnaissance problem.
I-PF able flexibly mitigate belief space complexity, address policy space complexity. order mitigate curse history, present complementary method
based sampling observations building look ahead reachability tree value iteration. translates considering future beliefs value iteration agent
likely given belief. approach similar spirit sparse sampling
techniques used generating partial look ahead trees action selection reinforcement
learning (Kearns, Mansour, & Ng, 2002; Wang, Lizotte, Bowling, & Schuurmans, 2005)
online planning POMDPs (Ross, Pineau, Paquet, & Chaib-draa, 2008). approaches
applied single agent reinforcement learning problems, focus multiagent setting
recursively apply technique solve models agents nesting level. Observation sampling also recently utilized DEC-POMDPs (Seuken & Zilberstein, 2007),
shown improve performance large problems. note approach
completely address curse history, beats back impact difficulty computing
I-POMDP solutions, substantially. report additional computational savings obtained
combine method I-PF, provide empirical results support.
Rest article structured following manner. review various state estimation methods relevance, use particle filters previous works Section 2.
Section 3, review traditional particle filtering technique concentrating bootstrap filters
299

fiD OSHI & G MYTRASIEWICZ

particular. briefly outline finitely nested I-POMDP framework Section 4 multiagent tiger problem used illustration Section 5. Section 6, discuss representations
nested beliefs inherent difficulty formulating them. order facilitate understanding,
give decomposition I-POMDP belief update Section 7. present I-PF
approximates finitely nested I-POMDP belief update Section 8. followed method
utilizes I-PF compute solutions I-POMDPs, Section 9. also comment
asymptotic convergence compute error bounds approach. Section 10, report
performance approximation method simple larger test problems. Section 11,
provide technique mitigating curse history, report empirical results.
Finally, conclude article outline future research directions Section 12.

2. Related Work
Several approaches nonlinear Bayesian estimation exist. Among these, extended Kalman filter (EKF) (Sorenson, 1985), popular. EKF linearises estimation problem
Kalman filter applied. required probability density function (p.d.f.) still approximated
Gaussian, may lead filter divergence, therefore increase error.
approaches include Gaussian sum filter (Sorenson & Alspach, 1971), superimposing grid
state space belief evaluated grid points (Kramer & Sorenson,
1988). latter approach, choice efficient grid non-trivial, method suffers
curse dimensionality: number grid points must considered exponential
dimensions state space. Recently, techniques utilize Monte Carlo (MC) sampling
approximating Bayesian state estimation problem received much attention. techniques general enough, that, applicable linear, well as, non-linear problem
dynamics, rate convergence approximation error zero independent dimensions underlying state space. Among spectrum MC techniques, two
particularly well-studied sequential settings Markov chain Monte Carlo (MCMC) (Hastings,
1970; Gelman et al., 2004), particle filters (Gordon et al., 1993; Doucet et al., 2001). Approximating I-POMDP belief update using former technique, may turn computationally
exhaustive. Specifically, MCMC algorithms utilize rejection sampling (e.g. Hastings, 1970)
may cause large number intentional models sampled, solved, rejected, one
utilized propagation. addition, complex estimation process I-POMDPs makes
task computing acceptance ratio rejection sampling computationally inefficient. Although
Gibbs sampling (Gelman et al., 2004) avoids rejecting samples, would involve sampling
conditional distribution physical state given observation history model other,
distribution others model given physical state. However, distributions
neither efficient compute easy derive analytically. Particle filters need reject solved
models compute new model replacement, propagating solved models time
resampling them. intuitively amenable approximating I-POMDP belief update
produce reasonable approximations posterior computationally feasible.
Particle filters previously successfully applied approximate belief update
continuous state space single agent POMDPs (Thrun, 2000; Poupart, Ortiz, & Boutilier, 2001).
Thrun (2000) integrates particle filtering Q-learning learn policy, Poupart et
al. (2001) assume prior existence exact value function present error bound analysis substituting POMDP belief update particle filters. Loosely related work
300

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

sampling algorithms appear (Ortiz & Kaelbling, 2000) selecting actions influence
diagrams, work focus sequential decision making. multiagent setting, particle filters employed collaborative multi-robot localization (Fox, Burgard, Kruppa,
& Thrun, 2000). application, emphasis predicting position robot,
actions robots, critical step approach. Additionally, facilitate fast localization, beliefs robots encountered motion considered fully
observable enable synchronization.
Within POMDP literature, approaches sampling methods also appeared
address curse dimensionality. important class algorithms prescribe substituting
complex belief space simpler subspace (Bertsekas, 1995; Tsitsiklis & Roy, 1996; Poupart
& Boutilier, 2003; Roy, Gordon, & Thrun, 2005). premise methods beliefs distributions physical states contain information required order
plan near-optimally. Poupart Boutilier (2003) use Krylov subspaces (Saad, 1996) directly
compress POMDP model, analyze effect compression decision quality.
ensure lossless compression, i.e. decision quality compressed belief compromised, transition reward functions must linear. Roy et al. (2005) proposed using principal
component analysis (Collins, Dasgupta, & R.E.Schapire, 2002) uncover low dimensional belief
subspace usually encompasses robots potential beliefs. method based observation beliefs along many real-world trajectories exhibit degrees freedom.
effectiveness methods problem specific; indeed, possible encounter problems
substantial belief compression may occur. applied I-POMDP framework,
effectiveness compression techniques would depend, example, existence agent
models whose likelihoods within agents belief change successive belief updates
existence correlated agent models. Whether models exist practice topic
future work.
Techniques address curse history POMDPs also exist. Poupart Boutilier (2004)
generate policies via policy iteration using finite state controllers bounded number nodes.
Pineau et al. (2006) perform point-based value iteration (PBVI) selecting small subset reachable belief points step belief simplex planning belief points.
Doshi Perez (2008) outline challenges develop PBVI I-POMDPs. Though
method mitigating curse history conceptually close point based selection methods,
focus plan computation initial belief known previously mentioned methods
typically utilized offline planning. approximate way solving POMDPs online
RTBSS approach (Paquet, Tobin, & Chaib-draa, 2005; Ross et al., 2008) adopts branch-andbound technique pruning look ahead reachability tree. approach focuses selecting
best action expand complementary approach sampling observations.
Further, extension multiagent setting formalized I-POMDPs may trivial due
need bounding heuristic function whose formulation multiagent settings remains
investigated.

3. Background: Particle Filter Single Agent Setting
act rationally uncertain settings, agents need track evolution state time,
based actions perform available observations. single agent settings,
state estimation usually accomplished technique called Bayes filter (Russell & Norvig,
301

fiD OSHI & G MYTRASIEWICZ

2003). Bayes filter allows agent maintain belief state world given
time, update belief time action performed new sensory information arrives.
convenience approach lies fact update independent past percepts
action sequences. agents belief sufficient statistic: fully summarizes
information contained past actions observations.
operation Bayes filter decomposed two-step process:
Prediction: agent performs new action, at1 , prior belief state updated:
Z
t1 t1
P r(s |a , b ) =
bt1 (st1 )T (st |st1 , at1 )dst1
(1)
st1

Correction: Thereafter, observation, ot , received, intermediate belief state,
P r(|at1 , bt1 ), corrected:
P r(st |ot , at1 , bt1 ) = O(ot |st , at1 )P r(st |at1 , bt1 )

(2)

normalizing constant, transition function gives uncertain effect
performing action physical state, observation function gives
likelihood receiving observation state performing action.
Particle filters (PF) (Gordon et al., 1993; Doucet et al., 2001) specific implementations
Bayes filters tailored toward making Bayes filters applicable non-linear dynamic systems.
Rather sampling directly target distribution often difficult, PFs adopt
method importance sampling (Geweke, 1989), allows samples drawn
tractable distribution called proposal distribution, . example, P r(S |ot , at1 , bt1 )
target posterior distribution, (S |ot , at1 , bt1 ) proposal distribution, support
(S |ot , at1 , bt1 ) includes support P r(S |ot , at1 , bt1 ), approximate target
posterior sampling N i.i.d. particles {s(n) , n = 1...N } according (S |ot , at1 , bt1 )
assigning particle normalized importance weight:
w(s
e (n) )
P r(s(n) |ot , at1 , bt1 )
w(n) = PN
w(s
e (n) ) =
(s(n) |ot , at1 , bt1 )
e (n) )
n=1 w(s

true probability, P r(s|ot , at1 , bt1 ), approximated by:


t1

P rN (s|o ,

t1

,b

)=

N
X

w(n) (s s(n) )

n=1
a.s.

() Dirac-delta function. N , P rN (s|ot , at1 , bt1 ) P r(s|ot , at1 , bt1 ).
applied recursively several steps, importance sampling leads large variance
weights. avoid degeneracy, Gordon et al. (1993) suggested inserting resampling step,
would increase population particles high importance weights.
beneficial effect focusing particles high likelihood regions supported observations increasing tracking ability PF. Since particle filtering extends importance
sampling sequentially appends resampling step, also called sequential importance
sampling resampling (SISR).
302

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

general algorithm particle filtering technique given Doucet et al. (2001).
concentrate specific implementation algorithm, previously studied
various names MC localization, survival fittest, bootstrap filter. implementation maintains set N particles denoted ebt1 independently sampled prior,
bt1 , takes action observation input. particle propagated forwards time,
using transition kernel environment. particle weighted likelihood
perceiving observation state particle represents, given observation
function O. followed (unbiased) resampling step, particles picked proportionately weights, uniform weight subsequently attached particle.
outline algorithm bootstrap filter Fig. 1. Crisan Doucet (2002) outline rigorous
proof convergence algorithm toward true posterior N .
Function PARTICLEFILTER(ebt1 , at1 , ot ) returns ebt
1. ebtmp , ebt
Importance Sampling
2. s(n),t1 ebt1
3.
Sample s(n),t (S |at1 , s(n),t1 )
4.
Weight s(n),t importance weight:
w
e(n) = O(ot |s(n),t , at1 )

5. ebtmp (s(n),t , w
e(n) )
P
(n) = 1
6. Normalize w
e(n) N
n=1 w
Selection
7. Resample replacement N particles {s(n),t , n = 1...N }
set ebtmp according importance weights.
8. ebt {s(n),t , n = 1...N }
9. return ebt
end function
Figure 1: particle filtering algorithm approximating Bayes filter.
Let us understand working PF context simple example single agent
tiger problem (Kaelbling, Littman, & Cassandra, 1998). single agent tiger problem resembles
game show agent choose open one two doors behind lies either
valuable prize dangerous tiger. Apart actions open doors, subject option
listening tigers growl coming left, right door. However, subjects hearing
imperfect, given percentages (say, 15%) false positive false negative occurrences.
Following Kaelbling et al. (1998), assume value prize 10, pain associated
encountering tiger quantified -100, cost listening -1.
Let agent prior belief according uninformed location
tiger. words, believes probability 0.5 tiger behind left door (TL),
similar probability tiger behind right door (TR). see agent
approximately updates belief using particle filter when, say, listens (L) hears growl
left (GL). Fig. 2 illustrates particle filtering process. Since agent uninformed
tigers location, start equal number particles (samples) denoting TL (lightly
303

fiD OSHI & G MYTRASIEWICZ

GL
~t-1
bi

~ tmp
bi

Propagate

Weight

~t
bi

Resample

L
Correction step

Prediction step

Figure 2: Particle filtering state estimation single agent tiger problem. light dark
particles denote states TL TR respectively. particle filtering process consists
three steps: Propagation (line 3 Fig. 1), Weighting (line 4), Resampling (line 7).

shaded) TR (darkly shaded). initial sample set approximately representative agents
prior belief 0.5. Since listening change location tiger, composition
sample set remains unchanged propagation. hearing growl left, light particles
denoting TL tagged larger weight (0.85) likely responsible
GL, dark particles denoting TR (0.15). Here, size particle proportional
weight attached particle. Finally, resampling step yields sample set time step t,
contains particles denoting TL TR. sample set approximately represents
updated belief 0.85 agent tiger behind left door. Note propagation
carries task prediction shown Eq. 1 approximately, correction step (Eq. 2)
approximately performed weighting resampling.

4. Overview Finitely Nested I-POMDPs
I-POMDPs (Gmytrasiewicz & Doshi, 2005) generalize POMDPs handle multiple agents.
including models agents state space. focus finitely nested I-POMDPs
here, computable counterparts I-POMDPs general. simplicity presentation
let us consider agent, i, interacting one agent, j. arguments generalize
setting two agents straightforward manner.
Definition 1 (I-POMDPi,l ). finitely nested interactive POMDP agent i, I-POMDPi,l , is:
I-POMDPi,l = hISi,l , A, Ti , , Oi , Ri
where:
ISi,l set interactive states defined ISi,l = Mj,l1 , l 1, ISi,0 = S,1
set states physical environment, Mj,l1 set possible models agent j.
1. agents participating interaction, K > 2, ISi,l = K1
j=1 Mj,l1

304

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

model, mj,l1 Mj,l1 , defined triple, mj,l1 = hhj , fj , Oj i, fj : Hj (Aj )
agent js function, assumed computable, maps possible histories js observations, Hj ,
distributions actions. hj element Hj , Oj function, also computable,
specifying way environment supplying agent input. simplicity, may
write model mj,l1 mj,l1 = hhj ,
b j i,
b j consists fj Oj .

specific class models (l 1)th level intentional models, j,l1 , agent j: j,l1 =
hbj,l1 , A, j , Tj , Oj , Rj , OCj i. bj,l1 agent js belief nested level l1, bj,l1 (ISj,l1 ),
OCj js optimality criterion. Rest notation standard. may rewrite j,l1 as,
b j includes elements intentional model
j,l1 = hbj,l1 , bj i, bj
belief called agent js frame. intentional models analogous types used
Bayesian games (Harsanyi, 1967).
mentioned Gmytrasiewicz Doshi (2005), may also ascribe subintentional
models, SMj , constitute remaining models Mj,l1 . Examples subintentional models
finite state controllers fictitious play models (Fudenberg & Levine, 1998).
consider models here, could accommodated straightforward manner.
order promote understanding, let us define finitely nested interactive state space
inductive manner:
ISi,0 = S,
j,0 = {hbj,0 , bj : bj,0 (ISj,0 ), = Aj },
ISi,1 = j,0 ,
j,1 = {hbj,1 , bj : bj,1 (ISj,1 )},
ISi,l

.
.
.
= j,l1 , j,l

.
.
.
= {hbj,l , bj : bj,l (ISj,l )}.

Recursive characterizations state spaces analogous appeared previously
game-theoretic literature (Mertens & Zamir, 1985; Brandenburger & Dekel, 1993; Battigalli &
Siniscalchi, 1999) led definitions hierarchical belief systems.
proposed mathematical formalizations type spaces Bayesian games. Additionally,
nested beliefs are, general, analogous hierarchical priors utilized Bayesian analysis
hierarchical data (Gelman et al., 2004). Hierarchical priors arise unknown priors assumed
drawn population distribution, whose parameters may unknown thereby
motivating higher level prior.
= Ai Aj set joint moves agents.
Ti transition function, Ti : [0, 1] describes results agents actions
physical states world. (It assumed actions directly change physical state
only, see Gmytrasiewicz & Doshi, 2005).
set agent observations.
Oi observation function, Oi : [0, 1] gives likelihood perceiving
observations state resulting performing action. (It assumed physical
state directly observable, models agent.)
Ri defined as, Ri : ISi R. agent allowed preferences physical
states models agents, usually physical state matter.
305

fiD OSHI & G MYTRASIEWICZ

4.1 Belief Update
Analogous POMDPs, agent within I-POMDP framework also updates belief acts
observes. However, two differences complicate belief update multiagent
settings, compared single agent ones. First, since state physical environment
depends actions performed agents, prediction physical state changes
made based predicted actions agent. probabilities others actions
obtained based models. Second, changes models agent
included update. Specifically, since agents model intentional update
agents beliefs due new observation included. words, agent
update beliefs based anticipates agent observes updates.
belief update function agent finitely nested I-POMDP framework is:
R

bti (ist ) =

ist1 :bjt1 =bjt

t1 )
bt1
i,l (is

P

at1
j

t1
t1 , ot ) (st1 , at1 , st )
P r(at1


j |j,l1 ) Oi (s ,

P
t1
t1 , ot ) ist1

(SEbt (bt1
j
j,l1 , aj , oj ) bj,l1 ) Oj (s ,
otj

j

(3)
normalization constant, Dirac-delta function, SEbt () abbreviation
j

t1
t1
denoting belief update, P r(at1
Bayes rational
j |j,l1 ) probability aj
t1
agent described j,l1 .
j also modeled I-POMDP, belief update invokes js belief update (via term
t1
SEbt (bt1
j,l1 , aj , oj )), turn invokes belief update on. recursion belief
j

nesting bottoms 0th level. level, belief update agent reduces POMDP
based belief update. 2 illustration belief update, additional details I-POMDPs,
compare multiagent planning frameworks, see (Gmytrasiewicz & Doshi, 2005).
manner similar belief update POMDPs, following proposition holds
I-POMDP belief update. proposition results noting Eq. 3 expresses belief
terms parameters previous time step only. complete proof belief update
proposition given Gmytrasiewicz Doshi (2005).

Proposition 1. (Sufficiency) finitely nested I-POMDPi,l agent i, current belief, i.e.,
probability distribution set j,l1 , sufficient statistic past history
observations.
4.2 Value Iteration
level l belief state I-POMDPi,l associated value reflecting maximum payoff
agent expect belief state:

R

ERi (is, ai )bi,l (is)d is+
U (hbi,l , bi i) = max
ai Ai
isISi,l

(4)
P
t1
b

P r(oi |ai , bi,l )U (hSEbi (bi,l , ai , oi ), i)
oi

2. 0th level model POMDP: agents actions treated exogenous events folded T, O, R.

306

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

P
where, ERi (is, ai ) = aj Ri (is, ai , aj )P r(aj |j,l1 ) (since = (s, j,l1 )).
Eq. 4 basis value iteration I-POMDPs, succinctly rewritten U =
HU t1 , H commonly known value backup operator. Analogous POMDPs, H
isotonic contracting, thereby making value iteration convergent (Gmytrasiewicz &
Doshi, 2005).
Agent optimal action, ai , case finite horizon discounting, element
set optimal actions belief state, OP (i ), defined as:

OP (hbi,l , bi i) = argmax
ai Ai



R

ERi (is, ai )bi,l (is)d is+

isISi,l



P

oi


b
P r(oi |ai , bi,l )U (hSEbi (bi,l , ai , oi ), i)

(5)

5. Example: Multiagent Tiger Problem
illustrate approximation methods, utilize multiagent tiger problem example.
multiagent tiger problem generalization single agent tiger problem outlined Section 3
multiagent setting. sake simplicity, restrict two-agent setting,
problem extensible agents straightforward way.
two-agent tiger problem, agent may open doors listen. make interaction
interesting, addition usual observation growls, added observation door
creaks, depends action executed agent. Creak right (CR) likely due
agent opened right door, similarly creak left (CL). Silence (S) good
indication agent open doors listened instead. assume accuracy
creaks 90%, accuracy growls 85% before. Again, tiger location chosen
randomly next time step agents opened doors current step. also
assume agents payoffs analogous single agent version. Note result
assumption agents actions impact original agents payoffs directly,
rather indirectly resulting states matter original agent. Table 1 quantifies
factors.
agent makes choice multiagent tiger problem, may find useful consider
believes location tiger, well whether agent listen
open door, turn depends agents beliefs, preferences capabilities.
particular, agent open doors, tigers location next time
step would chosen randomly. information agent tigers location till
then, would reduce zero. simplify situation somewhat assuming agent
js properties, except beliefs, known i, js time horizon equal is.
words, uncertainty pertains js beliefs frame.

6. Representing Prior Nested Beliefs
mentioned, infinity intentional models agent. Since agent unaware
true models interacting agents ex ante, must maintain belief possible candidate
models. complexity space precludes practical implementations I-POMDPs
307

fiD OSHI & G MYTRASIEWICZ

hai , aj
hOL,
hOR,
h, OLi
h, ORi
hL, Li
hL, Li

State
*
*
*
*
TL
TR

TL
0.5
0.5
0.5
0.5
1.0
0

TR
0.5
0.5
0.5
0.5
0
1.0

hai , aj
hOR, ORi
hOL, OLi
hOR, OLi
hOL, ORi
hL, Li
hL, ORi
hOR, Li
hL, OLi
hOL, Li

Transition function: Ti = Tj

TL
10
-100
10
-100
-1
-1
10
-1
-100

TR
-100
10
-100
10
-1
-1
-100
-1
10

hai , aj
hOR, ORi
hOL, OLi
hOR, OLi
hOL, ORi
hL, Li
hL, ORi
hOR, Li
hL, OLi
hOL, Li

TL
10
-100
-100
10
-1
10
-1
-100
-1

TR
-100
10
10
-100
-1
-100
-1
10
-1

Reward functions agents j

hai , aj
hL, Li
hL, Li
hL, OLi
hL, OLi
hL, ORi
hL, ORi
hOL,
hOR,

State
TL
TR
TL
TR
TL
TR



h GL, CL
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
1/6
1/6

h GL, CR
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
1/6
1/6

h GL,
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
1/6
1/6

h GR, CL
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
1/6
1/6

h GR, CR
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
1/6
1/6

h GR,
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
1/6
1/6

hai , aj
hL, Li
hL, Li
hOL, Li
hOL, Li
hOR, Li
hOR, Li
h, OLi
h, ORi

State
TL
TR
TL
TR
TL
TR



h GL, CL
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
1/6
1/6

h GL, CR
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
1/6
1/6

h GL,
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
1/6
1/6

h GR, CL
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
1/6
1/6

h GR, CR
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
1/6
1/6

h GR,
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
1/6
1/6

Observation functions agents j.
Table 1: Transition, reward, observation functions multiagent tiger problem.

simplest settings. Approximations based sampling use finite set sample points
represent complete belief state.
order sample nested beliefs first need represent them. Agent level 0 belief,
def

bi,0 (S), vector probabilities physical state: bi,0 = h pi,0 (s1 ), pi,0 (s2 ),
. . ., pi,0 (s|S| ) i. first second subscripts bi,0 denote agent level nesting,
P|S|
respectively. Since belief probability distribution, q=1 pi,0 (sq ) = 1. refer constraint
P|S|1
simplex constraint. may write, pi,0 (s|S| ) = 1 q=1 pi,0 (sq ), subsequently,
|S| 1 probabilities needed specify level 0 belief.
tiger problem, let s1 = L s2 = R. example level 0 belief tiger
def

problem, bi,0 = hpi,0 (T L), pi,0 (T R)i, h0.7, 0.3i assigns probability 0.7 L 0.3
R. Knowing pi,0 (T L) sufficient complete specification level 0 belief.
308

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

Agent first level belief, bi,1 (S j,0 ), vector densities js level 0 beliefs,
one combination state js frame possibly distinct other. Hence,
hs,bj

0.506

0.504

0.504

<TR,m>
<TR, >
(p )
j,0
(p

j,0

)

0.506

0.5
0.498

pp i,1i,1

>

0.502

<TL,

p p<TL,m>
(p
(p ) )
j,0
i,1 i,1
j,0

b |
|S||
hs,b
hs,b
j
b j | densities: bi,1 def
|S||
= h pi,1 j 1 , pi,1 j 2 , . . ., pi,1
i, hs, bj ik ,
b j | particular state js frame combination. simplex conk = 1, . . . , |S||
straint, sum integrals level 1 densities level 0 beliefs must 1. observe
level 1 densities may represented using family probability distributions
exponential family (Dobson, 2002) polynomials allow approximation function
arbitrary accuracy. such, densities could exhibit shape given satisfy simplex
constraint.

0.502
0.5
0.498
0.496

0.496

0.494

0.494
0

0.2

0.4
0.6
p
(TL)
j,0

0.8

0

1

0.2

0.4
p

0.6

j,0

0.8

1

0.8

1

(TL)

0.8

0.8

0.7

0.7

<TR, >
pp<TR,m>
(p (p) )
i,1
j,0 j,0
i,1

<TL,m>
> (p )
p<TL,
p
(p )
i,1 i,1
j,0j,0

(a)

0.6
0.5
0.4
0.3
0.2

0.6
0.5
0.4
0.3
0.2
0.1

0.1

0

0
0

0.2

0.4
p

0.6

j,0

0.8

0

1

0.2

0.4
p

(TL)

0.6

j,0

(TL)

(b)

Figure 3: Example level 1 beliefs two-agent tiger problem. (a) According belief,
uninformed js level 0 beliefs. Since marginal plot 0.5, also
unaware location tiger. (b) Agent believes j likely knows location
R 0.5 hT L,b
R 1 hT L,b
tiger ( 0 pi,1
(pj,0 )dpj,0 0.5 pi,1
(pj,0 )dpj,0 ), though unaware
marginal plot 0.5.

def

hT L,b

hT R,b

example level 1 belief i, bi,1 = hpi,1 j , pi,1 j i, tiger problem one according
uninformed js level 0 beliefs location tiger (see Fig. 3(a)).
superscript, bj , agent js frame known i. Another example level 1 belief one
according believes j likely knows location tiger (Fig. 3(b)).
Agent second level belief, bi,2 (S j,1 ), vector densities js level 1 beliefs
state js intentional frame. comparison level 0 level 1 beliefs, representing doubly-nested beliefs beliefs deeper nestings trivial.
distributions density functions whose representations need finite. example, let js
309

fiD OSHI & G MYTRASIEWICZ

singly-nested belief densities represented using family polynomials. Then, doublynested belief js densities vector normalized mathematical functions variables
variables parameters lower-level densities. lower level densities polynomials could degree therefore number coefficients, functions
represent doubly-nested beliefs may indefinite number variables. Thus computable representations level 2 beliefs trivially obtained. formalize observation using
Proposition 2, shows multiply-nested beliefs necessarily partial functions fail
assign probability elements (lower level beliefs) domain.
Proposition 2. Agent multiply nested belief, bi,l , l 2, strictly partial recursive function.
Proof. briefly revisit definition nested beliefs: bi,l (ISi,l ) = (S j,l1 ) = (S
b j i), Bj,l1 level l 1 belief simplex
b j set frames j.
hBj,l1 ,
b j i). state frame spaces
basis case, let l = 2, bi,2 (S hBj,1 ,
discrete, bi,2 may represented using collection density functions js beliefs, one
hs,b

discrete state js frame combination, pi,2 j (bj,1 ), bj,1 Bj,1 . Notice that, bj,1
singly-nested, collection densities js level 0 beliefs, one state
def

hs,b

hs,bi i|S||
b

hs,b

|


frame combination. Thus, mentioned let bj,1 = h pj,1 1 , pj,1 2 , . . ., pj,1
i.
Recall Section 4 models therefore belief density functions assumed

hs,bi i1

computable. Let x program length bits, l(x), encodes, say pj,1
g. define complexity density function,
g(x) =

hs,b
pj,1 1 }.

hs,b
pj,1 1 ,

as:

, language

hs,b
Cg (pj,1 1 )

= min {l(x) :

Cg () minimum length program language g computes argument.3
hs,

observe l(x) proportional number parameters describe pj,1 1 .
number parameters density need bounded, l(x) consequently complexity
density may finite. Intuitively, equivalent saying density could
shape.
hs,bj

Assume, way contradiction, level 2 density function, pi,2
hs,
pi,2 j

total recursive

total, halt
function. Construct Turing machine, , computes it.
inputs. Specifically, read set symbols input tape describe level 1 density
function (the program, x), finished reading halts leaves number 0
1 output tape. number output density function encoded . Note
execute input program x, simply parses enable identification. Thus
universal Turing machine. mentioned previously, minimum length program (and hence
complexity) encodes level l density function may infinite. Thus size set
symbols input tape , l(x), may infinite, may halt. contradiction.
hs,b

Thus, pi,2 j partial recursive function.
argument may extended inductively levels nesting.
multiply-nested beliefs general form partial recursive functions defined every possible lower level belief domain, restrictions complexity
3. Note complexity, Cg , within constant Kolmogorov complexity (Li & Vitanyi, 1997) density
bi i1
hs,

function, pj,1

.

310

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

nested beliefs (where complexity defined Proposition 2) needed allow computability well-defined. One way focus attention limited representations
involving bounded number parameters.
complications, general-purpose language representing nested beliefs
beyond scope article attempt here; topic continuing investigations. Instead, utilize specific examples doubly-nested deeply nested beliefs
experiments remainder article.

7. Decomposing I-POMDP Belief Update
Analogous Eqs. 1 2 Bayes filter, decompose I-POMDP belief update two
steps. decomposition facilitates better understanding belief update, also
plays pivotal role development approximation.
t1
Prediction: agent, say i, performs action at1
, agent j performs aj ,
predicted belief state is:

t1 t1
P r(ist |at1
, aj , bi,l ) =

R

t1
bt1 (ist1 )P r(at1
j |j,l1 )
t1 :bjt1 =bjt i,l
P
t1
t1 t1
Ti (st1 , at1
otj Oj (s , ai , aj , oj )
, aj , )
t1

t1
(SEbt (bt1
j,l1 , aj , oj ) bj,l1 )
j

(6)

Dirac-delta function, SEbt () abbreviation belief update, P r(at1
j
j

t1
t1
| j,l1
) probability at1
Bayes rational agent described j,l1
.
j

Correction: agent perceives observation, oti , corrected belief state weighted
sum predicted belief states possible action j:
t1
P r(ist |oti , at1
, bi,l ) =

X

t1
t1 t1 t1
Oi (st , at1
, aj , oi )P r(is |ai , aj , bi,l )

(7)

at1
j

normalizing constant.
Equations 6 7 along Proposition 1 may seen generalization single agent
Bayes filter (see Section 3) multiagent setting. general level, equations represent
application update hierarchical priors given observed data (Gelman et al., 2004)
problem state estimation multiagent settings.

8. Interactive Particle Filter Multiagent Setting
presented algorithm traditional bootstrap filter Section 3. mentioned before,
bootstrap filter MC sampling based randomized implementation POMDP belief update
(Bayes filter). generalize implementation approximate I-POMDP belief update
steps presented previously Section 7.
311

fiD OSHI & G MYTRASIEWICZ

8.1 Description
generalization PF multiagent case, call interactive particle filter
(I-PF), similar basic PF, involves key steps importance sampling selection.
resulting algorithm inherits convergence properties original algorithm (Doucet et al.,
2001). Specifically, approximate posterior belief generated filter converges truth
(as computed Eqs. 6 7) number particles (N ) tends infinity. Note presence
agents affect convergence because, (a) exact belief update provides
stationary point similarly reasons presence agents, (b) explicitly model
agents actions due nonstationarity environment vanishes.
extension PF multiagent setting turns nontrivial
faced predicting agents action(s), requires us deal interactive belief
hierarchy. Analogously I-POMDP belief update, I-PF reduces traditional PF
one agent environment.
I-PF, described Fig. 4, requires initial set N particles, ebt1
k,l , approximately
t1
representative agents prior belief, along action, ak , observation, otk ,
level belief nesting, l > 0. per convention, k stand either agent j, k
(n)
agent, j i, appropriate. particle, isk , sample set represents agents
possible interactive state, agents belief may set particles. Formally,
(n)
(n)
(n)
(n)
(n)
(n)
isk = hs(n) , k i, k = hebk,l1 , bk i. Note ebk,0 probability distribution
physical state space.
generate ebt1
k,l sampling N particles prior nested belief. Given prior nested
belief, simple recursive procedure first uses marginals physical states
frames current nesting level sample state frame agent. sample
N particles density lower level beliefs, conditioned sampled state frame
combination. belief multiply nested, operation recursively performed bottoming
lowest level agents flat (level 0) beliefs sampled.
interactive particle filtering proceeds propagating particle forward time. However, opposed traditional particle filtering, one-step process: (i) order
perform propagation, agents action must known. model ascribed
agent intentional, obtained solving agents model (using algorithm APPROXPOLICY described later Section 9) find distribution actions,
action sampled (lines 34 Fig. 4). Specifically, OPT set optimal actions obtained
solving model, P r(ak ) = |OP1 | ak OPT, 0 otherwise. (ii) Additionally,
analogous exact belief update, agents possible observations, must
update model (line 6). model intentional, must update belief state. l > 1,
updating agents belief requires recursively invoking I-PF performing belief update (lines 1214). recursion depth belief nesting terminates level nesting
becomes one, LEVEL0BELIEFUPDATE, described Fig. 5, performed (lines 810).4
addition using agents observation weighting, agents observations also participate weighting process (lines 1516). latter allows us distinguish js beliefs
likely given physical state. Though propagation weighting steps generate |k |N
appropriately weighted particles, resample N particles (line 19), using unbiased
4. physical state space also continuous large, would replace level 0 belief update
traditional particle filter. However, so, would loose theoretical bounds given Section 9.1

312

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

t1 t1
Function I-PARTICLEFILTER (ebk,l
, ak , ok , l > 0) returns ebtk,l
tmp

e
e
1. bk,l , bk,l
Importance Sampling
(n),t1
(n),t1
t1
2. isk
= hs(n),t1 , k
ebk,l

(n),t1
(n),t1
3.
P r(Ak |k
) APPROXPOLICY(k
, l 1)
(n),t1
t1
4.
Sample ak
P r(Ak |k
)
// Sample agents action
t1 (n),t1
5.
Sample s(n),t Tk (S |akt1 , ak
,s
) // Sample physical state

6.
ok k
// Update agents belief
7.
(l = 1)
// I-POMDP singly nested
(n),t
(n),t1 t1
8.
bk,0 LEVEL0BELIEFUPDATE(bk,0 , ak
, ok )
(n),t
(n),t b(n)
9.
k hbk,0 , k
(n),t
(n),t
10.
isk
hs(n),t , k
11.
else
// I-POMDP multiply nested
(n),t
(n),t1 t1
e
e
12.
bk,l1 I-PARTICLEFILTER(bk,l1 , ak , ok , l 1)
(n),t
(n),t
(n)
13.
k h ebk,l1 , bk
(n),t
(n),t
14.
isk
hs(n),t , k
(n),t
(n)
t1
15.
Weight isk : wt Ok (otk |s(n),t , akt1 , ak
)
(n)
(n)
(n),t t1 t1
16.
Adjust weight: wt wt Ok (ok |s
, ak , ak )

(n),t
(n)
ebtmp
17.
(isk , wt )
k,l
PN
(n)
(n)
18. Normalize wt n=1 wt = 1
Selection
(n),t
19. Resample replacement N particles {isk , n = 1...N }
tmp
set ebk,l according importance weights.
(n),t
e
20. btk,l {isk , n = 1...N }
21. return ebtk,l
end function

Figure 4: Interactive particle filtering approximating I-POMDP belief update. nesting
filters used update levels belief. k denotes either agent j, k
agent, j i, appropriate. Also see Fig. 6 visualization.

resampling scheme. Lines 215 represent simulation prediction step (Eq. 6), lines
1620 simulated implementation correction step (Eq. 7).
alternative approach within propagation step sample agents observation
hidden variable. may update belief given sampled observation.
Although statistically equivalent approach, involves additional step sampling,
contributes sources error I-PF. particular, lesser number particles,
agents beliefs resulting low probability observations may appear resampled
posterior. agents low probability observations less likely sampled.
original agents observation independent others belief, particles identical
physical states different beliefs weighted equally. beliefs resulting
low probability observations less frequent sample set, less likely picked
resampled posterior. comparison, weighting using agents observation removes
313

fiD OSHI & G MYTRASIEWICZ

t1 t1
Function LEVEL0BELIEFUPDATE (bk,0
, ak , ok ) returns btk,0
t1
t1
1. P r(ak ) 1/ak
// agents action noise
2. st
3.
sum 0
4.
st1
5.
P r(st |st1 , akt1 ) 0
t1
6.
ak
Ak
// Marginalize noise
+
t1
t1
7.
P r(st |st1 , akt1 ) Tk (st |st1 , akt1 , ak
)P r(ak
)
+
t1 t1 t1 t1
8.
sum P r(s |s , ak )bk (s )
9.
P r(otk |st , akt1 ) 0
t1
10.
ak
Ak
// Marginalize noise
t1
t1 +
t1 t1
11.
P r(ok |s , ak ) Ok (ok |s , ak , ak )P r(ak
)


t1
12.
bk,0 (s ) P r(ok |s , ak ) sum
13. Normalize belief, btk
14. return btk
end function

Figure 5: level 0 belief update similar exact POMDP belief update
agents actions treated noise. example, noise may simply uniform
distribution agents actions.

intermediate sampling step source error expense temporarily generating
larger number particles. Based preference reduced approximation error computational
efficiency, one two alternative steps may used.
visualization I-PF implementation shown Fig. 6. Note number particles
grows exponentially nesting level, due approach becomes intractable
larger number levels. method limit number particles descend
nesting level needed address source complexity. one line future work.
Notice I-PF could also viewed recursive implementation approximately
Rao-Blackwellised particle filter (RBPF) (Doucet, de Freitas, Murphy, & Russell, 2000),
conditional distribution models updated using RBPF. Doshi (2007) presented
Rao-Blackwellised I-PF (RB-IPF), conditional distribution updated using variational
Kalman filter. Although performance RB-IPF improves I-PF, restricted prior
beliefs Gaussian could generalized beyond single level nesting.
8.2 Illustration I-PF
illustrate operation I-PF using multiagent tiger problem introduced Section 5.
sake simplicity consider prior belief singly nested, i.e. l = 1. procedure
recursively performed deeply nested beliefs. Let uninformed js level 0 beliefs,
location tiger (see Fig. 3(a)). demonstrate operation I-PF
case listens (L) hears growl left creaks, hGL,Si.
Fig. 7, show initial sample set, ebt1
i,1 , consisting N = 2 particles approximately representative singly-nested beliefs. Since assume js frame known,
314

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP



time
depth

ok
b

t1

b

tmp

b

k,l

k,l

Propagation

Weight



k,l

Resample

t1

ak

b

(n),t1

b

k,l1

(n) (n)

(n) (n)

, k ,

, k ,
b (n),t1

b (n),t

k,l2

k,l2

(n) (n)

(n) (n)

s, k ,

s, k ,
b (n),t

b (n),t1

k,1

k,1

Level 1

(n),t

k,l1

(n) (n)

(n) (n)

s, k,

s, k,

t1

b k



b k

Figure 6: illustration nesting I-PF. Colors black gray distinguish filtering
two agents. propagation step involves updating agents beliefs,
perform particle filtering beliefs. filtering terminates reaches
level 1 nesting, level 0 belief update performed agent.

~ t-1
b i,1
t-1
<st- 1 = TL, b j,0
= 0.5>

<st- 1 = TR, b t-1
j,0 = 0.5>

ait-1 =L

Figure 7: initial sample set 2 particles approximately representative bt1
i,1 shown
Fig. 3(a).

particle interactive state consisting tigers location js level 0 belief. Let belief
0.5 j sampled flat line densities.
315

fiD OSHI & G MYTRASIEWICZ

~ t-1
bi,1
<st- 1 = TL, b t-1
j,0 = 0.5>

t-1
j =L

<st- 1 = TR, b t-1
j,0 = 0.5>

t-1
j =L
t-1
=L

Figure 8: initial sample set js optimal action shown particle.
mentioned before, propagation particles time step 1 two-step
process. first step, solve js POMDP compute optimal action belief 0.5.
js action listen since know location tiger. depict Fig. 8.
~ t-1
b i,1

Propagation
L,GR
L,GL


< st = TL , b j,0
= 0.15>

1 . Sample ~ T(S|st-1 ,ait-1 ,ajt-1 )

2 . forall otj bj,0
= SEj (bj,0t -1,ajt-1 ,ojt )


= 0.85>
< = TL, bj,0

L,GR

t-1
=L



< = TR, bj,0
= 0.85>

L,GL


< = TR, b j,0
= 0.15>

Figure 9: propagation particles time step 1 time step t. involves sampling
next physical state updating js beliefs anticipating observations (denoted
using dashed arrows). j may receive one two observations, 4
particles propagated sample set.

second step propagation sample next physical state particle using
transition function. Since j listen, location tiger remains unchanged. Next,
must update js beliefs. anticipating j might observe, updating belief
exactly given optimal action listening. Since j could receive one two possible observations
GL GR particle splits two. shown using dashed arrows going
particles initial sample set particles propagated sample set, Fig. 9. j
hears GL, updated belief 0.85 (that tiger behind left door), otherwise 0.15
hears GR. level belief nesting greater one, js belief update would performed
recursively invoking interactive particle filter js beliefs.
316

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

oit =GL,S

~ t-1
bi,1
Propagation

~
bi,1 tmp
Weighting


< = TL , bj,0t = 0.15>
w = 0.15*0. 765 = 0. 115



< = TL , bj,0t = 0.85>
w = 0.85*0. 765 = 0. 650

< = TR , bj,0t = 0.85>
w = 0.15*0. 135 = 0. 020

ait-1 =L

<st- 1 = TR , bj,0t = 0.15>
w = 0.85*0. 135 = 0. 115

Figure 10: weighting step two step process: particle first weighted likelihood j receives observations, followed adjusting weight using
probability making observation hGL,Si. Note resulting weights shown
normalized.

part weighting, first weight particle probability j receiving
observations. Particles larger weights contain beliefs j likely. Thereafter,
scale weight probability observing growl left creaks,
hGL,Si. understand weighting process, lets focus single particle. Weighting
remaining particles analogous.
consider particle top right sample set, ebtmp
i,1 , shown Fig. 10. Agent js
level 0 belief 0.85 particle due j hearing growl left listening.
probability j making observation given observation function, tiger
left 0.85. adjust weight probability perceiving hGL,Si tiger
left agents listening. probability given observation function
0.765 (see Table 1). final weight attached particle 0.65. Note weights
shown Fig. 10 normalized. normalization belief tiger left
0.85 (obtained adding normalized weights particles st =TL), 0.15 tiger
right. note conforms expectations.
final step I-PF unbiased resampling particles using weights
distribution. prevent exponential growth number particles 5 , resample N particles
resulting sample set, ebti,1 , approximately representative exactly updated belief.
belief nested deeper levels, mentioned example forms bottom step
recursive filtering process.
8.3 Performance I-PF
part empirical investigation performance I-PF, show, using standard
pseudo-distance metric visually, approximates exact state estimation closely.
5. propagation steps, N |j |t particles sample set.

317

fiD OSHI & G MYTRASIEWICZ

=GL,S

~
bi,1t-1
Propagation

~
bi,1 tmp
Weighting

~
bi,1t
Resampling


< = TL , bj,0t = 0.85>


< = TL , bj,0t = 0.85>

t-1

ai =L

Figure 11: final step unbiased resampling using weights distribution.
begin utilizing extended versions standard test problems proceed demonstrate performance larger problem.
8.3.1 ULTIAGENT IGER ACHINE AINTENANCE P ROBLEMS
analysis, first utilize two-agent tiger problem two physical states, described Section 5, two-agent version machine maintenance problem (MM) (Smallwood & Sondik, 1973) described detail Appendix A, three physical states.
problems physical states, interactive state space tends get large includes models
agent. Due absence general approximation techniques I-POMDPs,
use grid based numerical integration implementation exact filter baseline approximation comparison. obtained points numerical integration superimposing regular
grids differing resolutions interactive state space.
lineplots Fig. 12 show quality I-PF based approximation, measured
KL-Divergence becomes better number particles increases, problem domains.
remains true level 1 2 beliefs. KL-Divergence measures difference
two probability distributions giving relative entropy filtered posterior respect
near-exact one obtained numerical integration. Note performance IPF remains consistent two-state tiger three-state MM problem. However, level
2 belief approximations require considerably particles compared level 1 approximations,
achieve similar performance, indicating performance I-PF affected level
nesting. data point lineplots average 10 runs I-PF multiple prior belief
states. case tiger problem, posterior used comparison one obtained
agent listens hears growl left creaks. MM problem, posterior
obtained manufactures perceives defect product, used comparison. Two
prior level 1 beliefs agent playing tiger problem shown Fig. 3.
considered level 2 belief according agent unaware tigers location believes
equal probabilities either level 1 beliefs shown Fig. 3 likely. utilized
analogous beliefs machine maintenance problem.
comparison run times I-POMDP belief update implemented using grid based
numerical integration I-PF shown Table. 2. varied number grid points
318

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

Multiagent Tiger

Multiagent Machine Maintenance

600
Level 1
Level 2

Level 1
Level 2

600
KL-Divergence

KL-Divergence

500
400
300
200
100

500
400
300
200
100

0

0
10

100

1000

10000

10

No. Particles (N)

100

1000

10000

No. Particles (N)

(i)

(ii)

Figure 12: Anytime performance I-PF function number particles, (i) multiagent tiger problem, (ii) multiagent machine maintenance problem. performance
I-PF significantly improves increase number particles leading
toward convergence true posterior. vertical bars standard deviations.

Belief

Problem
Multiagent
Tiger

Level
1
Multiagent
MM

Multiagent
Tiger
Level
2
Multiagent
MM

Method
I-PF
Grid
based
I-PF
Grid
based
I-PF
Grid
based
I-PF
Grid
based

N=500
0.148s
0.001s
21.65s
0.18s
0.452s
0.009s
1m 0.28s
0.21s
2m 23.28s
1.1s
34m 40.36s
0.55s
1m 37.59s
0.17s
24m 55.33s
4.77s

N=1000
0.332s
0.007s
1m 25.19s
1.02s
0.931s
0.0146s
3m 27.87s
0.13s
11m 41.30s
1.52s
77m 2.9s
4.88s
8m 27.29s
1.65s
56m 21.97s
5.73s

Table 2: Comparison average running times grid based numerical integration I-PF
implementations platform (Pentium IV, 1.7GHz, 512MB RAM, Linux).

number particles two implementations, respectively. use initial beliefs
flat, number grid points particles provides comparable approximation
quality. I-PF implementation significantly outperforms numerical integration based implementation, providing comparable performance quality. Additionally, run times
grid based implementation increase significantly move two-state tiger prob319

fiD OSHI & G MYTRASIEWICZ

lem three-state MM problem, comparison increase I-PF level 1 beliefs.
Since level 1 multiagent tiger model 6 observations comparison 2 multiagent
MM, run times decrease move tiger MM problem level 2 beliefs.
Despite using equal number grid points particles, reduced run time I-PF
comparison grid based approach due to: (i) iterating grid points order
obtain belief interactive state consideration. contrast, I-PF iterates
particles level propagating weighting obtain posterior; (ii)
I-PF solves models approximately comparison solving exactly grid points;
(iii) grid based belief update considers optimal actions model, I-PF
samples single action distribution propagates corresponding particle using
action.
Level 1 Beliefs Multiagent Tiger Problem
Exact
Particle Filter

Exact
Particle Filter

Pr(TL,b_j)

Pr(TR,b_j)

Pr (TL,p )

Pr (TR,p )

j

j

25

5

20

4

15

3

10

2

5

1

0

0
3

3

2.5
0

2.5
0

2

0.2

Prb_j(TL)
j

0.8

Time steps (T)

0.4

1.5

0.6

2

0.2

Time steps (T)

0.4

Prb_jj (TL)

1 1

1.5

0.6
0.8

1 1

Figure 13: exact approximate p.d.f.s successive filtering steps. peaks
approximate p.d.f.s align correctly exact p.d.f.s, areas
approximate exact p.d.f.s approximately equal.

order assess quality approximations successive belief updates, graphed
probability density functions produced I-PF exact belief update. densities
arising three filtering steps level 1 belief agent (Fig. 3(a)) tiger
problem, shown Fig. 13. approximate p.d.f. average 10 runs I-PF
contained 5000 particles, shaped using standard Gaussian kernel. Gmytrasiewicz
Doshi (2005) provide explanation exact I-POMDP belief update shown here. Briefly,
prior belief flat line, posterior becomes segmented segments correspond
beliefs j likely based predicted action anticipated observations j. height
segment proportional likelihood js possible observation. action observation
sequence followed hL, GL, Si, hL, GL, Si, hOR, GL, Si. seen, I-PF produces
good approximation true densities.
320

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

8.3.2 UAV R ECONNAISSANCE P ROBLEM
Unmanned agents UAVs finding important applications tasks fighting forest
fires (Casbeer, Beard, McLain, Sai-Ming, & Mehra, 2005; Sarris, 2001), law enforcement (Murphy
& Cycon, 1998) reconnaissance warfare. agents must operate complex environments
characterized multiple parameters affect decisions, including, particularly warfare,
agents may antagonistic preferences. task complicated
agent may possess noisy sensors unreliable actuators.
consider task UAV performs low-altitude reconnaissance potentially
hostile theater may populated agents conflicting objectives serve ground
reconnaissance targets (see Fig. 14(a)). facilitate analysis, divide UAVs operating
theater 33 grid sectors consider ground reconnaissance target (T ), could
located 9 sectors. example, target could terrorist may hiding
safe house located sector. course, UAV unaware sector contains ,
aware location. assist goal spotting moving sector containing
it, UAV may receive noisy communication informs rows (similar colored
sectors Fig. 14) likely contain , though uncertainty. UAV option
moving four cardinal directions adjacent sector, hovering current
location listening communications.
target may informed (by collaborators) could danger spotted
UAV although high uncertainty, case may move adjacent diagonal
sector. Note actions both, UAV agent may affect physical state
problem. formulate decision problem UAV below:
physical state, s={rowi , sidei centeri , rowT , sideT centerT }, rowi sidei
centeri indicate row location UAV whether UAV located side columns
center column, respectively; joint action space, = Ai , Ai = {moveN ,. . .,moveW ,
listen} = {moveN ,. . .,moveW ,listen}. Here, movex moves UAV target direction indicated x, listen denotes act receiving communications location
UAV; Observation space UAV is, = {top-row (TR), center-row (CR), bottomrow (BR)}, example, TR indicates corresponding target one three
sectors top row; Transition function is, Ti : [0, 1]. Ti models fact
UAV target move deterministically surrounding sector; Observation function,
Oi : [0, 1] gives likelihood UAV informed correct row
target located; Reward function, Ri : [0, 1] formalizes goal
spotting reconnaissance target. UAV moves sector containing target , assume
target spotted game ends.
target may move, beneficial anticipate actions. Thus, UAV
tracks possible beliefs may location UAV. assume
UAV aware objectives conflict own, probabilities observations,
therefore frame. point size complexity problem, involving 36 physical
states, 5 actions 3 observations agent.
Analogously previous problem sets, measured quality estimation provided
I-PF larger problem. Fig. 14(b), show KL-Divergence approximate
distribution number particles allocated I-PF increased. KL-Divergence
decreases rapidly increase number particles level 1 2 beliefs. However,
321

fiD OSHI & G MYTRASIEWICZ

1600
Level 1
Level 2

1400
KL-Divergence



1200
1000
800
600
400
200



0
10

(a)

100
1000
No. Particles (N)

10000

(b)
Belief
Level
1

Level
2

Method
I-PF
Grid
based
I-PF

N=500
2.929s
0.894s
3m 37.07s
4.22s
N=100
2m 55.52s
25.61s

N=1000
5.251s
0.492s
7m 16.42s
0.27s
N=200
11m 10.43s
56.724s

(c)
Figure 14: (a) operating theater UAV i. problem may flexibly scaled adding
targets sectors. (b) posterior obtained I-PF approaches exact
number particles increase. (c) Comparison average running times
numerical integration I-PF implementations platform (Xeon, 3.0GHz,
2GB RAM, Linux).

notice magnitude divergence larger lower numbers particles comparison
previous problems. is, part, due larger state space problem demonstrates
I-PF fully address curse dimensionality. Thus, many particles
needed reach comparable levels divergence. also show comparison run times
I-POMDP belief update implemented using I-PF grid based numerical integration
table Fig. 14(c). identical number particles grid points selected,
provided comparable qualities estimations. unable run numerical integration
implementation level 2 beliefs problem.

9. Value Iteration Sample Sets
I-PF represents belief agent i, bi,l , using set N particles, ebi,l , value function
e denote required backup operator,
backup operator operates samples needed. Let H
322

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

e approximate value function, backup operation, U
et = H
eU
e t1 , is:
U


X
1 X
(n)
t1
e
e
e
b
b
e
e
ERi (is , ai )+
P r(oi |ai , bi,l )U (hI-PF(bi,l , ai , oi ), i)
U (hbi,l , i) = max
ai Ai N
oi

is(n) ebi,l

(8)

=
discount factor I-PF()
algorithm shown Fig. 4. set optimal action(s) given approximate belief, OPT(hebi,l , bj i),
ERi (is(n) , ai )

P

(n) , , )P r(a | (n) ),
j
j j
aj Ri (s

calculated returning action(s) maximum value:

P
P
1
OP (hebi,l , bi i) = argmax
P r(oi |ai , ebi,l )
ERi (is(n) , ai ) +
N
ai Ai

is(n) ebi,l



e (hI-PF(ebi,l , ai , oi ), bi i)
U

oi

(9)

Equations 8 9 analogous Eqs. 4 5 respectively, exact integration replaced
e H
Monte Carlo integration, exact belief update replaced I-PF. Note H
N .
algorithm computing approximately optimal finite horizon policy tree given initial
belief using value iteration l > 0 shown Fig. 18 Appendix B.
9.1 Convergence Error Bounds
use randomizing techniques PFs means value iteration necessarily
converge. because, unlike exact belief update, posteriors generated PF
finitely many particles guaranteed identical identical input. non-determinism
e N . 6
approximate belief update rules isotonicity contraction H
inability guarantee convergence value iteration implies must approximate
infinite horizon policy approximately optimal finite horizon policy. Let U value
e value approximate U value
optimal infinite horizon policy, U
optimal t-horizon policy tree. error bound (using supremum norm || ||) is,
e || = ||U U + U U
e || ||U U || + ||U U
e || (triangle inequality). Note
||U U




0
first term, ||U U ||, bounded ||U U ||. bound second term calculated
below:
e U ||
E = ||U
eU
e t1 HU t1 ||
= ||H
eU
e t1 H U
e t1 + H U
e t1 HU t1 ||
= ||H
(add zero)
t1
t1
t1
t1
e
e
e
e
||H U
H U || + ||H U
HU || (triangle inequality)
eU
e t1 H U
e t1 || + ||U
e t1 U t1 ||
||H
(contracting H)
t1
t1
t1
e
e
e
||H U
H U || + E

eU
e t1 H U
e t1 ||. analysis follows focus
turn attention calculating ||H
e t1 , U
et = H
eU
e t1 , bi,1 singly nested belief
level 1 beliefs. Let U = H U

6. One may turn PFs deterministic belief update operators (de-randomization) generating several posteriors
input. representative posterior formed taking convex combination different posteriors.
example, Thrun (2000) uses k-nearest neighborhood approach purpose.

323

fiD OSHI & G MYTRASIEWICZ

e |. Let
worst error made: bi,1 = argmax |U U
e policy tree (alpha vector)
bi,1 Bi,1

optimal ebi,1 (the sampled estimate bi,1 ), policy tree optimal bi,1 .
use Chernoff-Hoeffding (C-H) upper bounds (Theorem A.1.4, pg 265 Alon & Spencer, 2000) 7 ,
well-known tool analyzing randomized algorithms, derive confidence threshold 1
e , within 2 true estimate U (= E[]):
observed estimate, U


e

may write,

e > U + ) e2N 2 /(emax emin )2
P r(U


e
e < U ) e2N 2 /(emax emin )2
P r(U


e

e > U + U
e < U ) = P r(U
e > U + ) + P r(U
e < U )
P r(U





e

e

e

e




e > U + U
e < U )
P r(U



e

e

last term zero, equation becomes:

e < Ut ) 2e2N 2 /(emax emin )2
e > Ut + U
P r(U

e

e

e > U + U
e < U ) 1 P r(U U
e U + ).
may replace P r(U





e

e

e
simple operations, inequality becomes:
e Ut + ) 1 2e2N 2 /(emax emin )2
P r(Ut U

e

e within 2 true estimate U , least 1 . have:
Let probability U


e
2 /(e


1 = 1 2e2N

min )
max e

2

confidence probability least 1 , error bound is:
=

r

(e
max
emin )2 ln(2/)
2N

(10)

Rmin

emax
emin may loosely upper bounded Rmax1
. Note Eq. 10 also
used derive number particles, N , given . get desired bound, note
least probability 1 error bound 2 probability worst
eU
e t1 H U
e t1 || (1 )2 + Rmax Rmin .
possible suboptimal behavior may result: ||H
1
final error bound obtains:
Rmin
E (1 )2 + Rmax1
+ E t1


)
min )(1
+ (Rmax R
= (1 ) 2(1
1
(1)2

t)

(geometric series)

(11)

defined Eq. 10.
7. horizon t, samples ebi,1 i.i.d. However, horizons less t, samples generated I-PF
exhibit limited statistical independence, independent research (Schmidt, Siegel, & Srinivasan, 1995) reveals
C-H bounds still apply.

324

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

Proposition 3 (Error Bound). singly nested t-horizon I-POMDPi,1 , error introduced
approximation technique upper bounded given by:
e U || (1 )
||U

2(1 )
(Rmax Rmin )(1 )
+
1
(1 )2

defined Eq. 10.

levels belief nesting greater one, js beliefs also approximately represented using
samples. Hence approximation error due sampling, also due possible
incorrect prediction js actions based approximate beliefs. Since even slight deviation
exact belief may lead action turns worst value compared
optimal action, seems difficult derive bounds useful tighter usual
Rmin
difference best worst possible behavior ( Rmax
) case.
(1)2
9.2 Computational Savings
Since complexity solving I-POMDPs dominated complexity solving models
agents analyze reduction number agent models must solved.
K+1-agent setting number particles bounded N level, particle ebt1
k,l
level l contains K models level l 1. Solution level l 1 models requires
solution lower level models recursively. upper bound number models
solved O((KN )l1 ). Given K level l 1 models particle, N
possibly distinct particles, need solve O((KN )l ) models. upper bound number
models solved polynomial K fixed nesting level. contrasted
O((K| |K )l ) models need solved exact case, exponential K. Here,
among spaces models agents, largest space theoretically countably
infinite. Typically, N | |K , resulting substantial reduction computation. However, note
total number particles exponential nesting level, l. makes solutions large
nesting levels still intractable.

10. Empirical Performance
goal experimental analysis demonstrate empirically, (a) reduction error
increasing sample complexity, (b) savings computation time approximation technique used. use multiagent tiger problem introduced previously, multiagent
version machine maintenance (MM) problem (see Appendix A) test problems.
single-agent versions problems simple, multiagent versions sufficiently
complex motivate use approximation techniques solve them. Additionally,
demonstrate approach scales larger problems applying UAV reconnaissance
problem well.
10.1 Multiagent Tiger Machine Maintenance Problems
demonstrate reduction error, construct performance profiles showing increase
performance computational resources case particles allocated approximation algorithm. Figs. 15(a) (c) show performance profile curves agent
prior belief level 1 belief described previously Fig. 3(a), suitably modified MM
325

fiD OSHI & G MYTRASIEWICZ

Multiagent Tiger Problem
0
Expected Reward

Expected Reward

0
-5
-10
-15
-20
-25

-5
-10
-15
-20
-25

-30

-30
1

10
100
1000
No. Particles (N)

H=2:Exact
H=2:Approx

1

H=3:Exact
H=3:Approx

10
No. Particles (N)

H=2:Exact
H=2:Approx

100

H=3:Exact
H=3:Approx

(a)

(b)
Multiagent Machine Maintenance Problem
1
Expected Reward

Expected Reward

1
0.5
0
-0.5
-1

0.5
0
-0.5
-1

1

10
100
1000
No. Particles (N)

H=2:Exact
H=2:Approx

10000

H=3:Exact
H=3:Approx

1

10
No. Particles (N)

H=2:Exact
H=2:Approx

(c)

100

H=3:Exact
H=3:Approx

(d)

Figure 15: Anytime performance profiles: multiagent tiger problem using (a) level 1,
(b) level 2 belief prior agent i. multiagent MM using (c) level 1,
(d) level 2 belief prior. approximate policies gradually improve employ
increasing number particles.

problem. expected average rewards both, horizon 2 3 approach optimal expected
reward number particles increases. show analogous plots level 2 belief
Figs. 15(b) (d). cases average rewards accumulated 2
3 horizon policy tree (computed using APPROXPOLICY algorithm Fig. 18) playing
agent j simulated tiger MM problems plotted. compensate randomness sampling, generated policy tree 10 times independently other, averaged
326

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

100 runs time. Within run, location tiger js prior beliefs sampled
according prior belief. js policy computed using algorithm Fig. 18.
Problem

Error

Multiagent
tiger

Obs.
Et
Worst
Obs.
Et
Worst

Multiagent
MM

t=2
N=100 N=1000
5.61
0
64.73
41.53
209.00
0.28
0.23
4.58
2.05
8.84

t=3
N=100 N=1000
4.39
2.76
120.90
77.57
298.10
0.46
0.40
8.79
3.64
12.61

Table 3: Comparison worst case observed errors, theoretical error bounds trivial
Rmin
error bound ( Rmax
).
(1)2

Table 3, compare empirically determined error bound difference optimal
expected reward worst observed expected reward theoretical error bound (=0.1,
=0.9) Section 9.1, horizons 2 3. theoretical error bounds appear loose due
worst-case nature analysis (expectedly) much tighter trivial worst bounds,
become better number particles increases.
Problem
Multiagent
tiger

Method
Grid
SB

Multiagent
MM

Grid
SB

t=2
37.84s
0.6s
1.44s
0.05s
5m 26.57s
0.07s
5.75s
0.01s

Run times
t=3
t=4
11m 22.25s
*
1.34s
1m 44.29s 19m 16.88s
0.6s
17.5s
20m 45.69s
*
0.29s
34.52s
3m 24.9s
0.01s
0.04s

t=5
*
*
*
17m 58.39s
0.57s

Table 4: Run times Pentium IV 2.0 GHz, 2.0GB RAM Linux. * = program ran
memory.

Table 4 compares average run times sample-based approach (SB) grid based
approach, computing policy trees different horizons starting level 1 belief.
values policy trees generated two approaches similar. run times demonstrate
impact curse dimensionality grid based method shown higher run times
MM problem comparison tiger problem. I-PF based implementation though
immune curse reduces impact, affected curse history, illustrated
higher run times tiger problem (branching factor reachability tree: |Ai ||i | = 18)
compared MM problem (branching factor:|Ai ||i | = 8). unable compute
solutions using grid based implementation problems horizons beyond 3.
327

fiD OSHI & G MYTRASIEWICZ

Expected Reward

12
10
8
6
4
2

Problem

Method

UAV
Recon.

SB

Run times
t=2
t=3
8m 50.03s 20m 59.23s
5.26s
4.09s

0
10

100

1000

No. Particles (N)
H=2:Approx

H=3:Approx

(a)

(b)

Figure 16: (a) Anytime performance profile UAV reconnaissance problem horizons 2
3. Notice profile flattens number particles reaches 1,000
greater, thereby indicating corresponding average reward close optimal.
(b) Run times obtaining policy Xeon 3.0 GHz, 2.0GB RAM Linux.
horizon 2, used 500 particles 100 particles used horizon 3.
see (a), rewards policies numbers particles much less
converged values.

10.2 UAV Reconnaissance Problem
evaluate performance approach UAV reconnaissance problem, introduced previously Section 8.3. mentioned, larger problem consisting 36 physical
states, 5 actions 3 observations. show level 1 performance profile problem
Fig. 16(a) horizons 2 3. Due size problem, unable compute
exact value optimal policies. before, data point average reward obtained
simulating horizon 2 3 policy two-agent setting. Agent initial belief one according
uncertain physical state models agent. observe
profiles tend flatten number particles increases. corresponding expected reward
therefore close optimal value policies.
also show time taken generate good quality horizon 2 3 policy average.
indicate indeed possible obtain (approximate) policies large problems,
times needed somewhat large. Notice times significantly greater run
times multiagent tiger MM problems. is, part, due larger state space and,
show later article, part due larger numbers actions observations
agent.

11. Sampling Look Ahead Reachability Tree
Although able solve I-POMDPs large state spaces, unable generate
solutions large horizons. main reason exponential growth look ahead
reachability tree increasing horizons; referred curse history.
time step t, could (|Ai ||i |)t1 reachable belief states agent i. example,
328

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

multiagent tiger problem, second time step could 18 possible belief states, 324
third time step, 0.1 million fifth time step.
mitigate curse history, reduce branching factor look ahead reachability
tree sampling possible observations agent may receive. approach
completely address curse history, beats back impact curse substantially.
reachability tree expansion phase, agents actions used propagate belief.
t1
et1
Observations sampled propagated belief, oti P r(i |at1
, bi,l ) bi,l
propagated belief. value iteration phase, value backed (possibly) partial
reachability tree, agent performs action optimal root node tree.
convenience, let us label approach reachability tree sampling (RTS).
RTS shares conceptual underpinnings belief expansion models PBVI (Pineau
et al., 2006), differs method applicable online policy tree generation IPOMDPs, compared PBVIs use offline policy generation POMDPs. also similar
sparse sampling technique proposed selecting actions exploration reinforcement
learning (Kearns et al., 2002) online planning POMDPs (Ross et al., 2008) sampling
look ahead trees. distinction focus sampling observations given propagated
multiagent beliefs, latter approaches focused settings single agent learning. Consequently, process computing sampling distribution experimental settings differ.
11.1 Computational Savings
consider computational savings result sampling observations look ahead
reachability tree. sampling Ni < |i | observations propagated belief within
reachability tree, time step t, obtain (|Ai ||Ni |)t1 possible belief states,
assuming worst case occurs end sampling Ni distinct observations.
comparison (|Ai ||i |)t1 belief states complete reachability tree. Typically,
experiments demonstrate, number distinct sampled observations less |i |, resulting
significant computational savings.
Method
SB-No-RTS
SB-RTS

t=2
1.44s
0.05s
0.86s
0.02s

t=3
1m 44.29s
0.6s
15.17s
1.6s

Run times
t=4
t=5
19m 16.88s
*
17.5s
2m 52.9s
4m 13.43s
6.51s
27.51s

t=6
*

t=7
*

7m 29.9s
47.98s

11m 51.57s
20.05s

Table 5: Run times multiagent tiger problem Pentium IV 2.0 GHz, 2.0GB RAM
Linux. * = program ran memory.

illustration computational savings compare run times computing
policy tree multiagent tiger (Table 5) UAV reconnaissance (Table 6) problems
singly-nested beliefs. compare value iteration reachability tree sampled (SBRTS) value iteration reachability tree sampling (SB-No-RTS), algorithm
given Fig. 18. SB-RTS multiagent tiger problem, sampled eight times
observation distribution fifth horizon six times thereafter. algorithms,
used similar number particles I-PF. tiger problem total 6 observations,
329

fiD OSHI & G MYTRASIEWICZ

SB-RTS compute policy faster, able compute seven time
horizons. compared performance SB-No-RTS, results demonstrate
approach sampling reachability tree could yield significant computational savings.
Method
SB-No-RTS
SB-RTS

t=2
8m 50.03s
5.26s
8m 23.86s
20.46s

Run times
t=3
20m 59.23s
4.09s
19m 25.54s
89.08s

t=4
*
121m 16.2s
592s

Table 6: Run times UAV reconnaissance problem Xeon 3.0 GHz, 2.0GB RAM
Linux. * = program ran memory.

However, see Table 6, approach yield significant savings context
UAV problem small horizons. space observations problem small
(3 distinct observations), majority often selected sampling. Hence, smaller
horizons 2 3, observe significant decrease size look ahead
reachability tree. However, reduction look ahead trees horizon 4 enough allow
computation corresponding policy, though run time considerable. unable
obtain case RTS. Thus, UAV problem reveals important limitation
technique may provide significant computational savings space observations
small, particularly small horizons.
11.2 Empirical Performance
present performance profiles Fig. 17 multiagent tiger problem partial look
ahead reachability trees built sampling observations. Similar previous experiments,
performance profiles reflect average rewards accumulated following action
prescribed root approximate policy tree built online. plot average reward
accumulated 10 independent trials consisting 100 runs each, number
observation samples, Ni gradually increased. Within run, location tiger
js prior beliefs sampled according prior level 1 belief. Since combined RTS
I-PF, addition varying Ni , also vary number particles, Np , employed
approximate beliefs. expected performance profiles, expected reward initially increases
sharply, flattening Ni becomes large sampled observation distribution reaches
true one. Reflecting intuition, plots Np = 100 exhibit slightly better expected rewards
average Np = 50. increase large, note consistent across
observation samples. also obtained average reward similar number trials
random policy (null hypothesis) used i. horizon 3, random policy gathered average
reward -84.785 ( 37.9847), -108.5 ( 41.56) horizon 4. Even small number
observation samples, RTS significantly better random policy thereby demonstrating
usefulness partial tree expansion. However, note random policy poor baseline
comparison used due absence similar approximations I-POMDPs.
330

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

Multiagent Tiger Problem
0
Expected Reward

Expected Reward

0
-5
-10
-15
-20

-5
-10
-15
-20
-25

Np=50
Np=100

-25

Np=50
Np=100

-30
2

4

8

16

2

No. Obs. Samples

4

8

16

No. Obs. Samples

(a)

(b)

Figure 17: Performance profiles multiagent tiger problem (a) horizon 3, (b)
horizon 4 look ahead tree built sampling observations.

Due small number observations MM UAV reconnaissance problems,
observe significant increases expected reward observations sampled. Hence,
show performance profiles problems.
observed empirical expected reward close optimal expected reward
distinct observations sampled building reachability tree. observation combined computational savings demonstrated Section 11.1 indicate
approximation approach viable. Additionally, varying parameters Np Ni ,
flexibly, though partially, control effects curses dimensionality history,
respectively, solutions according application requirements. interesting line future
work investigate interplay parameters.

12. Discussion
described randomized methods obtaining approximate solutions finitely nested I-POMDPs
based novel generalization particle filtering multiagent settings. generalization
straightforward confronted interactive belief hierarchy multiagent
settings. proposed interactive particle filter descends levels interactive belief
hierarchies, samples propagates beliefs level. sampling methods
particle filters unable completely avoid curse dimensionality, serve focus
computational resources elements model matter agent.
However, interactive particle filter address policy space complexity. Though
value iteration using sample sets guaranteed converge asymptotically, established useful error bounds singly-nested I-POMDPs. generalization multiply-nested beliefs
proved difficult continue investigate it. provided performance profiles
multiagent tiger machine maintenance problems, demonstrated scalability using
larger UAV reconnaissance problem.
experiments show approach saves computation space models
scale (usefully) large values time horizons needs combined methods
331

fiD OSHI & G MYTRASIEWICZ

deal curse history. order reduce impact curse history, proposed
sample observations constructing look ahead tree reachability analysis phase
policy computation. sparse sampling technique effectively reduces branching factor
tree allows computation solutions larger horizons demonstrated.
method scale approximation technique pick subset actions addition
sampling observations building reachability tree. dampens exponential
growth reachability tree increasing horizons, permits solutions larger horizons.
However, approach must used cautiously want leave critical actions
policy.
Specific approaches speeding computation also remain explored. mentioned, number particles interactive particle filter grows exponentially number
nesting levels. regard, assign monotonically decreasing number particles
represent beliefs nested deeper levels exploiting insight cognitive psychology beliefs
nested deeper levels less likely influence optimal policy? Thus, decrease particles sampled rate r < 1, Np /(1 r) particles total, resulting
computational savings.
Finally, regards appropriate strategy level use nested models, note analogy classical POMDPs, amount detail modeling information included therein.
Adding nested level modeling analogous including details POMDP formulation. Then, solution I-POMDP optimal given level detail included model,
like classical POMDPs.

Acknowledgments
research supported part grant #FA9550-08-1-0429 AFOSR part grants
IRI-9702132 IRI-0119270 NSF. Versions parts article previously appeared
(Doshi & Gmytrasiewicz, 2005a) (Doshi & Gmytrasiewicz, 2005b). acknowledge
reviewers useful comments.

Appendix A. Multiagent Machine Maintenance Problem
extend traditional single agent version machine maintenance (MM) problem (Smallwood & Sondik, 1973) two-agent cooperative version. original MM problem involved
machine containing two internal components operated single agent. Either one components machine may fail spontaneously production cycle. internal component
failed, chance operating upon product, cause product
defective. agent may choose manufacture product (M) without examining it, examine product (E), inspect machine (I), repair (R) next production cycle.
examination product, subject may find defective. course, components
failed, probability product defective greater.
transition function, observation functions, reward functions two agents,
j, shown Table 7. Apart including two agents operate machine
production cycle, increased nondeterminism original problem make
realistic. also beneficial effect producing richer policy structure.
332

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

hai , aj
State not-defective defective
hM,M/Ei
*
0.5
0.5
hM,I/Ri
*
0.95
0.05
hE,M/Ei 0-fail
0.75
0.25
hE,M/Ei 1-fail
0.5
0.5
hE,M/Ei 2-fail
0.25
0.75
hE,I/Ri
*
0.95
0.05
hI/R,*i
*
0.95
0.05
hai , aj
State not-defective defective
hM/E,Mi
*
0.5
0.5
hI/R,Mi
*
0.95
0.05
hM/E,Ei 0-fail
0.75
0.25
hM/E,Ei 1-fail
0.5
0.5
hM/E,Ei 2-fail
0.25
0.75
hI/R,Ei
*
0.95
0.05
h*,I/Ri
*
0.95
0.05
Observation functions agents j.

hai , aj
State 0-fail 1-fail 2-fail
hM/E,M/Ei 0-fail 0.81
0.18
0.01
hM/E,M/Ei 1-fail
0.0
0.9
0.1
hM/E,M/Ei 2-fail
0.0
0.0
1.0
hM,I/Ri
0-fail
1.0
0.0
0.0
hM,I/Ri
1-fail 0.95
0.05
0.0
hM,I/Ri
2-fail 0.95
0.0
0.05
hE,I/Ri
0-fail
1.0
0.0
0.0
hE,I/Ri
1-fail 0.95
0.05
0.0
hE,I/Ri
2-fail 0.95
0.0
0.05
hI/R,*i
0-fail
1.0
0.0
0.0
hI/R,*i
1-fail 0.95
0.05
0.0
hI/R,*i
2-fail 0.95
0.0
0.05
Transition function agents j: Ti = Tj

hai , aj
hM,Mi
hM,Ei
hM,Ii
hM,Ri
hE,Mi
hE,Ei
hE,Ii
hE,Ri
hI,Mi
hI,Ei
hI,Ii
hI,Ri
hR,Mi
hR,Ei
hR,Ii
hR,Ri

0-fail
1.805
1.555
0.4025
-1.0975
1.5555
1.305
0.1525
-1.3475
0.4025
0.1525
-1.0
-2.5
-1.0975
-1.3475
-2.5
-4

1-fail 2-fail
hai , aj
0-fail
0.95
0.5
hM,Mi
1.805
0.7
0.25
hM,Ei
1.555
-1.025 -2.25
hM,Ii
0.4025
-1.525 -1.75
hM,Ri -1.0975
0.7
0.25
hE,Mi
1.555
0.45
0.0
hE,Ei
1.305
-1.275
-2.5
hE,Ii
0.1525
-1.775
-2.0
hE,Ri
-1.3475
-1.025 -2.25
hI,Mi
0.4025
-1.275
-2.5
hI,Ei
0.1525
-3.00
-5.00
hI,Ii
-1.0
-3.5
-4.5
hI,Ri
-2.5
-1.525 -1.75
hR,Mi -1.0975
-1.775
-2.0
hR,Ei
-1.3475
-3.5
-4.5
hR,Ii
-2.5
-4
-4
hR,Ri
-4
Reward functions agents j.

1-fail
0.95
0.7
-1.025
-1.525
0.7
0.45
-1.275
-1.775
-1.025
-1.275
-3.00
-3.5
-1.525
-1.775
-3.5
-4

2-fail
0.5
0.25
-2.25
-1.75
0.25
0.0
-2.5
-2.0
-2.25
-2.5
-5.00
-4.5
-1.75
-2.0
-4.5
-4

Table 7: Transition, observation reward functions multiagent MM problem.

Appendix B. Algorithm Value Iteration Sample Sets
show algorithm computing approximately optimal finite horizon policy tree given
initial belief using value iteration l > 0. l = 0, algorithm reduces POMDP
policy tree computation carried exactly.8 algorithm consists usual two steps:
compute look ahead reachability tree horizon part reachability analysis (see
Section 17.5 Russell & Norvig, 2003) lines 2-6 perform value backup reachability
8. large problems, exact POMDP solutions may replaced approximate ones. so, error
bounds longer applicable approximation technique considered.

333

fiD OSHI & G MYTRASIEWICZ

tree, lines 7-28. value beliefs leaves reachability tree simply one-step
expected reward resulting best action.
Function APPROXPOLICY(k , l > 0) returns (Ak )
(n)
(n)
1. eb0k,l {isk , n = 1...N |isk bk,l k }
//Initial sampled belief
Reachability Analysis
2. reach(0) eb0k,l
3. 1 1
4.
reach(t)
t1
5.
ebk,l
reach(t 1), ak Ak , ok k

t1
6.
reach(t) I-PARTICLEFILTER(ebk,l
, ak , ok , l)
Dynamic Programming
7. 1 downto 0
8.
ebtk,l reach(t)
e (h ebt , bk i) , OPT(h ebt , bk i)
9.
U
k,l
k,l
10.
ak Ak
e (hebt , bk i) 0
11.
U
ak
k,l
(n),t
(n)
12.
isk
= hs(n),t , k ebtk,l
(n)
(n)
13.
P r(Ak |k ) APPROXPOLICY(k , l 1)
14.
ak Ak
+ 1
(n)
(n),t
e (hebt , bk i)
, ak , ak )P r(ak |k )
15.
U
ak
k,l
N R(s
16.
(t < )
17.
ok k
18.
sum 0, ebt+1
k,l reach(t + 1)[|k |ak + ok ]
(n),t
(n)
19.
isk
= hs(n),t , k ebtk,l
(n)
(n)
20.
P r(Ak |k ) APPROXPOLICY(k , l 1)
t+1
21.
ak Ak ,
Sk
+
(n)
t+1
22.
sum Ok (ok |s , ak , ak )P r(is(n),t+1 |is(n),t , ak , ak )P r(ak |k )
+
e t1 (ebt+1 )
e (hebt , bk i)
N1 sum U
23.
U
ak
k,l
k,l
e (hebt , bk i))
eaT (hebt , bk i) previously best U
24.
new value U
k,l
k,l
k
e (hebt , bk i) > U
e (hebt , bk i)
25.
(U
ak
k,l
k,l
e (hebt , bk i) U
e (hebt , bk i)
26.
U
ak
k,l
k,l

27.
OPT(hebk,l , bk i)

28.
OPT(hebtk,l , bk i) ak
29. ak Ak
30.
(ak OPT(heb0k,l , bk i)
1
31.
P r(ak |k )
|OPT(heb0k,l ,bk i)|
32.
else
33.
P r(ak |k ) 0
34. return P r(Ak |k )

Figure 18: Computing approximately optimal finite horizon policy tree given model containing
initial sampled belief. l = 0, exact POMDP policy tree computed.

334

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

References
Alon, N., & Spencer, J. (2000). Probabilistic Method. John Wiley Sons.
Aumann, R. J. (1999). Interactive epistemology i: Knowledge. International Journal Game
Theory, 28, 263300.
Battigalli, P., & Siniscalchi, M. (1999). Hierarchies conditional beliefs interactive epistemology dynamic games. Journal Economic Theory, 88(1), 188230.
Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). complexity decentralized control markov decision processes. Mathematics Operations Research, 27(4),
819840.
Bertsekas, D. (1995). Dynamic Programming optimal control. Athena Scientific.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions
computational leverage. Journal Artificial Intelligence Research, 11, 194.
Brandenburger, A., & Dekel, E. (1993). Hierarchies beliefs common knowledge. Journal
Economic Theory, 59, 189198.
Casbeer, D., Beard, R., McLain, T., Sai-Ming, L., & Mehra, R. (2005). Forest fire monitoring
multiple small uavs. American Control Conference, pp. 35303535.
Collins, M., Dasgupta, S., & R.E.Schapire (2002). generalization principal component analysis
exponential family. Neural Information Processing Systems (NIPS), pp. 617624.
Crisan, D., & Doucet, A. (2002). survey convergence results particle filtering methods
practitioners. IEEE Transactions Signal Processing, 50(3), 736746.
Daum, F., & Huang, J. (2002). Mysterious computational complexity particle filters. Conference Signal Data Processing Small Targets, SPIE Proceedings Series, pp. 418426,
Orlando, FL.
Dobson, A. (2002). Introduction Generalized Linear Models, 3rd Ed. Chapman Hall.
Doshi, P. (2007). Improved state estimation multiagent settings continuous large dscrete
state spaces. Twenty Second Conference Artificial Intelligence (AAAI), pp. 712717.
Doshi, P., & Gmytrasiewicz, P. J. (2005a). Approximating state estimation multiagent settings
using particle filters. Autonomous Agents Multi-agent Systems Conference (AAMAS),
pp. 320327.
Doshi, P., & Gmytrasiewicz, P. J. (2005b). particle filtering based approach approximating
interactive pomdps. Twentieth National Conference Artificial Intelligence (AAAI), pp.
969974.
Doshi, P., & Perez, D. (2008). Generalized point based value iteration interactive pomdps.
Twenty Third Conference Artificial Intelligence (AAAI), pp. 6368.
Doshi, P., Zeng, Y., & Chen, Q. (2007). Graphical models online solutions interactive pomdps.
Autonomous Agents Multiagent Systems Conference (AAMAS), pp. 809816, Honolulu, Hawaii.
Doucet, A., de Freitas, N., Murphy, K., & Russell, S. (2000). Rao-blackwellised particle filtering
dynamic bayesian networks. Uncertainty Artificial Intelligence (UAI), pp. 176183.
335

fiD OSHI & G MYTRASIEWICZ

Doucet, A., Freitas, N. D., & Gordon, N. (2001). Sequential Monte Carlo Methods Practice.
Springer Verlag.
Fagin, R., Halpern, J., Moses, Y., & Vardi, M. (1995). Reasoning Knowledge. MIT Press.
Fox, D., Burgard, W., Kruppa, H., & Thrun, S. (2000). probabilistic approach collaborative
multi-robot localization. Autonomous Robots Heterogenous Multi-Robot Systems, 8(3),
325344.
Fudenberg, D., & Levine, D. K. (1998). Theory Learning Games. MIT Press.
Gelman, A., Carlin, J., Stern, H., & Rubin, D. (2004). Bayesian Data Analysis, Second Edition.
Chapman Hall/CRC.
Geweke, J. (1989). Bayesian inference econometric models using monte carlo integration. Econometrica, 57, 13171339.
Gmytrasiewicz, P., & Doshi, P. (2005). framework sequential planning multiagent settings.
Journal Artificial Intelligence Research, 24, 4979.
Gordon, N., Salmond, D., & Smith, A. (1993). Novel approach non-linear/non-gaussian bayesian
state estimation. IEEE Proceedings-F, 140(2), 107113.
Harsanyi, J. C. (1967). Games incomplete information played bayesian players. Management Science, 14(3), 159182.
Hastings, W. K. (1970). Monte carlo sampling methods using markov chains applications.
Biometrika, 57, 97109.
Heifetz, A., & Samet, D. (1998). Topology-free typology beliefs. Journal Economic Theory,
82, 324341.
Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning acting partially observable
stochastic domains. Artificial Intelligence, 101, 99134.
Kearns, M., Mansour, Y., & Ng, A. (2002). sparse sampling algorithm near-optimal planning
large markov decision processes. Machine Learning, 49, 193208.
Koller, D., & Lerner, U. (2001). Sampling factored dynamic systems. Doucet, A., Freitas,
N. D., & Gordon, N. (Eds.), Sequential Monte Carlo Methods Practice. Springer.
Kramer, S. C., & Sorenson, H. (1988). Recursive bayesian estimation using piecewise constant
approximations. Automatica, 24, 789801.
Li, M., & Vitanyi, P. (1997). Introduction Kolmogorov Complexity Applications.
Springer.
Mertens, J., & Zamir, S. (1985). Formulation bayesian analysis games incomplete
information. International Journal Game Theory, 14, 129.
Murphy, D., & Cycon, J. (1998). Applications mini vtol uav law enforcement. SPIE
3577:Sensors, C3I, Information, Training Technologies Law Enforcement.
Ortiz, L., & Kaelbling, L. (2000). Sampling methods action selection influence diagrams.
Seventeenth National Conference Artificial Intelligence (AAAI), pp. 378385, Austin, TX.
Paquet, S., Tobin, L., & Chaib-draa, B. (2005). online pomdp algorithm complex multiagent
environments. International Conference Autonomous Agents Multiagent Systems
(AAMAS), pp. 970977, Utrecht, Netherlands.
336

fiM ONTE C ARLO AMPLING ETHODS PPROXIMATING I-POMDP

Pineau, J., Gordon, G., & Thrun, S. (2006). Anytime point-based value iteration large pomdps.
Journal Artificial Intelligence Research, 27, 335380.
Poupart, P., & Boutilier, C. (2003). Value-directed compression pomdps. Neural Information
Processing Systems (NIPS), pp. 15471554.
Poupart, P., & Boutilier, C. (2004). Vdcbpi: approximate algorithm scalable large-scale
pomdps. Neural Information Processing Systems (NIPS), pp. 10811088.
Poupart, P., Ortiz, L., & Boutilier, C. (2001). Value-directed sampling methods belief monitoring
pomdps. Uncertainty Artificial Intelligence (UAI), pp. 453461.
Ross, S., Pineau, J., Paquet, S., & Chaib-draa, B. (2008). Online planning algorithms pomdps.
Journal Artificial Intelligence Research (JAIR), 32, 663704.
Roy, N., Gordon, G., & Thrun, S. (2005). Finding approximate pomdp solutions belief
compression. Journal Artificial Intelligence Research (JAIR), 23, 1 40.
Russell, S., & Norvig, P. (2003). Artificial Intelligence: Modern Approach (Second Edition).
Prentice Hall.
Saad, Y. (1996). Iterative Methods Sparse Linear Systems. PWS, Boston.
Sarris, Z. (2001). Survey uav applications civil markets. IEEE Mediterranean Conference
Control Automation, p. 11.
Schmidt, J. P., Siegel, A., & Srinivasan, A. (1995). Chernoff-hoeffding bounds applications
limited independence. SIAM Journal Discrete Mathematics, 8(2), 223250.
Seuken, S., & Zilberstein, S. (2007). Improved memory bounded dynamic programming decentralized pomdps. Uncertainty Artificial Intelligence (UAI), pp. 20092015.
Seuken, S., & Zilberstein, S. (2008). Formal models algorithms decentralized decision
making uncertainty. Journal Autonomous Agents Multiagent Systems, 17(2),
190250.
Smallwood, R., & Sondik, E. (1973). optimal control partially observable markov decision
processes finite horizon. Operations Research, 21, 10711088.
Sorenson, H. W., & Alspach, D. L. (1971). Recursive bayesian estimation using gaussian sums.
Automatica, 7, 465479.
Sorenson, H. W. (Ed.). (1985). Kalman Filtering: Theory Application. IEEE Press, New York.
Thrun, S. (2000). Monte carlo pomdps. Neural Information Processing Systems (NIPS), pp.
10641070.
Tsitsiklis, J., & Roy, B. V. (1996). Feature-based methods large scale dynamic programming.
Machine Learning, 22, 5994.
Wang, T., Lizotte, D., Bowling, M., & Schuurmans, D. (2005). Bayesian sparse sampling online
reward optimization. International Conference Machine Learning (ICML), pp. 956
963.

337

fiJournal Artificial Intelligence Research 34 (2009) 61-88

Submitted 04/08; published 02/09

Asynchronous Forward Bounding Distributed COPs
Amir Gershman
Amnon Meisels
Roie Zivan

AMIRGER @ CS . BGU . AC . IL
@ CS . BGU . AC . IL
ZIVANR @ CS . BGU . AC . IL

Department Computer Science,
Ben-Gurion University Negev,
Beer-Sheva, 84-105, Israel

Abstract
new search algorithm solving distributed constraint optimization problems (DisCOPs)
presented. Agents assign variables sequentially compute bounds partial assignments
asynchronously. asynchronous bounds computation based propagation partial
assignments. asynchronous forward-bounding algorithm (AFB) distributed optimization
search algorithm keeps one consistent partial assignment times. algorithm described detail correctness proven. Experimental evaluation shows AFB outperforms
synchronous branch bound many orders magnitude, produces phase transition
tightness problem increases. analogous effect phase transition
observed local consistency maintenance applied MaxCSPs. AFB algorithm
enhanced addition backjumping mechanism, resulting AFB-BJ algorithm.
Distributed backjumping based accumulated information bounds values processing concurrently queue candidate goals next move back. AFB-BJ algorithm
compared experimentally DisCOP algorithms (ADOPT, DPOP, OptAPO) shown
efficient algorithm DisCOPs.

1. Introduction
Distributed Constraint Optimization Problem (DisCOP) general framework distributed
problem solving wide range applications Multi-Agent Systems generated
significant interest researchers (Modi, Shen, Tambe, & Yokoo, 2005; Zhang, Xing, Wang, &
Wittenburg, 2005; Petcu & Faltings, 2005a; Mailler & Lesser, 2004; Ali, Koenig, & Tambe, 2005;
Silaghi & Yokoo, 2006). DisCOPs composed agents, holding one variables.
variable domain possible value assignments. Constraints among variables (possibly
held different agents) assign costs combinations value assignments. Agents assign values
variables communicate other, attempting generate solution globally
optimal respect costs constraints (Modi et al., 2005; Petcu & Faltings, 2004).
wide scope motivation research DisCOP, since distributed COPs
elegant model many every day combinatorial problems distributed nature. Take
example large hospital composed many wards. ward constructs weekly timetable
assigning nurses shifts. construction weekly timetable involves solving constraint
optimization problem ward. nurses every ward qualified work
Emergency Room. Hospital regulations require certain number qualified nurses (e.g.
Emergency Room) shift. imposes constraints among timetables different wards
generates complex Distributed COP (Solotorevsky, Gudes, & Meisels, 1996).
c
2009
AI Access Foundation. rights reserved.

fiG ERSHMAN , EISELS , & Z IVAN

Another example sensor networks tracking problem (Zhang, Xing, Wang, & Wittenburg,
2003; Zhang et al., 2005), task assign sensors tracking targets,
maximal number targets tracked sensor collection. solved using
DisCOP model.
DisCOP modeling also solve problems like log based reconciliation (Chong & Hamadi,
2006), copies data base exist several physical locations. Users perform actions
data base copies, user local copy. actions cause data base change,
initially copies identical, later actions change longer
identical. Logs user actions kept. problem merge logs, single log
keeps many actions possible. always possible keep local logs intact,
since actions constrained actions (for example reconcile deletion
item database later print update it).
DisCOPs represent real life problems cannot solved centrally several
reasons, among lack autonomy, single point failure privacy agents.
hospital wards example, wards want maintain degree autonomy local problems
involving constraints every single nurse. sensor example, sensors small
memory computing power therefore cannot solve problem centralized fashion.
database example, centralization possible, issues network bottleneck, computing
power single point failure encourage looking distributed solution.
present paper proposes new distributed search algorithm DisCOPs, Asynchronous
Forward-Bounding (AFB). AFB algorithm agents assign variables generate partial
solution sequentially. innovation proposed algorithm lies propagating partial solutions asynchronously. Propagation partial solutions enables asynchronous updating bounds
cost, early detection need backtrack, hence algorithms name AFB. form
propagating bounds asynchronously turns generate efficient form concurrent
computation participating agents. efficient algorithms use asynchronous
assignment processes, especially hard instances DisCOPs.
overall framework AFB algorithm based Branch Bound scheme. Agents
extend partial solution long lower bound cost exceed global bound,
cost best solution found far. proposed AFB algorithm, state
search process represented data structure called Current Partial Assignment (CPA). CPA
starts empty initializing agent records assignments sends next agent.
cost CPA sum costs constraints includes. Besides current assignment
cost, agents maintain CPA lower bound updated according information
receive yet unassigned agents. agent receives CPA, adds assignments
local variables partial assignment received CPA, assignment lower bound
smaller current global upper bound found. Otherwise, backtracks sending
CPA former agent revise assignment.
agent succeeds extend assignment CPA sends forward copies updated
CPA, requesting unassigned agents compute lower bound estimations cost partial
assignment. assigning agent receive estimations asynchronously time use
update lower bound CPA.
Gathering updated lower bounds future assigning agents, may enable agent discover
lower bound CPA sent forward higher current upper bound (i.e. inconsistent). discovery triggers creation new CPA copy CPA sent
forward. agent resumes search trying replace inconsistent assignment. time
62

fiA SYNCHRONOUS F ORWARD B OUNDING ISTRIBUTED COP

stamp mechanism proposed Nguyen, Sam-Hroud, Faltings (2004) used Meisels
Zivan (2007) used agents determine updated CPA discard obsolete CPAs.
concurrency AFB algorithm achieved fact forward-bounding performed concurrently asynchronously agents. form asynchronicity similar
employed Asynchronous Forward-Checking (AFC) algorithm distributed constraint
satisfaction problems (DisCSPs) (Meisels & Zivan, 2006; Meseguer & Jimenez, 2000). AFB
enhanced backjumping (Zivan & Meisels, 2007), resulting algorithm performs concurrently distributed forward bounding backjumping prunes search space DisCOPs
efficiently. demonstrated extensive experimental evaluation Section 6 AF B
demonstrates phase transition randomly generated DisCOPs (Larrosa & Schiex, 2004).
extensive evaluation includes comparisons performance AF B best DisCOP
search algorithms. include asynchronous branch bound like ADOPT (Modi et al., 2005),
well algorithms based principles - DPOP (Petcu & Faltings, 2005a) uses
two passes pseudo-tree Opt AP O,that divides DisCOP sub-problems (Mailler &
Lesser, 2004).
plan paper follows. Distributed Constraint Optimization presented Section 2. Section 3, AF B algorithm full details presented. Section 4 version
AF B algorithm enhanced conflict directed backjumping (CBJ) presented. correctness proof AF B algorithm presented Section 5. Section 6 extensive empirical
evaluation AF B algorithm presented. AF B compared state art DisCOP
algorithms, ADOP like AF B include centralization problems data
DP OP Opt AP (Petcu & Faltings, 2005a; Mailler & Lesser, 2004), based
different principles. Conclusions presented Section 7.

2. Distributed Constraint Optimization
Formally, DisCOP tuple < A, X , D, R >. finite set agents A1 , A2 , ..., . X
finite set variables X1 ,X2 ,...,Xm . variable held single agent (an agent may hold
one variable). set domains D1 , D2 ,...,Dm . domain Di contains finite set
values assigned variable Xi . R set relations (constraints). constraint
C R defines none-negative cost every possible value combination set variables,
form C : Di1 Di2 . . . Dik R+ {0}. binary constraint refers exactly two
variables form Cij : Di Dj R+ {0}. binary DisCOP DisCOP
constraints binary. assignment (or label) pair including variable, value
variables domain. partial assignment (PA) set assignments, variable
appears once. vars(PA) set variables appear PA, vars(P A) = {Xi |
Di (Xi , a) P A}. constraint C R form C : Di1 Di2 . . . Dik R+ {0}
applicable PA Xi1 , Xi2 , . . . , Xik vars(P A). cost partial assignment PA
sum applicable constraints PA assignments PA. full assignment partial
assignment includes variables (vars(P A) = X ). goal find full assignment
minimal cost.
paper, assume agent owns single variable, use term agent
variable interchangeably, assume agent Ai holds variable Xi (Modi et al., 2005; Petcu &
Faltings, 2005a; Mailler & Lesser, 2004). assume constraints binary
delay delivering message finite (Yokoo, 2000a; Modi et al., 2005). Furthermore, assume
static final order agents, known agents participating search process (Yokoo,
63

fiG ERSHMAN , EISELS , & Z IVAN

Figure 1: example DisCOP. variable two values R B, constraints
form shown table left.

2000a). assumptions commonly used DisCSP DisCOP algorithms (Yokoo, 2000a;
Modi et al., 2005).
Example 1 example DisCOP presented figure 1. 4 variables, variable
held different agent. domains variables contain exactly two values R B.
Lines variables represent (binary) constraints. cost constraints shown
table left. partial assignment {(X1 , R)} cost zero, since constraint
applicable it. partial assignment {(X1 , R), (X4 , R)} also cost zero, since
constraint applicable it. partial assignment {(X1 , R), (X2 , R)} cost two, due
constraint C1,2 . partial assignment {(X1 , R), (X2 , R), (X3 , B)} cost four, due
constraints C1,2 , C2,3 , C1,3 . One solution {(X1 , R), (X2 , B), (X3 , R), (X4 , R)}
cost five. solution since full assignment lower cost.

3. Asynchronous Forward Bounding
AFB algorithm single up-to-date current partial assignment passed among agents.
Agents assign variables hold up-to-date CPA.
CPA unique message passed agents, carries partial assignment
agents attempt extend complete optimal solution assigning variables
it. CPA also carries accumulated cost constraints assignments contains,
well unique time-stamp.
Due asynchronous nature algorithm, multiple CPAs may present instant,
however single CPA includes update date partial assignment. CPA
highest timestamp.
one agent performs assignment single CPA time. Copies CPA
sent forward concurrently processed multiple agents. unassigned agent computes
lower bound cost assigning value variable, sends bound back agent
64

fiA SYNCHRONOUS F ORWARD B OUNDING ISTRIBUTED COP

performed assignment. assigning agent uses bounds prune sub-spaces
search-space contain full assignment cost lower best full assignment
found far. total order among agents assumed (A1 assumed first agent order,
assumed last).
detail, every agent adds assignment CPA sends forward copies CPA,
messages term FB CPA, agents whose assignments yet CPA. agent
receiving FB CPA message computes lower bound cost increment caused adding
assignment variable. estimated cost sent back agent sent FB CPA
message via FB ESTIMATE messages. computation bound detailed section 3.1.
Notice possible assigning agent already sent CPA forward time
estimations received. estimations indicate CPA exceeds bound, agent
generate new CPA, different local assignment (and higher timestamp associated
it) continue search new CPA. timestamping mechanism insures
obsolete CPA (eventually) discarded regardless current location. timestamp
mechanism described section 3.3.
3.1 AFB - Computing Lower Bound Estimation Cost Increment
computation lower bound cost increment caused adding assignment
agents local variable done follows.
Denote cost((i, v), (j, u)) cost assigning Ai = v Aj = u. agent Ai
value domain v Di , denote minimal cost assignment (i,v) incurred
agent Aj hj (v) = minuDj (cost((i, v), (j, u))). define h(v), total cost assigning
value v, sum hj (v) j > i. Intuitively, h(v) lower bound cost
constraints involving assignment Ai = v agents Aj j > i. Note
bound computed per agent, since independent assignments higher priority
agents.
agent Ai , receives F B CP message, compute every v Di
cost increment assigning v value, i.e. sum cost v assignments
included CP A, h(v). sum these, denoted f (v). lowest calculated f (v)
among values v Di chosen lower bound estimation cost increment agent
Ai .
Figure 2 presents constraint network. Large ovals represent variables small circles represent values. presented constraint network, A1 already assigned value v1 A2 , A3 , A4
unassigned. Let us assume cost every constraint one. cost v3 increase
one due constraint current assignment thus f (v3 ) = 1. Since v4 constrained
v8 v9 , assigning value trigger cost increment A4 performs assignment.
Therefore h(v4 ) = 1 admissible lower bound cost constraints value
lower priority agents. Since v4 conflict assignments CPA, f (v4 ) = 1
well. f (v5 ) = 3 assignment conflicts assignment CPA addition
conflicts values two remaining agents.
Since h(v) takes account constraints Ai lower priority agents (Aj s.t. j > i),
unassigned lower priority agents need estimate cost constraints Ai . Therefore,
estimations accumulated summed agent initiated forward
bounding process compute lower bound cost complete assignment extended
CPA.
65

fiG ERSHMAN , EISELS , & Z IVAN

Figure 2: simple DisCOP, demonstration
formally define:
Definition 1 CPA current partial assignment, containing assignments made agents
A1 , . . . , Ai1 .
Let us define notions past, local future costs definitions 2, 3 4.
Definition 2 PC (Past-Cost) added cost assignments made higher priority agents
CPA (the costs incurred agents A1 , . . . , Ai1 .
Definition 3 LC(v) (Local-Cost) cost incurred CPA Ai would assign value v
add CPA. Therefore,
X
LC(v) =
cost((i, v), (j, w))
(Aj ,w)CP

Definition 4 FC(v) (Future-Cost) sum lower bounds cost increments caused
agents Ai+1 , . . . , CPA additional assignment Ai = v.
X
F C(v) =
minwDj (f (w)), s.t Ai = v added CP
j>i

definitions allow us compute lower bound cost full assignment
extended CPA, use bound order prune parts search space. agent
(Ai ) receives CPA, question, lower bound would extended
assignment Ai = v. PC LC(v) known agent, FC(v) computed
time, requesting future agents (lower priority agents) compute lower bounds
send back Ai . sum PC + LC(v) + FC(v) composes lower bound, used
prune search spaces. happen agent knows full assignment already
66

fiA SYNCHRONOUS F ORWARD B OUNDING ISTRIBUTED COP

found cost lower sum, therefore exploring search-space would lead
better cost solutions.
Thus, asynchronous forward bounding enables agents early detection partial assignments
cannot extended complete assignments cost smaller known upper bound,
initiate backtracks early possible.
3.2 AFB - Algorithm Description
AFB algorithm run agents DisCOP. agent first calls procedure
init responds messages receives ERM E message. algorithm
presented Figure 3.1. computation bounds, time-stamping mechanism
shown, explained text.
initialization, agent updates B cost best full assignment found far
since assignment found, set infinity (line 1). first agent (A1 ) creates
empty CPA begins search process calling assign CPA (lines 3-4), order find
value assignment variable.
agent receiving CPA (when received CPA MSG), first makes sure relevant. time
stamp mechanism used determine relevance CPA explained Section 3.3.
CPAs time-stamp reveals date CPA, message discarded.
case, agent processing message already received message implying
assignment agent higher priority itself, changed.
message discarded, agent saves received PA local CPA variable (line 7). Then,
agent checks received PA (without assignment variable) exceed
allowed cost B (lines 8-10). exceed bound, tries assign value
variable (or replace existing assignment case one already) calling assign CPA (line
13). bound exceeded, backtrack initiated (line 11) CPA sent higher
priority agent, since cost already high (even without assignment variable).
Procedure assign CPA attempts find value assignment, current agent, within
bounds current CPA. First, estimates related prior assignments cleared (line 19). Next,
agent attempts assign every value domain already try. CPA arrived
without assignment variable, tries every value domain. Otherwise, search
value continued value following last assigned value. assigned value must
sum cost CPA lower bound cost increment caused
assignment exceed upper bound B (lines 20-22). value found,
assignment higher priority agent must altered, backtrack called (line 23).
Otherwise, agent assigns selected value CPA.
agent last agent (An ), complete assignment reached, accumulated cost lower B, broadcasted agents (line 27). broadcast inform
agents new bound cost full assignment, cause update upper
bound B.
agent holding CPA (An ) continues search, updating bound B, calling
assign CPA (line 29). current value picked call, since CPAs cost
assignment equal B, procedure requires cost lower B.
agent continue search, testing values, backtracking case lead
improvement.
67

fiG ERSHMAN , EISELS , & Z IVAN

procedure init:
1. B
2. (Ai = A1 )
3.
generate CP A()
4.
assign CP A()
received (FB CPA, Aj , P A)
5. f estimation based received P A.
6. send (F B EST IM E, f , P A, Ai ) Aj
received (CPA MSG, P A)
7. CP P
8. empCP P
9. empCP contains assignment Ai , remove
10. (T empCP A.cost B)
11.
backtrack()
12. else
13. assign CP A()
received (FB ESTIMATE, estimate, P , Aj )
14. save estimate
15. ( CPA.cost + saved estimates) B )
16. assign CP A()
received (NEW SOLUTION, P A)
17. B CP P
18. B P A.cost
procedure assign CPA:
19. clear estimations
20. CP contains assignment Ai = w, remove
21. iterate (from last assigned value) Di found
v Di s.t. CP A.cost + f (v) < B
22. value exists
23.
backtrack()
24. else
25.
assign Ai = v
26. CP full assignment
27.
broadcast (NEW SOLUTION, CPA )
28.
B CP A.cost
29.
assign CP A()
30. else
31.
send(CPA MSG, CPA) Ai+1
32.
forall j >
33.
send(FB CPA, Ai , CPA) Aj
procedure backtrack:
34. clear estimates
35. (Ai = A1 )
36.
broadcast(TERMINATE)
37. else
38.
send(CPA MSG, CPA) Ai1

Figure 3: procedures AFB Algorithm

68

fiA SYNCHRONOUS F ORWARD B OUNDING ISTRIBUTED COP

agent holding CPA last agent (line 30), CPA sent forward
next unassigned agent, additional value assignment (line 31). Concurrently, forward bounding
requests (i.e. FB CPA messages) sent lower priority agents (lines 32-33).
Agent receiving forward bounding request (when received FB CPA) agent Aj ,
uses time-stamp mechanism ignore irrelevant messages. message relevant,
agent computes estimate (lower bound) cost incurred lowest cost assignment
variable (line 5). exact computation estimation described Section 3.1 (it
minimal f (v) v Di ). estimation attached message sent back
sender, FB ESTIMATE message.
agent receiving bound estimation (when received FB ESTIMATE) lower priority
agent Aj (in response forward bounding message) ignores estimate already
abandoned partial assignment (identified using time-stamp mechanism). Otherwise, saves
estimate (line 14) checks new estimate causes current partial assignment exceed
bound B (line 15). case, agent calls assign CP (line 16) order change
value assignment (or backtrack case valid assignment cannot found).
call backtrack made whenever current agent cannot find valid value (i.e.
bound B). case, agent clears saved estimates, sends CPA backwards
agent Ai1 (line 38). agent first agent (nowhere backtrack to), terminate broadcast
ends search process agents (line 36). algorithm reports optimal solution
cost B, full assignment cost B CP A.
3.3 Time-Stamp Mechanism
mentioned previously, AFB uses time-stamp mechanism (Nguyen et al., 2004; Meisels &
Zivan, 2007) determine relevance CPA. requirements mechanism
given two messages two different partial assignments, must determine one
obsolete. obsolete partial assignment one abandoned search process
one assigned agents changed assignment. requirement accomplished
time-stamping mechanism following way. agent keeps local running-assignment
counter. Whenever performs assignment increments local counter. Whenever sends
message containing assignment, agent copies current counter onto message.
message holds vector containing counters agents passed through. i-th element
vector corresponds Ai counter. vector fact time-stamp. lexicographical
comparison two vectors reveal time-stamp up-to-date.
agent saves copy knows up-to-date time-stamp. receiving
new message newer time-stamp, agent updates local saved latest time-stamp.
Suppose agent Ai receives message time-stamp lexicographically smaller
locally saved latest, comparing first 1 elements vector. means
message based combination assignments already abandoned message
discarded. messages time-stamp first 1 elemental equal greater
locally saved best time-stamp message processed further.
vectors counters might appear require lot space, number assignments
grow exponentially number agents. However, agent (Ai ) resets local counter
zero time assignments higher priority agents altered, counters remain small
(log size value domain), mechanism remain correct.
69

fiG ERSHMAN , EISELS , & Z IVAN

3.4 AFB - Example Run
Suppose run AFB DisCOP figure 1. X1 create empty CPA, assign first value
R pass CPA X2 . CPA travel X2 , X3 finally X4 , agent
assigning first value (R) along way finally X4 full assignment
total accumulated cost 8. cost broadcasted agents (line 27 figure 3.1)
new upper bound (instead infinity). Next, X4 call assign CP procedure (line 29).
call result new assignment X4 , value B, since resulting full assignment
cost 7. cause another broadcast update upper bound another
call assign CP A. next call, X4 empty domain forced backtrack
CPA X3 . CPA contains assignments X1 = X2 = X3 = R, total accumulated cost
6 upper bound. Therefore X3 call assign CP (line 13). Examining
remaining values, X3 explores assignment B result CPA cost 4
(line 21), current upper bound B. CPA sent X4 (line 31). X4 calls
assign CP procedure (line 13). value R result CPA cost 6, better
upper bound B 7, therefore broadcasted (line 27). next value, B, explored
X4 results CPA cost 5, also broadcasted. CPA sent backwards X3 .
X3 values try, also backtracks CPA, X2 . X2 assigns next value, B,
sends CPA X3 . addition X2 also sends copies CPA FB CPA messages X3
X4 (line 33). X3 receives FB CPA, computes estimation 3 (because X3
R would increase CPAs cost 3 B would increase 4), sends
information back X2 (line 6). Suppose X4 also receives F B CP A, replies
estimation 1. CPA explores sub-search X2 = B (passing X3
X4 ), estimations arrive X2 . X2 saves estimations adds up. leads
discovery backtrack needed, since CPAs cost 1 (because X1 = R, X2 = B)
additional estimations 4 results sum equal upper bound B (line 15). Therefore,
X2 abandons assignment attempts assign next value (calling assign CP - line 16).
Since X2 values, call results backtrack (line 23). CPA sent backtrack
higher timestamp value CPA previously sent forward X2 , former CPA
would eventually discarded.
3.5 Discussion - Concurrency, Robustness, Privacy Asynchronicity
point time run AFB, single most-up-to-date CPA system.
agent adds assignment holds it, assignments performed sequentially. One
might think would necessarily result poor performance, search process try
take advantage existing multiple computational resources available it. concurrency
AFB comes use forward-bounding mechanism. CPA held one
agent, many copies sent forward, collection agents compute concurrently lower
bounds CPA. CPA advances next agent, process repeats,
unassigned agents constantly kept working, either receive CPA,
need compute bounds partial assignment.
degree asynchronicity similar employed Asynchronous Forward-Checking
AFC algorithm DisCSPs (Meseguer & Jimenez, 2000; Meisels & Zivan, 2006). AFC performs
similar process agents receive forward-checking messages agents performed assignments. unassigned agents perform forward-checking (checking least
one value consistent previous assignments). AFB agents compute lower
70

fiA SYNCHRONOUS F ORWARD B OUNDING ISTRIBUTED COP

bound local cost increment due assignments made previous agents. Due
similarity named algorithm Asynchronous Forward-Bounding.
AFBs approach quite different used asynchronous assignments algorithms
ADOPT ABT (Modi et al., 2005; Bessiere, Maestre, Brito, & Meseguer, 2005).
algorithms search process attempts perform assignments concurrently collection
agents. Since many agents assigning variables simultaneously, probability
must handled algorithm, current agents view assignments made agents
incorrect. due fact agents concurrently alter assignments. algorithm
must able deal uncertainty.
search process performs assignments asynchronously may expected save time
since agents need wait assignments past agents reach them, done sequentially assigning algorithm. However, asynchronously assigning algorithms must also deal
inconsistencies caused message delay. example, several higher priority agents change
assignments messages received (the others delayed) computation
performed based inconsistent agent view. type scenario, computation based inconsistent partial assignment, completely avoided sequentially assigning
algorithms.
One variation AFB algorithm agents sent FB-CPA messages, send
messages subset target agents direct constraint sending
agent. may useful communication agents limited (agents may communicate agents direct conflict) would keep algorithm correct.
change may two effects. First, less agents return bounds sending agents.
bounds significant (greater zero) since take account constraints assignments previous agents (which may conflicted with) also constraints
receiving agent agents lower priority (constraint unassigned agents). Receiving less
lower bounds would invalidate correctness algorithm may cause search process needlessly explore sub-spaces could discovered dead-ends. Second,
detection obsolete CPAs may delayed since less agents receive higher timestamp (which
FB-CPA may contain). mechanism would remain correct since eventually another FB-CPA
CPA would reach agent receive FB-CPA, however may take
time single cycle messages (in words, time travel time
single message two agents). AFB algorithm intentionally presented algorithm sends FB messages unassigned agents, since constraint communication
agents assumed. case constraints exist, one attempts reduce number
messages sent algorithm, variation explored.
Privacy considered one main motivations solving problems distributively. common model distributed search algorithms DisCSPs DisCOPs enables assignments
Nogoods passed among agents (Yokoo, Ishida, Durfee, & Kuwabara, 1992; Yokoo, 2000b;
Bessiere et al., 2005; Modi et al., 2005; Zivan & Meisels, 2006; Meisels & Zivan, 2007). AF B follows model proposed Yokoo, sending assignments forward bounds partial assignments
(N ogoods) backwards. additional privacy drawback AF B fact agents learn
assignments non neighboring agents via CPAs receive neighbors.
problem easily solved AF B simple use encryption. every pair neighboring
agents share encryption key, agent would able learn assignments
neighbors receives CPA. use limited encryption DisCOP algorithms
recently proposed DP OP (Greenstadt, Grosz, & Smith, 2007).
71

fiG ERSHMAN , EISELS , & Z IVAN

If, due privacy, constraints partially known two constrained agents,
part constraint known constrained agents, bound computation
mechanism must adjusted AFB. type constraints discussed DisCSP algorithms (Brito, Meisels, Meseguer, & Zivan, 2008). best knowledge, DisCOP solver
far handled constraints. remains interesting possible extension AFB part
future work.
Robustness another important aspect distributed search algorithm. assumed
messages delivered order sent messages lost. However
message passing susceptible losses corruption data, AFB may terminate (if, say,
CPA message lost). also possible local data held agents corrupt (due
mechanical failure example). solution would build self-stabilizing algorithm.
Self stabilization distributed systems (Dijkstra, 1974) ability system respond
transient failures eventually reaching maintaining legal state. self stabilizing version
shown simple DFS algorithm DisCSPs (Collin, Dechter, & Katz, 1999). Based
self-stabilizing DFS algorithm, self-stabilizing version DPOP developed (Petcu &
Faltings, 2005b). However self-stabilizing DisCSP/DisCOP solvers best
authors knowledge. Clearly, thorough study robustness self-stabilization
required DisCOP algorithms.
conclude, AFB algorithm includes concurrent computation multiple agents, without
deal uncertainty comes asynchronous assignments. agent
receives message containing partial assignment knows certainty given partial assignment one supposed receive, result network delay inconsistency.
Therefore, AFB concurrent computation certainty working consistent partial assignments. results much better performance hard instances random DisCOPs,
demonstrated empirical evaluation section 6.

4. AFB CBJ
centralized distributed CSPs backjumping accomplished maintaining data
structures allow agent deduce latest agent (in order assignments
made) whose changed assignment could possibly lead solution. agent
found, assignments following agents unmade search process backjumps
agent (Prosser, 1993).
similar process designed branch bound based solvers COPs DisCOPs.
Consider sequence assignments agents A1 , A2 , A3 , A4 , A5 A5 determined
none possible value assignments lead full assignment cost lower cost
best full assignment found far. Clearly, A5 must backtrack.
chronological backtracking, search process would simply return previous agent,
namely A4 , change assignment. However, A5 sometimes determine value
change A4 would suffice reach full assignment lower cost. Intuitively, A5 safely
backjump A3 , compute lower bound cost full assignment extended
assignments A1 , A2 A3 , show bound greater equal cost best
full assignment found far. intuitive basis backjumping added AFB.
formally, let us consider scenario Ai decides backtrack, cost
best full assignment found far B (e.g. upper bound current state search).
current partial assignment includes assignments agents A1 , ..., Ai1 .
72

fiA SYNCHRONOUS F ORWARD B OUNDING ISTRIBUTED COP

Definition 5 CPA[1..k] set assignments made agents A1 , . . . , Ak current partial
assignment. define CP A[1..0] = {}.
Definition 6 FA[k] set full assignments, include assignments appearing
CPA[1..k]. words, set contains full assignments extended
assignments appearing CPA[1..k]. Naturally, FA[0] set possible full assignments.
backtrack, instead simply backtracking previous agent, Ai performs following
actions: computes lower bound cost full assignment FA[i-2]. bound
smaller B, backtracks Ai1 like would chronological backtracking. However,
bound greater equal B, backtracking Ai1 would little good. value
change Ai1 alone could result full assignment cost lower B. result, Ai knows
safely backjump Ai2 . may possible Ai backjump even further, depending
lower bound cost full assignment
FA[i-3]. bound smaller B, backjumps Ai2 . Otherwise, knows safely
backjump Ai3 . Similar checks made necessity backjump further.
backjumping procedure relies computation lower bounds sets full assignments (FA[k]). Next, show Ai compute lower bounds. Let us define
notions past, local future costs definitions 7, 8 9.
Definition 7 PC (Past-Costs) vector size n+1, k-th element (0 k n)
equal cost CPA[1..k].
Definition 8 LC(v) (Local-Costs) vector size n + 1 computed Ai held it,
k-th element (0 k n)
X
LC(v)[k] =
cost(Ai = v, Aj = vj )
(Aj ,vj )CP s.t jk

Since CPA held Ai includes assignments A1 , . . . , Ai1 ,
j i, LC(v)[i 1] = LC(v)[j]
Intuitively, LC(v)[i] accumulated cost value v Ai , respect assignments
CPA[1..i].
Definition 9 FCj (v) (Future-Costs) vector size n+1, k-th element (0 k n)
contains lower bound cost assigning value Aj respect partial assignment CPA[1..k]. Assume structure held agent Ai . k CPA[1..k] contains
assignment Ai = v, k < value v Ai irrelevant appear CPA[1..k].
vectors provide additive lower bounds full assignments start current
CPA k, FA[k]. PC[k] isPthe exact cost first k assignments, LC(v)[k] exact cost
assignment Ai = v, j>i F Cj (v)[k] lower bound assignments Ai+1 , ..., .
Therefore, sum
X
FALB(v)[k] = LC(v)[k] + P C[k] +
F Cj (v)[k]
j>i

73

fiG ERSHMAN , EISELS , & Z IVAN

Figure 4: example DisCOP
Full Assignment Lower Bound cost full assignment extended CPA[1..k]
Ai = v.
FA[k] contains full assignments extended CPA[1..k], limited assignments
Ai = v. go FALB(v)[k], possible values v Di produce lower
bound assignment FA[k].
Definition 10 FALB[k] = minvDi (F ALB(v)[k]).
FALB[k] lower bound cost full assignment extended CPA[1..k].
distributed branch bound algorithm, bound computed Ai . PC - cost
previous agents sent along value assignment messages Ai . LC(v) - cost
assigning v Ai computed Ai . Ai requests agents ordered it, Aj (j > i),
compute FCj send results back Ai . part already existing AFB mechanism
forward bounding.
AFB algorithm (Gershman, Meisels, & Zivan, 2007) Ai already requests unassigned
agents compute lower bounds CPA send back results. additional bounds
needed backjumping easily added existing AFB framework.
4.1 Backjumping Example
demonstrate backjumping possibility, consider DisCOP Figure 4 (again, large ovals
represent variables small circles represent values). Let us assume search begins
A1 assigning value sending CP forward A2 . A2 , A3 , A4 , A5 assign
value get full assignment cost 12. search continues, fully
exploring sub-space A1 = a, A2 = a, best assignment found A1 = a, A2 =
a, A3 = b, A4 = a, A5 = b total cost B=6. Assume A3 holding CP
receiving future agent (A4 A5 ). A3 exhausted value domain must
backtrack. computes:
F ALB(a)[1] = P C[1] + LC(a)[1] + (F C4 (a)[1] + F C5 (a)[1])
74

fiA SYNCHRONOUS F ORWARD B OUNDING ISTRIBUTED COP

= 0 + 2 + (3 + 2) = 7
F ALB(b)[1] = P C[1] + LC(b)[1] + (F C4 (b)[1] + F C5 (b)[1])
= 0 + 1 + (3 + 2) = 6
F ALB[1] = min(F ALB(a)[1], F LAB(b)[1]) = 6
F ALB[1] B, therefore A3 knows full assignment extended {A1 = a} would cost
least 6. full assignment cost already discovered, need explore
rest sub-space, safely backjump search process back A1 , change
value b. Backtracking A2 leaves search process within {A1 = a} sub-space,
A3 knows cannot lead full assignment lower cost.
4.2 AFB-BJ Algorithm
AFB-BJ algorithm run agents DisCOP. agent first calls procedure init responds messages receives TERMINATE message. algorithm
presented figures 5 6. pure AFB, timestamping mechanism used messages.
timestamping mechanism used AFB used AFB-BJ determine messages relevant obsolete. simplicity choose omit pseudo-code detailing calculation LC, PC, FC FALB, described Section 4.1.
algorithm starts agent calling init awaiting messages termination.
first, agent updates B cost best full assignment found far since
assignment found, set infinity (line 1). first agent (A1 ) creates empty
CPA begins search process calling assign CPA (lines 3-4), order find value
assignment variable.
agent receiving CPA (when received CPA MSG), checks time-stamp associated
it. date CP discarded. message discarded, agent saves
received PA local CPA variable (line 7). case CPA received higher priority
agent, estimations future agents F Cj longer relevant discarded,
domain values must reordered updated cost (lines 9-11). Then, agent attempts
assign next value calling assign CPA (line 16) backtrack needed (line 14).
Procedure assign CPA attempts find value assignment, current agent. assigned
value must sum cost CPA lower bound cost increment
caused assignment exceed upper bound B (lines 23). value found,
assignment higher priority agent must altered, backtrack called (line 25).
full assignment found better best full assignment known far,
broadcast agents (line 29). succeeding assign value, CPA sent forward
next unassigned agent (line 33). Concurrently, forward bounding requests (i.e. FB CPA messages)
sent lower priority agents (lines 34-35).
agent receiving bound estimation (when received FB ESTIMATE) lower priority
agent Aj (in response forward bounding message) ignores estimate already
abandoned partial assignment (identified using time-stamp mechanism). Otherwise, saves
estimate (line 17) checks new estimate causes current partial assignment exceed
bound B (line 18). case, agent calls assign CP (line 19) order change
value assignment (or backtrack case valid assignment cannot found).
75

fiG ERSHMAN , EISELS , & Z IVAN

procedure init:
1. B
2. (Ai = A1 )
3.
generate CP A()
4.
assign CP A()
received (FB CPA, Aj , P A)
5. V estimation vector PA[1..k] (0 k n)
6. send (F B EST IM E, V , P A, Ai ) Aj
received (CPA MSG, P A, Aj )
7. CP P
8. empCP P
9. (j = 1)
10. j re-initialize F Cj (v)
11. reorder domain values v Di LC(v)[i] (from low high)
12. (T empCP contains assignment Ai ) remove
13. (T empCP A.cost B)
14. backtrack()
15. else
16. assign CP A()
received (FB ESTIMATE, V , P , Aj )
17. F Cj (v) V
18. ( FALB(v)[i] B )
19. assign CP A()
received (NEW SOLUTION, P A)
20. B CP P
21. B P A.cost
Figure 5: Initialization message handling procedures AFB-BJ Algorithm

call backtrack made whenever current agent cannot find valid value (i.e.
bound B). case, agent calls backtrackTo() compute agent CPA
sent, backtracks search process (by sending CPA) back agent.
agent first agent (nowhere backtrack to), terminate broadcast ends search process
agents (line 37). algorithm reports optimal solution cost B,
full assignment corresponding cost B CP A.
function backtrackTo computes agent CPA sent. kernel
backjumping (BJ) mechanism. goes candidates, j 1 1, looking
first agent finds chance reaching full assignment lower cost
B. FALB(v)[j-1] lower bound cost full assignment extended CPA[1..j-1],
PC[j]-PC[j-1] cost added CPA Aj assignment. Since Aj picked lowest cost
value domain (its domain ordered line 11), addition two components
76

fiA SYNCHRONOUS F ORWARD B OUNDING ISTRIBUTED COP

procedure assign CPA:
22. CP contains assignment Ai = w, remove
23. iterate (from last assigned value) Di first value satisfying
v Di s.t. CP A.cost + f (v) < B
24. value exists
25.
backtrack()
26. else
27.
assign Ai = v
28.
CP full assignment
29.
broadcast (NEW SOLUTION, CPA )
30.
B CP A.cost
31.
assign CP A()
32.
else
33.
send(CPA MSG, CPA, Ai ) Ai+1
34.
forall j >
35.
send(FB CPA, Ai , CPA) Aj
procedure backtrack:
36. (Ai = A1 )
37.
broadcast(TERMINATE)
38. else
39.
j backtrackTo()
40.
remove assignments Aj+1 , .., Ai CP
41.
send(CPA MSG, CPA, Ai ) Aj
function backtrackTo:
42. j = 1 downto 1
43.
foreach v Di
44.
( FALB(v)[j-1] + (PC[j] - PC[j-1]) < B )
45.
return j
46. broadcast(TERMINATE)
Figure 6: assigning backtracking procedures AFB-BJ Algorithm.
produces accurate lower bound cost full assignment extended CPA[1..j-1].
safely added FALB since adds lower bound cost increment
agent FALB include lower bound.
Example 2 example presented section 4.1, A3 computed FALB(b)[1] added
past costs partial assignments (cost incurred A1 ), local cost A3 , lower
bound cost increment future agents (A4 A5 ). sum safely add cost
added A2 know A2 picked lowest cost assignment.
addition helps tighten FALB reduce search. combined bound smaller
B, surely combination assignments made Aj following agent could
raise cost, already high. case even backjumping back A1 prove
helpful, search process terminated (line 46).
77

fiG ERSHMAN , EISELS , & Z IVAN

5. Correctness AFB
order prove correctness AF B two claims must established. First, algorithm
terminates second algorithm terminates global upper bound B cost
optimal solution. prove termination one show AF B algorithm never goes
endless loop. prove last statement enough show partial assignment
cannot generated once.
Lemma 1 AF B algorithm never generates two identical CPAs.
Assume negation Ai highest priority agent (first order assignments)
generates CPA second time. lets consider possible events immediately
preceded creation.
Case 1 - Ai received CPA message lower priority agent. Let us denote agent Aj ,
j > i. Ai received message, executed lines 7-13 (see Figure 3.1). procedure
backtrack line 14 executed since know Ai generated CPA, procedure would
so. Therefore line 16 executed, procedure assign CPA invoked. Ai executed
lines 22-24. Line 25 executed since invoking backtrack procedure could lead
creation CPA. Therefore, line 24 value described line 23 found exist.
Line 23 searches value Ai remaining value domain, exploring value previously
attempted current set assignments higher priority agents. Since assumed Ai
highest priority agent generates CPA second time, combination higher
priority assignments repeat itself. Therefore, since Ai received current set higher
priority assignments Ai re-pick local value, set high priority assignments
repeat itself, therefore Ai cannot pick value would generate CPA
second time.
Case 2 - Ai received CPA message higher priority agent. Let us denote agent
Aj , j < i. Since assumed Ai highest priority agent generates CPA
second time, combination higher priority assignments repeat itself. Therefore
value Ai would assign next would generate unique CPA, one could generated
before.
Case 3 - Ai received CPA message itself. cannot since Ai never sends
message itself.
Case 4 - Ai received FB ESTIMATE message Aj . j > since FB ESTIMATE
sent response FB CPA messages. sent (line 34) agents lower priority
Ai . Since message caused creation CPA, condition line 19 must
evaluated true, procedure assign CPA line 19 invoked. Similar case 1, lines 22-24
executed line 25 not. Similar case 1, value found line 23. value
repeat value previously picked current set higher priority agent assignments.
time agent received current set higher priority agent assignments due
assumption Ai first generate CPA twice.
Case 5 - procedure init invoked. cannot since CPAs previously generated, CPA generated must unique.
events could immediately preceded creation second identical CPA,
therefore impossible event occur. completes proof lemma.
Termination follows immediately Lemma 1.

78

fiA SYNCHRONOUS F ORWARD B OUNDING ISTRIBUTED COP

Next, one needs prove upon termination complete assignment, corresponding
optimal solution, B CP (see Figure 3.1). one point termination
AF B algorithm, procedure backtrack. So, one needs prove search partial
assignment lead solution lower cost B discarded. Let us consider possible
cases agent discards CPA, changes value skips value let us show
cannot be. Skipping changing value done inside procedure assign CPA
lines 22-24. v value skipped over, condition line 23 holds
CP A.cost + f (v) B. Since B B CP A, CP A.cost + f (v) B B CP
means v could possibly lead solution cost lower B CP termination. Let
us consider possible cases value changed. occurs inside procedure
assign CPA. Let us consider possible cases procedure invoked result
value change.
Case 1 - invoking assign CPA init procedure (line 4). solution could lost since
first assignment performed, part search space skipped
assignment.
Case 2 - invoking assign CPA inside assign CPA procedure (line 31). happens
new best (so far) solution found. obviously changing assignment would lose
solution since saved broadcasted new current solution. discarded
better solution later found.
Case 3 - invoking assign CPA following received FB ESTIMATE message (line 19).
current partial assignment safely discarded, knowing solution lost since
condition line 18 indicated current partial assignment lower bound exceeds
best solution found far.
Case 4 - invoking assign CPA following received CPA MSG message (line 16) Aj
j > i. means CPA returned backtrack fully exploring current sub-space,
therefore changing current assignment would lead potential solution lost.
Case 5 - invoking assign CPA following received CPA MSG message (line 16) Aj
j < i. means CPA received higher priority agent. Ai yet pick
assignment, assignment make lose potential solutions.
Therefore, value skipped change CPA lead loss
potential solution. remaining event may lead solution skipped
CPA discarded. done time-stamping mechanism occurs
agent knows existence up-to-date CPA. CPA created agent
changed assignment calling assign CPA. showed case better solution
lost, therefore safe discard CPA.
conclusion, event value skipped changed CPA discarded, possible better solution lost. Therefore termination, AFB algorithm reports best solution
possible. completes correctness proof AF B algorithm.
order prove correctness AFB-BJ algorithm first prove correctness proposed backjumping method show combination AFB violate AFBs
correctness proven.
order prove correctness backjumping method one need show none
agents assignments algorithm backjumps over, lead solution lower
cost current upper bound. condition performing backjumping agent Aj
(line 44) lower bound cost full assignment extended assignments
79

fiG ERSHMAN , EISELS , & Z IVAN

Figure 7: Total non-concurrent computational steps AFB, ADOPT SBB low density
(p1 =0.4) Max-DisCSP

A1 , .., Aj1 assignment cost Aj exceeds global upper bound B. Since Aj picked
lowest cost value remaining domain (as domain ordered), extending assignments
A1 , .., Aj1 must lead cost greater equal B. Therefore, backjumping back Aj1
cannot discard potentially lower cost solutions. completes correctness proof
AFB-BJ backjumping (function backtrackTo) method.
Assuming correctness AFB, order prove correctness composite algorithm
AFB-BJ enough prove consistency lower bounds computed agents AFBBJ. lower bounds computed AFB-BJ include FC, LC PC described section 4. PC
contained CPA, updated agent receives adds assignment (not
shown code). LC(v) computed current agent Ai whenever assigns v value
assignment. FCj computed Aj line 5 (in figure 5), sent back Ai line 6. Ai
receives saves line 17. lower bounds contained inside vectors correct
PC exactly calculated holding CPA, LC exactly calculated current
agent Ai , bounds FCj bounds computed AFB proven
correct lower bounds assignment Aj . FCj bounds accurate based
current partial assignment since timestamp mechanism prevents processing bounds
based obsolete CPA. Whenever CPA altered higher priority agent, previous
bounds cleared (line 10 figure 5). completes correctness proof AF B BJ.

6. Experimental Evaluation
experiments performed simulator agents simulated threads
communicate message passing. Distributed Optimization problems used
presented experiments random Max-DisCSPs. network constraints,
experiments, generated randomly selecting probability p1 constraint among pair
variables probability p2 , occurrence violation (a non zero cost) among two
assignments values constrained pair variables. uniform random constraints networks
n variables, values domain, constraints density p1 tightness p2 commonly
used experimental evaluations CSP algorithms (cf. (Prosser, 1996)). Max-CSPs commonly
used experimental evaluations constraint optimization problems (COPs) (Larrosa & Schiex,
80

fiA SYNCHRONOUS F ORWARD B OUNDING ISTRIBUTED COP

Figure 8: Total number messages sent AFB, ADOPT SBB low density (p1 =0.4) MaxDisCSP

(a)

(b)

Figure 9: (a) Number none-concurrent steps performed ADOPT, AFB, AFB-minC AFBBJ high density Max-DisCSP (p1 = 0.7). (b) closer look p2 > 0.9

2004). experimental evaluations DisCOPs include graph coloring problems (Modi et al.,
2005; Zhang et al., 2005), subclass Max-DisCSP.
order evaluate performance distributed algorithms, two independent measures
performance used - run time, form non-concurrent steps computation (Zivan &
Meisels, 2006b), communication load, form total number messages sent (Lynch,
1997; Yokoo, 2000a).
first set experiments, performance AF B compared two algorithms.
synchronous B&B algorithm (SBB) (Hirayama & Yokoo, 1997) asynchronous distributed optimization algorithm (ADOP ) (Modi et al., 2005). Figure 7 presents average runtime number non-concurrent computation steps, randomly generated Max-DisCSPs
n = 10 agents, domain size = 10, constraint tightness p1 = 0.4. Figure 8 compares
81

fiG ERSHMAN , EISELS , & Z IVAN

(a)

(b)

Figure 10: (a) Number messages sent ADOPT, AFB, AFB-minC AFB-BJ high density
Max-DisCSP (p1 = 0.7). (b) closer look p2 > 0.9

algorithms problems total number messages sent. figures
clear ADOPT outperforms basic algorithm SBB, accordance past experimental evaluation two algorithms (Modi et al., 2005). also clear AFB outperforms
ADOPT large margin tight (high p2 ) problems. true measures.
second set experiments includes ADOPT algorithm three versions AFB algorithm: AFB, AFB-minC - variation AFB includes dynamic ordering values based
minimal cost (of current CPA), AFB-BJ composite backjumping forwardbounding algorithm. AFB-BJ uses value ordering heuristic AFB-minC. selected order show improved performance AFB-BJ indeed arise backjumping feature value ordering heuristic.
Figure 9 presents average run-time number non-concurrent computation steps,
algorithms: ADOPT, AFB, AFB-minC AFB-BJ, Max-DisCSPs n = 10 agents,
domain size = 10, constraint density p1 = 0.7. Asynchronous optimization (ADOPT)
much slower standard version AFB. Also clear figure, value ordering
heuristic greatly improves AFBs performance. added backjumping improves performance
much further. RHS figure provides zoom section graph
p2 = 0.9 p2 = 0.98. tight problems, ADOPT terminate reasonable
amount time terminated manually (and thus missing graph).
tightness values higher p2 > 0.9 AFB variants demonstrate phase
transition. phase transition behavior AFB algorithms similar lookahead algorithms centralized Max-CSPs (Larrosa & Meseguer, 1996; Larrosa & Schiex, 2004).
explanation phase transition problem difficulty increase exponentially
tightness point. problem becomes over-constrained many
combinations produce highest cost possible combinations fact equal quality,
easily pruned intelligent search.
82

fiA SYNCHRONOUS F ORWARD B OUNDING ISTRIBUTED COP

Figure 11: Number Non-Concurrent Constraint Checks (NCCCs) performed several DisCOP
solvers high density Max-DisCSP (p1 = 0.7) linear scale (top) logarithmic scale (bottom)

Figure 10 presents total number messages sent algorithms. results
measurement closely match results run-time, measured non-concurrent steps.
83

fiG ERSHMAN , EISELS , & Z IVAN

Figure 12: Number Non-Concurrent Constraint Checks (NCCCs) performed several DisCOP
solvers low density MaxDisCSP (p1 = 0.4) logarithmic scale

see ADOPT exponentially rapid growth messages. explanation
growth simple. Following message agent receives ADOPT, several VALUE messages
sent lower priority agents, single COST message sent higher priority agent (Modi
et al., 2005). average, least two messages sent every message received, therefore
total number messages system increases exponentially time.
third batch experiments, includes comparison two additional DisCOP solvers DPOP (Petcu & Faltings, 2005a) OptAPO (Mailler & Lesser, 2004). DPOP performs
linear number computational steps, step performs exponential number computations. number messages DPOP linear (2n) number agents. Similar ADOPT,
DPOP also uses pseudo-tree ordering agents use ordering
algorithms. OptAPO performs partial centralization problem, agents solve
part problem charge of. Therefore, algorithms, evaluation measures
use number (non-concurrent) computational steps inappropriate, since steps
exponentially time consuming. reason, performance algorithms must evaluated different metric. canonical choice number non-concurrent constraint checks
(N CCCs). implementation independent measure includes computations performed within
every single step (Zivan & Meisels, 2006b, 2006a, 2006). number messages sent also
good measure case, since DPOP sends exponentially large messages (but linear
number them) algorithms send exponential amount messages
linear size. Thus present results using N CCCs metric. repeat experimental setup previous experiment randomly generated problems, report total
number non-concurrent constraint checks (NCCCs) figure 11. results presented
logarithmic linear scales.
experiment OptAPO, SBB ADOPT terminate reasonable time
harder problem instances therefore partially absent graphs. computation
84

fiA SYNCHRONOUS F ORWARD B OUNDING ISTRIBUTED COP

DPOP composed agent sending message containing subtrees optimal cost
every possible combination higher priority constrained agents. given constraint density
size message agent sends would effected changing constraint tightness. Therefore, computation performed agent unaffected changing constraint
tightness (p2 ). DPOPs run time expected remain roughly tightness values
experiment. problems low constraint tightness DPOPs performance poor
compared rest algorithms. However, problem tightness increases gap
DPOPs run time rest algorithms narrows, p2 = 0.9 DPOP OptAPO
SBB roughly run time. p2 = 0.99 DPOP outperforms ADOPT, OptAPO SBB
(which terminate). AFB variants outperform DPOP whole range constraint
tightness orders magnitude. OptAPO appears perform slightly better SBB
AFB clearly outperforms orders magnitude. AFB variations produce phase
transition reported previous experiments, AF B BJ comes best performing
algorithm solving random DisCOPs.
results similar experiment low density (p1 = 0.4) Max-DisCSPs presented
figure 12 (notice logarithmic scale). high density problems, DPOP performance unaffected problem tightness, producing roughly similar results tightness values.
low tightness values, OptAPO AFB vastly superior DPOP OptAPO slightly outperforms AFB. tightness increases, OptAPO increases exponentially run-time become
worst performing algorithm. AFB outperforms DPOP tightness values except p2 = 0.9.

7. Conclusions
Asynchronous Forward-Bounding algorithm (AF B) uses asynchronous concurrent constraint propagation top distributed Branch Bound scheme. forward-bounding
protocol AF B maintains local consistency, prevents exploration dead-ends searchspace. run-time network load AFB evaluated asynchronous simulator
randomly generated ax DisCSP s. results evaluation revealed phase-transition
AF Bs performance, tightness problems increased beyond point.
DisCOP solver reported display behavior. similar phase-transition previously
reported centralized COP solvers, part work Larrosa et. al. (Larrosa & Meseguer,
1996; Larrosa & Schiex, 2004). phase-transition observed reported occur
COP solvers, enforce strong enough form local consistency (Larrosa & Meseguer, 1996;
Larrosa & Schiex, 2004). therefore attribute behavior AFB concurrent enforcement
local consistency.
AF B extended. One extension include value ordering heuristic. good ordering heuristic minimum-cost heuristic, values lower cost due assignments
higher priority agents selected first. named version algorithm AFB-minC.
experiments, use heuristic substantially improved performance AF B.
extension AF B enhanced backjumping mechanism. adding small
amount information bounding messages, agents detect lower bound
current partial assignment large (i.e. state inconsistent backtracking required)
able check whether backtracking previous agent indeed help reduce
lower bound resulting partial assignment consistent. Otherwise, search process
backtracks even further. resulting algorithm, AFB-BJ, performs significantly better
versions AFB. comparing AFB-minC AFB-BJ, shown backjumping
85

fiG ERSHMAN , EISELS , & Z IVAN

indeed affect performance, improvement standard AF B result
addition ordering heuristic.
AF B algorithm compared two algorithms based branch & bound
mechanism distributed form - ADOPT SBB (Yokoo, 2000b; Modi et al., 2005).
experimental evaluation clearly demonstrates substantial difference performance
algorithms. Asynchronous distributed optimization (ADOP ) outperforms SBB, AF B outperforms ADOP large margin measures performance. best knowledge evaluation ADOP increasingly tighter problems. experimental
evaluations measured ADOP scalability (by increasing number variables) increasing difficulty (tightness) problems fixed size. exponential growth number
messages ADOP also apparent Figures 8 10(a). Outperforming AF B two
extended versions AF B, AFB-minC AFB-BJ, AFB-BJ best performance.
proposed value ordering heuristic improves performance, adding backjumping
mechanism top that, performance even enhanced.
Although AF B ADOP perform concurrent computation nature concurrency used
different. Concurrency ADOP achieved performing asynchronous assignments. algorithm agent picks value assignment free change
time. Multiple agents may change assignments concurrently. Asynchronous assignments
introduce degree uncertainty regard consistency current partial assignment known agent. fact, scenarios agent may base computation
inconsistent partial assignment, combination assignments performed higher
priority agents aware others most-up-to-date assignment.
Two algorithms used comparisons AF B - ADOP DP OP - use
pseudo-tree ordering agents, allows independent subproblems solved concurrently.
good pseudo-tree ordering problematic find (it NP-hard find optimal ordering),
sometimes even best ordering good enough, due structure specific problem. Overall, orderings become less useful dealing problems high constraint
density.
order evaluate performance AFB, compared tested two
additional DisCOP algorithms. DPOP OptAPO use branch bound find
optimal solution. DPOP algorithm delivers possible partial assignments pseudo-tree
performs exponential number constraints checks two passes pseudo-tree (Petcu
& Faltings, 2005a). OptAPO partitions DisCOP sub-problems, solved mediator
sub-problem (Mailler & Lesser, 2004). performance algorithms expected
different algorithms use branch & bound search. fact, performance DPOP
randomly generated DisCOPs independent tightness problems. results
extensive empirical evaluations algorithms random DisCOPs described section 6
conclusive. AFB algorithm best performing DisCOP algorithm randomly
generated DisCOPs measures performance. performs less non-concurrent constraints
checks sends smaller number messages.
essence, idea behind AF B summed follows - run sequential assignment
optimization process concurrently run parallel many additional processes check consistency partial assignment. main search process slow. point time one
agent holds current partial assignment order extend it. Concurrency achieved via
forward bounding, performed concurrently.
86

fiA SYNCHRONOUS F ORWARD B OUNDING ISTRIBUTED COP

results experimental evaluation show adding concurrent maintenance bounds
sequential assignment process results efficient optimization algorithm (AF B). algorithm outperforms concurrent algorithms hard instances random DisCOPs.

References
Ali, S. M., Koenig, S., & Tambe, M. (2005). Preprocessing techniques accelerating DCOP
algorithm ADOPT.. AAMAS, pp. 10411048.
Bessiere, C., Maestre, A., Brito, I., & Meseguer, P. (2005). Asynchronous Backtracking without
adding links: new member ABT Family. Artificial Intelligence, 161:1-2, 724.
Brito, I., Meisels, A., Meseguer, P., & Zivan, R. (2008). Distributed Constraint Satisfaction
Partially Known Constraints. Constraints, press.
Chong, Y., & Hamadi, Y. (2006). Distributed Log-based Reconciliation. Proc. ECAI-06, pp.
108113.
Collin, Z., Dechter, R., & Katz, S. (1999). Self-Stabilizing Distributed Constraint Satisfaction.
Chicago Journal Theoretical Computer Science, 5.
Dijkstra, E. W. (1974). Self-stabilizing systems spite distributed control. Commun. ACM,
17(11), 643644.
Gershman, A., Meisels, A., & Zivan, R. (2007). Asynchronous Forward-Bounding Backjumping. Distributed Constraints Reasonning workshop, IJCAI-2007 Hyderabad, India.
Greenstadt, R., Grosz, B., & Smith, M. D. (2007). SSDPOP: improving privacy DCOP
secret sharing. AAMAS 07: Proceedings 6th international joint conference
Autonomous agents multiagent systems, pp. 13 New York, NY, USA. ACM.
Hirayama, K., & Yokoo, M. (1997). Distributed Partial Constraint Satisfaction Problem.. CP,
pp. 222236.
Larrosa, J., & Meseguer, P. (1996). Phase transition MAX-CSP. Proc. ECAI-96 Budapest.
Larrosa, J., & Schiex, T. (2004). Solving Weighted CSP Maintaining Arc Consistency.. Artificial
Intelligence, 159, 126.
Lynch, N. A. (1997). Distributed Algorithms. Morgan Kaufmann Series.
Mailler, R., & Lesser, V. (2004). Solving Distributed Constraint Optimization Problems Using
Cooperative Mediation. Proceedings Third International Joint Conference Autonomous Agents MultiAgent Systems (AAMAS04), pp. 438445. ACM.
Meisels, A., & Zivan, R. (2006). Asynchronous Forward-checking Distributed CSPs. Constraints, 16, 132156.
Meisels, A., & Zivan, R. (2007). Asynchronous Forward-checking Distributed CSPs. Constraints, 12(1).
Meseguer, P., & Jimenez, M. A. (2000). Distributed Forward Checking. Proc. CP-2000 Workshop
Distributed Constraint Satisfaction Singapore.
Modi, P. J., Shen, W., Tambe, M., & Yokoo, M. (2005). ADOPT: asynchronous distributed constraints optimization quality guarantees. Artificial Intelligence, 161:1-2, 149180.
87

fiG ERSHMAN , EISELS , & Z IVAN

Nguyen, T., Sam-Hroud, D., & Faltings, B. (2004). Dynamic Distributed Backjumping. Proc.
5th workshop distributed constraints reasoning DCR-04 Toronto.
Petcu,

A., & Faltings, B. (2004).
value ordering heuristic distributed resource allocation.
Proc. CSCLP04, Lausanne, Switzerland
http://liawww.epfl.ch/Publications/Archive/Petcu2004.pdf.

Petcu, A., & Faltings, B. (2005a). Scalable Method Multiagent Constraint Optimization..
Proc. IJCAI-05, pp. 266271.
Petcu, A., & Faltings, B. (2005b). S-DPOP: Superstabilizing, Fault-containing Multiagent Combinatorial Optimization. Proceedings National Conference Artificial Intelligence,
AAAI-05, pp. 449454.
Prosser, P. (1993). Hybrid Algorithms Constraint Satisfaction Problem. Computational
Intelligence, 9, 268299.
Prosser, P. (1996). Empirical Study Phase Transitions Binary Constraint Satisfaction Problems. Artificial Intelligence, 81, 81109.
Silaghi, M. C., & Yokoo, M. (2006). Nogood based asynchronous distributed optimization
(ADOPT-ng).. Proc. AAMAS06, pp. 13891396.
Solotorevsky, G., Gudes, E., & Meisels, A. (1996). Modeling Solving Distributed Constraint
Satisfaction Problems (DCSPs). Constraint Processing-96, pp. 5612 New Hamphshire.
Yokoo, M. (2000a). Algorithms Distributed Constraint Satisfaction: Review. Autonomous
Agents & Multi-Agent Sys., 3, 185207.
Yokoo, M. (2000b). Distributed Constraint Satisfaction Problems. Springer Verlag.
Yokoo, M., Ishida, T., Durfee, E., & Kuwabara, K. (1992). Distributed Constraint Satisfaction
Formalizing Distributed Problem Solving. IEEE Intern. Conf. Distrb. Comp. Sys., pp. 614
621.
Zhang, W., Xing, Z., Wang, G., & Wittenburg, L. (2003). analysis application distributed
constraint satisfaction optimization algorithms sensor networks. Proc. 2nd Intern.
Joint Conf. Autonomous Agents & Multi-Agent Systems (AAMAS-03), pp. 185192 Melbourne, Australia.
Zhang, W., Xing, Z., Wang, G., & Wittenburg, L. (2005). Distributed stochastic search distributed breakout: properties, comparishon applications constraints optimization problems sensor networks. Artificial Intelligence, 161:1-2, 5588.
Zivan, R., & Meisels, A. (2006). Dynamic Ordering Asynchronous Backtracking DisCSPs.
Constraints, 11, 179197.
Zivan, R., & Meisels, A. (2007). Conflict directed Backjumping MaxCSPs. IJCAI-2007
Hyderabad, India.
Zivan, R., & Meisels, A. (2006a). Concurrent search distributed CSPs.. Artif. Intell., 170(4-5),
440461.
Zivan, R., & Meisels, A. (2006b). Message delay DisCSP search algorithms. Annals Mathematics Artificial Intelligence, 46(4), 415439.

88

fiJournal Artificial Intelligence Research 34 (2009) 165208

Submitted 07/08; published 03/09

Behavior Bounding:
Efficient Method High-Level Behavior Comparison
Scott Wallace

wallaces@vancouver.wsu.edu

Washington State University Vancouver
14204 NE Salmon Creek Avenue
Vancouver, WA 98686

Abstract
paper, explore methods comparing agent behavior human behavior
assist validation. exploration begins considering simple method behavior
comparison. Motivated shortcomings initial approach, introduce behavior
bounding, automated model-based approach comparing behavior inspired,
part, Mitchells Version Spaces. show behavior bounding used
compactly represent human agent behavior. argue relatively low amounts
human effort required build, maintain, use data structures underlie
behavior bounding, provide theoretical basis arguments using notions
PAC Learnability. Next, show empirical results indicating approach effective
identifying differences certain types behaviors performs well
compared initial benchmark methods. Finally, demonstrate behavior
bounding produce information allows developers identify fix problems
agents behavior much efficiently standard debugging techniques.

1. Introduction
past decades, intelligent systems asked perform increasingly
complex mission critical tasks domains medical diagnosis (Shortliffe, 1987)
simulated aerial combat (Jones et al., 1999). Despite number successes,
complex agents yet become fully integrated mainstream software. Much
impasse may attributable fact developing agents often extremely
time consuming expensive.
Development requires three high-level steps: specification, implementation, validation. difficulties associated step determined properties agent
task intended perform. paper, focus class agents term
interactive human-level agents. agents typified training simulations
agents participate mixed human-computer teams accomplish particular training objective (e.g., Swartout et al., 2001; Traum et al., 2003; Jones et al., 1999; Rickel et al., 2002).
domains, agent plays role normally fulfilled expert human may
available training episodes. agents distinguished three properties.
First, agents performance judged based ability behave human expert
would behave similar situation. design criterion often particularly important
training simulations agents operate part mixed human-computer team playing role normally occupied another person. Second, like humans themselves,
interactive human-level agents must interact external, typically complex,

c
2009
AI Access Foundation. rights reserved.

fiWallace

environment order perform many tasks. Finally, unlike situation faced
design problems, complete specifications correct behavior often impracticable
impossible obtain. This, unfortunately, well documented property many
systems built model human domain experts (e.g., Tsai, Vishnuvajjala, & Zhang, 1999;
Weitzel & Kerschberg, 1989; Lee & OKeefe, 1994; Menzies, 1999). interactive humanlevel agents, specification task performed typically comes directly
human domain expert, result, comparing agents behavior
gold standard way determine design criteria met.
good example interactive human-level agent TacAir-Soar (Jones et al., 1999).
TacAir-Soar flies virtual military planes part simulated training exercise. Teammates
may TacAir-Soar agents human counterparts. agents intended
used enough human participants complex exercise,
agents must model expert-level behavior closely achieve training
results fully human team used. Thus, acceptable agents simply
achieve correct final states (e.g., shooting enemy planes). Instead,
agent must pursue trajectory state/action space emulates humans
trajectory (behavior). complex domains, meeting requirement challenging
expert may perform task differently different occasions.
many human-level agents, development steps specification implementation
often woven together knowledge acquisitionthe process
developer interviews human expert identify encode parameters correct
behavior. Often, process involves exposing rules procedures govern
expert decomposes task series goals, subgoals primitive actions (task
decomposition). rules procedures elicited, developer encode
knowledge form usable underlying agent architecture.
traditional approach knowledge acquisition rarely free errors. process
task decomposition works well enough identify relationships task goals
subgoals considered useful means acquiring encoding task
knowledge (e.g., Lee & OKeefe, 1994; Yen & Lee, 1993; Yost, 1996). However, finer
level granularity, knowledge acquisition highly prone errors. part, due
fact human participants stretched beyond areas expertise.
domain expert, means communicating tasks performed instead
simply performing them. engineer, means understanding problem space well
enough determine translate experts descriptions instructions
interpreted computer applied appropriate situations. Although
alternative methods knowledge acquisition proposed tested within limited
setting (e.g., van Lent & Laird, 1999), part incorporated
widespread use. result, developing complex intelligent agents remains time
consuming difficult process.
distinguishing characteristic work presented previous stated assumption correct specifications difficult impossible obtain. contrast
majority recent agent validation approaches using model checking temporal logic
(e.g., Bordini, Fisher, Visser, & Wooldridge, 2004, 2006; Fisher, 2005). systems seek
identify implementation errors proving whether particular implementation upholds
strict logical constraints (specifications). underlying assumption model checking
166

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

errors originate implementationnot specification. assumption
violated, system must tested gold standard behavior ensure correctness specification cannot fully trusted. sense, testing methods proposed
paper viewed complementary approach achieving objective:
correctly functioning agent.
work distinguished typical machine learning approaches
interested creating artifacts help person validate existing agents
behaviorwe necessarily need learn produce behavior. approach
intended applications current learning systems unable perform well
untrusted end users. revisit distinction traditional machine
learning approaches Sections 4 10.
1.1 Manual Semi-Automated Behavior Comparison
standard approach test-based validation requires knowledge developer domain expert monitor agents behavior large number scenarios
(Kirani, Zualkernan, & Tsai, 1994; Tsai et al., 1999). Although standard, clear
approach number significant drawbacks. Principal among
participation two humans required assess agents performance test.
time validation takes place, however, gross inadequacies agents behavior
corrected. Thus, although likely errors still exist,
manifestations probably relatively far between. means much
time spent validation useful identifying problems agents behavior.
improve upon standard validation approach, semi-automated method
makes efficient use domain expert developers time would highly
desirable could substantially decrease cost testing. paper, explore
issue meaningfully compare two actors trajectories state/action/goal
space (i.e., behavior) given set examples.
Comparison, paper, simply means identifying actors trajectories
similar different one another. Thus, interested comparison goes well
beyond simply indicating two actors achieved final states. Rather, take
account actions performed motivations behind actions. could
done simply comparing observed trajectories directly, inferring general model
actors trajectories comparing models. either case, key challenge
interested producing artifacts easy human interpret
could used assist tasks validation.
potential uses behavior comparison extend well beyond agent validation
many tasks humans may want know two actors perform tasks differently. Scoring modified (non-speech based) Turing test, example, requires humans
perform comparison two actors behavior. Similarly, consider human supervisor
examining students performance lesson intelligent tutoring training system. examination review could facilitated tutoring system capable
comparing students behavior differed internal gold standard could
relay information instructor manner easy interpret.
applications, basic process comparing behavior artifacts produced

167

fiWallace

remains constant. differences stem source behavior (e.g., human
machine, expert novice) results used (to identify programming errors,
score test, evaluate students performance). simplicity cohesiveness,
paper focus using behavior comparisons aid agent validation problem,
discussion results also applied tasks well.
1.2 Outline
remainder paper, examine two methods comparing interactive goaloriented behavior exhibited human-level agents human counterparts. begin describing primitive representation behavior upon
build comparison methods. Next, describe simple sequence-based comparison, deficiencies method lead us examine sophisticated model-based
approaches.
main contributions paper fourfold. First, Section 4, identify requirements useful comparison system. Then, beginning Section 5, describe novel
model-based approach comparing two actors behavior. approach, called behavior
bounding, uses hierarchical behavior representation built observations
human computer-agent behavior. Third, demonstrate behavior bounding meets
requirements useful behavior comparison system support claims
theoretical empirical evidence. Finally, show information behavior
boundings comparison significantly aid process identifying problems agents
behavior, thus speeding agent validation significant factor.

2. Behavior Traces
primitive, behavior represented trajectory though state/action/goal
space refer behavior trace. behavior trace sequence tuples
B = ((s, G, a)0 , (s, G, a)1 , . . . , (s, G, A)n ) tuple (s, G, a)i indicates environmental state (s), goals pursued actor (G), action performed
(a) ith sampling point. actors goals directly observable must
explicitly provided actor performing task. Goals important purposes
interested actors do, also interested
motivation behind actions.
project, make three main assumptions nature actors goals.
First, assume actors goals part actors internal state. goals
simply given task description. Although task certainly informs goal
selection, goals arise interactions agents internal desires
environmental situations encountered task. Second, assume actors
goals change environment changes task moves toward completion.
means goals used structure agents task subtasks appropriate
goals subgoals generally differ distinct phases task. Third, assume
actors choice goals (and actions) based upon static set knowledge.
is, agent learn.
Note defined it, behavior trace give complete information
agents internal state. Indeed, actor likely perform potentially large
168

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

amount reasoning order select G a. example, actor may perform
expected utility calculation look-ahead search. However, process information explicitly represented G completely absent behavior
trace. Although provides us limited amount information
perform behavior comparison, also ensures possible collect behavior
either human computer agent actors.
Behavior capture process collecting information actor build behavior trace. noted above, limiting information behavior trace critical ensure
behavior capture possible. state action portion behavior trace
captured simply observing actor perform specified task. Depending whether
actor human computer agent, way actor records goals
change task vary. computer agent, (G, s) pairs simply written file task performance. human expert, goal annotations made
verbally task performance immediately following task completion suggested
van Lent Laird (1999).

3. Sequence-Based Comparison
simple approach comparing actors behavior performed following
steps:
Acquire set behavior traces human expert agent specified task.
sets, H A, represent human experts agents behavior respectively
number different trials.
Extract relevant symbols behavior traces. information gathered
observation may irrelevant detecting errors. example, human experts
behavior never changes given different values state symbol z, z likely
irrelevant detecting errors. step, salient symbols sets H
used create two new sets sequences H .
Compare sequence , contents H . Compute minimal number
edit operations (insert, delete, modify) would required transform h,
h sequence H initially similar a. edit operation
indicates potential error.
Report deviations (after removing redundancies) humans agents
behavior. report summarizes potential errors.
simple approach performs detailed analysis behavior simply checking
agent expert reach final (goal) state. way, agents
externally observable behavior well aspects internal reasoning process
inspected ensure consistency human experts. addition, methodology
ability identify large number possible errors access
salient properties behavior trace. However, simple approach also suffers
number potentially serious flaws.

169

fiWallace

1. actors behavior represented set sequences. complexity
domain increases likely two effects noticed: average length
sequences H grow (i.e., complex tasks take longer solve),
sequences composed larger number symbols (e.g., state
space become richer). number distinct sequences lengths
P max
lmin lmax composed symbols grows ll=l
sl . Thus, enumerating
min
space likely infeasible. Moreover, interactive human-level agents
typically solve problems number different ways, typically operate
within complex domains, likely sequential approach described
section particularly susceptible effect.
2. sequence based comparison fails make assumptions actors
behavior may constrained. is, sequential behavior representation provides
method expressing priori knowledge symbols placed relative
one another within particular sequence. Instead, representation completely
unconstrained; sequences length l constructed making l independent
symbol selections. Although makes possible use simple approach
variety behavior (even behavior completely unstructured), also makes
impossible leverage regularities might exist large classes goal directed
tasks (such fact unlocking door must always accomplished
door opened).

4. Model Based Approaches
improve upon simple sequence-based method error detection, propose comparison method leverages abstract representation actors behavior. call
methods model-based compare instances actors behavior directly (as simple sequential approach would). Instead, methods compare
abstract representations actors behavior (models), identify similarities differences underlying behavior. Central approach considerations
influenced models design. choice models guided following design
requirements:
Low Complexity behavior model must significantly less complex representations define agent itself. requirement violated, two problems
may result. First, constructing model (either hand, automatically
observational framework) likely difficult constructing agents
knowledge base. Second, understanding model behavior represents
likely easier examining agents internal representation.
comparison used validate agents underlying knowledge base,
clearly undesirable results recursive validation problem. However,
achieve low complexity requirement using model represents behavior
relatively high level abstraction compared agents internal implementation.
Low Human Effort human effort required build behavior model must remain
low. argued one main uses behavior comparison would
170

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

reduce cost validating human-level agent. low human effort requirement
violated, original validation costs (due, example, time requirements
examining numerous test scenarios) simply replaced new costs,
resulting net benefit. achieve low cost requirement using
automated system build behavior representations series observations
little human supervision.
Compatibility must possible build use behavior model human
actors software agents. discussed previous sections, behavior comparison
number potential applications, many rely able examine
human software agent behavior. Thus, contents model must limited
data collected either types participants. Section 2,
described behavior traces could collected human actors
computer agents. result, achieve requirement using model
built behavior traces.
Efficiency computational costs associated building using model must
become infeasible complexity domain increases. Although primary
motivation automated behavior comparison replace human effort computational effort, must careful construct model way
become impossible use. achieve requirement using abstract
model actors behavior grow directly function number
behaviors encapsulates.
Efficacy good model must effective identifying similarities differences
two actors behavior. perhaps basic requirement presented.
However, desire effective model captures subtleties actors
behavior likely direct conflict previously presented requirements.
result, good model must balance need represent actors behavior precisely
thus able distinguish similarities differences behavior
overall needs. Unfortunately, little priori assurance
particular model effective. requirement must addressed
theoretical empirical testing model implemented.
Note unlike traditional machine learning tasks, necessarily need
produce model used perform task. is, need learn
policy set plan operators. described above, trade-off
models efficacy complexity. one end spectrum executable models
task. Here, efficacy maximized, model would necessarily complex
would likely difficult human use validate behavior
looking directly hand coded rules procedures. models certainly valuable
goal learn behavior directly set examples, variety approaches
pursued machine learning literature; closely related discussed
later Section 10. approach, however, attempts target different point
efficacy/complexity spectrum model cannot perfectly describe many complex
tasks, result model examined much quickly agents
171

fiWallace

internal implementation. Thus, standard approach machine learning literature
empirically evaluate learned model comparing optimal model
hand-coded model, interested something else: namely, whether model
maintain efficacy complex environments whether improve persons ability
quickly uncover fix problems existing agents. Sections 8.2 9 examine
issues.
4.1 Model-Based Diagnosis
Prior work model-based diagnosis (e.g., Anrig & Kohlas, 2002; Lucas, 1998) examined
detect errors given model correct behavior. general, however, models
systems relatively complicated intended identify problems mechanical
solid state devices opposed software agents. CLIPS-R (Murphy & Pazzani,
1994) system designed expressly ensure correct software agent behavior, bears
similarity approach.
CLIPS-R, behavior model consists set tuples (S , CSf , CE ),
specifies initial world state (S ), set constraints describing acceptable final world
states (CSf ), execution constraints (CE ) must met task performed. Final state constraints indicate facts environment agent must
either true false task complete (e.g., (not (gas-empty car))). Note
final state constraints define behavior model classical planning sense;
description sequence events lead final state. information
provided execution constraints (C E ), represented finite state machine
describing acceptable orderings agents observable actions. Execution constraints
used describe relationships actions. example, constraint might
specify action unlock-door always proceed open-door. Superficially,
requirements CLIPS-R approach seem relatively simple meet. However, two
serious problems exist.
First, specifying exact set execution constraints required correct operation
similar writing conditions rules. execution constraints govern behavior
fine level granularity, likely similarly difficult design
validate agents rule base (a recursive validation problem). case,
requirements low complexity low human effort would violated.
hand, constrain behavior higher level granularity, task level,
efficacy requirement called question: powerful enough work
complex environments human-level agents?
second serious problem arises CLIPS-R approach provides little guidance
determine appropriate constraints, especially appropriate execution constraints.
benefits approach hinge completely developers ability enumerate
adequate appropriate execution constraints particular task. Yet developer
enumerate constraints required judge whether agents behavior correct,
included agents knowledge base directly?
noted although problems mentioned may encountered
CLIPS-R used particular agent, likely become obvious
(and problematic) complexity agent domain increases. already noted,

172

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

exactly types agents environments interest us, concerns
raised particularly salient work interactive human-level agents.
contrast, original CLIPS-R work (Murphy & Pazzani, 1994) examines systems
ability correctly identify flaws two simple agents whose knowledge bases contain
nine fifteen rules respectively. agents examined CLIPS-R work performed
tasks akin classification highly interactive tasks
interest us.

5. Behavior Bounding
improvement CLIPS-R simple method presented Section 3,
approach behavior comparison, called behavior bounding, automatically efficiently
builds concise high-level models human experts agents behavior examining behavior traces meet first three requirements described Section 4.
human experts behavior model used identify boundaries acceptable behavior
manner reminiscent Mitchells Version Spaces (Mitchell, 1982). Potential errors
reported comparing model agent behavior boundaries. Behavior bounding used identify programming errors agents knowledge base also
identify discrepancies experts explanation task performed
expert actually performs task. contrast high-level model
built similarly agents knowledge base (as, presumably, CLIPS-R) using indirect
information interviews determine constraints met task
performance.
5.1 Hierarchical Model
Behavior bounding leverages assumption although knowledge acquisition highly
prone errors respect details task performed, high-level
information (specifically general relationships goals, sub-goals primitive actions) much reliable. Behavior boundings hierarchical behavior representation
inspired hierarchical models used And/Or trees, HTN planning (Erol, Hendler,
& Nau, 1994) GOMS modeling (John & Kieras, 1996) encode variety ways
particular tasks accomplished. Conceptually, behavior bounding encodes
three relationships. First, identifies decomposition relationships goals, sub-goals
primitive actions. Second, identifies ordering relationships nodes
hierarchy. Finally, behavior bounding identifies goals actions instantiated
saving generalized parameters (i.e., features internal world state directly
associated goals actions begin pursued).
hierarchical behavior representation (HBR) used approach And/Or
tree binary temporal constraints representing relationships actors
goals actions. representation, internal nodes correspond goals leaves
correspond primitive actions. nodes children indicate set sub-goals primitive
actions relevant accomplishing specified goal.
Figure 1 illustrates small subsection hierarchical behavior representation. Goal
nodes drawn ovals primitive actions rectangles. constraints
represented standard fashion arc across child nodes; temporal constraints
173

fiWallace

Fly-Mission

Achieve-Waypoint

Set
Altitude

Compute
Heading

Return-to-Base

Set
Heading

Contact
Tower

Set
VHF

Set
UHF

Contact
Teammates

Ensure
Adequate
Fuel

Send
Message

Figure 1: Hierarchical Behavior Representation

represented directed arcs sibling nodes. Note total order
siblings possible required representation. semantics nodes
representation necessarily indicate one subgoal (or action) required
accomplish given goal. Rather, node indicates simply complete set
subgoals (or actions) always required accomplish task. Thus, semantics
nodes preclude use temporal relations; merely state order
multiple goals/actions occur indeed one pursued.
HBR viewed simple constraint model based observations
actors behavior. encodes relationships Fisher uses temporal
logic models agents (Fisher, 2005): namely step rules (what goals/actions expect next);
sometimes rules (what goals/actions expect future). result, HBR
could used source types temporal logic constraints required model
checking (as case human-level agents) expert capable providing
logical constraints directly.
5.2 Building HBR Behavior Traces: Overview
Section 6 present detailed explanation HBR acquired behavior
traces along underlying algorithm. Here, present conceptual overview
process describing partial behavior trace left-hand side Figure 2
used build HBR right side figure.
Initially, begin empty HBR. behavior trace (Figure 2, left hand side)
processed single pass, reading beginning end. new goals actions
174

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

encountered, nodes added hierarchical representation. hierarchy goals
actor currently pursuing indicated behavior trace lines level
indentation. example, goal stack generated incrementally beginning
selection top-level goal decomposed lower-level goal
begin decomposed series primitive actions. goal considered completed
longer member actors goal stack. example, Figure 2, goal
Achieve-Waypoint completed actor commits performing new goal
level abstraction (i.e., goal Return-to-Base selected). behavior
trace processed, requirements goal completion tracked including subgoals
necessary accomplish current goal ordering well parameters
goal respective subgoals. requirements represented descendants
hierarchy constraints them. Note action subgoal
encountered multiple contexts (as descendant two distinct parents) HBR
create node context. appropriate parameters associated
goal/action interaction sibling goals/actions likely depend
higher-level context.
generation process results HBR right-hand side Figure 2 (note
parameters associated goal action, listed behavior trace
segment, displayed improve clarity figure). goal nodes (ovals)
children type And. addition, siblings totally ordered indicated
temporal constraints (directed arcs siblings). highly constrained nature
HBR (And goals total ordering) typical representations built single
behavior trace. behavior traces used generate structure, HBR
generalized cover input observations.
structural topological level, generalization occurs two ways. first
constraint turned constraint. example, Achieve-Waypoint
goal every time observed, completed pursuing
three subgoals: Set-Altitude; Compute-Heading; Set-Heading. second
behavior trace indicated Achieve-Waypoint successfully completed performing
subgoal Set-Heading, Achieve-Waypoint would become node
correspondingly indicate require subgoals accomplished.
Similarly, generalization binary temporal constraints occurs needed represent observed orderings goals actions. Returning example Figure 2,
Achieve-Waypoint observed occur once. Thus, representation HBR
indicates total order three subgoals. Achieve-Waypoint performed
second time new sequence three subgoals, ordering constraints
within HBR would change. example, Achieve-Waypoint performed
pursing: Compute-Heading; Set-Altitude; Set-Heading, order, temporal
constraint Set-Altitude Compute-Heading would removed. process building HBR underlying algorithm discussed detail
Section 6.
Generalization also occurs parameters associated goal action, effectively expanding set parameters associated node obser-

175

fiWallace

Set goal: Fly-Mission
Set goal parameter: (altitude 30000)
Set goal parameter: (patrol-speed 800)
Set goal: Achieve-Waypoint
Set goal parameter: (waypoint AZ-12)
Set goal parameter: (threat-level low)
Set goal parameter: (ETA 10 minutes)
Action: (set-altitude 30000)
Action: (compute-heading AZ-12)
Action: (set-heading)
Set goal: Return-to-Base
...

Fly-Mission

Achieve-Waypoint

Set
Altitude

Compute
Heading

Return-to-Base

Set
Heading

Figure 2: Constructing hierarchical behavior representation behavior trace
vations made1 . Consider Figure 2 parameter associated Set-Altitude
30000. later see Set-Altitude performed parameter 20000, HBR
contain generalization two observations, namely Set-Altitude
parameters range 2000030000. parameter associated goal action generalized cover observations behavior traces. numerical parameters,
generalization performed expanding acceptable range include new value.
symbolic parameters, generalization performed adding new symbol set
acceptable values.
5.3 Representational Simplicity
HBR discussed clearly much less complex representation behavior
agents underlying knowledge base. Indeed, hierarchical structure ensures
constraints cannot formed arbitrary goals actions. property also means
HBR may less complex even model used CLIPS-R, allows
arbitrary finite state machine describe acceptable sequences external actions.
Behavior bounding ensures high-level model behavior abstracting away internal
data-structures agent may use perform task cannot represented
hierarchy. possible store arbitrarily complex information HBR,
unlikely happen practice. Consider, example, depth first search uses
open list discriminate alternative behaviors. final result search
(a goal action) naturally captured HBR, forcing HBR capture details
search impractical requires pushing information captured open list
goal hierarchy.
specifically, consider agent using search select two potential actions: Set-Altitude; Set-Heading. First, note search process would
represented behavior boundings HBR agent explicitly made searching
goal. However, even Search explicit goal, information open-list (states
still need tested) would available HBR made ex1. purposes paper, parameter generalization less interesting structural generalization.
include brief discussion mainly completeness.

176

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

plicit parameter goal. Even formulation, however, would leave large amount
information search process unrepresented HBR. Specifically,
search encapsulated single goal without substructure, would impossible
determine manner various search nodes visited. order represent
information, would need push relevant data structures (in case
open-list) goal hierarchy itself. Thus, would need create explicit goals
(state, open-list) pair. approach pushing arbitrary information goal
hierarchy clearly undesirable unlikely occur frequently well designed
agent. Thus, reasonably certain behavior boundings HBR always
high-level, abstract, representation agents (or actors) behavior.
representational limitations HBR leads us ask: agents behavior
represented using simple structure, programmed
representation begin with? hypothesis representation sufficient
completely capture agents behavior, sufficient generate behavior.
human-level agents rely intermediate data-structures available
environment structure goal hierarchy (for example agents use
look-ahead select next goal action, perform expected utility calculation).
Rather, hypothesis representation provided behavior bounding sufficient
identify large class errors agent behavior without sacrificing efficiency. Moreover,
hypothesize behavior bounding help identify potential problem spots
agents knowledge (e.g., ordering actions specific goal) even exact error
cannot identified.
5.4 Representational Assumptions
contrast behavior representation used simple comparison described
Section 3, HBR makes three strong assumptions organization actors
knowledge effects organization actors behavior. assumptions
increase efficiency efficacy error detection certain types human-level agents.
first assumption used behavior bounding actors goals organized
hierarchically, abstract goals located toward top tree. Hierarchical task
structure exploited number agents agent architectures, thus assumption
particularly limiting. also assume point problem solving
process actor pursues set goals belonging different levels hierarchy.
set, referred goal stack, corresponds path hierarchy beginning
top node descending concrete sub-goal currently pursued
actor. goal stack assumption implies concurrent goals (two goals
simultaneously pursued depth hierarchy) cannot modeled explicitly
HBR. One way circumvent limitation implement concurrent goals
nested goals. test architecture (Soar) directly support concurrent
goals, approach typically taken achieve behavior. see
Section 8.2.5, approach allow us create use HBR may also result
representational problems. hierarchical goal assumptions described provide
important benefit constraining acceptable orderings goal actions agent
may pursue. property analyzed detail Section 8.1.

177

fiWallace

second assumption leveraged behavior bounding relates independence
goals. HBR, temporal constraints formed sibling nodes,
And/Or classification determines nodes children must performed
particular task. makes easy constrain way particular goal achieved,
difficult represent constraints arbitrary parts hierarchy. Although
may cause problems agent implementations, property significant benefits.
importantly, decreases number observations required build
model. Consider task requires completing two goals, could fulfilled
four distinct ways. behavior represented ordered pair (a 1 , a2 ) indicating
action taken fulfill goals one two respectively. sequential representation makes
assumptions goal independence (such one described Section 3) would
require sixteen distinct observations cover acceptable behavior space (one
distinct (a1 , a2 ) pair). contrast, behavior bounding would require four observations
long set observations included every possible value 1 every possible
value a2 2 . impact efficiency significant direct result leveraging
assumption goals likely add regular structure actors behavior.
Finally, recall Section 5.1 third assumption upon behavior bounding
built. knowledge acquisition relatively reliable correctly identifying
general goal/subgoal relationships expert uses perform target task even though
process knowledge acquisition prone errors attempting identify
rules necessary encode task. assumption provides justification
using behavior representation focuses relationships goals, subgoals
primitive actions purposefully neglecting much internal information
actor may use select behavior.
net effect building HBR based assumptions model meets
criteria set forth Section 4. model likely much concise
agents implementation (low complexity)we learning complete plan operators,
instead generalization actors trajectories goal/action space. addition,
HBR generated automatically examining actors behavior traces thus
meeting second requirement (low human effort). behavior traces
captured either human computer agent actors, HBR meets third requirement
(compatibility). following sections, present method behavior bounding
uses HBR perform comparisons. addition, examine remaining two
requirements ideal model-based approach (efficiency efficacy) detail.

6. Learnability
section, examine two aspects behavior boundings hierarchical representation:
effort required create maintain it, ability represent behavior efficiently.
requirements addressed overall learnability representation.
is, representation learned observations (as suggested),
requires human effort initiate learning process. learning procedure
efficient, data structures growth limited, say hierarchy
2. Thus, a1 , a2 {1, 2, 3, 4} pairs (1, 1), (2, 2), (3, 3), (4, 4), would sufficient cover
acceptable behavior space behavior bounding sequential representation.

178

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

represents behavior efficiently thus meets fourth requirement (efficiency) outlined
Section 4.
Create-Hierarchy(B, H)
1 W empty tree
2 lastStk nil // previous goal/action stack
3 (s, G, a) B
4
5
= 0 length[lastStk]
6

7
Goal-Completed(lastStk[i])
8
hg Find-Node(H, lastStk[i])
9
hg = nil
10

11
Add-SubTree(H, Parent(lastStk[i]), lastStk[i])
12
else
13
Generalize(H, hg , W, lastStk[i])
14
gi [G, a]
15

16
pg Parent(gi )
17
wg Find-Node(W, pg , gi )
18
wg = nil
19

20
wg Add-Node(W, pg , gi )
21
Constrain-Children(W, pg )
22
else
23
Out-of-Order(W, pg , wg )
24
Update-Constraints(W, pg , wg )
25
Generalize(wg , gi )
26
lastStk [G, a]
27 return H
Figure 3: Create-Hierarchy algorithm
Section 5.2 presented overview process behind building HBR
behavior trace. Create-Hierarchy algorithm (Figure 3) specifies process explicitly. algorithm takes two arguments input: B, behavior trace; H, HBR
representing previously observed behavior (or nil behavior yet observed).
Create-Hierarchy returns new HBR covering behavior H new observation B. Thus, calling procedure single behavior trace B H nil generates
hierarchical representation single behavior trace examining way goals
decompose subgoals primitive actions task performance. Iteratively calling
Create-Hierarchy different behavior traces augment generalize H
covers example traces. algorithm executed O(lN 2 ) time l

179

fiWallace

(maximum) length behavior trace N number nodes goal
hierarchy.
Classifying sample complexity hierarchical representation straightforward.
Hausslers equation (Haussler, 1988; Mitchell, 1997), know number
training examples required consistent learner learn target concept (with probability (1 ) error bound ) hypothesis space (H) where:
1
1
ln(|H|) + ln







(1)

HBR viewed ordered tuple P = (p 1 , p2 , . . . , p|N | ) pi
tuple containing type node (either Or) well list
L = (l1 , l2 , . . . , l|N | ) la = 1 iff gi ordered la . Note since ordering
constraints occur siblings, length list L would need
2
length |N | degenerate case. size hypothesis space bounded 2 |N |+|N |
worst case, based shape hierarchy may much smaller. Substituting size hypothesis space back Equation 1 find indeed grow
polynomially:
1
1
(|N |2 + |N |) ln(2) + ln
(2)


indicates required sample size polynomial respect number
goals hierarchy (|N |). This, together fact time required
incorporate new behavior trace learned HBR also polynomial |N |, shows
representation PAC-Learnable. means HBR efficiently represents
aggregate behavior well individual instance behavior, thus meeting fourth
requirement.






7. Identifying Errors
general, view behavior comparison method algorithm divides
space possible behaviors two regions: behaviors likely consistent
expert, behaviors likely inconsistent expert. simple
comparison method described Section 3 enumerating consistent behaviors.
model used behavior bounding, however, allows us divide space possible
behaviors efficiently refined regions without enumerating contents.
Intuitively, idea organize HBRs lattice; individual points lattice
used define boundaries different quality behaviors manner reminiscent
Mitchells Version Spaces (Mitchell, 1982).
Recall hierarchical behavior representation hierarchy nodes corresponding goals, subgoals primitive actions. Nodes linked hierarchically based
goal/subgoal decomposition relationships observed behavior traces. HBR
viewed consisting two parts:
1. basic structure hierarchy nodes labeled names
goals, subgoals actions connected parent/child relationships
manner corresponds observed behavior.
180

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

2. set constraints imposed upon nodes basic structure. Constraints include And/Or typing nodes, binary temporal constraints, constraints allowable parameter space goal, subgoal, action.
constraints formed specific general learning algorithm,
generalization process creates lattice HBRs related following manner: 1)
share basic structure; 2) differ specificity constraints.
Thus, hierarchical behavior representation allows us define ordering specific
general space behavior hierarchies starting maximally constrained
hierarchy (at top) iteratively removing constraints none remain.
Behavior bounding leverages ordering hierarchies efficiently partition
behavior space different regions. process begins using traces expert behavior (the specification) create corresponding HBR. created, identify
node occupies ordered space (call node Figure 4). node (the upper
boundary node) allows us easily determine agents behavior likely correct.
definition, correct behavior must consistent expert behavior. agent whose
behavior representation specialization experts (i.e., lies generalization lattice) exhibits behavior consistent experts therefore likely
correct. sequential approach behavior comparison, upper boundary
node allows us partition behavior space two regions: correct, incorrect.
second partition formed node representing completely unconstrained
version experts goal hierarchy. node illustrated bottom Figure 4 (labeled B). contains basic structure (goal/subgoal relationships) may constitute
acceptable agent behavior result could used identify behavior representations
known incorrect (because agents behavior hierarchy topologically inconsistent experts behavior hierarchy). representations would goal
decomposition structure inconsistent (i.e., contained different parent/child relationships than) lower boundary (nodes right side Figure 4 labeled neither
less specific A).
Together, upper lower boundaries create three regions behavior space.
Nodes specialization experts behavior (above upper boundary node)
correspond behavior likely correct. Nodes specialization
unconstrained version experts goal hierarchy (the lower boundary node) correspond behaviors known incorrect. region upper
lower boundary nodes corresponds behavior likely incorrect perhaps
lower probability region lower boundary node 3 .
Mitchell (1997) defines version space subset hypotheses (from hypothesis
space) consistent given set training examples. ordering hypothesis
space specific general, Mitchells learning algorithm (Mitchell, 1982, 1997) identifies
version space without enumerating contents. Instead, version space represented
concepts (hypotheses ordered hypothesis space) form upper lower
3. assume easier ensure HBR reflects correct agent topology
ensure constraints upper boundary nodes HBR adequately generalized. practice,
degree assumption holds depend properties agent HBR
corresponding lower boundary node formed (see Section 11 alternative method).

181

fiWallace

boundaries. S-Set G-Set specify specific hypotheses
general hypotheses version space respectively. training examples
obtained, S-Set becomes progressively general G-Set becomes increasingly specific converge correct hypothesis.
Mitchells S-Set G-Set used delimit set consistent hypotheses
without enumerating them, upper lower boundary nodes approach serve
similar purpose. upper boundary node (UBN) plays similar role S-Set.
However, S-Set used incrementally converge correct hypothesis (and
becomes increasingly general), upper boundary node viewed correct
hypothesis. Thus UBNs value delimiting portion lattice consistent
specification. lower boundary node, hand, plays similar role
G-Set. But, G-Set identifies hypotheses inconsistent training data,
lower boundary node simply identifies HBRs lattice
distinct topological structure.
boundaries established, quickly determine whether
arbitrary HBR specialization either boundary node. analysis, clearly
done polynomial time respect number distinct goals, subgoals,
actions, allows us quickly determine degree behaviors two actors are,
not, consistent one another. inconsistencies uncovered process form
basis behavior boundings error report displayed either standard text
format visually using graphical user interface. remainder paper,
use terminology appropriate comparing two actors playing roles either expert
novice. actor referred expert represents correct behavior specification.
actor referred novice expect exhibit partially incorrect behavior.
described Section 1, roles could played either software agents humans
depending situation hand.

8. Error Identification Efficacy
point, provided good deal support behavior bounding
HBR presenting analytical arguments behalf. final criteria must
addressed efficacy respect identifying errors. this, examine two
components HBR. First, provide analytic results indicating effectiveness
unconstrained hierarchical representation (the lower boundary) identifying behavior
known incorrect. Second, provide empirical evidence behavior
bounding whole effective distinguishing correct incorrect behavior.
8.1 Lower Boundary Node
first glance, obvious much behavior classified lower boundary
node. Without And/Or constraints binary temporal constraints, lower boundary
node specifies subgoals belong goals. specification,
lower boundary node constrains set allowable goal/sub-goal/action sequences.
effectiveness simple constraint mechanism quite surprising.
Consider unconstrained behavior representation branching factor b depth
d. Without loss generality, assume nodes uniquely labeled. simplicity,
182

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

Specific
G1
SG1,2

SG1,1

A1

A2

Behavior Representations
Inconsistent B

A3



G1
SG1,1

A1

SG1,2

A2

A3

G1
G1
A1

SG1,2

SG1,1

A3

A5

A7

B
A1

A2

A3

General

Figure 4: Imposing Order Behavior Space

1e+06

B=2
B=4
B=6

Maximum Sequences (log(log(y))

100000
10000
1000
100
10
1
0.1
2

3

4
5
6
Depth Hierarchy

7

8

Figure 5: Filtering Capability Lower Boundary Node

183

fiWallace

also assume level hierarchy, actor completes current goal
starting next goal. Then, could define actors behavior sequence symbols
chosen lowest level unconstrained hierarchy. behavior sequences
P
j
length bd , symbol repeated, b! |s = d1
j=0 b possible sequences
consistent goal decomposition unconstrained hierarchy. contrast,
bd ! sequences symbols may placed without necessarily conforming
unconstrained hierarchy. hierarchical structures reasonable size, makes
lower boundary node effective filtering exponential number potential behavior
sequences. example, small hierarchical structure depth 4 branching factor
2, 1 approximately 6.4 108 possible sequences length 16 consistent
goal decomposition specified unconstrained hierarchy. Figure 5 illustrates
filtering capability lower boundary node. x-axis figure indicates
depth hierarchy lines plotted branching factors 2,4, 6. y-axis
indicates ratio possible sequences accepted goal hierarchy number total
possible sequences unconstrained symbol set size; note y-axis
doubly-logarithmic (log log(y) plotted).
Although lower boundary node extremely simple data structure, information stores significant value. Used alone, identify large (exponentially
increasing) number behavior sequences inconsistent experts goal decomposition structure therefore incorrect.
8.2 Empirical Evaluation
empirical study two aims. First, want determine whether behavior bounding
identifies errors agent behavior well enough considered useful purposes
validation. Second, want compare behavior boundings effectiveness
simple sequential approaches described Section 3. end, implemented behavior bounding along two versions sequential approach serve benchmarks.
first benchmark, M1 , extracts sequence actions = (a 0 , a1 , . . . , )
behavior trace B = ((s, G, a)0 , (s, G, a)1 , . . . , (s, G, a)n ) second benchmark, M2 ,
extracts sequence goals G = (G 0 , G1 , . . . , gn ) B. cases comparison
performed computing minimal edit distance two behavior traces. Remember
sequential methods particularly efficient representations; grow exponentially length behavior trace exponential sample complexity.
However, reason, make interesting benchmarks efficacy.
Performance judged based ability to: 1) correctly identify errors agent behavior;
2) identify errors occurred; 3) produce minimal amounts spurious
information reporting errors. make assessment, must compare
errors identified automated comparison record errors manually
identified known actually occurred. requires manual inspection
behavior traces taxonomic classification possible differences. following
subsections begin describing errors classified move discuss
experimental method assessment process detail.

184

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

8.2.1 Behavioral Differences
simplest level, differences (potential errors) identified single discrepancy two particular symbols behavior traces particular pair
goals actions. type mismatch occur one three ways. before,
refer desired behavior captured experts behavior traces, untrusted
imperfect behavior captured novices behavior traces.
Commission novices behavior trace experts behavior trace contain
goal action symbol specified location goals actions inconsistent, error commission occurred. example, consider agent flying
tactical military aircraft patrolling air space two waypoints. Assume
specification correct behavior dictates agent travel way-points
enemy aircraft spotted point agent contact command center receive clearance engage enemy. situation, error
commission would occur agent contacts wingman instead command
center proceeds enter engagement.
Omission experts behavior trace contains goal action symbol
corresponding symbol novices behavior trace, error omission.
Following example above, omission would occur agent immediately begins
engage enemy without interjecting substitute goal action replace
missing call command center.
Intrusion final simple error type, intrusion, identical omission except
goal action symbol occurs novices behavior trace experts
behavior trace. intrusion would occur agent contacts command center
receives clearance engage enemy proceeds continue
waypoint returning back engage enemy.
experiments, often relatively straightforward classify errors
three categories. However, situations enough differences two
actors behavior difficult determine whether deviation commission
one forms. situations, marked error belonging either
category considered acceptable comparison method identify either form.
one simple errors listed occurs, may possible
identify relationship them. call related errors compound errors note
uncovering single compound error preferable identifying many simple errors
compound error concise description underlying problem. Note
clearly cannot consider possible relationships multiple errors
would problematic computational implications. Rather, interested relationships occur frequently practice. identify two compound errors. first
misplacement error two goal action symbols transposed novices
behavior trace; often due incomplete specification constraints one
goals actions take part error. second duplication error
one goal action symbols reoccurs inappropriately. computer agents,

185

fiWallace

E1 Primary (P)
Mismatch Error (M)

E2 & E3:
Commission Errors
(C), together
create E1.
also Primary Errors
causal chain.

Salience

E1 (P,M)

E2 (P,C)

E4 (S,C)

E5 (S,I)

E3 (P,C)

E6 (S,I)

E7 (S,I)

E3 gives rise 3
Secondary Errors
(S)
happen
Intrusions (I)

Figure 6: Multiple related errors result salience hierarchy

type error often occurs termination condition particular goal
action incorrectly specified.
Errors also occur among subsequences behavior trace. typically happens
novice begins pursue incorrect goal. situation, causal
relationship initial error sequence errors follows. define two
error forms based attributes: primary error first causally linked
sequence errors, secondary errors subsequent errors sequence. Although
problems right, secondary errors corrected simply correcting
primary error. Often occur higher level goal incorrectly selected
naturally led entire sequence incorrect behavior.
compound errors salient simple errors concisely
describe multiple simple errors well interactions them, primary error
salient secondary errors follow. Note since single error
act primary secondary error (if hierarchy cascading errors occurs),
primary/secondary relationship creates corresponding salience hierarchy. Figure 6
illustrates relationship. Towards top primary compound errors toward
bottom secondary individual errors. Correcting error level hierarchy
also resolve descendant errors.
8.2.2 Method
Ideally, empirical evaluation would directly examine much human effort saved
using behavior comparison methods development number complex
human-level agents. However, developing complex agents interested time
consuming task developing multiple independent versions beyond scope
experiment. Instead, selected approach identifies effectiveness error
detection methods without directly examining development time. Using method,
evaluate effectiveness error detection method examining ability identify
different types errors development versions (novice versions) particular agent.
examining number true errors detected, well false negatives false positives,
186

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

1. Acquire specification correct (expert) behavior.
2. Construct set flawed novice agents.
3. Identify general differences comparing experts novices
knowledge.
4. Acquire suitable behavior traces expert novice.
5. Manually catalog errors novice behavior trace.
6. Construct individual experiments partitioning behavior traces
multiple groups.
7. Evaluate well error detection method identifies cataloged
errors.
Figure 7: overview steps evaluation process

obtain measure relative strengths weakness approach without
directly examining development time impacted ongoing project. evaluation
process described seven high level steps outlined Figure 7 described detail
below.
evaluation begins specification correct behavior. normal development circumstances, specification correctness would domain experts behavior. experiments, however, replace domain expert correctly specified
expert-level agent, E, whose behavior attempt reproduce. idea replacing
human expert software agent may initially seem counterintuitive. all,
research seeks, large part, make easier create agents reproduce human behavior, behavior software agents. However, approach offers significant
advantages evaluations methods.
first advantage gained replacing human domain expert expertlevel agent ensure expert-level agent novice agent
(the agent validated) represent knowledge similar manner.
provides means determining experts novices behavior differ might
otherwise availablenot examine instances actors behavior
determine differences, also directly compare knowledge guides
behavior. attribute important conducting performance assessments.
second advantage gained replacing human expert software agent
test error detection methods efficacy without influenced
complications knowledge acquisition process. Moreover, since ultimately believe
many aspects human-level behavior duplicated software agents, replacing
human expert expert-level software agent change generality
measurements. hand, examining behavior already encoded
software agents knowledge, potential methodology bias us

187

fiWallace

toward examining behaviors easy encode software opposed complete
breadth human behavior.
expert-level agents, well novice agents described implemented
Soar (Laird, Newell, & Rosenbloom, 1987), forward-chaining rule based system. Soar
provides natural constructs defining goal-subgoal relationships required behavior bounding. addition, Soar provides programming interface allows behavior
traces captured easily. Although Soar naturally compatible behavior bounding, means agent architecture fits criteria. rule based
systems use task decomposition basis problem solving even goal hierarchy must implemented agents working memory. agent design easily
done CLIPS (Giarratano & Riley, 1998) demonstrated Wallace Laird (2000).
Apart rule-based systems, many agent architectures allow developers define
agents knowledge base behavior using task decomposition relations. Two
examples PRS (Ingrand, Georgeff, & Rao, 1992) PRODIGY (Veloso et al., 1995).
Given expert-level agent (E), begin second step constructing novice agents
(N0 , . . . , Nn ) partially correct implementations final desired behavior.
novices partially correct since pursue different sequences goals actions
expert-level agent. differences arise novice-level agents
knowledge expert-level agent. Instead, portion novices
knowledge base purposely corrupted. expert/novice pair (E, N ) later
examined comparison methods identify similarities differences
actors behavior.
Novices constructed number different ways, focus novices
generated introducing random changes expert-level agent. Introducing random changes helps ensure examine wide range possible errors
minimize potential bias experiments results. Moreover, effectively maintaining large body shared knowledge expert novice agents,
straightforward map novice agents correct knowledge onto experts knowledge
well isolate problematic knowledge specific portion novices knowledge base.
allows us take maximum advantage fact using expert-level
agent opposed human domain expert mitigates complications
arise counting elements confusion matrix.
major drawback constructing novice-level agents fashion
unclear whether manner manipulate agents knowledge base representative flaws would occur naturally development process.
comparison methods examined novice-level agents knowledge base directly, would
indeed serious concern. However, comparison methods identify errors
phenomenologicallyby examining agents behavior. result, main concern
novice-level agents construct generate types observable
errors development version agents. novice-level agents create flaws cover
error types identified Section 8.2.1. Thus, high degree
confidence changes introduced following experiments represent many
observable errors would expect see actual development environment.
constructed set novice-level agents, must determine exact
set behavioral errors capable producing. third step requires careful
188

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

manual examination knowledge used by, behavior produced by,
novice expert. begin process documenting errors analyzing
novices knowledge differs experts knowledge. Based analysis,
often identify general situations novices behavior diverge experts
behavior. general situations provide high-level description errors
arise. example, might able determine novice fail perform
specific action trying accomplish particular goal, might pursue goal
inappropriate occasions. However, consider difficult predict
behavior intelligent agent simply examining knowledge, surprising
many cases hard determine exact forms general errors
may manifest using information differences agents knowledge alone.
information require examinations behavior traces collected next
step.
fourth step acquire concrete examples experts novices behavior
gathering behavior traces, BT E BTNi , used compare agents
behavior. situations including examined study, human-level agents
capable performing specified task many different ways. order examine
significant range behaviors, traces selected randomly pool possible
behaviors examined ensure two properties hold: 1) two behavior traces
identical; 2) predicted errors actually occur least one novices
behavior trace.
examining novices behavior traces ensure second property
holds, also perform fifth step process cataloging specific form
forms error manifests. way, annotate attributes error
(e.g., whether primary secondary, omission commission). includes details
may clear initial assessment actors knowledge differed
(step 3). information cataloged process used later determine
set errors detected particular approach.
Cataloging errors occur behavior trace extremely tedious process
representing bulk experimental effort. result, try maximize use
behavior trace constructing families individual experiments evaluate
impact different sets observational data without capturing inspecting new behavior
traces.
Instead simply running one experiment (E, N ) pair, run multiple experiments using different subsets observational data. process begins
actor pair (E, Ni ) selected behavior traces, BT Ni BTE ,
captured inspected. point, split observations number
subsets: nij BTNi ek BTE form individual experiments. single experiment
consists examining comparison methods performance pair subsets (n ij
ek ). family experiments contains experiments compare n ij ek
particular novice/expert pair. Thus, comparing four expert/novice pairs results
four experiment families although total number individual experiments may much
larger. constructing experiment families way, able examine impact
different observational data without overwhelmed manual inspection task.

189

fiWallace

point ready begin evaluating individual error detection
methods. important recall error detection method relies examining
examples behavior suffers potential problem unless error manifests
examples examined, cannot detected. Thus, goal experiments
determine many errors occur novice behavior traces
identified particular error detection method. validation approach relies
testing, cannot hope identify errors occur captured behavior
traces.
Given two sets behavior traces, one corresponding expert-level agent
corresponding novice agents, automated error detection method examines
traces prepares report indicating similarities differences behaviors.
report less useful depending well error detection method
performs. definition, expert-level agent standard correct behavior,
true differences instances inappropriate behavior errors. examining
information report, determine whether information summary
maps error forms identified manual examination novices behavior traces.
so, instances true positives (correctly detected errors) improve error
detection methods performance score. time also want identify many
true negatives (as well false positives false negatives) identified. Used
real validation setting, opposed evaluation setting, process would much
same. critical difference determining whether information summary
maps true errors false positives would likely require additional investigation
either manually examining examples behavior examining novice agents
knowledge base.
8.2.3 Counting Errors
error forms identified Section 8.2.1 form sets mutually exclusive
membership forms salient others, must careful
true false positives negatives calculated. Consider, example, highlevel error description pilot always contact control tower prior
initiating landing. Suppose error manifests two ways: pilot failing
contact control tower completely, pilot contacting control tower
landing initiated. Depending circumstances, manifestations may take
form omission first case, omission plus intrusion second
case. addition, since second case involves action moved inappropriate
location agents behavior sequence, also instance misplacement error.
means depending set behavior traces examined, high level
error may manifest single simple error (perhaps omission), set three
errors (two simple errors misplacement). Exactly calculate errors
recognized depends errors manifest behavior traces,
errors detected automated system.
approach counting generalized following rules:

190

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

simple errors (omission, commission, intrusion) detected, count
true false positive depending whether correspond actual errors
novices behavior.
compound errors (duplication, misplacement) detected correctly, count true positives compound error simple errors comprise compound
error. compound error detected incorrectly count false positive.
primary error (first error causal string) detected correctly, count true
positives primary error secondary errors (subsequent errors
causal string) causally linked it.
False negatives counted first finding set errors identified
error detection method. count incremented minimum number
additional errors required cover true errors.
One side effects counting method number errors reported
(RP ) error detection method may longer sum FP + TP. Instead,
one piece information report map multiple true positives, thus TP
RP FP. illustrate differences brevity reports identify similar numbers
true positives, introduce metric Report Density use assess
comparison methods performance.
Report Density =

TP
RP

report density makes reference number errors go unidentified
particular behavior comparison metric, complete assessment requires use
second metric. experiments, use sensitivity calculated follows:
Sensitivity =

TP
TP + FN

Sensitivity measurements fall range [0, 1]. sensitivity goes one, errors
identified information summary. Conversely, sensitivity goes zero,
errors identified data summary. Thus, favor comparison methods
obtain higher report density without sacrificing sensitivity. following two
subsections put experimental framework assessment metrics described thus far
use evaluating performance behavior bounding benchmark sequential
methods two distinct domains.
8.2.4 Object Retrieval Domain
first test environment simulated object retrieval domain agent must
navigate grid-based world find collect pre-specified object (initial results appear Wallace & Laird, 2003). environment relatively simple
discrete (no real valued sensors) deterministic (no exogenous events). addition,
agents operating environment generate behavior sequences relatively short length:

191

fiWallace

1

BB
action
goal

Sensitiviy

0.8
0.6
0.4
0.2
0
1

2

3

4

5

6

7

Experiment Family

Figure 8: Sensitivity object retrieval domain

Expert Behavior
P


P

P


P

P
B

P
B

P

Novice Behavior

P


P



B

P


P

P


P

P
B

P

P
B

P
B

P

P


P



B

P


Figure 9: Limitations behavior boundings HBR experiment family seven

approximately 20 30 goal action elements generated agent visits approximately 65 states. agents complete goal hierarchy maximum depth 5
contains 32 goal action nodes together. Although environment simple many
ways, serve reasonable test behavior bounding. Critically, correct behavior
object retrieval domain requires reasoning (e.g., route planning) relies data
structures fully represented within goal/sub-goal hierarchy.
Figure 8 illustrates sensitivity across seven experiment families object
retrieval domain (ordering figure arbitrary). figure illustrates two main phenomena. first obvious overall, behavior bounding better identifying behavior errors either goal action based sequential comparison methods.
fact, behavior bounding equals betters sensitivity combined action goal
sequence described Section 3 final experiment family. poor performance final experiment family second phenomena. due limitations
hierarchical representation discuss below.

192

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

8

BB
action
goal

Report Density

7
6
5
4
3
2
1
0
1

2

3

4

5

6

7

Experiment Family

Figure 10: Report Density object retrieval domain

seventh experiment family, experts behavior contains traces particular goal decomposed two ways. simplicity, well call problematic goal P .
first way expert completes P pursuing two subgoals, B, following
sequence: A, B, A. second decomposition performed pursuing subgoals simplified sequence: A, B. Importantly, expert never attempt
following decomposition: P B, A. However, first behavior trace processed
form hierarchical behavior representation, over-generalization occurs. discussed
previously, HBR contains single node represent instance identically
named goals lineage. Thus first trace, containing decomposition
P A, B, A, processed, three nodes formedone P , A, B respectively.
accommodate fact observed occur B, temporal
constraints completely generalized two nodes. situation illustrated
left hand side Figure 9. Unfortunately, behavior representation fails capture
fact expert would never perform P B, A. Thus, novices behavior
traces processed (illustrated right hand side Figure 9), little surprise
HBR produced differences detected expert
novice. contrast, error readily identified goal-based benchmark approach
(M2 ). could address particular problem using modified version HBR
describe Section 11.2. However, even approach requires additional changes
agents internal representation particular behavior encoded correctly.
Behavior boundings ability detect errors maintaining concise reports
illustrated relatively high report density (see Figure 10). Recall report density
measures amount useful information error detection methods summary. Scores
one indicate average one error could detected discrepancy indicated
summary; scores less one indicate summary contains false positives. Report
density scores higher one also possible reports remain exceedingly
concise identifying high-level errors correspond multiple low-level errors.
behavior boundings ability concisely represent relationships goals via decomposition ordering constraints, well suited identifying misplacement goal-level

193

fiWallace

1

BB
action
goal

Sensitiviy

0.8
0.6
0.4
0.2
0
1

2

3

4

5

6

Experiment Family

Figure 11: Sensitivity MOUT domain

primary errors. Moreover, structures compared relatively small (compared set sequences compared sequential approach) behavior bounding
maintain relatively low false positive count.
Behavior boundings performance object-retrieval environment encouraging.
Overall, performs well benchmark sequential comparison approaches even
though internal representation behavior constrained desires maintain
efficiency across environments differing complexity.
8.2.5 MOUT Domain
contrast object retrieval domain, MOUT Environment represents significant
increase overall complexity. environment built top Unreal, commercial 3-D
video game. continuous, non-deterministic (exogenous events occur frequently)
much longer sequence lengths object retrieval domain: 30 200
goal/action elements generated agent visits approximately 4000 distinct states
per behavior trace (the state typically changes many times selection new
goal action). goal hierarchy MOUT domain larger object
retrieval domain containing 44 nodes maximum depth 6. Equally important
added complexity environment fact MOUT built independently
research behavior comparison techniques. Thus, provides important reference
point judging overall effectiveness techniques.
Figure 11 illustrates behavior boundings sensitivity compared sequential
approaches. Results particularly dramatic, behavior bounding
fewer instances zero sensitivity (inability identify errors) either sequential approaches. addition, figure points inherent scaling problems associated
sequential method illustrates dramatic effects complicated environments. Experiment families three six behavior boundings sensitivity drops
zero worthy note. Here, errors due one aspect hierarchical
behavior representation becoming over-generalized.

194

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

0.3

BB
action
goal

Report Density

0.25
0.2
0.15
0.1
0.05
0
1

2

3

4

5

6

Experiment Family

Figure 12: Report Density MOUT domain

behavior boundings strengths better illustrated examine report
density, Figure 12. Compared either sequential approaches, behavior
boundings report density exceedingly high. cases true errors detected,
report density averages near 0.20, detecting one true error every five differences
reported summary. Even though report density lower relatively simple
object-retrieval domain, still high enough useful testing agents knowledge
base. Equally worthy note fact even two benchmarks methods
sensitive behavior bounding, usefulness error reports questionable
best due exceedingly low report density.
Although behavior bounding clearly outperformed sequential methods MOUT
domain, obvious room improvement. identify efficacy low
compared object-retrieval domain, looked back domain
novice-level agents examined.
One noticeable source false positives due called floating operators. Floating
operators performed service parent goal. Essentially, goals
actions occur opportunistically, potentially location goal hierarchy
order respond dynamics environment without explicitly suspending
canceling agents goals. agent architectures, floating operators may
better described concurrent top-level goals. Soar support concurrent goals,
however, floating operators prevailing method encoding type
opportunistic behavior.
floating operators work service parent goal, effectively
break paradigm hierarchical behavior representation effects
twofold. First, likely cause over-generalization inappropriately changing
parents node type Or. Second, limited observations available, floating
operators result representations novice agents behavior inconsistent
structure experts behavior representation (i.e., floating operator may
observed different parts experts novices hierarchy). situation

195

fiWallace

0.7

BB (if)
BB

Report Density

0.6
0.5
0.4
0.3
0.2
0.1
0
1

2

3

4

5

6

Experiment Family

Figure 13: Report Density MOUT domain ignoring floating operators

result behavior representation fails satisfy basic structure requirements
lower boundary node.
number potential methods could used circumvent problems. One method would create level indirection experts native
behavior representation presented behavior traces. preprocessing behavior traces, would possible modify topology experts
goal hierarchy floating operators longer appeared (i.e., mapped
static locations hierarchy). Although could help circumvent issues floating operators, may require significant engineering resources process behavior traces.
importantly, however, introduces another source errors confusion
probably best avoided result. Another approach would tag floating operators
could treated differently Create-Hierarchy algorithm 4 . would
increase initial cost using behavior bounding validate agent likely
cost would remain minor. third method simply ignore floating operators
altogether. Although this, course, potential reducing number errors
detected, also likely significant payoff terms reducing false positives. Moreover, floating operators fit naturally behavior boundings
structure, likely errors occur floating operators might missed
even included HBR.
Figure 13 illustrates effect report density floating operators ignored
(note change scale y-axis). expected, number false positives reduced, thus
increasing report density experiment families 3 6 (where errors
correctly identified either method). Although effect somewhat subtle,
raise average report density (excluding experiment families 3 6) nearly factor
2, 0.18 0.35, effect makes already acceptable error summary
useful.
4. may possible tag floating operators automatically based occur goal
hierarchy generalizations cause, would safest require knowledge engineer
provide tags behavior comparison performed.

196

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

Modification
Manifestation
Distinct Behaviors
Consistent BTs
Avg. BT Length

Expert

Novice-A

Novice-B

N/A
N/A
4
N/A
67

New Proposal
Intrusion
12
4
69

Missing Preference
Commission
8
4
68

Table 1: Properties expert & novice agents validation efficacy test

9. Efficacy Validation Tool
shown behavior bounding acceptable performance two domains
distinct complexity argued would well suited detecting errors many
goal oriented environments. However, point, hypothesized
error reports provided behavior bounding decrease validation cost;
provided direct evidence.
substantiate claim, performed experiment five human participants
attempted find correct flaws agents behavior without information behavior boundings error report 5 . previous experiments, agents
implemented Soar architecture. participant member Soar research
group least six months Soar programming experience. Participants identified two
behavior flaws: one with, one without aid behavior boundings error report.
unaided situation, participants relied standard debugging tools techniques
already practice using. flaw identified, participants
corrected agents knowledge using VisualSoar, standard Soar development environment. aided situation, participants given behavior boundings error report
help make sense agents behavior. Thus, experiments presented below,
two conditions: aided, unaided. Condition within-subject variable,
say participant experiences both.
test-bed agent taken object retrieval domain discussed Section 8.2.4.
initial setup followed similar lines earlier experiments. began constructing
expert-level agent exhibited correct behavior. agent could perform task
four distinct similar ways required 78 Soar rules encode. Note normal
use, observations correct behavior likely come human experts. However,
creating correct agent first, possible describe precisely flawed agents differ
ideal (both behavior implementation). property critical
experiment.
creating expert-level agent, constructed two novice-level agents (Novice-A
Novice-B). participants task identify correct behavioral differences
novice agents expert-level agent. participant would
validate novice agents (using different method one), one primary
5. Initial results reported Wallace (2007).

197

fiWallace

desires construct novice-level agents way would similarly
difficult validate. help ensure case, limited differences
novices experts knowledge single rule. case Novice-A, one rule
added resulted agent performing different sequence actions expert.
case Novice-B, preference rule removed resulting two discrepancies: one
parameters agents internal goal, another parameters agents
primitive action. Aside differences mentioned above, behavior novicelevel agents similar expert respects.
Table 1 illustrates important properties expert-level novice-level
agents. first second rows indicate change made construct
novice agents form error results changes. third row
indicates many distinct behavior traces agent capable generating. value
important gives indication many behavior traces user might
need examine order get good understanding range behavior agent
capable producing. fourth row indicates many novices behavior traces
consistent expert behavior traces (i.e., error free). Finally, fifth row indicates
average length agents behavior trace. gives indication
much information must examined instance behavior.
worth noting flaws introduced agents minor standards. experiment, flawed behavior result deadlocks infinite loops.
Indeed, viewed classical sense, agents necessarily flawed.
successful achieving desired final state (finding lost object). However,
agents pursue trajectories state/action/goal space, participants task determine trajectories differ find correct
fault causes difference.
none participants used, even seen, graphical behavior comparisons generated behavior bounding, given short, 15 minute, tutorial
become familiar graphical behavior summary provided interface. addition, participants asked read short summary provided description
debugging task, summary agents behavior, plain English description
salient goals actions would pursued task performance. overview
intended familiarize users agents domain without requiring
participant build agent ground up.
point, participants randomly assigned agent validate. attempted
mitigate bias varying order aided unaided tests presented
well pairing agent validation method. experiment,
asked participants indicate ready modify agents knowledge
articulate changes believed required. allowed us measure
amount time needed identify behavioral flaw well total time required
correct agents behavior.
first phase debugging session, participants identified novice
agents behavior differed standard set correct expert-level agent.
unaided situation, specific instructions given identify errors. Participants
free look errors using whatever debugging techniques developed
course working Soar. Similarly, aided situation specific instructions
198

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

35

Identify
Correct
Fix

30

Unaided

25
20
15
10
5
0
0

5

10

15
20
Aided

25

30

35

Figure 14: Time required identify correct errors using two techniques

identify errors given. Participants generalized tutorial experience
interpret information behavior boundings error report identify changes
would required make flawed agents behave correctly. situations,
participant correctly identified error flawed agents behavior (e.g., saying
novice always perform action X action Y), elapsed time recorded.
call time required identify error.
second phase debugging session began participant determined
ready try modifying flawed agents knowledge order correct
observed error. Regardless whether error identified using standard techniques
behavior bounding first phase, participants used VisualSoar editing environment
(a standard part Soars development environment) portion task.
participant made changes, re-examined novice agents behavior ensure
problem fact corrected. participant confident
problem resolved, clock stopped time spent beginning
phase one end phase two recorded time needed correct agents
behavior6 .
Figure 14 shows time spent participant aided unaided tasks
highlights benefits behavior bounding. x-coordinate indicates time spent
debugging aided situation information behavior boundings error summary
used y-coordinate indicates time spent unaided situation
6. cases participant believed agents behavior corrected
fact errors remained.

199

fiWallace

participants normal debugging techniques used. Three sets points plotted:
time identify error; time correct error; time required fix (i.e.,
difference time correct time identify). line = x also plotted
reference; points lie left line indicate participant performed
better (i.e., faster) aided situation.
cluster points nearest origin (labeled fix legend) indicate
behavior bounding little effect time required fix agents
knowledge error identified. Instead, behavior boundings impact, expected,
comes reduction time required identify error. leads reduction
overall time required validation task. paired t-test used determine
statistical significance three timed operations illustrated figure.
surprisingly, test confirms statistically significant performance advantage gained
using information behavior bounding time identify time
correct error (p = .0006, p = .0002 respectively). paired t-test indicate
statistically significant difference times required simply fix error aided
unaided situations (p = .85), matching expectations.
data, seems safe conclude error report provided behavior
bounding does, fact, provide information relevant identifying differences
two agents behavior useful isolating faulty knowledge. Although one
level results may considered best cases constructed errors
believed would demonstrate effectiveness behavior bounding, number
reasons results may conservative side optimistic.
First, would expect HBR useful complexity domain
agents behavior increasesdevelopers wishing examine raw behavior traces
need look longer traces traces complex environments whereas
HBR, need view one data structure. Second, test conducted
clearly influenced design behavior boundings user interface. conducted
formal experiments increase quality interface, quite possible future
implementations would capable delivering information effectively user,
thus producing increase efficiency.

10. Related Work
noted previously Section 4, number areas artificial intelligence, particularly machine learning addressed problems closely related examined here.
following subsections, briefly comment salient areas.
10.1 Plan Recognition
behavior comparison described related keyhole plan recognition (Albrecht,
Zukerman, & Nicholson, 1998), closely, team monitoring overhearing
work Kaminka, Pynadath, Tambe (2002). team monitoring, objective
determine task agent set agents performing given limited observations
actions communications pass them. Plan recognition possible, part, complete team-level plan allows monitoring system identify
agents goals observational information acquired. enough information ob200

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

tained, single plan identified ascribed agent(s). behavior comparison,
objective similar. salient difference work plan recognition
given plan library; instead attempting recreate model
execution series observations order determine whether actors
pursue goals manner (i.e., plan library).
10.2 Learning Observation
number systems (e.g, van Lent & Laird, 1999; Wang, 1995; Konik & Laird, 2006)
also developed learn procedural rules plan operators observations
expert behavior. Wangs OBSERVER (Wang, 1995) learns STRIPS style operators;
van Lents KnoMic (van Lent & Laird, 1999) learns production rules Soar agent
architecture Koniks system (Konik & Laird, 2006) creates first order logic rules
later converted Soar productions. three systems use similar behavior traces
approach, although Wangs OBSERVER works primitive actions
notion non-atomic goals thus need annotate behavior traces.
systems, Koniks demonstrated within complex domain (a 3-D
virtual environment agent must learn successfully navigate series rooms).
key difference approach lies fundamental premise.
interested learning simple concise model behavior outside
third-party use validate existing (but untrusted) agent, systems aim
learn agents knowledge altogether. learning complete task knowledge clearly
important goal community, remain set important task domains (e.g.,
military mission critical applications) learned systems often treated
skepticism human coded systems still preferred. approach described,
however, could useful help bridge gap allowing skeptical parties validate
behavior learned systems. Thus, may seem surface solving
learning executable task knowledge problem one also solves behavior comparison
problem outlined, casein mission critical applications, agents
behavior still requires validation human loop sign correctness.
Moreover, knowledge learned instead engineered, validation task likely
become much difficult one document system field questions
function particular component.
10.3 Hierarchical Reinforcement Learning
Reinforcement Learning seeks provide methods agent learn approximate optimal behavioral strategy interacting environment. reinforcement learning, optimality defined reward function outside agents
control (it part environment) agent learns interaction
environment maximize function. Traditional (flat) approaches reinforcement
learning Q-Learning (Watkins & Dayan, 1992) may require long training time
converge optimal policy. Price Boutilier (2003) show reinforcement learning
facilitated observing mentor perform task Hierarchical Reinforcement
Learning (Dietterich, 2000; Andre & Russell, 2002; Marthi, Russell, Latham, & Guestrin,

201

fiWallace

2005) seeks, part, reduce complexity learning problem use
external domain knowledge form programmer-defined action hierarchy.
traditional Reinforcement Learning (RL) Hierarchical Reinforcement Learning (HRL) differ significantly approach three fundamental ways. First,
method described previous subsection, goal (H)RL learn
executable model behavior, model used help validate system.
Second, (H)RL, models learned via interaction environment
environmentally defined reward function. Instead, interested learning directly
observation expert behavior without experimental interaction environment.
Finally, unlike RL HRL, assume existence reward function
moreover interested optimal behavior sense close approximation human behavior.
Aside important differences, commonality Hierarchal Reinforcement Learning approach stems behavior model. open issue
Dietterichs presentation MAXQ (Dietterich, 2000) restated Barto Mahadevan (2003) whether programmer-supplied information (the MAXQ task-graph)
Hierarchical Reinforcement Learning could acquired automatically. subtask
MAXQ task-graph three tuple hT , Ai , Ri i. Ti (si ) partitions state space
active states Si terminal states Ti (a subtask executed current state
Si ). Ai set actions performed achieve subtask Ri (s0 |s, a)
pseudo reward function indicating desirable terminal state subtask.
approach could used help construct part MAXQ task graph directly
observations. First, goal/subgoal hierarchy build used directly identify
Ai , set actions performed subtask. Second, task parameters
learn tied information state (this relation observed directly
behavior trace). information combined temporal constraints learn
goal/action nodes could used identify conditions task could
entered (some properties active states identified predicate ). Together
could help construct MAX-Q task graph based observations experts
performance.
10.4 Inverse Reinforcement Learning
Inverse Reinforcement Learning (IRL) (e.g., Abbeel & Ng, 2004; Ramachandran & Amir,
2007) attempts reconstruction implicit reward function given set example behaviors. IRL combination RL used simple domains reproduce behavior
explicit reward function. would permit system to, example,
learn model human experts behavior 1) reconstructing experts implicit reward
function observing example behaviors 2) interacting environment
generate policy maximizes implicit reward. Together, technologies provide
potentially powerful alternative learning observation methods described previously. However, best knowledge IRL yet demonstrated within
hierarchical setting, learning observation methods still present current
state art learning hierarchical task knowledge.

202

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

11. Extensions Behavior Bounding Future Directions
experiments behavior bounding yielded encouraging results. Yet,
complex MOUT domain, results leave room improvement. Section 5,
noted representational limitations behavior boundings HBR. Here,
examine extensions behavior bounding could positively affect performance
briefly describe promising direction future work. leave implementation
extensions detailed discussion future work.
11.1 Manual Definition Lower Boundary Node
itself, lower boundary minimal specification parameters necessary
correct behavior. is, contain constraints required discriminate
correct incorrect behavior. Although suggested lower boundary node easily formed completely generalizing upper boundary node, better
approach may construct manually.
hierarchy represented lower boundary node simply identifies space
potentially acceptable goal decompositions. result, would logical create
structure early design phase expert knowledge acquired agent.
Lee OKeefe (1994) well Yen Lee (1993) argued independently
constructing overview ways goals decompose sub-goals primitive
actions important step knowledge acquisition. Moreover, argue identifying
relationship goals, sub-goals primitive actions helps organize agents
knowledge serves foundation knowledge acquisition. Thus, may
case constructing lower boundary node manually process introduces little
additional effort part domain expert knowledge engineer.
fact, may actually benefit knowledge acquisition making process structured
directed.
constructing lower boundary node hand relatively low cost process,
reasonable ask manual effort could leveraged improve behavior boundings
performance. One use manually constructed HBR help validate agents
design early implementation process. generally believed earlier
validation take place, less costly be. constructing lower boundary
hand, may possible identify whether agent adheres constraints
statically analyzing knowledgewithout needing see agent interact
environment.
11.2 Sometimes/Always Constraints
Another potentially useful modification HBR would change association
node type constraints. current version behavior bounding,
constraints associated parent goal nodes. Alternatively, might associate similar
labels child nodes Sometimes Always. Although change subtle,
would offer modestly representational power. semantics nodes
easily covered: node simply one children Always
node one children Sometimes. semantics Sometimes

203

fiWallace

Always also make possible encapsulate new decomposition relations occur
And/Or relation.
Recall problematic behavior Section 8.2.4 HBR fails correctly encode
proper decomposition relations (specifically goal P decompose subgoals
A, B, subgoals A, B B, A). Sometimes/Always constraints
encode decomposition, albeit additional layer subgoal added
task specification. introducing two new subgoals P decomposes C , 7
C decomposes , B decomposes , would able encode
correct behavioral patterns respect P, A, B caveat
interject two new goals C D. course, point discussion justify
ad-hoc modifications task structure, rather show concrete instance
Sometimes/Always constraints may add beneficial representational power.
Sometimes/Always constraints effect learnability construction cost
HBR. tested modification detail, preliminary results
MOUT data sets indicate minor improvement performance domain.
11.3 Additional Enhancements
Two additional enhancements HBR also left future work. first ability
deal concurrent goals actions. Soar support concurrent operators,
cannot tested within exiting system. However, support added
HBR, may possible avoid issues associated floating events
encountered MOUT domain. second enhancement would allow
one node constructed represent given action/goal within particular context.
current representation, two sibling nodes name (there
exactly one node represent identically named goal/actions within context).
keeps representation simple, also held responsible representational
problems like one discussed Section 8.2.4. disadvantage approach
unclear new nodes added hierarchy. new node added
time goal/action pursued, hierarchy grows much rapidly (directly
function length behavior tracing) increasing computational complexity
decreasing rate generalization.
11.4 Behavior Bounding Runtime Environment
promising direction additional future work use ideas presented paper,
specifically constraints contained upper-boundary nodes behavior representation, monitor agents behavior runtime. approach, recently
begun explore, provides mechanism determining agent may making
inappropriate decisions (Wallace, 2005b, 2005a). Inconsistencies agents desired
course action constraints specified upper boundary node could used
enforce social policies interaction protocols groups agents dynamically adjust agents degree autonomy begins make questionably choices.
Moreover, high-level constraints specified hierarchical behavior model require
7.



indicates ALWAYS node

204

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

direct knowledge agents underlying implementation language (only goal
decomposition). means approach could also used safeguard
implementation errors agents built third parties may adequately
validated.

12. Contributions
introduced behavior bounding, model-based approach comparing two actors
behavior. novel approach uses hierarchical behavior representation motivated
desire build high-level model behavior observations either human
computer agent performance efficient create maintain effective use.
demonstrated behavior bounding meets requirements providing
theoretical empirical support claims. Finally, shown information behavior boundings comparison significantly aid process identifying
problems agents behavior, thus speeding knowledge-base validation significant
factor.

Acknowledgments
would like thank John Laird help reviewing early versions paper, along
members UM Soar research group participated user study. Portions
work supported Office Naval Research contract N61339-99-C-0104.

References
Abbeel, P., & Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning.
Proceedings Twenty First International Conference Machine Learning,
pp. 18.
Albrecht, D. W., Zukerman, I., & Nicholson, A. E. (1998). Bayesian models keyhole
plan recognition adventure game. User Modeling User-Adapted Interaction,
8 (1-2), 547.
Andre, D., & Russell, S. J. (2002). State abstraction programmable reinforcement
learning agents. Proceedings Eighteenth National Conference Artificial
Intelligence, pp. 119125.
Anrig, B., & Kohlas, J. (2002). Model-based reliability diagnostic: common framework reliability diagnostics. Stumptner, M., & Wotawa, F. (Eds.), DX02
Thirteenth International Workshop Principles Diagnosis, pp. 129136, Semmering, Austria.
Barto, A. G., & Mahadevan, S. (2003). Recent advances hierarchical reinforcement
learning. Discrete Event Dynamic Systems: Theory Applications, 13, 343379.
Bordini, R. H., Fisher, M., Visser, W., & Wooldridge, M. (2004). State-space reduction
techniques agent verification. AAMAS 04: Proceedings Third International
Joint Conference Autonomous Agents Multiagent Systems, pp. 896903.
205

fiWallace

Bordini, R. H., Fisher, M., Visser, W., & Wooldridge, M. (2006). Verifying multi-agent
programs model checking. Autonomous Agents Multi-Agent Systems, 12, 239
256.
Dietterich, T. G. (2000). Hierarchical reinforcement learning MAXQ function
decomposition. Journal Artificial Intelligence Research, 13, 227303.
Erol, K., Hendler, J., & Nau, D. S. (1994). HTN planning: Complexity expressivity.
Proceedings Twelfth National Conference Artificial Intelligence, pp. 1123
1128. AAAI Press/MIT Press.
Fisher, M. (2005). Temporal development methods agent-based systems. Autonomous
Agents Multi-Agent Systems, 10, 4166.
Giarratano, J., & Riley, G. (1998). Expert Systems: Principles Programming. PWS
Publishing Co., Boston, MA.
Haussler, D. (1988). Quantifying inductive bias: AI learning algorithms Valiants learning framework.. Artificial Intelligence, 36, 177221.
Ingrand, F. F., Georgeff, M. P., & Rao, A. S. (1992). architecture real-time reasoning
system control. IEEE Expert, 7 (6), 3344.
John, B. E., & Kieras, D. E. (1996). GOMS family user interface analysis techniques:
Comparison contrast. ACM Transactions ComputerHuman Interaction, 3 (4),
320351.
Jones, R. M., Laird, J. E., Nielsen, P. E., Coulter, K. J., Kenny, P., & Koss, F. V. (1999).
Automated intelligent pilots combat flight simulation. AI Magazine, 20 (1), 2742.
Kaminka, G. A., Pynadath, D. V., & Tambe, M. (2002). Monitoring teams overhearing:
multi-agent plan-recognition approach. Journal Artificial Intelligence Research,
17, 83135.
Kirani, S. H., Zualkernan, I. A., & Tsai, W.-T. (1994). Evaluation expert system testing
methods. Communications ACM, 37 (11), 7181.
Konik, T., & Laird, J. E. (2006). Learning goal hierarchies structured observations
expert annotations. Machine Learning, 64 (13), 263287.
Laird, J. E., Newell, A., & Rosenbloom, P. S. (1987). Soar: architecture general
intelligence. Artificial Intelligence, 33 (1), 164.
Lee, S., & OKeefe, R. M. (1994). Developing strategy expert system verification
validation. IEEE Transactions Systems, Man Cybernetics, 24 (4), 643655.
Lucas, P. (1998). Analysis notions diagnosis. Artificial Intelligence, 105, 295343.
Marthi, B., Russell, S., Latham, D., & Guestrin, C. (2005). Concurrent hierarchical reinforcement learning. Proceedings International Joint Conference Artificial
Intelligence 2005, pp. 779785.
Menzies, T. (1999). Knowledge maintenance: state art. Knowledge Engineering Review, 14 (1), 146.
Mitchell, T. M. (1982). Generalization search. Artificial Intelligence, 18 (2), 203226.

206

fiBehavior Bounding: Efficient Method High-Level Behavior Comparison

Mitchell, T. M. (1997). Machine Learning. McGraw-Hill.
Murphy, P. M., & Pazzani, M. J. (1994). Revision production system rule-bases.
Proceedings Eleventh International Conference Machine Learning, pp. 199
207. Morgan Kaufmann.
Price, B., & Boutilier, C. (2003). Accelerating reinforcement learning implicit
imitation. Journal Artificial Intelligence Research, 19, 569629.
Ramachandran, D., & Amir, E. (2007). Bayesian inverse reinforcement learning. Proceedings International Joint Conference Artificial Intelligence 2007, pp. 2586
2591.
Rickel, J., Marcella, S., Gratch, J., Hill, R., Traum, D., & Swartout, W. (2002). Toward
new generation virtual humans interactive experiences. IEEE Intelligent
Systems, 17 (4), 3238.
Shortliffe, E. H. (1987). Computer programs support clinical decision making. Journal
American Medical Association, 258 (1), 6166.
Swartout, W., Hill, R., Gratch, J., Johnson, W. L., Kyriakakis, C., LaBore, C., Lindheim,
R., Marsella, S., Miraglia, D., Moore, B., Morie, J., Rickel, J., Thiebaux, M., Tuh,
L., Whitney, R., & Douglas, J. (2001). Toward holodeck: Integrating graphics,
sound, character story. Proceedings Fifth International Conference
Autonomous Agents, pp. 409416.
Traum, D., Rickel, J., Gratch, J., & Marsella, S. (2003). Negotiation tasks hybrid
human-agent teams simulation-based training. AAMAS 03: Proceedings
Second International Joint Conference Autonomous Agents Multiagent
Systems, pp. 441448.
Tsai, W.-T., Vishnuvajjala, R., & Zhang, D. (1999). Verification validation
knowledge-based systems. IEEE Transactions Knowledge Data Engineering,
11 (1), 202212.
van Lent, M. C., & Laird, J. E. (1999). Learning hierarchical performance knowledge
observation. Proceedings 1999 International Conference Machine
Learning, pp. 229238.
Veloso, M., Carbonell, J., Perez, A., Borrajo, D., Fink, E., & Blythe, J. (1995). Integrating planning learning: PRODIGY architecture. Journal Theoretical
Experimental Artificial Intelligence, 7 (1), 81120.
Wallace, S. A. (2005a). Abstract behavior representations self-assessment. AAAI
Spring Symposium Meta-Cognition Computation (ASSMC 2005). AAAI Technical Report SS-05-04., pp. 120125.
Wallace, S. A. (2005b). S-Assess: library self-assessment. Proceedings Fourth
International Conference Autonomous Agents Multiagent Systems (AAMAS05), pp. 256263.
Wallace, S. A. (2007). Enabling trust behavior metamodels. AAAI Spring Symposium Interaction Challenges Intelligent Agents (ASSICIA 2007). AAAI Technical Report SS-07-04., pp. 124131.
207

fiWallace

Wallace, S. A., & Laird, J. E. (2000). Toward methodology AI architecture evaluation:
Comparing Soar CLIPS. Jennings, N., & Lesperance, Y. (Eds.), Intelligent
Agents VI Proceedings Sixth International Workshop Agent Theories,
Architectures, Languages (ATAL-99), Lecture Notes Artificial Intelligence, pp.
117131. Springer-Verlag, Berlin.
Wallace, S. A., & Laird, J. E. (2003). Behavior Bounding: Toward effective comparisons
agents & humans. Proceedings Eighteenth International Joint Conference
Artificial Intelligence, pp. 727732.
Wang, X. (1995). Learning observation practice: incremental approach planning operator acquisition. Proceedings Twelfth International Conference
Machine Learning, pp. 549557.
Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8, 279292.
Weitzel, J. R., & Kerschberg, L. (1989). Developing knowledge-based systems: Reorganizing
system development life cycle. Communications ACM, 32 (4), 482488.
Yen, J., & Lee, J. (1993). task-based methodology specifying expert systems. IEEE
Expert, 8 (1), 815.
Yost, G. R. (1996). Implementing Sisyphus-93 task using Soar/TAQL. International
Journal Human-Computer Studies, 44, 281301.

208

fiJournal Artificial Intelligence Research 34 (2009) 443-498

Submitted 08/08; published 03/09

Wikipedia-based Semantic Interpretation
Natural Language Processing
Evgeniy Gabrilovich
Shaul Markovitch

gabr@yahoo-inc.com
shaulm@cs.technion.ac.il

Department Computer Science
TechnionIsrael Institute Technology
Technion City, 32000 Haifa, Israel

Abstract
Adequate representation natural language semantics requires access vast amounts
common sense domain-specific world knowledge. Prior work field based
purely statistical techniques make use background knowledge, limited
lexicographic knowledge bases WordNet, huge manual efforts
CYC project. propose novel method, called Explicit Semantic Analysis (ESA),
fine-grained semantic interpretation unrestricted natural language texts. method
represents meaning high-dimensional space concepts derived Wikipedia,
largest encyclopedia existence. explicitly represent meaning text terms
Wikipedia-based concepts. evaluate effectiveness method text categorization computing degree semantic relatedness fragments natural
language text. Using ESA results significant improvements previous state
art tasks. Importantly, due use natural concepts, ESA model
easy explain human users.

1. Introduction
Recent proliferation World Wide Web, common availability inexpensive storage
media accumulate time enormous amounts digital data, contributed
importance intelligent access data. sheer amount data available
emphasizes intelligent aspect accessno one willing capable browsing
small subset data collection, carefully selected satisfy ones
precise information need.
Research artificial intelligence long aimed endowing machines ability
understand natural language. One core issues challenge represent language semantics way manipulated computers. Prior work
semantics representation based purely statistical techniques, lexicographic knowledge, elaborate endeavors manually encode large amounts knowledge. simplest
approach represent text semantics treat text unordered bag words,
words (possibly stemmed) become features textual object.
sheer ease approach makes reasonable candidate many information retrieval
tasks search text categorization (Baeza-Yates & Ribeiro-Neto, 1999; Sebastiani,
2002). However, simple model reasonably used texts fairly long,
performs sub-optimally short texts. Furthermore, little address two
main problems natural language processing (NLP), polysemy synonymy.
c
2009
AI Access Foundation. rights reserved.

fiGabrilovich & Markovitch

Latent Semantic Analysis (LSA) (Deerwester, Dumais, Furnas, Landauer, & Harshman,
1990) another purely statistical technique, leverages word co-occurrence information large unlabeled corpus text. LSA use explicit human-organized
knowledge; rather, learns representation applying Singular Value Decomposition
(SVD) words-by-documents co-occurrence matrix. LSA essentially dimensionality reduction technique identifies number prominent dimensions data,
assumed correspond latent concepts. Meanings words documents
represented space defined concepts.
Lexical databases WordNet (Fellbaum, 1998) Rogets Thesaurus (Roget, 1852)
encode important relations words synonymy, hypernymy, meronymy.
Approaches based resources (Budanitsky & Hirst, 2006; Jarmasz, 2003) map text
words word senses, use latter concepts. However, lexical resources offer
little information different word senses, thus making word sense disambiguation
nearly impossible achieve. Another drawback approaches creation
lexical resources requires lexicographic expertise well lot time effort, consequently resources cover small fragment language lexicon. Specifically,
resources contain proper names, neologisms, slang, domain-specific technical
terms. Furthermore, resources strong lexical orientation predominantly contain information individual words, little world knowledge general.
inherently limited individual words, approaches require extra level
sophistication handle longer texts (Mihalcea, Corley, & Strapparava, 2006); example,
computing similarity pair texts amounts comparing word one text
word text.
Studies artificial intelligence long recognized importance knowledge
problem solving general, natural language processing particular. Back
early years AI research, Buchanan Feigenbaum (1982) formulated knowledge
power hypothesis, postulated power intelligent program perform
task well depends primarily quantity quality knowledge
task.
computer programs face tasks require human-level intelligence, natural language processing, natural use encyclopedia endow machine
breadth knowledge available humans. are, however, several obstacles
way using encyclopedic knowledge. First, knowledge available textual
form, using may require natural language understanding, major problem
right. Furthermore, even language understanding may enough, texts written
humans normally assume reader possesses large amount common-sense knowledge,
omitted even detailed encyclopedia articles (Lenat, 1997). Thus,
circular dependencyunderstanding encyclopedia articles requires natural language
understanding capabilities, latter turn require encyclopedic knowledge.
address situation, Lenat colleagues launched CYC project, aims
explicitly catalog common sense knowledge humankind.
developed new methodology makes possible use encyclopedia directly,
without need manually encoded common-sense knowledge. Observe encyclopedia consists large collection articles, provides comprehensive
exposition focused single topic. Thus, view encyclopedia collection con444

fiWikipedia-based Semantic Interpretation

cepts (corresponding articles), accompanied large body text (the article
contents). propose use high-dimensional space defined concepts order
represent meaning natural language texts. Compared bag words LSA
approaches, using concepts allows computer benefit huge amounts world
knowledge, normally accessible humans. Compared electronic dictionaries
thesauri, method uses knowledge resources order magnitude larger,
also uniformly treats texts arbitrarily longer single word. Even
importantly, method uses body text accompanies concepts order
perform word sense disambiguation. show later, using knowledge-rich concepts
addresses polysemy synonymy, longer manipulate mere words. call
method Explicit Semantic Analysis (ESA), uses knowledge concepts explicitly
defined manipulated humans.
approach applicable many NLP tasks whose input document (or shorter
natural language utterance), output decision based document contents.
Examples tasks information retrieval (whether document relevant), text
categorization (whether document belongs certain category), comparing pairs
documents assess similarity.1
Observe documents manipulated tasks given form
encyclopedic knowledge intend useplain text. key observation allows us
circumvent obstacles enumerated above, use encyclopedia directly, without
need deep language understanding pre-cataloged common-sense knowledge.
quantify degree relevance Wikipedia concept input text comparing
text article associated concept.
Let us illustrate importance external knowledge couple examples. Without using external knowledge (specifically, knowledge financial markets), one infer
little information brief news title Bernanke takes charge. However, using
algorithm developed consulting Wikipedia, find following concepts
highly relevant input: Ben Bernanke, Federal Reserve, Chairman
Federal Reserve, Alan Greenspan (Bernankes predecessor), Monetarism (an economic theory money supply central banking), inflation deflation. another
example, consider title Apple patents Tablet Mac. Without deep knowledge hitech industry gadgets, one finds hard predict contents news item. Using
Wikipedia, identify following related concepts: Apple Computer2 , Mac OS (the
Macintosh operating system) Laptop (the general name portable computers,
Tablet Mac specific example), Aqua (the GUI Mac OS X), iPod (another prominent product Apple), Apple Newton (the name Apples early personal digital
assistant).
ease presentation, examples showed concepts identified
ESA relevant input. However, essence method representing meaning text weighted combination Wikipedia concepts. Then,
1. Thus, consider tasks machine translation natural language generation, whose output
includes new piece text based input.
2. Note correctly identify concept representing computer company (Apple Computer)
rather fruit (Apple).

445

fiGabrilovich & Markovitch

depending nature task hand either use entire vectors concepts,
use relevant concepts enrich bag words representation.
contributions paper twofold. First, propose new methodology
use Wikipedia enriching representation natural language texts. approach,
named Explicit Semantic Analysis, effectively capitalizes human knowledge encoded
Wikipedia, leveraging information cannot deduced solely input texts
processed. Second, evaluate ESA two commonly occurring NLP tasks, namely, text
categorization computing semantic relatedness texts. tasks, using ESA
resulted significant improvements existing state art performance.
Recently, ESA used researchers variety tasks, consistently proved
superior approaches explicitly used large-scale repositories human
knowledge. Gurevych, Mueller, Zesch (2007) re-implemented ESA approach
German-language Wikipedia, found superior judging semantic relatedness
words compared system based German version WordNet (GermaNet). Chang,
Ratinov, Roth, Srikumar (2008) used ESA text classification task without explicit
training set, learning knowledge encoded Wikipedia. Milne Witten
(2008) found ESA compare favorably approaches solely based hyperlinks,
thus confirming wealth textual descriptions Wikipedia exlicitly superior
using structural information alone.

2. Explicit Semantic Analysis
meaning word cat? One way interpret word cat via
explicit definition: cat mammal four legs, belongs feline species,
etc. Another way interpret meaning cat strength association
concepts know: cat relates strongly concepts feline pet, somewhat
less strongly concepts mouse Tom & Jerry, etc.
use latter association-based method assign semantic interpretation words
text fragments. assume availability vector basic concepts, C1 , . . . , Cn ,
represent text fragment vector weights, w1 , . . . , wn , wi represents
strength association Ci . Thus, set basic concepts viewed
canonical n-dimensional semantic space, semantics text segment corresponds
point space. call weighted vector semantic interpretation vector
t.
canonical representation powerful, effectively allows us estimate
semantic relatedness text fragments distance space. following
section describe two main components scheme: set basic concepts,
algorithm maps text fragments interpretation vectors.
2.1 Using Wikipedia Repository Basic Concepts
build general semantic interpreter represent text meaning variety
tasks, set basic concepts needs satisfy following requirements:
1. comprehensive enough include concepts large variety topics.
446

fiWikipedia-based Semantic Interpretation

2. constantly maintained new concepts promptly added
needed.
3. Since ultimate goal interpret natural language, would like concepts
natural, is, concepts recognized used human beings.
4. concept Ci associated text di , determine strength
affinity term language.
Creating maintaining set natural concepts requires enormous effort many
people. Luckily, collection already exists form Wikipedia, one
largest knowledge repositories Web. Wikipedia available dozens languages,
English version largest all, contains 300+ million words nearly
one million articles, contributed 160,000 volunteer editors. Even though Wikipedia
editors required established researchers practitioners, open editing approach yields remarkable quality. recent study (Giles, 2005) found Wikipedia accuracy
rival Encyclopaedia Britannica. However, Britannica order magnitude
smaller, 44 million words 65,000 articles (http://store.britannica.com, visited
February 10, 2006).
appropriate encyclopedia, article comprises comprehensive exposition
single topic. Consequently, view Wikipedia article defining concept
corresponds topic. example, article artificial intelligence defines
concept Artificial Intelligence, article parasitic extraction circuit
design defines concept Layout extraction.3 body articles critical
approach, allows us compute affinity concepts words
input texts.
important advantage approach thus use vast amounts highly organized human knowledge. Compared lexical resources WordNet, methodology
leverages knowledge bases orders magnitude larger comprehensive.
Importantly, Web-based knowledge repositories use work undergo constant
development breadth depth steadily increase time. Compared Latent
Semantic Analysis, methodology explicitly uses knowledge collected organized
humans. semantic analysis explicit sense manipulate manifest concepts grounded human cognition, rather latent concepts used LSA. Therefore,
call approach Explicit Semantic Analysis (ESA).
2.2 Building Semantic Interpreter
Given set concepts, C1 , . . . , Cn , set associated documents, d1 , . . . , dn , build
sparse table n columns corresponds concept, rows

corresponds word occurs i=1...n di . entry [i, j] table corresponds
TFIDF value term ti document dj
[i, j] = tf (ti , dj ) log

n
,
dfi

3. use titles articles convenient way refer articles, algorithm treats
articles atomic concepts.

447

fiGabrilovich & Markovitch

term frequency defined
(

tf (ti , dj ) =

1 + log count(ti , dj ), count(ti , dj ) > 0
,
0,
otherwise

dfi = |{dk : ti dk }| number documents collection contain
term ti (document frequency).
Finally, cosine normalization applied row disregard differences document
length:
[i, j]
[i, j] pPr
,
2
l=1 [i, j]
r number terms.
semantic interpretation word ti obtained row table . is,
meaning word given vector concepts paired TFIDF scores,
reflect relevance concept word.
semantic interpretation text fragment, ht1 , . . . , tk i, centroid vectors
representing individual words. definition allows us partially perform word sense
disambiguation. Consider, example, interpretation vector term mouse.
two sets strong components, correspond two possible meanings: mouse (rodent) mouse (computing). Similarly, interpretation vector word screen
strong components associated window screen computer screen. text
fragment purchased mouse screen, summing two interpretation vectors boost computer-related components, effectively disambiguating words.
Table also viewed inverted index, maps word list
concepts appears. Inverted index provides efficient computation
distance interpretation vectors.
Given amount information encoded Wikipedia, essential control
amount noise present text. discarding insufficiently developed articles,
eliminating spurious association articles words. done setting
zero weights concepts whose weights given term low (see
Section 3.2.3).
2.3 Using Link Structure
natural electronic encyclopedia provide cross-references form
hyperlinks. result, typical Wikipedia article many links entries
articles conventional printed encyclopedias.
link structure used number ways. Observe link associated
anchor text (clickable highlighted phrase). anchor text always identical
canonical name target article, different anchor texts used refer
article different contexts. example, anchor texts pointing Federal
Reserve include Fed, U.S. Federal Reserve Board, U.S. Federal Reserve System,
Board Governors Federal Reserve, Federal Reserve Bank, foreign reserves
Free Banking Era. Thus, anchor texts provide alternative names, variant spellings,
related phrases target concept, use enrich article text
target concept.
448

fiWikipedia-based Semantic Interpretation

Furthermore, inter-article links often reflect important relations concepts
correspond linked articles. explore use relations feature generation
next section.
2.3.1 Second-order Interpretation
Knowledge concepts subject many relations, including generalization, meronymy
(part of), holonymy synonymy, well specific relations capital of,
birthplace/birthdate etc. Wikipedia notable example knowledge repository
features relations, represented hypertext links Wikipedia
articles.
links encode large amount knowledge, found article texts.
Consequently, leveraging knowledge likely lead better interpretation models.
therefore distinguish first-order models, use knowledge encoded
Wikipedia articles, second-order models, also incorporate knowledge encoded
inter-article links. Similarly, refer information obtained inter-article
links second-order information.
rule, presence link implies relation concepts connects.
example, article United States links Washington, D.C. (country
capital) North America (the continent country situated). also links
multitude concepts, definitely related source concept, albeit
difficult define relations; links include United States Declaration
Independence, President United States, Elvis Presley.
However, observations reveal existence link always imply
two articles strongly related.4 fact, many words phrases typical Wikipedia
article link articles entries corresponding concepts.
example, Education subsection article United States gratuitous
links concepts High school, College, Literacy rate. Therefore, order
use Wikipedia links semantic interpretation, essential filter linked concepts
according relevance text fragment interpreted.
intuitive way incorporate concept relations examine number top-scoring
concepts,
Eto boost scores concepts linked them. Let ESA(1) (t) =

(1)
(1)
w1 , . . . , wn interpretation vector term t. define second-level interpretation term


(2)

ESA(2) (t) = w1 , . . . , wn(2)

(2)

wi

(1)

= wi

X

+

E

(1)

wj

{j|link(cj ,ci )}

Using < 1 ensures linked concepts taken reduced weights.
experiments used = 0.5.
4. opposite also truethe absence link may simply due oversight. Adafre de Rijke
(2005) studied problem discovering missing links Wikipedia.

449

fiGabrilovich & Markovitch

2.3.2 Concept Generality Filter
new concepts identified links equally useful. Relevance newly
added concepts certainly important, criterion. Suppose
given input text Google search. additional concept likely
useful characterize input: Nigritude ultramarine (a specially crafted meaningless
phrase used search engine optimization contest) Website? suppose input
artificial intelligence concept likely contribute representation
input, John McCarthy (computer scientist) Logic? believe
examples, second concept would useful overly specific.
Consequently, conjecture add linked concepts sparingly, taking
general concepts triggered them. judge
generality concepts? may tricky achieve general case (no pun
intended), propose following task-oriented criterion. Given two concepts ca cb ,
compare numbers links pointing them. Then, say ca general
cb number incoming links least order magnitude larger, is,
log10 (#inlinks(ca )) log10 (#inlinks(cb )) > 1.
show examples additional concepts identified using inter-article links Section 4.5.1. Section 4.5.4 evaluate effect using inter-article links additional
knowledge source. section also specifically examine effect using
general linked concepts (i.e., adding concepts general concepts
triggered them).

3. Using Explicit Semantic Analysis Computing Semantic
Relatedness Texts
section discuss application semantic interpretation methodology
automatic assessment semantic relatedness words texts.5
3.1 Automatic Computation Semantic Relatedness
related cat mouse? preparing manuscript writing article? ability quantify semantic relatedness texts underlies many fundamental tasks computational linguistics, including word sense disambiguation, information
retrieval, word text clustering, error correction (Budanitsky & Hirst, 2006). Reasoning semantic relatedness natural language utterances routinely performed
humans remains unsurmountable obstacle computers. Humans judge text
relatedness merely level text words. Words trigger reasoning much deeper
level manipulates conceptsthe basic units meaning serve humans organize
share knowledge. Thus, humans interpret specific wording document
much larger context background knowledge experience. Lacking
elaborate resources, computers need alternative ways represent texts reason
them.
Explicit Semantic Analysis represents text interpretation vectors high-dimensional space concepts. representation, computing semantic relatedness texts
5. Preliminary results research reported Gabrilovich Markovitch (2007a).

450

fiWikipedia-based Semantic Interpretation

Building Semantic Interpreter

word1
wordi

Building weighted
inverted index
Wikipedia

wordn

Weighted list
concepts
(= Wikipedia
articles)

Weighted
inverted index

Using Semantic Interpreter

Text1

Semantic
interpreter

Vector
comparison

Relatedness
estimation

Text2
Weighted
vector
Wikipedia
concepts

Figure 1: Knowledge-based semantic interpreter

simply amounts comparing vectors. Vectors could compared using variety
metrics (Zobel & Moffat, 1998); use cosine metric throughout experiments
reported paper. Figure 1 illustrates process.
3.2 Implementation Details
used Wikipedia snapshot November 11, 2005. parsing Wikipedia XML
dump, obtained 1.8 Gb text 910,989 articles. Although Wikipedia almost
million articles, equally useful feature generation. articles correspond overly specific concepts (e.g., Metnal, ninth level Mayan underworld),
otherwise unlikely useful subsequent text categorization (e.g., specific dates
list events happened particular year). articles short,
cannot reliably classify texts onto corresponding concepts. developed set
simple heuristics pruning set concepts, discarding articles fewer
100 non stop words fewer 5 incoming outgoing links. also discard articles describe specific dates, well Wikipedia disambiguation pages, category pages
like. pruning, 171,332 articles left defined concepts used
feature generation. processed text articles first tokenizing it, removing
stop words rare words (occurring fewer 3 articles), stemmed remaining
words; yielded 296,157 distinct terms.
451

fiGabrilovich & Markovitch

3.2.1 Preprocessing Wikipedia XML Dump
Wikipedia data publicly available online http://download.wikimedia.org.
data distributed XML format, several packaged versions available: article texts,
edit history, list page titles, interlanguage links etc. project, use article
texts, ignore information article authors page modification history.
building semantic interpreter, perform number operations distributed
XML dump:
simplify original XML removing fields used feature
generation, author ids last modification times.
Wikipedia syntax defines proprietary format inter-article links, whereas name
article referred enclosed brackets (e.g., [United States]). map
articles numeric ids, article build list ids articles refers
to. also count number incoming outgoing links article.
Wikipedia defines redirection mechanism, maps frequently used variant names
entities canonical names. examples, United States America
mapped United States. resolve redirections initial preprocessing.
Another frequently used mechanism templates, allows articles include
frequently reused fragments text without duplication, including pre-defined
optionally parameterized templates fly. speed subsequent processing,
resolve template inclusions beginning.
also collect anchor texts point article.
preprocessing stage yields new XML file, used building feature
generator.
3.2.2 Effect Knowledge Breadth
Wikipedia constantly expanded new material volunteer editors contribute
new articles extend existing ones. Consequently, conjectured addition
information beneficial ESA, would rely larger knowledge base.
test assumption, also acquired newer Wikipedia snapshot March 26,
2006. Table 1 presents comparison amount information two Wikipedia
snapshots used. number articles shown table reflects total number
articles date snapshot. next table line (the number concepts
used) reflects number concepts remained pruning explained
beginning Section 3.2.
following sections confirm using larger knowledge base beneficial
ESA, juxtaposing results obtained two Wikipedia snapshots. Therefore,
dimensionality reduction performed, input text fragment represented space 171,332 features (or 241,393 features case later
Wikipedia snapshot); course, many features zero values, feature
vectors sparse.
452

fiWikipedia-based Semantic Interpretation

Combined article text
Number articles
Concepts used
Distinct terms

Wikipedia snapshot
November 11, 2005
1.8 Gb
910,989
171,332
296,157

Wikipedia snapshot
March 23, 2006
2.9 Gb
1,187,839
241,393
389,202

Table 1: Comparison two Wikipedia snapshots
3.2.3 Inverted Index Pruning
eliminate spurious association articles words setting zero weights
concepts whose weights given term low.
algorithm pruning inverted index operates follows. first sort
concepts given word according TFIDF weights decreasing order.
scan resulting sequence concepts sliding window length 100, truncate
sequence difference scores first last concepts window
drops 5% highest-scoring concept word (which positioned first
sequence). technique looks fast drops concept scores, would signify
concepts tail sequence loosely associated word (i.e.,
even though word occurred articles corresponding concepts,
truly characteristic article contents). evaluated principled approaches
observing values first second derivatives, data seemed
noisy reliable estimation derivatives. researchers studied use derivatives
similar contexts (e.g., Begelman, Keller, & Smadja, 2006), also found
derivative alone sufficient, hence found necessary estimate magnitude
peaks means. Consequently, opted use simple efficient metric.
purpose pruning eliminate spurious associations concepts
terms, mainly beneficial pruning inverted index entries common
words occur many Wikipedia articles. Using criteria, analyzed
inverted index Wikipedia version dated November 11, 2005 (see Section 3.2.2).
majority terms, either fewer 100 concepts non-zero weight,
concept-term weights decreased gracefully qualify pruning. pruned
entries 4866 terms total 296,157 terms. Among terms whose concept
vector pruned, term link largest number concepts non-zero
weight106,988of retained 838 concepts (0.8%); another example,
concept vector term number pruned 52,244 entries 1360 (2.5%).
average, 24% concepts retained. pruning rates second
Wikipedia version (dated March 23, 2006) similar these.
3.2.4 Processing Time
Using world knowledge requires additional computation. extra computation includes
(one-time) preprocessing step semantic interpreter built, well
actual mapping input texts interpretation vectors, performed online. standard workstation, parsing Wikipedia XML dump takes 7 hours, building
453

fiGabrilovich & Markovitch

semantic interpreter takes less hour. semantic interpreter built,
throughput (i.e., generation interpretation vectors textual input) several hundred words per second. light improvements computing semantic relatedness
text categorization accuracy report Sections 3 4, believe
extra processing time well compensated for.
3.3 Empirical Evaluation Explicit Semantic Analysis
Humans innate ability judge semantic relatedness texts. Human judgements
reference set text pairs thus considered correct definition, kind gold
standard computer algorithms evaluated. Several studies measured
inter-judge correlations found consistently high (Budanitsky & Hirst, 2006;
Jarmasz, 2003; Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, & Ruppin, 2002a),
r = 0.88 0.95. findings expectedafter all, consensus allows
people understand other. Consequently, evaluation amounts computing
correlation ESA relatedness scores human judgments.
better evaluate Wikipedia-based semantic interpretation, also implemented semantic interpreter based another large-scale knowledge repositorythe Open Directory
Project (ODP, http://www.dmoz.org), largest Web directory date. case
ODP, concepts Ci correspond categories directory (e.g., Top/Computers/Artificial Intelligence), text di associated concept obtained pooling
together titles descriptions URLs catalogued corresponding category. Interpretation text fragment amounts computing weighted vector ODP
concepts, ordered affinity input text. built ODP-based semantic
interpreter using ODP snapshot April 2004. implementation details
found previous work (Gabrilovich & Markovitch, 2005, 2007b).
3.3.1 Test Collections
work, use two datasets best knowledge largest publicly
available collections kind.6 test collections, use correlation
computer-assigned scores human scores assess algorithm performance.
assess word relatedness, use WordSimilarity-353 collection (Finkelstein et al.,
2002a; Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, & Ruppin, 2002b),
contains 353 noun pairs representing various degrees similarity.7 pair 13
16 human judgements made individuals university degrees either mothertongue-level otherwise fluent command English language. Word pairs
assigned relatedness scores scale 0 (totally unrelated words) 10 (very much
related identical words). Judgements collected word pair averaged
6. Recently, Zesch Gurevych (2006) discussed automatic creation datasets assessing semantic
similarity. However, focus work automatical generation set sufficiently
diverse word pairs, thus relieving humans need construct word lists manually. Obviously,
establishing gold standard semantic relatedness word pair still performed manually
human judges.
7. previous studies (Jarmasz & Szpakowicz, 2003) suggested word pairs comprising
collection might culturally biased.

454

fiWikipedia-based Semantic Interpretation

produce single relatedness score.8 Spearmans rank-order correlation coefficient used
compare computed relatedness scores human judgements; non-parametric,
Spearmans correlation coefficient considered much robust Pearsons
linear correlation. comparing results studies, computed
Spearmans correlation coefficient human judgments based raw data.
document similarity, used collection 50 documents Australian
Broadcasting Corporations news mail service (Lee, Pincombe, & Welsh, 2005; Pincombe,
2004). documents 51 126 words long, covered variety topics.
judges 83 students University Adelaide, Australia, paid
small fee work. documents paired possible ways,
1,225 pairs 812 human judgements (averaged pair). neutralize effects
ordering, document pairs presented random order, order documents
within pair randomized well. human judgements averaged
pair, collection 1,225 relatedness scores 67 distinct values. Spearmans
correlation appropriate case, therefore used Pearsons linear correlation
coefficient.
Importantly, instructions human judges test collections specifically directed
participants assess degree relatedness words texts involved. example,
case antonyms, judges instructed consider similar rather
dissimilar.
3.3.2 Prior Work
number prior studies proposed variety approaches computing word similarity
using WordNet, Rogets thesaurus, LSA. Table 2 presents results applying
approaches WordSimilarity-353 test collection.
Jarmasz (2003) replicated results several WordNet-based methods, compared
new approach based Rogets Thesaurus. Hirst St-Onge (1998) viewed
WordNet graph, considered length directionality graph path connecting two nodes. Leacock Chodorow (1998) also used length shortest graph
path, normalized maximum taxonomy depth. Jiang Conrath (1997),
later Resnik (1999), used notion information content lowest node subsuming two given words. Lin (1998b) proposed computation word similarity based
information theory. See (Budanitsky & Hirst, 2006) comprehensive discussion
WordNet-based approaches computing word similarity.
According Jarmasz (2003), Rogets Thesaurus number advantages compared
WordNet, including links different parts speech, topical groupings, variety relations word senses. Consequently, method developed authors
using Rogets source knowledge achieved much better results WordNet-based
methods. Finkelstein et al. (2002a) reported results computing word similarity using
8. Finkelstein et al. (2002a) report inter-judge agreement 0.95 WordSimilarity-353 collection.
also performed assessment inter-judge agreement dataset. Following Snow,
OConnor, Jurafsky, Ng (2008), divided human judges two sets averaged numeric
judgements word pair among judges set, thus yielding (353 element long) vector
average judgments set. Spearmans correlation coefficient vectors two sets
0.903.

455

fiGabrilovich & Markovitch

LSA-based model (Deerwester et al., 1990) trained Grolier Academic American Encyclopedia. Recently, Hughes Ramage (2007) proposed method computing semantic relatedness using random graph walks; results WordSimilarity353 dataset competitive reported Jarmasz (2003) Finkelstein et al.
(2002a).
Strube Ponzetto (2006) proposed alternative approach computing word similarity based Wikipedia, comparing articles whose titles words occur. discuss
approach greater detail Section 5.1.
Prior work assessing similarity textual documents based comparing
documents bags words, well LSA. Lee et al. (2005) compared number
approaches based bag words representation, used binary tfidf
representation word weights variety similarity measures (correlation, Jaccard,
cosine, overlap). authors also implemented LSA-based model trained set
news documents Australian Broadcasting Corporation (test documents whose
similarity computed came distribution). results experiments
reported Table 3.
3.3.3 Results
better understand Explicit Semantic Analysis works, let us consider similarity computation pairs actual phrases. example, given two phrases scientific article
journal publication, ESA determines following Wikipedia concepts found
among top 20 concepts phrase: Scientific journal, Nature (journal),
Academic publication, Science (journal), Peer review. compute
similarity RNA DNA, following concepts found shared among
top 20 lists: Transcription (genetics), Gene, RNA, Cell (biology).
presence identical concepts among top concepts characterizing phrase
allows ESA establish semantic similarity.
Table 2 shows results applying methodology estimating relatedness
individual words, statistically significant improvements shown bold. values
shown table represent Spearmans correlation human judgments
relatedness scores produced different methods. Jarmasz (2003) compared
performance 5 WordNet-based metrics, namely, proposed Hirst St-Onge
(1998), Jiang Conrath (1997), Leacock Chodorow (1998), Lin (1998b), Resnik
(1999). Table 2 report performance best metrics, namely,
Lin (1998b) Resnik (1999). WikiRelate! paper (Strube & Ponzetto, 2006),
authors report results many 6 different method variations, report
performance best one (based metric proposed Leacock Chodorow,
1998).
see, ESA techniques yield substantial improvements previous state
art results. Notably, ESA also achieves much better results another recently
introduce method based Wikipedia (Strube & Ponzetto, 2006). provide detailed
comparison approach latter work Section 5.1. Table 3 shows results
computing relatedness entire documents. tables, show statistical
significance difference performance ESA-Wikipedia (March 26, 2006
456

fiWikipedia-based Semantic Interpretation

Algorithm

WordNet-based techniques (Jarmasz, 2003)
Rogets Thesaurus-based technique (Jarmasz, 2003)
LSA (Finkelstein et al., 2002a)
WikiRelate! (Strube & Ponzetto, 2006)
MarkovLink (Hughes & Ramage, 2007)
ESA-Wikipedia (March 26, 2006 version)
ESA-Wikipedia (November 11, 2005 version)
ESA-ODP

Spearmans
correlation
human judgements
0.35
0.55
0.56
0.50
0.55
0.75
0.74
0.65

Stat.
significance
(p-value)
4 1016
1.3 106
3.4 106
8 109
1.6 106


0.0044

Table 2: Spearmans rank correlation word relatedness scores human judgements
WordSimilarity-353 collection
Algorithm

Bag words (Lee et al., 2005)
LSA (Lee et al., 2005)
ESA-Wikipedia (March 26, 2006 version)
ESA-Wikipedia (November 11, 2005 version)
ESA-ODP

Pearsons
correlation
human judgements
0.10.5
0.60
0.72
0.71
0.69

Stat.
significance
(p-value)
4 1019
5 108


0.07

Table 3: Pearsons correlation text relatedness scores human judgements Lee et
al.s document collection

version) algorithms9 using Fishers z-transformation (Press, Teukolsky,
Vetterling, & Flannery, 1997, Section 14.5).
test collections, Wikipedia-based semantic interpretation superior
ODP-based one; word relatedness task, superiority statistically significant
p < 0.005. believe two factors contribute phenomenon. First, axes
multi-dimensional interpretation space ideally independent possible.
hierarchical organization Open Directory reflects generalization relation
concepts obviously violates independence requirement. Second, increase
amount training data building ODP-based semantic interpreter, crawled
URLs listed ODP. allowed us increase amount textual data several
orders magnitude, also brought non-negligible amount noise,
common Web pages. hand, Wikipedia articles virtually noise-free,
9. Whenever range values available, compared ESA-Wikipedia best-performing method
range.

457

fiGabrilovich & Markovitch

mostly qualify Standard Written English. Thus, textual descriptions Wikipedia
concepts arguably focused ODP concepts.
also essential note experiments, using newer Wikipedia snapshot
leads better results (although difference performance two versions
admittedly small).
evaluated effect using second-order interpretation computing semantic
relatedness texts, yielded negligible improvements. hypothesize
reason finding computing semantic relatedness essentially uses available
Wikipedia concepts, second-order interpretation slightly modify weights
existing concepts. next section, describes application ESA text
categorization, trim interpretation vectors sake efficiency, consider
highest-scoring concepts input text fragment. scenario, secondorder interpretation positive effect actually improves accuracy text
categorization (Section 4.5.4). happens selected Wikipedia concepts
used augment text representation, second-order approach selectively adds
highly related concepts identified analyzing Wikipedia links.

4. Using Explicit Semantic Analysis Text Categorization
section evaluate benefits using external knowledge text categorization.10
4.1 Background Text Categorization
Text categorization (TC) deals assigning category labels natural language documents. Categories come fixed set labels (possibly organized hierarchy)
document may assigned one categories. Text categorization systems
useful wide variety tasks, routing news e-mail appropriate corporate
desks, identifying junk email, correctly handling intelligence reports.
majority existing text classification systems represent text bag words,
use variant vector space model various weighting schemes (Salton & McGill,
1983). Thus, features commonly used text classification weighted occurrence
frequencies individual words. State-of-the-art systems text categorization use variety
induction techniques, support vector machines, k-nearest neighbor algorithm,
neural networks. bag words (BOW) method effective easy medium
difficulty categorization tasks category document identified several
easily distinguishable keywords. However, performance becomes quite limited
demanding tasks, dealing small categories short documents.
various attempts extend basic BOW approach. Several studies
augmented bag words n-grams (Caropreso, Matwin, & Sebastiani, 2001; Peng
& Shuurmans, 2003; Mladenic, 1998; Raskutti, Ferra, & Kowalczyk, 2001) statistical
language models (Peng, Schuurmans, & Wang, 2004). Others used linguistically motivated
features based syntactic information, available part-of-speech tagging
shallow parsing (Sable, McKeown, & Church, 2002; Basili, Moschitti, & Pazienza, 2000).
Additional studies researched use word clustering (Baker & McCallum, 1998; Bekker10. Preliminary results research reported Gabrilovich Markovitch (2006).

458

fiWikipedia-based Semantic Interpretation

man, 2003; Dhillon, Mallela, & Kumar, 2003), neural networks (Jo, 2000; Jo & Japkowicz,
2005; Jo, 2006), well dimensionality reduction techniques LSA (Deerwester
et al., 1990; Hull, 1994; Zelikovitz & Hirsh, 2001; Cai & Hofmann, 2003). However,
attempts mostly limited success.
believe bag words approach inherently limited, use
pieces information explicitly mentioned documents,
vocabulary consistently used throughout. BOW approach cannot generalize
words, consequently words testing document never appeared training
set necessarily ignored. synonymous words appear infrequently training
documents used infer general principle covers cases. Furthermore,
considering words unordered bag makes difficult correctly resolve sense
polysemous words, longer processed native context.
shortcomings stem fact bag words method access wealth
world knowledge possessed humans, therefore easily puzzled facts terms
cannot easily deduced training set.
4.2 Using ESA Feature Generation
propose solution augments bag words knowledge-based features.
Given document classified, would like use ESA represent document
text space Wikipedia concepts. However, text categorization crucially different
computing semantic relatedness (cf. Section 3) two important respects.
First, computing semantic relatedness essentially one-off task, is, given
particular pair text fragments, need quantify relatedness prior
examples specific task. cases, words text fragments likely
marginal usefulness, especially two fragments one word long.
happens data available us limited two input fragments,
cases share words, all.
hand, supervised text categorization, one usually given collection
labeled text documents, one induce text categorizer. Consequently,
words occur training examples serve valuable featuresthis
bag words approach born. observed earlier work (Gabrilovich
& Markovitch, 2005, 2007b), ill-advised completely replace bag words
generated concepts, instead advantageous enrich bag words. Rather,
opt augment bag words carefully selected knowledge concepts, become
new features document. refer process feature generation,
actually construct new document features beyond bag words.
Second, enriching document representation text categorization possible
Wikipedia concepts extremely expensive computationally, machine learning
classifier learned augmented feature space. representation obviously
takes lot storage space, cannot processed efficiently multitude
concepts involved (whose number easily reach hundreds thousands). Therefore,
text categorization task, prune interpretation vectors retain number
highest-scoring concepts input text fragment.

459

fiGabrilovich & Markovitch

Using multi-resolution approach feature generation believe considering document single unit often misleading: text might diverse
readily mapped right set concepts, notions mentioned briefly may
overlooked. Instead, partition document series non-overlapping segments
(called contexts), generate features finer level. context mapped
number Wikipedia concepts knowledge base, pooling concepts
together describe entire document results multi-faceted classification. way,
resulting set concepts represents various aspects sub-topics covered
document.
Potential candidates contexts simple sequences words, linguistically motivated chunks sentences paragraphs. optimal resolution document segmentation determined automatically using validation set. earlier work (Gabrilovich & Markovitch, 2005, 2007b), proposed principled multiresolution approach simultaneously partitions document several levels linguistic abstraction (windows words, sentences, paragraphs, taking entire document
one big chunk), performs feature generation levels. rely
subsequent feature selection step eliminate extraneous features, preserving
genuinely characterize document.
essential emphasize using multi-resolution approach makes sense
interpretation vectors pruned retain number highest-scoring concepts context. explained above, exactly case text categorization.
Without pruning, producing interpretation vectors context summing
would equivalent simply multiplying weight concept constant
factor. order explain situation different presence pruning, let us
consider example. Suppose long document mentions particular
topic last paragraph. Since topic central document, N topscoring concepts documents interpretation vector unlikely cover topic.
Although likely covered concepts I, concepts lower weight
going pruned. However, produce interpretation vectors also
paragraph document, retain N highest-scoring concepts each,
concepts generated last paragraph cover . Consequently, representation joined set concepts generated document. many text categorization
tasks, documents labeled particular topic even mention topic briefly,
hence generating features describing topics important.
Feature generation Feature generation performed prior text categorization.
document transformed series local contexts, represented
interpretation vectors using ESA. top ten concepts vectors pooled together,
give rise generated features document, added bag words.
Since concepts approach correspond Wikipedia articles, constructed features also
correspond articles. Thus, set features generated document viewed
representing set Wikipedia articles relevant document contents.
constructed features used conjunction original bag words.
resulting set optionally undergoes feature selection, discriminative features
retained document representation.
460

fiWikipedia-based Semantic Interpretation

Basic
features

Feature
selection

Selected
features

Labeled
documents

Feature
valuation

Induction
algorithm

Classifier

Classifier

Classified
documents

Labeled
feature
vectors

Training

Testing
Testing
documents

Feature
valuation

Figure 2: Standard approach text categorization.

Feature generation
Feature
construction

Feature
selection

Generated
features

Wikipedia
Labeled
documents

Feature
valuation

Induction
algorithm

Classifier

Labeled
feature
vectors

Figure 3: Induction text classifiers using proposed framework feature generation.

Figure 2 depicts standard approach text categorization. Figure 3 outlines
proposed feature generation framework; observe Feature generation box replaces
Feature selection box framed bold Figure 2.
essential note use encyclopedia simply increase amount
training data text categorization; neither use text corpus collect
word co-occurrence statistics. Rather, use knowledge distilled encyclopedia
enrich representation documents, text categorizer induced
augmented, knowledge-rich feature space.
461

fiGabrilovich & Markovitch

4.3 Test Collections
section gives brief description test collections used evaluate methodology. provide much detailed description test collections Appendix B.
1. Reuters-21578 (Reuters, 1997) historically often used dataset text categorization research. Following common practice, used ModApte split (9603 training,
3299 testing documents) two category sets, 10 largest categories 90 categories
least one training testing example.
2. 20 Newsgroups (20NG) (Lang, 1995) well-balanced dataset 20 categories
containing 1000 documents each.
3. Movie Reviews (Movies) (Pang, Lee, & Vaithyanathan, 2002) defines sentiment
classification task, reviews express either positive negative opinion
movies. dataset 1400 documents two categories (positive/negative)
4. Reuters Corpus Volume (RCV1) (Lewis, Yang, Rose, & Li, 2004)
800,000 documents. speed experiments, used subset RCV1 17,808 training documents (dated 2027/08/96) 5,341 testing ones (2831/08/96). Following
Brank, Grobelnik, Milic-Frayling, Mladenic (2002), used 16 Topic 16 Industry
categories constitute representative samples full groups 103 354 categories,
respectively. also randomly sampled Topic Industry categories 5 sets
10 categories each.11
5. OHSUMED (Hersh, Buckley, Leone, & Hickam, 1994) subset MEDLINE,
contains 348,566 medical documents. document contains title, two-thirds
(233,445) also contain abstract. document labeled average 13 MeSH12
categories (out total 14,000). Following Joachims (1998), used subset documents
1991 abstracts, taking first 10,000 documents training next
10,000 testing. limit number categories experiments, randomly
generated 5 sets 10 categories each.13
Using 5 datasets allows us comprehensively evaluate performance
approach. Specifically, comparing 20 Newsgroups two Reuters datasets (Reuters21578 Reuters Corpus Volume 1), observe former substantially
noisy since data obtained Usenet newsgroups, Reuters datasets
significantly cleaner. Movie Reviews collection presents example sentiment
classification, different standard (topical) text categorization. Finally,
OHSUMED dataset presents example comprehensive taxonomy 14,000
categories. explain next section, also used dataset create collection
labeled short texts, allowed us quantify performance method
texts.
Short Documents also derived several datasets short documents test
collections described above. Recall one-third OHSUMED documents
titles abstract, therefore considered short documents as-is. used
range documents defined above, considered without abstracts;
yielded 4,714 training 5,404 testing documents. datasets, created
11. full definition category sets used available Table 8 (see Section B.4).
12. http://www.nlm.nih.gov/mesh
13. full definition category sets used available Table 9 (see Section B.5).

462

fiWikipedia-based Semantic Interpretation

short document original document taking title latter (with
exception Movie Reviews, documents titles).
noted, however, substituting title full document poor
mans way obtain collection classified short documents. documents first
labeled categories, human labeller saw document entirety. particular,
category might assigned document basis facts mentioned
body, even though information may well missing (short) title. Thus, taking
categories original documents genuine categories title often
misleading. However, know publicly available test collections short
documents, decided construct datasets explained above. Importantly, OHSUMED
documents without abstracts classified humans; working
OHSUMED-derived dataset thus considered pure experiment.
4.4 Experimentation Procedure
used support vector machines14 learning algorithm build text categorizers, since
prior studies found SVMs best performance text categorization (Sebastiani,
2002; Dumais, Platt, Heckerman, & Sahami, 1998; Yang & Liu, 1999). Following established
practice, use precision-recall break-even point (BEP) measure text categorization
performance. BEP defined terms standard measures precision recall,
precision proportion true document-category assignments among assignments predicted classifier, recall proportion true document-category
assignments also predicted classifier. obtained either tuning
classifier precision equal recall, sampling several (precision, recall) points
bracket expected BEP value interpolating (or extrapolating, event
sampled points lie side).
two Reuters datasets OHSUMED report micro- macro-averaged
BEP, since categories differ size significantly. Micro-averaged BEP operates
document level primarily affected categorization performance larger categories.
hand, macro-averaged BEP averages results individual categories, thus
small categories training examples large impact overall performance.
Reuters datasets (Reuters-21578 RCV1) OHSUMED used fixed
train/test split defined Section 4.3, consequently used macro sign test (S-test)
(Yang & Liu, 1999) assess statistical significance differences classifier performance. 20NG Movies performed 4-fold cross-validation, used paired t-test
assess significance. also used non-parametric Wilcoxon signed-ranks test (Demsar, 2006) compare baseline FG-based classifiers multiple data sets.
latter case, individual measurements taken (micro- macro-averaged) BEP
values observed dataset.
14. used SVM light implementation (Joachims, 1999) default parameters. earlier
work feature selection (Gabrilovich & Markovitch, 2004), conducted thorough experimentation
wide range values C parameter, found major importance
datasets; consequently, leave parameter default setting well.

463

fiGabrilovich & Markovitch

4.4.1 Text Categorization Infrastructure
conducted experiments using text categorization platform design
development named Hogwarts 15 (Davidov, Gabrilovich, & Markovitch, 2004). opted
build comprehensive new infrastructure text categorization, surprisingly software tools publicly available researchers, available allow
limited control operation. Hogwarts facilitates full-cycle text categorization
including text preprocessing, feature extraction, construction, selection weighting, followed actual classification cross-validation experiments. system currently
provides XML parsing, part-of-speech tagging (Brill, 1995), sentence boundary detection,
stemming (Porter, 1980), WordNet (Fellbaum, 1998) lookup, variety feature selection
algorithms, TFIDF feature weighting schemes. Hogwarts 250 configurable
parameters control modus operandi minute detail. Hogwarts interfaces
SVM, KNN C4.5 text categorization algorithms, computes standard measures
categorization performance. Hogwarts designed particular emphasis
processing efficiency, portably implemented ANSI C++ programming language
C++ Standard Template Library. system built-in loaders Reuters-21578
(Reuters, 1997), RCV1 (Lewis et al., 2004), 20 Newsgroups (Lang, 1995), Movie Reviews
(Pang et al., 2002), OHSUMED (Hersh et al., 1994), additional datasets
easily integrated modular way.
document undergoes following processing steps. Document text first tokenized, title words replicated twice emphasize importance. Then, stop
words, numbers mixed alphanumeric strings removed, remaining words
stemmed. bag words next merged set features generated
document analyzing contexts explained Section 4.2, rare features occurring
fewer 3 documents removed.
Since earlier studies found BOW features indeed useful SVM text
categorization16 (Joachims, 1998; Rogati & Yang, 2002; Brank et al., 2002; Bekkerman,
2003; Leopold & Kindermann, 2002; Lewis et al., 2004), take bag words
entirety (with exception rare features removed previous step). generated
features, however, undergo feature selection using information gain criterion.17 Finally,
feature weighting performed using ltc TF.IDF function (logarithmic term frequency
inverse document frequency, followed cosine normalization) (Salton & Buckley, 1988;
Debole & Sebastiani, 2003).
4.4.2 Baseline Performance Hogwarts
demonstrate performance basic text categorization implementation (column Baseline Table 4) consistent state art reflected
published studies (all using SVM). Reuters-21578, Dumais et al. (1998) achieved
15. Hogwarts School Witchcraft Wizardry educational institution attended Harry Potter
(Rowling, 1997).
16. Gabrilovich Markovitch (2004) described class problems feature selection bag
words actually improves SVM performance.
17. course, feature selection performed using training set documents.

464

fiWikipedia-based Semantic Interpretation

micro-BEP 0.920 10 categories 0.870 categories. 20NG18 , Bekkerman
(2003) obtained BEP 0.856. Pang et al. (2002) obtained accuracy 0.829 Movies19 .
minor variations performance due differences data preprocessing
different systems; example, Movies dataset worked raw HTML files
rather official tokenized version, order recover sentence paragraph
structure contextual analysis. RCV1 OHSUMED, direct comparison published results difficult limited category sets date span
documents speed experimentation.
4.4.3 Using Feature Generator
core engine Explicit Semantic Analysis implemented explained Section 3.2.
used multi-resolution approach feature generation, classifying document contexts level individual words, complete sentences, paragraphs, finally entire
document.20 context, features generated 10 best-matching concepts
produced feature generator.
4.5 Wikipedia-based Feature Generation
section, report results experimental evaluation methodology.
4.5.1 Qualitative Analysis Feature Generation
study process feature generation number actual examples.
Feature Generation per se illustrate approach, show features generated
several text fragments. Whenever applicable, provide short explanations generated
concepts; cases, explanations taken Wikipedia (Wikipedia, 2006).
Text: Wal-Mart supply chain goes real time
Top 10 generated features: (1) Wal-Mart; (2) Sam Walton; (3) Sears Holdings
Corporation; (4) Target Corporation; (5) Albertsons; (6) ASDA; (7) RFID; (8)
Hypermarket; (9) United Food Commercial Workers; (10) Chain store
Selected explanations: (2) Wal-Mart founder; (5) prominent competitors WalMart; (6) Wal-Mart subsidiary UK; (7) Radio Frequency Identification,
technology Wal-Mart uses extensively manage stock; (8) superstore
(a general concept, Wal-Mart specific example); (9) labor union
18. comparison results reported Bekkerman (2003) administered single test run (i.e.,
without cross-validation), taking first 3/4 postings newsgroup training, rest
testing.
19. comparison results reported Pang et al. (2002) administered single test run (i.e.,
without cross-validation), taking first 2/3 data opinion type training, rest
testing.
20. 20NG dataset exception, owing high level intrinsic noise renders identification
sentence boundaries extremely unreliable, causes word-level feature generation produce
many spurious classifications. Consequently, dataset restrict multi-resolution approach
individual paragraphs entire document only.

465

fiGabrilovich & Markovitch

trying organize Wal-Marts workers; (10) general concept,
Wal-Mart specific example
particularly interesting juxtapose features generated fragments
contain ambiguous words. end, show features generated two phrases
contain word bank two different senses, Bank America (financial
institution) Bank Amazon (river bank). readily seen, feature generation methodology capable performing word sense disambiguation
considering ambiguous words context neighbors.
Text: Bank America
Top 10 generated features: (1) Bank; (2) Bank America; (3) Bank
America Plaza (Atlanta); (4) Bank America Plaza (Dallas); (5) MBNA
(a bank holding company acquired Bank America); (6) VISA (credit
card); (7) Bank America Tower, New York City; (8) NASDAQ; (9) MasterCard; (10) Bank America corporate Center
Text: Bank Amazon
Top 10 generated features: (1) Amazon River; (2) Amazon Basin; (3) Amazon Rainforest; (4) Amazon.com; (5) Rainforest; (6) Atlantic Ocean; (7)
Brazil; (8) Loreto Region (a region Peru, located Amazon Rainforest);
(9) River; (10) Economy Brazil
method, however, 100% accurate, cases generates features
somewhat relevant even irrelevant input text. example, show outcome feature generation title earlier article
(Gabrilovich & Markovitch, 2006). concept, show list input words
triggered (the words stemmed sorted decreasing order
contribution).
Text: Overcoming Brittleness Bottleneck using Wikipedia: Enhancing Text Categorization Encyclopedic Knowledge
Top 10 generated features:
1. Encyclopedia (encyclopedia, knowledge, Wikipedia, text)
2. Wikipedia (Wikipedia, enhance, encyclopedia, text)
3. Enterprise content management (category, knowledge, text, overcome, enhance)
4. Performance problem (bottleneck, category, enhance)
5. Immanuel Kant (category, knowledge, overcome)
6. Tooth enamel (brittleness, text, enhance)
7. Lucid dreaming (enhance, text, knowledge, category)
8. Bottleneck (bottleneck)
9. Java programming language (category, bottleneck, enhance)
466

fiWikipedia-based Semantic Interpretation

10. Transmission Control Protocol (category, enhance, overcome)
generated features clearly relevant input, Encyclopedia,
Wikipedia, Enterprise content management. Others, however, spurious,
Tooth enamel Transmission Control Protocol. Since process
feature generation relies bag words matching concepts input text,
suffers BOW shortcomings mentioned (Section 4.1). Consequently,
features generated corresponding Wikipedia articles happen
share words input text, even though words characteristic
article whole. explained above, method successfully operate
presence extraneous features due use feature selection.
way, generated features informative predicting document categories
filtered out, informative features actually retained learning
classification model.
Using Inter-article Links Generating Additional Features Section 1,
presented algorithm generates additional features using inter-article links relations concepts. follows, show series text fragments,
fragment show (a) features generated regular FG algorithm, (b) features
generated using Wikipedia links, (c) general features generated using links.
see examples, features constructed using links often relevant
input text.
Text: Google search
Regular feature generation: (1) Search engine; (2) Google Video; (3) Google;
(4) Google (search); (5) Google Maps; (6) Google Desktop; (7) Google (verb);
(8) Google News; (9) Search engine optimization; (10) Spamdexing (search engine
spamming)
Features generated using links: (1) PageRank; (2) AdWords; (3) AdSense; (4)
Gmail; (5) Google Platform; (6) Website; (7) Sergey Brin; (8) Google bomb; (9)
MSN Search; (10) Nigritude ultramarine (a meaningless phrase used search
engine optimization contest 2004)
general features only: (1) Website; (2) Mozilla Firefox; (3) Portable
Document Format; (4) Algorithm; (5) World Wide Web
Text: programming tools
Regular feature generation: (1) Tool; (2) Programming tool; (3) Computer
software; (4) Integrated development environment; (5) Computer-aided software engineering; (6) Macromedia Flash; (7) Borland; (8) Game programmer;
(9) C programming language; (10) Performance analysis
Features generated using links: (1) Compiler; (2) Debugger; (3) Source code;
(4) Software engineering; (5) Microsoft; (6) Revision control; (7) Scripting
language; (8) GNU; (9) Make; (10) Linux
general features only: (1) Microsoft; (2) Software engineering; (3)
Linux; (4) Compiler; (5) GNU
467

fiGabrilovich & Markovitch

4.5.2 Effect Feature Generation
Table 4 shows results using Wikipedia-based feature generation, significant
improvements (p < 0.05) shown bold. different rows table correspond
performance different datasets subsets, defined Section 4.3.
consistently observed larger improvements macro-averaged BEP, dominated
categorization effectiveness small categories. goes line expectations
contribution encyclopedic knowledge especially prominent categories training examples. Categorization performance improved virtually
datasets, notable improvements 30.4% RCV1 18% OHSUMED.
Using Wilcoxon test, found Wikipedia-based classifier significantly superior baseline p < 105 micro- macro-averaged cases. results
clearly demonstrate advantage knowledge-based feature generation.
prior work (Gabrilovich & Markovitch, 2005, 2007b), also performed
feature generation text categorization using alternative source knowledge, namely,
Open Directory Project (ODP). results using Wikipedia competitive
using ODP, slight advantage Wikipedia. Observe also Wikipedia
constantly updated numerous volunteers around globe, ODP virtually
frozen nowadays. Hence, future expect obtain improvements
using newer versions Wikipedia.
Effect Knowledge Breadth also examined effect performing feature
generation using newer Wikipedia snapshot, explained Section 3.2.2. Appendix
reports results experiment, show small consistent improvement due
using larger knowledge base.
4.5.3 Classifying Short Documents
conjectured Wikipedia-based feature generation particularly useful
classifying short documents.
Table 5 presents results evaluation datasets defined Section 4.3.
majority cases, feature generation yielded greater improvement short documents regular documents. Notably, improvements particularly high
OHSUMED, pure experimentation short documents possible (see Section 4.3).
According Wilcoxon test, Wikipedia-based classifier significantly superior
baseline p < 2 106 . findings confirm hypothesis encyclopedic knowledge particularly useful categorizing short documents,
inadequately represented standard bag words.
4.5.4 Using Inter-article links Concept Relations
Using inter-article links generating additional features, observed improvements text categorization performance short documents. see Table 6,
absolute majority cases using links generate general features
superior strategy. explain Section 2.3, inter-article links viewed relations
concepts represented articles. Consequently, using links allows us
468

fiWikipedia-based Semantic Interpretation

Dataset

Baseline
micro macro
BEP BEP
Reuters-21578 (10 cat.) 0.925 0.874
Reuters-21578 (90 cat.) 0.877 0.602
RCV1 Industry-16
0.642 0.595
RCV1 Industry-10A
0.421 0.335
RCV1 Industry-10B
0.489 0.528
RCV1 Industry-10C
0.443 0.414
RCV1 Industry-10D
0.587 0.466
RCV1 Industry-10E
0.648 0.605
RCV1 Topic-16
0.836 0.591
RCV1 Topic-10A
0.796 0.587
RCV1 Topic-10B
0.716 0.618
RCV1 Topic-10C
0.687 0.604
RCV1 Topic-10D
0.829 0.673
RCV1 Topic-10E
0.758 0.742
OHSUMED-10A
0.518 0.417
OHSUMED-10B
0.656 0.500
OHSUMED-10C
0.539 0.505
OHSUMED-10D
0.683 0.515
OHSUMED-10E
0.442 0.542
20NG
0.854
Movies
0.813

Wikipedia
micro macro
BEP BEP
0.932 0.887
0.883 0.603
0.645 0.617
0.448 0.437
0.523 0.566
0.468 0.431
0.595 0.459
0.641 0.612
0.843 0.661
0.798 0.682
0.723 0.656
0.699 0.618
0.839 0.688
0.765 0.755
0.538 0.492
0.667 0.534
0.545 0.522
0.692 0.546
0.462 0.575
0.862
0.842

Improvement
micro macro
BEP
BEP
+0.8% +1.5%
+0.7% +0.2%
+0.5% +3.7%
+6.4% +30.4%
+7.0% +7.2%
+5.6% +4.1%
+1.4% -1.5%
-1.1% +1.2%
+0.8% +11.8%
+0.3% +16.2%
+1.0% +6.1%
+1.7% +2.3%
+1.2% +2.2%
+0.9% +1.8%
+3.9% +18.0%
+1.7% +6.8%
+1.1% +3.4%
+1.3% +6.0%
+4.5% +6.1%
+1.0%
+3.6%

Table 4: effect feature generation long documents

469

fiGabrilovich & Markovitch

Dataset

Baseline
micro macro
BEP BEP
Reuters-21578 (10 cat.) 0.868 0.774
Reuters-21578 (90 cat.) 0.793 0.479
RCV1 Industry-16
0.454 0.400
RCV1 Industry-10A
0.249 0.199
RCV1 Industry-10B
0.273 0.292
RCV1 Industry-10C
0.209 0.199
RCV1 Industry-10D
0.408 0.361
RCV1 Industry-10E
0.450 0.410
RCV1 Topic-16
0.763 0.529
RCV1 Topic-10A
0.718 0.507
RCV1 Topic-10B
0.647 0.560
RCV1 Topic-10C
0.551 0.471
RCV1 Topic-10D
0.729 0.535
RCV1 Topic-10E
0.643 0.636
OHSUMED-10A
0.302 0.221
OHSUMED-10B
0.306 0.187
OHSUMED-10C
0.441 0.296
OHSUMED-10D
0.441 0.356
OHSUMED-10E
0.164 0.206
20NG
0.699

Wikipedia
micro macro
BEP BEP
0.877 0.793
0.803 0.506
0.481 0.437
0.293 0.256
0.337 0.363
0.294 0.327
0.452 0.379
0.474 0.434
0.769 0.542
0.725 0.544
0.643 0.564
0.573 0.507
0.735 0.563
0.670 0.653
0.405 0.299
0.383 0.256
0.528 0.413
0.460 0.402
0.219 0.280
0.749

Improvement
micro
macro
BEP
BEP
+1.0%
+2.5%
+1.3% +5.6%
+5.9% +9.2%
+17.7% +28.6%
+23.4% +24.3%
+40.7% +64.3%
+10.8% +5.0%
+5.3% +5.9%
+0.8%
+2.5%
+1.0% +7.3%
-0.6%
+0.7%
+4.0% +7.6%
+0.8% +5.2%
+4.2% +2.7%
+34.1% +35.3%
+25.2% +36.9%
+19.7% +39.5%
+4.3% +12.9%
+33.5% +35.9%
+7.1%

Table 5: Feature generation short documents

470

fiWikipedia-based Semantic Interpretation

Dataset

Baseline

micro
BEP
Reuters-21578 (10 cat.) 0.868
Reuters-21578 (90 cat.) 0.793
RCV1 Industry-16
0.454
RCV1 Topic-16
0.763
20NG
0.699
Dataset
Reuters-21578 (10 cat.)
Reuters-21578 (90 cat.)
RCV1 Industry-16
RCV1 Topic-16
20NG







macro
BEP
0.774
0.479
0.400
0.529






Wikipedia

Wikipedia
+ links

micro macro
BEP BEP
0.877 0.793
0.803 0.506
0.481 0.437
0.769 0.542
0.749
Improvement
baseline
+1.0% +2.5%
+1.3% +5.6%
+5.9% +9.2%
+0.8% +2.5%
+7.1%

micro macro
BEP BEP
0.878 0.796
0.804 0.506
0.486 0.445
0.769 0.539
0.753
Improvement
baseline
+1.2% +2.8%
+1.4% +5.6%
+7.1% +11.3%
+0.8% +1.9%
+7.7%

Wikipedia
+ links
(more general
features only)
micro macro
BEP
BEP
0.880 0.801
0.809 0.507
0.488 0.444
0.775 0.545
0.756
Improvement
baseline
+1.4% +3.5%
+2.0% +5.8%
+7.5% +11.0%
+1.6% +3.0%
+8.1%

Table 6: Feature generation short documents using inter-article links
identify additional concepts related context analyzed, leads better
representation context additional relevant generated features.

5. Related Work
section puts methodology context related prior work.
past, number attempts represent meaning natural
language texts. Early research computational linguistics focused deep natural language
understanding, strived represent text semantics using logical formulae (Montague,
1973). However, task proved difficult little progress made
develop comprehensive grammars non-trivial fragments language. Consequently,
mainstream research effectively switched statistically-based methods (Manning
& Schuetze, 2000).
Although studies tried explicitly define semantic representation,
modus operandi frequently induces particular representation system. Distributional similarity methods (Lee, 1999) compute similarity pair words w1 w2 comparing
distributions words given two, e.g., comparing vectors probabilities P (v|w1 ) P (v|w2 ) large vocabulary V words (v V ). Therefore,
techniques seen representing meaning word w vector conditional
probabilities words given w. Dagan, Marcus, Markovitch (1995) refined
technique considering co-occurrence probabilities word left right contextual neighbors. example, word water would represented vector
left neighbors drink, pour, clean, vector right neighbors
molecule, level, surface. Lin (1998a) represented word meaning considering syntactic roles words co-occur sentence. example,
471

fiGabrilovich & Markovitch

semantics word water would represented vector triples (water,
obj-of, drink) (water, adj-mod, clean). Qiu Frei (1993) proposed method
concept-based query expansion; however, expanded queries additional words
rather features corresponding semantic concepts.
Latent Semantic Analysis probably similar method prior research,
explicitly represents meaning text fragment. LSA manipulating
vector so-called latent concepts, obtained SVD decomposition
word-by-document matrix training corpus. CYC (Lenat, 1995; Lenat, Guha, Pittman,
Pratt, & Shepherd, 1990) represents semantics words elaborate network
interconnected richly-annotated concepts.
contrast, method represents meaning piece text weighted vector
knowledge concepts. Importantly, entries vector correspond unambiguous
human-defined concepts rather plain words, often ambiguous. Compared
LSA, approach benefits large amounts manually encoded human knowledge,
opposed defining concepts using statistical analysis training corpus. Compared
CYC, approach streamlines process semantic interpretation depend
manual encoding inference rules. exception LSA, prior approaches
semantic interpretation explicitly represent semantics individual words, require
extra level sophistication represent longer texts. Conversely, approach represents
meaning texts uniform way regardless length.
5.1 Semantic Similarity Semantic Relatedness
study deal semantic relatedness rather semantic similarity
semantic distance, also often used literature. extensive survey
relatedness measures, Budanitsky Hirst (2006) argued notion relatedness
general similarity, former subsumes many different kind specific
relations, including meronymy, antonymy, functional association, others.
maintained computational linguistics applications often require measures relatedness
rather narrowly defined measures similarity. example, word sense
disambiguation use related words context, merely similar words.
Budanitsky Hirst (2006) also argued notion semantic distance might
confusing due different ways used literature.
approach estimating semantic relatedness words somewhat reminiscent
distributional (or co-occurrence) similarity (Lee, 1999; Dagan, Lee, & Pereira, 1999). Indeed, compare meanings words comparing occurrence patterns across
large collection natural language documents. However, compilation documents arbitrary, rather, documents aligned encyclopedia articles,
focused single topic. Furthermore, distributional similarity methods
inherently suitable comparing individual words, method compute
similarity arbitrarily long texts.
Prior work field mostly focused semantic similarity words, using R&G
(Rubenstein & Goodenough, 1965) list 65 word pairs M&C (Miller & Charles, 1991)
list 30 word pairs. similarity relation considered, using lexical resources
often successful enough, reaching Pearsons correlation 0.700.85 human
472

fiWikipedia-based Semantic Interpretation

judgements (Budanitsky & Hirst, 2006; Jarmasz, 2003). case, lexical techniques
even slight edge ESA-Wikipedia, whose correlation human scores 0.723
M&C 0.816 R&G. 21 However, entire language wealth considered
attempt capture general semantic relatedness, lexical techniques yield substantially inferior results (see Table 2). WordNet-based technique, consider
generalization (is-a) relation words, achieve correlation 0.330.35
human judgements (Budanitsky & Hirst, 2006; Jarmasz, 2003). Jarmasz & Szpakowiczs
ELKB system (Jarmasz, 2003) based Rogets Thesaurus (Roget, 1852) achieves higher
correlation 0.55 due use richer set relations.
Studying semantic similarity relatedness words related assessing similarity
relations. example task establish word pairs carpenter:wood
mason:stone relationally similar, words pairs stand relation
(profession:material). State art results relational similarity based Latent
Relational Analysis (Turney, 2006, 2005).
Sahami Heilman (2006) proposed use Web source additional knowledge
measuring similarity short text snippets. end, defined kernel function
sends two snippets queries search engine, compares bags words
two sets returned documents. major limitation technique
applicable short texts, sending long text query search engine likely
return even results all. hand, approach applicable
text fragments arbitrary length. Additional studies explored Web gather
information computing word similarity include (Turney, 2001) (Metzler, Dumais, &
Meek, 2007). main difference works method latter
uses structured representation human knowledge defined Wikipedia concepts.
above-mentioned based techniques inherently limited individual words,
adaptation comparing longer texts requires extra level complexity (Mihalcea
et al., 2006). contrast, method treats words texts essentially
way.
Strube Ponzetto (2006) also used Wikipedia computing semantic relatedness.
However, method, called WikiRelate!, radically different ours. Given pair
words w1 w2 , WikiRelate! searches Wikipedia articles, p1 p2 , respectively
contain w1 w2 titles. Semantic relatedness computed based various
distance measures p1 p2 . measures either rely texts
pages, path distances within category hierarchy Wikipedia. approach,
hand, represents word weighted vector Wikipedia concepts. Semantic
relatedness computed comparing two concept vectors.
Thus, differences two approaches are:
1. WikiRelate! process words actually occur titles Wikipedia articles.
ESA requires word appears within text Wikipedia articles.
2. WikiRelate! limited single words ESA compare texts length.
21. WikiRelate! (Strube & Ponzetto, 2006) achieved relatively low scores 0.310.54 domains.

473

fiGabrilovich & Markovitch

3. WikiRelate! represents semantics word either text article
associated it, node category hierarchy. ESA much
structured semantic representation consisting vector Wikipedia concepts.
Indeed, shown Section 3.3, richer representation ESA yields much better
results.
5.2 Feature Generation Text Categorization
date, quite attempts made deviate orthodox bag words
paradigm, usually limited success. particular, representations based phrases
(Lewis, 1992; Dumais et al., 1998; Fuernkranz, Mitchell, & Riloff, 1998), named entities
(Kumaran & Allan, 2004), term clustering (Lewis & Croft, 1990; Bekkerman, 2003)
explored. However, none techniques could possibly overcome problem
underlying various examples reviewed paperlack world knowledge.
Feature generation techniques found useful variety machine learning tasks
(Markovitch & Rosenstein, 2002; Fawcett, 1993; Matheus, 1991). techniques search
new features describe target concept better ones supplied
training instances. number proposed feature generation algorithms (Pagallo & Haussler, 1990; Matheus & Rendell, 1989; Hu & Kibler, 1996; Murphy & Pazzani, 1991; Hirsh
& Japkowicz, 1994) led significant improvements performance range classification tasks. However, even though feature generation established research area
machine learning, works applied text processing (Kudenko & Hirsh,
1998; Mikheev, 1998; Cohen, 2000; Scott, 1998; Scott & Matwin, 1999). contrast
approach, techniques use exogenous knowledge.
prior work (Gabrilovich & Markovitch, 2005, 2007b), assumed external
knowledge available form generalization hierarchy, used Open Directory
Project example. method, however, number drawbacks,
corrected using Wikipedia.
First, requiring knowledge repository define is-a hierarchy limits choice
appropriate repositories. Moreover, hierarchical organization embodies one particular
relation nodes (generalization), numerous relations, relatedness, meronymy/holonymy chronology, ignored. Second, large-scale hierarchies
tend extremely unbalanced, relative size branches disproportionately large small due peculiar views editors. phenomena indeed
common ODP. example, Top/Society branch heavily dominated one
childrenReligion Spirituality; Top/Science branch dominated
Biology child; considerable fraction mass Top/Recreation concentrated
Pets. Finally, learn scope every ODP concept, short textual descriptions
concepts augmented crawling Web sites cataloged ODP. procedure
allowed us accumulate many gigabytes worth textual data, price, texts
obtained Web often quite far formal writing plagued noise.
Crawling typical Web site often brings auxiliary material little
site theme, legal disclaimers, privacy statements, help pages.
paper proposed use world knowledge encoded Wikipedia, arguably largest knowledge repository Web. Compared ODP, Wikipedia
474

fiWikipedia-based Semantic Interpretation

possesses several advantageous properties. First, articles much cleaner typical
Web pages, mostly qualify standard written English. Although Wikipedia offers
several orthogonal browsing interfaces, structure fairly shallow, propose
treat Wikipedia essentially hierarchy. way, mapping tex fragments onto
relevant Wikipedia concepts yields truly multi-faceted classification text, avoids
problem unbalanced hierarchy branches. Moreover, requiring knowledge
repository hierarchically organized, approach suitable new domains,
ontology available. Finally, Wikipedia articles heavily cross-linked, way
reminiscent linking Web. conjectured links encode many interesting relations concepts, constitute important source information
addition article texts. explored using inter-article links Section 4.5.4.
5.2.1 Feature Generation Using Electronic Dictionaries
Several studies performed feature construction using WordNet electronic dictionary
(Fellbaum, 1998) domain-specific dictionaries (Scott, 1998; Scott & Matwin,
1999; Urena-Lopez, Buenaga, & Gomez, 2001; Wang, McKay, Abbass, & Barlow, 2003;
Bloehdorn & Hotho, 2004).
Scott Matwin (1999) attempted augment conventional bag-of-words representation additional features, using symbolic classification system Ripper (Cohen,
1995). study evaluated features based syntactically22 statistically motivated
phrases, well WordNet synsets 23 . latter case, system performed generalizations using hypernym hierarchy WordNet, completely replaced bag words
bag synsets. using hypernyms allowed Ripper produce general
comprehensible rules achieved performance gains small classification tasks, performance benefits could obtained larger tasks, even suffered
degradation classification accuracy. Consistent published findings
(Lewis, 1992; Dumais et al., 1998; Fuernkranz et al., 1998), phrase-based representation
also yield significant performance benefits bag-of-words approach.24
Urena-Lopez et al. (2001) used WordNet conjunction Rocchio (Rocchio, 1971)
Widrow-Hoff (Lewis, Schapire, Callan, & Papka, 1996; Widrow & Stearns, 1985, Chapter 6) linear classifiers fine-tune category vectors. Wang et al. (2003) used Medical
Subject Headings (MeSH, 2003) replace bag words canonical medical terms;
Bloehdorn Hotho (2004) used similar approach augment Reuters-21578 documents
WordNet synsets OHSUMED medical documents MeSH terms.
noted, however, WordNet originally designed powerful
knowledge base, rather lexical database suitable peculiar lexicographers
needs. Specifically, WordNet following drawbacks used knowledge base
text categorization:
22. Identification syntactic phrases performed using noun phrase extractor built top part
speech tagger (Brill, 1995).
23. synset WordNet notion sense shared group synonymous words.
24. Sebastiani (2002) casts use bag words versus phrases utilizing lexical semantics rather
compositional semantics. Interestingly, bag-of-words approaches (notably, KNN) may considered
context-sensitive assume independence either features (terms) categories (Yang
& Pedersen, 1997).

475

fiGabrilovich & Markovitch

WordNet fairly small coveragefor test collections used paper,
50% unique words missing WordNet. particular, many proper
names, slang domain-specific technical terms included WordNet,
designed general-purpose dictionary.
Additional information synsets (beyond identity) limited.
WordNet implements differential rather constructive lexical semantics
theory, glosses accompany synsets mainly designed distinguish
synsets rather provide definition sense concept. Usage examples
occasionally constitute part gloss serve purpose. Without
auxiliary information, reliable word sense disambiguation almost impossible.
WordNet designed professional linguists trained recognize minute
differences word senses. result, common words far many distinct
senses useful information retrieval (Mihalcea, 2003); example, word
make many 48 senses verb alone. fine-grained distinctions
synsets present additional difficulty word sense disambiguation.
approach techniques use WordNet manipulate collection
concepts. However, number crucial differences. previous studies
performed feature generation individual words only. approach handle arbitrarily long short text fragments alike. Considering words context allows approach
perform word sense disambiguation. Approaches using WordNet cannot achieve disambiguation information synsets limited merely words,
Wikipedia concepts associated huge amounts text. Even individual words,
approach provides much sophisticated mapping words concepts,
analysis large bodies texts associated concepts. allows us represent
meaning words (or texts) weighted combination concepts, mapping word
WordNet amounts simple lookup, without weights. Furthermore, WordNet
senses word mutually exclusive. approach, concepts reflect different
aspects input, thus yielding weighted multi-faceted representation text.
Appendix illustrate limitations WordNet specific example,
juxtapose WordNet-based Wikipedia-based representation.
5.2.2 Using Unlabeled Examples
best knowledge, exception studies used WordNet,
attempts date automatically use large-scale repositories structured background knowledge feature generation. interesting approach using nonstructured background knowledge proposed Zelikovitz Hirsh (2000). work
uses collection unlabeled examples intermediaries comparing testing examples
training ones. Specifically, unknown test instance appear
resemble labeled training instances, unlabeled examples similar may
used bridges. Using approach, possible handle situation
training test document words common. unlabeled documents
utilized define cosine similarity metric, used KNN algorithm
actual text categorization. approach, however, suffers efficiency problems,
476

fiWikipedia-based Semantic Interpretation

looking intermediaries compare every two documents makes necessary explore
combinatorial search space.
subsequent paper, Zelikovitz Hirsh (2001) proposed alternative way use
unlabeled documents background knowledge. work, unlabeled texts pooled
together training documents compute Latent Semantic Analysis (LSA) (Deerwester et al., 1990) model. LSA analyzes large corpus unlabeled text, automatically
identifies so-called latent concepts using Singular Value Decomposition. resulting
LSA metric facilitates comparison test documents training documents. addition unlabeled documents significantly increases amount data word
co-occurrence statistics estimated, thus providing solution text categorization problems training data particularly scarce. However, subsequent studies found
LSA rarely improve strong baseline established SVM, often even results
performance degradation (Wu & Gunopulos, 2002; Liu, Chen, Zhang, Ma, & Wu, 2004).
contrast LSA, manipulates virtual concepts, methodology relies using
concepts identified described humans.

6. Conclusions
paper proposed Explicit Semantic Analysisa semantic interpretation methodology natural language processing. order render computers knowledge
world, use Wikipedia build semantic interpreter, represents meaning
texts high-dimensional space knowledge-based concepts. concepts correspond Wikipedia articles, methodology provides fully automatic way tap
collective knowledge tens hundreds thousands people. conceptbased representation text contains information cannot deduced input
text alone, consequently supersedes conventional bag words representation.
believe important aspects proposed approach ability
address synonymy polysemy, arguably two important problems
NLP. Thus, two texts discuss topic using different words,
conventional bag words approach able identify commonality.
hand, mere fact two texts contain word necessarily
imply discuss topic, since word could used two texts two
different meanings. believe concept-based representation allows generalizations
refinements partially address synonymy polysemy.
Consider, example, following text fragment (taken Appendix C): group
European-led astronomers made photograph appears planet orbiting
another star. so, would first confirmed picture world beyond solar
system. fifth concept generated fragment Extrasolar planet,
exactly topic text, even though words mentioned input.
generated concepts (e.g., Astronomy Planetary orbit) also highly
characteristic astronomy-related texts. additions enrich text representation,
increase chances finding common features texts. also essential note
that, course, generated concepts need match features documents.
Even concepts match, gain valuable insights document contents.
477

fiGabrilovich & Markovitch

succeeded make automatic use encyclopedia without deep language understanding, specially crafted inference rules relying additional common-sense knowledge
bases. made possible applying standard text classification techniques match
document texts relevant Wikipedia articles.
Empirical evaluation confirmed value Explicit Semantic Analysis two common tasks natural language processing. Compared previous state art,
using ESA results significant improvements automatically assessing semantic relatedness words texts. Specifically, correlation computed relatedness scores
human judgements increased r = 0.56 0.75 (Spearman) individual words
r = 0.60 0.72 (Pearson) texts. contrast existing methods, ESA offers
uniform way computing relatedness individual words arbitrarily long text
fragments. Using ESA perform feature generation text categorization yielded consistent improvements across diverse range datasets. Recently, performance
best text categorization systems became similar, previous work mostly achieved small
improvements. Using Wikipedia source external knowledge allowed us improve
performance text categorization across diverse collection datasets.
noted although recent study (Giles, 2005) found Wikipedia accuracy rival Encyclopaedia Britannica, arguably Wikipedia articles
equally high quality. one hand, Wikipedia notion featured articles
(http://en.wikipedia.org/wiki/Featured Article), considered
best articles Wikipedia, determined Wikipedias editors. Currently, fewer
0.1% articles achieve status. hand, many articles incomplete (socalled stubs), might even contain information incorrect represent
consensus among editors. Yet cases, Wikipedia content might prone
spamming, despite editorial process attempts review recent changes. believe
method overly susceptible cases, long majority content
correct. Arguably, except outright vandalism, spamming would likely modify
articles contain information related topic article, important
essential majority readers. long newly added content remains
relevant gist article, method likely able correctly determine
input texts article relevant for. However, proper evaluation robustness
method presence imperfect content beyond scope article.
believe research constitutes step towards enriching natural language
processing humans knowledge world. hope Explicit Semantic
Analysis also useful NLP tasks beyond computing semantic relatedness
text categorization, intend investigate future work. Recently,
used ESA improve performance conventional information retrieval (Egozi,
Gabrilovich, & Markovitch, 2008). work, augmented queries documents
generated features, documents indexed augmented space words
concepts. Potthast, Stein, Anderka (2008) Sorg Cimiano (2008) adapted
ESA multi-lingual cross-lingual information retrieval.
another recent study, Gurevych et al. (2007) applied methodology computing
word similarity German, also information retrieval task searched job
descriptions given users description career interests, found method superior
WordNet-based approach. Importantly, study also confirms method
478

fiWikipedia-based Semantic Interpretation

easily adapted languages English, using version Wikipedia
corresponding desired target language.
future work, also intend apply ESA word sense disambiguation. Current
approaches word sense disambiguation represent contexts contain ambiguous words
using bag words augmented part-of-speech information. believe representation contexts greatly improved use feature generation map
contexts relevant knowledge concepts. Anecdotal evidence (such examples presented Section 4.5.1) implies method promise improving state art
word sense disambiguation. work capitalized inter-article links Wikipedia
several ways, future work intend investigate elaborate techniques
leveraging high degree cross-linking Wikipedia articles.
Wiki technology underlying Wikipedia project often used nowadays variety open-editing initiatives. include corporate intranets use Wiki primary
documentation tool, well numerous domain-specific encyclopedias topics ranging
mathematics Orthodox Christianity.25 Therefore, believe methodology
also used augmenting document representation many specialized domains.
also essential note Wikipedia available numerous languages, different
language versions cross-linked level concepts. believe information
leveraged use Wikipedia-based semantic interpretation improving machine
translation.
work proposes methodology Explicit Semantic Analysis using Wikipedia.
However, ESA also implemented using repositories human knowledge
satisfy requirements listed Section 2.1. Section 3.3 reported results
building ESA-based semantic interpreter using Open Directory Project (Gabrilovich
& Markovitch, 2005, 2007b). Zesch, Mueller, Gurevych (2008) proposed use Wiktionary computing semantic relatedness. future work, intend implement
ESA using additional knowledge repositories.
Finally, readers interested using Wikipedia work, main software
deliverable described work Wikipedia preprocessor (WikiPrep), available online
part SourceForge open-source project http://wikiprep.sourceforge.net.

Acknowledgments
thank Michael D. Lee Brandon Pincombe making available document similarity data. also thank Deepak Agarwal advice assessing statistical significance
results computing semantic relatedness. work partially supported funding
EC-sponsored MUSCLE Network Excellence.
first authors current address Yahoo! Research, 2821 Mission College Blvd, Santa
Clara, CA 95054, USA.

25. See http://en.wikipedia.org/wiki/Category:Online encyclopedias longer list examples.

479

fiGabrilovich & Markovitch

Appendix A. effect knowledge breadth text categorization
appendix, examine effect performing feature generation using newer
Wikipedia snapshot, defined Section 3.2.2. see Table 7, using
larger amount knowledge leads average greater improvements text categorization performance. Although difference performance two versions
admittedly small, consistent across datasets (a similar situation happens assessing
role external knowledge computing semantic relatedness, see Section 3.3.3).

Dataset

Baseline

micro
BEP
Reuters-21578 (10 cat.) 0.925
Reuters-21578 (90 cat.) 0.877
RCV1 Industry-16
0.642
RCV1 Industry-10A 0.421
RCV1 Industry-10B 0.489
RCV1 Industry-10C 0.443
RCV1 Industry-10D 0.587
RCV1 Industry-10E 0.648
RCV1 Topic-16
0.836
RCV1 Topic-10A
0.796
RCV1 Topic-10B
0.716
RCV1 Topic-10C
0.687
RCV1 Topic-10D
0.829
RCV1 Topic-10E
0.758
OHSUMED-10A
0.518
OHSUMED-10B
0.656
OHSUMED-10C
0.539
OHSUMED-10D
0.683
OHSUMED-10E
0.442
20NG
0.854
Movies
0.813
Average

macro
BEP
0.874
0.602
0.595
0.335
0.528
0.414
0.466
0.605
0.591
0.587
0.618
0.604
0.673
0.742
0.417
0.500
0.505
0.515
0.542

Wikipedia
(26/03/06)
micro macro
BEP BEP
0.935 0.891
0.883 0.600
0.648 0.616
0.457 0.450
0.527 0.559
0.458 0.424
0.607 0.448
0.649 0.607
0.842 0.659
0.802 0.689
0.725 0.660
0.697 0.627
0.838 0.687
0.762 0.752
0.545 0.490
0.667 0.529
0.553 0.527
0.694 0.550
0.461 0.588
0.859
0.850

Improvement
(26/03/06)
micro macro
BEP
BEP
+1.1% +1.9%
+0.7% -0.3%
+0.9% +3.5%
+8.6% +34.3%
+7.8% +5.9%
+3.4% +2.4%
+3.4% -3.9%
+0.2% +0.3%
+0.7% +11.5%
+0.8% +17.4%
+1.3% +6.8%
+1.5% +3.8%
+1.1% +2.1%
+0.5% +1.3%
+5.2% +17.5%
+1.7% +5.8%
+2.6% +4.4%
+1.6% +6.8%
+4.3% +8.5%
+0.6%
+4.5%
+2.50% +6.84%

Improvement
(05/11/05)
micro macro
BEP
BEP
+0.8% +1.5%
+0.7% +0.2%
+0.5% +3.7%
+6.4% +30.4%
+7.0% +7.2%
+5.6% +4.1%
+1.4% -1.5%
-1.1% +1.2%
+0.8% +11.8%
+0.3% +16.2%
+1.0% +6.1%
+1.7% +2.3%
+1.2% +2.2%
+0.9% +1.8%
+3.9% +18.0%
+1.7% +6.8%
+1.1% +3.4%
+1.3% +6.0%
+4.5% +6.1%
+1.0%
+3.6%
+2.11% +6.71%

Table 7: effect feature generation using newer Wikipedia snapshot (dated
March 26, 2006)

480

fiWikipedia-based Semantic Interpretation

Appendix B. Test Collections Text Categorization
Appendix provides detailed description test collections used evaluate
knowledge-based feature generation text categorization.
B.1 Reuters-21578
data set contains one year worth English-language stories distributed
Reuters newswire 19861987, arguably often used test collection
text categorization research. Reuters-21578 cleaned version earlier release named
Reuters-22173, contained errors duplicate documents.
collection contains 21578 documents (hence name) SGML format. those,
12902 documents categorized, i.e., assigned category label marked belonging
category. documents explicit classification; is,
reasonably belong categories (judged content), marked so. Several train/test splits collection defined, ModApte (Modified Apte)
commonly used one. ModApte split divides collection chronologically,
allocates first 9603 documents training, rest 3299 documents testing.
documents labeled 118 categories; 016 labels per document,
average 1.04. category distribution extremely skewed: largest category
(earn) 3964 positive examples, 16 categories one positive example.
Several category sets defined collection:
10 largest categories (earn, acq, money-fx, grain, crude, trade, interest, ship, wheat, corn).
90 categories least one document training set one testing set
(Yang, 2001).
Galavotti, Sebastiani, Simi (2000) used set 115 categories least one
training example (three categories, cottonseed, f-cattle sfr training
examples ModApte split).
full set 118 categories least one positive example either training
testing set.
Following common practice, used ModApte split two category sets, 10 largest
categories 90 categories least one training testing example.
B.2 20 Newsgroups (20NG)
20 Newsgroups collection (Lang, 1995) comprised 19997 postings 20 Usenet
newsgroups. documents single label, defined name newsgroup
sent to; 4% documents cross-posted, hence several
labels. newsgroup contains exactly 1000 positive examples, exception
soc.religion.christian contains 997.
categories quite close scope, example, comp.sys.ibm.pc.hardware
comp.sys.mac.hardware, talk.religion.misc soc.religion.christian. document
481

fiGabrilovich & Markovitch

posted single newsgroup may reasonably considered appropriate groups
(the author may simply known similar groups, thus cross-posted
message); naturally poses additional difficulty classification.
noted Internet news postings informal, therefore documents frequently contain non-standard abbreviated words, foreign words, proper
names, well large amount markup characters (used attribution authorship
message separation).
B.3 Movie Reviews
Movie Reviews collection (Pang et al., 2002) presents example sentiment classification, different standard (topical) text categorization. collection
contains 1400 reviews movies, half express positive sentiment (opinion)
movie, half negative. reviews collected rec.arts.movies.reviews
newsgroup, archived Internet Movie Database (IMDB, http://www.imdb.com).
classification problem case determine semantic orientation document, rather relate content one predefined topics. problem
arguably difficult topical text categorization, since notion semantic orientation quite general. saw collection opportunity apply feature generation
techniques new task.
Recent works semantic orientation include (Turney & Littman, 2002; Turney, 2002;
Pang et al., 2002).26 two former studies used unsupervised learning techniques based
latent semantic indexing, estimating semantic distance given document
two reference words represent polar opinions, namely, excellent poor.
latter work used classical TC techniques.
B.4 Reuters Corpus Version 1 (RCV1)
RCV1 newest corpus released Reuters (Lewis et al., 2004; Rose, Stevenson, &
Whitehead, 2002). considerably larger predecessor, contains 800,000
news items, dated August 20, 1996 August 19, 1997. stories labeled
3 category sets, Topics, Industries Regions.
Topics close nature category set old Reuters collection
(Reuters-21578). 103 topic codes, 3.24 categories per document
average. topics organized hierarchy, Hierarchy Policy required category assigned document, ancestors hierarchy
assigned well. result, many 36% Topic assignments
26. field genre classification, attempts establish genre document, somewhat related
sentiment classification. Examples possible genres radio news transcripts classified advertisements. work Dewdney, VanEss-Dykema, MacMillan (2001) cast problem text
categorization, using presentation features addition words. presentation features included
part speech tags verb tenses, well mean variance statistics sentence word length,
punctuation usage, amount whitespace characters. Using support vector machines actual
classification, authors found performance due presentation features alone least
good achieved plain words, combined feature set usually resulted
improvement several percentage points.

482

fiWikipedia-based Semantic Interpretation

due four general categories, CCAT, ECAT, GCAT, MCAT. Consequently, micro-averaged performance scores dominated categories
(Lewis et al., 2004), macro-averaging becomes interest.27 Minimum Code
Policy required document assigned least one Topic one Region
code.
Industries fine-grained Topics, therefore harder classification. categories also organized hierarchy, although Hierarchy
Policy partially enforced them. 351,761 documents labeled
Industry codes.
Region codes correspond geographical places, subdivided countries, regional groupings economic groupings. Lewis et al. (2004) argue
Region codes might suitable named entity recognition text categorization.
experiments used Topic Industry categories. Due sheer size
collection, processing categories set would unreasonably long, allowing
conduct experiments. speed experimentation, used subset corpus
17,808 training documents (dated August 2027, 1996) 5341 testing documents
(dated August 2831, 1996). Following scheme introduced Brank et al. (2002),
used 16 Topic 16 Industry categories, constitute representative sample
full groups 103 354 categories, respectively. also randomly sampled Topic
Industry categories 5 sets 10 categories each. Table 8 gives full definition
category sets used.
noted Lewis et al. (2004), original RCV1 distribution contains number
errors; particular, documents conform either Minimum Code
Hierarchy Policy, labeled erratic codes. Lewis et al. (2004) proposed procedure
correct errors, defined new version collection, named RCV1-v2 (as
opposed original distribution, referred RCV1-v1 ). experiments
based RCV1-v2.
B.5 OHSUMED
OHSUMED (Hersh et al., 1994) subset MEDLINE database, contains
348,566 references documents published medical journals period 19871991.
reference contains publication title, two-thirds (233,445) also contain
abstract. document labeled several MeSH categories (MeSH, 2003).
14,000 distinct categories collection, average 13 categories per
document. OHSUMED frequently used information retrieval text categorization
research.
Following Joachims (1998), used subset documents 1991 abstracts,
taking first 10,000 documents training next 10,000 testing. limit
number categories experiments, randomly generated 5 sets 10 categories
each. Table 9 gives full definition category sets used.
27. micro-averaged scores Topic codes much higher macro-averaged ones, see
Section 4.4.2.

483

fiGabrilovich & Markovitch

Set name
Topic-16
Topic-10A
Topic-10B
Topic-10C
Topic-10D
Topic-10E
Industry-16

Industry-10A
Industry-10B
Industry-10C
Industry-10D
Industry-10E

Categories comprising set
e142, gobit, e132, c313, e121, godd, ghea, e13, c183, m143,
gspo, c13, e21, gpol, m14, c15
e31, c41, c151, c313, c31, m13, ecat, c14, c331, c33
m132, c173, g157, gwea, grel, c152, e311, c21, e211, c16
c34, c13, gtour, c311, g155, gdef, e21, genv, e131, c17
c23, c411, e13, gdis, c12, c181, gpro, c15, g15, c22
c172, e513, e12, ghea, c183, gdip, m143, gcrim, e11, gvio
i81402, i79020, i75000, i25700, i83100, i16100, i1300003, i14000,
i3302021, i8150206, i0100132, i65600, i3302003, i8150103, i3640010,
i9741102
i47500, i5010022, i3302021, i46000, i42400, i45100, i32000, i81401,
i24200, i77002
i25670, i61000, i81403, i34350, i1610109, i65600, i3302020, i25700,
i47510, i9741110
i25800, i41100, i42800, i16000, i24800, i02000, i34430, i36101,
i24300, i83100
i1610107, i97400, i64800, i0100223, i48300, i81502, i34400, i82000,
i42700, i81402
i33020, i82003, i34100, i66500, i1300014, i34531, i16100, i22450,
i22100, i42900

Table 8: Definition RCV1 category sets used experiments

Appendix C. Additional Examples Feature Generation Text
Categorization
Appendix, list number additional feature generation examples.
Text: development T-cell leukaemia following otherwise successful treatment three patients X-linked severe combined immune deficiency (X-SCID)
gene-therapy trials using haematopoietic stem cells led re-evaluation
approach. Using mouse model gene therapy X-SCID, find
corrective therapeutic gene IL2RG act contributor genesis
T-cell lymphomas, one-third animals affected. Gene-therapy trials
X-SCID, based assumption IL2RG minimally oncogenic,
may therefore pose risk patients.
Top 10 generated features: (1) Leukemia; (2) Severe combined immunodeficiency; (3) Cancer; (4) Non-Hodgkin lymphoma; (5) AIDS; (6) ICD-10 Chapter
II: Neoplasms; Chapter III: Diseases blood blood-forming organs,
certain disorders involving immune mechanism; (7) Bone marrow transplant; (8) Immunosuppressive drug; (9) Acute lymphoblastic leukemia; (10) Multiple sclerosis

Selected explanations: (4) particular cancer type; (6) disease code ICD
International Statistical Classification Diseases Related Health Problems
Text: Scientific methods biology
484

fiWikipedia-based Semantic Interpretation

Set name
OHSUMED-10A

OHSUMED-10B

OHSUMED-10C

OHSUMED-10D

OHSUMED-10E

Categories comprising set
(parentheses contain MeSH identifiers)
B-Lymphocytes (D001402);
Metabolism, Inborn Errors (D008661);
Creatinine (D003404); Hypersensitivity (D006967);
Bone Diseases, Metabolic (D001851); Fungi (D005658);
New England (D009511); Biliary Tract (D001659);
Forecasting (D005544); Radiation (D011827)
Thymus Gland (D013950); Insurance (D007341);
Historical Geographic Locations (D017516);
Leukocytes (D007962); Hemodynamics (D006439);
Depression (D003863); Clinical Competence (D002983);
Anti-Inflammatory Agents, Non-Steroidal (D000894);
Cytophotometry (D003592); Hydroxy Acids (D006880)
Endothelium, Vascular (D004730);
Contraceptives, Oral, Hormonal (D003278);
Acquired Immunodeficiency Syndrome (D000163);
Gram-Positive Bacteria (D006094); Diarrhea (D003967);
Embolism Thrombosis (D016769);
Health Behavior (D015438); Molecular Probes (D015335);
Bone Diseases, Developmental (D001848);
Referral Consultation (D012017)
Antineoplastic Immunosuppressive Agents (D000973);
Receptors, Antigen, T-Cell (D011948);
Government (D006076); Arthritis, Rheumatoid (D001172);
Animal Structures (D000825); Bandages (D001458);
Italy (D007558); Investigative Techniques (D008919);
Physical Sciences (D010811); Anthropology (D000883)
HTLV-BLV Infections (D006800);
Hemoglobinopathies (D006453); Vulvar Diseases (D014845);
Polycyclic Hydrocarbons, Aromatic (D011084);
Age Factors (D000367); Philosophy, Medical (D010686);
Antigens, CD4 (D015704);
Computing Methodologies (D003205);
Islets Langerhans (D007515); Regeneration (D012038)

Table 9: Definition OHSUMED category sets used experiments

485

fiGabrilovich & Markovitch

Top 10 generated features: (1) Biology; (2) Scientific classification; (3) Science; (4) Chemical biology; (5) Binomial nomenclature; (6) Nature (journal);
(7) Social sciences; (8) Philosophy biology; (9) Scientist; (10) History
biology
Selected explanations: (5) formal method naming species biology
Text: quavering voices, parents grandparents killed World
Trade Center read names victims solemn recitation today, marking
third anniversary terror attacks. ceremony one many planned
United States around world honor memory nearly 3,000 victims
9/11.
Top 10 generated features: (1) September 11, 2001 attack memorials services; (2) United Airlines Flight 93; (3) Aftermath September 11, 2001
attacks; (4) World Trade Center; (5) September 11, 2001 attacks; (6) Oklahoma City bombing; (7) World Trade Center bombing; (8) Arlington National
Cemetery; (9) World Trade Center site; (10) Jewish bereavement
Selected explanations: (2) one four flights hijacked September 11, 2001;
(6) terrorist attack Oklahoma City 1995; (8) American military cemetery
Text: U.S. intelligence cannot say conclusively Saddam Hussein weapons
mass destruction, information gap complicating White House efforts
build support attack Saddams Iraqi regime. CIA advised top
administration officials assume Iraq weapons mass destruction.
agency given President Bush smoking gun, according U.S.
intelligence administration officials.
Top 10 generated features: (1) Iraq disarmament crisis; (2) Yellowcake forgery; (3) Senate Report Pre-War Intelligence Iraq; (4) Iraq weapons
mass destruction; (5) Iraq Survey Group; (6) September Dossier; (7) Iraq
war; (8) Scott Ritter; (9) Iraq War Rationale; (10) Operation Desert Fox
Selected explanations: (2) falsified intelligence documents Iraqs alleged
attempt purchase yellowcake uranium; (6) paper Iraqs weapons mass
destruction published UK government 2002; (8) UN weapons inspector
Iraq; (10) US UK joint military campaign Iraq 1998
another example, consider pair contexts contain word jaguar,
first one contains ambiguous word sense car model, second
onein sense animal.
Text: Jaguar car models
Top 10 generated features: (1) Jaguar (car); (2) Jaguar (S-Type); (3)
Jaguar X-type; (4) Jaguar E-Type; (5) Jaguar XJ; (6) Daimler Motor Company; (7) British Leyland Motor Corporation; (8) Luxury vehicles; (9) V8
engine; (10) Jaguar Racing
Top 10 generated features: (2), (3), (4), (5) particular Jaguar car models;
(6) car manufacturing company became part Jaguar 1960; (7)
486

fiWikipedia-based Semantic Interpretation

another vehicle manufacturing company merged Jaguar; (9) internal
combustion engine used Jaguar car models; (10) Formula One team
used Jaguar promote brand name
Text: Jaguar (Panthera onca)
Top 10 generated features: (1) Jaguar; (2) Felidae; (3) Black panther;
(4) Leopard; (5) Puma; (6) Tiger; (7) Panthera hybrid; (8) Cave lion; (9)
American lion; (10) Kinkajou
Top 10 generated features: (2) family include lions, tigers, jaguars,
related feline species; (10) another carnivore mammal
also show number examples generating features using inter-article links.
Text: artificial intelligence
Regular feature generation: (1) Artificial intelligence; (2) A.I. (film); (3)
MIT Computer Science Artificial Intelligence Laboratory; (4) Artificial
life; (5) Strong AI; (6) Swarm intelligence; (7) Computer Science; (8) Frame
problem; (9) Cognitive science; (10) Carl Hewitt
Features generated using links: (1) Robot; (2) John McCarthy (computer scientist); (3) Artificial consciousness; (4) Marvin Minsky; (5) Planner programming language; (6) Actor model (a model concurrent computation formulated
Carl Hewitt colleagues); (7) Logic; (8) Scientific Community Metaphor;
(9) Natural language processing; (10) Lisp programming language
general features only: (1) Robot; (2) Massachusetts Institute Technology; (3) Psychology; (4) Consciousness; (5) Lisp programming language
Text: group European-led astronomers made photograph appears
planet orbiting another star. so, would first confirmed picture
world beyond solar system.
Regular feature generation: (1) Planet; (2) Solar system; (3) Astronomy; (4)
Planetary orbit; (5) Extrasolar planet; (6) Pluto; (7) Jupiter; (8) Neptune; (9)
Minor planet; (10) Mars
Features generated using links: (1) Asteroid; (2) Earth; (3) Oort cloud (a
postulated cloud comets); (4) Comet; (5) Sun; (6) Saturn; (7) Moon; (8) Mercury
(planet); (9) Asteroid belt; (10) Orbital period
general features only: (1) Earth; (2) Moon; (3) Asteroid; (4) Sun; (5)
National Aeronautics Space Administration
Text: Nearly 70 percent Americans say careful eat,
even say diet essential good health, according new nationwide health
poll obesity ranked second among biggest health concerns.
Regular feature generation: (1) Veganism; (2) Vegetarianism; (3) Obesity; (4)
Atkins Nutritional Approach; (5) Binge eating disorder; (6) Dick Gregory; (7)
Nutrition; (8) Super Size Me; (9) Health insurance; (10) Eating disorder
487

fiGabrilovich & Markovitch

Selected explanations: (1) philosophy avoiding animal-derived food; (6)
American nutritionist; (7) documentary film individual eats
McDonalds fast food one full month.
Features generated using links: (1) Raw food diet; (2) Diabetes mellitus; (3)
Healthy eating; (4) Body mass index; (5) Omega-3 fatty acid (an important nutritional component); (6) Dieting; (7) Milk; (8) United States28 ; (9) Hypertension;
(10) Egg (food)
general features only: (1) United States; (2) Diabetes mellitus; (3) Cancer; (4) Food; (5) McDonalds

Appendix D. Comparing Knowledge Sources Feature Generation:
Wikipedia versus WordNet
demonstrate shortcomings WordNet source knowledge-based feature generation, juxtapose WordNet-based Wikipedia-based feature generation sample
sentence examined Section 4.5.1 (we repeat Wikipedia context classifications
readers convenience).
used WordNet version 1.6. follows, WordNet synsets denoted curly
braces, noun verb synsets followed immediate hypernym (more general
synset), applicable.
Text: Wal-Mart supply chain goes real time
relevant Wikipedia classifications:
Wal-Mart
Sam Walton Wal-Mart founder
Sears Holdings Corporation, Target Corporation, Albertsons prominent competitors Wal-Mart
RFID Radio Frequency Identification, technology Wal-Mart uses
extensively manage stock
Hypermarket superstore, general concept Wal-Mart specific
example
United Food Commercial Workers labor union trying
organize Wal-Marts workers
WordNet :
{Wal-Mart} { }; (word found WordNet)
{supply} {indefinite quantity}; {supply} {economic process}; {provision,
providing, supply supplying} {activity};
28. classification quite interesting, issue discussed input text fragment characteristic American life style.

488

fiWikipedia-based Semantic Interpretation

{supply provide, render, furnish} {give}; {issue, supply} {distribute};
{provide, supply ply, cater} {give}; {add, append, supply} {state, say,
tell};
{chain, concatenation} {series}; {chain, chemical chain} {unit, building
block}; {chain} {ligament}; {chain} {business, concern, business concern,
business organization}; {chain} {restraint, constraint}; {chain} {linear
unit}; {range, mountain range, range mountains, chain, mountain chain,
chain maountains} {geological formation, geology, formation}; {iron, irons,
chain, chains} {shackle, bond, hamper, trammel, trammels}; {chain, string,
strand} {necklace};
{chain} {fasten, fix, secure};
{go, spell, tour, turn} {shift, work shift, duty period}; {crack, fling, go, pass,
whirl, offer} {attempt, effort, endeavor, endeavour, try}; {go, go game}
{board game};
{travel, move, go, locomote}; {go, proceed, move} {act, move}; (28
verb senses omitted brevity)
{real number, real}; {real (Spanish coin)};
{real, existent}; {real (no less stated)}; {real, true}; {real (not
taken lightly)}; {real, tangible}; {actual, genuine, literal, real}; {real (economics)}; {substantial, real, material}; {real (of property)}; {veridical, real};
{real (founded practical matters)};
{very, really, real, rattling};
{time, clip} {case, instance, example}; {time} {time period, period,
period time, amount time}; {time} {moment, minute, second, instant}; {time} {abstraction}; {clock time, time} {reading, meter reading}; {fourth dimension, time} {dimension}; {time} {experience}; {meter,
time} {rhythmicity}; {prison term, sentence, time} {term};
{clock, time} {quantify, measure}; {time} {schedule}; {time}
{determine, shape, influence, regulate}; {time} {adjust, set};
Evidently, WordNet classifications overly general diverse context words
cannot properly disambiguated. Furthermore, owing lack proper names, WordNet
cannot possibly provide wealth information encoded Wikipedia, easily overcomes drawbacks WordNet. methodology proposed suffer
shortcomings.

489

fiGabrilovich & Markovitch

References
Adafre, S. F., & de Rijke, M. (2005). Discovering missing links Wikipedia. Proceedings
Workshop Link Discovery: Issues, Approaches Applications (LinkKDD2005), pp. 9097.
Baeza-Yates, R., & Ribeiro-Neto, B. (1999). Modern Information Retrieval. Addison Wesley,
New York, NY.
Baker, D., & McCallum, A. K. (1998). Distributional clustering words text classification. Croft, B., Moffat, A., Van Rijsbergen, C. J., Wilkinson, R., & Zobel, J. (Eds.),
Proceedings 21st ACM International Conference Research Development
Information Retrieval, pp. 96103, Melbourne, AU. ACM Press, New York, US.
Basili, R., Moschitti, A., & Pazienza, M. T. (2000). Language-sensitive text classification.
Proceedings RIAO-00, 6th International Conference Recherche dInformation
Assistee par Ordinateur, pp. 331343, Paris, France.
Begelman, G., Keller, P., & Smadja, F. (2006). Automated tag clustering: Improving search
exploration tag space. Proceedings Collaborative Web Tagging
Workshop, conjunction 15th International World Wide Web Conference,
Edinburgh, Scotland.
Bekkerman, R. (2003). Distributional clustering words text categorization. Masters
thesis, Technion.
Bloehdorn, S., & Hotho, A. (2004). Boosting text classification semantic features.
Proceedings MSW 2004 Workshop 10th ACM SIGKDD Conference
Knowledge Discovery Data Mining, pp. 7087.
Brank, J., Grobelnik, M., Milic-Frayling, N., & Mladenic, D. (2002). Interaction feature
selection methods linear classification models. Workshop Text Learning
held ICML-2002.
Brill, E. (1995). Transformation-based error-driven learning natural language processing: case study part speech tagging. Computational Linguistics, 21 (4),
543565.
Buchanan, B. G., & Feigenbaum, E. (1982). Forward. Davis, R., & Lenat, D. (Eds.),
Knowledge-Based Systems Artificial Intelligence. McGraw-Hill.
Budanitsky, A., & Hirst, G. (2006). Evaluating wordnet-based measures lexical semantic
relatedness. Computational Linguistics, 32 (1), 1347.
Cai, L., & Hofmann, T. (2003). Text categorization boosting automatically extracted
concepts. Proceedings 26th International Conference Research Development Information Retrieval, pp. 182189.
Caropreso, M. F., Matwin, S., & Sebastiani, F. (2001). learner-independent evaluation
usefulness statistical phrases automated text categorization. Chin, A. G.
(Ed.), Text Databases Document Management: Theory Practice, pp. 78102.
Idea Group Publishing, Hershey, US.
490

fiWikipedia-based Semantic Interpretation

Chang, M.-W., Ratinov, L., Roth, D., & Srikumar, V. (2008). Importance semantic
representation: Dataless classification. Proceedings 23rd AAAI Conference
Artificial Intelligence, pp. 830835.
Cohen, W. W. (1995). Fast effective rule induction. Proceedings 12th International
Conference Machine Learning (ICML-95), pp. 115123.
Cohen, W. W. (2000). Automatically extracting features concept learning web.
Proceedings 17th International Conference Machine Learning.
Dagan, I., Lee, L., & Pereira, F. C. N. (1999). Similarity-based models word cooccurrence
probabilities. Machine Learning, 34 (13), 4369.
Dagan, I., Marcus, S., & Markovitch, S. (1995). Contextual word similarity estimation
sparse data. Computer Speech Language, 9 (2), 123152.
Davidov, D., Gabrilovich, E., & Markovitch, S. (2004). Parameterized generation labeled
datasets text categorization based hierarchical directory. Proceedings
27th ACM International Conference Research Development Information
Retrieval, pp. 250257.
Debole, F., & Sebastiani, F. (2003). Supervised term weighting automated text categorization. Proceedings SAC-03, 18th ACM Symposium Applied Computing,
pp. 784788.
Deerwester, S., Dumais, S., Furnas, G., Landauer, T., & Harshman, R. (1990). Indexing
latent semantic analysis. Journal American Society Information Science,
41 (6), 391407.
Demsar, J. (2006). Statistical comparison classifiers multiple data sets. Journal
Machine Learning Research, 7, 130.
Dewdney, N., VanEss-Dykema, C., & MacMillan, R. (2001). form substance:
Classification genres text. Workshop HLT KM held ACL-2001.
Dhillon, I., Mallela, S., & Kumar, R. (2003). divisive information-theoretic feature clustering algorithm text classification. Journal Machine Learning Research, 3,
12651287.
Dumais, S., Platt, J., Heckerman, D., & Sahami, M. (1998). Inductive learning algorithms
representations text categorization. Proceedings 7th ACM International Conference Information Knowledge Management, pp. 148155.
Egozi, O., Gabrilovich, E., & Markovitch, S. (2008). Concept-based feature generation
selection information retrieval. AAAI08.
Fawcett, T. (1993). Feature Discovery Problem Solving Systems. Ph.D. thesis, UMass.
Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press, Cambridge, MA.
Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G., & Ruppin,
E. (2002a). Placing search context: concept revisited. ACM Transactions
Information Systems, 20 (1), 116131.
491

fiGabrilovich & Markovitch

Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G., & Ruppin,
E. (2002b). WordSimilarity-353 test collection..
Fuernkranz, J., Mitchell, T., & Riloff, E. (1998). case study using linguistic phrases
text categorization WWW. Sahami, M. (Ed.), Learning Text Categorization: Proceedings 1998 AAAI/ICML Workshop, pp. 512. AAAI Press,
Madison, Wisconsin.
Gabrilovich, E., & Markovitch, S. (2004). Text categorization many redundant features:
Using aggressive feature selection make SVMs competitive C4.5. Proceedings
21st International Conference Machine Learning, pp. 321328.
Gabrilovich, E., & Markovitch, S. (2005). Feature generation text categorization using world knowledge. Proceedings 19th International Joint Conference
Artificial Intelligence, pp. 10481053, Edinburgh, Scotand.
Gabrilovich, E., & Markovitch, S. (2006). Overcoming brittleness bottleneck using
Wikipedia: Enhancing text categorization encyclopedic knowledge. Proceedings 21st National Conference Artificial Intelligence, pp. 13011306.
Gabrilovich, E., & Markovitch, S. (2007a). Computing semantic relatedness using wikipediabased explicit semantic analysis. Proceedings 20th International Joint Conference Artificial Intelligence, pp. 16061611.
Gabrilovich, E., & Markovitch, S. (2007b). Harnessing expertise 70,000 human editors: Knowledge-based feature generation text categorization. Journal Machine
Learning Research, 8, 22972345.
Galavotti, L., Sebastiani, F., & Simi, M. (2000). Experiments use feature selection
negative evidence automated text categorization. Borbinha, J., & Baker, T.
(Eds.), Proceedings ECDL-00, 4th European Conference Research Advanced
Technology Digital Libraries, pp. 5968, Lisbon, Portugal.
Giles, J. (2005). Internet encyclopaedias go head head. Nature, 438, 900901.
Gurevych, I., Mueller, C., & Zesch, T. (2007). be? electronic career guidance
based semantic relatedness. Proceedings 45th Annual Meeting
Association Computational Linguistics.
Hersh, W., Buckley, C., Leone, T., & Hickam, D. (1994). OHSUMED: interactive
retrieval evaluation new large test collection research. Proceedings
17th ACM International Conference Research Development Information
Retrieval, pp. 192201.
Hirsh, H., & Japkowicz, N. (1994). Bootstrapping training-data representations inductive
learning: case study molecular biology. Proceedings Twelfth National
Conference Artificial Intelligence, pp. 639644.
Hirst, G., & St-Onge, D. (1998). Lexical chains representations context detection
correction malapropisms. WordNet: Electronic Lexical Database, pp.
305332. MIT Press, Cambridge, MA.
Hu, Y.-J., & Kibler, D. (1996). wrapper approach constructive induction.
Thirteenth National Conference Artificial Intelligence, pp. 4752.
492

fiWikipedia-based Semantic Interpretation

Hughes, T., & Ramage, D. (2007). Lexical semantic relatedness random graph walks.
Proceedings Conference Empirical Methods Natural Language Processing
(EMNLP).
Hull, D. A. (1994). Improving text retrieval routing problem using latent semantic
indexing. Croft, W. B., & Van Rijsbergen, C. J. (Eds.), Proceedings 17th ACM
International Conference Research Development Information Retrieval, pp.
282289, Dublin, Ireland. Springer Verlag, Heidelberg, Germany.
Jarmasz, M. (2003). Rogets thesaurus lexical resource natural language processing.
Masters thesis, University Ottawa.
Jarmasz, M., & Szpakowicz, S. (2003). Rogets thesaurus semantic similarity.
Proceedings International Conference Recent Advances Natural Language
Processing, pp. 111120.
Jiang, J. J., & Conrath, D. W. (1997). Semantic similarity based corpus statistics
lexical taxonomy. Proceedings 10th International Conference Research
Computational Linguistics, pp. 5763.
Jo, T. (2000). Neurotextcategorizer: new model neural network text categorization.
Proceedings International Conference Neural Information Processing, pp.
280285, Taejon, South Korea.
Jo, T. (2006). Dynamic Document Organization using Text Categorization Text Clustering. Ph.D. thesis, University Ottawa.
Jo, T., & Japkowicz, N. (2005). Text clustering using NTSO. Proceedings International Joint Conference Neural Networks, pp. 558563.
Joachims, T. (1998). Text categorization support vector machines: Learning many
relevant features. Proceedings European Conference Machine Learning,
pp. 137142.
Joachims, T. (1999). Making large-scale SVM learning practical. Schoelkopf, B., Burges,
C., & Smola, A. (Eds.), Advances Kernel Methods Support Vector Learning, pp.
169184. MIT Press.
Kudenko, D., & Hirsh, H. (1998). Feature generation sequence categorization. Proceedings 15th Conference American Association Artificial Intelligence,
pp. 733738.
Kumaran, G., & Allan, J. (2004). Text classification named entities new event
detection. Proceedings 27th ACM International Conference Research
Development Information Retrieval, pp. 297304.
Lang, K. (1995). Newsweeder: Learning filter netnews. Proceedings 12th International Conference Machine Learning, pp. 331339.
Leacock, C., & Chodorow, M. (1998). Combining local context WordNet similarity
word sense identification. WordNet: Electronic Lexical Database, pp. 265283.
MIT Press, Cambridge, MA.
Lee, L. (1999). Measures distributional similarity. Proceedings 37th Annual
Meeting ACL, pp. 2532.
493

fiGabrilovich & Markovitch

Lee, M. D., Pincombe, B., & Welsh, M. (2005). comparison machine measures text
document similarity human judgments. 27th Annual Meeting Cognitive
Science Society (CogSci2005), pp. 12541259.
Lenat, D. B. (1995). CYC: large-scale investment knowledge infrastructure. Communications ACM, 38 (11).
Lenat, D. B. (1997). 2001 2001: Common sense mind HAL. HALs
Legacy, pp. 194209. MIT Press.
Lenat, D. B., Guha, R. V., Pittman, K., Pratt, D., & Shepherd, M. (1990). CYC: Towards
programs common sense. Communications ACM, 33 (8).
Leopold, E., & Kindermann, J. (2002). Text categorization support vector machines:
represent texts input space. Machine Learning, 46, 423444.
Lewis, D. D. (1992). evaluation phrasal clustered representations text
categorization task. Proceedings 15th ACM International Conference
Research Development Information Retrieval, pp. 3750.
Lewis, D. D., & Croft, W. B. (1990). Term clustering syntactic phrases. Proceedings
13th ACM International Conference Research Development Information
Retrieval, pp. 385404.
Lewis, D. D., Schapire, R. E., Callan, J. P., & Papka, R. (1996). Training algorithms
linear text classifiers. Proceedings 19th ACM International Conference
Research Development Information Retrieval, pp. 298306.
Lewis, D. D., Yang, Y., Rose, T., & Li, F. (2004). RCV1: new benchmark collection
text categorization research. Journal Machine Learning Research, 5, 361397.
Lin, D. (1998a). Automatic retrieval clustering similar words. Proceedings
17th International Conference Computational Linguistics 36th Annual Meeting
Association Computational Linguistics, pp. 768774.
Lin, D. (1998b). information-theoretic definition word similarity. Proceedings
15th International Conference Machine Learning, pp. 296304.
Liu, T., Chen, Z., Zhang, B., Ma, W.-y., & Wu, G. (2004). Improving text classification
using local latent semantic indexing. ICDM04, pp. 162169.
Manning, C. D., & Schuetze, H. (2000). Foundations Statistical Natural Language Processing. MIT Press.
Markovitch, S., & Rosenstein, D. (2002). Feature generation using general constructor
functions. Machine Learning, 49 (1), 5998.
Matheus, C. J. (1991). need constructive induction. Birnbaum, L., & Collins, G.
(Eds.), Proceedings Eighth International Workshop Machine Learning, pp.
173177.
Matheus, C. J., & Rendell, L. A. (1989). Constructive induction decision trees.
Proceedings 11th International Conference Artificial Intelligence, pp. 645
650.
494

fiWikipedia-based Semantic Interpretation

MeSH (2003). Medical subject headings (MeSH).
http://www.nlm.nih.gov/mesh.

National Library Medicine.

Metzler, D., Dumais, S., & Meek, C. (2007). Similarity measures short segments text.
Proceedings 29th European Conference Information Retrieval, pp. 1627.
Mihalcea, R. (2003). Turning wordnet information retrieval resource: Systematic
polysemy conversion hierarchical codes. International Journal Pattern Recognition Artificial Intelligence (IJPRAI), 17 (1), 689704.
Mihalcea, R., Corley, C., & Strapparava, C. (2006). Corpus-based knowledge-based
measures text semantic similarity. AAAI06, pp. 775780.
Mikheev, A. (1998). Feature lattices maximum entropy models. Proceedings
17th International Conference Computational Linguistics, pp. 848854.
Miller, G. A., & Charles, W. G. (1991). Contextual correlates semantic similarity. Language Cognitive Processes, 6 (1), 128.
Milne, D., & Witten, I. (2008). effective, low-cost measure semantic relatedness
obtained wikipedia links. Proceedings AAAI-08 Workshop Wikipedia
Artificial Intelligence, conjunction 23rd AAAI Conference Artificial
Intelligence.
Mladenic, D. (1998). Turning Yahoo automatic web-page classifier. Proceedings
13th European Conference Artificial Intelligence, pp. 473474.
Montague, R. (1973). proper treatment quantification ordinary English.
Hintikka, J., Moravcsik, J., & Suppes, P. (Eds.), Approaches Natural Language, pp.
373398. Reidel, Dordrecht.
Murphy, P. M., & Pazzani, M. J. (1991). ID2-of-3: Constructive induction M-of-N concepts discriminators decision trees. Proceedings 8th International
Conference Machine Learning, pp. 183188. Morgan Kaufmann.
Pagallo, G., & Haussler, D. (1990). Boolean feature discovery empirical learning. Machine
Learning, 5 (1), 7199.
Pang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up? Sentiment classification using
machine learning techniques. Proceedings Conference Empirical Methods
Natural Language Processing, pp. 7986.
Peng, F., Schuurmans, D., & Wang, S. (2004). Augmenting naive Bayes classifiers
statistical language models. Information Retrieval, 7 (3-4), 317345.
Peng, F., & Shuurmans, D. (2003). Combining naive Bayes n-gram language models
text classification. Proceedings 25th European Conference Information
Retrieval Research (ECIR-03), pp. 335350.
Pincombe, B. (2004). Comparison human latent semantic analysis (LSA) judgements
pairwise document similarities news corpus. Tech. rep. DSTO-RR-0278, Information Sciences Laboratory, Defence Science Technology Organization, Department Defense, Australian Government.
Porter, M. (1980). algorithm suffix stripping. Program, 14 (3), 130137.
495

fiGabrilovich & Markovitch

Potthast, M., Stein, B., & Anderka, M. (2008). wikipedia-based multilingual retrieval
model. European Conference Information Retrieval.
Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (1997). Numerical
Recipes C: Art Scientific Computing. Cambridge University Press.
Qiu, Y., & Frei, H. (1993). Concept based query expansion. Proceedings ACM
International Conference Research Development Information Retrieval.
Raskutti, B., Ferra, H., & Kowalczyk, A. (2001). Second order features maximizing
text classification performance. De Raedt, L., & Flach, P. (Eds.), Proceedings
European Conference Machine Learning (ECML), Lecture notes Artificial
Intelligence (LNAI) 2167, pp. 419430. Springer-Verlag.
Resnik, P. (1999). Semantic similarity taxonomy: information-based measure
application problems ambiguity natural language. Journal Artificial
Intelligence Research, 11, 95130.
Reuters (1997). Reuters-21578 text categorization test collection, Distribution 1.0. Reuters.
daviddlewis.com/resources/testcollections/reuters21578.
Rocchio, J. J. (1971). Relevance feedback information retrieval. SMART Retrieval
System: Experiments Automatic Document Processing, pp. 313323. Prentice Hall.
Rogati, M., & Yang, Y. (2002). High-performing feature selection text classification.
Proceedings International Conference Information Knowledge Management (CIKM02), pp. 659661.
Roget, P. (1852). Rogets Thesaurus English Words Phrases. Longman Group Ltd.
Rose, T., Stevenson, M., & Whitehead, M. (2002). Reuters Corpus Volume 1from
yesterdays news tomorrows language resources. Proceedings Third International Conference Language Resources Evaluation, pp. 713.
Rowling, J. (1997). Harry Potter Philosophers Stone. Bloomsbury.
Rubenstein, H., & Goodenough, J. B. (1965). Contextual correlates synonymy. Communications ACM, 8 (10), 627633.
Sable, C., McKeown, K., & Church, K. W. (2002). NLP found helpful (at least one
text categorization task). Conference Empirical Methods Natural Language
Processing, pp. 172179.
Sahami, M., & Heilman, T. (2006). web-based kernel function measuring similarity
short text snippets. WWW06, pp. 377386. ACM Press.
Salton, G., & Buckley, C. (1988). Term weighting approaches automatic text retrieval.
Information Processing Management, 24 (5), 513523.
Salton, G., & McGill, M. (1983).
McGraw-Hill.

Introduction Modern Information Retrieval.

Scott, S. (1998). Feature engineering symbolic approach text classification. Masters
thesis, U. Ottawa.
Scott, S., & Matwin, S. (1999). Feature engineering text classification. Proceedings
16th International Conference Machine Learning, pp. 379388.
496

fiWikipedia-based Semantic Interpretation

Sebastiani, F. (2002). Machine learning automated text categorization. ACM Computing
Surveys, 34 (1), 147.
Snow, R., OConnor, B., Jurafsky, D., & Ng, A. Y. (2008). Cheap fast - good?
evaluating non-expert annotations natural language tasks. Proceedings
Conference Empirical Methods Natural Language Processing.
Sorg, P., & Cimiano, P. (2008). Cross-lingual information retrieval explicit semantic
analysis. Working Notes CLEF Workshop.
Strube, M., & Ponzetto, S. P. (2006). WikiRelate! Computing semantic relatedness using
Wikipedia. AAAI06, pp. 14191424, Boston, MA.
Turney, P. (2002). Thumbs thumbs down? Semantic orientation applied unsupervised classification reviews. Proceedings 40th Annual Meeting
Association Computational Linguistics, pp. 417424.
Turney, P. (2005). Measuring semantic similarity latent relational analysis. Proceedings
Nineteenth International Joint Conference Artificial Intelligence (IJCAI-05),
pp. 11361141, Edinburgh, Scotland.
Turney, P. (2006). Similarity semantic relations. Computational Linguistics, 32 (3), 379
416.
Turney, P., & Littman, M. L. (2002). Unsupervised learning semantic orientation
hundred-billion-word corpus. Tech. rep. ERB-1094, National Research Council
Canada.
Turney, P. D. (2001). Mining web synonyms: PMI-IR versus LSA TOEFL.
Proceedings Twelfth European Conference Machine Learning, pp. 491502.
Urena-Lopez, A., Buenaga, M., & Gomez, J. M. (2001). Integrating linguistic resources
TC WSD. Computers Humanities, 35, 215230.
Wang, B. B., McKay, R., Abbass, H. A., & Barlow, M. (2003). comparative study
domain ontology guided feature extraction. Proceedings 26th Australian
Computer Science Conference (ASCS-2003), pp. 6978.
Widrow, B., & Stearns, S. (1985). Adaptive Signal Processing. Prentice Hall.
Wikipedia (2006). Wikipedia, free encyclopedia.. http://en.wikipedia.org.
Wu, H., & Gunopulos, D. (2002). Evaluating utility statistical phrases latent
semantic indexing text classification. IEEE International Conference Data
Mining, pp. 713716.
Yang, Y. (2001). study thresholding strategies text categorization. Proceedings
24th International Conference Research Development Information
Retrieval, pp. 137145.
Yang, Y., & Liu, X. (1999). re-examination text categorization methods. Proceedings
22nd International Conference Research Development Information
Retrieval, pp. 4249.
Yang, Y., & Pedersen, J. (1997). comparative study feature selection text categorization. Proceedings 14th International Conference Machine Learning,
pp. 412420.
497

fiGabrilovich & Markovitch

Zelikovitz, S., & Hirsh, H. (2000). Improving short-text classification using unlabeled background knowledge assess document similarity. Proceedings 17th International Conference Machine Learning, pp. 11831190.
Zelikovitz, S., & Hirsh, H. (2001). Using LSI text classification presence
background text. Proceedings Conference Information Knowledge
Management, pp. 113118.
Zesch, T., & Gurevych, I. (2006). Automatically creating datasets measures semantic
relatedness. Proceedings ACL Workshop Linguistic Distances, pp. 1624,
Sydney, Australia.
Zesch, T., Mueller, C., & Gurevych, I. (2008). Using wiktionary computing semantic
relatedness. Proceedings 23rd AAAI Conference Artificial Intelligence,
pp. 861866.
Zobel, J., & Moffat, A. (1998). Exploring similarity space. ACM SIGIR Forum, 32 (1),
1834.

498

fiJournal Artificial Intelligence Research 34 (2009) 255-296

Submitted 10/08; published 03/09

Unsupervised Methods Determining Object Relation
Synonyms Web
Alexander Yates

yates@temple.edu

Temple University
Computer Information Sciences
1805 N. Broad St.
Wachman Hall 303A
Philadelphia, PA 19122

Oren Etzioni

etzioni@cs.washington.edu

University Washington
Computer Science Engineering
Box 352350
Seattle, WA 98195-2350

Abstract
task identifying synonymous relations objects, synonym resolution,
critical high-quality information extraction. paper investigates synonym resolution context unsupervised information extraction, neither hand-tagged
training examples domain knowledge available. paper presents scalable, fullyimplemented system runs O(KN log N ) time number extractions, N ,
maximum number synonyms per word, K. system, called Resolver, introduces
probabilistic relational model predicting whether two strings co-referential based
similarity assertions containing them. set two million assertions
extracted Web, Resolver resolves objects 78% precision 68% recall,
resolves relations 90% precision 35% recall. Several variations Resolvers
probabilistic model explored, experiments demonstrate appropriate
conditions variations improve F1 5%. extension basic Resolver
system allows handle polysemous names 97% precision 95% recall data
set TREC corpus.

1. Introduction
Web Information Extraction (WIE) systems (Zhu, Nie, Wen, Zhang, & Ma, 2005; Agichtein,
2006; Etzioni, Cafarella, Downey, Kok, Popescu, Shaked, Soderland, Weld, & Yates, 2005)
extract assertions describe relation arguments Web text. example:
(is capital of, D.C., United States)

WIE systems extract hundreds millions assertions containing millions different strings Web (e.g., TextRunner system Banko, Cafarella, Soderland,
Broadhead, & Etzioni, 2007). One problem becomes real challenge scale
WIE systems often extract assertions describe real-world object
relation using different names. example, WIE system might also extract
c
2009
AI Access Foundation. rights reserved.

fiYates & Etzioni

(is capital city of, Washington, U.S.)

describes relationship contains different name relation
argument.
Synonyms prevalent text, Web corpus exception. data set
two million assertions extracted Web crawl contained half-dozen different
names United States Washington, D.C., three capital
relation. top 80 commonly extracted objects average 2.9 extracted
names per entity, several many 10 names. top 100 commonly
extracted relations average 4.9 synonyms per relation.
refer problem identifying synonymous object relation names synonym
resolution. Previous techniques focused one particular aspect problem, either
objects relations. addition, techniques often depend large set training
examples, tailored specific domain assuming knowledge domains
schema. Due number diversity relations extracted, techniques
feasible WIE systems. Schemata available Web, hand-labeling
training examples relation would require prohibitive manual effort.
response, present Resolver, novel, domain-independent, unsupervised synonym
resolution system applies objects relations. Resolver clusters synonymous names together using probabilistic model informed string similarity
similarity assertions containing names. similarity metric outperforms
used similar systems cross-document entity coreference (e.g., Mann & Yarowsky,
2003) paraphrase discovery (Lin & Pantel, 2001; Hasegawa, Sekine, & Grishman, 2004)
respective tasks object relation synonym resolution. key questions
answered Resolver include:
1. possible effectively cluster strings large set extractions sets
synonyms without using domain knowledge, manually labeled training data,
external resources unavailable context Web Information Extraction?
Experiments include empirical demonstration Resolver resolve
objects 78% precision 68% recall, relations 90% precision 35%
recall.
2. scale synonym resolution large, high-dimensional data sets? Resolver
provides scalable clustering algorithm runs time O(KN log N ) number
extractions, N , maximum number synonyms per word, K. theory
compares well even fast approximate solutions clustering large data sets
large-dimensional spaces, practice Resolver successfully run
set assertions extracted 100 million Web pages.
3. formalize unsupervised synonym resolution, practical benefit
so? Resolver provides unsupervised, generative probabilistic model
predicting whether two object relation names co-refer, experiments show
significantly outperforms previous metrics distributional similarity.
particular, outperforms related metric based mutual information (Lin & Pantel,
2001) 193% AUC object clustering, 121% relation clustering.
256

fiUnsupervised Methods Determining Object Relation Synonyms

4. possible use special properties functions inverse functions improve
precision synonym resolution algorithm? basic version Resolvers
probabilistic model object synonymy independent relation extraction. However, intuitively clear certain relations, especially functions
inverse functions, provide especially strong evidence synonymy.
Several extensions Resolver system show without hurting recall,
precision object merging improved 3% using functions.
5. Resolver handle polysemous names, different meanings different
contexts? basic version Resolver assumes every name single
meaning, present extension basic system able automatically
handle polysemous names. manually-cleaned data set polysemous named
entities TREC corpus, Resolver achieves precision 97.3% recall
94.7% detecting proper noun coreference relationships, able outperform
previous work accuracy requiring large, unannotated corpus input.
next section discusses previous work synonym resolution. Section 3 describes
problem synonym resolution formally introduces notation terminology
used throughout. Section 4 introduces Resolvers probabilistic model. Section
5 describes Resolvers clustering algorithm. Section 6 presents experiments
basic Resolver system compare performance performance previous
work synonym resolution. Section 7 describes several extensions basic Resolver
system, together experiments illustrating gains precision recall. Section
8 develops extension Resolver relaxes assumption every string
single referent, compares Resolver experimentally previous work crossdocument entity resolution. Finally, Section 9 discusses conclusions areas future
work.

2. Previous Work
Synonym resolution encompasses two tasks, finding synonyms extracted objects
relations. Synonym resolution objects similar task cross-document
entity resolution (Bagga & Baldwin, 1998), objective cluster occurrences
named entities multiple documents coreferential groups. Pedersen Kulkarni
(Pedersen & Kulkarni, 2007; Kulkarni & Pedersen, 2008) cluster peoples names Web
documents emails using agglomerative clustering heuristic similarity function.
Li, Morie, Roth (2004a, 2004b) use Expectation-Maximization graphical
model databases common nicknames, honorifics, titles, etc.to achieve high accuracy
cross-document entity resolution task. Mann Yarowsky (2003) use combination
extracted features term vectors including proper names context cluster ambiguous
names Web. use Cosine Similarity Metric (Salton & McGill, 1983) together
hierarchical agglomerative clustering. Resolvers main contribution body
work proposes new, formal similarity measure works objects
relations, demonstrates theoretically empirically scale
millions extractions. Web People Search Task (WEPS) (Artile, Sekine, & Gonzalo,
2008), part SemEval 2007, involved 16 systems trying determine clusters documents
257

fiYates & Etzioni

containing references entity ambiguous person names like Kennedy.
Section 6, show Resolver significantly outperforms Cosine Similarity Metric
clustering experiments. experiments (Section 8) show Resolver
able achieve similar, slightly higher performance Li et al. dataset,
relying resources besides large corpus.
Coreference resolution systems, like synonym resolution systems, try merge references
object, apply arbitrary noun phrases rather named
entities. difficulty general problem, work considered techniques informed parsers (e.g., Lappin & Leass, 1994) training data (e.g., Ng & Cardie,
2002; McCarthy & Lehnert, 1995). Cardie Wagstaff (1999) use set extracted grammatical semantic features ad-hoc clustering algorithm perform unsupervised
coreference resolution, achieving better performance MUC-6 coreference task
supervised system. recently, Haghighi Klein (2007) use graphical model combining local salience features global entity features perform unsupervised coreference,
achieving F1 score 70.1 MUC-6. Two systems use automatically extracted information help make coreference resolution decisions, much like Resolver does. Kehler,
Appelt, Taylor, Simma (2004) use statistics automatically-determined predicateargument structures compare contexts pronouns potential antecedents.
find adding information system relies morpho-syntactic evidence
pronoun resolution provides little benefit. Bean Riloff (2004) use targeted
extraction patterns find semantic constraints relationship pronouns
antecedents, show use improve anaphora-resolution
system. Coreference resolution difficult general task synonym resolution
objects since deals arbitrary types noun phrases. However, systems coreference resolution also information available form local sequence
salience information, lost extraction process, address
relation synonymy.
Synonym resolution relations often called paraphrase discovery paraphrase acquisition NLP literature (e.g., Barzilay & Lee, 2003; Sekine, 2005). Previous work
area (Barzilay & Lee, 2003; Barzilay & McKeown, 2001; Shinyama & Sekine, 2003;
Pang, Knight, & Marcu, 2003) looked use parallel, aligned corpora,
multiple translations text multiple news reports story, find
paraphrases. Brockett Dolan (2005) used manually-labeled data train supervised model paraphrases. PASCAL Recognising Textual Entailment Challenge
(Dagan, Glickman, & Magnini, 2006) proposes task recognizing two sentences
entail one another, given manually labeled training data, many authors submitted
responses challenge. Resolver avoids use labor-intensive resources, relies
solely automatically acquired extractions large corpus.
Several unsupervised systems paraphrase discovery focused using corpusbased techniques cluster synonymous relations. Sekine (2005) uses heuristic similarity
measure cluster relations. Davidov Rappoport (2008) use heuristic clustering
method find groups relation patterns used extract instances. Hasegawa
et al. (2004) automatically extract relationships large corpus cluster relations,
using Cosine Similarity Metric (Salton & McGill, 1983) hierarchical clustering
technique like Resolvers. DIRT system (Lin & Pantel, 2001) uses similarity mea258

fiUnsupervised Methods Determining Object Relation Synonyms

sure based mutual information statistics identify relations similar given
one. Resolver provides formal probabilistic model similarity technique,
applies objects relations. Section 4.3 contains fuller description differences Resolver DIRT, Section 6 describes experiments show
Resolvers superior performance precision recall clustering using mutual
information similarity metric employed DIRT, well Cosine Similarity Metric.
Resolvers method determining similarity two strings example
broad class metrics called distributional similarity metrics (Lee, 1999),
significant advantages traditional distributional similarity metrics synonym
resolution task. metrics based underlying assumption, called
Distributional Hypothesis, Similar objects appear similar contexts. (Hindle, 1990)
Previous distributional similarity metrics, however, designed comparing words
based terms appearing document, rather extracted properties.
two important consequences: first, extracted properties nature sparser
appear narrow window around words consist longer strings
(at least, pairs words); second, extracted shared property provides stronger
evidence synonymy arbitrary word appears together synonym,
extraction mechanism designed find meaningful relationships. Resolvers
metric designed take advantage relational model provided Web Information
Extraction. Section 4.3 fully describes difference Resolvers metric
Cosine Similarity Metric (Salton & McGill, 1983), example traditional
distributional similarity metric. Experiments Section 6 demonstrate Resolver
outperforms Cosine Similarity Metric.
many unsupervised approaches object resolution databases, unlike
algorithm approaches depend known, fixed, generally small schema.
Ravikumar Cohen (2004) present unsupervised approach object resolution using Expectation-Maximization hierarchical graphical model. Several recent approaches leverage domain-specific information heuristics object resolution.
example, many (Dong, Halevy, & Madhavan, 2005; Bhattacharya & Getoor, 2005, 2006)
rely evidence observing strings appear arguments relation
simultaneously (e.g., co-authors publication). useful information
resolving authors citation domain, rare find relations similar properties extracted assertions. None approaches applies problem resolving
relations. Winkler (1999) provides survey area. Several supervised learning techniques make entity resolution decisions (Kehler, 1997; McCallum & Wellner, 2004; Singla
& Domingos, 2006), course systems depend availability training data,
even significant number labeled examples per relation interest.
One promising new approach clustering relational domain Multiple Relational Clusterings (MRC) algorithm (Kok & Domingos, 2007). approach, though
specific synonym resolution, find synonyms set unlabeled, relational extractions without domain-specific heuristics. approach quite recent, far
detailed experimental comparison conducted.
Resolvers probabilistic model partly inspired ball-and-urns abstraction
information extraction presented Downey, Etzioni, Soderland (2005) Resolvers
task probability model different theirs, many modeling as259

fiYates & Etzioni

sumptions (such independence extractions) made cases simplify
derivation models.
Previous work Resolver (Yates & Etzioni, 2007) discussed basic version
probabilistic model initial experimental results. work expands previous work includes new experimental comparison established mutual
information-based similarity metric; new extension basic system (property weighting); full proofs three claims; description fast algorithm calculating
Extracted Shared Property model.

3. Formal Synonym Resolution Problem
synonym resolution system WIE takes set extractions input returns
set clusters, cluster containing synonymous object strings relation strings.
precisely, input data set containing extracted assertions form =
(r, o1 , . . . , ), r relation string oi object string representing
arguments relation. Throughout work, assertions assumed binary,
n = 2.
output synonym resolution system clustering, set clusters,
strings D. Let set distinct strings D. clustering set C 2S
clusters C distinct, cover whole set:
[
=S
cC

c1 , c2 C. c1 c2 =
cluster output clustering constitutes systems conjecture strings
inside cluster synonyms, string outside cluster synonym
string cluster.
3.1 Single-Sense Assumption
formal representation synonym resolution described makes important simplifying assumption: assumed every string belongs exactly one cluster.
language, however, strings often multiple meanings; i.e., polysemous. Polysemous strings cannot adequately represented using clustering string
belongs exactly one cluster. paper, make single-sense assumption, Section 8 illustrates extension Resolver away
assumption.
example representational trouble posed polysemy, consider name
President Roosevelt. certain contexts, name synonymous President
Franklin D. Roosevelt, contexts synonymous President Theodore
Roosevelt. However, President Franklin D. Roosevelt never synonymous President Theodore Roosevelt. clustering three names, using notion
clustering described above, synonymy relationships accurately represented.
Others described alternate kinds clustering take polysemy account.
example, soft clustering allows string assigned many different clusters
260

fiUnsupervised Methods Determining Object Relation Synonyms

senses. One variation idea assign probability distribution every string,
describing prior probability string belongs cluster (Li & Abe, 1998;
Pereira, Tishby, & Lee, 1993). representations capture prior information
strings. is, represent idea particular string belong
cluster, probability belongs cluster, whether particular instance
string actually belong cluster. third type clustering, explicit
representation, stores instance string separately. string instance assigned
cluster appropriate instances context. Word sense disambiguation
systems assign senses WordNet (Miller, Beckwith, Fellbaum, Gross, & Miller.,
1990) implicitly use kind clustering (e.g., Ide & Veronis, 1998; Sinha & Mihalcea,
2007).
3.2 Subproblems Synonym Resolution
synonym resolution problem divided two subproblems: first, measure
similarity, probability synonymy, pairs strings S; second,
form clusters elements cluster high similarity one
another, relatively low similarity elements clusters.
Resolver uses generative, probabilistic model finding similarity
strings. strings si sj , let Ri,j random variable event si
f
denote event R
sj refer entity. Let Ri,j
i,j true, Ri,j denote
event false. Let Dx denote set extractions contain string
|D , )
x. Given S, first subtask synonym resolution find P (Ri,j
si
sj
pairs si sj . second subtask takes probability scores pairs
strings input. output clustering S. Sections 4 5 cover Resolvers
solutions subtask respectively.

4. Models String Comparisons
probabilistic model provides formal, rigorous method resolving synonyms
absence training data. two sources evidence: similarity strings
(i.e., edit distance) similarity assertions appear in.
second source evidence sometimes referred distributional similarity (Hindle, 1990).
Section 4.1 presents simple model predicting whether pair strings synonymous based string similarity. Section 4.2 presents model called Extracted
Shared Property (ESP) Model predicting whether pair strings co-refer based
distributional similarity. Section 4.3 compares ESP model methods
computing distributional similarity give intuition behaves. Finally, Sections 4.4 4.5 present method combining ESP model string similarity
model come overall prediction synonymy decisions two clusters
strings.
4.1 String Similarity Model
Many objects appear multiple names substrings, acronyms, abbreviations,
simple variations one another. Thus string similarity important source
261

fiYates & Etzioni

evidence whether two strings co-refer (Cohen, 1998). Resolvers probabilistic String
Similarity Model (SSM) assumes similarity function sim(s1 , s2 ): ST RIN G ST RIN G
[0, 1]. model sets probability s1 co-referring s2 smoothed version
similarity:
sim(s1 , s2 ) + 1

P (Ri,j
|sim(s1 , s2 )) =
+
increases, probability estimate transitions 1/ (at = 0) value
similarity function (for large ). particular choice make little difference
Resolvers results, long chosen resulting probability
never one zero. experiments below, = 20 = 5. Monge-Elkan string
similarity function (Monge & Elkan, 1996) used objects, Levenshtein string
edit-distance function used relations (Cohen, Ravikumar, & Fienberg, 2003).
4.2 Extracted Shared Property Model
Extracted Shared Property Model (ESP) outputs probability two strings corefer based similarity extracted assertions appear. example,
extractions (invented, Newton, calculus) (invented, Leibniz, calculus)
appeared data, Newton Leibniz would judged similar contexts
extracted data.
formally, let pair strings (r, s) called property object string
assertion (r, o, s) (r, s, o) D. pair strings (s1 , s2 ) instance
relation string r assertion (r, s1 , s2 ) D. Equivalently, property
p = (r, s) applies o, instance = (s1 , s2 ) belongs r. ESP model outputs
probability two strings co-refer based many properties (or instances)
share.
example, consider strings Mars Red Planet, appear data 659
26 times respectively. extracted assertions, share four properties.
example, (lacks, Mars, ozone layer) (lacks, Red Planet, ozone layer) appear
assertions data. ESP model determines probability Mars Red
Planet refer entity observing k, number properties apply
both; n1 , total number extracted properties Mars; n2 , total number
extracted properties Red Planet.
ESP models extraction assertions generative process, much like URNS
model (Downey et al., 2005). string si , certain number, Pi , properties
string written balls placed urn. Extracting ni assertions contain si
amounts selecting subset size ni labeled balls.1 Properties urn
called potential properties distinguish extracted properties.
model synonymy decisions, ESP uses pair urns, containing Pi Pj balls
respectively, two strings si sj . subset Pi balls exact
labels equal-sized subset Pj balls. Let size subset Si,j . Crucially,
ESP model assumes synonymous strings share many potential properties
possible, though potential properties extracted both. non1. Unlike URNS model, balls drawn without replacement. TextRunner data contains
one mention extraction, drawing without replacement tends model data accurately.

262

fiUnsupervised Methods Determining Object Relation Synonyms

synonymous strings, set shared potential properties strict subset potential
properties string. Thus central modeling choice ESP model is: si
) number shared potential properties (S )
sj synonymous (i.e., Ri,j = Ri,j
i,j
equal number potential properties smaller urn (min(Pi , Pj )), two
f
strings synonymous (Ri,j = Ri,j
) number shared potential properties
strictly less number properties smaller urn (Si,j < min(Pi , Pj )).
ESP model makes several simplifying assumptions order make probability
predictions. suggested ball-and-urn abstraction, assumes ball
string equally likely selected urn. data sparsity, almost
properties rare, would difficult get better estimate prior
probability selecting particular potential property. Second, balls drawn one
urn independent draws urn. finally, assumes without knowing
value k, every value Si,j equally likely, since better information.
). derivation
Given assumptions, derive expression P (Ri,j
sketched
below; see Appendix complete derivation. First, note
P Pj
ni nj total ways extracting ni nj assertions si sj . Given particular value
Si,j , number ways ni nj assertions extracted
share exactly k given
Count(k, ni , nj |Pi , Pj , Si,j ) =

Si,j
k

assumptions,

P

r,s0

P (k|ni , nj , Pi , Pj , Si,j ) =

Si,j k
r+s



r+s
r



Pi Si,j Pj Si,j
ni (k+r) nj (k+s)

Count(k, ni , nj |Pi , Pj , Si,j )

Pi P j
ni

(1)

(2)

nj

Let Pmin = min(Pi , Pj ). result follows Bayes Rule assumptions
above:
Proposition 1 two strings si sj Pi Pj potential properties (or instances),
appear extracted assertions Di Dj |Di | = ni |Dj | = nj ,
share k extracted properties (or instances), probability si sj co-refer is:

P (Ri,j
|Di , Dj , Pi , Pj ) =

P (k|ni , nj , Pi , Pj , Si,j = Pmin )
X
P (k|ni , nj , Pi , Pj , Si,j )

(3)

Si,j
kSi,j Pmin

Substituting equation 2 equation 3 gives us complete expression probability
looking for.
depends two hidden parameters, P P . Since
Note probability Ri,j

j
unsupervised synonym resolution labeled data estimate parameters
from, parameters tied number times respective strings si sj
extracted: Pi = N ni . discussion experimental methods Section 6 explains
parameter N set.
Appendix B illustrates technique calculating ESP model efficiently.
263

fiYates & Etzioni

4.3 Comparison ESP Distributional Similarity Metrics
Discovery Inference Rules Text (DIRT) (Lin & Pantel, 2001) system
similar previous work Resolver goals, DIRTs similarity metric
different ESP. Like ESP, DIRT operates triples extracted strings produces
similarity scores relations comparing distributions one relations arguments
anothers. DIRT system, however, extraction mechanism based
dependency parser. focus differences two systems similarity metrics,
compare performance set extracted triples produced TextRunner,
since extracted triples used DIRT available us. refer mutualinformation-based similarity metric employed DIRT system sM . important
note sM describe implementation similarity metric
described Lin Pantel (2001), complete DIRT system.
briefly describe sM applies set extractions. sM originally
applied relation strings, simplicity describe way here,
readily generalized metric computing similarity two argument-1
strings two argument-2 strings. notational convenience, let Dx=s set
extractions contain string position x. example, D2=Einstein would contain
extraction (discovered, Einstein, Relativity), extraction (talked with,
Bohr, Einstein). Similarly, let Dx=s1 ,y=s2 set extractions contain s1
s2 positions x respectively. Finally, let projection set extractions
= {(d1 , d2 , d3 )} onto one dimensions x given by:
projx (D) = {s|d1 ,d2 ,d3 .dx = (d1 , d2 , d3 ) D}
sM uses mutual information score determine much weight give
string set extractions similarity computation. string position
x, mutual information relation r position 1 given by:


|D1=r,x=s | |D|
mi1,x (r, s) = log
|D1=r | |Dx=s |
sM calculates similarity two relations first calculating similarity
sets first arguments relations, similarity
sets second arguments. Let r1 r2 two relations, let position
argument compared x. similarity function used is:
X
mi1,x (r1 , a) + mi1,x (r2 , a)
simx (r1 , r2 ) =

aprojx (D1=r1 )projx (D1=r2 )

X

mi1,x (r1 , a) +

X

mi1,x (r2 , a)

aprojx (D1=r2 )

aprojx (D1=r1 )

final similarity score two relations geometric average similarity scores
argument:
p
sM (r1 , r2 ) = sim2 (r1 , r2 ) sim3 (r1 , r2 )
(4)
Applying sM metric entities rather relations simply requires projecting onto
different dimensions relevant tuple sets.
264

fiUnsupervised Methods Determining Object Relation Synonyms

significant difference sM similarity metric ESP model
sM metric compares x arguments one relation x arguments
other, compares arguments one relation arguments other,
finally combines scores. contrast, ESP compares (x, y) argument pairs one
relation (x, y) pairs other. sM metric advantage
likely find matches two relations sparse data, disadvantage
matches find necessarily strong evidence synonymy. effect,
capturing intuition synonyms argument types domains
ranges, certainly possible non-synonyms similar domains ranges.
Antonyms obvious example. Synonyms defined domains ranges,
rather mapping them, ESP better captures similarity
mapping. Experiments (Section 6) compare ESP similarity metric
sM , given Equation 4.
previously mentioned, large body previous work similarity metrics
(e.g., Lee, 1999). compare ESP one popular metrics,
Cosine Similarity Metric (CSM), previously used synonym resolution
work (Mann & Yarowsky, 2003; Hasegawa et al., 2004). Like traditional distributional
similarity metrics, CSM operates context vectors, rather extracted triples. However, ESP model similar CSM regard. extracted string,
effect creates binary vector properties, ones representing properties apply
string zeros representing not. example, string Einstein would
context vector one position property (discovered, Relativity),
zero position property (invented, light bulb). ESP CSM
calculate similarities comparing vectors.
specific metric used compute CSM two vectors ~x ~y given by:
simCSM (~x, ~y ) =
=

~x ~y
||~x|| ||~y ||
P
yi
xiq
qP
P 2
2
xi
yi

Often, techniques like term weighting TFIDF (Salton & McGill, 1983) used CSM
create vectors boolean, rather dimensions different weights
according informative dimensions are. experimented TFIDF-like
weighting schemes, number times extraction extracted used
term frequency, number different strings property applies used
document frequency. However, found weighting schemes negative effects
performance, ignore them. two boolean vectors, CSM reduces
simple computation number shared properties k number extractions
string, n1 n2 respectively. given by:
simCSM boolean (~x, ~y ) =

k
n1 n2

(5)

CSM determines similar two context vectors dimension, adds
scores weighted sum. contrast, ESP highly non-linear number
265

fiYates & Etzioni

Similarity

shared properties. number matching contexts grows, weight
additional matching context also grows. Figure 1 compares behavior ESP CSM
number shared properties two strings increases. Holding number
extractions fixed assuming boolean vectors CSM, behaves linear function
number shared properties. hand, ESP shape
thresholding function: low value threshold point around k = 10,
point probability estimate starts increasing rapidly. effect ESP
much lower similarity scores CSM small numbers matching contexts, much
higher scores larger numbers matching contexts. threshold switches
depends n1 n2 , well P1 P2 , show experimentally
method estimating P1 P2 , though simple, effective. Experiments Section
6 compare ESP model CSM, computed using Equation 5.

CSM
ESP

0

5

10

15

20

Number shared properties

Figure 1: behavior Extracted Shared Property (ESP) model Cosine
Similarity Model (CSM) number shared properties two
strings varies. graph shows similarity results using two hypothetical strings
20 extracted properties each. ESP, property multiple N = 2. removed
scale axis, since scales two metrics directly comparable,
shape curves remains same.

4.4 Combining Evidence
potential synonymy relationship, Resolver considers two pieces probabilistic
e evidence ESP, let E evidence SSM.
evidence. Let Ei,j
i,j
method combining two uses Nave Bayes assumption piece evidence
266

fiUnsupervised Methods Determining Object Relation Synonyms

conditionally independent, given synonymy relationship:

e

e
P (Ei,j
, Ei,j
|Ri,j ) = P (Ei,j
|Ri,j )P (Ei,j
|Ri,j )

(6)

Given simplifying assumption, combine evidence find probability
coreference relationship applying Bayes Rule sides (we omit i, j indices
brevity):
P (Rt |E )P (Rt |E e )(1 P (Rt ))
(7)
P (Rt |E , E e ) = P



e

i{t,f } P (R |E )P (R |E )(1 P (R ))
4.5 Comparing Clusters Strings

algorithm merges clusters strings one another, using models. However,
models give probabilities synonymy decisions two individual strings,
two clusters strings.
experimented several different methods determining probability
synonymy individual probability scores pair strings, one taken
cluster. Initially, followed work Snow, Jurafsky, Ng (2006) incorporating
transitive closure constraints probabilistic modeling, made independence
assumptions. approach provides formal probabilistic framework problem
simple efficient calculate. experiments, found simply taking
mean geometric mean (or even harmonic mean) string pair scores provided
slightly improved results. completeness, provide brief explanation
probabilistic method combining string pair scores cluster pair scores.
Let clustering set synonymy relationships pairs strings
synonymy relationships obey transitive closure property. let probability
set assertions given clustering C be:
P (D|C) =



C
Ri,j


P (Di Dj |Ri,j
)



f
Ri,j
C

f
)
P (Di Dj |Ri,j

(8)

metric used determine two clusters merged likelihood ratio,
probability set assertions given merged clusters probability given
original clustering. Let C 0 clustering differs C two clusters
C merged C 0 , let C set synonymy relationships C 0
true, corresponding ones C false. metric given by:
0

Q

C
Ri,j

P (D|C )/P (D|C) = Q

|D )(1 P (Rt ))
P (Ri,j

j
i,j

C (1
Ri,j

|D ))P (Rt )
P (Ri,j

j
i,j

(9)

|D ) may supplied SSM, ESP, combination
probability P (Ri,j

j
model. experiments, let prior SSM model 0.5. ESP
1
)=
combined models, set prior P (Ri,j
min(Pi ,Pj ) , Pi Pj number
potential properties si sj respectively.

267

fiYates & Etzioni

5. Resolvers Clustering Algorithm
Synonym resolution Web requires clustering algorithm scale huge
number strings sparse, high-dimensional space. requirements difficult
clustering algorithm. hand, words handful
synonyms, clusters tend quite small. Greedy agglomerative approaches wellsuited type clustering problem, since start smallest possible clusters
merge needed.
Resolver clustering algorithm version greedy agglomerative clustering,
key modification allows scale sparse, high-dimensional spaces huge
numbers elements. standard greedy clustering algorithm begins comparing pair
data points, greedily merges closest pair. biggest hurdle scaling
algorithm initial step comparing every pair data points requires O(N 2 )
comparisons N points. Several proposed techniques able speed
process practice filtering initial pairs points compared; build
work provide novel technique new bound O(N log N ) comparisons,
mild assumptions.
algorithm outlined Figure 2. begins calculating similarity scores
pairs strings, steps 1-4. scores sorted best cluster pairs merged
pair clusters score threshold. novel part algorithm, step
4, compares pairs clusters share property, long ax
clusters share property. step limits number comparisons made
clusters, reason algorithms improved efficiency, explained below.
algorithm compares every pair clusters potential merged,
assuming two properties data. First, assumes pairs clusters shared
properties worth comparing. Since number shared properties key source
evidence approach, clusters almost certainly merged, even
compared, assumption quite reasonable. Second, approach assumes
clusters sharing properties apply many strings (at least ax) need
compared. Since properties shared many strings provide little evidence
strings synonymous, assumption reasonable synonym resolution.
use ax = 50 experiments. Less 0.1% distinct properties
thrown using cutoff, discarded properties apply many strings
(at least ax), number comparisons grows square
number strings property applies to, restriction drastically cuts
total number comparisons made. Table 1 shows number comparisons made
nave method comparing pairs strings set 2 million extractions,
number comparisons Resolver makes experiments. algorithm
achieves reduction factor 136 objects 486 relations number
comparisons made. unoptimized implementation Resolver able cluster
strings extractions approximately 30 minutes. Resolver also run
larger set containing 100 million extractions 1 million distinct strings,
able cluster approximately 3.5 days single machine.
268

fiUnsupervised Methods Determining Object Relation Synonyms

E := {e = (r, a, b)|(r, a, b) extracted assertion}
:= {s|s appears relation argument string E}
Cluster := {}
Elements := {}
1. S:
Cluster[s] := new cluster id
Elements[Cluster[s]] := {s}
2. Scores := {}, Index := {}
3. e = (r, a, b) E:
property := (a, b)
Index[property] := Index[property] {Cluster[r]}
property := (r, a)
Index[property] := Index[property] {Cluster[b]}
property := (r, b)
Index[property] := Index[property] {Cluster[a]}
4. property p Index:
|Index[p]| < Max:
pair {c1 , c2 } Index[p]:
Scores[{c1 , c2 }] := similarity(c1 , c2 )
5. Repeat merges performed:
Sort Scores
U sedClusters := {}
Repeat Scores empty top score < hreshold:
{c1 , c2 } := removeT opP air(Scores)
neither c1 c2 U sedClusters:
Elements[c1 ] := Elements[c1 ] Elements[c2 ]
e Elements[c2 ]:
Cluster[e] := c1
delete c2 Elements
U sedClusters := U sedClusters {c1 , c2 }
Repeat steps 2-4 recalculate Scores
Figure 2: Resolvers Clustering Algorithm
5.1 Algorithm Analysis
Let set extracted assertions. following analysis2 shows one iteration
merges takes time O(|D| log |D|). Let N C number comparisons strings
step 4. simplify analysis, consider properties contain relation
string argument 1 string. Let P roperties set properties
apply fewer ax strings, let Stringsp set strings particular
2. ax parameter allowed vary log |D|, rather remaining constant, analysis
leads slightly looser bound still better O(|D|2 ).

269

fiYates & Etzioni

Num. Strings

Compare

Resolver

Speedup

9,797
10,151

47,985,706
51,516,325

352,177
105,915

136x
486x

Objects
Relations

Table 1: Resolvers clustering algorithm cuts number comparisons made
pairs strings clustering data set 2.1 million TextRunner
extractions. Compare lists number comparisons would
made every string compared every one. Resolver reduces comparisons object strings factor 136 compared baseline,
comparisons relations strings factor 486.

property p applies to. number comparisons given size union
set comparisons made property, upper-bounded sum
maximum number comparisons made property:
fi
fi
fi
fi
[
fi
fi
fi
{pair = {s1 , s2 }|pair Stringsp }fifi
NC = fi
fi
fipP roperties
X

|{pair = {s1 , s2 }|pair Stringsp }|
pP roperties

X

=

pP roperties

|Stringsp | (|Stringsp | 1)
2

Since Stringsp contains ax elements, upper-bound expression

NC

X

pP roperties

|Stringsp | (M ax 1)
2

=

(M ax 1)

2



(M ax 1)
|D|
2

X

pP roperties

|Stringsp |

P
last step bounds p |Stringsp | |D|, since number
P extractions equal
number times property extracted. Since p |Stringsp | summing
properties apply fewer ax strings, |D| may greater sum.
Overall, analysis shows N C linear |D|. Note general bound
quite loose properties apply small number strings, far fewer
ax.
Step 5 requires time O(|D| log |D|) sort comparison scores perform one iteration merges. largest cluster size K, worst case algorithm take
270

fiUnsupervised Methods Determining Object Relation Synonyms

K iterations (and best case take log K). experiments, algorithm
never took 9 iterations.
analysis thus far related computational complexity |D|, size
input data set extractions. existing techniques, however, analyzed
terms |S|, number distinct strings clustered. order relate two
kinds analysis, observe linguistic data naturally obeys Zipf distribution
frequency distinct strings. is, common
string appears many times
1 z
extractions; next-most common appears roughly
times often parameter
2

1 z
z; next common appears roughly 3 times often; on. parameter z
known Zipf parameter, naturally-occurring text typically observed
around 1 (Zipf, 1932; Manning & Schuetze, 1999). characterize Zipf
distribution input data set extractions, rewrite
P number extractions
|D| terms number distinct strings |S|, since |D| = sS frequency(s). Following
line thought conclusion, find z < 1, data set,
|D| grows linearly |S|, complexity O(|D| log |D|) equivalent complexity
O(|S| log |S|). z = 1, O(|D| log |D|) equivalent bound O(|S| log2 |S|).
z > 1, bound O(|S|z log |S|). z = 2 asymptotic bound
O(|S|2 log |S|) worse O(|S|2 ) bound comparing string pairs, high
value z highly unlikely naturally occurring text. details complete
analysis, see Appendix C.
5.2 Relation Speed-Up Techniques
McCallum, Nigam, Ungar (2000) proposed widely-used technique pre-processing
data set reduce number comparisons made clustering. use cheap
comparison metric place objects overlapping canopies, use expensive metric cluster objects appearing canopy. Resolver clustering
algorithm fact adaptation canopy method: like Canopies method,
uses index eliminate many comparisons would otherwise need made.
method adds restriction strings compared share highfrequency properties. Canopy method works well high-dimensional data many
clusters, case problem. contribution observe
restrict comparisons novel well-justified way, obtain new theoretical
bound complexity clustering text data.
merge/purge algorithm (Hernandez & Stolfo, 1995) assumes existence
particular attribute data set sorted attribute, matching pairs
appear within narrow window one another. algorithm O(M log )
number distinct strings. However, attribute set attributes
comes close satisfying assumption context domain-independent information
extraction.
Resolvers clustering task part reduced task nearest-neighbor search,
several recent systems developed fast new algorithms. reduction works
follows: nearest-neighbor retrieval techniques used find similar
string every distinct string corpus, Resolvers merge criteria decide
pairs actually merge. Several fastest nearest-neighbor techniques
271

fiYates & Etzioni

perform approximate nearest-neighbor search: given error tolerance , techniques
return neighbor query node q 1 + times far q
true nearest neighbor q.
Examples nearest-neighbor techniques divided use hash-based
tree-based indexing schemes. Locality-Sensitive Hashing uses combination hashing
1
functions retrieve approximate nearest neighbors time O(n 1+ ) error tolerance .
given query point q willing accept neighbors distance
twice distance true nearest neighbor ( = 2), running time

1
O(n 2 ) = O( n) find single nearest neighbor (Gionis, Indyk, & Motwani, 1999).
recently, tree-based index structures metric cover trees (Beygelzimer, Kakade,
& Langford, 2006) hybrid spill trees (Liu, Moore, Gray, & Yang, 2004), offered
competitive even better performance Locality-Sensitive Hashing. tree-based
algorithms complexity O(d log n), dimensionality space, find
single nearest neighbor. Metric trees offer exact nearest-neighbor search, spill trees
offer faster search practice cost finding approximate solutions using
space index. indexing schemes powerful tools nearest-neighbor search,
dependence dimensionality space makes costly apply
case. Resolver operates space hundreds thousands dimensions (the number
distinct extract properties), fastest techniques applied
spaces around thousand dimensions (Liu et al., 2004). Resolver determines
exact nearest neighbor, fact exact distance relevant pairs points
mild assumptions stated above, operating huge-dimensional space.

5.3 Resolver Implementation
Resolver currently exists Java package containing 23,338 lines code. separate
modules calculating Extracted Shared Property Model String Similarity
Model, well clustering extractions. basic version system accepts file
containing tuples strings input, one tuple per line. Optionally, accepts manually
labeled clusters input well, use output precision recall scores.
output system two files containing object clusters relation clusters size
two more, respectively. Optionally, system also outputs precision recall scores.
Several options allow user run extensions basic Resolver system,
discussed Section 7.
Resolver currently part TextRunner demonstration system. demonstration system available keyword searches Web
http://www.cs.washington.edu/research/textrunner/. demonstration system contains
extractions several hundred million Web documents. extractions fed
Resolver resulting clusters added TextRunner index keyword searches return results member cluster containing keyword
searched for, displayed results condensed members cluster
repeated.
272

fiUnsupervised Methods Determining Object Relation Synonyms

6. Experiments
Several experiments test Resolver ESP, demonstrate improvement
related techniques paraphrase discovery, sM (Lin & Pantel, 2001) Cosine
Similarity Metric (CSM) (Salton & McGill, 1983; Hasegawa et al., 2004; Mann & Yarowsky,
2003). first experiment compares performance various similarity metrics,
shows Resolvers output clusters significantly better ESPs SSMs,
ESPs clusters turn significantly better sM CSMs. second
experiment measures sensitivity ESP model hidden parameter, shows
wide range parameter settings, able outperform sM
CSM models.
6.1 Experimental Setup
models tested data set 2.1 million assertions extracted Web crawl.
models run assertions, compare objects relations appear
least 25 times data, give distributional similarity models sufficient data
estimating similarity. Although restriction limits applicability Resolver,
note intuitive necessary unsupervised clustering, since
systems definition start knowledge string. must see number
examples reasonable expect make decisions them. also note
Downey, Schoenmackers, Etzioni (2007) shown different problem
bootstrapping techniques leverage performance high-frequency examples build
accurate models low-frequency items.
proper nouns3 compared, relation strings contain punctuation capital letters compared. helps restrict experiment strings
less prone extraction errors. However, models use strings
features. all, data contains 9,797 distinct proper object strings 10,151 distinct
proper relation strings appear least 25 times. created gold standard data set
manually clustering subset 6,000 object 2,000 relation strings. total, gold
standard data sets contains 318 true object clusters 330 true relation clusters
least 2 elements each.
noted previously (Section 3.1), polysemous strings pose particular representational
trouble creating gold standard data set, since correct clustering captures synonymy relationships polysemous strings, general. adopted
following data-oriented strategy: polysemous strings clustered strings
unless match every sense strings appeared data.
example, two U.S. Presidents named Roosevelt: Theodore Roosevelt
Franklin Delano Roosevelt. applying criterion above, gold standard data contained cluster FDR President Franklin Roosevelt, since referred Franklin
Delano Roosevelt unambiguously dataset. Likewise, President Theodore Roosevelt
Teddy Roosevelt put cluster. terms Roosevelt President
Roosevelt, however, used various places refer men, could
3. following heuristic used detect proper nouns: string consisted alphabetic
characters, whitespace, periods, first character every word capitalized, considered
proper noun. Otherwise, not.

273

fiYates & Etzioni

clustered either Franklin Roosevelt cluster Theodore Roosevelt cluster.
Since set senses data, gold standard contained separate
cluster containing two strings. Section 8.2 describes extension Resolver
handles polysemous names. criterion polysemy prevented 480 potential merges
gold standard data set object clusters might synonymous. prevented merges usually affected acronyms, first names, words like Agency might
refer number institutions, represent less 10% strings gold
standard object data set.
addition gold standard data set evaluation, manually created data
set development data containing 5 correct pairs objects, 5 correct pairs relations,
also 5 examples incorrect pairs each. 20 examples used
evaluation data. development data used estimate value ESP models
hidden parameter N , called property multiple (see Section 4.2). used simple hillclimbing search procedure find value N separately objects relations,
found N = 30 worked best objects development data, N = 500 relations.
Although amount data required set parameter effectively small,
nevertheless important topic future work come method estimate
parameter completely unsupervised manner order fully automate Resolver.
comparisons, calculated Cosine Similarity Metric (CSM) using technique described Section 4.3 Equation 5, sM metric defined Equation
4.
6.2 Clustering Analysis
first experiment compares precision recall clusterings output five similarity
metrics: two kinds previous work used paraphrase discovery, CSM sM ; two
components Resolver, ESP SSM; full Resolver system.
precision recall clustering measured follows: hypothesis clusters
matched gold clusters hypothesis cluster matches one
gold cluster, vice versa. mapping computed number elements
hypothesis clusters intersect elements matching gold clusters maximized.
intersecting elements marked correct. elements hypothesis cluster
intersect corresponding gold cluster marked incorrect, irrelevant
appear gold clustering all. Likewise, gold cluster elements
marked found matching hypothesis cluster contains element, found
otherwise. precision defined number correct hypothesis elements clusters
containing least two relevant (correct incorrect) elements, divided total number
relevant hypothesis elements clusters containing least two relevant items. recall
defined number found gold elements gold clusters size least two, divided
total number gold elements clusters size least two. consider
clusters size two order focus interesting cases.
model requires threshold parameter determine scores suitable
merging. experiments arbitrarily chose threshold 3 ESP model
(that is, data needs 3 times likely given merged cluster unmerged
clusters order perform merge) chose thresholds models hand
274

fiUnsupervised Methods Determining Object Relation Synonyms

Objects
Model
CSM
sM
ESP
SSM
Resolver

Prec.
0.51
0.52
0.56
0.62
0.71

Rec.
0.36
0.38
0.41
0.53
0.66

Relations
F1
0.42
0.44
0.47
0.57
0.68

Prec.
0.62
0.61
0.79
0.85
0.90

Rec.
0.29
0.28
0.33
0.25
0.35

F1
0.40
0.38
0.47
0.39
0.50

Table 2: Comparison cosine similarity metric (CSM), sM , Resolver components
(SSM ESP), Resolver system. Bold indicates score significantly
different score row p < 0.05 using chi-squared test one
degree freedom. Using test, Resolver also significantly different ESP,
sM , CSM recall objects, sM , CSM SSM recall relations.
Resolvers F1 objects 19% increase SSMs F1. Resolvers F1 relations
28% increase SSMs F1. significance tests performed F1 values.

difference ESP would roughly even precision
recall, although relations harder improve recall. Table 2 shows precision
recall models.
6.3 Sensitivity Analysis
ESP model requires parameter number potential properties string,
performance ESP strongly sensitive exact value parameter.
described Section 4.2, assume number potential properties multiple
N number extractions string. experiments, chose values
N = 30 objects N = 500 relations, since worked well held-out data.
However, Tables 3 4 show, actual values parameters may vary large
range, still enabling ESP outperform sM CSM.
experiments, measured precision recall similarity metrics,
without performing clustering. used similarity metrics sort pairs strings
(but pairs share least property) descending order similarity.
place threshold similarity, measure precision number
correct synonym pairs similarity greater divided total number pairs
similarity greater . measure recall number correct synonym pairs
similarity greater divided total number correct synonym pairs.
varying , create precision-recall curve measure area underneath
curve.
tables highlight two significant results. First, objects relations
ESP model outperforms CSM sM large amount parameter settings vary
close factor two either direction value determined development
data. Thus although required small amount data determine value
parameter, performance ESP overly sensitive exact value. Second,
275

fiYates & Etzioni

Metric

AUC

Fraction Max. AUC

Improvement Baseline

CSM
sM
ESP-10
ESP-30
ESP-50
ESP-90
SSM
Resolver

0.0061
0.0083
0.019
0.024
0.022
0.018
0.18
0.22

0.011
0.014
0.033
0.041
0.037
0.031
0.31
0.38

-21%
0%
136%
193%
164%
121%
0%
23%

Table 3: Area precision-recall Curve (AUC) object synonymy. ESP
model significantly outperforms sM CSM AUC wide range parameter settings. Likewise, Resolver significantly outperforms SSM AUC.
maximum possible AUC less one many correct string pairs share
properties, therefore compared clustering algorithm. third column
shows score fraction maximum possible area curve,
objects 0.57. improvement baseline column shows much ESP curves
improve sM , much Resolver improves SSM.

Metric

AUC

Fraction Max. AUC

Improvement Baseline

CSM
sM
ESP-50
ESP-250
ESP-500
ESP-900
SSM
Resolver

0.0035
0.0044
0.0048
0.0087
0.0096
0.010
0.022
0.029

0.034
0.042
0.046
0.083
0.093
0.098
0.24
0.31

-19%
0%
9.5%
98%
121%
133%
0%
31%

Table 4: Area precision-recall Curve (AUC) relation synonymy.
ESP model significantly outperforms sM CSM AUC wide range
parameter settings. Likewise, Resolver significantly outperforms SSM
AUC. maximum possible area less one many correct string pairs
share properties, therefore compared clustering algorithm.
third column shows score fraction maximum possible area curve,
relations 0.094. improvement baseline shows much ESP
curves improve sM , much Resolver improves SSM.

276

fiUnsupervised Methods Determining Object Relation Synonyms

ESP model clearly provides significant boost performance SSM model,
Resolvers performance significantly improves SSMs.
6.4 Discussion
experiments, ESP outperforms CSM sM . sensitivity analysis shows
remains true wide range hidden parameters ESP, objects
relations. Moreover, ESPs improvement comparison metrics holds true
metrics used clustering data. sM performance largely CSM
every experiment. Somewhat surprisingly, sM performs worse relation clustering
object clustering, even though designed relation similarity.
results show three distributional similarity models perform SSM
model objects relations, similarity experiments
clustering experiments. one exception clustering experiment relations,
SSM poor recall, thus lower F1 score ESP CSM.
expected, since ESP, sM , CSM make predictions based noisy signal.
example, Canada shares properties United States data U.S. does,
even though Canada appears less often U.S. Importantly, though, significant
improvement precision recall using combined model using SSM
alone. Resolvers F1 19% higher SSMs objects, 28% higher relations
clustering experiments.
Interestingly, distributional similarity metrics (ESP, sM , CSM) perform significantly worse task ranking string pairs clustering task. One reason
task ranking string pairs measure performance comparing
cluster two strings cluster two strings. greedy clustering process
one used Resolver, large groups correct clusters formed long
similarity metrics rank correct pair strings near top, able
improve estimates similarity comparing clusters. issue requires
investigation.
clearly room improvement synonym resolution task. Error analysis
shows Resolvers mistakes due three kinds errors:
1. Extraction errors. example, US News gets extracted separately World Report,
Resolver clusters together share almost
properties.
2. Similarity vs. Identity. example, Larry Page Sergey Brin get merged,
Angelina Jolie Brad Pitt, Asia Africa.
3. Multiple word senses. example, two President Bushes; also,
many terms like President Army refer multiple distinct entities.
Extraction systems improving accuracy time, address
errors. next two sections develop techniques address second third
kinds errors, respectively.
277

fiYates & Etzioni

7. Similar Identical Pairs
error analysis suggests, similar objects exact synonyms make
large fraction Resolvers errors. section describes three techniques dealing
errors.
example, Resolver likely make mistake pair Virginia West
Virginia. share many properties type (U.S. states),
high string similarity. Perhaps easiest approach determining
two synonymous simply collect data them. highly
similar, certainly share properties; different governors,
example. However, highly similar pairs two, amount data required
decide identical may huge, simply unavailable.
Fortunately, sophisticated techniques making decisions available data. One approach consider distribution words occur candidate synonyms. Similar words likely separated conjunctions (e.g., Virginia
West Virginia) domain-specific relations hold two objects
type (e.g., Virginia larger West Virginia). hand, synonyms
likely separated highly specialized phrases a.k.a. Section 7.1
describes method using information distinguish similar identical
pairs.
second approach consider candidate synonyms behave context
relations special distributions, like functions inverse functions. example,
x capital relation inverse function: every argument one x
argument4 . capitals extracted West Virginia Virginia, may
ruled synonymous pair capitals seen different.
hand, Virginia VA share capital, much stronger evidence two
shared random property, town called
Springfield located there. Section 7.2 describes method eliminating similar pairs
different values function inverse function, Section
7.3 illustrates technique assigning different weights different evidence based
close functional property is. Section 7.4 gives results techniques.
7.1 Web Hitcounts Synonym Discovery
names two similar objects may often appear together sentence,
relatively rare two different names object appear sentence.
Moreover, synonymous pairs tend appear idiosyncratic contexts quite different
contexts seen similar pairs. Resolver exploits fact querying
Web determine often pair strings appears together certain contexts large
corpus. hitcount high, Resolver prevent merge.
Specifically, given candidate synonym pair s1 s2 , Coordination-Phrase Filter
uses discriminator phrase (Etzioni et al., 2005) form s1 s2 . computes
4. also function.

278

fiUnsupervised Methods Determining Object Relation Synonyms

variant pointwise mutual information, given
coordination score(s1 , s2 ) =

hits(s1 s2 )2
hits(s1 ) hits(s2 )

filter removes consideration candidate pair coordination score
threshold, determined small development set. results
coordination-phrase filtering presented below.
Coordination-Phrase Filter uses one possible context candidate synonym pairs. simple extension use multiple discriminator phrases include common context phrases like unlike. complex approach could measure
distribution words found candidate pair, compare distribution
distributions found known similar known identical pairs.
important avenues investigation.
One drawback approach requires text containing pair objects
close proximity. pair rare strings, data extremely unlikely occur
type test exacerbates data sparsity problem. following two sections describe
two techniques suffer particular problem.
7.2 Function Filtering
Functions inverse functions help distinguish similar identical pairs.
example, Virginia West Virginia different capitals: respectively, Richmond
Charleston. facts extracted, Resolver knows
capital relation inverse function, ought prevent Virginia West Virginia
merging.
Given candidate synonym pair x1 x2 , Function Filter prevents merges
strings different values function. precisely, decides two
strings y1 y2 match string similarity high threshold. prevents
merge x1 x2 exists function f extractions f (x1 , y1 ) f (x2 , y2 ),
extractions y1 y2 match (and vice versa inverse
functions). Experiments described Section 7.4 show Function Filter improve
precision Resolver without significantly affecting recall.
Function Filter requires knowledge relations actually functions
inverse functions. Others investigated techniques determining properties
relations automatically (Popescu, 2007); experiments, pre-defined list functions
used. Table 5 lists set functions used experiments Function Filter.
functions selected manually inspecting set 500 common relations
TextRunners extractions, selecting reliably functional.
met criteria, partly polysemy data, partly extraction
noise.
7.3 Function Weighting
Function Filter uses functions inverse functions negative evidence,
also possible use positive evidence. example, relation married
strictly one-to-one, people set spouses small. pair
279

fiYates & Etzioni

capital
named
headquartered
born

capital city
named
headquartered
born

Table 5: set functions used Function Filter.

strings extracted spousee.g., FDR President Roosevelt share
property (married, Eleanor Roosevelt)this far stronger evidence two strings
identical shared random property, (spoke to, reporters).
several possibilities incorporating insight Resolver. First,
technique need method estimating function-ness property,
close property functional. define degree relation
number values expected hold true given x value. call property
high-degree expected apply many strings (highly non-functional), low-degree
expected apply strings (close functional).
degree property may estimated relation involved property
set extractions relation, may based many objects
property applies to. example, 100 unique extractions married
relation, 80 unique x argument strings 100 extractions,
average x string participates 100/80 = 1.25 married relations. One method might
assign every property containing married relation statistic degree.
hand, suppose two extractions property (married, John Smith).
second method assign degree 2 property.
also two possible ways incorporate degree information ESP
model. ESP model may altered directly models degrees properties process selecting balls urns, vastly complicates model
may make much computationally expensive. second option reweight
number shared properties strings based TF-IDF style weighting
properties, calculate ESP model using parameter instead. requires modifying ESP model handle non-integer values number shared
properties.
experiments far, one set options explored, others remain
future investigation. Weighted Extracted Shared Property Model (W-ESP) sets
degree property number extractions property. Second, strings
si sj share properties p P , sets value number shared properties
si sj
X

pP

1
degree(p)

ESP model changed handle continuous values number shared
properties changing factorials gamma functions, using Stirlings approximation
whenever possible.
280

fiUnsupervised Methods Determining Object Relation Synonyms

Model

Prec.

Rec.

F1

Resolver
Resolver
Resolver
Resolver
Resolver

0.71
0.74
0.78
0.71
0.78

0.66
0.66
0.68
0.65
0.68

0.68
0.70
0.73
0.68
0.73

+
+
+
+

Function Filtering
Coordination Phrase Filtering
Weighted ESP
Function Coord. Phrase Filtering

Table 6: Comparison object merging results Resolver system, Resolver plus
Function Filtering, Resolver plus Coordination-Phrase Filtering, Resolver
using Weighted Extracted Shared Property Model, Resolver plus
types filtering. Bold indicates score significantly different Resolvers
score p < 0.05 using chi-squared test one degree freedom. Resolver+
Coordination Phrase Filterings F1 objects 28% increase SSMs F1, 7%
increase Resolvers F1.

Unlike Function Filter, W-ESP model require additional knowledge
relations functional. unlike Coordination-Phrase Filter,
require Web hitcounts training phase. works extracted data, is.
7.4 Experiments
extensions Resolver attempt address confusion similar identical
pairs. Experiments extensions, using datasets metrics Section 6
demonstrate Function Filter (FF) Coordination-Phrase Filter (CPF) boost
Resolvers precision. Unfortunately, W-ESP model yielded essentially improvement
Resolver.
Table 6 contains results experiments. coordination-phrase filtering,
Resolvers F1 28% higher SSMs objects, 6% higher Resolvers F1
without filtering. function filtering promising idea, FF provides smaller benefit
CPF dataset, merges prevents are, exceptions,
subset merges prevented CPF. part due limited number
functions available data.
Function Filter Coordination-Phrase Filter consistently blocked merges
highly similar countries, continents, planets, people data, well
smaller classes. biggest difference CPF consistently hitcounts
similar pairs tend confused identical pairs. Perhaps amount
extracted data grows, functions extractions functions extracted,
allowing Function Filter improve.
Part appeal W-ESP model requires none additional inputs
two models require, applies property, rather subset
relations like Function Filter. Like TFIDF weighting Cosine Similarity
Metric, W-ESP model uses information distribution properties
data weight property. data extracted TextRunner, neither W-ESP
TFIDF weighting seems positive effect. experiments required test
281

fiYates & Etzioni

whether W-ESP might prove beneficial data sets TFIDF
positive effect.

8. Resolver Cross-Document Entity Resolution
point, made single-sense assumption, assumption every
token exactly one meaning. assumption defensible small domains,
named entities relations rarely multiple meanings, even cause
problems: example, names Clinton Bush refer two major players
American politics, well host people. extractions taken
multiple domains, assumption becomes problematic.
describe refinement Resolver system handles task CrossDocument Entity Resolution (Bagga & Baldwin, 1998), tokens names may
multiple referents, depending context. experiment compares Resolver
existing entity resolution system (Li et al., 2004a), demonstrates Resolver
handle polysemous named entities high accuracy. extension could theoretically
applied highly polysemous tokens common nouns, yet
empirically demonstrated.
8.1 Clustering Polysemous Names Resolver
Recall synonym resolution task defined finding clusters set distinct
strings found set extractions (Section 3). Cross-Document Entity Resolution
differs synonym resolution requires clustering set string
occurrences, rather set distinct strings. example, suppose document
contains two occurrences token DP, one means Design Pattern one
means Dynamic Programming. Synonym resolution systems treat DP single
item, implicitly cluster occurrences DP together. Cross-Document Entity
Resolution system treats occurrence DP separately, therefore potential
put occurrence separate cluster mean different things. way,
Cross-Document Entity Resolution system potential handle polysemous names
correctly.
change task definition, sources evidence similarity sparser.
occurrence named entity input, Resolver single TextRunner extraction describing occurrence. achieve reasonable performance, needs
information context named entity appears. change Resolvers representation entity occurrences include nearest E named entities
text surrounding occurrence. is, entity occurrence x represented
set named entities y, appears among nearest E entities text surrounding x. Suppose, instance, e1 occurrence DP Bellman Viterbi
context, e2 another occurrence OOPSLA Factory context. e1
would represented set {Bellman, Viterbi}, e2 would represented set
{Factory, OOPSLA}.
Table 7 summarizes major ways extended Resolver handle polysemous names. extensions place, Resolver proceed cluster occurrences
entities less way clusters entity names synonym resolution.
282

fiUnsupervised Methods Determining Object Relation Synonyms

Original Resolver

Extended Resolver

Input

set distinct strings S,
S, set extracted properties s.

SSM Compares
ESP Compares
Output

Character sequences
Sets extracted properties
clustering set distinct strings

bag string occurrences O,
occurrence O,
set named entities appearing close context.
Character sequences
Sets named entities
clustering set string
occurrences

Table 7: differences original Resolver system extended Resolver system handling polysemous names.

SSM model works above, ESP model calculates probabilities coreference
based sets named entities context rather extracted properties. clustering
algorithm eliminates comparisons occurrences share part contexts,
common contextual elements. end, Resolver produces sets coreferential entity occurrences, used annotate extractions containing entity
occurrences coreference relationships.
8.2 Experiment Cross-Document Entity Resolution
tested Resolvers ability handle polysemous names data set 300 documents 1998-2000 New York Times articles TREC corpus (Voorhees, 2002). Li
et al. (2004b) automatically ran named-entity tagger documents manually
corrected identify approximately 4,000 occurrences peoples names.
manually annotated occurrences form gold standard set coreferential clusters.
named entity occurrence data set, extracted set closest E
named entities, E set 100, represent context named entity occurrence.
ran Resolver cluster entity occurrences. set ESPs latent parameter N
30, experiments above. development data set merge
threshold, used following strategy: arbitrarily picked single occurrence
common name data set (Al Gore), found somewhat uncommon variant
name (Vice President Al Gore), set threshold value similarity
score pair (7.5). every round merging Resolvers clustering algorithm,
filtered top 20 proposed merges using Coordination Phrase Filter,
threshold used previous experiments.
Li et al. propose generative model entity coreference compare against.
model requires databases information titles, first names, last names, genders, nicknames, common transformations attributes peoples names help compute
probability coreference. uses Expectation-Maximization given data set
compute parameters, inference algorithm O(N 2 ) number word
occurrences N . Full details provided Li et al. (2004b).
283

fiYates & Etzioni

Resolver
Li et al.

Precision

Recall

F1

97.3
91.5

94.7
94.0

96.0
92.7

Table 8: Resolver outperforms system Li et al. Cross-Document Entity Resolution task involving polysemous peoples names. Differences statistically
significant precision recall using two-tailed Chi-Square test
one degree freedom (p < 0.01, = 910.9 precision = 20.6
recall).

Following Li et al., evaluate clusters using precision recall calculated follows:
let Op set entity occurrence pairs predicted coreferential (i.e.,
belong cluster), let Oa denote set correct coreferential pairs,
|Op Oa |
|Op Oa |
calculated manual clustering. precision P = |O
, recall R = |O
,
p|
a|
R
F1 = P2P+R
.
Table 8 shows results running Resolver data set, well best
results reported Li et al. (2004b) data. 5 follow-up work, Li et al. (2004a)
demonstrate unsupervised model outperforms three supervised techniques
learn parameters much different attributes (first name, honorifics, etc.) contribute
similarity occurrence pairs.
terms absolute performance, Resolver quite accurate dealing polysemous names data set. performance data set significantly higher
TextRunner extractions, partly extra information available terms
contexts occurrences, partly starting manually labelled
named entities, rather noisy extractions.
Resolvers precision significantly higher Li et al.s, roughly equal recall.
large sample sizes, differences precision recall statistically significant (two-tailed Chi-Square test one degree freedom, p < 0.01).
comparison Li et al.s system, Resolvers SSM model much less sophisticated,
compensates using Web data strong measure distributional similarity.
need rely manually curated databases expert knowledge domain,
case, similarity peoples names.

9. Conclusion Future Work
shown unsupervised scalable Resolver system able find clusters
coreferential object names extracted relations precision 78% recall
5. follow-up work, Li et al. (2004a) report F1 score 95.1 task using appears
model data, result calculated testing model 6 random splits
data averaging score. access random splits. One possible reason
reported results different splitting test data reduces number coreference
relations need found potential number incorrect coreference relations cause
system confusion.

284

fiUnsupervised Methods Determining Object Relation Synonyms

68% aid coordination-phrase filtering, find clusters coreferential
relation names precision 90% recall 35%. demonstrated significant
improvements using existing similarity metrics task employing novel
probabilistic model synonymy. much cleaner set extractions TREC
corpus, demonstrated Resolver able achieve 97% precision 95% recall
employing extension allowed cluster different senses name
different groups.
Perhaps critical aspect extending Resolver refining ability handle
polysemy. experiments needed test ability handle new types polysemous named entities extracted data manually cleaned,
case Li et al.s data. addition, plan incorporate ESP model system
unsupervised coreference resolution, including common nouns pronouns.
extend model include aspects local salience, important coreference
decisions noun phrases proper names.
Currently setting ESP models single hidden parameter using development
set. required amount data small, model might accurate
easier use hidden parameter set sampling data, rather using
development set must manually assembled. is, Resolver could inspect
substantial portion data, measure often new properties appear
remaining data. rate appearance new properties offer strong signal
set hidden parameter.
Several extensions Resolver dealt ruling highly similar non-synonyms
(Section 7), varying degrees success boosting Resolvers precision. also
considered another extension Resolver seeks use mutual recursion boost
recall, much like semi-supervised information extraction techniques use mutual bootstrapping entities patterns increase recall (Riloff & Jones, 1999). method
begins clustering objects, clusters relations using merged object clusters
properties (rather raw object strings), clusters objects using relation
clusters properties, on. Although far unable boost performance Resolver using technique TextRunner data, experiments artificial
simulations suggest suitable conditions, mutual recursion could boost recall
much 16%. remains important area future work determine
natural data technique indeed useful, investigate methods
increasing Resolvers recall.

Acknowledgments
research supported part Temple University, NSF grants IIS-0535284
IIS-0312988, ONR grant N00014-08-1-0431 well gifts Google, carried
University Washingtons Turing Center Temple Universitys Center
Information Science Technology. would like thank anonymous reviewers
JAIR associate editor charge paper helpful comments suggestions.
would also like thank KnowItAll group University Washington
feedback support.
285

fiYates & Etzioni

Appendix A. Derivation Extracted Shared Property Model
Extracted Shared Property (ESP) Model introduced Section 4. method
calculating probability two strings synonymous, given share certain
number extractions data set. appendix gives derivation model.
Let si sj two strings, set extracted properties Ei Ej . Let Ui
Uj set potential properties string, contained respective urns.
Let Si,j number properties shared two urns, |Ui Uj |. Let Ri,j

random variable synonymy relationship si sj , Ri,j = Ri,j
f
denoting event are, Ri,j
not. ESP model states
probability selecting observed number matching properties
probability Ri,j
two urns containing matching properties, divided probability selecting
observed number matching properties two urns may contain matching
non-matching properties:

Proposition 2 two strings si sj |Ui | = Pi |Uj | = Pj potential properties
(or instances), min(Pi , Pj ) = Pmin ; appear extracted assertions Ei Ej
|Ei | = ni |Ej | = nj ; share k extracted properties (or instances),
probability si sj co-refer is:

P (Ri,j
|Ei , Ej , Pi , Pj ) =
Pmin
k

P

Pj Pmin

r+s Pi Pmin
ni (k+r) nj (k+s)
r

Pi Si,j Pj Si,j
Si,j P
Si,j k r+s
r,s0
r+s
r
k
ni (k+r) nj (k+s)

P

kSi,j Pmin

r,s0

Si,j k
r+s



(10)

ESP model makes several simplifying assumptions:
1. Balls drawn urns without replacement.
2. Draws one urn independent draws urn.
3. ball string equally likely selected urn: U = {u1 , . . . , um }
X denotes random draw U , P (X = ui ) = |U1 | every ui .
4. prior probability Si,j , given number properties Ui Uj , uniform:
0smin(Pi ,Pj ) P (Si,j = s|Pi , Pj ) = min(Pi1,Pj )+1
5. Given extracted properties two strings number potential properties
each, probability synonymy depends number extracted properties
each, number shared properties extractions:
|E , E , P , P ) = P (Rt |k, n , n , P , P ).
P (Ri,j
j

j

j

j
i,j
6. Two strings synonymous share many potential properties
(|U U | = min(P , P )).
possible: Ri,j

j

j
proving Proposition 2, prove simple property urns assumptions
above.
286

fiUnsupervised Methods Determining Object Relation Synonyms

Lemma 1 Given n draws without replacement urn containing set properties
U , probability selecting particular set U |U1 | |S| = n, zero otherwise.
( |S| )
Proof Lemma 1: Let U = {u1 , . . . , um } denote elements U , let X1 , . . . , Xn
denote independent draws urn. n = 1, P (S = {ui }) = P (X1 = ui ) = |U1 |
assumption 3 above. suppose n = n0 , lemma holds every
n 0 < n0 .
P (S = {x1 , . . . , xn0 |xi U }) =

X

=

X





=

P (S n0 1 = {x1 , . . . , xi1 , xi+1 , . . . , xn0 })
P (Xn = xi )
1
1

|U |
|U | n0 + 1
n0 1

X (n0 1)!(|U | n0 + 1)!
|U |!



=
=
=

1
|U | n0 + 1

n0 (n0 1)!(|U | n0 + 1)(|U | n0 )!
|U |!(|U | n0 + 1)
n0 !(|U | n0 )!
|U |!
1

|U |
n0

2

Proof Proposition 2:
|E , E , P , P ), something
begin transforming desired expression, P (Ri,j

j

j
derived urn model. assumptions 5 6, get

P (Ri,j
|Ei , Ej , Pi , Pj ) = P (Si,j = Pmin |k, ni , nj , Pi , Pj )

(11)

Then, applying Bayes Rule, get
P (Si,j = Pmin |k, ni , nj , Pi , Pj ) =
P (k|Si,j = Pmin , ni , nj , Pi , Pj )P (Si,j = Pmin |ni , nj , Pi , Pj )
P
kSi,j Pmin P (k|ni , nj , Pi , Pj )P (Si,j |ni , nj , Pi , Pj )

(12)

Since assumed uniform prior Si,j (assumption 4), prior terms vanish,
leaving
P (k|Si,j = Pmin , ni , nj , Pi , Pj )

P (Ri,j
|Ei , Ej , Pi , Pj ) = P
(13)
kSi,j Pmin P (k|ni , nj , Pi , Pj )
second step derivation find suitable expression
P (k|Si,j , ni , nj , Pi , Pj )
287

fiYates & Etzioni

probability written fully as:
X
P (k|Si,j , ni , nj , Pi , Pj ) =

Ei Ui :|Ei |=ni
Ej Uj :|Ej |=nj
|Ei Ej |=k

X

Ei Ui :|Ei |=ni
Ej Uj :|Ej |=nj

P (Ei , Ej |Si,j , ni , nj , Pi , Pj )

P (Ei , Ej |Si,j , ni , nj , Pi , Pj )

(14)

assumption 2, P (Ei , Ej ) = P (Ei )P (Ej ). Lemma 1, P (Ei ) terms equal, since
sets size ni , likewise P (Ej ) terms. Thus, get desired probability
expression, simply need count number ways taking subsets two
urns share k properties.
X
1
P (k|Si,j , ni , nj , Pi , Pj ) =

Ei Ui :|Ei |=ni
Ej Uj :|Ej |=nj
|Ei Ej |=k

X

1

(15)

Ei Ui :|Ei |=ni
Ej Uj :|Ej |=nj

=


Pi
ni



Count(k, ni , nj |Si,j , Pi , Pj )
Count(ni , nj |Si,j , Pi , Pj )

(16)

ways picking set Ei ,
Count(ni , nj |Si,j , Pi , Pj ) =


Pi
Pj
ni
nj

(17)

complete derivation, need expression Count(k, ni , nj | Si,j , Pi , Pj ).
involves splitting relevant sets several parts. First Ui Uj contain
shared unshared properties. Let Ti,j = Ui Uj , Vi = Ui Ti,j , Vj = Uj Ti,j . Second,
selected sets urn, Ei Ej , properties come set
shared properties set unshared properties. Let K = Ei Ej , Fi = (Ei Ti,j ) K,
Fj = (Ej Ti,j ) K.
sets defined, set Ei Ej composed three distinct subsets:
shared subset (K); subset also selected shared potential properties, Ti,j ,
shared (Fi Fj ); remaining elements, chosen
complements shared properties (Vi Vj ). Since subsets distinct,
count separately multiply results arrive final count.
number ways selecting shared subset clearly Ski,j . sizes Fi
Fj unknown, however, must sum possibilities. Let r = |Fi |, = |Fj |.
Si,j k remaining shared potential
properties Ti,j choose
r + elements Fi Fj , r+s
ways split two distinct subsets.

ni (k + r) elements left choose Ei , nj (k + s) elements left choose
Ej . must selected unshared potential properties Vi Vj ,
sizes Pi Si,j Pj Si,j respectively. Putting pieces together,
288

fiUnsupervised Methods Determining Object Relation Synonyms

Count(k, ni , nj |Si,j , Pi , Pj ) =







Pj Si,j
r+s
Pi Si,j
Si,j X Si,j k
ni (k + r) nj (k + s)
r+s

k
r,s

(18)

ranges r somewhat involved. must obey following constraints:
1. r, 0
2. r ni k Pi + Si,j
3. nj k Pj + Si,j
4. r ni k
5. nj k
6. r + Si,j k
Plugging Equation 18 Equation 16, turn Equation 13 yields
desired result. 2

Appendix B. Fast Calculation Extracted Shared Property Model
ESP model expensive calculate done wrong way. use two techniques
speed calculation immensely. reference, full formulation model is:

P (Ri,j
|k, ni , nj , Pi , Pj ) =
Pmin
k

P

Pj Pmin

r+s Pi Pmin
ni (k+r) nj (k+s)
r

Pi Si,j Pj Si,j
Si,j P
Si,j k r+s
r,s0
r+s
r
k
ni (k+r) nj (k+s)

P

kSi,j Pmin

r,s0

Si,j k
r+s



(19)

Note equation involves three sums, ranging O(Pmin ), O(ni ), O(nj ) values
respectively. effect, O(n3 ) number extractions string. Furthermore,
step requires expensive operation calculating binomial coefficients. Fortunately,
several easy ways drastically speed calculation.
First, Stirlings approximation used calculate factorials (and therefore
binomial function). Stirlings approximation given by:

n
1
n
n! 2n +
3
en
avoid underflow overflow errors, log probabilities used everywhere possible.
calculation done using simple multiplications logarithm calculations.
Stirlings formula converges n! like O( n1 ); practice proved accurate enough
approximation n! n > 100. ESPs implementation, values n!
calculated once, stored future use.
289

fiYates & Etzioni

Second, calculation P (k|n1 , n2 , P1 , P2 ) sped simplifying expression get rid two sums. result following equivalent expression, assuming
without loss generality P2 P1 :
P (k|n1 , n2 , P1 , P2 ) =

P2 +1
n2 +1

Pn2

r
r=k k

P2 P1
n2 n1



P1 r
n1 k



(20)

simplification removes two sums, therefore changes complexity calculating ESP O(P2 n2 n1 ) O(n2 ). sufficient data set, larger
data sets might necessary introduce sampling techniques improve efficiency
even further.

Appendix C. Better Bound Number Comparisons Made
Resolver Clustering Algorithm
Section 4 showed Resolver clustering algorithm initially makes O(N log N ) comparisons strings data, N number extractions. Heuristic methods like Canopies method (McCallum et al., 2000) require O(M 2 ) comparisons,
number distinct strings data. claim O(N log N ) asymptotically
better O(M 2 ) Zipf-distributed data.
Zipf-distributed data controlled shape parameter, call z. claim
holds true shape parameter z < 2, shown below. Fortunately, natural
data shape parameter usually close z = 1, Resolver data
observed z < 1.
Let set distinct strings set extractions D. P
S, let freq(s)
denote number times appears extractions. Thus |D| = sS freq(s).
Let = |S| N = |D|.
Proposition 3 observed Zipf distribution shape parameter z,
1. z < 1, N = (M )
2. z = 1, N = (M log )
3. z > 1, N = (M z )
Proof: Let s1 , . . . , sM elements rank order highest frequency string
(s1 ) lowest frequency string (sM ). Since observed Zipf distribution shape
z
parameter z, freq(si ) = Miz . Given assumptions, z determine number
extractions made:
X
NM,z =
freq(s)
(21)
sS

X Mz
iz

=

1iM

290

(22)

fiUnsupervised Methods Determining Object Relation Synonyms

build recurrence relation value N changes (holding z constant)
noting
N2M,z =

X

1i2M

= (2M )z

(2M )z
iz
X 1

1iM

iz

(23)
+ (2M )z

= 2z NM,z + fz (M )

X

+1i2M

1
iz

(24)
(25)

P
)z
fz (M ) = +1i2M (2M
iz .
two important properties fz (M ).
z

)
z
1. Note every term sum fz (M ) less (2M
z = 2 . Thus fz (M )
bounded 2z , z held constant, fz (M ) = O(M ).

2. Every term sum least 1, fz (M ) fz (M ) = (M ); combining
two facts yields fz (M ) = (M ).
two properties fz (M ) used below.
use recurrence relation Master Recurrence Theorem (Cormen,
Leiserson, & Rivest, 1990) prove three claims proposition. reference,
Master Recurrence Theorem states following:
Theorem 1 Let 1 b 1 constants, let f (n) function, let (n)
defined non-negative integers recurrence
(n) = (n/b) + f (n)
T(n) bounded asymptotically follows.
1. f (n) = O(nlogb ) constant > 0, (n) = (nlogb )
2. f (n) = (nlogb ), (n) = (nlogb log n)
3. f (n) = (nlogb a+ ), constant > 0, af (n/b) cf (n)
constant c < 1 sufficiently large n, (n) = (f (n)).
First consider case z > 1. recurrence NM,z clearly made fit
form Theorem 1 setting = 2z , b = 2, f = fz (M ). Since fz (M ) bounded
2z = O(M ), also clearly bounded O(M logb ) = O(M z ),
take anything (0, z 1). Thus case one Theorem 1 applies,
NM,z = (M logb ) = (M z ).
Next consider case z = 1. Since fz=1 (M ) = (M ) (M logb ) = (M ),
case two Theorem 1 applies. Thus NM,z=1 = (M logb log ) = (M log ).
Finally, consider case z < 1. Unfortunately, regularity condition case
3 Theorem 1 hold fz (M ). Instead using Theorem 1, resort proof
induction.
291

fiYates & Etzioni

Specifically, show induction whenever z < 1 2, NM,z c ,
z
2z
c = Max( 2 2+1 , 22
z ). First, consider case = 2:
2
X
2z

N2,z =

i=1
z

iz

= 2 +1
2z + 1
2
=
2
cM
prove induction case.
NM,z 2z NM/2,z + fz (M/2)
2z cM/2 + fz (M/2)
z

(by induction hypothesis)

z

2 cM/2 + 2 M/2

= cM (2z1 + 2z1 /c)
2 2z
)
cM (2z1 + 2z1
2z
= cM

(by definition c)

2
data used Resolver experiments Section 4 shape parameter z < 1,
bound number comparisons made O(N log N ) = O(M log ). z = 1,
bound O(N log N ) = O(M log log(M log )) = O(M log2 ). z > 1,
bound O(M z log ). z = 2 would asymptotic performance O(M 2 log )
worse O(M 2 ). past experience guide, high value z
unlikely extractions naturally occurring text.

References
Agichtein, E. (2006). Web Information Extraction User Information Needs: Towards
Closing Gap. IEEE Data Engineering Bulletin issue Web-Scale Data, Systems,
Semantics, December.
Artile, J., Sekine, S., & Gonzalo, J. (2008). Web people search: results first evaluation
plan second. Proceeding 17th international conference
World Wide Web.
Bagga, A., & Baldwin, B. (1998). Entity-based cross-document coreferencing using
vector space model. COLING-ACL.
Banko, M., Cafarella, M. J., Soderland, S., Broadhead, M., & Etzioni, O. (2007). Open
information extraction web. IJCAI.
292

fiUnsupervised Methods Determining Object Relation Synonyms

Barzilay, R., & Lee, L. (2003). Learning Paraphrase: Unsupervised Approach Using
Multiple-Sequence Alignment. Proc. NAACL-HLT.
Barzilay, R., & McKeown, K. (2001). Extracting paraphrases parallel corpus.
Proceedings ACL/EACL.
Bean, D., & Riloff, E. (2004). Unsupervised learning contextual role knowledge
coreference resolution. Proceedings Annual Meeting North American
Chapter Association Computational Linguistics (HLT/NAACL).
Beygelzimer, A., Kakade, S., & Langford, J. (2006). Cover trees nearest neighbor.
Proceeings 23rd International Conference Machine Learning (ICML).
Bhattacharya, I., & Getoor, L. (2005). Relational Clustering Multi-type Entity Resolution. 11th ACM SIGKDD Workshop Multi Relational Data Mining.
Bhattacharya, I., & Getoor, L. (2006). Query-time entity resolution. KDD.
Brockett, C., & Dolan, W. B. (2005). Support vector machines paraphrase identification
corpus construction. International Workshop Paraphrasing.
Cardie, C., & Wagstaff, K. (1999). Noun Phrase Coreference Clustering. Proceedings
Joint Conference Empirical Methods Natural Language Processing
Large Corpora.
Cohen, W. W. (1998). Providing database-like access web using queries based
textual similarity. Proceedings ACM SIGMOD International Conference
Management Data.
Cohen, W., Ravikumar, P., & Fienberg, S. (2003). comparison string distance metrics
name-matching tasks. IIWeb.
Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1990). Introduction Algorithms.
MIT Press.
Dagan, I., Glickman, O., & Magnini, B. (2006). PASCAL Recognising Textual Entailment Challenge. Lecture Notes Computer Science, 3944, 177190.
Davidov, D., & Rappoport, A. (2008). Unsupervised Discovery Generic Relationships
Using Pattern Clusters Evaluation Automatically Generated SAT Analogy
Questions. Proceedings ACL.
Dong, X., Halevy, A., & Madhavan, J. (2005). Reference reconciliation complex information spaces. SIGMOD.
Downey, D., Etzioni, O., & Soderland, S. (2005). Probabilistic Model Redundancy
Information Extraction. IJCAI.
Downey, D., Schoenmackers, S., & Etzioni, O. (2007). Sparse information extraction: Unsupervised language models rescue. ACL.
293

fiYates & Etzioni

Etzioni, O., Cafarella, M., Downey, D., Kok, S., Popescu, A., Shaked, T., Soderland, S.,
Weld, D., & Yates, A. (2005). Unsupervised named-entity extraction web:
experimental study. Artificial Intelligence, 165 (1), 91134.
Gionis, A., Indyk, P., & Motwani, R. (1999). Similarity search high dimensions via
hashing. Proceedings 25th Conference Large Databases (VLDB).
Haghighi, A., & Klein, D. (2007). Unsupervised Coreference Resolution Nonparametric
Bayesian Model. Proceedings ACL.
Hasegawa, T., Sekine, S., & Grishman, R. (2004). Discovering relations among named
entities large corpora. Proceedings ACL.
Hernandez, M. A., & Stolfo, S. J. (1995). merge/purge problem large databases.
SIGMOD.
Hindle, D. (1990). Noun classification predicage-argument structures. ACL.
Ide, N., & Veronis, J. (1998). Word Sense Disambiguation: State Art. Computational Linguistics, 24 (1), 140.
Kehler, A. (1997). Probabilistic coreference information extraction. EMNLP.
Kehler, A., Appelt, D., Taylor, L., & Simma, A. (2004). (non)utility predicateargument frequencies pronoun interpretation. Proceedings Annual Meeting North American Chapter Association Computational Linguistics
(HLT/NAACL).
Kok, S., & Domingos, P. (2007). Statistical predicate invention. Proceedings
Twenty-Fourth International Conference Machine Learning.
Kulkarni, A., & Pedersen, T. (2008). Name Discrimination Email Clustering Using
Unsupervised Clustering Similar Contexts. Journal Intelligent Systems (Special
Issue: Recent Advances Knowledge-Based Systems Applications), 17 (13), 3750.
Lappin, S., & Leass, H. J. (1994). algorithm pronominal anaphora resolution. Computational Linguistics, 20 (4), 535561.
Lee, L. (1999). Measures distributional similarity. Proceedings 37th ACL.
Li, H., & Abe, N. (1998). Word clustering disambiguation based co-occurence data.
COLING-ACL, pp. 749755.
Li, X., Morie, P., & Roth, D. (2004a). Identification tracing ambiguous names:
Discriminative generative approaches. Proceedings National Conference
Artificial Intelligence (AAAI), pp. 419424.
Li, X., Morie, P., & Roth, D. (2004b). Robust reading: Identification tracing
ambiguous names. Proc. Annual Meeting North American Association
Computational Linguistics (NAACL), pp. 1724.
294

fiUnsupervised Methods Determining Object Relation Synonyms

Lin, D., & Pantel, P. (2001). DIRT Discovery Inference Rules Text. KDD.
Liu, T., Moore, A. W., Gray, A., & Yang, K. (2004). investigation practical approximate nearest neighbor algorithms. Proceedings 22nd Annual Conference
Neural Information Processing Systems (NIPS).
Mann, G., & Yarowsky, D. (2003). Unsupervised personal name disambiguation. CoNLL.
Manning, C. D., & Schuetze, H. (1999). Foundations Statistical Natural Language Processing. MIT Press.
McCallum, A., Nigam, K., & Ungar, L. (2000). Efficient clustering high-dimensional data
sets application reference matching. KDD.
McCallum, A., & Wellner, B. (2004). Conditional models identity uncertainty
application noun coreference. NIPS.
McCarthy, J., & Lehnert, W. (1995). Using decision trees coreference resolution.
Proceedings Fourteenth International Conference Artificial Intelligence.
Miller, G. A., Beckwith, R., Fellbaum, C., Gross, D., & Miller., K. J. (1990). Introduction
WordNet: on-line lexical database. International Journal Lexicography, 3(4),
235312.
Monge, A. E., & Elkan, C. (1996). Field Matching Problem: Algorithms Applications. Knowledge Discovery Data Mining, pp. 267270.
Ng, V., & Cardie, C. (2002). Improving machine learning approaches coreference resolution. ACL.
Pang, B., Knight, K., & Marcu, D. (2003). Syntax-based alignment multiple translations: Extracting paraphrases generating new sentences. Proceedings
HLT/NAACL.
Pedersen, T., & Kulkarni, A. (2007). Unsupervised Discrimination Person Names Web
Contexts. Proceedings Eighth International Conference Intelligent Text
Processing Computational Linguistics.
Pereira, F., Tishby, N., & Lee, L. (1993). Distributional clustering English words.
Proceedings 31st ACL.
Popescu, A.-M. (2007). Information Extraction Unstructured Web Text. Ph.D. thesis,
University Washington.
Ravikumar, P., & Cohen, W. W. (2004). hierarchical graphical model record linkage.
UAI.
Riloff, E., & Jones, R. (1999). Learning Dictionaries Information Extraction Multilevel Bootstrapping. Proceedings Sixteenth National Conference Artificial
Intelligence, pp. 474479.
295

fiYates & Etzioni

Salton, G., & McGill, M. (1983). Introduction Modern Information Retrieval. McGrawHill.
Sekine, S. (2005). Automatic Paraphrase Discovery based Context Keywords NE Pairs. International Workshop Paraphrasing.
Shinyama, Y., & Sekine, S. (2003). Paraphrase acquisition information extraction.
International Workshop Paraphrasing.
Singla, P., & Domingos, P. (2006). Entity Resolution Markov Logic. ICDM.
Sinha, R., & Mihalcea, R. (2007). Unsupervised graph-based word sense disambiguation
using measures word semantic similarity. Proceedings IEEE International
Conference Semantic Computing (ICSC 2007).
Snow, R., Jurafsky, D., & Ng, A. Y. (2006). Semantic taxonomy induction heterogenous evidence. COLING/ACL.
Voorhees, E. (2002). Overview TREC-2002 question-answering track. TREC.
Winkler, W. (1999). state record linkage current research problems. Tech. rep.,
U.S. Bureau Census, Washington, D.C.
Yates, A., & Etzioni, O. (2007). Unsupervised resolution objects relations
web. Proceedings HLT-NAACL.
Zhu, J., Nie, Z., Wen, J.-R., Zhang, B., & Ma, W.-Y. (2005). 2D Conditional Random Fields
Web Information Extraction. Proceedings 22nd International Conference
Machine Learning.
Zipf, G. K. (1932). Selective Studies Principle Relative Frequency Language.

296

fiJournal Artificial Intelligence Research 34 (2009) 89-132

Submitted 8/08; published 2/09

Policy Iteration Decentralized Control
Markov Decision Processes
Daniel S. Bernstein

bern@cs.umass.edu

Christopher Amato

camato@cs.umass.edu

Department Computer Science
University Massachusetts
Amherst, 01003 USA

Eric A. Hansen

hansen@cse.msstate.edu

Department CS Engineering
Mississippi State University
Mississippi State, MS 39762 USA

Shlomo Zilberstein

shlomo@cs.umass.edu

Department Computer Science
University Massachusetts
Amherst, 01003 USA

Abstract
Coordination distributed agents required problems arising many areas, including multi-robot systems, networking e-commerce. formal framework
problems, use decentralized partially observable Markov decision process (DECPOMDP). Though much work done optimal dynamic programming algorithms
single-agent version problem, optimal algorithms multiagent case
elusive. main contribution paper optimal policy iteration algorithm
solving DEC-POMDPs. algorithm uses stochastic finite-state controllers represent policies. solution include correlation device, allows agents correlate
actions without communicating. approach alternates expanding
controller performing value-preserving transformations, modify controller
without sacrificing value. present two efficient value-preserving transformations: one
reduce size controller improve value keeping
size fixed. Empirical results demonstrate usefulness value-preserving transformations
increasing value keeping controller size minimum. broaden applicability approach, also present heuristic version policy iteration algorithm,
sacrifices convergence optimality. algorithm reduces size
controllers step assuming probability distributions agents
actions known. assumption may hold general, helps produce higher
quality solutions test problems.

1. Introduction
Markov decision processes (MDPs) provide useful framework solving problems
sequential decision making uncertainty. settings, agents must base
decisions partial information system state. case, often better use
general framework partially observable Markov decision processes (POMDPs).
Even general problems team decision makers,
c
2009
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiBernstein, Amato, Hansen, & Zilberstein

local observations, must act together. Domains types problems arise
include networking, multi-robot coordination, e-commerce, space exploration systems.
decentralized partially observable Markov decision process (DEC-POMDP) provides
effective framework model problems. Though model recognized
decades (Witsenhausen, 1971), little work provably optimal algorithms
it.
hand, POMDPs studied extensively past decades
(Smallwood & Sondik, 1973; Simmons & Koenig, 1995; Cassandra, Littman, & Zhang, 1997;
Hansen, 1998a; Bonet & Geffner, 2000; Poupart & Boutilier, 2003; Feng & Zilberstein, 2004;
Smith & Simmons, 2005; Smith, Thompson, & Wettergreen, 2007). well known
POMDP reformulated equivalent belief-state MDP. belief-state MDP
cannot solved straightforward way using MDP methods continuous
state space. However, Smallwood Sondik showed implement value iteration
exploiting piecewise linearity convexity value function. work opened
door many algorithms, including approximate approaches policy iteration algorithms
policy represented using finite-state controller.
Extending dynamic programming POMDPs multiagent case straightforward. one thing, clear define belief state consequently form
belief-state MDP. multiple agents, agent uncertainty observations
beliefs agents. Furthermore, finite-horizon DEC-POMDP problem
two agents complete higher complexity class single-agent version
(Bernstein, Givan, Immerman, & Zilberstein, 2002), indicating fundamentally
different problems.
paper, describe extension policy iteration algorithm single agent
POMDPs multiagent case. single agent case, algorithm converges
limit, thus serves first nontrivial optimal algorithm infinite-horizon
DEC-POMDPs. optimal approaches (Hansen, Bernstein, & Zilberstein, 2004; Szer,
Charpillet, & Zilberstein, 2005) several approximate algorithms developed
finite-horizon DEC-POMDPs (Peshkin, Kim, Meuleau, & Kaelbling, 2000; Nair, Pynadath,
Yokoo, Tambe, & Marsella, 2003; Emery-Montemerlo, Gordon, Schnieder, & Thrun, 2004;
Seuken & Zilberstein, 2007), locally optimal algorithms proposed
infinite-horizon case (Bernstein, Hansen, & Zilberstein, 2005; Szer & Charpillet, 2005;
Amato, Bernstein, & Zilberstein, 2007).
algorithmic framework, policies represented using stochastic finite-state controllers. simple way implement give agent local controller.
case, agents policies independent. general class policies includes
allow agents share common source randomness without sharing observations. define class formally, using shared source randomness called correlation
device. use correlated stochastic policies DEC-POMDP context novel.
importance correlation recognized game theory community (Aumann,
1974), little work algorithms finding correlated policies.
iteration algorithm consists two phases. exhaustive backups,
add nodes controller, value-preserving transformations, change
controller without sacrificing value. first provide novel exposition existing single90

fiPolicy Iteration DEC-POMDPs

agent algorithms using two-phase view, go describe multiagent
extension.
many possibilities value-preserving transformations. paper,
describe two different types, performed efficiently using linear programming. first type allows us remove nodes controller, second
allows us improve value controller keeping size fixed. empirical
results demonstrate usefulness value-preserving transformations obtaining high
values keeping controller size minimum.
note work serves unify generalize previous work dynamic programming DEC-POMDPs. first algorithm finite-horizon case (Hansen et al., 2004)
extended infinite-horizon case viewed interleaving exhaustive backups
controller reductions. bounded policy iteration algorithm DEC-POMDPs (Bernstein et al., 2005), extends POMDP algorithm proposed Poupart Boutilier
(2003), viewed lens framework repeated application specific
value-preserving transformation.
optimal algorithm usually able return optimal solution
practice, also introduce heuristic version policy iteration algorithm.
approach makes use initial state information focus policy search reduces
controller size step. accomplish this, forward search initial state
distribution used construct set belief points agent would visit assuming
agents use given fixed policies. search conducted agent policy
iteration takes place using belief points guide removal controller nodes.
assumption agents use fixed policies causes algorithm longer
optimal, performs well practice. show concise higher-valued
solutions produced compared optimal method resources exhausted.
remainder paper organized follows. Section 2 introduces formal
models sequential decision making. Section 3 contains novel presentation existing
dynamic programming algorithms POMDPs. section 4, present extension
policy iteration POMDPs DEC-POMDP case, along convergence proof.
discuss heuristic version policy iteration section 5, followed experiments
using policy iteration heuristic policy iteration section 6. Finally, section 7 contains
conclusion discussion possible future work.

2. Formal Model Distributed Decision Making
begin description formal framework upon work based.
framework extends well-known Markov decision process allow distributed policy
execution. also define optimal solution model discuss two different
representations solutions.
2.1 Decentralized POMDPs
decentralized partially observable Markov decision process (DEC-POMDP) defined for~ T, R, ,
~ Oi,
mally tuple hI, S, A,
finite set agents.
91

fiBernstein, Amato, Hansen, & Zilberstein

Agent
Agent


s, r
System

a1

Agent


System

o, r
System

o1, r

a2

o2, r
Agent

(a)

(b)

(c)

Figure 1: (a) Markov decision process. (b) Partially observable Markov decision process.
(c) Decentralized partially observable Markov decision process two agents.

finite set states, distinguished initial state s0 .
~ = iI Ai set joint actions, Ai set actions agent i.

~ state transition function, defining distributions states
: SA
result starting given state agent performing action.
~ < reward function set agents set joint actions
R :
state.
~ = iI set joint observations, contains observations agent i.

~
~ observation function, defining distributions observations
O:A
set agents result agent performing action ending
given state.
special case DEC-POMDP one agent called partially
observable Markov decision process (POMDP).
paper, consider case process unfolds infinite sequence
stages. stage, agents simultaneously select action, receives
global reward based reward function local observation based observation
function. Thus, transitions, rewards observations depend actions
agents, agent must act based local observations. illustrated
Figure 1. objective agents maximize expected discounted sum
rewards received, thus cooperative framework. denote discount
factor require 0 < 1.
DEC-POMDP, decisions agent affect agents domain,
due decentralized nature model agent must choose actions based solely
local information. agent receives separate observation usually
provide sufficient information efficiently reason agents, solving DECPOMDP optimally becomes difficult. example, agent may receive different
92

fiPolicy Iteration DEC-POMDPs

(a)

(b)

Figure 2: set horizon three policy trees (a) two node stochastic controllers (b)
two agent DEC-POMDP.

piece information allow common state estimate estimate
agents decisions calculated. single estimates crucial single agent
problems, allow agents history summarize concisely,
generally available DEC-POMDPs. seen complexity finite-horizon
problem least two agents, NEXP-complete (Bernstein et al., 2002)
thus practice may require double exponential time. Like infinite-horizon POMDP,
optimally solving infinite-horizon DEC-POMDP undecidable may require infinite
resources, method able provide solution within optimal finite
time memory. Nevertheless, introducing multiple decentralized agents causes DECPOMDP significantly difficult single agent POMDP.
2.2 Solution Representations
local policy agent mapping local action observation histories actions
joint policy set policies, one agent problem. mentioned
above, optimal solution DEC-POMDP joint policy maximizes
expected sum rewards received finite infinite steps problem.
infinite-horizon problems, rewards discounted maintain finite sum. Thus,
optimal solution joint policy provides highest value starting given initial
state problem.
finite-horizon problems, local policies represented using policy tree seen
Figure 2a. Actions represented arrows stop figures (where agent
move given direction stay is) observations labeled wl wr
seeing wall left right respectively. Using representation, agent
takes action defined root node seeing observation, chooses
next action defined respective branch. continues action
leaf node executed. example, agent 1 would first move left wall seen
right, agent would move left again. wall seen left, agent
move final step. policy tree record entire local history
agent fixed horizon tree independent others
93

fiBernstein, Amato, Hansen, & Zilberstein

executed decentralized manner. representation useful finite-horizon
problems, infinite-horizon problems would require trees infinite height.
Another option used paper condition action selection internal
memory state. solutions represented set local finite-state controllers
(seen Figure 2b). controllers operate similar way policy trees
designated initial node following action selection node,
controller transitions next node depending observation seen. continues
infinite steps problem. Throughout paper, controller states referred
nodes help distinguish system states.
infinite number nodes may required define optimal infinite-horizon DECPOMDP policy, discuss way produce solutions within optimal
fixed number nodes. deterministic action selection node transitions
sufficient define -optimal policy, memory limited stochastic action selection
node transition may beneficial. simple example illustrating POMDPs
given Singh (1994), easily extended DEC-POMDPs. Intuitively,
randomness help agent break costly loops result forgetfulness.
formal description stochastic controllers POMDPs DEC-POMDPs given
sections 3.2.1 4.1.1 respectively, example seen Figure 2b. Agent 2
begins node 1 moves probability 0.89 stays place probability
0.11. agent stayed place wall seen left (observation wl),
next step, controller would transition node 1 agent would use
distribution actions again. wall seen right instead (observation wr),
0.85 probability controller transition back node 1 0.15
probability controller transition node 2 next step. finite-state
controller allows infinite-horizon policy represented compactly remembering
aspects agents history without representing entire local history.

3. Centralized Dynamic Programming
section, cover main concepts involved dynamic programming single agent case. provide foundation multiagent dynamic programming
algorithm described following section.
3.1 Value Iteration POMDPs
Value iteration used solve POMDPs optimally. algorithm complicated
MDP counterpart, efficiency guarantees. However, practice
provide significant leverage solving POMDPs.
begin explaining every POMDP equivalent MDP continuous
state space. Next, describe value functions MDP special structure
exploited. ideas central value iteration algorithm.
3.1.1 Belief State MDPs
convenient way summarize observation history agent POMDP
belief state, distribution system states. receives observations,
94

fiPolicy Iteration DEC-POMDPs

agent update belief state remove observations memory. Let b
denote belief state, let b(s) represent probability assigned state b.
agent chooses action belief state b subsequently observes o, component
successor belief state obeys equation
P
P (o|a, s0 ) sS P (s0 |s, a)b(s)
0 0
b (s ) =
,
P (o|b, a)


"
P (o|b, a) =

X

#
0

P (o|a, )

s0

X

0

P (s |s, a)b(s) .

sS

Note simple application Bayes rule.
shown Astrom (1965) belief state constitutes sufficient statistic
agents observation history, possible define MDP belief states
follows. belief-state MDP tuple h, A, T, Ri,
set distributions S.
set actions (same before).
(b, a, b0 ) transition function, defined
X
(b, a, b0 ) =
P (b0 |b, a, o)P (o|b, a).
oO

R(b, a) reward function, defined
R(b, a) =

X

b(s)R(s, a).

sS

combined belief-state updating, optimal solution MDP used
optimal solution POMDP constructed. However, since
belief state MDP continuous, |S|-dimensional state space, traditional MDP techniques
immediately applicable.
Fortunately, dynamic programming used find solution belief state
MDP. key result making dynamic programming practical proved Smallwood
Sondik (1973), showed Bellman operator preserves piecewise linearity
convexity value function. Starting piecewise linear convex representation
V , value function V t+1 piecewise linear convex, computed finite
time.
represent piecewise linear convex value function, one need store value
facet system state. Denoting set facets , store || |S|dimensional vectors real values.PFor single vector, , define value
belief state b V (b, ) = sS b(s)(s). Thus, go set vectors
value belief state, use equation
X
V (b) = max
b(s)(s).


sS

95

fiBernstein, Amato, Hansen, & Zilberstein

s1

s1

s2
(a)

s2
(b)

Figure 3: piecewise linear convex value function POMDP two states (a)
non-minimal representation piecewise linear convex value function
POMDP (b).

Figure 3a shows piecewise linear convex value function POMDP two states.
Smallwood Sondik proved optimal value function finite-horizon
POMDP piecewise linear convex. optimal value function infinite-horizon
POMDP convex, may piecewise linear. However, approximated
arbitrarily closely piecewise linear convex value function, value iteration
algorithm constructs closer closer approximations, shall see.
3.1.2 Pruning Vectors
Every piecewise linear convex value function minimal set vectors represents it. course, possible use non-minimal set represent function.
illustrated Figure 3b. Note removal certain vectors change
value belief state. Vectors necessary keep memory.
Formally, say vector dominated belief states b, vector
\ V (b, ) V (b, ).
dominated vectors necessary, would useful method
removing them. task often called pruning, efficient algorithm based
linear programming. given vector , linear program Table 1 determines
whether dominated. variables found make positive, adding
set improves value function belief state. not, dominated.
gives rise simple algorithm pruning set vectors obtain minimal
set . algorithm loops , removes vector , solves linear
program using \ . dominated, returned .
turns equivalent way characterize dominance useful.
Recall vector dominated, single vector
value least high states. sufficient exist set vectors
belief states, one vectors set value least high vector
question.
96

fiPolicy Iteration DEC-POMDPs

Variables: , b(s)
Objective: Maximize .
Improvement constraints:
X



b(s)(s) +



X

b(s)(s)



Probability constraints:
X



b(s) = 1,

b(s) 0



Table 1: linear program testing whether vector dominated.

1

2
convex combination
3

s1

s2

Figure 4: dual interpretation dominance. Vector 3 dominated belief states
either 1 2 . equivalent existence convex combination
1 2 dominates 3 belief states.

shown set exists convex combination
vectors value least high vector question states.
shown graphically Figure 4. take dual linear program dominance
given previous section, get linear program solution vector
probabilities convex combination. dual view dominance first used
POMDP context Poupart Boutilier (2003), useful policy iteration,
explained later.
3.1.3 Dynamic Programming Update
section, describe implement dynamic programming update go
value function Vt value function Vt+1 . terms implementation, aim take
minimal set vectors represents Vt produce minimal set vectors t+1
represents Vt+1 .
97

fiBernstein, Amato, Hansen, & Zilberstein

vector could potentially included t+1 represents value action
assignment vectors observations. combination action transition
rule hereafter called one-step policy. value vector one-step policy
determined considering action taken, resulting state transitioned
observation seen value assigned vector step t. given via
equation
X
it+1 (s) = R(s, (i)) +
P (s0 |s, (i))P (o|(i), s0 )t (i,o) (s0 ),
s0 ,o

index vector, (i) action, (i, o) index vector
transition upon receiving observation discount factor.
details derivation use formula provided Zhang Zhang (2001).
|A||t ||| possible one-step policies. simple way construct t+1
evaluate possible one-step policies apply pruning algorithm Larks
method (Lark III, 1990). Evaluating entire set one-step policies hereafter
called performing exhaustive backup. turns ways perform
dynamic programming update without first performing exhaustive backup.
describe two approaches this.
first approach uses fact simple find optimal vector
particular belief state. belief state b, optimal action determined via
equation
"
#
X
P (o|b, a)V (T (b|a, o)) .
= argmaxaA R(b, a) +


observation o, subsequent belief state, computed using
Bayes rule. get optimal transition rule, (o), take optimal vector
belief state corresponding o.
Since backed-up value function finitely many vectors, must finite set
belief states backups must performed. Algorithms identify belief
states include Smallwood Sondiks one-pass algorithm (1973), Chengs linear support
relaxed region algorithms (Cheng, 1988), Kaelbling, Cassandra Littmans
Witness algorithm (1998).
second approach based generating pruning sets vectors. Instead
generating vectors pruning, techniques attempt prune generation phase. first algorithm along lines incremental pruning algorithm
(Cassandra et al., 1997). Recently, improvements made approach (Zhang
& Lee, 1998; Feng & Zilberstein, 2004, 2005).
noted theoretical complexity barriers DP updates. Littman
et al. (1995) showed certain widely believed complexity theoretic assumptions,
algorithm performing DP update worst-case polynomial
quantities involved. Despite fact, dynamic programming updates successfully implemented part value iteration policy iteration algorithms,
described subsequent sections.
98

fiPolicy Iteration DEC-POMDPs

3.1.4 Value Iteration
implement value iteration, simply start arbitrary piecewise linear convex
value function, proceed perform DP updates. corresponds value iteration
equivalent belief state MDP, thus converges -optimal value function
finite number iterations.
Value iteration returns value function, policy needed execution.
MDP case, use one-step lookahead, using equation
"
#
X
X
(b) = argmaxaA
R(s, a)b(s) +
P (o|b, a)V ( (b, o, a)) ,
sS



(b, o, a) belief state resulting starting belief state b, taking action a,
receiving observation o. note state estimator must used well track
belief state. Using fact vector corresponds one-step policy,
extract policy value vectors:
!
X
(b) = argmaxk
b(s)k (s)


size resulting set dominant vectors may remain exponential, many
cases much smaller. significantly simplify computation.
completely observable case, Bellman residual provides bound
distance optimality. Recall Bellman residual maximum distance across
belief states value functions successive iterations. possible find
maximum distance two piecewise linear convex functions polynomial time
algorithm uses linear programming (Littman et al., 1995).
3.2 Policy Iteration POMDPs
value iteration, POMDP viewed belief-state MDP, policy mapping
belief states actions. early policy iteration algorithm developed Sondik used
policy representation (Sondik, 1978), complicated meet
success practice. shall describe different approach performed better
test problems. approach, policy represented finite-state controller.
3.2.1 Finite-State Controllers
Using finite-state controller, agent finite number internal states. actions
based internal state, transitions internal states occur
observations received. Internal states provide agents kind memory,
crucial difficult POMDPs. course, agents memory limited number
internal states possesses. general, agent cannot remember entire history
observations, would require infinitely many internal states. example finitestate controller seen considering one agents controller Figure 2b.
operation single controller agent decentralized case.
formally define controller tuple hQ, , A, , i,
99

fiBernstein, Amato, Hansen, & Zilberstein

Q finite set controller nodes.
set inputs, taken observations POMDP.
set outputs, taken actions POMDP.
: Q action selection function, defining distribution actions
selected node.
: Q Q transition function, defining distribution resulting
nodes initial node action taken.
state starting node controller, expected discounted sum
rewards infinite horizon. computed using following system linear
equations, one q Q:


X
X
V (s, q) =
P (a|q) R(s, a) +
P (o, s0 |s, a)P (q 0 |q, a, o)V (s0 , q 0 ) .
s0 ,o,q 0



P (a|q) probability action taken node q P (q 0 |q, a, o)
probability controller transition node q 0 node q action taken
observed.
sometimes refer value controller belief state. belief state b,
defined
X
V (b) = max
b(s)V (s, q).
q



Thus, assumed that, given initial state distribution, controller started
node maximizes value distribution. execution begun, however,
belief state updating. fact, possible agent encounter
belief state twice different internal state time.
3.2.2 Algorithmic Framework
describe policy iteration algorithm abstract terms, focusing key components necessary convergence. subsequent sections, present different possibilities
implementation.
Policy iteration takes input arbitrary finite-state controller. first phase
iteration consists evaluating controller, described above. Recall value iteration
initialized arbitrary piecewise linear convex value function, represented
set vectors. policy iteration, piecewise linear convex value function arises
evaluation controller. controller node value paired
state. Thus, node corresponding vector thus linear value function
belief state space. Choosing best node belief state yields piecewise linear
convex value function.
second phase iteration dynamic programming update. value iteration, update produces improved set vectors, vector corresponds
deterministic one-step policy. set vectors produced case,
100

fiPolicy Iteration DEC-POMDPs

Input: finite state controller, parameter .
1. Evaluate finite-state controller solving system linear equations.
2. Perform dynamic programming update add set deterministic nodes
controller.
3. Perform value-preserving transformations controller.
4. Calculate Bellman residual. less (1 )/2, terminate.
Otherwise, go step 1.
Output: -optimal finite-state controller.
Table 2: Policy Iteration POMDPs.
actions transition rules one-step policy cannot removed memory.
new vector actually node gets added controller. probability distributions added nodes deterministic. is, exhaustive backup context
creates new node possible action possible combinations observations
deterministic transitions current controller. results one-step policies considered dynamic programming update described above.
|A||t ||| possible one-step polices, number also defines number new nodes
added controller exhaustive backup.
Finally, additional operations performed controller. many
operations, describe two possibilities following section. restriction
placed operations decrease value belief state.
operation denoted value-preserving transformation.
complete algorithm outlined Table 2. guaranteed converge finitestate controller -optimal belief states within finite number steps. Furthermore, Bellman residual used obtain bound distance optimality,
value iteration.
3.2.3 Controller Reductions
performing DP update, potential nodes dominated get added
controller. However, update performed, old nodes may become
dominated. nodes cannot simply removed, however, nodes may transition
them. dual view dominance useful. Recall node
dominated, convex combination nodes value least high
states. Thus, remove dominated node merge dominating
convex combination changing transition probabilities accordingly. operation
proposed Poupart Boutilier (2003) built upon earlier work Hansen (1998b).
Formally, controller reduction attempts replace node q Q distribution
P (q) nodes q Q \ q S,
X
V (s, q)
P (q)V (s, q).
qQ\q

101

fiBernstein, Amato, Hansen, & Zilberstein

Variables: , x()
Objective: Maximize
Improvement constraints:


V (s, ) +

X

x()V (s, )



Probability constraints:
X



x() = 1,

x() 0



Table 3: dual linear program testing dominance vector . variable x()
represents P ().

achieved solving linear program Table 3. nodes used rather
vectors, replace x() x(q) dual formulation provides probability distribution nodes dominate node q. Rather transitioning q,
distribution used instead. shown distribution found
used merging, resulting controller value-preserving transformation
original one.
3.2.4 Bounded Backups
previous section, described way reduce size controller without
sacrificing value. method described section attempts increase value
controller keeping size fixed. focuses one node time, attempts
change parameters node value controller least high
belief states. idea approach originated Platzman (1980),
made efficient Poupart Boutilier (2003).
method, node q chosen, parameters conditional distribution
P (a, q 0 |q, o) determined. Determining parameters works follows.
assume original controller used second step on, try replace
parameters q better ones first step. words, look
parameters satisfy following inequality:


X
X
V (s, q)
P (a|q) R(s, a) +
P (q 0 |q, a, o)P (o, s0 |s, a)V (s0 , q 0 )


s0 ,o,q 0

S. Note inequality always satisfied original parameters.
However, often possible get improvement.
new parameters found solving linear program, shown Table 4.
Note size linear program polynomial sizes POMDP
controller. call process bounded backup acts like dynamic programming
102

fiPolicy Iteration DEC-POMDPs

Variables: , x(a), x(a, o, q 0 )
Objective: Maximize
Improvement constraints:



V (s, q) +

X



x(a)R(s, a) +

x(a) = 1,

a,

X

x(a, o, q 0 ) = x(a)

q0





x(a, o, q 0 )P (o, s0 |s, a)V (s0 , q 0 )

s0 ,o,q 0



Probability constraints:
X

X

x(a) 0,

a, o, q 0

x(a, o, q 0 ) 0

Table 4: linear program solved bounded backup. variable x(a) represents P (a|q), variable x(a, o, q 0 ) represents P (a, q 0 |q, o).

backup memory constraints. see this, consider set nodes generated DP
backup. nodes dominate original nodes across belief states, every
original node, must convex combination nodes set dominate
original node states. bounded backup finds convex combination.
shown bounded backup yields value-preserving transformation. Repeated application bounded backups lead local optimum, none
nodes improved further. Poupart Boutilier (2003) showed local optimum reached nodes value function touching value function
produced performing full DP backup. illustrated Figure 5.

4. Decentralized Dynamic Programming
previous section, presented dynamic programming POMDPs. key part
POMDP theory fact every POMDP equivalent belief-state MDP.
result known DEC-POMDPs, making difficult generalize value iteration
multiagent case. lack shared belief-state requires new set tools
developed solving DEC-POMDP. step direction, able develop
optimal policy iteration algorithm DEC-POMDPs includes POMDP version
special case. algorithm focus section.
first show extend definition stochastic controller multiagent
case. Multiagent controllers include correlation device, source randomness
shared agents. shared randomness increases solution quality minimally
increasing representation size without adding communication. single agent case,
policy iteration alternates exhaustive backups value-preserving transforma103

fiBernstein, Amato, Hansen, & Zilberstein

value function
DP update
value function
controller

s1

s2

Figure 5: local optimum bounded backups. solid line value function
controller, dotted line value function controller results
full DP update.

tions. convergence proof given, along efficient transformations extend
presented previous section.
4.1 Correlated Finite-State Controllers
joint policy agents represented using stochastic finite-state controller
agent. section, first define type controller agents act
independently. provide example demonstrating utility correlation,
show extend definition controller allow correlation among agents.
4.1.1 Local Finite-State Controllers
local controller, agents node based local observations received,
agents action based current node. local controllers defined
way POMDP controllers above, agent possessing controller
operates independently others. before, stochastic transitions action selection
allowed.
formally define local controller agent tuple hQi , , Ai , , i,
Qi finite set controller nodes.
set inputs, taken local observations agent i.
Ai set outputs, taken actions agent i.
: Qi Ai action selection function agent i, defining distribution
actions selected node agents controller.
: Qi Ai Qi transition function agent i, defining distribution
resulting nodes initial node action taken agents controller.
functions parameterize conditional distribution P (ai , qi0 |qi , oi ) represents combined action selection node transition probability agent i.
104

fiPolicy Iteration DEC-POMDPs

AB
BA
BB

AA

AA
AB
BA

+R

R

R

s1

s2

+R

BB

Figure 6: DEC-POMDP correlated joint policy yields reward
optimal independent joint policy.

taken together, agents controllers determine conditional distribution P (~a, ~q 0 |~q, ~o).
denoted independent joint controller. following subsection, show
independence limiting.
4.1.2 Utility Correlation
joint controllers described allow agents correlate behavior
via shared source randomness. use simple example illustrate utility
correlation partially observable domains agents limited memory.
example generalizes one given Singh (1994) illustrate utility stochastic
policies partially observable settings containing single agent.
Consider DEC-POMDP shown Figure 6. problem two states, two agents,
two actions per agent (A B). agents one observation,
thus cannot distinguish two states. example, consider
memoryless policies.
Suppose agents independently randomize behavior using distributions
P (a1 ) P (a2 ). agents choose either B according uniform distribution,
receive expected reward R2 per time step, thus expected long-term
R
reward 2(1)
. straightforward show independent policy yields higher
reward one states.
Next, let us consider even larger class policies agents may act
correlated fashion. words, consider joint distributions P (a1 , a2 ). Consider
policy assigns probability 21 pair AA probability 12 pair BB.
yields average reward 0 time step thus expected long-term reward
0. difference rewards obtained independent correlated policies
made arbitrarily large increasing R.
105

fiBernstein, Amato, Hansen, & Zilberstein

4.1.3 Correlated Joint Controllers
previous subsection, established correlation useful face
limited memory. subsection, extend definition joint controller allow
correlation among agents. this, introduce additional finite-state machine,
called correlation device, provides extra signals agents time step.
device operates independently DEC-POMDP process, thus provide
agents information agents observations. fact, random numbers
necessary operation could determined prior execution time made available
agents.
Formally, correlation device tuple hQc , c i, Qc set nodes c :
Qc Qc state transition function. step, device undergoes transition,
agent observes state.
must modify definition local controller take state correlation
device input. Now, local controller agent conditional distribution
form P (ai , qi0 |qc , qi , oi ). correlation device together local controllers form
joint conditional distribution P (~a, ~q 0 |~q, ~o), ~q = hqc , q1 , . . . , qn i. refer
correlated joint controller. Note correlated joint controller |Qc | = 1
effectively independent joint controller. Figure 7 contains graphical representation
probabilistic dependencies correlated joint controller.
value function correlated joint controller computed solving
~
following system linear equations, one ~q Q:

V (s, ~q) =

X



P (~a|~q) R(s, ~a) +

X

P (s0 , ~o|s, ~a)P (~q 0 |~q, ~a, ~o)V (s0 , ~q 0 ) .

s0 ,~
o,~
q0

~a

sometimes refer value controller initial state distribution.
distribution b, defined
V (b) = max
q~

X

b(s)V (s, ~q).



assumed that, given initial state distribution, controller started joint
node maximizes value distribution.
worth noting correlation increase value set fixed-size controllers,
value achieved larger set uncorrelated controllers. Thus,
correlation way make better use limited representation size, required
produce set optimal controllers. formalized following theorem,
proved Appendix A. theorem asserts existence uncorrelated controllers;
determining much extra memory needed replace correlation device remains
open problem.
Theorem 1 Given initial state correlated joint controller, always exists
finite-size joint controller without correlation device produces least
value initial state.
106

fiPolicy Iteration DEC-POMDPs

a1

q1

q2

a2

q2

o2

qc
o1

q1

Figure 7: graphical representation probabilistic dependencies correlated joint
controller two agents.

example above, higher value achieved two node uncorrelated controllers agent. problem starts s1 , first node agent would choose
transition second node would choose B. second node would
transition back first node. resulting policy consists agents alternating
R
choosing AA BB, producing expected long-term reward 1

higher correlated one node policy value 0. Thus, doubling memory
agent problem sufficient remove correlation device.
4.2 Policy Iteration
section, describe policy iteration algorithm. first extend definitions
exhaustive backup value-preserving transformation multiagent case. Following
that, provide description complete algorithm, along convergence proof.
4.2.1 Exhaustive Backups
introduced exhaustive backups section dynamic programming POMDPs.
stated one way implement DP update perform exhaustive backup,
prune dominated nodes created. efficient implementations
described thereafter. implementations involved interleaving pruning node generation.
multiagent case, open problem whether pruning interleaved
node generation. Nodes removed, show later subsection,
convergence require exhaustive backups. define DP updates multiagent
case, instead make exhaustive backups central component algorithm.
exhaustive backup adds nodes local controllers agents once,
leaves correlation device unchanged. agent i, |Ai ||Qi ||i | nodes added

Q local controller, one one-step policy. Thus, joint controller grows
|Qc | |Ai ||Qi ||Oi | joint nodes.
Note repeated application exhaustive backups amounts brute force search
space deterministic policies. converges optimality, obviously quite
inefficient. single agent case, must modify joint controller
107

fiBernstein, Amato, Hansen, & Zilberstein

adding new nodes. convergence, modifications must preserve value sense
made formal following section.
4.2.2 Value-Preserving Transformations
extend definition value-preserving transformation multiagent case.
following subsection, show definition allows convergence optimality
number iterations grows.
dual interpretation dominance helpful understanding multiagent valuepreserving transformations. Recall POMDP, say node dominated
convex combination nodes value least high states. Though
defined value-preserving transformation terms value function across belief
states, could equivalently defined every node original controller
dominating convex combination new controller.
multiagent case, concept belief state MDP, take
second approach mentioned above. particular, require dominating convex
combinations exist nodes local controllers correlation device. transformation controller C controller qualifies value-preserving transformation
C D, defined below.
~ R,
~ respectively.
Consider correlated joint controllers C node sets Q
say C exist mappings fi : Qi Ri agent fc : Qc Rc

X
V (s, ~q)
P (~r|~q)V (s, ~r)
~
r

~ Note relation transitive value-preserving
~q Q.
transformations also value-preserving transformations C.
~ R.
~ Examples
sometimes describe fi fc single mapping f : Q
efficient value-preserving transformations given later section. following subsection, show alternating exhaustive backups value-preserving
transformations yields convergence optimality.
4.2.3 Algorithmic Framework
policy iteration algorithm initialized arbitrary correlated joint controller.
first part iteration, controller evaluated via solution system linear
equations. Next, exhaustive backup performed add nodes local controllers.
Finally, value-preserving transformations performed.
contrast single agent case, Bellman residual testing convergence -optimality. resort simpler test -optimality based discount
rate number iterations far. Let |Rmax | largest absolute value
immediate reward possible DEC-POMDP. algorithm terminates iteration
t+1 |R
max |
1
. point, due discounting, value policy step
less . Justification test provided convergence proof. complete
algorithm sketched Table 5.
proving convergence, state key lemma regarding ordering exhaustive
backups value-preserving transformations. proof deferred Appendix.
108

fiPolicy Iteration DEC-POMDPs

Input: correlated joint controller, parameter .
1. Evaluate correlated joint controller solving system linear equations.
2. Perform exhaustive backup add deterministic nodes local controllers.
3. Perform value-preserving transformations controller.
t+1

|Rmax |
4. 1
, number iterations far, terminate. Else go
step 1.

Output: correlated joint controller -optimal states.
Table 5: Policy Iteration DEC-POMDPs.
Lemma 1 Let C correlated joint controllers, let C results
performing exhaustive backups C D, respectively. C C D.
Thus, value-preserving transformation mapping controller C
exhaustively backed up, value-preserving transformation mapping controller
C D. allows value-preserving transformations performed exhaustive
backups, ensuring value lost backup. state prove
main convergence theorem policy iteration.
Theorem 2 , policy iteration returns correlated joint controller -optimal
initial states finite number iterations.
Proof: Repeated application exhaustive backups amounts brute force search
space deterministic joint policies. Thus, exhaustive backups, resulting
controller optimal steps initial state. Let integer large enough
t+1 |Rmax |
. possible discounted sum rewards time steps small
1
enough optimality time steps implies -optimality infinite horizon.
recall lemma, states performing value-preserving transformations backup provides least much value performing backup.
inductive argument, performing steps policy iteration value-preserving transformation result exhaustive backups. argued large enough t, value
controller resulting exhaustive backups within optimal states.
Thus, result steps policy iteration also within optimal states. 2
4.3 Efficient Value-Preserving Transformations
section, describe extend controller reductions bounded backups
multiagent case. show operations value-preserving
transformations.
4.3.1 Controller Reductions
Recall single agent case, node removed belief states,
another node value least high. equivalent dual interpretation node
109

fiBernstein, Amato, Hansen, & Zilberstein

removed exists convex combination nodes value least
high across entire state space.
Using dual interpretation, extend rule removing nodes
multiagent case. rule applies removing nodes either local controller
correlation device. Intuitively, considering removal node local controller
correlation device, consider nodes controllers part
hidden state.
precisely, suppose considering removing node qi agent local controller. this, need find distribution P (qi ) nodes qi Qi \ qi
S, qi Qi , qc Qc ,
V (s, qi , qi , qc )

X

P (qi )V (s, qi , qi , qc ).

qi

Qi represents set nodes agents. Finding distribution
formulated linear program, shown Table 6a. case, success
finding parameters 0. linear program polynomial sizes
DEC-POMDP controllers, exponential number agents.
successful finding parameters make 0, merge
dominated node convex combination nodes changing incoming links
dominated controller node redirected based distribution P (qi ).
point, chance ever transitioning qi , thus removed.
rule correlation device similar. Suppose considering
removal node qc . case, need find distribution P (qc ) nodes qc Qc \ qc
~
~q Q,
V (s, ~q, qc )

X

P (qc )V (s, ~q, qc ).

qc

~ set tuples local controller nodes,
Note abuse notation use Q
excluding nodes correlation device. previous case, finding parameters
done using linear programming. shown Table 6b. linear program also
polynomial sizes DEC-POMDP controllers, exponential
number agents.
following theorem, states controller reductions value-preserving
transformations.
Theorem 3 controller reduction applied either local node node correlation device value-preserving transformation.
Proof: Suppose replaced agent node qi distribution nodes
Qi \ qi . Let us take fi identity map nodes except qi , map
new distribution. take fc identity map, take fj identity
map j 6= i. yields complete mapping f . must show f satisfies
condition given definition value-preserving transformation.
110

fiPolicy Iteration DEC-POMDPs

(a) Variables: , x(qi )
Objective: Maximize
Improvement constraints:
s, qi , qc

V (s, qi , qi , qc ) +

X

x(qi )V (s, qi , qi , qc )

qi

Probability constraints:
X

qi

x(qi ) = 1,

x(qi ) 0

qi

(b) Variables: , x(qc )
Objective: Maximize
Improvement constraints:
s, ~q V (s, ~q, qc ) +

X

x(qc )V (s, ~q, qc )

qc

Probability constraints:
X

qc

x(qc ) = 1,

x(qc ) 0

qc

Table 6: (a) linear program solved find replacement agent node qi .
variable x(qi ) represents P (qi ). (b) linear program solved find
replacement correlation node qc . variable x(qc ) represents P (qc ).

Let Vo value function original controller, let Vn value function
controller qi removed. controller reduction requires
Vo (s, ~q)

X

P (~r|~q)Vo (s, ~r)

~
r

~ Thus,
~q Q.

Vo (s, ~q) =

X

P (~a|~q) R(s, a) +


X

P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)Vo (s0 , ~q 0 )

s0 ,~
o,~
q0

~a




X
~a

P (~a|~q) R(s, a) +


X

P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)

s0 ,~
o,~
q0

111

X
~
r0

P (~r|~q)Vo (s, ~r 0 )

fiBernstein, Amato, Hansen, & Zilberstein


=

X

P (~a|~q) R(s, a) +


X

P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)P (~r|~q)Vo (s, ~r 0 )

s0 ,~
o,~
q 0 ,~
r0

~a

~ Notice formula right Bellman operator
~q Q.
new controller, applied old value function. Denoting operator Tn ,
system inequalities implies Tn Vo Vo . monotonicity, k 0,
Tnk+1 (Vo ) Tnk (Vo ). Since Vn = limk Tnk (Vo ), Vn Vo . sufficient
f satisfy condition definition value-preserving transformation.
argument removing node correlation device almost identical
one given above. 2
4.3.2 Bounded Dynamic Programming Updates
previous section, described way reduce size controller without
sacrificing value. Recall single agent case, could also use bounded backups
increase value controller keeping size fixed. technique
extended multiagent case. previous section, extension relies
improving single local controller correlation device, viewing nodes
controllers part hidden state.
first describe detail improve local controller. this, choose
agent i, along node qi . Then, oi , search new parameters
conditional distribution P (ai , qi0 |qi , oi ).
search new parameters works follows. assume original controller
used second step on, try replace parameters qi better
ones first step. words, look parameters satisfying following
inequality:


X
X
V (s, ~q)
P (~a|~q) R(s, a) +
P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)V (s0 , ~q 0 )
~a

s0 ,~
o,~
q0

S, qi Qi , qc Qc . search new parameters formulated
linear program, shown Table 7a. size polynomial sizes DEC-POMDP
joint controller, exponential number agents.
procedure improving correlation device similar procedure
improving local controller. first choose device node qc , consider changing
parameters first step. look parameters satisfying following inequality:


X
X
P (~a|~q) R(s, a) +
P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)V (s0 , ~q 0 )
V (s, ~q)
~a

s0 ,~
o,~
q0

~
~q Q.
previous case, search parameters formulated linear program.
shown Table 7b. linear program also polynomial sizes DECPOMDP joint controller, exponential number agents.
following theorem states bounded backups preserve value.
112

fiPolicy Iteration DEC-POMDPs

(a) Variables: , x(qc , ai ), x(qc , ai , oi , qi0 )
Objective: Maximize
Improvement constraints:
s, qi , qc

X

V (s, ~q, qc ) +

P (ai |qc , qi )[x(qc , ai )R(s, ~a) +

~a



X

0
x(c, ai , oi , qi0 )P (qi
|qc , qi , ai , oi )

s0 ,~
o,~
q 0 ,qc0

P (~o, s0 |s, ~a)P (qc0 |qc )V (s0 , ~q 0 , qc0 )]

Probability constraints:
X
qc
x(qc , ai ) = 1,

qc , ai , oi

x(qc , ai , oi , qi0 ) = x(qc , ai )

qi0

ai

qc , ai

X

x(qc , ai ) 0,

qc , ai , oi , qi0

x(qc , ai , oi , qi0 ) 0

(b) Variables: , x(qc0 )
Objective: Maximize
Improvement constraints:
s, ~q V (s, ~q, qc ) +

X

P (~a|qc , ~q)[R(s, ~a) +

X

P (~q 0 |qc , ~q, ~a, ~o)

s0 ,~
o,~
q 0 ,qc0

~a
0

P (s , ~o|s, ~a)x(qc0 )V (s0 , ~q 0 , qc0 )]

Probability constraints:
qc0

X

x(qc0 ) = 1,

qc0

x(qc0 ) 0

qc0

Table 7: (a) linear program used find new parameters agent node qi .
variable x(qc , ai ) represents P (ai |qi , qc ), variable x(qc , ai , oi , qi0 ) represents
P (ai , qi0 |qc , qi , oi ). (b) linear program used find new parameters
correlation device node qc . variable x(qc0 ) represents P (qc0 |qc ).

113

fiBernstein, Amato, Hansen, & Zilberstein

Theorem 4 Performing bounded backup local controller correlation device
produces new correlated joint controller value-preserving transformation
original.
Proof: Consider case node qi agent local controller changed.
define f deterministic mapping nodes original controller
corresponding nodes new controller.
Let Vo value function original controller, let Vn value function
new controller. Recall new parameters P (ai , qi0 |qc , qi , oi ) must satisfy
following inequality S, qi Qi , qc Qc :


X
X
Vo (s, ~q)
P (~a|~q) R(s, a) +
P (~q 0 |~q, ~a, ~o)P (s0 , ~o|s, ~a)Vo (s0 , ~q 0 ) .
~a

s0 ,~
o,~
q0

Notice formula right Bellman operator new controller, applied
old value function. Denoting operator Tn , system inequalities implies
Tn Vo Vo . monotonicity, k 0, Tnk+1 (Vo ) Tnk (Vo ). Since
Vn = limk Tnk (Vo ), Vn Vo . Thus, new controller value-preserving
transformation original one.
argument changing nodes correlation device almost identical one
given above. 2
4.4 Open Issues
noted beginning section known way convert DECPOMDP equivalent belief-state MDP. Despite fact, able develop
provably convergent policy iteration algorithm. However, policy iteration algorithm
POMDPs desirable properties besides convergence, yet
able extend multiagent case. Two properties described below.
4.4.1 Error Bounds
first property existence Bellman residual. single agent case,
possible compute bound distance optimality using two successive value
functions. multiagent case, policy iteration produces sequence controllers,
value function. However, way obtain error bound
value functions. now, bound distance optimality, must consider
discount rate number iterations completed.
4.4.2 Avoiding Exhaustive Backups
performing DP update POMDPs, possible remove certain nodes
consideration without first generating them. Section 3, gave high-level description
different approaches this. DEC-POMDPs, however, define
DP update instead used exhaustive backups way expand controller. Since
exhaustive backups expensive, would useful extend sophisticated
pruning methods POMDPs multiagent case.
114

fiPolicy Iteration DEC-POMDPs

Input: joint controller, desired number centralized belief points k, initial state
b0 fixed policy agent .
1. Starting b0 , sample set k belief points agent assuming
agents use fixed policy.
2. Evaluate joint controller solving system linear equations.
3. Perform exhaustive backup add deterministic nodes local controllers.
4. Retain nodes contribute highest value belief points.
5. agent, replace nodes lower value combination
nodes belief point.
6. controller sizes parameters change terminate. Else go step 2.
Output: new joint controller based sampled centralized belief points.
Table 8: Heuristic Policy Iteration DEC-POMDPs.

Unfortunately, case POMDPs, proofs correctness methods
use fact exists Bellman equation. Roughly speaking, equation allows us
determine whether potential node dominated analyzing nodes would
successors. currently analog Bellman equation
DEC-POMDPs, able generalize results.
one exception statement, however. exhaustive backup
performed agents except one, type belief state space
constructed agent question using system states nodes
agents. POMDP node generation methods applied agent.
general, though, seems difficult rule node one agent generating
nodes agents.

5. Heuristic Policy Iteration
optimal policy iteration method shows set controllers value arbitrarily close optimal found, resulting controllers may large many
unnecessary nodes may generated along way. exacerbated fact
algorithm cannot take advantage initial state distribution must attempt
improve controller initial state. way combat disadvantages,
developed heuristic version policy iteration removes nodes based
value given set centralized belief points. call centralized belief points
distributions system state general could known
full observability problem. result, algorithm longer optimal,
often produce concise controllers higher solution quality given
initial state distribution.
115

fiBernstein, Amato, Hansen, & Zilberstein

5.1 Directed Pruning
heuristic policy iteration algorithm uses sets belief points direct pruning process algorithm. two main advantages approach: allows simultaneous pruning agents focuses controller certain areas belief space.
first discuss benefits simultaneous pruning mention advantages
focusing small areas belief space.
mentioned above, pruning method used optimal algorithm always
remove nodes could removed agents controllers without losing
value. pruning requires agent consider controllers agents,
nodes removed one agent, agents may able prune nodes. Thus
pruning must cycle agents ceases agent remove
nodes. time consuming causes controller much larger
needs be.
Like game theoretic concept incredible threats1 , set suboptimal policies
agent may useful agents may employ similarly suboptimal policies. is, pruning conducted agent holding agents
policies fixed, polices useful set agent policies retained,
matter quality agent policies. agents policies may
retained highest value used conjunction suboptimal policies agents. cases, removing set suboptimal
policies simultaneously controller size reduced least maintaining value.
simultaneous pruning could reduce controller sizes thus increase scalability
solution quality. may possible define value-preserving transformation
problems, finding nontrivial automated way maintaining optimality algorithm remains open question.
advantage considering smaller part state space already shown
produce drastic performance increases POMDPs (Ji, Parr, Li, Liao, & Carin, 2007;
Pineau, Gordon, & Thrun, 2003) finite-horizon DEC-POMDPs (Seuken & Zilberstein,
2007; Szer & Charpillet, 2006). POMDPs, problem many states belief
space large dimensionality, many parts may never visited optimal policy.
Focusing subset belief states allow large part state space ignored
without significant loss solution quality.
problem large state space compounded DEC-POMDP case.
uncertainty state, also policies agents.
consequence, generalized belief space includes possible distributions
states system current policies agents must considered guarantee
optimality. results huge space contains many unlikely states policies.
uncertainty policies agents may utilize allow belief updates
normally calculated DEC-POMDPs, showed above, done
assuming probability distribution actions agents. limits number
policies need considered agents distributions chosen well,
may permit high-valued solution found.
1. incredible threat irrational strategy agent knows receive lower value choosing
it. possible agent choose incredible threat strategy, irrational so.

116

fiPolicy Iteration DEC-POMDPs

Variables: , x(qi ) belief point b
Objective: Maximize
Improvement constraints:

b, qi

X

X

b(s)
x(qi )V (qi , qi , s) V (~q, s)



X

Probability constraints:

qi

x(qi ) = 1 qi x(qi ) 0

qi

Table 9: linear program used determine node q agent dominated
point b initial nodes agents controllers. node q
may dominated distribution nodes, variable x(qi ) represents P (qi ),
probability starting node q agent i.

5.2 Belief Set Generation
mentioned above, heuristic policy iteration algorithm constructs sets belief points
agent later used evaluate joint controller remove dominated
nodes. generate belief point set, start initial state making assumptions agents, calculate resulting belief state action
observation pair agent. fixing policies agents, belief state update calculated way similar described POMDPs section 3.1.1.
procedure repeated resulting belief state desired number
points generated new points visited.
formally, assume agents fixed distribution action choice
system state. is, know P (~ai |s) determine probability
state results given belief point agents action observation. derivation
likelihood state s0 , given belief state b, agent action ai observation oi
shown below.

P (s0 |ai , oi , b) =

X

P (s0 , ~ai , ~oi , s|ai , oi , b)

~ai ,~
oi ,s

o|s, b, ~a, s0 )P (s0 , s, ~a, b)
~ai ,~
oi ,s P (~

P
=

P (oi , ai , b)
o|s, ~a, s0 )P (s0 |s, ~a, b)P (~a, s, b)
~ai ,~
oi ,s P (~

P
=

P (oi , ai , b)
o|s, ~a, s0 )P (s0 |s, ~a)P (~ai |a, s, b)P (~a, s, b)
~ai ,~
oi ,s P (~

P
=

P (oi , ai , b)
0 )P (s0 |s, ~
P
(~

|s,
~

,

a)P (~ai |ai , s, b)P (s|ai , b)P (ai , b)
~ai ,~
oi ,s

P
=

P (oi , ai , b)
117

fiBernstein, Amato, Hansen, & Zilberstein

P
=

o|s, ~a,
~ai ,~
oi ,s P (~

0 )P (s0 |s, ~
a)P (~ai |s)b(s)

P (oi |ai , b)


X

P (oi |ai , b) =

P (~o|s, ~a, s0 )P (s0 |s, ~a)P (~ai |s)b(s)

ai ,oi ,s,s0

Thus, given action probabilities agents, i, transition observation models system, belief state update calculated.
5.3 Algorithmic Framework
provide formal description approach Table 8. Given desired number
belief points, k, random action observation selection agent, sets
points generated described above. search begins initial state
problem continues given number points obtained. new points
found, process repeated ensure diverse set produced. arbitrary initial
controller evaluated value state initial node agents
controller retained. exhaustive backup procedure exactly one used
optimal algorithm, updating controller takes place two steps. First,
k belief points, highest-valued set initial nodes found. accomplish
this, value beginning combination nodes agents calculated
k points best combination kept. allows nodes contribute
values simultaneously pruned. Next, node agent pruned
using linear program shown Table 9. distribution nodes given agent
higher value belief points initial nodes agents controllers,
pruned replaced distribution. new controllers evaluated
value compared value previous controller. process backing
pruning continues controller parameters continue change.
Similar bounded policy updates used conjunction pruning
optimal policy iteration algorithm, nonlinear programming approach (Amato et al., 2007)
used improve solution quality heuristic case. accomplish this, instead
optimizing controller initial belief state problem, belief points
considered used. simple way achieve maximize sum
values initial nodes controllers weighted probabilities given
point. approach used pruning step may improve value
controllers.

6. Dynamic Programming Experiments
section describes results experiments performed using policy iteration.
flexibility algorithm, impossible explore possible ways implementing
it. However, experiment different implementation strategies gain
idea algorithm works practice. experiments run 3.40GHz
Intel Pentium 4 2GB memory. Three main sets experiments performed
single set test problems.
118

fiPolicy Iteration DEC-POMDPs

first set experiments focused exhaustive backups controller reductions.
results confirm value improvement obtained iterated application
two operations. improvement demonstrated also incorporating bounded
updates. However, exhaustive backups expensive, algorithm unable
complete iterations test problems.
second set experiments, addressed complexity issues using
bounded backups, exhaustive backups. bounded backups, able
obtain higher-valued controllers keeping memory requirements fixed. examined
sizes initial local controllers correlation device affected value
final solution.
third set experiments examined complexity issues caused exhaustive backups using point-based heuristic. allowed heuristic policy iteration algorithm
complete iterations optimal algorithm so, increased solution
quality largest solvable controllers. incorporating Amato et al.s NLP approach,
heuristic algorithm becomes slightly less scalable heuristic pruning alone,
amount value improvement per step increases. causes resulting controllers
domain highest value approach.
6.1 Test Domains
section, describe three test domains, ordered size problem representation. problem, transition function, observation function, reward
functions described. addition, initial state specified. Although policy iteration
require initial state input, one commonly assumed used
heuristic version algorithm. different initial states tried problem,
qualitatively similar results obtained. domains, discount factor 0.9
utilized.
loose upper bound, centralized policy calculated problem
agents share observations central agent decisions agents
made central agent. results POMDP number states,
action observation sets Cartesian products agents action observation
sets. value POMDP policy provided below, DEC-POMDP policies
constrained, optimal value may much lower.
Two Agent Tiger Problem
two agent tiger problem consists 2 states, 3 actions 2 observations (Nair et al.,
2003). domain includes two doors, one leads tiger large
treasure. agent may open one doors listen. either agent opens door
tiger behind it, large penalty given. door treasure behind
opened tiger door not, reward given. agents choose action
(i.e., opening door) larger positive reward smaller penalty given
reward cooperation. agent listens, small penalty given observation seen
noisy indication door tiger behind. listening change
location tiger, opening door causes tiger placed behind one
119

fiBernstein, Amato, Hansen, & Zilberstein

door equal probability. problem begins tiger equally likely located
behind either door. optimal centralized policy problem value 59.817.
Meeting Grid
problem, 16 states, 5 actions 4 observations, two robots must navigate
two-by-two grid. robot sense whether walls left right,
goal spend much time possible square agent.
actions move up, down, left, right, stay square. robot
attempts move open square, goes intended direction probability
0.6, otherwise either goes another direction stays square. move
wall results staying square. robots interfere
cannot sense other. reward 1 agents share square,
0 otherwise. initial state places robots diagonally across
optimal centralized policy problem value 7.129.
Box Pushing Problem
problem, 100 states, 4 actions 5 observations consists two agents
get rewarded pushing different boxes (Seuken & Zilberstein, 2007). agents begin
facing bottom corners four-by-three grid available actions
turning right, turning left, moving forward staying place. 0.9 probability
agent succeed moving otherwise stay place, two agents
never occupy square. middle row grid contains one large box
middle two small boxes. small boxes moved single agent, large
box moved agents pushing time. upper row
grid considered goal row, boxes pushed into. possible deterministic
observations agent consist seeing empty space, wall, agent, small
box large box. reward 100 given agents push large box
goal row 10 given small box moved goal row. penalty -5
given agent cannot move -0.1 given time step. box
moved goal row, environment resets original start state. optimal
centralized policy problem value 183.936.
6.2 Exhaustive Backups Controller Reductions
section, present results using exhaustive backups together controller
reductions. domain, initial controllers agent contained single node
self loop, correlation device. problem, first action
problem description used. resulted repeated actions opening
left door two agent tiger problem, moving meeting grid problem
turning left box pushing problem. reason starting smallest possible
controllers see many iterations could complete running memory.
iteration, performed exhaustive backup, alternated
agents, performing controller reductions nodes could removed. bounded
dynamic programming results, reductions completed bounded updates
also performed agents. experiments, attempted improve nodes
120

fiPolicy Iteration DEC-POMDPs

Iteration
0
1
2
3

Two Agent Tiger, |S| = 2, |Ai | = 3, |i | = 2
Exhaustive Sizes
Controller Reductions
Bounded Updates
(1, 1)
-150 (1,1 1s)
-150 (1,1 1s)
(3, 3)
-137 (3,3 1s)
-20 (3,3 12s)
(27, 27)
-117.8 (15, 15 7s)
-20 (15, 15 89s)
(2187, 2187)
-98.9 (255, 255 1301s) -20* (255, 255 3145s)

Iteration
0
1
2

Meeting Grid, |S| = 16, |Ai | = 5, |i | = 4
Exhaustive Sizes
Controller Reductions
Bounded Updates
(1, 1)
2.8 (1,1 1s)
2.8 (1,1 1s)
(5, 5)
3.4 (5,5 7s)
3.8 (5,5 145s)
(3125, 3125)
3.7 (80,80 821s)
4.78* (125,125 1204s)

Iteration
0
1
2

Box Pushing, |S| = 100, |Ai | = 4, |i | = 5
Exhaustive Sizes
Controller Reductions
Bounded Updates
(1, 1)
-2 (1,1 4s)
-2 (1,1 53s)
(4, 4)
-2 (2,2 108s)
6.3 (2,2 132s)
(4096, 4096)
12.8 (9,9 755s)
42.7* (16,17 714s)

Table 10: Results applying exhaustive backups, controller reductions bounded updates test problems. second column contains sizes controllers
exhaustive backups performed. third column contains
resulting value, sizes controllers, time required controller reductions
performed iteration. fourth column displays quantities bounded updates also used. * denotes backup
pruning performed, bounded updates exhausted given resources.

agent turn value could improved node agent.
iteration, recorded sizes controllers produced, noted sizes would
controller reductions performed. addition, recorded value
initial state total time taken reach given result.
results shown Table 10. exhaustive backups add many nodes,
unable complete many iterations without exceeding memory limits. expected,
smallest problem led largest number iterations completed. Although
could complete many iterations running memory, use controller
reductions led significantly smaller controllers compared approach applying
exhaustive backups. Incorporating bounded updates requires extra time, able
improve value produced step, causing substantial improvement cases.
also interesting notice controller sizes using bounded updates
always controller reductions completed. seen
two iterations meeting grid box pushing problems.
occur bounded updates change node value thus change number
location nodes pruned. box pushing problem, two agents also
121

fiBernstein, Amato, Hansen, & Zilberstein

different size controllers two steps. occur, even symmetric problems,
set actions necessary single agent.
6.3 Bounded Dynamic Programming Updates
saw previous experiments, exhaustive backups fill memory
quickly. leads naturally question much improvement possible without
exhaustive backups. section, describe experiment repeatedly
applied bounded backups, left size controller fixed. experimented
different starting sizes local controllers correlation device.
define trial run algorithm follows. start trial run, size
chosen local controllers correlation device. action selection
transition functions initialized deterministic, outcomes drawn according
uniform distribution. step consists choosing node uniformly random
correlation device one local controllers, performing bounded backup
node. 200 steps, run considered over. practice, found values often
stabilized fewer steps.
varied sizes local controllers maintaining number nodes
agent, varied size correlation device 1 2. domain,
increased number nodes required number steps could completed
four hours. general, runs required significantly less time terminate.
combination sizes, performed 20 trial runs recorded best value runs.
three problems, able obtain solutions higher value
exhaustive backups. Thus, see even though repeated application bounded
backups optimality guarantee, competitive algorithm
does. However, noted performed exhaustive comparison.
could made different design decisions approaches concerning starting
controllers, order nodes considered, factors.
Besides comparing exhaustive backup approach, wanted examine effect
sizes local controllers correlation device value. Figure 8 shows
graph best values plotted controller size. found that, part,
value increases increase size correlation device one node
two nodes (essentially moving independent correlated). worth noting
solution quality somewhat high variance problem, showing setting good
initial parameters important high-valued solutions.
small controllers, best value tends increase controller size. However,
large controllers, always case. explained considering
bounded backup works. new node parameters acceptable, must decrease
value combination states, nodes controllers, nodes
correlation device. becomes difficult numbers nodes increase, thus
easier get stuck local optimum. readily seen two agent tiger
problem extent meeting grid problem. Memory exhausted
phenomenon takes place box pushing problem.
122

fiPolicy Iteration DEC-POMDPs

(a)

(b)

(c)
Figure 8: Best value per trial run plotted size local controllers, (a)
two agent tiger problem, (b) meeting grid problem (c) box
pushing problem. solid line represents independent controllers (a correlation
device one node), dotted line represents joint controller including
two-node correlation device. Times ranged 1s one node controllers
without correlation four hours largest controller found correlation
problem.

6.4 Heuristic Dynamic Programming Updates
observed above, optimal dynamic programming approach complete small
number backups resources exhausted. Similarly, using bounded updates
fixed size controllers generate high value solutions, difficult pick
correct controller size initial parameters. alternative approaches,
also present experiments using heuristic dynamic programming algorithm.
Like optimal policy iteration experiments, initialized single node controllers
agent self loops correlation device. first actions used
backups performed memory exhausted. set belief points
problem generated given initial state distribution distribution
actions agents. meeting grid box pushing problems,
123

fiBernstein, Amato, Hansen, & Zilberstein

assumed agents chose action equal probability regardless state.
two agent tiger problem, assumed state agents listen probability 0.8
open door probability 0.1. simple heuristic policy chosen allow
state space sampled search. number belief points used
two agent tiger meeting grid problems ten twenty points used
box pushing problem.
iteration, performed exhaustive backup pruned controllers
described steps four five Table 8. nodes contributed highest
value belief point retained node examined using linear
program Table 9. results NLP approach, also improved set
controllers heuristic pruning optimizing nonlinear program whose objective
sum values initial nodes weighted belief point probabilities.
report value produced optimal heuristic approaches iteration
could completed four hours memory limits machine used.
nonlinear optimization performed NEOS server, provides set
machines varying CPU speeds memory limitations.
values iteration problem given Figure 9. see heuristic policy iteration (HPI) methods able complete iterations optimal
methods consequence produce higher values. fact, results HPI
almost always exactly optimal policy iteration algorithm without
bounded updates iterations completed optimal approach. Thus,
improvement occurs primarily due larger number backups performed.
also see incorporating bounded updates improves value optimal
algorithm, incorporating NLP approach heuristic approach produces even higher
value. Optimizing NLP requires small time overhead, substantially increases
value iteration. results highest controller value problem. Using
NLP also allows heuristic policy iteration converge six node controller
agent two agent tiger problem. Unfortunately, solution known
suboptimal. heuristic algorithm, unexpected, noted
even suboptimal solutions heuristic approach outperform methods
test problems.
6.5 Discussion
demonstrated policy iteration used improve correlated
independent joint controllers. showed using controller reductions together
exhaustive backups efficient terms memory using exhaustive backups
alone. However, due complexity exhaustive backups, even approach could
complete iterations test problems.
Using bounded backups alone provided good way deal complexity issues.
bounded backups, able find higher-valued policies previous
approach. experiments, able understand sizes local
controllers correlation device affect final values obtained.
heuristic policy iteration algorithm, demonstrated improvement
dealing complexity issues. heuristic approach often able continue
124

fiPolicy Iteration DEC-POMDPs

(a)

(b)

(c)
Figure 9: Comparison dynamic programming algorithms (a) two agent tiger
problem, (b) meeting grid problem (c) box pushing problem.
value produced policy iteration without bounded backups
well heuristic policy iteration without optimizing NLP
compared iteration time memory limit reached.

improving solution quality past point optimal algorithm exhausts resources.
efficient use limited representation size achieved incorporating NLP
approach well. fact, heuristic algorithm NLP improvements step
provided results least equal highest value obtained problem
sometimes markedly higher approaches. Furthermore, far
know, results highest published values three test domains.

7. Conclusion
present policy iteration algorithm DEC-POMDPs. algorithm uses novel policy representation consisting stochastic finite-state controllers agent along
correlation device. define value-preserving transformations show alternating
exhaustive backups value-preserving transformations leads convergence
125

fiBernstein, Amato, Hansen, & Zilberstein

optimality. also extend controller reductions bounded backups single agent
case multiagent case. operations value-preserving transformations
provably efficient. Finally, introduced heuristic version algorithm
scalable produces higher values test problems. algorithm serves
first nontrivial exact algorithm DEC-POMDPs, provides bridge large
body work dynamic programming POMDPs.
work provides solid foundation solving DEC-POMDPs, much work remains
addressing challenging problem instances. focused solving general DECPOMDPs, efficiency approaches could improved using structure found
certain problems. would allow specialized representations solution techniques
incorporated. describe key challenges general approach, along
preliminary algorithmic ideas extend work policy iteration.
Approximation Error Bounds Often, strict optimality requirements cause computational difficulties. good compromise search policies within
bound optimal. framework easily generalized allow this.
Instead value-preserving transformation, could define -value-preserving transformation, insures value states decreases . perform
transformations modifications linear programs. simply need
relax requirement value returned. easily shown using
-value-preserving transformation step leads convergence policy

within 1
optimal states.
controller reductions, relaxing tolerance may lead smaller controllers
value sacrificed. bounded backups, may help escaping local
optima. Though relaxing tolerance bounded backup could lead decrease
value states, small downward step could lead higher value overall
long run. currently working testing hypotheses empirically.
General-Sum Games general-sum game, set agents,
set strategies, strategy profile defined tuple strategies agents.
agent assigns payoff strategy profile. agents may noncooperative,
strategy profile may assigned different values agent.
DEC-POMDP model extended general-sum game allowing
agent reward function. case, strategies local policies,
strategy profile joint policy. model often called partially observable stochastic
game (POSG). Hansen et al. (2004) presented dynamic programming algorithm finitehorizon POSGs. algorithm shown perform iterated elimination dominated
strategies game. Roughly speaking, eliminates strategies useful
agent, regardless strategies agents.
Work remains done extending notion value-preserving transformation
noncooperative case. One possibility redefine value-preserving transformations
value preserved agents. closely related idea Pareto
optimality. general-sum game, strategy profile said Pareto optimal
exist another strategy profile yields higher payoff agents. seems
policy iteration using revised definition value-preserving transformation would tend
move controller direction Pareto optimal set. Another possibility
126

fiPolicy Iteration DEC-POMDPs

define value-preserving transformations respect specific agents. agent
transforms controller, joint controller move towards Nash equilibrium.
Handling Large Numbers Agents general DEC-POMDP representation presented paper grows exponentially number agents, seen growth
set joint actions observations well transition, reward observation
functions. Thus representation feasible large numbers agents. However,
compact representation possible agent interacts directly
agents. separate state space agent, factored transition probabilities,
reward function sum local reward functions clusters agents.
case, problem size exponential maximum number agents interacting
directly. idea closely related recent work graphical games (La Mura, 2000;
Koller & Milch, 2003).
compact representation, next question answer whether
adapt policy iteration work efficiently representation. indeed seems
possible. value-preserving transformations presented, nodes
agents considered part hidden state agent consideration.
techniques modify controller agent get value improvement possible
hidden states. agents state transitions rewards depend
agent, need consider agents nodes part hidden state.
specific compact representation along extensions different algorithms proposed
Nair et al. (2005).

Acknowledgments
thank Martin Allen, Marek Petrik Siddharth Srivastava helpful discussions
work. Marek Siddharth, particular, helped formalize prove Theorem 1.
anonymous reviewers provided valuable feedback suggestions. Support work
provided part National Science Foundation grants IIS-0535061
IIS-0812149, NASA cooperative agreement NCC-2-1311, Air Force
Office Scientific Research grants F49620-03-1-0090 FA9550-08-1-0181.

Appendix A. Proof Theorem 1
correlation device produces sequence values agents observe. Let X
set possible infinite sequences generated correlation device.
Let Vx (~q0 , s0 ) value correlated joint controller respect correlation
sequence x X, initial nodes ~q0 agent controllers, initial state s0 problem.
refer Vx (~q0 , s0 ) simply Vx value sequence x, given controllers
agents. define regular sequence sequence generated
regular expression. prove Theorem 1, establish following property.
Lemma 2 value sequence, whether regular non-regular, approximated
within sequence.
Proof: property holds thanks discount factor used infinite-horizon DECPOMDPs. Given sequence x value Vx , determine another sequence x0
127

fiBernstein, Amato, Hansen, & Zilberstein

|Vx0 Vx | < . sequence x0 constructed choosing first k elements x,
choosing arbitrary regular non-regular sequence remaining elements.
kR
max
long k chosen (1)
, |Vx0 Vx | < . 2
Theorem 1 Given initial state correlated joint controller, always exists
finite-size joint controller without correlation device produces least
value initial state.
Proof: Let E represent expected value joint controller correlation
device. Let V = {Vx | x X} set values produced possible correlation
device sequences. Let inf sup represent infimum supremum V respectively.
break proof two cases, depending relation expectation versus
supremum. show case regular sequence found produces
least value E. regular sequence found, sequence
generated finite-state controller embedded within agent. Thus,
finite number nodes added agents controllers provide equal greater
value, without using correlation device.
Case (1) inf E < sup
Based Lemma 2, regular sequence x approximate supremum
within . choose = sup E, Vx sup = E.
Case (2) E = sup
regular sequence, x, Vx = E, choose sequence.
regular sequence exists, show E 6= sup. give somewhat informal
argument, formally proven using cylinder sets discussed Parker
(2002). begin first choosing regular sequence. construct neighborhood around sequence (as described Lemma 2) choosing fixed length prefix

prefixP
length k well-defined probability defined
P sequence.
P
0)
1 |q 0 ) . . .
k1 |q k2 ) P (q 0 ) probability distribution
P
(q
P
(q
c
c c
c
c
qc0
qc1
qck1 P (qc
initial node correlation device P (qci |qci1 ) represents probability transitioning correlation device node qci node qci1 . set sequences possess
prefix probability equal prefix. assumed exists
regular sequence value less supremum, always choose prefix
length values sequences set less supremum.
probability set nonzero value sequences less
supremum, E 6= sup, contradiction.
Therefore, regular sequence found provides least value
expected value correlated joint controller. allows uncorrelated joint
controller produce least value given correlated one. 2

Appendix B. Proof Lemma 1
ease exposition, prove lemma assumption correlation
device. Including correlation device straightforward unnecessarily tedious.
128

fiPolicy Iteration DEC-POMDPs

Lemma 1 Let C correlated joint controllers, let C results
performing exhaustive backups C D, respectively. C C D.
Proof: Suppose given controllers C D, C D. Call sets joint
~ R,
~ respectively. follows exists function
nodes controllers Q
~
fi : Qi Ri agent ~q Q
V (s, ~q)

X

P (~r|~q)V (s, ~r).

~
r

define functions fi map two controllers C D. old
nodes, define fi produce output fi . remains specify results fi
applied nodes added exhaustive backup. New nodes C mapped
distributions involving new nodes D.
describe mapping formally, need introduce new notation. Recall
new nodes deterministic. new node ~r controller D, nodes action
denoted ~a(~r), transition rule denoted ~r 0 (~r, ~o). Now, mappings fi defined

P (~r|~q) = P (~a(~r)|~q)

YX

P (~q 0 |~q, ~a(~r), ~o)P (~r 0 (~r, ~o)|~q 0 )

q~ 0

~


~q controller C ~r controller D.
must show mapping f satisfies inequality given definition
value-preserving transformation. nodes added exhaustive
backup, straightforward. new nodes ~q controller C,
S,

V (s, ~q) =

X

P (~a|~q) R(s, ~a) +


X

P (s0 , ~o|s, ~a)P (~q 0 |~q, ~a, ~o)V (s0 , ~q 0 )

~
o,s0 ,~
q0

~a






X

P (~a|~q) R(s, ~a) +

X

P (s0 , ~o|s, ~a)P (~q 0 |~q, ~a, ~o)

~
o,s0 ,~
q0

~a

X

P (~r 0 |~q 0 )V (s0 , ~r 0 )

~
r0


=

X

P (~a|~q) R(s, ~a) +


X

P (s0 , ~o|s, ~a)P (~q 0 |~q, ~a, ~o)P (~r 0 |~q 0 )V (s0 , ~r 0 )

~
o,s0 ,~
q 0 ,~
r0

~a


=

X

=

X

P (~r|~q) R(s, ~a(~r)) +


X

P (s0 , ~o|s, ~a(~r))V (s0 , ~r 0 (~r, ~o))

~
o,s0

~
r

P (~r|~q)V (s, ~r).

~
r

2
129

fiBernstein, Amato, Hansen, & Zilberstein

References
Amato, C., Bernstein, D. S., & Zilberstein, S. (2007). Optimizing memory-bounded controllers decentralized POMDPs. Proceedings Twenty-Third Conference
Uncertainty Artificial Intelligence.
Astrom, K. J. (1965). Optimal control Markov decision processes incomplete state
estimation. Journal Mathematical Analysis Applications, 10, 174205.
Aumann, R. J. (1974). Subjectivity correlation randomized strategies. Journal
Mathematical Economics, 1, 6796.
Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). complexity decentralized control Markov decision processes. Mathematics Operations Research,
27 (4), 819840.
Bernstein, D. S., Hansen, E. A., & Zilberstein, S. (2005). Bounded policy iteration decentralized POMDPs. Proceedings Nineteenth International Joint Conference
Artificial Intelligence, pp. 12871292.
Bonet, B., & Geffner, H. (2000). Planning incomplete information heuristic search
belief space. Proceedings Fifth International Conference AI Planning
Scheduling, pp. 5261.
Cassandra, A., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: simple, fast,
exact method partially observable Markov decision processes. Proceedings
Thirteenth Annual Conference Uncertainty Artificial Intelligence, pp. 5461.
Cheng, H.-T. (1988). Algorithms Partially Observable Markov Decision Processes. Ph.D.
thesis, University British Columbia.
Emery-Montemerlo, R., Gordon, G., Schnieder, J., & Thrun, S. (2004). Approximate solutions partially observable stochastic games common payoffs. Proceedings
Third International Joint Conference Autonomous Agents Multi Agent
Systems, pp. 136143.
Feng, Z., & Zilberstein, S. (2004). Region-based incremental pruning POMDPs.
Proceedings Twentieth Conference Uncertainty Artificial Intelligence, pp.
146153.
Feng, Z., & Zilberstein, S. (2005). Efficient maximization solving POMDPs. Proceedings Twentieth National Conference Artificial Intelligence, pp. 975980.
Hansen, E. (1998a). Solving POMDPs searching policy space. Proceedings
Fourteenth Annual Conference Uncertainty Artificial Intelligence, pp. 211219.
Hansen, E. A. (1998b). Finite-Memory Control Partially Observable Systems. Ph.D.
thesis, University Massachusetts Amherst, Amherst, Massachusetts.
Hansen, E. A., Bernstein, D. S., & Zilberstein, S. (2004). Dynamic programming partially
observable stochastic games. Proceedings Nineteenth National Conference
Artificial Intelligence, pp. 709715.
Ji, S., Parr, R., Li, H., Liao, X., & Carin, L. (2007). Point-based policy iteration.
Proceedings Twenty-Second National Conference Artificial Intelligence, pp.
12431249.
130

fiPolicy Iteration DEC-POMDPs

Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning acting
partially observable stochastic domains. Artificial Intelligence, 101 (1-2), 99134.
Koller, D., & Milch, B. (2003). Multi-agent influence diagrams representing solving
games. Games Economic Behavior, 45 (1), 181221.
La Mura, P. (2000). Game networks. Proceedings Sixteenth Conference Uncertainty Artificial Intelligence, pp. 335342.
Lark III, J. W. (1990). Applications Best-First Heuristic Search Finite-Horizon Partially Observed Markov Decision Processes. Ph.D. thesis, University Virginia.
Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. (1995). Learning policies partially
observable environments: Scaling up. Proceedings Twelfth International Conference Machine Learning, pp. 362370.
Nair, R., Pynadath, D., Yokoo, M., Tambe, M., & Marsella, S. (2003). Taming decentralized
POMDPs: Towards efficient policy computation multiagent settings. Proceedings
Eighteenth International Joint Conference Artificial Intelligence, pp. 705
711.
Nair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2005). Networked distributed
POMDPs: synthesis distributed constraint optimization POMDPs. Proceedings Twentieth National Conference Artificial Intelligence, pp. 133139.
Parker, D. A. (2002). Implementation Symbolic Model Checking Probabilistic Systems.
Ph.D. thesis, University Birmingham, Birmingham, England.
Peshkin, L., Kim, K.-E., Meuleau, N., & Kaelbling, L. P. (2000). Learning cooperate via
policy search. Proceedings Sixteenth International Conference Uncertainty
Artificial Intelligence, pp. 489496.
Pineau, J., Gordon, G., & Thrun, S. (2003). Point-based value iteration: anytime algorithm POMDPs. Proceedings Eighteenth International Joint Conference
Artificial Intelligence, pp. 10251031.
Platzman, L. K. (1980). feasible computational approach infinite-horizon partiallyobserved Markov decision processes. Tech. rep., Georgia Institute Technology.
Reprinted Working Notes 1998 AAAI Fall Symposium Planning Using
Partially Observable Markov Decision Processes.
Poupart, P., & Boutilier, C. (2003). Bounded finite state controllers. Proceedings
Advances Neural Information Processing Systems 16.
Seuken, S., & Zilberstein, S. (2007). Improved memory-bounded dynamic programming
decentralized POMDPs. Proceedings Twenty-Third Conference Uncertainty Artificial Intelligence.
Simmons, R., & Koenig, S. (1995). Probabilistic navigation partially observable environments. Proceedings Fourteenth International Joint Conference Artificial
Intelligence, pp. 10801087.
Singh, S. (1994). Learning Solve Markovian Decision Processes. Ph.D. thesis, University
Massachusetts, Amherst, Massachusetts.
131

fiBernstein, Amato, Hansen, & Zilberstein

Singh, S. P., Jaakkola, T., & Jordan, M. I. (1994). Learning without state-estimation
partially observable markovian decision processes. Proceedings Eleventh
International Conference Machine Learning, pp. 284292.
Smallwood, R. D., & Sondik, E. J. (1973). optimal control partially observable
Markov processes finite horizon. Operations Research, 21 (5), 10711088.
Smith, T., & Simmons, R. (2005). Point-based POMDP algorithms: Improved analysis
implementation. Proceedings Twenty-First Conference Uncertainty
Artificial Intelligence, pp. 542547.
Smith, T., Thompson, D. R., & Wettergreen, D. S. (2007). Generating exponentially smaller
POMDP models using conditionally irrelevant variable abstraction. Proceedings
Seventeenth International Conference Applied Planning Scheduling.
Sondik, E. J. (1978). optimal control partially observable Markov processes
infinite horizon: Discounted costs. Operations Research, 26, 282304.
Szer, D., & Charpillet, F. (2005). optimal best-first search algorithm solving infinite
horizon DEC-POMDPs. Proceedings Sixteenth European Conference
Machine Learning, pp. 389399.
Szer, D., & Charpillet, F. (2006). Point-based dynamic programming DEC-POMDPs.
Proceedings Twenty-First National Conference Artificial Intelligence, pp.
12331238.
Szer, D., Charpillet, F., & Zilberstein, S. (2005). MAA*: heuristic search algorithm
solving decentralized POMDPs. Proceedings Twenty-First Conference
Uncertainty Artificial Intelligence, pp. 576590.
Witsenhausen, H. S. (1971). Separation estimation control discrete time systems.
Proceedings IEEE, 59 (11), 15571566.
Zhang, N. L., & Lee, S. S. (1998). Planning partially observable Markov decision
processes: Advances exact solution methods. Proceedings Fourteenth
Conference Uncertainty Artificial Intelligence, pp. 523530.
Zhang, N. L., & Zhang, W. (2001). Speeding convergence value iteration partially observable Markov decision processes. Journal Artificial Intelligence Research,
14, 2951.

132

fiJournal Artificial Intelligence Research 34 (2009) 2759

Submitted 12/07; published 01/09

Heuristic Search Approach Planning
Continuous Resources Stochastic Domains
Nicolas Meuleau

nicolas.f.meuleau@nasa.gov

NASA Ames Research Center
Mail Stop 269-3
Moffet Field, CA 94035-1000, USA

Emmanuel Benazera

ebenazer@laas.fr

LAAS-CNRS, Universite de Toulouse
7, av. du Colonel Roche
31077 Toulouse Cedex 4, France

Ronen I. Brafman

brafman@cs.bgu.ac.il

Department Computer Science
Ben-Gurion University
Beer-Sheva 84105, Israel

Eric A. Hansen

hansen@cse.msstate.edu

Department Computer Science Engineering
Mississippi State University
Mississippi State, MS 39762, USA

Mausam

mausam@cs.washington.edu

Department Computer Science Engineering
University Washington
Seattle, WA 981952350, USA

Abstract
consider problem optimal planning stochastic domains resource constraints,
resources continuous choice action step depends resource availability. introduce HAO* algorithm, generalization AO* algorithm performs
search hybrid state space modeled using discrete continuous state variables, continuous variables represent monotonic resources. Like heuristic search
algorithms, HAO* leverages knowledge start state admissible heuristic focus
computational effort parts state space could reached start state
following optimal policy. show approach especially effective resource
constraints limit much state space reachable. Experimental results demonstrate
effectiveness domain motivates research: automated planning planetary
exploration rovers.

1. Introduction
Many NASA planetary exploration missions rely rovers mobile robots carry suite
scientific instruments use characterizing planetary surfaces transmitting information back
Earth. difficulties communicating devices distant planets, direct human
control rovers tele-operation infeasible, rovers must able act autonomously
substantial periods time. example, Mars Exploration Rovers (MER), aka, Spirit
Opportunity, designed communicate ground twice per Martian day.
Autonomous control planetary exploration rovers presents many challenges research
automated planning. Progress made meeting challenges. example,
planning software developed Mars Sojourner MER rovers contributed significantly

c
2009
AI Access Foundation. rights reserved.

fiMeuleau, Benazera, Brafman, Hansen & Mausam

success missions (Bresina, Jonsson, Morris, & Rajan, 2005). many important
challenges must still addressed achieve ambitious goals future missions (Bresina,
Dearden, Meuleau, Ramakrishnan, Smith, & Washington, 2002).
Among challenges problem plan execution uncertain environments. planetary
surfaces Mars, uncertainty terrain, meteorological conditions, state
rover (position, battery charge, solar panels, component wear, etc.) turn, leads
uncertainty outcome rovers actions. Much uncertainty resource
consumption. example, factors slope terrain affect speed movement rate
power consumption, making difficult predict certainty long take rover
travel two points, much power consume so. limits
critical resources time battery power, rover plans currently conservative
based worst-case estimates time resource usage. addition, instructions sent
planetary rovers form sequential plan attaining single goal (e.g., photographing
interesting rock). action unintended outcome causes plan fail, rover
stops waits instructions; makes attempt recover achieve alternative
goal. result under-utilized resources missed science opportunities.
past decade, great deal research generate conditional
plans domains uncertain action outcomes. Much work formalized framework
Markov decision processes (Puterman, 1994; Boutilier, Dean, & Hanks, 1999). However,
Bresina et al. (2002) point out, important aspects rover planning problem adequately
handled traditional planning algorithms, including algorithms Markov decision processes.
particular, traditional planners assume discrete state space small discrete number
action outcomes. automated planning planetary exploration rovers, critical resources
time battery power continuous, uncertainty domain results
effect actions variables. requires conditional planner branch
discrete action outcomes, availability continuous resources, planner
must able reason continuous well discrete state variables.
Closely related challenges uncertain plan execution continuous resources
challenge over-subscription planning. rovers future missions much improved
capabilities. Whereas current MER rovers require average three days visit single rock,
progress areas automatic instrument placement allow rovers visit multiple rocks
perform large number scientific observations single communication cycle (Pedersen,
Smith, Deans, Sargent, Kunz, Lees, & Rajagopalan, 2005). Moreover, communication cycles
lengthen substantially distant missions moons Jupiter Saturn, requiring longer
periods autonomous behavior. result, space scientists future missions expected
specify large number science goals once, often present known oversubscription planning problem. refers problem infeasible achieve goals,
objective achieve best subset goals within resource constraints (Smith, 2004).
case rover, multiple locations rover could reach, many experiments
rover could conduct, combinations infeasible due resource constraints.
planner must select feasible subset maximizes expected science return. action
outcomes (including resource consumption) stochastic, plan maximizes expected science
return conditional plan prescribes different courses action based results
previous actions, including resource availability.
paper, present implemented planning algorithm handles problems
together: uncertain action outcomes, limited continuous resources, over-subscription planning.
formalize rover planning problem hybrid-state Markov decision process, is, Markov
decision process (MDP) discrete continuous state variables, use continuous
variables represent resources. planning algorithm introduce heuristic search algorithm
called HAO*, Hybrid-state AO*. generalization classic AO* heuristic search algorithm (Nilsson, 1980; Pearl, 1984). Whereas AO* searches discrete state spaces, HAO* solves

28

fiHAO*

planning problems hybrid domains discrete continuous state variables. handle
hybrid domains, HAO* builds earlier work dynamic programming algorithms continuous
hybrid-state MDPs, particular, work Feng et al. (2004).
Generalizing AND/OR graph search hybrid state spaces poses complex challenge,
consider special case problem. particular, continuous variables used represent
monotonic resources. search best conditional plan allows branching
values discrete variables, availability resources, violate
resource constraint.
well-known heuristic search efficient dynamic programming
uses reachability analysis guided heuristic focus computation relevant parts state
space. show problems resource constraints, including over-subscription planning
problems, heuristic search especially effective resource constraints significantly limit
reachability. Unlike dynamic programming, systematic forward search algorithm AO* keeps
track trajectory start state reachable state, thus check whether
trajectory feasible violates resource constraint. pruning infeasible trajectories, heuristic
search algorithm dramatically reduce number states must considered find
optimal policy. particularly important domain discrete state space huge
(exponential number goals), yet portion reachable initial state relatively
small, due resource constraints.

2. Problem Formulation Background
start formal definition planning problem tackling. special case
hybrid-state Markov decision process, first define model. discuss
include resource constraints formalize over-subscription planning model. Finally
review class dynamic programming algorithms solving hybrid-state MDPs, since
algorithmic techniques incorporated heuristic search algorithm develop
Section 3.
2.1 Hybrid-State Markov Decision Process
hybrid-state Markov decision process, hybrid-state MDP, factored Markov decision process
discrete continuous state variables. define tuple (N, X, A, P, R),
N discrete state variable, X = {X1 , X2 , ..., Xd } set continuous state variables, set
actions, P stochastic state transition model, R reward function. describe
elements detail below. hybrid-state MDP sometimes referred simply hybrid
MDP. term hybrid refer dynamics model, discrete. Another
term hybrid-state MDP, originates Markov chain literature, general-state
MDP.
Although hybrid-state MDP multiple discrete variables, plays role algorithms described paper, so, notational convenience, model discrete component
state space single variable N . focus continuous component. assume
N
domain continuous variable Xi X closed interval real line, X = Xi
hypercube continuous variables defined. state set hybrid-state
MDP set possible assignments values state variables. particular, hybrid
state pair (n, x) n N value discrete variable, x = (xi ) vector
values continuous variables.
State transitions occur result actions, process evolves according Markovian
state transition probabilities Pr(s0 | s, a), = (n, x) denotes state action
s0 = (n0 , x0 ) denotes state action a, also called arrival state. probabilities
decomposed into:

29

fiMeuleau, Benazera, Brafman, Hansen & Mausam

discrete marginals Pr(n0 |n, x, a). (n, x, a),

Pr(n0 |n, x, a) = 1;
R
continuous conditionals Pr(x0 |n, x, a, n0 ). (n, x, a, n0 ), x0 X Pr(x0 |n, x, a, n0 )dx0 =
1.
P

n0 N

assume reward associated transition function arrival state only, let
Rn (x) denote reward associated transition state (n, x). complex dependencies
possible, sufficient goal-based domain models consider paper.
2.2 Resource Constraints Over-Subscription Planning
model rover planning problem, consider special type MDP objective
optimize expected cumulative reward subject resource constraints. make following
assumptions:
initial allocation one non-replenishable resources,
action minimum positive consumption least one resource,
resources exhausted, action taken.
One way model MDP resource constraints formulate constrained MDP,
model widely studied operations research community (Altman, 1999).
model, action incurs transition-dependent resource cost, Cai (s, s0 ), resource
i. Given initial allocation resources initial state, linear programming used find
best feasible policy, may randomized policy. Although constrained MDP models
resource consumption, include resources state space. result, policy cannot
conditioned upon resource availability. problem resource consumption either
deterministic unobservable. good fit rover domain, resource
consumption stochastic observable, rover take different actions depending
current resource availability.
adopt different approach modeling resource constraints resources included
state description. Although increases size state space, allows decisions
made based resource availability, allows stochastic model resource consumption. Since
resources rover domain continuous, use continuous variables hybrid-state MDP
represent resources. Note duration actions one biggest sources uncertainty
rover problems, model time one continuous resources. Resource constraints
represented form executability constraints actions, (x) denotes set
actions executable state (n, x). action cannot executed state satisfy
minimum resource requirements.
discussed incorporate resource consumption resource constraints hybridstate MDP, next discuss formalize over-subscription planning. rover planning
problem, scientists provide planner set goals would like rover achieve,
goal corresponds scientific task taking picture rock performing
analysis soil sample. scientists also specify utility reward goal. Usually
subset goals feasible resource constraints, problem find feasible
plan maximizes expected utility. Over-subscription planning planetary exploration rovers
considered Smith (2004) van den Briel et al. (2004) deterministic domains.
consider over-subscription planning stochastic domains, especially domains stochastic
resource consumption. requires construction conditional plans selection goals
achieve change depending resource availability.
over-subscription planning, utility associated goal achieved once;
additional utility achieved repeating task. Therefore, discrete state must include set
Boolean variables keep track set goals achieved far rover, one Boolean
30

fiHAO*

variable goal. Keeping track already-achieved goals ensures Markovian reward structure,
since achievement goal rewarded achieved past. However, also
significantly increases size discrete state space. Maintaining history information ensure
Markovian reward structure simple example planning non-Markovian rewards (Thiebaux,
Gretton, Slaney, Price, & Kabanza, 2006).
2.3 Optimality Equation
rover planning problem consider special case finite-horizon hybrid-state MDP
termination occurs indefinite number steps. Bellman optimality equation
problem takes following form:
Vn (x)

=

Vn (x)

=

0 (n, x) terminal state; otherwise,
"

Z
X
max
Pr(n0 | n, x, a)
Pr(x0 | n, x, a, n0 ) (Rn0 (x0 ) + Vn0 (x0 )) dx0 .

aAn (x)

n0 N

(1)

x0

define terminal state state actions eligible execute, is, (x) = .
use terminal states model various conditions plan termination. includes situation
goals achieved; situation resources exhausted;
situation action results error condition requires executing safe sequence
rover terminating plan execution. addition terminal states, assume explicit
initial state denoted (n0 , x0 ).
Assuming resources limited non-replenishable, every action consumes
resource (and amount consumed greater equal positive quantity c), plan
execution terminate finite number steps. maximum number steps bounded
initial resource allocation divided c, minimal resource consumption per step. actual
number steps usually much less indefinite, resource consumption stochastic
choice action influences resource consumption. number steps takes
plan terminate bounded indefinite, call bounded-horizon MDP contrast
finite-horizon MDP. However, note bounded-horizon MDP converted
finite-horizon MDP specifying horizon equal maximum number plan steps,
introducing no-op action taken terminal state.
Note usually difference number plan steps time plan takes
execute. Since model time one continuous resources, time takes execute
plan step state action dependent, stochastic.
Given hybrid-state MDP set terminal states initial state (n0 , x0 ), objective
find policy, : (N X) A, maximizes expected cumulative reward; specifically,
optimal policy value function satisfies optimality equation given Equation (1).
rover domain, cumulative reward equal sum rewards goals achieved
reaching terminal state direct incentive save resources; optimal solution saves
resources allows achieving goals. However, framework general enough
allow reasoning cost availability resources. example, incentive
conserving resources could modeled specifying reward proportional amount
resources left unused upon entering terminal state. Note framework allows reasoning
cost availability resources without needing formulate problem
multi-objective optimization, stay standard decision-theoretic framework.
2.4 Dynamic Programming Continuous-State Hybrid-State MDPs
planning problem consider finite-horizon hybrid-state MDP, solved
algorithm solving finite-horizon hybrid-state MDPs. algorithms solving hybridstate (and continuous-state) MDPs rely form approximation. widely-used approach
31

fiMeuleau, Benazera, Brafman, Hansen & Mausam

Figure 1: Value function initial state simple rover problem: optimal expected return
function two continuous variables (time energy remaining).

discretize continuous state space finite number grid points solve resulting
finite-state MDP using dynamic programming interpolation (Rust, 1997; Munos & Moore,
2002). Another approach parametric function approximation; function associated
dynamic programming problem value function policy function approximated
smooth function k unknown parameters. general, parametric function approximation
faster grid-based approximation, drawback may fail converge, may
converge incorrect solution. Parametric function approximation used algorithms
solving continuous-state MDPs besides dynamic programming. Reinforcement learning algorithms
use artificial neural networks function approximators (Bertsekas & Tsitsiklis, 1996). approach
solving MDPs called approximate linear programming extended allow continuous
well discrete state variables (Kveton, Hauskrecht, & Guestrin, 2006).
review another approach solving hybrid-state (or continuous-state) MDPs assumes
problem special structure exploited dynamic
programming algorithm.
R
structure assumed approach ensures convolution x0 Pr(x0 | n, x, a, n0 )(Rn0 (x0 )+
Vn0 (x0 ))dx0 Equation (1) computed exactly finite time, value function computed
dynamic programming piecewise-constant piecewise-linear. initial idea approach
due work Boyan Littman (2000), describe class MDPs called time-dependent
MDPs, transitions take place along single, irreversible continuous dimension.
describe dynamic programming algorithm computing exact piecewise-linear value function
transition probabilities discrete rewards piecewise linear. Feng et al. (2004)
extend approach continuous state spaces one dimension, consider MDPs
discrete transition probabilities two types reward models: piecewise constant piecewise
linear. Li Littman (2005) extend approach allow transition probabilities
piecewise-constant, instead discrete, although extension requires approximation
dynamic programming algorithm.
problem structure exploited algorithms characteristic Mars rover domain
over-subscription planning problems. Figure 1 shows optimal value functions
initial state typical Mars rover problem function two continuous variables:
time energy remaining (Bresina et al., 2002). value functions feature set humps
plateaus, representing region state space similar goals pursued
optimal policy. sharpness hump plateau reflects uncertainty achieving
goal(s). Constraints impose minimal resource levels attempting actions introduce

32

fiHAO*

sharp cuts regions. Plateau regions expected reward nearly constant represent
regions state space optimal policy same, probability distribution
future histories induced optimal policy nearly constant.
structure value function exploited partitioning continuous state
space finite number hyper-rectangular regions. (A region hyper-rectangle
Cartesian product intervals dimension.) hyper-rectangle, value function
either constant (for piecewise-constant function) linear (for piecewise-linear function).
resolution hyper-rectangular partitioning adjusted fit value function. Large hyperrectangles used represent large plateaus. Small hyper-rectangles used represent regions
state space finer discretization value function useful, edges
plateaus curved hump time energy available. natural choice
data structures rectangular partitioning continuous space kd-trees (Friedman, Bentley,
& Finkel, 1977), although choices possible. Figures 6 10 Section 4.1 show value
functions initial state simple rover planning problem, created piecewise-constant
partitioning continuous state space.
continuous-state domains transition reward functions similarly partitioned
hyper-rectangles. reward function action piecewise-constant (or piecewiselinear) representation value function. transition function partitions state space
regions set outcomes action probability distribution set
outcomes identical. Following Boyan Littman (2000), relative absolute transitions
supported. relative outcome viewed shifting region constant . is,
two states x region, transition probabilitiesP r(x0 |x, a) P r(y 0 |y, a)
defined term probability , = (x0 x) = (y 0 y). absolute outcome
maps states region single state. is, two states x region,
P r(x0 |x, a) = P r(x0 |y, a). view relative outcome pair (, p), p probability
outcome, view absolute outcome pair (x0 , p). assumes
finite number non-zero probabilities, i.e., probability distribution discretized,
means state action, finite set states reached non-zero probability.
representation guarantees dynamic programming update piecewise-constant value
function results another piecewise-constant value function. Feng et al. (2004) show
transition functions finite horizon, exists partition continuous space
hyper-rectangles optimal value function piecewise constant linear.
restriction discrete transition functions strong one, often means transition
function must approximated. example, rover power consumption normally distributed,
thus must discretized. (Since amount power available must non-negative,
implementation truncates negative part normal distribution renormalizes.) continuous transition function approximated appropriately fine discretization, Feng et
al. (2004) argue provides attractive alternative function approximation approaches
approximates model solves approximate model exactly, rather finding
approximate value function original model. (For reason, sometimes refer
finding optimal policies value functions, even model approximated.)
avoid discretizing transition function, Li Littman (2005) describe algorithm allows
piecewise-constant transition functions, exchange approximation dynamic programming algorithm. Marecki et al.(2007) describe different approach class problems
probability distributions resource consumptions represented phase-type distributions dynamic programming algorithm exploits representation. Although use
work Feng et al. (2004) implementation, heuristic search algorithm develop
next section could use approach representing computing value
functions policies hybrid-state MDP.

33

fiMeuleau, Benazera, Brafman, Hansen & Mausam

3. Heuristic Search Hybrid State Space
section, present primary contribution paper: approach solving special
class hybrid-state MDPs using novel generalization heuristic search algorithm AO*.
particular, describe generalization algorithm solving hybrid-state MDPs
continuous variables represent monotonic constrained resources acyclic plan found
search algorithm allows branching availability resources.
motivation using heuristic search potentially huge size state space,
makes dynamic programming infeasible. One reason size existence continuous
variables. even consider discrete component state space, size
state space exponential number discrete variables. well-known, AO*
effective solving planning problems large state space considers states
reachable initial state, uses informative heuristic function focus
states reachable course executing good plan. result, AO* often find
optimal plan exploring small fraction entire state space.
begin section review standard AO* algorithm. consider
generalize AO* search hybrid state space discuss properties generalized
algorithm, well efficient implementations.
3.1 AO*
Recall AO* algorithm AND/OR graph search problems (Nilsson, 1980; Pearl, 1984).
graphs arise problems choices (the components), choice
multiple consequences (the component), case planning uncertainty.
Hansen Zilberstein (2001) show AND/OR graph search techniques used solving
MDPs.
Following Nilsson (1980) Hansen Zilberstein (2001), define AND/OR graph
hypergraph. Instead arcs connect pairs nodes ordinary graph, hypergraph
hyperarcs, k-connectors, connect node set k successor nodes. MDP
represented hypergraph, node corresponds state; root node corresponds start
state, leaf nodes correspond terminal states. Thus often use word state refer
corresponding node hypergraph representing MDP. k-connector corresponds
action transforms state one k possible successor states, probability attached
successor probabilities sum one. paper, assume AND/OR graph
acyclic, consistent assumption underlying MDP bounded-horizon.
AND/OR graph search, solution takes form acyclic subgraph called solution
graph, defined follows:
start node belongs solution graph;
every non-terminal node solution graph, exactly one outgoing k-connector (corresponding action) part solution graph successor nodes also belongs
solution graph;
every directed path solution graph terminates terminal node.
solution graph maximizes expected cumulative reward found solving following
system equations,

0 terminal
state; otherwise,
P

V (s) =
(2)
0
0
0
maxaA(s)
P
0
r(s |s, a) (R(s ) + V (s )) ,
V (s) denotes expected value optimal solution state s, V called
optimal evaluation function (or value function MDP terminology). Note identical
34

fiHAO*

optimality equation hybrid-state MDPs defined Equation (1), latter restricted
discrete state space. keeping convention literature MDPs, treat
value-maximization problem even though AO* usually formalized solving cost-minimization
problem.
state-space search problems formalized AND/OR graphs, optimal solution
graph found using heuristic search algorithm AO* (Nilsson, 1980; Pearl, 1984). Like
heuristic search algorithms, advantage AO* dynamic programming find
optimal solution particular starting state without evaluating problem states. Therefore,
graph usually supplied explicitly search algorithm. implicit graph, G, specified
implicitly start node start state successor function generates successors
states state-action pair. search algorithm constructs explicit graph, G0 , initially
consists start state. tip leaf state explicit graph said terminal
goal state (or state action taken); otherwise, said
nonterminal. nonterminal tip state expanded adding explicit graph outgoing
k-connectors (one action) successor states already explicit graph.
AO* solves state-space search problem gradually building solution graph, beginning
start state. partial solution graph defined similarly solution graph, difference
tip states partial solution graph may nonterminal states implicit AND/OR graph.
partial solution graph defined follows:
start state belongs partial solution graph;
every non-tip state partial solution graph, exactly one outgoing k-connector (corresponding action) part partial solution graph successor states also
belongs partial solution graph;
every directed path partial solution graph terminates tip state explicit graph.
value partial solution graph defined similarly value solution graph.
difference tip state partial solution graph nonterminal, value
propagated backwards. Instead, assume admissible heuristic estimate
H(s) maximal-value solution graph state s. heuristic evaluation function H said
admissible H(s) V (s) every state s. recursively calculate admissible heuristic
estimate V (s) optimal value state explicit graph follows:

0 terminal state,
V (s) =
nonterminal tip state,

H(s) isP
0
0
0
maxaA(s)
s0 P r(s |s, a) (R(s ) + V (s )) otherwise.

(3)

best partial solution graph determined time propagating heuristic estimates
tip states explicit graph start state. mark action maximizes
value state, best partial solution graph determined starting root
graph selecting best (i.e., marked) action reachable state.
Table 1 outlines AO* algorithm finding optimal solution graph acyclic AND/OR
graph. interleaves forward expansion best partial solution value update step
updates estimated state values best partial solution. simplest version AO*,
values expanded state ancestor states explicit graph updated.
fact, ancestor states need re-evaluated expanded state
reached taking marked actions (i.e., choosing best action state). Thus,
parenthetical remark step 2(b)i Table 1 indicates parent s0 state added
Z unless estimated value state changed state reached state
s0 choosing best action state s0 . AO* terminates policy expansion step

35

fiMeuleau, Benazera, Brafman, Hansen & Mausam

1. explicit graph G0 initially consists start state s0 .
2. best solution graph nonterminal tip state:
(a) Expand best partial solution: Expand nonterminal tip state best partial
solution graph add new successor states G0 . new state s0 added
G0 expanding s, s0 terminal state V (s0 ) := 0; else V (s0 ) := H(s0 ).
(b) Update state values mark best actions:
i. Create set Z contains expanded state ancestors explicit
graph along marked action arcs. (I.e., include ancestor states
expanded state reached following current best solution.)
ii. Repeat following steps Z empty.
A. Remove Z state descendant G0 occurs Z.
P
B. Set V (s) := maxaA(s) s0 P r(s0 |s, a) (R(s0 ) + V (s0 )) mark best action
s. (When determining best action resolve ties arbitrarily, give preference currently marked action.)
(c) Identify best solution graph nonterminal states fringe
3. Return optimal solution graph.
Table 1: AO* algorithm.
find nonterminal states fringe best solution graph. point, best solution
graph optimal solution.
Following literature AND/OR graph search, far referred solution found
AO* solution graph. following, AO* used solve MDP, sometimes
follow literature MDPs referring solution policy. also sometimes refer
policy graph, indicate policy represented form graph.
3.2 Hybrid-State AO*
consider generalize AO* solve bounded-horizon hybrid-state MDP. challenge
face applying AO* problem challenge performing state-space search hybrid
state space.
solution adopt search aggregate state space represented AND/OR
graph node distinct value discrete component state.
words, node AND/OR graph represents region continuous state space
discrete value same. Given partition continuous state space, use AND/OR
graph search techniques solve MDP parts state space reachable
start state best policy.
However, AND/OR graph search techniques must modified important ways allow search
hybrid state space represented way. particular, longer correspondence nodes AND/OR graph individual states. node corresponds
continuous region state space, different actions may optimal different hybrid states associated search node. case rover planning, example,
best action likely depend much energy time remaining, energy time
continuous state variables.
address problem still find optimal solution, attach search node set
functions (of continuous variables) make possible associate different values, heuristics,
actions different hybrid states map search node. before, explicit

36

fiHAO*

search graph consists nodes edges AND/OR graph generated far,
describes states considered far search algorithm. difference
use complex state representation set continuous functions allows
representation reasoning continuous part state space associated search
node.
begin describing complex node data structure, describe HAO*
algorithm.
3.2.1 Data Structures
node n explicit AND/OR graph G0 consists following:
value discrete state variable.
Pointers parents children explicit graph policy graph.
Openn () {0, 1}: Open list. x X, Openn (x) indicates whether (n, x)
frontier explicit graph, i.e., generated yet expanded.
Closedn () {0, 1}: Closed list. x X, Closedn (x) indicates whether (n, x)
interior explicit graph, i.e., already expanded.
Note that, (n, x), Openn (x) Closedn (x) = . (A state cannot open
closed.) parts continuous state space associated node
neither open closed. explicit graph contains trajectory start state
particular hybrid state, hybrid state considered generated, even search
node corresponds generated; states neither open closed.
addition, non-terminal states open closed. Note refer open
closed nodes; instead, refer hybrid states associated nodes open
closed.
Hn (): heuristic function. x X, Hn (x) heuristic estimate optimal
expected cumulative reward state (n, x).
Vn (): value function. open state (n, x), Vn (x) = Hn (x). closed state
(n, x), Vn (x) obtained backing values successor states, Equation (4).
n () A: policy. Note defined closed states only.
Reachablen () {0, 1}: x X, Reachablen (x) indicates whether (n, x) reachable
executing current best policy beginning start state (n0 , x0 ).
assume various continuous functions, represent information hybrid states associated search node, partition state space associated node
discrete number regions, associate distinct value action region. Given
partitioning, HAO* algorithm expands evaluates regions hybrid state space,
instead individual hybrid states. finiteness partition important order ensure
search frontier extended finite number expansions, ensure HAO*
terminate finite number steps. implementation HAO*, described Section 4, use piecewise-constant partitioning continuous state space proposed Feng et
al. (2004). However, method discrete partitioning could used, provided condition
holds; example, Li Littman (2005) describe alternative method partitioning.
Note two forms state-space partitioning used algorithm. First, hybrid state
space partitioned finite number regions, one discrete state,

37

fiMeuleau, Benazera, Brafman, Hansen & Mausam

regions corresponds node AND/OR graph. Second, continuous state space associated particular node partitioned smaller regions based piecewise-constant
representation continuous function, one used Feng et al. (2004).
addition complex representation nodes AND/OR graph, algorithm
requires complex definition best (partial) solution. standard AO*, oneto-one correspondence nodes individual states means solution policy
represented entirely graph, called (partial) solution graph, single action
associated node. HAO* algorithm, continuum states associated
node, different actions may optimal different regions state space associated
particular node. HAO* algorithm, (partial) solution graph sub-graph explicit
graph defined follows:
start node belongs solution graph;
every non-tip node solution graph, one outgoing k-connectors part
solution graph, one action optimal hybrid state associated
node, successor nodes also belongs solution graph;
every directed path solution graph terminates tip node explicit graph.
key difference definition may one optimal action associated
node, since different actions may optimal different hybrid states associated
node. policy represented solution graph, continuous functions n (.)
Reachablen (.). particular, (partial) policy specifies action reachable region
continuous state space. best (partial) policy one satisfies following optimality
equation:
Vn (x)

=

Vn (x)

= Hn (x) (n, x) nonterminal open state,
"

Z
X
=
max
Pr(n0 | n, x, a)
Pr(x0 | n, x, a, n0 ) (Rn0 (x0 ) + Vn0 (x0 )) dx0 .

Vn (x)

0 (n, x) terminal state,

aAn (x)

n0 N

(4)

x0

Note optimality equation satisfied regions state space reachable
start state, (n0 , x0 ) following optimal policy.
3.2.2 Algorithm
Table 2 gives high-level summary HAO* algorithm. outline, AO*
algorithm, consists iteration three steps; solution (or policy) expansion, use
dynamic programming update current value function policy, analysis reachability
identify frontier solution eligible expansion. detail, modified several
important ways allow search hybrid state space. following, discuss modifications
three steps.
Policy expansion nodes current solution graph identified one open
regions associated nodes selected expansion. is, one regions
hybrid state space intersection Open Reachable chosen expansion. actions
applicable states open regions simulated, results actions added
explicit graph. cases, means adding new node AND/OR graph.
cases, simply involves marking one regions continuous state space associated
existing node open. specifically, action leads new node, node added
explicit graph, states corresponding node reachable expanded
region(s) action consideration marked open. action leads
38

fiHAO*

1. explicit graph G0 initially consists start node corresponding start state (n0 , x0 ),
marked open reachable.
2. Reachablen (x) Openn (x) non-empty (n, x):
(a) Expand best partial solution: Expand one region(s) open states frontier
explicit state space reachable following best partial policy. Add new
successor states G0 . cases, requires adding new node AND/OR
graph. cases, simply involves marking one regions continuous
state space associated existing node open. States expanded region(s)
marked closed.
(b) Update state values mark best actions:
i. Create set Z contains node(s) associated expanded regions
states ancestor nodes explicit graph along marked action arcs.
ii. Decompose part explicit AND/OR graph consists nodes Z
strongly connected components.
iii. Repeat following steps Z empty.
A. Remove Z set nodes (1) belong connected
component, (2) descendant nodes occurs Z.
B. every node n connected component states (n, x)
expanded region node n, set
Vn (x) :=
"
max
aAn (x)

X

Pr(n0 | n, x, a)

Z


Pr(x0 | n, x, a, n0 ) (Rn0 (x0 ) + Vn0 (x0 )) dx0 ,

x0

n0 N

mark best action. (When determining best action resolve ties arbitrarily, give preference currently marked action.) Repeat
longer change value nodes.
(c) Identify best solution graph nonterminal states frontier. step
updates Reachablen (x).
3. Return optimal policy.
Table 2: HAO* algorithm.
existing node, region(s) Markov states node reachable expanded
region(s) marked closed, marked open. Expanded regions state space marked
closed. Thus, different regions associated node opened expanded
different times. process illustrated Figure 2. figure, nodes corresponding
distinct value discrete state represented rectangles, circular connectors represent
actions. node, see many distinct continuous regions exist. region
see whether closed (C) open (O), whether reachable initial state (R)
executing current best policy (OPT). instance, Figure 2(a), node At(Start)
single region marked closed reachable, node Lost two regions: smallest, open
reachable, largest, closed unreachable.
Dynamic programming standard AO*, value newly-expanded node n must
updated computing Bellman backup based value functions children n

39

fiMeuleau, Benazera, Brafman, Hansen & Mausam

At(Start)

At(Loc1)


At(Start)
C

C

R

R

Navigate
(Start, Loc1)

At(Loc1)

OPT

C

R

Navigate
(Start, Loc1)

OPT

R

Navigate
(Loc1, Loc2)

Lost


C

Lost


R



C

C

R

At(Loc2)

Panoramic
Camera



(a) expansion

Panoramic
Camera

(b) expansion

Figure 2: Expanding region state space. (a) expansion: nodes At(Start),
At(Loc1) Lost previously created. unique region At(Loc1)
next region expanded. (b) expansion: action Navigate(Loc1, Loc2)
applied expanded region added graph. action lead
either preexisting node Lost, new node At(Loc2). expanded region (in
At(Loc1)), well continuous regions reachable (in Lost At(Loc2)),
highlighted dotted framed. Following expansion, expanded region closed.
Discrete state At(Loc2) added graph reachable regions
open. Additionally, new open regions added node Lost.

explicit graph. expanded region state space associated node n,
action evaluated, best action selected, corresponding continuous value function
associated region. continuous-state value function computed evaluating
continuous integral Equation (4). use method computing integral.
implementation, use dynamic programming algorithm Feng et al. (2004). reviewed
Section 2.4, show continuous integral x0 computed exactly, long
transition reward functions satisfy certain conditions. Note that, hybrid-state
dynamic programming techniques Feng et al. (2004), dynamic programming backups may
increase number pieces value function attached updated regions (Figure 3(a)).
expanded regions continuous state space associated node n reevaluated, new values must propagated backward explicit graph. backward
propagation stops nodes value function modified, root node.
standard AO* algorithm, summarized Figure 1, assumes AND/OR graph
searches acyclic. extensions AO* searching AND/OR graphs contain
cycles. One line research concerned find acyclic solutions AND/OR graphs
contain cycles (Jimenez & Torras, 2000). Another generalization AO*, called LAO*, allows
solutions contain cycles loops order specify policies infinite-horizon MDPs (Hansen
& Zilberstein, 2001).

40

fiHAO*

At(Start)

At(Loc1)
C C C

At(Start)
C

C

R

R

Navigate
(Start, Loc1)

At(Loc1)

OPT

C C C

R R R

Navigate
(Loc1, Loc2)

Lost




C

OPT

C

R

At(Loc2)


OPT

R R R

Navigate
(Loc1, Loc2)

OPT

Navigate
(Start, Loc1)

At(Loc2)

Panoramic
Camera


R

(a) Dynamic programming

Lost




C

R

R

R

C

Panoramic
Camera

(b) Reachability analysis

Figure 3: Dynamic programming reachability analysis (Figure 2 continued). (a) Dynamic programming: optimal policy reevaluated Navigate(Loc1, Loc2) appears
optimal continuous states At(Loc2). Node At(Loc1) represented finer
partition continuous state space illustrate fact backup increased
number pieces value function associated expanded region. (b) Reachability analysis: newly created region At(Loc2) becomes reachable, well
regions Lost reached Navigate(Loc1, Loc2).

Given assumption every action positive resource consumption,
loops state space problem resources available decrease step.
surprisingly, loops AND/OR graph. possible AND/OR
graph represents projection state space onto smaller space consists
discrete component state. example, possible rover return
site visited before. rover actually state, since fewer resources
available. AND/OR graph represents projection state space include
continuous aspects state, resources, means rover visit state
projects node AND/OR graph state visited earlier, shown Figure 4.
result, loops AND/OR graph, even loops part AND/OR
graph corresponds solution. sense, phantom loops
appear projected state space, real state space.
Nevertheless must modify dynamic programming (DP) algorithm deal loops.
loops real state space, know exact value function
updated finite number backups performed correct order, one backup performed
state visited along path start state expanded node(s).
multiple states map AND/OR graph node, continuous region
state space associated particular node may need evaluated once. identify
AND/OR graph nodes need evaluated once, use following two-step
algorithm.

41

fiMeuleau, Benazera, Brafman, Hansen & Mausam

At(Start)

At(Location1)
energy = 80

At(Location1)
energy = 50

At(Location1)

At(Start)
energy = 100
At(Location2)
energy = 65

At(Location2)
energy = 35

At(Location2)

Figure 4: Phantom loops HAO*: solid boxes represent Markov states. Dashed boxes represent
search nodes, is, projection Markov states discrete components. Arrows
represent possible state transition. Bold arrows show instance phantom loop
search space.

First, consider part AND/OR graph consists ancestor nodes
expanded node(s). set Z nodes identified beginning DP step.
decompose part graph strongly connected components. graph strongly
connected components acyclic used prescribe order backups almost
way standard AO* algorithm. particular, nodes particular component
backed nodes descendant components backed up. Note
case acyclic graph, every strongly connected component single node. possible
connected component one node loops AND/OR graph.
loops AND/OR graph, primary change DP step algorithm
occurs time perform backups nodes connected component one
node. case, nodes connected component evaluated. Then, repeatedly
re-evaluated value functions nodes converge, is, change
values nodes. loops real state space, convergence
guaranteed occur finite number steps. Typically, occurs small number
steps. advantage decomposing AND/OR graph connected components
identifies loops localizes effect small number nodes. experiments test
domain, nodes graph need evaluated DP step,
small number nodes (and often none) need evaluated once.
Note decomposition nodes Z connected components method improving
efficiency dynamic programming step, required correctness. alternative repeatedly updating nodes Z values converge also correct, although
likely result many useless updates already converged nodes.
Analysis reachability Change value function lead change optimal policy,
and, thus, change states visited best policy. This, turn, affect
open regions state space eligible expanded. final step, HAO* identifies
best (partial) policy recomputes Reachablen nodes states explicit graph,
follows (see Figure 3(b)). node n best (partial) solution graph, consider
parents n0 solution graph, actions lead one parents n.
Reachablen (x) support Pn (x),
X Z
Pn (x) =
Reachablen0 (x0 ) Pr(n | n0 , x0 , a) Pr(x | n0 , x0 , a, n)dx0 ,
(5)
(n0 ,a)n

X

42

fiHAO*

is, Reachablen (x) = {x X : Pn (x) > 0}. Equation (5), n set pairs (n0 , a)
best action n0 reachable resource level:
n = {(n0 , a) N : x X, Pn0 (x) > 0, n0 (x) = a, Pr(n | n0 , x, a) > 0} .
clear restrict attention state-action pairs n , only.
performing reachability analysis, HAO* identifies frontier state space
eligible expansion. HAO* terminates frontier empty, is, find
hybrid states intersection Reachable Open.
3.3 Convergence Error Bounds
next consider theoretical properties HAO*. First, reasonable assumptions,
prove HAO* converges optimal policy finite number steps. discuss
use HAO* find sub-optimal policies error bounds.
proof convergence finite number steps depends, among things,
assumption hybrid-state MDP finite branching factor. implementation,
means region state space represented hyper-rectangle, set
successor regions action represented finite set hyper-rectangles.
assumption assumption number actions finite, follows every
assignment n discrete variables, set
{x|(n, x)is reachable initial state using fixed sequence actions}
union finite number open closed hyper-rectangles. assumption viewed
generalization assumption finite branching factor discrete AND/OR graph upon
finite convergence proof AO* depends.
Theorem 1 heuristic functions Hn admissible (optimistic), actions positive resource consumptions, continuous backups action application computable exactly finite
time, branching factor finite, then:
1. step HAO*, Vn (x) upper-bound optimal expected return (n, x),
(n, x) expanded HAO*;
2. HAO* terminates finite number steps;
3. termination, Vn (x) equal optimal expected return (n, x), (n, x) reachable
optimal policy, i.e., Reachablen (x) > 0.
Proof: (1) proof induction. Every state (n, x) assigned initial heuristic estimate,
Vn (x) = Hn (x) Vn (x) admissibility heuristic evaluation function. make
inductive hypothesis point algorithm, Vn (x) Vn (x) every state (n, x).
backup performed state (n, x),
"

Z
X
Vn (x) =
max
Pr(n0 | n, x, a)
Pr(x0 | n, x, a, n0 ) (Rn0 (x0 ) + Vn0 (x0 )) dx0
aAn (x)

x0

n0 N

"


max
aAn (x)

X
n0 N

0

Z

Pr(n | n, x, a)

0

0

0

Pr(x | n, x, a, n ) (Rn0 (x ) +
x0

= Vn (x) ,
last equality restates Bellman optimality equation.

43

Vn0 (x0 )) dx0



fiMeuleau, Benazera, Brafman, Hansen & Mausam

(2) action positive, bounded below, resource consumption, resources
finite non-replenishable, complete implicit AND/OR graph must finite.
reason, graph turned finite graph without loops: Along directed loop
graph, amount maximal available resources must decrease positive
lower-bound amount resources consumed action. node graph may
expanded number times bounded number ancestor. (Each time new
ancestor discovered, may lead update set reachable regions node.)
Moreover, finite branching factor implies number regions considered within node
bounded (because finite ways reaching node, contributes finite
number hyper-rectangles). Thus, overall, number regions considered finite,
processing required region expansion finite (because action application backups
computed finite time). leads desired conclusion.
(3) search algorithm terminates policy start state (n0 , x0 ) complete,
is, lead unexpanded states. every state (n, x) reachable
following policy, contradictory suppose Vn (x) > Vn (x) since implies complete
policy better optimal. Bellman optimality equation Equation (1), know
Vn (x) Vn (x) every state complete policy. Therefore, Vn (x) = Vn (x).
HAO* converges optimal solution, stopping algorithm early allows flexible
trade-off solution quality computation time. assume that, state,
done action terminates execution zero reward (in rover problem, would
start safe sequence), evaluate current policy step algorithm
assuming execution ends time reach leaf policy graph. assumption,
error current policy step algorithm bounded. show
using decomposition value function described Chakrabarti et al.(1988) Hansen
Zilberstein (2001). note point algorithm, value function decomposed
two parts, gn (x) hn (x),
gn (x)

=

gn (x)

=

0 (n, x) open state, fringe greedy policy; otherwise,
Z
X
0

Pr(n | n, x, )
Pr(x0 | n, x, , n0 ) (Rn (x) + gn0 (x0 )) dx0 ,

(6)

x0

n0 N


hn (x)

= Hn (x) (n, x) open state, fringe greedy policy; otherwise,
Z
X
hn (x) =
Pr(n0 | n, x, )
Pr(x0 | n, x, , n0 ) hn0 (x0 )dx0 ,
(7)
n0 N

x0

action maximizes right-hand side Equation (4). Note Vn (x) =
gn (x) + hn (x). use decomposition value function bound error best policy
found far, follows.
Theorem 2 step HAO* algorithm, error current best policy bounded
hn0 (x0 ).
Proof: state (n, x) explicit search space, lower bound optimal value given
gn (x), value achieved current policy done action
executed fringe states, upper bound given Vn (x) = gn (x) + hn (x), established
Theorem 1. follows hn0 (x0 ) bounds difference optimal value
current admissible value state (n, x), including initial state (n0 , x) ).
Note error bound initial state hn0 (x0 ) = Hn0 (x0 ) start algorithm;
decreases progress algorithm; hn0 (x0 ) = 0 HAO* converges optimal
solution.
44

fiHAO*

3.4 Heuristic Function
heuristic function Hn focuses search reachable states likely useful.
informative heuristic, scalable search algorithm. implementation
HAO* rover planning problem, described detail next section, used
simple admissible heuristic function assigns node sum rewards associated
goals achieved far. Note heuristic function depends
discrete component state, continuous variables; is, function Hn (x)
constant values x. obvious heuristic admissible, since represents
maximum additional reward could achieved continuing plan execution. Although
obvious heuristic simple could useful, experimental results present
Section 4 show is. considered additional, informed heuristic function solved
relaxed, suitably discretized, version planning problem. However, taking account
time required compute heuristic estimate, simpler heuristic performed better.
3.5 Expansion Policy
HAO* works correctly converges optimal solution matter continuous region(s)
node(s) expanded iteration (step 2.a). quality solution may
improve quickly using heuristics choose region(s) fringe expand
next.
One simple strategy select node expand continuous regions node
open reachable. preliminary implementation, expanded (the open regions of)
node likely reached using current policy. Changes value
states greatest effect value earlier nodes. Implementing strategy requires
performing additional work involved maintaining probability associated state.
probabilities available, one could also focus expanding promising node,
is, node integral Hn (x) times probability values x highest,
described Mausam, Benazera, Brafman, Meuleau, Hansen (2005).
Hansen Zilberstein (2001) observed that, case LAO*, algorithm efficient
expand several nodes fringe performing dynamic programming explicit
graph. cost performing update node largely dominates cost
expanding node. expand one node fringe iteration, might
perform DP backups expand several nodes common ancestors proceeding
DP. limit, might want expand nodes fringe algorithm iteration.
Indeed, variant LAO* proved efficient (Hansen & Zilberstein, 2001).
case LAO*, updates expensive loops implicit graph. HAO*,
update region induces call hybrid dynamic programming module open
region node. Therefore, technique likely produce benefit.
Pursuing idea, allowed algorithm expand nodes fringe
descendants fixed depth iteration. defined parameter, called expansion
horizon denoted k, represent, loosely speaking, number times whole fringe
expanded iteration. k = 1, HAO* expands open reachable regions
nodes fringe recomputing optimal policy. k = 2, expands regions
fringe children updating policy. k = 3 also consider grandchildren regions fringe, on. k tends infinity, algorithm essentially
performs exhaustive search: first expands graph reachable nodes, performs one
pass (hybrid) dynamic programming graph determine optimal policy. balancing
node expansion update, expansion horizon allows tuning algorithm behavior
exhaustive search traditional heuristic search. experiments showed value k
5 10 optimal solve hardest benchmark problems (see section 4).

45

fiMeuleau, Benazera, Brafman, Hansen & Mausam

Start

ObsPt3

Unsafe

C4
Obs
Pt4

Featureless
C6

W2
W3

W1

Obs
Pt5

ObsPt2

Audience

Demo

label

: Waypoint
Name

: Rock
: IP + CHAMP

ObsPt1
Far

: Science Cam.

Figure 5: K9 rover (top left) developed Jet Propulsion Laboratory NASA Ames
Research Center prototype MER rovers. used test advanced rover
software, including automated planners rovers activities. Right: topological map
2004 demo problem. Arrows labeled IP + CHAMP represent opportunity
deploy arm rock (instrument placement) take picture
CHAMP Camera. Arrows labeled Science Cam represent opportunity take
remote picture rock Science Camera.

3.6 Updating Multiple Regions
expansion policies described based expanding open regions one several
nodes simultaneously. allow leveraging hybrid-state dynamic programming techniques
Feng et al. (2004) Li Littman (2005). techniques may compute single
iteration piecewise constant linear value functions cover large range continuous states,
possibly whole space possible values. particular, back one iteration
continuous states included given bounds.
Therefore, several open regions node expanded iteration
HAO*, update simultaneously backing-up subset continuous states
includes regions. instance, one may record lower bounds upper bounds
continuous variable expanded regions, compute value function covers
hyper-rectangle bounds.
modification algorithm impact convergence. long value
expanded regions computed, convergence proof holds. However, execution time may adversely affected expanded regions proper subset region continuous states

46

fiHAO*

(a) Value function Vn (.) initial node.
first plateau corresponds analyzing R1, second plateau analyzing R2, third plateau
analyzing R1 R2.

(b) policy n (.) starting
node shows partitions resource space different actions
optimal. Dark: action; Grey:
navigation R2; Light: analysis
R1.

Figure 6: (a) Optimal value function initial state simple rover problem possible
values continuous resources (time energy remaining). value function
partitioned 3476 pieces. (b) Optimal policy set states.

backed-up. case, values states open reachable uselessly computed,
deviates pure heuristic search algorithm.
However, modification may also beneficial avoids redundant computation.
Hybrid-state dynamic programming techniques manipulate pieces value functions. Thus, several
expanded regions included piece value function, value computed
once. practice, benefit may outweigh cost evaluating useless regions. Moreover, cost
reduced storing value functions associated node graph,
computed values irrelevant regions saved case regions become eligible expansion
(i.e., open reachable) later. Thus, variant HAO* fully exploits hybrid-state dynamic
programming techniques.

4. Experimental Evaluation
section, describe performance HAO* solving planning problems simulated
planetary exploration rover two monotonic continuous-valued resources: time battery
power. Section 4.1 uses simple toy example problem illustrate basic steps
HAO* algorithm. Section 4.2 tests performance algorithm using realistic, real-size
NASA simulation rover analyzes results experiments. simulation uses
model K9 rover (see Figure 5) developed Intelligent Systems (IS) demo NASA
Ames Research Center October 2004 (Pedersen et al., 2005). complex real-size model
K9 rover uses command names understandable rovers execution language,
plans produced algorithm directly executed rover. experiments
reported Section 4.2, simplify NASA simulation model way.

47

fiMeuleau, Benazera, Brafman, Hansen & Mausam

Figure 7: First iteration HAO* toy problem. explicit graph marked dim edges
solution graph marked thick edges. Tip nodes 4, 5, 6 7 shown
constant heuristic functions expanded nodes 1, 2 3 shown backed
value functions.

planning problem consider, autonomous rover must navigate planar graph
representing surroundings authorized navigation paths, schedule observations
performed different rocks situated different locations. subset observational
goals achieved single run due limited resources. Therefore, oversubscribed
planning problem. also problem planning uncertainty since action uncertain
positive resource consumptions probability failing.
significant amount uncertainty domain comes tracking mechanism used
rover. Tracking process rover recognizes rock based certain features
camera image associated rock. mission operations, problem instance
containing fixed set locations, paths, rocks built last panoramic camera image
sent rover. logical rock problem instance corresponds real rock,
rover must associate two basis features detected instruments,
including camera. rover moves camera image changes, rover must keep track
features image evolve. process uncertain subject faults result
losing track rock. practice, tracking modeled following way:
order perform measurement rock, rover must tracking rock.
navigate along path, must tracking one rocks enables following path.
set rocks enable path part problem definition given planner.
decision start tracking rock must made rover begins move.
rover starts moving, may keep track rock already tracked voluntarily stop
tracking it, cannot acquire new rock tracked initially.

48

fiHAO*

Figure 8: Second iteration HAO* toy problem.
rover may randomly lose track rocks navigating along path. probability losing track rock depends rock path followed, part
problem definition given planner.
way reacquire rock whose track lost, intentionally accident.
number rocks tracked strongly influences duration resource consumption
navigate actions. higher number rocks tracked, costly navigate
along path. rover stop regularly check record aspect
rock tracked. creates incentive limit number rocks tracked
rover given set goals chosen path intends follow.
So, rover initially selects set rocks track tries keep set small possible
given goals. starts moving, may lose track rocks, may cause
reconsider set goals pursue route get corresponding rocks.
also purposely stop tracking rock longer necessary given goals left
achieve.
implementation HAO* uses dynamic programming algorithm developed Feng et
al. (2004) summarized Section 2.4 order perform backups hybrid state space,
partitions continuous state-space associated node piecewise-constant regions. uses
multiple-region updates described Section 3.6: upper bound resource
expanded regions computed, states included bounds minimal
possible resource levels updated.
experiments, use variant HAO* algorithm described Section 3.5,
parameter k sets number times whole fringe expanded iteration HAO*;
allows behavior algorithm tuned exhaustive search heuristic search.
used expansion horizon k = 2 simple example Section 4.1 default expansion
horizon k = 7 larger examples Section 4.2. Section 4.2.3 describes experiments
different expansion horizons.

49

fiMeuleau, Benazera, Brafman, Hansen & Mausam

Figure 9: Third iteration HAO* toy problem.
implementation HAO* uses simple heuristic described Section 3.4, augmented
small amount domain knowledge. value Hn (x) state (n, x) essentially equal
sum utilities goals yet achieved n. However, rover already moved
certain rock tracked state n, goals requiring rock tracked
included sum. reflects fact that, rover moved, cannot start tracking
rock more, thus goals require rock tracked unreachable. resulting
heuristic admissible (i.e., never underestimates value state), straightforward
compute. Note depend current resource levels, functions Hn (x)
constant values x.
4.1 Example
begin simple example rover planning problem order illustrate steps
algorithm. solve example using implementation HAO* use
solve realistic examples considered Section 4.2.
example, targets two rocks, R1 R2, positioned locations L1 L2,
respectively. rovers initial location L1, direct path L1 L2.
Analyzing rock R1 yields reward 10 analyzing rock R2 yields reward 20. rovers
action set simplified. Notably, features single action Pic(Rx) represents steps
analyzing rock Rx, stop tracking actions removed.
Figure 6 shows optimal value function optimal policy found HAO* starting
discrete state, resources ranging whole space possible values. Figures 7, 8 9
show step-by-step process HAO* solves problem. Using expansion horizon
k = 2, HAO* solves problem three iterations, follows:
Iteration 1: shown Figure 7, HAO* expands nodes 1, 2 3 computes heuristic
function new tip nodes 4, 5, 6 7. backup step yields value function estimates
nodes 1, 2 3. HAO* identifies best solution graph new fringe node 6.

50

fiHAO*

(a) 1012 pieces.

(b) 3465pieces.

(c) 6122pieces.

Figure 10: Optimal value functions initial state simple rover problem increasing initial resource levels (from left right). optimal return appears three
dimensional function carved reachable space heuristic function.
problem
name
Rover1
Rover2
Rover3
Rover4

rover
locations
7
7
9
11

paths

goals

fluents

actions

10
11
16
20

3
5
6
6

30
41
49
51

43
56
73
81

discrete
states
(approx.)
1.1 109
2.2 1012
5.6 1014
2.3 1015

reachable
discrete
states
613
5255
20393
22866

explicit
graph

optimal
policy

longest
branch

234
1068
2430
4321

50
48
43
44

35
35
43
43

Table 3: Size benchmark rover problems.
Iteration 2: shown Figure 8, HAO* expands nodes 6, 8, 9 10, starting
previous fringe node 6, computes heuristic functions new tip nodes 11, 12 13.
heuristic value node 12 zero because, state, rover lost track R2
already analyzed R1. backup step improves accuracy value function
several nodes. Node 11 new fringe node since 12 terminal node.
Iteration 3: shown Figure 9, HAO* expands node 11 node 14. search ends
iteration open node optimal solution graph.
comparison, Figure 10 shows value function found HAO* varies different initial
resource levels. figures, unreachable states assigned large constant heuristic value,
value function reachable states appears carved plateau heuristic.
4.2 Performance
Now, describe HAO*s performance solving four much larger rover planning problems using
NASA simulation model. characteristics problems displayed Tables 3. Columns
two six show size problems terms rover locations, paths, goals. also show
total number fluents (Boolean state variables) actions problem. Columns seven
ten report size discrete state space. total number discrete states two raised
power number fluents. Although huge state space, limited number
states reached start state, depending initial resource levels. eighth
column Table 3 shows number reachable discrete states initial time energy levels
set maximum value. (The maximum initial resource levels based scenario
2004 demo represent several hours rover activity.) shows simple reachability
51

fiMeuleau, Benazera, Brafman, Hansen & Mausam

700

500
400
300
200
100
0

reachable
created
expanded
optimal policy

600
Number discrete states

600
Number discrete states

700

reachable
created
expanded
optimal policy

500
400
300
200
100

0

100000

200000
300000
Initial energy

400000

0

500000

0

2000

4000
6000
Initial time

8000

10000

8000

10000

8000

10000

8000

10000

(a) Rover1
6000

reachable
created
expanded
optimal policy

5000

Number discrete states

Number discrete states

6000

4000
3000
2000
1000
0

0

100000

200000
300000
Initial energy

400000

4000
3000
2000
1000
0

500000

reachable
created
expanded
optimal policy

5000

0

2000

4000
6000
Initial time

(b) Rover2
25000

reachable
created
expanded
optimal policy

20000

Number discrete states

Number discrete states

25000

15000
10000
5000
0

0

100000

200000 300000
Initial energy

400000

20000
15000
10000
5000
0

500000

reachable
created
expanded
optimal policy

0

2000

4000
6000
Initial time

(c) Rover3
25000

reachable
created
expanded
optimal policy

20000

Number discrete states

Number discrete states

25000

15000
10000
5000
0

0

100000

200000 300000
Initial energy

400000

20000
15000
10000
5000
0

500000

reachable
created
expanded
optimal policy

0

2000

4000
6000
Initial time

(d) Rover4
Figure 11: Number nodes created expanded HAO* vs. number reachable discrete states.
graphs left column obtained fixing initial time maximum value
varying initial energy. graphs right column obtained fixing
initial energy maximum value varying initial time. Results obtained
k = 7.
52

fiHAO*

analysis based resource availability makes huge difference. partly due fact
planning domain, close K9 execution language, allow many fluents
true simultaneously. Columns nine ten show number discrete states explicit
graph optimal policy. precisely, former number nodes created HAO*,
is, subset reachable discrete states. number reachable discrete states, thus
size graph explore, may seem small compared discrete combinatorial problems
solved AI techniques. iteration, continuous approximation two-dimensional
backup necessary evaluate hybrid state space associated graph. Finally, last
column Table 3 shows length longest branch optimal policy initial
resource levels set maximum value.
largest four instances (that is, Rover4) exactly problem October 2004
demo. considered large rover problem. example, much larger
problems faced MER rovers never visit one rock single planning cycle.
4.2.1 Efficiency Pruning
first set simulations, try evaluate efficiency heuristic pruning HAO*, is,
portion discrete search space spared exploration use admissible
heuristics. purpose, compare number discrete states reachable given
resource level number nodes created expanded HAO*. also consider
number nodes optimal policy found algorithm.
Results four benchmark problems presented Figure 11. curves obtained
fixing one resource maximum possible value varying 0 maximum.
Therefore, represent problems mostly one resource constraining. result show,
notably, single resource enough constrain reachability state space significantly.
surprisingly, problems become larger initial resources increase, discrete
states become reachable. Despite simplicity heuristic used, HAO* able by-pass
significant part search space. Moreover, bigger problem, leverage
algorithm take simple heuristic.
results quite encouraging, number nodes created expanded
always reflect search time. Therefore, examine time takes HAO* produce solutions.
4.2.2 Search Time
Figure 12 shows HAO* search time set experiments. curves exhibit
monotonicity and, instead, appear show significant amount noise. surprising
search time always increase increase initial levels resource, although
search space bigger. shows search complexity depend size search
space alone. factors must explain complexity peaks observed Figure 12.
number nodes created expanded algorithm contain noise,
reason peaks computation time must time spent dynamic programming
backups. Moreover, search time appears closely related complexity optimal policy.
Figure 13 shows number nodes branches policy found algorithm, well
number goals pursued policy. shows that: (i) cases, increasing initial
resource level eliminates need branching reduces size optimal solution; (ii)
size optimal policy and, secondarily, number branches, explains peaks
search time curves. Therefore, question is: large solution graph induce long time
spent backups? two possible answers question: backups take longer
and/or backups performed. first explanation pretty intuitive.
policy graph contains many branches leading different combinations goals, value functions
contain many humps plateaus, therefore many pieces, impacts complexity
dynamic programming backups. However, time empirical evidence
53

fi18

18

16

16

14

14

12

12

Search time (s)

Search time (s)

Meuleau, Benazera, Brafman, Hansen & Mausam

10
8
6

10
8
6

4

4

2

2

0

0

100000

200000
300000
Initial energy

400000

0

500000

0

2000

4000
6000
Initial time

8000

10000

4000
6000
Initial time

8000

10000

180

160

160

140

140

120

120

Search time (s)

Search time (s)

(a) Rover1
180

100
80
60

100
80
60

40

40

20

20

0

0

100000

200000
300000
Initial energy

400000

0

500000

0

2000

25000

20000

20000
Search time (s)

Search time (s)

(b) Rover2
25000

15000
10000
5000
0

15000
10000
5000

0

100000

200000 300000
Initial energy

400000

0

500000

0

2000

4000
6000
Initial time

8000

10000

0

2000

4000
6000
Initial time

8000

10000

20000

20000

15000

15000
Search time (s)

Search time (s)

(c) Rover3

10000

5000

0

10000

5000

0

100000

200000 300000
Initial energy

400000

0

500000

(d) Rover4
Figure 12: HAO* search time. graphs left column obtained fixing initial time
maximum value, graphs right column obtained fixing
initial energy maximum. Results obtained k = 7.

54

fiHAO*

confirm hypothesis. Conversely, observe peak Figure 12 comes increase
number backups. work required explain this.
4.2.3 Expansion Horizon
results Section 4.2.1 show HAO* leverage even simple admissible heuristic prune
large portion search space. necessarily follow HAO* outperform
exhaustive search algorithm creates graph reachable states, executes one pass
dynamic programming graph find optimal policy. Although HAO* expands smaller
graph exhaustive search, must evaluate graph often. Section 3.5,
introduced parameter k expansion horizon order allow adjustment trade-off
time spent expanding nodes time spent evaluating nodes. study influence
parameter algorithm.
Figure 14 shows number nodes created expanded HAO* function
expansion horizon four benchmark problem instances. surprisingly, algorithm creates
expands nodes expansion horizon increases. Essentially, behaves like
exhaustive search k increased. two smallest problem instances, large enough
values k, number visited states levels total number reachable states
reached. two largest problem instances, interrupt experiments k reached
25 search time became long.
Figure 15 shows effect expansion horizon search time HAO*. smallest
problem instance (Rover1), HAO* clear advantage exhaustive search (with
k > 20), even though explores fewer nodes. three larger problem instances, HAO*
clear advantage. Rover2 problem instance, search time HAO* levels
k = 25, indicating limit reachable states reached. However, duration
exhaustive search several times longer HAO* smaller settings k. benefits
HAO* clearer two largest problem instances. k increased, algorithm
quickly overwhelmed combinatorial explosion size search space, simulations
eventually need interrupted search time becomes long. problem
instances smaller settings k, HAO* able efficiently find optimal solutions.
Overall, results show clear benefit using admissible heuristics prune
search space, although expansion horizon must adjusted appropriately order HAO*
achieve favorable trade-off node-expansion time node-evaluation time.

5. Conclusion
introduced heuristic search approach finding optimal conditional plans domains characterized continuous state variables represent limited, consumable resources. HAO*
algorithm variant AO* algorithm that, best knowledge, first algorithm deal following: limited continuous resources, uncertain action outcomes,
over-subscription planning. tested HAO* realistic NASA simulation planetary rover,
complex domain practical importance, results demonstrate effectiveness solving
problems large solved straightforward application dynamic programming. effective heuristic search exploit resource constraints, well admissible
heuristic, order limit reachable state space.
implementation, HAO* algorithm integrated dynamic programming algorithm Feng et al. (2004). However HAO* integrated dynamic programming
algorithms solving hybrid-state MDPs. Feng et al. algorithm finds optimal policies
limiting assumptions transition probabilities discrete, rewards either piecewiseconstant piecewise-linear. recently-developed dynamic programming algorithms hybridstate MDPs make less restrictive assumptions, also potential improve computational

55

fiMeuleau, Benazera, Brafman, Hansen & Mausam

30

3

20

2

10

1

0

0

100000

200000 300000
Initial energy

400000

40

0
500000

5

Nodes
Branches
Goals

4

30

3

20

2

10

1

0

0

2000

4000
6000
Initial time

8000

Number branches goals

4

50

Number nodes

40
Number nodes

5

Nodes
Branches
Goals

Number branches goals

50

0
10000

(a) Rover1

45

3

30

2

15

1

0

0

100000

200000 300000
Initial energy

400000

60

0
500000

5

Nodes
Branches
Goals

4

45

3

30

2

15

1

0

0

2000

4000
6000
Initial time

8000

Number branches goals

4

75

Number nodes

Nodes
Branches
Goals

60
Number nodes

5
Number branches goals

75

0
10000

(b) Rover2

45

3

30

2

15

1

0

0

100000

200000 300000
Initial energy

400000

60

0
500000

5

Nodes
Branches
Goals

4

45

3

30

2

15

1

0

0

2000

4000
6000
Initial time

8000

Number branches goals

4

75

Number nodes

Nodes
Branches
Goals

60
Number nodes

5
Number branches goals

75

0
10000

(c) Rover3

60

3

40

2

20

1

0

0

100000

200000 300000
Initial energy

400000

80

0
500000

5

Nodes
Branches
Goals

4

60

3

40

2

20

1

0

0

2000

4000
6000
Initial time

8000

Number branches goals

4

100

Number nodes

Nodes
Branches
Goals

80
Number nodes

5
Number branches goals

100

0
10000

(d) Rover4
Figure 13: Complexity optimal policy: number nodes, branches, goals optimal
policy setting Figure 11.

56

fiHAO*

700

Number discrete states

600
Number discrete states

6000

created
expanded

500
400
300
200
100
0

0

5

10
15
20
Expansion horizon

4000
3000
2000
1000
0

25

created
expanded

5000

0

5

10

(a) Rover1
14000

Number discrete states

Number discrete states

30

created
expanded

14000

10000
8000
6000
4000
2000
0

25

(b) Rover2
16000

created
expanded

12000

15
20
Expansion horizon

12000
10000
8000
6000
4000
2000

0

5

10
15
Expansion horizon

0

20

(c) Rover3

0

5

10
15
Expansion horizon

20

(d) Rover4

Figure 14: Influence expansion horizon number nodes visited algorithm.
efficiency (Li & Littman, 2005; Marecki et al., 2007). Integrating HAO* one algorithms
could improve performance further.
several interesting directions work could extended. developing HAO*, made assumptions every action consumes resource resources
non-replenishable. Without assumptions, state could revisited optimal
plan could loops well branches. Generalizing approach allow plans loops,
seems necessary handle replenishable resources, requires generalizing heuristic search
algorithm LAO* solve hybrid MDPs (Hansen & Zilberstein, 2001). Another possible extension
allow continuous action variables addition continuous state variables. Finally, heuristic
search approach could combined approaches improving scalability, hierarchical decomposition (Meuleau & Brafman, 2007). would allow handle even larger state
spaces result number goals over-subscription planning problem increased.
Acknowledgments
work funded NASA Intelligent Systems program, grant NRA2-38169. Eric Hansen
supported part NASA Summer Faculty Fellowship funding Mississippi
Space Grant Consortium. work performed Emmanuel Benazera working
NASA Ames Research Center Ronen Brafman visiting NASA Ames Research Center,
consultants Research Institute Advanced Computer Science. Ronen Brafman
supported part Lynn William Frankel Center Computer Science, Paul Ivanier
Center Robotics Production Management, ISF grant #110707. Nicolas Meuleau
consultant Carnegie Mellon University NASA Ames Research Center.

57

fiMeuleau, Benazera, Brafman, Hansen & Mausam

60

1800
1600
1400

40

Search time (s)

Search time (s)

50

30
20

1000
800
600
400

10
0

1200

200
0

5

10
15
20
Expansion horizon

0

25

0

5

10

(a) Rover1

30

30000
25000
Search time (s)

20000
Search time (s)

25

(b) Rover2

25000

15000
10000
5000
0

15
20
Expansion horizon

20000
15000
10000
5000

0

5

10
15
Expansion horizon

0

20

(c) Rover3

0

5

10
15
Expansion horizon

20

(d) Rover4

Figure 15: Influence expansion horizon overall search time.

References
Altman, E. (1999). Constrained Markov Decision Processes. Chapman HALL/CRC.
Bertsekas, D., & Tsitsiklis, J. (1996). Neural Dynamic Programming. Athena Scientific, Belmont,
MA.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions
computational leverage. Journal Artificial Intelligence Research, 11, 194.
Boyan, J., & Littman, M. (2000). Exact solutions time-dependent MDPs. Advances Neural
Information Processing Systems 13, pp. 17. MIT Press, Cambridge.
Bresina, J., Dearden, R., Meuleau, N., Ramakrishnan, S., Smith, D., & Washington, R. (2002).
Planning continuous time resource uncertainty: challenge AI. Proceedings
Eighteenth Conference Uncertainty Artificial Intelligence, pp. 7784.
Bresina, J., Jonsson, A., Morris, P., & Rajan, K. (2005). Activity planning mars exploration
rovers. Proceedings Fifteenth International Conference Automated Planning
Scheduling, pp. 4049.
Chakrabarti, P., Ghose, S., & DeSarkar, S. (1988). Admissibility AO* heuristics overestimate. Aritificial Intelligence, 34, 97113.
Feng, Z., Dearden, R., Meuleau, N., & Washington, R. (2004). Dynamic programming structured continuous Markov decision problems. Proceedings Twentieth Conference
Uncertainty Artificial Intelligence, pp. 154161.
58

fiHAO*

Friedman, J., Bentley, J., & Finkel, R. (1977). algorithm finding best matches logarithmic
expected time. ACM Trans. Mathematical Software, 3(3), 209226.
Hansen, E., & Zilberstein, S. (2001). LAO*: heuristic search algorithm finds solutions
loops. Artificial Intelligence, 129, 3562.
Jimenez, P., & Torras, C. (2000). efficient algorithm searching implicit AND/OR graphs
cycles. Artificial Intelligence, 124, 130.
Kveton, B., Hauskrecht, M., & Guestrin, C. (2006). Solving factored MDPs hybrid state
action variables. Journal Artificial Intelligence Research, 27, 153201.
Li, L., & Littman, M. (2005). Lazy approximation solving continuous finite-horizon MDPs.
Proceedings Twentieth National Conference Artificial Intelligence, pp. 11751180.
Marecki, J., Koenig, S., & Tambe, M. (2007). fast analytical algorithm solving markov decision
processes real-valued resources. Proceedings 20th International Joint Conference
Artificial Intelligence (IJCAI-07, pp. 25362541.
Mausam, Benazera, E., Brafman, R., Meuleau, N., & Hansen, E. (2005). Planning continuous resources stochastic domains. Proceedings Nineteenth International Joint
Conference Artificial Intelligence, pp. 12441251. Professional Book Center, Denver, CO.
Meuleau, N., & Brafman, R. (2007). Hierarchical heuristic forward search stochastic domains.
Proceedings 20th International Joint Conference Artificial Intelligence (IJCAI-07),
pp. 25422549.
Munos, R., & Moore, A. (2002). Variable resolution discretization optimal control. Machine
Learning, 49 (2-3), 291323.
Nilsson, N. (1980). Principles Artificial Intelligence. Tioga Publishing Company, Palo Alto, CA.
Pearl, J. (1984). Heuristics: Intelligent Search Strategies Computer Problem Solving. AddisonWesley.
Pedersen, L., Smith, D., Deans, M., Sargent, R., Kunz, C., Lees, D., & Rajagopalan, S. (2005).
Mission planning target tracking autonomous instrument placement. Proceedings
2005 IEEE Aerospace Conference., Big Sky, Montana.
Puterman, M. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming.
Wiley, New York, NY.
Rust, J. (1997). Using randomization break curse dimensionality. Econimetrica, 65, 487
516.
Smith, D. (2004). Choosing objectives over-subscription planning. Proceedings Fourteenth
International Conference Automated Planning Scheduling, pp. 393401.
Thiebaux, S., Gretton, C., Slaney, J., Price, D., & Kabanza, F. (2006). Decision-theoretic planning
non-Markovian rewards. Journal Artificial Intelligence Research, 25, 1774.
van den Briel, M., Sanchez, R., Do, M., & Kambhampati, S. (2004). Effective approaches partial
satisfation (over-subscription) planning. Proceedings Nineteenth National Conference
Artificial Intelligence, pp. 562569.

59

fi
