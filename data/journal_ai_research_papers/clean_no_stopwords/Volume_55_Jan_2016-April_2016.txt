Journal Artificial Intelligence Research 55 (2016) 1059-1090Submitted 10/15; published 04/16Learning Concept Graphs Online Educational DataHanxiao LiuWanliYiming YangJaime Carbonellhanxiaol@cs.cmu.edumawanli@cs.cmu.eduyiming@cs.cmu.edujgc@cs.cmu.eduSchool Computer ScienceCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213 USAAbstractpaper addresses open challenge educational data mining, i.e., problemautomatically mapping online courses different providers (universities, MOOCs, etc.)onto universal space concepts, predicting latent prerequisite dependencies (directedlinks) among concepts courses. propose novel approach inference withinacross course-level concept-level directed graphs. training phase, systemprojects partially observed course-level prerequisite links onto directed concept-level links;testing phase, induced concept-level links used infer unknown courselevel prerequisite links. Whereas courses may specific one institution, conceptsshared across different providers. bi-directional mappings enable system performinterlingua-style transfer learning, e.g. treating concept graph interlinguatransferring prerequisite relations across universities via interlingua. Experimentsnewly collected datasets courses MIT, Caltech, Princeton CMU showpromising results.1. Introductionlarge growing amounts online education data present open challengessignificant opportunities machine learning research enrich educational offerings. Oneimportant challenges automatically detect prerequisite dependenciesamong massive quantities online courses, support decision making curricula planning students, support course curriculum design teachers basedexisting course offerings. One example find coherent sequence courses amongMOOC offerings different providers respect implicit prerequisite relations.specific example would new student enters university MSPhD degree. interested machine learning data mining courses, findsdifficult choose among many courses look similar ambiguous course titlesher, Machine Learning, Statistical Machine Learning, Applied Machine Learning, Machine Learning Large Datasets, Scalable Analytics, Advanced Data Analysis,Statistics: Data Mining, Intermediate Statistics, Statistical Computing, on. Completing courses would imply taking forever graduate, possibly waste bigportion time due overlapping content. Alternately, wants choosesmall subset, courses include? order included coursesc2016AI Access Foundation. rights reserved.fiLiu, Ma, Yang, & CarbonellCourses University 2Courses University 1E&MMechanicsCalculusDifferential EqMatrixMatrixQuantumAlgorithmsTopologyJava ProgScalable AlgsNum AnalysisUniversal Concepts (e.g. Wikipedia Topics)Figure 1: framework two-level directed graphs: higher-level graphs courses(nodes) prerequisite relations (links). lower-level graph consists universal concepts (nodes) pairwise preference learning teaching concepts.links two levels system-assigned weights conceptscourse.without sufficient understanding prerequisite dependencies? Often prerequisitesexplicit within academic department implicit across departments. Moreover,already took several courses machine learning data mining Courseraundergraduate education, much courses overlap new ones? Without accurate representation content overlap courses overlappedcontent reflects prerequisite relations, difficult help find suitablecourses correct order. Universities solve problem old-fashioned way, viaacademic advisors, clear address problem context MOOCscross-university offerings courses unique IDs describeduniversally controlled consistent vocabulary.Ideally, would like universal graph whose nodes canonical discriminant concepts (e.g. convexity eigenvalues) taught broad range courses,whose links indicate pairwise preferences sequencing teaching concepts.example, learn concepts PageRank HITS, students alreadylearned concepts eigenvectors, Markov matrices irreducibility matrices.means directed links eigenvectors, Markov matrices irreducibility PageRankHITS concept graph. generalize further, many directed linksconcepts one course (say Matrix Algebra) concepts another course (sayWeb Mining Link Analysis sub-topic), may infer prerequisite relation two courses. Clearly, directed graph broad coverage universal1060fiLearning Concept Graphs Online Educational Dataconcepts crucial reasoning course content overlap prerequisite relationships, hence important educational decision making, curriculum planningstudents modularization course syllabus design instructors.obtain knowledge-rich concept graph? Manual specification obviously scalable number concepts reaches tens thousands larger. Usingmachine learning automatically induce graph based massive online course materials attractive alternative; however, statistical learning techniquesdeveloped problem, knowledge. Addressing open challenge principled algorithmic solutions novel contribution aim accomplish paper.call new method Concept Graph Learning (CGL). Specifically, propose multi-levelinference framework illustrated Figure 1, consists two levels graphscross-level links. Generally, course would cover multiple concepts, concept maycovered one course. Notice course-level graphs overlapdifferent universities universal course IDs. However, semantic concepts taught different universities overlap, want learn mappingsnon-universal courses universal concept space based online course materials.paper investigate problem concept graph learning (CGL) newcollections course syllabi (including course names, descriptions, listed lectures, prerequisite relations, etc.) Massachusetts Institute Technology (MIT), California InstituteTechnology (Caltech), Carnegie Mellon University (CMU) Princeton. syllabusdata allow us construct initial course-level graph university, mayenriched discovering latent prerequisite links. representing universalconcept space, study four representation schemes (Section 2.1), including 1) usingEnglish words course descriptions, 2) using sparse coding English words, 3) usingdistributed word embedding English words, 4) using large subset Wikipediacategories. representation schemes, provide algorithmic solutionsestablish mapping courses concepts, learn concept-level dependencies based observed prerequisite relations course level. second part, i.e.,explicit learning directed graph universal concepts, unique partproposed framework. concept graph learned, predict unobservedprerequisite relations among courses, including training set different universities. words, CGL enables interlingua-style transfer learningtrain models course materials universities predict prerequisiterelations courses universities. universal transferability particularlydesirable MOOC environments courses offered different instructors manyuniversities. mentioned before, course-level sub-graphs different universitiesoverlap other, prerequisite links local within sub-graph.Thus enable cross-university transfer, crucial project course-level prerequisitelinks different universities onto directed links among universal concepts.bi-directional inference two directed graphs makes CGL frameworkfundamentally different existing approaches graph-based link detection (Kunegis &Lommatzsch, 2009; Liben-Nowell & Kleinberg, 2007; Lichtenwalter, Lussier, & Chawla,2010), matrix completion (Candes & Recht, 2009; Fazel, 2002; Johnson, 1990) collaborative filtering (Su & Khoshgoftaar, 2009). is, approach requires explicit learning1061fiLiu, Ma, Yang, & Carbonellconcept-level directed graph optimal mapping two levels linksmethods (see Section 7 discussion).main contributions paper1 summarized as:1. novel framework within- cross-level inference prerequisite relationscourse-level concept-level;2. New algorithmic solutions scalable concept graph learning various (dense,sparse transductive) settings;3. New data collections multiple universities syllabus descriptions, prerequisitelinks lecture materials;4. first evaluation prerequisite link prediction within- cross-universitysettings.rest paper organized follows: Section 2 introduces formal definitionsframework optimization objectives; Section 3 provides scalable algorithmslearning large concept graphs; Section 4 extends new method learn sparse, parsimonious concept graph better interpretability; Section 5 explores unlabeled coursepairs leveraged significantly improve prediction performance learnedconcept graph; Section 6 describes new datasets collected study futurebenchmark evaluations, reports empirical findings; Section 7 discusses related workconcept graphs deployed benefit future educational applications;Section 8 summarizes main findings study.2. Framework & AlgorithmsLet us formally define methods following notation.n number courses training set;p dimension universal concept space (Section 2.1);X = [x1 , x2 , . . . , xn ]> Rnp collection n courses, xi Rp bag-ofconcepts representation i-th course;{1, +1}nn collection n2 binary indicators observed prerequisiterelations courses, i.e., yij = 1 means course j prerequisite coursei, yij = 1 otherwise.Rpp adjacency matrix concept graph, whose elements weightsdirected links among concepts. is, matrix model parameterswant optimize given training data X .1. journal paper substantially extended version previous paper (Yang, Liu, Carbonell, & Ma,2015).1062fiLearning Concept Graphs Online Educational Data2.1 Representation Schemesbest way represent contents courses learn universal conceptspace? explore different answers four alternate choices follows:1. Word-based Representation (Word): method uses vocabulary coursedescriptions plus listed keywords course providers (MIT, Caltech, CMUPrinceton) entire concept (feature) space. applied standard procedurestext preprocessing, including stop-word removal, term-frequency (TF) based termweighting, removal rare words whose training-set frequency one.use TF-IDF weighting relative small number documents(courses) datasets allow reliable estimates IDF part.2. Sparse Coding Words (SCW): method projects original n-dimensionalvector representations words (the columns course-by-word matrix X) ontosparse vectors smaller k-dimensional space using Non-negative Matrix Factorization (Lee & Seung, 1999), k much smaller n. One view lowerdimensional components system-discovered latent concepts. Intriguedsuccessful application sparse coding image processing (Hoyer, 2004), exploredapplication graph-based inference problem. applying existing sparsecoding algorithm (Kim & Park, 2008) training sets obtained k-dimensionalvector word; taking average word vectors course obtainedbag-of-concepts representation course. resulted n-by-k matrix X,representing training-set courses k-dimensional space latent concepts.set k = 100 experiments based cross validation.3. Distributed Word Embedding (DWE): method also uses dimension-reducedvectors represent words courses, similar SCW. However, lower dimensional vectors (continuous vector representations) words discovered neuralnetworks based word usage w.r.t. contextual, syntactic semantic information(Le & Mikolov, 2014). Intrigued popularity DWE recent researchNatural Language Processing domains (Collobert, Weston, Bottou, Karlen,Kavukcuoglu, & Kuksa, 2011; Chen, Perozzi, Al-Rfou, & Skiena, 2013), exploredapplication graph-based inference problem. Specifically, deploy Englishword embeddings trained Wikipedia articles (Al-Rfou, Perozzi, & Skiena, 2013),domain believed semantically close academic courses.vector representation course obtained aggregating vector representations words contains.4. Category-based Representation (Cat): method used large subsetWikipedia categories concept space. selected subset via pooling strategy follows: used words training-set courses form 3509 queries (onequery per course), retrieved top 100 documents per query based cosinesimilarity. took union Wikipedia category labels retrieveddocuments, removed categories retrieved three queriesless. process resulted total 10,051 categories concept space.categorization courses based earlier highly scalable very-large category1063fiLiu, Ma, Yang, & Carbonellspace work (Gopal & Yang, 2013): classifiers trained labeled Wikipediaarticles applied word-based vector representation course(weighted) category assignments.representation schemes may strengths weaknesses.Word simple natural rather noisy, semantically equivalent lexical variantsunified canonical concepts could systematic vocabulary variationacross universities. Also, scheme work cross-language settings, e.g., coursedescriptions English Chinese. Cat would less noisy better cross-languagesettings, automated classification step unavoidably introduce errors categoryassignments. SCW (sparse coding words) reduces total number model parametersvia dimensionality reduction, may lead robust training (avoiding overfitting)efficient computation, risk losing useful information projectionoriginal high-dimensional space lower dimensional space. DWE (distributed wordembedding) deploys recent advances representation learning word meanings context.However, reliable word embedding requires availability large volumes training text(e.g., Wikipedia articles); potential mismatch training domain (forlarge volumes data obtained easily) test domain (for large volumesdata hard costly obtain) could serious issue. Yet another distinction amongrepresentation schemes Word Cat produce human-understandable conceptslinks, SCW DWE produce latent factors harder interprethumans, although methods L1 regularization help interpretability (sec 6.5).exploring four representation schemes unified framework two-levelgraph based inference, examining effectiveness task link predictionprerequisite relations among courses, aim obtain deeper understandingstrengths weaknesses representational choices.2.2 Optimization Methodsdefine problem concept graph learning key part learning-to-predict prerequisite relations among courses, i.e., two-level statistical inference introducedSection 1 Figure 1. Given training set courses bag-of-concepts representation per course row matrix X, list known prerequisite links per course rowmatrix Y, optimize matrix whose elements specify direction (sign)strength (magnitudes) link concepts. propose two new approachesproblem: classification approach learning-to-rank approach. approachesdeploy extended versions SVM algorithms squared hinge loss,objective functions optimization different. also propose nearest-neighbor approach comparison, predicts course-level links (prerequisites) without learningconcept-level links.2.2.1 Classification Approach (CGL.Class)method, predict score prerequisite link course course j as:Fij = x>Axj1064(1)fiLearning Concept Graphs Online Educational DataFigure 2: weighted connections course course j via matrix encodesdirected links conceptsintuition behind formula shown Figure 2. easily verifiedquantity x>Axj summation weights paths node nodej graph, path weighted using product corresponding xik ,Akk0 xjk0 . words, assume prerequisite strength two coursescumulative effect prerequisite strengths concept pairs.criterion optimizing matrix given training data xi = 1, 2, . . . , n truelabels yij course pairs defined as:minARppi2Xh1 yij x>Ax+ kAk2Fj2+(2)i,j(1 v)+ = max(0, 1 v) denotes hinge function, k kF denotes matrixFrobenius norm. 1st term formula (2) empirical loss; 2nd term regularization term, controlling model complexity based large margin principle.choose use squared hinge loss (1 v)2+ first term gain first-order continuityobjective function, enabling efficient computation using accelerated gradient descent(Nesterov, 1983, 1988) (Section 3). efficiency improvement crucial operate pairs courses, thus much larger space normal classification(e.g. classifying individual courses).2.2.2 Learning-to-Rank Approach (CGL.Rank)Inspired learning-to-rank literature (Joachims, Li, Liu, & Zhai, 2007), exploredgoing beyond binary classifier previous approach one essentially learnsrank prerequisite preferences. Let set course pairs true labels yij = 1different js, pairs courses true labels yik = 1 different ks,want system give pairs higher scores pair .call partial-order preference links conditioned course i.1065fiLiu, Ma, Yang, & CarbonellLet union tuple sets {(i, j, k)|(i, j) , (i, k) } 1, 2, . . . , n.formulate optimization problem as:XminARpp(i,j,k)Thi2>1 x>+ kAk2FAxj xi Axk2+(3)equivalently, objective rewritten as:minARppX1 XAX(i,j,k)T>ij+ XAX>2ik ++kAk2F2(4)Solving optimization problem requires us extend standard packages SVM algorithms order improve computational efficiency number model parameters(p2 ) formulation large. example, vocabulary size 15,396 wordsMIT dataset, number model parameters 237 million. p(the number concepts) much larger n (the number courses), one may considersolving optimization problem dual space instead primal space. However,even dual space, number dual variables still O(n3 ), correspondingtriplets , kernel matrix order O(n6 ). going addresscomputational challenges Section 3.2.2.3 Nearest-Neighbor Approach (kNN)Different two approaches matrix plays central role, differentbaseline, propose predict prerequisite relationship pair courses basedmatrices X without A. Let (i0 , j 0 ) new pair courses test set.score course pair (i, j) training set respect new test pair as:ffffxi0 , xi xj 0 , xji, j = 1, 2, . . . , n(5)h, stands inner product two vectors. taking top-scored pairstraining set aggregating corresponding yij s, perform kNN-basedprediction yi0 j 0 new test pair. normalize vectors, dot-products (5)become cosine similarity. approach requires nearest-neighbor search on-demand;number course pairs test set large, online computation wouldsubstantial. Via cross validation, found k = 1 (1NN) works best problemcurrent datasets.2.2.4 Support Vector Machine (SVM)another baseline comparision also include SVM following objective:minpwRX h1 yij (xi xj )> wi,j++kwk222(6)Similar kNN approach unlike CGL, SVM optimization (6)involve learning matrix (the directed graph universal concepts). feature vector1066fiLearning Concept Graphs Online Educational Datacourse pair simply pairwise difference two vector representations (thetwo bags words) courses. model parameter vector w optimizedlabeled training set, used predict prerequisite relation among paircourses computing w> (xi xj ) whose sign indicates direction relationship,whose magnitude indicates strength relationship. sorting scorescourse pairs fixed test set, ranked list candidate prerequisitesobtained course i.3. Scalable AlgorithmsThough appears gradient-based method directly applicable CGL.Rankaccording (4), optimization computationally challenging due following facts:(a) large number course-level triplets (i, j, k) , n3 worstcase. makes gradient computation w.r.t. loss term (4) expensive.(b) large size matrix Rpp . example, number entries Wordrepresentation MIT dataset goes 237 million, making matrix manipulationscostly time space.tackle challenge (a), natural one consider Stochastic Gradient Descent(SGD). SGD avoids expensive summation O(n3 ) triplets taking noisy(instead exact) gradient step iteration, noisy gradient step computedsolely based one individual triplet randomly sampled training data. Stochasticoptimization recently successfully applied triplet-based loss functions,collaborative filtering implicit feedback (Rendle, Freudenthaler, Gantner, & SchmidtThieme, 2009). However, success SGD crucially relies assumptionnoisy gradient step sufficiently cheap, true case due challenge (b).tackle challenge (b), one would consider solving dual problem CGL.Rankdual space may smaller number coefficients learnin case,p2 entries folded kernel matrix. However, number dual variablesCGL.Rank equal number triplets, i.e., n3 worst case, scalable optimizationdual space still hard.following sections, first address (b) reformulating CGL.Rank problem(4) way optimization objective remains equivalent numbervariables substantially reduced (from p2 n2 ). Then, address problem (a)two specific algorithms substantially reduced number iterationsoptimization.3.1 Reduce Number VariablesTheorem 3.1 (Variable Reduction). Let kernel matrix K = XX > , let h,matrix inner product. minimizer CGL.Rank optimization (4) Bminimizer following optimization problemminBRnnX(i,j,k)Thi2ff1 (KBK)ij + (KBK)ik + KBK, B2+1067(7)fiLiu, Ma, Yang, & Carbonell= X > B X.Proof. First, let us introduce dummy matrix variable F = XAX > Rnn ,element F , denoted Fij , corresponds estimated strength prerequisitedependency course course j.F rewrite unconstrained optimization (4) constrained optimizationXminARpp ,F Rnn(1 Fij + Fik )2+ +(i,j,k)Tsubject F = XAXkAk2F2(8)>going show optimization (8), degree freedom optimalactually much smaller p2 .achieve this, introduce matrix dual variable Rnn corresponding n2equality constraints F = XAX > . Lagrangian (8) writtenL (A, F, ) =X(1 Fij + Fik )2+ +(i,j,k)TEkAk2F + F XAX > ,2(9)hard verity (8) convex optimization Slaters condition satisfied,hence strong duality holds. According stationarity condition, derivativeLagrangian w.r.t. vanish zero optimal.EkAk2F + F XAX > ,2> >=tr X XA= X > XL (A, F, )=(10)=0= = 1 X > X. worth noticing Rpp contains p2 variables,completely determined Rnn involves n2 variables (n p).let us define B 1 Rnn . Combining = 1 X > X = X > B Xconstraint F = XAX > , F = KB K K = XX > . Plugging backexpressions F (8) yields optimization (7).substantially reduced number variables optimization (7) allows us efficientlycompute store gradients concept graph learning.3.2 Reduce Number Iterationssection introduce two algorithms optimization (7) leads speedups reducing total number iterations.1068fiLearning Concept Graphs Online Educational Data3.2.1 Accelerated Gradient DescentAlthough gradient descent readily applicable (7), convergence become slowapproach optimal. smoothness objective function (with squaredhinge loss) enables us deploy Nestrerovs accelerated gradient descent (Nesterov, 1983,1988), ensuring faster convergence rate O(t2 ) rate O(t1 ) gradientdescent, number gradient steps.Recall Section 3 F = XAX > = KBK. Denote ijk = (1 Fij + Fik )+ei i-th unit vector Rn .gradient objective (7) w.r.t. BffF, B2(i,j,k)TX= 2ijk (Fij Fik ) + tr BKB > K2(i,j,k)ThX>= 2KtrBKeeK+ KBKijk tr BKej e>kB =X(1 Fij + Fik )2+ +(11)(i,j,k)T= 2KX>K + Fijk ei e>j ei ek(i,j,k)TP>Despite large number course-level triplets , matrix (i,j,k)T ijk ei e>eejksize n n still computed efficiently since majority triplets inactive (i.e. ijk = 0 large number triplet (i, j, k)) optimization. fact,number operations required evaluating ijk substantially reducedmaintaining specialized data structures order statistics tree (Cormen, Leiserson, Rivest, & Stein, 2001), recently exploited speed gradientcomputation rankSVM (Lee & Lin, 2014; Airola, Pahikkala, & Salakoski, 2011).Detailed implementation CGL.Rank accelerated gradient descent summarizedAlgorithm 1.3.2.2 Inexact Newton Methodoften turns bottleneck gradient computation, variable reduction,dense matrix-matrix multiplication complexity around O(n2.373 ) (Davie & Stothers,2013). multiplication affordable case since number courses ndealing thousands. However, scale substantially larger data collections, one may need either consider pruning per-iteration complexitytechniques low-rank kernel approximation (Williams & Seeger, 2001), reducing total number iterations. section, focus latter incorporatingsecond-order information using Newtons method.Newtons method going derive inexact (Dembo, Eisenstat, & Steihaug,1982). is, approximate Newton direction iteration approximately solving linear system via preconditioned Conjugate Gradient method (PCG)without inverting Hessian. fact, going avoid explicitly writing Hessian1069fiLiu, Ma, Yang, & CarbonellAlgorithm 1 CGL.Rank Nestrerovs Accelerated Gradient Descent1: procedure CGL.Rank.Nestrerov(X, T, , )2:K XX > , B 0, Q 03:t14:converge5:06:F KBK7:(i, j, k)8:ijk 1 Fij + Fik9:ijk > 010:ij ij + ijk11:ik ik ijk12:13:14:15:16:17:P B (F 2KK)B P + t1t+2 (P Q)QPtt+1X > BXreturn22entire optimization process, since Hessian H Rn n (correspondingn2 model parameters B Rnn ) extremely large.Denote Tensor (Kronecker) product operator matrices, eij =2ei ej (i n + j)-th unit vector Rn . Hessian (7) explicitly derivedH = 2 (K K) (K K) + K K(12)Pshorthand (i,j,k)T (eij eik ) (eij eik )> .Denote vec vectorization operator concatenates columns matrixsingle vector. BasedP (11) (12) one verify always true vec (B )Hvec (B) 2 (K K) (i,j,k)T (eij eik ). Therefore, Newton updatevec (B) vec (B) H 1 vec (B )X= vec (B) H 1 Hvec (B) 2 (K K)(eij eik )(i,j,k)T= 2H 1 (K K)X(eij eik )(13)(i,j,k)T= 2 [ (K K) + In2 ]1X(eij eik )(i,j,k)TThough even intractable compute n2 n2 matrix inside inverse operation(13), updated vec(B) (or matrix B) well approximated solving followinglinear system via PCG iterative methodX2(eij eik ) = (K K) vec (B) + vec (B)(14)(i,j,k)T1070fiLearning Concept Graphs Online Educational Datacomputation bottleneck PCG lies matrix-vector multiplication (K K) vec (B),22seems expensive requires huge dense matrix K K Rn n storedmemory. Interestingly, equivalently write expression vec (KBK) playingvec trick Tensor (Kronecker) product (Van Loan, 2000). reformulation,aforementioned matrix-vector multiplication becomes affordable since vec (KBK)computed O(n2.373 ) highly sparse.suggests complexity iteration PCG cheapgradient step. also empirically observed inexact Newton method requiressmall constant number (typically 3-5) Newton updates reach optimal,average Newton update 10 PCG iterations suffice yield good results.4. Learning Sparse Concept GraphNotice CGL algorithm studied far produces fully dense concept graph A. However, commonly believed dependencies among knowledge conceptshighly sparse. sparse concept graph also desirable visualization purposes thus allowing intuitive user-exploration. reasons, section modifyCGL algorithm produce sparse graphs (sparse-CGL).straightforward enforce sparsity replacing `2 -norm overPconcept graphoriginal CGL formulation `1 -norm defined kAk1 := i,j |aij |.case, sparse CGL optimization objective cast2X>min1 x>Ax+xAx+ kAk1(15)jkARpp+(i,j,k)TOptimization (15) viewed generalization LASSO (which known producesparse solutions), except using pairwise squared hinge loss coefficients matrix forms. Unlike LASSO one optimizes vector coefficient,parameter space (15) high-dimensional matrix extremely large size.Due presence `1 -norm objective function, previous parameterreduction techniques CGL longer applied sparse CGL. following,going focus directly carrying optimization w.r.t. A. feasiblestoring large highly sparse concept graph much cheaper dense case.4.1 Efficient Optimization Sparse-CGLAlthough sub-gradient methods directly applied minimizing non-smooth objective (15), suffer slow convergence rate theoretical empiricalperspectives. Commonly used optimization solvers `1 regularization coordinate descent (CD) (Tseng & Yun, 2009; Chang, Hsieh, & Lin, 2008) longerclosed-form solution case sub-step, needs many p2 steps goeven single cycle model parameters A.4.1.1 Proximal Gradient DescentAmong family first-order methods, proximal gradient descent (PGD) widelyapplied objective functions involving non-smooth components. enjoys several desir1071fiLiu, Ma, Yang, & Carbonellable computational properties, including order convergence rategradient descent even applied non-smooth objective functions. updating stepPGD efficiently performed long proximal operation efficient.proximal operator sparse CGL (15) defined1kA Zk22 + kZk12tkproxtk (A) := argminZRpp(16)tk step size k-th iteration. solution optimization (16)expressed concisely closed-form:proxtk (A) = Stk (A)(17): Rpp 7 Rpp known soft-thresholding operator (parameterized )applied element A. is, i, jAij Aij >[S (A)]ij = 0(18)AijAij + Aij <proximal operator proxtk (A) Stk (A), PGD iteratively appliesA(k) = proxtk A(k1) tk g(A(k1) ) = Stk A(k1) tk g(A(k1) )(19)model converges. expression g(A) denotes gradient firstterm optimization (15), recastXijk xi (xj xk )>(20)g A(k1) = 2(i,j,k)T= 2X >Xijk ei (ej ek )> X(21)(i,j,k)T>= 2X X(22)sparse CGL, PGD guaranteed reach global optimal since smoothnon-smooth components objective function (15) convex A.4.1.2 PGD Nesterovs AccelerationSimilar gradient descent CGL discussed previous section 3.2.1, convergencerate PGD sparse-CGL accelerated applying Nesterovs method.case, replace (19) PGDP (k) = Stk A(k1) tk g A(k1)(23)k1A(k) = P (k) +P (k) P (k1)(24)k+21072fiLearning Concept Graphs Online Educational DataAlgorithm 2 Sparse CGL.Rank Accelerated PGD1: procedure Sparse.CGL.Rank(X, T, , )2:0pp , P 0pp , Q 0pp3:k14:converge5:0nn6:F XAX >7:(i, j, k)8:ijk 1 Fij + Fik9:ijk > 010:ij ij + ijk11:ik ik ijk12:13:14:15:16:17:j = 1, 2 . . . pPj = Stk Aj + 2tk X > XjP + k1k+2 (P Q)QPk k+1returnuse P Rpp capture momentum information historical iterations.gradient descent PGD, convergence accelerated PGD guaranteed sparseCGL substantially faster rate.One might concerned (19) (23), gradient g A(k1) densematrix definition (20) therefore expensive memory consumption throughoutoptimization. address issue, recall soft-thresholding Stk operator applied(k)element-wisely input. Let Pi i-th column P (k) ,h(k)(k1)(25)Pi = Stk Aitk g A(k1)(k1)= Stk Ai+ 2tk X > N Xi(26)suggests sequentially threshold A(k1) tk g A(k1)via Stk column-by-(k)Pi s)column,without storingstore resulting sparse column vectors (theg A(k1) . Details accelerated PGD sparse CGL.Rank summarized Alg. 2.5. Transductive Concept Graph Learningreal scenarios, observed course-level prerequisite links highly sparse. example,1,173 2,694,681 (0.043%) possible links observed MIT datacollection. Meanwhile, notice features unlabeled course-level links alreadygiven, massively available training phase. argue transductivelearning particular effective case following reasons:1. helps better leverage unlabeled data allowing information propagatelabeled unlabeled course pairs.1073fiLiu, Ma, Yang, & Carbonell2. makes weaker assumptions unlabeled (missing) links. contrastprevious CGL formulation unobserved links implicitly treatednegative examples.derive transductive CGL, start following equivalent form CGL,derived eliminating original CGL optimization via constraint F = XAX > .XminF Rnn(1 Fij + Fik )2+ +(i,j,k)Tvec(F )> (K K)1 vec(F )2(27)viewing second term (27) negative log-likelihood, see originalCGL formulation essentially assumes vec(F ) sampled Gaussian prior distributioncovariance matrix K K K = XX > .allow transduction labeled unlabeled course-level links, proposereplace inverse pairwise kernel matrix K K (27) associated graph Lapla22cian matrix L Rn n (Chung, 1997), following previous work label propagation(Zhu, 2005) spectral kernel design (Zhang & Ando, 2006). Formally, define11L := 2 (D K K) 2(28)n2 n2 diagonal matrix diagonal elements equal corresponding row-sum (degree) K K. kernel matrix K symmetricallynormalized, hard show definition (28) reducesL = K K(29)case, vec(F )L vec(F ) becomes (normalized) manifold regularizer (Zhu, Ghahramani, & Lafferty, 2003) enforces predictions F smooth graphcourse-level links adjacency matrix K K. particular,X2vec(F )> L vec(F )kii0 kjj 0 Fi,j Fi0 ,j 0(30)(i,j),(i0 ,j 0 )Given two course-level links (i, j) (i0 , j 0 ), recall kii0 defines similarityi0 , kjj 0 denotes similarity j j 0 . Hence kii0 kjj 0 denotes similarity(i, j) (i0 , j 0 ), minimizing (30) essentially enforces similar course-level linksshare similar prerequisite strength.intuitions above, cast optimization transductive CGLminF RnnX(1 Fij + Fik )2+ +(i,j,k)Tvec(F )> L vec(F )2(31)5.1 Efficient Optimization Course-Level Linksworth mentioning though graph Laplacian matrix L course-level linksextremely large, also highly structured. result, able carry operationsinvolving L fairly efficiently. Moreover, going show explicit storagefull graph Laplacian avoided entire optimization.1074fiLearning Concept Graphs Online Educational DataAlgorithm 3 trans-CGL.Rank accelerated GD1: procedure CGL.Rank.Nestrerov(X, T, , )2:K XX > , F rand(n, n), Q 0nn3:t14:converge5:0nn6:(i, j, k)7:ijk 1 Fij + Fik8:ijk > 09:ij ij + ijk10:ik ik ijk11:12:13:14:15:16:P F (F KF K 2)F P + t1t+2 (P Q)QPtt+1X > K 1 F K 1 Xreturnfollowing, describe gradient computation carried transCGL. gradient transductive CGL objective (31) w.r.t. FXF =(1 Fij + Fik )2+ + vec1 (L vec(F ))(32)(i,j,k)TX= 2ijk (Fij Fik ) + vec1 [(I K K) vec(F )](33)(i,j,k)T= 2Xijk ei (ej ek )> + vec1 (vec(F )) vec1 [(K K) vec(F )] (34)(i,j,k)T= 2 + F KF K(35)hP>:=e(ee). order obtain last equality,k(i,j,k)T ijk japplied vectorization trick (Van Loan, 2000) Tensor (Kronecker) product thirdterm. Note expression gradient (35) doesnt involve tensor-type operation,22despite huge Laplacian matrix L Rn n original objective function.Second-order methods, inexact Newton method applicable trans-CGL.omit details since derivations similar CGL.5.2 Projecting Back Concept-Level Linksobjective transductive CGL (31) involves course-level prerequisite strengthF instead concept-level graph A. order recover optimal solutionF (31), consider solving following optimization problemminARppkAk22subject F = XAX >1075(36)fiLiu, Ma, Yang, & CarbonellUniversityMITCaltechCMUPrinceton# Courses232210488356# Prerequisites117376115090# Words1539656171955454Table 1: Datasets Statisticsbilinear system F = XAX > under-determined total number conceptsp assumed greater number courses n. multiple feasibleconcept graphs associated F , (36) aims pick concept graph minimum norm(which usually indicates strong generalization ability).Solution optimization (36) derived stationarity condition,written closed-form follows= X > K 1 F K 1 X(37)Details accelerated gradient descent trans-CGL.Rank, including recoverystep concept graph, summarized Alg. 3.6. Experimentscollected course listings, including course descriptions available prerequisite structure MIT OpenCourseWare, Caltech, CMU Princeton2 . first two complete course catalogs, latter two required spidering scraping, hencecollected Computer Science Statistics CMU, Mathematics Princeton.implies test within-university prerequisite discovery fourthoughMIT Caltech comprehensiveand cross-university university pairstraining university contains disciplines test university.Table 1 summarizes datasets statistics.evaluate performance, use Mean Average Precision (MAP)preferred metric information retrieval evaluating ranked lists, AreaCurve ROC (ROC/AUC simply AUC) popular link detection evaluations.6.1 Within-University Prerequisite Predictiontested methods dataset university. used one thirddata testing, remaining two thirds training validation. conducted5-fold cross validation training two-thirds, i.e., trained model 80%training/validation dataset, tuned extra parameters remaining 20%. repeatedprocess 5 times different 80-20% spit run. results 5 runsaveraged reporting results. Figure 3 Table 2 summarize results CGL.Rank,CGL.Class, 1NN SVM. methods used English words representationscheme first set experiments.2. datasets available http://nyc.lti.cs.cmu.edu/teacher/dataset/1076fiLearning Concept Graphs Online Educational DataMITCaltechCMUPrinceton1.00AUCMAP0.800.600.400.20ClRaL.L.CGCG1NNSVnkNSV1NClL.CGCGL.Rank1NNSVClL.CGCGL.Rank1NNSVClL.CGCGL.Rank0.00Figure 3: Different methods within-university prerequisite prediction: methodsused words concepts.AlgorithmCGL.RankCGL.Class1NNSVMCGL.RankCGL.Class1NNSVMCGL.RankCGL.Class1NNSVMCGL.RankCGL.Class1NNSVMDataMITMITMITMITCaltechCaltechCaltechCaltechCMUCMUCMUCMUPrincetonPrincetonPrincetonPrincetonAUC0.960.860.760.780.950.860.600.740.790.700.750.640.920.890.820.71MAP0.460.340.300.040.330.270.160.030.550.380.430.300.690.610.580.31Table 2: Results within-university prerequisite prediction using words concepts.1077fiLiu, Ma, Yang, & CarbonellNotice AUC scores methods much higher MAP scores.high AUC scores derive large part fact AUC gives equal weightsystem-predicted true positives, regardless positions system-produced rankedlists. hand, MAP weighs heavily true positives higher positionsranked lists. words, MAP measures performance system harder task:system needs find true positives (along false positives), also needsrank higher false positives possible order obtain high MAP score.Using concrete example, totally useless system makes positive negativepredictions random 50% chances AUC score 50%.system extremely low score MAP chance true positiverandomly appear top ranked list low true negatives dominatedomain. datasets domain course requiressmall number courses prerequisites. Back original point, regardlesspopularity AUC link detection evaluations, limitation recognized:relative performance among methods informative absolute values AUC.see Figure 3, relative ordering methods AUC MAPindeed highly correlated across datasets. MAP emphasizes positive instancestop portion ranked list, hence sensible measuring usefulnesssystem user interacts system-recommended ranked list prerequisitesper query (a course test set).Comparing results methods, see CGL.Rank clearly dominatesothers AUC MAP datasets. CGL.Class second best, outperforming 1NN SVM. Comparing 1NN SVM, two methods comparableAUC average; however, 1NN better SVM MAP four datasets.One may wonder SVM relatively poor performance course-level prerequisite prediction particular MAP metric, given works welltext classification learning rank retrieval general. argue SVMsuffering major difficulty prerequisite prediction due extremely sparselabeled positive training instances (prerequisite pairs). Recall text classificationSVM optimizes one w category; however, prerequisite prediction SVM usesw generate ranked lists possible queries (i.e., test-set courses). Thuslabeled training instances per query extremely sparse average, resulting poorperformance observed. words, SVM generalized much extremelysmall set labeled training instances optimizing global w. kNN (or 1NN)hand, suffers less SVM inference kNN based localtraining instances neighborhood query, instead global generalizationqueries.3 Nevertheless, neither kNN SVM competitive comparisonproposed CGL methods, evident set evaluation results.3. Notice SVM, described (6), essentially going produce identical ranked list prerequisitecourses given course. see this, consider given course i, SVM scores remaining courses>>scorej := (xi xj )> w = x>w xj w different js. Since given, first term xi wignored ranking shared across scores. result, ranked list becomesirrelevant xi itself. analysis holds course hence ranked list prerequisitesidentical courses, leading low MAP score. contrast, CGL scores course j>>scorej = x>Axj = (Axi ) xj := wi xj ranking coefficient wi personalized i-thcourse, hence able produce diverse ranked lists different courses.1078fiLearning Concept Graphs Online Educational Data1.001.00AUCMAP0.800.600.600.400.400.200.200.00WordCat.SCW0.00DWE(a) CGL.Rank MIT Data1.000.400.400.200.20Cat.SCWSCWDWEAUCMAP0.800.60WordCat.1.000.600.00Word(b) CGL.Rank Caltech DataAUCMAP0.80AUCMAP0.800.00DWE(c) CGL.Rank CMU DataWordCat.SCWDWE(d) CGL.Rank Princeton DataFigure 4: CGL.Rank different representation schemes within-university prerequisiteprediction6.2 Effects Representation SchemesFigure 4 Table 3 summarize results CGL.Rank four representationschemes described Section 2.1, i.e., Word, Cat, SCW (sparse coding words) DWE(distributed word embedding), respectively. scores AUC MAPscale, relative performance suggest Word Cat competitive(or Word slightly better datasets), followed SCWDWE. rest empirical results reported paper, focusperformance CGL.Rank Word representation scheme performsbetter, space limit allow us present results every possiblepermutation method, scheme, dataset metric.6.3 Cross-University Prerequisite Predictionset experiments, fixed test sets used within-universityevaluations, alter training sets across universities, yielding transfer learning results models trained data different universitytested. fixing test sets within- cross-university evaluations compare results common basis. competitive performance Catcomparison Word encouraging, given Wikipedia categories definedgeneral knowledge, classifiers (SVMs) used category assignment coursestrained Wikipedia articles instead course materials (because1079fiLiu, Ma, Yang, & CarbonellConceptWordCat.SCWDWEWordCat.SCWDWEWordCat.SCWDWEWordCat.SCWDWEDataMITMITMITMITCaltechCaltechCaltechCaltechCMUCMUCMUCMUPrincetonPrincetonPrincetonPrincetonAUC0.960.930.930.830.950.930.910.760.790.770.730.670.920.840.820.77MAP0.460.360.330.090.330.320.220.120.550.550.430.350.690.680.600.50Table 3: CGL.Rank four representationshuman assigned Wikipedia category labels courses). means Wikipediacategories indeed good coverage concepts taught universities (andprobably MOOC courses), pooling strategy selecting relevant subsetWikipedia categories reasonably successful.Table 4 Figure 5 show results CGL.Rank (using words concepts). RecallMIT Caltech data cover complete course catalogs, CMU datacover Computer Science Statistics, Princeton dataMathematics. implies measure transfer learning pairstraining university contains disciplines test university. comparing red bars(when training university test university same) blue bars (whentraining university test university), see performanceloss transfer learning, expected. Nevertheless, get transfer,first report successful transfer learning educational knowledge, especiallyprerequisite structures disjoint graphs, across different universities unifiedconcept graph. results therefore highly encouraging suggest continued effortsimprove. results also suggest interesting points, e.g., MIT mightbetter coverage topics taught Caltech, compared inverse. And, MIT coursesseem closer Princeton (Math) compared CMU.6.4 CGL vs Sparse-CGLsubsection evaluate performance CGL (CGL.Rank) sparse-CGL (Section 4) course-level prerequisite prediction. two methods compared budgetconstraints, mean number allowed links system-induced concept graph controlled condition comparison. words, control1080fiLearning Concept Graphs Online Educational DataTrainingMITCaltechCaltechMITCMUMITCaltechPrincetonMITCaltechTestMITMITCaltechCaltechCMUCMUCMUPrincetonPrincetonPrincetonMAP0.460.130.330.250.550.340.280.690.460.43AUC0.960.880.950.860.790.700.620.920.720.58Table 4: CGL.Rank within-university cross-university settings1.00MAP0.800.600.400.20U.Ca MI CMPrlte T.C Uchce.C Un.UCa MI Prilte T.P ncech rin.P ce nrince nnCMCa MIlte T.Mch.MCaltech.Calt.C ecal htech0.00<Traning Set>.<Test Set>Figure 5: Results CGL.Rank based words tasks within-university (red)cross-university (blue) prerequisite prediction.graph sparsity varying number allowed non-zero elements 4 , compareperformance two methods conditioned fixed degree graph sparsity.Figure 6 compares performance two methods datasets MIT, Caltech,CMU Princeton, task within-university prediction course-level prerequisiterelations. horizontal axis graph specifies number non-zero elementsallowed matrix A, ranging 27 212 . vertical axis graph MAP4. concept graph induced CGL typically dense, sparsified (given desired sparsity)keeping dominating elements setting remaining elements zero. sparseCGL, hand, sparse graphs directly induced optimization algorithm adjustingvalues `1 -regularization strength .1081fiLiu, Ma, Yang, & Carbonellleft AUC right data set. CGL curves blue,sparse-CGL curves red.Clearly, sparse-CGL consistently outperformed CGL experimentsbudget regions datasets, MAP AUC. results highly encouraging effective efficient interaction navigation users system-inducedconcept graph, interpretable relatively sparse graph often preferabledensely connected graph. budget constrained graphs would also lead scalablecurriculum planning fast computation on-demand recommendation.6.5 CGL vs Trans-CGLsubsection compare course-level prerequisite prediction performance CGL(in particular, CGL.Rank) transductive extension. two methods testedaforementioned four datasets words representation scheme. experiments conducted 5-fold cross-validation setting described 6.1.use MAP AUC evaluation metrics summarize results Table 5.Table 5 see link prediction performance trans-CGL dominatesCGL. justifies previous arguments making good use massive unlabeledcourse-level links provided training set help us get better prediction performance.Meanwhile, notice performance gain obtained trans-CGL MITlarge three institutions. Since MIT dataset substantially larger amount course-level labels, conjecture trans-CGL, manytransductive/semi-supervised learning approaches general, advantageousavailable supervision insufficient (compared amount hidden informationunlabeled data). validate thought, repeat experiments two methods down-sampled MIT dataset. is, use random subset availablecourse-level prerequisite links training, gradually vary size training subsetget multiple set results. results summarized Table 6.InstitutionMITCaltechCMUPrincetonMAPCGLtrans-CGL0.4820.4850.4770.4990.4820.5390.4360.445AUCCGLtrans-CGL0.9560.9570.9290.9410.8010.8180.6340.67Table 5: Comparison course prerequisite prediction performance CGLtrans-CGL MIT, Caltech, CMU Princeton. Results significancetests (paired t-tests) best method methoddataset denoted significance 1% level.6.6 Experiment Detailstested efficiency proposed algorithms (based optimization formulationvariable reduction) single machine Intel i7 8-core processor 32GB1082fiLearning Concept Graphs Online Educational DataMAP MITAUC MIT0.450.960.940.920.90.880.860.840.820.80.40.350.30.250.2CGLsparse-CGL0.150.1CGLsparse-CGL964048202410251625812964048202410251625812MAP CaltechAUC Caltech0.220.850.20.80.180.750.160.70.14CGLsparse-CGL0.120.1CGLsparse-CGL0.650.66940842042100.440.430.420.410.40.390.380.370.360.350.340.33251625812694084204210251625812MAP CMUAUC CMU0.820.80.780.760.740.72CGLsparse-CGLCGLsparse-CGL0.70.68694084204210251625812694084204210251625812MAP PrincetonAUC Princeton0.440.670.420.660.40.650.380.640.360.340.63CGLsparse-CGL0.320.3CGLsparse-CGL0.620.61964048202410251625812964048202410251625812Figure 6: Comparison CGL (blue curves) sparse-CGL (red curves) predictioncourse-level prerequisite relations within MIT, Caltech, CMU Princeton,respectively. x-axis graph specifies budget constraint logscale, number allowed non-zero elements matrix A. y-axismeasures performance MAP left, AUC right.1083fiLiu, Ma, Yang, & CarbonellTraining DataDown-sampling Rate1101918171615141312MAPCGLtrans-CGL0.1830.2020.1490.1530.1920.2130.2310.2490.2450.260.2640.2850.2540.2620.2580.2740.2890.307AUCCGLtrans-CGL0.8430.8740.8320.8380.8420.8450.8720.8750.8730.8720.8760.8920.8730.8680.8970.910.9050.914Table 6: Comparison course prerequisite prediction performance CGLtrans-CGL MIT dataset, training size varies 10% 50%.RAM. largest MIT dataset 981,009 training triplets 490,505 test triplets,CGL.Rank gradient descent took 37.3 minutes 1490 iterations reach convergence rate 103 . achieve objective value, accelerated gradient descenttook 3.08 minutes 401MB memory 103 iterations, inexact Newton methodtook 43.4 seconds 587MB memory. CGL.Class equally efficient CGL.Rankterms run time, though latter superior terms result quality. 1NNbaseline, took 2.88 hours since huge number (2 981, 009 490, 505) dot-productsneed computed fly.CPU time consumed trans-CGL.Rank similar CGL.Rank.sparse-CGL, took accelerated proximal gradient method 2.07 minutes reachconvergence rate 103 MIT 3.9GB peak memory consumption. resultingconcept graph 1519 nonzero entries.7. Discussion Related WorkWhereas task inferring prerequisite relations among courses concepts,task inferring concept network course network order transfer learnedrelations new, extensions SVM algorithms presented, workinspired methods related fields, primarily:collaborative filtering via matrix completion literature focused onegraph, bipartite graph u users preferences (e.g. movies products).Given known values u-by-m matrix, task estimate remaining values(e.g. unseen movies products would user like) (Su et al., 2009).done via methods affine rank minimization (Fazel, 2002) reduce convexoptimization (Boyd & Vandenberghe, 2004).Another line related research transfer learning (Do & Ng, 2005; Yang, Hanneke,& Carbonell, 2013; Zhang, Ghahramani, & Yang, 2008). seek transfer prerequisiterelations pairs courses within universities pairs also within universities,pairs span universities. inspired different transferlearning literature. Transfer learning traditionally seeks transfer informative features,priors, latent structures recently regularization penalties (Kshirsagar, Carbonell,& Klein-Seetharaman, 1990). Instead, transfer shared concepts mappingscourse-space concept-space induce prerequisite relations.Although evaluations primarily focus detecting prerequisite relations amongcourses, task one direct application automatically induced univer1084fiLearning Concept Graphs Online Educational DataLinear_algebraNumerical_analysisProbability_theoryApplied_mathematics_stubsFunctional_analysisIntegral_calculusCyberneticsMathematical_analysis_stubsRandomnessMathematical_optimizationOperations_researchDifferential_geometryFourier_analysisComputational_science Signal_processingDynamical_systemsFigure 7: visualization concept graph produced CGL.Rank based 10,051 academic Wikipedia categories, 2322 courses 1173 prerequisite relations MITOpenCourseWare. node denotes concept (i.e. Wikipedia category),strength link encodes prerequisite strength pair concepts. Concepts small degrees links weak strength removed viathresholding visualization purposes.sal concept graph. important applications include automated semi-automatedcurriculum planning personal education goals based different backgrounds students, modularization course syllabus design instructors. tasks requireinteraction humans (students teachers) system-induced concept graphwell system-recommended options optimal sequences ordered courses. Figure7 visualizes sub-graph system-induced concept space based course materials(including partially observed prerequisite structure) MIT OpenCourseWare: nodesWikipedia categories (concepts), links system-predicted partial ordersinstructors follow teaching concepts, students follow learningconcepts. one see, Linear Algebra, Probability Theory Functional Analysishubs (with high degree out-links) graph, indicating conceptsfundamental one acquire pursuing advanced topicsDifferential Geometry, Cybernetics Fourier Analysis.Let us illustrate potential use CGL.Rank developmentproblem inferring prerequisite relationships observed prerequisites amongcourses (almost) available, e.g., context MOOC. Recall MOOC courses1085fiLiu, Ma, Yang, & Carbonellweights Xbigdataanalyticsbioinfomethods2patterndiscoveryTarget coursesweightsweights XData_warehousingRelational_database_management_systemsData_securityDistributed_computing_architectureComputer_science_stubsProject_management_softwareBusiness_intelligenceInformationInformation_technology_managementInformation_scienceData_modelingProject_managementDesignData_managementSoftware_design_patternsUrban_studies_and_planningData_managementInformation_technologyDatabase_management_systemsDatabase_management_systemsTarget conceptsPrerequisite conceptsstatinferencegetdataexdatacompmethodsintrostatsdataanalysispredmachlearnPrerequisite coursesFigure 8: example prerequisite course recommendation Coursera using concept graph learned MIT OpenCourseWare dataset. map courses(red, left) student wants learn concept space Wikipedia categories, find prerequisite concepts map back Coursera courses(red, right). sizes concept nodes middle (green) proportional aggregated weights corresponding links, strengthscourse-concept mapping concept-concept prerequisite relations showncolor intensity edges (purple).offered different institutions across world, cross-provider prerequisite linksamong courses explicitly available. instance, among 900+ coursesCoursera currently offers, half mention anything requiredbackground; remaining half, mentioned background requirements oftenvague, e.g., Undergraduate-level networking know-how recommended courseCloud Networking. overly generic vague notions sufficiently informativestudents understand true prerequisite relationship among courses, alsouseful intelligent systems learn prediction latent prerequisites automatically. 5example Figure 8 illustrates key idea applying CGL.Rank algorithmaddress problems above. first column left consists three Courseracourses student wants take, i.e., Big Data Analytics Healthcare (bigdataanalytics), Bioinformatic Methods II(bioinfomethods2 ) Pattern Discovery DataMining(patterndiscovery), respectively. second column nodes top-10 universal concepts (Wikipedia categories) assigned classifiers (Section 2.1 Cat)courses, color intensity edges 1st 2nd columns reflects5. Coursera offer so-called specializations, specialization consists sequence coursestopic. unclear whether specializations may serve prerequisite relations.1086fiLearning Concept Graphs Online Educational Dataconfidence scores (matrix X) category assignments. size concept nodeproportional aggregated confidence scores corresponding edges, indicatingrelative importance concept. third column nodes top-10 prerequisiteconcepts (also Wikipedia categories) concepts shown 2nd column,color intensity edges 2st 3nd columns reflects automaticallyinduced strengths concept-level links (Matrix A) CGL.Rank algorithm,used MIT OpenCourseWare data training set. nodes 4th columnCoursera courses best cover prerequisite concepts 3rd column; edges3rd column similar 1st 2nd columns. Together,chained network allows us make inference connections targetcourses (the left-most column) prerequisite courses (the right-most column).reader see Figure 8, many courses prerequisites highlyrelevant focus primary dimension data analytics, still constitute incomplete set second dimension biotechnology represented.process refining methods thorough testing performedwe offerexample work-in-progress indicating current challenges future directions.8. Concluding Remarksconducted new investigation automatically inferring directed graphscourse level concept level, enable prerequisite prediction within-universitycross-university settings.proposed three approaches: classification approach (CGL.Class), learning rankapproach (CGL.Rank), nearest-neighbor search approach (kNN). CGL.ClassCGL.Rank (deploying adapted versions SVM algorithms) explicitly model concept-leveldependencies directed graph, support interlingua-style transfer learningacross universities, kNN makes simpler prediction without learning concept dependencies. tackle extremely high-dimensional optimization problems (e.g., 2 108links concept graph MIT courses), novel reformulation CGL.Rankenables deployment fast numerical solutions. newly collected datasetsMIT, Caltech, CMU Princeton, CGL.Rank proved best MAP ROC/AUC,computationally much efficient kNN.extended aforementioned CGL algorithm produce sparse conceptgraph based `1 -regularization (sparse-CGL), leverage information massiveunlabeled course pairs based graph-regularization (trans-CGL). developed scalableoptimization strategies support new formulations, conducted experimentsempirically showed sparse-CGL able give interpretable conceptgraph, trans-CGL significantly consistently improved performanceordinary CGL course prerequisite prediction.also tested four representation schemes course content: using original words,using Wikipedia categories concepts, using distributed word representation, usingsparse word encoding. first two: original words Wikipedia-derived concepts provedbest. results within- cross-university settings highly encouraging.1087fiLiu, Ma, Yang, & Carbonellenvision cross-university transfer learning approaches particularlyimportant MOOCs courses come different providers across institutions,seldom explicit prerequisite links. rich suite future work includes:Testing cross-institution cross-course-provider prerequisite links. testedcross-university transfer learning, inferred links within target university, rather cross-institutional links. natural extension current workpredict cross-institutional prerequisites. evaluation need labeled groundtruth cross-institutional prerequisites.Cross-language transfer. Using Wikipedia categories entries different languages, would interesting challenge infer prerequisite relations coursesdifferent languages mapping Wikipedia category/concept interlingua.Extensions inference single source multiple sources, single media(text) multiple media (including videos), single granularity level (courses)multiple levels (including lectures).Deploying induced concept graph personalized curriculum planning students (as Section 7) syllabus design course modularization teachers.Acknowledgmentsthank reviewers helpful comments. work supported partNational Science Foundation grants IIS-1216282, IIS-1350364 IIS-1546329.ReferencesMIT OpenCourseWare. http://ocw.mit.edu/index.htm. Accessed: 2016-03-31.Airola, A., Pahikkala, T., & Salakoski, T. (2011). Training linear ranking SVMs linearithmic time using redblack trees. Pattern Recognition Letters, 32 (9), 13281336.Al-Rfou, R., Perozzi, B., & Skiena, S. (2013). Polyglot: Distributed word representationsmultilingual NLP. CoNLL-2013, 183.Boyd, S., & Vandenberghe, L. (2004). Convex optimization. Cambridge university press.Candes, E. J., & Recht, B. (2009). Exact matrix completion via convex optimization.Foundations Computational mathematics, 9 (6), 717772.Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2008). Coordinate descent method large-scalel2-loss linear support vector machines. Journal Machine Learning Research, 9,13691398.Chen, Y., Perozzi, B., Al-Rfou, R., & Skiena, S. (2013). expressive power word embeddings. ICML 2013 Workshop Deep Learning Audio, Speech, LanguageProcessing.Chung, F. R. K. (1997). Spectral graph theory, Vol. 92. American Mathematical Soc.1088fiLearning Concept Graphs Online Educational DataCollobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).Natural Language Processing (almost) Scratch. Journal Machine Learning Research, 12, 24932537.Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001). Introduction algorithms.MIT Press McGraw-Hill.Davie, A. M., & Stothers, A. J. (2013). Improved bound complexity matrix multiplication. Proceedings Royal Society Edinburgh: Section Mathematics,143 (02), 351369.Dembo, R. S., Eisenstat, S. C., & Steihaug, T. (1982). Inexact Newton methods. SIAMJournal Numerical analysis, 19 (2), 400408.Do, C., & Ng, A. Y. (2005). Transfer learning text classification. ProceedingsNIPS-05.Fazel, M. (2002). Matrix rank minimization applications. Ph.D. thesis, Stanford University.Gopal, S., & Yang, Y. (2013). Recursive regularization large-scale classificationhierarchical graphical dependencies. Proceedings 19th ACM SIGKDDinternational conference Knowledge discovery data mining, pp. 257265. ACM.Hoyer, P. O. (2004). Non-negative matrix factorization sparseness constraints.Journal Machine Learning Research, 5, 14571469.Joachims, T., Li, H., Liu, T.-Y., & Zhai, C. (2007). Learning rank informationretrieval (LR4IR 2007). SIGIR Forum, Vol. 41, pp. 5862.Johnson, C. R. (1990). Matrix completion problems: survey. Proceedings SymposiaApplied Mathematics, Vol. 40, pp. 171198.Kim, H., & Park, H. (2008). Nonnegative matrix factorization based alternating nonnegativity constrained least squares active set method. SIAM Journal MatrixAnalysis Applications, 30 (2), 713730.Kshirsagar, M., Carbonell, J., & Klein-Seetharaman, J. (1990). Transfer learning basedmethods towards discovery host-pathogen protein-protein interactions. ProcISMB, Vol. 40, pp. 171198.Kunegis, J., & Lommatzsch, A. (2009). Learning spectral graph transformations linkprediction. Proceedings 26th Annual International Conference MachineLearning, pp. 561568. ACM.Le, Q., & Mikolov, T. (2014). Distributed representations sentences documents.Proceedings 31st International Conference Machine Learning (ICML-14),pp. 11881196.Lee, C.-P., & Lin, C.-J. (2014). Large-scale linear rankSVM. Neural computation, 26 (4),781817.Lee, D. D., & Seung, H. S. (1999). Learning parts objects non-negative matrixfactorization. Nature, 401 (6755), 788791.1089fiLiu, Ma, Yang, & CarbonellLiben-Nowell, D., & Kleinberg, J. (2007). link-prediction problem social networks.Journal American society information science technology, 58 (7), 10191031.Lichtenwalter, R. N., Lussier, J. T., & Chawla, N. V. (2010). New perspectives methodslink prediction. Proceedings 16th ACM SIGKDD international conferenceKnowledge discovery data mining, pp. 243252. ACM.Nesterov, Y. (1983). method solving convex programming problem convergencerate (1/k2). Soviet Mathematics Doklady, Vol. 27, pp. 372376.Nesterov, Y. (1988). approach construction optimal methods minimizationsmooth convex functions. Ekonomika Mateaticheskie Metody, 24, 509517.Rendle, S., Freudenthaler, C., Gantner, Z., & Schmidt-Thieme, L. (2009). BPR: Bayesianpersonalized ranking implicit feedback. Proceedings Twenty-Fifth Conference Uncertainty Artificial Intelligence, pp. 452461. AUAI Press.Su, X., & Khoshgoftaar, T. M. (2009). survey collaborative filtering techniques. Advances artificial intelligence, 2009, 4.Tseng, P., & Yun, S. (2009). coordinate gradient descent method nonsmooth separableminimization. Mathematical Programming, 117 (1-2), 387423.Van Loan, C. F. (2000). ubiquitous Kronecker product. Journal computationalapplied mathematics, 123 (1), 85100.Williams, C., & Seeger, M. (2001). Using Nystrom method speed kernel machines.Proceedings 14th Annual Conference Neural Information Processing Systems, No. EPFL-CONF-161322, pp. 682688.Yang, L., Hanneke, S., & Carbonell, J. (2013). theory transfer learning applicationsactive learning. Machine learning, 90 (2), 161189.Yang, Y., Liu, H., Carbonell, J., & Ma, W. (2015). Concept graph learning educationaldata. Proceedings Eighth ACM International Conference Web SearchData Mining, pp. 159168. ACM.Zhang, J., Ghahramani, Z., & Yang, Y. (2008). Flexible latent variable models multi-tasklearning. Machine Learning, 73 (3), 221242.Zhang, T., & Ando, R. (2006). Analysis spectral kernel design based semi-supervisedlearning. Advances neural information processing systems, 18, 1601.Zhu, X. (2005). Semi-supervised learning literature survey. Tech. rep. TR 1530, UniversityWisconsin - Madison.Zhu, X., Ghahramani, Z., & Lafferty, J. (2003). Semi-supervised learning using Gaussianfields harmonic functions. Proceedings ICML-03, Vol. 3, pp. 912919.1090fiJournal Artificial Intelligence Research 55 (2016) 443-497Submitted 11/14; published 02/16Optimally Solving Dec-POMDPs Continuous-State MDPsJilles Steeve Dibangoyejilles-steeve.dibangoye@insa-lyon.frUniv de LyonINSA-Lyon, CITI-Inria, F-69621, FranceChristopher Amatocamato@cs.unh.eduUniversity New HampshireDurham, NH, USAOlivier BuffetFrancois Charpilletolivier.buffet@inria.frfrancois.charpillet@inria.frInria Universite de Lorraine CNRSVillers-les-Nancy, F-54600, FranceAbstractDecentralized partially observable Markov decision processes (Dec-POMDPs) provide general model decision-making uncertainty decentralized settings, difficult solveoptimally (NEXP-Complete). new way solving problems, introduce ideatransforming Dec-POMDP continuous-state deterministic MDP piecewise-linearconvex value function. approach makes use fact planning accomplishedcentralized offline manner, execution still decentralized. new Dec-POMDP formulation, call occupancy MDP, allows powerful POMDP continuous-state MDPmethods used first time. provide scalability, refine approach combining heuristic search compact representations exploit structure present multi-agentdomains, without losing ability converge optimal solution. particular, introducefeature-based heuristic search value iteration (FB-HSVI) algorithm relies feature-basedcompact representations, point-based updates efficient action selection. theoretical analysisdemonstrates FB-HSVI terminates finite time optimal solution. include extensive empirical analysis using well-known benchmarks, thereby demonstrating approachprovides significant scalability improvements compared state art.1. IntroductionMany significant real-world problems involve decision making sequential multiagent environments. Examples include: exploration robots must coordinate perform experimentsunknown planet (Zilberstein, Washington, Bernstein, & Mouaddib, 2002); rescue robots that,disaster, must safely find victims quickly possible (Paquet, Chaib-draa, Dallaire, & Bergeron, 2010); optimized distributed congestion control noisy computer network (Winstein &Balakrishnan, 2013); sensor networks multiple sensors work jointly perform large-scalesensing task strict power constraints (Jain, Taylor, Tambe, & Yokoo, 2009); robot logisticsproblems communication limitations sensor uncertainty (Amato, Konidaris, Anders, Cruz,How, & Kaelbling, 2015). tasks require multiple decision makers, agents, coordinateactions order achieve common long-term goals. Additionally, uncertainty ubiquitousdomains, effects actions information received agents.c2016AI Access Foundation. rights reserved.fiDibangoye, Amato, Buffet & CharpilletMarkov decision processes (MDPs) address uncertainty system dynamics, assume centralization. Standard methods solving MDPs, e.g., linear dynamic programming (Bellman,1957; Puterman, 1994) heuristic search (Barto, Bradtke, & Singh, 1995; Hansen & Zilberstein, 2001), centralized planning execution phases. Partially observableMarkov decision processes (POMDPs) extend MDPs situations uncertaintystate system (Kaelbling, Littman, & Cassandra, 1998; Smith & Simmons, 2006; Shani,Pineau, & Kaplow, 2013), similarly assume centralized planning execution.use MDP POMDP models multiple agents present, centralized coordinator agent must global view underlying state (or belief state case POMDP)entire system, plan behalf teammates. Every time step, agent would transmit appropriate action agent must perform observe resulting state (orobservations agent POMDP). methods, collectively referred centralizedplanning centralized control, assume agents communicate step delay cost,either explicitly messages implicitly observations. Unfortunately, many practical applications, agents permitted share information delay cost; rather,agent possesses local, unshared observations, acts without full knowledgeothers observe plan do. characteristics led development rich bodyresearch decentralized decision-making uncertainty.decentralized partially observable Markov decision process (Dec-POMDP) standardformulation cooperative decision-making sequential settings without instantaneous, freenoiseless communication (Bernstein, Givan, Immerman, & Zilberstein, 2002). pastdecade, extensive research solution methods Dec-POMDPs, using methodsdynamic programming (Hansen, Bernstein, & Zilberstein, 2004; Boularias & Chaib-draa,2008; Amato, Dibangoye, & Zilberstein, 2009), optimization (Aras & Dutech, 2010; Amato, Bernstein, & Zilberstein, 2010) heuristic search (Szer, Charpillet, & Zilberstein, 2005; Oliehoek,Spaan, & Vlassis, 2008; Oliehoek, Spaan, Amato, & Whiteson, 2013). approaches directlysearch optimal solution space possible solutions (or policies) become intractablelarger problems. unexpected given worst-case NEXP complexity finite-horizonDec-POMDPs (Bernstein et al., 2002).key assumption many Dec-POMDP algorithms planning centralized longexecution remains decentralized (Hansen et al., 2004; Boularias & Chaib-draa, 2008; Amatoet al., 2009; Szer et al., 2005; Oliehoek et al., 2008, 2013). is, methods represent centralized planning decentralized control centralized planner generates tuple individualsolutions, one individual solution agent. use Dec-POMDP modelrequire centralized planning (e.g., Velagapudi, Varakantham, Sycara, & Scerri, 2011; Wu, Zilberstein, & Chen, 2011; Banerjee, Lyle, Kraemer, & Yellamraju, 2012), Dec-POMDPscooperative framework common assume centralized planning possible. Unfortunately,current algorithms take full advantage centralized planning assumption.article extends conference paper published IJCAI13 (Dibangoye, Amato, Buffet, &Charpillet, 2013), includes introduction centralized solution method recastsDec-POMDP continuous-state MDP detailed background, new theorems proofs,well concise representations policies value functions. novel method alsoable produce decentralized solutions, leverages work centralized planning methods significantly increase scalability. Furthermore, show optimal value function aforementioned MDP piecewise-linear convex function. form, theory POMDPs444fiOptimally Solving Finite-Horizon Dec-POMDPs(Kaelbling et al., 1998) applies, allowing POMDP algorithms produce optimal solutions DecPOMDPs. wide range POMDP algorithms, demonstrated significant scalability(Shani et al., 2013), applied. extend one heuristic search algorithm (Smith &Simmons, 2004) Dec-POMDP case, number states actions MDPgrows exponentially planning horizon, scalability remains limited.increase scalability, introduce novel mechanism refines centralized solutionmethodology present ways combine classical heuristic search compact representations,without losing ability converge optimal solution. incorporate compact representations, build feature-based dynamic programming (Tsitsiklis & van Roy, 1996), includesfeature extraction value prediction approximation methods. introduce feature-basedheuristic search value iteration (FB-HSVI) algorithm relies compact representations, pointbased updates efficient action selection. theoretical analysis demonstrates FB-HSVI terminates finite time optimal solution. combination POMDP theory compactrepresentations greatly reduce problem size solution efficiency retaining optimalsolutions Dec-POMDPs.Action2Observation2Observation2oa,z ( s)startAction2Observation2oa,z ( s)Statera (s)oa,z ( s)Statepa (s, s)ra (s)Agent 2s WorldStatepa (s, s)Hiddenoa,z ( s)oa,z ( s)Observation1Timeoa,z ( s)Observation1Agent 1s WorldObservation1Action1Action1t+1t+2Figure 1: graphical model two-agent Dec-POMDP model.445fiDibangoye, Amato, Buffet & Charpillet2. Background Dec-POMDPssection introduces basic components decentralized partially observable Markov decisionprocesses (Dec-POMDPs).2.1 Problem Definition NotationsConsider multiple agents faced task influencing stochastic system evolvestime (see two agent case Figure 1). every time step, agent receives privateobservation gives (possibly) incomplete noisy information current statesystem. Since states observable, agent cannot choose actions based states.Instead, consider complete history past actions observations choose action.Actions produce common immediate reward, system evolves new state nexttime step according probability distribution conditioned actions. next time step,agents face similar problem again, system may different state. goalagents choose actions based local action-observation sequences causesystem perform optimally respect shared performance criterion (which discussbelow). Dec-POMDP model formalizes interactions agents system.paper formulates solves general decentralized stochastic control problem processoperates finite planning horizon.Definition 1. Dec-POMDP represented tuple (I, , A, Z, p, o, r, b0 , ), where:finite set agents {1, 2, . . . , |I|};finite set n states;Ai finite set agent actions; Ai finite set joint actions;Z finite set agent observations; Z Z finite set joint observations;p = {pa | A} denotes transition model. pa n n stochastic matrix, pa (s, s)probability transitioning state agents choose joint action state s;= {oa,z | A, z Z} observation model. oa,z n 1 vector1 , oa,z ( s)probability observing z joint action performed resulting state s;r = {ra | A} reward function; ra 1 n reward vector, ra (s) boundedreward obtained executing joint action state s;b0 initial probability distribution states;number decision steps {0, 1, . . . , 1} (the problem horizon).defRemark 1. often use shorthand notation pa,z (s, s) = oa,z ( s)pa (s, s), s, , A,z Z, combining transition observation models. is, probability transitioningstate observing z joint action performed resulting state s.1. observation vector stochastic vector.446fiOptimally Solving Finite-Horizon Dec-POMDPsmake representation concrete, discuss simple Dec-POMDP, namelymulti-agent tiger problem (Nair, Tambe, Yokoo, Pynadath, & Marsella, 2003). revisitproblem later sections clarify ideas presented paper.Example 1 (Multi-agent tiger problem description). multi-agent tiger problem, two agentsstand front two closed doors. Behind one doors hungry tiger, behinddoor valuable treasure. agents know position either. listening,rather simply opening one doors, agents gain information positiontiger. listening cost entirely accurate (i.e., reveals correctinformation location tiger probability). Moreover, agents cannot communicate observations other. step, agent independently either listenopen one doors. one agents opens door treasure behind (whileagent listens also opens correct door), get reward. either agent opensdoor tiger, large penalty incurred. However, open tiger doortime, receive smaller penalty. agents must make decisions listeningopening doors based local observations. door opened agents receivereward penalty, problem starts tiger randomly repositioned.refer state multi-agent tiger world tiger left stl (tiger left)right str (tiger right). actions agent aol (open left), aor(open right), al (listen). two possible observations agent (evenopening door): hear tiger left zhl (hear left) hear tiger right zhr (hearright). reward function defined shown Table 1.actionsagentslistensopens good dooropens bad doorlistens2+9101opensgood door+9+20100opensbad door10110050Table 1: Reward function definition multi-agent tiger problemtransition observation models described detail follows. joint action(al , al ) change state world. joint action causes transition statestl probability 0.5 state str probability 0.5 essentially resetting problem.world state stl , joint action (al , al ) results observation zhl either agentprobability 0.85 observation zhr probability 0.15; conversely state str . matterstate world in, joint actions result either observation probability 0.5.example illustrates small Dec-POMDP. Even small Dec-POMDP, coordinationdifficult due uncertainty location tiger actions agent.2.2 PreliminariesGiven -step Dec-POMDP M, would like agents act way maximizecommon measure long-term return M. challenge Dec-POMDPs agentsstrategy typically must take agents strategies account. end, discuss agent447fiDibangoye, Amato, Buffet & Charpilletteam decision rules policies allow agents act based local information,attempting maximize joint objective.2.2.1 Private Decision Rules Policiesevery time step, agent chooses action executed based actions agentpreviously executed observations received. called policy. betterunderstand concept, introduce notions private histories decision rules.Definition 2. step-t private history agent length-t sequence actions observadeftions, ti = (ai0 , zi1 , . . . , zit1 , ait1 , zit ), ait zit denote actions observations agenttime step {0, 1, . . . , 1}., ai , zi ) initial privatestep-t private history ti follows recursion ti = (t1t1history 0 empty. Let set step-t private histories agent I, namely step-tprivate history set. private policy specifies private decision rules agent use time steps,one private decision rule time step.Definition 3. step-t private decision rule dti : 7 Ai prescribes agent private actionexecuted private history ti specified time step {0, 1, . . . , 1}.denote Dit set step-t private decision rules agent time step{0, 1, . . . , 1}, namely step-t private decision rule set agent I. Decision rules mayrandomized deterministic. Dec-POMDPs, MDPs, always exists deterministicdecision rule good randomized decision rule (Oliehoek et al., 2008; Puterman,1994). reason, focus deterministic (private) decision rules. Hence, private policiesprovide agent mapping action selection possible private history.def) sequence private decisionDefinition 4. (t + 1)-step private policy it:t+t = (dti , . . . , dt+trules agent time step time step + , {0, 1, . . . , 1} N.Example 2 (Multi-agent tiger private decision rule policy descriptions). Figure 2 showspair 2-step private policies 10:1 20:1 trees, agent 1 2, respectively.Agent 1Agent 2agent 2s decision rule d02 depends upon 02aolalzhlzhrzhlzhragent 2s decision rule d12 depends upon 12alaolaoraorFigure 2: pair private policies form trees decomposed decision rules twosteps multi-agent tiger problem.448fiOptimally Solving Finite-Horizon Dec-POMDPsagent 2 example, step-2 private policy 20:1 consists two private decision rules d02time steps = 0 = 1, respectively. step-0 private decision rule d02 maps emptyprivate history 02 = private action al . addition, step-1 private decision rule d12 mapsprivate histories (aol , zhl ) (aol , zhr ) private action aor cases. worth noticingprivate decision rules maintain private histories reachable past actions executedobservations received.d12 ,far, focused information agent execution phase including: privatehistories, decision rules policies. Nevertheless, goal Dec-POMDP planning findseparable joint policy. reason, next discuss joint histories, decision rulespolicies.2.2.2 Separable Joint Decision Rules Policiessection, extend private information available given agent, e.g., private histories,decision rules policies, joint information consists collection private data. Letjoint histories , separable joint decision rules dt separable joint policies t:t |I|-tuplesprivate histories (t1 , t2 , . . . , t|I| ), decision rules (dt1 , dt2 , . . . , dt|I| ) policies (1t:t , 2t:t , . . . , |I|t:t ),respectively time steps {0, 1, . . . , 1} N. Note conceptsseparable sense expressed |I|-tuple using private information, one privateconcept agent.Example 3 (Multi-agent tiger separable joint decision rule joint policy descriptions). Figure3 depicts 2-step separable joint policy 0:1 (10:1 , 20:1 ) tuple private policies, oneagent {1, 2}. group together private decision rules agents given time step,separable joint decision rule. example, initial time step = 0, tuple privatedecision rules d0 (d01 , d02 ) separable joint decision rule. addition, separable joint decisionrule d0 prescribes agents 1 2 actions al aol , respectively.Agent 1Agent 2separable joint decision rule d0 (d01 , d02 ) depends 0 (01 , 02 )aolalzhlzhrzhlzhrseparable joint decision rule d1 (d11 , d12 ) depends 1 (11 , 12 )alaolaoraorFigure 3: 2-step separable joint policy multi-agent tiger problem.2.3 Acting Optimallysection, discuss criterion used throughout paper compute optimal separablejoint policy starting initial belief-state. proceeding further, first cast DecPOMDPs MDP, namely information-state MDP.449fiDibangoye, Amato, Buffet & Charpillet2.3.1 Information-state Markov Decision Processesmentioned above, common assumption solving Dec-POMDPs planning takes placecentralized (offline) manner even though agents execute actions decentralized fashion (online).planning paradigm, centralized algorithm maintains, step, total availableinformation process controlled. centralized algorithm essentially performspolicy search space separable joint policies. Thus, separable joint decision rule choicesbased exhaustive information available centralized algorithm statisticsderived information. illustrated influence diagram Figure 4,separable joint decision rule time step depends previous separable joint decision rulesinitial belief state, hidden states. statistics summarizing exhaustive informationavailable centralized algorithm called information states (Hauskrecht, 2000). definedbelow, complete information states represent trivial case, i.e., exhaustive data.start0t+1d0dtdt+1r0rtrt+1Figure 4: Influence diagram information-state MDP. Information states (t t+1 ) represented cycles. Joint-decision-rule choices (dt dt+1 ) represented rectangles,depend current information state, underlying hidden states. Diamonds represent expected immediate rewards r0 , . . . , rt rt+1 . Dashed lines representindirect influence time.defDefinition 5. Dec-POMDPs, step-t complete information state Ct = (b0 , 0:t1 ) length-tsequence separable joint decision rules starting initial belief state b0 , time steps{0, 1, . . . , 1}. satisfies recursion: C0 = (b0 ), Ct+1 = (Ct , dt ).Example 4 (Multi-agent tiger complete information state description). Figure 5 depicts step-2complete information state sequence separable joint decision rules 2 (b0 , d0 , d1 ) startinginitial belief state b0 . Alternatively, complete information state consists separable jointpolicy represented separable joint policy tree together initial distribution b0 .worth noting that, complete information state, private history agent occursone joint history. interdependence joint histories makes Dec-POMDPssignificantly different centralized problems (e.g., MDPs POMDPs) since policies mustremain decentralized. interdependence also explains joint histories, sufficientoptimally planning MDPs POMDPs, longer sufficient optimally planningDec-POMDPs. Instead, rely complete information states.450fiOptimally Solving Finite-Horizon Dec-POMDPsb0separable joint decision rule d0al , aolzhl , zhlzhr , zhrzhr , zhlzhl , zhrseparable joint decision rule d1al , aoraol , aoraol , aoral , aorFigure 5: step-2 complete information state C2 (b0 , d0 , d1 ) multi-agent tiger problem.Separable joint decision rules groups private decision rules depicted Figure 3.need retain complete information states; instead, one rely compact information states. Recall information states quantities summarizing completeinformation states. collection random variables {t : {0, 1, . . . , }} taking valuesinformation state space defines information-state Markov decision process (-MDP).MDP, states information states actions separable joint decision rules illustratedinfluence diagram Figure 4. -MDP deterministic since next-step informationstate t+1 deterministic function previous information state joint-decision-rulechoice dt i.e., t+1 = P(t , dt ). Furthermore, taking separable joint decision rule dtinformation state , expected reward R(t , dt ).Definition 6. -MDP (S, A, P, R, 0 , ) w.r.t. Dec-POMDP given by:information-state set, defines set information states , every timestep {0, 1, . . . , 1};set separable joint decision rules, defines set separable jointdecision rules dt , every time step {0, 1, . . . , 1};P specifies next-step information state t+1 = P(t , dt ) taking separable joint decisionrule dt information state , every time step {0, 1, . . . , 1};PR specifies immediate expected reward R(t , dt ) = s, Pr(s, |t ) rdt () (s) gainedexecuting separable joint decision rule dt information state , every time step{0, 1, . . . , 1};0 initial information state; problems temporal horizon.451fiDibangoye, Amato, Buffet & Charpilletinformation-state MDP differs original Dec-POMDP statespace implicit. is, information-state space much large generatedstored memory. Instead, information states generated explored statespace search, typically discarded thereafter. Generating separable joint decision rulescorresponding expected rewards also sources complexity current methods discussed later. methods build upon assumption one always convert originalDec-POMDP information-state MDP using complete information states without losingoptimality (Szer et al., 2005; Oliehoek et al., 2008, 2013). Below, provide formal proofproperty sake completeness.Lemma 1. optimal separable joint policy complete-information-state MDPoptimal separable joint policy original Dec-POMDP M.Proof. demonstrating proof, need show optimal joint policiesseparable joint policies expected value. Throughout proof, use notationsdenote -steps separable joint policies Pb0 , () denote joint probability distributionparameter b0 . proof starts definition optimal separable joint policyinformation-state MDP M:def= arg max1XR(Ct , dt ).t=0Next, replace R(Ct , dt ) immediate expectation rewards received taking joint (separable) decision rule dt complete information state Ct := arg max1Xt=0nE(st ,t )Pb0 ,0:t () rdt (t ) (st ) ,(Def. R(Ct , dt )).following holds sum expectations equal expectation sums:1Xdef= arg max E(s0 ,a0 ,...,sT 1 ,aT 1 )Pb0 , ()r(s)= .t=0Hence, sufficient search optimal separable joint policy using find optimalseparable joint policy (and vice versa).lemma allows us interchangeably use either complete-information-state MDPsoriginal Dec-POMDP counterpart loss optimality.2.3.2 Optimality Criterionpaper, consider finite-horizon Dec-POMDP (and therefore finite-horizon -MDPM), optimality criterion find separable joint policy maximizes expected sumrewards planning horizon starting given belief state. find optimal separablejoint policy, first characterize expected value gained executing arbitrary separable joint policy t:T 1 starting arbitrary step-t information state. characterizationrepresents Dec-POMDP value function using -MDP notation.452fiOptimally Solving Finite-Horizon Dec-POMDPsDefinition 7. Let t:T 1 separable joint policy respect M. value function V M,t:T 1denotes expected cumulative reward obtained team agents executes t:T 1 timedef PT t1R(t+k , dt+k ),step onward. arbitrary information state , V M,t:T 1 (t ) = k=0t+k+1 = (t , dt , . . . , dt+k ), {0, 1, . . . , 1} k {0, 1, . . . , 1}.Example 5 (Multi-agent tiger expected values given separable joint policy informationstate). Figure 6 depicts mapping step-1 private histories private policies 11:T 1 21:T 1agents 1 2, respectively. private histories result agents taking one actionreceiving observation, i.e., agent 1 took action al received either observation zhl zhr .mapping ensures decentralized control since private histories map private policies. example,agent 1s private history (al , zhl ) maps private policy x. However, expected value oneagents private policy depends agents private policies. reason, relyjoint histories induced information state 1 illustrated Figure 7. Figure 7 depicts mappingjoint histories future separable joint policies. joint history pair private historiesFigure 6, one agent. example, joint history {(al , aol ), (zhl , zhl )} maps futureseparable joint policy (x, ) separable joint policy tree. contribution separable jointpolicy (x, ) expected value depends probability joint history {(al , aol ), (zhl , zhl )}initial belief.alaolprivate histories generatedd01 d02zhlzhrzhlzhr????xxfuture separable joint policy1:T 1 = (11:T 1 , 21:T 1 )Figure 6: Mappings private histories future private policies agent.Value function V M,t:T 1 satisfies following recursion:V ,t:T 1 (t ) = R(t , dt )+V M,t+1:T 1 (P(t , dt ))V M,t+1:T 1 (P(t , dt )) describes future value executing separable joint policy t+1:T 1time step + 1 onward starting information state t+1 = P(t , dt ).2.3.3 Bellmans Optimality Equationsstandard definitions optimality equations -step -MDP follow. first describeoptimal value given information state highest value separable joint policyinformation state. Let t:T 1 set separable joint policies respect M.( ) information state V ( ) def{0, 1, . . . , 1}, optimal step-t value function V M,t=M,t453fiDibangoye, Amato, Buffet & Charpilletb0al , aoljoint historiesinduced 1 = (b0 , d0 )zhl , zhlzhr , zhrzhr , zhlzhl , zhr?, ??, ??, ??, ?xxxxfuture separable jointpolicy 1:T 1Figure 7: Mappings joint histories future separable joint policies.0d01|A0 |d01|A1 |2222|A2 |Figure 8: information-state search tree search nodes information states arcssearch tree labeled separable joint decision rules.454fiOptimally Solving Finite-Horizon Dec-POMDPsmaxt:T 1 V M, (t ). optimality equations (or Bellmans optimality equations, see Bellman,1957; Puterman, 1994) are:def(P(t , dt )) ,St , {0, 1, . . . , 1},(1)V M,t(t ) = max R(t , dt ) + V M,t+1dtdef() = 0 time step = .added boundary condition V M,TNote optimal step-t value function written terms optimal step-(t + 1) valuefunction. recursion implies efficient procedure computing step-t value functionsdiscuss below. Moreover, optimal separable joint policy directly extracted)optimal value functions. Suppose (V M,tt{0,1,...,T 1} solutions optimality equations (1)subject boundary condition, clear optimal separable joint policy 0:T 1 =(dt )t{0,1,...,T 1} satisfies:(P(t , dt )) ,{0, 1, . . . , 1}.(2)dt arg maxdt R(t , dt ) + V M,t+1property implies optimal separable joint policy found first solving optimality equations, time step choosing separable joint decision rule attainsmaximum right hand side (2) {0, 1, . . . , 1}.2.4 Optimally Solving Dec-POMDPsprovide overview dynamic programming heuristic search principles exact methodsbuild upon. thorough introduction solution methods Dec-POMDPs, reader refersurveys (e.g., Oliehoek, 2012; Amato, Chowdhary, Geramifard, Ure, & Kochenderfer, 2013).Notice, however, dynamic programming methods explicitly consider informationstates (instead considering value functions underlying system states). construct separable joint policies last step horizon first evaluating possible separablejoint policies step pruning provably lower value full statespace (Hansen et al., 2004; Boularias & Chaib-draa, 2008; Amato et al., 2009). Heuristic searchtechniques implicitly use complete information states developing Dec-POMDP solution methods(Szer et al., 2005; Oliehoek et al., 2008; Oliehoek, Whiteson, & Spaan, 2009; Spaan, Oliehoek, &Amato, 2011), explicitly use C -MDP representation.2.4.1 Dynamic Programming MethodsOne class Dec-POMDP solution methods based dynamic programming (Howard, 1960).Here, set -step separable joint policies generated bottom (Hansen et al., 2004).step, step-t separable joint policies generated build separable joint policiesstep t+1. separable joint policy lower value separable joint policystates possible separable joint policies agents pruned (with linearprogramming). generation pruning steps continue desired horizon reachedseparable joint policy highest value initial state chosen. Given numberseparable joint policies grows doubly exponentially every generation step, importancepruning away unnecessary separable joint policies crucial. efficient dynamic programmingmethods developed, reducing number separable joint policies generated (Amatoet al., 2009) compressing separable joint policy representations (Boularias & Chaib-draa, 2008).455fiDibangoye, Amato, Buffet & Charpillet2.4.2 Heuristic Search MethodsAnother class Dec-POMDP solution methods based heuristic search techniques. Unlike dynamic programming methods, heuristic search algorithms take advantage initial completeinformation state. Separable joint policies built top using centralized heuristicsearch methods search tree shown Figure 8 (Szer et al., 2005).case, search node complete information state given horizon, t. completeinformation states evaluated horizon heuristic value added.resulting heuristic values over-estimates true value, allowing A*-style searchspace possible complete information states, expanding promising search nodes horizon+ 1 horizon t. principle, A*-style search methods find optimal separablejoint policy, practice doubly exponential growth search tree makes difficult. Recentwork included clustering probabilistically equivalent complete information states (Boularias &Chaib-draa, 2008) histories (Oliehoek et al., 2009), incrementally expanding nodessearch tree (Spaan et al., 2011; Oliehoek et al., 2013), greatly improving scalability originalalgorithms.2.4.3 Limitations Current Methodscurrent methods attempt reduce number separable joint policies informationstates considered, rely explicit representations consider possible joint histories (eventhough many may unreachable). Moreover, existing techniques fail generalize valuefunctions one information state information states, slows convergenceoptimal joint policy. Finally, even though solution methods use offline centralizedplanning phase, concise representation information state identified (until now)allows greater scalability. Simultaneous work one exception developed concise representations based observation histories, show value function resultingMDP piecewise linear convex (Oliehoek, 2013). able show piecewise linearconvex property develop novel algorithm exploit resulting structure. so,draw inspiration advances MDP POMDP algorithms discussed below.Significant progress made solving large MDPs POMDPs. One reasonprogress MDPs use approximate dynamic programming function approximation (Tsitsiklis & van Roy, 1996; De Farias & Van Roy, 2003; Powell, 2007) representstate system value functions concisely. POMDPs, efficient algorithmsdeveloped recasting problems belief MDPs utilize probability distributions statessystem, namely belief states (Smallwood & Sondik, 1973). belief MDP continuousstate MDP piecewise linear convex value function, allowing algorithms scale largeproblems sometimes retaining performance bounds (Smith & Simmons, 2004; Shani et al.,2013). take advantage advances recasting Dec-POMDP continuous-stateMDP piecewise linear convex optimal value function. resulting formulation opensdoor direct application POMDP methods opens research directions utilizing DecPOMDP structure centralized planning representations. discuss formulationprogress using structure remaining sections.456fiOptimally Solving Finite-Horizon Dec-POMDPs3. Solving Dec-POMDPs Continuous-State MDPscontribution section threefold. Section 3.1 introduces statistic (i.e., occupancystate) summarizes information state. Section 3.1.4 demonstrates occupancy statessufficient optimally solving Dec-POMDPs, i.e., occupancy states sufficient statistics. Occupancy states allow transforming information-state MDPs occupancy-state MDPs. Section 3.1.6 establishes fundamental property resulting MDP, namely piecewise linearity convexity optimal value function occupancy states. Remembermethods assume centralized offline planning actions separable joint decision rules ensure decentralized execution. contributions enable application vast collectionMDP POMDP solution methods Dec-POMDP problems. Finally, Section 3.2 introducesoccupancy-based heuristic search value iteration (OHSVI) algorithm solving occupancy-stateMDPs builds upon HSVI algorithm POMDPs (Smith, 2007).3.1 Summarizing Complete Information Statesproviding formal definition occupancy states, start brief motivation. Then,demonstrate occupancy state induces deterministic process Markov, namelyoccupancy-state Markov decision process. Finally, prove occupancy state sufficientstatistic optimal decision-making Dec-POMDPs.3.1.1 Occupancy Statediscussed Section 2.4.2, standard heuristic search methods solving Dec-POMDPs relycomplete information states (Szer et al., 2005; Oliehoek et al., 2008, 2009; Spaan et al., 2011).complete information states preserve ability find optimal separable joint policy (seeLemma 1), heuristic search methods using solve small toy problems. One reasonpoor behavior complete information states result redundant useless computations.particular, every time estimate immediate rewards R(C , d) entire multivariate probability distribution Pr(s, |C ) needs computed states joint histories (see Definition6). operation time-consuming involves exponentially many joint histories, including unreachable ones. Since operation occurs every time step, important reducetime required. end, introduce statistic called occupancy state maintainplace complete information state.Definition 8. step-t occupancy state, denoted , defined posterior probability distribudeftion state st joint history given complete information state Ct , i.e., (st , ) = Pr(st , |Ct ),{0, 1, . . . , }. denote step-t occupancy simplex, is, set possible step-toccupancy states.Example 6 (Multi-agent tiger complete information states occupancy states). Figure 9depicts complete information state (left-hand side) corresponding occupancy state (righthand side) joint histories states system. illustrate occupancy state tree,branches joint histories complete information state leaves state-probabilitypairs. Note initial belief assumed state types.occupancy state represents predictive model state system may endjoint history agents may experience given complete information state. such, occupancy457fiDibangoye, Amato, Buffet & CharpilletComplete Information State C1zhlCorresponding Occupancy State 1alaolal , aolzhrzhlzhrzhl , zhlstl????str.125 .125zhr , zhrstlstr.125 .125zhr , zhlstlstr.125 .125zhl , zhrstlstr.125 .125Figure 9: step-1 occupancy state 1 corresponds complete information state 1 .states, need maintain state joint history pairs reachable given completeinformation state.3.1.2 Markov Propertysection proves occupancy states induce process Markov. words, futureoccupancy states process depend upon present occupancy state next-stepseparable joint decision rule.Theorem 1. Occupancy state t+1 depends current occupancy state separable joint decision rule dt , i.e., arbitrary , A, zt+1 Z ,Xt+1 ( s, (t , , zt+1 )) = 1{at } (dt (t ))(s, ) pat ,zt+1 (s, s),(3)sS1{} () indicator function returns 1 actions chosen dt ,returns 0 otherwise.Proof. demonstrating theorem also derive procedure updating occupancy states.Let step-t information state, decompose = (t1 , dt1 ) i.e., information state t1 prior time-step plus known separable joint decision rule dt1 . Definition8, relate occupancy state information state follows: arbitrary state stjoint history ,def(st , ) = Pr(st , |t ).(4)substitution = (t1 , dt ) (4) yields(st , ) = Pr(st , |t1 , dt1 ).(5)expansion right-hand side (5) states system end time-step 1producesX(st , ) =Pr(st1 , st , |t1 , dt1 ).(6)st1458fiOptimally Solving Finite-Horizon Dec-POMDPsexpansion joint probability (6) product conditional probabilities resultsX(st , ) =Pr(at1 |t1 , dt1 ) Pr(st , zt |st1 , t1 , t1 , dt1 ) Pr(st1 , t1 |t1 , dt1 ).(7)st1first factor denotes joint action at1 separable joint decision rule dt1 prescribest1 . Since assume separable joint decision rules deterministic, Pr(at1 |t1 , dt1 ){0, 1}. fact, Pr(at1 |t1 , dt1 ) = 1 dt1 (t1 ) = at1 , otherwise Pr(at1 |t1 , dt1 ) = 0. So,Pr(at1 |t1 , dt1 ) = 1{at1 } (dt1 (t1 )), 1F indicator function.second factor right-hand side (7) transition probabilityXpat1 ,zt (st1 , st ) Pr(st1 , t1 |t1 , dt1 ).(8)(st , ) = 1{at1 } (dt1 (t1 ))st1last factor defines prior occupancy state t1 state st1 joint history t1 ,depend current separable joint decision rule dt1 . Overall (6) becomesX(st , ) = 1{at1 } (dt1 (t1 ))pat1 ,zt (st1 , st ) t1 (st1 , t1 ).st1Therefore, calculation occupancy state time-step requires occupancystate previous time-step 1 current separable joint decision rule.Equation (3) describes transitions continuous-state MDP states occupancystates actions separable joint decision rules. process, transitions deterministicstate space continuous. Next, formally define process occupancy states induce.3.1.3 Occupancy-State Markov Decision Processesconsider MDP described occupancy states; call occupancy-state Markovdecision process.Definition 9. Let (, A, R, P, b0 , ) occupancy-state Markov decision processrespect Dec-POMDP M, where:= t{0,1,...,T } occupancy simplex, 0 = b0 initial occupancy state;= t{0,1,...,T } separable joint decision rule set;defR : 7 R reward function: reward (t , dt ) R(t , dt ) =defPs, (s, ) rdt () (s);P : 7 transition function: next occupancy state t+1 = P(t , dt ) describedEquation (3) given (t , dt );b0 initial belief state;denotes planning horizon.459fiDibangoye, Amato, Buffet & CharpilletHere, states system represent centralized knowledge planneractions represent separable joint decision rules ensure decentralized execution. (occupancy)state updated using known transition observation functions Dec-POMDPgiven current (occupancy) state chosen separable joint decision rule. rewardsalso calculated (as expectation) using known reward model Dec-POMDP. optimalvalue functions solutions optimality equations:defV M,t(t ) = max R(t , dt ) + V M,t+1(P(t , dt )) , {0, 1, , 1},(9)dtdefadded boundary condition V () = 0 = .M,T)Notice solution (V M,tt{0,1,...,T 1} optimality equations Eq. (9) found,one always retrieve optimal separable joint policy starting initial occupancy state.achieved iteratively retrieving optimal separable joint decision rules decision steps{0, 1, . . . , 1}. decision step t, procedure selects current occupancy state(starting initial occupancy state 0 ). uses max operator retrieve optimal separablejoint decision rule dt current occupancy state :def(P(t , dt )) .(10)dt = arg maxdt R(t , dt ) + V M,t+1Thereafter, moves next occupancy state t+1 = P(t , dt ) makes current one.procedure repeats final decision epoch reached. sequence optimalseparable joint decision rules (d0 , d1 , . . . , dT 1 ) defines optimal separable joint policyoccupancy-state MDP M. Furthermore, Theorem 2 proves optimal separable joint policyoccupancy-state MDP optimal separable joint policy original Dec-POMDP M.Optimally solving continuous-state MDP, occupancy-state MDP, nontrivialtask. general, exact solution method solving general continuous-state MDPs.Methods often rely structural assumptions shape optimal value function (Tsitsiklis & van Roy, 1996; De Farias & Van Roy, 2003; Powell, 2007). next demonstrateuseful structure indeed exist occupancy MDPs form optimal value functionspiecewise linear convex functions occupancy states.3.1.4 Sufficiency Occupancy Statesfirst show occupancy state sufficient statistic optimal decision-making DecPOMDPs. Throughout remainder paper, call statistic sufficient statisticstatistic information state sufficient optimal decision making occupancy-state MDPs.Theorem 2. Occupancy state = Pr(s, |Ct )sS ,t sufficient statistic complete information state Ct , i.e., sufficient optimally solving occupancy-state MDPs. Furthermore,optimal joint policy occupancy-state MDP together correct estimationoccupancy states, also optimal information-state MDP (respectively Dec-POMDP M).Proof. demonstrating sufficiency occupancy state respect correspondinginformation state, need demonstrate (a) optimal value function occupancy stateidentical corresponding information state (b) future occupancy states dependupon current occupancy states (and next-step separable joint decision rule). proved (b)Theorem 1, remains prove statement (a). show induction.460fiOptimally Solving Finite-Horizon Dec-POMDPssufficiency occupancy state respect corresponding information state trivially holds last time step problem. fact, V (CT ) = V (T ) = 0 arbitraryM,TM,Tcomplete information state CT corresponding occupancy state (since horizonreached).assume statement (a) holds time-step + 1, show holds time-stept. arbitrary step-t information state, Bellmans optimality criterion prescribes following:V M,t(Ct ) = max R(t , dt ) + V M,t+1(Ct+1 ),dt(11)(t+1 ), t+1(Ct+1 ) = V M,t+1Ct+1 = (Ct , dt ). induction hypothesis, V M,t+1corresponds occupancy state associated Ct+1 . Hence, Equation (11) becomesV M,t(Ct ) = max R(Ct , dt ) + V M,t+1(t+1 ).dtMoreover, given R(Ct , dt ) =since = (Pr(s, |Ct )) s, :Ps,(12)rdt () (s) Pr(s, |Ct ) = R(t , dt ), following expression holds(t+1 ),(Ct ) = max R(t , dt ) + V M,t+1V M,tdt(13)( ) = maxends proof statement (a) time-step t, since V M,tdt R(t , dt )+V M,t+1 (t+1 ).consequence, statement (a) holds arbitrary complete information state Ct Starbitrary time-step {0, 1, . . . , 1}. Combining statements (a) (b), guaranteedfind optimal value function using occupancy states instead information states.such, optimal joint policy occupancy-state MDP M, together correct estimation occupancy states, also optimal information-state MDP (or original DecPOMDP M). Given optimal value function M, optimal joint policy = (dt )t{0,1,...,T 1}given by:dt = arg maxdt R(t , dt ) + V M,t+1(t+1 ),(14)arbitrary information state , {0, 1, . . . , 1}, t+1 correspond CtCt+1 = (Ct , dt ), respectively.theorem demonstrates optimally solving M, M, guaranteedfind optimal separable joint policy others.3.1.5 Belief States versus Occupancy StatesNote similarity occupancy state Dec-POMDPs belief statePOMDPs. Formally, step-t belief state bt = (P(s|t , b0 )) sS probability distributionstates conditioned step-t history . also sufficient statistic total data availablecentralized agent (i.e., action-observation history) algorithm rely findoptimal solution POMDPs. Similarly, occupancy state sufficient statistic total dataavailable centralized planner (i.e., history separable joint decision rules) algorithmrely find optimal separable joint policy Dec-POMDPs. However, occupancystate remains fundamentally different belief state. First, belief state sufficient461fiDibangoye, Amato, Buffet & Charpilletoptimal decision making Dec-POMDPs (because geared ensure separabilityjoint policy). Second, belief state defines time-invariant statistic, i.e., dimension beliefstates bounded number states. contrast, dimension occupancy states growsexponentially horizon. Also, unlike belief state, occupancy state plantime sufficient statistic used execution time. Instead, agents still conditionactions local action-observation histories Dec-POMDPs. differences make algorithmictheoretic transfers belief-state MDPs occupancy-state MDPs nontrivial.3.1.6 Piecewise-Linearity Convexity Propertypresent one main results paper piecewise-linearity convexityoptimal value function occupancy-state MDP.discussion, use vector (resp. matrix) representation operators R(, dt ) P(, dt ).Vector rdt = (rdt () (s))s, denotes immediate reward executing dt starting statejoint history (i.e., R(t , dt ) inner product rdt occupancy state ).Moreover, operator P(, dt ) transforms step-t occupancy state step-(t + 1) occupancy state,is, P(, dt ) describes transition matrix pdt P(t , dt ) = pdt every step-t occupancystate . linear transformations background, following holds.)Theorem 3. optimal value functions (V M,tt{0,1, ,T } (solutions Equations (9)) piecewiselinear convex functions occupancy states. Hence, {0, 1, , 1}, existsfinite set length-n|t | vector values that, arbitrary occupancy state ,have:V M,t(t ) = max ht , i,(15)ht , denotes inner productP P(s, )(s, ).Proof. show (15) holds induction. Since V (T ) = 0 (since horizonM,T( ) = maxreached), V M,ThT , i, () = 0 = {T }.( ) =Hence, property holds k = . Assume property holds k + 1, is, V M,kkmaxk k hk , k k + 1. want prove property k = i.e.,V M,t(t ) = max R(t , dt ) + V M,t+1 (P(t , dt )) ,.dtUsing linear transformations rdt pdt , following holds:defV M,t(t ) = max R(t , dt ) + V M,t+1 (P(t , dt )) ,dt!dt= max ht , r + max hP(t , dt ), t+1 ,t+1 t+1dtE= max max ht , rdt + pdt , t+1 ,dt t+1 t+1= max max ht , rdt + t+1 (pdt ) .(Inductive Hypothesis)(Rearranging Terms)dt t+1 t+1defFinally, let set length-n|t | vectors = rdt + t+1 (pdt ) separable joint( ) = maxdecision rules dt vectors t+1 t+1 , V M,tht , i. consequence,proof holds every time step {0, 1, . . . , 1}.462fiOptimally Solving Finite-Horizon Dec-POMDPsdemonstrated information states value functions represented vectorspace, occupancy-state space, without loosing optimality. Next, provide approachextending MDP POMDP solution methods occupancy-state Markov decision processes.3.2 Heuristic Search Solution Methodssection presents occupancy-based heuristic search value iteration (OHSVI) algorithmsolving occupancy-state MDPs. algorithm extends heuristic search value iteration (HSVI)algorithm POMDPs (Smith & Simmons, 2004) well heuristic search algorithmsA* (Hart, Nilsson, & Raphael, 1968) LRTA* (Korf, 1990).3.2.1 Heuristic Search Value Iteration Occupancy-State MDPsHSVI state-of-the-art algorithm solving POMDPs (Smith & Simmons, 2004). producessolutions maintaining two-sided bounds optimal value function updatingnumber sample trajectories. upper bounds, (U M,t )t{0,1, ,T } , represented (belief)state-value mappings, lower bounds, (L M,t )t{0,1, ,T } , represented vector sets.trajectory begins initial belief state continues time horizon reached.trajectory finished, upper lower bounds updated belief state, reverse ordervisit. trajectories also interrupted reached belief state upperlower bounds equal, since reason expand belief state whose optimal valueprovably known. Finally, often useful prune lower upper bounds maintain conciserepresentations, removing either dominated vectors points, respectively (Pineau, Gordon, &Thrun, 2006; Smith, 2007).Algorithm 1: OHSVI algorithmfunction OHSVI((L M,t )t{0,1, ,T } , (U M,t )t{0,1, ,T } )Stop(0 ) Explore (0 )function Stop(t )return U M,t (t ) = L M,t (t )function Explore (t )Stop(t )dt arg maxdt R(t , dt ) + U M,t+1 (P(t , dt ))Explore(P(t , dt ))update U M,t L M,tOHSVI (outlined Algorithm 1) operates similar manner described above, remainsfundamentally different HSVI. HSVI generates trajectories belief states OHSVI generates trajectories occupancy states. Also, HSVI generates trajectories (i) picking greedyaction respect upper bound (optimistic exploration), (ii) performing transitioncorresponding largest gap error. Due deterministic nature occupancy-state MDPs,OHSVI need HSVIs gap-based heuristic guide state exploration (Smith, 2007).Instead, OHSVI always executes greedy separable joint decision rule respect upperbounds, selects next occupancy state based greedy separable joint decision rule.463fiDibangoye, Amato, Buffet & CharpilletOHSVI also thought extension learning real-time A* (LRTA*) (Korf, 1990)takes advantage piecewise-linearity convexity optimal value function.V (t ) 0.2 v1t + 0.8 v2t0t2t1t(b) upper bound U M,t(t1 , v1 )(t2 , v2 )M,tM,tV (t ) ht , kt(a) lower bound L M,t= 0.2 t1 + 0.8 t2Figure 10: (a) Lower bounds represented using sets vectors, vectors dashed lines,solid lines represent upper surface vectors (lower-bound value function),circle projection target occupancy onto lower-bound value function.(b) Upper bounds represented using occupancy-value mappings, dashed linesdenotes convex hull formed points.OHSVI relies standard approaches represent lower upper bounds piece-wise linearconvex value functions: vector sets occupancy-value mappings, detail nextsection.3.2.2 Vector Sets : Lower BoundsHSVI algorithms (and depicted Figure 10(a)), lower bound L M,t represented finite collection n|t |-dimensional vectors, every time-step {0, 1, . . . , }(Smith, 2007; Kaelbling et al., 1998; Hauskrecht, 2000; Smallwood & Sondik, 1973). Lowerbounds iteratively updated using point-based backup steps follows: ,[={backup(t+1 , )} ,(16)backup(t+1 , ) = arg maxgt : dt ht , gdtt i,(17)dtgdtt = rdt + arg maxgt+1 :dtt+1 t+1ht , gdtt+1 i,gdtt+1 = t+1 (pdt ) ,(sum vectors Rn|t | )(18)(projection Rn|t+1 | 7 Rn|t | )(19)vector set prior backup vector set backup. Lower bounds(L M,t )t{0,1,...,T 1} initializes (t )t{0,1,...,T 1} single vector () = min sS ,aA (T t) ra (s),{0, 1, . . . , }. Notice vector representation suitable lower bounds.3.2.3 Occupancy-Value Mappings : Upper BoundsUpper bounds (U M,t )t{0,1,...,T 1} represented using mappings occupancy states reals,see e.g., Figure 10(b). upper bound convex hull current point set.464fiOptimally Solving Finite-Horizon Dec-POMDPspossible interpolate value occupancy states whose mapping currently maintainedoutdated. achieved using linear approximation methods (e.g., Hauskrecht, 2000;Smith, 2007). family, sawtooth linear interpolation maps every occupancy statepoint set upper-bound valueU M,t ( ) = min {v , v | ( 7 v ) },v = v + (v v ) D(, ),Xv =(s, ) v s, .(20)(21)(22)s,refer D(, ) = min s, : (s,)>0 (s, )/(s, ) sawtooth measure. update upperbound U M,t specific occupancy state using sawtooth, need compute new value ,add occupancy-value mapping U M,t , follows:[n=( 7 v ) ,(23)v = max R(, d) + U M,t+1 (P(, d)),(24)point set prior update point set update. upper boundU M,t initializes = { s, 7 vts, | } using optimal value underlying MDP cornerpoints, every {0, 1, . . . , }.Clearly, one eventually find optimal solution occupancy-state MDP using OHSVIfull lower upper bounds. However, quickly becomes intractable maintainbounds full occupancy-state space since number state joint-history pairs growshorizon increases. highlights necessity compact representations occupancystates, decision rules vector values.4. Solving Dec-POMDPs Lossless Compact Occupancy-State MDPsprevious sections show every Dec-POMDP represented occupancy-state MDPwithout losing optimality. difficulty using representation common algorithmssolving MDPs piecewise linear convex value functions quickly run time and/ormemory since state action spaces become intractably large real-world problems.surprising given NEXP-Complete worst-case complexity general Dec-POMDPs,realistic Dec-POMDP applications often significant structure.section, discuss optimally solving occupancy-state MDPs potentially reducingdimensionality occupancy states, decision rules value functions. Subsection 4.1,reduce dimensionality occupancy states decision rules constructing clustersequivalent histories. Next, define compact representations occupancy states decisionrules based upon clusters histories rather single histories. resulting compactMDP may exponentially fewer states actions original model, optimal valuefunction compact model may longer piecewise linear convex. Subsection 4.2,overcome limitation, allowing values one compact occupancy state generalizeanother one using parametric value functions. Finally, Subsection 4.3 presents feature-basedheuristic search value iteration algorithm theoretical guarantees.465fiDibangoye, Amato, Buffet & Charpillet4.1 Lossless Compact Occupancy-State MDPsdimension occupancy states decision rules typically grows exponentially horizon. this, often impractical compute store every component occupancystate decision rule representations. overcome limitation using compact representations occupancy states decision rules based notions equivalence histories.notions equivalence fundamental designing analyzing algorithms reducingdimensionality occupancy-state MDP, thereby improving scalability. Equivalence relations permit us aggregate histories convey information process. targetequivalence relations that, upon replacing group aggregated histories one elementgroup, allow us produce compact representations occupancy states decision rulesstill preserving ability find optimal solution.4.1.1 Probabilistic Equivalence History Space Aggregationfirst present equivalence notions build upon defining compact representationsproving preserve optimality. definitions inspired work conciseinformation states (Boularias & Chaib-draa, 2008; Oliehoek et al., 2013). Here, connectresearch occupancy-state MDPs, later provide natural algorithms constructing effectively solving them.Definition 10. Private histories , agent locally probabilistically equivalentrespect occupancy state denoted -LPE if, if, statehistoryagents I\{i}: Pr(s, | , ) = Pr(s, | , ).worth noticing -LPE used partition private history set agent I,time steps {0, 1, . . . , }. partition set nonempty subsets (called clusters)Bi1 , Bi2 , . . . , Bik private histories, Bi1 Bi2 . . . Bik = . distinguish twosets private histories agent occupancy state . first set, denoted (),consists private histories non-zero probability respect . second set, denoted\it (), consists private histories zero probability respect . differenceparticularly important, show later. fact, nonzero private histories play partdemonstrating -LPE preserves optimality. addition, useful note that, agent,-LPE groups together zero private histories w.r.t. cluster.Given private histories clustered convey information, representationscompact occupancy states, decision rules value functions depend uponclusters. Unfortunately, maintaining clusters still requires large amount memory, explains impetus labeled clusters. labeled cluster cluster along label; privatehistories cluster match corresponding label. Throughout paper, use followingconvention: cluster private histories maps minimum private history (of cluster)using lexicographical ordering. Specifically, label cluster chosen among private historiescluster, non-zero probability w.r.t. occupancy state. Therefore, labelcluster also private history cluster, leads clear relationship privatehistory, cluster corresponding label. Thus, representation compact occupancy states,decision rules value functions depend upon labels instead clusters.Nonetheless, compact representations make hard generalize value functionone compact occupancy state another. see later, generalization requires ability466fiOptimally Solving Finite-Horizon Dec-POMDPsquickly check whether given private history belongs specified cluster. group historiesusing -LPE, checking whether private history belongs cluster Bi whose label another privatehistory Bi replaced checking whether private histories -LPE,arbitrary agent I. Unfortunately, checking whether two private histories agent-LPE requires enumerating states agents nonzero histories w.r.t. , resultssubroutine complexity O(n|i()|) see Algorithm 3 Appendix B. Given callprocedure exponentially many private histories, importance replacing local probabilisticequivalence cheaper equivalence relation clear. end, introduce truncation probabilistic equivalence w.r.t. (denoted -TPE). providing formal definition -TPE,start motivation.many practical situations, important information process controlled lieshistory window (of fixed length) actions observations agents experienced.Based insight, want truncation probabilistic equivalence group private historiesagent share: (i) model states agent histories (i.e., -LPE)(ii) common suffix fixed length m. setting, private historiesclustered, checking particular private history belongs cluster replacedchecking whether private history label (another private history) clustershare suffix specified length significantly faster checking whethertwo private histories -LPE: O(m) versus O(n|i()|). is, private historiesclustered, two private histories suffixes length already known LPE though clustering process choice m. turns defining probabilistictruncation equivalence w.r.t. , need determine suffix length (i.e., history window)sufficient called local truncation parameter w.r.t. .local truncation parameter respect occupancy state be: (i) largeenough ensure nonzero private histories w.r.t. share suffix length also-LPE; (ii) small enough group maximum number private histories. straightforwardmethod (Algorithm 4 Appendix B) compute starts parameter = 0 proceedsfollows: (step 1) agent, nonzero private histories w.r.t. share common suffixlength also -LPE one another, set = terminate; (step 2) Otherwise,increment = + 1 go back step 1. algorithm guaranteed terminate(the current time step) iterations, i.e., = worst case. later case correspondsboundary case clustering done i.e., cluster corresponds single joint history.ready formally define truncation probabilistic equivalence relation.Definition 11. Private histories , agent truncation probabilistically equivalentrespect occupancy state denote -TPE if, if, hold lastprivate actions observations given found set histories discussed above.surprisingly, -TPE also partitions private history sets. However, often producesclusters -LPE since constrains -LPE non-zero private histories w.r.t. (since alsorequires suffixes same). contrast -LPE, spreads zero private histories w.r.t.different clusters.Recall advantage -TPE -LPE finding appropriate clusterprivate history (i.e., checking whether two private histories equivalent) cheaper using -TPEusing -LPE. former requires comparison suffix fixed length, whereaslatter needs comparison large distributions states joint histories.467fiDibangoye, Amato, Buffet & Charpilletworth noticing that, best knowledge, TPE LPE weakest assumptionsdate reduce dimensionality full occupancy states. Nevertheless, DecPOMDPs benefit data reduction approaches. precisely, unlikelyassumptions provide concise occupancy states totally unstructured domains. Fortunately,real-world domains often structured, demonstrated Section 5. Next, addressproblem using equivalence relations private histories find compact representationsoccupancy states, decision rules real vectors.4.1.2 Compact States, Actions, Vectors MDPsdefine compact representations occupancy states, decision rules real vectors basedupon aforementioned equivalence relations. Notice following definitions dependeither local truncation probabilistic equivalence relations labeled clustersgenerate. Intuitively, compact occupancy states distributions states joint labels, compact decision rules mappings labels private actions, compact real vectors mappings pairs states joint labels reals. Notationally, let Li label set agentoccupancy state generates (i.e., set labels generated histories make), Bi labeled cluster agent label Li let L = iI Li .Definition 12. compact occupancy state , denoted , distribution states jointlabels: (i )iI L ,XX Xdef(s, 1 , 2 , |I| ).(25)...(s, 1 , 2 , . . . , |I| ) =1 B1 2 B2|I| B|I|Example 7 (Multi-agent tiger full occupancy states compact occupancy states). Figure11 depicts full occupancy state (left-hand side) corresponding compact occupancy state(right-hand side) joint histories hidden states. obtain compact occupancy state,show (al , zhl ) (al , zhr ) 1 -LPE agent green, (al , zhl ) (aol , zhr ) 1 -LPEagent red. consequence, one group histories Bgreen {(al , zhl ), (al , zhr )} agentgreen, Bred {(aol , zhl ), (aol , zhr )} agent red, replace cluster single label.compact decision rule w.r.t. agent I, denoted di : Li 7 Ai , mapping labelset private action set. addition, compact private policy w.r.t. (0 , 1 , . . . , 1 ) agent I,denoted = (di )t{0,1,...,T 1} , sequence compact decision rules agent I. similardefinition follows compact separable joint policies = (i )iI .compact real vector w.r.t. , denoted Rn|L | , mapping pairs statejoint label L reals.Intuitively, distribution reassigns probability mass joint cluster (Bi )iI corresponding joint label (i )iI . Algorithms 3 4 (see Appendix B) present straightforward waycompute compact occupancy state using local truncation probabilistic equivalence relations,respectively. worth noticing that, given occupancy state , -LPE always producesnumber joint labels |L | less equal number labels -TPE produces.-TPE stricter form local probabilistic equivalence, discussed above. Hence,-LPE produces compact occupancy states, decision rules real vectors concise-TPE.468fiOptimally Solving Finite-Horizon Dec-POMDPsFull Occupancy State 1Compact Occupancy State 1al , aolal , aolzhl , zhlstlstr.125 .125zhr , zhrstlstr.125 .125zhr , zhlstlstr.125 .125zhl , zhrstlzhl , zhlstrstl.125 .125str0.5 0.5Figure 11: step-1 compact occupancy state 1 corresponds full occupancy state 1 .demonstrating compact occupancy states sufficient optimal decision makingoccupancy-state MDPs, rely notion compact occupancy-state MDPs.Definition 13. compact occupancy-state MDP w.r.t. occupancy-state MDP given tupleA, R, P, 0 , ):(,= t{0,1,...,T } space compact occupancy states, 0 = 0 initial compactoccupancy state;= t{0,1,...,T } space compact joint (decentralized) decision rules;PR : 7 R compact reward function R(, ) = s, (s, ) rd () (s);defP : 7 compact transition function P(, ) = , = P(, );denotes planning horizon.Analogous occupancy-state MDPs, compact occupancy states updated using knowntransition observation functions Dec-POMDP given current compact occupancy statechosen compact separable joint decision rule. rewards also calculated (as expectations) using known reward model Dec-POMDP. Finally, optimal value functionsolution optimality equations:defV M,t(t ) = max R(t , dt ) + V M,t+1( P(t , dt )) ,{0, 1, . . . , 1},(26)dt() = 0.added boundary condition V M,T4.1.3 Sufficiency Compact Occupancy StatesBelow, prove that, planning compact occupancy states instead full occupancystates, optimal separable joint policy compact model immediately induces corresponding optimal separable joint policy original model. proceeding further, firstdemonstrate optimality compact policies.469fiDibangoye, Amato, Buffet & CharpilletNote proof mirrors similar proof showing histories agent clusteredwithout change optimal policy loss value using probabilistic equivalencetraditional Dec-POMDP representation (Oliehoek et al., 2013). extend ideasoccupancy-state MDP case incorporate truncation probabilistic equivalence showcompact policies preserve optimality.Theorem 4 (Optimality compact policies). occupancy-state MDPs, exists optimal policies agent depend upon labels agent produced based either localtruncation probabilistic equivalence respect occupancy states optimal policies induce,private histories.Proof. construct proof induction. first show last step problem,agent policy depends labels, private histories.Given occupancy state 1 1 policies1 1 agents I\{i}, policy1 1 agent best response1 . is, chooses every private history1 1 private action maximize value based model (Pr(s, Ti1 |Ti 1 , 1 )) s,i1occupancy state 1 private history Ti 1 induce possible histories agentsresulting states system:X(27)Pr(s, Ti1 |Ti 1 , 1 ) ra ,T 1 (T 1 ) (s).1 (Ti 1 ) arg maxai Ais,Ti1assigning private actions private histories given 1 , nonzero private histories w.r.t. 1affect outcome. is, zero probability private histories w.r.t. 1 prescribed private action without losing optimality (e.g., private action identical associated labelsgenerated based either local truncation probabilistic equivalence relations). Hence, policy1 depends zero probability private histories w.r.t. 1 corresponding labels.Next, restrict attention nonzero probability private histories w.r.t. 1 . Recall privatehistories family -TPE one another also -LPE one another. Therefore,demonstrate property private histories family -LPE one another,proof follows immediately -TPE.Assume Ti 1 nonzero probability private history w.r.t. 1 . Ti 1 cluster1 -LPE private histories label 1 (another nonzero probability private history w.r.t. 1 ),equality (Pr(s, Ti1 |Ti 1 , 1 ))s,i = (Pr(s, Ti1 |iT 1 , 1 ))s,i holds. consequence,11policy 1 depends nonzero probability private history w.r.t. 1 correspondinglabels:X(28)Pr(s, Ti1 |iT 1 , 1 ) ra ,T 1(T 1 ) (s).1 (Ti 1 ) arg maxai Ais,Ti1Therefore, property holds last step problem, arbitrary agent I.allows us define policies last step mappings labels private actions, i.e., compactpolicies.Next, rely concept private policy tree, tree represents actionsnodes observations edges depth number stages go. root nodedetermines first private action taken. depending private observation received,agent executes another private action; leaf node reached. policy tree470fiOptimally Solving Finite-Horizon Dec-POMDPsit:T 1 portion private policy it:T 1 , prescribes private actions taken agentremaining stages starting given private history ti . abuse notation, letit:T 1 = it:T 1 (ti ), agents time steps {0, 1, . . . , }.induction step, show that, agent policy-tree step + 1 onward dependsprivate histories corresponding labels either equivalence relations,agents policy tree step onward also depends private historiescorresponding labels either equivalence relations. Given occupancy state policyt:T 1 t:T 1 agents I\{i}, policy t:T 1 t:T 1 agent follows:Xit:T 1 (ti ) arg maxi t:T 1Pr(s, ti |ti , ) t:T 1 ,t:T 1(t ) (s),(29)t:T 1s,tit:T 1 ,t:T 1 (t )associated vector(it:T 1 ,t:T 1 (t )) separable joint policy-treevalue. Again, assigning private actions private histories given , nonzero probabilityprivate histories w.r.t. matter. fact, property trivially holds zero probability privatehistories w.r.t. . nonzero probability private histories w.r.t. , property holds privatehistories belong cluster -TPE private histories since histories would belongcluster -LPE private histories, previously discussed.reason, restrict attention nonzero probability private histories ti w.r.t.belong cluster -TPE private histories label . Then, equality (Pr(s, ti |ti , ))s,ti =(Pr(s, ti |it , )) s,ti holds. Hence,it:T 1 (ti ) arg maxit:T 1 t:T 1XPr(s, ti |it , ) t:T 1 ,t:T 1(t ) (s).(30)s,tiConsequently, agent policy depends private history step problemcorresponding labels either equivalence relations. demonstrates compact policylose information.Theorem 5. Compact occupancy state based either local truncation probabilistic equivalence relations sufficient statistic occupancy state . Furthermore, optimal separablejoint policy compact occupancy-state MDP M, together correct estimation compact occupancy states, immediately induces optimal separable joint policy occupancy-stateMDP M.Proof. proof proceeds similarly Theorem 2. is, proving sufficiencycompact occupancy state respect corresponding full occupancy state, needdemonstrate: (a) optimal value function compact occupancy state identicalcorresponding occupancy state (b) next-step compact occupancy states depend uponcurrent compact occupancy states (and next-step compact separable joint decision rules).stated (b) Definition 13, statement (a) remains proved. show induction.sufficiency compact occupancy state respect corresponding occupancy( ) = V ( ) = 0state trivially holds last step problem. fact, V M,TM,Tarbitrary occupancy state corresponding compact occupancy state (since horizonreached). assume statement (a) holds time-step + 1, show471fiDibangoye, Amato, Buffet & Charpilletholds time-step t. arbitrary step-t occupancy state, Bellmans optimality criterionproduces following equality:defV M,t(t ) = max R(t , dt ) + V M,t+1(t+1 ),(31)dt(t+1 ) = V M,t+1(t+1 ).t+1 = P(t , dt ). inductive hypothesis, V M,t+1results equality:V M,t(t ) = max R(t , dt ) + V M,t+1( P(t , dt )).(32)dtaddition, Theorem 4 demonstrated restricting attention compact (joint) decisionrules A, preserve optimality. Thus, obtain:XXR(t , dt ) =rdt () ( s)s, (s, ) (s, ),(Theorem 4)s,=Xs,rdt ()( s) ( s, ),(Definition )s,= R(t , dt ),(dt () = dt ()).Hence,( P(t , dt )),V M,t(t ) = max R(t , dt ) + V M,t+1(33)dt(t ).= V M,t(34)Therefore, statement (a) holds arbitrary occupancy state arbitrary timestep {0, 1, . . . , 1}. Combining statements (a) (b), guaranteed find optimalvalue function using compact occupancy states instead occupancy states. addition,def)given optimal value function (V M,tt{0,1,...,T } , optimal compact policy = (dt )t{0,1,...,T 1}obtained successive one-step lookaheads: {0, 1, . . . , 1},( P(t , dt )),dt max R(t , dt ) + V M,t+1(35)dtdef0 = 0 t+1 = P(t , dt ). immediately induces separable joint policy =(dt )t{0,1,...,T 1} original occupancy-state MDP that, every agent every private history ti , set dti (ti ) = dti (it ), ti cluster label . Sinceyield expected value starting initial occupancy state, separable joint policyoptimal original occupancy-state MDP M.solving compact problem instead original one, circumvent exhaustive enumeration occupancy states decision rules preserve ability find optimal solutionoriginal problem. worth noticing optimal value function compact occupancy space PWLC. compact occupancy states expressed using differentlabel sets. However, exploiting PWLC property optimal value function fulloccupancy-state space, develop methods generalize value one compact occupancy472fiOptimally Solving Finite-Horizon Dec-POMDPsstate another. Next, propose method incrementally improving lower upper boundsnarrow range optimal value function. But, contrast OHSVI algorithm,using compact occupancy states compact real vectors. precisely, compact upperlower bounds defined full occupancy states, compact representations.4.2 Feature-Based Compact Boundsalgorithm, upper lower bounds crucial importance. narrow rangeoptimal value function, determine suboptimal regions search space, speedconvergence towards exact solution. approach approximates full lower- upper-boundheuristic functions heuristic functions defined full occupancy space (Tsitsiklis &van Roy, 1996; Hauskrecht, 2000; Roy, Gordon, & Thrun, 2005). new heuristic functionstypically compact (with respect traditional high-dimensional vector point sets discussedSection 3.2.2 Section 3.2.3), easier compute full bounds. heuristicfunctions formulated feature-based compact functions combine dimensionalityreduction (using clustering methods discussed above) function approximation (Tsitsiklis &van Roy, 1996). Thus, demonstrate new heuristic functions valid bounds,analyze dimension reduction approximation methods.4.2.1 Feature-Based Compact States VectorsOne think feature-based compact representations dimension reduction model representing high-dimensional bounds using bounds lower dimensionality. However, approachrequires set basis functions (or feature set), lower-dimensional bounds expressed effectively. basis function (or feature) function maps high-dimensional dataone salient feature problem hand. setting example, feature indicatorfunction whether private history matches one specified label. Features corefeature-based compact representations full occupancy states real vectors, ultimatelyserve represent upper lower bounds using either feature-based compact vector point setssimilar way standard vector point set representations.Definition 14. feature indicator function s,(i )iI : 7 {0, 1} one specified stateone specified joint label (i )iI L occupancy state induces i.e.,(i )iI ,(1, = and, I, belongs cluster label ,s,(i )iI ( s, ( )iI ) =0, otherwise.deffeature set w.r.t. , denoted , given = { s, |s , iI Li }.feature set w.r.t. represents partition state joint history space equivalencerelation -LPE -TPE induces. partition set nonempty subsets (B s, ) sS ,L (calledlabeled joint clusters) sS ,L B s, = . labeled joint cluster B s, consistscross-product singleton {s} labeled clusters B1 , B2 , . . . , B|I| equivalencerelation -LPE -TPE generates. Therefore, feature interpreted way check whetherstate joint history pair belongs specified labeled joint cluster. Thus, features provide473fiDibangoye, Amato, Buffet & Charpilletalternative (possibly lossy) representation compact occupancy states. One expresscompact representation full occupancy state using feature set , followsX X=s, (s, ) (s, ).sS ()s,Specifically, state labels Li agent I,def( s, 1 , 2 , . . . , |I| ) =XX...1 B1 2 B2=XX( s, 1 , 2 , |I| ),|I| B|I|def( = (1 , 2 , . . . , |I| ))( s, ),( s,)B s,=X Xs, (s, ) (s, ),sS ()analogous property holds feature-based compact real vectors w.r.t. . is, feature-basedcompact real vector | |-dimensional real vector expressed using .worth noticing standard feature-based approaches assume unique feature set dataexpressed based unique feature set, eases bound generalization (Tsitsiklis &van Roy, 1996). setting, however, different occupancy states yield different (possibly disjoint)feature sets. Hence, feature-based compact occupancy states (or real vectors) expressed baseddifferent feature sets, making hard transfer value one feature-based compact occupancystate another one. Bounds generalize naturally among feature-based compact occupancy statesshare feature set. remedy this, introduce basis change operationsfeature-based compact occupancy states real vectors.4.2.2 Change Feature Setenabling bound generalization, necessary work one feature set. Hence,important able easily transform feature vectors calculated respect one feature setcorresponding (possibly lossy) representations respect another feature set. easechange feature set, necessary match features original feature setdestination feature set. introduce two heuristic methods match features differentfeature sets, thereby allowing change feature sets.Definition 15. Let feature sets w.r.t. occupancy states , respectively.projection feature-based compact occupancy state onto feature set , denoted F (),given probability distribution:XdefF () =,(36)s, (s, ) (s, )s,s,F () reassigns probability mass (s, ) pair (s, ), s, , pair( s, ), s, , match, i.e., s, (s, ) = 1.474fiOptimally Solving Finite-Horizon Dec-POMDPschange feature set describes function original feature set destination featureset . particular, surjective function (i.e., every feature s, destination setleast one corresponding feature s, original set s, (s, ) = 1).transformation assigns feature destination set probability mass correspondingfeatures original set. heuristic method provides guarantee F () shareoptimal value. replacing range features original set single featuredestination set, produce compact possibly lossy representations, ultimately precludesability preserve value original compact occupancy state. Fortunately, since userepresentations provide bounds, even lossy representations generate useful bounds.Definition 16. Let feature sets w.r.t. occupancy states , respectively.projection feature-based compact real vector using feature set , denoted G ( ), givenfeature vector:Xdef(37)s, ( s, ) (s, )G ( ) =s,s,change feature set applies real vectors, describes function destinationfeature set original feature set . Specifically, every pair ( s, ) along feature s,destination set corresponding pair (s, ) along feature s, original seti.e., one s, ( s, ) = 1. transformation assigns pair ( s, )destination set value (s, ) corresponding pair (s, ) original set. Sincepairs original set values represented G ( ), resulting feature vectorlossy representation original vector . loss resulting feature vector dependsoriginal destination feature sets, choice equivalence relation histories.previously mentioned, make use either -LPE -TPE relations.Using -LPE, distinguish state joint-history pairs involve non-zeroprobability private history sets w.r.t. (i.e., non-zero probability pairs), state joint-historypairs involve zero probability private history sets w.r.t. (i.e., zero probability pairs).sake conciseness, compact real vectors maintain values associated non-zero probabilitypairs. zero probability pairs default value, example (T t) min sS ,aA ra (s).Hence, change feature set pairs ( s, ) destination set, corresponding non-zero probability pair (s, ) original set, inherits value (s, ). However,pairs ( s, ) destination set, corresponding zero probability pair (s, ) original set,inherits default (and loose) value. number zero probability pairs occupancystate far larger number non-zero probability pairs, feature vectors resultchange basis via -LPE multiple components loose value.Using -TPE, distinguish state joint-history pairs joint-historysuffixes length see Definition 10. many pairs destination set, wouldassociated corresponding zero probability pair original set using -LPE. Using -TPE, however, pairs associated non-zero probability pair original set.Specifically, pair ( s, ) destination set, whose probability zero respect , corresponding zero probability pair original set using -LPE. Yet, pair ( s, ) destinationset non-zero probability pair (s, ) original set share common length history suffix,( s, ) associated (s, ) using -TPE. Thus, feature vectors result change475fiDibangoye, Amato, Buffet & Charpilletfeature set using -TPE involve fewer components default, loose value using-LPE.Although heuristic methods change feature set guaranteed produce representations retain information compact counterparts, many cases resulting(possibly lossy) representation sufficient generalize bounds entire compact occupancyspace. bound property (i.e., ability overestimate underestimate optimal value)resulting representation determined examining methods change feature set.following theorem (proved Appendix) establishes F G mappings usepreserve bound property.Theorem 6. arbitrary feature set , change feature set based mappings GF preserve bound property i.e., ,(), V (F ());a. R, V M,tM,t() h, G ()i, V () h, G (G ())i.b. R| | , V M,tM,tTheorem 6 shows bound properties given occupancy state preserved occupancy state obtained upon change feature set using heuristic methods F G. Next,discuss approximate full upper lower bounds entire occupancy space.4.2.3 Compact Point-Value Mappings: Upper Boundsfull upper-bound value function approximated finite set points sawtoothinterpolation rule estimates value arbitrary point compact occupancy spacerelying points already experienced associated values. key aspect heuristicapproximation sawtooth interpolation rule compact occupancy states.full upper-bound value function, sawtooth interpolation approximate pointsexpressed within basis set. Here, demonstrate apply even pointsexpressed different feature sets means change feature set. Let = {(1 7 v1 ), (2 7v2 ), . . . , (k 7 vk )} set point-value pairs represents approximate function U definedcompact occupancy space, point satisfies Theorem 6a. approximatevalue arbitrary compact occupancy state based point-value pair ( 7 v ) obtainedusing sawtooth interpolation way similar calculation full upper-boundvalue function:v = v + (v v ) D(F (), ),v =Ps,(38)(s, ) v s, . Theorem 6a, know feature vector F () sharesupper bound v . addition, F () expressed using feature set. Hence sawtooth interpolation apply produce upper-bound value v compactoccupancy state based point-value pair ( 7 v ). optimization respect compactoccupancy state acquired choosing best overall upper-bound value pointvalue pairs :U ( ) = min {v , v | ( 7 v ) }.476(39)fiOptimally Solving Finite-Horizon Dec-POMDPsheuristic approximation differs full upper-bound value function requires change feature set F compact occupancy states instead full occupancy states.Hence, sawtooth interpolation computationally efficient full upperbound value function, compact occupancy states lower dimensionality. accuracy resulting upper-bound values, clear sawtooth interpolation comparesfull upper-bound value function (i.e., whether sawtooth interpolationweakens upper bounds). Yet, collection point-value pairs obtained selection compact occupancy states combined define approximate function upper-bound valuefunction discussed next.feature-based compact value function (U M,t )t{0,1,...,T } compact occupancy spacerepresented using sets (t )t{0,1,...,T } point-value pairs estimate values arbitrarycompact occupancy state. Initially, set contains |S | point-value pairs { s, 7 vts, | }represent step-t optimal value function underlying MDP. sawtooth interpolationestimates U M,t compact occupancy state follows:U M,t ( ) = min {v , v | ( 7 v ) },{0, 1, . . . , }.Set updated every compact occupancy state , using point-based backup stepfollows:[n=( 7 v ) ,{0, 1, . . . , }v = maxd R( , ) + U M,t+1 ( P( , )). Approximate function (U M,t )t{0,1,...,T }upper-bound value function similarly full upper-bound value function statedproven Appendix A.Theorem 7. Feature-based compact value function (U M,t )t{0,1,...,T } , iteratively updated, upperbounds optimal value function entire compact occupancy space.4.2.4 Compact Vector Sets: Lower Boundsfull lower bound full occupancy space approximated finite set compactreal vectors along associated feature sets linear function updates. take inspirationinitialization, evaluation update routines full lower-bound value function. One]important aspect approximation lies definition update operation, denoted backup.Let set compact real vectors represents approximate value function,compact real vector satisfies Theorem 6b. Then, new compact real vector compactoccupancy state compact decision rule computed efficiently full lowerbound value function (Section 3.2.2):gd = rd + arg maxg : , gd ,(40)gd = G P(,d ) () (pd ) expression projection G P(,d ) () onto feature set, G P(,d ) () expression feature set P(,d ) . optimization respectcompact occupancy state acquired choosing compact vector best overall477fiDibangoye, Amato, Buffet & Charpilletvalue vectors gd :] , ) = arg maxbackup(g:, gd .(41)principal difference respect full lower-bound value function lies use transformation G compact real vectors instead high-dimensional real vectors. Hence, backupoperation efficient full lower-bound value function, since operationsuse lower dimensional vectors, likely save significant time.change feature set may produce weaker bounds. Nonetheless, collection compact vectorsobtained selection compact occupancy states combined define approximatefunction lower-bound value function discussed next.feature-based compact value function (L M,t )t{0,1,...,T 1} compact occupancy spacerepresented using sets (t )t{0,1,...,T 1} compact real vectors along associated featuresets estimate values arbitrary compact occupancy state. Initially, set containssingle compact real vector given() = (T t) min ra (s),{0, 1, . . . , }.sS ,aA(42)max-vector rule estimates L M,t compact occupancy follows:L M,t (t ) =max(7t )tht , Gt (t )i,{0, 1, . . . , }.(43)Set updated every compact occupancy state , using point-based backup stepsfollows:[n] t+1 , )) .=(t 7 backup((44)Approximate function (L M,t )t{0,1,...,T 1} lower-bound value function since main differencerespect full lower-bound value function lies use G , preserves boundproperty. complete proof, reader refer Appendix A.Theorem 8. Feature-based compact value function (L M,t )t{0,1,...,T } , lower bounds optimal valuefunction entire compact occupancy space.4.3 Feature-Based Heuristic Search Value Iteration Algorithmsection presents feature-based heuristic search value iteration algorithm (FB-HSVI)iteratively updates feature-based compact lower- upper-bound representations. also discussFB-HSVIs theoretical guarantees.4.3.1 Algorithm DescriptionSimilar OHSVI (Algorithm 1), FB-HSVI (Algorithm 2) solves occupancy-state MDPs generating trajectories occupancy states iteratively updating lower upper bounds,case FB-HSVI, compact occupancy states feature-based compact lower(L M,t )t{0,1,...,T } upper bounds (U M,t )t{0,1,...,T } . FB-HSVI improves scalability OHSVI478fiOptimally Solving Finite-Horizon Dec-POMDPsseveral ways. First, FB-HSVI replaces full exact representations compact representations all:occupancy states, decision rules, lower upper bounds. addition, combines stopping criteriaHSVI (Smith, 2007) optimal classical heuristic search methods (e.g., Hart et al., 1968;Korf, 1990), may result efficient pruning unnecessary subspaces.Algorithm 2: FB-HSVI Algorithm.function FB-HSVI((L M,t )t{0,1, ,T } , (U M,t )t{0,1, ,T } )initialize L M,t U M,t {0, , 1}.Stop (0 , 0) Explore(0, 0)function Explore(t, gt )Stop (t , gt )dt arg maxdt R(t , dt ) + U M,t+1 ( P(t , dt ))Update U M,tExplore( P(t , dt ), R(t , dt ) + gt )Update L M,treturn gtfunction Stop(t , gt )return U M,t (t ) L M,t (t ) L M,0 (0 ) gt + U M,t (t )FB-HSVI differs OHSVI four main ways:1. compact representation occupancy states, significantly reduces search space;2. compact representation decision rules, speeds decision-rule selection;3. compact representation lower upper bounds, speeds convergence;4. enhanced value function generalization combination stopping criteria, resultsefficient pruning unnecessary subspaces.4.3.2 Stopping Criteriastopping criteria FB-HSVI build upon optimal classical heuristic search methods(e.g., Hart et al., 1968; Korf, 1990; Smith, 2007). determine stop current trajectorycompact occupancy states algorithm. Ideally, optimal criterion would measure distance current trajectory optimal trajectory, known. Instead, use twocriteria based upper lower bound values trajectories compact occupancy states.upper bound current trajectory, denote f (t ), sum two functions: (1)past trajectory-reward function g(0 , ), sum rewards starting compact occupancy state 0 current occupancy state ; (2) future trajectory-rewardcompact occupancy state , admissible heuristic estimate, e.g., upper-bound U M,t (t )compact occupancy state .first criterion relies fact reason expand occupancy statef (t ) less equal L M,0 (0 ), since cannot lead solution better current bestsolution; criterion previously used optimal classical heuristic search methods (e.g., Hart et al.,1968; Korf, 1990).479fiDibangoye, Amato, Buffet & CharpilletCriterion 1. trajectory occupancy states (0 , . . . , ) interrupted whenever heuristic valuef (0 ) less equal L M,0 (0 ) i.e., L M,0 (0 ) f (0 ). best solution found faroptimal expanded occupancy state frontier search space2heuristic-value f (t ) higher L M,0 (0 ).second criterion builds upon fact reason expand occupancy stateupper bound less equal lower bound (Smith, 2007).Criterion 2. trajectory occupancy states (0 , . . . , ) interrupted whenever upper boundU M,t (t ) less equal lower bound L M,t (t ) i.e., L M,t (t ) U M,t (t ). best solutionfound far optimal upper lower bounds initial occupancy state equal.interrupting trajectory satisfies either criterion 1 2, FB-HSVI preservesability find optimal separable joint policy, shown below.4.3.3 Convergence GuaranteesTheorems 7 8 show feature-based compact functions (L M,t )t{0,1, ,T } (U M,t )t{0,1, ,T }iteratively updated FB-HSVI (Algorithm 2) upper lower-bounds optimal value function.Next, prove upon update trajectory compact occupancy state boundsdepreciated, least one compact occupancy state improves bounds. Since finitenumber compact occupancy states, bounds ultimately converge optimal valueinitial occupancy state. Here, use argument show FB-HSVI converges optimalseparable joint policy finite number iterations. end, compact occupancy statesaid finished either criterion 1 2 satisfied; otherwise finished. Moreover,compact occupancy states last time step finished since criterion 2 satisfiedlast time step .Theorem 9. FB-HSVI algorithm always terminates finite number trials solution found termination separable joint policy lower bound induces optimal.Proof. First, show contradiction algorithm cannot terminate optimal solution found. Suppose algorithm terminates finding optimal solution valuef (0 ). Then, sequence f (0 ) values generated planning f 0 (0 ), f 1 (0 ), . . . , f k (0 ),f 0 (0 ) initial lower bound solution found, f 1 (0 ) value firstsolution found, f k (0 ) last solution found. addition, know hypothesisf 0 (0 ) < f 1 (0 ) < . . . < f k (0 ) f (0 ), last inequality holds assumptionalgorithm may terminate suboptimal solution.consider optimal path 0 , 1 , . . . , leading initial occupancy state terminaloccupancy state. assumption optimal path found, mustoccupancy state along path generated expanded. possiblef (t ) f k (0 ). admissibility f , know f (t ) f (0 ) therefore f (t )f (0 ) > f (0 ) {0, 1, . . . , k}. contradiction, follows algorithmcannot terminate optimal solution found.Next, show trial turns least one finished occupancy state finished one.Suppose algorithm yet terminated trial executed. Let last two occupancy2. Typically, search algorithms involves expanding nodes adding unexpanded neighboring nodes priorityqueue, called frontier search space.480fiOptimally Solving Finite-Horizon Dec-POMDPsstates encountered forward expansion t+1 . Given trial terminatedt+1 , know trial, finished t+1 finished. t+1 resultsgreedy separable joint decision rule selection , know finishedupdated. two scenarios possible, corresponds stoppingcondition:either t+1 yields optimal value, also yield optimal value updated,making finished occupancy state update;t+1 f (t+1 ) lower equal L M,0 (0 ), also f (t ) lowerequal L M,0 (0 ) updated.Thus, executing trial causes occupancy state , finished, become finished.Finally, show algorithm terminates finite number trials. end,note search graph algorithm tree similar Figure 8, bounded branchingfactor | | depth {0, 1, . . . , }. hypothesis, occupancy states appear depth <finished. Thus, total number occupancy states depths time step upperbounded total number information states depths time step :1| Tt=0St | = |A ||I||Z |T |A |T 1,|Z ||A | 1(45)= maxiI Ai Z = maxiI Z . Given least one occupancy state becomes finished1 | trials, causingtrial, initial occupancy state must become finished |Tt=0algorithm terminate.Another important property FB-HSVI refines upper lower bounds throughout planning. Since compact occupancy state expansions interleaved updates, FB-HSVIoffers anytime solution. Furthermore, cutting FB-HSVI trials time, knowdifference current best solution optimal one bounded.Theorem 10. iteration FB-HSVI, current solution separable joint policyinduced current lower bound within = U M,0 (0 ) L M,0 (0 ) optimal solution.Proof. Formally, difference value executing separable joint policy lb inducedcurrent lower bound instead optimal separable joint policy written follows:V M, (0 ) V M,lb (0 ) = V M, (0 ) L M,0 (0 ),(V M,lb (0 ) = L M,0 (0 ))(46)U M,0 (0 ) L M,0 (0 ).(V M, (0 ) U M,0 (0 ))(47)Consequently, whenever FB-HSVI interrupted, current solution within givenoptimal solution.5. Experimentssection empirically demonstrates validates importance feature-based heuristicsearch value iteration (FB-HSVI) algorithm. show FB-HSVI outperforms existing exactalgorithms tested domains literature FB-HSVI solve problemsunprecedented time horizons.481fiDibangoye, Amato, Buffet & Charpillet5.1 Experimental Setupdiscussed throughout paper, many key components affect performanceFB-HSVI. key components include (upper lower) bound representations, boundupdate methods, history compression, value generalization, initial upper bound.present three variants FB-HSVI, denoted OHSVI, FB-HSVI-LPE FB-HSVI-TPE.two latter differ notion history equivalence use feature-based compactrepresentations (see Table 2). equivalence relation given, FB-HSVI refers default(and better performing) implementation, FB-HSVI-TPE.AlgorithmOHSVIFB-HSVI-LPEFB-HSVI-TPEBound Representationsfullfeature-based compactfeature-based compactCompressionnonelpetpeTable 2: review selected algorithmic components use.selected benchmarks goal spanning range properties may affectperformance Dec-POMDP solver. Table 3, review selected domains properties. domains downloaded http://masplan.org.Dec-TigerMabcGrid-SmallRecycling-RobotsBox PushingMars Roversdomain parametersN |S | |Ai | |Z |223224222 165224322 100 452 256 68=2656125639062565613.34 1071.69 1014|0:t | different=5= 103.43 10301.39 109771.84 10193.23 106165.42 10443.09 101431303.43 101.39 109775.23 109401.25 10293974672851.88 102.57 10238723869Table 3: Domain parameters maximum number separable joint policies per horizons.5.2 Empirical Analysis Algorithmssection, compare FB-HSVI exact solvers. exact Dec-POMDP solvers considered state-of-the-art methods including: GMAA*-ICE (Oliehoek et al., 2013), IPG (Amatoet al., 2009), MILP (Aras & Dutech, 2010), LPC (Boularias & Chaib-draa, 2008). IPGLPC perform dynamic programming, GMAA*-ICE performs heuristic search MILP mixedinteger linear programming method. Results GMAA*-ICE (provided Matthijs Spaan), IPG,MILP, LPC conducted different machines. this, timing results directly comparable, likely differ small constant factor. three FB-HSVIvariants (Table 2) implemented framework, using identical basic operations,occupancy state value function updates, separable joint decision rule selection. terminate FB-HSVI whenever distance lower upper bounds within = 0.01.time limit set 1000ms.482fiOptimally Solving Finite-Horizon Dec-POMDPs5.2.1 Comparing Exact PlannersMILPLPCIPG ICE OHSVImulti-agent tiger0.32 0.010.16155.4 0.01 28.5672286 10834723456789104.9720.171.795342345103070100recycling robot0.30360.041.07360.55542.072696.818127223456789102030meeting 3x3 grid93.029FB-HSVIL0, (0 )0.030.401.369.6524.4233.1141.2158.5165.574.005.19084.80277.026410.3819.993512.21715.57215.1840.010.100.300.340.521.132.132.937.00010.66013.38016.48631.86393.402216.47308.780.030.040.791.301.882.5518.0624.3934.42291.1456.60.00.1330.4320.8941.4912.192.963.804.6814.3524.33Table 4: Experiments comparing computation times (in seconds) exact solvers (part 1).Time limit violations indicated , indicate unknown values. Bold entriescorrespond best known results benchmarks, terms computationaltime expected value.Tables 4 5 show performance results exact algorithms. algorithm, reportedcomputation time, includes time compute heuristic values appropriate (sincealgorithms use heuristics). also reported best expected cumulative rewardL M,0 (0 ) initial occupancy state. Tables 4 5 clearly show FB-HSVI allows significant improvement state-of-the-art solvers: tested benchmarks provide resultslonger horizons solved previously (the bold entries). many cases, (epsilon)optimal solution found horizons order magnitude larger previouslysolvable. two main reasons FB-HSVIs performance. First, searches space483fiDibangoye, Amato, Buffet & Charpilletpolicies mapping lower-dimensional features actions, whereas exact solvers searchspace policies mapping full histories actions. addition, uses value function mapping occupancy states reals allowing generalize value function unvisited occupancystates whereas solvers use value functions mapping partial policies reals. FB-HSVI performs best domain possesses structure results compact value function,recycling robot mabc domains.MILPLPC2345103050100IPGICEOHSVIbroadcast channel0.0363.446234567891000.65162400.184.0977.42345678910cooperative box-pushing1.07360.2946.4354011382596234567891083389grid small360.911361512242605Mars rovers1.00.0271.01.881103FB-HSVIL0, (0 )0.020.220.320.330.7814.041.7473.322.993.894.799.2927.4245.5090.7600.10.731.393.518.3042.269.03581.20.370.911.552.242.973.714.475.236.030.10.4570.6225.85410.724.9628.97184.3293.717.60066.08198.593107.72120.67156.42191.22210.27223.740.100.230.470.823.975.8122.826.562.75.809.3810.1813.2618.6220.9022.4724.3126.31Table 5: Experiments comparing computation times (in seconds) exact solvers (part 2).484fiOptimally Solving Finite-Horizon Dec-POMDPs5.2.2 Choosing Method Keep Information Concisecompare local truncation probabilistic equivalence notions introduced maintain concise representations occupancy states, decision rules, value functions.Box-PushingMars Rover400TPELPE103|H||H|3002001001021010234Horizon526Recycling Robot345 6 7Horizon8910Grid-Small40102|H||H|30201010102345 6 7Horizon89102345HorizonDec-Tiger76mabc20100|H||H|1510505002345Horizon627345 6 7Horizon8910Figure 12: Comparison compression methods maintain concise data FB-HSVI-LPEFB-HSVI-TPE. graphs shows memory requirements convergencetime exceeds y-axis given various number planning horizons x-axis.Clearly, algorithms use feature-based compact representations provide significant savingsnumber maintained histories (e.g., OHSVI). Using OHSVI,number generated histories grows (in worst case) exponentially planning horizon.exponential growth explains OHSVI, use history aggregation,485fiDibangoye, Amato, Buffet & Charpilletcannot scale beyond planning horizon = 4 tested domains (see Tables 4 5).Recycling Robot horizon = 5, experimental results together Table 3 show algorithmsuse compression methods maintain 30 orders magnitude less separable joint policiesalgorithms not. number histories retained important occupancy states,decision rules, value functions mappings reachable histories (or corresponding labels).end, compare LPE TPE selection benchmarks various planninghorizons.previously discussed, though LPE yields compact occupancy states conciseresult TPE, latter eases generalization bounds, speedsconvergence optimal solution. Figure 12 reports total number joint labels denoted|L| explicitly maintained FB-HSVI-LPE FB-HSVI-TPE various planninghorizons. observe TPE yields concise bound representations LPEbenchmarks planning horizons (i.e., using TPE number |L| lower using LPE).particular, notice that, tested domains, bounded number labels sufficientrepresenting optimal near-optimal value functions. TPE often succeeds identifyingmemory-bounded parametric space, resulting concise value functions, whereas LPE oftenfails.Recycling Robot problem example, TPE yields 6 joint labels (i.e.,histories) horizons whereas LPE maintains 38 different joint labels, numberkeeps growing planning horizon increases. fact, (Dibangoye, Amato, & Doniec, 2012;Becker, Zilberstein, Lesser, & Goldman, 2004) demonstrated Recycling-Robot problem,recent private observation sufficient summarize past private histories agent(i.e., four joint observations necessary). Here, TPE yields 6 joint labelsrelies joint action-observation histories rather joint observation histories. BroadcastChannel domain, TPE yields 4 joint labels horizons whereas LPE produces20 different joint labels. Again, results due underlying structure BroadcastChannel domain. scenario, future states world conditionally independentjoint histories. Hence, TPE forget joint histories, reason states. Anotherdomain interest Dec-Tiger problem. problem, = 6, TPE produces30 joint labels horizons whereas LPE maintains 126 different joint labels.assumption always exists optimal separable joint policy periodic (with period3) Dec-Tiger domain. words, exists optimal separable joint policydepends histories upon recent three action-observation pairs. Also, manyscenarios equivalence relations would fail identify memory-bounded spacehistories, even space exists. example, important information history mayspread time steps, necessarily last ones.6. Discussiondemonstrated method solve Dec-POMDPs largerpreviously solved, many practical applications much larger domains consideredpaper. result, additional methods may necessary solve large problemspermit construction concise feature space preserving optimality. concernsince numbers states histories impact occupancy states, separable joint decision rules,value functions. Maintaining objects large feature spaces prohibitive. highlights486fiOptimally Solving Finite-Horizon Dec-POMDPsnecessity addressing scalability issue FB-HSVI concise (and possiblylossy) feature spaces. direction, already extended general methodology presentedpaper along two lines: error-bounded approximations tractable subclasses.6.1 Error-Bounded ApproximationsFB-HSVI find optimal solution maintains concise representations preserveoptimality. advantage liability. one hand, problems reasonablesize, algorithm find optimal solution. hand, many realistic applications,run time memory. scalability limitations FB-HSVI maintainsaccurate estimates (compact) occupancy states, value functions decision rules. improvescalability Dec-POMDP solvers, many researchers investigated approximate solutions.notable example family includes memory-bounded dynamic programming (MBDP)algorithm finite-horizon Dec-POMDPs (Seuken & Zilberstein, 2007; Carlin & Zilberstein, 2008;Dibangoye, Mouaddib, & Chaib-draa, 2009; Kumar & Zilberstein, 2010; Wu, Zilberstein, & Chen,2010). dynamic programming methods require bounded computational resourcesproduce heuristic solutions empirically perform well standard Dec-POMDP benchmarks.However, methods possess theoretical guarantees concerning qualitysolutions.Recently, introduced framework monitoring error FB-HSVI replacing exactestimate (compact) occupancy states, decision rules value functions, approximatecounterparts (Dibangoye, Mouaddib, & Chaib-draa, 2011; Dibangoye, Buffet, & Charpillet, 2014).resulting algorithm solve Dec-POMDP instances larger planning horizon stillproviding strong theoretical guarantees.also worth noting that, FB-HSVI trial-based, used anytime algorithm. is, alternates generation occupancy-state trajectory updatecurrent best value function. algorithm proceeds, current (best) value functionimproved expense increased computational time. algorithm terminated eithersatisfactory value function attained, allocated planning time exceeded. eithercase, algorithm always provide online performance bounds returned value functionillustrating far optimal value function current one is.future, also would like explore using occupancy states observation histories(rather action-observation histories), shown sufficient (along actionobservation histories) simultaneous work (Oliehoek, 2013). inclusion observationhistories could lead scalability gains reducing dimensionality feature space.6.2 Tractable SubclassesMany attempts address scalability issues Dec-POMDPs rely use tractable subclasses. subclasses additional assumptions allow concise representationsoccupancy states, decision rules value functions; therefore speed convergencetowards optimal solution.instance, already shown occupancy states states (and agent histories) used transition- observation-independent Dec-MDPs (Becker et al., 2004) (wherestate fully determined joint observation) greatly increase scalability preservingoptimality (Dibangoye et al., 2012; Dibangoye, Amato, Doniec, & Charpillet, 2013). restrict487fiDibangoye, Amato, Buffet & Charpilleting attention decentralized Markov policies (i.e., mappings private states private actions)reduce complexity significantly (NP versus NEXP), make possible optimally solvelarger problems. plan investigate forms tractable structures including temporal dependencies constraints induce structured domains single multi-agent settings(Dibangoye, Chaib-draa, & Mouaddib, 2008; Dibangoye, Shani, Chaib-Draa, & Mouaddib, 2009;Pajarinen, Hottinen, & Peltonen, 2013). line research, introduced novel approachcalled structural analysis means discovering underlying structural properties embeddedcertain decentralized decision-making problems (Dibangoye, Buffet, & Simonin, 2015).also applied general methodology presented paper scale numberagents involved process. end, consider domains exhibit locality interactions (Dibangoye, Amato, Buffet, & Charpillet, 2015, 2014). Examples include networkeddistributed partially observable Markov decision processes (ND-POMDPs) (Nair, Varakantham,Tambe, & Yokoo, 2005). plan explore applying methodology FB-HSVI DecPOMDPs agents joint dynamics rewards, well domains delayed communication (Ooi & Wornell, 1996; Grizzle, Marcus, & Hsu, 1981; Oliehoek & Spaan, 2012), meansreducing memory burden.secondary (but less important) issue concerning scalability Dec-POMDPs pertainsefficient methods update occupancy states value functions planning stage. localityinteraction among agents may exploited statically (e.g., Nair et al., 2005; Kumar & Zilberstein,2009; Amato, Konidaris, & Kaelbling, 2014) dynamically (e.g., Canu & Mouaddib, 2011)considering factorization graphical models representation hence improve scalability.critical issue number occupancy states necessary obtain good solution mayexponential planning horizon. So, techniques efficiently update occupancystates value functions great importance.7. Conclusionpaper describes novel way representing Dec-POMDPs, continuous-state MDPspiecewise-linear convex value functions, scalable algorithm generating -optimal solutions. summarize key contributions below.exploiting assumption centralized planning decentralized execution, methodrecasts Dec-POMDP problem equivalent deterministic centralized fully observableMDP (using information available agents). Next, identify concise statisticoccupancy state represents state resulting fully observable MDP, calloccupancy-state MDP. demonstrate optimal value functions occupancy MDPspiecewise linear convex functions occupancy states. also prove optimalsolution occupancy-state MDP optimal solution corresponding Dec-POMDP.also present feature-based heuristic search value iteration (FB-HSVI) algorithm findoptimal solution occupancy-state MDP. algorithm builds theory solvingPOMDPs MDPs, occupancy-state MDP allows methods directly appliedDec-POMDPs first time. believe FB-HSVI major step forward scalable exactsolutions Dec-POMDPs. scalability achieved defining feature-based compact occupancy states decision rules use equivalence relations private histories.concise representations permit us circumvent exhaustive enumeration otherwiseintractable number occupancy states decision rules.488fiOptimally Solving Finite-Horizon Dec-POMDPsAnother aspect improved scalability stems generalization value function.achieved piecewise linear convex functions occupancy-state MDP.show that, although feature-based compact lower upper bounds longer piecewiselinear convex, still generalize value functions entire feature-based compactoccupancy-state space.Experimentally, show FB-HSVI able outperform current state-of-the-art exactDec-POMDP solvers common benchmark domains. results show -optimal solutionsfound larger horizons problems horizons sometimes ordermagnitude larger previously solved.8. Acknowledgementswould like thank Matthijs Spaan providing results GMAA*-ICE well FransOliehoek, Akshat Kumar anonymous reviewers helpful comments. Research supported part AFOSR MURI project #FA9550-09-1-0538.Appendix A. Correctness Feature-Based Compact BoundsA.1 Proof Theorem 6(). Hence, obtain successively:(a) hypothesis, , v V M,t()v V M,t(48)= V M,t()Xdef= max(s, ) (s, )max= max( = F () Theorem 5),(49)(by definition V M,t()),(50)( projected onto ),(51)s,X X(s, ) (s, ) ( s, )s,s,Xs,( s, ) ( s, ),(52)s,X Xs, ( s, ) ( s, ) (s, )max( projected onto ),(53)s, s,def= V M,t(F ()),(54)ends proof Theorem 6.a.() h, G ()i arbitrary R| | .(b) hypothesis, V M,t() h, G (G ())i arbitrary feature set , successively show:prove V M,tXXXdefs, (s, )s, ( s, ) ( s, ),(s, )h, G (G ())i =s,s,s,(55)XXX( s, )(s, )=s, ( s, ) s, (s, ) , (Re-ordering). (56)s,s,s,489fiDibangoye, Amato, Buffet & Charpilletproceeding further, need prove quantity s, (s, ) greater equal quantitys, s, ( s, ) s, (s, ) (in bracket last expression above). end, startinterpretations expression. first expression asks whether state-history pair (s, )belongs cluster along feature s, , affirmative answer results value s, (s, ) = 1otherwise 0. Let , feature whose cluster includes state-history pair (s, ). Then,second expression asks whether state-history pairs ( , ) (s, ) belong cluster alongPfeature s, , affirmative answer result value s, s, ( s, ) s, (s, ) = 1otherwise 0. Clearly, second expression stricter form first expression, hence s, (s, )PPgreater equal s, s, ( s, ) s, (s, ). Thus, replacing s, s, ( s, ) s, (s, )s, (s, ), obtain:XXX(s, )( s, )h, G (G ())i =s, ( s, ) s, (s, )(57)s,s,s,XX(s, )s, (s, ) ( s, ),(58)Ps,s,def= h, G ()i(59)(),V M,t(60)ends proof Theorem 6.b.A.2 Proof Theorem 7proof proceeds induction. Heuristic function (U M,t )t{0,1,...,T } , initially upper boundsoptimal value function, since initialized using underlying MDP value function.induction step, assume heuristic function (U M,t )t{0,1,...,T } represented using point sets(t )t{0,1,...,T } upper bounds optimal value function.,Next, show that, arbitrary time step {0, 1, . . . , }, heuristic function U M,tupdate U M,t resulting upper bound v occupancy state , also upper bound V M,tentire compact occupancy space . is,:( ).( ) V M,tU M,t ( ) U M,t(61)( ). Using sawtooth interpolation approach3 ,first show : U M,t ( ) U M,tobtain successively:def( ) = min v , v | ( 7 v ) { 7 v } ,U M,t(see sawtooth definition)(62)= min v , U M,t ( ) ,(63)U M,t ( ),(64)proves first part expression (Eq. 61).( ) V ( ). end, distinguishNow, show : U M,tM,tupdate U M,t .3. Here, adapted sawtooth interpolation replace full occupancy states compact occupancy states.490fiOptimally Solving Finite-Horizon Dec-POMDPsupdate,following holds:( ),( ) V M,tU M,t:(65)inductive hypothesis.update, obtain two important results. one hand, resultingvalue v upper bound :defv = max R(, ) + U M,t+1 ( P(, )),(66)( P(, )),max R(, ) + V M,t+1(67)def().= V M,t(68)hand, show v one extrapolate upper bound value vcompact occupancy state . mainly thanks Theorem 6.a, demonstrate:() v holds, arbitrary, expression V (F ()) v holdsexpression V M,tM,twell. sawtooth interpolation method concludes argument follows:v = v + (v v ) D(F (), ),def( ).V M,t(69)(70)fact, sawtooth interpolation always generate upper-bound one compact occupancystate upper bound compact occupancy state long expressedfeature set.A.3 Proof Theorem 8proof proceeds induction well. Heuristic function (L M,t )t{0,1,...,T } initially lower boundsoptimal value function since initialize using value function associated worstseparable joint policy. one prescribes agents joint action yields minimumdefreward, time steps, i.e., L M,t () = (T t) min s,a ra (s), {0, 1, . . . , }.induction step, assume heuristic function (L M,t )t{0,1,...,T } represented using compactvector sets (t )t{0,1,...,T } lower bounds optimal value function. Next, showarbitrary time step {0, 1, . . . , }, heuristic function LM,t , results update lowerbound L M,t produces compact vector along feature set , also lower bound V M,tentire compact occupancy space . is,( ).L M,t ( ) LM,t ( ) V M,t:(71)first show : L M,t ( ) LM,t ( ). fact,defL M,t ( ) = max h , G ()i,(72)max{ }def= LM,t ( ),491h , G ()i,(73)(74)fiDibangoye, Amato, Buffet & Charpilletproves first part.( ). end, distinguishNow, show : LM,t ( ) V M,tupdate L M,t .update,induction hypothesis, have::LM,t ( ) V M,t( ).(75)update, obtain: ,def( P( , )),V M,t( ) = max R( , ) + V M,t+1(76)= max h , rd + max h pd , Gt+1=h , rd + Gmaxh , rd + GA,t+1maxA,t+1maxA,t+1pp()i,(77)() (pd ) i,(re-arranging terms )(78)() (pd ) i,(replace t+1 t+1 )(79)(Lemma 6)(80)(retain one element).(81)h , G (rd + Gpp() (pd ) ))i,] , t+1 ))i,h , G (backup(( ), proveMerging together arguments update, i.e., LM,t ( ) V M,tsecond part proof. ends proof.Appendix B. Subroutinessection gives subroutines required compute feature-based compact occupancy statesusing either local truncation probabilistic equivalence relations (Algorithm 3).Algorithm 3: Compact feature-based occupancy state LPE.function Compact-LPE(t){(s, ) : (s, ) > 0}foreach (s, )S\{(s, )} (s, ) (s, )foreach ( s, )AreStateJointHistoryPairsLPE((s, ), ( s, ), )S\{( s, )} (s, ) (s, ) + ( s, )returnfunction Compact-TPE(t)mt getTruncationParam(t) ()foreach (s, )S\{(s, )} (s, ) (s, )foreach ( s, )AreStateJointHistoryPairsTPE((s, ), ( s, ), mt )S\{( s, )} (s, ) (s, ) + ( s, )return492fiOptimally Solving Finite-Horizon Dec-POMDPsAlgorithm 4: Subroutines compact feature-based occupancy state LPE TPE.function ArePrivateHistoriesLPE(ti, ti , )foreach ti(t )Pr(s, ti |0 , ) , Pr(s, ti |0 , ) return Falsereturn Truefunction AreStateJointHistoryPairsLPE((s, htiiiI ), ( s, hti iiI ), ), return FalseforeachArePrivateHistoryLPE(ti, ti , ) return Falsereturn Truefunction getTruncationParam(t)m0foreachforeach ti , ti (t )Suffix(ti, m) = Suffix(ti, m)ArePrivateHistoriesLPE(ti , ti , ) + 1;returnfunction ArePrivateHistoriesTPE(ti, ti , mt )return Suffix(ti, mt ) = Suffix(ti, mt )function AreStateJointHistoryPairsTPE((s, htiiiI ), ( s, hti iiI ), mt ), return FalseforeachArePrivateHistoryLPE(ti, ti , mt ) return Falsereturn TrueReferencesAmato, C., Bernstein, D. S., & Zilberstein, S. (2010). Optimizing fixed-size stochastic controllersPOMDPs decentralized POMDPs. Journal Autonomous Agents Multi-AgentSystems, 21(3), 293320.Amato, C., Chowdhary, G., Geramifard, A., Ure, N. K., & Kochenderfer, M. J. (2013). Decentralized control partially observable Markov decision processes. 54th IEEE ConferenceDecision Control.Amato, C., Dibangoye, J. S., & Zilberstein, S. (2009). Incremental policy generation finitehorizon DEC-POMDPs. Proceedings Nineteenth International Conference Automated Planning Scheduling.Amato, C., Konidaris, G. D., Anders, A., Cruz, G., How, J. P., & Kaelbling, L. P. (2015). Policysearch multi-robot coordination uncertainty. Proceedings Robotics: ScienceSystems Conference.Amato, C., Konidaris, G. D., & Kaelbling, L. P. (2014). Planning macro-actions decentralized POMDPs. Proceedings Thirteenth International Conference AutonomousAgents Multiagent Systems.493fiDibangoye, Amato, Buffet & CharpilletAras, R., & Dutech, A. (2010). investigation mathematical programming finite horizondecentralized POMDPs. Journal Artificial Intelligence Research, 37, 329396.Banerjee, B., Lyle, J., Kraemer, L., & Yellamraju, R. (2012). Sample bounded distributed reinforcement learning decentralized POMDPs. Proceedings Twenty-Sixth AAAIConference Artificial Intelligence, pp. 12561262, Toronto, Canada.Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamic programming. Artificial Intelligence, 72(1-2), 81138.Becker, R., Zilberstein, S., Lesser, V. R., & Goldman, C. V. (2004). Solving transition independentdecentralized Markov decision processes. Journal Artificial Intelligence Research, 22,423455.Bellman, R. E. (1957). Dynamic Programming. Dover Publications, Incorporated.Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). complexity decentralized control Markov decision processes. Mathematics Operations Research, 27(4).Boularias, A., & Chaib-draa, B. (2008). Exact dynamic programming decentralized POMDPslossless policy compression. Proceedings Eighteenth International ConferenceAutomated Planning Scheduling, pp. 2027.Canu, A., & Mouaddib, A.-I. (2011). Collective decision partial observability - dynamiclocal interaction model. IJCCI (ECTA-FCTA), pp. 146155.Carlin, A., & Zilberstein, S. (2008). Value-based observation compression DEC-POMDPs.Proceedings Seventh International Conference Autonomous Agents MultiagentSystems.De Farias, D. P., & Van Roy, B. (2003). linear programming approach approximate dynamicprogramming. Operations Research, 51(6), 850865.Dibangoye, J. S., Amato, C., Buffet, O., & Charpillet, F. (2014). Exploiting separability multiagent planning continuous-state MDPs. Proceedings Thirteenth InternationalConference Autonomous Agents Multiagent Systems, pp. 12811288.Dibangoye, J. S., Amato, C., Buffet, O., & Charpillet, F. (2015). Exploiting separability multiagent planning continuous-state MDPs (extended abstract). Proceedings TwentyFifth International Joint Conference Artificial Intelligence, pp. 42544260.Dibangoye, J. S., Amato, C., & Doniec, A. (2012). Scaling decentralized MDPs heuristicsearch. Proceedings Twenty-Eighth Conference Uncertainty Artificial Intelligence, pp. 217226.Dibangoye, J. S., Amato, C., Doniec, A., & Charpillet, F. (2013). Producing efficient error-boundedsolutions transition independent decentralized MDPs. Proceedings Twelfth International Conference Autonomous Agents Multiagent Systems, pp. 539546.Dibangoye, J. S., Buffet, O., & Simonin, O. (2015). Structural results cooperative decentralized control models. Proceedings Twenty-Fifth International Joint ConferenceArtificial Intelligence, pp. 4652.Dibangoye, J. S., Mouaddib, A.-I., & Chaib-draa, B. (2009). Point-based incremental pruningheuristic solving finite-horizon DEC-POMDPs. Proceedings Eighth International Conference Autonomous Agents Multiagent Systems, pp. 569576.494fiOptimally Solving Finite-Horizon Dec-POMDPsDibangoye, J. S., Mouaddib, A.-I., & Chaib-draa, B. (2011). Toward error-bounded algorithmsinfinite-horizon Dec-POMDPs. Proceedings Tenth International ConferenceAutonomous Agents Multiagent Systems, pp. 947954.Dibangoye, J. S., Shani, G., Chaib-Draa, B., & Mouaddib, A.-I. (2009). Topological order planner POMDPs. Proceedings Twenty-Second International Joint ConferenceArtificial Intelligence, pp. 16841689.Dibangoye, J. S., Amato, C., Buffet, O., & Charpillet, F. (2013). Optimally solving Dec-POMDPscontinuous-state MDPs. Proceedings Twenty-Fourth International Joint ConferenceArtificial Intelligence.Dibangoye, J. S., Buffet, O., & Charpillet, F. (2014). Error-bounded approximations infinitehorizon discounted decentralized POMDPs. Proceedings Twenty-Fourth EuropeanConference Machine Learning, pp. 338353.Dibangoye, J. S., Chaib-draa, B., & Mouaddib, A.-I. (2008). novel prioritization techniquesolving Markov decision processes. Proceedings 21th International ConferenceFlorida Artificial Intelligence Research Society, pp. 537542.Grizzle, J. W., Marcus, S. I., & Hsu, K. (1981). Decentralized control multiaccess broadcastnetwork. 20th IEEE Conference Decision Control including SymposiumAdaptive Processes, Vol. 20, pp. 390391.Hansen, E. A., Bernstein, D. S., & Zilberstein, S. (2004). Dynamic programming partially observable stochastic games. Proceedings Nineteenth National Conference ArtificialIntelligence, pp. 709715.Hansen, E. A., & Zilberstein, S. (2001). LAO*: heuristic search algorithm finds solutionsloops. Artificial Intelligence, 129(1-2), 3562.Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). formal basis heuristic determinationminimum cost paths. IEEE Trans. Systems Science Cybernetics, 4(2), 100107.Hauskrecht, M. (2000). Value-function approximations partially observable Markov decisionprocesses. Journal Artificial Intelligence Research, 13, 3394.Howard, R. A. (1960). Dynamic Programming Markov Processes. M.I.T. Press.Jain, M., Taylor, M. E., Tambe, M., & Yokoo, M. (2009). DCOPs meet real world: Exploringunknown reward matrices applications mobile sensor networks. ProceedingsTwenty-Second International Joint Conference Artificial Intelligence, pp. 181186.Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning acting partiallyobservable stochastic domains. Artificial Intelligence, 101(1-2), 99134.Korf, R. E. (1990). Real-time heuristic search. Artificial Intelligence, 42(2-3), 189211.Kumar, A., & Zilberstein, S. (2009). Constraint-based dynamic programming decentralizedPOMDPs structured interactions. Proceedings Eighth International ConferenceAutonomous Agents Multiagent Systems, pp. 561568.Kumar, A., & Zilberstein, S. (2010). Point-based backup decentralized POMDPs: complexitynew algorithms. Proceedings Ninth International Conference AutonomousAgents Multiagent Systems, pp. 13151322.495fiDibangoye, Amato, Buffet & CharpilletNair, R., Tambe, M., Yokoo, M., Pynadath, D. V., & Marsella, S. (2003). Taming decentralizedPOMDPs: Towards efficient policy computation multiagent settings. ProceedingsEighteenth International Joint Conference Artificial Intelligence, pp. 705711.Nair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2005). Networked distributed POMDPs:synthesis distributed constraint optimization POMDPs. Proceedings TwentiethNational Conference Artificial Intelligence, pp. 133139.Oliehoek, F. A. (2012). Decentralized POMDPs. Wiering, M., & van Otterlo, M. (Eds.), Reinforcement Learning: State Art, Vol. 12, pp. 471503. Springer Berlin Heidelberg,Berlin, Germany.Oliehoek, F. A. (2013). Sufficient plan-time statistics decentralized POMDPs. ProceedingsTwenty-Fourth International Joint Conference Artificial Intelligence.Oliehoek, F. A., Spaan, M. T. J., Amato, C., & Whiteson, S. (2013). Incremental clusteringexpansion faster optimal planning Dec-POMDPs. Journal Artificial IntelligenceResearch, 46, 449509.Oliehoek, F. A., Spaan, M. T. J., & Vlassis, N. A. (2008). Optimal approximate Q-value functions decentralized POMDPs. Journal Artificial Intelligence Research, 32, 289353.Oliehoek, F. A., & Spaan, M. T. (2012). Tree-based solution methods multiagent POMDPsdelayed communication. Proceedings Twenty-Sixth AAAI Conference ArtificialIntelligence.Oliehoek, F. A., Whiteson, S., & Spaan, M. T. J. (2009). Lossless clustering histories decentralized POMDPs. Proceedings Eighth International Conference AutonomousAgents Multiagent Systems, pp. 577584.Ooi, J. M., & Wornell, G. W. (1996). Decentralized control multiple access broadcast channel:Performance bounds. Proc. 35th IEEE Conference Decision Control, Vol. 1,pp. 293298. IEEE.Pajarinen, J., Hottinen, A., & Peltonen, J. (2013). Optimizing spatial temporal reuse wirelessnetworks decentralized partially observable Markov decision processes. IEEE Transactions Mobile Computing, 13(4). Preprint.Paquet, S., Chaib-draa, B., Dallaire, P., & Bergeron, D. (2010). Task allocation learning multiagent environment: Application robocuprescue simulation. Multiagent Grid Systems, 6(4), 293314.Pineau, J., Gordon, G. J., & Thrun, S. (2006). Anytime point-based approximations largePOMDPs. Journal Artificial Intelligence Research, 27, 335380.Powell, W. B. (2007). Approximate Dynamic Programming: Solving Curses Dimensionality(Wiley Series Probability Statistics). Wiley-Interscience.Puterman, M. L. (1994). Markov Decision Processes, Discrete Stochastic Dynamic Programming.Wiley-Interscience, Hoboken, New Jersey.Roy, N., Gordon, G. J., & Thrun, S. (2005). Finding approximate POMDP solutions beliefcompression. Journal Artificial Intelligence Research, 23, 140.496fiOptimally Solving Finite-Horizon Dec-POMDPsSeuken, S., & Zilberstein, S. (2007). Improved memory-bounded dynamic programming DECPOMDPs. Proceedings Twenty-Third Conference Uncertainty Artificial Intelligence.Shani, G., Pineau, J., & Kaplow, R. (2013). survey point-based POMDP solvers. JournalAutonomous Agents Multi-Agent Systems, 27(1), 151.Smallwood, R. D., & Sondik, E. J. (1973). optimal control partially observable Markovdecision processes finite horizon. Operations Research, 21(5), 10711088.Smith, T. (2007). Probabilistic Planning Robotic Exploration. Ph.D. thesis, RoboticsInstitute, Carnegie Mellon University, Pittsburgh, PA.Smith, T., & Simmons, R. (2004). Heuristic search value iteration POMDPs. ProceedingsTwentieth Conference Uncertainty Artificial Intelligence, pp. 520527, Arlington,Virginia, United States.Smith, T., & Simmons, R. G. (2006). Focused real-time dynamic programming MDPs: Squeezing heuristic. Proceedings Twenty-First AAAI Conference ArtificialIntelligence, pp. 12271232.Spaan, M. T. J., Oliehoek, F. A., & Amato, C. (2011). Scaling optimal heuristic search DecPOMDPs via incremental expansion. Proceedings Twenty-Third International JointConference Artificial Intelligence, pp. 20272032.Szer, D., Charpillet, F., & Zilberstein, S. (2005). MAA*: heuristic search algorithm solvingdecentralized POMDPs. Proceedings Twenty-First Conference UncertaintyArtificial Intelligence, pp. 568576.Tsitsiklis, J. N., & van Roy, B. (1996). Feature-based methods large scale dynamic programming. Machine Learning, 22(1-3), 5994.Velagapudi, P., Varakantham, P., Sycara, K., & Scerri, P. (2011). Distributed model shapingscaling decentralized POMDPs hundreds agents. Proceedings Tenth International Conference Autonomous Agents Multiagent Systems, pp. 955962.Winstein, K., & Balakrishnan, H. (2013). TCP ex Machina: Computer-generated congestion control.SIGCOMM, Hong Kong.Wu, F., Zilberstein, S., & Chen, X. (2010). Point-based policy generation decentralizedPOMDPs. Proceedings Ninth International Conference Autonomous AgentsMultiagent Systems, pp. 13071314.Wu, F., Zilberstein, S., & Chen, X. (2011). Online planning multi-agent systems boundedcommunication. Artificial Intelligence, 175(2), 487511.Zilberstein, S., Washington, R., Bernstein, D. S., & Mouaddib, A.-I. (2002). Decision-theoreticcontrol planetary rovers. Revised Papers International Seminar AdvancesPlan-Based Control Robotic Agents, pp. 270289, London, UK. Springer-Verlag.497fiJournal Artificial Intelligence Research 55 (2016) 603-652Submitted 07/15; published 03/16Large-Scale Election Campaigns:Combinatorial Shift BriberyRobert Bredereckrobert.bredereck@tu-berlin.deTU Berlin,Berlin, GermanyPiotr Faliszewskifaliszew@agh.edu.plAGH University Science Technology,Krakow, PolandRolf NiedermeierNimrod Talmonrolf.niedermeier@tu-berlin.denimrodtalmon77@gmail.comTU Berlin,Berlin, GermanyAbstractstudy complexity combinatorial variant Shift Bribery problemelections. standard Shift Bribery problem, given electionvoter preference order set candidates outside agent,briber, pay voter rank bribers favorite candidate given numberpositions higher. goal ensure victory bribers preferred candidate.combinatorial variant problem, introduced paper, models settingspossible affect position preferred candidate multiple votes, either positivelynegatively, single bribery action. variant problem particularlyinteresting context large-scale campaign management problems (which,technical side, modeled bribery problems). show that, general, combinatorial variant problem highly intractable; specifically, NP-hard, hardparameterized sense, hard approximate. Nevertheless, provide parameterizedalgorithms approximation algorithms natural restricted cases.1. Introductionstudy computational complexity election campaign management casecampaign actions (such airing TV advertisement, launching web-based campaign, organizing meetings voters) may large-scale effects affect multiplevoters. Further, interested settings actions positive effects (for example, voters may choose rank promoted candidate higherfind arguments presented given advertisement appealing) well negative ones(for example, voters find advertisement aggressive). Thus,setting, two major issues faced campaign manager (a) choosing actionsc2016AI Access Foundation. rights reserved.fiBredereck, Faliszewski, Niedermeier, & Talmonpositively affect many voters possible (b) balancing negative effectscampaigning actions (for example, concentrating negative effects votersdisregard promoted candidate anyway).research falls within field computational social choice, subarea multiagentsystems. use standard election model, given set C candidatescollection V voters, represented preference order (that is, rankingcandidates preferred one least preferred one). assumeknow preferences voters. perfect knowledge impossiblepractice, assumption convenient simplification models fact may(approximate) information preelection polls sources.consider two voting rules, Plurality rule (where pick candidateranked first voters) Borda rule (where candidate c getsvoter v many points candidates v prefers c to, pickcandidate points). rules chosen Plurality rulewidespread rule practice Borda rule well-studiedcontext campaign management.Within computational social choice, term campaign management (introducedElkind, Faliszewski, & Slinko, 2009; Elkind & Faliszewski, 2010) alternative namebribery family problems (introduced Faliszewski, Hemaspaandra, & Hemaspaandra, 2009a) cases one focuses modeling actions available electioncampaigns: result money spent campaign manager, voters changevotes. paper study campaign management Shift Briberyproblem (Elkind et al., 2009; Elkind & Faliszewski, 2010; Bredereck, Chen, Faliszewski,Nichterlein, & Niedermeier, 2014a; Bredereck, Faliszewski, Niedermeier, & Talmon, 2016).Shift Bribery candidate p want win, voter vprice v (i) voter willing shift p forward positions preferenceorder1 , ask lowest cost ensuring p winner (see Section 1.1references campaign management problems).Shift Bribery problem one major drawback model campaign management. incapable capturing large-scale effects campaign actions. particular,one puts forward TV spot promoting given candidate, voters reactpositively rank candidate higher, oblivious it, reactnegatively, ranking candidate lower. Shift Bribery cannot model correlatedeffects. paper introduce study Combinatorial Shift Bribery problem, allowing campaign actions effects, positive negative, whole groupsvoters.interested understanding realistic model campaign managementaffects complexity problem. Indeed, Shift Bribery is, computationally,well-behaved problem. example, Plurality rule solvable polynomial timeBorda rule NP-complete (Elkind et al., 2009), polynomial-time2-approximation algorithm (Elkind et al., 2009; Elkind & Faliszewski, 2010)fixed-parameter (FPT) algorithms, either exact capable finding solutions arbitrarilyclose optimal ones (Bredereck et al., 2014a). work, ask extent1. course, price necessarily reflect direct money transfer voter, rather costconvincing voter change mind.604fiCombinatorial Shift Briberyretain good computational properties allow large-scale effects.results surprising positively negatively:1. Combinatorial Shift Bribery becomes NP-complete W[1]-hard evenPlurality rule, even restrictive choice parameters, even correlatedeffects particular campaign actions limited two voters. Moreover,hardness results imply good, general approximation algorithms existallow negative effects campaign actions.2. spite above, still possible derive relatively good (approximation)algorithms, Plurality rule Borda rule, providedrestrict effects campaign actions positive eitherinvolve voters each, involve groups consecutive voters (with respectordering voters might correspond, example, time).results summarized Table 1 Section 4. generality problemcombinatorial nature natural obtain many hardness results. Yet,extent strength surprising, fact also find nontrivial landscapetractable cases.1.1 Related Workwork builds top two main research ideas. First, studying campaign management/bribery problems, and, second, studying combinatorial variants electionproblems.study computational complexity bribery elections initiatedFaliszewski et al. (2009a), continued number researchers (Faliszewski, Hemaspaandra, Hemaspaandra, & Rothe, 2009b; Hazon, Lin, & Kraus, 2013; Mattei, Goldsmith,& Klapper, 2012a; Mattei, Pini, Rossi, & Venable, 2012b). Elkind et al. (2009) ElkindFaliszewski (2010) realized formalism election bribery problems usefulpoint view planning election campaigns. particular, definedSwap Bribery problem restricted variant, Shift Bribery. former possible, given price, swap two adjacent candidates given vote. latter,allowed shift preferred candidate forward. Various problems, modelingdifferent flavors campaign management, studied, including, example,possibility alter number approved/ranked candidates (Baumeister, Faliszewski,Lang, & Rothe, 2012; Faliszewski, Reisch, Rothe, & Schend, 2014; Schlotter, Faliszewski,& Elkind, 2011). Different (positive) applications bribery problems include, example,Margin Victory problem, goal briber prevent candidate winning. possible low cost, suggests electioncould tampered (Cary, 2011; Magrino, Rivest, Shen, & Wagner, 2011; Xia,2012; Reisch, Rothe, & Schend, 2014).point view, related works Elkind et al. (2009), ElkindFaliszewski (2010), Bredereck et al. (2014a, 2016), Dorn Schlotter (2012).former ones study Shift Bribery, generalize (parameterized complexityShift Bribery studied Bredereck et al., 2014a, Shift Bribery multiwinner605fiBredereck, Faliszewski, Niedermeier, & Talmonelections studied Bredereck et al., 2016), whereas work Dorn Schlotter(2012) pioneers use parameterized complexity analysis (swap) bribery problems.work largely inspired Bulteau, Chen, Faliszewski, Niedermeier,Talmon (2015) Chen, Faliszewski, Niedermeier, Talmon (2015), introducedstudied combinatorial variants election control. Election control well-studiedtopic computational social choice, initiated Bartholdi, Tovey, Trick (1992)studied numerous researchers (we point readers Faliszewski, Hemaspaandra,& Hemaspaandra, 2010; Faliszewski & Rothe, 2015, detailed account). Briefly put,control problems model attempts changing election results changing structure. standard types control include adding, deleting, partitioning candidatesvoters. Control problems, especially related adding deleting voters, quiterelevant issues campaign management, and, indeed, Section 5 show connection Combinatorial Shift Bribery (combinatorial) control addingvoters (Bulteau et al., 2015).idea combinatorial shift bribery somewhat related problem lobbyingmultiple referenda, introduced Christian, Fellows, Rosamond, Slinko (2007)(parameterized study provided Bredereck, Chen, Hartung, Kratsch, Niedermeier,Suchy, & Woeginger, 2014b; probabilistic variant studied, also parameterizedsense, Binkele-Raible, Erdelyi, Fernau, Goldsmith, Mattei, & Rothe, 2014). There,number yes/no elections goal ensure electionmajority voters vote yes. single lobbying action convince one votervote yes elections. combinatorial shift bribery single electionsingle action affect multiple voters, whereas lobbying problem multipleelections action affects one voter.stress use term combinatorial variants election problemsdifferent one used well-established line work regarding combinatorialcandidate spaces (see Lang & Xia, 2015, works, example, Boutilier, Brafman,Hoos, & Poole, 2004; Conitzer, Lang, & Xia, 2009; Mattei et al., 2012b). work useterm combinatorial refer combinations voters affected briberyaction.1.2 Organization Paperproviding preliminaries Section 2, give formal definition Combinatorial Shift Bribery problem Section 3. Section 4 give overviewresults. shed light connections Combinatorial Shift Briberyproblem Combinatorial Control Section 5. Then, Section 6, presentseries strong hardness results covering classes shift actions restrictive sets parameters (for example, many results already apply case twocandidates). Section 7, develop several exact algorithms special cases Combinatorial Shift Bribery, Section 8 describe approximation algorithmsCombinatorial Shift Bribery. proofs available appendices(either given proof relies ideas already presented proofs, orascase Theorem 9when proof particularly involved). end conclusionsSection 9.606fiCombinatorial Shift Bribery2. Preliminariessection, briefly describe model elections, define two voting rulesstudy, review basic concepts parameterized complexity.2.1 Electionselection E = (C, V ) consists set C = {c1 , . . . , cm } candidates collectionV = (v1 , . . . , vn ) voters. voter represented preference order, is,linear ranking candidates preferred one least preferred one;use voters preference orders interchangeably. example, C = {c1 , c2 , c3 },voter v1 may preference order v1 : c1 c2 c3 indicate likes c1 best,c2 , c3 (for clarity, treat voters females candidates males).assume arbitrary (but fixed) canonical order set candidates (for example, one could order candidates lexicographically names).subset C candidates, writing within preference order means listingdidates canonical order, writing means listing reverseorder.2.2 Voting Rulesvoting rule R function that, given election E = (C, V ), outputs set R(E) C(tied) election winners. candidate c R(E) said R-winnerelection E. consider two election rules, Plurality rule Borda rule.assign points candidates output highest score. Pluralityrule, candidate receives one point voter ranks first. Bordarule, candidate receives points voter prefers candidate exactlyones.use nonunique-winner model. is, candidates selected givenvoting rule viewed equally successful winners (in practice, course, one usesort tie-breaking rule resolve situation, disregarding ties simplifiesanalysis; however, interested reader consult papers effects tie-breakingcomplexity election problems, e.g. Obraztsova & Elkind, 2011; Obraztsova, Elkind,& Hazon, 2011).2.3 Parameterized Complexityassume familiarity standard notions regarding algorithms complexity theory,briefly review notions regarding parameterized complexity theory (Downey & Fellows,2013; Flum & Grohe, 2006; Niedermeier, 2006).parameterized complexity theory measure complexity given problemrespect input size particular parameter problem. Typicalparameters election problems include number candidates, number voters,solution size (for example, number campaign actions one perform; seeBetzler, Bredereck, Chen, & Niedermeier, 2012, survey parameterized complexityvoting). say parameterized problem fixed-parameter tractable (is FPT)algorithm given input instance parameter k solves problem607fiBredereck, Faliszewski, Niedermeier, & Talmong(k)|I|O(1) time, g computable function |I| length encodingI. also hierarchy hardness classes parameterized problems,two important levels formed classes W[1] W[2]. convenientway defining classes appropriate reduction notion completeproblems. Specifically, say parameterized problem reduces parameterizedproblem B two computable functions, h h0 , following properties:given instance parameter k, h(I) outputs FPT time (i.e., time g(k)|I|O(1)computable function g) instance 0 B parameter k 0 h0 (k),yes-instance 0 yes-instance B. words, hmany-one reduction B allowed run FPT time, requiredoutput instance whose parameter upper-bounded function input instancesparameter.class W[1] defined class problems parameterically reduceClique problem, W[2] class problems parameterically reduce SetCover problem, problems parameterized solution size (that is,value h definitions).CliqueInput: undirected graph G = (V (G), E(G)) integer h.Question: set H h vertices edgepair vertices H?Set CoverInput: universe set X, family subsets X, integer h.Question: subset 0 h subsets whose union gives X?sometimes consider special variants problems describe detailwithin relevant proofs.parameterized problem contained class XP algorithm that, giveninstance parameter k, solves time |I|g(k) , g computablefunction. holds FPT W[1] W[2] XP. point readers interesteddetails regarding parameterized complexity theory (and design parameterizedalgorithms) textbooks Downey Fellows (2013), Flum Grohe (2006),Niedermeier (2006).3. Combinatorial Shift Bribery Problemsection first define Combinatorial Shift Bribery problem fullgenerality and, then, describe simplify remainderstudy.3.1 DefinitionLet R voting rule. definition R-Combinatorial Shift Bribery somewhat involved, therefore first define necessary components. given electionE = (C, V ) preferred candidate p C. goal ensure p R-winnerelection. end, number possible actions choose from.608fiCombinatorial Shift BriberyLet := |C| number candidates E let n := |V | numbervoters. shift action f n-dimensional vector (possibly negative) integers,f = (f (1) , . . . , f (n) ). R-Combinatorial Shift Bribery given family F =(f1 , . . . , f ) shift actions. particular shift action models possible campaigningaction, airing TV spot organizing meeting voters. componentsgiven shift action measure effects action particular voters. givensubset F 0 F available shift actions, define effect F 0 voter vi (1 n)P(i)E (i) (F 0 ) = fj F 0 fj . Further, shift action fj (1 j ) comes nonnegativeinteger cost w(fj ) application.voter vi (1 n) individual threshold function : Z Z describingshift actions affect voter. require (0) = 0 nondecreasing.Let F 0 collection shift actions. applying shift actions F 0 , voter vi(1 n) shifts preferred candidate p > 0 positions forward (a) E (i) (F 0 ) > 0,(b) (t) E (i) (F 0 ) < (t + 1). shift > 0 positions back (a) E (i) (F 0 ) < 0,(b) (t) E (i) (F 0 ) > (t 1).Finally, given nonnegative integer B, budget.PWe ask existencecollection F 0 F available shift actions total cost fj F 0 w(fj ) Bapplying p R-winner given election. case,say F 0 successful. Consider following example.Example 1. Consider election below, set candidates C = {a, b, c, p},collection voters V = (v1 , v2 , v3 ), p preferred candidate. threeavailable shift actions, unit cost (i.e., w(f1 ) = w(f2 ) = w(f3 ) = 1).electionv1 : c b pv2 : b c pv3 : p b cshift actions2604 0 2030f1f2f3threshold functions that:1. 1 (1) = 4, 1 (0) = 0, 1 (1) = 6, 1 (2) = 100.2. 2 (0) = 0, 2 (1) = 2, 2 (2) = 2 (3) = 100.3. 3 (3) = 3 (2) = 100, 3 (1) = 3, 3 (0) = 0.use Borda rule. Candidates a, b, c, p have, respectively, 4, 6, 4, 4 points.easy see applying single shift action ensure ps victory. However,applying shift actions F 0 = {f2 , f3 } results p winner. total effecttwo shift actions (6, 2, 3). According threshold functions, means pshifted forward one position v1 v2 , shifted back one position v3 .shifts, modified election looks follows:609fiBredereck, Faliszewski, Niedermeier, & Talmonv10v20v30election:cpba:bapc:apbcis, apply shift actions F 0 = {f2 , f3 }, candidate c3 points, candidates 5 points each. Thus, a, b, p tied winnersF 0 indeed successful set shift actions.4Formally, given voting rule R, define R-Combinatorial Shift Briberyproblem follows:R-Combinatorial Shift BriberyInput: election E = (C, V ), C = {c1 , . . . , cm } set candidatesV = (v1 , . . . , vn ) collection voters, set F = {f1 , . . . , f } shiftactions costs w(f1 ), . . . , w(f ), threshold functions 1 , . . . , n , nonnegative integer budget B. One candidates designated preferredcandidate p.Question: subset F 0 F shift actions total cost Bapply shift actions F 0 candidate p R-winnerresulting election?definition quite complicated, captures important features campaigning. example, use threshold functions allows us model votersunwilling change position preferred candidate beyond certain range, irrespective strength campaign. fact different shift actions differentcosts models fact particular actions (for example, airing TV spots organizingmeetings) may come different costs.3.2 Relation Standard Shift Briberynecessary comment relation Combinatorial Shift Briberyproblem non-combinatorial variant, Shift Bribery (Elkind et al., 2009; Elkind &Faliszewski, 2010).non-combinatorial variant Shift Bribery problem defined similarlycombinatorial one, voters threshold functions insteadcollection shift actions costs, voter vi shift-bribery pricefunction . cost shifting preferred candidate forward positions vipreference order (t) (only forward shifts allowed). require (0) = 0functions nondecreasing. Formally, following definition (Rvoting rule).R-Shift BriberyInput: election E = (C, V ), C = {c1 , . . . , cm } set candidates V = (v1 , . . . , vn ) collection voters, collection (1 , . . . , n )shift-bribery price functions, nonnegative integer budget B. One610fiCombinatorial Shift Briberycandidates designated preferred candidate p.Question:vector (s1 , . . . , sn ) natural numbers (a)Pni=1 (si ) B (b) voter vi shift p forward si positions,p R-winner resulting election?intuitively seems R-Shift Bribery simpler combinatorialcousin, making observation formal requires care.Proposition 1. Let R voting rule. holds R-Shift Bribery many-one reducesR-Combinatorial Shift Bribery polynomial time.Proof. Consider instance R-Shift Bribery election E = (C, V ),C = {c1 , . . . , cm } V = (v1 , . . . , vn ), collection shift-bribery price functions(v1 , . . . , vn ), budget B. Without loss generality, take c1 preferredcandidate denote p. form instance R-Combinatorial ShiftBribery election, budget, preferred candidate,shift actions, costs, voters threshold functions constructedPtmj,follows: voter vi , set threshold function (t) = j=1 2number positions possible shift preferred candidateforward vi preference order, create shift action fi,t zero effectvoters vi , effect 2mt ; costP fi,t w(fi,t ) = (t) (t 1).sequence (s1 , . . . , sn ) ni=1 (si ) B p R-winnerelection voter vi shift p forward si positions, alsosolution constructed instance Combinatorial Shift Bribery: vi ,use shift actions fi,1 , . . . , fi,si , total bribery cost ShiftBribery instance and, implementing shifts, vi preferred candidateshifted exactly si positions.assume constructed Combinatorial Shift Bribery instance yesinstance. Consider subset F 0 shift actions whose total cost Bensure p R-winner election (and recall shift action usedonce). voter vi V , define si largest integer shiftactions fi,1 , . . . , fi,si belong F 0 . Let us fix voter vi . claim applyingshift actions F 0 (in Combinatorial Shift Bribery instance), preferredcandidate shifted forward exactly si positions. definition si , immediateshifted forward least si positions. shifted forwardpositionsfollowing reason: shift actions fi,1 , . . . , fi,si total effectP mjvi equal sj=12, equal (si ). definition, shift action fi,si +1F 0 . sum remaining shift actions effect vi smaller than:Xj=si +2mj2=ms2X2j = 2msi 1 1.j=0However, (si + 1) (si ) = 2msi 1 . means even used shift actionsaside fi,si +1 , vi preference order still would shift p exactly si positions.conclusion, means implementing shift actions F 0 ensuresvoter vi shift p forward exactly si positions. Further, vi611fiBredereck, Faliszewski, Niedermeier, & Talmonw(fi,1 ) + + w(fi,si ) = (si ). Therefore, sequence (s1 , . . . , sn ) witnessesinput instance Shift Bribery yes-instance total cost shiftsB (as combinatorial instance) ensure p winner (ascombinatorial instance).Since reduction clearly runs polynomial time, proof complete.construction proof somewhat involved, especially one takesaccount simply shows Combinatorial Shift Bribery problem indeedgeneralizes much simpler, non-combinatorial, one. Nonetheless, somewhat contriveduse threshold functions seems necessary. Indeed, Combinatorial ShiftBribery problem restricted shift actions positive entries exactly onevoter each, used simple linear threshold functions, would obtain ShiftBribery case convex price functions (Bredereck et al., 2014a).general variant Shift Bribery problem which, example, NP-hardnessresults Elkind et al. (2009) hold (as shown Bredereck et al., 2014a), nonethelessgeneral one.3.3 General Hardness Resultturns Combinatorial Shift Bribery problem, defined Section 3.1above, general allows following, sweeping, hardness result.2Theorem 2. Plurality rule Borda rule, Combinatorial ShiftBribery NP-hard even five voters two candidates budget constraints.Borda rule, Combinatorial Shift Bribery NP-hard also three votersfour candidates.Proof. reduce following (weakly NP-hard) variant Subset Sum problem(it simple exercise show NP-hardness reduction classic SubsetSum problem):Subset Sum (Zero Variant)Input: set := {a1 , . . . , } integers.PQuestion: nonempty set A0 ai A0 ai = 0?Given instance = {a1 , . . . , } Subset Sum (Zero Variant), constructinstance Plurality-Combinatorial Shift Bribery two candidates. SincePlurality rule Borda rule coincide elections two candidates, hardnessresult transfers Borda-Combinatorial Shift Bribery (and, fact, almostnatural voting rules).construct following election:2. Note, however, prove weak NP-hardness. is, result may hold assumeoccurring numbers encoded unary. contrary, hardness proofs paper givestrong hardness results independent number encoding issues.612fiCombinatorial Shift Briberyelectionv1 : pv2 : pv3 : pv4 : pv5 : pshift actionsa1a11 ... 10000f1...fnis, element ai A, set F shift actions contains one shift action fieffect ai v1 , effect ai v2 , effect 1 v3 , effect two voters.voter threshold functions follows. Candidate p shifted last positionv1 v2 effect voters negative (that is, 1 (1) = 2 (1) = 1).Candidate p shifted top position third voter effect positive (thatis, 3 (1) = 1). set cost shift action one set budgetn. Thus budget allows us pick combination shift actions.direction, let A0 non-empty subset whose element-wise sum equalszero. applying F 0 := {fi | ai A0 }, p winner: Since A0 sums zero,effect first two voters. effect third voter positive, A0non-empty. Thus p preferred three five voters wins election.direction, let F 0 F subset shift actions makes p winner.Then, F 0 must non-empty p win initial election. claimelement-wisesum A0 := {ai | fi F 0 } zero. sake contradiction, assumePai A0 ai 6= 0. sum negative, would negative effectfirst voter, would preferred three voters five, would win election.sum positive, would effect second voter takingrole first one.Using similar idea, show reduce Subset Sum (Zero Variant)Borda-Combinatorial Shift Bribery three voters four candidates. Giveninput before, construct following instance:electionv1 : p d1 d2 d3v2 : p d1 d2 d3v3 : d1 d2 d3 pshift actions3a13an3a1 . . . 3an33f1...fnis, element ai A, F contains one shift action fi effect 3ai v1 ,effect 3ai v2 , effect 3 v3 . voter vi threshold function(t) = t. effect, p shifted last position first second votereffect voters negative, shifted top position third voteeffect positive. shift action unit cost, set budgetn (i.e., pick combination shift actions).613fiBredereck, Faliszewski, Niedermeier, & TalmonObserve d1 original winner election obtains seven points whereasp obtains six points.direction, let A0 non-empty subset whose element-wise sum equalszero. apply shift actions F 0 := {fi | ai A0 } p becomes winner: Since A0 sumszero, effect first two voters. effect third voter positiveA0 non-empty. Thus, p preferred candidate voterswins election.direction, let F 0 F subset shift actions makes p winner.Then, F 0 must non-empty p win initial election. showelement-wisesum A0 := {ai | fi F 0 } zero. sake contradiction assumePai A0 ai 6= 0. sum negative, would negative effectfirst voter p would obtain six points, whereas d1 would obtain seven. sumpositive, would effect roles first second voterswitched.Effectively, Theorem 2 shows studying large-scale effects campaign actionsfull-fledged R-Combinatorial Shift Bribery problem leads hopelesslyintractable problem: hardness even elections fixed numbercandidates fixed number voters.3.4 Restricted Variants Combinatorial Shift BriberyGiven hardness results Theorem 2, throughout remainder paperfocus restricted variants Combinatorial Shift Bribery problem. assumeindividual threshold functions identity functions (that is, voterinteger t, holds (t) = t), assume shift action unitcost, consider restricted types shift actions. assumptions requireadditional discussion.restrictions threshold functions costs shift actions seembasic and, fact, even satisfied instances built proof Theorem 2. reason assuming that, one hand, seems beyond pointstudy instances involved Theorem 2, and, hand,interact restrictions, leading tractable cases. But, importantconsequences.First, using identity threshold functions means model societies pronepropaganda. identity threshold functions cannot differentiate votersless responsive actions. Second, assuming every shift actionunit cost models settings costs particular campaign actions similarenough small differences irrelevant; actual number actionschoose perform sufficiently good approximation real cost. true,example, case organizing meetings voters, often comparableprices. also likely case shift actions model actions airing TVspots: spot similar cost produce/broadcast. greatest disadvantageassuming unit costs longer model mixed campaigns use actionsseveral different types (meetings voters, TV spots, web campaigns, etc.).614fiCombinatorial Shift Briberyrestrictions types allowed shift actions even greater impactnature campaigns study. study following classes shift actions:Unrestricted Shift Actions. put restrictions allowed shift actions;models general (and, naturally, least tractable) setting.Bounded-Effect Shift Actions. consider parameter requireshift action f = (f (1) , . . . , f (n) ) holds j (1 j n),|f (j) | . still general setting, assume campaigningaction limited impact voter.Unit-Effect Shift Actions. class bounded-effect shift actions = 1.given voter, applying given shift action either leave preferred candidatep unaffected shift p one position down.Interval Shift Actions. subclass unit-effect shift actions never affectvoters negatively, shift action interval votersaffected positively (the interval respect order votersinput collection V ). class shift actions models campaigns associatedtime window certain voters reached, campaigns local givenneighborhoods3 (for example, include putting multiple posters, organizingmeetings, etc.). speak 1z -interval shift actions mean interval shift actionsshift action affects z voters.Unit-Effect Two Voters Shift Actions. subclass unit-effect shift actionsaffect two voters most. focus shift actions affect voterspositively, denoted (+1, +1)-shift actions, affect one voter positivelyone voter negatively, denoted (+1, 1)-shift actions. reason studyingfamilies model particularly natural types election campaigns,rather establish limits tractability problem. example,consider (+1, 1)-shift actions understand intractable shift actionsnegative effects; (+1, 1)-shift actions simplest shift actions typemay useful campaign (one would never deliberately use shift actionaffects preferred candidate negatively).Figure 1 presents difference bounded-effect shift actions, unit-effect shiftactions, unit-effect two voters shift actions, interval shift actions graphically.discuss next section, type allowed shift actions huge impactcomputational complexity problem.3. neighborhood scenario, take simplified view society voters lives line.course, would natural take two-dimensional neighborhoods account. viewinteresting direction future research, time consider simple settingspossible. time window scenario, natural ordering voters point timecast votes affected campaign.615fiBredereck, Faliszewski, Niedermeier, & Talmon2211121111111= 2; = 51112Unit-Effect111z11(+1, 1)11(+1, +1)11zFigure 1: Restrictions shift actions. visualize (from left right, top bottom):shift action maximum effect = 2 single shift action maximum number= 5 voters affected single shift action; unit-effect shift action; shift actioneffect +1 one voter effect 1 another voter (+1, 1); shift actioneffect +1 two voters (+1, +1); shift action effect +1 intervalsize z 1z . intended interpretation voters listed vertically, topbottom.4. Overview Resultsprovide high-level overview results. turns even rather strongrestrictions place (that is, restrictions defined Section 3.4), Combinatorial ShiftBribery computationally hard settings. present questunderstanding border tractability intractability Combinatorial ShiftBribery. end, employ following techniques ideas.1. seek regular complexity results (NP-hardness results) parameterizedcomplexity results (FPT algorithms, W[1]-hardness W[2]-hardness results,XP algorithms).2. consider structural restrictions sets available shift actions.3. seek approximation algorithms inapproximability results (that is, approximation hardness results).616fiCombinatorial Shift Briberyparameterized complexity results, consider following parameters: (a)number n voters, (b) number candidates, (c) budget B, (d)maximum effect single shift action, (e) maximum number voters affectedsingle shift action.discussions (in)approximability Combinatorial Shift Bribery regardtask minimizing cost ensuring preferred candidates victory. meansthat, example, 2-approximation algorithm decide possible ensurepreferred candidates victory all, and, so, output successful set shiftactions total cost twice high optimal one.summarize results Table 1. results show Combinatorial ShiftBribery highly intractable. Theorems 5, 6, 7, show problem computationally hard (in terms NP-hardness, W[2]-hardness, inapproximability even FPTalgorithms) Plurality rule Borda rule, even various restrictedforms unit-effect shift actions, even two candidates. means that, essence,problem hard natural voting rules, since two candidates natural voting rulesboil Plurality rule.Further, Theorem 8 Theorem 11 show problems W[1]-hard eventake number candidates budget joint parameter, even extremelyrestricted shift actions. problem remains hard (for case Borda rule)parameterized number voters (Theorem 9). contrary, casePlurality parameterization number voters obtain tractability.obtain several approximability results. essence, results possiblecases shift actions negative results. intuitive reason factshift actions negative effects, computationally hard checkwhether preferred candidate win even without restrictions budget.approximation algorithms based results non-combinatorialvariant problem, due Elkind et al. (2009) Elkind Faliszewski (2010).Either use non-combinatorial algorithms directly, subroutines algorithms,derive results plugging Combinatorial Shift Bribery-specific blocksframework developed Elkind et al. (2009) Elkind Faliszewski (2010).5. Connection Combinatorial Controlstudy combinatorial variants problems modeling ways affecting election resultsinitiated Bulteau et al. (2015), considered combinatorial control adding voters(Combinatorial-CCAV) Plurality rule Condorcet rule. turnsPlurality rule reduce problem (Combinatorial) CCAV(Combinatorial) Shift Bribery. non-combinatorial variants problemsgive much since problems easily seen polynomial-time solvable.However, strong hardness results Plurality-Combinatorial-CCAVtransfer case Plurality-Combinatorial Shift Bribery. Formally, PluralityCombinatorial-CCAV defined follows (Bulteau et al., 2015).Plurality-Combinatorial-CCAVInput: set C candidates preferred candidate p C, collection V617fiBredereck, Faliszewski, Niedermeier, & TalmonTable 1: Overview results. show exact algorithms approximation algorithms Plurality-Combinatorial Shift Bribery Borda-CombinatorialShift Bribery, different restrictions shift actions (see Figure 1). Results markedfollow work Elkind et al. (2009), follow work Brederecket al. (2014a), follow work Elkind Faliszewski (2010), followwork Bredereck et al. (2016). Note variants XPparameterized budget B (Observation 1).shift actionsregularShift Bribery(convex prices)ruleexact complexityapproximabilityPluralitypoly.-time solvable (O)BordaNP-complete(O),FPT B (),W[1]-hard n ()unit effect2-approximablepoly. time (,O),FPT-approximationscheme n ()W[2]-h B eveninapproximable even= 2 (Thm. 5),FPT-time B evenXP n (Prop. 12)= 2 (Thm. 6)PluralityFPT n (Thm. 13)BordaW[1]-hard n (Thm. 9)inapproximable evenFPT-time n (Cor. 10)(+1, 1)NP-h even = 2 (Thm. 7),W[1]-h Bcombined (Thm. 8)Plurality(+1, +1)1z -intervalsinapproximableeven = 2 (Thm. 7)FPT n (Thm 13)W[1]-h B2-approximablecombined (Thm. 8)poly. time (Thm. 15)PluralityFPT n (Thm. 13)BordaW[1]-h B (Thm. 11)618z-approximablepoly. time (Thm. 14)2z-approximablepoly. time (Thm. 14)2-approximablemz time (Thm. 16)fiCombinatorial Shift Briberyregistered voters (having preference orders C), collection W unregistered voters (having preference orders C), bundling function : W 2W(for w W holds w (w)), budget k.Question: collection W 0 Wk voters p0winner modified election (C, V w0 W 0 (w ))?Intuitively, unregistered voter w W , bundle, (w) (given explicitlyinput), add w election (for example, somehow convincingvote), voters bundle also join election (for example, people choosevote influence friend).Theorem 3. Plurality-Combinatorial-CCAV polynomial-time many-one reduciblePlurality-Combinatorial Shift Bribery. instance Plurality-CombinatorialCCAV candidates, reduction outputs instance Plurality-CombinatorialShift Bribery + 1 candidates.Proof. Consider input instance Plurality-Combinatorial-CCAV candidate setC, collection registered voters V , collection unregistered voters W , bundling function, preferred candidate p C, limit k number voters add.form instance Plurality-Combinatorial Shift Bribery, follows.form candidate set C 0 = C {d}, new candidate. formset voters V 0 following way.1. voter v V , include v V 0 , preference orders extended ranklast.2. voter w W ranks p first, include V 0 two voters, xw ,preference order form p , x0w , preference order formp .3. voter w W ranks candidate c C \ {p} first, includeV 0 voter xw preference order p c , voter x0w preference orderp .4. include 4|W ||C| voters V 0 preference orders achievefollowing effects: (a) c C score s(c) election (C, V ), c ranked first4|W | + s(c) voters V 0 , (b) ranked first exactly 2|W | voters V 0 .achieve effects, c C \ {p} include 4|W | voters rank c first,include 3|W | voters rank p first, include |W | voters rank first.voter w W , introduce shift action fw following effects:(w), w0 ranks p first fw effect 1 xw0 (but x0w0 ) w0 rankscandidate C \ {p} first, fw effect 1 xw0 effect +1 x0w0 (allentries zeros). finishes construction. provide proof correctnessfollowing example reduction.w0Example 2. Consider following input Plurality-Combinatorial-CCAV,preferred candidate p budget k 1.619fiBredereck, Faliszewski, Niedermeier, & Talmonregistered votersv1 : punregistered votersw1 : pbundling function(w1 ) = {w1 , w3 }v2 : pw2 : p(w2 ) = {w2 }v3 : pw3 : p(w3 ) = {w2 , w3 }construct following input Plurality-Combinatorial Shift Bribery; noticenumber entries shift action 33.electionv1 : pv2 : pv3 : pxw1 : px0w1 : pxw2 : px0w2 : pxw3 : px0w3 : p12 dummies :9 dummies : p3 dummies :shift actions0000 0 00 0 01 0 00 0 00 1 10 1 11 0 10 0 00 0 00 0 0000fw1fw2fw3Note adding voter w1 input election Plurality-Combinatorial-CCAVresults p winner election. Correspondingly, applying shift action fw1 resultsp winner input election Plurality-Combinatorial Shift Bribery. 4see correctness construction, note applying shift action corresponding bundle voter w W effect differences scorescandidates C adding bundle (w) original control instance.specifically, disregarding score now, following.w0 (w) ranks p first, increase score p one,w0 (w) ranks candidate c C \ {p} first, increase scorec one. Further, score candidate never grow beyond 4|W | PluralityCombinatorial Shift Bribery instance score p never fall 4|W |.Therefore, never prevent p winner.Thus, reduction correct. Furthermore, reduction computed polynomial time outputs Plurality-Combinatorial Shift Bribery instance onecandidate input Plurality-Combinatorial-CCAV instance. also observe output instance uses unit-effect shift actions affect twicemany voters largest bundle input instance.620fiCombinatorial Shift BriberyBased proof Theorem 3 results Bulteau et al. (2015), obtainfollowing result.Corollary 4. Plurality-Combinatorial Shift Bribery W [2]-hard respectbudget B even = 3, W [1]-hard respect B even shift actions uniteffect 6 voters, NP-hard even shift actions unit effects4 voters.Proof. result follows applying reduction proof Theorem 3Plurality-Combinatorial-CCAV instances produced reductions Theorems 2, 1,4 Bulteau et al. (2015), respectively.6. Hardness Resultsresults previous section show bound hit hard instancesCombinatorial Shift Bribery even restricted setting. section explorerestrictive hard settings are. results organized type shift actionsallowed.6.1 Results General Unit-Effect Shift Actionsstart considering unit-effect shift actions. allowed effects positive only,obtain NP-hardness W[2]-hardness parameterizing budget B.allow also negative unit-effects, problem gets even harder go beyondhope approximation algorithm, even approximation algorithm allowedrun FPT time parameterizing budget B. Quite strikingly, results holdeven two candidates.Theorem 5. Plurality rule Borda rule, Combinatorial ShiftBribery NP-hard W[2]-hard parameter budget B, even two candidateseven shift action effects either +1 0 voter.Proof. provide parameterized reduction Set Cover (recall Section 2.3). Let(S, X, h) instance Set Cover, = {S1 , . . . , Sm } family subsetsuniverse X = {x1 , . . . , xn }, h number sets use cover X.construct instance Plurality-Combinatorial Shift Bribery two candidates. Note that, since Borda rule Plurality rule coincide elections twocandidates, hardness result transfers Borda-Combinatorial Shift Bribery.construction follows. p candidates.element xi X create element voter vi preference order p. Create another setn dummy voters preference order p. set F shift actions containsset Sj function fj effect +1 element voters correspondingelements set (that is, fj [i] = 1 xi Sj fj [i] = 0 otherwise). Finally,set B := h. finishes construction. Clearly, reduction computedpolynomial time. Consider following example applying reduction.Example 3. Let input Set Cover X = {x1 , x2 , x3 , x4 , x5 } ={S1 , S2 , S3 }, S1 = {1, 2, 5}, S2 = {2, 3}, S3 = {3, 4}, h = 2. constructfollowing input Plurality-Combinatorial Shift Bribery.621fiBredereck, Faliszewski, Niedermeier, & Talmonelectionv1 : pv2 : pv3 : pv4 : pv5 : p5 dummies : pshift actions0011 1 00 1 10 0 11 0 0000f1f2f3Note {S1 , S3 } set cover, and, analogously, choosing f1 f3 results pwinner election.4remains show set cover size h successfulset shift actions size h.part, assume set cover 0 size h. Then, applying0F = {fj | Sj 0 } makes p win election: Since 0 set cover, p preferredcandidate n element voters and, hence, winner election.part, assume set shift actions F 0 F size hwhose application makes p win election. Then, p must preferred candidateelement voters bribed election shift action effect dummyvoter. Since n element voters n dummy voters, 0 := {Sj | fj F 0 } setcover. Finally, since B = h, 0 size h.Allowing also negative (but unit) effects voters, adapt reductionTheorem 5 show strong inapproximability result. inapproximability result followssince corresponding reduction, yes-instances, correct solutions useexact given budget.Theorem 6. Unless W[2] = FPT, Combinatorial Shift Bribery inapproximable (inFPT time parameter B) Plurality rule Borda rule, even twocandidates unit-effect shift actions.Proof. modify reduction Theorem 5 show inapproximability result.Let (S, X, h) Set Cover instance = {S1 , . . . , Sm } X = {x1 , . . . , xn }.Without loss generality, assume |S| > h. construct instance PluralityCombinatorial Shift Bribery two candidates follows. (Since twocandidates only, proof applies case Borda-Combinatorial Shift Briberywell.)|S|element xi X, create |S| element voters vi1 , . . . , vi , preferenceorder p, set Sj create set voter vj0 preference order p d.Create |S| |X| + |S| 2h dummy voters, preference order p. set Fshift actions contains, set Sj , shift action fj effect 1element voter corresponding element set effect 1 set votercorresponding set. Finally, set B := h. completes construction,clearly computable polynomial time.622fiCombinatorial Shift BriberyNext, show successful set shift actions size hset cover size h.part, assume set cover 0 size h. Then, F 0 ={fj | Sj 0 } successful set shift actions: since 0 set cover, ppreferred candidate |S| |X| element voters also preferred candidateleast |S| h set voters (corresponding sets set cover). Moreover,preferred candidate |S| |X| + |S| 2h dummy voters also preferredcandidate h set voters (corresponding sets set cover). Hence,either p wins p tie winners.part, assume successful set shift actions F 0 Fsize h. Then, p must preferred candidate element votersbribed election: element voter p, would least|S| 1 element voters p (the element voters correspondingelement). Thus would total |S|(|X| 1) element voters |S| setvoters prefer p, least |S| |X| + |S| 2h dummy voters |S| element votersprefer d. Since assumed |S| > h, would mean p winner. Thus,must 0 := {Sj | fj F 0 } set cover, and, due budget constraint,follows |S 0 | h.Finally, show Plurality-Combinatorial Shift Bribery inapproximableeven FPT time parameterized budget. Assume, sake contradiction, successful set shift actions F 0 F |F 0 | > B exists. Then, bribedelection, least |S| |X| + |S| 2h dummy voters also |F 0 | h + 1 set voters prefer d,|S| |X| element voters |S| (h + 1) set voters prefer p. Thus,unique winner. Hence, successful bribery action must optimal respectbudget FPT-algorithm Plurality-Combinatorial Shift Bribery (parameterized budget) would solve W[2]-hard problem Set Cover (parameterizedsolution size) FPT time; contradiction assumption FPT 6= W[2].6.2 Results Shift Actions Unit Effect Two Votersprevious section limit number voters affected shift action.focus case unit-effect shift action affect two voters. First show Combinatorial Shift Bribery remains NP-hard hardapproximate (+1, 1)-shift actions. provide parameterized hardness results(+1, 1) (+1, +1)-shift actions. proof relatively similar oneTheorem 6 defer Appendix A.Theorem 7. Unless P = NP, Combinatorial Shift Bribery inapproximable (inpolynomial time) Plurality rule Borda rule, even two candidates(+1, 1)-shift actions.opposed Theorem 6, result yield W[2]-hardness parameter budget B. proof uses reduction Set Covervalue budget size universe set X. insist parameterizedhardness unit effects two voters, accept larger sets candidates.However, increase large: show W[1]-hardness CombinatorialShift Bribery jointly parameterized budget number candidates.623fiBredereck, Faliszewski, Niedermeier, & TalmonTheorem 8. Plurality rule Borda rule, Combinatorial ShiftBribery W[1]-hard combined parameter (m, B), even either(+1, 1)-shift actions (+1, +1)-shift actions.Proof. four cases consider. begin Plurality rule (+1, +1)-shiftactions.Plurality Rule (+1, +1)-Shift Actions. describe parameterized reduction W[1]-hard Clique problem, parameterized solution size, PluralityCombinatorial Shift Bribery (+1, +1)-shift actions, parameterized (m, B).Let (G, h) instance Clique V (G) = {u1 , . . . , un0 } E(G) = {e1 , . . . , em0 }.create following instance Plurality-Combinatorial Shift Bribery. setcandidates {p} D, = {d1 , . . . , dh1 }. vertex ui V (G), create vertex voter vi preference order p. Moreover, create n0 2h dummyvoters preference order p each. edge {ui , uj } E(G), createshift action f{ui ,uj } effect 1 vertex voters vi vj , effect 0voters. Finally, set budget B := h2 . completes construction,computable polynomial time. Consider following example.Example 4. following graph, looking clique size h = 3.u2u3u5u4u1u7u6construct following input Plurality-Combinatorial Shift Bribery.electionv1 : d1 d2 pv2 : d1 d2 pv3 : d1 d2 pv4 : d1 d2 pv5 : d1 d2 pv6 : d1 d2 pv7 : d1 d2 p1 dummy : p d1 d2shift actions1111 0 00 0 00 1 00 0 10 0 00 0 0000100000100000110000001010fu1 ,u2fu1 ,u7fu5 ,u6fu5 ,u7fu1 ,u4fu1 ,u5Note (v1 , v5 , v7 ) form clique size 3 input graph Clique, and, accordingly, applying set shift actions {fu1 ,u5 , fu1 ,u7 , fu5 ,u7 } results p winnerelection Plurality-Combinatorial Shift Bribery.4624fiCombinatorial Shift BriberyWithout loss generality, assume d1 ranked first (arbitrary fixed)order D. Observe n0 vertex voters h dummy voters rank d1 first.also n0 h dummy voters rank p first. Hence, make p win election,one needs h additional voters rank p first (and, effect, rank d1 first).remains show constructed instance contains successful set shift actions F 0 size h (G, h) contains clique size h.part, let H V (G) set h vertices forming clique let E 0 E(G)set edges vertices H. Then, observe F 0 = {f{ui ,uj } |{ui , uj } E 0 } successful set shift actions: vertex voter vi correspondingclique vertex ui H, candidate p shifted h 1 positions forward. means that,total, h vertex voters rank p first p ties winner election.part, let F 0 successful set shift actions. Since dummy votersaffected shift action, follows order make p winner election,p must shifted top position least h vertex voters. is, total, p mustshifted h (h 1) positions forward. Since F 0 size B = h2 = h (h 1)/2shift action affects two vertex voters, F 0 must size exactly h2 affectingexactly h vertex voters. construction, implies h2 edges G incidentexactly h different vertices possible h vertices form clique.finishes proof Plurality rule (+1, +1)-shift actions.remaining cases proof quite similar (although, technically, involved)present Appendix B.quite natural consider Combinatorial Shift Bribery also differentperspective. Instead asking happens small number candidates, mightask complexity Combinatorial Shift Bribery small number voters(see, example, Brandt, Harrenstein, Kardel, & Seedig, 2013; Chen et al., 2015,motivation looking elections voters interesting). caseobtain hardness Borda rule. Indeed, later show PluralityCombinatorial Shift Bribery FPT parameter number voters. proofnext theorem quite involved available Appendix C.Theorem 9. Borda-Combinatorial Shift Bribery W[1]-hard respect number n voters, even (+1, 1)-shift actions budget constraints.proof Theorem 9 reduce Strongly Regular MulticoloredClique problem, and, importantly, impose budget constraints. Thus, follows approximation algorithm Borda-Combinatorial Shift Bribery (running FPT time parameterized number voters) would yield FPT algorithm Strongly Regular Multicolored Clique parameterizedsolution size. effect, following corollary.Corollary 10. Unless W[1] = FPT, Borda-Combinatorial Shift Bribery inapproximable even FPT-time parameter n, even (+1, 1)-shift actions.results Theorem 9 Corollary 10 compare interestinglynon-combinatorial variant Borda-Shift Bribery. recently, complexityBorda-Shift Bribery parameterized number voters unknown. Eventually625fiBredereck, Faliszewski, Niedermeier, & Talmon(in different paper, submitting one journal publication) shownproblem W[1]-hard (Bredereck et al., 2016), far simpler proofone used here. Nonetheless, Theorem 9 Corollary 10 still carry significant value.Earlier, Bredereck et al. (2014a) shown FPT approximation schemeBorda-Shift Bribery parameterized number voters, Corollary 10 showsresult generalize combinatorial setting.6.3 Results Interval Shift Actionsconclude discussion hardness results considering Combinatorial ShiftBribery interval shift actions. previous section allowed shift actionsnon-zero effects two voters each, two voters could chosen arbitrarily. show hardness result case positively affect multiplevoters, voters form consecutive interval input election.Theorem 11. Plurality rule Borda rule, Combinatorial ShiftBribery NP-hard even interval shift actions.Proof. consider Plurality rule first give many-one reduction followingvariant strongly NP-hard Numerical Matching Target Sums problem.Numerical Matching Target SumsInput: Three sets integers = {a1 , . . . , }, B = {b1 , . . . , bt }, X ={x1 , . . . , xt }, (1) numbers encoded unary, (2) 3t numbersdistinct, (3) two numbers B sumnumber X.Question: elements B paired [t]sum ith pair exactly xi ?standard variant problem, presented classic text GareyJohnson (1979), restrictions integers sets A, B, X.assume numbers encoded unary problem strongly NPhard. Further, Hulett, Will, Woeginger (2008) shown problem remainsNP-hard case 3t integers distinct. Finally, see thirdrestriction change complexity problem suffices consider followingtransformation: Given instance (A, B, X) Numerical Matching TargetSums, add 2 max(A B X) + 1 integer B X. producesequivalent instance two numbers, B, sumnumber X.Plurality Rule. Let (A, B, X) instance Numerical MatchingTarget Sums let denote largest integer B X. create instancePlurality-Combinatorial Shift Bribery follows. set candidates is:C := {p, d, ca1 , . . . , cat , cb1 , . . . , cbt , cx1 , . . . , cxt }.create following voters.626fiCombinatorial Shift Bribery1. pair integers ai x` X, introduce:(a) One voter preference ordercai p C \ {p, cai },(b) ai voters preference ordercx` p C \ {p, cx` },(c) 2y (ai + 1) voters preference orderp C \ {p, d}.voters called (ai , x` )-voters exactly 2y them.pair (ai , x` ), construct shift action faxi` effect 1 exactly set (ai , x` )voters.2. pair integers bj B x` X, introduce:(a) One voter preference ordercbj p C \ {p, cbj },(b) bj voters preference ordercx` p C \ {p, cx` },(c) 2y (bj + 1) voters preference orderp C \ {p, d}.voters called (bj , x` )-voters exactly 2y them.pair (bj , x` ), construct shift action fbxj` effect 1 exactly set (bj , x` )voters.3. Let q := 4ty. create sufficiently many dummy voters ensure that, altogether,candidates following scores:(a) p q points,(b) i, cai cbi q + 4ty + 1 points each,(c) ` [t], cx` q + 4ty + x` points.shift action affects dummy voters.627fiBredereck, Faliszewski, Niedermeier, & TalmonFinally, set budget B := 2t. completes reduction. easy seecomputable polynomial time (because numbers encoded unary)order voters shift action effects consecutive interval z := 2yvoters.remains show constructed instance Plurality-Combinatorial ShiftBribery contains successful set F 0 shift actions size 2t (A, B, X)yes-instance Numerical Matching Target Sums.part, let := {(ai1 , bj1 ), . . . , (ait , bjt )} solution Numerical Matching Target Sums, is, set integer pairs integer Boccurs exactly ai` + bj` = x` holds ` [t]. ObserveF 0 := {faxi` , fbxj` | (ai` , bj` ) S} successful set shift actions. Since integer``B occurs exactly (some pair of) S, candidate cai candidate cbjloses one point. Since ai` + bj` = x` ` [t], candidate cx` loses x` points.construction, p gains 4ty points set shift actions size 2t. Thus, p winselection.part, let F 0 successful set shift actions size 2t (ifsuccessful action smaller size could extend size 2t shift actionsnegative effects). applying shift actions F 0 , p gains 4ty points.make p winner election, candidate cai candidate cbj needs lose onepoint, candidate cx` needs lose x` points. Thus, ai exactlyx`x`one fai F 0 bj B exactly one fbj F 0 . Since integersB X distinct two integers B suminteger X, x` X least one shift action faxi` effect ai``voters prefer cxl , one shift action fbxj` effect bj` voters prefer cx` . Since`candidates cx` |F 0 | = 2t, follows exactly two shift actionseffect voters preferring cx` . Since cx` lose least x` points, holdsai` + bj` x` . fact, pigeonhole principle, holds ai` + bj` x` . Hence,successful set 2t shift actions, solution NumericalMatching Target Sums instance.Borda Rule. Borda rule, almost reduction works. Specifically,still exists integer q set requirements requiredproof Plurality rule hold Borda rule (with respect differentq). Importantly, since p second position preference profilesvoters, holds score differences, applying shift actions, similarPlurality rule Borda rule. Thus, proof correctness Plurality ruletransfers Borda rule.Throughout section shown number hardness resultsrestrictive assumptions regarding available shift actions. following sectionsseek positive algorithmic results.628fiCombinatorial Shift Bribery7. Exact Algorithmsspite pessimism looming previous section, section show twoexact FPT XP algorithms R-Combinatorial Shift Bribery. Then, Section 8,present several efficient approximation algorithms.begin observing R-Combinatorial Shift Bribery solved polynomial time, provided assume budget B constant. reasonneed choose B shift actions available ones, number shiftactions available upper-bounded input size.Observation 1. Plurality-Combinatorial Shift Bribery Borda-Combinatorial Shift Bribery XP parameterized budget B.restrict instances contain bounded-effect shift actions,show R-Combinatorial Shift Bribery solved polynomial time, providednumber n voters treated constant.Proposition 12. maximum effect every shift action upper-bounded universal constant, Plurality-Combinatorial Shift Bribery Borda-Combinatorial Shift Bribery XP parameterized number n voters.Proof. Let value bounding, component-wise, effect shift action. First,observe (2 + 1)n types different shift actions. Second, observeone knows budget spent type shift actions, one easily checkwhether corresponding set shift actions makes p winner election. Thus usefollowing algorithm: try possibilities distributing budget B among(2 + 1)n types shift actions check whether one makes p winner.so, accept. Otherwise reject.Proposition 12 holds even shift action comes individual costvoter individual threshold function, can, given budget, alwaysselect cheapest set shift actions given type. Further, expressing probleminteger linear program (ILP) using famous result Lenstra (1983),Plurality rule strengthen XP-membership FPT-membership.Theorem 13. bounded-effect shift actions (where treat bound universalconstant), Plurality-Combinatorial Shift Bribery FPT parameterizednumber n voters.Proof. Given instance Plurality-Combinatorial Shift Bribery n voters,algorithm proceeds follows. First, guess subset votersguarantee p ranked first (there 2n guesses try). guessed set voters,test whether p would winner election p shifted top positionguessed voters ranked first remaining voters. guessedsubset V 0 voters test positive, check whether possible ensure(by applying shift actions whose cost exceed budget) voters V 0rank p first. follows.Let universal constant bounding, component-wise, effect shift action.Observe (2+1)n types different shift actions. shift action629fiBredereck, Faliszewski, Niedermeier, & Talmontype z, introduce variable xz denoting number times shift action type zpresent solution. voter vi , denote svi (p) position p originalpreference order vi . voter vi V 0 , add following constraint:PPxsvi (p).z[,]{z:fz effect vi }ensures p indeed shifted top position vi preference list. addbudget constraint:Xxz B,ensuring solution respects budget. Finally, shift action type z addconstraint ensuring use many shift actions type z availableinput. finishes description ILP. result Lenstra (1983),solve ILP FPT time, (2 + 1)n integer variables.Roughly speaking, Theorem 13 reason Theorem 9 applyPlurality rule. setting, Plurality-Combinatorial Shift Bribery tractable.Note Theorem 13 applies case shift action unit cost,i.e., case focus paper. Nonetheless, believe possiblelift Theorem 13 case shift action individual cost, applyingideas Bredereck, Faliszewski, Niedermeier, Skowron, Talmon (2015a).8. Approximation Algorithmsexplore possibility finding approximate solutions Combinatorial ShiftBribery. focus approximating cost shift actions necessary ensure psvictory (for example, 2-approximate algorithm finds solution ensures ps victorywhenever possible, uses twice many shift actions necessary).Theorems 6 7, know cannot hope find approximate algorithmscases Combinatorial Shift Bribery shift actions negative effects.Thus, section, focus unit-effect shift actions positive effects.also simplifies situation always check possible ensure ps victory:suffices apply available shift actions check p winner (indeed,able perform check heart inapproximability results Section 6).approximation algorithms proceed either directly invoking algorithmsnon-combinatorial variant Shift Bribery Elkind et al. (2009) ElkindFaliszewski (2010), plugging algorithms framework. startformer approach describe latter.Theorem 14. shift action effects either 0 1 voter, PluralityCombinatorial Shift Bribery -approximated polynomial-time BordaCombinatorial Shift Bribery 2-approximated polynomial time, denotes maximum number voters affected shift action.Proof. general idea approximation algorithms split shift actionaffects 0 voters 0 shift actions, affecting single voter only. effect630fiCombinatorial Shift Briberyconstruct non-combinatorial instance Shift Bribery solve exactly,case Plurality rule, 2-approximately, case Borda rule.Specifically, construction goes follows. Let (i) denote number shift actionsaffecting voter i. Given instance Combinatorial Shift Bribery, forminstance Shift Bribery identical, except instead shift actions,price functions voters: set price function voterj (i), shifting p j positions costs j, j > (i), shifting p j positionscosts (2B + 1)j (where B total number shift actions available; noteexponential function (2B + 1)j ensures price functions convexeasily identify situations one shifts p (i) positions).4describe use construction case Plurality rulecase Borda rule.Plurality Rule. first translate input instance non-combinatorialPlurality-Shift Bribery instance described above. Then, apply known, exact,polynomial-time algorithm Plurality-Shift Bribery (Elkind et al., 2009)instance. Let cost solution found non-combinatorial instance.> B, impossible ensure ps victory combinatorial instance (becausenumber available shift actions insufficient).B, obtain solution F Plurality-Combinatorial Shift Briberyinstance follows. voter v (non-combinatorial) bribed electionranks p first, select shift actions combinatorial instance v ranks p first.Note |F | F indeed (combinatorial) solution.sake contradiction, assume successful set shift actions F 0size smaller |F |/. However, easy see set shift actions wouldcorrespond bribery cost smaller non-combinatorial instance. Sincecost optimal solution non-combinatorial instance, contradiction.Borda Rule. case Borda-Combinatorial Shift Bribery follows analogously, instead using polynomial-time exact algorithm non-combinatorialinstance, use 2-approximation algorithm Borda-Shift Bribery (Elkind et al.,2009; Elkind & Faliszewski, 2010). Let cost solution found. > 2B,impossible ensure ps victory.Otherwise, obtain solution F combinatorial instance, vote vnon-combinatorial solution shifts p positions, include shift actionsaffect voter. |F | s, F correct solution combinatorialinstance.existed solution F 0 combinatorial instance used less |F |/(2)shift functions, would solution non-combinatorial instancecost smaller |F |/2 s/2. Since used 2-approximate algorithm noncombinatorial instance, impossible.mention might possible improve approximation ratio given Theorem 14, least Borda rule. idea might cast problem variant4. Strictly speaking, need ensure price functions convex, variantShift Bribery generalize paper, stick consistency.631fiBredereck, Faliszewski, Niedermeier, & TalmonSet Multicover problem, generalization Set Cover problemelement covering requirement. Then, one could use approximation algorithm Set Multicover problem (for example, one suggested Rajagopalan& Vazirani, 1998) plug 2-approximation algorithm Elkind Faliszewski(2010).achieve better approximation guarantees Borda rule,restrict allowed shift actions. obtain results use framework ElkindFaliszewski (2010). essence, shown following: given variantShift Bribery, either Plurality rule Borda rule, one providefunction computes obtain highest number points preferredcandidate given budget B, 2-approximation algorithm variantShift Bribery.5 Note get-most-points-for-p algorithm solve ShiftBribery. maximizes score p, ensure candidate receiveshigher score. Indeed, optimal solution might increase score p smaller extent,expense dangerous opponents.Theorem 15. Borda-Combinatorial Shift Bribery 2-approximable polynomialtime (+1, +1)-shift actions.Proof. discussion preceding theorem statement, suffices provide functiongiven instance Combinatorial Shift Bribery budget B finds setshift actions obtain highest possible number points preferred candidatep without exceeding budget.general idea achieving compute maximum b-matching auxiliarymultigraph (multigraphs allow multiple edges vertices). b-matchingmultigraph G function b : V (G) N (called covering function) edge-inducedsubgraph G vertex u degree b(u). known b-matchingcomputed polynomial time (Gabow, 1983).construct auxiliary multigraph G follows. voter vi create vertexui . shift action effect 1 voter ui effect 1 voter uj , create edge{ui , uj }. Then, define covering function b b(ui ) number positionsp shifted forward preference order voter vi (that is, position ppreference order voter vi ).G b-matching size least B, corresponds set shift actionsincrease score p 2B, highest gain possible. G b-matchingsize k < B, take shift actions corresponding edges b-matching(these shift actions maximize number points p gain shift actionsmove p within two votes) greedily select shift actions pushes p forwardone vote, use budget (at point, every shift action affect p singlevote only). Thus function computes highest point gain possible p, givenbudget.Next, consider interval shift actions. is, fix order votersrestrict shift action effect voters comprise intervals. (In fact,5. fact, result applies scoring rules, paper focus Plurality ruleBorda rule only.632fiCombinatorial Shift Briberycould also allow holes inside intervals.) Unfortunately, algorithm requires XPtime parameterization length longest interval.Theorem 16. Plurality rule Borda rule, Combinatorial ShiftBribery 2-approximated XP-time interval shift actions, provided take, upper bound number voters affected shift action, parameter.Proof. per discussion preceding Theorem 15, suffices describe find setshift actions maximize number points preferred candidate p gainsgiven budget.end, use dynamic programming algorithm. Consider inputCombinatorial Shift Bribery election E = (C, V ), preferred candidate p,budget B spend increasing ps score. Let := |C| n := |V |.V = (v1 , . . . , vn ). algorithm uses following table partial results. numbers x, y, s0 , . . . , s1 table entry:[x, y, s0 , s1 , . . . , s1 ]denotes maximum number additional points candidate p gain voters v1 , . . . , vx condition (1) exactly shift actions used,affects voters set {v1 , . . . , vx }, (2) {0, ..., 1},candidate p shifted position si preference order voter vxi . is, iteratevoters store effect applied shift actions last voters.size table n B m+1 .algorithm almost Plurality rule Borda rule.difference computing scores candidates. Let z, 0 z 1, denoteposition p preference order voter (position 0 means p rankedfirst). Then, score(z) mean score p gains voter. Pluralityrule score(z) = 1 z = 0 score(z) = 0 otherwise. Borda rulescore(zi ) = zi 1. set voters vector z1 , . . . , zt (for [n] zi{0, . . . , 1}) denotes positions p preference orders voters,write score(z1 , . . . , zt ) mean score p gains voters. is:score(z1 , . . . , zt ) =Xscore(zi ).i[t]Given preparation, ready describe algorithm (jointly Pluralityrule Borda rule).Initialization. initialize entries [, y, s0 , s1 , . . . , s1 ] table follows.check whether set shift actions effects voters(v1 , . . . , v ) applying set shift actions moves candidate p positions s0 , . . . , s1 preference orders voters v1 , . . . , v , respectively.exists, set [, y, s0 , s1 , . . . , s1 ] score(s0 , s1 , . . . , s1 ). Otherwise, set[, y, s0 , s1 , . . . , s1 ] . (We explain check set shift actions existsend proof.)633fiBredereck, Faliszewski, Niedermeier, & TalmonRecursion Step. compute table entries [x, y, s0 , s1 , . . . , s1 ] x > , onecompute subsets shift actions (for [y]) whose last affected voter vx , ensuretogether yi shift actions whose last affected voter set {v1 , . . . , vx1 }thatj, 0 j 1, p shifted position sj preference order vxj .specifically, update phase compute x, < x n, y, 0B, vector (s0 , . . . , s1 ) {0, . . . , 1} table entry [x, y, s0 , s1 , . . . , s1 ]follows. say vector (s0 , s1 , . . . , s1 ) {0, . . . , m} (x, i)-realizable(0 y), set shift actions whose last affected voter vxj, 0 j 1, shifts candidate p sj positions preferenceorder voter vxj . write R(x, i) denote set vectors {0, . . . , 1}(x, i)-realizable (we describe compute R(x, i) later). Then, compute[x, y, s0 , s1 , . . . , s1 ] follows:[x, y, s0 , s1 , . . . , s1 ] = max{T [x 1, i, , s0 s1 , . . . , s1 s1 , ]+ score(s0 , s1 , . . . , s1 ) score(s1 s1 , . . . , s1 s1 ) |0 y, 0 1, (s0 , s1 , . . . , s1 ) R(x, i)}Informally, realizable total effect shift actions whose last affected votervx , number points candidate p gains number additional pointscandidate p gains shift actions last affected voter (v1 , . . . , vx1 )plus number additional points candidate p gains shift actionslast affected voter vx (to avoid double counting, expressed differencemiddle line formula).next show compute R(x, i). try every vector (s0 , . . . , s1 ) {0, . . . ,1} check (x, i)-realizable. Perhaps easiest wayformulate problem integer linear program (ILP) constant numbervariables.Let (s0 , . . . , s1 ) vector want check (x, i)-realizable.subset Q {0, . . . , 1}, say shift action type Q affects exactlyvoters vxi Q. subset Q, introduce integer variable xQ ,denoting number shift actions type Q used (x, i)-realization vector.solve following ILP:XxQ =(1)xQ{0} =(2)Q{0,...,1}XQ{1,...,1}Xj : 0 j 1xQ = sj(3)jQ(Note middle constraint ensures last affected voter vx .) Sincenumber variables ILP 2 , follows famous result Lenstra (1983)ILP solved XP time respect parameter (indeed, evenFPT time). Using ILP without middle constraint, checkvectors (s0 , . . . , s1 ) use initialization step.634fiCombinatorial Shift BriberyComing back dynamic program, clear finding obtain maximum score p respecting budget found taking maximumtable entries [n, B 0 , s0 , s1 , . . . , s1 ], possible values B 0 , 0 B 0 B,(s0 , s1 , . . . , s1 ) {0, . . . , 1} .section showed indeed possible achieve approximationalgorithms special cases Combinatorial Shift Bribery problem,settings algorithms efficient quite restrictive. meanspractice one might want seek good heuristics use algorithms guidanceinitial search.9. Conclusiondefined combinatorial variant Shift Bribery problem (Elkind et al., 2009;Elkind & Faliszewski, 2010; Bredereck et al., 2014a) studied computationalcomplexity. motivation research desire understand computationaldifficulty imposed correlated, large-scale effects campaign actions. respect,work motivated combinatorial study election control, studied Bulteau et al.(2015) Chen et al. (2015). found even various restricted specialcases numerous parameterizations, Combinatorial Shift Bribery problemhighly intractable worst case. Nonetheless, found initial positive results,mainly form approximation algorithms. Interestingly, approximation resultsquite strongly rely results non-combinatorial Shift Bribery.number research directions motivated work. example, Plurality-Combinatorial Shift Bribery Borda-Combinatorial ShiftBribery solved polynomial-time (+1, +1) shift actions interval actionsassumption number candidates constant?generally, results suggest studying restrictions problem.example, since parameterizing number available shift actions immediately givesfixed-parameter tractability results, natural question whether natural parameterizations exist could also lead positive results.Naturally, one might consider voting rules well. interesting Condorcetconsistent rules, Copeland rule, since rules tend behave rather differentlyscoring rules. mention results hold voting rules:specifically, Theorem 2, Theorem 5, Theorem 6, Theorem 7 hold voting rulestheorems hold elections two candidates, voting rulesbehave elections two candidates; Observation 1 Theorem 12basically brute-force algorithms results hold voting rules well;statements regarding Borda rule Theorem 14, Theorem 15, Theorem 16 holdscoring rules, since underlying 2-approximation algorithm Elkind Faliszewski(2010) works scoring rules.Further, might also interesting consider domain restrictions regarding voterspreferences (for example, single-crossing seems particularly natural context intervalshift actions, since means shift action affects voters somewhat similarpreferences), well-demonstrated restricting domain voters lead635fiBredereck, Faliszewski, Niedermeier, & Talmontractability (see Theorem 10 Bulteau et al., 2015, example combinatorialcontrol setting). However, pursuing direction would require careful discussionshift actions applied. example, allow single-crossing election ceasesingle-crossing bribery?AcknowledgmentsRobert Bredereck supported DFG project PAWS (NI 369/10). Nimrod Talmon supported DFG Research Training Group Methods Discrete Structures (GRK 1408) currently Weizmann Institute Science. Piotr Faliszewskisupported DFG project PAWS (NI 369/10) AGH University grant11.11.230.124 (statutory research).preliminary short version work presented 2015 InternationalConference Autonomous Agents Multiagent Systems (AAMAS 15) (Bredereck,Faliszewski, Niedermeier, & Talmon, 2015b).Appendix A. Proof Theorem 7Theorem 7. Unless P = NP, Combinatorial Shift Bribery inapproximable (inpolynomial time) Plurality rule Borda rule, even two candidates(+1, 1)-shift actions.Proof. give many-one reduction Set Cover. Let (S, X, h) Set Coverinstance, = {S1 , . . . , Sm } X = {x1 , . . . , xn } (we assume every elementbelongs least one set). construct instance Plurality-Combinatorial ShiftBribery. set budget B := |X|. candidate set {p, d}, ppreferred candidate. element voter vi element xi , preferenceorder p. set voter vjS set Sj , preference order p d. also|X| + |S| 2h 1 dummy voters, preference order p. elementxi set Sj , xi Sj construct shift action fji effect +1 vieffect 1 vjS . completes construction. easy see computablepolynomial time.Next, show successful set shift actions (note sizeset important, is, allow infinite budget) set coversize h.part, assume set cover 0 size h. showbuild successful set shift actions. start F 0 = element xi ,choose arbitrary set Sj 0 contains xi add corresponding functionfji F 0 . applying F 0 , observe p becomes winner: |X| element voters|S| h set voters prefer p |X| + |S| 2h 1 dummy voters h set voters prefer d.part, assume successful set shift actions F 0 F .Let h0 number applying shift actions F 0 , p preferredexactly |S| h0 set voters (that is, shift actions F 0 correspond h0 sets S). pwinner, majority voters (i.e., least |X| + |S| h voters) must prefer p. Thus,applying F 0 , least X (h h0 ) element voters prefer p. meanscollection h0 sets jointly cover least |X| (h h0 ) elements. Since every636fiCombinatorial Shift Briberyelement belongs set, extend collection set cover addingh h0 sets (in worst case, one set uncovered element). provesset cover (S, X, h) completes part.Note argumentation made assumptions regarding size F 0 .Hence, finding solution Plurality-Combinatorial Shift Bribery instance,including approximate solutions approximation factor, implies finding set coversize h. means unless P = NP, Plurality-Combinatorial Shift Briberyinapproximable polynomial time.Appendix B. Remaining Cases Proof Theorem 8Theorem 7. Unless P = NP, Combinatorial Shift Bribery inapproximable (inpolynomial time) Plurality rule Borda rule, even two candidates(+1, 1)-shift actions.Borda Rule (+1, +1)-Shift Actions. slightly modify reduction used Plurality rule (+1, +1)-shift actions. Specifically, describe parameterized reduction W[1]-hard Clique problem, parameterized solutionsize, Borda-Combinatorial Shift Bribery (+1, +1)-shift actions, parameterized(m, B).Let (G, h) instance Clique V (G) = {u1 , . . . , un0 } E(G) = {e1 , . . . , em0 }.create instance Borda-Combinatorial Shift Bribery follows. setcandidates {p} D, = {d1 , . . . , dh1 }. create following voters.1. vertex ui V (G), create corresponding vertex voter vi preferenceorder:d1 dh1 p.2. create n0 2h dummy voters, preference order:p d2 dh1 d1 .3. create h dummy voters, preference order:dh1 p d2 dh2 d1 .4. create n0 h dummy voters, preference order:p d1 dh1 .5. create n0 h dummy voters, preference order:d1 p d2 dh1 .edge {ui , uj } E(G), create shift action f{ui ,uj } effect 1 vertexvoters vi vj effect 0 voters. Finally, set budget B := h2 .completes construction, computable polynomial time.637fiBredereck, Faliszewski, Niedermeier, & Talmonproof correctness follows lines proof Plurality rule(+1, +1)-shift actions, instead counting number approvals, need computeBorda scores candidates. Indeed, reason additional dummyvoters.particular, construction ensures d1 original winner electiondifference Borda score p Borda score d1 exactly h2 .Furthermore, shift action increase score p two. Hence, makep co-winner one must increase score p h(h 1) decrease score d1 h.possible shift actions correspond edges clique size h.Plurality Rule (+1, 1)-Shift Actions. still reduce W[1]hard Clique problem, parameterized solution size, reduction bitinvolved.Let (G, h) Clique instance graph G n0 := |V (G)| vertices0:= |E(G)| edges. construct Plurality-Combinatorial Shift Bribery instancefollows. Let set candidates {p, d} D, := {d1 , . . . , dh1 }, createfollowing voters:1. vertex vi , createpreference order:h3(h)vertex voters vi1 , . . . , vi 3 corresponding vi ,d1 dh1 p.2. edge ej = {vi1 , vi2 }, create corresponding edge voter wj preferenceorder:p d1 dh1 .3. Create 2 h2 + (n0 2h) h3 m0 dummy voters, preference order:p d1 dh1 .edge ej = {vi1 , vi2 }, construct 2h3shift actions, denoted( h)(h)fe1j ,vi , . . . , fej3,vi1 fe1j ,vi , . . . , fej3,vi2 ,21h3], (a) fezj ,vi effect +1 viz1 effect 1 wj , (b)1fezj ,vi effect +1 viz2 effect 1 wj . Finally, set budget B := 2 h2 h3 .2completes construction. easy see computable polynomial timeparameterized reduction.Observe that, initially, edge voters dummyvoters preferp, vertexhh0voters prefer d1 . Therefore, initial score p 2 2 + (n 2h) 3 , initial scored1 n0 h3 . assume, without loss generality, means d1winner election (instances satisfying assumption solved constanttime).remains show constructed instance contains successful set shift actions F 0 size h (G, h) contains clique size B. generalz [638fiCombinatorial Shift Briberyidea choose shift actions corresponding edges connecting nodesh-size clique, ensure p becomes preferred candidate h h3additional vertex voters, making d1 preferred candidate h2 additionaledge voters.Formally, part, let H V (G) set h vertices forming clique letE 0 E(G) set edges connecting vertices H. choose following setshift actions:h00zz]}.F = {fej ,vi , fej ,vi | ej = {vi1 , vi2 } E , z [123show F 0 successful setshift actions. end, observe vertexhz0voter vi vi V z [ 3 ], candidate p shifted h 1 positions forward, thereforep becomes preferred candidate voters. means h h3 additional vertexvoters prefer p (and, thus, prefer d1 anymore). Furthermore, p shifted backwardsvoters {wj | ej E 0 }, is, d1 becomes preferred candidatehh02 edge voters p remains preferred candidate 2 edge voters. Thus,p tie winners.part, let F 0 successful set shift actions.p winnermakeelection, p must shifted top position least h h3 h2 vertex voters (notype voters affected positively). pigeonholeprinciple, vertexhvoters correspond least h different vertices (there 3 voters correspondingvertex). effect, least h2 edge voters must effected negatively d1 becomespreferred candidate.Thus, make p win election p must shiftedtopposition least h h3 vertex voters. implies |F 0 | (h1)h h3 = 2 h2 h3 = Band, hence, |F 0 | = B. Itfollows p shifted backwards making d1 preferredcandidate exactly h2 edge voters p must shifted top positionexactly h h3 vertex voters corresponding exactly h different vertices. construction,implies h vertices form clique, done.Borda Rule (+1, 1)-Shift Actions. Borda rule, reduction is,again, bit involved, main idea Plurality rule.Let (G, h) instance Clique graph G n0 := |V (G)| vertices0:= |E(G)| edges. construct Borda-Combinatorial Shift Bribery instancefollows. set candidates {p, d} D, := {d1 , . . . , dh1 }, createfollowing voters:(h)1. vertex vi , create h3 vertex voters vi1 , . . . , vi 3 corresponding vi ,preference order:p.2. edge ej = {vi1 , vi2 }, create corresponding edge voter wj preferenceorder:d1 dh2 p dh1 .hh 20 h002 + (n 3 + )(h 1) ( 3 h )3. Let L :=. Without loss generality,h1assume L integer (this requires simple modifications input clique639fiBredereck, Faliszewski, Niedermeier, & Talmoninstance only). create L dummy voters, preference order:p dh1 d1 .edge ej = {vi1 , vi2 }, construct 2h3shift actions, denoted(h)( h)fe1j ,vi , . . . , fej3,vi1 fe1j ,vi , . . . , fej3,vi2 ,12h3], (a) fezj ,vi effect +1 viz1 effect 1 wj , (b)1fezj ,vi effect +1 viz2 effect 1 wj . Finally, set budget B := 2 h2 h3 .2completes construction. easy see computable polynomial time.proof correctness follows lines proof correctness Pluralityrule and, thus, omitted.z [Appendix C. Proof Theorem 9Theorem 9. Borda-Combinatorial Shift Bribery W[1]-hard respect number n voters, even (+1, 1)-shift actions budget constraints.Proof. reduce following W[1]-hard problem (Mathieson & Szeider, 2012, Lemma3.2).Strongly Regular Multicolored CliqueInput: Two integers, h, undirected graph G = (V, E),vertex one h colors [h], vertex adjacent exactlyvertices color different own.Question: exist clique size h containing one vertexcolor class?Given instance Strongly Regular Multicolored Clique, constructinstance Combinatorial Shift Bribery, Borda rule. general ideareduction follows. set important candidates consists preferredcandidate p candidates correspond edges. technical reasons,edge e = {v, v 0 }, introduce two candidates, e1 e2 ; one associatedtouching vertex v associated touching vertex v 0 . (In fact,introduce edge candidates vertex candidates, useensure correct structure election appropriate bribery behavior.) build twogroups voters, vertex-selecting voters edge-electing voters. first groupimplements picking vertices clique (one vertex color), secondgroup implements picking edges (one edge pair colors). ensure givenset shift actions chance successful, must hold h verticesh2 edges picked. Importantly, holds even unbribed election.make sure p wins election picked voters edges correspond clique (with vertices color). end, define voterstwo numbers, , that:640fiCombinatorial Shift Bribery1. h vertices picked vertex-selecting voters, different color.vertex-selecting voters give points edge candidate associatedtouching one selected vertices, + 1 points edge candidates.means picking vertex decrease score edge candidatesedges touch vertex.2. h2 edges picked edge-selecting voters, one edge pair colors.edge-selecting voters give + 1 points edge candidate correspondspicked edge, points remaining edge candidates. means that,picking edge, increase score candidates corresponding it.3. Candidate p gets + + 1 points, irrespective shift actions apply.Note unbribed election every candidate gets + + 2 points palways gets + + 1 points. Thus challenge ensure every candidate gets+ + 1 points. description, possible pick verticesedges correspond size h clique (of vertices different colors). Indeed,selected edge e touch two selected vertices, e1 e2 would receive+ 1 points edge-selecting candidates least one would receive + 1points vertex-selecting voters. effect, p would winner.Without loss generality, assume edges vertices selected unbribedelection form clique (otherwise would trivial solution inputproblem could output fixed yes-instances Borda-Combinatorial ShiftBribery).Construction. formally describe reduction, give exampleapplying simple instance, finally show correctness reduction.illustrate aspects correctness proof using example.Candidates. set candidates somewhat involved. important candidatespreferred candidate p sets edge candidates, E1 E2 , defined below.Let E(G) = {e1 , . . . , e } set edges graph G. create two edge-candidatesets: E 1 = {e11 , . . . , e1 } E 2 = {e21 , . . . , e2 }. [h], let ni numbervertices G color let V = {v1i , . . . , vni } set vertices.color vertex vji V , define neighborhood vji follows:0N (vji ) := {e1` | e` = {vji , vji 0 } E < i0 }0{e2` | e` = {vji , vji 0 } E > i0 }.(This, perhaps bit strange way using color numbers pick edge candidates eitherE 1 E 2 , implementing fact edge e E(G) two candidates, e1e2 , associated touching different endpoints e.)technical reasons need candidates follows. adjust scorescandidates, introduce single dummy candidate d. create twocandidate sets E 0 = {e01 , . . . , e0 } E 3 = {e31 , . . . , e3 } act guardsedge-selecting voters. V create two candidate sets U := {uij | vji V }641fiBredereck, Faliszewski, Niedermeier, & TalmonU 0i = {u0ij | vji V } U := 1ih U U 0 := 1ih U 0i act guardsvertex-selecting voters.final set candidates C := U U 0 E 0 E 1 E 2 E 3 {p, d}.Vertex-Selecting Voters. describe vertex-selecting voters. colorvertex vji , define following parts preference orders (for j = 1, assumeuij1 u0ij1 uini u0ini respectively):A(vji ) : uij N (vji ) u0ij ,B(vji ) : uij1 N (vji ) u0ij1 .color create three pairs voters. voters first pair, wi wi0 ,following preference orders:wi : p A(v1i ) A(v2i ) A(v3i ) A(vni ) Ri ,wi0 : Ri B(v1i ) B(vni ) B(vni 1 ) B(v2i ) p,Ri set remaining candidates, is, Ri := C \ ({p} U U 0i N (v1i )N (vni )). voters second pair, qi qi0 , preference ordersreverse wi reverse wi0 , respectively. Finally, voters last pair, qiqi0 , preference orders:qi : C \ ({d} N (v1i )) N (v1i ),qi0 : N (v1i ) C \ ({d} N (v1i )) d.effect, first two pairs voters jointly give 2(|C| 1) points candidates.last pair gives |C| 1 points candidates N (v1i ) |C| pointscandidates (except d, receives less |C| 1 points).Let := h(2(|C|1)+|C|)1. Altogether, vertex-selecting voters give followingscores candidates: candidates N (v11 )N (v12 ) N (v1h ) receive pointscandidates, except d, receive + 1 points (d receives less points). Thus,unbribed election v11 , . . . , v1h selected vertices.color i, introduce (ni 1) ((h 1) + 2) shift actions effect 1voter wi effect +1 voter wi0 . understand number shiftactions comes from, note that: (1) vertex vji , |N (vji )| = (h 1) (eachvertex connected vertices color different own), (2) A(vji )B(vji ) candidates N (vji ) surrounded two vertex candidates, (3)integer, 1 ni 1, applying t((h 1) + 2) shift actions effectcandidates N (v1i ) gain one point (i.e., v1i ceases selected), candidates) lose one point (i.e., vN (vt+1t+1 becomes selected), candidate changesscore (later argue applying numbers shift actions multiples((h 1) + 2) cannot ensure ps victory).642fiCombinatorial Shift BriberyEdge-Selecting Voters. edge-selecting voters, need following additionalnotation. Let Ex,y denote set candidates representing edges verticescolor x color y, is,q{0,1,2,3}Ex,y := {e`| e` = {vjx , vjy0 } E}.write nx,y denote number edges vertices color x color y.idx,yz refer index z-th edge vertices color x y. example,e3 , e7 e57 three edges vertices colors 1 2, n1,2 = 3,1,21,2id1,21 = 3, id2 = 7, id3 = 57.pair {x, y} distinct colors edge eidx,y, introduce followingjx,yparts preference orders (for j = nx,y , assume idj+1 = idx,y1 ):R(eidx,y) : e0idx,y e1idx,y e2idx,y e3idx,y ,j):S(eidx,yjjjje0idx,yj+1e1idx,yje2idx,yjje3idx,y .j+1pair {x, y} distinct colors introduce three pairs voters. voters0 , following preference orders:first pair, wx,y wx,ywx,y : R(eidx,y) p R(eidx,y) R(eidx,y) R(eidx,y) Rx,y ,nx,y1230) S(eidx,y) S(eidx,y) S(eidx,y) p,wx,y: Rx,y S(eidx,ynx,yn121x,yRx,y set remaining candidates, is, Rx,y := C \ ({p} Ex,y ).0 , preference orders reverse wvoters second pair, qx,y qx,yx,y00 ,reverse wx,y , respectively. Finally, voters last pair, qx,y qx,yfollowing preference orders:qx,y : e1idx,y e2idx,y C \ ({d, e1idx,y , e2idx,y }),111112021qx,y : C \ ({d, eidx,y , eidx,y }) eidx,y eidx,y d.1111first two pairs voters jointly give 2(|C| 1) points candidates.last pair gives |C| points e1idx,y e2idx,y , |C| 1 points candidates11(except d, receivesless|C|1points).Let := 3 h2 (|C| 1). Altogether, pair distinct colors {x, y}, edgeselecting voters give + 1 points candidates e1idx,y e2idx,y . candidates receive11points (except d, receives less points). Thus unbribed electionselected edges exactly first edges pair colors (that is, edgesform eidx,y, pair distinct colors {x, y}).1pair {x, y} distinct colors, create 4(nx,y 1) shift actions effect 10 . intuition behind shift actions similarvoter wx,y effect +1 voter wx,ycase vertex-selecting voters. make following observations: (1)edge eidx,y four candidates listed R(eidx,y ) four candidates listed S(eidx,y ),```(2) integer, 1 nx,y 1, apply 4t shift actions, candidates643fiBredereck, Faliszewski, Niedermeier, & Talmonv11v12e3e1v21e4e2v22e5e6v13v23V 1 = {v11 , v21 }, V 2 = {v12 , v22 }, V 3 = {v13 , v23 }, h = 3, = 1Figure 2: 3-colored graph six vertices vertex adjacent one vertexcolor classes V 1 , V 2 V 3 , own.ceases selected), candidates e1idx,y e2idx,ye1idx,y e2idx,y lose one point (edge eidx,y11t+11t+1gain one point (edge eidx,ybecomes selected), scores candidates remaint+1unchanged (we later argue apply number shift actionsmultiple 4, p certainly winner resulting election).conclude construction, set budget B := (that is, use manyshift actions like). easy verify reduction computable polynomialtime introduce number voters function h (thus,parameterized reduction). proving correctness construction, considerfollowing example (we refer correctness proof well).Example 5. Consider Strongly Regular Multicolored Clique instance (d, h, G)= 1, h = 3, graph G Figure 2. construction produces following setcandidates:C := U U 0 E 0 E 1 E 2 E 3 {p, d},01 02 02 03 03U = {u11 , u12 , u21 , u22 , u31 , u32 }, U 0 = {u011 , u2 , u1 , u2 , u1 , u2 }E = {ei1 , ei2 , . . . , ei6 }, 0 3.Furthermore, have:N (v11 ) := {e11 , e12 },N (v21 ) := {e13 , e16 },N (v12 ) := {e23 , e14 },N (v22 ) := {e21 , e15 },N (v13 ) := {e22 , e25 },N (v23 ) := {e24 , e26 }.644fiCombinatorial Shift Briberyvertex-selecting group voters, create following voters. colori, create two voters wi wi0 :111101w1 : p u11 e11 e12 u011 u2 e3 e6 u2 R ,11101w10 : R1 u12 e11 e12 u012 u1 e3 e6 u1 p,222102w2 : p u21 e23 e14 u021 u2 e1 e5 u2 R ,22102w20 : R2 u22 e23 e14 u022 u1 e1 e5 u1 p,332203w3 : p u31 e22 e25 u031 u2 e4 e6 u2 R ,32203w30 : R3 u32 e22 e25 u032 u1 e4 e6 u1 p,Ri := C \ ({p} U U 0i N (v1i ) N (vni )), 1 3. votersadd voter reversed preferences. (This means that, far, candidates obtaintotal score.) finish group voters creating color two voters,qi qi0 , preference orders:qi : C \ ({d} N (v1i )) N (v1i ),qi0 : N (v1i ) C \ ({d} N (v1i )) d.ensures color i, candidates N (v1i ) get points,candidates get + 1 points (except gets points). create 4 shift actionseffect 1 voter wi effect +1 voter wi0 .edge-selecting second group voters, recall Ex,y denotes set candidates representing edges vertices color x color y. Specifically, have:E1,2 :={e01 , e11 , e21 , e31 e03 , e13 , e23 , e33 },E1,3 :={e02 , e12 , e22 , e32 e06 , e16 , e26 , e36 },E2,3 :={e04 , e14 , e24 , e34 e05 , e15 , e25 , e35 }.0 , follows:pair {x, y} distinct colors create two voters, wx,y wx,yw1,2 : e01 e11 e21 e31 p e03 e13 e23 e33 R1,2 ,0w1,2: R1,2 e01 e13 e23 e31 e03 e11 e21 e33 p,w1,3 : e02 e12 e22 e32 p e06 e16 e26 e36 R1,3 ,0w1,3: R1,3 e02 e16 e26 e32 e06 e12 e22 e36 p,w2,3 : e04 e14 e24 e34 p e05 e15 e25 e35 R2,3 ,0w2,3: R2,3 e04 e15 e25 e34 e05 e14 e24 e35 p,Rx,y := C \ ({p} E[x, y]). voters add voter reversed0preferences. Further, pair {x, y} distinct colors, add two voters qx,y qx,y645fiBredereck, Faliszewski, Niedermeier, & Talmonfollows:qx,y : e1idx,y e2idx,y C \ ({d, e1idx,y , e2idx,y }),111112120qx,y : C \ ({d, eidx,y , eidx,y }) eidx,y eidx,y d.1111Altogether, pair {x, y} distinct colors, candidates e1idx,y e2idx,y get +1 points11candidates get points (except d, gets less points). pair {x, y}distinct colors, create 4 shift actions effect 1 voter wx,y effect +10 .voter wx,y4Properties Construction. discuss several properties construction.properties play significant rule showing correctness reduction.illustrate arguments, come back example time time. beginlooking scores candidates.Lemma 1. following claims hold:1. unbribed election, every candidate receives + + 2 points everycandidate {p} U U 0 E 0 E 3 receives exactly + + 1.2. every bribed election, score p exactly + + 1.3. applying successful set shift actions, score p + + 1scores candidates + + 1.Proof. easy see first claim holds based discussion givethroughout construction. second claim holds (a) applying every shiftaction decreases one score p one vote increases one another vote(there sufficiently shift actions whole instance applying shiftaction always moves p within two votes shift action acts). last claimfollows directly second one.(Lemma)Let us consider process selecting vertices. description vertexselecting voters said that, initially, color vertex v1i selected,integer t, 1 ni 1, apply t((h 1) + 2) shift actions affect voters wi wi0 ,v1i ceases selected vt+1becomes selected. argue applynumber shift actions divisible ((h 1) + 2), p winnerresulting election.see case, recall preference orders voter wi wi0exactly (h 1) candidates E 1 E 2 pair candidates {uij , u0ij }.Furthermore, p passes candidate uij preference order voter wi (increasinguij score one), must also pass candidate uij preference order voter wi0(decreasing uij score one). Otherwise, uij would end score + + 2 and,1, p would winner (there possibilities influence score uijshifting p preference lists wi wi0 ). Hence, p also passes candidate u0ijcandidates uij u0ij preference lists wi wi0 . This, however,means p winner election, number applied shift actions646fiCombinatorial Shift BriberyUnbribed voters w2 w20 :222102w2 : p u21 e23 e14 u021 u2 e1 e5 u2 R22102w20 : R2 u22 e23 e14 u022 u1 e1 e5 u1 pApplying two shift actions effect -1 w2 +1 w20 :+1 +1222102w2 : u21 e23 p e14 u021 u2 e1 e5 u2 R2-1-12 e2 p e1 u02w20 : R2 u22 e23 e14 u02u21151+2Applying (h 1) + 2 = 4 shift actions effect -1 w2 +1 w20 :+1 +1 +1 +1222102w2 : u21 e23 e14 u021 p u2 e1 e5 u2 R4-1-1-1-12022 e2 e1 u02w2 : R u2 e23 e14 u02pu21151+4Figure 3: Illustration bribery actions affecting first voter group running example(Example 5). Note that, unbribed election, every candidate U U 0 obtains+ + 1 points total. color one type shift actionsaffects voter wi wi0 : shift actions effect 1 voter wi effect +1voter wi0 . shift action affect voter first group. Applyingmultiple ((h 1) + 2) shift actions effect 1 voter wi effect +1 voter wi0ensures candidates U U 0i receive + + 1 points total, whereasapplying number shift actions implies candidate Ureceives + + 2 points and, hence, p cannot win. illustrate color 2running example.effects voters wi wi0 multiple ((h 1) + 2) (p passes candidate uij ,candidate u0ij , h candidates between). Figure 3 provides illustrationreasoning.next discuss selecting edges. case vertex-selecting voters, description construction argued (a) initially pair {x, y} distinctcolors, edge eidx,yselected (b) applying 4t, 1 nx,y 1, shift actions10 , e x,y ceases selected e x,y becomes selected.affect voters wx,y wx,yid1idt+1argue used number shift actions multiple four,p certainly would winner election.0see case, note designed preference orders wx,y wx,ycandidates e0idx,y e3idx,y , j {2, . . . , nx,y }, follow p vote wx,yjj0 . effect, apply shift action affectsorder precede p wx,y647fiBredereck, Faliszewski, Niedermeier, & Talmon0 :Unbribed voters w2,3 w2,3w2,3 : e04 e14 e24 e34 p e05 e15 e25 e35 R2,30 :R2,3 e0 e1 e2 e3 e0 e1 e2 e3 pw2,3455454450 :Applying two shift actions effect -1 w2,3 +1 w2,3+1 +1w2,3 : e04 e14 e24 e34 e05 e15 p e25 e35 R2,32-1-102,30123012w2,3 :R e4 e5 e5 e4 e5 e4 p e4 e35+20 :Applying four shift actions effect -1 w2,3 +1 w2,3+1 +1 +1 +1w2,3 : e04 e14 e24 e34 e05 e15 e25 e35 p R2,34-1-1-1-10 :R2,3 e0 e1 e2 e3 p e0 e1 e2 e3w2,345545445+4Figure 4: Illustration bribery actions affecting second voter group runningexample. Note unbribed election, every candidate E 0 E 4 obtains ++ 1 points total. pair colors x one type shift0 : shift actions effect 1 voter wactions affects voter wx,y wx,yx,y0 . shift action affect voter secondeffect +1 voter wx,ygroup. Applying multiple 4 shift actions effect 1 voter wx,y effect +10ensures candidates E 0 E 4 receive + + 1 pointsvoter wx,yvoters, whereas applying number shift actions impliescandidate E 0 receives + + 2 points and, hence, p cannot win. illustratecolor pair 2 3 running example.0voters wx,y wx,ynumber times multiple four, onecandidates obtains + + 2 points. Since way affect scorecandidates, Lemma 1, case p cannot winner. illustrate effectFigure 4.Solution Example 5. complete correctness proof, let us illustratesolution example.unbribed election selects vertex v11 , v12 , v13 edges e1 , e2 e4 . Hence,example, candidate e24 receives + +2 points p (who receives + +1 points)winner.applying four shift actions effect 1 w2 effect +1 w20 , select v22instead v12 vertex color 2 clique (as depicted bottom Figure 3).648fiCombinatorial Shift Bribery0 , select eapplying four shift actions effect 1 w2,3 effect +1 w2,35instead e4 edge color 2 color 3 clique (as depictedbottom Figure 4). Now, candidate {e11 , e12 , e21 , e22 , e15 , e25 } receives + 1 pointsedge-selecting voters, points vertex-selecting voters. Everycandidate receives + 1 points vertex-selecting voterspoints edge-selecting voters. Hence, p (with + + 1 points) winner.solution corresponds left 3-colored triangle Figure 2.Correctness. remains show successful set shift actionsconstructed Borda-Combinatorial Shift Bribery instanceh-colored clique graph G.part, assume h-colored clique H V (G). Without lossgenerality, let H = {vz11 , . . . , vzhh } let EH := {{v, v 0 } | v, v 0 H}. Furthermore, let zx,ydenote index edge Ex,y representing edge EH vertexEH .color x vertex color y. is, zx,y = j eidx,yjeasy verify following set shift actions successful:1. color [h], include (zi 1)((h 1) + 2) shift actions effectsvoters wi wi0 .2. pair {x, y} distinct colors, include 4(zx,y 1) shift actions effects0 .voters wx,y wx,ywords, select vertices edges corresponding clique. effect,scores candidates + + 1 (except d, receives lower score). pamong tied winners.part, assume successful set shift actions considerelection applying shift actions. construction, know edge-selectingvoters pick exactly one edge pair distinct colors. Hence graph inducededges contains vertices h different colors. graph contains hvertices, graph must h-colored clique (this graph cannot contain fewerh vertices). sake contradiction, let us assume graph containsh vertices. Thus two selected edges, ej ej 0 , incident two different vertices,vi ej vi0 ej 0 , color. construction (and way vertex-selectingvoters work), least one sets N (vi ) N (vi0 ) candidates set receive+ 1 points vertex-selecting voters. However, since ej ej 0 selectededge-selecting voters, voters give + 1 points candidates e1j , e2j ,e1j 0 , e2j 0 . Hence, least one candidates receives + + 2 points total and,Lemma 1, p winner. contradiction, graph inducedselected edges must h-colored clique.ReferencesBartholdi, III, J. J., Tovey, C. A., & Trick, M. A. (1992). hard controlelection. Mathematical Computer Modelling, 16 (89), 2740.649fiBredereck, Faliszewski, Niedermeier, & TalmonBaumeister, D., Faliszewski, P., Lang, J., & Rothe, J. (2012). Campaigns lazy voters:Truncated ballots. Proceedings 11th International Conference AutonomousAgents Multiagent Systems (AAMAS 12), pp. 577584. IFAAMAS.Betzler, N., Bredereck, R., Chen, J., & Niedermeier, R. (2012). Studies computationalaspects votinga parameterized complexity perspective. Multivariate Algorithmic Revolution Beyond, Vol. 7370 LNCS, pp. 318363. Springer.Binkele-Raible, D., Erdelyi, G., Fernau, H., Goldsmith, J., Mattei, N., & Rothe, J. (2014).complexity probabilistic lobbying. Discrete Optimization, 11, 121.Boutilier, C., Brafman, R. I., Hoos, C. D. H. H., & Poole, D. (2004). CP-nets: toolrepresenting reasoning conditional ceteris paribus preference statements.Journal Artificial Intelligence Research, 21, 135191.Brandt, F., Harrenstein, P., Kardel, K., & Seedig, H. G. (2013). takes few:hardness voting constant number agents. Proceedings 12th International Conference Autonomous Agents Multiagent Systems (AAMAS 13),pp. 375382. IFAAMAS.Bredereck, R., Chen, J., Faliszewski, P., Nichterlein, A., & Niedermeier, R. (2014a). Pricesmatter parameterized complexity shift bribery. Proceedings 28thAAAI Conference Artificial Intelligence (AAAI 14), pp. 13981404. AAAI Press.Bredereck, R., Chen, J., Hartung, S., Kratsch, S., Niedermeier, R., Suchy, O., & Woeginger,G. (2014b). multivariate complexity analysis lobbying multiple referenda.Journal Artificial Intelligence Research, 50, 409446.Bredereck, R., Faliszewski, P., Niedermeier, R., Skowron, P., & Talmon, N. (2015a). Elections candidates: Prices, weights, covering problems. FourthInternational Conference Algorithmic Decision Theory (ADT 2015), Vol. 9346LNCS, pp. 414431. Springer.Bredereck, R., Faliszewski, P., Niedermeier, R., & Talmon, N. (2015b). Large-scale election campaigns: Combinatorial shift bribery. Proceedings 14th InternationalConference Autonomous Agents Multiagent Systems (AAMAS15), pp. 6775.Bredereck, R., Faliszewski, P., Niedermeier, R., & Talmon, N. (2016). Complexity shiftbribery committee elections. Proceedings Twenty-Ninth AAAI ConferenceArtificial Intelligence (AAAI 16).Bulteau, L., Chen, J., Faliszewski, P., Niedermeier, R., & Talmon, N. (2015). Combinatorialvoter control elections. Theoretical Computer Science, 589, 99120.Cary, D. (2011). Estimating margin victory instant-runoff voting. Presented2011 Electronic Voting Technology Workshop/Workshop Trustworthy Elections.Chen, J., Faliszewski, P., Niedermeier, R., & Talmon, N. (2015). Elections voters: Candidate control easy. Proceedings 29th AAAI ConferenceArtificial Intelligence (AAAI 15), pp. 20452051.Christian, R., Fellows, M. R., Rosamond, F. A., & Slinko, A. (2007). complexitylobbying multiple referenda. Review Economic Design, 11 (3), 217224.650fiCombinatorial Shift BriberyConitzer, V., Lang, J., & Xia, L. (2009). hard control sequential elections viaagenda?. Proceedings 21st International Joint Conference ArtificialIntelligence (IJCAI 10), pp. 103108. AAAI Press.Dorn, B., & Schlotter, I. (2012). Multivariate complexity analysis swap bribery. Algorithmica, 64 (1), 126151.Downey, R. G., & Fellows, M. R. (2013). Fundamentals Parameterized Complexity.Springer.Elkind, E., & Faliszewski, P. (2010). Approximation algorithms campaign management.Proceedings 6th International Workshop Internet Network Economics(WINE 10), Vol. 6484 LNCS, pp. 473482. Springer.Elkind, E., Faliszewski, P., & Slinko, A. (2009). Swap bribery. Proceedings 2ndInternational Symposium Algorithmic Game Theory (SAGT 09), Vol. 5814LNCS, pp. 299310. Springer.Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2010). Using complexity protectelections. Communications ACM, 53 (11), 7482.Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. A. (2009a). hard briberyelections?. Journal Artificial Intelligence Research, 35, 485532.Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L. A., & Rothe, J. (2009b). LlullCopeland voting computationally resist bribery constructive control. JournalArtificial Intelligence Research, 35, 275341.Faliszewski, P., Reisch, Y., Rothe, J., & Schend, L. (2014). Complexity manipulation,bribery, campaign management Bucklin Fallback voting. Proceedings13th International Conference Autonomous Agents Multiagent Systems(AAMAS 14), pp. 13571358. IFAAMAS.Faliszewski, P., & Rothe, J. (2015). Control bribery voting. Brandt, F., Conitzer,V., Endriss, U., Lang, J., & Procaccia, A. D. (Eds.), Handbook ComputationalSocial Choice, chap. 7. Cambridge University Press.Flum, J., & Grohe, M. (2006). Parameterized Complexity Theory. Springer.Gabow, H. N. (1983). efficient reduction technique degree-constrained subgraphbidirected network flow problems. Proceedings 15th Annual ACM SymposiumTheory Computing (STOC 83), pp. 448456. ACM.Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide TheoryNP-Completeness. Freeman.Hazon, N., Lin, R., & Kraus, S. (2013). change groups collective decision?.Proceedings 23rd International Joint Conference Artificial Intelligence(IJCAI 13), pp. 198205. AAAI Press.Hulett, H., Will, T. G., & Woeginger, G. J. (2008). Multigraph realizations degreesequences: Maximization easy, minimization hard. Operations Research Letters,36 (5), 594596.651fiBredereck, Faliszewski, Niedermeier, & TalmonLang, J., & Xia, L. (2015). Voting combinatorial domains. Brandt, F., Conitzer, V.,Endriss, U., Lang, J., & Procaccia, A. D. (Eds.), Handbook Computational SocialChoice, chap. 9. Cambridge University Press.Lenstra, H. W. (1983). Integer programming fixed number variables. MathematicsOperations Research, 8 (4), 538548.Magrino, T., Rivest, R., Shen, E., & Wagner, D. (2011). Computing margin victory IRV elections. Presented 2011 Electronic Voting Technology Workshop/Workshop Trustworthy Elections.Mathieson, L., & Szeider, S. (2012). Editing graphs satisfy degree constraints: parameterized approach. Journal Computer System Sciences, 78 (1), 179191.Mattei, N., Goldsmith, J., & Klapper, A. (2012a). complexity bribery manipulation tournaments uncertain information. Proceedings 25th International Florida Artificial Intelligence Research Society Conference (FLAIRS 12),pp. 549554. AAAI Press.Mattei, N., Pini, M., Rossi, F., & Venable, K. (2012b). Bribery voting combinatorialdomains easy. Proceedings 11th International Conference AutonomousAgents Multiagent Systems (AAMAS 12), pp. 14071408. IFAAMAS.Niedermeier, R. (2006). Invitation Fixed-Parameter Algorithms. Oxford University Press.Obraztsova, S., & Elkind, E. (2011). complexity voting manipulationrandomized tie-breaking. Proceedings 22nd International Joint ConferenceArtificial Intelligence (IJCAI 11), pp. 319324. AAAI Press.Obraztsova, S., Elkind, E., & Hazon, N. (2011). Ties matter: Complexity voting manipulation revisited. Proceedings 10th International Conference AutonomousAgents Multiagent Systems (AAMAS 11), pp. 7178.Rajagopalan, S., & Vazirani, V. V. (1998). Primal-dual RNC approximation algorithmsset cover covering integer programs. SIAM Journal Computing, 28 (2),525540.Reisch, Y., Rothe, J., & Schend, L. (2014). margin victory Schulze, Cup,Copeland elections: Complexity regular exact variants. ProceedingsSeventh European Starting AI Researcher Symposium (STAIRS-2014), pp. 250259.IOS Press.Schlotter, I., Faliszewski, P., & Elkind, E. (2011). Campaign management approvaldriven voting rules. Proceedings 25th AAAI Conference Artificial Intelligence (AAAI 11), pp. 726731. AAAI Press.Xia, L. (2012). Computing margin victory various voting rules. Proceedings13th ACM Conference Electronic Commerce (EC 12), pp. 982999. ACMPress.652fiJournal Artificial Intelligence Research 55 (2016) 10251058Submitted 07/15; published 04/16Semi-supervised Learning Induced Word SensesState Art Word Sense DisambiguationOsman Baskayaobaskaya@ku.edu.trDepartment Computer Sciences EngineeringKoc UniversityIstanbul, TurkeyDavid Jurgensjurgens@stanford.eduDepartment Computer ScienceStanford UniversityStanford, CA, USAAbstractWord Sense Disambiguation (WSD) aims determine meaning word context, successful approaches known benefit many applications Natural Language Processing. Although supervised learning shown provide superior WSDperformance, current sense-annotated corpora contain sufficient number instances per word type train supervised systems words. unsupervisedtechniques proposed overcome data sparsity problem, techniquesoutperformed supervised methods. paper, propose new approachbuilding semi-supervised WSD systems combines small amount sense-annotateddata information Word Sense Induction, fully-unsupervised techniqueautomatically learns different senses word based used. three experiments, show sense induction models may effectively combined ultimatelyproduce high-performance semi-supervised WSD systems exceed performancestate-of-the-art supervised WSD techniques trained sense-annotated data.anticipate results released software also benefit evaluation practicessense induction systems working low-resource languages demonstratingquickly produce accurate WSD systems minimal annotation effort.1. IntroductionWord Sense Disambiguation (WSD) identifies particular meaning word context,whether bass refers fish instrument. Correctly performing WSD groundsambiguous natural language concrete semantic representation, numerousbenefits downstream applications, knowledge extraction (Navigli & Ponzetto,2010; Hartmann, Gurevych, & Lap, 2013) machine translation (Carpuat & Wu, 2007;Chan, Ng, & Chiang, 2007). Traditionally, supervised approaches WSD offeredsuperior performance (Kilgarriff & Rosenzweig, 2000; Mihalcea, Chklovski, & Kilgarriff,2004; Agirre & Soroa, 2007; Navigli, 2009). However, major limitation building highperformance supervised WSD systems limited amount sense-annotatedcorpora training models word types, largest corpora containinghundreds thousands annotated tokens (Petrolito & Bond, 2014). result,unsupervised WSD techniques proposed fill need high-coverage systemsc2016AI Access Foundation. rights reserved.fiBaskaya & Jurgens(Yarowsky, 1995; Agirre et al., 2014; Moro et al., 2014). techniques capabledisambiguating many word types, surpassed accuracy supervisedsystems take advantage sense-annotated data available.alternative approach supervised WSD build semi-supervised approaches usingWord Sense Induction (WSI), often referred Word Sense Induction Disambiguation (WSID) models (Agirre et al., 2006). Word Sense Induction fully unsupervisedtechnique examines contexts word used order learn (a) wordsdifferent meanings, referred induced senses, (b) disambiguate newusage word instance one induced senses. induced senses learnedWSI method provide key link building WSID system: labeling usagesinduced senses reference sense inventory WordNet (Fellbaum,1998) OntoNotes (Hovy et al., 2006), mapping function learned effectivelytransform annotation using induced senses annotation using senses reference inventory. WSI model constructed large corpus examples,induced senses become associated many contextually-disambiguating features.result, features associated induced sense used disambiguation,are, proxy sense mapping, used features disambiguatingreference senses despite contextual features potentially never seendata annotated references senses. Thus, close correspondence existsinduced reference senses, robust WSID system created ultimately abledisambiguate usages contexts unlike seen data annotated reference sensesvirtue features associated induced senses. contrast, discriminatorycapabilities supervised WSD system limited features observed training data. Hence, ability WSID system leverage induced sense annotationspotentially remove knowledge acquisition bottleneck requiring significant amountssense-annotated data (Gale, Church, & Yarowsky, 1992).Despite potential WSID, little analysis done constructmodels maximize performance. Instead, WSID systems primarilyevaluated within SemEval tasks focusing word sense induction (Agirre & Soroa,2007; Manandhar, Klapaftis, Dligach, & Pradhan, 2010; Jurgens & Klapaftis, 2013).WSID performance evaluations promising, three important open questions remain.First, current evaluations, WSID systems used technique Agirre et al.(2006) converting induced sense annotations reference inventory. However,performance impact process measured, alternative methodstested. Second, current WSID evaluations controlled distributionfrequency senses training test data, significantly affect performanceexpected generalizability results (Agirre & Martinez, 2000); settingsraise question much current systems performances attributable easedisambiguating due test datas sense distribution. Third, despite potentialadvantages WSID low-resource languages, study directly compared WSIDsupervised WSD systems equal conditions test whether one setuppreferred based amount sense-annotated data available.Addressing questions previously hindered lack large sense-annotateddata set. However, overcome limitation using recent resource PilehvarNavigli (2013), approximates polysemous nouns WordNet using pseudowords1026fiSemi-supervised Learning Induced Word Senses State Art WSDaccurately model difficulty disambiguating nouns. Here, pseudowordmade two monosemous lemmas, referred pseudosenses,models particular sense word. example, disemous noun pic two WordNetsenses: (1) motion picture, (2) photograph. two senses representedmonosemous nouns movie photo, respectively. simulate sense-annotated data,occurrences pseudowords pseudosenses replaced unique token (e.g., replacingusages movie photo token denoting pseudoword); then, analogousdisambiguation task, WSD system shown occurrence pseudoword askeddecide pseudosense originally present. Crucially, (1) pseudowordsapproximate real-world disambiguation difficulty (2) pseudosense-annotated dataeasily created sampling occurrences pseudosenses corpus, resource enables performing comprehensive evaluation WSID arbitrarily-large amountsannotated data direct generalizability real-world WSD performance (Pilehvar &Navigli, 2014).paper offers following four key contributions. First, provide comprehensive evaluation setting WSID tests systems millions instances two ordersmagnitude previous evaluations thereby providing statistically-robust resultsevaluated terms. Furthermore, evaluation setting uses high-quality pseudowordseffectively simulate properties WordNet senses, allows us preciselycontrol sense distribution test training data order measure effectperformance. Second, show method transforming induced sensesWordNet senses significant impact WSID performance, using appropriate method, WSID performance significantly outperforms formerly-competitive baselinesmultiple tests sets. Third, demonstrate combining WSI models ensembleWSID provides statistically-significant performance improvements pseudowordreal-world data. Fourth, direct comparisons state-of-the-art supervised WSD system, demonstrate ensemble WSID system outperforms supervised WSDfewer several hundred sense-annotated instances available, indicating WSIDindeed overcome knowledge acquisition bottleneck.results offer two important practical implications. researchers workinglow-resource languages, comparisons WSID supervised WSD demonstraterelatively small amount sense-annotated data needed state-of-theart performance WSD. Second, demonstrate current approach creatingWSID systems artificially masks true capabilities underlying WSI models,thus future evaluations WSID systems conducted SemEvalconsider using evaluation construction procedure described herein.2. Word Sense Induction Disambiguation SystemsWSID system consists two key components: WSI model function convertsmodels sense annotations another sense inventory, formalized Agirreet al. (2006). First, WSI model induces senses base corpus. Second, trainingcorpus labeled using induced senses WSI model sensesreference inventory. co-labeled corpus serves training data building classifierpredicts reference sense label given induced sense annotation.1027fiBaskaya & Jurgensconstructing WSID systems, two key questions examined: (1)impact sense mapping function WSID performance, (2) whether multiple WSImodels may effectively combined. Answering questions essential identifyingdegree performance WSID system due capabilitiesunderlying WSI model versus mapping process used create it. follows,first describe WSI models used paper illustrate learn senses.Then, formalize sense mapping function define range possibilitiesmay computed propose function used effectively combine WSImodels WSID ensemble.2.1 WSI ModelsMultiple techniques proposed effectively learn different meaningsword (Navigli, 2012), many approaches using either (a) graph-based representationswords semantic relationships (b) distributional approaches identifying regularities words contexts. Therefore, increase robustness results, four recentWSI methods selected experiments. Models balanced usinglexical distributions using graphs: AI-KU (Baskaya, Sert, Cirik, & Yuret, 2013)HDP (Lau, Cook, McCarthy, Newman, & Baldwin, 2012), use token statisticsinduce senses, Chinese Whispers (Biemann, 2006) SquaT (Di Marco & Navigli,2012), construct graphs induce senses. Following prior evaluations (Manandharet al., 2010; Jurgens, 2012; Jurgens & Klapaftis, 2013), disambiguation, allowmodels report multiple senses per context; setting, induced sense denotesprototypical meaning annotation represents much current usage resembles meanings. Following, summarize models induction disambiguationprocedures.2.1.1 AI-KUBaskaya et al. (2013) represent context target word using high probabilitylexical substitutes according statistical language model. language model builtidentify relative probabilities 4-gram sequences FastSubs (Yuret, 2012) applied identify words appear position target word context.example, one instance bass may substitutes fish, another instancemay guitar. instance represented 100 substitutes, sampledprobability distribution most-probable 100 substitutes instance; substitutes transformed vector representation, reflecting sampled frequencieseach. instance-substitute vectors projected lower dimensionality using S-CODE (Maron, Lamar, & Bienenstock, 2010). final S-CODE based vectorsclustered using k-means. Much like Schutze (1992), AI-KU requires specifying numberclusters ahead time, often setting k larger necessary number. However,determine number senses, AI-KU performs post-processing step remove clusterscontain instances, likely artifacts forcing k clustersnon-empty. remaining clusters treated senses word.1028fiSemi-supervised Learning Induced Word Senses State Art WSD2.1.2 HDPLau et al. (2012) propose system based Hierarchical Dirichlet Process (HDP)(Teh, Jordan, Beal, & Blei, 2006), nonparametric extention Latent Dirichlet allocation(Blei, Ng, & Jordan, 2003; Griffiths & Steyvers, 2004). HDP automatically infersnumber topics topics probability distribution generating tokenscorpus. sense induction, HDP model inferred contexts target word,produces distribution topics context. topic treated distinctsense word. Given new context word, HDP model used infertopic distribution, thereby identifying senses present. output, reportfull distribution senses context, weighted probabilities.2.1.3 Chinese Whispers (CW)Biemann (2006) proposes inducing senses using Chinese Whispers (CW), nonparametric graph clustering algorithm. CW form unsupervised label propagationvertex initially assigned unique label label propagation run eitherconvergence fixed number iterations completed. graph constructedlexically-associated terms, vertices assigned cluster typically form topicallyrelated groups. sense induction, graph constructed words associatedspecific term (e.g., computing statistical associations corpus) CWcompletes, vertex cluster considered features distinct sense term.CW, graphs constructed three steps. First, 2 association computedwords base corpus. Then, words associated words pseudosensesranked 2 1000 words largest 2 retained.retained neighbors, additional edges added 1000 neighbors highest 2 , excluding edges pseudosenses. sense features useddisambiguate new contexts computing overlap content words context.disambiguation, report senses containing least one word context, weightednumber matching features.2.1.4 SquaTNavigli Crisafulli (2010) construct co-occurrence graph, prunedinduce sense clusters. Term co-occurrences base corpus scored using Dice2c(w1 ,w2 )coefficient: two terms w1 , w2 , Dice(w1 , w2 ) = c(wc(w) frequency1 )+c(w2 )occurrence. Edges added graph Dice coefficient greaterthreshold . graph construction begins co-occurrences target termproceeds add edges newly-included neighbors. framework allowsmultiple pruning methods induction; adopt Squares pruning method,shown perform best. Simply, edges removed ratio observed potentialsquares (closed paths length 4) edge participates threshold .pruned, resulting disconnected components graph denote separate senses.efficiency, use noun, verb, adjective lemmas graphs. resultinggraph produces sets lemmas associated sense, sense disambiguation performedway Chinese Whispers.1029fiBaskaya & Jurgens2.2 Sense Mapping Functionsmapping function supervised classifier that, given annotation oneinduced senses, produces new sense annotation instance using sensedifferent inventory, induced senses essentially acting features. Agirre et al.(2006) proposed first mapping function based matrix multiplication,used 39 systems participating WSID shared task evaluations (Agirre & Soroa,2007; Manandhar et al., 2010; Jurgens & Klapaftis, 2013) many subsequent papersWSID (e.g., see Brody & Lapata, 2009; Klapaftis & Manandhar, 2010; Van de Cruys &Apidianaki, 2011; Lau et al., 2012; Wang, Bansal, Gimpel, Ziebart, & Yu, 2015). senseco-occurrence matrix computed training corpus, columns denotingn induced senses rows denoting reference senses; cell (i, j) recordstwo senses co-occurrence frequencies. sense mapping, induced sense annotationrepresented n-dimensional vector u non-zero values dimensions denotingannotated senses. product uM produces m-dimensional vector v containingdistribution reference senses; sense largest corresponding value vresulting annotation.mapping function widely used WSID systems, comes two limitations: (1) induced senses considered equally informative producing referencesense annotation, (2) weights assigned sense annotation effectively incorporated instances induced labeling multiple senses (Jurgens, 2012),due part methods relative simplicity machine learning technique. Therefore,constructing WSID systems, evaluate six alternate supervised learning algorithmsperforming mapping function: Support Vector Machines (SVMs) linearradial basis function (RBF) kernels, Decision Trees based either entropy Gini impurity, naive Bayes classifiers using either Multinomial Bernoulli distributions. sixclassifiers trained feature vectors induced sense distinct featureproduce single sense label reference inventory. Feature vectors weightedvalues provided WSI models annotation, except SVM classifier,whose instance weights scaled [0,1], Bernoulli naive Bayes positive values set 1 due requirement binary data. Classifiers implementedusing SciKit (Pedregosa et al., 2011).2.3 Ensemble WSID ModelMany WSI models including used exploit different sources lexical informationinducing senses thus identify different features distinguishing senses.prior work WSD combined complementary WSD systems improve performanceensemble model (Pedersen, 2000; Florian & Yarowsky, 2002; Brody, Navigli, &Lapata, 2006; Sgaard & Johannsen, 2010), work pursued analogous ensembleapproach WSID.1 propose new heterogeneous ensemble WSID system builtoutput four WSI models. instance, output WSI systemscombined instance labeled induced senses systems, shownFigure 1; combined annotations used features mapping function1. note Stevens (2012) suggested using consensus clustering sense induction way creatingensemble; however, quantitative analysis performed.1030fiSemi-supervised Learning Induced Word Senses State Art WSDSingle Model WSIDWSISys.Input contextbank fishing...Inducedsense A#2MappingFunctionWordNetsensebank#n#1MappingFunctionWordNetsensebank#n#1Ensemble WSIDWSISys.Inducedsense A#2WSISys. BInducedsense B#7WSISys. CInducedsense C#1Figure 1: comparison single-model ensemble WSID systems, showingensemble combines output multiple WSI models using single mapping function.predict sense. WSI models capture different aspects context,ensemble-based system potentially identify induced senses combinations thereofproduce accurate mapping senses reference sense inventory.3. Experimental Designevaluate WSID WSD systems, first two experiments use common pseudoworddisambiguation task. Following, describe task data.3.1 Pseudoword DisambiguationPseudowords provide analogous form polysemous data evaluating WSD systems.pseudoword made two monosemous lemmas, referred pseudosenses.occurrences pseudosenses corpus replaced unique token.corresponding disambiguation task, WSD system must decide pseudosensesoriginally present given occurrence token, effectively simulating traditionalsense disambiguation task. Independently proposed Gale et al. (1992) Schutze(1992), pseudoword disambiguation fills important evaluation gap large amountssense-annotated data unavailable.However, disambiguation performance pseudowords guaranteed modeldifficulty disambiguating real words. example, Schutze (1998) uses pseudowordpseudosenses banana door, semantically dissimilar; decidingwords pseudosenses akin disambiguating sense homograph,known easier disambiguation task (Ide & Veronis, 1998; Navigli et al., 2007).contrast, polysemous homographs instead senses semanticrelated way therefore may appear similar contexts (Apresjan, 1974; Rodd,Gaskell, & Marslen-Wilson, 2002; Palmer, Dang, & Fellbaum, 2007; Martnez Alonso et al.,2013). Thus, constructing pseudowords arbitrarily-selected monosemous terms underestimates difficulty sense disambiguation results based pseudowordswould necessarily generalize.1031fiBaskaya & Jurgensnoundoublespiccadrawertapestryheadshotpseudosensesbadminton, tennismovie, photocalcium, californiadesk, treasurer, cartoonistcomplexity, cloth, rugphoto, soccer, gunfireTable 1: Examples pseudowords polysemous nouns WordNet monosemouslemmas comprise pseudosensesPilehvar Navigli (2013) propose solution problem appropriately choosingpseudosenses disambiguation difficulty mirrors real-world data. pseudoword dataset created pseudoword models sense properties one15,935 polysemous nouns WordNet 3.0. Specifically, pseudosenses selected closelymimic inter-sense similarities corresponding polysemous word mining WordNets ontology find monosemous words (pseudosenses) whose structural arrangementclose correspondence polysemous words senses ontology.noun hierarchy WordNet contains sufficient structure, dataset generatednouns.2 inclusion parts speech ultimately desirable, nounsalone represent significant challenge WSD systems multiple evaluations focused entirely disambiguating nouns (e.g., see Navigli, Jurgens, & Vannella, 2013). Table1 shows example nouns corresponding pseudosenses used experiments.practical utility pseudowords demonstrated Pilehvar Navigli(2014), showed pseudowords disambiguation difficulty accurately mirroredcorresponding polysemous words. Specifically, WSD systems trainednoun portion Senseval-3 dataset (Mihalcea et al., 2004) dataset madenouns corresponding pseudowords. resulting disambiguation performancepseudosense-annotated dataset highly correlated performance Senseval3 data. results indicate using pseudowords, pseudosense-annotateddatasets used closely approximate real-world WordNet WSD performance, therebyavoiding performance over-estimates caused early methods constructing pseudowords (Gaustad, 2001).3.2 Dataexperiments performed subset data Pilehvar Navigli (2013).original dataset includes pseudosenses likely introduce noise results dueerrors part speech tagging word takes part named entitypresent WordNet. Therefore, control possible sources noise, excludepseudosenses (1) lemma also plural form another lemma, e.g., spirits, (2)2. However, note principle, pseudowords could constructed comparativelyshallower verb hierarchy WordNet potentially adjectives using data Tsvetkov et al.(2014).1032fiSemi-supervised Learning Induced Word Senses State Art WSDF(Pseudowords correspondence X)Pilehvar Navigli (2013) pseudowordsPseudowords used study10.90.80.70.60.50.40.30.20.101101001000Degree pseudoword correspondence real data(lower better)Figure 2: cumulative distribution functions degree correspondenceword derived pseudoword, specified dataset Pilehvar Navigli (2013).Lower values indicate closer correspondence.lemma may another part speech, e.g., freezing, (3) lemma occurs fewer1,000 contexts Gigaword part named entity. test thirdcondition, used TreeTagger (Schmid, 1994) identify named entity mentions partspeech tagging corpus. third criteria necessary ensure sufficient numberinstances available training testing, discussed later section 4.1.dataset provides rating pseudoword indicating closely pseudosenses model senses corresponding word WordNet. example, pseudoword doubles shown Table 1 two pseudosenses, monosemous words tennisbadminton, closely model two senses (1) badminton played two players,(2) tennis played two players. Replacing one pseudosensesmonosemous word desk would lower resulting pseudowords degree correspondencesince desk similar either words senses thus, disambiguation taskwould potentially easier due dissimilarity contexts pseudosensesappear.subset data used experiments selected follows. First, pseudowords filtered according three aforementioned criteria. Second, remainingpseudowords ranked according degree correspondence. Third, 920 sensesselected ranking match distribution polysemy values WordNet(e.g., percentage disemous lemmas). third step performedorder ensure degrees polysemy dataset representativedistribution full dataset Pilehvar Navigli (2013). Figure 2 shows degreecorrespondence real words (a) pseudowords used study (b)full dataset Pilehvar Navigli (2013), highlighting subset usedexperiments significantly higher correspondence real-world data fulldataset therefore maximally representative expected real-world performance.1033fiBaskaya & JurgensWordNetPseudowords12000700Number words5008000400600030040002002000Number pseudowords6001000010000051015202530Number senses (degree polysemy)35Figure 3: Distributions number senses polysemous nouns WordNetnumber pseudosenses selected pseudowords, chosen closely matchpolysemy distribution WordNetPseudowords final dataset two twelve pseudosenses. Figure 3shows polysemy distribution number senses selected pseudowords, compared polysemy distribution nouns WordNet. mapping functions described previously Section 2.2 parametric, five additional high-correspondencedisemous pseudowords also selected use parameter tuning, number sensescommon dataset therefore representative.3.3 Sense Distributionsfrequency distribution words senses often peaked, one two senses occurring frequently rest (Passonneau, Salleb-Aouissi, & Ide, 2009). particularsense distribution word greatly affect WSID performance, artificially-inflatedperformance settings one sense occurs frequently induced sensesmapped sense; cases, WSID performance generalizable datasetssense distribution may vary. Controlling effect sense distributionreal-world test data requires significant number annotated instances select, currently possible existing annotated corpora. However, usingpseudowords, sense distribution may precisely controlled gathering requirednumber usages pseudosense match desired distribution.Precisely controlling sense distribution allows us measure WSID performancewithin two extremes. first distribution, leverage correspondencepseudowords senses WordNet senses simulate real-world sense distributions basedSemCor (Miller et al., 1993).Specifically, noun corresponding pseudoword, measure frequenciesnouns senses SemCor, determines relative frequencypseudowords pseudosenses. However, words still infrequent SemCor1034fiSemi-supervised Learning Induced Word Senses State Art WSDaccurately measure expected frequency senses. Therefore, words fewerten occurrences use average sense distribution computed wordspolysemy least ten occurrences SemCor. refer resultingdataset SemCor sense distribution.SemCor distribution measures difficulty WSID expected settingsenses likely occur others. However, presence majorityclass potentially mask important underlying performance differences systems;one sense likely, models performance necessarily representativeability distinguish senses word. Therefore, second distribution, words senses appear uniform probability. trainingtest data uniform sense distribution, WSD systems cannot use often-effectivestrategy always choosing most-frequent sense seen training data. Furthermore,Uniform-distributed data allows measuring ability sense mapping functionfind correspondence induced reference senses induced sensesequivalent amounts data. uniform sense distribution representativereal-world data, comparison models performances SemCor- Uniformdistributed datasets provides critical insight disambiguation capabilitiesexpected generalizability new data arbitrary perturbations underlying sensedistribution. example, model performs well SemCor-distributed dataUniform-distributed data, result suggests model effective identifying most-frequent sense; contrast, models perform well SemCor-Uniform-distributed data would expected maintain accuracy datasense distributions based ability discriminate senses one sensefrequent.4. Experiment 1: Evaluating WSID Mappingfirst experiment measures impact sense mapping function two ways. First,given wide-spread use Agirre et al. (2006) mapping function, assess whethersix alternatives described Section 2.2 consistently improve WSID performance.Second, assess whether proposed sense mapping functions effectively fuseinduced sense annotations multiple WSI models produce accurate ensemble model.4.1 Experimental Setupfollows, detail parameters training WSI systemstraining test data constructed.4.1.1 WSID SystemsWSI models trained base corpus, ukWaC (Baroni, Bernardini, Ferraresi,& Zanchetta, 2009), though emphasize models induce senses corpusdifferent ways. WSI parameter values used pseudowords. AI-KUuses settings language model, S-CODE Fastsubs (Yuret, 2012) algorithmsreported Baskaya et al. (2013). Using setup SemEval 2013 WSI task,AI-KU calculates lexical substitutes using SRILM (Stolcke, 2002) ukWaC1035fiBaskaya & JurgensTraining partitionsTraining instancesData partitionsUniform(Test partitionTest instancesSemCor((a)(b)))(c)Figure 4: schematic cross validation. Data partitions initially contain equalnumber instances per pseudosense (a), shown different colored boxes. foldvalidation, four partitions used training one test (b). instancespartition sampled according distribution (shown italics), producestraining test datasets (c).(Ferraresi, Zanchetta, Baroni, & Bernardini, 2008) corpus construct 4-gramlanguage model. k-means algorithm used AI-KU, k arbitrarily set 10,parameter tuning. HDP uses two parameters specify variabilitysenses corpus, 0 , set values reported Lauet al. (2012) Lau, Cook, Baldwin (2013). SquaT parameters set0.00125 0.25, respectively, limited grid search showed values producedsufficiently large graphs pseudowords. Chinese Whispers model nonparametric,parameter choices needed.total, twenty eight WSID configurations built combinationseven sense mapping functions (Sec. 2.2) four WSI models (Sec. 2.1). Additionally,seven ensemble WSID systems built training mapping functionsinduced sense labelings four WSI systems using default configuration.4.1.2 Cross-Validation EvaluationSystems evaluated using five-fold cross validation, modifications ensurefolds comparable sizes distribution types training data leakedtest data changing sense distribution test data. Initially, corpusinstances pseudoword divided five partitions, partition containsnumber instances pseudosense. instances partitionfiltered match desired sense distribution; filtering process deterministicpartition always instances particular distribution across folds. Figure4 visualizes process. evaluation, four filtered partitions form training dataone partition used test data. Importantly, setup ensures instances remainconsistent partition used different folds validation. notecase ensemble WSID system, underlying WSI models trainedtraining data identical folds, ensuring separation test training data.1036fiSemi-supervised Learning Induced Word Senses State Art WSDreported experiments use sense distributions training testing (either SemCor Uniform). However, evaluation setup sufficiently generalsupport using arbitrary distributions, including different distributions trainingtesting data, shown example Figure 4; results using additional combinationsdistributions reported Supplementary Material.4.1.3 Evaluation Datadata partitions drawn Gigaword corpus (Graff, Kong, Chen, &Maeda, 2003). Instances pseudosenses filtered ensure correct part speechremove occurrences pseudosense part named entity. Ultimatelypartition test data contained 200 instances senses, filteredaccording desired distribution. SemCor-distributed data, frequent sense200 instances, senses proportional numbers based relativesense frequencies. note setup chosen instead using fixed numberinstances per partition bias results polysemous wordswhose rarer sense would comparatively fewer instances fixed-size settingcase, WSID accuracy would significantly affected models abilityidentify senses. number instances varies SemCor-distributeddata, corresponding Uniform-distributed data pseudoword balancedtotal size, evenly distributed senses.Two baselines used test data: Random Frequent Sense (MFS).Random baseline simply picks randomly among senses; MFS baselineselects frequent sense word, often performs competitively skeweddistributions SemCor outperformed many WSID models previous studies(McCarthy et al., 2004; Kilgarriff, 2004; Navigli, 2009). Note Uniform sensedistribution, MFS Random baselines equivalent.4.1.4 ScoringSystems evaluated using standard WSD precision, recall F1 metrics (Navigli,2009). Precision measures percentage sense assignments provided WSID systemidentical gold standard. Recall measures percentage instancescorrectly labeled system. system labels instances, precision recallequivalent. number instances per term scales number senses,precision recall considered microaverages WSD performance across words.4.1.5 Parameter TuningFive disemous words used tune parametric mapping function. Using gridsearch parameter values, WSID configuration scored using identical fivefold cross-validation process. parameter values produced highest average F1across folds selected use WSID models mapping function.1037fiBaskaya & JurgensSquaTCWAI-KUHDPEnsembleMFSRandom1F1 Score0.90.80.70.60.50.4ee.Teci))pyront(G(E)BF(R)reaB6)020lNB.(Nal(L.TecSVSVllietiaomtinulourneirrAgFigure 5: Average performance WSID systems training testing data followSemCor sense distribution4.2 Results Discussionresults WSID systems using single WSI model demonstrate high sense disambiguation performance possible using suitable sense mapping functionmultiple WSI models may effectively combined ensemble.4.2.1 Single-Model ResultsWSID system evaluation showed clear impact choice mapping function.Results untuned WSID systems using SemCor distribution shown Figure5.3 nearly systems, Agirre et al. (2006) method mapped induced sensesmost-frequent sense seen SemCor-distributed training data, ignoring sensedistinctions recognized WSI model. Agirre et al. method produceWSID systems outperform using Multinomial Nave Bayes, performancesays little discriminative capabilities WSID systems effectively preventsmeaningful comparison, hindering testing development new WSID systems.contrast performance using Agirre et al. mapping function, SVMBayesian functions produced two WSID systems outperformed MFSbaseline. best performance using single WSI model comes SVMlinear kernel, provides slightly higher performance RBF kernel. Indeed,WSID system using AI-KU model linear kernel SVM mapping function3. WSID systems, tuning parameters mapping function provided little performanceimprovement either sense distribution. therefore omit tuned results brevity, reportscores Supplementary Materials.1038fiSemi-supervised Learning Induced Word Senses State Art WSDSquaTCWAI-KUHDPEnsembleMFSRandom1F1 Score0.90.80.70.60.50.4ee.Teci))pyront(G(E)BF(R)B6)020lNrea(L.TecSVSVB.(NaliaomtinulllietourneirrAgFigure 6: Average performance WSID systems training testing data followUniform sense distribution3.8% increase F1 score MFS baseline, statistically significant p < 106using McNemars test significance.impact choice sense mapping function WSID performance evenevident Uniform-distribution dataset. results, shown Figure 6, revealsignificant differences discriminatory capabilities WSID systems. WSID systemsusing Agirre et al. mapping function perform well average, indicating functioncapable learning effective correspondence senses single sensedominates frequency. Nevertheless, WSI models enjoy consistently-higher performanceusing SVM mapping function, statistically significant improvementp < 106 . Furthermore, even worst-performing model, SquaT, still abledouble performance MFS baseline using SVM Decision Tree mappingfunctions.single-model results Uniform distribution also provide insight models would expected perform new datasets sense distributions differSemCor-distributed data. Systems performances relatively closetested SemCor dataset differed 0.04 F1 linear-kernel SVM;contrast, systems differed 0.278 F1 Uniform-distributed data,indicating significant differences WSI models abilities find meaningful sense distinctions. Furthermore, clear difference seen distributional graph-basedWSI approaches, suggesting distributional techniques may robust potentialchanges corpuss sense distribution.overall ranking individual-model WSID systems consistent acrossdifferent mapping functions. However, rank oscillation appear HDPAI-KU models Uniform distribution setting CW SquaT1039fiBaskaya & Jurgensmodels SemCor distribution setting. cases, SVM-based mapping functionsprovide highest average performance across systems produce identical rankings.such, view ranking differences mapping functions artifactmapping function itself, rather due actual performance differencessystems.4.2.2 Ensemble-Model Resultsnearly WSID configurations, ensemble WSID system obtains substantial performance gains MFS baseline best WSID system built singleWSI model. SemCor-distributed data (Fig. 5), ensemble linear kernel SVMproduces highest performance WSID configurations, achieving 9.4% increaseF1 MFS 4.2% increase next-closest system (AI-KU). Furthermore,except using Agirre et al. (2006) Multinomial Nave Bayes mapping functions,ensemble WSID systems outperforms individual WSID systems. testingtraining Uniform sense distribution, ensemble WSID system achieves evensubstantial gains WSID systems, shown Figure 6, 13.6% increaseF1 next-closest system.results distributions indicate using linear kernel SVM sensemapping provides consistently superior WSID performance robust variationschoice WSI model. Furthermore, annotations induced senses containcomplementary sources information, case ensemble sense labeling,SVM mapping function able produce better quality sense annotations.success ensemble using mapping functions methodAgirre et al. (2006) highlights potential obstacle research community: priorattempts building ensemble WSID methods may considered, performancebenefit would observed due current community-wide practice usingmethod Agirre et al. Further, work raises possibility new mapping functionscould developed more-effectively combined induced senses.4.2.3 Quantifying Impact Polysemy Disambiguation PerformanceGiven high performance using linear-kernel SVM mapping function, performed follow-up analysis measure performance effect relative numbersenses per word. analysis separates improvement relative difficultydisambiguation provides more-complete picture WSID performance. Performancespseudowords six senses combined due words relative infrequency. Figures 7 8 show performances per term SemCor Uniform sensedistributions, respectively, using box whisker plot. Whiskers denote maximumminimum F1 pseudoword, boxes denote first third quartiles,middle line denotes median performance. baselines performances changepolysemy, plotted horizontal line.seen Figures 7 8, ensemble WSID system offers superior performanceacross levels polysemy. example, although one sense disemous words occursvast majority instances SemCor data, ensemble WSID performancestill able surpass MFS baseline nearly words (as shown left-most1040fiSemi-supervised Learning Induced Word Senses State Art WSD1F1 Score0.80.60.40.20SquaTCWAI-KUHDP2EnsembleMFSRandom345Pseudoword Polysemy6Figure 7: Performance WSID systems using linear-kernel SVM different polysemySemCor-distributed training testing data1F1 Score0.80.60.40.20SquaTCWAI-KUHDP2EnsembleMFSRandom3456Pseudoword PolysemyFigure 8: Performance WSID systems using linear-kernel SVM different polysemyUniform-distributed training testing datacluster boxes Figure 7). results settings indicate ensemble WSIDmodel would offer superior performance new data arbitrary sense distributions.Indeed, current datasets, words either two three senses (87%),ensemble WSID model sees smallest improvement MFS. evaluated1041fiBaskaya & Jurgenscorpus containing words senses, overall performance improvementWSID ensemble MFS baseline would even higher reported mainresults Experiment 1 (Figure 5).5. Experiment 2: Comparing WSID Supervised WSDsufficient sense-annotated data available, supervised machine learning typicallyshown produce best-performing WSD systems. However, results Experiment 1 indicate high WSD performance also possible using semi-supervised WSIDmodel. approaches require amount sense-annotated data, raisesquestion circumstances one approach expected outperformother. Therefore, Experiment 2, perform direct comparison semi-supervisedWSID systems current state art supervised WSD, using identical trainingdata. results experiment direct implications sense annotation effortsdeciding much data necessary high performance.5.1 Experimental Setupfollows, describe configuration supervised WSD system usedcomparison training data created.5.1.1 Supervised WSDcomparison, use It-Makes-Sense (IMS) (Zhong & Ng, 2010), state-of-the-art supervised WSD algorithm. disambiguate usage sentence-length context, IMS extractsfeatures consisting neighboring lemmas POS along neighboring collocation pairs. IMS uses linear-kernel SVM feature vectors predicting sense.experiments, IMS trained using default algorithmic parameter values specifiedpublicly-available implementation.experiments intentionally measuring disambiguation ability fullytrained IMS system provided authors;4 rather, experiments intended directly compare results using current state-of-the-art supervised WSD algorithm.would possible retrain WSID systems training data used authors fully-trained model, annotated corpora used original experimentsreadily available sense distributions corpora controlled for, makingconclusions experiment difficult generalize.5.1.2 Training Test DataExperiment 2, multiple datasets created increasing amounts training dataorder measures ability WSID supervised WSD condition.datasets generated similarly instances allocated sense distributionsExperiment 1. pseudoword, SemCor-distributed training data constructedselecting k instances frequent sense, senses assigned proportionalnumber instances based relative frequency. different number4. http://www.comp.nus.edu.sg/~nlp/software.html1042fiSemi-supervised Learning Induced Word Senses State Art WSDAIKUHDPCWSquaTEnsembleIMSMFSF1 Score10.90.807505002502001501075502510kFigure 9: Performance IMS WSID systems SemCor-distributed datainstances per word SemCor-distributed training data, Uniform-distributeddata created way account difference: Given specific k wordn total instances senses, corresponding Uniform-distributed trainingndata constructed includinginstances pseudowords senses.notational clarity, use k denote equivalently-sized Uniform-distributed datasetwhose corresponding SemCor-distributed training data k instances.Training test data generated Gigaword data used previousexperiments, using five folds cross validation. Training data generated k = {10,25, 50, 75, 100, 150, 200, 250, 500, 750}. WSID IMS systems trainedk k datasets created four partitions tested fifth, full partition.test set identical Experiment 1 instances usedtesting, resulting performances k k directly comparable resultsExperiment 1.5.1.3 WSID SystemsWSID systems constructed using procedure used previous experiments(cf. Sec. 4.1). simplicity, report WSID systems using linear kernel SVM,provided highest performance. Full results configuration availableSupplementary Material.5.2 Results Discussionresults reveal WSID systems offer superior performance IMSannotated instances available. Figures 9 10 show resulting F1 scores1043fiBaskaya & JurgensAIKUHDPCWSquaTEnsembleIMSMFSRandom0.9F1 Score0.80.70.60.507505002502001501075502510k^Figure 10: Performance IMS WSID systems Uniform-distributed dataSemCor- Uniform-distributed data, x-axis drawn log scale. random baseline(F1=0.432) omitted Figure 9 better visual contrast.SemCor-distributed data, IMS outperforms single-model WSID systems nearlyvalues k, though AI-KU HDP models closely competitive differing less1% F1 k 75. contrast single-model WSID systems, ensemble WSIDsystem outperforms IMS starting k=25 k=500. ensemble performancedifferences 25 k 250 statistically significant p < 106 IMS WSIDperformances k=500 statistically equivalent. Given publicly-distributed IMSsystem trained average 35 instances per word type, results suggesttraining ensemble WSID model data would provide superior performance.training instances available, cases words, IMS algorithmable correctly learn back-off strategy always selecting frequent sensetraining data, thereby ensuring performs least well MFS baseline.contrast, mapping function WSID models slightly noisier learnaccurate mapping induced senses reference senses, resulting performanceMFS baseline.similar trend seen testing Uniform-distributed data (Figure 10), thoughensemble WSID system outperforms IMS k = 10 k=200, pointstatistically equivalent, HDP model initially outperforms IMS wellk = 25. results Uniform-distributed setting indicate WSID modelsprovide accurate discriminatory techniques.Together results suggest ensemble WSID models offer significant advantages supervised WSD except little large amounts sense-annotated dataavailable. Indeed, 97 11,685 polysemous lemmas SemCor fewer200 instances, suggests ensemble WSID system may offer better performance existing supervised systems trained corpus. results also1044fiSemi-supervised Learning Induced Word Senses State Art WSDindicate using unsupervised features WSI models, ensemble WSIDsystem able break knowledge acquisition bottleneck acquire informationdisambiguation available annotated data alone.Last, note increasing amount training data consistently improvesperformance IMS, providing decreasing benefit WSID models. contrasthighlights difference systems learn. Training WSID model additionalsense-annotated data cannot directly improve disambiguation performance contextualfeatures used disambiguation fixed underlying WSI model, independent training data. contrast, providing data supervised WSD systemmay enable learn new features disambiguation. Nevertheless, performanceWSID depend sufficient sense-annotated data train correct mappingfunction, shown large performance improvements k=10 k=25 shownFigures 9 10 increase training data provides substantial improvementsense mapping.6. Experiment 3: Evaluation SemEval SystemsExperiments 1 2 demonstrated WSID models capable accurate WSDindividual WSI models combined ensemble, resulting systemcapable outperforming fully-supervised WSD. However, experiments performedcontrolled conditions pseudoword data. Therefore, Experiment 3, test whetherobserved performance improvements carry real-world data. three tests usingthirty WSI models two sense inventories, evaluate impact newmapping function ensemble construction extensions prior WSID evaluations.6.1 Experimental Setupevaluate ensemble WSID setup sense-annotated data, use three SemEvaltasks included WSID evaluation: 2007 Task 2 (Agirre & Soroa, 2007), 2010 Task 10(Manandhar et al., 2010), 2013 Task 13 (Jurgens & Klapaftis, 2013). repeattasks exact evaluation setup, exception sense mapping function originallyused task replaced SVM using linear kernel.Two significant differences exist tasks setup compared earlier experiments. 2007 2010 tasks use OntoNotes senses (Hovy et al., 2006), knowncoarse-grained WordNet senses pseudowords models. Second,2013 task also focuses instances multiple meanings may evident (e.g., dueambiguity syllepsis) therefore includes gold standard data instancesmultiple sense labels. experimental setup focused instancesone sense interpretation, adopt single-sense evaluation described JurgensKlapaftis (2013) using subset 4122 instances task data singlesense annotation.Ensemble systems created using induced sense answers systemsparticipated task linear kernel SVM perform sense mapping.intentionally use original WSI models rather four models used earlierexperiments order test benefits proposed WSID configuration different settings quantify generalizability. However, note two highest-performing1045fiBaskaya & JurgensEnsemblesSemEvalMFSBest SystemAll-SystemsBest-Configuration2007201020130.7870.5870.4770.8160.6240.6400.8280.6800.6400.6700.657Table 2: comparison best-performing system SemEval WSID taskproposed ensemble method.WSI systems prior experiments, AI-KU HDP, also participated 2013 task,ensemble results task expected similar. task, considertwo ensembles: (1) outputs WSI systems, (2) outputs best configuration system, measured according WSID performance originaltask. note 2007 task allowed one configuration per system, oneensemble produced.6.2 Resultsresults ensemble single WSI-model systems, described next, demonstratebenefits using new WSID construction procedure.6.2.1 Ensemble Resultsthree tasks, ensemble WSID configuration shows performance improvementsbest-performing system MFS. Table 2 shows results three tasks,including scores best-performing system task originally tasks MFSscore. Improvements MFS significant p < 106 using McNemars testsignificance. Similarly, improvements best systems task significantp < 106 ensemble configurations, exception ensemble SemEval2007, significant p < 104 . Furthermore, note improvementensemble task larger difference tasks best secondbest systems, indicating substantial increase performance. results demonstrateconsistent performance improvements SVM-based ensemble WSID model evenusing different sense inventories entirely different sets WSI systems.6.2.2 Single-Model ResultsSingle-system WSID models varied whether use SVM mapping function improved performance, shown Figure 11.5 SemEval-2007, systems obtained lowerF1 using SVM, average decrease 0.032 change overallsystem ranking. contrast, systems performed better 2010 2013 tasksusing SVM, average F1 increases 0.043 0.004. Although mixed trendsinitially seem contradictory prior experiments results showing consistent benefitSVM, performance differences partly due differences task setup5. Full score details available Supplementary Material.1046fiSemi-supervised Learning Induced Word Senses State Art WSD0.80.7Linear-kernel SVM0.7Linear-kernel SVMLinear-kernel SVM0.90.60.50.40.30.70.70.80.9Agirre et al. (2006)(a) SemEval-20070.60.50.30.4 0.5 0.6 0.7Agirre et al. (2006)(b) SemEval-20100.50.60.7Agirre et al. (2006)(c) SemEval-2013Figure 11: Comparisons F1 scores system SemEval tasks usinglinear-kernel SVM mapping function (y-axis) versus Agirre et al. (2006) (x-axis).Points diagonal indicate improvement using SVM function.WSI systems designed label usage single induced sense.contrast, systems prior experiments reported multiple induced senses per instance, weighted applicable sense instance. multiple sensesprovide richer feature set training enable recognizing cases lower-weightedinduced senses provide information correct sense annotation. WSI systemreports single sense, WSID system performance upper-bound basedreference sense highest conditional probability, given induced sense.Even single induced sense reported, using SVM mapping function stillsignificantly impact resulting performance, shown 2010 task. Here, multiple systems lowest ranked achieved significant improvements F1 seeing0.30 absolute increases. performances differences also highlight unique feature2010 task; Pedersen (2010) submitted four systems generated random sense assignments, ranked high 18th 26 systems. SVM-based rankingcorrectly assigns four random-answer submissions lowest four ranks. Indeed,overall system ranking task changes dramatically originally-reportedranking (Spearmans =0.14). results (Fig. 11b) suggest systems performing random chance, systems actually differ little abilities taskpreviously low-ranked systems actually offer superior performance. Thus,overall performance task high, SVM-based moel revealstrue discriminatory capabilities WSI systems, partially maskedmany induced senses mapped frequent reference sense, artificially increasingperformance.7. Related Workpresent study touches upon three bodies prior work word sense inductionrelationship WSD, semi-supervised WSD, work pseudowords.7.1 Word Sense Induction DisambiguationPurandare Pedersen (2004) Niu, Li, Srihari, Li (2005a) produce sense induction models assign induced senses directly reference senses, rather1047fiBaskaya & Jurgenscreating mapping function converts induced sense annotations. report findinginduced senses closely correspond existing definitions reference sense inventoriesneither analyze performance disambiguating new instances reference senses,role WSID systems study. noted Section 2, Agirre et al. (2006)first formalized WSID process. experiments, WSID system builtHyperLex WSI model (Veronis, 2004) mapping function; resulting systemobtained 0.06 improvement F1 score MFS baseline default parameters0.11 improvement MFS tuned, suggesting high WSID performance possible. Last, Jurgens (2012) notes potential WSI models annotateitems multiple senses proposes modification mapping function Agirreet al. improve performance multiple induced senses annotation weighted.experiments, WSID systems new mapping function able outperformMFS baseline.7.2 PseudowordsSince first proposed word sense disambiguation (Gale et al., 1992; Schutze, 1992),pseudowords incorporated evaluations multiple tasks, specific recommendations improve construction tasks modeling selectionalpreferences (Chambers & Jurafsky, 2010), modeling word co-occurrence (Dagan, Lee, &Pereira, 1999), machine translation (Duan, Zhang, & Li, 2010), tasks Information Retrieval (Stokoe, 2005) even improving word embeddings (Liu, Liu, Chua, & Sun, 2015).Indeed WSD, new techniques proposed adapting pseudowordslanguages Chinese (Lu, Ting, & Sheng, 2004; Lu, Wang, Yao, Liu, & Li, 2006)creating pseudowords accurately model difficulty WSD (Nakov &Hearst, 2003; Otrusina & Smrz, 2010; Pilehvar & Navigli, 2013), though approachPilehvar Navigli (2013) used shown closely correlatereal-world performance.related study work analyzing word senses using pseudowords.Cook Hirst (2011) simulate sense properties lemmas Senseval-3 lexicalsample task (Mihalcea et al., 2004) order model process lemmas acquirenew senses; however, pseudowords analyzed contextual features rather usingWSD done study. test discriminatory ability WSI models, JurgensStevens (2011) create set disemous pseudowords pseudosenses varyingdegrees similarity. represent full range pseudosense similarities, similaritywords pseudosenses measured using corpus-based distributional similaritypseudosenses paired pseudoword based similar corpus frequenciespositions along similarity spectrum. Sense induction models tested accordingability discriminate pseudosenses different similarity levels. However, pseudosenses used study monosemous limits ability replicateeffect new corpora, may potentially different sense distributions polysemous pseudosenses. Last, similar study Pilehvar Navigli (2014),used pseudoword dataset analyze supervised unsupervisedWSD. findings corroborate study, indicating pseudowords1048fiSemi-supervised Learning Induced Word Senses State Art WSDPilehvar Navigli (2013) used design WSD-related evaluations mirrorreal-world performance.7.3 Semi-supervised WSDBeyond WSID, approaches applied semi-supervised learning WSD. Mihalcea (2004) applies co-training self-training supervised classifier Lee Ng(2002), showing techniques reduce disambiguation error many wordsusing high-confidence automatically-labeled examples; however, techniques requiredparameter tuning, parameter set providing high performance words. Pham,Ng, Lee (2005) investigate four semi-supervised techniques, showing spectral graph transduction co-training performed best, performance highpurely-supervised WSD methods. Rather use automatically-labeled data, Yuret(2007) generates new contexts existing training data using lexical substitution,ultimately improve performance new substitutesadded. contrast, works seen improvement fully-supervisedsystems using semi-supervised techniques. Niu, Ji, Tan (2005b) construct graphword uses, edges weighted usages contextual similarity. annotated instances labeled graph senses label propagation rungraph infer remaining instances labels, resulting performance superiorSVM-based WSD comparison system. Similarly, Kubler Zhekova (2009) ablefilter automatically-annotated data based expected quality supplementtraining data. combination manually- automatically-annotated dataprovided slight improvement original data, though note approachable automatically annotate small number contexts per word, illustrating mainchallenge semi-supervised learning significantly increasing number training instances. Martinez, De Lacalle, Agirre (2008) identify monosemous synonyms targetnouns using WordNet query examples synonyms Web createcorpus automatically sense-annotated examples training. WSD performanceclosely related distribution word senses, additional heuristics used estimate sense distribution testing data (McCarthy et al., 2004) trainingsupervised WSD system automatically-produced data. resulting system attainedsignificantly higher performance unsupervised systems still outperformedfully-supervised systems.common thread works need extensive filtering unlabeledinstances order obtain performance improvements; including many lower-qualityexamples typically resulted performance supervised technique traineddata. contrast, several WSID systems tested outperformed state artwithout need filtering instances used WSI algorithms. differenceneed filtering suggests WSI may robust noise unlabeled instancesWSI could even potential preprocessing step finding instancesparadigmatic induced senses later use input semi-supervised techniques.1049fiBaskaya & Jurgens8. Conclusionpaper presents comprehensive analysis construction evaluation WSIDsystems. Systems tested using novel evaluation design incorporating 920 pseudowordsdata set Pilehvar Navigli (2013), whose pseudosenses closely approximateproperties disambiguation difficulty noun senses WordNet 3.0. testsmillion instances, provide three empirical contributions. First, demonstratechoice mapping function used convert induced senses significantlyaffect WSID performance linear kernel SVM significantly improves uponcurrent state art practices (Agirre et al., 2006), performance increases 3.8%F1 settings. Second, demonstrate using linear kernel SVM, joining multiple WSI models ensemble WSID system yields large improvements,seen using prior state art (an 8.5% F1 increase). benefitensemble setup demonstrated tests real sense-annotated data using multiple ensemble configurations different sense inventories, highlightingrobustness. Third, direct comparison state art supervised WSDsystem (Zhong & Ng, 2010), demonstrate ensemble WSID system offers superior performance supervised system using training data excepthundreds annotated instances available, suggesting WSID viable mechanism overcoming knowledge acquisition bottleneck. lineresearch, released implementations WSI models implementation pseudoword testing framework freely-available open source software(https://github.com/osmanbaskaya/mapping-impact). Furthermore, practical resulteffort, intended release large-scale all-words WSID system based ensemblemodel.results study motivate three interesting avenues future work planexplore. First, results indicate annotated instances necessaryrelatively high WSD performance. Recent work shown controllingdifficulty humans annotating contexts (as measured using Passonneau& Carpenter, 2014), quality training data and, subsequently, performanceWSD system may improved (Lopez de Lacalle & Agirre, 2015). Together, findingssuggest high performance WSID systems could quickly created appropriatelycurating instances annotated training data. future work, intendmeasure effect annotation selection WSID examine whether WSI processmight also informative instances select human annotation.Second, experiments conducted English-language pseudowords. futurework, plan develop analogous pseudoword data WordNet ontologies languages (Bond & Foster, 2013) replicate experiments multilingual datameasure potential language-specific effects sense-annotated data sparse. alsoplan investigate using translation cross-lingual sense mappings transfer information English languages way gathering annotations WSDsystems, analogous done part speech tagging (Duong et al., 2014).Third, examined WSI models trained tested multi-domain ukWaCcorpus. Typically, WSD performed much worse tested novel domains,typically contain dissimilar contexts training data cases,1050fiSemi-supervised Learning Induced Word Senses State Art WSDwords may different dominant senses (Magnini et al., 2002; Preiss & Stevenson,2013). However, prior works shown small amount sense-annotationnovel domain significantly improve WSD performance new domain (Khapra et al.,2010). future work, evaluate whether WSI system used effectivelyannotate new instances novel domain instead requiring manual annotation, thusproviding unsupervised method domain adaptation.Acknowledgmentsthank Mohammad Taher Pilehvar many thoughtful discussions assistancepseudoword dataset. also thank reviewers comments suggestions.ReferencesAgirre, E., de Lacalle, O. L., & Soroa, A. (2014). Random walks knowledge-based wordsense disambiguation. Computational Linguistics, 40 (1), 5784.Agirre, E., & Martinez, D. (2000). Exploring automatic word sense disambiguationdecision lists web. Proceedings COLING-2000 Workshop Semantic Annotation Intelligent Content, pp. 1119. Association ComputationalLinguistics.Agirre, E., Martnez, D., de Lacalle, O. L., & Soroa, A. (2006). Evaluating optimizing parameters unsupervised graph-based WSD algorithm. ProceedingsTextGraphs: First Workshop Graph Based Methods Natural LanguageProcessing, pp. 8996. Association Computational Linguistics.Agirre, E., & Soroa, A. (2007). Semeval-2007 task 02: Evaluating word sense inductiondiscrimination systems. Proceedings Fourth International WorkshopSemantic Evaluations, pp. 712. ACL.Apresjan, J. D. (1974). Regular polysemy. Linguistics, 12 (142), 532.Baroni, M., Bernardini, S., Ferraresi, A., & Zanchetta, E. (2009). WaCky wide web:collection large linguistically processed web-crawled corpora. LanguageResources Evaluation, 43 (3), 209226.Baskaya, O., Sert, E., Cirik, V., & Yuret, D. (2013). Ai-ku: Using substitute vectorsco-occurrence modeling word sense induction disambiguation. ProceedingsSeventh International Workshop Semantic Evaluation (SemEval), pp. 300306.Biemann, C. (2006). Chinese whispers: efficient graph clustering algorithm application natural language processing problems. Proceedings First WorkshopGraph Based Methods Natural Language Processing, pp. 7380. AssociationComputational Linguistics.Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. JournalMachine Learning Research, 3, 9931022.1051fiBaskaya & JurgensBond, F., & Foster, R. (2013). Linking extending open multilingual wordnet. Proceedings 51st Annual Meeting Association Computational Linguistics(ACL), pp. 13521362.Brody, S., & Lapata, M. (2009). Bayesian word sense induction. Proceedings 12thConference European Chapter Association Computational Linguistics(EACL), pp. 103111. Association Computational Linguistics.Brody, S., Navigli, R., & Lapata, M. (2006). Ensemble methods unsupervised wsd.Proceedings 21st International Conference Computational Linguistics44th annual meeting Association Computational Linguistics (COLINGACL), pp. 97104. Association Computational Linguistics.Carpuat, M., & Wu, D. (2007). Improving statistical machine translation using word sensedisambiguation. Proceedings Joint Conference Empirical Methods Natural Language Processing Computational Natural Language Learning (EMNLPCoNLL), pp. 6172. Association Computational Linguistics.Chambers, N., & Jurafsky, D. (2010). Improving Use Pseudo-Words EvaluatingSelectional Preferences. Proceedings 48th Annual Meeting AssociationComputational Linguistics (ACL). Association Computational Linguistics.Chan, Y., Ng, H., & Chiang, D. (2007). Word sense disambiguation improves statisticalmachine translation. Proceedings Association Computational Linguistics.Association Computational Linguistics.Cook, P., & Hirst, G. (2011). Automatic identification words novel infrequentsenses. Proceedings 25th Pacific Asia Conference Language InformationComputation (PACLIC), pp. 265274.Dagan, I., Lee, L., & Pereira, F. C. (1999). Similarity-based models word cooccurrenceprobabilities. Machine Learning, 34 (1-3), 4369.Di Marco, A., & Navigli, R. (2012). Clustering diversifying web search resultsgraph-based word sense induction. Computational Linguistics, 39 (4).Duan, X., Zhang, M., & Li, H. (2010). Pseudo-word phrase-based machine translation. Proceedings 48th Annual Meeting Association ComputationalLinguistics (ACL), pp. 148156. Association Computational Linguistics.Duong, L., Cohn, T., Verspoor, K., Bird, S., & Cook, P. (2014). get1000 tokens? case study multilingual pos tagging resource-poor languages.Proceedings Conference Empirical Methods Natural Language Processing(EMNLP), pp. 886897. Association Computational Linguistics.Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press.Ferraresi, A., Zanchetta, E., Baroni, M., & Bernardini, S. (2008). Introducing evaluatingukWaC, large web-derived corpus English. Proceedings 4th WebCorpus Workshop (WAC).Florian, R., & Yarowsky, D. (2002). Modeling consensus: Classifier combination wordsense disambiguation. Proceedings Conference Empirical Methods NaturalLanguage Processing (EMNLP), pp. 2532. Association Computational Linguistics.1052fiSemi-supervised Learning Induced Word Senses State Art WSDGale, W. A., Church, K. W., & Yarowsky, D. (1992). Work statistical methodsword sense disambiguation. AAAI Fall Symposium Probabilistic ApproachesNatural Language, pp. 5460.Gaustad, T. (2001). Statistical corpus-based word sense disambiguation: Pseudowords vs.real ambiguous words. Proceedings ACL Student Research Workshop, pp.6166. Association Computational Linguistics.Graff, D., Kong, J., Chen, K., & Maeda, K. (2003). English Gigaword, LDC2003T05..Linguistic Data Consortium.Griffiths, T. L., & Steyvers, M. (2004). Finding scientific topics. Proceedings NationalAcademy Sciences, 101 (suppl 1), 52285235.Hartmann, S., Gurevych, I., & Lap, U. K. P. (2013). Framenet way babel: Creatingbilingual framenet using wiktionary interlingual connection. Proceedings51th Annual Meeting Association Computational Linguistics (ACL 2013).Association Computational Linguistics.Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., & Weischedel, R. (2006). OntoNotes:90% solution. Proceedings 2006 Conference North American ChapterAssociation Computational Linguistics Human Language Technology(NAACL-HLT), pp. 5760. Association Computational Linguistics.Ide, N., & Veronis, J. (1998). Introduction special issue word sense disambiguation:state art. Computational linguistics, 24 (1), 240.Jurgens, D. (2012). Evaluation Graded Sense Disambiguation using Word SenseInduction. Proceedings First Joint Conference Lexical ComputationalSemantics (*SEM). Association Computational Linguistics.Jurgens, D., & Klapaftis, I. (2013). SemEval-2013 Task 13: Word Sense Induction GradedNon-Graded Senses. Proceedings Seventh International WorkshopSemantic Evaluation (SemEval). Association Computational Linguistics.Jurgens, D., & Stevens, K. (2011). Measuring impact sense similarity word senseinduction. Proceedings First Workshop Unsupervised Learning NLP,pp. 113123. Association Computational Linguistics.Khapra, M., Kulkarni, A., Sohoney, S., & Bhattacharyya, P. (2010). words domainadapted wsd: Finding middle ground supervision unsupervision.Proceedings 48th Annual Meeting Association Computational Linguistics (ACL), pp. 15321541. Association Computational Linguistics.Kilgarriff, A., & Rosenzweig, J. (2000). Framework results english senseval. Computers Humanities, 34 (1), 1548.Kilgarriff, A. (2004). dominant commonest sense word?. Text, SpeechDialogue, pp. 103111. Springer.Klapaftis, I. P., & Manandhar, S. (2010). Word sense induction & disambiguation usinghierarchical random graphs. Proceedings Conference Empirical MethodsNatural Language Processing (EMNLP), pp. 745755. Association ComputationalLinguistics.1053fiBaskaya & JurgensKubler, S., & Zhekova, D. (2009). Semi-supervised learning word sense disambiguation:Quality vs. quantity.. Proceedings Conference Recent Advances NaturalLanguage Processing (RANLP).Lau, J. H., Cook, P., & Baldwin, T. (2013). unimelb: Topic Modelling-based Word SenseInduction. Proceedings Seventh International Workshop Semantic Evaluation (SemEval), pp. 307311. Association Computational Linguistics.Lau, J. H., Cook, P., McCarthy, D., Newman, D., & Baldwin, T. (2012). Word senseinduction novel sense detection. Proceedings 13th ConferenceEuropean Chapter Association Computational Linguistics (EACL 2012).Association Computational Linguistics.Lee, Y. K., & Ng, H. T. (2002). empirical evaluation knowledge sources learningalgorithms word sense disambiguation. Proceedings Conference Empirical Methods Natural Language Processing (EMNLP), pp. 4148. AssociationComputational Linguistics.Liu, Y., Liu, Z., Chua, T.-S., & Sun, M. (2015). Topical word embeddings. Proceedings29th AAAI Conference Artificial Intelligence (AAAI).Lopez de Lacalle, O., & Agirre, E. (2015). Crowdsourced word sense annotationsdifficult words examples. Proceedings 11th International ConferenceComputational Semantics (IWCS).Lu, Z., Ting, L., & Sheng, L. (2004). Combining neural networks statistics chinese word sense disambiguation. Proceedings Third SIGHAN WorkshopChinese Language Processing.Lu, Z., Wang, H., Yao, J., Liu, T., & Li, S. (2006). equivalent pseudoword solutionchinese word sense disambiguation. Proceedings 21st International Conference Computational Linguistics 44th annual meeting AssociationComputational Linguistics (COLING-ACL), pp. 457464. Association Computational Linguistics.Magnini, B., Strapparava, C., Pezzulo, G., & Gliozzo, A. (2002). role domain information word sense disambiguation. Natural Language Engineering, 8 (4), 359373.Manandhar, S., Klapaftis, I. P., Dligach, D., & Pradhan, S. S. (2010). SemEval-2010 Task14: Word Sense Induction & Disambiguation. Proceedings Fifth InternationalWorkshop Semantic Evaluation (SemEval), pp. 6368. Association Computational Linguistics.Maron, Y., Lamar, M., & Bienenstock, E. (2010). Sphere Embedding: ApplicationPart-of-Speech Induction. Lafferty, J., Williams, C. K. I., Shawe-Taylor, J., Zemel,R. S., & Culotta, A. (Eds.), Advances Neural Information Processing Systems 23(NIPS), pp. 15671575.Martinez, D., De Lacalle, O. L., & Agirre, E. (2008). use automatically acquiredexamples all-nouns word sense disambiguation.. Journal Artificial IntelligenceResesarch (JAIR), 33, 79107.Martnez Alonso, H., et al. (2013). Annotation regular polysemy: empirical assessmentunderspecified sense. Ph.D. thesis, Universitat Pompeu Fabra.1054fiSemi-supervised Learning Induced Word Senses State Art WSDMcCarthy, D., Koeling, R., Weeds, J., & Carroll, J. (2004). Finding Predominant WordSenses Untagged Text. Proceedings 42nd Annual Meeting AssociationComputational Linguistics (ACL), p. 279, Morristown, NJ, USA. AssociationComputational Linguistics.Mihalcea, R. (2004). Co-training self-training word sense disambiguation. Proceedings Conference Natural Language Learning (CoNLL). AssociationComputational Linguistics.Mihalcea, R., Chklovski, T., & Kilgarriff, A. (2004). Senseval-3 English Lexical Sample Task. Proceedings Third International Workshop EvaluationSystems Semantic Analysis Text (Senseval), pp. 2528. AssociationComputational Linguistics.Miller, G. A., Leacock, C., Tengi, R., & Bunker, R. T. (1993). semantic concordance.Proceedings workshop Human Language Technology, pp. 303308. AssociationComputational Linguistics.Moro, A., Raganato, A., & Navigli, R. (2014). Entity linking meets word sense disambiguation: unified approach. Transactions Association ComputationalLinguistics, 2, 231244.Nakov, P. I., & Hearst, M. A. (2003). Category-based pseudowords. Proceedings2003 Conference North American Chapter Association ComputationalLinguistics Human Language Technology (NAACL-HLT), pp. 6769. AssociationComputational Linguistics.Navigli, R. (2009). Word sense disambiguation: survey.(CSUR), 41 (2), 10.ACM Computing SurveysNavigli, R. (2012). quick tour word sense disambiguation, induction related approaches. SOFSEM 2012: Theory practice computer science, pp. 115129.Springer.Navigli, R., & Crisafulli, G. (2010). Inducing word senses improve web search result clustering. Proceedings Conference Empirical Methods Natural LanguageProcessing (EMNLP), pp. 116126. Association Computational Linguistics.Navigli, R., Jurgens, D., & Vannella, D. (2013). Semeval-2013 task 12: Multilingual wordsense disambiguation. Proceedings 7th International Workshop SemanticEvaluation (SemEval).Navigli, R., Litkowski, K. C., & Hargraves, O. (2007). Semeval-2007 Task 07: Coarse-grainedEnglish All-words Task. Proceedings 4th International Workshop SemanticEvaluations (SemEval), pp. 3035. Association Computational Linguistics.Navigli, R., & Ponzetto, S. P. (2010). Babelnet: Building large multilingual semanticnetwork. Proceedings 48th Annual Meeting Association Computational Linguistics (ACL), pp. 216225. Association Computational Linguistics.Niu, C., Li, W., Srihari, R. K., & Li, H. (2005a). Word independent context pair classification model word sense disambiguation. Proceedings Ninth ConferenceComputational Natural Language Learning (CoNLL), pp. 3339. AssociationComputational Linguistics.1055fiBaskaya & JurgensNiu, Z., Ji, D., & Tan, C. (2005b). Word sense disambiguation using label propagation basedsemi-supervised learning. Proceedings 43rd Annual Meeting AssociationComputational Linguistics (ACL), pp. 395402. Association ComputationalLinguistics.Otrusina, L., & Smrz, P. (2010). new approach pseudoword generation.. Proceedings Seventh International Conference Language Resources Evaluation(LREC).Palmer, M., Dang, H. T., & Fellbaum, C. (2007). Making fine-grained coarse-grainedsense distinctions, manually automatically. Natural Language Engineering,13 (02), 137163.Passonneau, R. J., & Carpenter, B. (2014). benefits model annotation. Transactions Association Computational Linguistics, 2, 311326.Passonneau, R., Salleb-Aouissi, A., & Ide, N. (2009). Making sense word sense variation. Proceedings NAACL HLT Workshop Semantic Evaluations: RecentAchievements Future Directions.Pedersen, T. (2000). Simple Approach Building Ensembles Naive Bayesian ClassifiersWord Sense Disambiguation. Proceedings 1st North American chapterAssociation Computational Linguistics conference (NAACL), pp. 6369.Association Computational Linguistics.Pedersen, T. (2010). Duluth-WSI: SenseClusters Applied Sense Induction TaskSemEval-2. Proceedings 5th International Workshop Semantic Evaluations, pp. 363366.Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,M., Prettenhofer, P., Weiss, R., Dubourg, V., et al. (2011). Scikit-learn: Machinelearning python. Journal Machine Learning Research, 12, 28252830.Petrolito, T., & Bond, F. (2014). survey WordNet annotated corpora. ProceedingsSeventh Global Wordnet Conference, pp. 236245.Pham, T. P., Ng, H. T., & Lee, W. S. (2005). Word sense disambiguation semisupervised learning. Proceedings 19th AAAI Conference Artificial Intelligence (AAAI).Pilehvar, M. T., & Navigli, R. (2013). Paving way large-scale pseudosense-annotateddataset. Proceedings 2013 Conference North American ChapterAssociation Computational Linguistics: Human Language Technologies (NAACLHLT), pp. 11001109. Association Computational Linguistics.Pilehvar, M. T., & Navigli, R. (2014). large-scale pseudoword-based evaluation frameworkstate-of-the-art word sense disambiguation. Computational Linguistics, 40 (4), 837881.Preiss, J., & Stevenson, M. (2013). Unsupervised domain tuning improve word sensedisambiguation.. Proceedings 2013 Conference North American Chapter Association Computational Linguistics: Human Language Technologies(NAACL-HLT), pp. 680684. Association Computational Linguistics.1056fiSemi-supervised Learning Induced Word Senses State Art WSDPurandare, A., & Pedersen, T. (2004). Word Sense Discrimination Clustering ContextsVector Similarity Spaces. Proceedings Conference ComputationalNatural Language Learning (CoNLL), pp. 4148. Boston.Rodd, J., Gaskell, G., & Marslen-Wilson, W. (2002). Making sense semantic ambiguity:Semantic competition lexical access. Journal Memory Language, 46 (2),245266.Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. ProceedingsInternational Conference New Methods Language Processing.Schutze, H. (1992). Dimensions meaning. Proceedings Supercomputing 92, pp.787796.Schutze, H. (1998). Automatic word sense discrimination. Computational Linguistics, 24 (1),97123.Sgaard, A., & Johannsen, A. (2010). Robust semi-supervised ensemble-based methodsword sense disambiguation. Advances Natural Language Processing, pp. 401405. Springer.Stevens, K. (2012). Evaluating unsupervised ensembles applied word sense induction. Proceedings ACL 2012 Student Research Workshop, pp. 2530. AssociationComputational Linguistics.Stokoe, C. (2005). Differentiating homonymy polysemy information retrieval. Proceedings conference Human Language Technology Empirical MethodsNatural Language Processing (HLT-EMNLP), pp. 403410. Association Computational Linguistics.Stolcke, A. (2002). SRILM Extensible Language Modeling Toolkit. ProceedingsInternational Conference Spoken Language Processing vol. 2, pp. 901904.Teh, Y. W., Jordan, M. I., Beal, M. J., & Blei, D. M. (2006). Hierarchical dirichlet processes.Journal American Statistical Association, 101 (476), 15661581.Tsvetkov, Y., Schneider, N., Hovy, D., Bhatia, A., Faruqui, M., & Dyer, C. (2014). Augmenting english adjective senses supersenses. Proceedings LanguageResources Evaluation Conference (LREC).Van de Cruys, T., & Apidianaki, M. (2011). Latent Semantic Word Sense InductionDisambiguation. Proceedings 49th Annual Meeting AssociationComputational Linguistics (ACL), pp. 14761485. Association Computational Linguistics.Veronis, J. (2004). HyperLex: lexical cartography information retrieval. ComputerSpeech & Language, 18 (3), 223252.Wang, J., Bansal, M., Gimpel, K., Ziebart, B. D., & Yu, C. T. (2015). sense-topic modelword sense induction unsupervised data enrichment. TransactionsAssociation Computational Linguistics, 3, 5971.Yarowsky, D. (1995). Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. Proceedings 33rd annual meeting Association ComputationalLinguistics (ACL), pp. 189196. Association Computational Linguistics.1057fiBaskaya & JurgensYuret, D. (2007). Ku: Word sense disambiguation substitution. Proceedings4th International Workshop Semantic Evaluations (SemEval), pp. 207213. Association Computational Linguistics.Yuret, D. (2012). FASTSUBS: Efficient Exact Procedure Finding LikelyLexical Substitutes Based N-Gram Language Model. IEEE Signal ProcessingLetters, 19 (11), 725728.Zhong, Z., & Ng, H. (2010). makes sense: wide-coverage word sense disambiguation system free text. Proceedings ACL 2010 System Demonstrations. AssociationComputational Linguistics.1058fiJournal Artificial Intelligence Research 55 (2016) 131-163Submitted 3/2015; published 1/2016Distributional Correspondence IndexingCross-Lingual Cross-Domain Sentiment Classification.Alejandro Moreo FernandezAndrea Esulialejandro.moreo@isti.cnr.itandrea.esuli@isti.cnr.itIstituto di Scienza e Tecnologie dellInformazioneConsiglio Nazionale delle Ricerche56124 Pisa,Fabrizio Sebastianifsebastiani@qf.org.qaQatar Computing Research InstituteHamad bin Khalifa UniversityPO Box 5825, Doha, QAAbstractDomain Adaptation (DA) techniques aim enabling machine learning methods learn effective classifiers target domain available training data belongsdifferent source domain. paper present Distributional CorrespondenceIndexing (DCI) method domain adaptation sentiment classification. DCI derivesterm representations vector space common domains dimensionreflects distributional correspondence pivot, i.e., highly predictive termbehaves similarly across domains. Term correspondence quantified means distributional correspondence function (DCF). propose number efficient DCFsmotivated distributional hypothesis, i.e., hypothesis according termssimilar meaning tend similar distributions text. Experiments showDCI obtains better performance current state-of-the-art techniques cross-lingualcross-domain sentiment classification. DCI also brings significantly reducedcomputational cost, requires smaller amount human intervention. final contribution, discuss challenging formulation domain adaptation problem,cross-domain cross-lingual dimensions tackled simultaneously.1. IntroductionAutomated text classification methods usually rely training set labelled examplesorder learn classifier predict classes unlabelled documents. Oneimportant bottleneck supervised machine learning methods dealdependence high-quality annotated examples order modeltrained. Deploying model domain examples available thusentails substantial human labelling effort.Transfer learning (TL see e.g., Pan & Yang, 2010; Pan, Zhong, & Yang, 2012) focuses alleviating problem leveraging training examples different, althoughrelated, source domain (a.k.a. out-domain, auxiliary domain) amountavailable labelled examples higher. TL allows making use examples ordertrain effective classifier target domain (a.k.a. in-domain), thus allowingdiminish completely away cost involved manual generation trainingc2016AI Access Foundation. rights reserved.fiMoreo, Esuli, & Sebastianidocuments target domain. consequence, TL applied one fundamental assumptions traditional machine learning, i.e., training testdata randomly drawn distribution (the so-called iid assumption),longer holds.One applicative scenario particular interest TL sentiment classification, taskclassifying opinion-laden documents conveying positive negative sentiment towards given entity (e.g., product, policy, political candidate). Determining usersstance towards entity utmost importance market research, customer relationship management, social sciences, political science among others, severalautomated methods proposed purpose (see e.g., Liu, 2012; Pang & Lee,2008). However, sentiment classification needs deal completely new entity,likely amount available, pre-labelled opinion-laden documents scarce evennull. cases, promptly generating sentiment classifier might become difficult, dueconsiderable cost time involved producing representative corpus trainingdocuments.sentiment classification, TL finds natural application domain adaptation (DA),i.e., task adapting sentiment classifier operate new domain. example, might want use training set book reviews written English classifymovie reviews written English, classify book reviews written German.former case typically known cross-domain adaptation, second one instead known cross-lingual adaptation (Pan et al., 2012). article usenotation Ls Cs Lt Ct indicate domain adaptation setup, Ls Ltsource target languages, Cs Ct source target domains, respectively.Therefore, previously discussed examples written EnglishBooksEnglishDVDs(or simply BooksDVDs short), example cross-domain adaptation,EnglishBooksGermanBooks, example cross-lingual adaptation.common practice text classification represent dataset term-documentmatrix according so-called bag-of-words model (BoW), value Mijfunction frequency term fi document dj dataset whole.Accordingly, rows (resp., columns) regarded vectorial representations terms(resp., documents) vector space generated documents (resp., terms). Here,expectation distances vectors vector space model (VSM) reflectsemantic distance terms documents. Since term dimensionvector space documents lie, terms represented orthogonal dimensionseven similar meanings. example, term beautiful vieweddissimilar nice awful; base vector space therefore unawaresemantic nuance, lies hidden joint term distributions. Statistical methods likeLatent Semantic Indexing (LSA Deerwester, Dumais, Landauer, Furnas, & Harshman,1990; Landauer & Dumais, 1997) Latent Dirichlet Allocation (LDA Blei, Ng, &Jordan, 2003) attempt discover hidden correlations among terms. Discoveringsemantic correspondences becomes crucial dealing different domains;example, adapting target domain book reviews source domain filmreviews, identifying cross-domain semantic correspondences among important terms (e.g.,book film, writer director, length duration) might helpful task, likelydecision boundaries model hinge upon terms.132fiDistributional Correspondence Indexingrespect aspect, general assumption domain adaptation largesets unlabelled documents source target domain available.leveraging unlabelled collections, various techniques applied order betterexplore term distributions two domains, attempt map similaritiesterms two domains. idea rests belief meaningterm somehow determined distribution text terms co-occurs with,idea generally referred distributional hypothesis (Harris, 1954).discovery hidden correlations highly predictive terms, goalimproving document representations, studied Structural Learning paradigm(Ando & Zhang, 2005). correlations discovered learning predictive structuresinput data auxiliary binary problems consist predicting term presenceusing surrounding terms unlabelled data applying LSA predictors. framework extended Structural Correspondence Learning domainadaptation (SCL Blitzer, McDonald, & Pereira, 2006). SCL unifies latent spacecorrespondences among terms different domains derive auxiliary prediction problems pivot terms highly predictive terms expected behave similar waydomains (e.g., intriguing, annoying, captivating film book reviews). SCLfirst applied cross-domain adaptation sentiment classification (Blitzer, Dredze, &Pereira, 2007). later applied cross-lingual adaptation (Prettenhofer & Stein, 2010)enhancing pivot term one equivalent translations target language(e.g., intriguing intrigante, annoying noioso, captivating travolgente EnglishBooksItalianBooks cross-lingual adaptation). Even though SCL successfully validatedcross-domain cross-lingual scenarios, suffers considerable computationalcost, deriving intermediate optimizations auxiliary predictive problemsuse LSA.method present paper, dub Distributional Correspondence Indexing (DCI), inspired SCL follows different, simpler approach,direct application distributional hypothesis. propose mine distributionalcorrespondences term small set pivot terms. hypothesizecorrespondences approximately invariant across domains termsequivalent roles two domains. example, expect distributional correspondence source term fs = book pivots p1s = intriguing, p2s = annoying,p3s = captivating, approximately similar distributional correspondencetarget term ft = film pivots p1t , p2t , p3t (Books DVDs cross-domainadaptation). Analogously, expect distributional correspondence source(English) term fs = book pivots p1s = intriguing, p2s = annoying, p3s = captivating,approximately similar distributional correspondence target (Italian)term ft = libro pivot translations p1t = intrigante, p2t = irritante p3t = accattivante (EnglishBooks ItalianBooks cross-lingual adaptation). Contrarily SCL, definedistributional correspondences distributional correspondence functiondirectly mines term vectors unlabelled collections, computed efficiently.present work extension work Esuli Moreo Fernandez (2015),preliminary intuitions underlying method applied cross-lingual case.present improved version DCI compares favourably respect stateart extensive experimental comparisons carried two popular senti133fiMoreo, Esuli, & Sebastianiment classification datasets cover cross-domain cross-lingual adaptation.experiments also show DCI substantially smaller computational costrespect competition. cross-lingual scenario, show DCI require smalleramount human intervention order create cross-lingual pivots. final contribution, explore general complex formulation domain adaptationproblem combines cross-domain setting cross-lingual setting; presentexperimental results compare method state-of-the-art methods.rest article structured follows. Section 2 overviews related workdomain adaptation. Section 3 formally states problem presents notationgoing use. Section 4 formally defines distributional correspondence functions,method whole described Section 5. Section 6 presents experimentsanalysis results, Section 7 concludes.2. Related Worksection offers brief overview main related methods literature domainadaptation sentiment classification. Traditionally, two different types approachesdomain adaptation identified. first group instance transfer methods aimre-weighting relative importance training document, order compensatedisagreements source target marginal probability distributions (Dai,Xue, Yang, & Yu, 2007; Gao, Fan, Jiang, & Han, 2008). methods appliedcross-domain (and cross-lingual) adaptation problems, since cannot solveproblem posed fact that, cross-lingual setting, term sets sourcetarget domains disjoint. second group feature-representation transfer methods project documents domains common vector space standardclassification algorithms could applied. DCI method propose belongsfeature-representation transfer class, thus applied cross-domaincross-lingual settings. following review relevant work crossdomain setting (Section 2.1) cross-lingual setting (Section 2.2). interestedreader check (Pan & Yang, 2010; Pan et al., 2012) surveys transfer learningmethods.2.1 Cross-Domain AdaptationStructural Correspondence Learning method (Blitzer et al., 2006), already discussedintroduction, extends Structural Learning paradigm Ando Zhang (2005)introducing concept pivot features. SCL applied cross-domain adaptation (Blitzer et al., 2007) leveraging notion predictive power pivot. similar criterion adopted discern domain-specific domain-independentterms Spectral Feature Alignment (SFA Pan, Ni, Sun, Yang, & Chen, 2010), methodclustering domain-specific terms source target domains latent spacemining relationships domain-independent terms.Aside sets unlabelled documents domain, methods take advantageexternal general-purpose knowledge resources order bridge gap domains.example, Wang, Domeniconi, Hu (2008) extended co-clustering approachpropagate labels two domains using Wikipedia represent documents134fiDistributional Correspondence Indexingmeans concepts. recently, Bridging Information Gap method (Xiang, Cao, Hu,& Yang, 2010) followed similar motivation, exploiting Wikipedia Open DirectoryProject general-purpose knowledge base. sentiment classification, relatedmethods use sentiment lexicon external resource. Joint Sentiment-Topic Model(JSTM), proposed He, Lin, Alani (2011) extension Latent Dirichlet Allocation, consists augmenting terms polarity-bearing topics using sentiment lexiconrepository prior word sentiment. JSTM found perform better SCLcomparably SFA. Denecke (2009) studied viability SentiWordNet, well-knownsentiment lexicon, lexicon cross-domain sentiment adaptation; main drawbackmethods dependence availability suitable public resources/ lexicons language application targets. Li, Pan, Jin, Yang, Zhu(2012a) alleviated constraint automatically co-extracting topic lexicon sentiment lexicon target domain, exploiting information source domain.Similarly, Bollegala, Weir, Carroll (2011) obtained sentiment-sensitive thesaurusaugment term vectors. main peculiarity approach lexicon createdmining multiple source domains. Similarly, following completely different approachbased deep learning architectures, Stacked Denoising Autoencoder (SDAsh ) method(Glorot, Bordes, & Bengio, 2011) exploits unlabelled information contained multipledomains order improve vector representations terms unsupervised fashion.Glorot et al. found method (which use one baselines experiments)scale well large multi-domain collections, outperforming SCL SFA using 22different domains unlabelled documents.branches research related cross-domain methods binary classificationexist tested sentiment classification topic classification, usingpopular datasets Reuters-21578, 20-Newsgroups, SRAA. relevant examples include Spectral Domain Transfer Learning (Ling, Dai, Xue, Yang, & Yu, 2008), MatrixTrifactorization (Zhuang, Luo, Xiong, He, Xiong, & Shi, 2011), Topic Correlation Analysis(Li, Jin, & Long, 2012b), Topic-Bridged Probabilistic LSA (Xue, Dai, Yang, & Yu,2008). Aside fact methods explicitly designed classify according sentiment, approaches also faced different problem setting, i.e., testset target domain available though labels omitted modellingclassification hypothesis, collection unlabelled documents availablesource target domain. approaches thus fall domain transductivelearning (see e.g., Joachims, 1999), thus directly related approach,completely inductive.2.2 Cross-Lingual Adaptationreview prior work cross-lingual adaptation, covering three different types approaches: (i) methods relying automatic machine translation, (ii) methods exploitingparallel corpora, (iii) methods exploiting unlabelled topic-specific collections.Rigutini, Maggini, Liu (2005) proposed method cross-lingual adaptationfirst translates source documents target language means automatic machine translation service. Then, Expectation Maximization method refines translatedrepresentations mining statistical properties large sets unlabelled documents135fiMoreo, Esuli, & Sebastianitarget language. Along lines, Wan, Pan, Li (2011) proposed bi-weightingmethod re-weight terms training instances order correct word driftmachine translation could introduced translation process. Motivatedlack labelled Chinese sentiment corpora, Wan (2009) proposed instead English-Chineseco-training approach based automatic machine translation. method translateslabelled English (source) documents Chinese (target), Chinese unlabelled documents English. classifier created languages, laterused classify respective set unlabelled documents improve model. Finally,Chinese test document attached translation equivalent English giveninput classifier.Even though machine translation represents promising solution cross-lingual problems (a solution presumably become viable field machinetranslation evolves), current machine translation services always free-to-use,available language pairs either, computationally expensive. thingsequal, cross-lingual methods rely thus preferable.Latent Semantic Analysis well-known technique originated within monolingualtext analysis (Deerwester et al., 1990) later extended deal cross-lingual retrieval (Dumais, Letsche, Littman, & Landauer, 1997) multilingual classification (Xiao& Guo, 2013). LSA consists mapping original term-document matrix lowerdimensional latent semantic space captures (linear) relations among originalterms documents. cross-lingual context, mapping performed via singular value decomposition original term-document matrix extracted multilingualdocuments. main problem use LSA cross-lingual applicationsdependence parallel corpus. order relax constraint, Xiao Guo (2014)proposed method induces cross-lingual terms via matrix completion. methodrequires small set parallel documents used construct dual-languageco-occurrence matrix; LSA applied completed dual-language matrix orderproduce low-dimensional interlingual representation. Cross-lingual Kernel CanonicalCorrelation Analysis (KCCA Vinokourov, Shawe-Taylor, & Cristianini, 2002) producesinstead semantic cross-lingual representation investigating correlations alignedbilingual fragments. KCCA takes advantage kernel functions order map aligned textshigh-dimensional space manner correlations mappingsmutually maximized. Finally, Oriented Principal Component Analysis (OPCA Platt,Toutanova, & Yih, 2010) finds discriminative projection maximizes documentvariance across languages, time minimizing distance comparabledocuments, thus avoiding use artificially concatenated documents.techniques based correlation analysis discussed ratherexpensive computational point view, use requires availabilityparallel comparable corpus. reason Moen Marsi (2013) proposed useRandom Indexing (Sahlgren, 2005), computationally lighter indexing approachapproximates LSA (Kanerva, Kristofersson, & Holst, 2000), alternative use crosslingual information retrieval. Cross-lingual Random Indexing requires monolingualcorpus language, plus bilingual dictionary.Even though approaches discussed succeed discovering hiddencorrelations terms belonging different languages, still based136fiDistributional Correspondence Indexingavailability suitable parallel corpus bilingual dictionary. response, GliozzoStrapparava (2005, 2006) provided means automatically obtaining MultilingualDomain Model (MDM) defining soft relations words domain topics. Makinglack bilingual dictionary parallel corpus, MDM automaticallyobtained comparable corpora performing LSA. similar vein, Rapp (1999)proposed method acquiring bilingual dictionary based assumptioncorrelation word co-occurrence patterns different languages (Rapp, 1995).dimensions co-occurrence matrices rearranged make translationequivalents word correspond identical positions vector, usingsmall bilingual dictionary. Word translation performed vector similaritytwo co-occurrence matrices, ignoring dimensions aligned. dictionaryiteratively expanded inclusion newly translated terms. Koehn Knight(2002) proposed method automatically constructing word-level translation lexicontaking monolingual corpus language input, neither requiring corporaparallel even comparable, requiring availability initial dictionary.Roughly speaking, done first taking words identical spelling (cognates)similar spelling initial entries dictionary, expanding dictionaryassuming context, frequency, word correlations approximately preservedacross languages. recently, Peirsman Pado (2010) proposed method induceselectional preferences resource-poor languages also takes advantage cognates.bilingual vector space initially derived taking cognates dimensionsvector space. space, subsequently bootstrapped large (unparsed) corporalanguages, allows direct word translations performed based vector distances.realistic cross-lingual setting expect sort labelled corpusavailable target domain, machine translation tools, available, stillexpensive. Therefore, Prettenhofer Stein (2010, 2011) assume word translationoracle available limited budget calls (450, experiments).resulting word translations allow Structural Correspondence Learning (SCL see Section2.1) applied cross-lingual domain adaptation pairing source pivotequivalent translation target language. Even though cross-lingual SCL succeedsrelaxing constraints discussed thanks fact needlinguistic resource, still suffers considerable computational cost derivingneed perform intermediate optimizations structural problems, needuse LSA. Following similar strategy relying bilingual pivots, DCI method requiressignificantly fewer word translations, avoids use expensive statistical analysistechnique.method bears resemblance Explicit Semantic Analysis (ESA), methodindexes given text respect set explicitly given external categories(Gabrilovich & Markovitch, 2007). work Sorg Cimiano (2008, 2012) differentlanguage-specific views Wikipedia articles considered external categoriessemantic term vectors defined. dimension thus models strengthassociation given term given article cross-lingual information retrievalsetting (CL-ESA). Arguably, main difference method CL-ESA relieshigh-dimensional spaces (about 10,000 dimensions) interlingual universal concepts,DCI instead projects term low-dimensional space (about 100 dimensions)137fiMoreo, Esuli, & Sebastianihighly predictive concepts, i.e., bilingual pivots. Additionally, method requiresort external resource apart word translation oracle, strengthassociation rather defined terms distributional correspondence, computed efficientlyunlabelled sets (Section 4).3. Problem Statementsection formally state problem set notation going usethroughout paper.Sentiment classification may viewed task approximating unknown targetfunction : X Y, indicates documents ought classified, meansfunction : X Y, called classifier, X denotes space documents= {+1, 1} denotes space labels, indicating positive (+1) negative (-1)sentiment. domain adaptation (see e.g., Pan & Yang, 2010) customary definedomain pair = hF, P (X)i, P (X) marginal probability distributiongoverns likelihood documents (represented term space F ) drawn.Given source domain Ds = hFs , Ps (X)i target domain Dt = hFt , Pt (X)i, domainadaptation subtask transfer learning consists improving accuracyclassifier Dt using knowledge Ds , domain Ds 6= Dt .precondition Ds 6= Dt leads two different interpretations domain adaptation problem. one side, cross-domain adaptation (e.g., DVDs Books) typicallycharacterized Fs = Ft Ps 6= Pt ; is, term space common triviallymade common joining two term spaces marginal probability distributionsdiffer. side, cross-lingual adaptation (e.g., EnglishBooks GermanBooks)typically characterized Fs 6= Ft Ps = Pt ; is, documents drawn similardistributions described different term spaces.paper also define investigate third case domain adaptation,term spaces probability distributions differ. crossdomain/cross-lingual adaptation problem, characterized Fs 6= Ft Ps 6= Pt .argue case particular interest, since enables cross-lingual adaptationperformed even absence auxiliary dataset acts bridge two-stepsadaptation (e.g., EnglishBooks GermanBooks GermanDVDs). example, sentiment classifier resource-poor language needs deployed analyses sentimentnew topic, common scenario one want leverage dataresource-rich language (e.g., English) different, already known topic. scenariorealistic generalization domain adaptation problem; best knowledge,nobody tackled published work.final note regarding notation, use subscripts indicate whetherdata drawn source target domain, respectively. Accordingly,Us denotes unlabelled source dataset, Ut refers unlabelled target dataset.Similarly, rs et denote training set test set, respectively.138fiDistributional Correspondence Indexing4. Distributional Correspondence Functionsgoal section introduce Distributional Correspondence Functions (DCFs).first characterize family DCFs propose specific ones.4.1 DefinitionDCFs family real-valued functions quantify degree correspondencetwo terms f f j comparing context distribution vectors f f junlabelled collection U . context distribution vector unit-length n-dimensionalvector models term relates set contexts. context text unitterm could appear (e.g., sentence, fixed-size window, entire document);fci denotes value vector term f context c, fci = 0 f appearcontext c. cases fci > 0 determined weighting function use,might lead different interpretations DCF, e.g., probability functionevent space (Section 4.2), kernel vector space (Section 4.3). define piprevalence f , i.e., portion contexts fci > 0, i.e.,pi =|{c|fci > 0}|n(1)n dimensionality vector space, i.e., number different contexts.work take documents contexts, fci = 0 means term f appeardocument c, pi portion documents unlabelled collection fappears.DCF function : Rn Rn R, sign (f , f j ) indicates polaritycorrespondence, i.e., positive values indicate positive correlation negative valuesindicate negative correlation; (f , f j ) = 0 indicates null correspondence. forceDCFs (f , f j ) = 0 expected correspondence measuredrandomly chosen pairs vectors prevalence pi pj terms f f jset. rationale choice high-prevalence terms higher probabilitynon-zero values number common positions, case measures(see Section 4.3) record level correspondence due non-perfect orthogonalityvectors, happens much rarely low-prevalence terms. wantfactor bias, centering DFC expected correspondence value.4.2 Probability-Based DCFsection discuss probability-based DCFs derived information theory.distribution P (f ) term f across contexts modelled using binomial eventspace, thus considering presence absence term context; P (f )thus denotes probability f occurs random context, P (f ) denotesprobability f occur it.first part Table 1 shows probability-based DCFs investigate. considerPointwise Mutual Information (P I, ratio joint distributionproduct marginal distributions), simple probabilistic function (here calledjLinear ) contrasts probabilities f conditioned f j f , respectively.139fiMoreo, Esuli, & Sebastianialso consider Mutual Information (M I, reduction entropy distribution dueobservation another distribution) asymmetric version. rationaleasymmetry per se symmetric respect positive negative correlation;is, two cases (a) f occurs contexts f j alsooccurs, (b) f occurs contexts f j occur, obtainscore. scenario two kinds correlation must kept distinct,high positive correlation indicates semantic similarity, high negative correlationindicates lack semantic similarity. reason invert sign DCFnegative correlation case, defining function detects negative correlationusing true positive rate (tpr = P (f , f j )/P (f j )) true negative rate (tnr =jjP (f , f )/P (f )), i.e.,+1 (tpr + tnr > 1)j(f , f ) =(2)1otherwisemultiplying (f , f j ) I, yield Asymmetric Mutual Information (AMI, seeTable 1).4.3 Kernel-Based DCFssection consider different kernel functions DCFs. Kernel functions similarityfunctions, typically used e.g., within support vector machines operate high- (andpotentially infinite-) dimensional spaces. case values context vectornumeric, thus indicating relative importance term given context,usually computed function frequency term context corpus,e.g., tf idf . consider normalized context vectors, i.e., weighting document-byterm matrix normalize term vectors unit length.popular vector similarity measure probably cosine similarity, measurescosine angle them, definedcos(f , f j ) =hf , f jkf kkf j k(3)also consider DCFs popular kernels: polynomial kernel RadialBasis Function (RBF a.k.a. Gaussian) kernel, i.e.,polynomiala,b (f , f j ) = (a + hf , f j i)bj(4)j 2RBF (f , f ) = exp{kf f k }(5)qPj 2njkf f j k =c=1 (fc fc ) Euclidean distance f f .Since non-zero values frequency vector always positive, turnsexpected value dot product Euclidean distance two randomdistributional vectors greater zero. order satisfy necessary conditionimposed DCFs kernels centred zero factoring bias. Letri rj two unit-length vectors prevalences pi pj , whose non-zero valuesindependently distributed random; expected value non-zero positions pi n1pj n1 , respectively. expected value dot product Euclidean140fiDistributional Correspondence Indexingdistance ri rj are, respectively,11= pi pjpi n pj n21111j+ n(pi pi pj )E[kr r k] = npi pj+ n(pj pi pj )pi npj npi npj n= 2(1 pi pj )E[hri , rj i] = pi pj n(6)(7)resulting DCFs, obtained factoring expected values correspondingkernels, reported second part Table 1.Table 1: Mathematical forms DCFs discussed work.Probability-based DCFsLinearPointwise Mutual InformationAsymmetric Mutual InformationMathematical formP (f |f j ) P (f |f j )P (f , f j )P (f )P (f j )X(f , f j )log2Xx{f ,f } y{f j ,f j }Kernel-based DCFsCosinePolynomialRBFP (x, y) log2P (x, y)P (x)P (y)Mathematical formhf , f jpi pjjkf kkf k(a + hf , f j i)b (a +pi pj )bn2exp{kf f j k2 } exp 4 1 pi pj5. Distributional Correspondence Indexingsection explain Distributional Correspondence Indexing (DCI) methoddetail, paying special attention step workflow.DCI method works first identifying small set pivot terms (or pivots,short Section 5.1), projects term new vector spacedimension measures correspondence term pivot (Section 5.2). termthus assigned new vectorial representation referred termprofile, simply profile. term space post-processed first normalizingdimension (Section 5.3) unifying source target term profiles certainterms expect behave similarly domains, pivots (Section 5.4).Documents projected cumulating (i.e., summing) profiles termsoccur (Section 5.5), classifier trained usual. order141fiMoreo, Esuli, & Sebastianiclarify explanation use running example throughout different steps.aim tracking case Books Electronics, i.e., domain adaptationBooks domain Electronics one (more details datasets givenSection 6.1).5.1 Pivot SelectionPivots terms shared across source target domains, meant linkthem, thus enabling knowledge transfer process. Blitzer et al. (2006) defined pivotsterms occur frequently source target domains behave similarlydomains. Subsequent work (Blitzer et al., 2007; Prettenhofer & Stein, 2010; Pan et al., 2010)extended definition pivots take account also co-occurrence relationterms class labels source domain, shown informative termsprediction task better pivots. idea later adapted cross-lingual settingPrettenhofer Stein (2010), fixed frequency threshold , called support,used filter infrequent pivot candidates, ranking remaining candidatesmutual information respect labels source domain. Prettenhofer Steinalso introduced notion word translation oracle (WTO), i.e., translator that, givenword source language, provides translation target language. methodPrettenhofer Stein assumes possibility issuing limited number callsoracle.pointed Pan et al. (2010), pivot selection using mutual information helpidentify predictive terms source domain, guarantee termsact similarly domains. respect, think even though supportthreshold might serve filter problematic candidates, strategy suboptimal.example, candidate occurring 29 times 31 times 50,000 sourcetarget domain, respectively, discarded support set = 30,pivot occurring 5000 times 31 times could chosen, even clearsecond case role two domains different.good pivot highly task-dependent, also present similar degreedomain-dependence two domains. formalized intuition via function(f ) = (f )st (f )(8)(f ) terms strength pivot, (f ) quantifies informativeness (toestimated training set rs ) term f classification task, st (f )measures cross-consistency term f estimated Us Ut , quantifiessimilarly term behaves two domains.Following previous research, instantiate (f ) via mutual information. Ideally,st : F [0, 1] defined way st (f ) 1 distributionf consistent across domains, st (f ) 0 importance f varies lotone domain another. Given lack labelling information target domain,adopt simple heuristic relates prevalence domains, might expectcomparable prevalence use text terms posses similar degree domain142fiDistributional Correspondence Indexingdependence1 . thus definest (f ) =min{psi , pti }max{psi , pti }(9)psf (resp., pti ) prevalence f measured set Us (resp., Ut ). top-rankedterms according value frequency greater Us Ut ,selected pivots; user defined parameter indicating number pivots select.Table 2 exemplifies pivot selection process Books Electronics adaptation.instance, cross-consistency weight succeeds penalizing adjective boring, mightgood candidate predicting polarity book reviews rather uninformativeelectronic devices, thus pushed top-100 list (only 10 elementsshown due space limitations).Table 2: Top-10 terms ranked according mutual information (Is (f )) (left), mutualinformation combined cross-consistency ((f ) = (f )st (f )) (right).#12345678910termwasteboringdisappointingexcellentwastebaddontmoneyscore0.0290.0290.0290.0260.0210.0210.0210.0190.0180.018termwasteexcellentbaddontwastehighly recommenddisappointinggreatmoneyscore0.0280.0220.0200.0180.0170.0170.0140.0130.0120.0125.2 Term Profilesimplement rather direct application distributional hypothesis, followingintuition semantics term captured distributional correspondencepivots. thus build term profile f~ source target term f (includingpivot terms) m-dimensional vectorf~ = ((f , p1 ), (f , p2 ), . . . , (f , pm ))(10)f pi context distribution vector unlabelled collection termf profiled ith pivot, respectively, selected DCF.Table 3 displays term profiles associated four relevant terms runningexample, i.e., boring, excellent, waste, reliable. Note excellent waste pivotsopposite polarity, two domain-dependent terms, i.e., boring1. Note st () function meant capture cross-domain drift, thereby ignorecross-lingual setting simply defining st (f ) = 1 case domain knowledgecollections.143fiMoreo, Esuli, & Sebastianiinformative Books reviews reliable informative Electronics reviews.DCF used example cosine kernel. Note boring representationtarget side, term appear Electronics dataset. Note alsopivot terms associated vectorial representations somehow closedomains. happen reliable, domain-dependent termplays different roles two domains. also note correspondence tendspositive terms similar polarity (e.g., waste bad), negativeotherwise (e.g., excellent bad). Finally, notice cos(waste, waste) 6= 1cos(excellent, excellent) 6= 1, due correction factor introduced cosine formula(see Table 1).Table 3: Term profiles generated terms boring, excellent, waste, reliable (rows)different dimensions (columns) source (left) target (right).first 5 dimensions, corresponding pivots waste, excellent, bad, no, dont,shown due space restrictions.boringexcellentwastereliablewaste0.058-0.0300.957-0.012excellent-0.0420.938-0.030-0.002Booksbad0.029-0.0510.007-0.007-0.029-0.082-0.0130.004dont0.004-0.0650.1610.016...............waste-0.0420.9590.001excellent0.927-0.0420.014Electronicsbad-0.023 -0.0210.028 0.0250.005 -0.004dont-0.0180.138-0.008...............5.3 Normalization Term Profilesdimension space reflects distributional correspondence given pivot.Pivots high prevalence likely generate high DCF values, could leaddominant dimensions profile vectors; could detrimental learningphase. avoid effect, center profile dimension expected valuerescale standard deviation (Equation 11), values profile dimensionsapproximately normally distributed N (0, 1), i.e.,f~0 =f~1 1 f~2 2f~m,,...,12!(11)f~i ith dimension term profile f~ (see Equation 10),mean standard deviation ith dimension, respectively. normalization, term profile vectors rescaled unit length.Table 4 demonstrates effect term normalization example terms discussedTable 3. Note that, normalization, source target profiles pivot termsseem get closer vectorial space. Furthermore, target representation reliableturns consistent intuitions, reflects negative correspondencewaste, stronger positive correspondence excellent.144fiDistributional Correspondence IndexingTable 4: Effect term normalization terms boring, excellent, waste, reliable.boringexcellentwastereliablewaste0.223-0.0210.555-0.091excellent-0.1630.861-0.0170.020Booksbad0.105-0.0380.005-0.056-0.125-0.080-0.0080.076dont0.020-0.0500.0940.169...............waste-0.0350.572-0.011excellent0.909-0.0320.119Electronicsbad-0.024 -0.0380.016 0.0180.005 -0.146dont-0.0230.087-0.138...............5.4 Unification Term Profilesassume pivot terms behave similarly two languages, unify termprofiles simply averaging source profile target profile normalizingresult unit length. Unification also applied profiles terms appearsource target domains frequency greater support .rationale behind unification correct possible misalignment sourcetarget term profiles terms receive vectorial representationdomains, pivot terms proper nouns. done order equalize acrossdomains contribution term document representation (see below).Table 5 shows term profiles running example unification. Noteboring experience change target counterpart even exist. Termreliable also affected normalization, frequency Books domainexceed support , set 30 example. Finally, term profiles pivotsexcellent waste unified, i.e., computed average respective sourcetarget profile representations, normalized unit norm.Table 5: Term profile unification terms boring, excellent, waste, reliable.boringexcellentwastereliablewaste0.223-0.0280.570-0.091excellent-0.1630.893-0.0250.020Booksbad0.105-0.0310.010-0.056-0.125-0.0590.0050.076dont0.020-0.0370.0910.169...............waste-0.0280.570-0.011excellent0.893-0.0250.119Electronicsbad-0.031 -0.0590.010 0.0050.005 -0.146dont-0.0370.091-0.138...............5.5 Document IndexingFinally, train test documents indexed profile space via weighted sumprofile vectors associated terms. is, document dj representedm-dimensional vectorXd~j =wij f~0(12)fi djwij weight term fi document dj according weighting function (inexperiments used standard cosine-normalized tf idf ), f~i0 normalizedunified term profile vector fi .145fiMoreo, Esuli, & Sebastiani6. Experimentssection experimentally compare DCI method, implemented using differentDCFs, state-of-the-art methods proposed literature.6.1 Datasetstest method two popular, publicly available sentiment datasets: Multi-DomainSentiment Dataset (version 2.0) Webis-CLS-10. former dataset frequently usedevaluating cross-domain adaptation, latter often used evaluatingcross-lingual methods. also use Webis-CLS-10 explore cross-domain/crosslingual setting.6.1.1 Multi-domain Sentiment Dataset (version 2.0)Multi-Domain Sentiment (MDS) dataset, first proposed Blitzer et al. (2007), contains English product reviews taken Amazon.com four domains Books (B),DVDs (D), Electronics (E), Kitchen (K) appliances. order facilitate reproducibilityallow fair comparison results reported previous literature, usedpre-processed version dataset used previous evaluations, made publiclyavailable Blitzer et al. (see MSD dataset, 2007) . pre-processed version, termsextracted taking unigrams bigrams; reviews originally rated higher 3stars labelled positive, rated lower 3 stars negative; reviewsintermediate ratings removed. dataset comprises 1000 positive reviews1000 negative reviews four domains, set unlabelled documents ranging 3,586 5,945 documents domain. Table 6 shows number labelledunlabelled documents, number distinct terms total number termsdataset. According evaluation procedure followed proposersmethods compare against, randomly split labelled dataset training set1600 instances test set 400 instances.Table 6: Main characteristics Multi-Domain Sentiment dataset (version 2.0).DomainBooksDVDsElectronicsKitchenLabelled2,0002,0002,0002,000Unlabelled4,4653,5865,6815,945Terms195,887188,778111,40793,474Occurrences445,793370,844392,699351,1626.1.2 Webis-CLS-10Webis-CLS-10, first proposed Prettenhofer Stein (2010), cross-lingual sentimentcollection consisting Amazon product reviews written four languages (English (E),German (G), French (F), Japanese (J)), covering three product domains (Books (B),DVDs (D), Music (M)). language-domain pair 2,000 training documents, 2,000 test documents, 9,000 50,000 unlabelled documents depending146fiDistributional Correspondence Indexinglanguage-domain combination (see Table 7 details). used preprocessed version dataset made publicly available authors (see Webis-CLSdataset, 2010), terms correspond uni-grams. Following work PrettenhoferStein, consider English source language, since far realisticscenario. Documents either labelled positive negative, following procedure Blitzer et al. (2007). Positive negative examples balanced sets (seeTable 7 details). labelled dataset split perfectly balanced training set2,000 instances test set 2,000 instances. split proposed PrettenhoferStein; baseline methods compare use exactly corpustraining test.Table 7: Main characteristics Webis-CLS-10 dataset.DomainEBEDEMGBGDGMFBFDFMJBJDJMLabelled4,0004,0004,0004,0004,0004,0004,0004,0004,0004,0004,0004,000Unlabelled50,00030,00025,22050,00050,00050,00032,8709,35815,94050,00050,00050,000Terms62,49950,12438,632105,360100,26595,95252,66426,11739,00151,17953,31853,078Occurrences6,289,0144,001,6782,664,9556,618,0376,303,3715,688,8742,427,178714,1051,371,8007,637,3257,263,7966,284,6536.2 Evaluation MetricsFollowing practice common related literature, adopt standard accuracyevaluation measure. Accuracy measures proportion correctly classified documentstotal number outcomes (Equation 13), i.e.,Acc =TP + TNTP + FP + FN + TN(13)P , N , F P , F N stand numbers true positives, true negatives, falsepositives, false negatives, respectively. Note measure perfectly adequatechoice since datasets balanced respect positive negative classes.6.3 Baseline Methodsexperimentally compare DCI, using different DCFs, different baseline methods proposed literature cross-domain cross-lingual domain adaptation. limitcomparison algorithms evaluated corpora, report results takenoriginal papers. explicitly mentioned, results baseline algorithms147fiMoreo, Esuli, & Sebastianiobtained using datasets, though obviously using different random splits.MDS dataset, following common practice, run experiments multiple randomsplits average them. Webis-CLS-10 dataset instead used split proposedPrettenhofer Stein (2010) (more details below).upper bound, implemented method (hereafter called Upper) trainsSVM classifier training set target domain. lower bound insteadimplemented method trains SVM classifier source domain appliestrained classifier directly target domain, i.e., without carrying sortknowledge transfer (NoTrans). considering various languages, also reportmachine translation baseline (MT), first translates target documentssource language (i.e., English experiments) giving input SVMclassifier trained source domain; used pre-translated documents providedPrettenhofer Stein (2011).cross-domain adaptation baselines consider Structural CorrespondenceLearning using Mutual Information select pivots (SCL-MI Blitzer et al., 2007), SpectralFeature Alignment (SFA Pan et al., 2010), multiple sources Sentiment Sensitive Thesaurus(SST Bollegala et al., 2011), Stacked Denoising Autoencoder (Glorot et al., 2011)trained domain pairs (SDA) trained 22 domains available former versionMDS dataset (a method authors abbreviate SDAsh ).cross-lingual adaptation baselines consider cross-lingual Latent SemanticIndexing (LSI Dumais et al., 1997), cross-lingual Kernel Canonical Correlation Analysis(KCCA Vinokourov et al., 2002), Oriented Principal Component Analysis (OPCAPlatt et al., 2010), Two-Step Learning method (TSL Xiao & Guo, 2013), Semi-SupervisedMatrix Completion (SSMC Xiao & Guo, 2014), cross-lingual version StructuralCorrespondence Learning (SCL Prettenhofer & Stein, 2011).Since published results compare cross-lingual/cross-domainadaptation, baseline consider SCL-MI (Prettenhofer & Stein, 2011) reusingpublicly available source code (Natural Language Understanding Toolkit, 2011) runningexperiments.6.4 Implementation Details Parameter Settingimplemented method (see DCI-source, 2015) part JaTeCS (2015) framework. used popular SVMlight (2008) implementation Support Vector Machineslearning device, default parameters, DCI baselines NoTrans, Upper,MT.experiments set support (see Section 5.1) = 30, following indications Prettenhofer Stein (2010) Webis-CLS-10 dataset. Since amountunlabelled documents MDS Dataset one order magnitude smaller, caseset = 1.emulate word oracle sake fair comparison reused bilingual dictionary2 created evaluating cross-lingual SCL Prettenhofer Stein (2010).dictionary emulates context-unaware word-translation oracle, i.e., source word2. Note used different pivot selection criterion, detailed Section 5.1, therefore oraclecould queried translate words never considered cited work, thus might148fiDistributional Correspondence Indexingmapped likely translation; potential problems arising ambiguitysingle words simply disregarded.One important factor take consideration number calls issuedoracle; oracle simulates human translator, number thus indicatorhuman effort required perform domain adaptation. limited number translationstop 2m terms highest mutual information, number pivotswords. order perform comparisons methods, fixed number pivots= 100, corresponds minimal setup tested Prettenhofer Stein(2010). Section 6.6 explore impact accuracy due variations valuem.Parameters polynomial RBF kernel DCFs optimized via grid searchBooks DVDs cross-domain adaptation MDS dataset, EnglishBooksGermanBooks cross-lingual adaptation Webis-CLS-10 dataset (as done previousresearch, Prettenhofer & Stein, 2010). actual values ended using b = 0.5= 0.82 MDS, b = 0.8 = 0.88 Webis-CLS-10. set = 0(homogeneous) polynomial DCF cases, perceive consistentimprovement justifies complex grid search exploration two parameters.used normalized tf idf weighting criterion represent co-occurrence matricesexperiments.6.5 Experimental Resultssection present results experiments using different DCFs. Experiments presented different domain adaptation setups, including cross-domain adaptation (Section 6.5.1), cross-lingual adaptation (Section 6.5.2), cross-domain/cross-lingualadaptation (Section 6.5.3). Additional related experiments conducted Section6.6.sake brevity consistently notation Ls Cs Lt Ct introducedearlier, use single upper-case character (as defined Section 6.1) denotelanguages domains involved experimental setup. example, EB GDdenotes experiment EnglishBooks used source GermanDVDs usedtarget.6.5.1 Cross-Domain ResultsTable 8 reports results obtained MDS dataset, including (a) performance averagesproduct category, i.e., computed averaging results obtained productcategory considered target domain, (b) global averages. results reportedcorrespond accuracy different methods3 computed via 5-fold cross-validation,i.e., using 1,600 training documents 400 test documents run. direct comparison many methods (He et al., 2011; Xia & Zong, 2011; Denecke, 2009;Ponomareva & Thelwall, 2012) would also feasible principle. omitted direct comnot present dictionary. cases happened rarely however, preferred simply skipcandidates rather completing bilingual dictionary, order guarantee fair comparison.3. Missing results ones reported original papers.149fiMoreo, Esuli, & Sebastianiparisons methods since previous research shown comparable,superior, SFA.Table 8: Cross-domain adaptation MDS dataset.TaskED EBEE EBEK EBEB EDEE EDEK EDEB EEED EEEK EEEB EKED EKEE EKBooksDVDsElectronicsKitchenAverageNoTrans0.7280.7070.7090.7720.7060.7270.7080.7300.8270.7450.7400.8400.7150.7350.7550.7750.745Upper0.8440.8440.8440.8470.8470.8470.8690.8690.8690.9020.9020.9020.8440.8470.8690.9020.866SCL-MI0.7970.7540.6860.7580.7620.7690.7590.7410.8680.7890.8140.8590.7460.7630.7890.8210.780SFA0.7750.7570.7480.8140.7720.7660.7250.7670.8510.7880.8080.8680.7600.7840.7810.8210.786SST0.7630.7880.8360.8520.810SDA0.7240.7680.8070.8040.9020.8350.8060.8720.8020.8440.8030.7770.7660.8470.8270.8080.812SDAsh0.7680.7800.8370.8550.9050.8540.8240.8750.8200.8460.8210.8110.7950.8710.8400.8260.833Linear0.8250.7660.7830.8080.7680.7880.8100.8220.8550.8340.8580.8640.7910.7880.8290.8520.815PMI0.8270.7630.7830.8110.7790.7890.8220.8320.8510.8390.8560.8640.7910.7930.8350.8530.818AMI0.8110.7530.7690.8060.7650.7810.7930.8120.8430.8220.8460.8510.7780.7840.8160.8400.804Cos0.8240.7640.7900.8170.7740.7990.8220.8240.8580.8350.8640.8680.7930.7970.8350.8560.820Poly0.8300.7760.7910.8290.7990.8070.8260.8330.8630.8440.8610.8740.7990.8110.8410.8600.828RBF0.8250.7650.7840.8150.7710.7980.8210.8260.8570.8350.8630.8670.7910.7950.8350.8550.819configurations DCI outperform compared methods, exceptionSDAsh ; MI-based DCF performed slightly worse SST average.noted, however, SST SDAsh use different problem setting, since trainmultiple domains. concretely, SST trained three four domainsMDS dataset create sentiment thesaurus, SDAsh exploited 22 domains includedprevious version MDS dataset train auto-encoder. Furthermore, SDASDAsh rely deep learning approach, paradigm requires significantcomputational power many parameters tuned. Notwithstanding this, DCIpolynomial kernel DCF (with two parameters) obtained three best averagedresults (i.e., Books, Electronics, Kitchen) five, leveraging additionaldomain, requiring low computational cost (as later discussed Section 6.7).Table 9 reports experiments cross-domain adaptation using Webis-CLS-10 dataset.results consistent previous observations, i.e., polynomial cosinebased DCFs best performers, followed PMI RBF functions. casebest results obtained DCI close Upper, surprisingly surpassED EB EM EB. conjecture improvement may due largersize unlabelled sets, one order magnitude greater respect MDSdataset, thus allowing robust evaluations cross-domain consistency functionst () DCF.6.5.2 Cross-Lingual ResultsTable 10 reports results Webis-CLS-10 dataset cross-lingual adaptation.discussed earlier, source language always English, target languages includeGerman, French, Japanese.DCI outperformed MT baseline average cases PMI DCFGerman case. kernel-based DCFs outperformed compared methods terms150fiDistributional Correspondence IndexingTable 9: Cross-domain performance Webis-CLS-10 dataset.TaskED EBEM EBEB EDEM EDEB EMED EMBooksDVDsMusicAverageNoTrans0.8030.7830.7980.7780.7860.8040.7930.7880.7950.792Upper0.8290.8290.8310.8310.8450.8450.8290.8310.8450.835SCL-MI0.8390.8230.8100.7970.8040.8230.8310.8040.8140.816Linear0.8400.8280.7980.8020.8250.8310.8340.8000.8280.821PMI0.8430.8380.8120.8210.8350.8330.8410.8170.8340.830AMI0.8310.8260.7880.7980.8160.8150.8290.7930.8160.812Cos0.8510.8400.8180.8210.8380.8290.8460.8190.8340.833Poly0.8550.8410.8180.8220.8360.8320.8480.8200.8340.834RBF0.8480.8380.8060.8160.8310.8270.8430.8110.8290.828Table 10: Cross-lingual performance Webis-CLS-10 dataset.TaskEB GBED GDEM GMEB FBED FDEM FMEB JBED JDEM JMGermanFrenchJapaneseAverageUpper0.8680.8350.8590.8620.8720.8900.8120.8340.8420.8540.8750.8290.852MT0.8080.8000.7910.8210.7950.7650.6920.7220.7140.8000.7940.7090.767SCL-MI0.8330.8090.8290.8130.8040.7810.7700.7640.7730.8240.7990.7690.797LSI0.7760.7960.7270.7920.7780.7260.7380.7540.7340.7660.7650.7420.758KCCA0.7910.7760.6950.7670.7820.7480.7920.7820.7350.7540.7660.7700.763OPCA0.7470.7660.7140.7460.7050.7180.7450.7370.7500.7420.7230.7440.736TSL0.7920.8190.7260.8130.8200.7660.7940.7930.7620.7790.8000.7830.787SSMC0.8190.8230.8130.8310.8270.8050.7380.7760.7750.8180.8210.7630.801Linear0.7980.8260.8440.7460.8230.8160.7790.8220.8260.8230.7950.8090.809PMI0.7140.8190.8500.7610.8230.8270.7310.7680.8160.7940.8040.7720.790AMI0.7970.8000.8370.7680.8010.8180.7110.7970.8070.8110.7960.7720.793Cos0.8270.8220.8560.8420.8270.8440.7580.8010.8390.8350.8380.7990.824Poly0.8370.8330.8440.8190.8060.8400.7540.7950.8320.8380.8220.7940.818RBF0.8290.7880.8010.8440.8460.8030.7820.7610.8260.8060.8310.7900.809average accuracy, best result obtained one kernel-based DCFs 1112 cases. best performing DCFs cosine polynomial DCFs.6.5.3 Cross-Domain/Cross-Lingual ResultsTable 11 reports experiments cross-domain/cross-lingual setting.setting arguably difficult one, since term spacemarginal probabilities domains differ, reflected noticeable degradationMT results. Notwithstanding this, consistent observations could derived results. cosine polynomial DCFs confirm superiority respectcompared methods. best result obtained DCI 17 18 cases; halfcases cosine DCF obtained best result.6.5.4 Statistical Significance TestsStatistical significance tests (paired t-test accuracy values Table 8) indicateDCI configurations, exception MI, significantly better p < 0.01SCL, SCL-MI, SFA MDS dataset. polynomial DCF, obtained10 best results 12, higher-confidence levels obtained, i.e., p < 0.001. t-testruns Webis-CLS-10 reveals kernel-based DCFs Linear DCF151fiMoreo, Esuli, & SebastianiTable 11: Cross-domain/cross-lingual accuracy Webis-CLS-10 dataset.TaskED GBEM GBEB GDEM GDEB GMED GMED FBEM FBEB FDEM FDEB FMED FMED JBEM JBEB JDEM JDEB JMED JMGermanFrenchJapaneseBooksDVDsMusicAverageUpper0.8680.8680.8350.8350.8590.8590.8620.8620.8720.8720.8890.8890.8120.8120.8340.8340.8420.8420.8540.8740.8290.8470.8470.8630.852MT0.7890.7510.7740.7730.7680.7680.7880.7650.7830.7800.7710.7450.7000.6420.7080.6930.6730.7100.7710.7720.6880.7390.7520.7390.743SCL-MI0.8230.8250.7840.7920.8110.8240.7900.7840.7800.7450.7620.7570.7250.7080.7420.7560.7420.7760.8100.7700.7420.7760.7670.7790.774Linear0.8230.7910.7900.7780.7860.8440.7440.8100.8100.7980.8220.8360.7380.7110.8130.7920.8260.8170.8020.8030.7830.7700.7970.8220.796PMI0.7640.8210.7960.8290.8120.8440.7980.8330.8160.8220.7530.8260.6750.6210.6630.8280.6990.8040.8110.8080.7150.7520.7920.7900.778AMI0.8110.7050.7880.7720.7930.8280.7470.7850.7880.7610.7940.8270.7150.6360.7100.7210.8110.7620.7830.7840.7260.7330.7570.8030.768Cos0.8240.8120.8270.8340.8430.8160.8480.8450.8230.8410.8330.8470.7610.7210.8050.7900.8310.8160.8260.8400.7870.8020.8200.8310.818Poly0.8180.7910.8250.8140.8330.8350.8460.8430.7930.8290.8240.8490.7410.6890.7890.7630.8260.8170.8190.8310.7710.7880.8020.8310.807RBF0.8240.8000.7830.8080.8070.8320.8520.7890.8410.7750.8290.8550.7410.7220.7820.7110.8270.8040.8090.8240.7650.7880.7830.8260.799better SCL-MI statistical significance confidence level p < 0.01; CosineDCF obtained p = 0.854 107 Polynomial DCF p = 0.522 105 .6.6 Experimentssection presents experiments aimed testing influence different parameters modules DCI, performance standard text classification setting.Regarding effect parameters, show trend plots representative cases,considering Linear function representative example probabilistic-based DCFCosine function example kernel-based DCF. plot involves threesettings, one scenario: cross-domain adaptation, cross-lingual adaptation,cross-domain/cross-lingual adaptation. sake brevity selected illustrativeexamples, omitting experiments showing similar behaviour. First, investigated sensitivity value parameter m, indicates number pivots select. Figure1 shows performance varies variation m, 5 500 pivots.overall tendency displayed plots performance tends stabiliseincreases. Adaptations involving cross-lingual setting seem strongly affected152fiDistributional Correspondence IndexingCosine DCFAccuracyLinear DCF0.90.90.850.850.80.80.750.750.70.7ED->GD0.650.65ED->GM0.60.60.550.55ED->EB0.50.501002003004005000100200300400500Figure 1: Variation accuracy variation number pivots different setups.number pivots. attribute effect limited capability pivotreflect term correspondence imprecision introduced context-unawaresingle-word translations oracle. negative effect seems however reducenumber pivots increases. method scales well number pivots termsefficiency (see below), might indication simply increasing pivot setsize could feasible alternative rather moving complicated definitionscross-lingual pivots order cover translation nuances. Larger fluctuations couldobserved < 75, also surprising peaks performance extremely small valuesm. example, DCI obtained 81.1% accuracy Linear DCF ED GMadaptation 30 pivots, baselines obtained 76.8% (MT) 82.4% (SCLMI, uses 450 pivots). similar experiment reported Prettenhofer Stein(2010) CL-SCL, varying parameter range [100, 800]. direct comparisonshows method achieves better accuracy smaller values m. Givennumber calls oracle directly related (i.e., calls cross-lingual case,2m cross-domain/cross-lingual case due cross-consistency reweighting, seeSection 5.1), follows DCI requires less human effort creating bilingual pivots.unlabelled collection plays key role domain adaptation, responsibleterm-distribution representation; thus expect better estimations distributions larger collections. investigated unlabelled set size affectsperformance method. plot accuracy score obtained different reductionratios preserving balance Figure 2.expected, observed trend shows accuracy high large unlabelled collections, performance tends stabilize addition unlabelled examples. Betterperformance observed cross-domain experiments, even smaller distributionalrepresentations.also validated empirically different elements constitute DCImethod, including cross-distortion factor pivot selection, dimensionality standardization, unification process. reasons conciseness reportglobal improvement linear DCF averaged dataset, since consistent variationsobservable DCFs. found consistent improvement 0.47% 1.02153fiMoreo, Esuli, & SebastianiAccuracyLinear DCFCosine DCF0.90.90.850.850.80.80.750.750.70.7EB->EM0.650.65EB->JB0.60.60.550.55EB->JD0.50.500.20.40.6unlabeled reduction ratio0.8100.20.40.6unlabeled reduction ratio0.81Figure 2: Variation accuracy variation unlabelled corpus size differentsetups.accuracy due cross-consistency pivot selection, 1.785% 3.63 due dimensionalitystandardization, 1.261% 2.18 due unification.experiments reveal classification performance seems benefit adaptation involves semantically close domains, case BooksDVDs MDSdataset, EnglishGerman Webis-CLS-10. Analogously, performance seems degrade source target domains dissimilar, example KitchenBooksMDS EnglishJapanese Webis-CLS-10. noticed literaturereducing distance representations source target domains crucialorder allow better knowledge transfer. Given probability distributionsunknown, distance sometimes computed approximation (the proxy Adistance Ben-David, Blitzer, Crammer, & Pereira, 2006) considers source targetdocuments two samples drawn distribution. proxy A-distance computeddA = 2(1 2), error produced SVM trained discriminatesource target domains.Figure 3 graphically compares, MDS dataset, proxy A-distancesdomains (i) raw representations, (ii) DCI representations. dAclearly reduced cross-domain space generated DCI, contributes explainimprovement performance respect baseline NoTrans raw representation. reduction even noticeable semantically close domainsElectronicsKitchen BooksDVDs. Hence, DCI projects domains commonvector space source target distributions get effectively closer other,thus facilitating transfer knowledge them.Finally, Table 12 reports performance accuracy text classification setting, is,assuming test data follows marginal distribution representedterm space training data. case, consider baselines (a) well-knownBoW representation tf idf weighting, (b) SCL-MI.Even though amount experiments text classification case smallallow substantial claim, surprising that, runs, DCI 100 dimensionsyielded better results traditional BoW representation considering terms.topic investigate future research.154fiDistributional Correspondence IndexingProxy A-distance raw data2DKDEBKBD1.75EK1.51.2510.750.50.50.7511.251.51.752Proxy A-distance DCIFigure 3: Proxy A-distances domains MDS dataset. vertical axis displays dA raw data (NoTrans), horizontal axis displays dAvector space produced DCI using cosine DCF. abscissa coordinatepoint (e.g., BK) averaged dA produced domain adaptationdirections (e.g., EB EK EK EB).Table 12: Text Classification performance Webis-CLS-10 dataset.TaskEB EBED EDEM EMAverageBoW0.8290.8310.8450.835SCL-MI0.8280.8150.8320.825Linear0.8480.8190.8380.835PMI0.8550.8260.8460.842AMI0.8360.7980.8250.820Cos0.8540.8180.8410.838Poly0.8560.8210.8440.840RBF0.8530.8090.8350.8326.7 Efficiencycomputational cost DCI asymptotically bound cost projecting f termstwo domains m-dimensional space, could roughly estimatedO(f mc), c component due cost comparing two term distributionmodels, depends average prevalence c terms unlabelled corpus155fiMoreo, Esuli, & Sebastianitypically much smaller effective number unlabelled documents resultsparsity.Note probabilistic functions discussed Section 4 implementedefficiently using sparse data structures. example, calculating joint probabilityP (v, w) achieved O(c) steps intersecting two hash sets c expected elements.kernel-based DCFs discussed depend dot product Euclidean distance,also computed O(c) iterating non-zero values. Thus, DCIcomputational cost O(f mc); note fixed parameter, overall costalso considered O(f c). relevant alternatives typically involve singularvalue decomposition matrix multiplication, thus resulting O(df c) algorithms,number documents contexts collection.performed efficiency tests comparing DCI SCL Webis-CLS-10 dataset.test run combinations source target classes target languages,amounts 36 runs. dedicated computer4 run experimentsnumber 10 threads. Table 13 shows averaged time scores obtained.Table 13: Running time (in seconds) DCI two different DCFs (linear cosine)SCL.Min (s)Max (s)Average (s)Standard DeviationLinear6.40622.11911.5533.976Cosine7.50127.03217.7745.516SCL-MI449.988859.719678.83498.324SCL suffers much higher computational costs DCI. average, DCI reduced98.3% 97.4% computational cost respect SCL linear cosineDCFs, respectively. SCL required = 450 binary optimization problems translations,required performing LSA predictive parameters. DCI-based method obtained better results substantially less time. efficiency tests suggest DCI couldscale well larger datasets.6.8 Embeddingsfinal note, intuitions behind DCI strong relationships behindword embeddings, deep learning, research area gained interestlast renewed years. Neural language models trained obtain meaningful term representations seem capture interesting language regularities (Bengio, 2009). Althoughdeep learning applied cross-domain adaptation Glorot et al. (2011) (a workused baseline Section 6.5.1), cross-lingual adaptation requires additional effort.is, consistently obtain bilingual word embeddings, large unlabelled datasetsaligned corpora (Zou, Socher, Cer, & Manning, 2013) bilingual dictionaries (Mikolov, Le,& Sutskever, 2013) typically required. Assuming small set words translated (no4. Computer specifications: 64-bit Intel Core (TM) Genuine-Intel I7 12 processors 3.47GH, 24GBRAM, running Ubuntu 14.04.2 LTS.156fiDistributional Correspondence Indexing200 words experiments), method obtains term profiles performconsistently languages classification task. Table 14 illustrates semanticproperties captured term profiles; lists similar (via cosine similarity)target terms given source term.Table 14: Five similar terms three target languages (German, French,Japanese) given three terms (beautifully, classical, delightful) EnglishMusic domain.beautifullyschone ( beautiful)liebevoll ( loving)sehnsucht ( longing)ungewohnlich ( unusual)phantastisch ( fantastic)0.6350.5960.5330.5100.507classicaladagioMartenotCharles-Marievioloncelle ( cello)soliste ( soloist)0.7670.7460.7360.7270.720delightful( attractive)( portrayed)( scenes)( delicate)( taste)0.6100.5460.5450.5420.538word embeddings, even assuming external resources available, additional optimization problem, posed geometrical transformation involving scalingrotating data matrices, subsequently required order align two embeddingspaces. done Mikolov et al. (2013) forcing embedding representationswords bilingual dictionary get closer matrixtransformation. Apart additional computational cost may involve, believemethod might directly applicable scenario cross-domaincross-lingual adaptations tackled simultaneously. main reasonfinal transformation aims aligning meaning words taken bilingual dictionary domains, could play different roles across domains, i.e.,pivots. embeddings generated DCI require computationally expensivepost-processing, correspondences roles different terms domains turndirectly captured DCF scores pivots.illustrate this, used LSI plot bidimensional space important termprofiles EB GM adaptation obtained DCI cosine DCF. Figure 4shows two zoomed-in areas bilingual space. Noticeably, left-most part plotseems represent positive sentiment, right-most one seems capturenegative sentiment. relevant semantic correspondences could directly observed.Semantically related English words, expecting, expected, hoping, projectedclose together space. interestingly, related semantics seem preservedacross languages, e.g., English words boring, irritating, bored German words erschreckend(terrifying), acherlich (ridiculous), schrecklich (terribly) projected regionsspace close other. interesting cases could regarded examples crosssemantic correspondence. example classic love (book genres) English reviewsprojected close folk rock (music genres) German reviews, incidentalsemantic correspondence emerged due juxtaposition cross-domain crosslingual adaptation.157fiMoreo, Esuli, & SebastianiFigure 4: Vector profiles word embeddings obtained EB GM adaptation. Zoompositive (left) negative (right) sentiment area. plot obtainedapplying LSI terms deemed highly informative mutual information.preliminary experiments suggest DCI embeddings could potentially useful tasks natural language processing. however require dedicatedinvestigation defer future work.7. Conclusions Future Workproposed Distributional Correspondence Indexing, efficient method domainadaptation represents terms vectorial space based distributional correspondence respect small, fixed set terms. representation motivatedHarris distributional hypothesis notion pivot term Blitzer et al. (2006);method indexes documents different domains common vector space basedsemantic correspondence.Empirical evaluation two popular sentiment analysis benchmarks showsmethod outperforms several state-of-the-art approaches different domain-adaptation settings, including cross-domain cross-lingual sentiment adaptation. also proposed extended formulation domain adaptation problem, tackles crossdomain adaptation cross-language adaptation time; presentexperiments system compares favourably related approaches.point view efficiency, show method require modest computational resources,indication DCI scale well huge collections; particular,cross-lingual case required smaller amount human intervention competing approaches order create pivot set. presented high-performance DCFsparameter-free, valuable characteristic domain adaptation setting,given expected count labelled data drawn target distributionparameters could optimized.bilingual pivots created context-unaware word-translator oracle represent arguably oversimplified naive approach translation problem. Notwithstanding this,DCI seems compensate aggregative contribution partial semantics scat158fiDistributional Correspondence Indexingtered several pivots. regard, interested enhancing concept pivotscross-lingual adaptation general direction better captures context-awaremulti-word translation, attempt polylingual case. possible directionsmight include enriching term representation incorporate part-of-speech tagssyntactic information, also keeping track contexts given termappeared. Moreover, motivated empirical evidences cross-domain experiments suggested comparable performance could achieved even extremelyreduced sets pivots, investigate sophisticated pivot selection techniquesbetter characterizing concept pivot geometrical properties vector spacegenerate. also plan put test DCI domains settings, includingmulti-class multi- single-label datasets, highly imbalanced classes, transductiveproblems.Acknowledgementsgrateful Xavier Glorot sending us additional details experiments reported previous work (Glorot et al., 2011).paper extension short paper Esuli Moreo Fernandez (2015).Fabrizio Sebastiani leave Consiglio Nazionale delle Ricerche, Italy.ReferencesAndo, R. K., & Zhang, T. (2005). framework learning predictive structuresmultiple tasks unlabeled data. Journal Machine Learning Research, 6,18171853.Ben-David, S., Blitzer, J., Crammer, K., & Pereira, F. (2006). Analysis representationsdomain adaptation. Proceedings 20th Annual Conference NeuralInformation Processing Systems (NIPS 2006), pp. 137144, Vancouver, CA.Bengio, Y. (2009). Learning deep architectures AI. Foundations Trends MachineLearning, 2 (1), 1127.Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. JournalMachine Learning Research, 3, 9931022.Blitzer, J., Dredze, M., & Pereira, F. (2007). Biographies, Bollywood, boom-boxesblenders: Domain adaptation sentiment classification. Proceedings 45thAnnual Meeting Association Computational Linguistics (ACL 2007), pp.440447, Prague, CZ.Blitzer, J., McDonald, R., & Pereira, F. (2006). Domain adaptation structural correspondence learning. Proceedings 4th Conference Empirical MethodsNatural Language Processing (EMNLP 2006), pp. 120128, Sydney, AU.Bollegala, D., Weir, D., & Carroll, J. (2011). Using multiple sources construct sentimentsensitive thesaurus cross-domain sentiment classification. Proceedings49th Annual Meeting Association Computational Linguistics (ACL 2011),pp. 132141, Portland, US.159fiMoreo, Esuli, & SebastianiDai, W., Xue, G.-R., Yang, Q., & Yu, Y. (2007). Transferring nave Bayes classifiers textclassification. Proceedings 22nd AAAI Conference Artificial Intelligence(AAAI 2007), pp. 540545, Vancouver, CA.DCI-source (2015) http://hlt.isti.cnr.it/dciext/.Deerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A.(1990). Indexing latent semantic analysis. Journal American SocietyInformation Science, 41 (6), 391407.Denecke, K. (2009). SentiWordNet scores suited multi-domain sentiment classification?. Proceedings 4th International Conference Digital InformationManagement (ICDIM 2009), pp. 3338, Ann Arbor, US.Dumais, S. T., Letsche, T. A., Littman, M. L., & Landauer, T. K. (1997). Automatic crosslanguage retrieval using latent semantic indexing. Working Notes AAAISpring Symposium Cross-language Text Speech Retrieval, pp. 1824, Stanford,US.Esuli, A., & Moreo Fernandez, A. (2015). Distributional correspondence indexing crosslanguage text categorization. Proceedings 37th European ConferenceInformation Retrieval (ECIR 2015), pp. 104109, Wien, AT.Gabrilovich, E., & Markovitch, S. (2007). Computing semantic relatedness using Wikipediabased explicit semantic analysis. Proceedings 20th International Joint Conference Artifical Intelligence (IJCAI 2007), pp. 16061611, San Francisco, US.Gao, J., Fan, W., Jiang, J., & Han, J. (2008). Knowledge transfer via multiple modellocal structure mapping. Proceedings 14th ACM International ConferenceKnowledge Discovery Data Mining (KDD 2008), pp. 283291, Las Vegas, US.Gliozzo, A., & Strapparava, C. (2005). Cross-language text categorization acquiringmultilingual domain models comparable corpora. Proceedings ACLWorkshop Building Using Parallel Texts, pp. 916, Ann Arbor, US.Gliozzo, A., & Strapparava, C. (2006). Exploiting comparable corpora bilingual dictionaries cross-language text categorization. Proceedings 44th AnnualMeeting Association Computational Linguistics (ACL 2006), pp. 553560,Sydney, AU.Glorot, X., Bordes, A., & Bengio, Y. (2011). Domain adaptation large-scale sentimentclassification: deep learning approach. Proceedings 28th InternationalConference Machine Learning (ICML 2011), pp. 513520, Bellevue, US.Harris, Z. S. (1954). Distributional structure. Word, 10 (23), 146162.He, Y., Lin, C., & Alani, H. (2011). Automatically extracting polarity-bearing topicscross-domain sentiment classification. Proceedings 49th Annual MeetingAssociation Computational Linguistics (ACL 2011), pp. 123131, Portland,US.JaTeCS (2015) http://hlt.isti.cnr.it/jatecs/.160fiDistributional Correspondence IndexingJoachims, T. (1999). Transductive inference text classification using support vectormachines. Proceedings 16th International Conference Machine Learning(ICML 1999), pp. 200209, Bled, SL.Kanerva, P., Kristofersson, J., & Holst, A. (2000). Random indexing text samples latent semantic analysis. Proceedings 22nd Annual Conference CognitiveScience Society, p. 1036, Austin, US.Koehn, P., & Knight, K. (2002). Learning translation lexicon monolingual corpora.Proceedings ACL 2002 Workshop Unsupervised Lexical Acquisition, pp.916, Philadelphia, US.Landauer, T. K., & Dumais, S. T. (1997). solution Platos problem: latentsemantic analysis theory acquisition, induction, representation knowledge.Psychological Review, 104 (2), 211240.Li, F., Pan, S. J., Jin, O., Yang, Q., & Zhu, X. (2012a). Cross-domain co-extraction sentiment topic lexicons. Proceedings 50th Annual Meeting AssociationComputational Linguistics (ACL 2012), pp. 410419, Jeju Island, KR.Li, L., Jin, X., & Long, M. (2012b). Topic correlation analysis cross-domain text classification. Proceedings 26th AAAI Conference Artificial Intelligence (AAAI2012), pp. 9981004, Toronto, CA.Ling, X., Dai, W., Xue, G.-R., Yang, Q., & Yu, Y. (2008). Spectral-domain transfer learning.Proceedings 14th ACM International Conference Knowledge DiscoveryData Mining (KDD 2008), pp. 488496, Las Vegas, US.Liu, B. (2012). Sentiment Analysis Opinion Mining. Morgan Claypool Publishers,San Rafael, US.Mikolov, T., Le, Q. V., & Sutskever, I. (2013). Exploiting Similarities among LanguagesMachine Translation. ArXiv e-prints, arXiv:1309.4168 [cs.CL].Moen, H., & Marsi, E. (2013). Cross-lingual random indexing information retrieval.Proceedings 1st International Conference Statistical Language SpeechProcessing (SLSP 2013), pp. 164175, Tarragona, ES.MSD dataset (2007) http://www.cs.jhu.edu/~mdredze/datasets/sentiment/.Natural Language Understanding Toolkit (2011) https://github.com/pprett/nut.Pan, S. J., Ni, X., Sun, J.-T., Yang, Q., & Chen, Z. (2010). Cross-domain sentiment classification via spectral feature alignment. Proceedings 19th InternationalConference World Wide Web (WWW 2010), pp. 751760, Raleigh, US.Pan, S. J., & Yang, Q. (2010). survey transfer learning. IEEE TransactionsKnowledge Data Engineering, 22 (10), 13451359.Pan, W., Zhong, E., & Yang, Q. (2012). Transfer learning text mining. Aggarwal,C. C., & Zhai, C. (Eds.), Mining Text Data, pp. 223258. Springer, Heidelberg, DE.Pang, B., & Lee, L. (2008). Opinion mining sentiment analysis. Foundations TrendsInformation Retrieval, 2 (1/2), 1135.161fiMoreo, Esuli, & SebastianiPeirsman, Y., & Pado, S. (2010). Cross-lingual induction selectional preferencesbilingual vector spaces. Proceedings 8th Annual Conference NorthAmerican Chapter Association Computational Linguistics (NAACL 2010),pp. 921929, Los Angeles, US.Platt, J. C., Toutanova, K., & Yih, W.-t. (2010). Translingual document representationsdiscriminative projections. Proceedings 8th Conference EmpiricalMethods Natural Language Processing (EMNLP 2010), pp. 251261, Cambridge,US.Ponomareva, N., & Thelwall, M. (2012). neighbours help? exploration graph-basedalgorithms cross-domain sentiment classification. Proceedings 2012 JointConference Empirical Methods Natural Language Processing ComputationalNatural Language Learning (EMNLP/CoNLL 2012), pp. 655665, Jeju Island, KR.Prettenhofer, P., & Stein, B. (2010). Cross-language text classification using structuralcorrespondence learning. Proceedings 48th Annual Meeting AssociationComputational Linguistics (ACL 2010), pp. 11181127, Uppsala, SE.Prettenhofer, P., & Stein, B. (2011). Cross-lingual adaptation using structural correspondence learning. ACM Transactions Intelligent Systems Technology, 3 (1), Article 13.Rapp, R. (1995). Identifying word translations non-parallel texts. Proceedings33rd Annual Meeting Association Computational Linguistics (ACL 1995), pp.320322, Cambridge, US.Rapp, R. (1999). Automatic identification word translations unrelated EnglishGerman corpora. Proceedings 37th Annual Meeting AssociationComputational Linguistics (ACL 1999), pp. 519526, College Park, US.Rigutini, L., Maggini, M., & Liu, B. (2005). EM-based training algorithm crosslanguage text categorization. Proceedings 3rd IEEE/WIC/ACM International Conference Web Intelligence (WI 2005), pp. 529535, Compiegne, FR.Sahlgren, M. (2005). introduction random indexing. Proceedings WorkshopMethods Applications Semantic Indexing, Copenhagen, DK.Sorg, P., & Cimiano, P. (2008). Cross-language information retrieval explicit semanticanalysis. Working Notes 2008 Cross-Language Evaluation Forum (CLEF2008), Aarhus, DE.Sorg, P., & Cimiano, P. (2012). Exploiting Wikipedia cross-lingual multilingualinformation retrieval. Data Knowledge Engineering, 74, 2645.SVMlight (2008) http://svmlight.joachims.org/.Vinokourov, A., Shawe-Taylor, J., & Cristianini, N. (2002). Inferring semantic representation text via cross-language correlation analysis. Proceedings 16th AnnualConference Neural Information Processing Systems (NIPS 2002), pp. 14731480,Vancouver, CA.Wan, C., Pan, R., & Li, J. (2011). Bi-weighting domain adaptation cross-language textclassification. Proceedings 22nd International Joint Conference ArtificialIntelligence (IJCAI 2011), pp. 15351540, Barcelona, ES.162fiDistributional Correspondence IndexingWan, X. (2009). Co-training cross-lingual sentiment classification. Proceedings47th Annual Meeting Association Computational Linguistics4th International Joint Conference Natural Language Processing (ACL/IJCNLP2009), pp. 235243, Singapore, SN.Wang, P., Domeniconi, C., & Hu, J. (2008). Using Wikipedia co-clustering-based crossdomain text classification. Proceedings 8th IEEE International ConferenceData Mining (ICDM 2008), pp. 10851090, Pisa, IT.Webis-CLS dataset (2010)http://www.uni-weimar.de/en/media/chairs/webis/research/corpora/corpus-webis-cls-10/.Xia, R., & Zong, C. (2011). POS-based ensemble model cross-domain sentimentclassification. Proceedings 5th International Joint Conference NaturalLanguage Processing (IJCNLP 2011), pp. 614622, Chiang Mai, TH.Xiang, E. W., Cao, B., Hu, D. H., & Yang, Q. (2010). Bridging domains using worldwide knowledge transfer learning. IEEE Transactions Knowledge DataEngineering, 22 (6), 770783.Xiao, M., & Guo, Y. (2013). novel two-step method cross-language representationlearning. Proceedings 27th Annual Conference Neural Information Processing Systems (NIPS 2013), pp. 12591267, Lake Tahoe, US.Xiao, M., & Guo, Y. (2014). Semi-supervised matrix completion cross-lingual textclassification. Proceedings 28th AAAI Conference Artificial Intelligence(AAAI 2014), pp. 16071614, Quebec City, CA.Xue, G.-R., Dai, W., Yang, Q., & Yu, Y. (2008). Topic-bridged PLSA cross-domain textclassification. Proceedings 31st ACM International Conference ResearchDevelopment Information Retrieval (SIGIR 2008), pp. 627634, Singapore,SN.Zhuang, F., Luo, P., Xiong, H., He, Q., Xiong, Y., & Shi, Z. (2011). Exploiting associationsword clusters document classes cross-domain text categorization.Statistical Analysis Data Mining, 4 (1), 100114.Zou, W. Y., Socher, R., Cer, D. M., & Manning, C. D. (2013). Bilingual word embeddingsphrase-based machine translation. Proceedings 11th Conference EmpiricalMethods Natural Language Processing (EMNLP 2013), pp. 13931398, Seattle, US.163fiJournal Artificial Intelligence Research 55 (2016) 953-994Submitted 09/15; published 04/16Bilingual Distributed Word RepresentationsDocument-Aligned Comparable DataIvan Vuliciv250@cam.ac.ukUniversity CambridgeDepartment Theoretical Applied Linguistics9 West Road, CB3 9DP, Cambridge, UKMarie-Francine Moensmarie-francine.moens@cs.kuleuven.beKU LeuvenDepartment Computer ScienceCelestijnenlaan 200A, 3001 Heverlee, BelgiumAbstractpropose new model learning bilingual word representations non-paralleldocument-aligned data. Following recent advances word representation learning,model learns dense real-valued word vectors, is, bilingual word embeddings (BWEs).Unlike prior work inducing BWEs heavily relied parallel sentence-aligned corpora and/or readily available translation resources dictionaries, article revealsBWEs may learned solely basis document-aligned comparable data without additional lexical resources syntactic information. present comparisonapproach previous state-of-the-art models learning bilingual word representations comparable data rely framework multilingual probabilistictopic modeling (MuPTM), well distributional local context-counting models.demonstrate utility induced BWEs two semantic tasks: (1) bilingual lexiconextraction, (2) suggesting word translations context polysemous words. simpleyet effective BWE-based models significantly outperform MuPTM-based contextcounting representation models comparable data well prior BWE-based models,acquire best reported results tasks three tested language pairs.1. Introductionhuge body work distributional semantics word representation learning almostexclusively revolves around distributional hypothesis (Harris, 1954) - ideastates similar words occur similar contexts. current corpus-based approachessemantics rely contextual evidence one way another. Roughly speaking, wordrepresentations typically learned using two families distributional context-basedmodels: (1) global matrix factorization models latent semantic analysis (LSA)(Landauer & Dumais, 1997) generative probabilistic models latent Dirichletallocation (LDA) (Blei, Ng, & Jordan, 2003), model word co-occurrencedocument paragraph level; (2) local context window models represent wordssparse high-dimensional context vectors, model word co-occurrence levelselected neighboring words (Turney & Pantel, 2010), generative probabilistic modelslearn probability distribution vocabulary word context window latentvariable (Deschacht & Moens, 2009; Deschacht, De Belder, & Moens, 2012).c2016AI Access Foundation. rights reserved.fiVulic & Moenshand, dense real-valued vectors known distributed representationswords word embeddings (WEs) (e.g., Bengio, Ducharme, Vincent, & Janvin, 2003; Collobert & Weston, 2008; Mikolov, Chen, Corrado, & Dean, 2013a; Pennington, Socher, &Manning, 2014) introduced recently, first part neural network based architectures statistical language modeling. WEs serve richer coherent wordrepresentations ones obtained aforementioned traditional distributionalsemantic models, illustrative comparative studies available recently publishedrelevant work (e.g., Mikolov, Yih, & Zweig, 2013d; Baroni, Dinu, & Kruszewski, 2014; Levy,Goldberg, & Dagan, 2015).natural extension interest monolingual multilingual word embeddingsoccurred recently (e.g., Klementiev, Titov, & Bhattarai, 2012; Hermann & Blunsom, 2014b).operating multilingual settings, highly desirable learn embeddings wordsdenoting similar concepts close shared bilingual embedding space (e.g.,representations English word school Spanish word escuelasimilar). BWEs may used myriad multilingual natural languageprocessing tasks beyond, fundamental tasks leaning bilingual meaningrepresentations, e.g., computing cross-lingual multilingual semantic word similarityextracting bilingual word lexicons using induced bilingual embedding space (see Figure 1). However, models critically require (at least) sentence-aligned parallel datareadily-available translation dictionaries induce bilingual word embeddings (BWEs)consistent closely aligned different languages.1.1 Contributionsbest knowledge, article presents first work showcase bilingual word embeddings may induced directly basis comparable data withoutadditional bilingual resources sentence-aligned parallel data translation dictionaries. focus document-aligned comparable corpora (e.g., Wikipedia articlesaligned inter-wiki links, news texts discussing theme).new bilingual embedding learning model makes use pseudo-bilingual documentsconstructed merging content two coupled documents document pair,propose evaluate two different strategies construct pseudo-bilingualdocuments: (1) merge randomly shuffle strategy randomly permutes wordslanguages pseudo-bilingual document, (2) length-ratio shuffle strategy,deterministic method retains monolingual word order intermingling wordscross-lingually. additional pre-training shuffling strategies ensure sourcelanguage words target language words occur contexts source targetlanguage word. monolingual model skip-gram negative sampling (SGNS)word2vec package (Mikolov, Sutskever, Chen, Corrado, & Dean, 2013c)trained shuffled pseudo-bilingual documents. procedure, steer semantically similar words different languages towards similar representationsshared bilingual embedding space, effectively use available bilingual contexts insteadmonolingual ones. model treats documents bags-of-words (i.e., includesyntactic information) even rely sentence boundary information.954fiBilingual Distributed Word Representations Document-Aligned Datasummary, main contributions article are:(1) present BWE Skip-Gram (BWESG), first model induces bilingual wordembeddings directly document-aligned non-parallel data. test evaluate twomain variants model based pre-training shuffling step. main strengthpresented model lies favourable trade-off simplicity effectiveness.(2) provide qualitative quantitative analysis model. draw analogiescomparisons prior work inducing word representations data type:document-aligned comparable corpora (e.g., models relying multilingual probabilistictopic modeling framework (MuPTM)).(3) demonstrate utility induced BWEs word type level task bilingual lexicon extraction (BLE) Wikipedia data three language pairs. BLE modelbased BWEs significantly outperforms MuPTM-based context-counting BLEmodels, acquires best reported scores benchmarking BLE datasets.(4) demonstrate utility induced BWEs word token level tasksuggesting word translations context (SWTC) (Vulic & Moens, 2014) threelanguage pairs. SWTC model based BWEs significantly outscores bestscoring MuPTM-based SWTC models setting without use parallel datatranslation dictionaries, acquires best reported results benchmarking SWTC datasets.(5) also present comparison state-of-the-art BWE induction models (Mikolov,Le, & Sutskever, 2013b; Hermann & Blunsom, 2014b; Gouws, Bengio, & Corrado, 2015)BLE SWTC. Results reveal simple yet effective approach on-paroutperforms BWE induction models rely parallel data readily availabledictionaries learn shared bilingual embedding spaces. addition, preliminary experiments BWESG parallel Europarl data demonstrate model also usefultrained sentence-aligned data, reaching performance benchmarking BWEinduction models parallel data (e.g., Hermann & Blunsom, 2014b).2. Related Worksection motivate opt building model inducing bilingualword embeddings comparable document-aligned data. clearer overview,split related work three broad clusters: (1) monolingual word embeddings, (2) bilingualword embeddings, (3) bilingual word representations document-aligned data.2.1 Monolingual Word Embeddingsidea representing words continuous real-valued vectors dates way back mid80s (Rumelhart, Hinton, & Williams, 1986; Elman, 1990). idea met resurgencedecade ago (Bengio et al., 2003), neural language model learns word embeddingspart neural network architecture statistical language modeling. work inspiredapproaches learn word embeddings within neural-network language modelingframework (Collobert & Weston, 2008; Collobert, Weston, Bottou, Karlen, Kavukcuoglu, &Kuksa, 2011). Word embeddings tailored capture semantics encode continuous955fiVulic & MoensMonolingualvsBilingualFigure 1: toy 3D shared bilingual embedding space Gouws et al. (2015):monolingual spaces words similar meanings similar representations, bilingual spaces words two different languages similar meaningssimilar representations (both mono- cross-lingually).notion semantic similarity (as opposed semantically poorer discrete representations),necessary share information words text units.Recently, skip-gram continuous bag-of-words (CBOW) model Mikolov etal. (2013a, 2013c) revealed full neural-network structure neededlearn high-quality word embeddings (with extremely decreased training times comparedfull-fledged neural network models, see Mikolov et al., 2013a full analysiscomplexity models). models fact simple single-layered architectures,objective predict words context given word (skip-gram)predict word given context (CBOW). Similar models called vector log-bilinear modelsrecently proposed (Mnih & Kavukcuoglu, 2013). models inspired skip-gramCBOW GloVe (Global Vectors Word Representation) (Pennington et al., 2014),combines local global contexts word unified model, modelrelies dependency-based contexts instead simpler word-based contexts (Levy &Goldberg, 2014a), new models steadily emerging (e.g., Lebret & Collobert, 2014;Lu, Wang, Bansal, Gimpel, & Livescu, 2015; Stratos, Collins, & Hsu, 2015; Trask, Gilmore,& Russell, 2015; Liu, Jiang, Wei, Ling, & Hu, 2015).interesting finding discussed recently (Levy & Goldberg, 2014b): popular skip-gram model negative sampling (SGNS) (Goldberg & Levy, 2014) simplymodel implicitly factorizes word-context matrix, cells containing pointwisemutual information (PMI) scores respective word context pairs, shiftedglobal constant. words, SGNS performs exactly thing traditionaldistributional models (i.e., context counting plus context weighting and/or dimensionalityreduction), slight improvement performance SGNS (Baroni et al., 2014; Levyet al., 2015).low-dimensional vectors, besides improving computational efficiency, leadbetter generalizations, even allowing generalize vocabularies observed labelleddata, hence partially alleviating ubiquitous problem data sparsity. utilityvalidated proven various semantic tasks semantic word similarity,synonymy detection word analogy solving (Mikolov et al., 2013d; Baroni et al., 2014;Pennington et al., 2014). Moreover, word embeddings proven serve useful956fiBilingual Distributed Word Representations Document-Aligned Dataunsupervised features plenty downstream NLP tasks named entity recognition,chunking, semantic role labeling, part-of-speech tagging, parsing, selectional preferences(Turian, Ratinov, & Bengio, 2010; Collobert et al., 2011; Chen & Manning, 2014).Due simplicity, well efficacy consequent popularity various tasks(Mikolov et al., 2013c; Levy & Goldberg, 2014b), clear advantage similarity taskscompared traditional models distributional semantics (Levy et al., 2015)article focus adaptation SGNS (Mikolov et al., 2013c). Section 3,provide brief overview model, follow new bilingualmodel based SGNS.2.2 Bilingual Word EmbeddingsBilingual word representations could serve useful source knowledge problemscross-lingual information retrieval (Levow, Oard, & Resnik, 2005; Vulic, De Smet, & Moens,2013), statistical machine translation (Wu, Wang, & Zong, 2008), document classification(Ni, Sun, Hu, & Chen, 2011; Klementiev et al., 2012; Hermann & Blunsom, 2014b; Chandar, Lauly, Larochelle, Khapra, Ravindran, Raykar, & Saha, 2014; Vulic, De Smet, Tang,& Moens, 2015), bilingual lexicon extraction (Tamura, Watanabe, & Sumita, 2012; Vulic& Moens, 2013a), knowledge transfer annotation projection resource-richresource-poor languages myriad NLP tasks dependency parsing, POS tagging, semantic role labeling selectional preferences (Yarowsky & Ngai, 2001; Pado &Lapata, 2009; Peirsman & Pado, 2010; Das & Petrov, 2011; Tackstrom, Das, Petrov, McDonald, & Nivre, 2013; Ganchev & Das, 2013; Tiedemann, Agic, & Nivre, 2014; Xiao& Guo, 2014). interesting application domains machine translation (e.g., Zou,Socher, Cer, & Manning, 2013; Wu, Dong, Hu, Yu, He, Wu, Wang, & Liu, 2014; Zhang, Liu,Li, Zhou, & Zong, 2014) cross-lingual information retrieval (e.g., Vulic & Moens, 2015).Moreover, making transition monolingual bilingual settings buildingshared bilingual embedding space (see Figure 1 illustrative example), one ableextend rather generalize semantic tasks semantic similarity computation, synonymy detection word analogy computation across languages. Following successmonolingual settings, body recent work word representation learning thereforefocused learning bilingual word embeddings (BWEs).current research inducing BWEs critically relies sentence-aligned paralleldata readily available bilingual lexicons achieve coherence representations acrosslanguages (e.g., build similar representations similar concepts different languagesJanuary-januari, dog-hund sky-hemel). may cluster current work threedifferent groups: (1) models rely hard word alignments obtained paralleldata constrain learning BWEs (Klementiev et al., 2012; Zou et al., 2013; Wu et al.,2014); (2) models use alignment parallel data sentence level (Kocisky,Hermann, & Blunsom, 2014; Hermann & Blunsom, 2014a, 2014b; Chandar et al., 2014; Shi,Liu, Liu, & Sun, 2015; Gouws et al., 2015); (3) models critically require readilyavailable bilingual lexicons (Mikolov et al., 2013b; Faruqui & Dyer, 2014; Xiao & Guo,2014). main disadvantage models limited availability parallel databilingual lexicons, resources scarce and/or domain-restricted plentylanguage pairs. work, significantly alleviate requirements: unlike prior work,957fiVulic & Moensshow BWEs may induced solely basis document-aligned comparabledata without additional need parallel data bilingual lexicons. Note (intheory) work Hermann Blunsom (2014b), Chandar et al. (2014) may alsoextended setting document-aligned data, two models originallyrely sentence embeddings computed aggregations single word embeddingsplus sentence alignments. work, testing comparing BiCVM modelHermann Blunsom, show models work well practicereplacing strong bilingual signal coded parallel sentences noisy bilingualsignal given document alignments non-parallel data.2.3 Bilingual Word Representations Document-Aligned DataPrior work inducing bilingual word representations early days followed tradition window-based context-counting distributional models (Rapp, 1999; Gaussier, Renders, Matveeva, Goutte, & Dejean, 2004; Laroche & Langlais, 2010) requiredbilingual lexicon critical resource. order tackle issue, recent work reliessupervision-lighter framework multilingual probabilistic topic modeling (MuPTM)(Mimno, Wallach, Naradowsky, Smith, & McCallum, 2009; Boyd-Graber & Blei, 2009;De Smet & Moens, 2009; Ni, Sun, Hu, & Chen, 2009; Zhang, Mei, & Zhai, 2010; Fukumasu,Eguchi, & Xing, 2012) similar models latent structure induction (Haghighi,Liang, Berg-Kirkpatrick, & Klein, 2008; Daume III & Jagarlamudi, 2011).Words setting represented real-valued vectors conditional topic probability scores P (zk |wi ), regardless actual language. Topics zk fact latentinter-lingual concepts discovered directly multilingual comparable data using multilingual topic model bilingual LDA. discuss MuPTM-based representationsdetail Section 4.1.MuPTM-based bilingual word representations induced comparable data demonstrated utility tasks cross-lingual semantic similarity computation bilinguallexicon extraction (Vulic, De Smet, & Moens, 2011; Liu, Duh, & Matsumoto, 2013)suggesting word translations context (Vulic & Moens, 2014). work, comparestate-of-the-art MuPTM-based word representations induced typecomparable corpora BWEs learned new model two semantic tasks.Another recent model (Sgaard, Agic, Martnez Alonso, Plank, Bohnet, & Johannsen,2015) also able learn document-aligned data. count-based modelbuilds binary word vectors denoting occurrence word document pair.Dimensionality reduction applied post-hoc induced sparse vectors. Sincelinks documents known, model able learn cross-lingual correspondenceswords and, consequently, bilingual word representations. Exactly ideaalready introduced baseline model Vulic et al. (2011), TF-IDF weightsused instead binary indices, dimensionality reduction applied post-hoc.model Vulic et al. surpassed baseline models document-aligned databriefly discussed Section 4.1, model Sgaard et al. obtains resultssimilar BWE baselines compared work (described Section 4.2).958fiBilingual Distributed Word Representations Document-Aligned Data3. BWESG: Model Architecturenew bilingual model extension SGNS bilingual settings documentaligned comparable training data. section describes underlying SGNS twovariants SGNS-based BWE induction model.3.1 Skip-Gram Negative Sampling (SGNS)departure point log-linear SGNS Mikolov et al. (2013c) implementedword2vec package.1 SGNS model learns word embeddings (WEs) similar wayneural language models (Bengio et al., 2003; Collobert & Weston, 2008), withoutnon-linear hidden layer.monolingual setting, assume one language L vocabulary V , corpuswords w V , along contexts c V c , V c context vocabulary.Contexts word wn typically neighboring words context window size cs(i.e., wncs , . . . , wn1 , wn+1 , . . . , wn+cs ), effectively holds V c V .2word type w V associated vector w~ Rd (its pivot word representationpivot word embedding, see Figure 2), vector w~c Rd (its context embedding).dimensionality vectors, which, model input parameter,set advance training procedure commences. entries vectorslatent, treated parameters learned model. short, ideaskip-gram model scan corpus (which typically unannotated, Mikolovet al., 2013a) word word turn (i.e., pivot words), learn pairs(word, context word). learning goal maximize ability predicting contextwords pivot word corpus. Let ob = 1 denote pair words (w, v)observed corpus thus belongs training set D. probability (w, v)defined softmax function:P (ob = 1|w, v, ) =11 + exp(w~ v~c )(1)word token w corpus treated turn pivot pairs word tokens(w, w 1),...,(w, w t(cs)) appended D, t(cs) integer sampleduniform distribution {1, . . . , cs}.3 global training objective J maximizeprobabilities pairs indeed observed corpus:J = arg maxXlog(w,v)D11 + exp(w~ v~c )(2)parameters model, is, pivot context word embeddingslearned. One may see objective function trivial solution setting1. https://code.google.com/p/word2vec/2. Testing options context selection dependency-based contexts (Levy & Goldberg, 2014a)beyond scope work, shown contexts may lead gainsfinal WEs (Kiela & Bottou, 2014).3. original skip-gram model utilizes dynamic window sizes, cs denotes maximum windowsize. Moreover, model takes account sentence boundaries context selection, is, selectscontext words words occurring sentence pivot word.959fiVulic & Moensw~ = v~c , w~ v~c = V al, V al large enough number (Goldberg & Levy, 2014).order prevent trivial training scenario, negative sampling procedure comespicture (Collobert & Weston, 2008; Mikolov et al., 2013c).short, idea behind negative sampling present model set D0artificially created sampled negative pivot-context word pairs (w, v 0 ),assumption serve negative examples, is, occur observed/positive(word, context) pairs training corpus. model adjust parametersway also maximize probability negative pairs occurcorpus. interested reader may find details negative samplingprocedure, new exact objective function along derivation elsewhere (Levy &Goldberg, 2014b), illustrative purposes simplicity, present approximativeobjective function negative sampling Goldberg Levy:XX11J = arg maxlog(3)+log1 + exp(w~ v~c )1 + exp(w~ v~c0 )(w,v)D(w,v 0 )D0free parameters updated using stochastic gradient descent backpropagation,learning rate typically controlled Adagrad (Duchi, Hazan, & Singer, 2011)global linearly decreasing learning rate. optimizing objective eq. (3),model incrementally pushes observed pivot WEs towards context WEs collocatescorpus. words distributional hypothesis - training, words occursimilar contexts end similar word embeddings. words, linkterminology distributional hypothesis modeling assumptions SGNS - wordspredict similar contexts end similar word embeddings.3.2 Final Model - BWESG: BWE Skip-Gramnext step, propose novel method extends SGNS work bilingualdocument-aligned comparable data. Let us assume possess document-alignedcomparable corpus, defined C = {d1 , d2 , . . . , dN } = {(dS1 , dT1 ), (dS2 , dT2 ), . . . , (dSN , dTN )}.dj = (dSj , dTj ) denotes pair aligned documents source language LS targetlanguage LT respectively, N number pairs corpus. V Vvocabularies associated languages LS LT . goal learn shared bilingualembedding space given data (Figure 1) document alignments bilingualsignal training. present two strategies that, coupled SGNS, leadshared bilingual spaces. overview architecture learning BWEs documentaligned comparable data two strategies given Figures 2(a) 2(b).3.2.1 Merge Shufflefirst step, merge two documents dSj dTj aligned document pair djsingle pseudo-bilingual document d0j . Following that, randomly shufflenewly constructed pseudo-bilingual document. shuffle (random) permutationword tokens given two different languages forming pseudo-bilingual document.pre-training shuffling step (see Figure 2(a)) assures word w, regardlessactual language, obtains word collocates vocabularies. idea obtainingbilingual contexts pivot word pseudo-bilingual document steer960fiBilingual Distributed Word Representations Document-Aligned Data(a) Merge Shuffle(b) Length-Ratio ShuffleFigure 2: architecture BWE Skip-Gram (BWESG) model learning bilingualword embeddings document-aligned comparable data two different pretraining strategies: (1) non-deterministic merge shuffle, (2) deterministiclength-ratio shuffle. Source language words documents drawn grayboxes, target language words documents drawn blue boxes.right side figures (separated vertical dashed lines) illustratespseudo-bilingual document constructed pair two aligned documents.final model towards constructing shared bilingual space. Since model dependsalignment document level, order ensure bilingual contexts insteadmonolingual contexts, intuitive assume larger window sizes lead betterbilingual embeddings. test hypothesis effect window size Section 7.3.another interpretation, since model relies (pseudo-bilingual) document levelco-occurrence, window size parameter controls amount random datadropout, is, number positive document-level training examples. localityfeature SGNS preserved due shuffling procedure.961fiVulic & Moens3.2.2 Length-Ratio Shufflenon-deterministic uncontrollable nature merge shuffle procedure openspossibility accidentally obtaining bad shuffles result sub-optimal wordrepresentations. Therefore, also propose deterministic strategy building pseudobilingual documents suitable bilingual training. Source target language wordsinserted (initially empty) pseudo-bilingual document turn based ratiodocument lengths, word order preserved. Document lengths measured termsword tokens, let us denote mS mT aligned document pair (dSj , dTj ).Let us assume, without loss generality, mS mT . procedure proceedsfollows (if mT > mS procedure proceeds analogous manner roles dSjdTj reversed):1. Pseudo-bilingual document d0j empty: d0j = {}.mSc.2. Compute ratio: R = b3. Scan aligned documents dS dT simultaneously (3.1) append R wordtokens dSj d0j ; (3.2) append 1 word token dTj . Repeat steps 3.13.2 word tokens dTj inserted d0j .4. Insert remaining mS mod mT word tokens dSj d0j .Using simple example, assume English (EN) document {F rodo, Sam, orcs,goblins, ordor, ring} Spanish (ES) document {anillo, orcos, mago}: pseudobilingual document would formed inserting 1 Spanish word 2 English words (aslength ratio 6:3 = 2:1). final pseudo-bilingual document is:{F rodoEN , SamEN , anilloES , orcsEN , goblinsEN , orcosES , ordorEN , ringEN , magoES }.another interpretation, length-ratio shuffle strategy constructs single permutation/shuffle pseudo-bilingual document controlled word order two aligneddocuments well length ratio. before, model relies pseudo-bilingualdocument level co-occurrence, window size parameter controls amount (nownon-random) data dropout. difference lies fact procedure keeps wordorder intact monolingually constructing pseudo-bilingual document.final BWE Skip-gram (BWESG) model relies monolingual variantSGNS (or monolingual induction model) trained shuffled/permutedpseudo-bilingual documents (using proposed strategies).4 model learnsword embeddings source target language words aligned shared embeddingdimensions. BWESG-based representation word w, regardless actual language,d-dimensional vector: w~ = [f1 , . . . , fk , . . . , fd ]. fk R denotes value kth shared inter-lingual feature within d-dimensional shared bilingual embedding space.Since words share embedding space, semantic similarity words maycomputed monolingually across languages. extensively use propertyevaluation tasks.4. also experimenting GloVe CBOW, falling short SGNS average.962fiBilingual Distributed Word Representations Document-Aligned Data4. Baseline Representation Modelsquickly navigate approaches bilingual word representation learningdocument-aligned comparable data. set models comparison may roughlyclustered two main groups: (Group I) pre-BWE baseline representation modelsdocument-aligned data, (Group II) benchmarking BWE induction modelsoriginally developed learning document-aligned comparable data. essential compare BWESG model frameworks learning representationsdocument-aligned data (Group I), also crucial detect main strengthsBWESG model compared approaches BWE learning frameworkalso adjusted learn document-aligned data (Group II).4.1 Group I: Baseline Representation Models Document-Aligned Databriefly describe three benchmarking Group models.4.1.1 Basic-MuPTMearly approaches (e.g., Dumais, Landauer, & Littman, 1996; Carbonell, Yang, Frederking, Brown, Geng, Lee, Frederking, E, Geng, & Yang, 1997) tried mine topical structuredocument-aligned comparable texts using monolingual topic model (e.g., LSALDA) trained pseudo-bilingual documents target document simply appendedsource language counterpart, used discovered latent topical structureshared semantic space words documents two languages mayrepresented uniform way.recent work multilingual probabilistic topic modeling (MuPTM) (Mimno et al.,2009; De Smet & Moens, 2009; Vulic et al., 2011) showed word representations higherquality may built multilingual topic model bilingual LDA (BiLDA) trainedjointly document-aligned comparable corpora retaining structure corpusintact (i.e., need construct pseudo-bilingual documents).MuPTM discovers latent structure observed data form K latentcross-lingual topics z1 , . . . , zK optimally describe generation observed data.Extracting latent cross-lingual topics actually implies learning per-document topic distributions document corpus (probability scores P (zk |dj )), discoveringlanguage-specific representations topics given per-topic word distributionslanguage (probability scores P (wiS |zk ) P (wiT |zk )). Latent cross-lingual topicsfact distributions vocabulary words, language-specific representationlanguage. Per-document topic distributions per-topic word distributionsobtained training topic model multilingual data. representationword w V (or analogous manner w V ) K-dimensional vector:w~ = [P (z1 |w), . . . , P (zk |w), . . . , P (zK |w)].call representation model (RM) Basic-MuPTM (BMu). Since numbertopics, is, number vector dimensions K typically high (Dinu & Lapata, 2010;Vulic et al., 2011), additional feature pruning (Reisinger & Mooney, 2010) may employedorder retain descriptive dimensions MuPTM-based representation,963fiVulic & Moensshown improve performance several semantic tasks (e.g., BLESWTC) (Vulic & Moens, 2013a; Vulic et al., 2015).multilingual topic model typically trained Gibbs sampling (Geman & Geman,1984; Steyvers & Griffiths, 2007; Vulic et al., 2015). Similar SGNS/BWESG trainingprocedure, Gibbs sampling MuPTM/BiLDA also scans training corpus wordword, cyclically updates topic assignments word token. However, unlikeBWESG uses subset document-level training examples, Gibbs samplingMuPTM uses words source language document well wordscoupled target language document influence topic assignment pivot word.BWESG design relying data dropout leads decreased training times computationcosts obtain final representations compared Basic-MuPTM.4.1.2 Association-MuPTMAnother representation also based MuPTM framework: contains associationscores P (wa |w) w, wa V V (Vulic & Moens, 2013a) dimensionsrealPP(wvalued word vectors. association scores computed P (wa |w) = K|zk )k=1P (zk |w) (Griffiths, Steyvers, & Tenenbaum, 2007), word vector (|V | + |V |)S |w), P (w |w), . . . , P (wdimensional vector: w~ = [P (w1S |w), . . . , P (w|V|w)].S|1|V |Basic-MuPTM, original word representation may also pruned post-hoc. callrepresentation model Association-MuPTM (AMu). Since approach reliesMuPTM training plus additional |V | |V | computations estimate association scores,cost obtaining Association-MuPTM representations even higher BasicMuPTM, leads robust word representations BLE task (Vulic & Moens,2013a). Basic-MuPTM Association-MuPTM produce high-dimensionalreal-valued vectors plenty near-zero dimensions (the number dimensions typically measured thousands) pruned afterwards pruning parameter often set ad-hoc, BWESG produces lower-dimensional dense real-valued vectors,additional post-hoc feature pruning required BWESG.4.1.3 Traditional-PPMItraditional approach building bilingual word representations (cross-lingual) distributional semantics compute weighted co-occurrence scores (e.g., using PMI, TF-IDF)pivot words context words window predefined size, plus external bilingual lexicon align context words/dimensions across languages (Gaussier et al.,2004; Laroche & Langlais, 2010). weighting function (WeF), standard choicedistributional semantics yields optimal near-optimal results group semantictasks (Bullinaria & Levy, 2007), smoothed positive pointwise mutual informationstatistic (Pantel & Lin, 2002; Turney & Pantel, 2010). Furthermore, order induce context words without need readily available lexicon, employ bootstrappingprocedure Peirsman Pado (2011), Vulic Moens (2013b). representationmodel called Traditional-PPMI (TPPMI). word representation R-dimensionalvector: w~ = [sc1 (w, c1 ), . . . , sck (w, ck ), . . . , scK (w, cK )]. dimensions vector spaceK one-to-one word translation pairs ck = (cSk , cTk ), sck (w, ck ) weighted co964fiBilingual Distributed Word Representations Document-Aligned Dataoccurrence score pivot word w k-th context feature, one computesco-occurrence score using cSk w V , cTk w V .Vector dimensions ck = (cSk , cTk ) Traditional-PPMI representation similarmodels WeFs typically frequent reliable translation pairscorpus. opposed BWESG, obtained word vectors high-dimensional(typically thousands dimensions) sparse real-valued vectors. addition, traditionalPPMI purely local distributional model deriving distributional context knowledgenarrow context windows (typically 3-10 surrounding words, e.g., Laroche & Langlais, 2010).bootstrapping approach (Vulic & Moens, 2013b) use induce TraditionalPPMI representation starts automatically learned seed lexicon one-to-one translation pairs obtained using model (e.g., Basic-MuPTM Association-MuPTM),gradually detects new dimensions shared bilingual semantic space. referinterested reader relevant literature (Vulic & Moens, 2013b) details.4.2 Group II: BWE Induction Models Adjusted Document-Aligned Dataprovide quick overview three representative benchmarking BWE modelslearn different types bilingual monolingual data.4.2.1 BiCVMHermann Blunsom (2014b) introduced model called BiCVM (Bilingual CompositionalVector Model) learns bilingual word embeddings sentence-aligned parallel corpusC = {s1 , s2 , . . . , sN } = {(sS1 , sT1 ), (sS2 , sT2 ), . . . , (sSN , sTN )}.5 sj = (sSj , sTj ) denotes pairaligned sentences. model assumes aligned sentences meaning,implies sentence representations similar. Assume two functions fg map sentences given source language respectively semanticrepresentations Rd , representation dimensionality. energymodel given two sentences (sSj , sTj ) C defined as: E(sSj , sTj ) = ||f (sSj ) g(sTj )||.goal minimize E semantically equivalent sentences (i.e., aligned sentences)corpus. order prevent model degenerating, use noise-contrastivelarge-margin update ensures representations non-aligned sentences observecertain margin other. every pair parallel sentences (sSj , sTj ), samplenumber additional negative sentence pairs (sSj , nTneg ) corpus (i.e., sampledpairs observed positive pairs C). noise samples used formulatinghinge loss follows: E(sSj , sTj ) = max(mrg + E(sSj , sTj , nTneg ), 0), mrgmargin, E(sSj , sTj , nTneg ) = E(sSj , sTj ) E(sSj , nTneg ). loss minimized everypair parallel sentences corpus L2-regularization model parameters.number noise samples per positive pair hyper-parameter model.semantic signal propagated aligned sentences back individual words obtainbilingual word embeddings. BiCVM model originally built sentencealigned parallel data, exactly idea may applied document-aligned non-paralleldata. paper, test ability learn noisier comparable data. BWESG5. similar (but expensive) model also learns parallel sentence-aligned dataalso introduced Chandar et al. (2014).965fiVulic & Moensmodel compared BiCVM inducing BWEs data types: comparableparallel.4.2.2 Mikolovs MappingAnother collection BWE induction models (Mikolov et al., 2013b; Faruqui & Dyer, 2014;Dinu, Lazaridou, & Baroni, 2015; Lazaridou, Dinu, & Baroni, 2015) assumes followingsetup: first, two monolingual embedding spaces, RdimS RdimT , induced separatelytwo languages using standard monolingual model SGNS (Mikolovet al., 2013a, 2013c). dimS dimT denote dimensionality monolingual embeddingspaces source target language respectively. bilingual signal providedform word translation pairs (xi , yi ), xi V , yi V , x~i RdimS , y~i RdimT .Training cast multivariate regression problem: implies learning functionmaps source language vectors training data corresponding targetlanguage vectors. standard approach (Mikolov et al., 2013b; Dinu et al., 2015)assume linear map W RdimS dimT , L2 -regularized least-squares error objective(i.e., ridge regression) used learn map W: learned solving followingoptimization problem (typically stochastic gradient descent):minWRdimS dimT ||XW Y||2F + ||W||2F .X matrices obtained respective concatenation source languagetarget language vectors training pairs. linear map W estimated,previously unseen source language word vector x~u may straightforwardly mappedtarget language embedding space RdimT Wx~u . mapping vectors ~x, x V ,target embedding space RdimT fact serves bilingual embedding space (Figure 1).Although main strength model ability learn embeddings largermonolingual training sets, model may also adjusted settingtraining data document-aligned comparable data follows: (1) Automatically learnseed lexicon reliable one-to-one translation pairs document-aligned data usingbootstrapping approach Vulic Moens (2013b), (2) Train two separate monolingualembedding spaces two separated halves document-aligned data set (i.e., usingsource language documents target language documents), (3) Learn mappingtwo spaces using pairs Step 1.4.2.3 BilBOWAAnother collection BWE induction models jointly optimizes two monolingual objectives,cross-lingual objective acting cross-lingual regularizer training (Klementiev et al., 2012; Gouws et al., 2015; Soyer, Stenetorp, & Aizawa, 2015). ideabehind joint training may summarized simplified formulation (Luong, Pham, &Manning, 2015): (MonoS + MonoT ) + Bi.monolingual objectives onoS onoT ensure similar words language assigned similar embeddings aim capture semantic structurelanguage, whereas cross-lingual objective Bi ensures similar words across languages assigned similar embeddings, ties two monolingual spaces togetherbilingual space. Parameters govern influence monolingual bilingual966fiBilingual Distributed Word Representations Document-Aligned Datacomponents.6 bilingual signal models, acting cross-lingual regularizer joint training, provided sentence-aligned parallel data. Althoughuse data sources, models differ choice monolingual cross-lingualobjectives. work, opt BilBOWA model Gouws et al. (2015) representative model included comparisons, due previous solid performancerobustness BLE task, reduced complexity reflected fast computationsmassive datasets, well public availability. short, BilBOWA model combinesSGNS monolingual objectives together cross-lingual objective minimizes L2 -loss bag-of-word vectors parallel sentences. detailsexact training procedure, refer interested reader Gouws et al.swork.Again, although main strength model ability learn embeddingslarger monolingual training sets, model may also adjusted settingdocument- sentence-aligned data by: (1) using two halves aligned corpus separate monolingual training, (2) using alignment signal bilingual training.5. Word Representations Semantic Word SimilarityAssume induced bilingual word representations, regardless chosenRM. Given two words wi wj , irrespective actual language, may computedegree semantic similarity applying similarity function (SF) vector: sim(w , w ) = SF (,representationswwwjjwj ). Different choices (or rather familiesof) SFs cosine, Kullback-Leibler Jensen-Shannon divergence, Hellingerdistance, Jaccard index, etc. (Lee, 1999; Cha, 2007), different RMs typically requiredifferent SFs produce optimal near-optimal results various semantic tasks.working word embeddings, standard choice SF cosine similarity (cos) (Mikolovet al., 2013c), also typical choice traditional distributional models (Bullinaria& Levy, 2007). similarity computed follows:wwjsim(wi , wj ) = cos(wi , wj ) =| |||wwj(4)hand, good choice SF working probabilistic RMsBasic-MuPTM Association-MuPTM RS Hellinger distance (Pollard, 2001; Cha,2007; Kazama, Saeger, Kuroda, Murata, & Torisawa, 2010), displays excellent resultsBLE task (Vulic & Moens, 2013a). similarity words wi wj usingHellinger distance computed follows:vu K qq2X1 uP (fk0 |wi ) P (fk0 |wj )(5)sim(wi , wj ) =2 i=1Note Hellinger distance applicable word representations probabilitydistributions, case Basic-MuPTM Association-MuPTM. P (fk0 |wi ) de6. Setting = 0 reduces model setting similar BiCVM (Hermann & Blunsom, 2014b). = 1results models Klementiev et al. (2012), Gouws et al. (2015), Soyer et al. (2015).967fiVulic & Moensnotes probability score k-th dimension (fk0 ) vector representationBasic-MuPTM Association-MuPTM.7word wi , build ranked list RL(wi ) consists words wjranked according respective semantic similarity scores sim(wi , wj ). Additionally,label ranked list RL(wi ) pruned position RLM (wi ). Since mayretain language labels words training multilingual settings (e.g., language labelsmarked different colors Figure 2), may compute: (1) monolingual similarity,e.g., given wi V , retain wj V ranked list (analogous wi V ),(2) cross-lingual similarity (CLSS), e.g., given wi V , retain wj V , (3)multilingual similarity, retain words wj V V . computing CLSSwi , similar word cross-lingually called cross-lingual nearest neighbor.employ models context-insensitive CLSS word type level extract bilingual lexicons document-aligned sentence-aligned data, comparerepresentation models BLE task Section 7.5.1 Context Sensitive Models (Cross-Lingual) Semantic Similaritycontext-insensitive models semantic similarity provide ranked lists semanticallysimilar words invariably isolation, operate level word types.explicitly encode different word senses. practice, means that, given sentencecoach team satisfied game yesterday., context-insensitiveCLSS models able detect Spanish word entrenador similarpolysemous English word coach context sentence Spanish wordautocar, although autocar listed semantically similar word coach globally/invariably without observed context. another example, Spanish wordspartido, encuentro, cerilla correspondencia highly similar another ambiguousEnglish word match observed isolation, given Spanish sentence unable find match pocket light cigarette., clear strengthcross-lingual semantic similarity change context cerilla exhibits strongcross-lingual semantic similarity match within particular sentential context.goal build BWE-based models cross-lingual semantic similaritycontext, similar context-aware CLSS models proposed Vulic Moens (2014). Twokey questions are: (i) provide BWE-based representations beyond word levelrepresent context word token?; (ii) use contextual knowledgecontext-sensitive model semantic similarity?Following Vulic Moens (2014), given word token w context (e.g., windowwords, sentence, paragraph, document), build context set rather contextbag Con(w) = {cw1 , . . . , cwr } harvesting r neighboring words chosen context scope(e.g., context bag may comprise content-bearing words sentencepivot word token, so-called sentential context). order present context Con(w)d-dimensional embedding space, need apply model semantic compositionlearn d-dimensional vector representation Con(w).7. Prior work shown results Basic-MuPTM Association-MuPTM slightly highercosine replaced Hellinger distance. Therefore, particular case optedHellinger distance report competitive baseline.968fiBilingual Distributed Word Representations Document-Aligned DataFormally, given word w, may specify vector representation context bagCon(w) d-dimensional vector/embedding:Con(w) = cw1 ? cw2 ? . . . ?cwr(6), . . . ,d-dimensional WEs learned data, ? compositionalcwcw1rvector operator addition, point-wise multiplication, tensor product, etc.plethora models semantic composition proposed relevant literature, differing choice vector operators, input structures required knowledge(Mitchell & Lapata, 2008; Baroni & Zamparelli, 2010; Rudolph & Giesbrecht, 2010; Socher,Huval, Manning, & Ng, 2012; Blacoe & Lapata, 2012; Clarke, 2012; Hermann & Blunsom,2014b; Milajevs, Kartsaklis, Sadrzadeh, & Purver, 2014), name few. work,driven observed linear linguistic regularities embedding spaces (Mikolov et al.,2013d), opt simple addition (denoted +) Mitchell Lapata (2008)compositional operator, due simplicity, ease applicability bag-of-wordscontexts, relatively solid performance various compositional tasks (Mitchell &Lapata, 2008; Milajevs et al., 2014). d-dimensional embedding Con(w) then:Con(w) = cw1 + cw2 + . . . +cwr(7)use BWE-based RM, may compute context-sensitive semantic similarity score sim(wi , tj , Con(wi )) tj wi given context Con(wi ) sharedbilingual embedding space follows:sim(wi , tj , Con(wi )) = SF (wi0 , tj )(8)tj V target language word, tj word representation, wi0 newcontextualized vector representation wi modulated context Con(wi ), is,context-aware representation. Vulic Moens (2014) introduced linear interpolationtwo d-dimensional vectors plausible solution modulation/contextualization.modulation representation wi computed follows:+wi0 = (1 )wCon(wi )(9)word embedding w computed word type level,wCon(wi )embedding context bag computed using eq. (7), interpolation parameter.Another set similar models yield context-sensitive similarity computationsproposed recently, displayed competitive results regardlesssimplicity (Melamud, Levy, & Dagan, 2015). Here, present two best scoring contextsensitive models adapt bilingual setting:PSF (wi , tj ) + cwi Con(wi ) SF (cwi , tj )Add-Melamud: sim(wi , tj , Con(wi )) =|Con(wi )| + 1Mult-Melamud: sim(wi , tj , Con(wi )) = |Con(wi )|+1 SF (wi , tj )SF (cwi , tj )cwi Con(wi )969fiVulic & MoensNote Mult model one avoid negative values, simple shift allpositives interval required, e.g., shifted cosine score becomes cos0 (x, y) = cos(x,y)+1.2Unlike models Vulic Moens, two models aggregate single wordrepresentations one vector represents context, compute similarity scoresseparately word context. details regarding models,refer interested reader original Melamud et al.s work .employ models context-sensitive CLSS word token level comparerepresentation models task suggesting word translations context Section 8.6. Training Setupsection, provide insight training data experimental setupBWESG model baseline models.6.1 Training Datainduce bilingual word embeddings well directly comparable baseline representations prior work, use dataset comprising subset comparable Wikipedia data available three language pairs (Vulic & Moens, 2013b, 2014)8 : (i)collection 13, 696 Spanish-English Wikipedia article pairs (ES-EN), (ii) collection18, 898 Italian-English Wikipedia article pairs (IT-EN), (iii) collection 7, 612Dutch-English Wikipedia article pairs (NL-EN). corpora theme-aligned comparable corpora, is, aligned document pairs discuss similar themes, generaldirect translations other. directly comparable prior work twoevaluation tasks (Vulic & Moens, 2013b, 2014), retain nouns occur least5 times corpus. Lemmatized word forms recorded available, originalforms otherwise. TreeTagger (Schmid, 1994) used POS tagging lemmatization.preprocessing steps vocabularies comprise 7,000 13,000 noun typeslanguage language pair, training corpora quite small: rangingapproximately 1.5M tokens NL-EN 4M ES-EN. Exactly trainingdata vocabularies used train representation models comparison (bothGroup Group II, see Section 4).also demonstrate simple straightforward train BWESG parallel sentence-aligned data using modeling principles. purpose, useEuroparl.v7 (Koehn, 2005) three language pairs obtained OPUS website(Tiedemann, 2012).9 preprocessing step, retain words occurringleast 5 times corpus. corpus contains approximately 2M parallel sentences,vocabularies order magnitude larger smaller Wikipedia data(i.e., varying 45K EN word types 75K NL word types), corpora sizesapproximately 120M tokens. Data statistics two data sources, Wikipedia vs Europarl,provided Table 1. statistics reveal different nature two corpora,significantly variance noise reported Wikipedia data.8. Available online: people.cs.kuleuven.be/~ivan.vulic/software/9. http://opus.lingfil.uu.se/970fiBilingual Distributed Word Representations Document-Aligned DataCorpus:WikipediaEuroparlPair:ES-ENIT-ENNL-ENES-ENIT-ENNL-ENAverage length (OTHER)Average length (EN)Average length difference1111741278415412551129102292832929427274Table 1: Training data statistics: Non-parallel document-aligned Wikipedia vs parallelsentence-aligned Europarl three language pairs. = ES, NL.Lengths measured word tokens. Averages rounded closest integer.6.2 Trained BWESG Modelstest effect random shuffling merge shuffle BWESG strategy,trained BWESG model 10 random corpora shuffles three training corpora.also train BWESG length-ratio shuffle strategy. parameters setdefault suggested parameters SGNS word2vec package: stochastic gradientdescent (SGD) linearly decreasing global learning rate 0.025, 25 negative samples,subsampling rate 1e 4, 15 epochs.varied number dimensions = 100, 200, 300. also trainedBWESG = 40 directly comparable readily available sets BWEsprior work (Chandar et al., 2014). Moreover, test effect window size finalresults, i.e., number positives used training, varied maximum windowsize cs 4 60 steps 4.10make pre-training training code BWESG publicly available, alongBWESG-based bilingual word embeddings three language pairs at:http://liir.cs.kuleuven.be/software.php.6.3 Baseline Representations: Groupparameters baseline representation models (i.e., topic models settings,number dimensions K, values feature pruning, window size, weightingsimilarity functions) optimized prior work. Therefore, settings adopteddirectly previous work (Griffiths et al., 2007; Bullinaria & Levy, 2007; Dinu & Lapata,2010; Vulic & Moens, 2013a, 2013b; Kiela & Clark, 2014), encourage interestedreader check details exact parameter setup relevant literature. provideshort overview here.Basic-MuPTM Association-MuPTM, work Vulic Moens (2013a),bilingual latent Dirichlet allocation (BiLDA) model trained K = 2000 topicsstandard values hyper-parameters: = 50/K, = 0.01 (Steyvers & Griffiths,2007). Post-hoc semantic space pruning employed pruning parameter set200 Basic-MuPTM 2000 Association-MuPTM. refer readerrelevant paper details.Traditional-PPMI, work Vulic Moens (2013b), seed lexiconautomatically obtained bootstrapping initial seed lexicon reliable pairs stem10. remind reader slightly abuse terminology here, BWESG windows includelocality component more.971fiVulic & Moensming Association-MuPTM model (with parameters AssociationMuPTM listed above). window size fixed 6 directions.refer reader paper details.6.4 Baseline Representations: Group IIbaseline BWE models trained number dimensions BWESG:= 100, 200, 300. model-specific parameters taken suggested prior work.BiCVM, use tool released authors.11 train additive model,hinge loss margin mrg = original paper, batch size 50, noise parameter10. models trained 200 iterations.Mikolov, train two monolingual SGNS models using original word2vecpackage, SGD global learning rate 0.025, 25 negative samples, subsampling rate1e 4, 15 epochs. seed lexicon required learn mapping two monolingual spaces exactly Traditional-PPMI.BilBOWA, use SGD global learning rate 0.15 training12 , 25 negativesamples, subsampling rate 1e 4, 15 epochs. BilBOWA Mikolov, varywindow size way BWESG.6.5 Similarity FunctionsUnless stated otherwise, similarity function used similarity computationsRMs cosine (cos). exceptions Basic-MuPTM Association-MuPTMHellinger distance (HD) used since consistently outperformed cosinetwo RM types prior work (see Footnote 7).6.6 Roadmap Experimentsfirst experiment, quickly visually inspect obtained lists semantically similar words using BWESG bilingual representation model. Following that, compareBWESG-based models bilingual lexicon extraction (BLE) suggesting word translations context (SWTC) groups baseline models discussed Section 4.experiments results BLE task presented Section 7, experimentsresults SWTC presented Section 8.7. Evaluation Task I: Bilingual Lexicon ExtractionOne may employ context-insensitive CLSS models Section 5 extract bilinguallexicons automatically data.11. https://github.com/karlmoritz/bicvm12. Suggestions parameter values received personal correspondence authors. software available online: https://github.com/gouwsmeister/bilbowa972fiBilingual Distributed Word Representations Document-Aligned DataSpanish-English (ES-EN)(1)reina(2)reina(3)reina(Spanish)(English)reytronomonarcaherederomatrimoniohijoreinoreinadoregenciaduquequeen(+)heirthronekingroyalreignsuccessionprincessmarriageprinceItalian-English (IT-EN)(1)madreDutch-English (NL-EN)(2)madre(3)madre(Combined) (Italian)(English)(Combined) (Dutch)queen(+)reytronoheirthronemonarcaherederokingmatrimonioroyalmother(+)fathersisterwifedaughtersonfriendchildhoodfamilycousinmother(+)padremogliefathersorellafigliafigliosisterfratellowifepadremogliesorellafigliafigliofratellocasaamicomaritodonna(1)schilder(2)schilder(3)schilder(English)(Combined)kunstschilderpainter(+)schilderijpaintingkunstenaar portraitolieverfartistportretcanvasschilderen brushfranscubismnederlands artcomponist poetbeeldhouwer drawingpainter(+)kunstschilderpaintingschilderijkunstenaarportraitolieverfportretschilderenartistTable 2: Example lists top 10 semantically similar words 3 language pairs obtained using BWESG (length-ratio shuffle); = 200, cs = 48; (col 1.) sourcelanguage words (ES/IT/NL) listed target language words skipped(monolingual similarity); (2) target language words (EN) listed (crosslingual similarity); (3) words languages listed (multilingual similarity). correct one-to-one translation marked (+).7.1 Task Descriptionharvesting cross-lingual nearest neighbors, one able build bilingual lexiconone-to-one translation pairs (wiS , wjT ). test validity BWEs baselinerepresentations BLE task.7.2 Experimental SetupTest Data language pair, evaluate standard 1,000 ground truth one-to-onetranslation pairs built three language pairs (ES/IT/NL-EN) Vulic Moens(2013a, 2013b). Translation direction ES/IT/NL EN. data available online.13Evaluation Metrics Since build one-to-one bilingual lexicon harvestingone-to-one translation pairs, lexicon quality best reflected Acc1 score, is,number source language (ES/IT/NL) words wiS ground truth translation pairstop ranked word cross-lingually correct translation language(EN) according ground truth total number ground truth translation pairs(=1000) (Gaussier et al., 2004; Tamura et al., 2012; Vulic & Moens, 2013b). Similar trendsobserved within lenient setting Acc5 Acc10 scores, omitresults clarity fact actual BLE performance best reflected Acc1 .13. http://people.cs.kuleuven.be/ ivan.vulic/software/973fiVulic & MoensSpanish-English (ES-EN)Italian-English (IT-EN)BWESGBMuAMuTPPMIBWESGBMuAMuTPPMIcebollacebollacebollacebollagolfogolfogolfogolfoonion(+)dishmarinadecuisinesoupsaucecheesecoriandervegetabletortilladessertsaladnutwalnutricetoastporridgepaddytuberpotatodessertwalnutsaladnuthazelnutporridgericemarinadetoastpaddysaucecheesegarlicsaladchilionion(+)cuisineflavorbreaddishgulf(+)coastcoastlinebayislandpeninsulasettlementshoretourismferrywhaledolphincoastsubordercadmiumferrymonsoonfjordisthmusmainlandcoastisthmuscoastlinefjordferrymonsoonmainlandseasideislesubordercoastseaislandbaylagoonharbourbeachshoreriverlakeTable 3: Example lists top 10 semantically similar words ES-EN IT-EN, obtainedusing BWESG (length-ratio, = 200, cs = 48), three representationmodels Group I. correct translation marked (+).7.3 Results DiscussionTable 2 displays top 10 semantically similar words monolingually, across-languagescombined/multilingually one ES, NL word, Table 4 shows first setBLE results.7.3.1 Experiment 0: Qualitative Analysis ComparisonBWESG able find semantically coherent lists words three directions similarity (i.e., monolingual, cross-lingual, multilingual). combined (multilingual) rankedlists, words languages represented top similar words. initial qualitative analysis already demonstrates ability BWESG induce shared bilingualembedding space using document alignments bilingual signals.14another brief analysis, qualitatively compare cross-lingual ranked lists acquiredBWESG three baseline CLSS/BLE models Group I. lists oneES word one word presented Table 3. two example words, BWESGmodel able rank actual correct translations nearest cross-lingualneighbors. already symptomatic word gulf, correct translationgolfo, occur ranked list RL10 (golf o) case three baselinemodels. soon quantitatively confirm initial suspicion, demonstrateBWESG superior three baseline models BLE task.aside, Table 3 also clearly reveals difficulty judging quality modelscomputing semantic similarity/relatedness solely based observed outputmodels. lists RL10 (cebolla) RL10 (golf o) appear significantly different across14. also conducted small experiment solving word analogies using monolingual English embedding spaces, repeated experiment vocabulary bilingual EnglishSpanish/Italian/Dutch embedding spaces. results follow findings Faruqui Dyer (2014),slight (and often insignificant) fluctuations SGNS vectors reported (e.g., fluctuations < 1% average experiments) moving monolingual bilingual embeddingspaces. may conclude linguistic regularities established monolingual embedding spaces(Mikolov et al., 2013d) induced SGNS also hold bilingual embedding spaces induced BWESG.974fiBilingual Distributed Word Representations Document-Aligned DataPair:BWESGMerge Shufflecs:16,MINcs:16,AVGcs:16,MAXcs:48,MINcs:48,AVGcs:48,MAXES-ENIT-ENNL-ENd=100d=200d=300d=100d=200d=300d=100d=200d=3000.6070.6170.6250.6580.6650.6750.6000.6130.6300.6760.6850.6940.5770.5960.6130.6720.6880.7050.5850.5990.6070.6620.6690.6770.5970.6010.6060.6770.6830.6920.5710.5830.5960.6720.6830.6890.2930.3000.3070.3780.3890.3940.2440.2540.2670.3660.3810.3950.2190.2240.2330.3540.3630.377d=100d=200d=300d=100d=200d=300d=100d=200d=3000.6270.6780.6100.7010.6020.7030.6130.6790.6140.6890.5950.6920.3030.3970.2750.3960.2370.382BWESGShufflingcs:16cs:48d=100d=200d=300d=100d=200d=300d=100d=200d=3000.2180.5110.1760.4970.1390.4800.2090.5230.1980.5400.1620.5260.0700.2140.0680.1980.0490.197BMuAMuTPPMI0.4410.5180.5770.4410.5180.5770.4410.5180.5770.5750.6180.6470.5750.6180.6470.5750.6180.6470.2370.2360.2060.2370.2360.2060.2370.2360.206BWESGLength-Ratiocs:16cs:48Table 4: BLE performance terms Acc1 scores tested BLE models SpanishEnglish, Italian-English Dutch-English bilingual word representationslearned document-aligned Wikipedia data. BWESG mergeshuffle report maximum (MAX), minimum (MIN) average (AVG) scores10 random corpora shuffles. Highest scores per column bold.four models, yet lists contain words appear semantically related sourceword. Therefore, require systematic quantitative task-oriented comparisoninduced word representations.7.3.2 Experiment I: BWESG vs GroupTable 4 shows first set results BLE task: report scores two differentBWESG strategies well BWESG model shuffle pseudo-bilingualdocuments. previous best reported Acc1 scores baseline representationstraining+test combination also reported table. zooming tablemultiple times, summarize important findings.BWESG vs Baseline Representations results clearly reveal superior performance BWESG model BLE relies new framework inducingbilingual word embeddings document-aligned comparable data BLE modelsrelying previously used bilingual word representations type trainingdata. increase Acc1 scores best scoring baseline models 22.2% ES-EN,7% IT-EN 67.5% NL-EN.BWESG Shuffling Strategy Although BWESG strategies display resultsestablished baselines, clear advantage length-ratio shuffle strategy,displays solid robust performance across variety parameters threelanguage pairs. Another advantage strategy fact deterministicoutcome suffer sub-optimal random shuffles. summary, suggest975fiAcc1 scoresVulic & Moens0.70.70.60.60.50.50.40.40.30.30.20.20.50.40.30.20.10.1= 100= 200= 3000.048 12 16 20 24 28 32 36 40 44 48 52 56 60Window size(a) Spanish-English0.1= 100= 200= 3000.048 12 16 20 24 28 32 36 40 44 48 52 56 60Window size(b) Italian-English= 100= 200= 3000.048 12 16 20 24 28 32 36 40 44 48 52 56 60Window size(c) Dutch-EnglishFigure 3: Acc1 scores BLE task BWESG length-ratio shuffle 3 languagepairs, varying values parameters cs d. Solid (red) horizontal linesdenote highest baseline Acc1 scores language pair. Thicker dottedlines refer BWESG without shuffling.using length-ratio shuffle strategy future work, along line optstrategy experiments.results also reveal shuffling universally useful, BWESG without shufflingrelies largely monolingual contexts cannot reach performance BWESGshuffling. partial remedy problem train BWESG documentlevel training pairs (i.e., increasing window size), leads prohibitivelyexpensive models, nonetheless BWESG without shuffling larger cs-s still fallsshort BWESG shuffling strategies (see also Figures 3(a)-3(c)).Window Size: Number Training Pairs results confirm intuition largerwindow sizes, i.e., training examples lead better results BLE task.embedding dimensions d-s, BWESG exhibits superior performance cs = 48cs = 16, performance cs = 48 cs = 60 seems relatively stable: intuitively,training pairs leads slightly better BLE performance, curve slowly flattens(Figures 3(a)-3(c)). finding reveals even coarse tuning parametersmight lead optimal near-optimal scores BLE BWESG.Differences across Language Pairs lower increase Acc1 scores IT-EN attributed fact test set IT-EN comprises words occurrence frequencies 200 training data (Vulic & Moens, 2013a), two test setscomprise randomly sampled words covering frequency spectra. expected, modelscomparison able effectively utilize distributional signals higher-frequency words,BWESG still displays best performance, improvements Acc1 scoresstatistically significant (using McNemars statistical significance test, p < 0.05).15Further, lowest overall scores models comparison observed NL-EN.attribute using less training data NL-EN compared ES-EN IT-EN(i.e., training corpora ES-EN IT-EN almost triple size training corporaNL-EN). However, observe increase obtained BWESG evenprominent setting limited training data. lower results TPPMI compared15. McNemars significance test common NLP literature, especially Acc1 scoresreported. utilizes standard 22 contingency table, may observed paired versioncommon chi-square test. reader referred original work McNemar (1947).976fiBilingual Distributed Word Representations Document-Aligned DataPair:BWESGLength-Ratiocs:48ES-ENIT-ENNL-ENd=100d=200d=300d=100d=200d=300d=100d=200d=3000.6780.7010.7030.6790.6890.6920.3970.3960.382Mikolovcs:4cs:8cs:16cs:48cs:600.1870.3050.3440.3110.3240.1510.3060.3960.3750.3890.2820.4200.4860.4770.4790.3680.4620.4720.4580.4600.3820.5180.5390.5360.5380.5330.5820.6020.5910.5970.0420.0760.1170.1320.1510.0680.0950.1610.1780.1800.1200.1450.1840.2020.209BiCVMiterations:2000.3420.3840.4030.3090.3660.3770.0680.0840.083Table 5: BLE results: Comparison BWESG (1) BWE induction modelMikolov et al. (2013b) relying SGNS, (2) BiCVM: BWE induction modelHermann Blunsom (2014b) initially developed parallel sentence-aligneddata. models trained document-aligned training Wikipediadata exactly vocabularies.two baseline models also attributed overall lower quality sizeNL-EN training data, reflected lower quality seed lexicons necessarystart bootstrapping procedure Vulic Moens (2013b).Computational Complexity BWESG trained larger values cs yields richersemantic representations, also naturally leads increased training times. However, duelightweight design supporting SGNS, times order magnitudelower training times Basic-MuPTM Association-MuPTM. Typically, severalhours needed train BWESG = 300 cs 48 60, whereas takes twothree days train bilingual topic model K = 2000 training set usingmulti-threaded architectures 10 Intel(R) Xeon(R) CPU E5-2667 2.90GHz processors.BWESG model scales expected (i.e., training time increases linearly windowsize parameters equal), enjoys advantages (training time-wisememory-wise) original word2vec package. logical explanation behaviourfollows interpretation SGNS provided Levy Goldberg (2014a), e.g., usingwindow size 48 instead window size 16 basically means using 3 times positiveexamples training (e.g., approximately 15 minutes needed train 300-dimensionalES-EN BWESG embeddings cs = 16 using Wikipedia data opposed 46minutes cs = 48, measured 10 Intel(R) Xeon(R) processors).7.3.3 Experiment II: BWESG vs BWE Induction Models (Group II)experiments conducted using BWESG length-ratio shuffle strategy.Note models comparison use exactly data sources vocabulariesBWESG Group models previous section. results BiCVMMikolov model summarized Table 5: comparison reveals clear prominentadvantage BWESG model given data training setup.report absolute scores BilBOWA model setup much lowertwo baseline models. BiCVM model, although theory fit learn977fiVulic & Moens0.60.620.60.55Acc1 scores0.580.50.560.540.450.4248Window size= 100= 200= 300= 100= 200= 30016(BWESG)(BWESG)(BWESG)(BilBOWA)(BilBOWA)(BilBOWA)0.52= 100 (BiCVM)= 200 (BiCVM)= 300 (BiCVM)0.54824(a) Spanish-English8Window size1648(b) Italian-English0.70.66Acc1 scores0.620.580.540.50.46248Window size1648(c) Dutch-EnglishFigure 4: Comparison BWESG (solid curves) two models rely parallel training data: (1) BilBOWA (dotted curves), (2) BiCVM: BWE induction modelinitially developed parallel sentence-aligned data (dashed horizontallines). models trained sentence-aligned training Europarldata exactly vocabularies. BLE performed searchspace models. x axes log scale.document-aligned data, unable compete BWESG learning BWEsnoisier setting non-parallel data.also present preliminary study compare BWSESG Group II modelssetup parallel sentence-aligned data. Results summarized Figures 4(a)-4(c).16preliminary results clearly demonstrate BWESG able learn BWEsparallel data without slightest change modeling principles. BilBOWAmodel displays better results lower values cs parameter, surprise,16. Note absolute scores directly comparable BLE scores model trainedWikipedia data (Tables 4 5) due different training data, different preprocessing stepsvocabularies. Different vocabularies also result different BLE search spaces coverages testsets (e.g., common Spanish nouns test set nadador (swimmer) colmillo(tusk) observed Europarl due domain shift).978fiBilingual Distributed Word Representations Document-Aligned DataBWESG model comparable even better baseline models larger windowsizes. BiCVM model, implicitly utilizes entire sentence span trainingalso outperforms BWESG smaller windows, BWESG performs significantlybetter larger windows. BWESG performance flattens quickerWikipedia data (compare results cs = 16 cs = 48), easily explaineddecreased length aligned items provided Table 1 (i.e., sentences vs documents).English-Spanish, also compare BWESG pre-trained 40-dimensional embeddings Chandar et al. (2014), embeddings also inducedEuroparl data. models Acc1 score 0.432 = 40, BWESG obtains Acc1scores 0.502 (d = 40, cs = 8), 0.535 (d = 40, cs = 16) 0.529 (d = 40, cs = 48).8. Evaluation Task II: Suggesting Word Translations Contextanother task, test ability BWEs produce context-sensitive semantic similaritymodeling (see Section 5.1), turn may used solve task suggesting wordtranslations context (SWTC) proposed recently (Vulic & Moens, 2014). goalbuild BWESG-based models SWTC given sentential context, similarprior work. show new BWESG-based SWTC models outperform bestSWTC models Vulic Moens, well SWTC models rely baselineword representations discussed Section 4.8.1 Task DescriptionGiven occurrence polysemous word wi V context occurrence,SWTC task choose correct translation target language LT particularoccurrence wi given set C(wi ) = {t1 , . . . , ttq }, C(wi ) V , tq possibletranslations/meanings. may refer C(wi ) inventory translation candidateswi . task suggesting word translations context (SWTC) may interpretedranking tq translation candidates respect observed local context Con(wi )occurrence word wi . best scoring translation candidate accordingscores sim(wi , tj , Con(wi )) (see Section 5.1) ranked list correct translationparticular occurrence wi observing local context Con(wi ).8.2 Experimental SetupTest Data use SWTC test set introduced recently (Vulic & Moens, 2014). testset comprises 15 polysemous nouns three languages (ES, NL) along setstranslation candidates (i.e., sets C). polysemous noun, test sets provide24 sentences extracted Wikipedia illustrate different senses translationspivot polysemous noun, accompanied annotated correct translation sentence. yields 360 test sentences language pair (and 1080 test sentences total).additional set 100 sentences (5 polysemous nouns plus 20 sentencesnoun) used development set tune parameter (see Section 5.1)language pairs models comparison. summary, final aim may formulatedfollows: polysemous word wi ES/IT/NL, goal suggest correcttranslation English given sentential context.979fiVulic & MoensEvaluation Metrics Since task present list possible translations SWTCmodel, let model decide single likely translation given wordsentential context, measure performance Top 1 accuracy (Acc1 ).8.3 Results Discussioncompare Group Group II models. Note Group modelsheld previously best reported SWTC scores training Wikipedia dataalso use work.8.3.1 Experiment I: BWESG vs GroupModels Comparison (1) BWESG+add. RM: BWESG. SF: cos. Composition: addition. = 1.0. value suggests context used disambiguatemeaning polysemous word guess likely translation context.17(2) BMu+HD+S. RM: BasicMuPTM. SF: Hellinger distance. Composition: SmoothedFusion18 Vulic Moens (2014). = 0.9.(3) BMu+Cue+S. RM: BasicMuPTM. SF: Cue Association measure (Steyvers & Griffiths, 2007; Vulic & Moens, 2013a). Composition: Smoothed-Fusion. = 0.9.Cue similaritymodels computed association scorePKis tailored probabilistic00P (ti |wi ) = k=1 P (ti |zk )P (zk |wi ), zk denotes k-th latent feature, P (zk |wi0 ) denotes modulated probability score obtained smoothing probabilistic representations wi context Con(wi ).(4) TPPMI+add. RM: Traditional-PPMI. SF: cos. Composition: addition. = 0.9.Again, parameters baseline representation models adopted directlyprior work optimized development sets comprising additional 100 sentences (Vulic & Moens, 2014). addition, BMu+HD+S BMu+Cue+S also relyprocedure context sorting pruning (Vulic & Moens, 2014), idea retaincontext words semantically similar given pivot polysemous word,use computations. procedure, however, produces significant gainsprobabilistic models (BMu+HD+S BMu+Cue+S), therefore, employmodels. BMu+HD+S BMu+Cue+S context sorting pruningbest scoring models introductory SWTC paper Vulic Moenscurrently produce state-of-the-art SWTC results test sets.19Table 6 summarizes results comparison Group models SWTC task.NO-CONTEXT refers context-insensitive majority baseline (i.e., always choosingsemantically similar translation candidate obtained BWESG word type level,without taking account context information).17. also experimented context-sensitive CLSS models proposed Melamud et al. (2015),report actual scores model, although displaying similar relative rankingdifferent representation models, consistently outperformed models Vulic Moens (2014)evaluation runs: 0.75-0.80 vs 0.60-0.65 models Melamud et al. (2015).18. short, Smoothed-Fusion probabilistic variant context-sensitive modeling idea presentedequations (7)-(9). details, check work Vulic Moens (2014).19. omit results Association-MuPTM RM since SWTC models based Association-MuPTMconsistently outperformed SWTC models based Basic-MuPTM across different settings.980fiBilingual Distributed Word Representations Document-Aligned DataPair:BWESG+addLength-Ratiocs:16cs:48ES-ENIT-ENNL-ENd=100d=200d=300d=100d=200d=300d=100d=200d=3000.794*0.752*0.767*0.758*0.752*0.764*0.817*0.814*0.7890.831*0.7940.814*0.778*0.797*0.769*0.789*0.767*0.775*BWESG+addShufflingcs:16cs:48d=100d=200d=300d=100d=200d=300d=100d=200d=3000.7170.7310.7170.6920.6940.6860.7470.7750.7280.7780.7280.7580.7220.7390.6860.7330.6780.719NO-CONTEXT0.4060.4060.4060.4080.4080.4080.4330.4330.433BMu+HD+SBMu+Cue+STPPMI+add0.6640.7030.6190.6640.7030.6190.6640.7030.6190.7310.7610.7060.7310.7610.7060.7310.7610.7060.6690.7120.6140.6690.7120.6140.6690.7120.614Table 6: comparison SWTC models Spanish-English, Italian-English DutchEnglish bilingual word representations learned document-alignedWikipedia data. asterisk (*) denotes statistically significant improvementsBWESG+add strongest baseline according McNemars statisticalsignificance test (p < 0.05). Highest scores per column bold.BWESG vs Baseline Representations results reveal BWESG outperformsbaseline bilingual word representations Group also SWTC task. improvements prominent reported values parameters cs, oftenstatistically significant even compared strongest baseline (which finetuned BMu+Cue+S model context sorting pruning three language pairsVulic & Moens, 2014). increase Acc1 scores strongest baseline 12.9%ES-EN, 11.9% IT-EN, 12.4% NL-EN. obtained results surpass previousstate-of-the-art scores currently best reported results SWTC datasetsusing non-parallel data learn semantic representations.BWESG Shuffling Strategy Although BWESG without shuffling (due reduced complexity SWTC task compared BLE) already displays encouraging results,clear advantage length-ratio shuffle strategy, displays excellentperformance three language pairs. simple words, shuffling useful.Dimensionality Number Training Pairs Unlike BLE task, highest Acc1 scores average obtained using lower-dimensional word embeddings (i.e.,= 100). phenomenon may attributed effect semantic compositionreduced complexity SWTC task compared BLE task. First, although enlarging dimensionality embeddings leads increased semantic expressiveness withinSenses:2 senses3 senses4 sensesModelAcc1Acc1Acc1BMu+Cue+SBWESG+add0.8270.8340.6190.8040.4170.583Table 7: comparison best scoring baseline model BMu+Cue+S best scoringBWESG+add model different clusters words (2-sense, 3-sense 4-sensewords) Spanish-English.981fiVulic & Moensshared bilingual embedding space, may harmful working compositionmodels, since simple additive model semantic composition may produce erroneous dimensions constructing higher-dimensional context embeddings singleword embeddings. Second, due design, SWTC task requires coarser-grained representations BLE. BLE task goal detect translation wordvocabulary typically spans (tens of) thousands words, SWTC taskgoal detect likely translation word given sentential context,small closed vocabulary 2-4 possible translations translation inventory.Therefore, highly likely even low-dimensional embeddings sufficient produce plausible rankings SWTC task, time, sufficientexpressive enough find correct translations BLE. training pairs (i.e., largerwindows) still yield better results average SWTC task. summary, choicerepresentation granularity dependent actual task, consequently leadsconclusion optimal values cs largely task-specific (compare also resultsTable 4 Table 6).Testing Polysemy order test whether gain performance BWESG+addderived mostly effective handling easiest set words, is, bisemouswords (polysemous words 2 translation candidates), performed additional experiment, measured Acc1 scores separately words 2, 3,4 different senses. Results indicate performance gain comes mostly gainstrisemous tetrasemous words, scores bisemous words comparable.Table 7 shows Acc1 different clusters words ES-EN, similar scoring patternsobserved IT-EN NL-EN.Differences across Language Pairs Due reduced complexity SWTC, mayalso observe relatively higher results NL-EN compared ES-EN IT-EN,opposed relative performance BLE task, scores NL-ENmuch lower scores ES-EN IT-EN. Since SWTC less difficult taskrequires coarse-grained representations, even limited amounts training data may sufficient learn word embeddings useful specific task. findingline recent work Gouws Sgaard (2015).8.3.2 Experiment II: BWESG vs. BWE Induction Models (Group II)test BWE induction models SWTC task, using training setupsets embeddings introduced Section 7.3.3 BLE task. representationsplugged context-sensitive CLSS modeling framework Section 5.1,optimization parameters SWTC conducted mannerBWESG. results Mikolov model BiCVM summarized Table 8.results BilBOWA similar BiCVM, report brevity.BWESG outperforms BWE induction models SWTC taskconfirms utility cross-lingual semantic modeling. model Mikolov et al. (2013b)constitutes stronger baseline: Good results SWTC task modelinteresting finding per se. model competitive BWESGbaseline representations models document-aligned data difficult BLE taskusing noisy one-to-one translation pairs, performance less complex SWTC982fiBilingual Distributed Word Representations Document-Aligned DataPair:BWESG+addLength-Ratiocs:16cs:48ES-ENIT-ENNL-ENd=100d=200d=300d=100d=200d=300d=100d=200d=3000.7940.7520.7670.7580.7520.7640.8170.8140.7890.8310.7940.8140.7780.7970.7690.7890.7670.775cs:4cs:8cs:16cs:48cs:600.7420.7670.7690.6780.6360.7390.7500.7440.6420.6580.7250.7470.7470.6690.6560.7330.7670.7580.7140.7250.7060.7470.7550.7140.7250.6920.7440.7580.7470.7420.6920.6940.7250.7250.7220.7000.6970.7000.7110.7280.7000.6720.6890.7080.722BiCVMiterations:2000.5470.5670.5390.6360.6640.6420.5860.5670.581MikolovTable 8: SWTC results: Comparison BWESG (1) BWE induction modelMikolov et al. (2013b) relying SGNS, (2) BiCVM: BWE induction modelHermann Blunsom (2014b) initially developed parallel sentence-aligneddata. models trained document-aligned training Wikipediadata exactly vocabularies.task reduced search space solid even model relies imperfect settranslation pairs learn mapping two monolingual embedding spaces.8.3.3 Discussionanalyzing influence pre-training shuffling results two different evaluationtasks, may safely establish utility inducing bilingual word embeddings usingBWESG model. already presented two shuffling strategies work, oneline future work investigate different possibilities blending words twodifferent vocabularies pseudo-bilingual documents structured systematicmanner. instance, one approach generating pseudo-training sentences learningtextual perceptual modalities recently introduced (Hill & Korhonen,2014). However, straightforward extend approach generationpseudo-bilingual training documents.Another idea vein build artificial training data higher-quality startingnoisy comparable data by: (1) computing semantically similar words monolinguallyacross-languages noisy data, (2) retaining highly reliable pairs similarwords using automatic selection procedure (Vulic & Moens, 2012), (3) building pseudobilingual documents using reliable context word pairs. words, questions is:possible choose positive training pairs systematically reduce noise stemming non-parallel data? construction artificial training data trainingdata would proceed bootstrapping fashion, model ablesteadily reduce noise inherently present comparable data. idea improvingcorpus comparability touched upon previous work (Li & Gaussier, 2010; Li,Gaussier, & Aizawa, 2011).entire framework proposed article theory completely languagepair agnostic make language pair dependent modeling assumptions,acknowledge fact three language pairs comprise languages coming983fiVulic & Moensphylum, is, Indo-European language family. Future extensions also include portingframework distant language pairs share rootsalphabet (e.g., English-Chinese/Hindi/Arabic), benchmarking test setsstill scarce variety semantic tasks (e.g., SWTC) (Camacho-Collados, Pilehvar,& Navigli, 2015). believe larger window sizes may solve difficulties differentword orderings (e.g., Chinese-English).9. Conclusions Future Workproposed described Bilingual Word Embeddings Skip-Gram (BWESG), simple yet effective bilingual word representation learning model able induce bilingual word embeddings solely basis document-aligned comparable data. BWESGbased omnipresent skip-gram negative sampling (SGNS). presentedtwo ways build pseudo-bilingual documents monolingual SGNS (ormonolingual induction model) may trained produce shared bilingual embeddingspaces. BWESG model make language-pair dependent assumptionsrequires language-pair specific external resources bilingual lexicons, predefined category/ontology knowledge parallel data. showed model may trainednon-parallel parallel data without changes modeling principles, which, complemented simplicity lightweight design makes potentially usefultool researchers machine translation information retrieval.employed induced BWEs two semantic tasks: (1) bilingual lexicon extraction(BLE), (2) suggesting word translations context (SWTC). new BWESG-basedBLE SWTC models outperform previous state-of-the-art models BLE SWTCdocument-aligned comparable data related BWE induction models (Mikolov et al.,2013b; Chandar et al., 2014; Gouws et al., 2015). findings article followrecently published surveys Baroni et al. (2014), Levy et al. (2015) regardingsolid robust performance neural word representations/word embeddings semantictasks: new BWESG-based models BLE SWTC significantly outscore previousstate-of-the-art distributional approaches tasks across different parameter settings.Even encouraging fact new state-of-the-art results attained usingdefault parameter settings BWESG model suggested word2vec packagewithout development set. (finer) tuning model parameters future workmay lead higher-quality bilingual embedding spaces.Several straightforward lines future research already tackled Section 7Section 8. instance, current length-ratio shuffling strategy may replacedadvanced shuffling method future work. Moreover, BWEs induced BWESGmay used semantic tasks besides ones discussed work, wouldinteresting experiment types context aggregation selection beyondbag-of-words assumption, dependency-based contexts (Levy & Goldberg, 2014a),objective functions training vein proposed Levy Goldberg(2014b). Similar evolution multilingual probabilistic topic modeling, another pathfuture work may lead investigating bilingual models learning BWEsable jointly learn separate documents aligned document pairs, without needconstruct pseudo-bilingual documents.984fiBilingual Distributed Word Representations Document-Aligned Datanatural step text representation learning research extend focussingle word representations composite phrase, sentence document representations(Hermann & Blunsom, 2013; Kalchbrenner, Grefenstette, & Blunsom, 2014; Le & Mikolov,2014; Soyer et al., 2015; Kiros, Zhu, Salakhutdinov, Zemel, Torralba, Urtasun, & Fidler,2015; Hill, Cho, Korhonen, & Bengio, 2016). article, relied simple composition model based vector addition, shown model performs excellentSWTC task. However, long run model means sufficienteffectively capture complex compositional phenomena data. Several modelsaim learn sentence document embeddings proposed recently,critically rely sentence-aligned parallel data. yet seen build structuredmultilingual phrase, sentence document embeddings solely basis comparable data. low-cost multilingual embeddings beyond word level extractedcomparable data may find application variety tasks statistical machinetranslation (Mikolov et al., 2013b; Zou et al., 2013; Zhang et al., 2014; Wu et al., 2014),semantic tasks multilingual semantic textual similarity (Agirre, Banea, Cardie, Cer,Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, & Wiebe, 2014), cross-lingual informationretrieval (Vulic et al., 2013; Vulic & Moens, 2015) cross-lingual document classification(Klementiev et al., 2012; Hermann & Blunsom, 2014b; Chandar et al., 2014).another future research path, may use knowledge BWEs obtainedBWESG document-aligned data learn bilingual correspondences (e.g., word translation pairs lists semantically similar words across languages) may turnused learning large unaligned multilingual datasets (Mikolov et al., 2013b; Al-Rfou,Perozzi, & Skiena, 2013). long run, idea may lead large-scale learning models huge amounts multilingual data without requirement parallel datamanually built bilingual lexicons.Acknowledgmentswork done Ivan Vulic postdoctoral researcher Department Computer Science, KU Leuven supported PDM Kort fellowship (PDMK/14/117).work also supported SCATE project (IWT-SBO 130041) ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (648909). would alsolike thank anonymous reviews helpful suggestions helped us greatlyimprove presentation work.ReferencesAgirre, E., Banea, C., Cardie, C., Cer, D., Diab, M., Gonzalez-Agirre, A., Guo, W., Mihalcea, R., Rigau, G., & Wiebe, J. (2014). SemEval-2014 task 10: Multilingual semantictextual similarity. Proceedings 8th International Workshop SemanticEvaluation (SEMEVAL), pp. 8191. Association Computational Linguistics.Al-Rfou, R., Perozzi, B., & Skiena, S. (2013). Polyglot: Distributed word representationsmultilingual NLP. Proceedings Seventeenth Conference ComputationalNatural Language Learning (CoNLL), pp. 183192.985fiVulic & MoensBaroni, M., Dinu, G., & Kruszewski, G. (2014). Dont count, predict! systematic comparison context-counting vs. context-predicting semantic vectors. Proceedings52nd Annual Meeting Association Computational Linguistics (ACL),pp. 238247.Baroni, M., & Zamparelli, R. (2010). Nouns vectors, adjectives matrices: Representing adjective-noun constructions semantic space. Proceedings 2010Conference Empirical Methods Natural Language Processing (EMNLP), pp.11831193.Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). neural probabilistic languagemodel. Journal Machine Learning Research, 3, 11371155.Blacoe, W., & Lapata, M. (2012). comparison vector-based representations semantic composition. Proceedings 2012 Joint Conference Empirical Methods Natural Language Processing Computational Natural Language Learning(EMNLP-CoNLL), pp. 546556.Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. JournalMachine Learning Research, 3, 9931022.Boyd-Graber, J., & Blei, D. M. (2009). Multilingual topic models unaligned text.Proceedings 25th Conference Uncertainty Artificial Intelligence (UAI),pp. 7582.Bullinaria, J. A., & Levy, J. P. (2007). Extracting semantic representations wordco-occurrence statistics: computational study. Behavior Research Methods, 39 (3),510526.Camacho-Collados, J., Pilehvar, M. T., & Navigli, R. (2015). framework construction monolingual cross-lingual word similarity datasets. Proceedings53rd Annual Meeting Association Computational Linguistics 7thInternational Joint Conference Natural Language Processing (ACL-IJCNLP), pp.17.Carbonell, J. G., Yang, J. G., Frederking, R. E., Brown, R. D., Geng, Y., Lee, D., Frederking,Y., E, R., Geng, R. D., & Yang, Y. (1997). Translingual information retrieval:comparative evaluation. Proceedings 15th International Joint ConferenceArtificial Intelligence (IJCAI), pp. 708714.Cha, S.-H. (2007). Comprehensive survey distance/similarity measures probability density functions. International Journal Mathematical Models MethodsApplied Sciences, 1 (4), 300307.Chandar, S., Lauly, S., Larochelle, H., Khapra, M. M., Ravindran, B., Raykar, V. C., &Saha, A. (2014). autoencoder approach learning bilingual word representations.Proceedings 27th Annual Conference Advances Neural InformationProcessing Systems (NIPS), pp. 18531861.Chen, D., & Manning, C. (2014). fast accurate dependency parser using neuralnetworks. Proceedings 2014 Conference Empirical Methods NaturalLanguage Processing (EMNLP), pp. 740750.986fiBilingual Distributed Word Representations Document-Aligned DataClarke, D. (2012). context-theoretic framework compositionality distributionalsemantics. Computational Linguistics, 38 (1), 4171.Collobert, R., & Weston, J. (2008). unified architecture natural language processing:Deep neural networks multitask learning. Proceedings 25th InternationalConference Machine Learning (ICML), pp. 160167.Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. P. (2011).Natural language processing (almost) scratch. Journal Machine LearningResearch, 12, 24932537.Das, D., & Petrov, S. (2011). Unsupervised part-of-speech tagging bilingual graphbased projections. Proceedings 49th Annual Meeting AssociationComputational Linguistics: Human Language Technologies (ACL-HLT), pp. 600609.Daume III, H., & Jagarlamudi, J. (2011). Domain adaptation machine translationmining unseen words. Proceedings 49th Annual Meeting AssociationComputational Linguistics: Human Language Technologies (ACL-HLT), pp. 407412.De Smet, W., & Moens, M.-F. (2009). Cross-language linking news stories Webusing interlingual topic modeling. Proceedings CIKM 2009 WorkshopSocial Web Search Mining (SWSM@CIKM), pp. 5764.Deschacht, K., De Belder, J., & Moens, M.-F. (2012). latent words language model.Computer Speech & Language, 26 (5), 384409.Deschacht, K., & Moens, M.-F. (2009). Semi-supervised semantic role labeling usinglatent words language model. Proceedings 2009 Conference EmpiricalMethods Natural Language Processing (EMNLP), pp. 2129.Dinu, G., & Lapata, M. (2010). Measuring distributional similarity context. Proceedings 2010 Conference Empirical Methods Natural Language Processing(EMNLP), pp. 11621172.Dinu, G., Lazaridou, A., & Baroni, M. (2015). Improving zero-shot learning mitigatinghubness problem. ICLR Workshop Papers.Duchi, J. C., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods onlinelearning stochastic optimization. Journal Machine Learning Research, 12,21212159.Dumais, S. T., Landauer, T. K., & Littman, M. (1996). Automatic cross-linguistic information retrieval using Latent Semantic Indexing. Proceedings SIGIR WorkshopCross-Linguistic Information Retrieval, pp. 1623.Elman, J. L. (1990). Finding structure time. Cognitive Science, 14, 179211.Faruqui, M., & Dyer, C. (2014). Improving vector space word representations using multilingual correlation. Proceedings 14th Conference European ChapterAssociation Computational Linguistics (EACL), pp. 462471.Fukumasu, K., Eguchi, K., & Xing, E. P. (2012). Symmetric correspondence topic modelsmultilingual text analysis. Proceedings 25th Annual Conference AdvancesNeural Information Processing Systems (NIPS), pp. 12951303.987fiVulic & MoensGanchev, K., & Das, D. (2013). Cross-lingual discriminative learning sequence modelsposterior regularization. Proceedings 2013 Conference EmpiricalMethods Natural Language Processing (EMNLP), pp. 19962006.Gaussier, E., Renders, J.-M., Matveeva, I., Goutte, C., & Dejean, H. (2004). geometricview bilingual lexicon extraction comparable corpora. Proceedings42nd Annual Meeting Association Computational Linguistics (ACL), pp.526533.Geman, S., & Geman, D. (1984). Stochastic relaxation, Gibbs distributions,Bayesian restoration images. IEEE Transactions Pattern Analysis MachineIntelligence, 6 (6), 721741.Goldberg, Y., & Levy, O. (2014). Word2vec explained: Deriving Mikolov et al.s negativesampling word-embedding method. CoRR, abs/1402.3722.Gouws, S., Bengio, Y., & Corrado, G. (2015). BilBOWA: Fast bilingual distributed representations without word alignments. Proceedings 32nd International ConferenceMachine Learning (ICML), pp. 748756.Gouws, S., & Sgaard, A. (2015). Simple task-specific bilingual word embeddings.Proceedings 2015 Conference North American Chapter AssociationComputational Linguistics: Human Language Technologies (NAACL-HLT), pp.13861390.Griffiths, T. L., Steyvers, M., & Tenenbaum, J. B. (2007). Topics semantic representation.Psychological Review, 114 (2), 211244.Haghighi, A., Liang, P., Berg-Kirkpatrick, T., & Klein, D. (2008). Learning bilingual lexiconsmonolingual corpora. Proceedings 46th Annual Meeting Association Computational Linguistics: Human Language Technologies (ACL-HLT), pp.771779.Harris, Z. S. (1954). Distributional structure. Word, 10 (23), 146162.Hermann, K. M., & Blunsom, P. (2013). role syntax vector space modelscompositional semantics. Proceedings 51st Annual Meeting AssociationComputational Linguistics (ACL), pp. 894904.Hermann, K. M., & Blunsom, P. (2014a). Multilingual distributed representations withoutword alignment. Proceedings 2014 International Conference LearningRepresentations (ICLR).Hermann, K. M., & Blunsom, P. (2014b). Multilingual models compositional distributedsemantics. Proceedings 52nd Annual Meeting Association Computational Linguistics (ACL), pp. 5868.Hill, F., Cho, K., Korhonen, A., & Bengio, Y. (2016). Learning understand phrasesembedding dictionary. Transactions ACL, 4, 1730.Hill, F., & Korhonen, A. (2014). Learning abstract concept embeddings multi-modaldata: Since probably cant see mean. Proceedings 2014 ConferenceEmpirical Methods Natural Language Processing (EMNLP), pp. 255265.988fiBilingual Distributed Word Representations Document-Aligned DataKalchbrenner, N., Grefenstette, E., & Blunsom, P. (2014). convolutional neural networkmodelling sentences. Proceedings 52nd Annual Meeting AssociationComputational Linguistics (ACL), pp. 655665.Kazama, J., Saeger, S. D., Kuroda, K., Murata, M., & Torisawa, K. (2010). Bayesianmethod robust estimation distributional similarities. Proceedings 48thAnnual Meeting Association Computational Linguistics (ACL), pp. 247256.Kiela, D., & Bottou, L. (2014). Learning image embeddings using convolutional neuralnetworks improved multi-modal semantics. Proceedings 2014 ConferenceEmpirical Methods Natural Language Processing (EMNLP), pp. 3645.Kiela, D., & Clark, S. (2014). systematic study semantic vector space model parameters.Proceedings 2nd Workshop Continuous Vector Space ModelsCompositionality (CVSC), pp. 2130.Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Torralba, A., Urtasun, R., & Fidler,S. (2015). Skip-thought vectors. Proceedings 28th Annual ConferenceAdvances Neural Information Processing Systems (NIPS).Klementiev, A., Titov, I., & Bhattarai, B. (2012). Inducing crosslingual distributed representations words. Proceedings 24th International Conference Computational Linguistics (COLING), pp. 14591474.Koehn, P. (2005). Europarl: parallel corpus statistical machine translation. Proceedings 10th Machine Translation Summit (MT SUMMIT), pp. 7986.Kocisky, T., Hermann, K. M., & Blunsom, P. (2014). Learning bilingual word representations marginalizing alignments. Proceedings 52nd Annual MeetingAssociation Computational Linguistics (ACL), pp. 224229.Landauer, T. K., & Dumais, S. T. (1997). Solutions Platos problem: Latent Semantic Analysis theory acquisition, induction, representation knowledge.Psychological Review, 104 (2), 211240.Laroche, A., & Langlais, P. (2010). Revisiting context-based projection methods termtranslation spotting comparable corpora. Proceedings 23rd InternationalConference Computational Linguistics (COLING), pp. 617625.Lazaridou, A., Dinu, G., & Baroni, M. (2015). Hubness pollution: Delving crossspace mapping zero-shot learning. ACL, pp. 270280.Le, Q. V., & Mikolov, T. (2014). Distributed representations sentences documents.Proceedings 31th International Conference Machine Learning (ICML),pp. 11881196.Lebret, R., & Collobert, R. (2014). Word embeddings Hellinger PCA. Proceedings14th Conference European Chapter Association ComputationalLinguistics (EACL), pp. 482490.Lee, L. (1999). Measures distributional similarity. Proceedings 37th AnnualMeeting Association Computational Linguistics (ACL), pp. 2532.989fiVulic & MoensLevow, G.-A., Oard, D. W., & Resnik, P. (2005). Dictionary-based techniques crosslanguage information retrieval. Information Processing Management, 41 (3), 523547.Levy, O., & Goldberg, Y. (2014a). Dependency-based word embeddings. Proceedings52nd Annual Meeting Association Computational Linguistics (ACL),pp. 302308.Levy, O., & Goldberg, Y. (2014b). Neural word embedding implicit matrix factorization.Proceedings 27th Annual Conference Advances Neural InformationProcessing Systems (NIPS), pp. 21772185.Levy, O., Goldberg, Y., & Dagan, I. (2015). Improving distributional similarity lessonslearned word embeddings. Transactions ACL, 3, 211225.Li, B., & Gaussier, E. (2010). Improving corpus comparability bilingual lexicon extraction comparable corpora. Proceedings 23rd International ConferenceComputational Linguistics (COLING), pp. 644652.Li, B., Gaussier, E., & Aizawa, A. (2011). Clustering comparable corpora bilinguallexicon extraction. Proceedings 49th Annual Meeting AssociationComputational Linguistics: Human Language Technologies (ACL-HLT), pp. 473478.Liu, Q., Jiang, H., Wei, S., Ling, Z.-H., & Hu, Y. (2015). Learning semantic word embeddings based ordinal knowledge constraints. Proceedings 53rd AnnualMeeting Association Computational Linguistics 7th InternationalJoint Conference Natural Language Processing (ACL-IJCNLP), pp. 15011511.Liu, X., Duh, K., & Matsumoto, Y. (2013). Topic models + word alignment = flexibleframework extracting bilingual dictionary comparable corpus. Proceedings17th Conference Computational Natural Language Learning (CoNLL), pp.212221.Lu, A., Wang, W., Bansal, M., Gimpel, K., & Livescu, K. (2015). Deep multilingual correlation improved word embeddings. Proceedings 2015 ConferenceNorth American Chapter Association Computational Linguistics: HumanLanguage Technologies (NAACL-HLT), pp. 250256.Luong, T., Pham, H., & Manning, C. D. (2015). Bilingual word representations monolingual quality mind. Proceedings 1st Workshop Vector Space ModelingNatural Language Processing, pp. 151159.McNemar, Q. (1947). Note sampling error difference correlatedproportions percentages. Psychometrika, 12 (2), 153157.Melamud, O., Levy, O., & Dagan, I. (2015). simple word embedding model lexicalsubstitution. Proceedings 1st Workshop Vector Space Modeling NaturalLanguage Processing, pp. 17.Mikolov, T., Chen, K., Corrado, G. S., & Dean, J. (2013a). Efficient estimation wordrepresentations vector space. Proceedings 2013 International ConferenceLearning Representations (ICLR): Workshop Papers.990fiBilingual Distributed Word Representations Document-Aligned DataMikolov, T., Le, Q. V., & Sutskever, I. (2013b). Exploiting similarities among languagesmachine translation. CoRR, abs/1309.4168.Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013c). Distributedrepresentations words phrases compositionality. Proceedings27th Annual Conference Advances Neural Information Processing Systems(NIPS), pp. 31113119.Mikolov, T., Yih, W., & Zweig, G. (2013d). Linguistic regularities continuous space wordrepresentations. Proceedings 14th Meeting North American Chapter Association Computational Linguistics: Human Language Technologies(NAACL-HLT), pp. 746751.Milajevs, D., Kartsaklis, D., Sadrzadeh, M., & Purver, M. (2014). Evaluating neural wordrepresentations tensor-based compositional settings. Proceedings 2014Conference Empirical Methods Natural Language Processing (EMNLP), pp.708719.Mimno, D., Wallach, H., Naradowsky, J., Smith, D. A., & McCallum, A. (2009). Polylingualtopic models. Proceedings 2009 Conference Empirical Methods NaturalLanguage Processing (EMNLP), pp. 880889.Mitchell, J., & Lapata, M. (2008). Vector-based models semantic composition. Proceedings 46th Annual Meeting Association Computational Linguistics(ACL), pp. 236244.Mnih, A., & Kavukcuoglu, K. (2013). Learning word embeddings efficiently noisecontrastive estimation. Proceedings 27th Annual Conference AdvancesNeural Information Processing Systems (NIPS), pp. 22652273.Ni, X., Sun, J.-T., Hu, J., & Chen, Z. (2009). Mining multilingual topics Wikipedia.Proceedings 18th International World Wide Web Conference (WWW), pp.11551156.Ni, X., Sun, J.-T., Hu, J., & Chen, Z. (2011). Cross lingual text classification miningmultilingual topics Wikipedia. Proceedings 4th International ConferenceWeb Search Web Data Mining (WSDM), pp. 375384.Pado, S., & Lapata, M. (2009). Cross-lingual annotation projection semantic roles.Journal Artificial Intelligence Research, 36, 307340.Pantel, P., & Lin, D. (2002). Discovering word senses text. Proceedings 8thACM SIGKDD International Conference Knowledge Discovery Data Mining(KDD), pp. 613619.Peirsman, Y., & Pado, S. (2010). Cross-lingual induction selectional preferencesbilingual vector spaces. Proceedings 11th Meeting North AmericanChapter Association Computational Linguistics: Human Language Technologies (NAACL-HLT), pp. 921929.Peirsman, Y., & Pado, S. (2011). Semantic relations bilingual lexicons. ACM TransactionsSpeech Language Processing, 8 (2), article 3.991fiVulic & MoensPennington, J., Socher, R., & Manning, C. (2014). Glove: Global vectors word representation. Proceedings 2014 Conference Empirical Methods NaturalLanguage Processing (EMNLP), pp. 15321543.Pollard, D. (2001). Users Guide Measure Theoretic Probability. Cambridge UniversityPress.Rapp, R. (1999). Automatic identification word translations unrelated EnglishGerman corpora. Proceedings 37th Annual Meeting AssociationComputational Linguistics (ACL), pp. 519526.Reisinger, J., & Mooney, R. J. (2010). mixture model sharing lexical semantics.Proceedings 2010 Conference Empirical Methods Natural LanguageProcessing (EMNLP), pp. 11731182.Rudolph, S., & Giesbrecht, E. (2010). Compositional matrix-space models language.Proceedings 48th Annual Meeting Association Computational Linguistics (ACL), pp. 907916.Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representationsback-propagating errors. Nature, 323, 533536.Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. ProceedingsInternational Conference New Methods Language Processing.Shi, T., Liu, Z., Liu, Y., & Sun, M. (2015). Learning cross-lingual word embeddings viamatrix co-factorization. Proceedings 53rd Annual Meeting AssociationComputational Linguistics 7th International Joint Conference NaturalLanguage Processing (ACL-IJCNLP), pp. 567572.Socher, R., Huval, B., Manning, C. D., & Ng, A. Y. (2012). Semantic compositionalityrecursive matrix-vector spaces. Proceedings 2012 Joint ConferenceEmpirical Methods Natural Language Processing Computational NaturalLanguage Learning (EMNLP-CoNLL), pp. 12011211.Sgaard, A., Agic, v., Martnez Alonso, H., Plank, B., Bohnet, B., & Johannsen, A. (2015).Inverted indexing cross-lingual nlp. Proceedings 53rd Annual MeetingAssociation Computational Linguistics 7th International Joint Conference Natural Language Processing (ACL-IJCNLP), pp. 17131722.Soyer, H., Stenetorp, P., & Aizawa, A. (2015). Leveraging monolingual data crosslingual compositional word representations. Proceedings 2015 InternationalConference Learning Representations (ICLR).Steyvers, M., & Griffiths, T. (2007). Probabilistic topic models. Handbook Latent Semantic Analysis, 427 (7), 424440.Stratos, K., Collins, M., & Hsu, D. (2015). Model-based word embeddings decompositions count matrices. Proceedings 53rd Annual Meeting AssociationComputational Linguistics 7th International Joint Conference NaturalLanguage Processing (ACL-IJCNLP), pp. 12821291.Tackstrom, O., Das, D., Petrov, S., McDonald, R., & Nivre, J. (2013). Token typeconstraints cross-lingual part-of-speech tagging. Transactions ACL, 1, 112.992fiBilingual Distributed Word Representations Document-Aligned DataTamura, A., Watanabe, T., & Sumita, E. (2012). Bilingual lexicon extraction comparable corpora using label propagation. Proceedings 2012 Joint ConferenceEmpirical Methods Natural Language Processing Computational NaturalLanguage Learning (EMNLP-CoNLL), pp. 2436.Tiedemann, J. (2012). Parallel data, tools interfaces OPUS. Proceedings8th International Conference Language Resources Evaluation (LREC), pp.22142218.Tiedemann, J., Agic, Z., & Nivre, J. (2014). Treebank translation cross-lingual parserinduction. Proceedings 18th Conference Computational Natural LanguageLearning (CoNLL), pp. 130140.Trask, A., Gilmore, D., & Russell, M. (2015). Modeling order neural word embeddingsscale. Proceedings 32nd International Conference Machine Learning(ICML), pp. 22662275.Turian, J. P., Ratinov, L., & Bengio, Y. (2010). Word representations: simple generalmethod semi-supervised learning. Proceedings 48th Annual MeetingAssociation Computational Linguistics (ACL), pp. 384394.Turney, P. D., & Pantel, P. (2010). frequency meaning: Vector space modelssemantics. Journal Artifical Intelligence Research, 37 (1), 141188.Vulic, I., De Smet, W., & Moens, M.-F. (2011). Identifying word translations comparable corpora using latent topic models. Proceedings 49th Annual MeetingAssociation Computational Linguistics: Human Language Technologies (ACLHLT), pp. 479484.Vulic, I., De Smet, W., & Moens, M.-F. (2013). Cross-language information retrieval modelsbased latent topic models trained document-aligned comparable corpora.Information Retrieval, 16 (3), 331368.Vulic, I., De Smet, W., Tang, J., & Moens, M. (2015). Probabilistic topic modelingmultilingual settings: overview methodology applications. InformationProcessing Management, 51 (1), 111147.Vulic, I., & Moens, M.-F. (2012). Detecting highly confident word translations comparable corpora without prior knowledge. Proceedings 13th ConferenceEuropean Chapter Association Computational Linguistics (EACL),pp. 449459.Vulic, I., & Moens, M.-F. (2013a). Cross-lingual semantic similarity words similarity semantic word responses. Proceedings 14th MeetingNorth American Chapter Association Computational Linguistics: HumanLanguage Technologies (NAACL-HLT), pp. 106116.Vulic, I., & Moens, M.-F. (2013b). study bootstrapping bilingual vector spacesnon-parallel data (and nothing else). Proceedings 2013 Conference Empirical Methods Natural Language Processing (EMNLP), pp. 16131624.Vulic, I., & Moens, M.-F. (2014). Probabilistic models cross-lingual semantic similarity context based latent cross-lingual concepts induced comparable data.993fiVulic & MoensProceedings 2014 Conference Empirical Methods Natural LanguageProcessing (EMNLP), pp. 349362.Vulic, I., & Moens, M.-F. (2015). Monolingual cross-lingual information retrieval modelsbased (bilingual) word embeddings. Proceedings 38th Annual International ACM SIGIR Conference Research Development Information Retrieval(SIGIR), pp. 363372.Wu, H., Dong, D., Hu, X., Yu, D., He, W., Wu, H., Wang, H., & Liu, T. (2014). Improvestatistical machine translation context-sensitive bilingual semantic embeddingmodel. Proceedings 2014 Conference Empirical Methods Natural Language Processing (EMNLP), pp. 142146.Wu, H., Wang, H., & Zong, C. (2008). Domain adaptation statistical machine translation domain dictionary monolingual corpora. Proceedings 22ndInternational Conference Computational Linguistics (COLING), pp. 9931000.Xiao, M., & Guo, Y. (2014). Distributed word representation learning cross-lingualdependency parsing. Proceedings 18th Conference Computational NaturalLanguage Learning (CoNLL), pp. 119129.Yarowsky, D., & Ngai, G. (2001). Inducing multilingual POS taggers NP bracketersvia robust projection across aligned corpora. Proceedings 2nd MeetingNorth American Chapter Association Computational Linguistics (NAACL),pp. 200207.Zhang, D., Mei, Q., & Zhai, C. (2010). Cross-lingual latent topic extraction. Proceedings48th Annual Meeting Association Computational Linguistics (ACL),pp. 11281137.Zhang, J., Liu, S., Li, M., Zhou, M., & Zong, C. (2014). Bilingually-constrained phraseembeddings machine translation. Proceedings 52nd Annual MeetingAssociation Computational Linguistics (ACL), pp. 111121.Zou, W. Y., Socher, R., Cer, D., & Manning, C. D. (2013). Bilingual word embeddingsphrase-based machine translation. Proceedings 2013 Conference EmpiricalMethods Natural Language Processing (EMNLP), pp. 13931398.994fiJournal Artificial Intelligence Research 55 (2016) 361-387Submitted 04/15; published 02/16Bayesian Optimization Billion Dimensionsvia Random EmbeddingsZiyu Wangziyu.wang@cs.ox.ac.ukDepartment Computer Science, University OxfordFrank Hutterfh@cs.uni-freiburg.deDepartment Computer Science, University FreiburgMasrour Zoghim.zoghi@uva.nlDepartment Computer Science, University AmsterdamDavid Mathesondavidm@cs.ubc.caDepartment Computer Science, University British ColumbiaNando de Freitasnando@cs.ox.ac.ukDepartment Computer Science, University OxfordCanadian Institute Advanced ResearchAbstractBayesian optimization techniques successfully applied robotics, planning,sensor placement, recommendation, advertising, intelligent user interfaces automaticalgorithm configuration. Despite successes, approach restricted problemsmoderate dimension, several workshops Bayesian optimization identifiedscaling high-dimensions one holy grails field. paper, introducenovel random embedding idea attack problem. resulting Random EMbeddingBayesian Optimization (REMBO) algorithm simple, important invariance properties, applies domains categorical continuous variables. presentthorough theoretical analysis REMBO. Empirical results confirm REMBOeffectively solve problems billions dimensions, provided intrinsic dimensionalitylow. also show REMBO achieves state-of-the-art performance optimizing47 discrete parameters popular mixed integer linear programming solver.1. IntroductionLet f : X R function compact subset X RD . address following globaloptimization problemx? = arg max f (x).xXparticularly interested objective functions f may satisfy onefollowing criteria: closed-form expression, expensive evaluate,easily available derivatives, non-convex. treat f blackbox functionallows us query function value arbitrary x X . address objectiveschallenging nature, adopt Bayesian optimization framework.nutshell, order optimize blackbox function f , Bayesian optimization usesprior distribution captures beliefs behavior f , updates priorsequentially acquired data. Specifically, iterates following phases: (1) usec2016AI Access Foundation. rights reserved.fiWang, Hutter, Zoghi, Matheson, & de Freitast=2objective fn (f( ))observation (x)acquisition maxacquisition function (u( ))t=3new observation (xt )t=4posterior mean (( ))posterior uncertainty(( ) ( ))Figure 1: Three consecutive iterations Bayesian optimization toy one-dimensionalproblem. unknown objective function approximated Gaussian process (GP) iteration. figure shows mean confidence intervalsprocess. also shows acquisition function lower green shadedplots. acquisition high GP predicts high objective (exploitation) prediction uncertainty high (exploration). Notearea far left remains under-sampled, (despite high uncertainty)correctly predicted unlikely improve highest observation.prior decide input x X query f next; (2) evaluate f (x); (3) updateprior based new data hx, f (x)i. Step 1 uses so-called acquisition functionquantifies expected value learning value f (x) x X . procedureillustrated Figure 1.362fiBayesian Optimization Billion Dimensionsrole acquisition function trade exploration exploitation; popularchoices include Thompson sampling (Thompson, 1933; Hoffman, Shahriari, & de Freitas,2014), probability improvement (Jones, 2001), expected improvement (Mockus, 1994),upper-confidence-bounds (Srinivas, Krause, Kakade, & Seeger, 2010), online portfolios(Hoffman, Brochu, & de Freitas, 2011). typically optimized choosingpoints predictive mean high (exploitation) variance large(exploration). Since typically analytical expression easy evaluate,much easier optimize original objective function, using off-the-shelfnumerical optimization algorithms.1term Bayesian optimization coined several decades ago Jonas Mockus (1982).popular version method known efficient global optimization experimentaldesign literature since 1990s (Jones, Schonlau, & Welch, 1998). Often, approximation objective function obtained using Gaussian process (GP) priors.reason, technique also referred GP bandits (Srinivas et al., 2010). However,many approximations objective proposed, including Parzen estimators (Bergstra, Bardenet, Bengio, & Kegl, 2011), Bayesian parametric models (Wang& de Freitas, 2011), treed GPs (Gramacy, Lee, & Macready, 2004) random forests(Brochu, Cora, & de Freitas, 2009; Hutter, 2009; Hutter, Hoos, & Leyton-Brown, 2011).may suitable GPs number iterations grows without bound,objective function believed discontinuities. also note oftenassumptions smoothness objective function encoded without useBayesian paradigm, leading similar algorithms theoretical guarantees (see,example, Bubeck, Munos, Stoltz, & Szepesvari, 2011, references therein).rich literature Bayesian optimization, details refer readerstutorial treatments (Brochu et al., 2009; Jones et al., 1998; Jones, 2001; Lizotte, Greiner,& Schuurmans, 2011; Mockus, 1994; Osborne, Garnett, & Roberts, 2009) recent theoretical results (Srinivas et al., 2010; Bull, 2011; de Freitas, Smola, & Zoghi, 2012).Bayesian optimization demonstrated outperform state-of-the-art blackbox optimization techniques function evaluations expensive numberallowed function evaluations therefore low (Hutter, Hoos, & Leyton-Brown, 2013).recent years, found increasing use machine learning community (Rasmussen,2003; Brochu, de Freitas, & Ghosh, 2007; Martinez-Cantin, de Freitas, Doucet, & Castellanos, 2007; Lizotte, Wang, Bowling, & Schuurmans, 2007; Frazier, Powell, & Dayanik,2009; Azimi, Fern, & Fern, 2010; Hamze, Wang, & de Freitas, 2013; Azimi, Fern, & Fern,2011; Hutter et al., 2011; Bergstra et al., 2011; Gramacy & Polson, 2011; Denil, Bazzani,Larochelle, & de Freitas, 2012; Mahendran, Wang, Hamze, & de Freitas, 2012; Azimi, Jalali,& Fern, 2012; Hennig & Schuler, 2012; Marchant & Ramos, 2012; Snoek, Larochelle, &Adams, 2012; Swersky, Snoek, & Adams, 2013; Thornton, Hutter, Hoos, & Leyton-Brown,1. optimization step fact circumvented using treed multi-scale optimistic optimizationrecently demonstrated Wang de Freitas (2014). also exist several involved Bayesiannon-linear experimental design approaches constructing acquisition function, utilityoptimized involves entropy aspect posterior. includes work HennigSchuler (2012) finding maxima functions, works Kueck, de Freitas, Doucet (2006)Kueck, Hoffman, Doucet, de Freitas (2009) learning functions, work Hoffman, Kueck,de Freitas, Doucet (2009) estimating Markov decision processes. works rely expensiveapproximate inference methods computing intractable integrals.363fiWang, Hutter, Zoghi, Matheson, & de Freitas2013). Despite many success stories, approach restricted problems moderatedimension, typically 10. course, great many problemsneeded. However, advance state art, need scale methodologyhigh-dimensional parameter spaces. goal paper.difficult scale Bayesian optimization high dimensions. ensure globaloptimum found, require good coverage X , dimensionality increases,number evaluations needed cover X increases exponentially. result,little progress challenging problem, exceptions. Bergstra et al. (2011) introduced non-standard Bayesian optimization method based tree one-dimensionaldensity estimators applied successfully optimize 238 parameters complex vision architecture (Bergstra, Yamins, & Cox, 2013). Hutter et al. (2011) used random forestsmodels Bayesian optimization achieve state-of-the-art performance optimizing76 mixed discrete/continuous parameters algorithms solving hard combinatorialproblems, successfully carry combined model selection hyperparameter optimization 768 parameters Auto-WEKA framework (Thornton et al., 2013).Eggensperger, Feurer, Hutter, Bergstra, Snoek, Hoos, Leyton-Brown (2013) showedtwo methods indeed yielded best performance high-dimensional hyperparameter optimization (e.g., deep belief networks). However, based weakuncertainty estimates fail even optimization simple functionslack theoretical guarantees.linear bandits case, Carpentier Munos (2012) recently proposed compressedsensing strategy attack problems high degree sparsity. Also recently, Chen,Castro, Krause (2012) made significant progress introducing two stage strategyoptimization variable selection high-dimensional GPs. first stage, sequentiallikelihood ratio tests, couple tuning parameters, used select relevantdimensions. This, however, requires relevant dimensions axis-alignedARD kernel. Chen colleagues provide empirical results synthetic examples (of400 dimensions), provide key theoretical guarantees.Many researchers noted certain classes problems dimensionschange objective function significantly; examples include hyper-parameter optimizationneural networks deep belief networks (Bergstra & Bengio, 2012), wellmachine learning algorithms various state-of-the-art algorithms solving N P-hardproblems (Hutter, Hoos, & Leyton-Brown, 2014). say problems loweffective dimensionality. take advantage property, Bergstra Bengio (2012)proposed simply use random search optimization rationale pointssampled uniformly random dimension densely cover low-dimensionalsubspace. such, random search exploit low effective dimensionality without knowingdimensions important. paper, exploit property newBayesian optimization variant based random embeddings.Figure 2 illustrates idea behind random embeddings nutshell. Assume knowgiven = 2 dimensional black-box function f (x1 , x2 ) = 1 importantdimensions, know two dimensions important one.perform optimization embedded 1-dimensional subspace defined x1 = x2since guaranteed include optimum.364fiBayesian Optimization Billion Dimensionsx1x1x*gddEmx2Importantx*Unimportantx2Figure 2: function D=2 dimesions d=1 effective dimension: verticalaxis indicated word important right hand side figure. Hence,1-dimensional embedding includes 2-dimensional functions optimizer.efficient search optimum along 1-dimensional randomembedding original 2-dimensional space.first demonstrated recent IJCAI conference paper (Wang, Zoghi, Hutter,Matheson, & de Freitas, 2013), random embeddings enable us scale Bayesian optimizationarbitrary provided objective function low intrinsic dimensionality. Importantly,algorithm associated idea, called REMBO, restricted casesaxis-aligned intrinsic dimensions applies d-dimensional linear subspace.Djolonga, Krause, Cevher (2013) recently proposed adaptive, expensive,variant REMBO theoretical guarantees.journal version work, expand presentation provide details throughout. particular, expand description strategy selectingboundaries low-dimensional space setting kernel length scale parameter;show means additional application (automatic configuration random forestbody-part classifiers) performance technique collapseproblem obvious low effective dimensionality. experiments (Section4) also show REMBO solve problems previously untenable high extrinsic dimensions, REMBO achieve state-of-the-art performance optimizing 47discrete parameters popular mixed integer linear programming solver.2. Bayesian Optimizationmentioned introduction, Bayesian optimization two ingredients needspecified: prior acquisition function. work, adopt GP priors.review GPs briefly refer interested reader book RasmussenWilliams (2006). GP distribution functions specified mean function m()365fiWang, Hutter, Zoghi, Matheson, & de Freitascovariance k(, ). specifically, given set points x1:t , xi RD ,f (x1:t ) N (m(x1:t ), K(x1:t , x1:t )),K(x1:t , x1:t )i,j = k(xi , xj ) serves covariance matrix. common choice ksquared exponential function (see Definition 7 page 371), many choicespossible depending degree belief smoothness objective function.advantage using GPs lies analytical tractability. particular, givenobservations x1:t corresponding values f1:t , fi = f (xi ), new point x ,joint distribution given by:f1:tm(x1:t )K(x1:t , x1:t ) k(x1:t , x )N,.fk(x , x1:t )k(x , x )simplicity, assume m(x1:t ) = 0 = 0. Using Sherman-MorrisonWoodbury formula, one easily arrive posterior predictive distribution:f |Dt , x N ((x |Dt ), (x |Dt )),data Dt = {x1:t , f1:t }, mean variance(x |Dt ) = k(x , x1:t )K(x1:t , x1:t )1 f1:t(x |Dt ) = k(x , x ) k(x , x1:t )K(x1:t , x1:t )1 k(x1:t , x ).is, compute posterior predictive mean () variance () exactlypoint x .iteration Bayesian optimization, one re-compute predictive meanvariance. two quantities used construct second ingredient Bayesianoptimization: acquisition function. work, report results expectedimprovement acquisition function (Mockus, 1982; Vazquez & Bect, 2010; Bull, 2011):u(x|Dt ) = E(max{0, ft+1 (x) f (x+ )}|Dt ).definition, x+ = arg maxx{x1:t } f (x) element best objective valuefirst steps optimization process. next query is:xt+1 = arg max u(x|Dt ).xXNote utility favors selection points high variance (points regionswell explored) points high mean value (points worth exploiting). alsoexperimented UCB acquisition function (Srinivas et al., 2010; de Freitas et al.,2012) found yield similar results. optimization closed-form acquisitionfunction carried off-the-shelf numerical optimization procedures, DIRECT (Jones, Perttunen, & Stuckman, 1993) CMA-ES (Hansen & Ostermeier, 2001);based GP model blackbox function f require additionalevaluations f .Bayesian optimization procedure shown Algorithm 1.366fiBayesian Optimization Billion DimensionsAlgorithm 1 Bayesian Optimization1: Initialize D0 .2: = 1, 2, . . .3:Find xt+1 RD optimizing acquisition function u: xt+1 = arg maxxX u(x|Dt ).4:Augment data Dt+1 = Dt {(xt+1 , f (xt+1 ))}.5:Update kernel hyper-parameters.6: end3. Random Embedding Bayesian Optimizationintroducing new algorithm theoretical properties, need definemean effective dimensionality formally.Definition 1. function f : RD R said effective dimensionality de ,de D,exists linear subspace dimension de x> RDx RD , f (x> + x ) = f (x> ), denotes orthogonalcomplement ;de smallest integer property.call effective subspace f constant subspace.definition simply states function change along coordinatesx , refer constant subspace. Given definition,following theorem shows problems low effective dimensionality solved viarandom embedding.Theorem 2. Assume given function f : RD R effective dimensionalityde random matrix RDd independent entries sampled according N (0, 1)de . Then, probability 1, x RD , exists Rdf (x) = f (Ay).Proof. Please refer appendix.Theorem 2 says given x RD random matrix RDd , probability1, point Rd f (x) = f (Ay). implies optimizer x?RD , point y? Rd f (x? ) = f (Ay? ). Therefore, instead optimizinghigh dimensional space, optimize function g(y) = f (Ay) lower dimensionalspace. observation gives rise new Random EMbedding Bayesian Optimization(REMBO) algorithm (see Algorithm 2). REMBO first draws random embedding (givenA) performs Bayesian optimization embedded space.many practical optimization tasks, goal optimize f compact subsetX RD (typically box), f often evaluated outside X . Therefore,REMBO selects point Ay outside box X , projects Ay onto Xevaluating f . is, g(y) = f (pX (Ay)), pX : RD RD standard projectionoperator box-constraint: pX (y) = arg minzX kz yk2 ; see Figure 3. still needdescribe REMBO chooses bounded region Rd , inside performs367fiWang, Hutter, Zoghi, Matheson, & de Freitasd=1Algorithm 2 REMBO: Bayesian Optimization Random Embedding. Blue text denotes parts changed compared standard Bayesian Optimization.1: Generate random matrix RDd2: Choose bounded region set Rd3: Initialize D0 .4: = 1, 2, . . .5:Find yt+1 Rd optimizing acquisition function u: yt+1 = arg maxyY u(y|Dt ).6:Augment data Dt+1 = Dt {(yt+1 , f (Ayt+1 ))}.7:Update kernel hyper-parameters.8: endD=2Convex projectiondingEmFigure 3: Embedding = 1 = 2. box illustrates 2D constrained spaceX , thicker red line illustrates 1D constrained space Y. NoteAy outside X , projected onto X . set must chosen large enoughprojection image, AY, onto effective subspace (vertical axisdiagram) covers vertical side box.Bayesian optimization. important REMBOs effectiveness dependssize Y. Locating optimum within easier small, set smallmay actually contain global optimizer. following theorem, showchoose way depends effective dimensionality deoptimizer original problem contained low dimensional space constantprobability.Theorem 3. Suppose want optimize function f : RD R effective dimensionde subject box constraint X RD , X centered around 0. Supposeeffective subspace f span de basis vectors,let x?> X optimizer f inside . random matrixindependent standard Gaussianentries, exists optimizer y? Rdde????f (Ay ) = f (x> ) ky k2 kx> k2 probability least 1 .Proof. Please refer appendix.368fiBayesian Optimization Billion DimensionsTheorem 3 says set X original space box constraint,exists anoptimizer x?> X de -sparse probability least 1 ,ky? k2 de kx?> k2 f (Ay? ) = f (x?> ). box constraint X = [1, 1]D (whichalways achievable rescaling), probability least 1de ?de p?de .ky k2kx> k2Hence, choose Y, must ensure ball radius de /, centred origin, liesinside Y.practice, foundunlikely optimizer falls corner?box constraint, implying kx> k < de . Thus setting big may unnecessarilywasteful. improve understanding effect, developed simulation study,drew random Gaussian matrices, used map various potential optimizers? Y, studied norms y? .x?> corresponding points y>>Assume simplicity presentation axis-aligned de -dimensional (theargument applies > de ). section random matrix maps pointsrandom Gaussian matrix dimension de de . Let us call sectionmatrix B. Since random Gaussian matrices rotationally invariant distribution,orthonormal matrix random Gaussian matrix B, OB = B.1is, OB B equal distribution. Similarly, B1 , OB1 = BOT= B1 .Therefore, B1 also rotationally invariant. Hence, kB1 x> k = kB1 x0> k longkx> k2 = kx0> k2 . Following equivalence supremum norm projected vectors,suffices choose point largest norm [1, 1]de simulations. chosex> = [1, 1, , 1].conducted simulations several embedding dimensions, de {1, 2, , 50},drawing 10000 random Gaussian matrices computing kB1 xk . foundempirical probability 1 (for decreasing values ), case1max{log(de ), 1}.simulations indicate could set = 1 max{log(de ), 1}, 1 max{log(de ), 1} e .Wedid experiments particular chose = log(d)/ d,[ d, d]d . Note Theorem 3 useful choice, suggestsroom improve aspect theory.careful readers may wonder effect extrinsic dimensionality D.following theorem, show given intrinsic dimensions, extrinsicdimensionality effect all; words, REMBO invariantaddition unimportant dimensions.kB1 xk <Theorem 4 (Invariance addition unimportant dimensions). Let f : Rde RN, de , define fD : RD R fD adds de truly unimportant(D2 D1 )d randomdimensions f : fD (z) = f (z1:de ). Let A1 RD1A0 RA1Gaussian matrices D2 D1 let A2 =. Then, REMBO run usingA0369fiWang, Hutter, Zoghi, Matheson, & de Freitasdimension de bounded region yields exactly function valuesrun A1 fD1 run A2 fD2 .Proof. need show Rd , fD1 (A1 y) = fD2 (A2 y) sincestep REMBO (line 6 Algorithm 2) one differs twoalgorithm runs. function evaluation step yields results every Rd ,two REMBO runs behave identically since algorithmotherwise identicalA1A1deterministic selection Step 1. Since A2 =, A2 =.A0A0Since D2 D1 de , first de entries D2 1 vector A2 first de entriesA1 y. thus fD1 (A1 y) = f ([A1 y]1:de ) = f ([A2 y]1:de ) = fD2 (A2 y).Finally, show REMBO also invariant rotations sense given different rotation matrices, running REMBO would result distributions observedfunction values. argument made concise following results.Lemma 5. Consider function f : RD R. Let fR : RD R fR (x) = f (Rx)orthonormal matrix R RDD . Then, REMBO run bounded region yieldsexactly sequence function values run f run R1fR matrix RDd .Proof. REMBO uses f (resp. fR R1 A) one spot (in line 6). Thus,proof trivial showing f (Ayt+1 ) = fR (R1 Ayt+1 ) simple algebra:fR (R1 Ayt+1 ) = f (RR1 Ayt+1 ) = f (Ayt+1 ).Theorem 6 (Invariance rotations). Consider function f : RD R. Let fR : RD RfR (x) = f (Rx) orthonormal matrix R RDD . Then, given randomGaussian matrices A1 RDd A2 RDd , REMBO run bounded region yieldsdistribution sequence function values run A1 f runA2 fR .Proof. Since R orthonormal, R1 A1 = A2 . Therefore, REMBO run boundedregion yields distribution sequence function values run R1 A1fR run A2 fR . also Lemma 5 REMBO run boundedregion yields exactly sequence function values run A1 frun R1 A1 fR . conclusion follows combining previous arguments.3.1 Increasing Success Rate REMBOTheorem 3 guarantees contains optimum probability least 1;probability optimizer lies outside Y. several ways guardproblem. One simply run REMBO multiple times different independentlydrawn random embeddings. Since probability failure embedding ,probability optimizer included considered space k independentlydrawn embeddings k . Thus, failure probability vanishes exponentially quickly370fiBayesian Optimization Billion Dimensionsnumber REMBO runs, k. Note also independent runs triviallyparallelized harness power modern multi-core machines large compute clusters.Another way increasing REMBOs success rate increasedimensionalityuses internally. > de , probability 1 dde different embeddingsdimensionality de . is, need select de columns RDd representde relevant dimensions x. algorithm achieve setting remainingde sub-components d-dimensional vector zero. Informally, sinceembeddings, likely one include optimizer.experiments, assess merits shortcomings two strategies.3.2 Choice KernelSince REMBO uses GP-based Bayesian optimization search region Rd ,need define kernel two points y(1) , y(2) Y. begin standarddefinition squared exponential kernel:Definition 7. Let KSE (y) = exp(kyk2 /2). Given length scale ` > 0, definecorresponding squared exponential kernel!y(1) y(2)(2)(1)k` (y , ) = KSE`possible work two variants kernel. First, use k`d (y1 , y2 )Definition 7. refer kernel low-dimensional kernel. also adoptimplicitly defined high-dimensional kernel X :!(1) ) p (Ay(2) )p(AyXXk`D (y(1) , y(2) ) = KSE,`pX : RD RD projection operator box-constraint (seeFigure 3).Note using high-dimensional kernel, fitting GP dimensions. However, search space longer box X , instead given muchsmaller subspace {pX (Ay) : Y}. Importantly, practice easier maximizeacquisition function subspace.kernel choices strengths weaknesses. low-dimensional kernelbenefit requiring construction GP space intrinsic dimensionalityd, whereas high-dimensional kernel requires GP constructed spaceextrinsic dimensionality D. However, low-dimensional kernel may waste time exploringregion embedding outside X (see Figure 2) two points far apartregion may projected via pX nearby points boundary X . highdimensional kernel affected problem search conducted directly{pX (Ay) : Y} distances calculated X Y.choice kernel also depends whether variables continuous, integercategorical. categorical case important often encounter optimization371fiWang, Hutter, Zoghi, Matheson, & de FreitasAlgorithm 3 Bayesian Optimization Hyper-parameter Optimization.input Threshold .input Upper lower bounds U > L > 0 hyper-parameter.input Initial length scale hyper-parameter ` [L, U ].1: Initialize C = 02: = 1, 2, . . .3:Findp xt+1 optimizing acquisition function u: xt+1 = arg maxxX u(x|Dt ).4:2 (xt+1 ) <5:C =C +16:else7:C=08:end9:Augment data Dt+1 = {Dt , (xt+1 , f (xt+1 ))}10:mod 20 = 0 C = 511:C = 512:U = max{0.9`, L}13:C=014:end15:Learn hyper-parameter optimizing log marginal likelihood usingDIRECT CMA-ES: ` = arg maxl[L,U ] log p(f1:t+1 |x1:t+1 , l)16:end17: endproblems contain discrete choices. define kernel categorical variables as:(1)(2)(1)(2) 2k (y , ) = exp h(s(Ay ), s(Ay )) ,2y(1) , y(2) Rd , function maps continuous d-dimensional vectors discreteD-dimensional vectors, h defines distance two discrete vectors.detail, s(x) first uses pX project x x [1, 1]D . dimension xi x,maps xi discrete value scaling rounding. experiments, following Hutter(1)(2)(2009), defined h(x(1) , x(2) ) = |{i : xi 6= xi }| impose artificial orderingvalues categorical parameters. essence, measure distancetwo points low-dimensional space Hamming distance mappingshigh-dimensional space.3.3 Hyper-parameter OptimizationBayesian optimization (and therefore REMBO), difficult manually estimatetrue length scale hyper-parameter problem hand. avoid manual stepsachieve robust performance across diverse sets objective functions, paperadopted adaptive hyper-parameter optimization scheme. length scale GPs oftenset maximizing marginal likelihood (Rasmussen & Williams, 2006; Jones et al., 1998).However, demonstrated Bull (2011), approach, implemented naively, mayguarantee convergence. true approaches maximize marginal372fiBayesian Optimization Billion Dimensionslikelihood, also approaches rely Monte Carlo sampling posteriordistribution (Brochu, Brochu, & de Freitas, 2010; Snoek et al., 2012) numberdata small, unless prior informative.Here, propose optimize length scale parameter ` maximizing marginallikelihood subject upper bound U decreased algorithm startsexploiting much. Full details given Algorithm 3. say algorithmp exploiting standard deviation maximizer acquisition function(xt+1 ) less threshold 5 consecutive iterations. Intuitively, meansalgorithm emphasize exploration (searching new parts space,predictive uncertainty high) 5 consecutive iterations. criterionmet, algorithm decreases upper bound U multiplicatively re-optimizeshyper-parameter subject new bound. Even criterion met hyperparameter re-optimized every 20 iterations. optimization acquisitionfunction, algorithm runs DIRECT (Jones et al., 1993) CMA-ES (Hansen &Ostermeier, 2001) uses result best two options. astute reader maywonder difficulty optimizing acquisition functions. REMBO, however,found optimization acquisition function problem sinceneed optimize low-dimensional space acquisition function evaluationscheap, allowing us tens thousands evaluations seconds (empirically) sufficecover low-dimensional space well.motivation algorithm rather err side small lengthscale: given squared exponential kernel k` , smaller length scale another kernelk, one show function f RKHS characterized k also elementRKHS characterized k` . Thus, running expected improvement, one safely usek` instead k kernel GP still preserve convergence (Bull, 2011). argue(with small enough lower bound L) algorithm would eventually reduce upperbound enough allow convergence. Also, algorithm would explore indefinitelyL required positive. experiments, set initial constraint [L, U ][0.01, 50] set = 0.002.want stress fact argument known hold classkernels continuous domains (e.g. squared exponential Matern class kernels).Although believe similar argument could made integer categoricalkernels, rigorous arguments concerning convergence kernels remain challengeBayesian optimization.4. Experimentsstudy REMBO empirically. first use synthetic functions small intrinsic dimensionality de = 2 extrinsic dimension 1 billion demonstrate REMBOsindependence D. Then, apply REMBO automatically optimize 47 parameters widely-used mixed integer linear programming solver demonstrateachieves state-of-the-art performance. However, also warn blind applicationREMBO. illustrate this, study REMBOs performance tuning 14 parameters random forest body part classifier used Kinect. application,= 14 parameters appear important, REMBO (based = 3) finds373fiWang, Hutter, Zoghi, Matheson, & de Freitasreasonable solutions (better random search comparable domain expertsachieve), standard Bayesian optimization outperform REMBO (and domain experts) moderate-dimensional spaces. optimistically, random forest tuningapplication shows REMBO fail catastrophically clearoptimization problem low effective dimensionality.4.1 Experimental Setupexperiments, used single robust version REMBO automatically setsGPs length scale parameter described Section 3.3. code REMBO, welldata used experiments publicly available https://github.com/ziyuw/rembo.experiments required substantial computational resources, computational expense experiment depending mostly cost evaluatingrespective black-box function. synthetic experiments Section 4.2 requiredminutes run method, optimizing mixed integer programming solverSection 4.4 required 4-5 hours per run, optimizing random forest classifierSection 4.5 required 4-5 days per run. total, used half year CPU timeexperiments paper. first two experiments, study effecttwo methods increasing REMBOs success rate (see Section 3.1) running differentnumbers independent REMBO runs different settings internal dimensionalityd.4.2 Bayesian Optimization Billion Dimensionsexperiments section employ standard de = 2-dimensional benchmark functionBayesian optimization, embedded D-dimensional space. is, add 2additional dimensions affect function all. precisely, functionwhose optimum seek f (x1:D ) = g(xi , xj ), g Branin functiong(x1 , x2 ) = (x25.1 2 51x1 + x1 6)2 + 10(1) cos(x1 ) + 10248j selected using random permutation. measure performance optimization method, used optimality gap: difference bestfunction value found optimal function value.evaluate REMBO using fixed budget 500 function evaluations spreadacross multiple interleaved runs example, using k = 4 interleaved REMBO runs,allowed 125 function evaluations. study choices kconsidering several combinations values. results Table 1 demonstrateinterleaved runs helped improve REMBOs performance. note 13/50 REMBOruns, global optimum indeed contained box REMBO searched= 2; reason poor mean performance REMBO = 2 k = 1.However, remaining 37 runs performed well, REMBO thus performed wellusing multiple interleaved runs: failure rate 13/50=0.26 per independentrun, failure rate using k = 4 interleaved runs 0.264 0.005. One could easilyachieve arbitrarily small failure rate using many independent parallel runs. Usinglarger also effective increasing probability optimizer falling REMBOs374fiBayesian Optimization Billion DimensionsFigure 4: Comparison random search (RANDOM), Bayesian optimization (BO), methodChen et al. (2012) (HD BO), REMBO. Left: = 25 extrinsic dimensions;Right: = 25, rotated objective function; Bottom: = 109 extrinsicdimensions. plot means 1/4 standard deviation confidence intervalsoptimality gap across 50 trials.box time slows REMBOs convergence (such interleavingseveral short runs loses effectiveness).Next, compared REMBO standard Bayesian optimization (BO) randomsearch, extrinsic dimensionality = 25. Standard BO well known performwell low dimensions, degrade tipping point 15-20 dimensions.results = 25 (see Figure 4, left) confirm BO performed rather poorlycritical dimensionality (merely tying random search). REMBO,hand, still performed well 25 dimensions.One important advantage REMBO contrast approach Chenet al. (2012) require effective dimension coordinate aligned.demonstrate fact empirically, rotated embedded Branin function orthogonal rotation matrix R RDD . is, replaced f (x) f (Rx). Figure 4 (middle)shows REMBOs performance affected rotation.Finally, since REMBO independent extrinsic dimensionality longintrinsic dimensionality de small, performed well = 1 000 000 000 dimensions375fiWang, Hutter, Zoghi, Matheson, & de Freitask105421d=20.0022 0.00350.0004 0.00110.0001 0.00030.1514 0.91540.7406 1.8996d=40.1553 0.16010.0908 0.12520.0654 0.08770.0309 0.06870.0143 0.0406d=60.4865 0.47690.2586 0.37020.3379 0.31700.1643 0.18770.1137 0.1202Table 1: Optimality gap de = 2-dimensional Branin function embedded = 25dimensions, REMBO variants using total 500 function evaluations.variants differed internal dimensionality number interleavedruns k (each run allowed 500/k function evaluations). showmean standard deviations optimality gap achieved 500 functionevaluations.(see Figure 4, right). best knowledge, existing methodrun high dimensionality random search.reference, also evaluated method Chen et al. (2012) functions,confirming handle rotation gracefully: performed best nonrotated case = 25, performed worst rotated case. could usedefficiently = 1, 000. Based Mann-Whitney U test Bonferronimultiple-test correction, performance differences statistically significant, exceptRandom vs. standard BO. Finally, comparing REMBO method Chen et al. (2012),also note REMBO much simpler implement results reliable(with interleaved runs).4.3 Synthetic Discrete Experimentsection, test high-dimensional kernel synthetic experiment. Specifically,optimize Branin function, restrict domain 225 discrete pointsregular grid. above, added 23 additional irrelevant dimensions make problem25-dimensional total.used small fixed budget 100 function evaluations algorithms involvedproblem would require 225 evaluations solved completely.used k = 4 interleaved runs REMBO. compare REMBO random searchstandard BO. REMBO, use high-dimensional kernel handle discretenature problem. result comparison summarized Figure 5. StandardBO suffered high extrinsic dimensionality performed slightly worserandom search. REMBO, hand, performed well setting.4.4 Automatic Configuration Mixed Integer Linear Programming SolverState-of-the-art algorithms solving hard computational problems tend parameterizeseveral design choices order allow customization algorithm new problem domains. Automated methods algorithm configuration recently demonstratedsubstantial performance gains state-of-the-art algorithms achieved fully376fiBayesian Optimization Billion DimensionsFigure 5: Comparison random search (RANDOM), Bayesian optimization (BO),REMBO. = 25 extrinsic dimensions. plot means 1/4 standard deviation confidence intervals optimality gap across 50 trials.automated fashion (Mockus, Mockus, & Mockus, 1999; Hutter, Hoos, Leyton-Brown, &Stutzle, 2009; Hutter, Hoos, & Leyton-Brown, 2010; Vallati, Fawcett, Gerevini, Hoos, &Saetti, 2011; Bergstra et al., 2011; Wang & de Freitas, 2011). successes ledparadigm shift algorithm development towards active design highly parameterized frameworks automatically customized particular problem domainsusing optimization (Hoos, 2012; Bergstra et al., 2013; Thornton et al., 2013). resulting algorithm configuration problems shown low dimensionality (Hutteret al., 2014), here, demonstrate REMBO exploit low dimensionalityeven discrete spaces typically encountered algorithm configuration. use configuration problem obtained Hutter et al. (2010), aiming configure 40 binary7 categorical parameters lpsolve (Berkelaar, Eikland, & Notebaert, 2016) , popularmixed integer programming (MIP) solver downloaded 40 000 timeslast year. objective minimize optimality gap lpsolve obtain timelimit five seconds MIP encoding wildlife corridor problem computationalsustainability (Gomes, van Hoeve, & Sabharwal, 2008). Algorithm configuration usuallyaims improve performance representative set problem instances, effectivemethods need solve two orthogonal problems: searching parameter space effectivelydeciding many instances use evaluation (to trade computational overhead over-fitting). contribution first problems; focuseffectively different methods search parameter space, consider configurationsingle problem instance.Due discrete nature optimization problem, could apply REMBOusing high-dimensional kernel categorical variables kD (y(1) , y(2) ) described Section 3.2. proven theoretical guarantees discrete optimization377fiWang, Hutter, Zoghi, Matheson, & de FreitasFigure 6: Performance various methods configuration lpsolve; show optimality gap lpsolve achieved configurations found various methods (lower better). Left: single run method; Right: performancek = 4 interleaved runs.problems, REMBO appears effectively exploit low effective dimensionality leastparticular optimization problem.Figure 6 (left) compares BO, REMBO, baseline random search ParamILS(Hutter et al., 2009) SMAC (Hutter et al., 2011). ParamILS SMAC specificallydesigned configuration algorithms many discrete parameters definecurrent state art problem. Nevertheless, SMAC vanilla REMBOmethod performed best. Based Mann-Whitney U test Bonferroni multiple-testcorrection, yielded statistically significantly better results Randomstandard BO; performance differences significant. figure showsREMBO = 5 avoid clutter, optimize parameter;value tried (d = 3) resulted indistinguishable .synthetic experiment, REMBOs performance could improvedusing multiple interleaved runs. However, shown Hutter, Hoos, Leyton-Brown(2012), multiple independent runs also improve performance SMAC especiallyParamILS. Thus, fair, re-evaluated approaches using interleaved runs. Figure6 (right) shows ParamILS REMBO benefitted interleaving k = 4 runs.However, statistical test results change, still showing SMAC REMBOoutperformed Random BO, significant performance differences.4.5 Automatic Configuration Random Forest Kinect Body Part Classifierevaluate REMBOs performance optimizing 14 parameters randomforest body part classifier. classifier closely follows proprietary system usedMicrosoft Kinect (Shotton, Fitzgibbon, Cook, Sharp, Finocchio, Moore, Kipman, & Blake,2011) available https://github.com/david-matheson/rftk.378fiBayesian Optimization Billion DimensionsFigure 7: Left: ground truth depth, ground truth body parts predicted body parts;Right: features specified offsets u v.begin describing details dataset classifier order buildintuition objective function parameters optimized. dataused consists pairs depth images ground truth body part labels. Specifically,used 1 500 pairs 320x240 resolution depth body part images,synthesized random pose CMU mocap dataset. Depth, ground truth bodyparts predicted body parts (as predicted classifier described below) visualizedone pose Figure 7 (left). 19 body parts plus one background class.20 possible labels, training data contained 25 000 pixels, randomly selected500 training images. validation test data contained pixels 500 validationtest images, respectively.random forest classifier applied one pixel P time. nodedecision trees, computes depth difference two pixels described offsetsP compares threshold. training time, many possible pairs offsetsgenerated random, pair yielding highest information gain trainingdata points selected. Figure 7 (right) visualizes potential feature pixelgreen box: computes depth difference pixels red box whitebox, specified respective offsets u v. training time, u v drawn twoindependent 2-dimensional Gaussian distributions, parameterizedtwo mean parameters 1 2 three covariance terms 11 , 12 , 22 (21 = 12symmetry). constitute 10 parameters need optimized,range [-50,50] mean components [1, 200] covariance terms. Lowcovariance terms yield local features, high terms yield global features. Nextten parameters, random forest classifier four standard parameters, outlinedTable 2. well known computer vision many parameters describedimportant. Much research devoted identifying best values, resultsdataset specific, without definitive general answers.objective optimizing RF classifier parameters find parameter settinglearns best classifier given time budget five minutes. enable competitiveperformance short amount time, node tree random subset379fiWang, Hutter, Zoghi, Matheson, & de FreitasTable 2: Parameter ranges random forest classifier. purpose optimization,maximum tree depth number potential offsets transformedlog space.ParameterRangeMax. tree depthMin. No. samples non leaf nodesNo. potential offsets evaluateBootstrap per tree sampling[1 60][1 100][1 5000][T F]data points considered. Also note parameters include numbertrees random forest; since performance improves monotonically , createdmany trees possible time budget. Trees constructed depth first returnedcurrent state time budget exceeded. Using fixed budget resultssubtle optimization problem complex interactions variousparameters (maximum depth, number potential offsets, number trees accuracy).unclear priori whether low-dimensional subspace 14 interacting parameters exists captures classification accuracy resulting random forests.performed large-scale computational experiments REMBO, random search,standard Bayesian optimization (BO) study question. experiment, usedhigh-dimensional kernel REMBO avoid potential over-exploration problemslow-dimensional kernel described Section 3.2. believed = 14 dimensionswould small enough avoid inefficiencies fitting GP dimensions. beliefconfirmed observation standard BO (which operates = 14 dimensions)performed well problem.Figure 8 (left) shows results obtained single run random search,BO, REMBO. Remarkably, REMBO clearly outperformed random search, even based= 3 dimensions.2 However, since extrinsic dimensionalitymoderate = 14, standard Bayesian optimization performed well, sincelimited low-dimensional subspace outperformed REMBO. Nevertheless, severalREMBO runs actually performed well, comparably best runs BO. Consequently, running k = 4 interleaved runs method, REMBO performed almostwell BO, matching performance 450 function evaluations (see Figure8, right).conclude parameter space RF classifier appearclear low effective dimensionality; since extrinsic dimensionality moderate,leads REMBO perform somewhat worse standard Bayesian optimization,still possible achieve reasonable performance based little = 3 dimensions.2. Due large computational expense experiment (in total half year CPU time),performed conclusive experiments = 3; preliminary runs REMBO = 4 performedsomewhat worse = 3 budget 200 function evaluations, still improvingpoint.380fiBayesian Optimization Billion DimensionsFigure 8: Performance various methods optimizing RF parameters body partclassification. methods, show RF accuracy (mean 1/4 standarddeviation across 10 runs) 2.2 million non background pixels 500pose validation set, using RF parameters identified method.results test set within 1% results validation set. Left:performance single run method; Right: performance k = 4interleaved runs.5. Conclusiondemonstrated possible use random embeddings Bayesian optimization optimize functions extremely high extrinsic dimensionality providedlow intrinsic dimensionality de . Moreover, resulting REMBO algorithmcoordinate independent requires simple modification original Bayesianoptimization algorithm; namely multiplication random matrix. proved REMBOsindependence theoretically empirically validated optimizing low-dimensionalfunctions embedded previously untenable extrinsic dimensionalities 1 billion.also theoretically empirically showed REMBOs rotational invariance. Finally,demonstrated REMBO achieves state-of-the-art performance optimizing 47 discrete parameters popular mixed integer programming solver, thereby providingevidence observation (already put forward Bergstra, Hutter colleagues) that,many problems great practical interest, number important dimensions indeedappears much lower extrinsic dimensionality.note central idea work using otherwise unmodified optimization procedure randomly embedded space principle could applied arbitraryoptimization procedures. Evaluating effciency technique proceduresinteresting topic future work.381fiWang, Hutter, Zoghi, Matheson, & de FreitasAcknowledgementsthank Christof Schotz proofreading draft article. Frank Hutter gratefullyacknowledges funding German Research Foundation (DFG) Emmy Noethergrant HU 1900/2-1.Appendix A. Proof Theorem 2Proof. Since f effective dimensionality de , exists effective subspace RD ,rank(T ) = de . Furthermore, x RD decomposes x = x> + x ,x> x . Hence, f (x) = f (x> + x ) = f (x> ). Therefore, without lossgenerality, suffice show x> , exists Rdf (x> ) = f (Ay).Let RDde matrix, whose columns form orthonormal basis . Hence,x> , exists c Rde x> = c. Let us assumerank de . rank de , exists (T A)y = c.orthogonal projection Ay onto givenAy = c = x> .Thus Ay = x> + x0 x0 since x> projection Ay onto . Consequently,f (Ay) = f (x> + x0 ) = f (x> ).remains show that, probability one, matrix rank de . LetAe RDde submatrix consisting de columns A, i.i.d. samples distributed according N (0, I). Then, ai i.i.d. samples N (0, ) =2N (0de , Ide de ), Ae , considered element Rde , sample2N (0d2e , Id2e d2e ). hand, set singular matrices Rde Lebesguemeasure zero, since zero set polynomial (i.e. determinant function)polynomial functions Lebesgue measurable. Moreover, Normal distribution absolutely continuous respect Lebesgue measure, matrix Ae almostsurely non-singular, means rank de true A, whosecolumns contain columns Ae .Appendix B. Proof Theorem 3Proof. Since X box constraint, projecting x? get x?> X . Also, sincex? = x?> + x x , f (x? ) = f (x?> ). Hence, x?> optimizer.using argument appeared Proposition 1, easy see probability1 x Rd Ay = x + x x . Let matrix whosecolumns form standard basis . Without loss generality, assume= de0Then, shown Proposition 2, exists y? Rd Ay? = x?> . Notecolumn A,Ide 0ai N 0,.0 0382fiBayesian Optimization Billion DimensionsTherefore Ay? = x?> equivalent By? = x?> B Rde de random matrixindependent standard Gaussian entries x?> vector contains first deentries x?> (the rest 0s). Theorem 3.4 (Sankar, Spielman, & Teng, 2003),de1P kB k2.Thus, probability least 1, ky? k kB1 k2 kx?> k2 = kB1 k2 kx?> k2de?kx> k2 .ReferencesAzimi, J., Fern, A., & Fern, X. (2010). Batch Bayesian optimization via simulation matching.Advances Neural Information Processing Systems, pp. 109117.Azimi, J., Fern, A., & Fern, X. (2011). Budgeted optimization concurrent stochasticduration experiments. Advances Neural Information Processing Systems, pp.10981106.Azimi, J., Jalali, A., & Fern, X. (2012). Hybrid batch Bayesian optimization. International Conference Machine Learning.Bergstra, J., Bardenet, R., Bengio, Y., & Kegl, B. (2011). Algorithms hyper-parameteroptimization. Advances Neural Information Processing Systems, pp. 25462554.Bergstra, J., & Bengio, Y. (2012). Random search hyper-parameter optimization. Journal Machine Learning Research, 13, 281305.Bergstra, J., Yamins, D., & Cox, D. D. (2013). Making science model search: Hyperparameter optimization hundreds dimensions vision architectures.International Conference Machine Learning, pp. 115123.Berkelaar, M., Eikland, K., & Notebaert, P. (2016). lpsolve : Open source (Mixed-Integer)Linear Programming system. http://lpsolve.sourceforge.net/.Brochu, E., Brochu, T., & de Freitas, N. (2010). Bayesian interactive optimizationapproach procedural animation design. Proceedings 2010 ACM SIGGRAPH/Eurographics Symposium Computer Animation, pp. 103112.Brochu, E., Cora, V. M., & de Freitas, N. (2009). tutorial Bayesian optimizationexpensive cost functions, application active user modeling hierarchicalreinforcement learning. Tech. rep. UBC TR-2009-23 arXiv:1012.2599v1, Dept.Computer Science, University British Columbia.Brochu, E., de Freitas, N., & Ghosh, A. (2007). Active preference learning discretechoice data. Advances Neural Information Processing Systems, pp. 409416.Bubeck, S., Munos, R., Stoltz, G., & Szepesvari, C. (2011). X-armed bandits. JournalMachine Learning Research, 12, 16551695.Bull, A. D. (2011). Convergence rates efficient global optimization algorithms. JournalMachine Learning Research, 12, 28792904.383fiWang, Hutter, Zoghi, Matheson, & de FreitasCarpentier, A., & Munos, R. (2012). Bandit theory meets compressed sensing highdimensional stochastic linear bandit. Artificial Intelligence Statistics, pp. 190198.Chen, B., Castro, R., & Krause, A. (2012). Joint optimization variable selection highdimensional Gaussian processes. International Conference Machine Learning.de Freitas, N., Smola, A., & Zoghi, M. (2012). Exponential regret bounds Gaussian process bandits deterministic observations. International Conference MachineLearning.Denil, M., Bazzani, L., Larochelle, H., & de Freitas, N. (2012). Learning attenddeep architectures image tracking. Neural Computation, 24 (8), 21512184.Djolonga, J., Krause, A., & Cevher, V. (2013). High dimensional Gaussian process bandits.Advances Neural Information Processing Systems, pp. 10251033.Eggensperger, K., Feurer, M., Hutter, F., Bergstra, J., Snoek, J., Hoos, H., & Leyton-Brown,K. (2013). Towards empirical foundation assessing Bayesian optimizationhyperparameters. NIPS Workshop Bayesian Optimization Theory Practice.Frazier, P., Powell, W., & Dayanik, S. (2009). knowledge-gradient policy correlatednormal beliefs. INFORMS journal Computing, 21 (4), 599613.Gomes, C. P., van Hoeve, W., & Sabharwal, A. (2008). Connections networks: hybridapproach. International Conference Integration Artificial IntelligenceOperations Research, Vol. 5015, pp. 303307.Gramacy, R. B., Lee, H. K. H., & Macready, W. G. (2004). Parameter space explorationGaussian process trees. International Conference Machine Learning, pp.4552.Gramacy, R., & Polson, N. (2011). Particle learning gaussian process models sequentialdesign optimization. Journal Computational Graphical Statistics, 20 (1),102118.Hamze, F., Wang, Z., & de Freitas, N. (2013). Self-avoiding random dynamics integercomplex systems. ACM Transactions Modelling Computer Simulation, 23 (1),9:19:25.Hansen, N., & Ostermeier, A. (2001). Completely derandomized self-adaptation evolutionstrategies. Evolutionary Computation, 9 (2), 159195.Hennig, P., & Schuler, C. (2012). Entropy search information-efficient global optimization. Journal Machine Learning Research, 98888, 18091837.Hoffman, M., Brochu, E., & de Freitas, N. (2011). Portfolio allocation Bayesian optimization. Uncertainty Artificial Intelligence, pp. 327336.Hoffman, M., Kueck, H., de Freitas, N., & Doucet, A. (2009). New inference strategiessolving Markov decision processes using reversible jump MCMC. UncertaintyArtificial Intelligence, pp. 223231.384fiBayesian Optimization Billion DimensionsHoffman, M., Shahriari, B., & de Freitas, N. (2014). correlation budget constraintsmodel-based bandit optimization application automatic machine learning.Artificial Intelligence Statistics.Hoos, H. H. (2012). Programming optimization. Communications ACM, 55 (2),7080.Hutter, F. (2009). Automated Configuration Algorithms Solving Hard ComputationalProblems. Ph.D. thesis, University British Columbia, Vancouver, Canada.Hutter, F., Hoos, H., & Leyton-Brown, K. (2014). efficient approach assessinghyperparameter importance. International Conference Machine Learning.Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2010). Automated configuration mixedinteger programming solvers. Conference Integration Artificial IntelligenceOperations Research, pp. 186202.Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2011). Sequential model-based optimizationgeneral algorithm configuration. Learning Intelligent Optimization, pp.507523.Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2012). Parallel algorithm configuration.Learning Intelligent Optimization, pp. 5570.Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2013). evaluation sequential modelbased optimization expensive blackbox functions. Proceedings GECCO-13Workshop Blackbox Optimization Benchmarking (BBOB13).Hutter, F., Hoos, H. H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: automaticalgorithm configuration framework. Journal Artificial Intelligence Research, 36,267306.Jones, D. R., Perttunen, C. D., & Stuckman, B. E. (1993). Lipschitzian optimization withoutLipschitz constant. J. Optimization Theory Applications, 79 (1), 157181.Jones, D. (2001). taxonomy global optimization methods based response surfaces.Journal Global Optimization, 21 (4), 345383.Jones, D., Schonlau, M., & Welch, W. (1998). Efficient global optimization expensiveblack-box functions. Journal Global optimization, 13 (4), 455492.Kueck, H., de Freitas, N., & Doucet, A. (2006). SMC samplers Bayesian optimal nonlinear design. IEEE Nonlinear Statistical Signal Processing Workshop, pp. 99102.Kueck, H., Hoffman, M., Doucet, A., & de Freitas, N. (2009). Inference learningactive sensing, experimental design control. Pattern Recognition ImageAnalysis, Vol. 5524, pp. 110.Lizotte, D., Greiner, R., & Schuurmans, D. (2011). experimental methodologyresponse surface optimization methods. Journal Global Optimization, 53 (4), 138.Lizotte, D., Wang, T., Bowling, M., & Schuurmans, D. (2007). Automatic gait optimizationGaussian process regression. International Joint Conference ArtificialIntelligence, pp. 944949.385fiWang, Hutter, Zoghi, Matheson, & de FreitasMahendran, N., Wang, Z., Hamze, F., & de Freitas, N. (2012). Adaptive MCMCBayesian optimization. Journal Machine Learning Research - Proceedings Track,22, 751760.Marchant, R., & Ramos, F. (2012). Bayesian optimisation intelligent environmentalmonitoring. NIPS workshop Bayesian Optimization Decision Making.Martinez-Cantin, R., de Freitas, N., Doucet, A., & Castellanos, J. A. (2007). Active policylearning robot planning exploration uncertainty. Robotics, ScienceSystems.Mockus, J. (1982). Bayesian approach global optimization. Systems ModelingOptimization, Vol. 38, pp. 473481. Springer.Mockus, J. (1994). Application Bayesian approach numerical methods globalstochastic optimization. J. Global Optimization, 4 (4), 347365.Mockus, J., Mockus, A., & Mockus, L. (1999). Bayesian approach randomizationheuristic algorithms discrete programming. American Math. Society.Osborne, M. A., Garnett, R., & Roberts, S. J. (2009). Gaussian processes global optimisation. Learning Intelligent Optimization, pp. 115.Rasmussen, C. E. (2003). Gaussian processes speed hybrid Monte Carlo expensiveBayesian integrals. Bayesian Statistics 7.Rasmussen, C. E., & Williams, C. K. I. (2006). Gaussian Processes Machine Learning.MIT Press.Sankar, A., Spielman, D., & Teng, S. (2003). Smoothed analysis condition numbersgrowth factors matrices. Tech. rep. Arxiv preprint cs/0310022, MIT.Shotton, J., Fitzgibbon, A., Cook, M., Sharp, T., Finocchio, M., Moore, R., Kipman, A.,& Blake, A. (2011). Real-time human pose recognition parts single depthimages. IEEE Computer Vision Pattern Recognition, pp. 12971304.Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical Bayesian optimization machine learning algorithms. Advances Neural Information Processing Systems,pp. 29602968.Srinivas, N., Krause, A., Kakade, S. M., & Seeger, M. (2010). Gaussian process optimizationbandit setting: regret experimental design. International ConferenceMachine Learning, pp. 10151022.Swersky, K., Snoek, J., & Adams, R. P. (2013). Multi-task Bayesian optimization.Advances Neural Information Processing Systems, pp. 20042012.Thompson, W. R. (1933). likelihood one unknown probability exceeds anotherview evidence two samples. Biometrika, 25 (3/4), 285294.Thornton, C., Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2013). Auto-WEKA: Combined selection hyperparameter optimization classification algorithms. ACMSIGKDD Conference Knowledge Discovery Data Mining, pp. 847855.Vallati, M., Fawcett, C., Gerevini, A. E., Hoos, H. H., & Saetti, A. (2011). Generatingfast domain-optimized planners automatically configuring generic parameterisedplanner. ICAPS Planning Learning Workshop.386fiBayesian Optimization Billion DimensionsVazquez, E., & Bect, J. (2010). Convergence properties expected improvement algorithm fixed mean covariance functions. Journal Statistical PlanningInference, 140, 30883095.Wang, Z., & de Freitas, N. (2011). Predictive adaptation hybrid Monte CarloBayesian parametric bandits. NIPS Deep Learning Unsupervised Feature Learning Workshop.Wang, Z., & de Freitas, N. (2014). Bayesian multiscale optimistic optimization. ArtificialIntelligence Statistics.Wang, Z., Zoghi, M., Hutter, F., Matheson, D., & de Freitas, N. (2013). Bayesian optimization high dimensions via random embeddings. International Joint ConferenceArtificial Intelligence, pp. 17781784.387fiJournal Artificial Intelligence Research 55 (2016) 499-564Submitted 07/15; published 02/16Module Extraction Expressive Ontology Languagesvia Datalog ReasoningAna Armas RomeroMark KaminskiBernardo Cuenca GrauIan Horrocksana.armas@cs.ox.ac.ukmark.kaminski@cs.ox.ac.ukbernardo.cuenca.grau@cs.ox.ac.ukian.horrocks@cs.ox.ac.ukDepartment Computer Science,University Oxford,Wolfson Building, Parks Road,Oxford, OX1 3QD, UKAbstractModule extraction task computing (preferably small) fragmentontology preserves class entailments signature interest . Extractingmodules minimal size well-known computationally hard, often algorithmicallyinfeasible, especially highly expressive ontology languages. Thus, practical techniquestypically rely approximations, provably captures relevant entailments,guaranteed minimal. Existing approximations ensure preservessecond-order entailments w.r.t. , stronger condition requiredmany applications, may lead unnecessarily large modules practice.paper propose novel approach module extraction reduced reasoningproblem datalog. approach generalises existing approximations elegant way.importantly, allows extraction modules tailored preserve specifickinds entailments, thus often significantly smaller. evaluation widerange ontologies confirms feasibility benefits approach practice.1. IntroductionModule extraction task computing, given ontology signature interest, (preferably small) subset (a module) preserves class -entailmentsrelevant application hand. module therefore indistinguishablew.r.t. relevant -entailments, application safely rely insteadtasks concern symbols .Module extraction received great deal attention recent years (Seidenberg& Rector, 2006; Stuckenschmidt, Parent, & Spaccapietra, 2009; Cuenca Grau, Horrocks,Kazakov, & Sattler, 2008; Kontchakov, Wolter, & Zakharyaschev, 2010; Del Vescovo, Parsia, Sattler, & Schneider, 2011; Nortje, Britz, & Meyer, 2013; Gatens, Konev, & Wolter,2014). Modules found numerous applications ontology reuse (Cuenca Grau et al.,2008; Jimenez-Ruiz, Cuenca Grau, Sattler, Schneider, & Berlanga Llavori, 2008), matching(Jimenez-Ruiz & Cuenca Grau, 2011), debugging (Suntisrivaraporn, Qi, Ji, & Haase, 2008;Ludwig, 2014) classification (Armas Romero, Cuenca Grau, & Horrocks, 2012; Tsarkov& Palmisano, 2012; Cuenca Grau, Halaschek-Wiener, Kazakov, & Suntisrivaraporn, 2010).c 2016 AI Access Foundation. rights reserved.fiArmas Romero, Kaminski, Cuenca Grau, & Horrockspreservation relevant entailments formalised via inseparability relations (Konev,Lutz, Walther, & Wolter, 2009). strongest notion model inseparability, requires must possible turn model model (re-)interpretingsymbols outside ; case, preserves second-order -entailments(Konev, Lutz, Walther, & Wolter, 2013). weaker flexible notiondeductive inseparability, requires entail -formulasparticular query language. Unfortunately, decision problems associated moduleextraction generally high complexity even undecidable, especially expressiveontology languages. model inseparability, checking whether module w.r.t.undecidable even restricted lightweight description logic (DL) EL (Konevet al., 2013), standard reasoning tractable (Baader, Brandt, & Lutz, 2005).deductive inseparability, problem typically decidable lightweight DLsreasonable query languages, albeit still high worst-case complexity; instance,ExpTime-complete EL consider concept inclusions query language (Lutz& Wolter, 2010). Practical algorithms ensure minimality extracted modulesknown ELI ontologies satisfying particular acyclicity condition (Konev et al.,2013) well dialects DL-Lite (Kontchakov et al., 2010). best knowledge, complexity module extraction ontology languages basedDLs, variants datalog (Cal, Gottlob, Lukasiewicz, Marnette, & Pieris, 2010),remains largely unexplored.Practical module extraction techniques typically based sound approximations,ensure computed fragment module (i.e., inseparable w.r.t.), provide minimality guarantee. popular techniques basedfamily polynomially checkable conditions based notion syntactic locality(Cuenca Grau, Horrocks, Kazakov, & Sattler, 2007a; Cuenca Grau et al., 2008; Sattler,Schneider, & Zakharyaschev, 2009). locality-based module enjoys numberdesirable properties w.r.t. signature interest:(P1) model inseparable O, thus preserving second-order -entailments O.(P2) depleting, sense \ inseparable empty ontology;implies relevant information left behind extracting O.(P3) self-contained, preserves relevant entailments w.r.t. ,also w.r.t. symbols signature.(P4) justification-preserving, sense subset-minimal fragmentpreserving -entailment (each justification) contained M.(P5) computed efficiently, even ontologies expressive description logics.Model inseparability ensures modules used regardless query languagerelevant application hand. Depletingness self-containment identified important properties ontology reuse modular ontology development tasks(Sattler et al., 2009; Jimenez-Ruiz et al., 2008). Finally, preservation justificationsenables use modules optimising debugging explanation services (Schlobach &Cornet, 2003; Kalyanpur, Parsia, Horridge, & Sirin, 2007), well incremental reasoning(Suntisrivaraporn, 2008; Cuenca Grau et al., 2010).500fiModule Extraction Expressive Ontology Languages via Datalog ReasoningLocality-based module extraction techniques easy implement, surprisinglyeective practice. main drawback extracted modules ratherlarge, limits usefulness applications (Del Vescovo, Klinov, Parsia, Sattler, Schneider, & Tsarkov, 2013). One way address issue develop techniquesapproximate minimal modules closely, still fulfilling properties (P1)(P4).Eorts direction confirmed locality-based modules far optimalpractice (Gatens et al., 2014); however, techniques apply rather restrictedontology languages utilise algorithms high worst-case complexity.Another approach computing smaller modules weaken properties (P1)(P4),stronger many applications require. particular, model inseparabilitystrong condition, deductive inseparability w.r.t. query language suitableapplication hand would usually suffice.paper, propose novel approach reduces module extraction reasoningproblem basic rule-based language datalog (Abiteboul, Hull, & Vianu, 1995; Dantsin,Eiter, Gottlob, & Voronkov, 2001). connection module extraction datalogfirst observed Suntisrivaraporn (2008), showed locality ?-module extractionEL ontologies could reduced propositional datalog reasoning. approach takesconnection much farther, generalises locality-based modules elegant way.key distinguishing features approach follows:applicable ontology languages based description logics, alsoexpressive rule-based knowledge representation formalisms extend datalogexistential quantification disjunction head rules (Cal et al., 2010;Bourhis, Morak, & Pieris, 2013; Alviano, Faber, Leone, & Manna, 2012).sensitive dierent inseparability relations proposed literature;particular, extract deductively inseparable modules query languagetailored specific requirements application hand. allows usrelax property (P1) extract significantly smaller modules.cases, modules depleting capture justifications relevant entailments; moreover, approach adapted either ensure dispenseself-containment, depending application needs.ensures tractability module extraction DL-based ontology languages,also enables use highly scalable o-the-shelf datalog reasoners.implemented approach using RDFox datalog reasoner (Motik, Nenov,Piro, Horrocks, & Olteanu, 2014). evaluation complex, real-world, ontologies showsmodule size consistently decreases consider weaker inseparability relations,could significantly improve usefulness modules applications.2. PreliminariesSection 2.1 introduce language first-order rules, powerful enoughfully capture expressive rule-based ontology languages datalog (Cal et al.,2010), datalog,_ (Bourhis et al., 2013; Alviano et al., 2012), well mainstream501fiArmas Romero, Kaminski, Cuenca Grau, & Horrocksdescription logics (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003).results paper hold arbitrary knowledge bases consisting first-orderrules, hence applicable wide range knowledge representation formalisms.Section 2.2 introduce syntax first-order semantics description logicSROIQ (Horrocks, Kutz, & Sattler, 2006), underpins W3C standard ontologylanguage OWL 2 (Motik, Patel-Schneider, & Parsia, 2012; Cuenca Grau, Horrocks, Motik,Parsia, Patel-Schneider, & Sattler, 2008). introduce normal form SROIQestablish correspondence first-order rules. Finally, Section 2.3 briefly recallwell-known hyperresolution calculus first-order logic (Bachmair & Ganzinger, 2001),exploit many technical results show module preservesrequired consequences given ontology.Throughout paper, assume basic familiarity first-order logic usestandard first-order logic notions, predicates, constants, variables, terms, atoms,formulas, sentences, interpretations entailment (written |=). define signatureset predicates; furthermore, given first-order sentence , use Sig( ) denotesignature . say -sentence Sig( ) . Analogously, denoteCt( ) set constants . definitions extend naturally sets sentences;indeed, later paper speak -rules, -datasets, -ontologiesobvious meaning. restriction signatures contain predicates separatetreatment constants convenient working inseparability relations later on.set function-free sentences F 0 (model) conservative extension set Fmodel F model J F 0 domainAI = AJ 2 Sig(F) aI = aJ 2 Ct(F).deviate slightly standard definition first-order logic definitioninclude nullary symbols > ?, interpreted true false respectively every first-order interpretation. Similarly, consider first-order logic withoutequality hence assume interpreted identity relationdomain every interpretation. Instead, treat ?, > ordinary predicates,meaning axiomatise explicitly every knowledge base. assume ?nullary, > unary binary. Given set F function-free sentences,define following sets sentences F ? , F > , F .F ? empty F contains occurrences ?, singleton set {?} otherwise.F > empty F contains occurrence >; otherwise, set{ 8x1 , . . . , xn [A(x1 , . . . , xn ) ! >(xi )] | 2 Sig(F) n-ary, 1 n }F empty F contains occurrences ; otherwise, consists sentences(EQ1)(EQ5) given next. Sentence (EQ1) instantiated constant 2 Ct(F);furthermore, sentences (EQ2) (EQ5) instantiated n-ary predicate502fiModule Extraction Expressive Ontology Languages via Datalog ReasoningSig(F) xi x = x1 , . . . , xn :!aa(EQ1)8x [A(x) ! xi xi ](EQ2)8x, [x ! x]8x, y, z [x ^ z ! x z]8x, [A(x) ^ xi ! A(x1 , . . . , xi(EQ3)(EQ4)1 , y, xi+1 , . . . , xn )](EQ5)consider substitutions functional mappings two sets terms. Givensubstitution term domain , abuse notation expression denotes t. Substitutions applied formulas: given atom A(t1 , ..., tn ),A(t1 , ..., tn ) = A(t1 , ..., tn ), given non-atomic formula ,result applying atoms . application extended sets formulas naturalway. Given two substitutions , composition substitutiont( ) = (t ) domain . say compatiblecoincide intersection domains. compatible, unionsubstitution [ t( [ ) = domain t( [ ) =domain . Finally, use dom( ) (resp. range( )) denote domain(resp. range) .2.1 Rule-Based First-Order LanguagesRule-based languages prominent knowledge representation formalisms closely relatedontology languages (Dantsin et al., 2001; Bry, Eisinger, Eiter, Furche, Gottlob, Ley, Linse,Pichler, & Wei, 2007; Cal et al., 2010). paper, focus monotonic formalismshence rule languages seen fragments first-order logic. nextdefine general notion first-order rule underpins datalog datalog,_families languages (Cal et al., 2010; Alviano et al., 2012).fact function-free ground atom. finite set facts called dataset. rule rfunction-free first-order sentence form8x['(x) ! 9y (x, y)](1)x disjoint vectors variables, ' (possibly empty) conjunction distinctatoms constants variables x; built atoms constantsvariables x [ using conjunction (^) disjunction (_). Note fact alsorule. Formula ' rule body 9y (x, y) rule head. head ruleempty, case represent . Universal quantifiers rules omittedbrevity. Rules required safe, is, universally quantified variableshead must occur body. rule datalog head either empty consistssingle atom variables universally quantified. Note that, set Ffunction-free sentences, set F ? [ F > [ F contains datalog rules.(first-order) ontology finite set rules satisfying O? [ O> [ O.assume w.l.o.g. dierent rules share existentially quantified variablesrule empty head ? ! .503fiArmas Romero, Kaminski, Cuenca Grau, & Horrocksroles(R, x, y)(R , x, y)==R(x, y)R(y, x)(?c , x)(>c , x)(o, x)(C, x)(C1 u C2 , x)(C1 C2 , x)(9R.C, x)(8R.C, x)(9R.Self, x)( nR.C, x)( nR.C, x)axioms(C1 v C2 )(R1 Rm v S)(Disj(R1 , R2 ))(Ref(R))===========?>(x)xo(C, x)(C1 , x) ^ (C2 , x)(C1 , x) _ (C2 , x)9y[(R, x, y) ^ (C, y)]8y[(R, x, y) ! (C, y)](R, x, x) VV9x1 , . . . , xn [ ((R, x, xi ) ^ (C, xi )) ^ i6=j (xi xj )]VW8x1 , . . . , xn+1 [ ((R, x, xi ) ^ (C, xi )) ! i6=j xi xj ]concepts====8x[(C1 , x) ! V(C2 , x)]8x1 , . . . , xm+1 [ i=1 (Ri , xi , xi+1 ) ! (S, x1 , xm+1 )]8x, y[(R1 , x, y) ^ (R2 , x, y) ! ?]8x[>(x) ! (R, x, x)]Figure 1: Semantics SROIQ via translation first-order logic.datalog program ontology containing datalog rules. Given datalog programP dataset D, materialisation, denoted P(D), set facts entailedP [ D. materialisation computed time polynomial size usingforward chaining (Abiteboul et al., 1995; Dantsin et al., 2001).conclude section, define languages typically used querying first-orderontologies. define Boolean positive existential query (Boolean PEQ) non-emptysentence q built function-free atoms using 9, ^ _; query holds w.r.t.ontology |= q. Boolean PEQ conjunctive query (CQ) disjunctionfree. following proposition, proof straightforward, establishes usefulconnection Boolean PEQ evaluation entailment first-order rules.VProposition 1. Let ontology, r = ni=1 (x) ! 9y (x, y) rule, letsubstitution mapping universally quantified variables r fresh distinct constants.Then, |= r [ { }ni=1 |= 9y .2.2 Description LogicsDescription logics (DLs) (Baader et al., 2003) family knowledge representationformalisms correspond decidable fragments first-order logic. DLs logical formalisms underpinning standard ontology languages: OWL DL baseddescription logic SHOIN (Horrocks, Patel-Schneider, & van Harmelen, 2003), whereasrevision OWL 2 based expressive logic SROIQ (Horrocks et al., 2006;Cuenca Grau et al., 2008; W3C OWL Working Group, 2012).504fiModule Extraction Expressive Ontology Languages via Datalog Reasoningbasic building blocks SROIQ pairwise disjoint countable sets atomicconcepts, correspond unary predicates, atomic roles, correspond binarypredicates, individuals, correspond constants. role R either atomicrole inverse atomic role S. Complex concepts constructed accordingfollowing grammar, ?c >c special bottom top concepts,atomic concept, R role, individual n 1:C ::= ?c | >c | | {o} | C | C1 u C2 | C1 C2 |9R.C | 8R.C | 9R.Self |nR.C | nR.Cassume concept expressions form 1R.C replaced equivalent9R.C. general concept inclusion axiom (GCI) expression form C1 v C2 ,C1 C2 concepts. role inclusion axiom (RIA) expression formR1 Rm v R Ri role R atomic role. role disjointness axiomexpression form Disj(R1 , R2 ) R1 R2 roles. Finally, reflexivity axiomexpression form Ref(R) R role.SROIQ ontology finite set GCIs, RIAs, role disjointness reflexivity axioms. order ensure decidability basic reasoning tasks, SROIQ ontologymust satisfy certain additional conditions (e.g., set RIAs must satisfy regularitycondition); conditions are, however, immaterial results paperrefer reader work Horrocks et al. (2006) details.semantics SROIQ given direct translation first-order logic(Baader et al., 2003; Motik, 2006) using mapping function Figure 1. GivenSROIQ ontology O, let FO = { () | 2 }; define?>(O) = FO [ FO[ FO[ FOfirst-order interpretation model model (O).Note (O) always set first-order rules defined Section 2.1. However,always polynomially normalised entailment preserving SROIQ ontologyO0 (O0 ) set rules. next define normalised SROIQ ontologiesassume onwards (unless otherwise stated) SROIQ ontologies normalised.Definition 2. SROIQ ontology normalised consists axioms formv ?c v {o} >c v {o} v v B1 B2 A1 u A2 v Bv 9R.B v 9R.Self 9R.A v B 9R.Self v v nR.BR1 R2 v R v Disj(R, S) Ref(R)A(i) , B(i) atomic concepts, individual, R(i) , atomic roles, n1.Table 1 shows application normalised axioms. Clearly, (O) set ruleswhenever normalised. Moreover, establishes bijection (O)case. Since (O) semantically equivalent, thus natural identify them,shall remainder paper.505fiArmas Romero, Kaminski, Cuenca Grau, & Horrocksv ?cv {o}>c v{o} vv B1 B2A1 u A2 v Bv 9R.Bv 9R.Self9R.A v B9R.Self vv nR.BR1 R2 vR vSDisj(R, S)Ref(R)()A(x) ! ?A(x) ! x>(x) ! A(x)( ! A(o))A(x) ! B1 (x) _ B2 (x)A1 (x) ^ A2 (x) ! B(x)A(x) ! 9y[R(x, y) ^ B(y)]A(x) ! R(x, x)R(x, y) ^ A(y) ! B(x)R(x, x) ! A(x)VWA(x) ^ n+1i=1 [R(x, yi ) ^ B(yi )] !i6=j yi yjR1 (x, y) ^ R2 (y, z) ! S(x, z)R(x, y) ! S(y, x)R(x, y) ^ S(x, y) ! ?>(x) ! R(x, x)Table 1: Correspondence normalised SROIQ axioms rules.Proposition 3. Let SROIQ ontology let O0 result exhaustivelyapplying rewriting rules Figure 2. Then, O0 satisfies following properties:(i) normalised; (ii) size polynomial size (assuming unary encodingnumbers); (iii) conservative extension O.Proof. easy see rewrite rule always applicable every axiomnormalised; furthermore, rule applicable normalised axioms. Thus, O0 normalised.Furthermore, note rules Figure 2 syntactic variant structural transformation first-order logic (Nonnengart & Weidenbach, 2001). implies O0computed time polynomial size (assuming unary encoding numbers),also conservative extension O.2.3 Hyperresolution ProofsReasoning w.r.t. ontologies realised means hyperresolution (Robinson, 1965;Bachmair & Ganzinger, 2001), generalises forward chaining datalog.HyperresolutionVis applicablesets first-order clausesuniversally quantified senWtences form ! j j j atoms (possibly containing function symbols). Thus, applicable ontologies containing existentially quantified rulesSkolemisation subsequent transformation Conjunctive Normal Form (CNF).rule r form (1) existentially quantified variable r, let fyrfunction symbol globally unique r arity |x|, let sk substitutionsk (y) = fyr (x) r y. Skolemisation r sentencesk(r) = '(x) ! (x, y)sk506fiModule Extraction Expressive Ontology Languages via Datalog ReasoningR1?c v Cv {o}9S .D v C9S.D v CmS .D v CmS.D v Cu C1 v C2C1 v C2C1 C2 v C38S .C1 v C28S.C1 v C29S .Self v C(m 1)S .C1 v C2(m 1)S.C1 v C2C v >c{o} vC v 9S .DC v 9S.DC v 8S .DC v 8S.DC v 9S .SelfC v nS .DC v nS.DC1 v C2C1 v C2C1 v C2 u C3C1 v mS .C2C1 v mS.C2D1 v D2R2 R 3 Rk vQRvSR Q vSDisj(R, Q )Disj(Q , R)Ref(Q ))))))))))))))))))))))))))))))))))))X v {o}, v X9P.X v D, v P9S.X v C, v XmP.D v C, v P>c v (m 1)S.D CX u C1 v C2 , v X> c v C1 C2C1 v C3 , C2 v C38P.C1 v C2 , P v>c v 9S.X C2 , X u C v ?c9P.Self v C, v P(m 1)P.C1 v C2 , P v>c v mS.C1 C2{o} v X, X vC v 9P.D, P vC v 9S.X, X vC v 8P.D, v PC v 8S.X, X vC v 9P.Self, P vC v nP.D, v PC v nS.X, X vC1 v X C2 , X vC 1 u C 2 v ?cC1 v C2 , C1 v C3C1 v mP.C2 , P vC1 v 9S.Xi , Xi v C2 , Xi u Xj v ?c (1i<jm)D1 v X, X v D2R1 R2 v P, P R3 Rk vP R v S, Q v PR P v S, Q v PDisj(R, P ), Q v PDisj(P, R), Q v PRef(Q)Figure 2: Normalisation SROIQ axioms, C(i) concepts, D(i) non-atomicconcepts dierent ?c >c , X fresh atomic concept, Qatomic roles, R(i) roles, P fresh atomic role, 2, n 1, k 3.CNF sk(r) set first-order clauses conservative extension sk(r).CNF obtained polynomial time using standard structural transformation(Nonnengart & Weidenbach, 2001). paper consider arbitrary fixed function507fiArmas Romero, Kaminski, Cuenca Grau, & Horrocksmapping rule r CNF sk(r). function extends ontologiesobvious way, refer (O) clausification O. well-known propertiesSkolemisation structural transformation |= (O) |=ontology first-order sentence Sig(O).VWLet r = ni=1 !j=1 j clause let 'i = _ 1 n grounddisjunctions atoms single atom; furthermore,letW general unifierW(MGU) , . ground disjunction atoms ni=1 _j=1 j hyperresolventr '1 , . . . , 'n . disjunction empty, case denote . LetC set clauses, dataset ' disjunction ground atoms. hyperresolutionproof (or simply proof ) ' C [ pair = (T, ) directed, rootedtree, mapping nodes disjunctions ground atomsnode v following properties satisfied:1. v root (v) = ',2. v leaf either ( ! (v)) 2 C (v) 2 D,3. v children w1 , . . . , wn(w1 ), . . . , (wn ).(v) hyperresolvent clause Csupport , denoted supp(), set clauses C take partdescribed properties 2 3 above. write C [ ` ' indicate existsproof ' C [ D. Hyperresolution sound (if C [ ` ' C [ |= '), completefollowing sense: C [ |= ' exists ' C [ ` (Robinson,1965). particular, C unsatisfiable C [ ` .Given proofs = (T, ) 0 = (T 0 , 0 ), say embeddable 0exists mapping : ! 0 satisfying following properties v 2 : (i) vleaf , (v) leaf 0 ; (ii) w ancestor v (w) ancestor(v) 0 , (iii) (v) 0 ((v)) [ {?}. Furthermore, given substitution , sayembeddable 0 modulo embeddable proof (T 0 , 0 ),0 (v) = 0 (v) v 2 0 .3. Module Extractionsection, recapitulate key notions inseparability relation moduleproposed description logic literature (Cuenca Grau et al., 2008; Kontchakovet al., 2010; Konev et al., 2013; Sattler et al., 2009; Konev et al., 2009). Furthermore,required, adapt notions setting first-order rules prove basicresults exploited throughout paper.3.1 Inseparability Relations ModulesIntuitively, given ontology signature , module w.r.t. subsetindistinguishable w.r.t. reasoning tasks predicatesconsidered interest. indistinguishability criteria depend specific task hand,usually formalised means inseparability relations.508fiModule Extraction Expressive Ontology Languages via Datalog ReasoningDefinition 4. inseparability relation family = { | set predicates }equivalence relations ontologies satisfying following properties:O0 conservative extension = Sig(O), O0 ;O1 implies O2 O1 O2 .first property ensures inseparability stable model-preserving transformations, whereas second one ensures consistent monotonicityfirst-order logic. following definition captures common inseparability relationsstudied literature.Definition 5. signature , say ontologies O000-model inseparable (O), every model (resp. ) exists0Jmodel J (resp. O) domain = 2 .-query inseparable (O q O0 ) Boolean PEQ q dataset[ |= q O0 [ |= q.-fact inseparable (O f O0 ) fact[ |= O0 [ |= .dataset-implication inseparable (O O0 ) -rule r form A(x) ! B(x)|= r O0 |= r.observations notions introduced Definition 5 order. First,note definition query fact inseparability quantification queriesdatasets relative signature ; standard convention adoptedliterature (Lutz & Wolter, 2010; Baader, Bienvenu, Lutz, & Wolter, 2010). Second,restriction Boolean queries definition query inseparability strictly technical:obvious extension non-Boolean queries leads equivalent definition. Finally,observe notion fact inseparability natural generalisation inseparabilityw.r.t. atomic instance queries description logics (Lutz & Wolter, 2010).Example 6. Let us consider ontology Oex Figure 3, serve runningexample. Let us also consider signatures fragments Mi Oex given next:1 = {B, C, D, H}M1 = {r5 , r6 , r7 , r8 }3 = {A, C, D, R}M3 = {r1 , r2 }2 = {A, B}M2 = ;non-tautological 1 -implication entailed Oex D(x) ! H(x), alsofollows M1 ; thus, M1 1 -implication inseparable Oex . Furthermore, subsetOex containing M1 entail D(x) ! H(x) hence 1 -implicationinseparable Oex . see later on, requirement fact inseparabilitystronger implication inseparability; indeed, M1 1 -fact inseparableOex since, D1 = {B(a), C(a)}, Oex [ D1 |= D(a) M1 [ D1 6|= D(a).509fiArmas Romero, Kaminski, Cuenca Grau, & Horrocksr1r2r3r4r5r6r7r8::::::::A(x) ! 9y1 [R(x, y1 ) ^ B(y1 )]v 9R.BA(x) ! R(x, o)v 9R.{o}B(x) ^ C(x) ! D(x)BuC vDR(x, y) ^ C(y) ! E(x)9R.C v ED(x) ! F (x) _ G(x)DvF tGF (x) ! 9y2 S(x, y2 )F v 9S.>cS(x, y) ! H(x)9S.>c v HG(x) ! H(x)GvHFigure 3: Example ontology Oex rule DL notation.checked M2 2 -fact inseparable Oex . is, however, 2 -queryinseparable: D2 = {A(a)}, Oex [ D2 |= 9yB(y) M2 [ D2 6|= 9yB(y).Finally, consider M3 3 . see later on, M3 3 -query inseparableOex ; however, 3 -model inseparable. Indeed, interpretation= {a, o}, AI = {a}, B = C = {o}, = ; RI = {(a, o)} model .3interpretation, however, cannot extended model r3 (or, consequently,model O) without reinterpreting A, C, R. also see ensure 3 -modelinseparability suffices extend M3 rule r3 .Model inseparability characterised terms preservation second-order consequences (Konev et al., 2013): ontologies O0 -model inseparablesecond-order -sentence ' |= ' O0 |= '. Additionally,show next, query fact inseparability characterised terms preservationfirst-order rules datalog rules, respectively.Proposition 7. following statements hold signature pairontologies O1 O2 :1. O1 q O2 O1 |= r , O2 |= r holds -rule r non-empty head.2. O1 f O2 O1 |= r , O2 |= r holds datalog -rule r non-empty head.qProof. prove first statement;Vnthe second one analogous. Suppose O1 O2consider arbitrary rule r = i=1 (x) ! 9y (x, y) 6= ,substitution mapping universally quantified variables r fresh distinct constants.Furthermore, consider dataset = { }ni=1 , Boolean PEQ q = 9y (x, y)(note q indeed Boolean PEQ since hypothesis non-empty). Proposition 1,Oi |= r Oi [ |= q. Together O1 q O2 , implies O1 |= r , O2 |= r.Assume O1 |= r , O2 |= r holds -rule r non-emptyhead. LetVq Boolean PEQ -dataset consider rule r =2D ! q.Proposition 1 Oi [D |= q Oi |= r, since, assumption, O1 |= r , O2 |= r,follows O1 [ |= q , O2 [ |= q hence O1 O2 -query inseparable.immediately follows inseparability relations Definition 5 naturallyordered strongest weakest non-trivial given next:qf( ( (510fiModule Extraction Expressive Ontology Languages via Datalog ReasoningFurthermore, identify classes entailments relevant inseparability relation.Definition 8. inseparability relation 2 {m, q, f, i}, let relS functionmapping ontology signature set relevant entailments follows:8{ | |= second-order -sentence }=>><{ r | |= r r -rule non-empty head }= qrelS (O, ) ={r||=rrdatalog-rulenon-emptyhead}= f>>:{ r | |= r r form A(x) ! B(x) A, B 2 } =following theorem establishes inseparability relations Definition 5fully characterised preservation relevant -entailments Definition 8.Theorem 9. Let O0 ontologies, signature, let 2 {m, q, f, i}. Then,O0 relS (O, ) = relS (O0 , ).Proof. direct consequence Definitions 5 8, Proposition 7 characterisationmodel inseparability terms second-order entailments (Konev et al., 2013).Inseparability relations allow us formalise modules well desirable properties.Definition 10. Let ontology, signature, inseparability relation,let O. say -module M. Furthermore,minimal M0 ( -module O;self-contained S[Sig(M) M;depleting \ ;; strongly depleting \ S[Sig(M) ;.Finally, define justification sentence |= subset-minimalO0 O0 |= . say justification-preservingrelS (O, ) justification O0 O0 M.Example 11. Consider ontologies signatures Example 6. seeMi module Oex ; particular, M1 i1 -module, M2 f2 -module , M3q3 -module, M3 [ {r3 }3 -module.inseparability requirement ensures modules used instead reasoning purposes, provided entailments relevant application handcaptured given inseparability relation contain symbols .Minimality ensures module contains little irrelevant information possiblestill satisfying inseparability requirement. Although minimality clearly desirableapplications modules (e.g., reasoning small ontology subsets typicallypreferable reasoning whole ontology), extracting modules minimal sizeinvariably hard (and often algorithmically infeasible) (Lutz & Wolter, 2010; Konev511fiArmas Romero, Kaminski, Cuenca Grau, & Horrockset al., 2013); thus, practical techniques aim computing modules typically muchsmaller O, albeit necessarily minimal.Self-contained modules inseparable w.r.t. relevant signature, also w.r.t. signature. Depletingness ensures relevant informationleft behind extracting module O, i.e., \ inseparableempty ontology. basic form depletingness formulated terms ,whereas stronger variant requires inseparability w.r.t. symbols well. Selfcontained depleting modules especially well-suited ontology reuse modularontology development applications. instance, self-contained depleting,developer remodel sub-domain characterised replacingnew set axioms, guarantee changes performedunintended interactions rest O.Justification-preservation enables use modules ontology debugging repair(Schlobach & Cornet, 2003; Kalyanpur et al., 2007; Kalyanpur, Parsia, Sirin, & Hendler,2005; Horridge, Parsia, & Sattler, 2008; Kalyanpur, Parsia, Sirin, & Cuenca Grau, 2006).justification entailment useful form explanation; furthermore, ontologyrepair services typically rely computation justifications unintended entailment first step towards obtaining repair plan. Computing justifications, however,computationally intensive task practical module extraction techniqueseectively exploited optimise process (Suntisrivaraporn et al., 2008).conclude section briefly discussing impact normalisation moduleextraction. pointed Section 2.2, technical results applicable ontologiesconsisting rules; referring DL ontologies, implicitly assume givenrule form therefore normalised. argue normalisation techniques stemmingstructural transformation preserve inseparability hence possible obtainmodule DL ontology module normalisation computed.Definition 12. normalisation function norm maps SROIQ ontologies normalisedSROIQ ontologies s.t. following holds ontologies domain:norm(O) conservative extension O;O1 O2 implies norm(O1 ) norm(O2 ).Definition 12 captures standard normalisation techniques stemming structural transformation, one discussed Section 2.2. Furthermore, typicallystraightforward practice keep track correspondence axiomsoriginal ontology norm(O). shown following proposition,correspondence allows us efficiently obtain module module norm(O)computed.Proposition 13. Let signature, inseparability relation, norm normalisation function. Then, norm(M) norm(O).Proof. definition, satisfies O1 O2 O1 implies O2 O.Therefore, norm(M) contains -module norm(O) norm(M) -modulenorm(O) itself. hand, since norm(O) (resp. norm(M)) conservative512fiModule Extraction Expressive Ontology Languages via Datalog Reasoningv ?cv {o}>c v{o} vv B1 B2A1 u A2 v Bv 9R.Bv 9R.Self9R.A v B9R.Self vv mR.BR 1 R2 vRvSDisj(R, S)Ref(R)?-local w.r.t.2/2/nevernever2/A1 2/ A2 2/2/2/R 2/ 2/R 2/2/ R 2/ B 2/R1 2/ R2 2/R 2/R 2/ 2/never>-local w.r.t.nevernever2/2/B1 2/ B2 2/B 2/{R, B} \ = ;R 2/B 2/B 2/never2/2/neverR 2/Table 2: Syntactic locality normalised SROIQ axiomsextension (resp. M), asume w.l.o.g. contains symbolsSig(norm(O)) \ Sig(O), norm(O) (resp. norm(M) M). Sinceequivalence relation, follows norm(M) norm(O).3.2 Syntactic Localitymany inseparability relations introduced Section 3.1, checking whethermodule w.r.t. typically high complexity, often undecidable, evenrather lightweight ontology languages (Lutz & Wolter, 2010; Konev et al., 2013).Consequently, practical module extraction techniques typically based approximations, ensure computed module, yet necessarily minimalone. One approximation often exploited practice based notionsyntactic locality (Cuenca Grau et al., 2007a; Cuenca Grau, Horrocks, Kazakov, & Sattler,2007b; Sattler et al., 2009; Cuenca Grau et al., 2008).Intuitively, normalised SROIQ axiom ?-local (resp. >-local) treating atomicconcepts roles outside ? (resp. >) concept role, respectively, leadsaxiom obvious tautology.Definition 14. normalised SROIQ axiom ?-local (resp. >-local) w.r.t. signaturesatisfies conditions given second (resp. third) column Table 2.normalised SROIQ ontology ?-local (resp. >-local) w.r.t. axioms?-local (resp. >-local) w.r.t. . Finally, say local w.r.t. either ?-local>-local w.r.t. .key properties ?- >-locality, established existing literature (CuencaGrau et al., 2008; Sattler et al., 2009), summarised following proposition.513fiArmas Romero, Kaminski, Cuenca Grau, & HorrocksProposition 15. Let SROIQ ontology, signature x 2 {?, >}.1. x-local w.r.t.;.2. \M x-local w.r.t. [Sig(M), self-contained, stronglydepleting, justification-preserving-module O.Property 2 Proposition 15 immediately suggests notion locality-based module.Definition 16. Let normalised SROIQ ontology, signature, x 2 {?, >}.x-module w.r.t. , denoted Mx[O,] , smallest subset\ x-local w.r.t. [ Sig(M).?> -module w.r.t. least fixpoint sequence {Mi }i 1M1 = M?2:[O,] Mi defined followsMi =(M>[MiM?[Mi1 ,]1 ,]oddevenExample 17. Consider ontology Oex Figure 3 signature = {B, C, D, R}.>?>M?[O ex ,] = {r3 r8 }, M[O ex ,] = {r1 r3 }, M[O ex ,] = {r3 }.Locality-based modules Definition 16 computed polynomial time. Furthermore, Proposition 15, self-contained, strongly depleting justificationpreserving. are, however, generally minimal, even amongst strongly depletingself-contained modules.4. Overviewprovide high-level overview approach module extraction, basednovel reduction reasoning problem datalog. approach builds recenttechniques exploit datalog engines ontology reasoning (Kontchakov, Lutz, Toman,Wolter, & Zakharyaschev, 2011; Stefanoni, Motik, & Horrocks, 2013; Zhou, Nenov, CuencaGrau, & Horrocks, 2014; Zhou, Cuenca Grau, Nenov, Kaminski, & Horrocks, 2015).connection module extraction datalog first observed (Suntisrivaraporn,2008), shown ?-module extraction lightweight DL EL+reduced propositional datalog reasoning.approach takes connection much farther providing unified frameworksupports module extraction arbitrary ontologies consisting first-order rules, wellwide range inseparability relations. Modules obtained using approachtailored requirements application hand. addition significantlysmaller practice, modules preserve features syntactic locality modules:widely applicable, efficiently computed practice, satisfy widerange additional properties.follows, fix w.l.o.g. arbitrary ontology signature Sig(O).Unless otherwise stated, definitions theorems parameterised .stated Section 2, assume rules share variables.514fiModule Extraction Expressive Ontology Languages via Datalog Reasoningoverall strategy extract module roughly summarisedfollowing steps:11. Choose substitution mapping existentially quantified variables freshSkolem constants, obtain datalog program P(a) Skolemising rules using obtain function-free rules ' !may contain ^ _ head;,(b) replacing resulting rules ' ! set { ' ! | atom }datalog rules; way, disjunctions head rules turned conjunctions split dierent datalog rules.Clearly, program P logically entails thus preserves consequences.2. Choose -dataset D0 initial facts compute materialisation P [ D0 .3. Choose set Dr relevant facts materialisation (possibly containing symbolsoutside ), compute supporting rules P 0 P fact.4. Output subset rules correspond rule P 0 .subset described fully determined substitution datasetsD0 Dr . main intuition behind module extraction approach pick, D0 Dr (and hence also M) proof -consequence 'preserved inseparability relation interest embedded collectionproofs P [ D0 relevant fact Dr . way, ensure containsnecessary rules entail '.Example 18. illustrate strategy might work practice, consider runningexample ontology Oex Figure 3 signature = {B, C, D, H}.Assume goal compute module -implication inseparableOex . Recall Example 6 sentence ' = D(x) ! H(x) non-trivial-implication entailed Oex , therefore requirement |= '.Furthermore, note proving Oex |= ' amounts proving Oex [ {D(a)} |= H(a)fresh constant (cf. Proposition 1 Section 2.1).Figure 4(a) depicts hyperresolution proof showing H(a) derivedD(a) set clauses corresponding r5 r8 , rule r6 transformedclause r6 = F (x) ! S(x, fyr26 (x)). follows = {r5 r8 } -implication inseparableOex since covers support . Moreover, minimal since H(a) cannotderived subset {r5 r8 }.approach, take D0 Dr contain, respectively, initial fact D(a)fact H(a) proved. also make map variables y1 y2 fresh constants cy1cy2 , respectively. resulting datalog program P shown Figure 5.Figure 4(b) depicts proofs 0 00 H(a) P [ {D(a)}. support proof 00datalog program consists rules r500 r8 , stem rules r5 r8 Oex ;1. simplicity, section overlook certain technical details presence constantsO. thoroughly addressed later on.515fiArmas Romero, Kaminski, Cuenca Grau, & HorrocksH(a)r7H(a)r7H(a)r8S(a, fyr26 (a)) _ H(a)S(a, cy2 )G(a)r60r500F (a) _ H(a)r8F (a)D(a)F (a) _ G(a)r5D(a)D(a)0r6r5000(a)(b)Figure 4: Proofs H(a) D(a) (a) Oex (b) corresponding datalog programr10r2r3r4r50r60r7r8::::::::A(x) ! R(x, cy1 )r100 : A(x) ! B(cy1 )A(x) ! R(x, o)B(x) ^ C(x) ! D(x)R(x, y) ^ C(y) ! E(x)D(x) ! F (x)r500 : D(x) ! G(x)F (x) ! S(x, cy2 )S(x, y) ! H(x)G(x) ! H(x)Figure 5: Datalog program obtained Oex using = {y1 7! cy1 , y2 7! cy2 }see, however, {r5 , r8 } ( hence entail '. situationarises consider 0 only, case would recover rules r5 r7 .datalog program strengthening Oex one particular proofdatalog program may translate back proof original ontology. Indeed,order compute M, need consider supports 0 00 , casewould successfully recover M.example, approach would allow us compute minimal module. is,however, case general: since P strengthening given ontology mayproofs P [ D0 facts Dr correspond proofs -consequenceontology, may lead inclusion unnecessary rules module.following sections describe approach formally.516fiModule Extraction Expressive Ontology Languages via Datalog ReasoningSection 5, define general notion module setting, capturesdegrees freedom framework uniquely specifies datalog program Pmodule corresponding specific choices , D0 , Dr . Furthermore,establish key correspondence proofs original ontology setsproofs P [ D0 , exploit many subsequent technical results.Section 6, describe concrete module settings inseparability relations introduced Section 3.1, namely implication (Section 6.1), fact (Section 6.2),query (Section 6.3), model inseparability (Section 6.4) also showlocality ?-modules precisely captured instantiation framework.Section 7, consider variants inseparability relations Section 3.1 studiedliterature, describe specific module settings them. results showframework easily adapted capture new inseparability relationshence illustrate generality versatility approach.Section 8, show modules consistent intuition strongerinseparability relations lead larger modules. this, introduce notionhomomorphism module settings, allow us establish containment relations modules specified Sections 6 7.Section 9, study additional properties modules. showdepleting justification-preserving inseparability relations previous sections. modules, however, may strongly depleting self-contained;although may beneficial, allows us extract smaller modules, properties still important ontology reuse scenarios. Hence, propose techniqueensures extracted modules also strongly depleting self-contained.Section 10, briefly discuss complexity module extraction withinframework show tractability DL-based ontology languages.Finally, Section 11, discuss optimality module settings introducedSections 6 7. particular, although modules minimal general,aim determining whether modules obtained settings Sections 6 7smallest possible within framework.5. Notion Module Settingsection present framework module extraction. key notionmodule setting, captures declarative way main elements approachdiscussed Section 4.Definition 19. module setting tuple= h, D0 , Drsubstitution mapping constant existentially quantified variable(possibly fresh) constant;D0 dataset mentioning predicates ;517fiArmas Romero, Kaminski, Cuenca Grau, & HorrocksDr dataset mentioning predicates Sig(O) [ {?}.rule r = '(x) ! 9y (x, y) O, let(r) = { (' ! ) |program P}definedP =supportatom[(r)r2Osetsupp( ) = { r | r 2 supp() proof P [ D0 fact Dr }.Finally, F = { r 2 | supp( ) \ (r) 6= ; }, modulefollowing subset O:= F [ F ? [ F > [ F .definedmapping datasets D0 Dr constitute degrees freedomframework, Definition 19 ensures specific choices parameters modulesetting fully determine module .datalog program P obtained applying rule rtime splitting head atoms r dierent rules. application turnsexistentially quantified variables (possibly fresh) constants hence transformsset rules variables universally quantified; additionally, mapsconstants occurring (possibly dierent) constants. Since requiredinjective, possible map existentially quantified variable constantconstant. see next, P strengthening sensepreserves consequences coupled arbitrary dataset. Analogous datalogstrengthenings exploited overestimate reasoning outcomes description logicontologies (Krotzsch, Rudolph, & Hitzler, 2008b; Stefanoni et al., 2013; Krotzsch, Rudolph,& Hitzler, 2008a; Zhou et al., 2014; Zhou, Nenov, Cuenca Grau, & Horrocks, 2013; Zhou,Cuenca Grau, Horrocks, Wu, & Banerjee, 2013).support supp( ) collects datalog rules participating proof P [ D0relevant fact Dr . Intuitively, support captures image moduleP . Finally, module consists rules corresponding datalogrule support supp( ).Example 20. Let us reconsider Example 18 Section 4, chose map variablesy1 y2 fresh constants cy1 cy2 , whereas D0 Dr contain, respectively, initialfact D(a) fact H(a) proved. Definition 19 ensures P consists preciselydatalog rules Figure 5. support supp( ) consists rules support0 00 shown Figure 4. Finally, consists rules r5 r8 , required.following lemma establishes key correspondence hyperresolution proofs(the clausification of) sets proofs datalog program P . correspondence already manifest Figure 4 running example. Given arbitrary dataset518fiModule Extraction Expressive Ontology Languages via Datalog Reasoningsubstitution constants constants compatible (i.e.,coincide intersectiondomains), lemma shows proofWndisjunction facts ' = i=1 (O) [ corresponding set proofsdisjunct P [ . implies, particular, P indeed strengtheningO. Furthermore, set support structure preserving: every clause(r) participating , proof datalog rule (r) support;finally, proof embeddable hence structure compatible(cf. Section 2.3).2Lemma 21. Let module setting let corresponding substitution. Let dataset arbitrary substitution constants constantscompatible . Finally, let ' (possibly empty) disjunction facts= (T, ) proof ' (O) [ D. exists non-empty set proofsP [ satisfying following properties:1. 0 2 proofproof .2 ' [ {?}. Furthermore,2'2. r 2 (r) \ supp() 6= ;, either r = ? ! exists 0 2(r) \ supp(0 ) 6= ;.3. 0 2 embeddable modulo [ .Proof. prove results suffices show(a) 2 ' exists proof 0 P [ embeddablemodulo [ ,(b) r 2 s.t. (r) \ supp() 6= ;, either r = ? ! exists 2 ' [ {?}proof 0 P [ embeddable modulo [ ,(r) \ supp(0 ) 6= ;.order able reason induction depth , proveproperties hold even ' disjunction (not necessarily function-free) ground atoms.d=0supp() = ; ' fact '( [ ) = ' 2 exists trivialproof 0 P [ '( [ ), clearly embeddable via [ .supp() 6= ; (O) must contain clause form ( ! '). Since,assumption, rule empty head ? ! , must case' 6= 2 ' exists proof 0 P [ = ( [ )depth 0 supported ( ! ) 2 (r) embeddable modulo [ .either case, properties satisfied.2. Lemma 21, well subsequent technical results, prove statements (O)rather O. cases, consider extension functional terms fyr (t)occurring (O) mapped whenever domain ; slight abuse notationsake simplicity, also refer extended substitution .519fiArmas Romero, Kaminski, Cuenca Grau, & Horrocksd>0Let = (T, ) v root w1 , . . . , wn children v. Consider clause2 (O)V ' hyperresolvent (w1 ), . . . , (wn ). Then, mustform ni=1 i0 ! '0(wi ) = _ 1 n,W' = ni=1 _ '0 MGU ,01 n.(a) Let 2 '. need find proof 0 = (T 0 , 0 ) ( [ ) P [embeddable modulo [ .2 induction hypothesisfind proof.2 '0 holds= 00 2 '0 . induction hypothesis, proof = (T , ) P [( [ ) embeddable modulo [ proper subproof .MGU i0 , = i0 , ( [ )MGU ( [ ) i0 . Indeed, = i0 implies ( [ ) = ( i0 )( [ );hand, since eect functional term f (t)depend t, ( i0 )( [ ) = ( i0 ( [ ))( ( [ )); moreover,[ extends domain constants O,0( i0 ( [ ))( ( [ )) = ( i0 )( ( [ )),Vnand0 thus 0 ( [ ) = ( )( ( [ )).hence combine ( i=1 ! ) 2 P , obtain proof( 0 )( ( [)) = ( 0 )( [) = ( [) P [D clearly also embeddablemodulo [ .(b) Consider r 2 exists r0 2 (r) \ supp(). need find2 ' [ {?} proof 0 = (T 0 , 0 ) ( [ ) P [ embeddablemodulo [ rule (r) support.Assume first r0 = r 6= ? ! . must '0 6= since,assumption, ? ! rule empty head. pickV0 2 '0 0 proof ( 0 )( [ ) P [ supported( ni=1 i0 ! 0 ) 2 (r), saw considering property (a).Assume r0 6= r 6= ? ! . must r0supports proof _ subproof . Since depth < d,i.h. must i00 2 _ [ {?} proof 00i = (T 00 , 00 ) i00 ( [ )P [ supported rule (r) embeddable .00000002 [ {?} ' [ {?} = proof looking for. =(and 6= ?) combine 00 suitable proofs j ( [ )remaining j (which know exist i.h.), before, construct proof 0P [ ( [ ) 2 ', embeddable modulo [ .Lemma 21 Proposition 1 establish datasets D0 Dr chosenensure preserves required -consequences.Suppose required preserve -consequence r = '(x) ! 9y (x, y)O. Proposition 1, given substitution mapping variables x distinct constantsc, must case [ '(c) |= 9y (c, y). Since P strengthening alsoP [ '(c) |= 9y (c, y). Assume choose D0 Dr satisfyfollowing requirements:520fiModule Extraction Expressive Ontology Languages via Datalog Reasoning1. instantiation '(c) body r must embeddable D0 ;2. set facts materialisation P [ '(c) satisfying instantiation9y (c, y) head r must embeddable Dr .completeness hyperresolution, given P datalog program, mustexist fact s.t. |= 9y (c, y) P [ '(c) ` . Lemma 21 Definition 19,aforementioned requirements D0 Dr suffice guarantee preserveconsequence r.Example 22. Consider running example Section 4 associated modulesetting = h, D0 , Dr given Example 20. see choices D0 = {D(a)}Dr = {H(a)} satisfy sufficient requirements entail ' = D(x) ! H(x).First, instantiation body D(x) ' isomorphic (and hence embeddable into)D0 . Second, materialisation P [ {D(a)} consists facts F (a), S(a, cy2 ) H(a),latter isomorphic instantiation head '; since chose Drconsist precisely H(a), second requirement also satisfied.following theorem makes precise aforementioned sufficient requirementspreserve entailment -rule r. Furthermore, shows whenever entailsr, also contains justifications r original ontology O.Theorem 23. Let r = '(x) ! 9y (x, y) rule= h, D0 , Dr module settingsubstitution mapping variables x pairwise distinctconstants, exists another substitution compatible(' ) D0 ,(( ) ) 0 [ {?} Dr substitutionP [ (' ) |= (( ) ) 0 .0mapping variables constants|= r |= r. Furthermore, O0 justification r O, O0 .Proof. Since O, follows monotonicity first-order logic |= r whenever|= r. prove opposite direction implication, suffices showO0 justification r O, O0 .= then, minimality O0 , given substitution mapping variables x freshdistinct constants, must proof (O) [ ' supp() \ (r) 6= ;r 2 O0 . assumption, exists substitution compatible(' ) D0 . Lemma 21, r 2 O0 either r = ? ! existsproof ? P [ (' ) ? supp(? ) \ (r) 6= ;. r = ? ! then, sinceassumption rule empty head ? ! , particular must alsocase |= '(x) ! ?. suffices show case also |= '(x) ! ?(as next considering 6= ): then, follows ? 2 Sig(M ) thereforer = ? ! 2 ontology. r 6= ? ! then, since (' ) D0? 2 Dr , follows r 2 .W6= assume w.l.o.g. = ni=1 > 0 conjunctionatoms. fresh predicate Q, consider ontologyOQ = {(x, y)! Q(x) | 1 n }521fiArmas Romero, Kaminski, Cuenca Grau, & Horrocksimmediate that, subset O00 O, O00 |= r O00 [ OQ |= '(x) ! Q(x).000Therefore, minimality O0 , must OQQ [ OQ0 , given substitutionjustification '(x) ! Q(x) [ OQ . minimality O0 [ OQmapping variables x fresh distinct constants, must proof Q(x)0 .(O [ OQ ) [ ' (r) \ supp() 6= ; r 2 O0 [ OQassumption, substitution compatible(' ) D0 . Since OQ contain existentially quantified variables,constants occur already O, exists module setting Q = hQ , D0Q , DrQ[ OQ Q = , therefore P Q = P [ OQ . Lemma 21,0 either = ? ! exists proof 0 P Q [ (' )2 O0 O0 [ OQeither (Q(x) ) ? s0 2 Q (s) \ supp(0 ) 6= ;.= ? ! must proof (O [ OQ ) [ ' disjunctionform ? _ (with possibly empty). Lemma 21 exists proof ? P Q [ (' )?. Furthermore, since ? mention Q, neither body ruleP Q = P [OQ , proof ? must actually proof P [(' ) . (' )s D0? 2 Dr , follows supp(? ) supp( ). Hence ? 2 Sig(M ), consequentlys=?!2M .Otherwise, 0 proof = ? then, before, 0 must proof P [ (' ) .Then, since (' )s D0 ? 2 Dr , s0 2 supp( ) thus 2 . 0proof = (Q(x) ) , let 0 = (T, ) v root w1 , . . . , wm children.0rule applied top 0 must OQ therefore dierentVm .particular, rule must form (x, y) ! Q(x), (( ) ) = j=1 (wj )extension y. Clearly, substitution 0 domain(( ) ) = (( ) ) 0 . rule s0 must thus support proof 0jP Q [ (' )s (wj ). Since r mention Q neither (wj ),0 must fact proof P [ (' ) . implies P [ (' )s |= (wj )hence assumption (wj ) 2 Dr . Finally, since (' ) D0 , s0 2 supp( )consequently 2 .6. Modules Inseparability RelationTheorem 23 tells us choose module setting corresponding modulepreserves particular consequence O. However, order -modulegiven inseparability relation S, must preserve one, (possiblyinfinitely many) relevant consequences relS (O, ) (recall Theorem 9 Section 3).section consider inseparability relation 2 {m, q, f, i}, formulatespecific module setting provably yields -module O. Later Section 11consider optimality module settingsthat is, whether may existdierent setting yields smaller module relevant inseparability relation.6.1 Implication Inseparabilityrunning example immediately suggests natural module settingguarantees implication inseparability.522= hi , D0i , DrifiModule Extraction Expressive Ontology Languages via Datalog Reasoningexample, pick substitution general possible Skolemising existentially quantified variable distinct fresh constant mapping constantsoccurring themselves. pick D0i Dri rely application sufficientconditions established Theorem 23 (quadratically many) -implicationsA(x) ! B(x) x = (x1 , . . . , xn ). precisely, capture instantiationsbody A(x) define D0 contain fact A(c1A , . . . , cnA ) involving fresh constants ciA uniquelyassociated predicate A; furthermore, capture head implication, defineDr contain fact B(c1A , . . . , cnA ). way, dataset D0i contains linearly manyDri quadratically many facts size signature .Definition 24. existentially quantified variable O, let cy fresh constant.Furthermore, 2 arity n, let cA = (c1A , . . . , cnA ) array fresh constants.module setting = hi , D0i , Dri defined follows:= { 7! cy | existentially quantified } [ { c 7! c | c 2 Ct(O) },D0i = { A(cA ) | 2 };Dri = { B(cA ) | 6= B predicates arity } [ {?}.module setting reminiscent datalog encodings typically used checkwhether concept subsumed another concept B w.r.t. lightweight ontology(Krotzsch et al., 2008b; Stefanoni et al., 2013). There, existentially quantified variablesrules also skolemised fresh constants produce datalog program P,checked whether P [ {A(a)} |= B(a).module setting captures implication inseparability straightforward consequence Theorem 23.Theorem 25. O.Proof. Consider arbitrary rule form A(x) ! B(x) x = (x1 , . . . , xn ) vectordistinct variables A, B distinct n-ary predicates . Let substitutionmapping x1 , . . . , xn distinct constants c1 , . . . , cn , another substitutionci = ciA . definition (A(x) ) 2 D0i (B(x) ) 2 Dri , thus,Theorem 23, follows |= A(x) ! B(x) |= A(x) ! B(x).6.2 Fact InseparabilityTheorem 9, fact inseparability requires preservation datalog -rules entailedO. Thus, contrast implication inseparability, may require preservationlarge (and possibly even infinite) set entailments. Unsurprisingly, module settingcannot used capture fact inseparability illustrated following example.Example 26. Consider Oex 1 = {B, C, D, H}, = {r5 r8 }. seenExample 18, = {B(a), C(a)} case Oex [ |= D(a), also[ 6|= D(a); hence, 1 -fact inseparable Oex .Equivalently, Oex entails datalog rule r3 = B(x) ^ C(x) ! D(x), whereashence Theorem 23 longer applicable since D, instantiates bodyr3 , cannot embedded D0i = {B(cB ), C(cC ), D(cD ), H(cH )}.523fiArmas Romero, Kaminski, Cuenca Grau, & HorrocksThus, next define suitable module setting f = hf , D0f , Drf capture factinseparability. previous case, exploit sufficient conditions givenTheorem 23. end, first need make sure D0f (resp. Drf ) captures possiblebody (resp. head) instantiations possible datalog rules may entailedO. achieve choosing D0f Drf constrained -datasetpossible, typically referred literature critical dataset (Marnette,2009; Cuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang, 2013).Definition 27. Let signature let fresh constant. critical -datasetdefined follows:nz }| {= { A(, . . . , ) | n-ary predicate }Indeed, straightforward see every -dataset (and hence datalog rulemapping every constant .instantiation) embeddedcase , choose substitution f general possible mappingexistentially quantified variables distinct fresh constants. However, contrast ,require constants occurring mapped rather themselves.choice justified following example.Example 28. Consider Oex = {A, C, E}. Clearly, = {A(a), C(o)}Oex [ |= E(a) due rules r2 r4 Oex . pick f , mapsconstant Oex itself, would obtain f = ; even choose D0f Drfcritical -dataset. Indeed, relevant fact E() would provable P f [ D0f .ready definefformally.Definition 29. Let constants cy Definition 24, let fresh constant.module setting f = hf , D0f , Drf defined follows:f = { 7! cy | existentially quantified } [ { c 7! | c 2 Ct(O) },,D0f =[ {?}.Drf =Example 30. datalog program generated f Oex coincides Figure 5rules except r2 , becomesr20 : A(x) ! R(x, )consider 1 = {B, C, D, H} = {B(a), C(a)} Example 26, clearlyP f [D0f ` D() 2 Drf since {B(), C()} D0f . unique proof D() P f [D0fsupported r3 ; guarantees r3 2 f thus corresponds directlyproof D(a) f [ D. Hence, f [ |= D(a), required.consider signature = {A, C, E} Example 28, observeFigure 6 choice mapping constant ensures module containsnecessary rules r2 r4 .524fiModule Extraction Expressive Ontology Languages via Datalog ReasoningE()r4E(a)r4R(a, c)r2R(, )C(o)C()r20A(a)A()(a)(b)Figure 6: Proofs (a) E(a) Oex [ {A(a), C(o)} (b) E() Pexploit Theorem 23 showTheorem 31.fff[ D0fcaptures fact inseparability.f O.Proof. Let r = ' !datalog rule . Letsubstitution mappingvariables r pairwise distinct constants, substitution mapping constantrange . definition f (' ) D0f ( ) Drf thus,Theorem 23 follows |= r f |= r. Finally, Proposition 7, impliesf f O.6.3 Query InseparabilityPositive existential queries constitute much richer query language facts allowexistentially quantified variables. Thus, query inseparability requirement inevitablyleads larger modules.Example 32. Consider Oex = {A, B}. Given -dataset = {A(a)}-query q = 9yB(y), Oex [ |= q (due rule r1 ). case, however, fempty thus f [ 6|= q. Indeed, additional facts materialisationP f [ {A(), B()} R(, cy1 ) B(cy1 ), hence neither r10 r100 (cf. Figure 5)constrained enoughsupp( f ). suggests that, although critical -datasetembed every -dataset, may need consider additional relevant facts captureproofs -queries. particular, rule r1 implies B non-empty extensionwhenever does: dependency checked q. capturedconsidering fact B(cy1 ) relevant, case r1 would included module.Theorem 9, query inseparability requires preservation -rules entailed(and datalog). particular, first-order rules may involveexistentially quantified variables, correspond framework Skolem constants.naturally suggests module setting q diers f -facts involvingSkolem constants (and mentioning ) also considered relevant.Definition 33. Let constants cy Definition 29. define module settingqqqq = h , D0 , Dr follows:q = f ,525fiArmas Romero, Kaminski, Cuenca Grau, & HorrocksB(fyr11 (a))B(cy1 )r100r1,2A(a)A()(a)(b)Figure 7: Proofs (a) B(fyr11 (a)) Oex [ {A(a)} (b) B(cy1 ) Pq[ D0q,D0q =Drq = { A(a1 , . . . , ) | 2 , aj either cy } [ {?}.Example 34. Coming back Example 32, observe Figure 7 proofB(fyr11 (a)) (Oex )[{A(a)} supported r1,2 : A(x) ! B(fyr11 (x)) recoveredproof B(cy1 ) P q [ D0q supported r100 . definition q ensuresB(cy1 ) 2 Drq , hence r1 2 q .Theorem 35.qq O.Proof. Let r = ' ! 9y rule non-empty head. Let substitutionmapping variables r pairwise distinct constants substitution maps0 Dqconstant range . definition q (' ) D0q , alsorsubstitution 0 mapping variables r constants Ct(D0q [ Drq ) [ range(q ).Thus, Theorem 23, follows |= r q |= r. Proposition 7, impliesq q O.6.4 Model InseparabilityModel inseparability diers substantially previous inseparability relations:rather preservation rule-shaped consequences, requires preservationmodels (and hence second-order consequences). Theorem 23, repeatedlyexploited show modules preserve required entailments, relies properties hyperresolution (a first-order logic calculus); hence, applicable showpreservation second-order logic consequences. particular, following exampleillustrates, modules generated q may -model inseparable O.Example 36. Consider Oex = {A, C, D, R}, case q = {r1 , r2 }.saw Example 6, interpretation = {a, o}, AI = {a}, B = C = {o},DI = ; RI = {(a, o)} model q ; however, readily checkedcannot extended model without reinterpreting A, C, R.Intuitively, constructing model O, fixing interpretation certain predicates restricts ways remaining predicates interpreted. restrictions obviously determined dependencies introduced rules O.capture model inseparability, need ensure preserves relevant dependencies predicates . this, pick D0 way model526fiModule Extraction Expressive Ontology Languages via Datalog ReasoningR(, )R(, )A()A()r1D()r3r20B()C()r1A()Figure 8: proofs P[ D0m facts Drm Example 39.embedded materialisation P [ D0 ; turn, choose Drway captures -reducts models. case, proofsP [ D0 facts Dr capture dependencies predicates .Example 37. Note Example 36 cannot embedded materialisationP q [ D0q since facts {B, C} contains C() B(cy1 ),constant cannot mapped cy1 .; now, contrastcapture models O, pick D0 =q,choose substitution maps existentially quantified variables constants. Furthermore, ensure Dr captures -reducts models,well.pick DrDefinition 38. module setting= hm , D0m , Drm follows:= { 7! | existentially quantified } [ { c 7! | c 2 Ct(O) },,D0m =[ {?}.Drm =Example 39. Consider Oex , Example 36. substitution mapsexistentially quantified variables r1 r6 . Thus, rules r1 r6 Oex correspondfollowing rules P :r1r6r1 : A(x) ! R(x, ), r1 : A(x) ! B()r6 : F (x) ! S(x, )materialisation P [ D0m contains facts A(), C(), D(), R(, ), B(). Consequently, possible embed interpretation aforementioned materialisation mapping .Figure 8 shows (non-trivial) proofs P [ D0m facts Drm . observemodule consists rules r1 r3 . Clearly, model extendedmodel Oex since predicates occur head rule Oex \M .Theorem 40 shows module -model inseparable O. Indeed, everymodel extended model following way: (i) predicatesoccurring materialisation P [ D0m interpreted empty, (ii) predicatessupport (and hence occurring ) interpreted I, (iii)predicates arity n interpreted ( )n .527fiArmas Romero, Kaminski, Cuenca Grau, & HorrocksTheorem 40.O.Proof. Let model . W.l.o.g., assume defined Sig(O).Let J interpretation domain I,82 [ Sig(supp( ))<Jarity(A)=2 Sig(P (D0m )) \ ( [ Sig(supp( ))):;otherwiseNote [ Sig(supp( )) Sig(P (D0m )).Consider r : ' ! 2 O. show J |= r.Assume first = . r = ? ! need check ? interpretedfalse. ? 2/ Sig(P (D0m )) case definition J . ? 2 Sig(P (D0m ))then, since ? 2 Drm , must ? 2 Sig(supp( )) therefore also ? 2 Sig(M ).implies ? ! 2 thus, since model , must case? interpreted false. Since ? 2 Sig(supp( )), definition J?J ?I false.Assume 6= . Sig( ) 6 Sig(P (D0m )) then, P [ D0m mentionsone constant (namely, ), follows also Sig(') 6 Sig(P (D0m )) therefore 'J = ;J |= r. Hence, following assume Sig( ) Sig(P (D0m )).Sig( ) \ ( [ Sig(supp( ))) = ;, AJ = arity(A) 2 Sig( )immediate J |= r. Otherwise suppose exists substitutionvariables r J |= ' (if substitution exists J |= r holds trivially).'J 6= ; must Sig(') Sig(P (D0m )). Let substitution mapsvariables ; constant P [D0m , folows ' P (D0m )(D ).thus alsoP0assumption, exists = A(, . . . , ) 22 [Sig(supp( )).' P (D0m ), proof ,r P [ D0m supported rule(r). 2 then, definition , 2 Dr consequently r 2 .If, hand, 2/ , must 0 2 Drm proof 0 0 P [ D0mproof A(, . . . , ) subproof 0 . Replacing subproof A,rresults another proof 0 P [ D0m supported rule (r). Thusr 2 case well. Rules (r) body r, r 2 impliesSig(') Sig(supp( )), thus J agree Sig('). assumption, J |= ' ,(D ),|= ' well, and, since r 2 , also |=. Finally, sinceP0JSig( ) Sig(P (D0 )) therefore, J |=. Sincearbitrary, conclude J |= r.modules generated similar spirit locality-based modulescertain symbols outside signature module interpreted either empty setuniversal relation (of relevant arity) interpretation domain.show later on, M?[O,] whenever normalised SROIQ ontology. However,illustrated following example, modules incomparable >- ?> -modules.Example 41. = Oex = {D, F } following:= {r5 }M>[O,] = {r1 -r3 }528M?>[O,] = ;.fiModule Extraction Expressive Ontology Languages via Datalog Reasoning?>Consequently, neither contained M>[O,] , M[O,] . see conversealso true, consider = {r1 , r9 }, r9 = C(x) ! B(x), = {A, C, R}. Then,following:?>M>[O,] = M[O,] == {r1 }already mentioned modules generated included locality?-modules. conclude section, show modified precisely capture?-modules. this, suffices modify making Dr critical datasetentire signature (instead ) given following definition.Definition 42. module settingb= hb , D0b , Drb follows:b = ,,D0b =Drb = DSig(O)[ {?}.following proposition showsSROIQ ontology relative .bcoincides ?-locality moduleProposition 43. normalised SROIQ ontology,b= M?[O,] .Proof. definition, M?[O,] smallest subset every axiom \b?-local w.r.t. [ Sig(M). show M?[O,] , suffices show that,r 2 \ b , r ?-local w.r.t. [ Sig(M b ). Consider r 2 \ b . Since Drb containsfacts occur P b (D0b ), supp( b ) consists rules supportproof P b [D0b . Furthermore, Sig(M b )[ = Sig(P b (D0b )). Therefore,proof P b [ D0b rule b (r) support.constant mentioned P b [ D0b , means predicate bodyr occur Sig(P b (D0b )) = Sig(supp( b )) [ . observedTables 1 2, implies r ?-local w.r.t. Sig(supp( b )) [ .0bbsee b M?[O,] , consider r 2 . must exist r 2 (r)0b?r 2 supp() proof P b [ D0 . show r 2 M[O,] , let us reasoninduction depth .= 0 body r empty thus r ?-local w.r.t. signature.follows r 2 M?[O,] .> 0 suffices consider case r0 rule applied top , sincealready know induction hypothesis that, 2 ruleb (s) support (proper) subproof , 2 M?[O,] . SincebSig(D0 ) = , implies that, = (T, ) v root w1 , . . . , wnchildren, Sig( (wi )) Sig(M?[O,] ) [ . Consequently, body rmust signature fully contained Sig(M?[O,] ) [ . follows r??-local w.r.t. Sig(M[O,] ) [ hence r 2 M?[O,] .529fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks7. Additional Inseparability Relationsbulk research module extraction focused inseparability relationsconsidered Section 6. section show framework seamlesslyadapted interesting inseparability relations.7.1 Classification InseparabilityClassificationthe problem identifying subsumption relationships pairsatomic concepts pairs atomic roles DL ontologyis fundamental reasoningtask ontology engineering. Classifying first-order ontology amounts computing,predicate O, entailed implications A(x) ! B(x) B predicatearity A, referred subsumer O.Locality ?-modules successfully exploited optimising classification DLontologies (Tsarkov & Palmisano, 2012; Cuenca Grau et al., 2010; Armas Romero et al.,2012). addition model-inseparable given ontology satisfyingproperties Proposition 15 Section 3.2, ?-modules enjoy additional propertymakes well-suited optimising classification (Cuenca Grau et al., 2007a, 2010):Proposition 44. Let ontology, signature, r rule form A(x) !2 either = ? = B(x) B 2 Sig(O). Then, |= r M?[O,] |= r.follows Proposition 44 ?-module = {A} captures subsumers O, hence indistinguishable w.r.t. implicationsbody. capture additional property ?-modules meansfollowing inseparability relation.Definition 45. Ontologies O0 -classification inseparable (O c O0 )rule r form A(x) ! , 2 either== B(x)B 2 Sig(O [ O0 ), |= r O0 |= r. Furthermore, relc function mappingontology signature following set rules:relc (O, ) = {r=A(x) !| |= r, 2=?=B(x) B 2 Sig(O)}follows straightforwardly definition c O0 holds relc (O, ) coincides relc (O0 , ); hence, Theorem 9 Section 3 trivially extends classificationinseparability. Furthermore, readily checked c ( non-trivialsignature , hence classification inseparability (as expected) stronger requirementimplication inseparability. Finally, although ?-modules ensure classification inseparability O, also model-inseparable (a much stricter requirement) and, shownSection 13, much larger necessary ontology classification.Example 46. Consider example ontology Oex . observe classification inseparability stronger requirement implication inseparability considering = {G}.Clearly, = ; -implication inseparable Oex , whereas -classification inseparability requires rule r8 contained M. see ?-modules dier minimial classification-inseparable modules consider = {A}; case,ex therefore empty ontologyM?[O ex ,] = {r1 , r2 }, subsumersexalready -classification inseparable .530fiModule Extraction Expressive Ontology Languages via Datalog Reasoningfollowing module setting extends Definition 24 capture classificationinseparability. one would naturally expect, required modification extendDri facts involving predicates outside .Definition 47. 2 arity n, let cA = (c1A , . . . , cnA ) array freshconstants. module setting c = (c , D0c , Drc ) defined follows:c = ,D0c = D0i ,Drc = { B(cA ) | 2 B 2 Sig(O) distinct predicates arity } [ {?}.Example 48. Consider = Oex = {A}. Since fact Drc provableP c [ D0c module c empty thus also minimal case.show c captures implication inseparability using argument analogousproof Theorem 25.Theorem 49.cc O.7.2 Weak Query InseparabilityOne possible applications modules based query inseparability optimisequery answering. particular, -query inseparable O, [ |= qM[D |= q -query q -dataset D; thus, replace answerarbitrary query w.r.t. arbitrary data provided symbols deemed relevant.aforementioned notion query inseparability useful situationsdata unknown frequently changing, many situations data ontologyconsidered fixed hence one could potentially extract smaller modules requiringrobust extensions arbitrary data.Botoeva, Kontchakov, Ryzhikov, Wolter, Zakharyaschev (2014) investigated restricted notion query inseparability well-suited cases dataontology considered fixed. paper, refer restricted notionweak query inseparability.0Definition 50. Ontologies O0 -weak query inseparable (O wq)Boolean PEQ q |= q O0 |= q. Furthermore, relwq functionmapping ontology signature setrelwq (O, ) = { q | |= q q Boolean PEQ Sig(q) }0Again, Theorem 9 extends naturally weak query inseparability; indeed, wqrelwq (O, ) coincides relwq (O0 , ). Moreover, requirements Definition 50 weaker query inseparability hence q ( wq.result, weak query inseparability may yield smaller modules, show Section 13.next propose module setting wq captures weak query inseparability.main dierence wq q Section 6.3 initial dataset D0wq chosen531fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks. natural choice given longer requirementempty ratherarbitrary -datasets must embeddable D0wq . Furthermore, wq diers qmaps constants Ct(O) (same ).Definition 51. Let constants cy , existentially quantified variable O,Definition 24. module setting wq = hwq , D0wq , Drwq defined follows:wq = ,D0wq = ;,Drwq = { A(a1 , . . . , ) | 2 , aj either Ct(O) equals cy } [ {?}.Example 52. Consider extension Oex fact ! D(i) (seen ground rule)signature = {B, C, D, H}. readily checked q = {r3 , r5 r8 },whereas wq = {r5 r8 }.Theorem 53.wqwqO.Proof. Boolean PEQ q seen rule ( ! q). Consequently, wq wqfollows Theorem 23 similar argument proof Theorem 35.8. Module ContainmentIntuitively, expressive language preservation consequencesrequired, larger modules need be. instance, since f ( ,expected module obtained implication inseparability containedmodule f fact inseparability. next show modules Sections 6 7consistent intuition.first step introduce notion homomorphism module settings,allow us establish containment corresponding modules.Definition 54. module setting= h, D0 , Dr i, let Ct( ) denote set constants occurring D0 , Dr , range . substitution : Ct( ) ! Ct( 0 )homomorphism 0 following conditions hold:= 0 ;D0 D00 ;Dr Dr0 .write,!0denote homomorphism0exists.fact ,! 0 (witnessed homomorphism ) implies 0general , sense proof P [ D0 fact Dr embedded0(via ) support-preserving way proof P [ D00 fact Dr0 . follows0supp( ) contained supp( 0 ) (modulo ) hence also .Theorem 55. ,0s.t.,!0,0.532fiModule Extraction Expressive Ontology Languages via Datalog ReasoningProof. Let = h, D0 , Dr 0 = h0 , D00 , Dr0 i, let : Ct( ) ! Ct( 0 ) homomorphism 0 . Since Dr Dr0 , suffices show rule r 2proof P [ D0 fact 2 Dr supp() \ (r) 6= ;, exists proof 000P [ D00 supp(0 ) \ (r) 6= ;. prove following general claim.Let r rule proof fact (not necessarily Dr ) P [ D0supp() \ (r) 6= ;. show induction depth proof000 P [ D00 supp(0 ) \ (r) 6= ;.d=0Since, assumption, supp() \ (r) 6= ;, r = ( ! ) ( ! ) 2 (r)0= 2 , also ( ! 0 ) 2 (r). Since definedconstants O, holds ( ) = () = 0 , hence proof00P [ D00 supported rule (r).d>0Let = (T, ) v root w1 , . . . , wn children v. Letrule used derive (v) (w1 ), . . . , (wn ). Finally, letsubproof (wi ). supp(i ) \ (r) 6= ;, claim follows0induction hypothesis since 2 P . Otherwise, 2 (r). Moreover,0induction hypothesis, every (wi ) proof P [ D00 , claim follows0since 2 (r).straightforward construct homomorphisms module settings Sections 6 7 accordance containment relationship corresponding inseparability relations. following result, establishes intuitive relationshipsmodules, follows immediately Theorem 55.Corollary 56.fwqqcbbqalready illustrated examples throughout Sections 6 7, containmentrelations strict many . Furthermore, easily checkedcontainment relations complete, sense module settings unrelatedCorollary 56 (e.g., f wq ) incomparable.9. Depletingness, Self-Containment, Justification-Preservationminimal requirement module preserve relevant consequences w.r.t.given inseparability relation. argued Section 3, however, applicationsdesirable modules satisfy additional properties. section, establish whethermodules Sections 6 7 satisfy (strong) depletingness, self-containment,justification-preservation properties enjoyed locality-based modules.establish results, convenient abstract away notion modulesetting fixed consider instead families module settings; is,functions assign module setting pair .533fiArmas Romero, Kaminski, Cuenca Grau, & HorrocksDefinition 57. module setting family functionmaps pair ontologysignature module setting . Given inseparability relationS, sayS-admissible if, pair , (O,) -moduleO. Furthermore, saydepleting (resp. strongly depleting, self-contained,(O,)justification-preserving).Finally, 2 {m, q, f, i, c, wq}, denote (S-admissible) familyinduced module setting defined Sections 6 7.9.1 Depletingness Justification-Preservationdiscussed Section 3, depletingness ensures \ inseparableempty ontology hence relevant information left behind extracting M.illustrated following example, modules depleting.Example 58. Consider consisting following rules let = {A, B}:s1 = A(x) ! B(x) ^ C(x)s2 = A(x) ! D(x) ^ E(x)s3 = D(x) ! B(x)Clearly, M1 = {s1 } M2 = {s2 , s3 } implication-inseparable hence-modules. However, neither depleting.next show modules defined Sections 6 7 depleting.Proposition 59.depleting 2 {m, q, f, i, c, wq}.Proof. Let arbitrary let = (O, ). Theorems 25, 31, 35, 40,49 53, suffices show (O\M, ) = ;. definition module (cf. Definition 19), holds (O\M, ) O\M. Furthermore, since O\M O, follows(O\M, ) M. Consequently, (O\M, ) = ;.next show modules also justification-preserving hence seamlessly exploited ontology debugging applications.Proposition 60.justification-preserving 2 {m, q, f, i, c, wq}.Proof. Let 2 relS (O, ) let O0 justification O. need checkO0 (O,) . Since 2 relS (O, ) O0 |= , follows definition relS02 relS (O0 , ). Theorems 25, 31, 35, 40, 49 53, (O ,) O0 ,0therefore (O ,) |= . definition module (cf. Definition 19) immediate0O0 implies (O ,) (O,) . Finally, minimality O0 ,0(O ,) = O0 therefore O0 (O,) .conclude addressing eect normalisation properties. Similarlytreatment normalisation Proposition 13 Section 3, showrecover depleting justification-preserving module SROIQ ontology onemodule normalisation norm(O).Proposition 61. Let inseparability relation, letmodule setting familyS-admissible depleting. Let norm normalisation function. LetSROIQ ontology let following holds:534fiModule Extraction Expressive Ontology Languages via Datalog Reasoning1.(norm(O),)norm(M)2. norm(O\M) norm(O)\norm(M).Then, depleting justification-preserving -module O.Proof. Let O0 = norm(O). Proposition 13 implies -module O0 .0next show depleting. Since (O ,) norm(M), O0 \norm(M)00O0 \M (O ,) . Proposition 59 implies O0 \M (O ,) ;, hence monotonicityfirst-order logic also O0 \norm(M) = norm(O\M) ;. Since norm(O\M) conservativeextension O\M, Definition 4 norm(O\M) O\M, thus O\M ;.show justification-preserving, let ' 2 relS (O, ) consider justification ' O. Suppose 2 O\M. 2 O\M0norm() norm(O\M) = O0 \norm(M). Since (O ,) norm(M), implies0norm() \ (O ,) = ;. hand, since norm(O) conservative extensionO, ' 2 relS (norm(O), ). Also, norm(O) conservative extensionO, norm(O) |= ' must justification O0 ' norm(O) O0 .00Proposition 60, (O ,) justification-preserving, consequently O0 (O ,) .Furthermore, minimality proper subset whose normalisation in0cludes O0 . follows norm() \ O0 6= ; hence norm() \ (O ,) 6= ;.contradiction stems assuming 2 \ M. Therefore M, i.e.,justification-preserving.9.2 Self-Containment Strong Depletingnesscontrast locality-based modules, modules obtained using approach neitherstrongly depleting, self-contained. see this, consider following example.Example 62. Let = {A, D} = {r10 r15 }r10r11r12r13r14r15======( ! A(o))A(x) ! 9y.[R(x, y) ^ B(y)]A(x) ! 9y.[R(x, y) ^ C(y)]R(x, y) ! D(x)B(x) ! C(x)( ! C(i))Let M1 = {r10 r13 } M2 = {r11 r13 }. checkf=M=Mqc=M= M2=Mwq= M1Clearly, |= C(i) wq 6|= C(i) C signature M1 ; hence, wqself-contained. Furthermore, wq strongly depleting since \M wq |= C(i).remaining inseparability relations, observe |= B(x) ! C(x) Mi 6|= B(x) ! C(x)1 2. Since B C signatures M1 M2 followsnone modules self-contained (note implications relevant consequencesrelations wq). Furthermore, since also \ Mi |= B(x) ! C(x),conclude modules also strongly depleting.535fiArmas Romero, Kaminski, Cuenca Grau, & HorrocksSelf-containment strong depletingness always needed applications. Thus,fact modules satisfy default beneficial may allowus compute smaller modules.However, mentioned Section 3, properties useful certain ontologyreuse scenarios. next show framework adapted satisfyproperties whenever required. achieved via fixpoint constructionmodules computed w.r.t. iterative extensions initial signature. fixpointconstructions reminiscent standard algorithms computing locality modules(Cuenca Grau et al., 2007a, 2008).Definition 63. Let module setting family inseparability relation S.define family Sself function mapping least fixpointsequence {Mi }i 0 defined next:0 ==1[ Sig(Mi1)Mi => 0(O,i)0aforementioned fixpoint well-defined: since [ Sig(O) 0[ Sig(O) finite, must i0 0 i0 = j Mi0 = Mjj > i0 . show that, adaptation, modules satisfy required properties.Proposition 64.selfself-contained strongly depleting 2 {m, q, f, i, c, wq}.Proof. Let = Sself (O, ). immediate self-contained -module O.Strong depletingness follows Proposition 59 2 {m, q, f, i, c, wq}.construction Definition 63 straightforwardly adapted case nonnormalised SROIQ ontologies following approach Proposition 61.10. Complexity Module Extractionsection, argue modules efficiently computed many practicallyrelevant cases. this, analyse complexity following decision problem.Definition 65. Let L class ontologies let 2 {m, q, f, i, c, b, wq} inseparability relation. decision problem isInModule[L,S] follows:Input: ontology 2 L, signature rule r 2 O.Output: True r 2(O,).Furthermore, consider following classes ontologies, strongly connected DL-based ontology languages.Definition 66. Let k fixed non-negative integer. class Lkarity consistsontologies predicates arity k.graph conjunction atoms ' undirected graph G' = (V, E) Vset variables occurring ', E contains edge pair variables536fiModule Extraction Expressive Ontology Languages via Datalog Reasoningoccur together atom '. tree decomposition G' tree = (W, F ),exists labelling mapping vertex w 2 W subset (w) V ,following conditions satisfied:v 2 V , exists v 2 W v 2 (w),{v, v 0 } 2 E, exists w 2 W {v, v 0 } (w),v 2 V , set { w 2 W | v 2 (w) } induces (connected) subtree .width tree decomposition maxw2W (| (w)| 1). treewidth 'minimum width tree decompositions G' . treewidth rule definedtreewidth body. Finally, class Lktw consists ontologies ruletreewidth k.rules correspong SROIQ ontologies fixed predicate arity,also bodies tree-shaped (see Section 2.2). latter implies rules stemmingSROIQ ontologies treewidth one.already discussed, appealing feature approach module extractiondelegated o-the-shelf datalog reasoner, regardless languageontologies expressed. following proposition establishes datalog program initial dataset exploited approach polynomial size; furthermore,datalog transformation definition module setting (see Definition 19Section 5) alter shape rules original ontology significant way.Proposition 67. Let ontology Sig(O) signature. Furthermore, let2 {m, q, f, i, c, b, wq} (O, ) = = h, D0 , Dr i. Then, P D0 size linear|O|. Furthermore, Lkarity (resp. Lktw ) fixed k, P .Proof. clear Definition 5 (r) contains datalog rule atomhead r. Thus, P clearly size linear w.r.t. |O|. Furthermore, D0 contains one factpredicate , since Sig(O), follows D0 also size linear w.r.t.|O|. Finally, transformation increase arity predicates bodyrule (r) coincides r hence preserves treewidth.computational properties datalog programs bounded arity and/or treewidthwell-understood: fact entailment NP-complete combined complexity programsbounded arity, complexity drops PTime additionally restrictprograms bounded treewidth. Bounded arity predicates implies correspondingmaterialisation polynomially bounded size, thus computed polynomialnumber steps (i.e., applications immediate consequence operator); furthermore,bounded treewidth rule bodies implies step performed polynomialtime (Grohe, Schwentick, & Segoufin, 2001; Chekuri & Rajaraman, 2000).complexity module extraction, however, determined datalogreasoning, also complexity computing support proofs involvedrelevant entailment.following theorem, proved Zhou et al. (2014), establishes computingsupport also reduced standard datalog reasoning. Given program P, dataset537fiArmas Romero, Kaminski, Cuenca Grau, & HorrocksD, set facts F , main idea extend P additional rulesfacts responsible computing support proofs facts F P [ D.support recorded means fresh predicates: auxiliary predicates Q usedrecord relevant facts form Q(c); furthermore, rule r 2 P representedfresh constant dr , fresh unary predicate Rel used capture relevant rulesP support.Theorem 68 (Zhou et al., 2014). Let P datalog program, let dataset,let F set facts materialisation P [ D. Let Rel fresh unary predicateand, predicate Q occurring P [ D, let Q fresh predicate arity.Furthermore, let dr fresh constant r 2 P.Let (D, F ) dataset (D, F ) = [ { P (c) | P (c) 2 F } let (P)smallestV datalog program including P containing following rulesr=j=1 B1 (xj ) ! H(x) P:H(x) ^ B1 (x1 ) ^ . . . ^ Bm (xm ) !H(x) ^ B1 (x1 ) ^ . . . ^ Bm (xm ) !Rel(dr )Bj (x1 )...H(x) ^ B1 (x1 ) ^ . . . ^ Bm (xm ) ! Bj (xm )Then, rule 2 P support proof P [ fact F Rel(ds )materialisation (P) [ (D, F ).datalog program (P) (resp. dataset (D, F )) Theorem 68 size polynomial |P| (resp. |D| |F |) use predicates arity greaterpredicates used P. However, (P) may larger treewidth P. nextargue case SROIQ ontologies increase treewidth bounded.Proposition 69. Let normalised SROIQ ontology Sig(O) signature.Furthermore, let module setting . (P ) treewidth 2.Proof. rule r 2 whose head formed (one more) atomsunary, mention one variable, straightforward ( (r)) stillconsists rules tree-shaped bodies. r 2 mentiontwo variables, also straightforward ( (r)) also consists rulestree-shaped bodies. Finally, r mentions two variables, head containsatoms mention one variable, r must one following forms:VWA(x) ^ m+1i=1 [R(x, yi ) ^ B(yi )] !i6=j yi yjThen, bodies rules ( (r)) formA(x) ^m+1^i=1[R(x, yi ) ^ B(yi )] ^ yi1 yi21 1 < i2Hence, treewidth 2 due cycle length 3 formed R(x, yi1 ), R(x, yi2 )yi1 yi2 .538fiModule Extraction Expressive Ontology Languages via Datalog ReasoningR1 (x, y) ^ R2 (y, z) ! S(x, z)body single rule ( (r)) form R1 (x, y)^R2 (y, z)^S(x, z),treewidth 2 due cycle length 3 formed three atoms.cases involving equality body rules, namely x ^ z ! x z,A(x1 , x2 ) ^ x1 ! A(y, x2 ), A(x1 , x2 ) ^ x2 ! A(x1 , y), analogousprevious case.following theorem establishes two practically relevant cases module extraction framework performed polynomial time. first show modulesensuring model-inseparability computable polynomial time arbitrary ontologies signatures. Then, establish modules remaining inseparabilityrelations considered paper also computable polynomial time classesontologies whose extended datalog program (P ) Definition 68 boundedpredicate arity treewidth.Theorem 70. Let L class ontologies, 2 {m, q, f, i, c, wq}, LS,class datalog programs:LS, = { (P(O,)following) | 2 L, Sig(O) }problem isInModule[L,S] decidable polynomial time either following conditions satisfied:= m,0LS, Lkarity \ Lktw fixed non-negative integers k k 0 .Proof. Consider arbitrary ontology signature . Let (O, ) = h, D0 , DrP = P (O,) , F = Dr \P(D0 ). Proposition 67, P D0 computed timepolynomial size O, (P). Therefore, suffices show (D0 , F )obtained polynomial time materialisation (P) [ (D0 , F ).= Dr (D0 , F ) computed linear time. Furthermore, easysee computing materialisation (P) [ (D0 , F ) also feasible linear timesince boils propositional datalog reasoning.00Consider 2 {q, f, i, c, wq}. Note (P) 2 Lkarity \Lktw implies P 2 Lkarity \Lktw .Hence, P(D0 ) computed time polynomial size O. Since F alwaysset facts P(D0 ) predicates , computed polynomial time.Moreover, dataset (D0 , F ) computed polynomial time well thus,0since (P) 2 Lkarity \ Lktw , materialisation (P) [ (D0 , F ).Tractability module extraction w.r.t. SROIQ ontologies immediate consequence Theorem 70 Proposition 69.Corollary 71. Let 2 {m, q, f, i, c, wq} let normalised SROIQ ontology.module computable polynomial time.539fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks11. Optimalityalready discussed, general, modules minimal correspondinginseparability relation. is, however, interest determine module setting familiesyield smallest possible modules given inseparability relation within limitsframework. end, next present study suitable notion optimalityapplicable module setting families.notion module setting family Definition 57 rather generalestablish relationship dierent module settings family. order study optimality, makes sense restrict families satisfying certainuniformity conditions. Roughly speaking, consider family uniform (i) existentially quantified variables constants ontologies treated homogeneously withinsetting (i.e., dierent existential variables receive treatment, dierentconstants) well consistently across dierent settings; (ii) signatures treatedmonotonically across settings (i.e., 0 members family ontologysignatures 0 0 , treat predicates Sig(O)\0exactly way).Definition 72. module setting family uniform if, pair ontologies O, O0signatures , 0 , module settings (O, ) = h, D0 , Dr (O0 , 0 ) = h0 , D00 , Dr0satisfy following properties, Ex(F) denotes set existentially quantifiedvariables F:1. = 0 , |Ct(O)| |Ct(O0 )| |Ex(O)| |Ex(O0 )|, exists injectivesubstitution : dom() ! dom(0 ) mapping variables variables constantsconstants= 0 ,D0 = { A(c) | A(c) 2 D00 , c constants Ct( (O, )) },Dr = { A(c) | A(c) 2 Dr0 , c constants Ct( (O, )) }.2. = O0 0 ,= 0 ,D0 = { A(c) 2 D00 | 2 },{ A(c) 2 Dr | 2 } = { A(c) 2 Dr0 | 2 },{ A(c) 2 Dr | 2 Sig(O)\0 } = { A(c) 2 Dr0 | 2 Sig(O)\0 }.inseparability relation, let class uniform module setting families0S-admissible. say S-optimal 2 (O,) (O,)every 0 2 pair .easy see S-admissible families , 2 {m, q, f, i, c, wq},uniform. Furthermore, following theorem establishes , , c , wqalso optimal respective inseparability relations. proof theorem rathertechnical deferred appendix.540fiModule Extraction Expressive Ontology Languages via Datalog ReasoningTheorem 73. familyS-optimal 2 {m, i, c, wq}.contrast, families f q fact query inseparability optimal.see this, consider ontology consisting following rules:A(x) ! B(x)B(x) ! A(x)C(x) ! D(x)Furthermore, let = {A, C, D}. module setting f = f (O, ) yields f = O.Indeed, = {A(a)} non-trivial proof A(a) [ involves rulesA(x) ! B(x) B(x) ! A(x), included module. However,clear = {C(x) ! D(x)} already -fact inseparable O.define dierent setting = h, D0 , Dr i, whose corresponding module precisely M. this, idea define D0 Dr way aforementionedproofs tautological statements avoided. Consider D0 Dr follows:D0 = {X(c0Y ) | X, 2 } [ {X(c1Y ) | X, 2 X 6= }Dr = {Y (c1Y ) | 2 }Datasets D0 Dr disjoint, guarantees proofs -tautologies takenaccount therefore indeed M. construction D0 Dr givenexample ontology signature generalised define uniform module settingfamily provides counter-example optimality f . is, however, pricepay smaller modules, namely increase size module settings. Indeed,(O, ) size exponential , whereas size f (O, ) remains polynomial.exponential blowup clearly undesirable practice.following theorem establishes f q optimal. do, however,work well practice, shown evaluation presented Section 13. prooftheorem works proposing better module setting families which, cases, incuraforementioned exponential blowup. conjecture blowup unavoidableoptimal module setting family fact query inseparability (if family exists).case Theorem 73, proof technical deferred appendix.Theorem 74. familyS-optimal 2 {f, q}.12. Related WorkModule extraction received great deal attention literature. Section 12.1discuss complexity inseparability checking dierent ontology languagesinseparability relations. Section 12.2 recapitulate existing module extraction techniques based inseparability, provide brief overview practical applications.Finally, Section 12.3 discuss number related problems, forgetting, uniforminterpolation, partition-based reasoning.12.1 Inseparability RelationsInseparability relations originate notions model deductive conservative extensions description modal logics (Antoniou & Kehagias, 2000; Ghilardi, Lutz, &541fiArmas Romero, Kaminski, Cuenca Grau, & HorrocksWolter, 2006a; Ghilardi, Lutz, Wolter, & Zakharyaschev, 2006b), constitute foundation module extraction techniques (Konev et al., 2009).Model inseparability undecidable DLs extend EL (Lutz & Wolter, 2010).is, however, tractable ELI ontologies acyclic (Konev et al., 2013); furthermore,coNexpTimeNP -complete description logic ALCI signatures restrictedNconsist atomic concepts (Konev et al., 2013). DL-LiteNbool DL-LitehorncoNexpTime-hard (Kontchakov et al., 2010), matching upper bound knownbest knowledge.complexity query inseparability studied mainly lightweight description logics. ExpTime-complete EL (Lutz & Wolter, 2010), p2 -completeNcoNP-complete DL-LiteNbool DL-Litehorn , respectively (Kontchakov et al., 2010),HExpTime-complete DL-LiteHcore DL-Litehorn (Konev, Kontchakov, Ludwig,Schneider, Wolter, & Zakharyaschev, 2011; Botoeva et al., 2014). Baader et al. (2010)considered variant query inseparability signature datasets restrictedsignature queries not, identified decidable sufficient conditionsinseparability ELI, checked polynomial time EL. complexityweak query inseparability (see Section 7.2) studied Botoeva et al. (2014);known P-complete DL-Litecore , DL-Litehorn ELH, ExpTime-completeHDL-LiteHcore DL-Litehorn , 2ExpTime-complete Horn-ALCHI Horn-ALCI.complexity implication classification inseparability coincidesstandard reasoning tasks subsumption checking (Konev et al., 2009). description logic literature implication inseparability studied general form:given L-ontologies O0 signature , problem determine whetherO0 entail concept inclusion axioms C v C (possibly complex) L-concepts . setting, inseparability found p2 -completeNDL-LiteNbool , coNP-complete DL-Litehorn (Kontchakov et al., 2010), ExpTime-completeEL (Lutz & Wolter, 2010), 2ExpTime-complete ALC (Ghilardi et al., 2006a), ALCI,ALCQ ALCQI (Konev et al., 2009), undecidable ALCQIO (Konev et al., 2009).Note variant implication inseparability highly dependent ontology language, whereas results largely logic-independent. leave investigationinseparability relations within framework interesting problem future work.12.2 Module ExtractionPractical module extraction techniques typically based approximations, ensurecomputed module (model) inseparable given ontology, yet necessarily minimal. One approximation, discussed detail Section 3, basedsyntactic locality (Cuenca Grau et al., 2007a, 2008; Sattler et al., 2009). implementation ?-, >- ?> -module extraction integrated OWL API (Horridge &Bechhofer, 2011), alternative implementation downloaded separate Javalibrary.3 semantic counterpart syntactic locality, semantic locality, proposedwork Cuenca Grau et al. (2007b). Deciding semantic locality is, given DL,hard checking satisfiability w.r.t. empty TBox, hence tractable logics restricted expressivity. reason modules based syntactic locality,3. https://www.cs.ox.ac.uk/isg/tools/ModuleExtractor/542fiModule Extraction Expressive Ontology Languages via Datalog Reasoningextracted polynomial time, preferred choice practice. Furthermore, exhaustive comparison syntactic semantic locality modules revealeddierence significant practical cases (Del Vescovo et al.,2013).Reachability-based modules (Suntisrivaraporn, 2008; Nortje, Britz, & Meyer, 2012;Nortje et al., 2013) refinement syntactic locality modules. Availablethree flavours (?-, >- ?> -reachability), also computed polynomial time.?-reachability modules coincide ?-modules, >- ?> -reachability modulesgenerally subsets syntactic locality counterparts. refinement comescost losing depletingness, although reachability modules still self-containedpreserve justifications consequences reference signature .Konev et al. (2013) Gatens et al. (2014) developed module extraction techniquesacyclic ELI acyclic ALCQI, respectively. techniques ensure modulesself-contained depleting, case ELI also minimal. polynomialalgorithm ELI implemented system MEX, general, non-tractablealgorithm ALCQI implemented system AMEX. contrast localityreachability modules, applicability techniques limited relatively restrictedclass ontologies, tractability guaranteed even restricted class.Kontchakov et al. (2010) exploited decidability query inseparability DL-LiteNboolDL-LiteNhorn module extraction. techniques yield minimal minimal depleting modules complexity corresponding inseparability relation(p2 -complete coNP-complete, respectively).Baader et al. (2010) proposed exponential-time algorithms extracting modulesELI ontologies preserve variant query inseparability. Furthermore, showedcomputing modules feasible polynomial time EL ontologies.Recently, Rousset Ulliana (2015) studied modularity context deductivetriple stores, is, RDF triple stores equipped set datalog rules. preservationproperties modules, however, dierent ones considered work.Del Vescovo et al. (2011) considered problem finding polynomial representationmodules ontology, particular notion module. proposed representation called atomic decomposition applicable notion module satisfiescertain properties include self-containment depletingness. atomic decomposition ontology suitable notion module computed polynomial timeusing module extraction algorithm oracle.Module extraction identified key task support knowledge reuse (CuencaGrau et al., 2008; Jimenez-Ruiz et al., 2008). Modules also exploited optimiseontology matching (Jimenez-Ruiz & Cuenca Grau, 2011) computation justifications (Suntisrivaraporn et al., 2008; Ludwig, 2014) ontology debugging explanation.Finally, module extraction techniques applied optimising ontology classification(Armas Romero et al., 2012; Tsarkov & Palmisano, 2012; Suntisrivaraporn, 2008; CuencaGrau et al., 2010) integrated reasoners Chainsaw.543fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks12.3 Related ProblemsModule extraction strongly related notions forgetting (uniform) interpolation(Eiter, Ianni, Schindlauer, Tompits, & Wang, 2006; Ludwig & Konev, 2014; Koopmann &Schmidt, 2014; Konev, Walther, & Wolter, 2009; Nikitina & Rudolph, 2014; Wang, Wang,Topor, & Pan, 2010). uniform interpolant L-ontology signatureL-ontology O0 mentions symbols inseparable w.r.t.given inseparability relation. contrast modules, uniform interpolantsrequired subsets cannot contain symbol outside (all remainingsymbols thus forgotten). latter requirement implies uniform interpolantsgiven may always exist (Konev et al., 2009; Lutz & Wolter, 2011; Wang,Wang, Topor, Pan, & Antoniou, 2014).Amir McIlraith (2005) investigated partition-based reasoning techniques propositional first-order logic, goal improve efficiency reasoningknowledge base first dividing axioms related partitions. topologypartitions described means graph, nodes represent partitions edgetwo partitions labelled symbols common. graphstructure exploited distributed message-passing algorithm, correctnessensured Craigs interpolation theorem first-order logic. Similar problemsreasoning techniques studied context description logics Konev, Lutz,Ponomaryov, Wolter (2010) well Schlicht Stuckenschmidt (2009). keyconcern partition-based reasoning find partitioning exhibits suitable balance number partitions, size, number common symbolspartitions order enable efficient distributed reasoning. setting, partitionsnecessarily capture meaning given signature input knowledge basetherefore fundamentally dierent modules.Konev, Ludwig, Walther, Wolter (2012) studied problem computing logicaldierence ontologies O0 is, set queries receive dierent answersw.r.t. O0 . Computing logical dierence (or concise representation thereof)identified valuable resource ontology versioning tasks (Jimenez-Ruiz, CuencaGrau, Horrocks, & Berlanga Llavori, 2011) closely related inseparability checking;indeed, inseparable ontologies empty dierence.Finally, Zhou et al. (2014) proposed hybrid approach ontology-based query answering bulk computation delegated datalog reasoner. Given ontologyO, dataset , query q(x), candidate answer tuple c, core technique approachcompute fragments O0 D0 [ |= q(c) O0 [ D0 |= q(c).Similarly modules, fragments computed first strengthening datalog program P exploiting datalog reasoner identify axioms factsresponsible validity q(c). contrast query inseparable modules, however,fragment O0 [ D0 guaranteed preserve fixed query q(c) w.r.t. fixed datasetD, rather queries w.r.t. datasets reference signature.13. Implementation Evaluationsection, present prototype module extraction system discuss resultsevaluation suite real-world ontologies.544fiModule Extraction Expressive Ontology Languages via Datalog Reasoning13.1 PrisM Systemimplemented prototype system module extraction Java, called PrisM,bundles RDFox black-box datalog reasoner (Motik et al., 2014). PrisMavailable online academic license.4PrisM accepts input OWL 2 ontology O, signature parameterindicates relevant inseparability relation. system currently supports wholeOWL 2, exception datatypes; furthermore, supports inseparabilityrelations 2 {m, q, f, i, c, wq} defined Sections 6 7.PrisM computes S-module Mout w.r.t. according following steps.1. Compute normalisation norm(O) O, norm normalisation functiondefined rules Figure 2 Section 2 (see also Proposition 3). Then, applymapping Figure 1 obtain equivalent set rules O0 = (norm(O)).2. Consider S-module setting = (S , D0S , DrS ) O0 defined Section 6(for 2 {m, q, f, i}) 7 (for 2 {c, wq}). Then, compute corresponding modulefollows:(a) Construct datalog program PD0S DrS .(b) Compute support supp(specified Definition 19, datasetsS)exploiting result Theorem 68. this,S)specified Definition 19.compute datalog program (P ) dataset (D0S , DrS ) usingtransformationdefined Theorem 68;construct materialisation (P ) [ (D0S , DrS ) using RDFox;construct supp( ) set rules r 2 P Rel(dr ) 2(c) Constructsupp(3. Return Mout consisting axioms 2 (norm()) \6= ;.first step normalises input OWL 2 ontology set rules O0 . PrisM providesoptimisation O0 constructed locality-based module O, ratheritself. Specifically, use ?-module M?[O,] classification inseparability,?>?> -module M[O,] inseparability relations. optimisationcompromise correctness overall procedure since model inseparability strongerinseparability relations.second step computes relevant S-module set rules O0 signature . essentially done following Definition 19; computationallydemanding part construction support supp( ), achieved via materialisation using RDFox black-box manner.Finally, third step constructs module Mout input ontologymodule corresponding set rules O0 . this, PrisM keeps track correspondence axioms normalisation norm(O). Correctnessstep ensured Proposition 13.4. http://www.cs.ox.ac.uk/isg/tools/PrisM/545fiArmas Romero, Kaminski, Cuenca Grau, & HorrocksID00001000040002400026000290003200347003500035100354004630047100477005120054500774007750077800786nameACGT-v1.0BAMS-simplifiedDOLCEGALEN-no-FITGALEN-doctoredGALEN-undoctoredLUBM-one-uniOBIAERONIF-gross-anatomyFly-anatomy-XPFMA-liteGazetteerLipidMolecule-roleRNA-v0.2Roberts-familySNOMEDNCI-v12.04epredicates2,0191,19960329,0733,7403,762682,9653554,1668,04778,986150,9811,2899,22233818354,98293,628rules5,51218,9762,14866,1917,4477,81884,77110,9526697,13442,107168,828382,1585,222153,0209382,020191,891193,453disjunctive1050530000771151000541034118,32365existential25916,78218426,9732,3672,71581,1681001,5069,43342,734156,7438936,276907360,37776,957expressivitySROIQ(D)ALEHIF +SHOIN (D)ALEHALEHIF +ALEHIF +ALEHI + (D)SHOIN (D)SROIQ(D)SROIF(D)ALERI +ALEH+ALE +ALCHINALE +SRIQ(D)SROIQ(D)SHSH(D)Table 3: Test ontologies13.2 Evaluationevaluated system set test ontologies identified work Glimm,Horrocks, Motik, Stoilos, Wang (2014) non-trivial reasoning. ontologiesnormalised prior module extraction make DL axioms equivalent rules.details ontologies given Table 3.5 first second columnstable indicate ontology ID name Oxford Ontology Repository. thirdfourth columns provide number predicates rules resulting ontologynormalisation. fifth sixth columns specify many rules containdisjunction existential quantification head. Finally, last column indicatesDL expressivity6 normalised ontology given OWL API.experiments performed server 2 Intel Xeon E5-2670 2.60GHz processors, 8 physical cores serve 2 virtual cores each, making total 32virtual cores. experiments allocated 90GB RAM, RDFox always run16 threads. compared module sizes extraction times using systemlocality-based ?- ?> -modules, computed using OWL API.followed experimental methodology work Del Vescovo et al. (2013), twokinds signatures considered:genuine signatures, correspond signature individual axioms,random signatures, include signatures several axioms.5. ontologies used experiments available download https://krr-nas.cs.ox.ac.uk/2015/jair/PrisM/testOntologies.zip.6. refer reader work Baader et al. (2003) detailed account DL naming conventions.546fiModule Extraction Expressive Ontology Languages via Datalog ReasoningUnlike work Del Vescovo et al. (2013), defined random signatures simplyrandom subsets ontology signature, extracted signatures using randomisedgraph sampling algorithm. first represented syntactic dependencies symbols(normalised) ontology graph, traversed graph randomised wayvisited set number n nodes. symbols corresponding visited nodestaken random signature.7 advantage approach yieldssignatures semantically connected, believe likely casepractical applications. number n chosen default 0.1% total graphincreased two orders magnitude cases resulting signaturestypically contained less 15 predicates thus small provide additionalinformation w.r.t. genuine signatures.kind signature ontology, considered sample 400runs averaged module sizes module extraction times. one hand,compared modules produced c (Section 7) ?-modules.kind modules literature guarantee notion classification inseparability.hand, compared modules produced , q , wq , f ,(Sections 6 7) ?> -modules. discussed Section 12.2, system(to best knowledge) capable computing modules specific deductiveinseparability relations considered paper. Furthermore, module extractionsystems ensure model inseparability, MEX AMEX, applicablerather restricted ontology languages. Consequently, ?> -modules seemed best availableoption comparison approach.Tables 4 5 provide average number rules kind module genuinerandom signatures, respectively. tables, total number rules normalisedontology provided top comparison purposes, whereas average sizesignatures considered specified towards bottom. Table 5 additionally includespercentage n dependency graph covered random walks randomsignatures obtained.observe module size consistently decreases consider weaker inseparability relations. modules produced c several orders magnitude smaller?-modules, cases 00463, 00471, 00477 00545. Although ratherextreme cases, observed cases least 25% decrease size (see 00026, 00029,00032, 00347, 00350, 00351, 00786). modules model inseparability improve reasonably ?> -modules cases, although greatest dierence size course?> -modules -modules, reaching one order magnitude ontologies(see 00471 00477, also 0004, 00463 00786 genuine signatures). realisticontologies, small proportion predicate pairs related atomic implication,often still case considering datalog overestimation ontology;thus, dierence size ?> -modules rather unsurprising.naturally also big dierence size wq -modules modules wheneverontologies mention constants, since former case obviously empty.worth observing that, even though several cases modules qmodules similar size (e.g. 00471), also cases dier signifi7. functionality required perform random walks currently integrated RDFox.547fiArmas Romero, Kaminski, Cuenca Grau, & Horrockstotal?c>?qwqf||000015,51267855867458456351456355820000418,97618,30616,94218,29718,29717,151017,1086553000242,1481,0008839909108848758848822004630047100477total 42,107 168,828 382,158? 22,348 47,192 214,82011212<1c>?221209217128107128q000wq801<1f121<1||3230002666,19114,2539,79913,74913,6869,44805,9623,2793000297,4471879411411296096183000327,818690479596592533053313030034784,77184,72623,65158,18644,24444,36843,76131,32211,23420035010,952803558768624596538596558200351669133741309777647767200512005455,222 153,020261 143,3998663423212910029<127<1230077493880768080780787620077500778007862,020 191,891 193,4531,9164331,1401,4914263901,9134271,1381,4984261,1381,4924263851,490001,4924263711,491397120233003547,1348266187866756261116266172Table 4: Average module sizes genuine signatures.cantly (e.g. 00786). Similarly, q modules f modules similar size cases(e.g. 00350) others (e.g. 00471), happens q wq (exemplified ontologies 00775 00354), f (see ontologies 00512 00004).observations suggest modules faithfully reflect dierencesinseparability relations considered, could oer significant advantagespractical applicationsTables 6 7 provide average module extraction time (in milliseconds) genuinerandom signatures, respectively. extraction modules consistently slowerlocality-based modules; however, average extraction time rarely exceeds1 minute, often 10 seconds (especially genuine signatures).suggests modules feasible practical applications. Furthermore, sinceextraction time invariably spent datalog reasoner, used blackbox, future advancements area datalog reasoning lead performancegains systems implementing technique.14. Conclusion Future Workpaper, proposed novel approach module extraction based reductiondatalog reasoning. contrast existing techniques, approach applicabledescription logics, also highly expressive first-order rule formalisms. Furthermore,techniques easily customised capture wide range inseparabilityrelations studied literature. cases modules satisfy many desirable properties,548fiModule Extraction Expressive Ontology Languages via Datalog Reasoningtotal?c>?qwqf||%000015,5128576918547597365177356884310000418,97618,90417,60718,89418,89418,579018,5362,511821000242,1481,0539331,044964942875942931201004630047100477total 42,107 168,828 382,158? 23,139 49,345 215,88659540238c>?9821,6581,0509731,4501,0497571,4501,049q000wq6647416f3337416||28154312%0.10.10.10002666,19127,77117,87927,18427,17518,315018,25517,6461070.1000297,4471,8901,2231,7261,7191,38001,3641,0601041000327,8183,2792,4833,1083,1012,63302,6202,31410710034784,77184,73258,20380,15363,03163,03162,45558,98050,36211100035010,9521,7951,0841,7581,6111,3895391,3891,0809210035166931520831127426581241191561000512005455,222 153,0201,555 143,4481,199288371681914774140076654675661910.100774938371338371369368036833858100077500778007862,020 191,891 193,4531,979 11,766 16,8201,527 11,3427,9741,977 11,762 16,8171,561 11,651 16,8171,557 11,6448,9691,506001,557 11,3428,4151,526 11,3426,22842202326100.10.1003547,1341,5371,2401,5011,3881,2791131,2781,238791Table 5: Average module sizes random signatures.makes well-suited applications ontology reuse, debugging, modularontology development, reasoning optimisation. Last, least, modulesefficiently computed reusing o-the-shelf datalog reasoners experimentalevaluation confirms suitability practice.envisage many directions future work, outline next.State-of-the-art modular DL reasoners, Chainsaw, currentlyrely ?-modules split workload fully-fledged OWL reasonerefficient reasoner lightweight DL. would natural exploit modulesclassification inseparability (cf. Section 7.1) instead ?-modules sincereasoners focus mainly classification tasks. believe using modulesmodular reasoning significantly improve separation workload leadbetter use lightweight reasoner.far, use modules optimising data reasoning tasks, fact entailmentquery answering, rather limited. Indeed, well-known ?-moduleswell-suited tasks (Cuenca Grau et al., 2008). PAGOdAreasoning system know exploits techniques akin module extractiondata reasoning (Zhou et al., 2014, 2015). would interesting investigatetechniques could exploited improve PAGOdAs performance. Furthermore,549fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks?c>?qwqf?c>?qwqf000012784643831857857846847000046436,7849518,24462,68032,97262,79735,15800024111,066191,0131,0741,0691,0741,072004631224,17619977548545546345600471 00477506 1,1549,729 52,529805 1,8511,543 4,995824 1,792790 1,753788 1,772792 1,7590002627330,25641825,88429,05127,27228,25428,4990002924295392312412292462340003227989428269038619108900034727414,82346315,26814,68810,67714,49510,2850035095747124724798787784789003513130512613614013113200512 0054518363378 34,70530616166 2,78322559521055222958022957900774516111147161154164164007751096318857955966967965007787221,6571,0561,5901,7081,6121,7001,691007865693,4498993,3403,4753,4193,5263,47900354316,588437223,5307673,8403,826Table 6: Average extraction times milliseconds genuine signatures.?c>?qwqf?c>?qwqf00001261,0761011,0111,0641,0561,0581,078000045836,3768418,114171,49732,474179,75934,86600024101,162181,0771,1321,1531,1221,1440002628456,12545749,79255,34351,34254,85454,06800029352,989542,6752,8142,6702,7852,76500032395,737604,9005,5335,1705,4775,3680034727615,70848520,88520,65415,39020,82215,063003501072,0651441,9362,0772,0102,0672,1080035144239384423390417427004631385,4411921,6151,4311,3321,3911,3820047154913,2917933,2022,6692,6382,6402,664004771,17251,7071,7686,0733,1913,1573,1573,223005122320,712372,1882,9392,7562,9642,8910054534231,6165762,5045915675695620077457681064075071574876700775101,064209671,0461,0281,0471,0480077884134,9801,21020,40934,33020,29333,56534,7740078669844,4631,07041,28343,78543,00744,60444,168003542913,664451,4678,5911,5918,3588,376Table 7: Average extraction times milliseconds random signatures.also envision potential applications incremental stream reasoning,data frequently changing queries ontologies seen fixed.conjecture optimal module setting families fact query inseparabilityincur exponential blowup w.r.t. ones chosen remains open.550fiModule Extraction Expressive Ontology Languages via Datalog Reasoninguse ?-modules exploit debugging explanation systems DL ontologiesproved rather successful (Suntisrivaraporn et al., 2008). Given modulesalso justification-preserving, would interesting evaluate eectivenessmodules implication inseparability setting.Finally, use module extraction techniques far largely restricteddescription logics. techniques are, however, widely applicable couldexploited number reasoning tasks ontology languages datalogdatalog,_ , currently gaining momentum.Acknowledgmentspaper extended version conference publication (Armas Romero, Kaminski,Cuenca Grau, & Horrocks, 2015). work supported Royal SocietyUniversity Research Fellowship, EPSRC projects Score!, MaSI3 , DBOnto,EU FP7 project Optique. would like thank anonymous refereesvaluable comments suggestions.Appendix A. Proofs Section 11Theorem 73. familyS-optimal 2 {m, i, c, wq}.prove result 2 {m, i, c, wq} separately.Theorem 75.m-optimal.Proof. Suppose m-optimal. must m-admissible, uniformfamily (O,) 6 (O,) . Let (O, ) = hm , D0m , Drm(O, ) = h, D0 , Dr i. Theorem 55, (O, ) 6,! (O, ), hencemapping : Ct( (O, )) ! Ct( (O, )) must either 6= D0m 6 D0Drm 6 Dr .Suppose two existentially quantified variables y1 y2y1 6= y2 . Consider ontology O0 consisting following rules:p1p2p3p4::::A(x) ! S(x, b)A(x) ! 9y1 [R(x, y1 ) ^ B(y1 )]A(x) ! 9y2 [R(x, y2 ) ^ C(y2 )]S(x, b) ^ R(x, z) ^ B(z) ^ C(z) ! D(x)signature 0 = {A, D, R} let (O0 , 0 ) = h0 , D00 , Dr0 i. second propertyuniformity, 0 substitution (O0 , ;) (O0 , ), therefore,first property uniformity, y1 0 6= y2 0 . follows p4 never applied0000P (O , ) (D00 ) hence support (O0 , 0 ) (O , ) {p1 , p2 , p3 }.interpretation = {i}, AI = B = C = {i}, DI = ;, aI =00RI = = {(i, i)} model (O , ) , cannot extended model00O0 without changing interpretation A, R. follows (O , )551fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks0m-admissible. origin0 -module , contradicting hypothesiscontradiction assumption map existentially quantifiedvariables constant, therefore must constant c = cvariable universally quantified O. Suppose existsconstant b0 b0 6= c. first property uniformity 0 must alsomap b constant y1 y2 . p4 never applied0000P (O , ) (D00 ) (O , ) {p1 , p2 , p3 }, which, already shown,contradiction. Consequently, must map constants c well.means exists substitution : Ct( (O, )) ! Ct( (O, ))= . particular must = c. hypothesis, must caseeither D0m 6 D0 Drm 6 Dr .Suppose D0m 6 D0 . must predicate X 2 X(c, . . . , c) 2/ D0 .00Consider ontology consisting following rules:p5 : X(x, . . . , x) ! 9yR(x, y)p6 : R(x, x) ! A(x)signature 00 = {X, A}, let (O00 , 00 ) = h00 , D000 , Dr00 i. uniformity , 00maps c, fact X(c, . . . , c) initial dataset (O, {X}),(O, 00 ), D000 . Consider dataset= {A(a), A(c)} [ { X(t) | 2 {a, c}arity(X) , 6= (c, . . . , c) }fresh constant. substitution maps c constants(O00 , 00 ) homomorphism (O00 , 00 ) = h00 , D, (Dr00 )i. Theorem 55,0000implies (O , ) . Clearly, R-fact P (D) R(a, c), p50000support therefore (O , ) {p5 }. interpretation J000J = {i}, X J = (i, . . . , i), AJ = ; RJ = {(i, i)} model (O , ) ,cannot extended model O00 without changing interpretation X A.000000follows (O , )00 -module , contradicts hypothesism-admissible. contradiction stems assumption D0m 6 D0 , thereforemust D0m D0 .hypothesis, implies Drm 6 Dr , must predicate 2(c, . . . , c) 2/ Dr . Consider ontology O000 consisting following rules:p7 : A(x) ! 9yR(x, y)p8 : R(x, x) ! (x, . . . , x)signature 000 = {A, }, let (O000 , 000 ) = h000 , D0000 , Dr000 i. uniformity, 000 maps c. Consider dataset = {A(c), (c, . . . , c), A(b), (b, . . . , b)},fresh constant. mapping maps c constants(O000 , 000 ) b homomorphism (O000 , 000 ) = h000 , D, Dr000 i. Theorem 55,00000implies (O , ) . easy see R-facts materialisationP [ R(a, c) R(c, c), proof P [ supported p8 proof(c, . . . , c). However, uniformity , (c, . . . , c) relevant facts(O, {Y }), (O, {A, }), Dr000 . follows p8 support000000(O , ) {p7 }. interpretation K K = {i}, AK = {i},552fiModule Extraction Expressive Ontology Languages via Datalog Reasoning000RK = {(i, i)} K = ; model (O , ) , cannot extended model000000O000 without changing interpretation . follows (O , )000m-admissible.000 -module , contradicts hypothesisorigin contradiction assumption Drm 6 Dr , therefore mustDrm Dr . mapping thus witness (O, ) ,! (O, ), ultimatelycontradicting assumption m-optimal.Theorem 76.i-optimal.Proof. Consider module setting family i0 pair ontologysignature , i0 (O, ) = hi0 , D0i0 , Dri0 follows:i0 = { 7! cy | existentially quantified } [ { c 7! c | c 2 Sig(O) constant }D0i0 = { A(cA,B ) | 6= B predicates arity }Dri0 = { B(cA,B ) | 6= B predicates arity } [ Dri0?cy fresh constant, cA,B = c1A,B , . . . , cnA,B , array fresh constantspair A, B 2 distinct n-ary predicates, Dri0? = {?} contains two distinctpredicates arity Dri0? = ; otherwise.First, show i-optimal i0 i-optimal proving (O,) =0 (O,) . Let us fix arbitrary , let (O, ) = hi , D0i , Drii0 (O, ) = hi0 , D0i0 , Dri0 i. easy see i0 (O, ) ,! (O, ), thereforeTheorem 55 0 (O,) (O,) . (O,) 0 (O,) , first(O,)(O,)note P= P 0. assume w.l.o.g. contains least twopredicates A, B arityotherwise non-trivial -implications. Givenproof = (T, ) P (O,) [ D0i B(cA ) (resp. ?) proof 0 = (T, 0 )0 (v) = (v) node v , substitution cA = cA,B , proofB(cA,B ) (resp. ?) P 0 (O,) [ D0i0 satisfying supp(0 ) = supp(). Consequently,supp( (O, )) supp( i0 (O, )) hence (O,) 0 (O,) .Now, suppose i0 i-optimal. Then, must uniform, i-admissible family0 (O,) 6 (O,) . Let (O, ) = h, D0 , Dr i. Sincei0 injective, exists mapping : Ct( i0 (O, )) ! Ct( (O, )) i0 = .condition determines eect dom(i0 ), since dom(i0 ) disjointset { cA,B | 6= B predicates arity }, asume eitherD0i0 D0 , two distinct predicates A, B 2 arityD0 contains A-facts. Suppose latter case, consider ontologyO0 = {A(x) ! B(x)}. uniformity , initial dataset (O0 , ) also contains0A-facts therefore support (O0 , ) empty (O ,) = ; 6|= A(x) ! B(x).contradicts i-admissibility , hence must D0i0 D0 . Finally, supposeDri0 6 Dr ; particular must contain two distinct n-ary predicates, since otherwiseDri0 = ; thus trivially Dri0 6 Dr . case two possible situations:?2/ Dr .Let A, B 2 distinct predicates arity consider ontology553fiArmas Romero, Kaminski, Cuenca Grau, & HorrocksO00 = {A(x) ! C(x) _ D(x), C(x) ! B(x), D(x) ! ?} C fresh predicates. Clearly, O00 |= A(x) ! B(x) uniformity , ? set00relevant facts (O00 , ), therefore D(x) ! ? 2/ (O ,) . Consequently00(O ,) 6|= A(x) ! B(x), contradicting assumption i-admissible.B(cA,B ) 2/ Dr pair distinct A, B 2 arity.Consider O000 = {A(x) ! B(x)}. uniformity , B-facts set00relevant facts (O000 , {A, B}), therefore (O ,{A,B}) = ; |6 = A(x) ! B(x),contradicting assumption i-admissible.follows Dri0 Dr , homomorphism (O, ) i0 (O, ), thus0 (O,) (O,) , ultimately contradicts assumption i0 (resp. )i-optimal.Theorem 77.cc-optimal.Proof. Analogous Theorem 76Theorem 78.wqwq-optimal.Proof. Suppose wq wq-optimal. must uniform wq-admissiblewqfamilys.t. (O,) 6 (O,) . Let (O, ) = h, D0 , Drwq (O, ) = h wq , wq, wq i. Theorem 55, given mapping : Ct( wq ) ! Ct( )r0must either wq 6= D0wq 6 D0 Drwq 6 Dr .Since wq injective Dwq = ;, assume wq = ,wqD0 D0 Drwq 6 Dr . must predicate X 2 array csize arity(X) constants Ct(O)[{ cy | exist. quant. } X(c) 2/ Dr .Consider ontology O0 = {( ! X(c))}; clearly, O0 |= X(c). uniformity, X(c) also0relevant facts (O0 , ), implies (O ,) = ; 6|= X(c). contradictswqwq-admissibility , hence must Dr Dr , makes homomorphism.follows wq wq-optimal.Theorem 74. familyS-optimal 2 {f, q}.Again, prove result 2 {f, q} separately.fProposition 79. familyf-optimal.Proof. Consider arbitrary fixed pair . Let Ct(O) = {c1 , . . . , cn }. Furthermore, predicate B 2 array v 2 {1, . . . , arity(B) + n}arity(B) considerset constants { iB,v | 0 arity(B) + n }1. 0B,v , . . . , arityB,v (B) fresh constantsarity(B)+n2. arityB,v (B), . . . , B,varity(B)+iB,v= ci 2 Ct(O) 1 n.w1wnLet wB,v denote array (B,v , . . . , B,v ) whenever w = (w1 , . . . , wn ), consideruniform module setting familyf0f (O, )0= hf0 , D0f0 , Drf0 follows:f0 = { 7! cy | existentially quantified } [ { c 7! c | c 2 Ct(O) }554fiModule Extraction Expressive Ontology Languages via Datalog Reasoningarity(B) ,D0f0 = {A(wB,v ) | A, B 2 , v 2 {1, . . . , arity(B) + n}vw 2 {0, . . . , arity(B) + n}arity(A) , A(wB,v ) 6= B(B,v )}Drf0 = { B(vB,v ) | B 2 , v 2 {1, . . . , arity(B)}arity(B) } [ {?}show family f0 f-admissible. Consider datalog rule r = ' ! 2relf(O,) . W.l.o.g. assume 2/ 'otherwise r would tautological henceentailed ontology. Let substitution mapping variables r distinctfresh constants. Since2/ 'injective, also2/ ' . factmust form B(c) B 2 (if B = ? would c = ;) considerinjective substitution mapping constants c [ Ct(O) {1, . . . , arity(B) + n}satisfying c = arity(B) + c = ci 2 Ct(O). Let another substitution definedconstants Sig(' [ )cB,c c 2 c [ Ct(O)c =0B,c otherwiseNote compatible f0 .f0arity(B) .easy see ( ) = B(cB,c ) 2 Dr since c 2 {1, . . . , arity(B) + n}whand, fact (' ) must form A(B,(c) ) 2w 2 {0, . . . , arity(B) + n}arity(A) .2/ ' injective, easy seecwA(B,c ) 6= B(B,c ). Consequently, (' ) Drf0 . Theorem 23, follows |= rf0 (O,) |= r, hence f0 f-admissible.Finally, consider = {A(x) ! B(x), B(x) ! A(x)} = {A}. easy seeff(O,) = 6 ; = 0 (O,) , thus f f-optimal.Proposition 80. familyqq-optimal.Proof. Consider arbitrary fixed pair . Let Ct(O) = {c1 , . . . , cn } let{y1 , . . . , ym } existentially quantified variables mentioned O, {cy1 , . . . , cym }corresponding set fresh constants. Furthermore, predicate B 2array v 2 {1, . . . , arity(B) + n + m}arity(B) consider set { iB,v | 0 arity(B) + n + }constants1. 0B,v , . . . , arityB,v (B) fresh constants,arity(B)+n2. arityB,v (B), . . . , B,varity(B)+n+13. B,varity(B)+iB,varity(B)+n+m, . . . , B,v= ci 2 Ct(O) 1 n,arity(B)+n+iB,v= cyi 1w1wnLet wB,v denote array (B,v , . . . , B,v ) whenever w = (w1 , . . . , wn ), consideruniform module setting family q0 q0 (O, ) = hq0 , D0q0 , Drq0 follows:q0 = { 7! cy | existentially quantified } [ { c 7! c | c 2 Ct(O) }arity(B) ,D0q0 = {A(wB,v ) | A, B 2 , v 2 {1, . . . , arity(B) + n + m}vw 2 {0, 1, . . . , arity(B) + n}arity(A) , A(wB,v ) 6= B(B,v )}555fiArmas Romero, Kaminski, Cuenca Grau, & HorrocksDrq0 = { B(vB,v ) | B 2 , v 2 {1, . . . , arity(B) + n + m}arity(B) } [ {?}show family q0 q-admissible.Consider rule r = '(x) ! 9y (x, y) 2 relq (O, ), justification O0 rqrule 2 O0 . show q0 (O, ) q-admissible suffices show 0 (O,) .Wnassume w.l.o.g. ? 2/ ' (otherwise r would tautological) = i=1> 0 conjunction atoms. Similarly proof Theorem 23,consider fresh predicate Q ontologyOQ = {(x, y)! Q(x) | 1 n }well module setting Q = hq0 , D0Q , DrQ [ OQ , satisfying particularqP Q = P 0 (O,) [OQ . argued proof Theorem 23, given 2 O0 substitutionmapping variables x fresh distinct constants, must proof = (T, )Q(x) (O [ OQ ) [ ' (s) \ supp() 6= ;. Furthermore, thanks O0justification, assume laconic, particular leaf node v 2ancestor w v must (v) 6 (w).qLemma 21, either = ? ! exists proof 0 = (T 0 , 0 ) P 0 (O,) [(' )q0 either Q(x) ? embeddable modulo q0 supp(0 ) \q0 (O,) (s) 6= ;.= ? ! must rule s0 O0 ? 2 Sig(O0 ). shownext, considering case 6= ? ! , must s0 2 . Therefore ? 2 Sig(M ),consequently = ? ! 2 .Otherwise, 0 = (T, ) proof Q(x) , discussed proof Theorem 23,qrule 0 (O,) (r) mustused subproof 00 = (T 00 , 00 ) 0 proof( 0 )q0 2 extension 0 y. Let v root 00w1 , . . . , wn leaves, definition must 00 (v) = ( 0 )q0 .qv also leaf 00 must rule form ( ! ( 0 )q0 ) 0 (O,) (s),terms ( 0 )q0 must constants domain q0 . impliesq( 0 )q0 2 Drq0 , consequently ( ! ( 0 )q0 ) 2 supp( q0 (O, )) 2 0 (O,) .Suppose v leaf 00 . First all, note (' )q0 = ' due 0qmodifying constants ' using functional terms, therefore 00 (wi ) 2 '0 (0 ) q0 2i. functional terms/ ' , since constants cyconstants range fresh hypothesis, therefore 00 (wi ) 6= 00 (v)i. Suppose functional terms 0 . 0 q0 = 0 . know0 , hence 00 , embeddable modulo q0 . follows must existnode v 0 2 leaf (v) (v 0 )q0 , also collection0 ' 0leaves w10 , . . . , wn0 00 (wi ) = (wi0 )q0 2 ' q0 . Since neitherquse functional terms, 0 modify constants, must fact (v) (v 0 )00 (w ) = (w 0 ) i. Furthermore, (w ) 6 (v 0 ) i, implies0 mentioningthat, also case 00 (wi ) 6= 00 (v) i. Therefore, regardless00000functional terms,(wi ) 6= (v) i. Now, factmust formB(c) B 2 since 2 hypothesis r 2 relq (O, ). Let injectivesubstitution mapping constants c [ range() {1, . . . , arity(B) + n + m} satisfying(i) c = arity(B) + c = ci 2 Ct(O) (ii) c = arity(B) + n + c = cyi . Let556fiModule Extraction Expressive Ontology Languages via Datalog Reasoninganother substitution defined constants mentioned 00cB,c c 2 c [ range()c =0B,c otherwiseNote compatible . Since c 2 {1, . . . , arity(B) + n + m}arity(B) , easyf0see 00 (v) = B(cB,c ) 2 Dr . hand, previously observed,00(wi ) mentions constants set {cy1 , . . . , cym }, hence 00 (wi ) mustarity(A) . Furthermore,form A(wB,(c) ) 2 w 2 {0, . . . , arity(B) + n}00 (v) i, follows also 00 (w ) 6= 00 (v) thus 00 (w ) 2 f06=0i. proof 00 = (T 00 , 00 ) 00 (v) = ( 00 (v)) clearly proofqqP 0 (O,) [ D0q0 00 (v) support 00 , therefore 2 0 (O,) .qFinally, 0 proof ? must proof P 0 (O,) . Since assumption? 2/ ', labels leaves 0 must also dierent ?. check alsoqcase 2 0 (O,) , suffices follow similar argument onemapping 0 defined constants mentioned 0cc 2 Ct(O) [ {cy1 , . . . , cym }c =0B,c otherwise00 (wi)proved family q0 q-admissible. use showq q-optimal. proof Theorem 79, consider ontology = {A(x) ! B(x), B(x) ! A(x)} signature = {A}, observeqq(O,) = 6 ; = 0 (O,) , thus q q-optimal.ReferencesAbiteboul, S., Hull, R., & Vianu, V. (1995). Foundations Databases. Addison-Wesley.Alviano, M., Faber, W., Leone, N., & Manna, M. (2012). Disjunctive datalog existentialquantifiers: Semantics, decidability, complexity issues. Theory PracticeLogic Programming, 12 (4-5), 701718.Amir, E., & McIlraith, S. A. (2005). Partition-based logical reasoning first-orderpropositional theories. Artificial Intelligence, 162 (1-2), 4988.Antoniou, G., & Kehagias, A. (2000). note refinement ontologies. InternationalJournal Intelligent Systems, 15 (7), 623632.Armas Romero, A., Cuenca Grau, B., & Horrocks, I. (2012). MORe: Modular combinationOWL reasoners ontology classification. Cudre-Mauroux, P., Heflin, J., Sirin, E.,Tudorache, T., Euzenat, J., Hauswirth, M., Parreira, J. X., Hendler, J., Schreiber, G.,Bernstein, A., & Blomqvist, E. (Eds.), Proceedings 11th International SemanticWeb Conference, Part I, Vol. 7649 Lecture Notes Computer Science, pp. 116.Springer.Armas Romero, A., Kaminski, M., Cuenca Grau, B., & Horrocks, I. (2015). Ontology moduleextraction via datalog reasoning. Bonet, B., & Koenig, S. (Eds.), Proceedings29th AAAI Conference Artificial Intelligence, pp. 14101416. AAAI Press.557fiArmas Romero, Kaminski, Cuenca Grau, & HorrocksBaader, F., Bienvenu, M., Lutz, C., & Wolter, F. (2010). Query predicate emptinessdescription logics. Lin, F., Sattler, U., & Truszczynski, M. (Eds.), Proceedings12th International Conference Principles Knowledge RepresentationReasoning. AAAI Press.Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. Kaelbling, L. P.,& Saffiotti, A. (Eds.), Proceedings 19th International Joint Conference Artificial Intelligence, pp. 364369. Professional Book Center.Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (Eds.).(2003). Description Logic Handbook: Theory, Implementation, Applications.Cambridge University Press.Bachmair, L., & Ganzinger, H. (2001). Resolution theorem proving. Robinson, J. A.,& Voronkov, A. (Eds.), Handbook Automated Reasoning, pp. 1999. ElsevierMIT Press.Botoeva, E., Kontchakov, R., Ryzhikov, V., Wolter, F., & Zakharyaschev, M. (2014). Queryinseparability description logic knowledge bases. Baral, C., Giacomo, G. D.,& Eiter, T. (Eds.), Proceedings 14th International Conference PrinciplesKnowledge Representation Reasoning. AAAI Press.Bourhis, P., Morak, M., & Pieris, A. (2013). impact disjunction query answeringguarded-based existential rules. Rossi, F. (Ed.), Proceedings 23rdInternational Joint Conference Artificial Intelligence. IJCAI/AAAI.Bry, F., Eisinger, N., Eiter, T., Furche, T., Gottlob, G., Ley, C., Linse, B., Pichler, R., & Wei,F. (2007). Foundations rule-based query answering. Antoniou, G., Amann, U.,Baroglio, C., Decker, S., Henze, N., Patranjan, P., & Tolksdorf, R. (Eds.), Proceedings3rd International Reasoning Web Summer School, Tutorial Lectures, Vol. 4636Lecture Notes Computer Science, pp. 1153. Springer.Cal, A., Gottlob, G., Lukasiewicz, T., Marnette, B., & Pieris, A. (2010). Datalog+/-:family logical knowledge representation query languages new applications.Proceedings 25th Annual ACM/IEEE Symposium Logic ComputerScience, pp. 228242. IEEE Computer Society.Chekuri, C., & Rajaraman, A. (2000). Conjunctive query containment revisited. TheoreticalComputer Science, 239 (2), 211229.Cuenca Grau, B., Halaschek-Wiener, C., Kazakov, Y., & Suntisrivaraporn, B. (2010). Incremental classification description logics ontologies. Journal Automated Reasoning,44 (4), 337369.Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2007a). right amount:Extracting modules ontologies. Williamson, C. L., Zurko, M. E., PatelSchneider, P. F., & Shenoy, P. J. (Eds.), Proceedings 16th International WorldWide Web Conference, pp. 717726. ACM.Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2007b). logical frameworkmodularity ontologies. Veloso, M. M. (Ed.), Proceedings 20th InternationalJoint Conference Artificial Intelligence, pp. 298303.558fiModule Extraction Expressive Ontology Languages via Datalog ReasoningCuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2008). Modular reuse ontologies: Theory practice. Journal Artificial Intelligence Research, 31, 273318.Cuenca Grau, B., Horrocks, I., Krotzsch, M., Kupke, C., Magka, D., Motik, B., & Wang, Z.(2013). Acyclicity notions existential rules application query answeringontologies. Journal Artificial Intelligence Research, 47, 741808.Cuenca Grau, B., Horrocks, I., Motik, B., Parsia, B., Patel-Schneider, P. F., & Sattler, U.(2008). OWL 2: next step OWL. Journal Web Semantics, 6 (4), 309322.Dantsin, E., Eiter, T., Gottlob, G., & Voronkov, A. (2001). Complexity expressivepower logic programming. ACM Computing Surveys, 33 (3), 374425.Del Vescovo, C., Klinov, P., Parsia, B., Sattler, U., Schneider, T., & Tsarkov, D. (2013).Empirical study logic-based modules: Cheap cheerful. Alani, H., Kagal, L.,Fokoue, A., Groth, P. T., Biemann, C., Parreira, J. X., Aroyo, L., Noy, N. F., Welty,C., & Janowicz, K. (Eds.), Proceedings 12th International Semantic Web Conference, Part I, Vol. 8218 Lecture Notes Computer Science, pp. 84100. Springer.Del Vescovo, C., Parsia, B., Sattler, U., & Schneider, T. (2011). modular structureontology: Atomic decomposition. Walsh, T. (Ed.), Proceedings 22nd International Joint Conference Artificial Intelligence, pp. 22322237. IJCAI/AAAI.Eiter, T., Ianni, G., Schindlauer, R., Tompits, H., & Wang, K. (2006). Forgetting managing rules ontologies. Proceedings 2006 IEEE / WIC / ACM InternationalConference Web Intelligence, pp. 411419. IEEE Computer Society.Gatens, W., Konev, B., & Wolter, F. (2014). Lower upper approximations depletingmodules description logic ontologies. Schaub, T., Friedrich, G., & OSullivan,B. (Eds.), Proceedings 21st European Conference Artificial Intelligence, Vol.263 Frontiers Artificial Intelligence Applications, pp. 345350. IOS Press.Ghilardi, S., Lutz, C., & Wolter, F. (2006a). damage ontology? case conservative extensions description logics. Doherty, P., Mylopoulos, J., & Welty, C. A.(Eds.), Proceedings 10th International Conference Principles KnowledgeRepresentation Reasoning, pp. 187197. AAAI Press.Ghilardi, S., Lutz, C., Wolter, F., & Zakharyaschev, M. (2006b). Conservative extensionsmodal logic. Governatori, G., Hodkinson, I. M., & Venema, Y. (Eds.), Proceedings6th Advances Modal Logic Conference, pp. 187207. College Publications.Glimm, B., Horrocks, I., Motik, B., Stoilos, G., & Wang, Z. (2014). HermiT: OWL 2reasoner. Journal Automated Reasoning, 53 (3), 245269.Grohe, M., Schwentick, T., & Segoufin, L. (2001). evaluation conjunctivequeries tractable?. Vitter, J. S., Spirakis, P. G., & Yannakakis, M. (Eds.), Proceedings 33rd Annual ACM Symposium Theory Computing, pp. 657666.ACM.Horridge, M., & Bechhofer, S. (2011). OWL API: java API OWL ontologies.Semantic Web, 2 (1), 1121.Horridge, M., Parsia, B., & Sattler, U. (2008). Laconic precise justifications OWL.Sheth, A. P., Staab, S., Dean, M., Paolucci, M., Maynard, D., Finin, T. W., &559fiArmas Romero, Kaminski, Cuenca Grau, & HorrocksThirunarayan, K. (Eds.), Proceedings 7th International Semantic Web Conference, Vol. 5318 Lecture Notes Computer Science, pp. 323338. Springer.Horrocks, I., Kutz, O., & Sattler, U. (2006). even irresistible SROIQ. Doherty,P., Mylopoulos, J., & Welty, C. A. (Eds.), Proceedings 10th International Conference Principles Knowledge Representation Reasoning, pp. 5767. AAAIPress.Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). SHIQ RDFOWL: making web ontology language. Journal Web Semantics, 1 (1), 726.Jimenez-Ruiz, E., & Cuenca Grau, B. (2011). LogMap: Logic-based scalable ontologymatching. Aroyo, L., Welty, C., Alani, H., Taylor, J., Bernstein, A., Kagal, L., Noy,N. F., & Blomqvist, E. (Eds.), Proceedings 10th International Semantic WebConference, Part I, Vol. 7031 Lecture Notes Computer Science, pp. 273288.Springer.Jimenez-Ruiz, E., Cuenca Grau, B., Horrocks, I., & Berlanga Llavori, R. (2011). Supportingconcurrent ontology development: Framework, algorithms tool. Data & KnowledgeEngineering, 70 (1), 146164.Jimenez-Ruiz, E., Cuenca Grau, B., Sattler, U., Schneider, T., & Berlanga Llavori, R.(2008). Safe economic re-use ontologies: logic-based methodology toolsupport. Bechhofer, S., Hauswirth, M., Homann, J., & Koubarakis, M. (Eds.),Proceedings 5th European Semantic Web Conference, Vol. 5021 Lecture NotesComputer Science, pp. 185199. Springer.Kalyanpur, A., Parsia, B., Horridge, M., & Sirin, E. (2007). Finding justifications OWLDL entailments. Aberer, K., Choi, K., Noy, N. F., Allemang, D., Lee, K., Nixon,L. J. B., Golbeck, J., Mika, P., Maynard, D., Mizoguchi, R., Schreiber, G., & CudreMauroux, P. (Eds.), Proceedings 6th International Semantic Web Conference2nd Asian Semantic Web Conference, Vol. 4825 Lecture Notes ComputerScience, pp. 267280. Springer.Kalyanpur, A., Parsia, B., Sirin, E., & Cuenca Grau, B. (2006). Repairing unsatisfiableconcepts OWL ontologies. Sure, Y., & Domingue, J. (Eds.), Proceedings3rd European Semantic Web Conference, Vol. 4011 Lecture Notes ComputerScience, pp. 170184. Springer.Kalyanpur, A., Parsia, B., Sirin, E., & Hendler, J. A. (2005). Debugging unsatisfiable classesOWL ontologies. Journal Web Semantics, 3 (4), 268293.Konev, B., Kontchakov, R., Ludwig, M., Schneider, T., Wolter, F., & Zakharyaschev, M.(2011). Conjunctive query inseparability OWL 2 QL tboxes. Burgard, W., &Roth, D. (Eds.), Proceedings 25th AAAI Conference Artificial Intelligence.AAAI Press.Konev, B., Ludwig, M., Walther, D., & Wolter, F. (2012). logical dierencelightweight description logic EL. Journal Artificial Intelligence Research, 44, 633708.Konev, B., Lutz, C., Ponomaryov, D. K., & Wolter, F. (2010). Decomposing descriptionlogic ontologies. Lin, F., Sattler, U., & Truszczynski, M. (Eds.), Proceedings560fiModule Extraction Expressive Ontology Languages via Datalog Reasoning12th International Conference Principles Knowledge RepresentationReasoning. AAAI Press.Konev, B., Lutz, C., Walther, D., & Wolter, F. (2009). Formal properties modularisation.Stuckenschmidt, H., Parent, C., & Spaccapietra, S. (Eds.), Modular Ontologies:Concepts, Theories Techniques Knowledge Modularization, Vol. 5445 LectureNotes Computer Science, pp. 2566. Springer.Konev, B., Lutz, C., Walther, D., & Wolter, F. (2013). Model-theoretic inseparabilitymodularity description logic ontologies. Artificial Intelligence, 203, 66103.Konev, B., Walther, D., & Wolter, F. (2009). Forgetting uniform interpolation largescale description logic terminologies. Boutilier, C. (Ed.), Proceedings 21stInternational Joint Conference Artificial Intelligence, pp. 830835.Kontchakov, R., Lutz, C., Toman, D., Wolter, F., & Zakharyaschev, M. (2011). combined approach ontology-based data access. Walsh, T. (Ed.), Proceedings22nd International Joint Conference Artificial Intelligence, pp. 26562661. IJCAI/AAAI.Kontchakov, R., Wolter, F., & Zakharyaschev, M. (2010). Logic-based ontology comparisonmodule extraction, application DL-Lite. Artificial Intelligence, 174 (15),10931141.Koopmann, P., & Schmidt, R. A. (2014). Count forget: Uniform interpolation SHQontologies. Demri, S., Kapur, D., & Weidenbach, C. (Eds.), Proceedings 7thInternational Joint Conference Automated Reasoning, Vol. 8562 Lecture NotesComputer Science, pp. 434448. Springer.Krotzsch, M., Rudolph, S., & Hitzler, P. (2008a). Description logic rules. Ghallab,M., Spyropoulos, C. D., Fakotakis, N., & Avouris, N. M. (Eds.), Proceedings18th European Conference Artificial Intelligence, Vol. 178 Frontiers ArtificialIntelligence Applications, pp. 8084. IOS Press.Krotzsch, M., Rudolph, S., & Hitzler, P. (2008b). ELP: Tractable rules OWL 2. Sheth,A. P., Staab, S., Dean, M., Paolucci, M., Maynard, D., Finin, T. W., & Thirunarayan,K. (Eds.), Proceedings 7th International Semantic Web Conference, Vol. 5318Lecture Notes Computer Science, pp. 649664. Springer.Ludwig, M. (2014). Just: tool computing justifications w.r.t. ELH ontologies.Bail, S., Glimm, B., Jimenez-Ruiz, E., Matentzoglu, N., Parsia, B., & Steigmiller, A.(Eds.), Proceedings 3rd International Workshop OWL Reasoner Evaluation,Vol. 1207 CEUR Workshop Proceedings, pp. 17. CEUR-WS.org.Ludwig, M., & Konev, B. (2014). Practical uniform interpolation forgetting ALCtboxes applications logical dierence. Baral, C., Giacomo, G. D., & Eiter, T.(Eds.), Proceedings 14th International Conference Principles KnowledgeRepresentation Reasoning. AAAI Press.Lutz, C., & Wolter, F. (2010). Deciding inseparability conservative extensionsdescription logic EL. Journal Symbolic Computation, 45 (2), 194228.561fiArmas Romero, Kaminski, Cuenca Grau, & HorrocksLutz, C., & Wolter, F. (2011). Foundations uniform interpolation forgettingexpressive description logics. Walsh, T. (Ed.), Proceedings 22nd InternationalJoint Conference Artificial Intelligence, pp. 989995. IJCAI/AAAI.Marnette, B. (2009). Generalized schema-mappings: termination tractability.Paredaens, J., & Su, J. (Eds.), Proceedings 28th ACM SIGMOD SymposiumPrinciples Database Systems, pp. 1322. ACM.Motik, B. (2006). Reasoning Description Logics Using Resolution DeductiveDatabases. Ph.D. thesis, Univesitat Karlsruhe (TH), Karlsruhe, Germany.Motik, B., Nenov, Y., Piro, R., Horrocks, I., & Olteanu, D. (2014). Parallel materialisationdatalog programs centralised, main-memory RDF systems. Brodley, C. E., &Stone, P. (Eds.), Proceedings 28th AAAI Conference Artificial Intelligence,pp. 129137. AAAI Press.Motik, B., Patel-Schneider, P. F., & Parsia, B. (2012). OWL 2 web ontology languagestructural specification functional-style syntax..Nikitina, N., & Rudolph, S. (2014). (Non-)succinctness uniform interpolants generalterminologies description logic EL. Artificial Intelligence, 215, 120140.Nonnengart, A., & Weidenbach, C. (2001). Computing small clause normal forms.Robinson, J. A., & Voronkov, A. (Eds.), Handbook Automated Reasoning, pp. 335367. Elsevier MIT Press.Nortje, R., Britz, K., & Meyer, T. (2012). normal form hypergraph-based moduleextraction SROIQ. Gerber, A., Taylor, K., Meyer, T., & Orgun, M. (Eds.),Proceedings 8th Australasian Ontology Workshop, Vol. 969 CEUR WorkshopProceedings, pp. 4051. CEUR-WS.org.Nortje, R., Britz, K., & Meyer, T. (2013). Reachability modules description logicSRIQ. McMillan, K. L., Middeldorp, A., & Voronkov, A. (Eds.), Proceedings19th International Conference Logic Programming, Artificial IntelligenceReasoning, Vol. 8312 Lecture Notes Computer Science, pp. 636652. Springer.Robinson, J. A. (1965). Automatic deduction hyper-resolution. International JournalComputer Mathematics, 1 (3), 227234.Rousset, M.-C., & Ulliana, F. (2015). Extracting bounded-level modules deductiveRDF triplestores. Bonet, B., & Koenig, S. (Eds.), Proceedings 29th AAAIConference Artificial Intelligence, pp. 268274. AAAI Press.Sattler, U., Schneider, T., & Zakharyaschev, M. (2009). kind moduleextract?. Grau, B. C., Horrocks, I., Motik, B., & Sattler, U. (Eds.), Proceedings22nd International Workshop Description Logics, Vol. 477 CEUR WorkshopProceedings. CEUR-WS.org.Schlicht, A., & Stuckenschmidt, H. (2009). Distributed resolution expressive ontologynetworks. Polleres, A., & Swift, T. (Eds.), Proceedings 3rd InternationalConference Web Reasoning Rule Systems, Vol. 5837, pp. 87101. Springer.Schlobach, S., & Cornet, R. (2003). Non-standard reasoning services debuggingdescription logic terminologies. Gottlob, G., & Walsh, T. (Eds.), Proceedings562fiModule Extraction Expressive Ontology Languages via Datalog Reasoning18th International Joint Conference Artificial Intelligence, pp. 355362. MorganKaufmann.Seidenberg, J., & Rector, A. L. (2006). Web ontology segmentation: Analysis, classificationuse. Carr, L., Roure, D. D., Iyengar, A., Goble, C. A., & Dahlin, M. (Eds.),Proceedings 15th International World Wide Web Conference, pp. 1322. ACM.Stefanoni, G., Motik, B., & Horrocks, I. (2013). Introducing nominals combinedquery answering approaches EL. desJardins, M., & Littman, M. L. (Eds.),Proceedings 27th AAAI Conference Artificial Intelligence. AAAI Press.Stuckenschmidt, H., Parent, C., & Spaccapietra, S. (Eds.). (2009). Modular Ontologies:Concepts, Theories Techniques Knowledge Modularization, Vol. 5445 LectureNotes Computer Science. Springer.Suntisrivaraporn, B. (2008). Module extraction incremental classification: pragmaticapproach EL+ ontologies. Bechhofer, S., Hauswirth, M., Homann, J., &Koubarakis, M. (Eds.), Proceedings 5th European Semantic Web Conference,Vol. 5021 Lecture Notes Computer Science, pp. 230244. Springer.Suntisrivaraporn, B., Qi, G., Ji, Q., & Haase, P. (2008). modularization-based approachfinding justifications OWL DL entailments. Domingue, J., & Anutariya,C. (Eds.), Proceedings 3rd Asian Semantic Web Conference, Vol. 5367 LectureNotes Computer Science, pp. 115. Springer.Tsarkov, D., & Palmisano, I. (2012). Chainsaw: metareasoner large ontologies.Horrocks, I., Yatskevich, M., & Jimenez-Ruiz, E. (Eds.), Proceedings 1st International Workshop OWL Reasoner Evaluation, Vol. 858 CEUR WorkshopProceedings. CEUR-WS.org.W3C OWL Working Group (2012). OWL 2 web ontology language document overview(second edition). W3C recommendation, World Wide Web Consortium.Wang, K., Wang, Z., Topor, R. W., Pan, J. Z., & Antoniou, G. (2014). Eliminating conceptsroles ontologies expressive descriptive logics. Computational Intelligence,30 (2), 205232.Wang, Z., Wang, K., Topor, R. W., & Pan, J. Z. (2010). Forgetting knowledge basesDL-Lite. Annals Mathematics Artificial Intelligence, 58 (1-2), 117151.Zhou, Y., Cuenca Grau, B., Horrocks, I., Wu, Z., & Banerjee, J. (2013). Makingtriple store: Query answering OWL 2 using RL reasoner. Schwabe, D.,Almeida, V. A. F., Glaser, H., Baeza-Yates, R. A., & Moon, S. B. (Eds.), Proceedings22nd International World Wide Web Conference, pp. 15691580. InternationalWorld Wide Web Conferences Steering Committee / ACM.Zhou, Y., Cuenca Grau, B., Nenov, Y., Kaminski, M., & Horrocks, I. (2015). PAGOdA:Pay-as-you-go ontology query answering using datalog reasoner. Journal ArtificialIntelligence Research, 54, 309367.Zhou, Y., Nenov, Y., Cuenca Grau, B., & Horrocks, I. (2013). Complete query answeringhorn ontologies using triple store. Alani, H., Kagal, L., Fokoue, A., Groth,P. T., Biemann, C., Parreira, J. X., Aroyo, L., Noy, N. F., Welty, C., & Janowicz, K.563fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks(Eds.), Proceedings 12th International Semantic Web Conference, Part I, Vol.8218 Lecture Notes Computer Science, pp. 720736. Springer.Zhou, Y., Nenov, Y., Cuenca Grau, B., & Horrocks, I. (2014). Pay-as-you-go OWL queryanswering using triple store. Brodley, C. E., & Stone, P. (Eds.), Proceedings28th AAAI Conference Artificial Intelligence, pp. 11421148. AAAI Press.564fiJournal Artificial Intelligence Research 55 (2016) 317-359Submitted 08/15; published 02/16Adaptive Contract Design Crowdsourcing Markets:Bandit Algorithms Repeated Principal-Agent ProblemsChien-Ju Hoch624@cornell.eduCornell University, Ithaca, NY, USAAleksandrs Slivkinsslivkins@microsoft.comMicrosoft Research, New York, NY, USAJennifer Wortman Vaughanjenn@microsoft.comMicrosoft Research, New York, NY, USAAbstractCrowdsourcing markets emerged popular platform matching availableworkers tasks complete. payment particular task typically settasks requester, may adjusted based quality completed work,example, use bonus payments. paper, study requestersproblem dynamically adjusting quality-contingent payments tasks. considermulti-round version well-known principal-agent model, whereby roundworker makes strategic choice effort level directly observablerequester. particular, formulation significantly generalizes budget-free online taskpricing problems studied prior work. treat problem multi-armed banditproblem, arm representing potential contract. cope large (andfact, infinite) number arms, propose new algorithm, AgnosticZooming,discretizes contract space finite number regions, effectively treating regionsingle arm. discretization adaptively refined, promising regionscontract space eventually discretized finely. analyze algorithm,showing achieves regret sublinear time horizon substantially improvesnon-adaptive discretization (which competing approach literature).results advance state art several different topics: theory crowdsourcingmarkets, principal-agent problems, multi-armed bandits, dynamic pricing.1. IntroductionCrowdsourcing harnesses human intelligence common sense complete tasksdifficult accomplish using computers alone. Crowdsourcing markets, Amazon Mechanical Turk CrowdFlower, platforms designed match available human workerstasks complete. Using platforms, requesters may post tasks wouldlike completed, along amount money willing pay. Workerschoose whether accept available tasks complete work.course human workers equal, human-produced work. tasks,proofreading English text, easier workers others, requiring lesseffort produce high quality results. Additionally, workers dedicatedothers, willing spend extra time make sure task completed properly. encouragehigh quality results, requesters may set quality-contingent bonus payments topbase payment task, rewarding workers producing valuable output.c2016AI Access Foundation. rights reserved.fiHo, Slivkins, & Vaughanviewed offering workers contract specifies much paid basedquality output.1examine requesters problem dynamically setting quality-contingent paymentstasks. consider setting time evolves rounds. round, requesterposts new contract, performance-contingent payment rule specifies different levelspayment different levels output quality. random, unidentifiable workerarrives market strategically decides whether accept requesters taskmuch effort exert; choice effort level directly observable requester.worker completes task (or chooses complete it), requester observesworkers output, pays worker according offered contract, adjustscontract next round. properties random worker (formally: distributionworkers types) known requester, may learned time.goal requester maximize expected utility, value receives completedwork minus payments made. call dynamic contract design problem.concreteness, consider special case worker strategically chooseperform task low effort high effort, task may completed eitherlow quality high quality. low effort incurs cost results low quality,turn brings value requester. high effort leads high qualitypositive probability (which may vary one worker another, unknownrequester). requester observes quality completed tasks, therefore cannotalways infer effort level. example captures two main tenets model:properties random worker unknown requester workers strategicdecisions unobservable.treat dynamic contract design problem multi-armed bandit (MAB) problem,arm representing potential contract. Since action space large (potentiallyinfinite) well-defined real-valued structure, natural consider algorithmuses discretization. algorithm, AgnosticZooming, divides action spaceregions, chooses among regions, effectively treating region single metaarm. discretization defined adaptively, promising areasaction space eventually discretized finely less promising areas.general idea adaptive discretization appeared prior work MAB (Kleinberg,Slivkins, & Upfal, 2008; Bubeck, Munos, Stoltz, & Szepesvari, 2011a; Slivkins, 2014, 2011),approach adaptive discretization new problem-specific. main difficulty,compared prior work, algorithm given information linksobservable numerical structure contracts expected utilities thereof.analyze performance, propose concept called width dimension measuresnice particular problem instance is. show AgnosticZooming achievesregret sublinear time horizon problem instances small width dimension.particular, width dimension d, achieves regret O(log (d+1)/(d+2) )rounds. problem instances large width dimension, AgnosticZooming matchesperformance naive algorithm uniformly discretizes space runs1. tasks, labeling websites relevant particular search query not, verifyingquality work may difficult completing task. tasks assigned batches,batch containing one instances correct answer already known (often calledgold data). Quality-contingent payments based known instances.318fiAdaptive Contract Design Crowdsourcing Marketsstandard bandit algorithm. illustrate general results via corollaries specialcases, including high-low example described above. support theoretical resultssimulations.Further, consider special case setting worker chooses whetheraccept reject given task. special case corresponds dynamic task pricingproblem previously studied literature. results significantly improve priorwork problem.contributions summarized follows. define broad, practically important setting crowdsourcing markets; identify novel problem-specific structure,algorithm regret bounds; distill ideas prior work workstructures; argue approach productive deriving corollaries comparingprior work; identify analyze specific examples theory applies.main conceptual contributions model adaptive discretization approachmentioned above. Finally, paper prompts research dynamic contract designalong several directions outline conclusion.1.1 Related Workwork builds three areas research. First, model viewed multi-roundversion classical principal-agent model contract theory (Laffont & Martimort,2002). single round model corresponds basic principal-agent setting,adverse selection (unknown workers type) moral hazard (unobservable workers decisions). Unlike much existing work contract theory, prior worker typesknown principal, may learned time. Accordingly, techniquesdifferent employed contract theory.Second, methods build developed rich literature MABcontinuous outcome spaces. closest line work Lipschitz MAB (Kleinberget al., 2008), algorithm given distance function arms,expected rewards arms assumed satisfy Lipschitz-continuity (or relaxationthereof) respect distance function (Agrawal, 1995; Kleinberg, 2004; Auer,Ortner, & Szepesvari, 2007; Kleinberg et al., 2008; Bubeck et al., 2011a; Slivkins, 2014).related techniques idea adaptive discretization (Kleinberg et al., 2008;Bubeck et al., 2011a; Slivkins, 2014), particular, zooming algorithm (Kleinberget al., 2008; Slivkins, 2014). However, zooming algorithm cannot applied directlysetting required numerical similarity information immediatelyavailable. problem also arises web search advertising, naturalassume algorithm observe tree-shaped taxonomy arms (Kocsis &Szepesvari, 2006; Munos & Coquelin, 2007; Pandey, Agarwal, Chakrabarti, & Josifovski,2007) used explicitly reconstruct relevant parts underlying metricspace (Slivkins, 2011; Bull, 2013). take different approach, using notion virtualwidth estimate similarity information. Explicit comparisons resultsprior MAB work made throughout paper.Finally, work follows several theoretical papers pricing crowdsourcingmarkets (Kleinberg & Leighton, 2003; Badanidiyuru, Kleinberg, & Singer, 2012; Singer& Mittal, 2013; Singla & Krause, 2013; Badanidiyuru, Kleinberg, & Slivkins, 2013).319fiHo, Slivkins, & Vaughanparticular, Badanidiyuru et al. (2012) Singla Krause (2013) study versionsetting simple, single-price contracts (independent output), focusdealing global budget constraint.thorough literature review (including discussion related empiricalwork) found Section 9.2. Setting: Dynamic Contract Design Problemsection, formally define problem set solve discussimplications several aspects model.2.1 Modelstart describing static model, captures happens single roundinteraction requester worker. described above, versionstandard principal-agent model (Laffont & Martimort, 2002). define dynamicmodel, extension static model multiple rounds, new worker arrivinground. detail objective pricing algorithm simplifyingassumptions make throughout paper. Finally, compare settingclassic multi-armed bandit problem.2.1.1 Static Modelbegin description occurs interaction requestersingle worker. requester first posts task may completed worker,contract specifying worker paid completes task. taskcompleted, requester pays worker specified contract, requesterderives value completed task; normalization, assume value derived[0, 1]. requesters utility given task value minus paymentworker.worker observes contract decides whether complete task,also chooses level effort exert, turn determines cost (in terms time,energy, missed opportunities) distribution quality work. modelquality, assume (small) finite set possible outcomes resultworker completing task (or choosing complete it), realized outcomedetermines value requester derives task. realized outcomeobserved requester, contract requester offers mappingoutcomes payments worker.emphasize two crucial (and related) features principal-agent model:mapping effort level outcomes randomized, effort leveldirectly observed requester. line standard observationcrowdsourcing even honest, high-effort workers occasionally make errors.workers utility given task payment requester minuscost corresponding chosen effort level. Given contract offered, workerchooses effort level strategically maximize expected utility. Crucially,chosen effort level directly observable requester.320fiAdaptive Contract Design Crowdsourcing Marketsworkers choice perform task modeled separate effort level zerocost (called null effort level) separate outcome zero value zero payment(called null outcome) null effort level deterministically leads nulloutcome, effort level lead outcome.mapping outcomes requesters value called requesters valuefunction. mapping effort levels costs called cost function,mapping effort levels distributions outcomes called production function.purposes paper, worker completely specified two functions;say cost function production function comprise workers type. Unliketraditional versions principal-agent problem, setting workers typeobservable requester, prior given.2.1.2 Dynamic Modeldynamic model consider paper natural extension static modelmultiple rounds multiple workers. still concerned single requester.round, new worker arrives. assume stochastic environmentworkers type round i.i.d. sample fixed unknown distributiontypes, called supply distribution. requester posts new task contracttask. tasks type, sense set possible effortlevels set possible outcomes tasks. worker strategicallychooses effort level maximize expected utility task. Basedchosen effort level workers production function, outcome realized.requester observes outcome (but workers effort level) pays workeramount specified contract. type arriving worker never revealedrequester. requester adjust contract one round another, totalutility sum utility rounds. simplicity, assume numberrounds known advance, though assumption relaxed using standarddoubling trick (Cesa-Bianchi & Lugosi, 2006) full executions algorithmrepeated phases exponentially increasing time horizons.2.1.3 Dynamic Contract Design ProblemThroughout paper, take point view requester interacting workersdynamic model. algorithms examine dynamically choose contracts offerround goal maximizing requesters expected utility. probleminstance consists several quantities, known algorithm,not. known quantities number outcomes, requesters valuefunction, time horizon (i.e., number rounds). latent quantitiesnumber effort levels, set worker types, supply distribution. algorithmadjusts contract round round observes realized outcomes receivesfeedback.focus contracts bounded (offer payments [0, 1]), monotone (assignequal higher payments outcomes higher value requester). Let Xset bounded, monotone contracts. compare given algorithm givensubset candidate contracts Xcand X. Letting OPT(Xcand ) optimal utility321fiHo, Slivkins, & Vaughancontracts Xcand , goal minimize algorithms regret R(T |Xcand ), definedOPT(Xcand ) minus algorithms expected utility.subset Xcand may finite infinite, possibly Xcand = X. naturalexample finite Xcand set bounded, monotone contracts paymentsinteger multiples > 0; call uniform mesh granularity ,denote Xcand ().2.1.4 NotationLet v() value function requester, v() denoting value outcome .Let set outcomes let number non-null outcomes.index outcomes = {0, 1, 2 , . . . , m} order increasing value (ties brokenarbitrarily), convention 0 null outcome.Let ci () fi () cost function production function type i.cost choosing effort level e ci (e),Pthe probability obtaining outcomechosen effort e fi (|e). Let Fi (|e) = 0 fi ( 0 |e) probability obtainingoutcome least good chosen effort e.Recall contract x function outcomes (non-negative) payments.contract x offered worker sampled i.i.d. supply distribution, V (x)expected value requester, P (x) 0 expected payment, U (x) = V (x)P (x)expected utility requester. Let OPT(Xcand ) = supxXcand U (x).2.1.5 Assumption: First-Order Stochastic Dominance (FOSD)Given two effort levels e e0 , say e FOSD e0 type Fi (|e) Fi (|e0 )outcomes , strict inequality least one outcome.2 say typesatisfies FOSD assumption two distinct effort levels, one effort level FOSDtype i. assume types satisfy assumption.2.1.6 Assumption: Consistent Tie-Breakingmultiple effort levels maximize expected utility given worker contract x,assume tie broken consistently sense worker chooses effortlevel contract leads particular tie. assumption minor;avoided (with minor technical complications) adding random perturbationscontracts. assumption implicit throughout paper.2.2 Discussionjumping results, discuss implications several aspects modeldetail.2.2.1 Number Outcomesresults assume small number outcomes. regime important practiceseveral reasons. First, tasks naturally small number outcomes.2. mimics standard notion FOSD two distributions linearly ordered set.322fiAdaptive Contract Design Crowdsourcing Marketsexample, binary labeling task four possible outcomes completed:{yes/no} {correct/incorrect}. Second, often makes sense group together multipleoutcomes similar value requester (such false positives false negatives)value known precisely. added benefit contracts becomesimpler workers perspective. Third, even task completed manydifferent ways, quality may difficult evaluate fine granularity; good exampletranslation sentence. Fourth, even fine-grained quality evaluation exists,error count speech transcription tasks, may difficult make consistentacross different tasks.Even = 2 non-null outcomes, setting studied before. specialcase = 1 equivalent dynamic pricing problem Kleinberg Leighton(2003); obtain improved results it, too.2.2.2 Benchmarkbenchmark OPT() considers contracts bounded monotone. practice,restricting contracts may appealing human parties involved. However,restriction without loss generality: problem instances monotonecontracts optimal; see Appendix example. Further, clear whetherbounded monotone contracts optimal among monotone contracts.benchmark OPT(Xcand ) relative given set Xcand , typically finitediscretization contract space. two reasons this. First, crowdsourcing platforms may require payments multiples minimum unit (e.g., onecent), case natural restrict attention contracts satisfyingconstraint. Second, achieving guarantees relative OPT(X) full generalityproblem appears beyond reach techniques. many machine learningscenarios, useful consider restricted benchmark set set alternatives compare to.3 settings, considered important handle arbitrary benchmark sets,do.One known approach obtain guarantees relative OPT(X) startfinite Xcand X, design algorithm guarantees relative OPT(Xcand ), then,separate result, bound discretization error OPT(X) OPT(Xcand ). choiceXcand drives tradeoff discretization error regret R(T |Xcand ),one choose Xcand optimize tradeoff. However, one upper-bounddiscretization error (very) simple special cases (see Section 5), unclear whetherextended full generality dynamic contract design.2.2.3 Alternative Worker ModelsOne crucial tenets model workers maximize expected utility.rationality assumption standard economics, often used makeproblem amenable rigorous analysis. However, considerable literaturesuggesting practice workers may deviate rational behavior. Thus,worth pointing results rely heavily rationality assumption.3. particularly relevant analogy contextual bandits policy sets (Dudik, Hsu, Kale, Karampatziakis,Langford, Reyzin, & Zhang, 2011).323fiHo, Slivkins, & VaughanFOSD assumption (which also fairly standard) circumvented, too. fact,assumptions regarding worker behavior serve enable us prove Lemma 3.1,specifically guarantee collective worker behavior satisfies naturalincrement payment property used proof Lemma 3.1: requester increasesincrement payment particular outcome (as described next section),probability obtaining outcome least good also increases. particular,property consistent worker behavior takes account long-term effectschanges reputation scores. also consistent workers acting upon subjective(and possibly incorrect) beliefs offered contract, beliefsguaranteed base payment may actually depend quality submitted work.42.2.4 Minimum Wageethical legal reasons one may want enforce form minimum wage.expressed within model minimal payment completed task, i.e.,non-null outcome. algorithm easily modified accommodateconstraint. Essentially, suffices restrict action space contracts pay leastcompleted task. Formally, increment space defined Section 3[, 1] [0, 1]m1 rather [0, 1]m , quadrants cell definedsplitting cell half dimension. results easily carry version(restricting Xcand contracts pay least completed task). omitdiscussion issue sake simplicity.2.2.5 Comparison Multi-Armed Bandits (MAB)Dynamic contract design modeled special case MAB problemadditional, problem-specific structure. basic MAB problem defined follows.algorithm repeatedly chooses actions fixed action space collects rewardschosen actions; available actions traditionally called arms. specifically, timepartitioned rounds, round algorithm selects arm receivesreward chosen arm. information, reward algorithmwould received choosing alternative arm, revealed. MAB problemstochastic rewards, reward arm given round i.i.d. sampledistribution depends arm round. standard measurealgorithms performance regret respect best fixed arm, defineddifference expected total reward benchmark (usually best fixed arm)algorithm.Thus, dynamic contract design naturally modeled MAB problemstochastic rewards, arms correspond monotone contracts. prior workMAB large infinite action spaces often assumes known upper bounds similarity arms. precisely, prior work would assume algorithm givenmetric contracts expected rewards Lipschitz-continuous respect4. worker model incorporates subjective beliefs suggested Ho, Slivkins, Suri,Vaughan (2015) based experimental evidence, model satisfies increment payment propertymentioned above.324fiAdaptive Contract Design Crowdsourcing MarketsD, i.e., upper bounds |U (x)U (y)| D(x, y) two contracts x, y.5 However,setting upper bounds absent. hand, problemauxiliary structure compared standard MAB setting. particular, algorithmsreward decomposes value payment, determined outcome,turn probabilistically determined workers strategic choice effortlevel. Effectively, auxiliary structure provides soft information similaritycontracts, sense numerically similar contracts usually (but always)induce similar response workers.2.2.6 Applicability ModelDespite considerable generality, model somewhat idealized. Let us discuss severalpotential concerns regarding applicable realistic model is.implicit intuition behind using performance-based payments incentivize better quality. growing empirical literature incentives crowdsourcing marketsfinds happens types tasks others. particular, experiments work Ho et al. (2015) suggest happens taskeffort-responsive, sense one obtain higher quality work increasing effort,effort levels costly worker. observation consistentworker model: indeed, effort-responsiveness task joint property productionfunction cost function implies significant response sufficiently increasedquality-based payments. Ho et al. propose pilot experiments determine whether giventype tasks effort-responsive, turn would shed light whether use qualitybased payments tasks practice. comprehensive discussion relatedempirical work found Section 9. also observe model resultssingle non-null outcome applicable novel even task effort-responsive.Following bulk prior work dynamic pricing MAB, assumecollective worker response given contract, given distribution outcomes,depend contracts offered past, algorithm usedchoose future contracts. Thus, model possibility price experimentationmay alter future worker responses, workers may try game system.effects easy model extremely difficult analyze, even relativelysimple scenario single non-null outcome emphasis adaptive discretization.Additionally, available empirical work provide sufficient guidancechoose realistic model effects among theoretically plausible alternatives.leave future work.MAB point view, model incorporate possibilityworker response may intrinsically change time, fact requesters mayhard budget constraints total amount money spend. reflectslimitations state-of-the-art work MAB: adaptive discretization, budgets, adversarial change time fairly well-understood separately, two (letalone three) studied jointly. said, conjecture techniqueswould useful generalizing dynamic pricing dynamic contract design richersettings.5. upper bound informative D(x, y) < 1.325fiHo, Slivkins, & VaughanLikewise, model scenario requesters stream tasks overwhelmscrowdsourcing market causes drastic change available worker population(and therefore worker response). particular, assume worker poolsufficiently large accommodate requester. relatively benign assumptionlarge crowdsourcing system.deploy dynamic selection prices contracts practice (regardless particularalgorithm used) crowdsourcing platform needs enable requesters changeprices/contracts relatively fast response observed worker responses. featurecurrently instrumented commercial platforms Amazon Mechanical Turk,appears easily implementable engineering point view. believe mainhurdle would incorporate dynamic price/contract selection overall economicdesign market. Given multitude existing crowdsourcing markets relativeease deploying new market designs, believe direction well worth studying.3. Algorithm: AgnosticZoomingsection, specify algorithm. call AgnosticZooming zoomspromising areas action space, without knowing precisemeasure similarity contracts. zooming viewed dynamicform discretization. stating algorithm itself, discuss discretizationaction space detail, laying groundwork approach.3.1 Discretization Action Spaceround, AgnosticZooming algorithm partitions action space several regions chooses among regions, effectively treating region meta-arm.section, discuss subsets action space used regions, introduceuseful notions properties subsets.3.1.1 Increment Space Cellsdescribe approach discretization, useful think contracts termsincrement payments. Specifically, represent monotone contract x : [0, )vector x [0, )m , number non-null outcomes x = x()x( 1)0 non-null outcome . (Recall convention 0 null outcomex(0) = 0.) call vector increment representation contract x, denoteincr(x). Note x bounded, incr(x) [0, 1]m . Conversely, call contractweakly bounded monotone increment representation lies [0, 1]m .contract necessarily bounded.discretize space weakly bounded contracts, viewed multi-dimensionalunit cube. precisely, define increment space [0, 1]m conventionevery vector represents corresponding weakly bounded contract. region discretization closed, axis-aligned m-dimensional cube increment space; henceforth,cubes called cells. size cell length one side. cell calledrelevant contains least one candidate contract. relevant cell called atomiccontains exactly one candidate contract, composite otherwise.326fiAdaptive Contract Design Crowdsourcing Marketscomposite cell C, algorithm use two contracts: maximal corner,denoted x+ (C), increment payments maximal, minimal corner,denoted x (C), increment payments minimal. two contractscalled anchors C. atomic cell C, algorithm use one contract:unique candidate contract, also called anchor C. Note anchorsnecessarily candidate contracts.3.1.2 Virtual Widthtake advantage problem structure, essential estimate similarcontracts within given composite cell C are. Ideally, would like know maximaldifference expected utility:width(C) = supx,yC |U (x) U (y)| .estimate width using proxy, called virtual width, expressed termsanchors:VirtWidth(C) = V (x+ (C)) P (x (C)) V (x (C)) P (x+ (C)) .(1)definition one crucial place problem structure used. (Notedifference utility anchors.) useful due following lemma (provedSection 3.3).Lemma 3.1. types satisfy FOSD assumption consistent tie-breaking holds,width(C) VirtWidth(C) composite cell C.Recall proof lemma place paper useassumptions worker behavior. developments hold model workerbehavior satisfies Lemma 3.1.3.2 Description Algorithmideas place, ready describe algorithm. high-leveloutline AgnosticZooming simple. algorithm maintains set active cellscover increment space times. Initially, single active cellcomprising entire increment space. round t, algorithm chooses one activecell Ct using upper confidence index posts contract xt sampled uniformly randomamong anchors cell. observing feedback, algorithm may choosezoom Ct , removing Ct set active cells activating relevant quadrantsthereof, quadrants cell C defined 2m sub-cells half sizeone corners center C. remainder section, specifycell Ct chosen (the selection rule), algorithm decides whether zoomCt (the zooming rule).Let us first introduce notation. Consider cell C active round t. LetU (C) expected utility single round C chosen algorithm,i.e., average expected utility anchor(s) C. Let nt (C) number timescell chosen round t. Consider rounds C chosen327fiHo, Slivkins, & Vaughanalgorithm round t. Let Ut (C) average utility rounds.composite cell C, let Vt+ (C) Pt+ (C) average value average paymentrounds anchor x+ (C) chosen. Similarly, let Vt (C) Pt (C) averagevalue average payment rounds anchor x (C) chosen. Accordingly,estimate virtual width composite cell C timeWt (C) = Vt+ (C) Pt (C) Vt (C) Pt+ (C) .(2)bound deviations, define confidence radiuspradt (C) = crad log(T )/nt (C),(3)absolute constant crad ; analysis, crad 16 suffices. showhigh probability sample averages defined stay within radt (C) respectiveexpectations. high probability event holds, width estimate Wt (C) alwayswithin 4 radt (C) VirtWidth(C).algorithm pseudocode summarized Algorithm 1. selection rulezooming rule explained detail below.ALGORITHM 1: AgnosticZoomingInputs: subset Xcand X candidate contracts.Data structure: Collection cells. Initially, = { [0, 1]m }.round = 1Let Ct = argmaxCA (C), () defined Equation (4).Sample contract xt u.a.r. among anchors Ct . \\ Anchors defined Section 3.1.Post contract xt observe feedback.|Ct Xcand | > 1 5 radt+1 (Ct ) < Wt+1 (Ct ){all relevant quadrants Ct } \ {Ct }. \\ C relevant |C Xcand | 1.3.2.1 Selection Ruleselection rule follows. round t, algorithm chooses active cell Cmaximal index (). (C) upper confidence bound expected utilitycandidate contract C, defined(Ut (C) + radt (C)C atomic cell,(C) =(4)Ut (C) + Wt (C) + 5 radt (C) otherwise.nt (C) = 0, Ut (C) Wt (C) initialized finite values. Since radt (C)infinite nt (C) = 0, AgnosticZooming first select cell never selectedtime t.3.2.2 Zooming Rulezoom composite cell CtWt+1 (Ct ) > 5 radt+1 (Ct ),328fiAdaptive Contract Design Crowdsourcing Marketsi.e., uncertainty due random sampling, expressed confidence radius, becomessufficiently small compared uncertainty due discretization, expressed virtualwidth. never zoom atomic cells.3.2.3 Notes Integer Paymentspractice may necessary allow contracts payments integermultiples amount , e.g., whole cents. (In case assume candidatecontracts property, too.) redefine two anchors compositecell: maximal (resp., minimal) anchor nearest allowed contract maximal(resp., minimal) corner. Width redefined supremum allowed contractsgiven cell. modifications, analysis goes without significantchanges. omit discussion issue.3.3 Proof Lemma 3.1 (virtual width)two vectors x, x0 <m , write x0 x x0 pointwise dominates x, i.e., x0j xjj. two monotone contracts x, x0 , write x0 x incr(x0 ) incr(x).Claim 3.2. Consider worker whose type satisfies FOSD assumption two weaklybounded contracts x, x0 x0 x. Let e (resp., e0 ) effort levels exertedworker offered contract x (resp., x0 ). e FOSD e0 .Proof. sake contradiction, assume e FOSD e0 . Note e 6= e0 .Let workers type. Recall Fi (|e) denotes probability generatingoutcome 0 given effort level e. Define F = ( Fi (1|e) , . . . , Fi (m|e) ), define F0similarly e0 .Let x x0 increment representations x x0 . Given contract x,workers expected utility effort level e Ui (x|e) = x F ci (e). Since e optimaleffort level given contract, Ui (x|e) Ui (x|e0 ), thereforex F x F0 ci (e) ci (e0 ).Similarly, since e0 optimal effort level given contract x0 ,x0 F0 x0 F ci (e0 ) ci (e).Combining two inequalities, obtain(x x0 ) (F F0 ) 0.(5)Note Equation (5) holds equality Ui (x|e) = Ui (x|e0 ) Ui (x0 |e) =Ui (x0 |e0 ), worker breaks tie e e0 different way two differentcontracts. contradicts consistent tie-breaking assumption. However, Equation (5)cannot hold strict equality, either, x0 x (since e FOSD e0 )F F0 Fi (|e) > Fi (|e0 ) outcome > 0. Therefore obtaincontradiction, completing proof.proof Claim 3.2 place paper directly use consistenttie-breaking assumption. (But rest paper relies claim.)329fiHo, Slivkins, & VaughanClaim 3.3. Assume types satisfy FOSD assumption. Consider weakly boundedcontracts x, x0 x0 x. V (x0 ) V (x) P (x0 ) P (x).Proof. Consider worker, let type. Let e e0 chosen effort levelscontracts x x0 , respectively. FOSD assumption, either e = e0 , e0 FOSDe, e FOSD e0 . Claim 3.2 rules latter possibility.Define vectors F F0 proof Claim 3.2. Note F0 F.P = x F P 0 = x0 F0 expected payment contracts x x0 ,respectively. Further, letting v denote increment representation requesters valueoutcome, V = v F V 0 = v F0 expected requesters value contractsx x0 , respectively. Since x0 x F0 F, follows P 0 P V 0 V . Sinceholds worker, also holds expectation workers.finish proof Lemma 3.1, consider composite cell C anchors x+ =x = x (C), fix contract x C. Since x+ x x , Claim 3.3follows V (x+ ) V (x) V (x ) P (x+ ) P (x) P (x ). Therefore |U (x)U (y)| VirtWidth(C). Taking supremum x C over, obtain width(C)VirtWidth(C), claimed.x+ (C)4. Regret Bounds Discussionpresent main regret bound AgnosticZooming. Formulating result requiresnew, problem-specific structure. Stated terms structure, result somewhat difficult access. explain significance, state several corollaries, compareresults prior work.4.1 Main Resultstart main regret bound. Like algorithm itself, regret bound parameterized set Xcand candidate contracts; goal bound algorithmsregret respect candidate contracts.Recall OPT(Xcand ) = supxXcand U (x) optimal expected utility candidatecontracts. algorithms regret respect candidate contracts R(T |Xcand ) =OPT(Xcand ) U , time horizon U expected cumulative utilityalgorithm.Define badness (x) contract x X difference expected utilityoptimal candidate contract x: (x) = OPT(Xcand ) U (x). Let X = {x Xcand :(x) }.interested cells potentially used AgnosticZooming.Formally, recursively define collection feasible cells follows: (i) cell [0, 1]mfeasible, (ii) feasible cell C, relevant quadrants C feasible. Notedefinition feasible cell implicitly depends set Xcand candidate contracts:definition, feasible cell one contains candidate contract.Let F denote collection feasible, composite cells C VirtWidth(C). Xcand , let F (Y ) collection cells C F overlap ,let N (Y ) = |F (Y )|; sometimes write N (Y |Xcand ) place N (Y ) emphasizedependence Xcand .330fiAdaptive Contract Design Crowdsourcing MarketsUsing structure defined above, main theorem stated follows. provetheorem Section 6.Theorem 4.1. Consider dynamic contract design problem types satisfyingFOSD assumption constant number outcomes. Consider AgnosticZooming, parameterized set Xcand candidate contracts. Assume max(2m + 1, 18).absolute constant 0 > 0 > 0,XR(T |Xcand ) + O(log )=2j : jNN 0 (X |Xcand ).(6)Remark 1. discussed Section 2.2, target practically important case smallnumber outcomes. impact larger exponential dependenceO() notation, and, importantly, increased number candidate policies (typicallyexponential given granularity).Remark 2. regret bounds depend number worker types, lineprior work dynamic pricing. Essentially, bandit approaches tenddepend expected reward given arm (and perhaps also variance),finer properties distribution.Equation (6) shape similar several regret bounds literature,discussed below. make apparent, observe regret bounds banditsmetric spaces often stated terms covering numbers. (For fixed collectionF subsets given ground set X, covering number subset X relativeF smallest number subsets F sufficient cover .) numbersN (Y |Xcand ) are, essentially, covering feasible cells virtual width close. make point precise follows. Let -minimal cell cell Fcontain cell F . Let Nmin (Y ) covering number relativecollection -minimal cells, i.e., smallest number -minimal cells sufficientcover .N (Y ) dlog 1 e Nmin (Y ) Xcand 0,(7)smallest size feasible cell.6 Thus, Equation (6) easily restatedusing covering numbers Nmin () instead N ().4.2 Corollary: Polynomial RegretLiterature regret-minimization often states polynomial regret bounds formR(T ) = O(T ), < 1. covering-number regret bounds precise versatile, exponent polynomial regret bound expresses algorithms performanceparticularly succinct lucid way.bandits metric spaces exponent typically determined appropriately defined notion dimension, covering dimension,7 succinctly6. prove Equation (7), observe cell C F (Y ) exists -minimal cell C 0 C,-minimal cell C 0 exist dlog 1 e cells C F (Y ) C 0 C.7. Given covering numbers N (), covering dimension smallest 0 N (Y ) =O(d ) > 0.331fiHo, Slivkins, & Vaughancaptures difficulty problem instance. Interestingly, dependencedimension typically shape; = (d + 1)/(d + 2), several different notionsdimension. line tradition, define width dimension:nWidthDim = inf 0 : N 0 (X |Xcand ) > 0 , > 0.(8)Note width dimension depends Xcand problem instance, parameterized constant > 0. optimizing choice Equation (6), obtainfollowing corollary.Corollary 4.2. Consider setting Theorem 4.1. > 0, let = WidthDim .R(T |Xcand ) O( log ) (1+d)/(2+d) .(9)width dimension similar zooming dimension work Kleinberget al. (2008) near-optimality dimension work bandits metric spaces(Bubeck et al., 2011a).4.3 Comparison Prior Workcompare results previous work non-adaptive discretization banditsmetric spaces.4.3.1 Non-Adaptive DiscretizationOne approach prior work directly applicable dynamic contract designproblem non-adaptive discretization. algorithm, call NonAdaptive,runs off-the-shelf MAB algorithm, treating set candidate contracts Xcand arms.8concreteness, following prior work (Kleinberg & Leighton, 2003; Kleinberg,2004; Kleinberg et al., 2008), use well-known algorithm UCB1 (Auer, Cesa-Bianchi, &Fischer, 2002) off-the-shelf MAB algorithm.compare AgnosticZooming NonAdaptive, useful derive several worstcase corollaries Theorem 4.1, replacing N (X ) various (loose) upper bounds.9Corollary 4.3. setting Theorem 4.1, regret AgnosticZooming upperbounded follows:P(a) R(T |Xcand ) + =2j : jN O(|X | /), (0, 1).p(b) R(T |Xcand ) O( |Xcand |).O() notation hides logarithmic dependence .best known regret bounds NonAdaptive coincide Corollary 4.3poly-logarithmic factors. However, regret bounds Theorem 4.1 may significantlybetter ones Corollary 4.3. discuss next section,context specific example.8. simplify proofs lower bounds, assume candidate contracts randomlypermuted given MAB algorithm.9. use facts X Xcand , N (Y ) N0 (Y ), N0min (Y ) |Y | subsets X.332fiAdaptive Contract Design Crowdsourcing Markets4.3.2 Bandits Metric SpacesConsider variant dynamic contract design algorithm given prioriinformation similarity contracts: function : Xcand Xcand [0, 1]|U (x) U (y)| D(x, y) two candidate contracts x, y. algorithm givenfunction (call algorithm D-aware), machinery bandits metric spaces(Kleinberg et al., 2008; Bubeck et al., 2011a) used perform adaptive discretizationobtain significant advantage NonAdaptive. argue obtain similarresults AgnosticZooming without knowing D.practice, similarity information would coarse, probably aggregated accordingpredefined hierarchy. formalize idea, hierarchy representedcollection F subsets Xcand , D(x, y) function smallest subsetF containing x y. hierarchy F natural given structurecontract space. One natural hierarchy collection feasible cells,corresponds splitting cells half dimension. Formally, D(x, y) = f (Cx,y )f f (Cx,y ) width(Cx,y ), Cx,y smallest feasible cell containingx y.Given shape D, let us state regret bounds D-aware algorithms workKleinberg et al. (2008) Bubeck et al. (2011a). simplify notation, assumeaction space restricted Xcand . regret bounds similar shapeTheorem 4.1:R(T |Xcand ) + O(log )(X )N()X=2j :jN,(10)numbers N () similar high-level meaning N (), nearly coincideNmin () D(x, y) = VirtWidth(Cx,y ). One use Equation (10) derivepolynomial regret bound like Equation (9).precise comparison, focus results work Kleinberg et al.(2008). (The regret bounds Bubeck et al., 2011a similar spirit, statedterms slightly different structure.) covering-type regret bound workKleinberg et al. (2008) focuses balls radius according distance D,N (Y ) smallest number balls sufficient cover . special caseD(x, y) = VirtWidth(Cx,y ) balls radius precisely feasible cells virtual width. similar (albeit technically same) -minimal cellsdefinition Nmin ().Further, covering numbers N (Y ) determine zooming dimension:nZoomDim = inf 0 : N/8(X ) > 0 , > 0.(11)definition coincides covering dimension worst case, muchsmaller nice problem instances X significantly small subset Xcand .definition, one obtains polynomial regret bound version Equation (9) = ZoomDim .conclude AgnosticZooming essentially matches regret bounds D-awarealgorithms, despite fact D-aware algorithms access much information.333fiHo, Slivkins, & Vaughan5. Special Case: High-Low Exampleapply machinery Section 4 special case, show AgnosticZoomingsignificantly outperforms NonAdaptive.basic special case one non-null outcome. Essentially,worker makes strategic choice whether accept reject given task (where rejectcorresponds null effort level), choice fully observable. settingstudied (Kleinberg & Leighton, 2003; Badanidiyuru et al., 2012; Singla & Krause,2013; Badanidiyuru et al., 2013); call dynamic task pricing. contractcompletely specified price p non-null outcome. supply distributionsummarized function S(p) = Pr[accept|p], corresponding expected utilityU (p) = S(p)(v p), v value non-null outcome. special casealready quite rich, S() arbitrary non-decreasing function. usingadaptive discretization, achieve significant improvement prior work; see Section 8discussion.consider somewhat richer setting workers strategic decisionsobservable; salient feature setting, called moral hazard contracttheory literature. two non-null outcomes (low high), two non-null effortlevels (low high). Low outcome brings zero value requester, high outcomebrings value v > 0. Low effort level inflicts zero cost worker leads low outcomeprobability 1. assume workers break ties effort levels consistentway: high better low better null. (Hence, low effort incurs zero cost,possible outcomes low high.) call high-low example;perhaps simplest example features moral hazard.example, workers type consists pair (ch , h ), ch 0 costhigh effort h [0, 1] probability high outcome given high effort. Notedynamic task pricing equivalent special case h = 1.following claim states crucial property high-low example.Claim 5.1. Consider high-low example fixed supply distribution. probability obtaining high outcome given contract x Pr[high outcome | contract x] dependsp = x(high) x(low); denote probability S(p). Moreover, S(p) non-decreasingp. Therefore:expected utility U (x) = S(p)(v p) x(low).discretization error OPT(X) OPT(Xcand ()) 3, > 0.bound discretization error, essential S(p) non-decreasing p.Recall Xcand (), uniform mesh granularity > 0, consists bounded,monotone contracts payments N.purposes, supply distribution summarized via function S(). DenoteU (p) = S(p)(v p). Note U (x) maximized setting x(low) = 0, caseU (x) = U (p). Thus, algorithm knows given high-low example, setx(low) = 0, thereby reducing dimensionality search space. problemessentially reduces dynamic task pricing S().However, general algorithm know whether presented highlow example (because effort levels observable). followsconsider algorithms restrict x(low) = 0.334fiAdaptive Contract Design Crowdsourcing Markets5.1 Nice Supply Distributionfocus supply distribution nice, sense S() satisfiesfollowing two properties:S(p) Lipschitz-continuous: |S(p) S(p0 )| L|p p0 | constant L.U (p) strongly concave, sense U 00 () exists satisfies U 00 () C < 0.L C absolute constants. call strongly Lipschitz-concave.properties fairly natural. example, satisfied hworker types marginal distribution ch piecewise uniformdensity 1 , absolute constant 1.show choice Xcand X, AgnosticZooming small width dimensionsetting, therefore small regret.Lemma 5.2. Consider high-low example strongly Lipschitz-concave supply distribution. width dimension 21 , given Xcand X. Therefore,AgnosticZooming Xcand regret R(T |Xcand ) = O(log ) 3/5 .contrast performance NonAdaptive, parameterized naturalchoice Xcand = Xcand (). focus R(T |X): regret w.r.t. best contract X.show AgnosticZooming achieves R(T |X) = O(T 3/5 ) wide range Xcand , whereasNonAdaptive cannot better R(T |X) = O(T 3/4 ) Xcand = Xcand (), > 0.Lemma 5.3. Consider setting Lemma 5.2. Then:(a) AgnosticZooming Xcand Xcand (T 2/5 ) regret R(T |X) = O(T 3/5 log ).(b) NonAdaptive Xcand = Xcand () cannot achieve regret R(T |X) < o(T 3/4 )problem instances, > 0. 105.2 ProofsProof Claim 5.1. Consider contract x x(low) = b x(high) = b + p,worker type (ch , h ). worker exerts high effort, pays cost ch receivesexpected payment h (p + b) + (1 h )b, total expected payoff ph + b ch .expected payoff exerting low effort b. Therefore choose exert high effortph + b ch b, i.e., ch /h p, choose exert low effort otherwise.ThereforePr[high outcome | contract x] = E h 1{ch /h p} .(ch ,h )function p, call S(p). Moreover, non-decreasing function simplyexpression inside expectation non-decreasing p.trivially follows U (x) = S(p)(v p) x(low).upper-bound discretization error using standard approach workdynamic pricing (Kleinberg & Leighton, 2003). Fix discretization granularity > 0.> 0, exists contract x X OPT(X) U (x ) < . Round x (high)10. lower bound holds even UCB1 NonAdaptive replaced MAB algorithm.335fiHo, Slivkins, & Vaughanx (low) down, respectively, nearest integer multiple ; let x Xcand ()resulting contract. Denoting p = x(high) x(low) p = x (high) x (low),see p p p + 2. followsU (x) U (x ) 3 OPT(X) 3.Since holds > 0, conclude OPT(X) OPT(Xcand ()) 3.Proof Lemma 5.2. calculate width dimension, need count numberfeasible cells increment space (i) virtual width larger equalO() (ii) overlap X , set contracts badness smaller .first characterize X . use xp,b denote contract x(high) = p + bx(low) = b. benefit representation that, p b would two axesincrement space. Let xp ,0 optimal contract. Since U (xp,b ) strongly concavep, know b, exist constants c1 c2 p [0, 1],c1 (p p)2 U (xp ,b ) U (xp,b ) c2 (p p)2 . Also know U (xp ,b ) = U (xp ,0 ) b.Therefore.X = {xp,b : (p p )2 + b O()}also writeX = {xp,b : p h ( ) p p + h ( ) b O()}Intuitively, X contains contracts {xp,b } p O( ) away p b O()away b = 0.Next characterize virtual width cell. use Cp,b,d denote cellsize anchors {xp,b , x(p+d),(b+d) }. derive expected payment valuetwo anchors as:P + (Cp,b,d ) = (p + d)S(p + d) + b +V + (Cp,b,d ) = vS(p + d)P (Cp,b,d ) = pS(p) + bV (Cp,b,d ) = vS(p)definition, get (we use dF represent S(p + d) S(p) simplification)VirtWidth(Cp,b,d ) = (v + p)dF + dS(p) + dF + d.count number feasible cells virtual width larger h ()overlaps X . Note since total number feasible cells Cp,b,d largesmall, treat number cells large constant. Also, relevantcell Cp,b,d , p p . Therefore, care feasible cells Cp,b,d smallp close p .Since S(p) Lipschitz, dF = O(d). Therefore, relevant cell Cp,d ,VirtWidth(Cp,b,d ) = O(d)Given two arguments, know number cells virtual widthlarger also overlaps X O(/) O( /) = O(1/2 ). Thereforewidth dimension 1/2.336fiAdaptive Contract Design Crowdsourcing MarketsProof Sketch Lemma 5.3(b). Consider version NonAdaptive runs off-theshelf MAB algorithm ALG candidate contracts Xcand = Xcand (). ALG, armscandidate contracts; recall arms randomly permutedgiven ALG.Fix > 0. easy construct problem instance discretization error Error ,OPT(X) OPT(Xcand ()) (). Note Xcand contains N = ( 2 ) suboptimal contracts suboptimal w.r.t. OPT(Xcand ). (For example, contracts x x(low) > 0suboptimal.)Fix problem instance MAB N suboptimal arms. Using standard lowerbound arguments MAB, one show one runs ALG problem instanceobtainedby randomly permuting arms I, expected regret roundsleast ( N ).Therefore, R(T |Xcand ) ( N ). followsR(T |X) ( N ) + Error ( / + ) (T 3/4 ).6. Proof Main Regret Bound (Theorem 4.1)prove main result Section 4. high-level approach defineclean execution algorithm execution high-probability eventssatisfied, derive bounds regret conditional clean execution. analysisclean execution involve probabilistic arguments. approach tendssimplify regret analysis.start listing simple invariants enforced AgnosticZooming:Invariant 6.1. round execution AgnosticZooming:(a) active cells relevant,(b) candidate contract contained active cell,(c) Wt (C) 5 radt (C) active composite cell C.Note zooming rule essential ensure Invariant 6.1(c).Throughout, say algorithm activates cell cell addedcollection active cells. cell stays active activated.6.1 Analysis RandomnessDefinition 6.2 (Clean Execution). execution AgnosticZooming called cleanround active cell C holds|U (C) Ut (C)| radt (C),|VirtWidth(C) Wt (C)| 4 radt (C)(12)(if C composite).(13)Lemma 6.3. Assume crad 16 max(1 + 2m , 18). Then:(a) Pr [ Equation (12) holds rounds t, active cells C ] 1 2 2 .(b) Pr [ Equation (13) holds rounds t, active composite cells C ] 1 16 2 .Consequently, execution AgnosticZooming clean probability least 1 1/T .337fiHo, Slivkins, & VaughanLemma 6.3 follows standard concentration inequality known ChernoffBounds. However, one needs careful conditioning details.Proof Lemma 6.3(a). Consider execution AgnosticZooming. Let N totalnumber activated cells. Since 2m cells activated one round,N 1 + 2m 2 . Let Cj min(j, N )-th cell activated algorithm. (If multiplequadrants activated round, order according fixed orderingquadrants.)Fix feasible cell C j 2 . claimPr [ |U (C) Ut (C)| radt (C) rounds | Cj = C ] 1 2 4 .(14)Let n(C) = n1+T (C) total number times cell C chosen algorithm.N: 1 n(C) let Us requesters utility round Cchosen s-th time. Further, let DC distribution U1 , conditionalevent n(S) 1. (That is, per-round reward choosing cell C.) Let U10 , . . . , UT0family mutually independent random variables, distribution DC .n , conditional event {Cj = C} {n(C) = n}, tuple (U1 , . . . , Un )joint distribution tuple (U10 , . . . , Un0 ). Consequently, applying ChernoffBounds latter tuple, followsfihfifi qPfiPr fiU (C) n1 ns=1 Us fi n1 crad log(T ) fi {Cj = C} {n(C) = n}1 2 2crad 1 2 5 .Taking Union Bound n , plugging radt (Cj ), nt (Cj ), Ut (Cj ),obtain Equation (14).Now, let us keep j fixed Equation (14), integrate C. precisely, let usmultiply sides Equation (14) Pr[Cj = C] sum feasible cells C.obtain, j 2 :Pr [ |U (Cj ) Ut (Cj )| radt (Cj ) rounds ] 1 2 4 .(15)(Note obtain Equation (15), need take Union Boundfeasible cells C.) conclude, take Union Bound j 1 + 2 .Proof Sketch Lemma 6.3(b). showfifiPr fiV + (C) Vt+ (C)fi radt (C) rounds t, active composite cells C 14,T2(16)similarly V (), P + () P (). four statements proved similarly,using technique Lemma 6.3(a). follows, sketch proof onefour cases, namely Equation (16).given composite cell C, interested rounds anchor x+ (C)selected algorithm. Letting n+(C) number times anchor chosentime t, let us define corresponding notion confidence radius:1 crad log+radt (C) =.2n+(C)338fiAdaptive Contract Design Crowdsourcing Marketstechnique proof Lemma 6.3(a), establish followinghigh-probability event:fifi +fiV (C) V + (C)fi rad+ (C).(17)precisely, provePr [ Equation (17) holds rounds t, active composite cells C ] 1 2 2 .Further, need prove w.h.p. anchor x+ (C) played sufficiently often.111Noting E[n+(C)] = 2 nt (C), establish auxiliary high-probability event:n+(C)12nt (C) 14 radt (C).(18)precisely, use Chernoff Bounds show that, crad 16,Pr [ Equation (18) holds rounds t, active composite cells C ] 1 2 2 .(19)Now, letting n0 = (crad log )1/3 , observent (C) n0nt (C) < n01n+(C) 4 nt (C)radt (C) 1+(C),fifirad+t (C) radfiV (C) V + (C)fi radt (C).fifiTherefore, Equations (17) (18) hold, fiV + (C) Vt+ (C)fi radt (C).completes proof Equation (16).6.2 Analysis Clean Executionrest analysis focuses clean execution. Recall Ct cell chosenalgorithm round t.Claim 6.4. clean execution, I(Ct ) OPT(Xcand ) round t.Proof. Fix round t, let x candidate contract. Invariant 6.1(b), existsactive cell, call Ct , contains x .claim (Ct ) U (x ). consider two cases, depending whether Ctatomic. Ct atomic anchor unique, U (Ct ) = U (x ), (Ct ) U (x )clean execution. Ct composite(Ct ) U (Ct ) + VirtWidth(Ct )U (Ct )U (x )+clean executionwidth(Ct )Lemma 3.1definition width, since x Ct .proved (Ct ) U (x ). Now, selection rule (Ct ) (Ct )U (x ). Since holds candidate contract x , claim follows.11. constantproof.14Equation (18) enable consistent choice n0 remainder339fiHo, Slivkins, & VaughanClaim 6.5. clean execution, round t, index (Ct ) upper-boundedfollows:(a) Ct atomic I(Ct ) U (Ct ) + 2 radt (Ct ).(b) Ct composite I(Ct ) U (x) + O(radt (Ct )) contract x Ct .Proof. Fix round t. Part (a) follows (Ct ) = Ut (Ct ) + radt (Ct ) definitionindex, Ut (Ct ) U (Ct ) + radt (Ct ) clean execution.part (b), fix contract x Ct . Then:Ut (Ct ) U (Ct ) + radt (Ct )clean executionU (x) + width(Ct ) + radt (Ct )definition widthU (x) + VirtWidth(Ct ) + radt (Ct )Lemma 3.1U (x) + Wt (Ct ) + 5 radt (Ct )clean execution.(Ct ) = Ut (Ct ) + Wt (Ct ) + 5 radt (Ct )(20)definition indexU (x) + 2 Wt (Ct ) + 10 radt (Ct )Equation (20)U (x) + 20 radt (Ct )Invariant 6.1(c).relevant cell C, define badness (C) follows. C composite, (C) =supxC (x) maximal badness among contracts C. C atomic x Cunique candidate contract C, (C) = (x).Claim 6.6. clean execution, (C) O(radt (C)) round activecell C.Proof. Claims 6.4 6.5, (Ct ) O(radt (Ct )) round t. Fix roundlet C active cell round. C never selected round t, claimtrivially true. Else, let recent round C selectedalgorithm. (C) O(rads (C)). claim follows since rads (C) = radt (C).Claim 6.7. clean execution, cell C selected O(log /((C))2 ) times.Proof. Claim 6.6, (C) O(radT (C)). claim follows definition radTEquation (3).Let n(x) n(C) number times contract x cell C, respectively, chosenalgorithm. regret algorithmR(T |Xcand ) =PxXn(x) (x)Pcells Cn(C) (C).(21)next result (Lemma 6.8) upper-bounds right-hand side Equation (21) cleanexecution. Lemma 6.3, suffices complete proof Theorem 4.1Lemma 6.8. Consider clean execution AgnosticZooming. (0, 1),Pcells Cn(C) (C) + O(log )340P=2j : jN|F (X2 )|.fiAdaptive Contract Design Crowdsourcing Marketsproof Lemma 6.8 relies simple properties (), stated below.Claim 6.9. Consider two relevant cells C Cp . Then:(a) (C) (Cp ).(b) (C) > 0, C overlaps X .Proof. prove part (a), one needs consider two cases, depending whether cell Cpcomposite. is, claim follows trivially. Cp atomic, C atomic, too,(C) = (Cp ) = (x), x unique candidate contract Cp .part (b), exists candidate contract x C. easy see (x) (C)(again, consider two cases, depending whether C composite.) So, x X .Proof Lemma 6.8. Let denote sum question. Let collectioncells ever activated algorithm. Among cells, consider badnessorder :G := { C : (C) [, 2) } .Claim 6.7, algorithm chooses cell C G O(log /2 ) times,n(C) (C) O(log /).Fix (0, 1) observe cells C (C) contribute. Therefore suffices focus G , /2. followsP+ O(log ) =2i /2 |G | .(22)bound |G | follows. Consider cell C G . cell called leaf neverzoomed (i.e., removed active set) algorithm. C activatedround cell Cp zoomed on, Cp called parent C. consider two cases,depending whether C leaf.(i) Assume cell C leaf. Since (C) < 2, C overlaps X2 Claim 6.9(b).Note C zoomed round, say round 1.5 radt (C) Wt (C)zooming ruleVirtWidth(C) + 4 radt (C)clean execution,radt (C) VirtWidth(C). Therefore, using Claim 6.6,(C) O(radt (C)) O(VirtWidth(C)).follows C F() (X2 ).(ii) Assume cell C leaf. Let Cp parent C. Since C Cp , (C)(Cp ) Claim 6.9(a). Therefore, invoking case (i),(C) (Cp ) O(VirtWidth(Cp )).Since (C) < 2, C overlaps X2 Claim 6.9(b), therefore Cp .follows Cp F() (X2 ).fifiCombing two cases, follows |G | (2m + 1) fiF() (X2 )fi. Plugging(22) making appropriate substitution () simplify resulting expression,obtain regret bound Theorem 4.1341fiHo, Slivkins, & Vaughan7. Simulationsevaluate performance AgnosticZooming simulations. AgnosticZoomingcompared two versions NonAdaptive use, respectively, two standard banditalgorithms: UCB1 (Auer et al., 2002) Thompson Sampling (Thompson, 1933)Gaussian priors. algorithms, round numerical score (called index )computed arm, arm maximal index chosen. UCB1, indexarm high-confidence upper bound expected reward arm. ThompsonSampling, index sampled independently Bayesian posterior distributionarms expected reward.7.1 Setupconsider generalized version high-low example Section 5requesters value low outcome could nonzero. results reported below,set requesters values V (high) = 1 V (low) = .3, probability obtaininghigh outcome given high effort h = .8. explicitly report results,additionally tried wide range alternative values V (high), V (low), h foundsimilar qualitatively. Intuitively, varying requesters values hchanges contracts algorithms converge (that is, optimal arms),impact problem structure; width dimension settings.generalized high-low example, workers type characterized cost chhigh effort. consider three supply distributions:Uniform: ch uniformly distributed [0, 1].Homogeneous: ch every worker.Two-type: ch uniformly distributed two values, c0h c00h .first two distributions represent extreme cases workers eitherextremely homogeneous extremely diverse. third distribution one way getmiddle ground. distribution, run algorithm 100 times.12Homogeneous Supply Distribution, ch drawn uniformly random [0, 1] run.Two-Type Supply Distribution, c0h c00h drawn independently uniformly[0, 1] run.UCB1 AgnosticZooming, replace logarithmic confidence termssmall constants. find beneficial practice algorithms, consistentprior work (Radlinski, Kleinberg, & Joachims, 2008; Slivkins, Radlinski, & Gollapudi,2013). algorithms, tried several different constants found performancesensitive particular constant used long order 1.results reported below, set confidence terms equal 1. UCB1, meansgiven arm played na times,p index average reward plus 1/ na .AgnosticZooming, means radt () = 1/nt ().three algorithms run Xcand = Xcand (), > 0 parameterspecifying granularity discretization.12. standard errors plots order 0.001 less. (Note pointaverage 100 runs also average previous rounds.)342fiAdaptive Contract Design Crowdsourcing Markets7.2 Overview Results.Across simulations, AgnosticZooming either outperforms nearly matches NonAdaptive.performance appear suffer large hidden constants appearanalysis. find AgnosticZooming converges faster NonAdaptivenear-optimal smaller. consistent intuition AgnosticZoomingfocuses exploring promising regions contract space. large,AgnosticZooming converges slowly NonAdaptive, eventually achieves similarperformance. Further, find AgnosticZooming small performs well comparedNonAdaptive larger . particular, much worse initially, much bettereventually.simulations suggest time horizon known advance one tune, NonAdaptive achieve near-optimal performance. However, real applications approximately optimal may difficult compute, may knownadvance. AgnosticZooming performs consistently well wide range thereforerequire prior knowledge careful tuning .7.3 Detailed Resultsalgorithm, compute time-averaged cumulative utility rounds givenb (T, ), various values .granularity , denoted Ub (T, ) changesFirst, fix time horizon 5,000 rounds, study U. results shown Figure 1. observe AgnosticZooming either closelymatches outperforms versions NonAdaptive across supply distributionsvalues . AgnosticZooming performs consistently well differentperformance versions NonAdaptive decreases rapidly small.Second, study three algorithms perform time. Specifically, plotb (T, ), three values , namely 0.02, 0.08, 0.2. Since setting =vs. U0.08 close optimal examples, values represent, respectively, values small, adequate, large. results shown Figure 2.small values , AgnosticZooming quickly zooms promising regions contract space, leading faster converge alternatives. However, large,AgnosticZooming converges slowly, eventually achieves similar performance.regime, AgnosticZooming reap benefits adaptive discretizationmesh candidate contracts sparse, still suffers overhead. suggeststime horizon known advance one optimize given ,NonAdaptive achieve near-optimal performance. AgnosticZooming performs consistently different choices therefore require either prior knowledgecareful tuning .demonstrate benefit know tune , compareperformance AgnosticZooming small NonAdaptive differentb (T, ). See Figure 3.values . algorithm choice , plot vs. Ushow results Uniform Supply Distribution since results distributions similar. Additionally, omit results Thompson Sampling sinceUCB1 performed better experiments.13 find small , AgnosticZooming13. conjecture replaced logarithmic confidence term UCB1 1.343fi0.40.40.30.30.20.20.10.0AgnosticZoomingUCB1ThompsonSampling0.10.20.000.050.100.15Average UtilityAverage UtilityHo, Slivkins, & Vaughan0.10.0AgnosticZoomingUCB1ThompsonSampling0.10.20.200.00(a) Uniform Supply Distribution0.050.100.150.20(b) Homogeneous Supply Distribution0.4Average Utility0.30.20.10.0AgnosticZoomingUCB1ThompsonSampling0.10.20.000.050.100.150.20(c) Two-Type Supply DistributionFigure 1: requesters average per-round utility 5,000 rounds vs. choiceinitial discretization .small converges nearly fast NonAdaptive larger . large,AgnosticZooming small matches NonAdaptive optimal .Finally, Figure 4, confirm intuition OPT(Xcand ()) decreasesgranularity . end, run AgnosticZooming 50,000 rounds (so algorithmtime nearly converge optimal contract), examine average utilitylast 5,000 rounds. expected, see average requester utility achievablesmall significantly higher utility achievable larger.simulation results suggest AgnosticZooming performs well across differentsupply distributions different settings , requiring careful tuning algorithmparameters. Given smaller value , better payoff optimalcontract OPT(Xcand ()), AgnosticZooming small good algorithm varietysettings.344fiAdaptive Contract Design Crowdsourcing MarketsAgnosticZooming0.40.30.20.10.00.10.2ThompsonSamplingUniform: =0.02Uniform: =0.08Uniform: =0.20Two-Type: =0.02Two-Type: =0.08Two-Type: =0.20Homogeneous: =0.02Homogeneous: =0.08Homogeneous: =0.20Average Utility0.40.30.20.10.00.10.2UCB10.40.30.20.10.00.10.201000 2000 3000 4000 500001000 2000 3000 4000 500001000 2000 3000 4000 5000TimeFigure 2: requesters average per-round utility time different supply distributions discretization sizes.8. Application Dynamic Task Pricingdiscuss dynamic task pricing, seen special case dynamic contractdesign exactly one non-null outcome. identify important familyproblem instances AgnosticZooming out-performs NonAdaptive.8.1 Backgrounddynamic task pricing problem, basic version, defined follows.one principal (buyer) sequentially interacts multiple agents (sellers).round t, agent arrives, one item sale. principal offers price pt item,agent agrees sell pt ct , ct [0, 1] agents privatecost item. principal derives value v item bought; utilityvalue bought items minus payment. time horizon (the number rounds)known. private cost ct independent sample fixed distribution,called supply distribution. interested prior-independent version,supply distribution known principal. algorithms goal chooseoffered prices pt maximize expected utility principal.345fiHo, Slivkins, & VaughanUniform0.5AgnosticZooming: = .02UCB1: = .02UCB1: = .08UCB1: = .200.4Average Utility0.30.20.10.00.10.2010002000Time300040005000Figure 3: requesters average per-round utility time using AgnosticZoomingsmall compared NonAdaptive three different values .UniformAverage Utility Last 5k rounds0.40.30.20.10.00.1AgnosticZooming0.20.000.050.100.150.200.250.30Figure 4: Average requester utility last 5,000 rounds 50,000-round runAgnosticZooming different values .Dynamic task pricing seen special case dynamic contract designexactly one non-null outcome (which corresponds sale). Indeed,special case exactly one non-null effort level e without loss generality (becausenon-null effort levels deterministically lead non-null outcome).346fiAdaptive Contract Design Crowdsourcing MarketsOne crucial simplification compared full generality dynamic contract designdiscretization error easily bounded above: 14OPT(X) OPT(Xcand ())> 0.Worst-case regret bounds implicit prior work dynamic inventory-pricing (Kleinberg & Leighton, 2003).15 Let NonAdaptive() denote algorithm NonAdaptive Xcand =Xcand (). Then, analysis work Kleinberg Leighton (2003), NonAdaptive()achieves regret R(T ) = O(T + 2 ). optimized R(T ) = O(T 2/3 )= O(T 1/3 ). Moreover, matching lower bound: R(T ) = (T 2/3 )algorithm.Further, folklore result NonAdaptive() achieves regret R(T ) = O(T 2/3 )= (T 1/3 ). (We sketch lower-bounding example proof Lemma 8.4,make paper self-contained.)8.2 Preliminariescontract summarized single number: offered price p non-nulloutcome. Let F (p) probability worker accepting task price p, letU (p) = F (p) (v p) corresponding expected utility algorithm.Note contracts trivially monotone optimal contract boundedwithout loss generality. follows OPT(X) = supp0 U (p), optimal expectedutility possible prices.cell C price interval C = [p, p0 ] [0, 1], virtual widthVirtWidth(C) = v F (p0 ) p F (p) v F (p) p0 F (p0 ) .8.3 Results: General Caseusing AgnosticZooming Xcand = X.First, let us prove reasonable choice worst case: namely,achieve optimal O(T 2/3 ) regret.Lemma 8.1. Consider dynamic task pricing problem. AgnosticZooming Xcand =X achieves regret O(T 2/3 log ).Proof Sketch. Fix > 0. key observation VirtWidth(C) eitherp0 p 4 , F (p0 ) F (p) 4 . Call C red cell former happens, blue cellotherwise. Therefore collection mutually disjoint cells virtual widthO( 1 ) red cells O( 1 ) blue cells, hence O( 1 ) cells total.follows O( 1 ) active cells virtual width .So, notation Theorem 4.1 N () O( 1 ). follows widthdimension 1, turn implies desired regret bound.14. Recall Xcand () denotes set prices [0, 1] integer multiples given > 0; callset additive -mesh.15. algorithmic result dynamic task pricing easy modification analysis workKleinberg Leighton (2003) dynamic inventory-pricing. lower bound work KleinbergLeighton also translated dynamic inventory-pricing dynamic task pricing withoutintroducing new ideas. omit details version.347fiHo, Slivkins, & Vaughan8.4 Results: Nice Problem Instancesfocus problem instances piecewise-uniform costs bounded density. Formally,say instance dynamic task pricing k-piecewise-uniform costs interval[0,1] partitioned k N sub-intervals supply distribution uniformsub-interval. problem instance -bounded density, 1 supplydistribution probability density function almost everywhere, density1. Using full power Theorem 4.1, obtain following regret bound.Theorem 8.2. Consider dynamic task pricing problem k-piecewise-uniform costs-bounded density, absolute constants k N > 1. AgnosticZoomingXcand = X achieves regret R(T ) = O(T 3/5 ).Proof Sketch. Since supply distribution density , follows F ()Lipschitz-continuous function Lipschitz constant . follows cell virtualwidth least diameter least (/), > 0. (Note cellsimply sub-interval [p, q] [0, 1], diameter simply q p.)Second, claim X contained union k intervals diameter O( ).see this, consider partition [0, 1] k subintervals supply distributionuniform density subinterval. Let [pj , qj ] j-th subinterval. Let pjlocal optimum U () subinterval, let Xj, = {x [pj , qj ] : U (pj ) U (x) }.X j Xj, . show Xj, [pj , pj + ] = O( ).Recall N0 (X ) number feasible cells virtual width least 0overlap X . follows N0 (X ) k times maximal numberfeasible cells diameter least (/) overlap interval diameter O( ).Therefore: N0 (X ) = O(k3/2 1/2 log 1 ). Moreover, less sophisticated upperbound N0 (X ): number feasible cell diameter least (/).N0 (X ) = O(/)(log 1 ). theorem follows plugging upper boundsN0 (X ) Equation (6).8.5 Comparison NonAdaptiveConsider NonAdaptive(0 ), 0 = (T 1/3 ) granularity required optimalworst-case performance. Call problem instance nice 2-piecewise-uniform costs-bounded density, sufficiently large absolute constant ; say = 4concreteness. claim AgnosticZooming outperforms NonAdaptive(0 ) niceproblem instances.Lemma 8.3. NonAdaptive(0 ) achieves regret R(T ) = (T 2/3 ) worst casenice problem instances.Proof Sketch. Recall k = 2 supply distribution density 1 interval[0, p0 ], density 2 interval [p0 , 1], numbers 1 , 2 , p0 . pick p0sufficiently far point Xcand (0 ). Note function U () parabolatwo intervals. adjust densities U () achieves maximump0 , maximum either two parabolas sufficiently far p0 .discretization error Xcand (0 ) least (0 ), implies regret (0 ).348fiAdaptive Contract Design Crowdsourcing Markets8.6 Lower Bound NonAdaptiveprovide specific lower-bounding example worst-case performance NonAdaptive(),arbitrary > 0. Let F family problem instances k-piecewiseuniform costs -bounded density, k N = 4.Lemma 8.4. Let R (T ) maximalproblem inp regret NonAdaptive()2/3stances F. R (T ) = (T + /) (T ).Proof Sketch. piecewise-uniform costs, F (0) = 0 F (p) = 1. Assumeprincipal derives value v = 1 item. expected utility price pU (p) = F (p)(1 p).Fix > 0. Use following problem instance. Let P = [ 25 , 35 ] {4j + : j N}.Set U (p) = 41 p P0 . Further, pick p P/2 set U (p ) = 41 + ().defines F (p) p P {0, 1, p }. rest prices, define F () via linearinterpolation. completes description problem instance.show X consists N = ( 1 ) candidate contracts. Therefore, using stanpdard lower-bounding arguments MAB, obtain R(T |Xcand ) ( N ) = ( /).Further, show discretization error least (), implying R(T )R(T |Xcand ) + (T ).9. Related Workpaper related three different areas: contract theory, market design crowdsourcing, online decision problems. outline connectionsareas.9.1 Contract Theorymodel viewed extension classic principal-agent model contracttheory (Laffont & Martimort, 2002). basic version classic model,single principal interacts single agent whose type (specified cost functionproduction function, described Section 2) generally assumed known.principal specifies contract mapping outcomes payments principal commitsmake agent. agent chooses action (i.e., effort level) stochasticallyresults outcome order maximize expected utility given contract.principal observes outcome, cannot directly observe agents effort level, creatingmoral hazard problem. goal principal design contract maximizeexpected utility, difference utility receivesoutcome payment makes. maximization written constrainedoptimization problem, shown linear contracts optimal.adverse selection variation principal-agent problem relaxes assumptionagents type known. existing literature principal-agent problemadverse selection focuses applying revelation principle (Laffont & Martimort, 2002).setting, principal offers menu contracts, contract chosen agentreveals agents type. problem selecting menu contracts maximizesprincipals expected utility formulated constrained optimization.349fiHo, Slivkins, & Vaughanwork differs classic setting consider principal interactingmultiple agents, principal may adjust contract time online manner.Several authors considered extensions classic model multiple agents.Levy Vukina (2002) show multiple agents optimal set individuallinear contracts agent rather single uniform contract agents, offervariety descriptive explanations common see uniform contractspractice. Babaioff, Feldman, Nisan (2006) consider setting one principalinteracts multiple agents, observes single outcome functionagents effort levels. Misra, Nair, Daljord (2012) consider variantalgorithm must decide set uniform contract many agentsselect subset agents hire.Alternative online versions problem considered literature well.dynamic principal agent problem (Sannikov, 2008; Williams, 2009; Sannikov, 2012),single principal interacts single agent repeatedly period time. agentchoose exert different effort different time, outcome time functionefforts exerted agent t. principal cannot observe agentsefforts observe outcome. goal principal design optimalcontract time maximize payoff. work different line work sinceconsider setting multiple agents different, unknown types. algorithmneeds learn distribution agent types design optimal contract accordingly.Conitzer Garera (2006) study online principal agent problem similarsetting ours. However, focus empirically comparing different online algorithms,including bandit approaches uniform discretization, gradient ascent, Bayesianupdate approaches problem. goal provide algorithm nice theoreticalguarantees.Bohren Kravitz (2013) study setting outcome unverifiable.address issue, propose assign bundle tasks worker. verifyoutcome, task bundle chosen verifiable task non-trivialprobability. verifiable task either gold standard task known answertask assigned multiple workers verification. payment task bundleconditional outcome verified tasks. setting, assume taskoutcome verifiable. relax assumption adopting similar approaches.9.2 Incentives Crowdsourcing SystemsResearchers recently begun examine design incentive mechanisms encouragehigh-quality work crowdsourcing systems. Jain, Chen, Parkes (2012) explore waysaward virtual points users online question-and-answer forums improvequality answers. Ghosh Hummel (2011, 2013) Ghosh McAfee (2011) studydistribute user generated content (e.g., Youtube videos) users encourageproduction high-quality internet content people motivated attention. Ho,Zhang, Vaughan, van der Schaar (2012) Zhang van der Schaar (2012) considerdesign two-sided reputation systems encourage good behavior workersrequesters crowdsourcing markets. also consider crowdsourcing markets,350fiAdaptive Contract Design Crowdsourcing Marketswork differs focuses design monetary contracts, perhapsnatural incentive scheme, incentivize workers exert effort.problem closest studied context crowdsourcingsystems online task pricing problem requester unlimited supplytasks completed budget B spend (Badanidiyuru et al., 2012; Singer& Mittal, 2013). Workers private costs arrive online, requester sets singleprice arriving worker. goal learn optimal single fixed price time.work viewed generalization task pricing problem, specialcase setting number non-null outcomes fixed 1.also empirical work examining workers behavior varies basedfinancial incentives offered crowdsourcing markets. Mason Watts (2009) studyworkers react changes performance-independent financial incentives. study,increasing financial incentives increases number tasks workers complete,quality output. Yin, Chen, Sun (2013) provide potential explanationphenomenon using concept anchoring effect: workers cost completing taskinfluenced first price worker sees task. Horton Chilton (2010) runexperiments estimate workers reservation wage completing tasks. showmany workers respond rationally offered contracts, whereas workers appearedtarget payment mind.recent research studies effects performance-based payments (PBPs). Harris(2011) runs MTurk experiments resume screening, workers get bonusperform well. concludes quality work better PBPsuniform payments. Yin et al. (2013) show varying magnitude bonusmuch effect certain settings. Ho et al. (2015) perform comprehensive setexperiments aimed determining whether, when, PBPs increase qualitysubmitted work. results suggest PBPs increase quality tasksincreased time effort leads higher quality work. results also suggestworkers may interpret contract performance-based even stated (sincerequesters always option reject work). Based evidence, proposenew model worker behavior extends principal-agent model explicitly reflectworkers subjective beliefs likelihood paid.Overall, previous empirical work demonstrates workers crowdsourcing marketsrespond change financial incentives, behavior alwaysfollow traditional rational-worker model similar people real-world market.work, start analysis rational-worker assumption ubiquitous economic theory, demonstrate results still hold without assumptionslong collective worker behavior satisfies natural properties (namely, longLemma 3.1 holds). note results hold generalized worker model proposed Ho et al. (2015), consistent experimental evidence discussedabove.351fiHo, Slivkins, & Vaughan9.3 Sequential Decision Problemssequential decision problems, algorithm makes sequential decisions time. Twodirections relevant paper multi-armed bandits (MAB) dynamicpricing.MAB studied since 1933 (Thompson, 1933) operations research, economics,several branches computer science including machine learning, theoretical computerscience, AI, algorithmic economics. survey prior work MAB beyond scopepaper; reader encouraged refer work Cesa-Bianchi Lugosi (2006)Bubeck Cesa-Bianchi (2012) background prior-independent MAB,work Gittins, Glazebrook, Weber (2011) background Bayesian MAB.briefly discuss lines work MAB directly relevant paper.setting modeled prior-independent MAB stochastic rewards: reward given arm i.i.d. sample time-invariant distribution, neitherdistribution Bayesian prior known algorithm. basic formulation(with small number arms) well understood (Lai & Robbins, 1985; Auer et al., 2002;Bubeck & Cesa-Bianchi, 2012). handle problems large infinite number arms,one typically needs side information similarity arms. typical way modelside information, called Lipschitz MAB (Kleinberg et al., 2008), algorithmgiven distance function arms, expected rewards assumed satisfyLipschitz-continuity (or relaxation thereof) respect distance function (Agrawal,1995; Kleinberg, 2004; Auer et al., 2007; Kleinberg et al., 2008; Bubeck et al., 2011a;Slivkins, 2014). related paper idea adaptive discretizationoften used setting (Kleinberg et al., 2008; Bubeck et al., 2011a; Slivkins, 2014),particularly zooming algorithm (Kleinberg et al., 2008; Slivkins, 2014). particular,general template algorithm similar one zooming algorithm (butselection rule zooming rule different, reflecting lack prioriknown similarity information).settings (including ours), numerical similarity information required Lipschitz MAB immediately available. example, applications web searchadvertising natural assume algorithm observe tree-shaped taxonomy arms (Kocsis & Szepesvari, 2006; Munos & Coquelin, 2007; Pandey et al., 2007;Slivkins, 2011; Bull, 2013). particular, Slivkins (2011) Bull (2013) explicitly reconstruct (the relevant parts of) metric space defined taxonomy. differentdirection, Bubeck, Stoltz, Yu (2011b) study version Lipschitz MABLipschitz constant known, essentially recover performance NonAdaptivesetting.MAB partial monitoring (Audibert & Bubeck, 2010; Bartok, Foster, Pal,Rakhlin, & Szepesvari, 2014; Antos, Bartok, Pal, & Szepesvari, 2013), roundalgorithm receives auxiliary feedback rewards round (along rewardchosen arm), goal take advantage auxiliary feedback. Dynamictask pricing cast framework: given price p accepted, higherprice would too, rejected, lower price would be. However,aware way link dynamic task pricing existing results partial monitoring352fiAdaptive Contract Design Crowdsourcing Marketsvia connection. general version dynamic contract design appear fitpartial monitoring framework, essentially due moral hazard.Dynamic pricing (a.k.a. online posted-price auctions) refers settingsprincipal interacts agents arrive time offers agent pricetransaction, selling buying item. version principal sells itemsextensively studied operations research, typically Bayesian setting; seework den Boer (2015) literature review. study prior-independent,non-parameterized formulations initiated work Blum, Kumar, Rudra,Wu (2003) Kleinberg Leighton (2003) continued several others (Besbes& Zeevi, 2009; Babaioff, Dughmi, Kleinberg, & Slivkins, 2015; Besbes & Zeevi, 2012; Wang,Deng, & Ye, 2014; Badanidiyuru et al., 2013; Badanidiyuru, Langford, & Slivkins, 2014).Further, Badanidiyuru et al. (2012) Singla Krause (2013) studied versionprincipal buys items, equivalently commissions tasks; call versiondynamic task pricing. Modulo budget constraints, essentially special casesetting round worker offered chance perform task specifiedprice, either accept reject offer. particular, workers strategic choicedirectly observable. general settings studied (Badanidiyuru et al., 2013,2014; Agrawal & Devanur, 2014; Agrawal, Devanur, & Li, 2015).16 However, work(after initial papers, see Blum et al., 2003 Kleinberg & Leighton, 2003) focusedmodels constraints principals supply budgets, implyimproved results specialized unconstrained settings.10. ConclusionsMotivated applications crowdsourcing markets, define dynamic contract designproblem, multi-round version principal-agent model unobservable strategicdecisions. treat problem multi-armed bandit problem, design algorithmproblem, derive regret bounds compare favorably prior work. mainconceptual contribution, aside identifying model, adaptive discretizationapproach rely Lipschitz-continuity assumptions. provably improveuniform discretization approach prior work, general caseillustrative special cases. theoretical results supported simulations.generality shortcomings model discussed Section 2.2.believe dynamic contract design problem deserves study, severaldirections outline below.1. clear whether provable results improved, perhaps using substantiallydifferent algorithms relative different problem-specific structures. particular, oneneeds establish lower bounds order argue optimality; lower boundsdynamic contract design currently known.2. adaptive discretization approach may fine-tuned improve performancepractice. particular, definition index (C) given feasible cell C16. papers Badanidiyuru et al. (2014) Agrawal Devanur (2014) concurrent independent work respect conference publication paper, work Agrawal et al.(2015) subsequent work.353fiHo, Slivkins, & Vaughanmay re-defined several different ways. First, use information Csophisticated way, similar sophisticated indices basic K-armedbandit problem; example, see work Garivier Cappe (2011). Second, indexincorporate information cells. Third, defined smoother,probabilistic way, e.g., Thompson Sampling (Thompson, 1933).3. Deeper insights structure (static) principal-agent problem needed,primarily order optimize choice Xcand , set candidate contracts.natural target uniform mesh Xcand (). optimize granularity , oneneeds upper-bound discretization error OPT(Xcand ) OPT(Xcand ()) termsfunction f () f () 0 0. first-order open question resolvewhether done general case, provide specific example cannot.related open question concerns effect increasing granularity: upper-bounddifference OPT(Xcand ()) OPT(Xcand (0 )), > 0 > 0, terms function 0 .Further, known whether optimal mesh contracts fact uniform mesh.Also interest effect restricting attention monotone contracts.prove monotone contracts may optimal (Appendix A), significancephenomenon unclear. One would like characterize scenarios restrictingmonotone contracts alright (in sense best monotone contract good,much worse, best contract), scenarios restriction resultssignificant loss. latter scenarios, different algorithms may needed.4. much extensive analysis special cases order. general resultsdifficult access (which appears inherent property general problem),immediate direction special cases deriving lucid corollaries current regretbounds. particular, desirable optimize choice candidate contracts. Apartmassaging current results, one also design improved algorithms derivespecialized lower bounds. Particularly appealing special cases concern supply distributionsmixtures small number types, supply distributions belong(simple) parameterized family unknown parameter.Going beyond current model, natural direction incorporate budget constraint, extending corresponding results dynamic task pricing. main difficultysettings distribution two contracts may perform much betterfixed contract; see work Badanidiyuru et al. (2013) discussion. Effectively,algorithm needs optimize distributions. first step, one use nonadaptive discretization conjunction general algorithms bandits budgetconstraints, sometimes called bandits knapsacks (Badanidiyuru et al., 2013; Agrawal& Devanur, 2014). However, clear choose optimal mesh contracts(as discussed throughout paper), mesh likely uniform (becauseuniform special case dynamic task pricing budget; see Badanidiyuru et al., 2013 discussion). eventual target research direction marryadaptive discretization techniques prior work bandits knapsacks.354fiAdaptive Contract Design Crowdsourcing MarketsAcknowledgmentsthank anonymous reviewers useful comments. Much researchcompleted Ho intern Microsoft Research. research partially supported NSF grant IIS-1054911. opinions, findings, conclusions, recommendations authors alone.Appendix A. Monotone Contracts May Optimalsection provide example problem instance monotone contractssuboptimal (at least restricting attention contracts non-negativepayoffs). example, three non-null outcomes (i.e., = 3), two non-nulleffort levels, low effort high effort, denote e` eh respectively.single worker type. Since one type, drop subscriptdescribing cost function c. let c(e` ) = 0, let c(eh ) positive value less0.5(v(2) v(1)). worker chooses low effort, outcome equally likely 13. worker chooses high effort, equally likely 2 3. easy verifytype satisfies FOSD assumption. Finally, simplicity, assume workersbreak ties high effort effort level favor high effort,workers break ties low effort null effort level favor low effort.Lets consider optimal contract. Since single worker typeworkers type break ties way, consider separately best contractwould make workers choose null effort level, best contract would makeworkers choose low effort, best contract would make workers choose higheffort, compare requesters expected value each.Since c(e` ) = 0 workers break ties low effort null effort favor loweffort, contract would cause workers choose null effort; workers alwaysprefer low effort null effort.easy see best contract (in terms requester expected value) wouldmake workers choose low effort would set x(1) = x(3) = 0 x(2) sufficiently lowworkers would enticed choose high effort; setting x(2) = 0 sufficient.case, expected value requester would 0.5(v(1) + v(3)).lets consider contracts cause workers choose high effort. worker chooseshigh effort, expected value requester0.5(v(2) x(2) + v(3) x(3)).(23)Workers choose high effort0.5(x(1) + x(3)) 0.5(x(2) + x(3)) c(eh )0.5x(1) 0.5x(2) c(eh ).(24)find contract maximizes requesters expected value workers choosehigh effort, want maximize Equation 23 subject constraint Equation 24.Since x(3) doesnt appear Equation 24, set 0 maximize Equation 23.355fiHo, Slivkins, & VaughanSince x(1) appear Equation 23, set x(1) = 0 make Equation 24easy possible satisfy. see optimal occurs x(2) = 2c(eh ).Plugging contact x Equation 23, expected utility case 0.5(v(2) +v(3)) c(eh ). Since assumed c(eh ) < 0.5(v(2) v(1))), strictly preferableconstant 0 contract, fact unique optimal contract. Since x(2) > x(3),unique optimal contract monotonic.ReferencesAgrawal, R. (1995). continuum-armed bandit problem. SIAM J. Control Optimization, 33 (6), 19261951.Agrawal, S., & Devanur, N. R. (2014). Bandits concave rewards convex knapsacks.15th ACM Conf. Economics Computation (EC).Agrawal, S., Devanur, N. R., & Li, L. (2015). Contextual bandits global constraintsobjective.. Technical report, arXiv:1506.03374.Antos, A., Bartok, G., Pal, D., & Szepesvari, C. (2013). Toward classification finitepartial-monitoring games. Theor. Comput. Sci., 473, 7799.Audibert, J., & Bubeck, S. (2010). Regret Bounds Minimax Policies PartialMonitoring. J. Machine Learning Research (JMLR), 11, 27852836.Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis multiarmedbandit problem.. Machine Learning, 47 (2-3), 235256.Auer, P., Ortner, R., & Szepesvari, C. (2007). Improved Rates Stochastic ContinuumArmed Bandit Problem. 20th Conf. Learning Theory (COLT), pp. 454468.Babaioff, M., Dughmi, S., Kleinberg, R. D., & Slivkins, A. (2015). Dynamic pricinglimited supply. ACM Trans. Economics Computation, 3 (1), 4.Babaioff, M., Feldman, M., & Nisan, N. (2006). Combinatorial agency. 7th ACM Conf.Electronic Commerce (EC).Badanidiyuru, A., Kleinberg, R., & Singer, Y. (2012). Learning budget: posted pricemechanisms online procurement. 13th ACM Conf. Electronic Commerce(EC), pp. 128145.Badanidiyuru, A., Kleinberg, R., & Slivkins, A. (2013). Bandits knapsacks. 54thIEEE Symp. Foundations Computer Science (FOCS).Badanidiyuru, A., Langford, J., & Slivkins, A. (2014). Resourceful contextual bandits.27th Conf. Learning Theory (COLT).Bartok, G., Foster, D. P., Pal, D., Rakhlin, A., & Szepesvari, C. (2014). Partial monitoring- classification, regret bounds, algorithms. Math. Oper. Res., 39 (4), 967997.Besbes, O., & Zeevi, A. (2009). Dynamic pricing without knowing demand function:Risk bounds near-optimal algorithms. Operations Research, 57, 14071420.Besbes, O., & Zeevi, A. J. (2012). Blind network revenue management. Operations Research,60 (6), 15371550.356fiAdaptive Contract Design Crowdsourcing MarketsBlum, A., Kumar, V., Rudra, A., & Wu, F. (2003). Online learning online auctions.14th ACM-SIAM Symp. Discrete Algorithms (SODA), pp. 202204.Bohren, J. A., & Kravitz, T. (2013). Incentives spot market labor outputunverifiable. Working paper.Bubeck, S., & Cesa-Bianchi, N. (2012). Regret Analysis Stochastic NonstochasticMulti-armed Bandit Problems. Foundations Trends Machine Learning, 5 (1),1122.Bubeck, S., Munos, R., Stoltz, G., & Szepesvari, C. (2011a). Online Optimization XArmed Bandits. J. Machine Learning Research (JMLR), 12, 15871627.Bubeck, S., Stoltz, G., & Yu, J. Y. (2011b). Lipschitz bandits without lipschitz constant.22nd Intl. Conf. Algorithmic Learning Theory (ALT), pp. 144158.Bull, A. D. (2013). Adaptive-treed bandits. Tech. rep. 1302.2489, arxiv.org.Cesa-Bianchi, N., & Lugosi, G. (2006). Prediction, learning, games. Cambridge Univ.Press.Conitzer, V., & Garera, N. (2006). Online learning algorithms online principal-agentproblems (and selling goods online). International Conference Machine Learning(ICML).den Boer, A. V. (2015). Dynamic pricing learning: Historical origins, current research,new directions. Surveys Operations Research Management Science. Forthcoming.Dudik, M., Hsu, D., Kale, S., Karampatziakis, N., Langford, J., Reyzin, L., & Zhang, T.(2011). Efficient optimal leanring contextual bandits. 27th Conf. UncertaintyArtificial Intelligence (UAI).Garivier, A., & Cappe, O. (2011). KL-UCB Algorithm Bounded Stochastic BanditsBeyond. 24th Conf. Learning Theory (COLT).Ghosh, A., & Hummel, P. (2011). game-theoretic analysis rank-order mechanismsuser-generated content. 12th ACM Conf. Electronic Commerce (EC).Ghosh, A., & Hummel, P. (2013). Learning incentives user-generated content: Multiarmed bandits endogenous arms. Proc. 4th Conference InnovationsTheoretical Computer Science (ITCS).Ghosh, A., & McAfee, P. (2011). Incentivizing high-quality user-generated content. 20thIntl. World Wide Web Conf. (WWW).Gittins, J., Glazebrook, K., & Weber, R. (2011). Multi-Armed Bandit Allocation Indices.John Wiley & Sons.Harris, C. G. (2011). Youre hired! examination crowdsourcing incentive modelshuman resource tasks. CSDM.Ho, C., Slivkins, A., Suri, S., & Vaughan, J. W. (2015). Incentivizing high quality crowdwork. 24th Intl. World Wide Web Conf. (WWW).Ho, C.-J., Zhang, Y., Vaughan, J. W., & van der Schaar, M. (2012). Towards social normdesign crowdsourcing markets. HCOMP.357fiHo, Slivkins, & VaughanHorton, J. J., & Chilton, L. B. (2010). labor economics paid crowdsourcing. 11thACM Conf. Electronic Commerce (EC).Jain, S., Chen, Y., & Parkes, D. (2012). Designing incentives online question-and-answerforums. Games Economic Behavior.Kleinberg, R. (2004). Nearly tight bounds continuum-armed bandit problem.18th Advances Neural Information Processing Systems (NIPS).Kleinberg, R., & Leighton, T. (2003). value knowing demand curve: Boundsregret online posted-price auctions.. 44th IEEE Symp. FoundationsComputer Science (FOCS), pp. 594605.Kleinberg, R., Slivkins, A., & Upfal, E. (2008). Multi-armed bandits metric spaces.40th ACM Symp. Theory Computing (STOC), pp. 681690.Kleinberg, R. D., & Leighton, F. T. (2003). value knowing demand curve: Boundsregret online posted-price auctions. IEEE Symp. Foundations ComputerScience (FOCS).Kocsis, L., & Szepesvari, C. (2006). Bandit Based Monte-Carlo Planning. 17th EuropeanConf. Machine Learning (ECML), pp. 282293.Laffont, J.-J., & Martimort, D. (2002). Theory Incentives: Principal-AgentModel. Princeton University Press.Lai, T. L., & Robbins, H. (1985). Asymptotically efficient Adaptive Allocation Rules.Advances Applied Mathematics, 6, 422.Levy, A., & Vukina, T. (2002). Optimal linear contracts heterogeneous agents.European Review Agricultural Economics.Mason, W., & Watts, D. (2009). Financial incentives performance crowds.HCOMP.Misra, S., Nair, H. S., & Daljord, O. (2012). Homogenous contracts heterogeneousagents: Aligning salesforce composition compensation. Working Paper.Munos, R., & Coquelin, P.-A. (2007). Bandit algorithms tree search. 23rd Conf.Uncertainty Artificial Intelligence (UAI).Pandey, S., Agarwal, D., Chakrabarti, D., & Josifovski, V. (2007). Bandits Taxonomies:Model-based Approach. SIAM Intl. Conf. Data Mining (SDM).Radlinski, F., Kleinberg, R., & Joachims, T. (2008). Learning diverse rankings multiarmed bandits. 25th Intl. Conf. Machine Learning (ICML), pp. 784791.Sannikov, Y. (2008). continuous-time version principal-agent problem.Review Economics Studies.Sannikov, Y. (2012). Contracts: theory dynamic principal-agent relationshipscontinuous-time approach. 10th World Congress Econometric Society.Singer, Y., & Mittal, M. (2013). Pricing mechanisms crowdsourcing markets. 22ndIntl. World Wide Web Conf. (WWW).Singla, A., & Krause, A. (2013). Truthful incentives crowdsourcing tasks using regretminimization mechanisms. 22nd Intl. World Wide Web Conf. (WWW).358fiAdaptive Contract Design Crowdsourcing MarketsSlivkins, A. (2011). Multi-armed bandits implicit metric spaces. 25th AdvancesNeural Information Processing Systems (NIPS).Slivkins, A. (2014). Contextual bandits similarity information. J. Machine LearningResearch (JMLR), 15 (1), 25332568. Preliminary version COLT 2011.Slivkins, A., Radlinski, F., & Gollapudi, S. (2013). Ranked bandits metric spaces: Learning optimally diverse rankings large document collections. J. Machine LearningResearch (JMLR), 14 (Feb), 399436. Preliminary version 27th ICML, 2010.Thompson, W. R. (1933). likelihood one unknown probability exceeds anotherview evidence two samples.. Biometrika, 25 (3-4), 285294.Wang, Z., Deng, S., & Ye, Y. (2014). Close gaps: learning-while-doing algorithmsingle-product revenue management problems. Operations Research, 62 (2), 318331.Williams, N. (2009). dynamic principal-agent problems continuous time. WorkingPaper.Yin, M., Chen, Y., & Sun, Y.-A. (2013). effects performance-contingent financialincentives online labor markets. AAAI.Zhang, Y., & van der Schaar, M. (2012). Reputation-based incentive protocols crowdsourcing applications. Infocom.359fiJournal Artificial Intelligence Research 55 (2016) 209-248Submitted 03/15; published 01/16Synthetic Treebanking Cross-LingualDependency ParsingJorg Tiedemannjorg.tiedemann@helsinki.fiDepartment Modern Languages, University HelsinkiP.O. Box 24, FI-00014 University Helsinki, FinlandZeljko Agiczeljko.agic@hum.ku.dkCenter Language Technology, University CopenhagenNjalsgade 140, 2300 Copenhagen S, DenmarkAbstractparse languages treebanks available? contributionaddresses cross-lingual viewpoint statistical dependency parsing, attemptmake use resource-rich source language treebanks build adapt modelsunder-resourced target languages. outline benefits, indicate drawbackscurrent major approaches. emphasize synthetic treebanking: automatic creationtarget language treebanks means annotation projection machine translation.present competitive results cross-lingual dependency parsing using combinationvarious techniques contribute overall success method.include detailed discussion impact part-of-speech label accuracy parsingresults provide guidance practical applications cross-lingual methods trulyunder-resourced languages.1. IntroductionLanguages dialects army navy famous saying popularizedsociolinguist Max Weinreich. modern times, quote could rephrasedand languagesdefinedas dialects part-of-speech tagger, treebank, machine translationsystem. Even though proposition would disqualify languages world,true existence many languages threatened due insufficient resourcestechnical support. Natural language processing (NLP) becomes increasingly importantpeoples everyday life look, example, success word prediction, spellingcorrection, instant on-line translation. Building linguistic resources tools, however,expensive time-consuming, one great challenges computational linguisticsport existing models new languages domains.Modern NLP requires data, often annotated explicit linguistic information,tools learn them. However, sufficient quantities electronic data sourcesavailable handful languages whereas languagesprivilege draw resources (Bender, 2011; Uszkoreit & Rehm, 2012; Bender,2013). Speakers low-density languages countries live able investlarge data collection time-consuming annotation efforts, goal cross-lingualc2016AI Access Foundation. rights reserved.fiTiedemann & AgicNLP share rich linguistic information poorly supported languages, makingpossible build tools resources without starting scratch.paper, consider task statistical dependency parsing (Kubler, McDonald,& Nivre, 2009). Top-performing dependency parsers typically trained dependencytreebanks include several thousands manually annotated sentences. statisticalparsing models known robust efficient, yielding high accuracy unseentexts. However, even moderately-sized treebanks take lot time resources produce(Abeille, 2003), point, unavailable scarce even major languages.Thus, similar areas NLP research, face challenge posed abstract:parse languages dependency treebanks available? Withoutannotated training data basically four options data-driven NLP:1. build parsing models learn raw data using unsupervised machinelearning techniques.2. manually annotated data scarcely available, resort various approachessemi-supervised learning, leveraging various sources fortuitous data (Sgaard,2013).3. transfer existing models tools new languages.4. transfer data resource-rich languages resource-poor languagesbuild tools data sets.four viewpoints studied intensively connection dependency parsingNLP general. parsing, first option especially difficult unsupervisedapproaches still fall far behind rest field (Sgaard, 2012). Unsupervised modelsalso difficult evaluate applications build labeled information problemsmaking use structures produced models. Semi-supervised learning eitheraugments well-resourced environments improved cross-domain robustness, largelycoincides cross-lingual approaches loosely defined (Sgaard, 2013).Therefore, surprising final two options attracted quite popularitygained lot merit enabling parsing low-resource languages. paper,exclusively look techniques.basic idea behind transfer approaches tools resources existresource-rich source languages used build corresponding tools resources underresourced target languages means adaptation. statistical dependency parsingcross-lingual approach essentially means either take parsing model applyanother language use treebanks train parsers new language targetlanguage adaptation taking place workflow stages. can, thus, dividemain approaches cross-lingual dependency parsing two categories: model transferdata transfer.Model transfer methods appealing property focus languageuniversals structures identified various languages without side-stepping(semi-)automatic creation annotated data target language. strongline research looking identification cross-lingual features usedport models tools new languages. One biggest drawbacks extreme210fiSynthetic Treebanking Cross-Lingual Dependency Parsingabstraction generic features cannot cover language-specific properties naturallanguages. Therefore, methods often restricted closely related languagesperformance usually far fully supervised target-specific parsing models.Data transfer methods, hand, emphasize creation artificial trainingdata used standard machine learning techniques build modelstarget language. work focused annotation projection useparallel data, is, documents translated languages. Statistical alignmenttechniques make possible map linguistic annotation one language another.Another recent approach proposes translation treebanks (Tiedemann, Agic, & Nivre,2014) enables projection annotation without parsing unrelated parallel corpora.methods create synthetic data sets without manual intervention and, therefore,group techniques general term synthetic treebanking, main focuspaper.structure paper follows. brief outlook contributionswork, first provide overview cross-lingual dependency parsing approaches.that, discuss depth experiments synthetic treebanks, inspectannotation projection parallel data sets translated treebanks. also includethorough study impact part-of-speech (PoS) tagging cross-lingual parsing.concluding final remarks prospects future work, discuss impactcontribution comparison selected recent approaches, terms empiricalassessment underlying requirements imposed truly under-resourced languages.1.1 Contributionspaper addresses annotation projection treebank translation detailedsystematic investigation various techniques strategies. build previouswork cross-lingual parsing (Tiedemann et al., 2014; Tiedemann, 2014, 2015) extendstudy detailed discussions advantages drawbacks method. alsoinclude new idea back-projection integrates machine translation parsingworkflow. main contributions following:1. provide overview various approaches cross-lingual dependency parsingdetailed discussions properties utilized techniques.2. present new competitive cross-lingual parsing results using synthetic treebanks.ground results discussion related work implications trulyunder-resourced languages.3. provide thorough study impact PoS tagging cross-lingual dependencyparsing.delving details let us first review selected current approachescross-lingual dependency parsing connect work presented paper relatedresearch.211fiTiedemann & Agic2. Current Approaches Cross-Lingual Dependency Parsingsection provides overview cross-lingual dependency parsing. discusspreviously outlined annotation projection model transfer approaches depthincluding recent developments field. Cross-lingual parsing combines many effortsdependency treebanking, creating standards PoS syntactic annotations.start outlining current practices empirical evaluation cross-lingual parsers,linguistic resources used benchmarking.2.1 Treebanks Evaluationsupervised setting, cross-lingual dependency parsing amounts training parsertreebank, applying target text. However, empirical quality assessmentparser target data introduces certain additional constraints. evaluatesupervised cross-lingual parsers, require least following three components:1. parser generators: trainable, language-independent dependency parsing systems,2. dependency treebanks source languages,3. held-out evaluation sets target languages.years following venerable CoNLL 2006 2007 shared task campaignsdependency parsing (Buchholz & Marsi, 2006; Nivre, Hall, Kubler, McDonald, Nilsson,Riedel, & Yuret, 2007), many mature parsers made publicly available across differentparsing paradigms. resolves first point list, choosing applyandcomparing betweendifferent approaches parsing cross-lingual setup nowadaysmade trivial abundant parser availability. easily benchmark respectablenumber parsers accuracy, processing speed, memory requirements.Experimental setup cross-lingual parsing thus amounts choosing trainingtesting data, defining evaluation metrics.2.1.1 Intrinsic Extrinsic Evaluationperform intrinsic extrinsic evaluation dependency parsing. intrinsic evaluation,typically apply evaluation metrics gauge various aspects parsing accuracyheld-out data, extrinsic evaluation, parsers scored gains yieldedsubsequentor downstreamtasks make use dependency parses additionalinput.Dependency parsers intrinsically evaluated labeled (LAS) unlabeled (UAS)attachment scores: portions correctly paired heads dependents dependencytrees, without keeping track edge labels, respectively. Sometimes alsoevaluate labeled (LEM) unlabeled (UEM) exact match scores, determine oftenparsers correctly parse entire sentences. detailed exposition dependencyparser evaluation, see work Nivre (2006) Kubler et al. (2009), also notePlank et al. (2015) provide detailed insight correlations variousdependency parsing metrics human judgements quality parses.212fiSynthetic Treebanking Cross-Lingual Dependency Parsingmonolingual intrinsic evaluation scenario, either predefined held-out testdata disposal, cross-validate slicing treebank training testsets. cases, treebank test sets belong resource,created using annotation scheme, turn typically stemsunderlying syntactic theory. However, given heterogenous development syntactictheories, subsequently treebanks different languages (Abeille, 2003),necessarily hold cross-lingual setup. Moreover, excluding recent treebankingdevelopmentswhich discuss bit sectionprior 2013, oddsrandomly sampling pool publicly available treebanks drawing source-targetpair annotated (or even similar) scheme virtually non-existent.syntactic annotation schemes generally differ in: (a) rules attaching dependentsheads, (b) dependency relation labels, is, syntactic tagsets. Given twotreebanks incompatible syntactic annotations, without performing conversions,likely expect similarities head attachment rules, syntactic tagsets.fact present initial cross-lingual parsing experiments (Zeman & Resnik,2008; McDonald, Petrov, & Hall, 2011; Sgaard, 2011). initial efforts chartingcross-lingual dependency parsing mainly used CoNLL shared task datasets,evaluated UAS. rare exceptions are, example, generally under-resourcedSlavic languages (Agic, Merkler, & Berovic, 2012) subscribing (slightly modified versionsof) Prague Dependency Treebank scheme (Bohmova, Hajic, Hajicova, & Hladka, 2003).recently, substantial effort undertaken bridging annotation scheme gapdependency treebanking facilitate uniform syntactic processing worlds languages.effort resulted two editions Google Universal Treebanks (UDT) (McDonald et al.,2013), turn recently superseded Universal Dependencies project (UD)(Nivre et al., 2015). projects, Stanford typed dependencies (SD) (De Marneffe,MacCartney, & Manning, 2006) used adaptable basis designing underlyingannotation scheme, applying using human expert annotators several languages.datasets made possible first reliable cross-lingual dependency parsing experiments,namely ones McDonald et al. (2013), also enabled use LAS defaultevaluation metric, like monolingual parsing. reasons, UDT UDde facto standard datasets benchmarking cross-lingual parsers today, CoNLLdatasets still used mainly backward compatibility previous research. anothereffort, HamleDT dataset (Zeman et al., 2014), 30 treebanks automatically convertedPrague scheme, SD, also frequently used evaluation campaigns.currently note preference UDT UD, since producedmanual annotation.Given short exposition dependency treebanking relation cross-lingualparsing, paper, opt using UDT experiments. choice sourcestargets, Cartesian product dataset: treat available languagessources targets. common approach cross-lingual parsing,even research uses English source-only language, treatslanguages targets.extrinsic evaluation cross-lingual parsing much less developed, althougharguments favor convincing. Namely, underlying goal cross-lingualparsing enabling processing actual under-resourced languages. languages,213fiTiedemann & Agiceven parsing test sets may readily available. conducting empirical evaluationsextreme cases, might resort downstream applications (Elming et al., 2013).choice downstream tasks might pose separate challenge case, devisingfeasible (and representative) tasks extrinsic evaluation cross-lingual dependency parsingremains largely unaddressed. paper, deal intrinsic evaluation.2.1.2 Part-of-Speech Taggingnoted brief introduction model transfer, dependency parsers make heavy usePoS features. syntactic annotations, sources targets may mayshared PoS annotation layers, moreover, PoS taggers may may availabletarget languages.issue PoS compatibility arguably less difficult resolve structurallabeling differences dependency trees, PoS tags less straightforwardlymapped one another. point, also note recent approaches learningPoS tag conversions (Zhang, Reichart, Barzilay, & Globerson, 2012), systematicallyfacilitate conversions. Furthermore, efforts UDT/UD also build shared PoSrepresentation, so-called Universal PoS (UPoS) (Petrov et al., 2012). UD extendsUPoS specification introducing additional PoS tags17 instead initial 12andproviding support standardized morphological features noun gendercase, verb tense. said, added features yet readily available,shared representation UDT/UD amounts 12- 17-tag-strong PoS tagset.treatment source languages respect PoS tagging, workcross-lingual parsing presumes existence taggers, even tests gold standard PoSinput. Recently, Petrov (2014) argued strongly use predicted PoS cross-lingualparsing, make realistic testing environment, especially increasedavailability weakly supervised PoS taggers (Li et al., 2012; Garrette et al., 2013).paper, experiment gold standard predicted PoS features order stressimpact tagging accuracy parsing performance. also discuss implicationschoices enabling processing truly under-resourced languages.2.2 Model Transferproceed sketch main approaches cross-lingual dependency parsing: modeltransfer, annotation projection, treebank translation. also reflect usagecross-lingual word representations cross-lingual parsing, particularly emphasizeannotation projection treebank translation approaches.Simplistic model transfer amounts applying source models targetsadaptation, still rather successful closely related languages (Agic et al.,2014). However, flavor model transfer recently attracted fair amountinterest owes availability cross-lingually harmonized annotation (Petrov et al., 2012)makes possible use shared PoS features across languages. straightforwardtechnique train delexicalized parsers heavily rely UPoS tags. Figure 1 illustratesbasic idea behind models. simple technique shown successclosely related languages (McDonald et al., 2013). Several improvements achievedusing multiple source languages (McDonald et al., 2011; Naseem, Barzilay, & Globerson,214fiSynthetic Treebanking Cross-Lingual Dependency Parsinglabel 2label 1label 3(1) delexicalizepos1src1lexicalizedparserpos2pos3src2 src3pos4src4(4) re-trainpos2trg1pos1trg2(2) traindelexicalizedparser(3) parsepos3trg3label 1pos4trg4label 3label 2Figure 1: illustration delexicalized model transfer, implicationlexicalization option self-training.2012), additional cross-lingual features used transfer models newlanguage, cross-lingual word clusters (Tackstrom, McDonald, & Uszkoreit, 2012)word-typology information (Tackstrom, McDonald, & Nivre, 2013b). waysre-lexicalize models well. Figure 1 suggests self-learning procedure adds lexicalinformation data sets automatically annotated using delexicalizedmodels. Various data selection techniques used focus reliable cases improvevalue induced lexical features.advantage transferred models require parallel data, leastgeneric form. However, reasonable models require kind targetlanguage adaptation parallel comparable data sets usually necessary performadaptations. largest drawback model transfer strong abstractionlanguage-specific features universal properties. many fine-grained linguisticdifferences, kind coarse-grained universal knowledge often informative enough(Agic et al., 2014). Consequently, large majority recent approaches aim bridgingrepresentational deficiency.2.3 Cross-Lingual Word RepresentationsModel transfer requires abstract features capture universal properties languages.use cross-lingual word clusters already mentioned previous section,benefits monolingual clustering dependency parsing well-known (Koo,Carreras, & Collins, 2008). Recently, distributed word representations entered NLPvarious models (Collobert et al., 2011). so-called word embeddings capturedistributional properties words continuous vector representations usedmeasure syntactic semantic relations even across languages (Mikolov, Le, & Sutskever,2013). monolingual variety found many applications NLP. Distributed wordrepresentations cross-lingual dependency parsing first applied recently XiaoGuo (2014). explore word embeddings another useful abstraction enablesrobust model transfer across languages. However, apply techniques215fiTiedemann & Agicold CoNLL data sets cannot provide labeled attachment scores comparable resultssettings.Several recent publications show bilingual word embeddings learned alignedbitexts improve semantic representations. Faruqui Dyer (2014) use canonical correlationanalysis find cross-lingual projections monolingual vector space models. Zou, Socher,Cer, Manning (2013) learn bilingual word embeddings fixed word alignments.Klementiev, Titov, Bhattarai (2012) treat cross-lingual representation learningmultitask learning problem cross-lingual interactions based word alignmentsword embeddings shared across various tasks. techniquessignificant value improved model transfer may act necessary target languageadaptation move beyond language universals feature transfer models.cross-lingual parsing, envision word representations valuable additionmodel transfer direction regularization. said, usage maintainspreviously listed advantages drawbacks model transfer, adds another prerequisite:availability parallel texts inducing embeddings.recent developments creating cross-lingual embeddings without parallel text (Gouws &Sgaard, 2015) applicability dependency parsing yet verified. Here,note recent contribution Sgaard et al. (2015), use inverted indexingcross-lingually overlapping Wikipedia articles produce truly inter-lingual word embeddings.show competitive scores cross-lingual dependency parsing, addresscontribution related work discussion.2.4 Annotation Projectionuse parallel corpora automatic word alignment transferring linguistic annotation source language new target language quite long tradition NLP.pioneering work Yarowsky, Ngai, Wicentowski (2001) followed numberresearchers, various tasks, transfer dependency annotation among others(Hwa et al., 2005). basic idea use existing tools models annotate sourceside parallel corpus use alignment guide mapping annotationtarget side corpus. Assuming source language annotation sufficientlycorrect aligned target language reflects syntactic patterns,train parsers projected data bootstrap tools languages without explicit linguisticresources syntactically annotated treebanks. Figure 2 illustrates general ideaannotation projection case syntactic dependencies parser model induction.Note PoS labels typically projected well along dependency relations.first attempts directly map dependency information coming diverse treebanksresulted rather poor performance. work, Hwa et al. (2005) rely additionalpost-processing rules transform results reasonable structures. arguedprevious subsection, one main problems early work incompatibilitytreebanks individually developed various languages following differentguidelines using different label sets. latter also reason labeledattachment scores could reported work, makes difficult placecross-lingual approaches relation standard models trained target language.216fiSynthetic Treebanking Cross-Lingual Dependency Parsinglabel 2label 1pos1src1(1) parselabel 3pos2pos3src2 src3pos4(2) projecttrg1trg2trg3trg4pos2pos1pos3pos4label 1parserparsersrc4word-aligned bitextlexicalizedlexicalizedlabel 3(3) trainlabel 2Figure 2: illustration syntactic annotation projection system cross-lingualdependency parsing.Less frequent, also possible, scenario source side parallelcorpus contains manual annotation (Agic et al., 2012). addresses problem createdprojecting noisy annotations, presupposes parallel corpora manual annotation,rarely available. Additionally, problem incompatible annotation still remains.introduction cross-lingually harmonized treebanks changed situation significantly (McDonald et al., 2013). data sets use identical labels adhere similarannotation guidelines make possible directly compare structures projectedlanguages. work Tiedemann (2014), explore projection strategiesdiscuss success annotation projection comparison cross-lingual approaches. work builds direct correspondence assumption (DCA) proposedHwa et al. (2005). define several projection heuristics make possible projectdependency structure given word alignments target language sentence.basic procedures cover different types word alignments. One-to-one alignmentsstraightforward case dependency relations simply copied. Unalignedsource language tokens covered additional DUMMY nodes capture relationsconnected token source language (see left-most graph Figure 3).Many-to-one links resolved keeping link head aligned sourcelanguage tokens deleting links (see graph middle). One-to-manyalignments handled introducing additional DUMMY nodes act immediateparent target language, capture dependency relation sourceside annotation (see right-most graph Figure 3). Many-to-many alignmentstreated two steps. First apply rule one-to-many alignmentsmany-to-one rule. Finally, unaligned target language tokens simply droppedremoved target sentence.issues explicitly covered original publication algorithm.example, entirely clear sequence rules appliedlabels projected. rules, example, change alignment structuremay cause additional unaligned source tokens need handled rules.implementation, first apply one-to-many rule cases sentence217fiTiedemann & Agiclabel 2label 1label 2label 3label 1pos1src1pos2trg1pos2pos3src2 src3pos1trg2pos3pos4src4pos1pos1pos2pos3pos2pos1pos4src1src2 src3src4pos1trg1trg2trg3src2dummy dummyDUMMY trg1trg2dummylabel 3pos3src3pos2trg3pos3trg4label 2dummylabel 1label 2label 2pos2src1pos4pos4trg3 DUMMYlabel 1label 1label 3label 1label 2Figure 3: Annotation projection heuristics special alignment types: Unaligned sourcewords (left graph), many-to-one alignments (center), one-to-many alignments(right graph).applying many-to-one rule and, thereafter, resolving unaligned source tokens.final step includes mapping dependency relations remaining one-to-onealignments. one-to-many alignments, transfer PoS dependency labelsnewly created DUMMY node (following rule one-to-one alignments resolvingone-to-many link) previously aligned target language tokens obtain DUMMYPoS labels dependency relation governing DUMMY node also labeledDUMMY (see Figure 3).Projecting syntactic dependency annotation creates several problems well. Firstall, crossing word alignments cause large amount non-projectivity projecteddata. percentage non-projective structures goes 50% UDTdata (Tiedemann et al., 2014). Furthermore, projection heuristics lead conflictingannotation shown authentic example illustrated Figure 4. issues putadditional burden learning algorithms many cross-lingual errors causedcomplex ambiguous cases.Nevertheless, Tiedemann (2014) demonstrates annotation projection competitivecross-lingual methods merits explored Tiedemann (2015).ccadpobjTous ses produits sont de qualitetdune fraicheur exemplaires .... high- quality DUMMY ...adpobjdummydummyccadded nonprojectivityinconsistenciesFigure 4: Issues annotation projection illustrated real-life example.218fiSynthetic Treebanking Cross-Lingual Dependency Parsinglabel 2Treebank translationlabel 1pos1src1label 3pos2pos3src2 src3pos4src4(1) translate(2) projecttrg1trg2trg3trg4pos2pos1pos3pos4label 1lexicalizedparserlabel 3(3) trainlabel 2Figure 5: illustration synthetic treebanking approach translation.2.5 Translating Treebanksnotion translation cross-lingual parsing first introduced Zhao, Song, Kit,Zhou (2009), use bilingual lexicon lookup-based target adaptation. similarmethod also adopted Durrett et al. (2012). simplistic lookup approach usedAgic et al. (2012), exploit availability parallel corpus two closely relatedlanguages, one side corpus dependency treebank. former evaluatesUAS 9 languages CoNLL datasets, latter research dealsCroatian Slovene smaller scale.Tiedemann et al. (2014) first use full-scale statistical machine translation (SMT)synthesize treebanks SMT-facilitated target language adaptations cross-lingualparsing. use UDT LAS evaluation, also performing subset experimentsCoNLL 2007 data backward compatibility. paper, often refer to,build work. Figure 5 illustrates general idea technique,proceed discuss implications.sketched introduction, core synthetic treebanking ideaconcept automatic source-to-target treebank translation. workflow consistsfollowing steps:1. Take source-target parallel corpus large monolingual target language corpustrain (ideally top-performing) SMT system, orif availableapply existingsource-target machine translation system.2. Given source language treebank, translate target language. Word-alignoriginal sentence translation, preserve phrase alignments providedSMT system.3. Use alignments project dependency annotations source treebanktarget translation, turn creating artificial (or synthetic) treebanktarget language.4. Train target language parser synthesized treebank, apply (or evaluate)target language data.219fiTiedemann & Agicsketch treebank translation opens large parameter tuning search space,also outlines various properties approach. discuss briefly, deferreader detailed expositions many intricacies papers (Tiedemann et al.,2014; Tiedemann, 2014, 2015).2.5.1 Componentsprerequisites building SMT-supported cross-lingual parsing system are: (a)availability parallel corpora, (b) platform building state-of-the-art SMT systems, (c)algorithms robust annotation projection, (d) previously listed resources neededcross-lingual parsing general: treebanks parsers.Parallel corpora available large number language pairs, even outsidebenchmarking frameworks CoNLL UDT. size domains paralleldata influences quality SMT, subsequently cross-lingual parsers.SMT community typically experiments Europarl dataset (Koehn, 2005),many datasets also freely available cover many languages,OPUS collection (Tiedemann, 2012). Ideally, parallel corpora used SMTlarge, source-target pairs, may necessarily case. Moreover,corpora might spread across domains interest, leading decreased performance.Domain dependence thus inherent choice parallel corpora training SMTsystems. Here, note recent contribution Agic et al. (2015), learn hundred PoStaggers truly under-resourced languages using label propagation multi-parallelBible corpus, indicating possibility bootstrapping NLP tools even hostileenvironments, subsequent applicability tools across domains.paper, opt using Moses (Koehn et al., 2007) de facto standardplatform conducting SMT research. summary, since approach SMT goesbeyond dictionary lookup Durrett et al. (2012), mainly experiment phrasebased models, gaining target language adaptations form lexicalfeatures reordering. projection algorithms synthetic treebankingwhole transferred annotation projection approaches. do, however, considervarious parametrizations, Tiedemann et al. (2014) previously proposed novelalgorithm, Tiedemann (2014) thoroughly compared various approaches annotationprojection.2.5.2 Advantages DrawbacksAutomatic translation advantage use manually verified annotationsource language treebank given word alignment, integral parttranslation model. Recent advances statistical machine translation (SMT) combinedever-growing availability parallel corpora making realistic alternative.relation annotation projection obvious involve parallel data one sideannotated. However, use direct translation brings two important advantages.First all, using SMT, accumulate errors two sources: tooltaggerparserused annotate source language bilingual corpus, noise comingalignment projection. Instead, use gold standard annotation sourcelanguage safely assumed much higher quality automatic220fiSynthetic Treebanking Cross-Lingual Dependency ParsingInput: source tree S, target sentence ,word alignment A, phrase segmentation POutput: syntactic heads head[],word attributes attr[]123456789101112131415161718192021222324252627Input: node s, source tree root ROOT,target sentence , word alignmentOutput: node t*1 == ROOTtreeSize = max distance root(S) ;2return ROOT ;attr = [] ;3 endhead = [] ;use phrase4 unaligned src(s,A)25= head of(s,S) ;segmentationunaligned trg(t,A)6== ROOT2 trg phrase(t,P)7return ROOT;function:find_aligned:[sx ,..,sy ] = aligned to(t) ;8end= findhighest([s,..,sy ],S);9 endInput:sourcetree S,xtargetsentenceT,Input: node s, source tree root ROOT,wordalignmentA, phrase segmentationP10 p = 0 ; target sentence , word alignment= findaligned(s,S,T,A);Output:headsOutput:node t*11 t* = undef;attr[t] =syntacticDUMMY; head[],walk treewordattributes12 2dothenhead[t]= ; attr[]1 aligned(s,A)== ROOTunaligned132position(t,T)> p1 treeSize = max distance root(S) ;return ROOT;end14t* = ;3 endelse 2 attr = [] ;attach 15p = position(t,T);4unaligned; src(s,A)[s3 xhead,..,sy=] =[] alignedto(t) ;highest node425= head of(s,S) ;16end= find highest([sx ,..,sy ],S) ;5unalignedtrg(t,A)6== ROOT17 endattr[t]=ifattr(s);t; 2 trg phrase(t,P)7 ;return ROOT ;18 return t*s6= head of(s,S)7[sx ,..,sy ] = aligned to(t) ;8end8= find aligned(s,S,T,A);= find highest([sx ,..,sy ],S) ;9 end==Figure 3: Procedure find aligned().10 p = 0 ;9= find aligned(s,S,T,A) ;heuristics[s,..,s]=srcphrase(s,P);x11 t* = undef ;10attr[t] = DUMMY ;s* = find highest([sx ,..,sy ],S) ;multiple targets:12 2 aligned(s,A)11head[t] = ;= head of(s*,S) ;13position(t,T) > ptakeright-most12endfeaturesoptionsoptimizedusingMaltOpt =elsefind aligned(s,S,T,A) ;14t* = ;13p = position(t,T)head[t] [s=xt,..,s; ] = aligned to(t) ;timizer.15The accuracygiven ;in Table 3 set1416endend15= find highest([sx ,..,sy ],S) ;labeledattachmentscores (LAS). include17 endattr[t] = attr(s) ;end 16punctuationevaluation.Ignoring punctua18 return t* ;17= head of(s,S) ;endtion generally leads slightly higher scoresFigure3: Procedurefinddoaligned().notedexperimentsreportDUMMYnodesnumbershere.proposedNote alsoTiedemanncolumns et al.represent target languages (used testing),features options optimized using MaltOpwhilerows denote source languages (usedtimizer. accuracy given Table 3 settraining),McDonaldal. (2013).labeledattachment etscores(LAS). includeFrompunctuationtable,weourseebaselineevaluation.Ignoringpunctuascorescompatibleonestion generally leads slightly higheroriginalscoreschine translationannotationprojection.Here,presented(McDonaldetofal.,cross-domain2013),annotationobtainedusingtool traineddata,especiallylightnotedourbyexperimentsreportFigure 2: Annotationprojectionalgorithm.experimentsalsolookdelexicalizedmodelstrainedincludedTable3reference.differencesnumbershere.Notealsocolumnsaccuracy drops. Moreover, using SMT may help bypassing domain shift problems,translated treebanks show effect machinedueparserselection,usetransitionrepresenttargetlanguages(usedtesting),common applying tools trained (and evaluated) one resource texttranslationdelexicalizedwithout additionalfeatures.transferlexicalparsingfollowing basedap- parserrowsdenotesourcelanguages (usedbeamsearchperceptronanother domain.proach McDonald et al. (2013). Second,learninginalongtraining),inofMcDonaldet Nivreal. (2013).linesZhang(2011)5.1 BaselineResultspresentresults obtained parsers trainedtable,seebaselinewhereasrelygreedytransition-basedparsSecondly, assume SMT produce output much closertarget language treebanks produced using ingma-withscorescompatibleonesIninthethefoloriginallinearsupportvectormachines.Firstpresentbaselineparsingscores.inputmanualtranslationsparalleltextsusuallyare.Evenmayseemlikechine translation annotation projection. Here,presented(McDonaldet al.,as2013),lowing,experimentscompareresultsbaselinebaselines exploreare: (i)monolingualbaseshortcominggeneral,caseannotationprojectionratheradvantage,also look delexicalized models trainedweonhaveincludedTablesetup3 forinreference.differencescomparableexperiments.line, i.e., training testing using lantranslatedshow effect machinemakestreebanksmoreto straightforwardlesserror-pronetransferannotationdueparserselection,usetransitionHowever, improvements shownalsoguage datatranslationwithoutUniversalDependency Treeadditional lexicalfeatures.basedparserbeamsearchperceptronsourcetarget.Furthermore,alignmentwordsphrasesinherentlyapply comparison (McDonald et al., 2013).bank (ii) delexicalized baseline, i.e., applyinglearningalonglines ZhangNivre (2011)providedoutputcommonSMTmodels.Hence,additionalproceduresdelexicalizedparsersacrosslanguages.5.1 Baseline Resultswhereas weTreebanksrely greedy transition-based pars5.2TranslatedbeForperformedtoptheMaltParsertranslatedcorpus. Recentresearch(Zhaoetal.,2009;Durrettmonolingualbaseline,moding linear support vector machines. folFirst present baseline parsing scores.et2012)attemptedaddresssyntheticdatatowecreationresultssyntacticels al.,trainedoriginaltreebanksuniNowturnthewillexperimentstranslatedtree- asvialowing,compareparsingbaselinebaselines explore are: (i) monolingual baseversalPOSlabelslexicalfeaturesleavbanks.considertwosetups.First,lookatmodelsbilingual line,lexica.Tiedemannet al.using(2014)extendproposingthreedifferentweideacomparablesetupexperiments.i.e., trainingtestinglan-inglanguage-specificfeaturesextheeffecttranslationtrainingdelexicalHowever,mostandimprovementsshownalsoautomatictranslationbased Dependencyinducedbilinguallexicaphrase-basedtranslationguage dataUniversalTreeist bankoriginalTheauthorsdelexicalizedizedIncomparisonway, algorithmperformaetdirectapply(McDonaldal., 2013).thetreebanks.(ii)work,delexicalizedbaseline,i.e.,applyingmodels.proposeparsers.newprojectionavoidsparsers trained universalPOSlanguages.labelscomparison baseline performance presentedparsersacrosscreation delexicalizedDUMMYnodestarget languagediscussed section 2.4.5.2Translatedabove.secondsetupTreebanksconsiders fully lexfor languageappliedmonolingual baseline, MaltParser modTheproceduresummarizedpseudo-codeshownFigure6.languages elswithoutmodification.models,with uniicalizedNowmodelstrainedtranslatedontreebanks.trainedoriginaltreebanksturntheonexperimentstranslated treet = find aligned(s,S,T,A) ;==Figure202: Annotation[sxprojection,..,sy ] = srcalgorithm.phrase(s,P) ;Figure 6:21 Annotationprojectionwithouts* = findhighest([sx ,..,s],S) ;22 (2014).= head of(s*,S) ;23= find aligned(s,S,T,A) ;delexicalizedtransfer parsingfollowing ap24head[t] = ;proach 25McDonaldendet al. (2013). Second,end obtained parsers trainedpresent the26 results27 endtarget languagetreebanks produced using ma1819versal POS labels lexical features leaving language-specific features they221exist original treebanks. delexicalizedparsers trained universal POS labelslanguage appliedlanguages without modification. models,banks. consider two setups. First, lookeffect translation training delexicalized parsers. way, perform directcomparison baseline performance presentedabove. second setup considers fully lexicalized models trained translated treebanks.fiTiedemann & Agicrootpadpmodnsubjadpmod adpobjadpobjdetamodPRON VERB ADP NOUN ADJ ADP DET NOUN .Ils tiraientballes reelles surlafoule .firing live rounds crowd .PRON PRON VERB ADP NOUN ADP DET NOUN .adpmod adpobjnsubjnsubjdetadpobjadpmodprootFigure 7: example sentence translated French English projections usingalgorithm shown Figure 6. boxes indicate segmentation usedphrase-based translation model.key feature algorithm makes use segmentation sentencesphrases together counterparts language appliedunderlying translation model. use information handle unaligned tokenswithout creating additional DUMMY nodes described Figure 6. However, contraryexpectations, algorithm work well practice Tiedemannet al. (2014) show empirically simple word-to-word translation model outperformsphrase-based systems projection algorithm cases. Part problemambiguous projection PoS labels handling one-to-many many-to-one alignments.example shown Figure 7. assigned pronouns duelinks French Ils certainly confuses model trained projected data.treebank translation approach using phrase-based SMT exploredTiedemann (2014). Tiedemann (2015) introduces use syntax-based SMT crosslingual dependency parsing. work, authors propose several improvementsDCA-based projection heuristics originally developed Hwa et al. (2005). Simpletechniques reduce number DUMMY elements projected data helpsignificantly improve results cross-lingual parsing. also realized placementDUMMY nodes crucial. Strategies choose positions minimize riskadditional non-projectivity useful improve parser model induction. mainlyuse techniques developed work experiments described section 3.drawbacks synthetic treebanking approach related hybrid nature:a) inherits syntax projection risks annotation projection approachsuccess bound projection quality, b) critically depends qualitySMT, turn depends size quality underlying parallel corpora.222fiSynthetic Treebanking Cross-Lingual Dependency Parsinglatter point, experiments Tiedemann et al. (2014) reveal compellingrobustness cross-lingual parsing SMT noise framework, paperalso argue projection synthetic texts simpler projection actualparallel text. Another important drawback need large parallel data sets trainreasonable translation models languages consideration. Alternatively,handcrafted rule-based system could applied well. However, systems datasets rarely available low-resource languages. hand, techniquesimprove machine translation via bridge languages. Tiedemann Nakov (2013)demonstrate small amounts parallel data successfully used buildingtranslation models truly under-resourced languages. approach creating synthetictraining data statistical machine translation low resource languages fits wellspirit synthetic treebanking.2.6 Truly Under-Resourced Languages?point, outlined underlying concepts major approachescross-lingual dependency parsing today. also discussed intricacies enablingcross-lingual parser evaluation. Here, proceed discuss two outlooksnamely,way implement cross-lingual parsers, way evaluate parsingaccuracyreflect dependency parsing truly under-resourced languages.makes language under-resourced? Following Uszkoreit Rehm (2012),acknowledge many facets involved attempting address question. Generally,however, under-resourced language distinguished lacking basic NLP-enablinglinguistic resources, PoS-tagged corpora treebanks. paper, take dependency parsing-oriented viewpoint, allows casting issue under-resourcednessspecific terms dependency parsing enablement given language. Thus, languageunder-resourced cannot build dependency parser or, otherwise said,dependency treebank exists language. Since statistical dependency parsing criticallydepends availability PoS tagging, make additional requirement,turn implies following three levels resource availability. Note listparsing-oriented specialization general discussion low-resource languagesintroduction.1. PoS-tagged corpus treebank available given language,virtue those, hands PoS tagger dependency parserlanguage. call languages well-resourced resource-rich languagesdependency parsing viewpoint, use dedicated native language resourcesparse texts written language.2. given language, PoS tagging parsing resources available.includes annotated corpora NLP tools. address languagesunder-resourced low-resource languages, cannot natively parsesyntactic dependencies, neither annotate PoS tags.3. PoS-tagged corpus PoS tagger available given language,treebanks parsers exist it. Even NLP support languages223fiTiedemann & AgicPoS annotation, still approach under-resourced viewpointdependency parsing.want parse languages group 2 syntactic dependencies, must addressissuesthe unavailability supporting resources PoS tagging dependencyparsingand often even basic processing facilities sentence splitters tokenizers.NLP, often call languages truly under-resourced. Group 3 somewhat easier,presumably address dependency-syntactic processing layer.recent years, field dealt extensivelyand large, separatelywithproviding low-resource languages PoS taggers dependency parsers. Taking twoexamples account, Das Petrov (2011) show bootstrap accurate taggersusing parallel corpora, Agic et al. (2015) take under-resourcedness extremepresuming severe data sparsity still manage yield reasonable PoS taggerslarge number low-resource languages. thus safe conclude evenseverely under-resourced languages, reasonable PoS taggers made available using onetechniques, already available off-the-shelf.reasoning underlies current approaches cross-lingual dependency parsing,presume availability PoS annotations, natively publicly availablerelated research. Since also required least intrinsically evaluate resultingparsers, conduct empirical assessments exclusive group languages leastsyntactically annotated test data available. effect, evaluating proxy,truly under-resourced languages enjoy even basic test set availability. topthat, various top-performing approaches cross-lingual parsingsuch previouslydiscussed annotation projection, treebank translation, word representation-supportedmodel transferintroduce additional constraints requirements. often, presumeavailability large source-target parallel corpora. One might argue accordinglymake poor case low-resource languages amassing prerequisites methodswork, thus departing definition low-resource language. turn,favor current approaches, argue following.current research enabling PoS tagging under-resourced languages justifiesseparate handling cross-lingual dependency parsing presuming availabilityPoS tagging. refer reader work Tackstrom et al. (2013a)detailed exposition state-of-the-art results, together previously mentionedwork bootstrapping taggers.McDonald et al. (2013) validate evaluation proxy showing uniformsyntactic representation partially enables inferential reasoning performanceported parsers truly under-resourced languages. Namely, show typologicalsimilarity plays important role predicting quality transferred parsers.built by, example, Rosa Zabokrtsky (2015), use data-drivenlanguage similarity metric actually predict best sources given targetscross-lingual parsing.remaining prerequisites top-level cross-lingual parsing, treebanktranslation approach argue paper, amount source-target parallel224fiSynthetic Treebanking Cross-Lingual Dependency Parsingcorpora possibly also monolingual target corpora. may first seemsubstantial added requirement, note text corpora readilyavailable expert-annotated linguistic resources, collections OPUS(Tiedemann, 2012) provide large quantities cross-domain data many languages.claim, Agic et al. (2015) illustrate annotation projection couldapplied learn PoS taggers hundreds, possibly even thousands languages usingnothing translations (parts of) Bible simple setup.concluding, duly note perceived disconnect evaluating cross-lingualparsers actually enabling dependency parsing languages lack respectiveresources. argue former constitutes empirical research, latterprimarily engineering feat, thus obliged follow field adheringformer contribution. However, note devising multiple systematicdownstream evaluation scenarios truly under-resourced languages sorely neededpoint fields development, would resolve important disconnect cross-lingualNLP research.proceed discuss core paper: empirical validation synthetictreebanking approach cross-lingual parsing. reflect prerequisitestruly under-resourced languages related work discussion follows expositionsynthetic treebanking.3. Synthetic Treebanking Experimentssection, discuss series experiments systematically explore variouscross-lingual parsing models based annotation projection treebank translation. Here,assess properties specific approach, compare intrinsicallybaseline. provide comparison selected recent work section 4.setup, always use test sets provided Universal Dependency Treebankversion 1 (UDT) (McDonald et al., 2013) cross-lingually harmonized annotationmakes possible perform fair evaluations across languages including labeled attachment scores (LAS), use primary evaluation metric. Similar previousliterature, include punctuation calculation LAS ensure comparabilityrelated literature (Tiedemann, 2014). experiments, apply mate-tools (Bohnet,2010) train graph-based dependency parsers, gives us competitive performancesettings. leave Korean experiments due factbitexts domain languages, need annotationprojection SMT training. Thus, experiment using five languages: English (en),French (fr), German (de), Spanish (es), Swedish (sv).3.1 Baselineinitial baseline delexicalized model straightforward train providedtraining data UDT. Table 1 lists attachment scores achieved applyingmodels across languages. scores confirm results McDonald et al. (2013); minordifferences due different choices training algorithms. Note alwaysuse columns represent target languages test rows refer source languages225fiTiedemann & Agicused training, projection translation. also always report scores sourcetarget pairs, reporting averages highest per-target scores might arguably makebiased insight methods.target languageLASdeenesfrsvde70.8448.6047.1646.7752.53en45.2882.4447.3147.9448.24es48.9056.2571.4562.6652.95fr49.0958.4762.3973.7155.02sv52.2459.4254.6354.8974.55mate-tools (coarse)mate-tools (full)78.3880.3491.4692.1182.3083.6582.3082.1784.5285.97Table 1: Results delexicalized models. comparison also LASslexicalized models bottom table. coarse uses coarse-grained PoS labelsfull adds even fine-grained PoS information.see, results around 10 LAS points fully lexicalized modelssignificant drops observed training languages even thoughquite closely related. unexpected considering naive approach usingcoarse-grained PoS label sequences without modification type informationtraining models. note, however, decrease accuracy drastictypologically closest language pair (French-Spanish). following section,discuss various ways adapting cross-lingual models target language,start annotation projection aligned parallel corpora.3.2 Improved Annotation ProjectionAnnotation projection used connection word-aligned bilingual parallel corpora(bitexts). experiments, use Europarl (Koehn, 2005) language pairfollowing basic setup Tiedemann (2014). baseline model applies DCAprojection heuristics presented Hwa et al. (2005) first 40,000 sentencesbitext corpus (repetitions sentences included). Word alignments producedusing IBM model 4 implemented GIZA++ (Och & Ney, 2003) trained typicalpipeline common statistical machine translation using Moses toolbox (Koehnet al., 2007). use entire Europarl corpus version 7 train alignment modelsobtain proper statistics reliable parameter estimates. asymmetric alignmentssymmetrized intersection grow-diag-final-and heuristics. resultsbaseline projection model given Table 2.value word-aligned bitext clearly seen performance crosslingual parser models. outperform naive delexicalized models large margin.However, still pretty far away supervised monolingual models evenrelated language pairs. Tiedemann (2015) discusses various improvementsprojection algorithm significant effects performance trained models. One226fiSynthetic Treebanking Cross-Lingual Dependency Parsingdeenesfrsvde62.2860.4661.2762.96en53.2749.3453.4651.07es57.6962.2966.5161.82fr60.4965.5468.1064.99sv65.2566.9764.6762.75Table 2: Baseline performance LAS DCA-based annotation projection 40,000parallel sentences tested target language test sets.problem DCA algorithm creation DUMMY nodes labels disturbtraining procedures. Many nodes easily removed without loosing muchinformation. Figure 8 illustrates approach deletes DUMMY leaf nodes collapsesdependency relations run via internal DUMMY nodes single out-going edges.Adding modification DCA projection heuristics achieve significantimprovements various language pairs. Table 3 summarizes LASs modelsnew treatment DUMMY nodes.Tiedemann (2015) also introduces new procedure treating one-to-many wordalignments. original algorithm, cause additional DUMMY nodes actparents aligned target language tokens. new approach takes advantagedifferent alignment symmetrization algorithms uses high-precision links comingintersection asymmetric word alignments find head multi-word unit,whereas links high-recall symmetrization used attach words headword. Figure 9 illustrates procedure means sentence pair Europarl.Finally, Tiedemann (2015) also proposes discard trees remaining DUMMYnodes. may remove 90% training examples assuming availabilitylarge bitexts makes possible project additional sentences fill training data.Discarding projected trees DUMMY nodes effectively removes sentence pairsnon-literal translations complex alignment structures case less suitedlabel 2label 2label 1label 1label 3src1src2 src3src4pos1pos2pos4pos3label 2label 3label 1src1src2 src3src4pos1pos2pos3pos4dummypos2pos4pos3DUMMY trg1trg2trg3DUMMYtrg1trg2trg3label 3src1src2 src3src4pos1pos2pos3pos4pos1pos2pos4trg1trg2trg3pos1dummylabel 2dummylabel 3label 1label 2label 1Figure 8: Removing DUMMY nodes projected parse trees: (i) Delete DUMMY leafnodes. (ii) Collapse unary productions DUMMY nodes.227fiTiedemann & Agicdedeenesfrsv**62.97+0.6959.880.5861.59+0.3262.160.80en53.54+0.2748.850.4953.120.3451.31+0.24es60.17+2.48**63.80+1.51fr62.35+1.86**66.47+0.9368.55+0.45*****67.00+0.4962.58+0.76sv66.99+1.7467.19+0.22**65.33+0.66**64.52+1.77**65.38+0.39Table 3: Results collapsing dependency relations unary dummy nodes removingdummy leaves (difference annotation projection baseline superscript).Improvements marked ** statistically significant according McNemarstest p < 0.01 improvements marked * statistically significantp < 0.05.rootpdobjdetamodnsubjadpmodadpobjPRON VERB DET ADJNOUNWir wollen eine echte WettbewerbskulturwanttruePRON VERB DET ADJnsubjamoddetdobjcultureNOUNADPNOUNEuropa..competition Europe .DUMMY DUMMY ADP NOUN .adpobjDUMMYDUMMYadpmodrootpFigure 9: Projecting German English using alternative treatment one-tomany word alignments. Dotted lines links grow-diag-final-andsymmetrization heuristics solid lines refer links intersection wordalignments.annotation projection. Table 4 summarizes results method tested setup.observe significant improvements language pairs compared baselineapproach two cases also better results previous setting shownTable 3.228fiSynthetic Treebanking Cross-Lingual Dependency Parsingdedeenesfrsven53.80+0.53****50.10+0.7653.88+0.42****52.36+1.29*********63.52+1.2460.65+0.19****62.49+1.22****63.83+0.87es61.34+3.65***63.18+0.89**********68.15+1.6463.29+1.47fr62.32+1.83**67.04+1.50*68.81+0.71****sv68.20+2.95****67.74+0.77***65.79+1.12**64.83+2.08****66.12+1.13Table 4: Discarding trees include DUMMY nodes; results 40,000 accepted trees.Results marked ** * significantly better projection baseline(with p < 0.01 p < 0.05, respectively) results marked **** ***also significantly better ones Table 3 (with p < 0.01 p < 0.05,respectively).3.3 Phrase-Based Treebank TranslationTreebank translation interesting alternative annotation projection. mainadvantage skip noisy source-side annotation out-of-domain bitextable project information source target language. Furthermore, word alignmenttightly coupled statistical translation models makes straightforwarduse links projection. Finally, advantage projection machinetranslation prefers literal translations similar syntactic structures. Unrestricted humantranslations much varied proper alignment translation equivalentsnecessarily straightforward. machine translation, mapping tokenstoken n-grams essential favors successful annotation projection. largestdrawback is, course, translation quality. Machine translation difficult taskuse annotation projection requires least level quality even thoughnecessarily interested semantically adequate translations.first approach applies model proposed Tiedemann et al. (2014), usingstandard phrase-based SMT model translate source language treebanks targetlanguage. projection based DCA heuristics similar ones appliedannotation projection described previous section. also apply modificationDUMMY node handling introduced before. However, cannot apply alternativetreatment one-to-many alignments different types word alignmenttranslation model. also filter trees remaining DUMMY nodeswould cause serious reduction already small-sized treebanks. contrastprojection bitexts cannot add data fill training data.experiments, MT setup generic uses Moses toolboxtraining, tuning decoding (Koehn et al., 2007). translation models trainedentire Europarl corpus version 7 without language-pair-specific optimization. Wordalignments essentially used experiments annotationprojection section 3.2. tuning use MERT (Och, 2003) newstest2011 dataprovided annual workshop statistical machine translation (WMT).1 Swedish1. http://www.statmt.org/wmt14.229fiTiedemann & Agicuse sample OpenSubtitles2012 corpus (Tiedemann, 2012). languagemodel standard 5-gram model based combination Europarl News dataprovided source. apply modified Kneser-Ney smoothing without pruning,applying KenLM tools (Heafield, Pouzyrevsky, Clark, & Koehn, 2013) estimatingLM parameters.dedeenesfrsven56.24+2.70**50.65+1.80**55.69+2.57**53.01+1.70****59.413.5653.945.94**57.054.54**58.573.59******es57.652.5263.760.0468.66+1.6662.69+0.11fr59.063.29**67.99+1.52**69.70+1.15**sv64.622.3767.52+0.33**62.732.60**62.771.75**64.760.62Table 5: Results phrase-based treebank translation (difference corresponding annotation projection model DUMMY node removal Table 3 superscript).Results marked ** significantly different projection results (withp < 0.01).results experiments phrase-based SMT summarized Table 5.large extent, confirm findings Tiedemann (2014) translation approachadvantages projection automatically annotated parallel corpora.language pairs, labeled attachment scores significantly projectionresults even though parsers trained much smaller data sets (the treebankstypically much smaller 40,000 sentences language pairs). striking alsooutcome German target language, seems hardest languagetranslate data set. surprising German general considereddifficult target language setup languages are, example, supportedWMT. also applies use German source language surprising exceptiontranslating English. Overall, good results English may influencedstrong impact language model draw large monolingual resources.3.4 Syntax-Based Treebank TranslationTiedemann (2015) introduces use syntax-based SMT another alternative treebanktranslation. standard syntax-based MT models supported Moses basedsynchronous phrase-structure grammars induced word-aligned parallel data.Several modes available. case, mostly interested tree-to-stringmodels use synchronous tree substitution grammars (STSGs). assumptionstructural relations induced parallel corpus fixed givensource-side analysis improve projection syntactic relations used combinationsyntax-based translation.order make possible use dependency information framework synchronous STSGs convert projective dependency trees bracketing structureused train tree-to-string models Moses. use yield word230fiSynthetic Treebanking Cross-Lingual Dependency ParsingPRON VERB PRON . PRON ADP DETNOUNPRT VERBIchbitteSie , sichzu einer Schweigeminute zu erhebennsubjdobjauxdetadpobjpadpmoddobjrootxcompROOTnsubjVERBdobjpPRONbittePRON.Sie,IchxcompadpobjPRON ADPsichauxadpmoddobjzudtNOUNVERBPRT erhebenzuDET SchweigeminuteeinerFigure 10: dependency tree taken automatically annotated parallel datalossy conversion constituency representation.define span sentence forms constituent label takenrelation word head.Dependency trees certainly optimal kind constituency-based SMTmodel usually flat provide deep hierarchical structurescommon phrase-structure trees. However, previous research shownvaluable syntactic information pushed model waybeneficial projecting dependency relations. Note use PoS tags additionalpre-terminal nodes enrich information given system.training models used data sets word alignmentsused phrase-based SMT. However, require number additional steps listed below:tag source side parallel corpus PoS tagger trained UDTtraining data using HunPos (Halacsy, Kornai, & Oravecz, 2007).231fiTiedemann & Agicparse tagged corpus using MaltParser model trained UDTfeature model optimized MaltOptimizer (Ballesteros & Nivre, 2012).2projectivize trees using MaltParser convert nested tree annotationsexplained (Tiedemann, 2015).extract synchronous rule tables word aligned bitext source sidesyntax score rules using Good Turing discounting. use size limitreplacing sub-phrases non-terminals source side restrict numbernon-terminals right-hand side extracted rules three. Furthermore,allow consecutive non-terminals source side increase coverage,allowed default settings hierarchical rule extractor Moses.tune model using MERT data sets before.Finally, convert training data UDT source language translatetarget language using tree-to-string model created above.results approach listed Table 6. see syntax-based modelssuperior phrase-based models almost cases. majority languagepairs also see improvement annotation projection approach even thoughtraining data much smaller. confirms findings Tiedemann (2015)outperforms results large margin due parsing model used experiments.dedeenesfrsv**62.670.3057.132.75**61.410.18**61.730.43en58.60+5.06****es61.00+0.83**64.58+0.78****52.65+3.8056.83+3.71**52.13+0.82**68.97+1.9762.340.24fr63.45+1.1068.45+1.9869.37+0.82**sv67.88+0.89**68.16+0.97**63.551.7862.561.96**64.500.88Table 6: Results syntax-based treebank translation (difference correspondingannotation projection model Table 5 superscript). Numbers bold facebetter corresponding phrase-based SMT model. Results marked** significantly different phrase-based translation results (p < 0.01);significantly different projection model (p < 0.01 p < 0.05,respectively).3.5 Translation Back-ProjectionAnother possibility cross-lingual parsing integration translation actualparsing pipeline. basic idea use tools languages, dependencyparsers, without modification adjusting input match expectations tool,2. use MaltParser efficiency reasons. parsing performance slightly baselinemodels trained mate-tools parsing fast require parsing bitexts.232fiSynthetic Treebanking Cross-Lingual Dependency Parsinglabel 2label 1pos1src1label 3pos2pos3src2 src3(2) parselexicalizedparserpos4src4(3) project(1) translatetrg1trg2trg3trg4pos2pos1pos3pos4label 1label 3label 2Figure 11: Translation back-projection: Input data translated source languageexisting parsers (step 1), parsed source language (step 2) and, finally,parse tree projected back original target language.example, translating language parser accepts. muchspirit text normalization approaches frequently used NLP historicaldocuments user-generated content input modified wayexisting tools standard language applied. Figure 11 illustrates approachapplied dependency parsing.advantage approach rely optimized parsers trainedmanually corrected treebanks. However, several significant drawbacks. Firstall, loose efficiency due additional translation step required parsingtime. crucial disadvantage rules approach many applicationsrequire parsed information large scale data sets real-time responses. Anotherimportant drawback noise coming translation leading kind input,parser usually trained and, therefore, hard time handle correctly.Finally, also problem back-projection. Unfortunately, straightforwardreverse projection heuristics discussed earlier. cannot introduce DUMMY nodesfill gaps required projecting entire structure DUMMY labelsuseful either. projection heuristics discussed section 3.2 help avoid DUMMY nodesand, therefore, apply extensions experiments. Another problem relatedunaligned target words. DCA algorithm (including modified versions discussedfar), tokens simply deleted attached dependency tree all.method, however, possible back-projection tokens needattached. reason, implement new rule attaches unaligned tokeneither preceding consecutive word attached tree themselves.case simply attach ROOT. Another problem labeladded dependency due lack knowledge setlabel DUMMY. way, get credit LAS may least improveUASs. test approach using syntax-based SMT translation model.results listed table 7.233fiTiedemann & Agicdeenesfrsvde44.8617.4236.6923.7737.4423.8336.8426.12en35.9217.3541.917.4342.0011.4635.2315.84es32.9024.7948.0814.2155.5410.9731.9629.86fr36.6823.8148.1917.3554.7813.3233.7431.25sv45.5619.6951.7415.2343.2321.4442.3920.36Table 7: Back-projection results comparison annotation projection baselinesection 3.2 (Table 3).scores low, even fall behind baseline delexicalizedmodels. extreme drop performance actually bit surprising consideringstrong disadvantages discussed may expected well. Another reasonextreme differences performance also fact need rely predicted PoSlabels translated data piping source language parser.certainly strong disadvantage procedure comparison evaluations basedgold standard PoS annotation entirely fair. See also section 3.8 discussionsimpact PoS label accuracy parsing performance.3.6 Annotation Projection Translation Qualityinteresting question whether correlation translation qualityperformance cross-lingual parsers based translated treebanks. approximationtreebank translation quality computed BLEU scores well-established MT testsets WMT shared task, case newstest 2012.3Figure 12 illustrates correlation BLEU scores obtained newstest dataLASs corresponding cross-lingual parsers. First all, seeMT performance phrase-based syntax-based models quite comparablenoticeable exceptions syntax-based SMT significantly better (French-EnglishFrench-Spanish, rather surprising). However, looking language pairssee increased parsing performance seem due improvementstranslation rather due better fit models syntactic annotation projection(see German, example). Nevertheless, observe weak correlation BLEUscores LAS within class models one notable outlier, Spanish-English.correlation reflects importance syntactic relation languages successmachine translation annotation projection. Closely related languages like FrenchSpanish top level tasks whereas French Spanish map wellGerman. Translations English exception evaluation. Translation modelsoften work well direction whereas annotation projection English underperformsexperiments.3. Note leave Swedish test test set available language.234fiSynthetic Treebanking Cross-Lingual Dependency Parsinglabeled attachment score (LAS)7068RPB-SMT = 0.46366Rsyntax-SMT = 0.340fr-esen-fren-fr64es-fres-frfr-esen-esen-es62en-de60fr-dede-frde-es58fr-deen-dees-de56de-frde-en de-esfr-ende-en fr-en54es-dees-enes-en52101520253035BLEUFigure 12: Correlation BLEU scores cross-lingual parsing accuracy (usingPearsons correlation coefficient).3.7 System Combination Multi-Source Modelsfar, interested transferring syntactic information one source languagetarget language using one specific model cross-lingual parsing. However,approaches easily combined focus creation synthetictraining data. least two possibilities explored.1. combine data several source languages increase amount trainingdata obtain evidence various languages projected target language.2. Several models combined benefit various strengths modelmay work complementary information.paper, opt simple approach test ideas. concatenatedata sets augment training data train standard parsing models usual. First,look multi-source models within paradigm. Table 8 lists labeled attachmentscores obtain combining data sets source languages train targetlanguage parsers projected annotations.table, see able achieve significant improvementslanguages models except Spanish. Furthermore, English Frenchobtain overall best result presented paper combined syntax-based SMTprojections. final system combination, merge data sets languagesmodels. results parsers trained combined data sets shownTable 9.4. results multi-source multi-model system combinations provided Tiedemann (2015).235fiTiedemann & AgicLASbest published result4best individual modelde60.9463.83en56.5858.60es68.4568.97fr69.1569.70sv68.9568.20annotation projectionphrase-based SMTsyntax-based SMT66.7661.8565.8955.3060.9461.5667.3768.0868.6069.4871.5472.7871.9571.6972.14Table 8: Results combining projected data source languages train target languageparsing models. Numbers italics worse one models traineddata individual language pairs.LASUASACCde67.6075.2781.99en57.0564.5472.75es69.3676.8582.22fr72.0379.2183.06sv73.4081.2883.04Table 9: Results combining projected data source languages train target languageparsing models. Additionally LAS also includes unlabeled attachment scores(UAS) label accuracy (ACC) make easier compare resultsrelated work.German, French Swedish yields yet another significant improvementlabeled attachment scores close 70% even above. results represent highestscores reported task far outperform previously published scoreslarge margin. expect sophisticated system combinations would pushresults even further.3.8 Gold vs. Predicted PoS Labelscommon evaluate results gold PoS labels given test settarget language treebank. disregard impact PoS qualityoften presentrelated workmakes unrealistic evaluation scenario. previous section,discussed results use gold standard annotation order make possible compareresults baselines related work. section, look detailsreplacing PoS labels predicted values. Here, report resultstreebank translation approach using syntax-based SMT test case. approachesshow similar trends.first experiment looks case annotated data available targetlanguage training PoS taggers. use HunPos (Halacsy et al., 2007) train modelstraining data language use replace gold standard tags testsets PoS labels models predict. results experiments appliedtranslated treebanks section 3.4 shown Table 10.236fiSynthetic Treebanking Cross-Lingual Dependency ParsingdeenesfrsvPoS taggerde58.703.9753.373.7657.184.2357.634.10en56.492.1150.891.7654.941.8950.171.96es57.523.4861.253.3364.324.6559.362.98fr59.993.4664.324.1365.244.1360.893.61sv62.685.2063.974.1958.784.7758.224.3495.2497.5695.3795.0895.86Table 10: Results cross-lingual parsing predicted PoS labels coming taggerstrained target language treebanks. numbers superscript givedifference result gold standard labels (Table 6). last row showsoverall accuracy PoS tagger.see PoS labels strong impact parsing performance. languagepairs, observe significant drop LAS even quite accurate taggers,proves one need careful applying models real-life scenarios. nextexperiment stresses point even more. Here, replace PoS labels tagspredicted taggers trained noisy translated treebanks projectedannotation. Note need remove training examples DUMMY labels reduceerrors tagger.deenesfrsvde85.3382.3983.7684.79enesfrsv81.32 81.23 82.41 84.2984.41 85.56 86.3281.0589.37 83.2680.64 89.9584.1181.66 86.05 84.81Table 11: PoS tagging accuracy models trained translated treebanks.Table 11 lists accuracy taggers trained noisy projected data.observe significant drop tagger performance completely plausible consideringsubstantial noise added translation projection also consideringlimited size data use training. Treebanks considerably smallerannotated corpora usually taken training PoS classifiers. applyingtaggers test sets observe dramatic drop parsing performance expected.Table 12 lists results experiments.findings conclude cross-lingual techniques still require lotimprovement become practically useful low-resource scenarios real world.done experiment annotation projection approach observedbehavior even though rely larger data sets training taggers.performance drop using predicted PoS labels trained noisy data sets amounts237fiTiedemann & Agicdeenesfrsvde51.896.8144.598.7849.727.4647.949.69en46.0410.4547.813.0849.045.9044.235.94es48.618.9159.371.8861.303.0255.024.34fr50.369.6362.371.9559.815.4352.798.10sv52.739.9560.433.5452.126.6651.107.12Table 12: Results cross-lingual parsing predicted PoS labels coming taggerstrained projected treebanks. difference results predicted labelsTable 10 shown superscript.10 LAS points cases similar see treebank translation approach.omit results add new information discussion.Finally, also need check whether system combinations multi-source modelshelp improve quality cross-lingual parsers predicted PoS labels. this,use strategy section 3.7 concatenate various data files trainparser models combine models language pairs. words, usemodels trained section 3.7 evaluate test sets automatically taggedPoS labels. Again, use two settings: 1) apply PoS taggers trained manuallyverified data setsthe monolingual target language treebanks, 2) use PoS taggerstrained projected translated treebanks. latter data setsdisposal and, therefore, expect better PoS model well. Table 13 lists finalresults comparison ones obtained gold standard annotation.PoSPoSPoSPoSde78.3870.8452.5367.60en91.4682.4448.2461.56es82.3071.4562.6669.36fr82.3073.7162.3972.78sv84.5274.5559.4273.40monolingual PoS tagger accuracycombined projected PoS tagger accuracy95.2488.4797.5688.2495.3788.0695.0889.8395.8688.07monolingual baseline predicted PoSdelexicalized monolingual predicted PoSbest delexicalized cross-lingual predicted PoScombined cross-lingual predicted PoScombined cross-lingual projected PoS model73.0364.2548.3663.1457.8488.3872.8143.8755.1651.6676.5960.4952.9464.9961.4076.7964.0652.4767.9163.8677.8365.7749.8467.9361.58monolingual baselinedelexicalized monolingualbest delexicalized cross-lingualbest cross-lingual modelgoldgoldgoldgoldTable 13: comparison models evaluated gold standard PoS annotation (fourtop-level systems) models tested automatically tagged data.First all, see best cross-lingual models outperform delexicalizedcross-lingual models large margin. come close delexicalized models trained238fiSynthetic Treebanking Cross-Lingual Dependency Parsingtarget language data exception English works much betteroriginal data set. lower part table, observe scores drop significantlygold standard PoS labels replaced predicted tags. Note four systemsusing predicted PoS labels apply tagger trained monolingual verified target languagedata gives quite high accuracy. final system table oneapplies PoS model trained projected translated data. tagger modelsmuch less accurate, shown middle Table 13, influence degradationvisible attachment scores obtained systems. However, models reflectreal-world scenario annotated available target language, eventraining PoS taggers. advantage projection translation approachesmodel possible all, whereas delexicalized transfer models always requireexisting tools produce shared features used prediction system. Note alsocross-lingual models outperform delexicalized models trainedverified target language datawith English striking exceptionwhich remarkablegiven noisy data trained on.3.9 Impact Dataset Sizeslarge, data-driven dependency parsers benefit introducing additionaltraining data. subsection, control amount training data providedmethod, observe impact LAS cross-lingually. experiment improvedannotation projection (see Section 3.2), introduce 60 thousand sentencesprojected dependency trees. five target UDT languages experiment,provide four learning curves representing four source languages. plot resultsFigure 13.observe virtually transferred parsers benefit introduction additionaltraining data, albeit improvements slight models levelaround 20 thousand sentences. source languages follow LAS learningcurve patterns targets, observe trend violations specificsource-target pairs. that, observe clear source-target preferences,source orderings LAS mostly remain training set sizes.lower-ranked sources benefit even degrade introducing training data,example, Spanish parser induced German data, English parser createdprojecting Swedish trees. said, worth noting best source-targetpairs, targets always benefit introducing source data: German EnglishSwedish, English German French, Spanish French vice versa,Swedish German English. clear indicator future improvements,method apparently benefits adding data. time, learningcurves show benefits truly under-resourced languages, largest relative gainsalready reached relatively modest quantities 20 thousand sentence pairs. Moreover,typological groupings former list top-performing source-target pairs quiteapparent, case throughout experiments.239fienesfrsv0LAS (projected Spanish)LAS (projected English)636261605958575655545310k 20k 30k 40k 50knr projected sentences6866646260deenfrsv58565410k 20k 30k 40k 50knr projected sentencesLAS (projected Swedish)0545352515049484746454460kdeesfrsv0LAS (projected French)LAS (projected German)Tiedemann & Agic60k60k7068666462deenessv60585606867666564636261605910k 20k 30k 40k 50knr projected sentences10k 20k 30k 40k 50knr projected sentences60kdeenesfr010k 20k 30k 40k 50knr projected sentences60kFigure 13: impact training data: Different sizes projected data training crosslingual parsing models.4. Comparison Related Worksectionhaving thoroughly analyzed synthetic treebankingwe revert top-leveldiscussion cross-lingual parsing. it, contrast approach several selectedalternatives related work, sketch properties viewpoint enablingdependency parsing truly under-resourced languages. proceed outliningcomparison.already compared various synthetic treebanking approaches one anotherdelexicalized transfer baseline McDonald et al. (2013) section 3. Here,aim introducing number top-performing representatives methods discussed240fiSynthetic Treebanking Cross-Lingual Dependency Parsingoverview section: competitive model transfer approach, approach dealingdistributed word representations, annotation projection-motivated approach.replicating approaches would time-consuming, constrain searchapproaches also report scores UDT version 1 respective publication,compare referencing. select following approaches discussion.Delex: delexicalized model transfer baseline McDonald et al. (2013).report scores Sgaard et al. (2015) used arc-factored adaptationmate-tools parser, replication original, convenientlyreport multiple metrics. discuss metrics below, note usedgold PoS.Multi: reimplementation McDonald et al. (2011) multi-source projected system(multi-proj. original paper) Xia (2014). providecompetitive baseline system. original work predates UDT evaluatesheterogenous CoNLL treebanks, Xia (2014) evaluate UDTtreebanks report scores. Note parsing model preprocessinginherent setup, differing original setup McDonald et al.(2011). setup details described text, Xia.Proj: improved annotation projection approach described section 3.2.final approach subsection, dependency relationsunary dummy nodes collapsed, dummy leaves removed, Europarl treesremaining dummy nodes discarded (see Table 4). scores given goldPoS tags.Trans G & P: report best syntax-based cross-lingual treebank translationscores gold predicted PoS, respectively. PoS predictions comeHMM tagger (Halacsy et al., 2007). taggers trained target languagetreebanks, score 95% average (see Table 10).Comb G & P: multi-source syntax-based cross-lingual parsers.build Trans G & P approaches: instead single sources, multipletreebanks translated target languages, providing combined synthetictreebanks train parsers on. before, also report scores gold HunPospredicted PoS.Rosa: multi-source delexicalized transfer approach Rosa Zabokrtsky(2015), weighted variant. method, target parsed multiplesources, parse assigned weight based empirically establishedlanguage similarity metric. target sentence, multiple parses constitutedigraph, top (Sagae & Lavie, 2006)-style maximum spanning tree votingscheme implemented. use gold PoS tags.Sgaard: model, delexicalized model transfer augmented inter-lingualword representations based inverted indexing via Wikipedia concept links (Sgaardet al., 2015). choose recent illustrative example leveraging word241fiTiedemann & AgicTargetlanguagedeenesfrsvBaselinesDelex Multi56.8063.2166.0067.4969.2172.5774.6075.87Proj72.6562.7974.9276.1376.96Synthetic treebankingTransG TransP CombG70.6265.1075.7176.3376.9867.5963.6272.1672.9573.6175.2764.5476.8579.2181.28CombP71.7963.1573.2076.0676.83Recent approachesRosa Sgaard Xia56.8042.6072.7050.8056.5664.0366.2267.3274.0175.6076.9379.27Table 14: Comparison cross-lingual parsing methods. contrast rest paper,report UAS scores attain maximum coverage results reportedrelated work.embeddings improving cross-lingual dependency parsing. use embeddingsenabled version Bohnets parser (Bohnet, 2010) gold PoS tags. reportmulti-source results.Xia: approach Xia (2014) novel method leverages Europarltrain probabilistic parsing models resource-poor languages maximizingcombination likelihood parallel data confidence unlabeled data.report best approach (marked +U paper), makes useparallel unlabeled data. use top-performing PoS taggers trainedtarget languages, reaching least 95% accuracy.discussing results, make number remarks comparison. First,target language, report best obtained score method, rather possiblymisleading averages complex source-target matrices. related work, Englishused target language. Second, contrast remainder paperandcontrary guidelines evaluating cross-lingual parsers following McDonald et al.(2013)we report UAS only. targeted exclusively facilitating comparisonrelated work, contributions part still report UAS scores, evenworking UDT. see unfortunate, also note LAS-enabledreplication study exceeds scope match focus contribution. Third,also related able control experiment parameters, noteissue reporting scores gold predicted PoS, different ways obtainingpredicted annotations. record differences list above. Finally, notereferenced contributions explicitly state whether scoring includedpunctuation not, whereas include experiments.results given Table 14 proceed discuss detail,reflecting methods intricacies requirements process.table, visually group methods baselines (Delex, Multi),proposed approaches (Proj, Trans, Comb), selected recent contributions crosslingual dependency parsing (Rosa, Sgaard, Xia). design, highlightbest scores, results directly comparable, especially respect lackcontrol sources features facilitating parsing, PoS tags. also noteRosa evaluated HamleDT treebanks (Rosa, Masek, Marecek, Popel, Zeman,242fiSynthetic Treebanking Cross-Lingual Dependency Parsing& Zabokrtsky, 2014) UDT, still provide reference, implementsinteresting addition Delex sort intermediate step towards Multi.first observe Rosa Sgaard rarely surpass Delex baseline.come surprise, baseline uses advanced graph-based dependencyparser (Bohnet, 2010): contrast, Rosa uses arc-factored parser (McDonald, Pereira,Ribarov, & Hajic, 2005), Sgaard implements first-order version parserBohnet (2010) leverages cross-lingual word representations. said, discrepancyfirst- second-order graph-based parsers appears factorexplaining slight (if any) gains provided two approaches. Namely, Rosaapproach multi-source delexicalized parsing based maximum spanning tree-stylevoting, uses empirically obtained dataset similarity metrics weighting arcsvoting schemes. such, even yields slight improvements respective fairbaselinesas provided paper describing approach (Rosa & Zabokrtsky, 2015)itstill bound impoverished feature representation informing parser, inheritedDelex builds on, preventing method reaching higher accuracies. Sgaardattempts alleviate introducing cross-lingual word representations featurespace. report approach, Sgaard et al. (2015) observe slight improvementsbaselines, apparent word representations utilize work muchbetter NLP tasks dont involve syntactic representations, indicating mightappropriate facilitating cross-lingual parsing substantially.considered Rosa Sgaardcomparing two approaches Delexbaseline, establishing inferiority remaining approaches, including synthetictrebankingwe turn interesting part discussion, contributionscompared one another, Xia. also include competitive Multi baselineMcDonald et al. (2011) discussion.improved annotation projection Proj appears competitive method,none approaches surpass large margin. also consistently beatsMulti, albeit PoS annotations comparable. Syntax-based treebank translation(Trans) surpasses narrow margin four five targets, Germanexception, multi-source variant (Comb) adds approximately 3-5 LAS pointsdifference, English exception. approaches using predicted PoS tagscontrasted Xia, noting datasets, tagging approach (HunPos)performs slightly (Stanford) average. observe Xia exhibits slightadvantage top approach (CombP) across targets, also noteon topdifferences taggersthat approach also utilizes unlabeled data semi-supervisedparser augmentation. said, Xia (2014) document minor decreasesremoving unlabeled sources, implement arc-factored dependency parserpipeline. Thus, note i) synthetic treebanking approaches Xia currentlyrepresent competitive approaches cross-lingual dependency parsing,slight empirical edge latter, ii) research neededin formextensive replicative survey cross-lingual parsingto empirically gauge variousintricacies two approaches, influential contributions field,work McDonald et al. (2011) Xiao Guo (2014). also note recentcontribution Rasooli Collins (2015), also deals parallel corporaprojections, showing promising results.243fiTiedemann & Agicpoint, viewpoint enabling processing truly under-resourcedlanguages, interesting mark following observation. Table 14, apparentdisconnect scores methods exploit parallel data sources (Multi, Proj,Trans, Comb, Xia), methods dont (Delex, Rosa, Sgaard): methodsmake use parallel resources perform significantly better. clearindicator reaching top-level cross-lingual parsing performance, leastcurrent line-up standard dependency parsers, need lexical features providedparallel corpora. observation appears us clear guideline future workcross-lingual parsing, enablement NLP under-resourced languages.5. Conclusions Future Workpaper discussed various approaches cross-lingual dependency parsing,reviewing comparing number commonly used methods. Furthermore, includedextensive study annotation projection treebank translation, presentedcompetitive results cross-lingual dependency parsing task parsing datacross-lingually harmonized annotation included Universal Dependency Treebank.future work includes incorporation cross-lingual word embeddings modeltransfer another component system combinations discuss paper.also look wider range languages using growing set harmonized data setsUniversal Dependencies project. Especially interesting use techniques trulyunder-resourced languages. explore cross-lingual parsing means bootstrappingtools languages. also aim implementing large-scale replicative surveycross-lingual dependency parsing, show contribution empiricalassessment would timely beneficial fast-developing field.Acknowledgementsthank four anonymous reviewers detailed comments, significantlycontributed improving quality publication. also acknowledge Joakim Nivrediscussions synthetic treebanking, Hector Martnez Alonso suggestionsimproving readability paper.ReferencesAbeille, A. (2003). Treebanks: Building Using Parsed Corpora. Springer.Agic, Z., Merkler, D., & Berovic, D. (2012). Slovene-Croatian Treebank Transfer UsingBilingual Lexicon Improves Croatian Dependency Parsing. Proceedings IS-LTC,pp. 59.Agic, Z., Hovy, D., & Sgaard, A. (2015). Bit Bible: LearningPOS Taggers Truly Low-resource Languages. Proceedings ACL, pp. 268272.Agic, Z., Tiedemann, J., Merkler, D., Krek, S., Dobrovoljc, K., & Moze, S. (2014). Crosslingual Dependency Parsing Related Languages Rich Morphosyntactic Tagsets.Proceedings LT4CloseLang, pp. 1324.244fiSynthetic Treebanking Cross-Lingual Dependency ParsingBallesteros, M., & Nivre, J. (2012). MaltOptimizer: Optimization Tool MaltParser.Proceedings EACL, pp. 5862.Bender, E. M. (2011). Achieving Evaluating Language-independence NLP.Linguistic Issues Language Technology, 6 (3), 126.Bender, E. M. (2013). Linguistic Fundamentals Natural Language Processing: 100Essentials Morphology Syntax. Morgan & Claypool Publishers.Bohmova, A., Hajic, J., Hajicova, E., & Hladka, B. (2003). Prague DependencyTreebank. Treebanks, pp. 103127. Springer.Bohnet, B. (2010). Top Accuracy Fast Dependency Parsing Contradiction.Proceedings COLING, pp. 8997.Buchholz, S., & Marsi, E. (2006). CoNLL-X Shared Task Multilingual DependencyParsing. Proceedings CoNLL, pp. 149164.Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).Natural language processing (almost) scratch. Journal Machine LearningResearch, 12, 24932537.Das, D., & Petrov, S. (2011). Unsupervised Part-of-Speech Tagging Bilingual GraphBased Projections. Proceedings ACL, pp. 600609.De Marneffe, M.-C., MacCartney, B., & Manning, C. D. (2006). Generating Typed Dependency Parses Phrase Structure Parses. Proceedings LREC, pp. 449454.Durrett, G., Pauls, A., & Klein, D. (2012). Syntactic Transfer Using Bilingual Lexicon.Proceedings EMNLP-CoNLL, pp. 111.Elming, J., Johannsen, A., Klerke, S., Lapponi, E., Martinez Alonso, H., & Sgaard, A.(2013). Down-stream Effects Tree-to-dependency Conversions. ProceedingsNAACL, pp. 617626.Faruqui, M., & Dyer, C. (2014). Improving Vector Space Word Representations UsingMultilingual Correlation. Proceedings EACL, pp. 462471.Garrette, D., Mielens, J., & Baldridge, J. (2013). Real-World Semi-Supervised LearningPOS-Taggers Low-Resource Languages.. Proceedings ACL, pp. 583592.Gouws, S., & Sgaard, A. (2015). Simple Task-specific Bilingual Word Embeddings.Proceedings NAACL.Halacsy, P., Kornai, A., & Oravecz, C. (2007). HunPos Open-source Trigram Tagger.Proceedings ACL, pp. 209212.Heafield, K., Pouzyrevsky, I., Clark, J. H., & Koehn, P. (2013). Scalable Modified Kneser-NeyLanguage Model Estimation. Proceedings ACL, pp. 690696.Hwa, R., Resnik, P., Weinberg, A., Cabezas, C., & Kolak, O. (2005). Bootstrapping Parsersvia Syntactic Projection across Parallel Texts. Natural Language Engineering, 11 (3),311325.Klementiev, A., Titov, I., & Bhattarai, B. (2012). Inducing Crosslingual DistributedRepresentations Words. Proceedings COLING, pp. 14591474.245fiTiedemann & AgicKoehn, P. (2005). Europarl: Parallel Corpus Statistical Machine Translation.Proceedings MT Summit, pp. 7986.Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B.,Shen, W., Moran, C., Zens, R., Dyer, C. J., Bojar, O., Constantin, A., & Herbst, E.(2007). Moses: Open Source Toolkit Statistical Machine Translation. ProceedingsACL, pp. 177180.Koo, T., Carreras, X., & Collins, M. (2008). Simple Semi-supervised Dependency Parsing.Proceedings ACL, pp. 595603.Kubler, S., McDonald, R., & Nivre, J. (2009). Dependency Parsing. Morgan & ClaypoolPublishers.Li, S., Graca, J. V., & Taskar, B. (2012). Wiki-ly Supervised Part-of-speech Tagging.Proceedings EMNLP-CoNLL, pp. 13891398.Ma, X., & Xia, F. (2014). Unsupervised Dependency Parsing Transferring Distributionvia Parallel Guidance Entropy Regularization. Proceedings ACL), pp. 13371348.McDonald, R., Nivre, J., Quirmbach-Brundage, Y., Goldberg, Y., Das, D., Ganchev, K.,Hall, K., Petrov, S., Zhang, H., Tackstrom, O., Bedini, C., Bertomeu Castello, N.,& Lee, J. (2013). Universal Dependency Annotation Multilingual Parsing.Proceedings ACL, pp. 9297.McDonald, R., Pereira, F., Ribarov, K., & Hajic, J. (2005). Non-Projective DependencyParsing using Spanning Tree Algorithms. Proceedings EMNLP, pp. 523530.McDonald, R., Petrov, S., & Hall, K. (2011). Multi-Source Transfer DelexicalizedDependency Parsers. Proceedings EMNLP, pp. 6272.Mikolov, T., Le, Q. V., & Sutskever, I. (2013). Exploiting Similarities among LanguagesMachine Translation. http://arxiv.org/pdf/1309.4168.pdf.Naseem, T., Barzilay, R., & Globerson, A. (2012). Selective Sharing MultilingualDependency Parsing. Proceedings ACL, pp. 629637.Nivre, J. (2006). Inductive dependency parsing. Springer.Nivre, J., Bosco, C., Choi, J., de Marneffe, M.-C., Dozat, T., Farkas, R., Foster, J., & Ginter,F. e. a. (2015). Universal dependencies 1.0..Nivre, J., Hall, J., Kubler, S., McDonald, R., Nilsson, J., Riedel, S., & Yuret, D. (2007).CoNLL 2007 Shared Task Dependency Parsing. Proceedings CoNLLShared Task Session EMNLP-CoNLL 2007, pp. 915932.Och, F. J. (2003). Minimum Error Rate Training Statistical Machine Translation.Proceedings ACL, pp. 160167.Och, F. J., & Ney, H. (2003). Systematic Comparison Various Statistical AlignmentModels. Computational Linguistics, 29 (1), 1952.Petrov, S. (2014). Towards Universal Syntactic Processing Natural Language. Proceedings LT4CloseLang, p. 66.246fiSynthetic Treebanking Cross-Lingual Dependency ParsingPetrov, S., Das, D., & McDonald, R. (2012). Universal Part-of-Speech Tagset.Proceedings LREC, pp. 20892096.Plank, B., Martnez Alonso, H., Agic, v., Merkler, D., & Sgaard, A. (2015). DependencyParsing Metrics Correlate Human Judgments?. Proceedings CONLL, pp.315320.Rasooli, M. S., & Collins, M. (2015). Density-Driven Cross-Lingual Transfer DependencyParsers. Proceedings EMNLP.Rosa, R., Masek, J., Marecek, D., Popel, M., Zeman, D., & Zabokrtsky, Z. (2014). HamleDT2.0: Thirty Dependency Treebanks Stanfordized. Proceedings LREC, pp. 23342341.Rosa, R., & Zabokrtsky, Z. (2015). KLcpos3 - Language Similarity Measure DelexicalizedParser Transfer. Proceedings ACL, pp. 243249.Sagae, K., & Lavie, A. (2006). Parser Combination Reparsing. Proceedings NAACL,pp. 129132.Sgaard, A. (2011). Data Point Selection Cross-language Adaptation DependencyParsers. Proceedings ACL, pp. 682686.Sgaard, A. (2012). Unsupervised Dependency Parsing Without Training. Natural LanguageEngineering, 18 (02), 187203.Sgaard, A. (2013). Semi-Supervised Learning Domain Adaptation Natural LanguageProcessing. Morgan & Claypool Publishers.Sgaard, A., Agic, Z., Martnez Alonso, H., Plank, B., Bohnet, B., & Johannsen, A. (2015).Inverted Indexing Cross-lingual NLP. Proceedings ACL, pp. 17131722.Tackstrom, O., Das, D., Petrov, S., McDonald, R., & Nivre, J. (2013a). Token TypeConstraints Cross-lingual Part-of-speech Tagging. Transactions AssociationComputational Linguistics, 1, 112.Tackstrom, O., McDonald, R., & Nivre, J. (2013b). Target Language AdaptationDiscriminative Transfer Parsers. Proceedings NAACL, pp. 10611071.Tackstrom, O., McDonald, R., & Uszkoreit, J. (2012). Cross-lingual Word ClustersDirect Transfer Linguistic Structure. Proceedings NAACL, pp. 477487.Tiedemann, J. (2014). Rediscovering Annotation Projection Cross-Lingual ParserInduction. Proceedings COLING, pp. 18541864.Tiedemann, J., Agic, Z., & Nivre, J. (2014). Treebank Translation Cross-Lingual ParserInduction. Proceedings CoNLL, pp. 130140.Tiedemann, J., & Nakov, P. (2013). Analyzing use character-level translationsparse noisy datasets. Proceedings RANLP, pp. 676684.Tiedemann, J. (2012). Parallel Data, Tools Interfaces OPUS. ProceedingsLREC, pp. 22142218.Tiedemann, J. (2015). Improving Cross-Lingual Projection Syntactic Dependencies.Proceedings NoDaLiDa.247fiTiedemann & AgicUszkoreit, H., & Rehm, G. (2012). Language White Paper Series. Springer.Xiao, M., & Guo, Y. (2014). Distributed Word Representation Learning Cross-LingualDependency Parsing. Proceedings CoNLL, pp. 119129.Yarowsky, D., Ngai, G., & Wicentowski, R. (2001). Inducing Multilingual Text AnalysisTools via Robust Projection Across Aligned Corpora. Proceedings HLT, pp. 18.Zeman, D., Dusek, O., Marecek, D., Popel, M., Ramasamy, L., Stepanek, J., Zabokrtsky,Z., & Hajic, J. (2014). HamleDT: Harmonized Multi-language Dependency Treebank.Language Resources Evaluation, 48 (4), 601637.Zeman, D., & Resnik, P. (2008). Cross-Language Parser Adaptation RelatedLanguages. Proceedings IJCNLP, pp. 3542.Zhang, Y., Reichart, R., Barzilay, R., & Globerson, A. (2012). Learning MapUniversal POS Tagset. Proceedings EMNLP-CoNLL, pp. 13681378.Zhao, H., Song, Y., Kit, C., & Zhou, G. (2009). Cross Language Dependency Parsing UsingBilingual Lexicon. Proceedings ACL-IJCNLP, pp. 5563.Zou, W. Y., Socher, R., Cer, D., & Manning, C. D. (2013). Bilingual Word EmbeddingsPhrase-Based Machine Translation. Proceedings EMNLP, pp. 13931398.248fiJournal Artificial Intelligence Research 55 (2016) 409-442Submitted 07/15; published 02/16Automatic Description Generation Images: SurveyModels, Datasets, Evaluation MeasuresRaffaella Bernardibernardi@disi.unitn.itUniversity Trento, ItalyRuket Cakiciruken@ceng.metu.edu.trMiddle East Technical University, TurkeyDesmond Elliottd.elliott@uva.nlUniversity Amsterdam, NetherlandsAykut ErdemErkut ErdemNazli Ikizler-Cinbisaykut@cs.hacettepe.edu.trerkut@cs.hacettepe.edu.trnazli@cs.hacettepe.edu.trHacettepe University, TurkeyFrank Kellerkeller@inf.ed.ac.ukUniversity Edinburgh, UKAdrian Muscatadrian.muscat@um.edu.mtUniversity Malta, MaltaBarbara Plankbplank@cst.dkUniversity Copenhagen, DenmarkAbstractAutomatic description generation natural images challenging problemrecently received large amount interest computer vision natural language processing communities. survey, classify existing approaches basedconceptualize problem, viz., models cast description either generation problem retrieval problem visual multimodal representationalspace. provide detailed review existing models, highlighting advantagesdisadvantages. Moreover, give overview benchmark image datasetsevaluation measures developed assess quality machine-generatedimage descriptions. Finally extrapolate future directions area automatic imagedescription generation.1. Introductionpast two decades, fields natural language processing (NLP) computervision (CV) seen great advances respective goals analyzing generatingtext, understanding images videos. fields share similar set methods rooted artificial intelligence machine learning, historically developedseparately, scientific communities typically interacted little.Recent years, however, seen upsurge interest problems requirecombination linguistic visual information. lot everyday tasks nature,e.g., interpreting photo context newspaper article, following instructionsconjunction diagram map, understanding slides listening lecture.c2016AI Access Foundation. rights reserved.fiBernardi et al.addition this, web provides vast amount data combines linguistic visualinformation: tagged photographs, illustrations newspaper articles, videos subtitles,multimodal feeds social media. tackle combined language vision tasksexploit large amounts multimodal data, CV NLP communities movedcloser together, example organizing workshops language visionheld regularly CV NLP conferences past years.new language-vision community, automatic image description emergedkey task. task involves taking image, analyzing visual content, generatingtextual description (typically sentence) verbalizes salient aspectsimage. challenging CV point view, description could principletalk visual aspect image: mention objects attributes,talk features scene (e.g., indoor/outdoor), verbalize peopleobjects scene interact. challenging still, description could even referobjects depicted (e.g., talk people waiting train, eventrain visible arrived yet) provide background knowledgecannot derived directly image (e.g., person depicted Mona Lisa).short, good image description requires full image understanding, thereforedescription task excellent test bed computer vision systems, one muchcomprehensive standard CV evaluations typically test, instance, accuracyobject detectors scene classifiers limited set classes.Image understanding necessary, sufficient producing good description.Imagine apply array state-of-the-art detectors image localize objects(e.g., Felzenszwalb, Girshick, McAllester, & Ramanan, 2010; Girshick, Donahue, Darrell,& Malik, 2014), determine attributes (e.g., Lampert, Nickisch, & Harmeling, 2009; Berg,Berg, & Shih, 2010; Parikh & Grauman, 2011), compute scene properties (e.g., Oliva &Torralba, 2001; Lazebnik, Schmid, & Ponce, 2006), recognize human-object interactions (e.g., Prest, Schmid, & Ferrari, 2012; Yao & Fei-Fei, 2010). result wouldlong, unstructured list labels (detector outputs), would unusable imagedescription. good image description, contrast, comprehensive concise(talk important things image), formally correct,i.e., consists grammatically well-formed sentences.NLP point view, generating description natural language generation (NLG) problem. task NLG turn non-linguistic representationhuman-readable text. Classically, non-linguistic representation logical form,database query, set numbers. image description, input image representation (e.g., detector outputs listed previous paragraph), NLGmodel turn sentences. Generating text involves series steps, traditionallyreferred NLP pipeline (Reiter & Dale, 2006): need decide aspectsinput talk (content selection), need organize content (textplanning) verbalize (surface realization). Surface realization turn requires choosing right words (lexicalization), using pronouns appropriate (referential expressiongeneration), grouping related information together (aggregation).words, automatic image description requires full image understanding,also sophisticated natural language generation. makes interesting410fiAutomatic Description Generation Images: Surveytask embraced CV NLP communities.1 Notedescription task become even challenging take accountgood descriptions often user-specific. instance, art critic require differentdescription librarian journalist, even photograph. brieflytouch upon issue talk difference descriptions captionsSection 3 discuss future directions Section 4.Given automatic image description interesting task, drivenexistence mature CV NLP methods availability relevant datasets, largeimage description literature appeared last five years. aim surveyarticle give comprehensive overview literature, covering models, datasets,evaluation metrics.sort existing literature three categories based image descriptionmodels used. first group models follows classical pipeline outlined above:first detect predict image content terms objects, attributes, scene types,actions, based set visual features. Then, models use content informationdrive natural language generation system outputs image description.term approaches direct generation models.second group models cast problem retrieval problem. is, createdescription novel image, models search images database similarnovel image. build description novel image based descriptionsset similar images retrieved. novel image described simplyreusing description similar retrieved image (transfer), synthesizingnovel description based description set similar images. Retrieval-based modelssubdivided based type approach use represent imagescompute similarity. first subgroup models uses visual space retrieve images,second subgroup uses multimodal space represents images text jointly.overview models reviewed survey, categoryfall into, see Table 1.Generating natural language descriptions videos presents unique challengesimage-based description, additionally requires analyzing objectsattributes actions temporal dimension. Models aim solve description generation videos proposed literature (e.g., Khan, Zhang, &Gotoh, 2011; Guadarrama, Krishnamoorthy, Malkarnenkar, Venugopalan, Mooney, Darrell,& Saenko, 2013; Krishnamoorthy, Malkarnenkar, Mooney, Saenko, & Guadarrama, 2013;Rohrbach, Qiu, Titov, Thater, Pinkal, & Schiele, 2013; Thomason, Venugopalan, Guadarrama, Saenko, & Mooney, 2014; Rohrbach, Rohrback, Tandon, & Schiele, 2015; Yao, Torabi,Cho, Ballas, Pal, Larochelle, & Courville, 2015; Zhu, Kiros, Zemel, Salakhutdinov, Urtasun,Torralba, & Fidler, 2015). However, existing work description generation usedstatic images, focus survey.2survey article, first group automatic image description models threecategories outlined provide comprehensive overview models1. Though image description approaches circumvent NLG aspect transferring human-authoreddescriptions, see Sections 2.2 2.3.2. interesting intermediate approach involves annotation image streams sequences sentences, see work Park Kim (2015).411fiBernardi et al.category Section 2. examine available multimodal image datasets usedtraining testing description generation models Section 3. Furthermore, reviewevaluation measures used gauge quality generated descriptionsSection 3. Finally, Section 4, discuss future research directions, including possiblenew tasks related image description, visual question answering.2. Image Description ModelsGenerating automatic descriptions images requires understanding humansdescribe images. image description analyzed several different dimensions (Shatford, 1986; Jaimes & Chang, 2000). follow Hodosh, Young, Hockenmaier (2013)assume descriptions interest survey article onesverbalize visual conceptual information depicted image, i.e., descriptionsrefer depicted entities, attributes relations, actionsinvolved in. Outside scope automatic image description non-visual descriptions,give background information refer objects depicted image (e.g.,location image taken took picture). Also, relevantstandard approaches image description perceptual descriptions, captureglobal low-level visual characteristics images (e.g., dominant color imagetype media photograph, drawing, animation, etc.).following subsections, give comprehensive overview state-of-the-art approaches description generation. Table 1 offers high-level summary field, usingthree categories models outlined introduction: direct generation models, retrieval models visual space, retrieval model multimodal space.2.1 Description Generation Visual Inputgeneral approach studies group first predict likely meaninggiven image analyzing visual content, generate sentence reflectingmeaning. models category achieve using following general pipelinearchitecture:1. Computer vision techniques applied classify scene type, detect objects present image, predict attributes relationships holdthem, recognize actions taking place.2. followed generation phase turns detector outputs wordsphrases. combined produce natural language descriptionimage, using techniques natural language generation (e.g., templates, n-grams,grammar rules).approaches reviewed section perform explicit mapping imagesdescriptions, differentiates studies described Section 2.2 2.3,incorporate implicit vision language models. illustration sample modelshown Figure 1. explicit pipeline architecture, tailored problem hand,constrains generated descriptions, relies predefined sets semantic classesscenes, objects, attributes, actions. Moreover, architecture crucially assumes412fiAutomatic Description Generation Images: SurveyReferenceGenerationFarhadi et al. (2010)Kulkarni et al. (2011)Li et al. (2011)Ordonez et al. (2011)Yang et al. (2011)Gupta et al. (2012)Kuznetsova et al. (2012)Mitchell et al. (2012)Elliott Keller (2013)Hodosh et al. (2013)Gong et al. (2014)Karpathy et al. (2014)Kuznetsova et al. (2014)Mason Charniak (2014)Patterson et al. (2014)Socher et al. (2014)Verma Jawahar (2014)Yatskar et al. (2014)Chen Zitnick (2015)Donahue et al. (2015)Devlin et al. (2015)Elliott de Vries (2015)Fang et al. (2015)Jia et al. (2015)Karpathy Fei-Fei (2015)Kiros et al. (2015)Lebret et al. (2015)Lin et al. (2015)Mao et al. (2015a)Ortiz et al. (2015)Pinheiro et al. (2015)Ushiku et al. (2015)Vinyals et al. (2015)Xu et al. (2015)Yagcioglu et al. (2015)RetrievalVisual Space Multimodal SpaceXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXTable 1: overview existing approaches automatic image description.categorized literature approaches directly generate description image(Section 2.1), approaches retrieve images via visual similarity transfer description new image (Section 2.2), approaches frame task retrievingdescriptions images multimodal space (Section 2.3).413fiBernardi et al.Figure 1: automatic image description generation system proposed Kulkarni et al.(2011).accuracy detectors semantic class, assumption always metpractice.Approaches description generation differ along two main dimensions: (a) imagerepresentations derive descriptions from, (b) address sentence generation problem. terms representations used, existing models conceptualizedimages number different ways, relying spatial relationships (Farhadi et al., 2010),corpus-based relationships (Yang et al., 2011), spatial visual attributes (Kulkarniet al., 2011). Another group papers utilizes abstract image representationform meaning tuples capture different aspects image: objects detected,attributes detections, spatial relations them, scene type(Farhadi et al., 2010; Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Mitchell et al.,2012). recently, Yatskar et al. (2014) proposed generate descriptions denselylabeled images, incorporate object, attribute, action, scene annotations. Similarspirit work Fang et al. (2015), rely prior labeling objects,attributes, etc. Rather, authors train word detectors directly imagesassociated descriptions using multi-instance learning (a weakly supervised approachtraining object detectors). words returned detectors fedlanguage model sentence generation, followed re-ranking step.first framework explicitly represent structure image relatesstructure description Visual Dependency Representations (VDR) methodproposed Elliott Keller (2013). VDR captures spatial relationsobjects image form dependency graph. graph relatedsyntactic dependency tree description image.3 initial work using VDRsrelied corpus manually annotated VDRs training, recent approachesinduce VDRs automatically based output object detector (Elliott & de Vries,2015) labels present abstract scenes (Ortiz et al., 2015).4 idea explicitlyrepresenting image structure using description generation picked3. VDRs proven useful description generation, also image retrieval (Elliott,Lavrenko, & Keller, 2014).4. Abstract scenes schematic images, typically constructed using clip-art. employed avoidneed object detector, labels positions objects know. example ZitnickParikhs (2013) dataset, see Section 3 details.414fiAutomatic Description Generation Images: SurveyLin et al. (2015), parse images scene graphs, similar VDRsrepresent relations objects scene. generate scenegraphs using semantic grammar.5Existing approaches also vary along second dimension, viz., approachsentence generation problem. one end scale, approaches usen-gram-based language models. Examples include works Kulkarni et al. (2011)Li et al. (2011), generate descriptions using n-gram language models trainedsubset Wikipedia. approaches first determine attributes relationshipsregions image regionprepositionregion triples. n-gram languagemodel used compose image description fluent, given language model.approach Fang et al. (2015) similar, uses maximum entropy language modelinstead n-gram model generate descriptions. gives authors flexibilityhandling output word detectors core model.Recent image description work using recurrent neural networks (RNNs) alsoregarded relying language modeling. classical RNN language model: capturesprobability generating given word string, given words generated far.image description setup, RNN trained generate next word givenstring far, also set image features. setting, RNN thereforepurely language model (as case n-gram model, instance), hybridmodel relies representation incorporates visual linguistic features.return detail Section 2.3.second set approaches use sentence templates generate descriptions.(typically manually) pre-defined sentence frames open slots need filledlabels objects, relations, attributes. instance, Yang et al. (2011) fillsentence template selecting likely objects, verbs, prepositions, scene typesbased Hidden Markov Model. Verbs generated finding likely pairingobject labels Gigaword external corpus. generation model ElliottKeller (2013) parses image VDR, traverses VDRs fill slotssentence templates. approach also performs limited content selectionlearning associations VDRs syntactic dependency trees training time;associations allow select appropriate verb description test time.approaches used linguistically sophisticated approaches generation.Mitchell et al. (2012) over-generate syntactically well-formed sentence fragmentsrecombine using tree-substitution grammar. related approach pursuedKuznetsova et al. (2014), tree-fragments learnt training set existingdescriptions fragments combined test time form new descriptions.Another linguistically expressive model recently proposed Ortiz et al. (2015).authors model image description machine translation VDRsentence pairsperform explicit content selection surface realization using integer linear programlinguistic constraints.systems presented far aimed directly generating novel descriptions. However,argued Hodosh et al. (2013), framing image description natural language generation (NLG) task makes difficult objectively evaluate quality novel descriptions5. Note graphs also used image retrieval Johnson, Krishna, Stark, Li, Shamma, Bernstein,Fei-Fei (2015) Schuster, Krishna, Chang, Fei-Fei, Manning (2015).415fiBernardi et al.Figure 2: description model based retrieval visual space proposed Ordonezet al. (2011).introduces number linguistic difficulties detract attention underlying image understanding problem (Hodosh et al., 2013). time, evaluationgeneration systems known difficult (Reiter & Belz, 2009). Hodosh et al. therefore propose approach makes possible evaluate mapping imagessentences independently generation aspect. Models follow approachconceptualize image description retrieval problem: associate imagedescription retrieving ranking set similar images candidate descriptions.candidate descriptions either used directly (description transfer)novel description synthesized candidates (description generation).retrieval images ranking descriptions carried two ways:either visual space multimodal space combines textual visualinformation space. following subsections, survey work follows twoapproaches.2.2 Description Retrieval Visual Spacestudies group pose problem automatically generating descriptionimage retrieving images similar query image (i.e., new imagedescribed); illustrated Figure 2. words, systems exploit similarityvisual space transfer descriptions query images. Compared modelsgenerate descriptions directly (Section 2.1), retrieval models typically require large amounttraining data order provide relevant descriptions.terms algorithmic components, visual retrieval approaches typically followpipeline three main steps:1. Represent given query image specific visual features.2. Retrieve candidate set images training set based similarity measurefeature space used.3. Re-rank descriptions candidate images making use visualand/or textual information contained retrieval set, alternatively combinefragments candidate descriptions according certain rules schemes.One first model follow approach Im2Text model Ordonez et al.(2011). GIST (Oliva & Torralba, 2001) Tiny Image (Torralba, Fergus, & Freeman, 2008)416fiAutomatic Description Generation Images: Surveydescriptors employed represent query image determine visually similarimages first retrieval step. retrieval-based models consider resultstep baseline. re-ranking step, range detectors (e.g., object, stuff,pedestrian, action detectors) scene classifiers specific entities mentionedcandidate descriptions first applied images better capture visual content,images represented means detector classifier responses. Finally,re-ranking carried via classifier trained semantic features.model proposed Kuznetsova et al. (2012) first runs detectors classifiers used re-ranking step Im2Text model query image extractrepresent semantic content. Then, instead performing single retrieval combiningresponses detectors classifiers Im2Text model does, carriesseparate image retrieval step visual entity present query image collect related phrases retrieved descriptions. instance, dog detected givenimage, retrieval process returns phrases referring visually similar dogstraining set. specifically, step used collect three different kinds phrases.Noun verb phrases extracted descriptions training set basedvisual similarity object regions detected training images queryimage. Similarly, prepositional phrases collected stuff detection queryimage measuring visual similarity detections query trainingimages based appearance geometric arrangements. Prepositional phrasesadditionally collected scene context detection measuring global scene similarity computed query training images. Finally, description generatedcollected phrases detected object via integer linear programming (ILP)considers factors word ordering, redundancy, etc.method Gupta et al. (2012) another phrase-based approach. retrievevisually similar images, authors employ simple RGB HSV color histograms,Gabor Haar descriptors, GIST SIFT (Lowe, 2004) descriptors image features. Then, instead using visual object detectors scene classifiers, relytextual information descriptions visually similar images extractvisual content input image. Specifically, candidate descriptions segmented phrases certain type (subject, verb), (subject, prep, object),(verb, prep, object), (attribute, object), etc. best describe input image determined according joint probability model based image similarity Google search counts, image represented triplets form{((attribute1, object1), verb), (verb, prep, (attribute2, object2)), (object1, prep, object2)}.end, description generated using three top-scoring triplets based fixedtemplate. increase quality descriptions, authors also apply syntacticaggregation subject predicate grouping rules generation step.Patterson et al. (2014) first present large-scale scene attribute datasetcomputer vision community. dataset includes 14,340 images 707 scenecategories, annotated certain attributes list 102 discriminativeattributes related materials, surface properties, lighting, affordances, spatial layout.allows train attribute classifiers dataset. paper, authorsalso demonstrate responses attribute classifiers used globalimage descriptor captures semantic content better standard global image417fiBernardi et al.descriptors GIST. application, extended baseline model Im2Textreplacing global features automatically extracted scene attributes, giving betterimage retrieval description results.Mason Charniaks (2014) description generation approach differs modelsdiscussed formulates description generation extractive summarizationproblem, selects output description considering textual informationfinal re-ranking step. particular, authors represented images using sceneattributes descriptor Patterson et al. (2014). visually similar images identified training set, next step, conditional probabilities observing worddescription query image estimated via non-parametric density estimationusing descriptions retrieved images. final output description determined using two different extractive summarization techniques, one dependingSumBasic model (Nenkova & Vanderwende, 2005) based Kullback-Leiblerdivergence word distributions query candidate descriptions.Yagcioglu et al. (2015) proposed average query expansion approach basedcompositional distributed semantics. represent images, use features extractedrecently proposed Visual Geometry Group convolutional neural network (VGG-CNN;Chatfield, Simonyan, Vedaldi, & Zisserman, 2014). features activationslast layer deep neural network trained ImageNet, proveneffective many computer vision problems. Then, original query expandedaverage distributed representations retrieved descriptions, weightedsimilarity input image.approach Devlin et al. (2015) also utilizes CNN activations global imagedescriptor performs k-nearest neighbor retrieval determine imagestraining set visually similar query image. selects descriptioncandidate descriptions associated retrieved images best describesimages similar query image, like approaches MasonCharniak (2014) Yagcioglu et al. (2015). approach differs termsrepresent similarity description select best candidatewhole set. Specifically, propose compute description similarity basedn-gram overlap F-score descriptions. suggest choose outputdescription finding description corresponds description highestmean n-gram overlap candidate descriptions (k-nearest neighbor centroiddescription) estimated via n-gram similarity measure.2.3 Description Retrieval Multimodal Spacethird group studies casts image description generation retrieval problem,multimodal space (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014).intuition behind models illustrated Figure 3, overall approachcharacterized follows:1. Learn common multimodal space visual textual data using trainingset imagedescription pairs.418fiAutomatic Description Generation Images: Survey2. Given query, use joint representation space perform cross-modal (imagesentence) retrieval.Figure 3: Image descriptions retrieval task proposed works Hodosh et al.(2013), Socher et al. (2014), Karpathy et al. (2014)6 .contrast retrieval models work visual space (Section 2.2),unimodal image retrieval followed ranking retrieved descriptions, imagesentence features projected common multimodal space. Then, multimodalspace used retrieve descriptions given image. advantage approachallows bi-directional models, i.e., common space also useddirection, retrieving appropriate image query sentence.section, first discuss seminal paper Hodosh et al. (2013) descriptionretrieval, present recent approaches combine retrieval approachform natural language generation. Hodosh et al. map images sentencescommon space. joint space used image search (findplausible image given sentence) image annotation (find sentence describesimage well), see Figure 3. earlier study authors proposed learn common meaning space (Farhadi et al., 2010) consisting triple representation formhobject, action, scenei. representation thus limited set pre-defined discreteslot fillers, given training information. Instead, Hodosh et al. use KCCA,kernelized version CCA, Canonical Correlation Analysis (Hotelling, 1936), learnjoint space. CCA takes training dataset image-sentence pairs, i.e., Dtrain = {hi, si},thus input two different feature spaces, finds linear projections newly induced common space. KCCA, kernel functions map original items higher-orderspace order capture patterns needed associate image text. KCCAshown previously successful associating images (Hardoon, Szedmak, & ShaweTaylor, 2004) image regions (Socher & Fei-Fei, 2010) individual words settags.Hodosh et al. (2013) compare KCCA approach nearest-neighbor (NN) baselineuses unimodal text image spaces, without constructing joint space. drawbackKCCA applicable smaller datasets, requires two kernel matrices6. Source http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/419fiBernardi et al.kept memory training. becomes prohibitive large datasets.attempts made circumvent computational burden KCCA, e.g.,resorting linear models (Hodosh & Hockenmaier, 2013). Alternatively, Sun, Gan,Nevatia (2015) used automatically discovered concepts images form semanticspace, performed sentence retrieval accordingly. However, recent work descriptionretrieval instead utilized neural networks construct joint space image descriptiongeneration.Socher et al. (2014) use neural networks building sentence image vector representations mapped common embedding space. noveltywork use compositional sentence vector representations. First, image wordrepresentations learned single modalities, finally mapped commonmultimodal space. particular, use DT-RNN (Dependency Tree Recursive NeuralNetwork) composing language vectors abstract word order syntactic difference semantically irrelevant. results 50-dimensional word embeddings.image space, authors use nine layer neural network trained ImageNet data,using unsupervised pre-training. Image embeddings derived taking outputlast layer (4,096 dimensions). two spaces projected multi-modal spacemax-margin objective function intuitively trains pairs correct imagesentence vectors high inner product. authors show model outperforms previously used KCCA approaches work Hodosh Hockenmaier(2013).Karpathy et al. (2014) extend previous multi-modal embeddings model. Ratherdirectly mapping entire images sentences common embedding space,model embeds fine-grained units, i.e., fragments images (objects) sentences(dependency tree fragments), common space. final model integrates global(sentence image-level) well finer-grained information outperforms previousapproaches, DT-RNN (Socher et al., 2014). similar approach pursuedPinheiro et al. (2015), propose bilinear phrase-based model learns mappingimage representations sentences. constrained language model usedgenerate representation. conceptually related approach pursued Ushikuet al. (2015): authors use common subspace model maps feature vectorsassociated phrase nearby regions space. generation, beamsearch based decoder templates used.Description generation systems difficult evaluate, therefore studies reviewedtreat problem retrieval ranking task (Hodosh et al., 2013; Socher et al.,2014). approach valuable enables comparative evaluation,retrieval ranking limited availability existing datasets descriptions.alleviate problem, recent models developed extensions multimodalspaces; able rank sentences, also generate (Chen & Zitnick,2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015; Kiros et al., 2015; Lebret et al., 2015;Mao et al., 2015a; Vinyals et al., 2015; Xu et al., 2015).Kiros et al. (2015) introduced general encoder-decoder framework image descriptionranking generation, illustrated Figure 4. Intuitively method works follows.encoder first constructs joint multimodal space. space used rankimages descriptions. second stage (decoder) uses shared multimodal420fiAutomatic Description Generation Images: SurveyFigure 4: encoder-decoder model proposed Kiros et al. (2015).representation generate novel descriptions. model, directly inspired recentwork machine translation, encodes sentences using LongShort Term Memory (LSTM)recurrent neural network, image features using deep convolutional network (CNN).LSTM extension recurrent neural network (RNN) incorporates builtin memory store information exploit long range context. Kiros et al.s (2015)encoder-decoder model, vision space projected embedding space LSTMhidden states; pairwise ranking loss minimized learn ranking imagesdescriptions. decoder, neural-network-based language model, able generate noveldescriptions multimodal space.Another work carried time similar latterdescribed paper Donahue et al. (2015). authors propose model alsobased LSTM neural architecture. However, rather projecting vision spaceembedding space hidden states, model takes copy static imageprevious word directly input, fed stack four LSTMs. AnotherLSTM-based model proposed Jia et al. (2015), added semantic image informationadditional input LSTM. model Kiros et al. (2015) outperforms priorDT-RNN model (Socher et al., 2014); turn, Donahue et al. report outperformwork Kiros et al. (2015) task image description retrieval. Subsequent workincludes RNN-based architectures Mao et al. (2015a) Vinyals et al. (2015),similar one proposed Kiros et al. (2015) achieve comparableresults standard datasets. Mao, Wei, Yang, Wang, Huang, Yuille (2015b) proposeinteresting extension Mao et al.s (2015a) model learning novel visual concepts.Karpathy Fei-Fei (2015) improve previous models proposing deep visualsemantic alignment model simpler architecture objective function. keyinsight assume parts sentence refer particular unknown regionsimage. model tries infer alignments segments sentences regionsimages based convolutional neural networks image regions, bidirectionalRNN sentences structured objective aligns two modalities. Wordsimage regions mapped common multimodal embedding. multimodalrecurrent neural network architecture uses inferred alignments learn generate421fiBernardi et al.novel descriptions. Here, image used condition first state recurrentneural network, generates image descriptions.Another model generate novel sentences proposed (Chen & Zitnick, 2015).contrast previous work, model dynamically builds visual representationscene description generated. is, word read generatedvisual representation updated reflect new information. accomplishsimple RNN. model achieves comparable better results prior studies,except recently proposed deep visual-semantic alignment model (Karpathy & Fei-Fei,2015). model Xu et al. (2015) closely related also uses RNN-basedarchitecture visual representations dynamically updated. Xu et al.s (2015)model incorporates attentional component, gives way determiningregions image salient, focus description regions.resulting improvement description accuracy, also makes possible analyzemodel behavior visualizing regions attended wordgenerated model.general RNN-based ranking generation approach also followed Lebretet al. (2015). Here, main innovation linguistic side: employ bilinearmodel learn common space image features syntactic phrases (noun phrases, verbphrases, prepositional phrases). Markov model utilized generate sentencesphrase embedding. visual side, standard CNN-based features used.results elegant modeling framework, whose performance broadly comparablestate art.Finally, two important directions less explored are: portability weakly supervised learning. Verma Jawahar (2014) evaluate portability bi-directionalmodel based topic models, showing performance significantly degrades. highlight importance cross-dataset image description retrieval evaluation. Another interesting observation models require training set fully-annotatedimage-sentence pairs. However, obtaining data large quantities prohibitively expensive. Gong et al. (2014) propose approach based weak supervision transfersknowledge millions weakly annotated images improve accuracy descriptionretrieval.2.4 Comparison Existing Approachesdiscussion previous subsections makes clear approach imagedescription particular strengths weaknesses. example, methodscast task generation problem (Section 2.1) advantage typesapproaches produce novel sentences describe given image. However,success relies heavily accurately estimate visual contentwell able verbalize content. particular, explicitly employ computervision techniques predict likely meaning given image; methodslimited accuracy practice, hence fail identify important objectsattributes, valid description generated. Another difficulty liesfinal description generation step; sophisticated natural language generation crucial422fiAutomatic Description Generation Images: Surveyguarantee fluency grammatical correctness generated sentences. comeprice considerable algorithmic complexity.contrast, image description methods cast problem retrievalvisual space problem transfer retrieved descriptions novel image (Section 2.2)always produce grammatically correct descriptions. guaranteed design,systems fetch human-generated sentences visually similar images. main issueapproach requires large amounts images human-written descriptions.is, accuracy (but grammaticality) descriptions reduces sizetraining set decreases. training set also needs diverse (in additionlarge), order visual retrieval-based approaches produce image descriptionsadequate novel test images (Devlin et al., 2015). Though problem mitigatedre-synthesizing novel description retrieved ones (see Section 2.2).Approaches cast image description retrieval multimodal space problem(Section 2.3) also advantage generating human-like descriptionsable retrieve appropriate ones pre-defined large pool descriptions.However, ranking descriptions requires cross-modal similarity metric comparesimages sentences. metrics difficult define, compared unimodalimage-to-image similarity metrics used retrieval models work visual space.Additionally, training common space images sentences requires large trainingset images annotated human-generated descriptions. plus side,multimodal embedding space also used reverse problem, i.e., retrievingappropriate image query sentence. something generation-basedvisual retrieval-based approaches capable of.3. Datasets Evaluationwide range datasets automatic image description research. imagesdatasets associated textual descriptions differ certainaspects size, format descriptions descriptions collected. review common approaches collecting datasets, datasets themselves,evaluation measures comparing generated descriptions ground-truth texts.datasets summarized Table 2, examples images descriptions givenFigure 5. readers also refer dataset survey Ferraro, Mostafazadeh, Huang,Vanderwende, Devlin, Galley, Mitchell (2015) analysis similar ours. providesbasic comparison existing language vision datasets. limitedautomatic image description, reports simple statistics quality metricsperplexity, syntactic complexity, abstract concrete word ratios.3.1 Image-Description DatasetsPascal1K sentence dataset (Rashtchian et al., 2010) dataset commonlyused benchmark evaluating quality description generation systems.medium-scale dataset, consists 1,000 images selected Pascal 2008object recognition dataset (Everingham, Van Gool, Williams, Winn, & Zisserman, 2010)includes objects different visual classes, humans, animals, vehicles.423fiBernardi et al.ImagesTextsJudgmentsObjectsPascal1K (Rashtchian et al., 2010)VLT2K (Elliott & Keller, 2013)Flickr8K (Hodosh & Hockenmaier, 2013)Flickr30K (Young et al., 2014)Abstract Scenes (Zitnick & Parikh, 2013)IAPR-TC12 (Grubinger et al., 2006)MS COCO (Lin et al., 2014)1,0002,4248,10831,78310,00020,000164,06253556155PartialYesCollectedPartialPartialCompleteSegmentedPartialBBC News (Feng & Lapata, 2008)SBU1M Captions (Ordonez et al., 2011)Deja-Image Captions (Chen et al., 2015)3,3611,000,0004,000,00011VariesCollected7Table 2: Image datasets automatic description generation models. splitoverview image description datasets (top) caption datasets (bottom) seemain text explanation distinction.image associated five descriptions generated humans Amazon MechanicalTurk (AMT) service.Visual Linguistic Treebank (VLT2K; Elliott & Keller, 2013) makes use imagesPascal 2010 action recognition dataset. augments images three, twosentence descriptions per image. descriptions collected AMT specificinstructions verbalize main action depicted image actors involved (firstsentence), also mentioning important background objects (second sentence).subset 341 images Visual Linguistic Treebank, object annotationavailable (in form polygons around objects mentioned descriptions).subset, manually created Visual Dependency Representations (see Section 2.1) alsoincluded (three VDRs per images, i.e., total 1023).Flickr8K dataset (Hodosh et al., 2013) extended version Flickr30Kdataset (Young et al., 2014) contain images Flickr, comprising approximately 8,00030,000 images, respectively. images two datasets selecteduser queries specific objects actions. datasets contain five descriptions per image collected AMT workers using strategy similar Pascal1Kdataset.Abstract Scenes dataset (Zitnick & Parikh, 2013; Zitnick, Parikh, & Vanderwende,2013) consists 10,000 clip-art images descriptions. images createdAMT, workers asked place fixed vocabulary 80 clip-art objectsscene choosing. descriptions sourced worker-createdscenes. authors provided descriptions two different forms. firstgroup contains single sentence description image, second group includes twoalternative descriptions per image. two descriptions consist three simplesentences sentence describing different aspect scene. main advantagedataset affords opportunity explore image description generation without7. Kuznetsova et al. (2014) ran human judgments study 1,000 images dataset.424fiAutomatic Description Generation Images: Survey1. One jet lands airport another takesnext it.2. Two airplanes parked airport.3. Two jets taxi past other.4. Two parked jet airplanes facing opposite directions.5. two passenger planes grassy plain1. several people chairs small childwatching one play trumpet2. man playing trumpet front little boy.3. People sitting sofa man playinginstrument entertainment.(a) Pascal1K8(b) VLT2K91. man snowboarding structure snowyhill.2. snowboarder jumps air snowyhill.3. snowboarder wearing green pants trickhigh bench4. Someone yellow pants rampsnow.5. man performing trick snowboard highair.1. yellow building white columns background2. two palm trees front house3. cars parking front house4. woman child walking square(c) Flickr8K10(d) IAPR-TC12111. cat anxiously sits park staresunattended hot dog someone leftyellow bench1. blue smart car parked parking lot.2. vehicles wet wide city street.3. Several cars motorcycle snow covered street.4. Many vehicles drive icy street.5. small smart car driving city.(e) Abstract Scenes12(f) MS COCO13Figure 5: Example images descriptions benchmark image datasets.425fiBernardi et al.need automatic object recognition, thus avoiding associated noise.recent version dataset created part visual question-answering(VQA) dataset (Antol, Agrawal, Lu, Mitchell, Batra, Zitnick, & Parikh, 2015). contains50,000 different scene images realistic human models five single-sentencedescriptions.IAPR-TC12 dataset introduced Grubinger et al. (2006) one earliestmulti-modal datasets contains 20,000 images descriptions. images originally retrieved via search engines Google, Bing Yahoo, descriptionsproduced multiple languages (predominantly English German). imageassociated one five descriptions, description refers different aspectimage, applicable. dataset also contains complete pixel-level segmentationobjects.MS COCO dataset (Lin et al., 2014) currently consists 123,287 images fivedifferent descriptions per image. Images dataset annotated 80 object categories, means bounding boxes around instances one categoriesavailable images. MS COCO dataset widely used image description, something facilitated standard evaluation server recentlybecome available14 . Extensions MS COCO currently development, includingaddition questions answers (Antol et al., 2015).One paper (Lin et al., 2015) uses NYU dataset (Silberman, Kohli, Hoiem, &Fergus, 2012), contains 1,449 indoor scenes 3D object segmentation.dataset augmented five descriptions per image Lin et al.3.2 Image-Caption DatasetsImage descriptions verbalize seen image, i.e., refer objects,actions, attributes depicted, mention scene type, etc. Captions, hand,typically texts associated images verbalize information cannot seenimage. caption provides personal, cultural, historical context image(Panofsky, 1939). Images shared social networking photo-sharing websitesaccompanied descriptions captions, mixtures types text. imagesnewspaper museum typically contain cultural historical texts, i.e., captionsdescriptions.BBC News dataset (Feng & Lapata, 2008) one earliest collectionsimages co-occurring texts. Feng Lapata (2008) harvested 3,361 news articlesBritish Broadcasting Corporation News website, constraint articleincludes image caption.8.9.10.11.12.Source http://nlp.cs.illinois.edu/HockenmaierGroup/pascal-sentences/index.htmlSource http://github.com/elliottd/vltSource https://illinois.edu/fb/sec/1713398Source http://imageclef.org/photodataSource http://research.microsoft.com/en-us/um/people/larryz/clipart/SemanticClassesRender/Classes_v1.html13. Source http://mscoco.org/explore14. Source http://mscoco.org/dataset/#captions-eval426fiAutomatic Description Generation Images: SurveySBU1M Captions dataset introduced Ordonez et al. (2011) differsprevious datasets web-scale dataset containing approximately one millioncaptioned images. compiled data available Flickr user-provided imagedescriptions. images downloaded filtered Flickr constraintimage contained least one noun one verb predefined control lists. resultingdataset provided CSV file URLs.Deja-Image Captions dataset (Chen et al., 2015) contains 4,000,000 images180,000 near-identical captions harvested Flickr. 760 million images downloadedFlickr calendar year 2013 using set 693 nouns queries. imagecaptions normalized lemmatization stop word removal create corpusnear-identical texts. instance, sentences bird flies blue sky birdflying blue sky normalized bird fly blue sky (Chen et al., 2015). Imagecaption pairs retained captions repeated one user normalizedform.3.3 Collecting DatasetsCollecting new imagetext datasets typically performed crowd-sourcing harvesting data web. images datasets either sourcedexisting task computer vision community Pascal challenge (Everinghamet al., 2010) used Pascal1K VLT2K datasets directly Flickr,case Flickr8K/30K, MS COCO, SBU1M Captions, Deja-Image Captions datasets,crowdsourced, case Abstract Scenes dataset. texts imagedescriptiondatasets usually crowd-sourced Amazon Mechanical Turk Crowdflower; whereastexts imagecaption datasets harvested photo-sharing sites,Flickr, news providers. Captions usually collected without financial incentivewritten people sharing images, journalists.Crowd-sourcing descriptions images involves defining simple taskperformed untrained workers. Examples task guidelines used Hodosh et al.(2013) Elliott Keller (2013) given Figure 6. instances, care takenclearly inform potential workers expectations task. particular,explicit instructions given descriptions written, examplesgood texts provided. addition, Hodosh et al. provided extensive examplesexplain would constitute unsatisfactory texts. options available controlquality collected texts: minimum performance rate workers commonchoice; pre-task selection quiz may used determine whether workerssufficient grasp English language (Hodosh et al., 2013).issue remuneration crowd-sourced workers controversial, higher payments always lead better quality crowd-sourced environment (Mason & Watts,2009). Rashtchian et al. (2010) paid $0.01/description, Elliott Keller (2013) paid $0.04average 67 seconds work produce two-sentence description. bestknowledge, information available datasets.427fiBernardi et al.(a) Mechanical Turk Interface used collect Flickr8K dataset15 .(b) Mechanical Turk Interface used collect VLT2K dataset.Figure 6: Examples Mechanical Turk interfaces collecting descriptions.3.4 Evaluation MeasuresEvaluating output natural language generation (NLG) system fundamentallydifficult task (Dale & White, 2007; Reiter & Belz, 2009). common way assessquality automatically generated texts subjective evaluation human experts.15. Source Appendix work Hodosh et al. (2013)428fiAutomatic Description Generation Images: SurveyNLG-produced text typically judged terms grammar content, indicatingsyntactically correct relevant text is, respectively. Fluency generatedtext sometimes tested well, especially surface realization technique involvedgeneration process. Automatically generated descriptions imagesevaluated using NLG techniques. Typically, judges provided imagewell description evaluation tasks. Subjective human evaluationsmachine generated image descriptions often performed Mechanical Turk helpquestions. far, following Likert-scale questions used test datasetsuser groups various sizes.description accurately describes image (Kulkarni et al., 2011; Li et al., 2011;Mitchell et al., 2012; Kuznetsova et al., 2012; Elliott & Keller, 2013; Hodosh et al.,2013).description grammatically correct (Yang et al., 2011; Mitchell et al., 2012;Kuznetsova et al., 2012; Elliott & Keller, 2013, inter alia).description incorrect information (Mitchell et al., 2012).description relevant image (Li et al., 2011; Yang et al., 2011).description creatively constructed (Li et al., 2011).description human-like (Mitchell et al., 2012).Another approach evaluating descriptions use automatic measures,BLEU (Papineni, Roukos, Ward, & Zhu, 2002), ROUGE (Lin & Hovy, 2008), TranslationError Rate (Feng & Lapata, 2013), Meteor (Denkowski & Lavie, 2014), CIDEr (Vedantam, Lawrence Zitnick, & Parikh, 2015). measures originally developed evaluate output machine translation engines text summarization systems,exception CIDEr, developed specifically image description evaluation.measures compute score indicates similarity system outputone human-written reference texts (e.g., ground truth translations summaries).approach evaluation subject much discussion critique (Kulkarniet al., 2011; Hodosh et al., 2013; Elliott & Keller, 2014). Kulkarni et al. found weaklynegative correlation human judgments unigram BLEU Pascal 1KDataset (Pearsons = -0.17 0.05). Hodosh et al. studied Cohens correlationexpert human judgments binarized unigram BLEU unigram ROUGE retrieveddescriptions Flickr8K dataset. found best agreement humansBLEU ( = 0.72) ROUGE ( = 0.54) system retrieved sentences originally associated images. Agreement dropped one reference sentenceavailable, reference sentences disjoint proposal sentences.concluded neither measure appropriate image description evaluationsubsequently proposed imagesentence ranking experiments, discussed detail below. Elliott Keller analyzed correlation human judgments automaticevaluation measures retrieved system-generated image descriptions Flickr8KVLT2K datasets. showed sentence-level unigram BLEU, point429fiBernardi et al.time de facto standard measure image description evaluation, weaklycorrelated human judgments. Meteor (Banerjee & Lavie, 2005), less frequently usedtranslation evaluation measure, exhibited highest correlation human judgments.However, Kuznetsova et al. (2014) found unigram BLEU strongly correlatedhuman judgments Meteor image caption generation.first large-scale image description evaluation took place MS COCOCaptions Challenge 2015,16 featuring 15 teams dataset 123,716 training images41,000 images withheld test dataset. number reference texts testingimage either five 40, based insight measures may benefitlarger reference sets (Vedantam et al., 2015). automatic evaluation measuresused, image description systems outperformed humanhuman upper bound,17whether five 40 reference descriptions provided. However, none systemsoutperformed humanhuman evaluation judgment elicitation task used. Meteorfound robust measure, systems beating human text onetwo submissions (depending number references); systems outperformedhumans seven five times measured CIDEr; according ROUGE BLEU,system nearly always outperformed humans, confirming unsuitabilityevaluation measures.models approach description generation problem cross-modal retrieval perspective (Hodosh & Hockenmaier, 2013; Hodosh et al., 2013; Socher et al., 2014;Gong et al., 2014; Karpathy et al., 2014; Verma & Jawahar, 2014) also able use measures information retrieval, median rank (mRank), precision k (S@k),recall k (R@k) evaluate descriptions return, addition text-similaritymeasures reported above. evaluation paradigm first proposed Hodosh et al.,reported high correlation human judgments imagesentence based rankingevaluations.Table 3, summarize image description approaches discussed survey,list datasets evaluation measures employed approaches.seen recent systems (starting 2014) converged uselarge description datasets (Flickr8K/30K, MS COCO) employ evaluation measuresperform well terms correlation human judgments (Meteor, CIDEr). However,use BLEU, despite limitations, still widespread; also use human evaluationmeans universal literature.4. Future Directionssurvey demonstrates, CV NLP communities witnessed upsurgeinterest automatic image description systems. help recent advances deeplearning models images text, substantial improvements quality automatically generated descriptions registered. Nevertheless, series challengesimage description research remain. following, discuss future directionsline research likely benefit from.16. Source http://mscoco.org/dataset/cap201517. Calculated collecting additional human-written description, comparedreference descriptions.430fiAutomatic Description Generation Images: SurveyReferenceApproachFarhadi et al. (2010)Kulkarni et al. (2011)Li et al. (2011)Ordonez et al. (2011)Yang et al. (2011)MultRetrievalGenerationGenerationVisRetrievalGenerationDatasetsGupta et al. (2012)Kuznetsova et al. (2012)Mitchell et al. (2012)Elliott Keller (2013)Hodosh et al. (2013)Pascal1KPascal1KPascal1KSBU1MIAPR,Flickr8K/30K,COCOVisRetrievalPascal1K, IAPRVisRetrievalSBU1MGenerationPascal1KGenerationVLT2KMultRetrieval Pascal1K, Flickr8KGong et al. (2014)Karpathy et al. (2014)Kuznetsova et al. (2014)Mason Charniak (2014)Patterson et al. (2014)Socher et al. (2014)Verma Jawahar (2014)Yatskar et al. (2014)Chen Zitnick (2015)MultRetrievalMultRetrievalGenerationVisRetrievalVisRetrievalMultRetrievalMultRetrievalGenerationMultRetrievalDonahue et al. (2015)MultRetrievalDevlin et al. (2015)Elliott de Vries (2015)Fang et al. (2015)VisRetrievalGenerationGenerationJia et al. (2015)GenerationKarpathy Fei-Fei (2015) MultRetrievalKiros et al. (2015)Lebret et al. (2015)Lin et al. (2015)Mao et al. (2015a)Ortiz et al. (2015)Pinheiro et al. (2015)Ushiku et al. (2015)MultRetrievalMultRetrievalGenerationMultRetrievalGenerationMultRetrievalGenerationVinyals et al. (2015)MultRetrievalXu et al. (2015)Yagcioglu et al. (2015)MultRetrievalVisRetrievalMeasuresBLEUHuman, BLEUHuman, BLEUBLEU, ROUGE, Meteor,CIDEr, R@kHuman, BLEU, ROUGEHuman, BLEUHumanHuman, BLEUHuman, BLEU, ROUGE,mRank, R@kSBU1M, Flickr30KR@kFlickr8K/30K, COCOBLEU, Meteor, CIDErSBU1MHuman, BLEU, MeteorSBU1MHuman, BLEUSBU1MBLEUPascal1KmRank, R@kIAPR, SBU1M, Pascal1K BLEU, ROUGE, P@kdataHuman, BLEUFlickr8K/30K, COCOBLEU, Meteor, CIDEr,mRank, R@kFlickr30K, COCOHuman, BLEU, mRank,R@kCOCOBLEU, MeteorVLT2K, Pascal1KBLEU, MeteorCOCOHuman, BLEU, ROUGE,Meteor, CIDErFlickr8K/30K, COCOBLEU, MeteorFlickr8K/30K, COCOBLEU, Meteor, CIDEr,mRank, R@kFlickr8K/30KR@kFlickr30K, COCOBLEU, R@kNYUROUGEIAPR, Flickr30K, COCO BLEU, mRank, R@kAbstract ScenesHuman, BLEU, MeteorCOCOBLEUPascal1K, IAPR, SBU1M, BLEUCOCOPascal1K,SBU1M, BLEU, Meteor, CIDEr,Flickr8K/30KmRank, R@kFlickr8K/30K, COCOBLEU, MeteorFlickr8K/30K, COCOHuman, BLEU, Meteor,CIDErTable 3: overview approaches, datasets, evaluation measures reviewedsurvey organised chronological order.431fiBernardi et al.4.1 Datasetsearliest work image description used relatively small datasets (Farhadi et al., 2010;Kulkarni et al., 2011; Elliott & Keller, 2013). Recently, introduction Flickr30K,MS COCO large datasets enabled training complex modelsneural networks. Still, area likely benefit larger diversified datasetsshare common, unified, comprehensive vocabulary. Vinyals et al. (2015) arguecollection process quality descriptions datasets affect performancesignificantly, make transfer learning datasets effective expected.show learning model MS COCO applying datasets collecteddifferent settings SBU1M Captions Pascal1K, leads degradation BLEUperformance. surprising, since MS COCO offers much larger amount trainingdata Pascal1K. Vinyals et al. put it, largely due differencesvocabulary quality descriptions. learning approaches likely suffersituations. Collecting larger comprehensive datasets developinggeneric approaches capable generating naturalistic descriptions across domainstherefore open challenge.supervised algorithms likely take advantage carefully collected largedatasets, lowering amount supervision exchange access larger unsuperviseddata also interesting avenue future research. Leveraging unsupervised databuilding richer representations description models another open research challengecontext.4.2 MeasuresDesigning automatic measures mimic human judgments evaluating suitability image descriptions perhaps urgent need area image description(Elliott & Keller, 2014). need dramatically observed latest evaluation results MS COCO Challenge. According existing measures, including latest CIDErmeasure (Vedantam et al., 2015), several automatic methods outperform human upper bound (this upper bound indicates similar human descriptions other).counterintuitive nature result confirmed fact human judgments used evaluation, output even best system judged worsehuman generated description time (Fang et al., 2015). However, sinceconducting human judgment experiments costly, major need improved automatic measures highly correlated human judgments. Figure 7 plotsEpanechnikov probability density estimate (a non-parametric optimal estimator) BLEU,Meteor, ROUGE, CIDEr scores per subjective judgment Flickr8K dataset. human judgments obtained human experts (Hodosh et al., 2013). BLEUconfirmed unable sufficiently discriminate lowest three humanjudgments, Meteor CIDEr show signs moving towards useful separation.4.3 Diversity OriginalityCurrent algorithms often rely direct representations descriptions see training time, making descriptions generated test time similar. results many432fiAutomatic Description Generation Images: SurveyHuman JudgementHuman Judgement0.10PerfectMinor mistakesaspectsrelation0.000.000.020.050.040.100.060.080.15PerfectMinor mistakesaspectsrelation020406080100020BLEU406080100MeteorHuman JudgementPerfectMinor mistakesaspectsrelation00110023200453006PerfectMinor mistakesaspectsrelation4007Human Judgement0.00.20.40.60.80.001.00.050.100.150.20CIDErROUGEFigure 7: Probability density estimates BLEU, Meteor, ROUGE, CIDEr scoreshuman judgments Flickr8K dataset. y-axis shows probability density,x-axis score computed measure.433fiBernardi et al.repetitions limits diversity generated descriptions, making difficult reachhuman levels performance. situation demonstrated Devlin et al. (2015),show best model able generate 47.0% unique descriptions. Systems generate diverse original descriptions repeat alreadyseen, also infer underlying semantics therefore remain open challenge. ChenZitnick (2015) related approaches take step towards addressing limitationscoupling description visual representation generation.Jas Parikh (2015) introduces notion image specificity, arguing domain image descriptions uniform, certain images specific others.Descriptions non-specific images tend vary lot people tend describe nonspecific scene different aspects. notion effects description systemsmeasures investigated detail.4.4 TasksAnother open challenge visual question-answering (VQA). natural languagequestion-answering based text significant goal NLP research longtime (e.g., Liang, Jordan, & Klein, 2012; Fader, Zettlemoyer, & Etzioni, 2013; Richardson, Burges, & Renshaw, 2013; Fader, Zettlemoyer, & Etzioni, 2014), answering questionsimages task recently emerged. Towards achieving goal, MalinowskiFritz (2014a) propose Bayesian framework connects natural language questionanswering visual information extracted image parts. recently, imagequestion answering methods based neural networks developed (Gao, Mao,Zhou, Huang, & Yuille, 2015; Ren, Kiros, & Zemel, 2015; Malinowski, Rohrbach, & Fritz,2015; Ma, Lu, & Li, 2016). Following effort, several datasets taskreleased: DAQUAR (Malinowski & Fritz, 2014a) compiled scene depth imagesmainly focuses questions type, quantity color objects; COCOQA (Ren et al., 2015) constructed converting image descriptions VQA formatsubset images MS COCO dataset; Freestyle Multilingual Image Question Answering (FM-IQA) Dataset (Gao et al., 2015), Visual Madlibs dataset (Yu, Park,Berg, & Berg, 2015) VQA dataset (Antol et al., 2015), built imagesMS COCO, time question-answer pairs collected via human annotatorsfreestyle paradigm. Research emerging field likely flourish near future. ultimate goal VQA build systems pass (recently developed)Visual Turing Test able answer arbitrary questions imagesprecision human observer (Malinowski & Fritz, 2014b; Geman, Geman, Hallonquist, &Younes, 2015).multilingual repositories image description interesting directionexplore. Currently, among available benchmark datasets, IAPR-TC12dataset (Grubinger et al., 2006) multilingual descriptions (in English German).Future work investigate whether transferring multimodal features monolingual description models results improved descriptions compared monolingual baselines.434fiAutomatic Description Generation Images: Surveywould interesting study different models new tasks multilingual multimodalsetting using larger syntactically diverse multilingual description corpora.18Overall, image understanding ultimate goal computer vision natural language generation one ultimate goals NLP. Image descriptiongoals interconnected topic therefore likely benefit individual advancestwo fields.5. Conclusionssurvey, discuss recent advances automatic image description closely relatedproblems. review analyze large body existing work highlighting commoncharacteristics differences existing research. particular, categorizerelated work three groups: (i) direct description generation images, (i) retrievalimages visual space, (iii) retrieval images multimodal (joint visuallinguistic) space. addition, provided brief review existing corporaautomatic evaluation measures, discussed future directions vision languageresearch.Compared traditional keyword-based image annotation (using object recognition,attribute detection, scene labeling, etc.), automatic image description systems producehuman-like explanations visual content, providing complete picture scene.Advancements field could lead intelligent artificial vision systems,make inferences scenes generated grounded image descriptionstherefore interact environments natural manner. could alsodirect impact technological applications visually impaired peoplebenefit accessible interfaces.Despite remarkable increase number image description systems recentyears, experimental results suggest system performance still falls short human performance. similar challenge lies automatic evaluation systems using referencedescriptions. measures tools currently use sufficiently highly correlated human judgments, indicating need measures dealcomplexity image description problem adequately.Acknowledgmentsthank anonymous reviewers useful comments. work partially supported European Commission ICT COST Action iV&L Net: European Network Integrating Vision Language (IC1037). RC, AE, EE, NICfunded Scientific Technological Research Council Turkey (TUBITAK) research grant 113E116. FK would like acknowledge ERC funding starting grant203427 Synchronous Linguistic Visual Processing. DE supported ERCIMABCDE Fellowship 2014-23.18. Multimodal Translation Shared Task 2016 Workshop Machine Translation useEnglish German translated version Flickr30K corpora. See http://www.statmt.org/wmt16/multimodal-task.html details.435fiBernardi et al.ReferencesAntol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., & Parikh, D. (2015).Vqa: Visual question answering. International Conference Computer Vision.Banerjee, S., & Lavie, A. (2005). METEOR: Automatic Metric MT EvaluationImproved Correlation Human Judgments. Annual Meeting Association Computational Linguistics Workshop Intrinsic Extrinsic EvaluationMeasures MT and/or Summarization.Berg, T. L., Berg, A. C., & Shih, J. (2010). Automatic attribute discovery characterization noisy web data. European Conference Computer Vision.Chatfield, K., Simonyan, K., Vedaldi, A., & Zisserman, A. (2014). Return devildetails: Delving deep convolutional nets. British Machine Vision Conference.Chen, J., Kuznetsova, P., Warren, D., & Choi, Y. (2015). Deja image-captions: corpusexpressive descriptions repetition. North American Chapter AssociationComputational Linguistics.Chen, X., & Zitnick, C. L. (2015). Minds eye: recurrent visual representation imagecaption generation. IEEE Conference Computer Vision Pattern Recognition.Dale, R., & White, M. E. (Eds.). (2007). Workshop Shared Tasks ComparativeEvaluation Natural Language Generation: Position Papers.Denkowski, M., & Lavie, A. (2014). Meteor Universal: Language Specific Translation Evaluation Target Language. Conference European Chapter Association Computational Linguistics Workshop Statistical Machine Translation.Devlin, J., Cheng, H., Fang, H., Gupta, S., Deng, L., He, X., Zweig, G., & Mitchell, M.(2015). Language Models Image Captioning: Quirks Works.Annual Meeting Association Computational Linguistics.Donahue, J., Hendricks, L. A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko,K., & Darrell, T. (2015). Long-term recurrent convolutional networks visual recognition description. IEEE Conference Computer Vision Pattern Recognition.Elliott, D., & de Vries, A. P. (2015). Describing images using inferred visual dependencyrepresentations. Annual Meeting Association Computational Linguistics.Elliott, D., & Keller, F. (2013). Image Description using Visual Dependency Representations. Conference Empirical Methods Natural Language Processing.Elliott, D., & Keller, F. (2014). Comparing Automatic Evaluation Measures ImageDescription. Annual Meeting Association Computational Linguistics.Elliott, D., Lavrenko, V., & Keller, F. (2014). Query-by-Example Image Retrieval usingVisual Dependency Representations. International Conference ComputationalLinguistics.Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., & Zisserman, A. (2010).PASCAL Visual Object Classes (VOC) Challenge. International Journal ComputerVision, 88 (2), 303338.436fiAutomatic Description Generation Images: SurveyFader, A., Zettlemoyer, L., & Etzioni, O. (2013). Paraphrase-driven learning open question answering. Annual Meeting Association Computational Linguistics.Fader, A., Zettlemoyer, L., & Etzioni, O. (2014). Open question answering curatedextracted knowledge bases. ACM SIGKDD Conference Knowledge DiscoveryData Mining.Fang, H., Gupta, S., Iandola, F., Srivastava, R., Deng, L., Dollar, P., Gao, J., He, X.,Mitchell, M., Platt, J., Zitnick, C. L., & Zweig, G. (2015). captions visualconcepts back. IEEE Conference Computer Vision Pattern Recognition.Farhadi, A., Hejrati, M., Sadeghi, M. A., Young, P., Rashtchian, C., Hockenmaier, J., &Forsyth, D. (2010). Every picture tells story: Generating sentences images.European Conference Computer Vision.Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010). Object detection discriminatively trained part-based models. IEEE Transactions PatternAnalysis Machine Intelligence, 32 (9), 16271645.Feng, Y., & Lapata, M. (2008). Automatic Image Annotation Using Auxiliary Text Information. Annual Meeting Association Computational Linguistics.Feng, Y., & Lapata, M. (2013). Automatic caption generation news images. IEEETransactions Pattern Analysis Machine Intelligence, 35 (4), 797812.Ferraro, F., Mostafazadeh, N., Huang, T., Vanderwende, L., Devlin, J., Galley, M., &Mitchell, M. (2015). survey current datasets vision language research.Conference Empirical Methods Natural Language Processing.Gao, H., Mao, J., Zhou, J., Huang, Z., & Yuille, A. (2015). talking machine?dataset methods multilingual image question answering. InternationalConference Learning Representations.Geman, D., Geman, S., Hallonquist, N., & Younes, L. (2015). Visual turing test computervision systems. Proceedings National Academy Sciences, 112 (12), 36183623.Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies accurate object detection semantic segmentation. IEEE Conference ComputerVision Pattern Recognition.Gong, Y., Wang, L., Hodosh, M., Hockenmaier, J., & Lazebnik, S. (2014). Improving ImageSentence Embeddings Using Large Weakly Annotated Photo Collections. EuropeanConference Computer Vision.Grubinger, M., Clough, P., Muller, H., & Deselaers, T. (2006). IAPR TC-12 benchmark:new evaluation resource visual information systems. International ConferenceLanguage Resources Evaluation.Guadarrama, S., Krishnamoorthy, N., Malkarnenkar, G., Venugopalan, S., Mooney, R., Darrell, T., & Saenko, K. (2013). Youtube2text: Recognizing describing arbitraryactivities using semantic hierarchies zero-shot recognition. International Conference Computer Vision.Gupta, A., Verma, Y., & Jawahar, C. V. (2012). Choosing linguistics vision describeimages. AAAI Conference Artificial Intelligence.437fiBernardi et al.Hardoon, D. R., Szedmak, S., & Shawe-Taylor, J. (2004). Canonical correlation analysis:overview application learning methods. Neural Computation, 16 (12),26392664.Hodosh, M., & Hockenmaier, J. (2013). Sentence-based image description scalable,explicit models. IEEE Conference Computer Vision Pattern RecognitionWorkshops.Hodosh, M., Young, P., & Hockenmaier, J. (2013). Framing Image Description Ranking Task: Data, Models Evaluation Metrics. Journal Artificial IntelligenceResearch, 47, 853899.Hotelling, H. (1936). Relations two sets variates. Biometrika, 0, 321377.Jaimes, A., & Chang, S.-F. (2000). conceptual framework indexing visual informationmultiple levels. IST SPIE Internet Imaging.Jas, M., & Parikh, D. (2015). Image specificity. IEEE Conference Computer VisionPattern Recognition.Jia, X., Gavves, E., Fernando, B., & Tuytelaars, T. (2015). Guiding long-short termmemory model image caption generation. International Conference Computer Vision.Johnson, J., Krishna, R., Stark, M., Li, L.-J., Shamma, D. A., Bernstein, M., & Fei-Fei, L.(2015). Image retrieval using scene graphs. IEEE Conference Computer VisionPattern Recognition.Karpathy, A., & Fei-Fei, L. (2015). Deep visual-semantic alignments generating imagedescriptions. IEEE Conference Computer Vision Pattern Recognition.Karpathy, A., Joulin, A., & Fei-Fei, L. (2014). Deep Fragment Embeddings BidirectionalImage Sentence Mapping. Advances Neural Information Processing Systems.Khan, M. U. G., Zhang, L., & Gotoh, Y. (2011). Towards coherent natural language description video streams. International Conference Computer Vision Workshops.Kiros, R., Salakhutdinov, R., & Zemel, R. S. (2015). Unifying visual-semantic embeddingsmultimodal neural language models. Advances Neural Information Processing Systems Deep Learning Workshop.Krishnamoorthy, N., Malkarnenkar, G., Mooney, R., Saenko, K., & Guadarrama, S. (2013).Generating Natural-Language Video Descriptions Using Text-Mined Knowledge.Annual Conference North American Chapter Association Computational Linguistics: Human Language Technologies.Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg, A. C., & Berg, T. L. (2011). Babytalk: Understanding generating simple image descriptions. IEEE ConferenceComputer Vision Pattern Recognition.Kuznetsova, P., Ordonez, V., Berg, A. C., Berg, T. L., & Choi, Y. (2012). CollectiveGeneration Natural Image Descriptions. Annual Meeting AssociationComputational Linguistics.438fiAutomatic Description Generation Images: SurveyKuznetsova, P., Ordonezz, V., Berg, T. L., & Choi, Y. (2014). TREETALK: Compositioncompression trees image descriptions. Conference Empirical MethodsNatural Language Processing.Lampert, C. H., Nickisch, H., & Harmeling, S. (2009). Learning detect unseen objectclasses between-class attribute transfer. IEEE Conference Computer VisionPattern Recognition.Lazebnik, S., Schmid, C., & Ponce, J. (2006). Beyond bags features: Spatial pyramidmatching recognizing natural scene categories. IEEE Conference ComputerVision Pattern Recognition.Lebret, R., Pinheiro, P. O., & Collobert, R. (2015). Phrase-based image captioning.International Conference Machine Learning.Li, S., Kulkarni, G., Berg, T. L., Berg, A. C., & Choi, Y. (2011). Composing simple imagedescriptions using web-scale n-grams. SIGNLL Conference ComputationalNatural Language Learning.Liang, P., Jordan, M. I., & Klein, D. (2012). Learning dependency-based compositionalsemantics. Computational Linguistics, 39 (2), 389446.Lin, C.-Y., & Hovy, E. (2008). Automatic evaluation summaries using n-gram cooccurrence statistics. Annual Conference North American ChapterAssociation Computational Linguistics: Human Language Technologies.Lin, D., Fidler, S., Kong, C., & Urtasun, R. (2015). Generating multi-sentence naturallanguage descriptions indoor scenes. British Machine Vision Conference.Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., & Zitnick,C. L. (2014). Microsoft COCO: Common objects context. European ConferenceComputer Vision.Lowe, D. (2004). Distinctive image features scale-invariant keypoints. InternationalJournal Computer Vision, 60 (4), 91110.Ma, L., Lu, Z., & Li, H. (2016). Learning answer questions image using convolutionalneural network. AAAI Conference Artificial Intelligence.Malinowski, M., & Fritz, M. (2014a). multi-world approach question answeringreal-world scenes based uncertain input. Advances Neural Information Processing Systems.Malinowski, M., & Fritz, M. (2014b). Towards visual turing challenge. AdvancesNeural Information Processing Systems Workshop Learning Semantics.Malinowski, M., Rohrbach, M., & Fritz, M. (2015). Ask neurons: neural-basedapproach answering questions images. International Conference Computer Vision.Mao, J., Xu, W., Yang, Y., Wang, J., & Yuille, A. L. (2015a). Deep captioning multimodal recurrent neural networks (m-RNN). International Conference LearningRepresentations.439fiBernardi et al.Mao, J., Wei, X., Yang, Y., Wang, J., Huang, Z., & Yuille, A. L. (2015b). Learning likechild: Fast novel visual concept learning sentence descriptions images.International Conference Computer Vision.Mason, R., & Charniak, E. (2014). Nonparametric Method Data-driven Image Captioning. Annual Meeting Association Computational Linguistics.Mason, W. A., & Watts, D. J. (2009). Financial incentives performance crowds.ACM SIGKDD Workshop Human Computation.Mitchell, M., Han, X., Dodge, J., Mensch, A., Goyal, A., Berg, A. C., Yamaguchi, K.,Berg, T. L., Stratos, K., Daume, III, H., & III (2012). Midge: generating imagedescriptions computer vision detections. Conference European ChapterAssociation Computational Linguistics.Nenkova, A., & Vanderwende, L. (2005). impact frequency summarization. Tech.rep., Microsoft Research.Oliva, A., & Torralba, A. (2001). Modeling shape scene: holistic representationspatial envelope. International Journal Computer Vision, 42 (3), 145175.Ordonez, V., Kulkarni, G., & Berg, T. L. (2011). Im2text: Describing images using 1 millioncaptioned photographs. Advances Neural Information Processing Systems.Ortiz, L. M. G., Wolff, C., & Lapata, M. (2015). Learning Interpret DescribeAbstract Scenes. Conference North American Chapter AssociationComputational Linguistics.Panofsky, E. (1939). Studies Iconology. Oxford University Press.Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: method automatic evaluation machine translation. Annual Meeting AssociationComputational Linguistics.Parikh, D., & Grauman, K. (2011). Relative attributes. International ConferenceComputer Vision.Park, C., & Kim, G. (2015). Expressing image stream sequence naturalsentences. Advances Neural Information Processing Systems.Patterson, G., Xu, C., Su, H., & Hays, J. (2014). SUN Attribute Database: Beyond Categories Deeper Scene Understanding. International Journal Computer Vision,108 (1-2), 5981.Pinheiro, P., Lebret, R., & Collobert, R. (2015). Simple image description generator vialinear phrase-based model. International Conference Learning RepresentationsWorkshop.Prest, A., Schmid, C., & Ferrari, V. (2012). Weakly supervised learning interactionshumans objects. IEEE Transactions Pattern Analysis MachineIntelligence, 34 (3), 601614.Rashtchian, C., Young, P., Hodosh, M., & Hockenmaier, J. (2010). Collecting image annotations using amazons mechanical turk. North American Chapter AssociationComputational Linguistics: Human Language Technologies Workshop CreatingSpeech Language Data Amazons Mechanical Turk.440fiAutomatic Description Generation Images: SurveyReiter, E., & Belz, A. (2009). investigation validity metrics automatically evaluating natural language generation systems. Computational Linguistics,35 (4), 529588.Reiter, E., & Dale, R. (2006). Building Natural Language Generation Systems. CambridgeUniversity Press.Ren, M., Kiros, R., & Zemel, R. (2015). Image question answering: visual semantic embedding model new dataset. International Conference Machine LearningtDeep Learning Workshop.Richardson, M., Burges, C. J., & Renshaw, E. (2013). MCTest: challenge datasetopen-domain machine comprehension text. Conference Empirical MethodsNatural Language Processing.Rohrbach, A., Rohrback, M., Tandon, N., & Schiele, B. (2015). dataset movie description. International Conference Computer Vision.Rohrbach, M., Qiu, W., Titov, I., Thater, S., Pinkal, M., & Schiele, B. (2013). TranslatingVideo Content Natural Language Descriptions. International ConferenceComputer Vision.Schuster, S., Krishna, R., Chang, A., Fei-Fei, L., & Manning, C. D. (2015). Generatingsemantically precise scene graphs textual descriptions improved image retrieval. Conference Empirical Methods Natural Language Processing VisionLanguage Workshop.Shatford, S. (1986). Analyzing subject picture: theoretical approach. Cataloging& Classification Quarterly, 6, 3962.Silberman, N., Kohli, P., Hoiem, D., & Fergus, R. (2012). Indoor segmentation supportinference RGBD images. European Conference Computer Vision.Socher, R., & Fei-Fei, L. (2010). Connecting modalities: Semi-supervised segmentationannotation im- ages using unaligned text corpora. IEEE ConferenceComputer Vision Pattern Recognition.Socher, R., Karpathy, A., Le, Q. V., Manning, C. D., & Ng, A. (2014). Grounded Compositional Semantics Finding Describing Images Sentences. TransactionsAssociation Computational Linguistics, 2, 207218.Sun, C., Gan, C., & Nevatia, R. (2015). Automatic concept discovery parallel textvisual corpora. International Conference Computer Vision.Thomason, J., Venugopalan, S., Guadarrama, S., Saenko, K., & Mooney, R. (2014). Integrating Language Vision Generate Natural Language Descriptions VideosWild. International Conference Computational Linguistics.Torralba, A., Fergus, R., & Freeman, W. T. (2008). 80 million tiny images: large dataset nonparametric object scene recognition. IEEE Transactions PatternAnalysis Machine Intelligence, 30 (11), 19581970.Ushiku, Y., Yamaguchi, M., Mukuta, Y., & Harada, T. (2015). Common subspace modelsimilarity: Phrase learning caption generation images. InternationalConference Computer Vision.441fiBernardi et al.Vedantam, R., Lawrence Zitnick, C., & Parikh, D. (2015). Cider: Consensus-based image description evaluation. IEEE Conference Computer Vision PatternRecognition.Verma, Y., & Jawahar, C. V. (2014). Im2Text Text2Im: Associating Images TextsCross-Modal Retrieval. British Machine Vision Conference.Vinyals, O., Toshev, A., Bengio, S., & Erhan, D. (2015). Show tell: neural imagecaption generator. IEEE Conference Computer Vision Pattern Recognition.Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., Zemel, R., & Bengio, Y.(2015). Show, attend tell: Neural image caption generation visual attention.International Conference Machine Learning.Yagcioglu, S., Erdem, E., Erdem, A., & Cakici, R. (2015). Distributed RepresentationBased Query Expansion Approach Image Captioning. Annual MeetingAssociation Computational Linguistics.Yang, Y., Teo, C. L., Daume, III, H., & Aloimonos, Y. (2011). Corpus-guided sentence generation natural images. Conference Empirical Methods Natural LanguageProcessing.Yao, B., & Fei-Fei, L. (2010). Grouplet: structured image representation recognizinghuman object interactions. IEEE Conference Computer Vision PatternRecognition.Yao, L., Torabi, A., Cho, K., Ballas, N., Pal, C., Larochelle, H., & Courville, A. (2015).Describing videos exploiting temporal structure. International ConferenceComputer Vision.Yatskar, M., Galley, M., Vanderwende, L., & Zettlemoyer, L. (2014). See Evil, SayEvil: Description Generation Densely Labeled Images. Joint ConferenceLexical Computation Semantics.Young, P., Lai, A., Hodosh, M., & Hockenmaier, J. (2014). image descriptions visualdenotations: New similarity metrics semantic inference event descriptions.Transactions Association Computational Linguistics, 2, 6778.Yu, L., Park, E., Berg, A. C., & Berg, T. L. (2015). Visual madlibs: Fill blankdescription generation question answering. International Conference Computer Vision.Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., & Fidler, S.(2015). Aligning books movies: Towards story-like visual explanations watchingmovies reading books. International Conference Computer Vision.Zitnick, C. L., Parikh, D., & Vanderwende, L. (2013). Learning visual interpretationsentences. International Conference Computer Vision.Zitnick, C. L., & Parikh, D. (2013). Bringing semantics focus using visual abstraction.IEEE Conference Computer Vision Pattern Recognition.442fiJournal Artificial Intelligence Research 55 (2016) 1-15Submitted 11/15; published 01/16Introduction Special Issue Cross-LanguageAlgorithms ApplicationsMarta R. Costa-jussamarta.ruiz@upc.eduTALP Research CenterUniversitat Politecnica de CatalunyaJordi Girona 13, 08034, BarcelonaSrinivas Bangaloresbangalore@interactions.netInteractions Labs41 Spring Street,Murray Hill, NJ 07974, USAPatrik Lambertpatrik.lambert@upf.eduComputational Linguistics GroupUniversitat Pompeu FabraRoc Boronat 138, 08018 Barcelona, SpainLlus Marquezlmarquez@qf.org.qaQatar Computing Research InstituteHamad Bin Khalifa UniversityTornado Tower (10th floor), PO.Box 5825,West Bay, Doha, QatarElena Montiel-Ponsodaelena.montiel@upm.esOntology Engineering GroupUniversidad Politecnica de MadridCampus de Montegancedo s/n, Boadilla del Monte,28660 MadridAbstractincreasingly global nature everyday interactions, need multilingual technologies support efficient effective information access communicationcannot overemphasized. Computational modeling language focusNatural Language Processing, subdiscipline Artificial Intelligence. One currentchallenges discipline design methodologies algorithms crosslanguage order create multilingual technologies rapidly. goal JAIR specialissue Cross-Language Algorithms Applications (CLAA) present leading research area, emphasis developing unifying themes could leaddevelopment science multi- cross-lingualism. introduction, providereader motivation special issue summarize contributionspapers included. selected papers cover broad range cross-lingualtechnologies including machine translation, domain language adaptation sentimentanalysis, cross-language lexical resources, dependency parsing, information retrievalknowledge representation. anticipate special issue serve invaluableresource researchers interested topics cross-lingual natural language processing.c2016AI Access Foundation. rights reserved.fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsoda1. IntroductionDue increasingly global nature society, commonplace usencounter information multitude languages communicate across languageseveryday lives. rapid growth multilinguality information-driven societyreflected number languages used Internet. decade,Internet transformed predominantly English information sourcelinguistically variegated information source today. Multilingual accessprocessing pose novel challenges core Artificial Intelligence discipline speechnatural language processing which, solved, provide transformational technologiesinformation access broader population.complexity processing multiple languages computational model emergesdue different syntactic structures concept, also differentunderlying conceptual structures. challenges necessitate development crosslanguage natural language processing tools able translate link structuresconcepts across different languages.Cross-language extensions popular applications tasks information retrieval, question answering, sentiment analysis lexical disambiguation, among others,developed respond present needs global society. Similarly, multilingual resources novel ways represent multilingual knowledge emergedspread recent years. Researchers developers made considerable advancesapplications leveraging relevant data available diverse languages. multilingual processing becoming key research issue, given interdisciplinary nature,range approaches linguistic statistical perspectives exploredorder create viable cross-lingual technology.Interestingly, challenges cross-lingual natural language processing appealsacademic industrial research. provides new business opportunities breakinglanguage barriers fragment potential market. opportunities include, instance, possibility companies institutions learn users differentlocations think products (cross-language sentiment analysis), perform documentsearch multiple languages (cross-language information retrieval question answering),link knowledge bases clients using different languages (cross-language knowledge representation), expand market several linguistically diverse markets (machinetranslation localization).importance cross-lingual natural language processing clearly seenattention received recent workshops, invited talks, publicationscommunitys premier conferencesAssociation Computational Linguistics (ACL), European Association Computational Linguistics (EACL), North American AssociationComputational Linguistics (NAACL), Empirical Methods Natural Language Processing(EMNLP), International Conference Computational Linguistics (COLING), ExtendedSemantic Web Conference (ESWC), International Semantic Web Conference (ISWC), International Conference Language Resources Evaluation (LREC), InternationalConference Knowledge Capture (KCAP). past eighteen months, oneevery five papers conferences related cross-language algorithms applications (figures vary 17% 25%, average 20.4%). recently published book2fiSpecial Issue Cross-Language Algorithms Applicationsfocused multilingual natural language processing techniques (Bikel & Zitouni, 2012)highlights importance research area. book comprehensivelypresents, 600 pages, basic theory relevant techniques 16different aspects multilingual natural language processing.1.1 Cross-Language Algorithms Applications Special Issuespecial issue intended provide broad view recent advancescurrent research directions pursued area multilingual natural languageprocessing linguistic, computational language resource creation perspectives.Active research multiple recent workshops area (e.g., HyTra, see Costa-juss,Banchs, Rapp, Lambert, Eberle & Babych, 2013; WMT, see Bojar, Chatterjee, Federmann,Haddow, Huck, Hokamp, Koehn, Loncheva, Monz, Negri, Post, Scarton, Specia & Turchi,2015; CLEF, see Forner, Moller, Paredes, Rosso & Stein, 2013; Promise, see Bener, Minku& Turhan, 2014; MSW, see Gracia, MacGrae & Vulcu, 2015; AKBC, see Suchanek,Riedel, Singh & Talukdar, 2012) spanning cross-language natural language applications,tools crowdsourcing resource creation, deeper relationships bridginglanguage barriers modalities perception summarized special issue.response solicitation papers late 2014, received 34 research papersvariety cross-lingual technologies applied language processing tasks.papers carefully reviewed least three reviewers drawn pool100 reviewers. Based reviews extensive follow discussions, selected 8high quality papers offer exciting research directions span range topicscross-lingual language processing including machine translation, domain languageadaptation sentiment cross-language lexical resources, dependency parsing, information retrieval knowledge representation. introduction covers exactly eightpapers accepted special issue official timeline. paperssummarized Section 2, organized topic. attempt accommodate broadersample interesting papers cross-language algorithms applications, papersappropriate topic meet special issue deadlines also addedJAIR web page1 accepted journal.Finally, intended special issue disconnected melange success stories,provide underlying theme unifies research directions area. hopecollection provides reader opportunity observe similarities differencesacross topics, algorithms applications. anticipate scientific communityview cross-lingual speech language processing fertile productive fieldresearch, potential developing science technologieslasting impact everyday lives.2. Special Issue Overviewsection, survey topics cover papers special issuepapers themselves. topics machine translation, domain language adaptation1. http://www.jair.org/specialtrack-claa.html3fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsodasentiment analysis cross-language lexical resources, dependency parsing, informationretrieval knowledge representation.2.1 Machine Translationkey technology multilingual information society Machine Translation (MT)source language speech text automatically converted target languagespeech text.Web content generated multiple languages Internet users becoming linguistically diverse, machine translation provides cheapest quickest wayunderstand multilingual information. Besides core application necessarymultilingual world, machine translation shown highly relevant componenttechnology many cross-language natural language processing tasks, sentimentanalysis, information retrieval knowledge representation.Historically, machine translation approaches categorized rule-drivendata-driven approaches. Rule-based machine translation (Hutchins & Sommers, 1992) requires deep linguistic knowledge language pairs involved translation significant amount human labor. Since rules based linguistic intuitions, easieridentify issues extend them. recently, data-driven approacheslearn translation models minimal human supervision large bilingualparallel corporatexts source text paired target text (Sanchez-Martnez& Forcada, 2009). Data-driven translation systems find probable target text givensource text. systems extended phrase-based, syntax-based, hierarchical phrase-based neural-based systems order capture longer contextssentence. Phrase-based systems (Koehn, Och, & Marcu, 2003) use sequences wordsbilingual units, called phrases, whose probability computed log-linear combination feature functions including translation language models. Syntax-basedsystems use syntactic units extracted parse trees (Quirk, Menezes, & Cherry, 2005).Hierarchical phrase-based systems combine phrase-based syntax-based approachesusing synchronous context-free grammars (Chiang, 2007). Neural-based end-to-end translation systems typically use encoder-decoder approach learn embedded representationsinput sentence (encoder), used context generate wordstranslation (decoder) (Bahdanau, Cho, & Bengio, 2015).boundaries rule-based statistical machine translation narrowedproposals hybrid machine translation systems (Costa-jussa, 2015).special issue, two research papers machine translation combine rules,statistics machine learning.Integrating Rules Dictionaries Shallow-Transfer Machine Translation Phrase-Based Statistical Machine Translation Sanchez-Cartagena,Perez-Ortiz Sanchez-Martnez presents hybrid approach machine translationintegrating rules dictionaries shallow-transfer system (in particular, Apertium,see Armentano-Oller & Forcada, 2006) phrase-based system (in particular, Moses,see Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens,Dyer, Bojar, Constantin & Herbst, 2007). Deep linguistic knowledge rule-based systems transferred statistical system. integration especially useful4fiSpecial Issue Cross-Language Algorithms Applicationsparallel corpus available training phrase-based system small translating out-of-domain texts. authors discuss different methods enriching translationtables (including previous work authors references). Finally,worth mentioning approach used Apertiums rules one best-rankedsystems WMT 2011 international evaluation campaign Spanish-English (SanchezCartagena, Sanchez-Martnez, & Perez-Ortiz, 2011).Two relevant features paper are: (i) integration rules statistics presented, takes advantage deep knowledge underlying rule-based system,especially way linguistic resources used rule-based system segmenting source-language sentences; (ii) complete analysis hybrid system, includingautomatic manual evaluation, different corpora sizes, domains language pairs,comparison another popular hybrid MT system (Eisele, Federmann, Uszkoreit, SaintAmand, Kay, Jellinghaus, Hunsicker, Herrmann, & Chen, 2008).Cross-Lingual Bridges Models Lexical Borrowing Tsvetkov Dyerintroduces hybrid model lexical borrowing, demonstrated machine translation alleviate problem lexical coverage low-resourced languages. authorsstart hypothesis languages borrow terms languages pointexistence. propose computational model linguistic borrowing, intendedidentify donor words resource-rich language given loan word resource-poorlanguage. model develops set morpho-phonological transformations combining linguistic constraints using optimality theory machine learning score loanwordcandidates. model consists three parts: (i) conversion orthographic word formspronunciations,(ii) generation loan word pronunciation candidates, (iii) ranking generated candidates using optimality-theoretic constraints. first two stepsrule-based third learned data.lexical borrowing model applied Swahili-English, Maltese-English, Romanian-English MT. authors leverage model indirect way. instance, improving resource-poor Swahili-English MT system, identify translation candidatesout-of-vocabulary (OOV) Swahili words borrowed Arabic, using Arabic-to-Swahiliborrowing model resource-rich Arabic-English MT system. experimental resultsshow approach effectively reduces impact OOV source words improvestranslation quality significantly.one considers lexical borrowing different task transliteration cognateidentification could first computational model lexical borrowing useddownstream natural language processing application.2.2 Domain Language Adaptation Sentimentrapidly growing repository user-generated, subjective texts Internetform blogs, social networks, information channels consumer sites expressing opinions various issues, sentiment towards products, services personal perspectivesevents.Sentiment analysis task analyzing opinions, sentiments emotions expressedtowards entities products, services, organizations, issues, various attributesentities (Liu, 2012). two main sentiment analysis approaches presented5fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsodaliterature machine learning approach (mostly supervised learning) opinionlexicon-based approach, based rules.necessary resourcesthe training data, subjective text analyzedanalysis outcomeare available required language, cross-language sentimentanalysis methods needed bootstrap system. main cross-language sentimentanalysis approaches described literature via lexicon transfer, via corpus transfer, via test translation via joint classification. lexicon transfer approach,source sentiment lexicon transferred target language lexicon-based classifier built target language (Mihalcea, Banea, & Wiebe, 2007). corpus transferapproach consists transferring source training corpus target languagebuilding corpus-based classifier target language (Banea, Mihalcea, & Wiebe, 2008).test translation approach, test sentences target language translatedsource language classified using source language classifier (Bautin, Vijayarenu, & Skiena, 2008). Work joint classification includes co-training (Wan, 2009), jointlearning (Lu, Tan, Cardie, & K. Tsou, 2011) structural correspondence learning (Wei &Pal, 2010; Prettenhofer & Stein, 2010).authors already studied impact automatic sentiment analysis transferring lexicons corpora via machine translation (Mihalcea et al., 2007; Banea et al.,2008).Translation Alters Sentiment Mohammad, Salameh Kiritchenko goesstep analysis two respects. First, authors conduct systematicevaluation impact automatic manual translation automatic manualsentiment analysis. Second, authors perform qualitative quantitative analysisunderstand reasons obtained results. summary, paper provides deeperunderstanding sentiments altered common cross-language settings.experiments performed using Arabic sentiment analysis systemEnglishArabic machine translation system, showing state-of-the-art performance.authors first show automatic sentiment analysis English translations (even coming MT) achieve competitive results. Interestingly, also show automaticsentiment analysis automatic translations outperforms manual sentiment annotationautomatically translated text. qualitative quantitative analysis resultsalso reveals interesting facts. example, sentiment expressions often mistranslatedneutral expressions. automatic sentiment analysis system recover consistent translation errors learning true sentiments mistranslated words. commoncauses translation failing preserve sentiments sarcasm, metaphoric expressions,incorrect word-reordering.Distributional Correspondence Indexing Cross-Lingual Cross-DomainSentiment Classification Esuli, Moreo Sebastiani proposes novel domain adaptation method, also evaluated language adaptation. paper explores generalcomplex formulation domain adaptation problem combines cross-domaincross-language settings. proposed adaptation method, called Distributional Correspondence Indexing, inspired Structural Correspondence Learning followsdifferent, simpler approach, direct application distributional hypothesis.approach assumes terms across domains and/or languages show similar distributional properties relative small set pivot terms, behave similarly across6fiSpecial Issue Cross-Language Algorithms Applicationsdomains/languages. authors show approach outperforms existing methodscross-domain/language sentiment classification, lower computational cost.Since digital documents increasing variety topics languages produced,often need processed immediately, better solutions tackle bottleneckscarcity training data increasing importance. idea leveraging resourceslanguage learn classifier another language similar transfer learningdomain adaptation. contribution paper goes direction unifying domainlanguage adaptation framework.2.3 Lexical Resourcesnatural language applications question-answering, sentiment analysis, document classification, mention few, demand lexical knowledge differentnatural languages, termed cross-language lexical resources, also well-knownmultilingual lexical resources. range non-structured resources parallel corpora (EU JRC-Acquis Corpus, see Steinberger, Pouliquen, Widiger, Ignat, Erjavec,Tufis, & Varga, 2006), glossaries (e.g., IFLA Multilingual Glossary Art Librarians, seeLibraries, 1996) machine-readable dictionaries (Oxford online dictionaries2 ),structured resources terminological databases (IATE3 ), thesauri (AGROVOC 4 ),lexicons (EuroWordNet, see Vossen, 1998; MultiWordNet, see Pianta, Bentivogli, & Girardi, 2002), ontologies (e.g., EUROVOC SKOS, see Smedt & Vatanat, 2009; FAOgeopolitical ontology, see Kim, Iglesias-Sucasas, & Viollier, 2013).creation multilingual resources involves manual costly processes,many approaches pursue automation several steps development process.Without aim exhaustive, briefly describe typical approaches methodsresult multilingual lexical resources. (i) well-known family multilingualwordnets developed around Princeton English WordNet (Fellbaum, 1998). Basically, twoapproaches followed: (a) merging approach wordnets created separatelymapped afterwards English WordNet, difficulties mapping taskinvolves; (b) expansion approach English WordNet translatedcorresponding target languages, facilitating subsequent mapping task. (ii) Onlinecollaborative resources, example, Wiktionary5 . take advantage wisdomcrowd also Internet bots automatically generate entries, wellalgorithms import lexical information machine readable dictionaries. Similarly,Wikipedia whole family Wikimedia projects created collaborativemanner constitute de facto multilingual resources thanks hyperlinks amongentries different languages. resources built similar way take advantagestructured information contained include YAGO (Mahdisoltani, Biega, &Suchanek, 2015) BabeLNet (Navigli & Ponzetto, 2012). (iii) Mono- multilingualcontent linguistic datasets exposed linked according Linked Data paradigm6 ,set best practices publishing structured data linking datasets.2.3.4.5.6.http://www.oxforddictionaries.comhttp://iate.europa.euhttp://aims.fao.org/vest-registry/vocabularies/agrovoc-multilingual-agricultural-thesaurushttps://www.wiktionary.orghttp://linkeddata.org7fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsodaresources published linked data multilingual others monolingual,linked datasets close domains, become multilingual graphnavigable data. mapping linking step crucial approach. (iv) Translationresources also considered subtype lexical resources also usedpurposes obtaining multilingual data, source translations, meansmapping linking monolingual resources (for details machine translation seesection 2.1).sense, sufficiently demonstrated quality coveragelexical translation resources vital applications built top them,performance applications depends directly them. paper includedspecial issue describes analyzes resources detail, alsoevaluates coverage correctness context ontology mapping.Effectiveness Automatic Translations Cross-lingual Ontology MappingAbu Helou Palmonari presents large-scale study effectiveness severallexical translation resources purpose obtaining candidate matchesontology concepts lexicalized different languages. Many works cross-language ontologymapping rely multilingual translation resources obtain translation candidatesconcept lexicalization source ontology, subsequently support selectionpotential matches target ontology. paper evaluates machine translation service, GoogleTranslate7 , multilingual encyclopedic dictionary semantic network,BabelNet8 , correctness coverage suggested translations mapping selection capabilities (word-disambiguation). evaluation based wordnets differentnatural languages (Arabic, Italian, Slovene Spanish) manual alignments providedwordnet English WordNet. perform three experiments takingaccount various types lexical units (monosemous words, polysemous words, one-wordunits multiple-word units), define specific measures (translation correctness, wordsense coverage, synset coverage synonym coverage) serve better evaluatequality translations. results experiments provide insights coveragecorrectness resources provide, effect combining resultsresources, impact taking account translation directionality, differencesword categories mapped lexicalizations, among others.results paper directly applied improve cross-language ontologymapping task, also contribute speed development multilingual lexicalresources, general, help building truly multilingual Linked Open DataCloud, particular.2.4 Cross-Language Dependency Parsingnatural language application relies language processing tools part-ofspeech taggers parsers confronted significant challenge scaling newlanguages, languages may set tools. Building toolsdesired language requires manual annotation sufficient amounts texts machine learning programs trained on. Annotation efforts undertaken7. https://translate.google.com8. http://babelnet.org8fiSpecial Issue Cross-Language Algorithms Applicationsinvaluable resources varying amounts texts created past decadescertain languages, example, Penn Treebank (Marcus, Marcinkiewicz, & Santorini,1993), French Treebank (Abeille, 2003), NEGRA Treebank (Skut, Brants, & Uszkoreit,1998), Prague Dependency Treebank (Hajic, Bohmova, Hajicova, & Vidova-Hladka, 2000).However, task annotation effort time-consuming, expensive requires highlyskilled personnel.late nineties, availability texts pair languages realizedparallel text corpora, researchers (Bangalore, 1998; Yarowsky, Ngai, & Wicentowski, 2001)explored idea transferring annotations one language pair secondlanguage pair order rapidly create annotated resource train languageprocessing tools second language. line research followed numberresearchers projecting variety annotations language corpora usingannotated corpus bootstrap language processing tools target language.particular interest projection annotations part-of-speech tags, phrase structureannotations, dependency structure annotations.Synthetic Treebanking Cross-lingual Dependency Parsing TiedemannAgic presents detailed discussion options transplant dependency treesone language another order train dependency parsers target language.paper introduces two options bootstrapping dependency parser language: modeltransfer approach dependency parsing model transferred new language,annotation transfer approach; paper advocates annotation transfer approachcreation synthetic treebanks. paper provides comprehensive analysisvarious options creating synthetic treebank using parallel corpus including:(i) projecting parsers output source text onto target text (ii) translatingexisting high quality treebank source language target language. central ideacreating synthetic treebanks involves use statistical machine translation models,phrase-based syntax-based, order translate texts source languagetreebanks target language, project source language structurestranslated target language sentence mediated word alignment information producedtranslation process. challenges reconciling dependency structureslexical translations richer one-to-one discussed length. paper reportsextensive parsing accuracy results parsers trained projected treebanks largeset language pairs, studies correlation quality translation modelquality target language parser.2.5 Cross-Language Information Retrievalincrease number webpages multiple languages, search queryWeb needs retrieve webpages authored languages languagequery. Cross-language information retrieval technology intersection machinetranslation information retrieval addresses challenge.Cross-language information retrieval employed different strategies matchingquery set multilingual documents: cognate-based matching (Montalvo, Martnez,Casillas, & Fresno, 2007), matching query document translation, matchingmapping interlingua (Banchs & Costa-jussa, 2013). popular ap9fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsodaproaches query translation addressed either employing bilingual dictionaries (Hedlund, Airio, Keskustalo, Lehtokangas, Pirkola, & Jarvelin, 2004) using machinetranslation (Kishida, 2008). Approaches combine techniques foundwork work Zhang, Jones, Zhang (2008), example. quality querytranslation indirectly observed final retrieval results. Kishida (2008) showsregressive model ease search given query translation qualityexplain 60% variation performance. Kettunen (2009) showslong topics, correlations achieved retrieval results machine translationmetrics high (almost 90%) short topics correlation lower still clear(almost 60%). Cross-language video retrieval, another related cross-language informationretrieval task, involves automatic speech recognition.Utilisation Metadata Fields Query Expansion Cross-Lingual SearchUser-Generated Internet Video Khwileh, Jones Ganguly presents onefirst use-cases cross-language video retrieval social media content. paper focuseschallenges associated user generated informal content (i.e., noise, sparsenessmetadata, content different lengths, informal language register, etc.) ratherprofessionally produced content. Noise errors propagate stepprocessing: speech recognition automatic translation query expansion.authors use query translation approach bridge vocabulary gapusers query relevant content video application. Automatic translationdone using Google Translate retrieval expansion done DivergenceRandomness IR model. explore effectiveness three different sourcesinformation: transcripts automatic speech recognition, video titles, descriptions.Among three sources, leveraging video titles improves retrieval performanceexperiments. addition, authors propose adaptive query expansion techniqueautomatically selects reliable source expansion based well establishedquery performance prediction technique. Results show approach robustparticular setting.2.6 Cross-Language Knowledge RepresentationKnowledge representation systems aim formalize representations world,certain domain knowledge, way interpretable computers.achieved identifying domain concepts relations exist among them,representing information formal framework, e.g., using Description Logicformalism. Knowledge representation systems intended language-independentmeaning representations. However, different representations domain knowledge co-exist, since design certain knowledge representation mayparticular vision world, specific interests, certain application mind,among factors.One difficulties cross-language knowledge representation regards conceptualdifferences observed across languages cultures. Indeed, certain representations prone reflect cultural particularities shared understoodway cultural systems. involves existence certain conceptsexist knowledge systems, relevant them, different10fiSpecial Issue Cross-Language Algorithms Applicationsgranularity levels representation concepts (Espinoza, Montiel-Ponsoda, & GomezPerez, 2009). reasons, cross-language knowledge representation challengecurrent multilingual Web. Two main approaches followed obtain knowledge representation systems support several languages: (i) inclusion lexicalizationsseveral natural languages describe concepts relations formalized certainknowledge representation system; (ii) existence several knowledge representation systemswhose concepts relations expressed different natural language,linked mapped establish correspondences equivalences them. formerapproach commonly applied internationalized standardized domains, whereaslatter typical culturally-influenced domains, termed Cimiano, Montiel-Ponsoda,Buitelaar, Espinoza, Gmez-Prez (2010).globalized interconnected world, cross-language information access increasing importance. several approaches cross-language document similarityreported, including machine translation, probabilistic topic models, classificationmatrix factorization, little previous work task linking documents acrosslanguages refer events (Pouliquen, Steinberger, Ignat, Ksper, & Temnikova, 2004; Pouliquen, Steinberger, & Deguernel, 2008; Leban, Fortuna, Brank, & Grobelnik, 2014). actually difficult computationally expensive task, especiallymany language pairs involved, small number real-life workingsystems performing task exist. One example existing service providing crosslanguage cluster linking European Media Monitor (EMM) (Pouliquen et al., 2008).special issue includes contribution tackling task, topic modelingused represent knowledge expressed documents, linking task basedsimilarity-based entity-related features.News Across Languages - Cross-Lingual Document Similarity EventTracking Rupnik, Muhic, Leban, Skraba, Fortuna Grobelnik addresses problem event tracking large multilingual stream, and, specifically, linkcollections articles different languages refer event. authorsconsider major languages also less-resourced languages. approach based representations documents analogous multilingual topics, valid multiplelanguages. representations learned using Wikipedia training corpus.used compute cross-language similarities documents regardless language. posterior cross-language cluster linking performed two steps. First,speed-up process, similarity function used identify small set potentiallinking candidates cluster. Then, final decision taken based supervisedclassification model whose features include similarity-based entity-related features.comprehensive experimental study, authors show canonical correlation analysisbest-performing method compute multilingual similarities. Moreover, showsimilarity-based features greatly benefit additional semantic extraction-basedfeatures.Acknowledgementsauthors want thank Dan Roth, Mark Sammons anonymous revieweruseful comments suggestions previous versions document. work11fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsodasupported 7th Framework Program European CommissionInternational Outgoing Fellowship Marie Curie Action (IMTraP-2011-29951), IntraEuropean Fellowship CrossLingMind-2011-300828 project LIDER (610782);European Regional Development Fund (ERDF/FEDER); Spanish Ministerio deEconoma Competitividad SpeechTech4All project (TEC2012-38939-C03-02)project 4V: volumen, velocidad, variedad validez en la gestion innovadora dedatos (TIN2013-46238-C4-2-R).ReferencesAbeille, A. (2003). Treebanks: Building Using Parsed Corpora. Springer.Armentano-Oller, C., & Forcada, M. L. (2006). Open-source machine translationsmall languages: Catalan aranese occitan. Workshop Strategies developing machine translation minority languages, pp. 5154.Bahdanau, D., Cho, K., & Bengio, Y. (2015). Neural Machine Translation JointlyLearning Align Translate. CoRR, abs/1409.0473.Banchs, R., & Costa-jussa, M. R. (2013). Cross-Language Document Retrieval usingNon-linear Semantic Mapping. Applied Artificial Intelligence Journal, 27 (9), 781802.Banea, C., Mihalcea, R., & Wiebe, J. (2008). Bootstrapping Method Building Subjectivity Lexicons Languages Scarce Resources. Proceedings International Conference Linguistic Resources Evaluation (LREC), pp. 27642767,Marrakech, Morocco.Bangalore, S. (1998). Transplanting Supertags English Spanish. ProceedingsTAG+4 Workshop.Bautin, M., Vijayarenu, L., & Skiena, S. (2008). International Sentiment Analysis NewsBlogs. Proc. International Conference Weblogs Social Media,pp. 1926, Seattle, U.S.A.Bener, A., Minku, L., & Turhan, B. (Eds.). (2014). PROMISE 14: Proceedings 10thInternational Conference Predictive Models Software Engineering, New York,NY, USA. ACM.Bikel, D., & Zitouni, I. (2012). Multilingual Natural Language Processing Applications:Theory Practice. IBM Press.Bojar, O., Chatterjee, R., Federmann, C., Haddow, B., Huck, M., Hokamp, C., Koehn, P.,Logacheva, V., Monz, C., Negri, M., Post, M., Scarton, C., Specia, L., & Turchi, M.(2015). Findings 2015 Workshop Statistical Machine Translation. Proceedings Tenth Workshop Statistical Machine Translation, pp. 146, Lisbon,Portugal.Chiang, D. (2007). Hierarchical Phrase-Based Translation. Computational Linguistics,33 (2), 201228.Cimiano, P., Montiel-Ponsoda, E., Buitelaar, P., Espinoza, M., & Gomez-Perez, A. (2010).Note Ontology Localization. Journal Applied Ontology, 5(2), 127137.12fiSpecial Issue Cross-Language Algorithms ApplicationsCosta-jussa, M. R. (2015). Much Hybridization Machine Translation Need?.Journal American Society Information Technology (JASIST), 6 (10), 21602165.Costa-jussa, M. R., Banchs, R., Rapp, R., Lambert, P., Eberle, K., & Babych, B. (2013).Workshop hybrid approaches translation: Overview developments. Proceedings Second Workshop Hybrid Approaches Translation, pp. 16, Sofia,Bulgaria.Eisele, A., Federmann, C., Uszkoreit, H., Saint-Amand, H., Kay, M., Jellinghaus, M., Hunsicker, S., Herrmann, T., & Chen, Y. (2008). Hybrid Architectures Multi-EngineMachine Translation. Proceedings Translating Computer 30. ASLIB/IMI,ASLIB.Espinoza, M., Montiel-Ponsoda, E., & Gomez-Perez, A. (2009). Ontology Localization.Proceedings 5th International Conference Knowledge Capture (KCAP09),pp. 3340.Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press.Forner, P., Moller, H., Paredes, R., Rosso, P., & Stein, B. (2013). Information AccessEvaluation. Multilinguality, Multimodality, Visualization. Springer.Gracia, J., MacCrae, J., & Vulcu, G. (Eds.). (2015). Proceedings Fourth WorkshopMultilingual Semantic Web. CEUR.Hajic, J., Bohmova, A., Hajicova, E., & Vidova-Hladka, B. (2000). Prague Dependency Treebank: Three-Level Annotation Scenario. Abeille, A. (Ed.), Treebanks:Building Using Parsed Corpora, pp. 103127. Amsterdam:Kluwer.Hedlund, T., Airio, E., Keskustalo, H., Lehtokangas, R., Pirkola, A., & Jarvelin, K. (2004).Dictionary-based Cross-Language Information Retrieval: Learning ExperiencesCLEF 2000-2002. Information Retrieval, 7 (1), 99119.Hutchins, W. J., & Sommers, H. L. (1992). Introduction Machine Translation, Vol.362. Academic Press, New York.Kettunen, K. (2009). Choosing Best MT Programs CLIR PurposesCan MTMetrics Helpful?. Proceedings 31th European Conference IR ResearchAdvances Information Retrieval, pp. 706712.Kim, S., Iglesias-Sucasas, M., & Viollier, V. (2013). FAO Geopolitical Ontology: Reference country-Based Information. Journal Agricultural Food Information,14 (1).Kishida, K. (2008). Prediction Performance Cross-language Information Retrieval Using Automatic Evaluation Translation. Library Information Science Research,30 (2), 138144.Koehn, P., Och, F., & Marcu, D. (2003). Statistical Phrase-Based Translation. Proceedings Conference North American Chapter AssociationComputational Linguistics Human Language Technology (NAACL-HLT), pp. 4854.13fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-PonsodaKoehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan,B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., & Herbst, E.(2007). Moses: Open Source Toolkit Statistical Machine Translation. Proceedings45th Annual Meeting ACL Interactive Poster DemonstrationSessions, ACL 07, pp. 177180.Leban, G., Fortuna, B., Brank, J., & Grobelnik, M. (2014). Event registry: Learningworld events news. Proceedings Companion Publication 23rdInternational Conference World Wide Web Companion (WWW Companion14),pp. 107110, Seoul, Korea.Libraries, I. S. o. A. (Ed.). (1996). Multilingual Glossary Art Librarians: EnglishIndexes Dutch, French, German, Italian, Spanish Swedish. De Gruyter.Liu, B. (2012). Sentiment Analysis Opinion Mining. Synthesis Lectures HumanLanguage Technologies. Morgan & Claypool Publishers.Lu, B., Tan, C., Cardie, C., & K. Tsou, B. (2011). Joint Bilingual Sentiment ClassificationUnlabeled Parallel Corpora. Proceedings Annual Meeting Association Computational Linguistics: Human Language Technologies (ACL-HLT), pp.320330, Portland, Oregon, USA.Mahdisoltani, F., Biega, J., & Suchanek, F. M. (2015). YAGO3: Knowledge BaseMultilingual Wikipedias. Proceedings Conference Innovative Data SystemsResearch (CIDR 2015).Marcus, M. P., Marcinkiewicz, M. A., & Santorini, B. (1993). Building Large AnnotatedCorpus English: Penn Treebank. Computational Linguistics, 19 (2), 313330.Mihalcea, R., Banea, C., & Wiebe, J. (2007). Learning Multilingual Subjective Language viaCross-Lingual Projections. Proceedings Annual Meeting AssociationComputational Linguistics (ACL), pp. 976983, Prague, Czech Republic.Montalvo, S., Martnez, R., Casillas, A., & Fresno, V. (2007). Multilingual News Clustering:Feature Translation vs. Identification Cognate Named Entities. Pattern RecognitionLetters, 28 (16), 23052311.Navigli, R., & Ponzetto, S. P. (2012). BabelNet: Automatic Construction, Evaluation Application Wide-Coverage Multilingual Semantic Network. ArtificialIntelligence, 193, 217250.Pianta, E., Bentivogli, L., & Girardi, C. (2002). MultiWordNet: developing alignedmultilingual database. Proceedings Frist International Conference GlobalWordNet.Pouliquen, B., Steinberger, R., & Deguernel, O. (2008). Story Tracking: Linking SimilarNews Time Across Languages. Proceedings COLING 2008 Workshop Multi-source Multilingual Information Extraction Summarization, pp.4956, Manchester, UK.Pouliquen, B., Steinberger, R., Ignat, C., Ksper, E., & Temnikova, I. (2004). Multilingualcross-lingual news topic tracking. Proceedings International ConferenceComputational Linguistics (COLING), pp. 959965, Geneva, Switzerland. COLING.14fiSpecial Issue Cross-Language Algorithms ApplicationsPrettenhofer, P., & Stein, B. (2010). Cross-Language Text Classification Using StructuralCorrespondence Learning. Proceedings Annual Meeting AssociationComputational Linguistics (ACL), pp. 11181127, Uppsala, Sweden.Quirk, C., Menezes, A., & Cherry, C. (2005). Dependency Treelet Translation: SyntacticallyInformed Phrasal SMT. Proceedings Annual Meeting AssociationComputational Linguistics (ACL), pp. 271279.Sanchez-Cartagena, V. M., Sanchez-Martnez, F., & Perez-Ortiz, J. A. (2011). Universitat dAlacant hybrid machine translation system WMT 2011. ProceedingsSixth Workshop Statistical Machine Translation, pp. 457463, Edinburgh,Scotland.Sanchez-Martnez, F., & Forcada, M. L. (2009). Inferring Shallow-Transfer Machine Translation Rules Small Parallel Corpora. Journal Artificial Intelligence Research,34, 605635.Skut, W., Brants, T., & Uszkoreit, H. (1998). Linguistically Interpreted Corpus GermanNewspaper Text. Proceedings ESSLLI Workshop Recent AdvancesCorpus Annotation, Saarbrucken, Germany.Smedt, J. D., & Vatanat, B. (2009). https://lists.w3.org/archives/public/public-eswthes/2010feb/att-0023/ontology.html..Steinberger, R., Pouliquen, B., Widiger, A., Ignat, C., Erjavec, T., Tufis, D., & Varga, D.(2006). JRC-Acquis: multilingual aligned parallel corpus 20+ languages.Proceedings 5th International Conference Language Resources Evaluation (LREC2006), Genoa, Italy.Suchanek, F., Riedel, S., Singh, S., & Pratim Talukdar, P. (Eds.)., (2012). AKBC-WEKEX12: Proceedings Joint Workshop Automatic Knowledge Base ConstructionWeb-scale Knowledge Extraction, Stroudsburg, PA, USA. Association Computational Linguistics.Vossen, P. (Ed.). (1998). EuroWordNet: Multilingual Database Lexical SemanticNetworks. Kluwer Academic Publishers, Norwell, MA, USA.Wan, X. (2009). Co-Training Cross-Lingual Sentiment Classification. ProceedingsJoint Conference 47th Annual Meeting ACL 4th International Joint Conference Natural Language Processing AFNLP, pp. 235243,Singapore.Wei, B., & Pal, C. (2010). Cross Lingual Adaptation: Experiment Sentiment Classifications. Proceedings Annual Meeting Association ComputationalLinguistics (ACL), pp. 258262, Uppsala, Sweden.Yarowsky, D., Ngai, G., & Wicentowski, R. (2001). Inducing Multilingual Text AnalysisTools via Robust Projection across Aligned Corpora. Proceedings First International Conference Human Language Technology Research, pp. 18, San Diego,CA, USA.Zhang, Y., Jones, G. J., & Zhang, K. (2008). Dublin City University CLEF 2007:Cross-Language Speech Retrieval Experiments. Lecture Notes Computer Science,703711.15fiJournal Artificial Intelligence Research 55 (2016) 685714Submitted 11/15; published 03/16Quadratization Roof Duality Markov Logic NetworksRoderick de Nijsrsdenijs@tum.deInstitute Computer ScienceAlbrechtstr. 28, 49076 Osnabruck, GermanyChristian LandsiedelDirk WollherMartin Busschristian.landsiedel@tum.dedw@tum.demb@tum.deLehrstuhl fur Steuerungs- und RegelungstechnikTheresienstr. 90, 80333 Munchen, GermanyAbstractarticle discusses quadratization Markov Logic Networks, enablesefficient approximate MAP computation means maximum flows. procedurerelies pseudo-Boolean representation model, allows handling modelsorder. employed pseudo-Boolean representation used identify problemsguaranteed solvable low polynomial-time. Results common benchmarkproblems show proposed approach finds optimal assignments variablesexcellent computational time approximate solutions match quality ILPbased solvers.1. IntroductionFirst-order probabilistic models promising paradigm overcoming limitationsclassical first-order logic ability capture uncertainty often presentreal-world problems. allow describing relational knowledge compactly,size representation independent number objects domain.knowledge models defined use parfactors (Poole, 2003),templates representing large numbers factors graphical model describesprobability distribution possible world configurations. underlying graphicalmodel finite, possible ground first-order model perform inferencepropositional level. reason, interest identify tractable cases propositionalproblems first-order level well finding efficient approximate algorithms caseexact inference possible. Although computing typical inference queriespropositional model NP-Hard general, problems lend traditionaloptimization approaches.article deals models whose MAP problem represented optimization finite number binary variables. case Markov Logic Networks(MLNs), parfactors weighted logic rules propositionalizeddefine Markov Random Field Boolean random variables. contributionswork stem great part representing parfactors using Boolean polynomials knownpseudo-Boolean functions. First, shown models certain parfactors,2016 AI Access Foundation. rights reserved.fide Nijs, Landsiedel, Wollherr, & BussMAP computation equivalent maximizing polar unimodular pseudo-Booleanfunction, done low-polynomial time. allows identify MLNstractable MAP problem. Secondly, shown quadratization techniques pseudoBoolean functions generalized parfactors. One benefit transformationsenable using Quadratic Pseudo-Boolean Optimization (QPBO) groundmodel, popular algorithm computer vision yet evaluated MLNs.literature quadratizations reviewed, show previous workdiscussing quadratization MLNs (Fierens, Kersting, Davis, Chen, & Mladenov, 2013)equivalent particular choice pseudo-Boolean quadratization within quadratization framework parfactors. Based generalized roof duality (Kahl & Strandmark,2012), new quadratization also introduced.Experimental evaluation quadratization techniques combination QPBOalgorithm show large benefits performance attained real-world problems,sophisticated quadratization techniques deliver better resultsone employed Fierens et al. (2013).combination quadratization QPBO shown competitive strategyapproaching MAP problem MLNs.1.1 Outlineremainder article organized follows: rest section reviewsoptimization methods available MLNs related work. Section 2, mathematical background, Markov Random Fields pseudo-Boolean functions presented,along notation used article. Section 3 describes transformation parfactors pseudo-Boolean form, discusses cases identified tractable.Section 4 presents general framework quadratization parfactors well comparison existing approach MLNs. Different quadratization strategies discussed.Section 5, thorough computational evaluation approach performed datasetsliterature well new problems. article concludes discussionmethods results Section 6.1.2 Related Workuse first-order representations provides compact flexible way encode knowledge model design phase. trade-off convenience groundmodels generally large treewidths, making MAP estimation commonqueries NP-hard general. ground model may thousands millionsvariables interactions, need fast memory-efficient optimization algorithms. section tries give overview recent prominent methodsinference models.Various algorithms based heuristic random search used approximateMAP solution. Alchemy system (Richardson & Domingos, 2006) implementsprobabilistic hill-climbing algorithm named MaxWalkSat. lazy variant MaxWalkSat (Singla & Domingos, 2006b) also developed, which, splitting networkactive inactive part, considerably reduces memory footprint algorithm.Tuffy system (Niu, Re, Doan, & Shavlik, 2011) reformulated algorithm within686fiQuadratization Roof Duality Markov Logic Networksrelational database faster grounding additional scalability. also capabledetecting weakly connected components ground network, used parallelizing inference. extension parallelization inference MLNs basedpartitioning network grounding found using minimum cuts, usedimportance sampling inference framework (Beedkar, Del Corro, & Gemulla, 2013).alternative approach based conversion ground factors linear constraints,MAP problem formulated integer linear program (ILP). Oneadvantage formulation solution linear relaxation gives optimisticestimate optimal cost. cost optimal solution liesparticular assignment optimistic estimate. make approach practicallarger problems, necessary reduce number linear constraints consideredsolver. purpose, cutting plane algorithm MLNs presented (Riedel,2009). iterative approach ignores constraints far satisfied previous intermediate solutions, includes become unsatisfied nextiteration. RockIt system (Noessner, Niepert, & Stuckenschmidt, 2013) showsstructurally similar constraints created first-order model aggregatedsingle one, presents parallelization scheme splits problem multipleILPs. combination techniques achieves excellent execution times. third typealgorithms perform queries propositionalized network, operate insteadpotentially much smaller lifted network. Algorithms lifted MAP seensignificant advances recent years (Apsel & Brafman, 2012; Sarkhel, Venugopal, Singla, &Gogate, 2014; Mittal, Goyal, Gogate, & Singla, 2014). However, approachesapplied efficiently problems specific types relations evidence,discussed article.article, used polynomial representation potential functions FOPMs. use polynomials representation Bayesian networkssuggested name network polynomials (Darwiche, 2003). polynomialscompiled arithmetic circuits used inference (Huang, Chavira, & Darwiche,2006). idea carried first-order models create tractable subset MarkovLogic Networks (Domingos & Webb, 2012), whose network polynomial used makequeries efficient.also used fact maximization pseudo-Boolean functiontransformed maximization quadratic pseudo-Boolean function. pseudoBoolean functions interpreted factor graphs, transformation seenreduction MAP problem general binary factor graph MAP problempairwise binary factor graph. thorough study reductions inference problemsgeneral factor graphs restricted factor graph models presented EatonGhahramani (2013). Close work also idea pairwise MLNs (Fierenset al., 2013), relies transformation logical formulas compute quadraticMLN equivalent original MLN higher order. detailed comparisonapproach given Section 4.4.687fide Nijs, Landsiedel, Wollherr, & Buss2. Preliminariessection review concepts required understanding restarticle define useful notational conventions.2.1 First-Order Logic ConceptsFirst-order logic makes statements objects world. object belongscertain domain, domains seen semantic type object. Referencesobjects made use terms. Terms either constants, refer specificobject, logical variables, represent range objects, functions, mapterms terms. Like objects, logical variables constants typed, meaningrepresent objects certain domain. predicate represents relationarguments. predicate applied specific terms atom. Atoms also called positiveliterals logical negation negative literals.first-order logic formula expression involving atoms, connected connectives (, , , , , =) quantifiers (,). Atoms formulas said groundexpression contained terms constants.2.2 Notation Conventionsatom P (t1 , . . . , tn ) created applying predicate P arity n tuple terms(t1 , . . . , tn ). arguments predicate typed, associated domain.Ground atoms represented x literals (first-order ground) representedu; negation written u. Logical variables (logvars) denoted capital lettersX, Y, Z. Vectors atoms, ground atoms, terms denoted a, x, t, respectively;instance, first-order expression F involves multiple atoms written F (a) =F (a1 , a2 , . . . , ). easily go Boolean real values, logical True Falsereinterpreted represent 1 0 necessary. Replacing literal negatedequivalent, e.g., u 1 u, complementation operation. Also, superscript (),B, used specifically refer positive negated literals, e.g., u(1) = uu(0) = u. Observe u() = u(1) .substitution terms set different terms 0 according mapping0 denoted . substitution operation applied first-order expression fwritten f .ground substitution mapping C non-constant terms constant termsC. Given set logical variables L, set possible ground substitutions satisfyconstraints C written gr(L : C). case C = , number groundsubstitutions size Cartesian product domains L.2.3 Markov Random FieldsMarkov Random Field undirected graphical model definedP (x) =N1(xi ),Z688fiQuadratization Roof Duality Markov Logic Networksfactors nonnegative functions, xi tuples binary random variables,normalization constant. Normally exponential functions, quantityPZNlog (xi ) referred energy function, whose minimum defines MAPstate P (x).2.4 First-Order Probabilistic ModelsFirst-order probabilistic models way expressing probability distributions,MRFs, large degree structure. models conveniently specified usingparfactors (Poole, 2003). Parfactors composed (parametrized) potential functionconstraints valid ground substitutions parameters. parfactor grepresented tuple (C, (a)), C set constraints (a) potentialparfactor, real-valued function first-order atoms a. potential functiongiven table associates value 2n truth states atoms (Poole,2003; de Salvo Braz, 2007) function. ground substitution defines factorclique potential Markov Random Field Bayes Network. logical variablesappear parfactor denoted L, LV (a) used specifically refer logicalvariables appear atoms. set valid ground substitutions parfactorlogical variables L constraints C (discussed Section 2.4.2) denoted gr(L : C).purposes, set parfactors G defines Markov Random Field log-linear formsum groundingsXX1P (x) = expg (ag ) ,(1)ZgG gr(Lg :Cg )x contains propositional variables arise grounding first-order atoms.Markov Logic Networks first-order probabilistic models use set weightedfirst-order logic rules specify Markov Random Field. rules generally specifiedmanually capture available knowledge intuitions domain. weightscapture relative importance rules set manually learnt data.Markov Logic Networks easily described using parfactors. this, weightedfirst-order logic rule Markov Logic Network associated parfactor emptyconstraint set potential function takes value weight satisfyingassignments rule, 0 otherwise. models, (1) assigns high probabilitiesworld states satisfy many ground parfactors positive weightnegative weight.common types queries (1) marginal probabilitiespropositional variables (P (x1 ), P (x2 ), . . .) probable configuration unknown variables x = arg maxx P (x), also referred Maximum-a-Posteriori (MAP)assignment.order able ground models, take similar assumptions originalMLN formulation, namely domains assumed finite, unique names domain closure. practice also allow assume logical atoms function-free.However, although potential functions MLNs {0, w}-valued, w R, formulationallows potentials take different values R every assignment.689fide Nijs, Landsiedel, Wollherr, & Buss2.4.1 Evidencemodel may conditioned truth value certain ground atoms. symbolsPT PF represent sets ground atoms predicate P known TrueFalse, respectively. instance, P (o) PT denotes ground atom P (o)known True. case evidence available, PT PF empty. setground atoms predicate P unknown truth value represented PU . PUempty, predicate fully observed.2.4.2 Constraintsground substitutions logical variables parfactor subject constraints.representation, substitution constraint parfactor conjunctionset individual constraints associated parfactor. Disjunctions constraintsexpressed multiple parfactors representation. Constraints usedfollowing cases:1. Expressing (in)equality relation logical variables. two logical variablesX, belonging domain, X = X 6= respectively restrictground substitutions map X either differentconstants.2. Expressing ground substitutions must map logical variables elementsset objects P denoted C (t, P ), P generally one evidencegroups PT , PF , PU predicate P .Constraints principle also expressed potential function using fully observedauxiliary predicates, normally done MLN formalism. instance, X 6=enforced taking conjunction potential parfactor new fullyobserved atom AreDifferent(X, ). Similarly, constraint C (X, PF ) representedtaking conjunction potential new atom BelongsToFalseEvidenceP (X).However, expressing relationships form constraints simplifies discussionSection 3.Example 1. friends similar smoking behavior described parfactorg potential function F riends(X, ) Smokes(X) Smokes(Y ) constraint X 6=. people domain {A, B}, ground substitutions associated parfactorgr((X, ) : {X 6= }) = {(X, ) (A, B), (X, ) (B, A)}.2.5 Pseudo-Boolean Functionsfunction f : Bn R called pseudo-Boolean function. Let x = [x1 , x2 , . . . , xn ](1)(1)vector n binary variables. Consider also set literals, L := {x1 , . . . , xn ,(0)(0)x1 , . . . , xn }. pseudo-Boolean function terms expressed literals Lcoefficients ei R, = 0, . . . , n written(x) = e0 + e1 m1 (x1 ) + e2 m2 (x2 ) + . . . + en mn (xn )690(2)fiQuadratization Roof Duality Markov Logic Networksmi (xi ) monomials literals L,mi (xi ) :=()xi,ji,j .jei 0 0 < n (2), pseudo-Boolean function said posiform.many possible posiform representations pseudo-Boolean function.standard polynomials, order (or degree) pseudo-Boolean functionterm highest degree. pseudo-Boolean function expressed setpositive literals, i.e. = 1 i, called multi-linear polynomial representationXXXeij xi xj +eijk xi xj xk + . . . ,ei xi +(x) = e0 +1i<jn1i<j<knei , eij , eijk , . . . R. representation unique always obtainedanother representation eliminating negated literals using complementation x(0) =1 x(1) . Conversely, posiform always obtained pseudo-Boolean function.Complementing literal term ei mi (xi ) ei < 0 produces two new terms, oneorder e0i > 0 one one order lower e00i < 0. applyingprocedure starting highest-order terms, negative terms eliminated.3. Parfactors Pseudo-Boolean Potentialschapter describes potential functions parfactors described manipulated terms pseudo-Boolean functions, equivalent model representationstranslated pseudo-Boolean formulation. representation allows easy recognition cases inference ground probabilistic model tractable.detailed Section 3.3.3.1 First-Order Pseudo-Boolean FunctionsPotential functions parfactors defined first-order atoms. Therefore, callpseudo-Boolean function first-order atoms first-order pseudo-Boolean function.Substitutions applied individual term, pseudo-Boolean function(a) form (2) substitution(a) = e0 + e1 m1 (a1 ) + e2 m2 (a2 ) + + en mn (an ).Remark 1. notions term order different pseudo-Boolean functionsfirst-order logic. However, expect context generally clear enough avoidconfusions.3.2 Conversion Potential Representationsgeneral, potential function n variables given form table convertedpseudo-Boolean form simple technique (Boros & Hammer, 2002), creates oneterm 2n configurations nonzero coefficient table.number terms reduced converting pseudo-Boolean function691fide Nijs, Landsiedel, Wollherr, & Bussmulti-linear polynomial. However, potentials Markov Logic Networks givenfirst-order logic sentences expressed conjunctive normal form. allowstranslated directly pseudo-Boolean form. Namely, clause Ureplaced_^u=1u,(3)uUuUconjunctions replaced products. typical case single-clauseformula u1 u2 . . . un takes value w satisfied, equivalent compact pseudoBoolean representation w wu1 u2 . . . un . course, inverse procedure usedtransform potential form back logic representation.Markov Random Field results grounding model like (1) representedP (x) =1exp (T (x)) ,Zenergy function obtained summing groundings parfactorsG, normalization constant Z unknown.3.3 Tractable Classestractability MAP estimate MLN analyzed considering classestractable pseudo-Boolean functions. First, discuss classes polynomial-time optimizablepseudo-Boolean functions general case.3.3.1 Classes Tractable Pseudo-Boolean Functionssection gives overview relevant tractable classes pseudo-Boolean functions.Figure 1 illustrates relations function classes.Supermodular functions satisfy f (x1 ) + f (x2 ) f (x1 x2 ) + f (x1 x2 ), elementwise AND/OR operations binary x1 , x2 . functions maximizablestrong polynomial time (Orlin, 2009). However, recognition supermodularitypolynomials order 4 co-NP-Complete (Gallo & Simeone, 1989).Supermodular functions expressible quadratic functions written maximizations auxiliary variables quadratic supermodular functions. instance,x1 x2 x3 = maxw x1 w + x2 w + x3 w 2w, w B. functions certainstructure, expressibility recognized efficiently (Zivny & Jeavons, 2008). However, unknown whether recognition expressible functions easiergeneral supermodularity recognition problem (Zivny, Cohen, & Jeavons, 2009). Onemay try obtain equivalent quadratic supermodular function solving linearprogram (Ramalingam, Russell, Ladicky, & Torr, 2011).Polar functions (Billionnet & Minoux, 1985) supermodular functionsterm positive coefficient composed positive negative literals,e.g., x1 x2 x3 + 2x1 x2 + x2 x4 x5 . form strict subset set expressiblefunctions orders 4.692fiQuadratization Roof Duality Markov Logic NetworksUnimodular functions functions converted polar functions switching subset variables. instance, f (x1 , x2 , x3 ) = x1 x2 x3 + 2x1 x22 using switchedassociated polar function g(x1 , x2 , x3 ) = x1 x2 x3 + 2x1 xvariable x2 = 1 x2 . Undoing switching operation maximizer polarfunction gives solution original problem. Unimodular functions recognizable polynomial time (Crama, 1989). switching operations usedmake function supermodular (but polar) permutable supermodular function (Schlesinger, 2007), mainly interesting optimization functionsnonbinary discrete variables.Remark 2. f supermodular function, f submodular, thus resultsmaximization supermodular function minimization submodular functioninterchangeable. overview given section, keep context maximizationsupermodular functions, since reflects many original publications.Permutable SupermodularSupermodularPermutable SupermodularExpressibleUnimodularExpressiblePolarPolarSupermodularUnimodularFigure 1: Relations classes functions. Left: pseudo-Boolean functionsorder. Right: third-order (cubic) case. Shaded regions optimizablemaximum flows.main interest expressible supermodular functions quadratic supermodularfunctions optimized computing maximum flow O(n3 ), considerablyfaster O(n6 ) complexity general supermodular maximization. However,process transforming expressible supermodular function equivalent quadraticrepresentation introduces additional variables. Consequently, k auxiliary variablesintroduced, true complexity O((n + k)3 ).Note although recognition general supermodularity expressibilityhard problems, classes closed conical combinations, sumssupermodular functions also supermodular.3.3.2 Tractable Parfactor ModelsAlthough inference NP-hard general, possible guarantee certain inference tasks tractable restricting expressiveness formalism potential functions (Domingos & Webb, 2012). Translating results classespseudo-Boolean functions above, possible easily identify models tractable MAPinference. this, following result usedProposition 1. parfactors expressible potentials, maximization groundmodel expressed maximization supermodular quadratic function.693fide Nijs, Landsiedel, Wollherr, & BussProof. follows facts a) definition, expressible functions writtenmaximization supermodular quadratic pseudo-Boolean function b) sumssupermodular functions also supermodular.Consequently, potentials converted supermodular expressible pseudoBoolean function, model optimized computing maximum flow.result applies potential, {0, w}-valued potentials used MLNs.hence possible consider potentials defined table taking 2ndistinct values. However, flexibility makes harder enforce expressibility designlevel. classes logic rules used MLNs guaranteed expressible,used recognize design MLNs tractable MAP problem.3.3.3 Polar MLNsLet = {a1 , a2 , . . . , } set first order atoms, J {1, 2, . . . , n} B.logic potentials form^ () ^ (1)aiaj.(4)iIjJtransformed equivalent polar posiform following casesJ = . case (4) single conjunction pseudo-Boolean form()ai .iIJ 6= . conjunctions conflict. case disjunctionreplaced sum without changing underlying truth table expression.equivalent polar posiform() (1)ai +aj.iIjJJ = |J|= 1. easy see^ ()^(1)ai aj=iI()ai(1)aj,iI{j}employed transform expression previous case.MLNs parfactors polar tractable MAP problemcomputed maximum flow.Example 2. MLN potential positive weight potential (P (X) Q(Y ) (Z))(R(X) (Z)) form (4) = 1, = {P (X), Q(Y ), (Z)}, J = {R(X), (Z)}.J = {T (Z)} 6= , equivalent polar potential P (X)Q(Y )T (Z) +R(X) (Z).694fiQuadratization Roof Duality Markov Logic Networks3.3.4 Unimodular MLNsUnimodular functions also solved efficiently converted polar functions. Finding variables need switched obtain polar function performedpolynomial-time (Crama, 1989). recognition procedure may efficient performed first-order level, would require describe switches ground atomsgrounding. show switches ground variables alsorepresented compactly first-order level, makes possible decide whethermodel unimodular without analyzing ground model. Define switching operationreplacing literal l = P (t)() expression P (t)(1) , P new predicate.new literal switched predicate interpreted referring switched ground atoms.However, representation may become inconsistent ground atoms representedswitched non-switched atom intersect, ground atom switchtreated independent variables.Definition 1 (Shattering). (de Salvo Braz, 2007) set parfactors G shatteredgroundings every pair atoms appearing parfactors G either identicaldisjoint.model shattered partitioning parfactors (de Salvo Braz, 2007).Definition 2 (Consistent switch). shattered model switched atoms consistentevery pair atoms identical groundings, either neither atomsswitched.shattered model, inconsistencies avoided ensuring switchingoperation preserves consistency. Namely, starting shattered modelswitched atoms, switching atoms groundings ensures consistencyrequirement fulfilled times. switching sets parfactors polar,ground model unimodular polar representation directly available.Example 3. Consider model G parfactors (, P (X)Q(Y ) (Z)) (,Q(X)T (Y )). model shattered obtain switched model G0 parfactors (, P (X)Q(Y ) (Z)) (, Q(X) (Y )). switching termsparfactors either positive negative literals assume nonnegativevalues, ground model polar pseudo-Boolean function.Unfortunately, shattering condition strong enough partitioning parfactors guarantee unimodular model recognized. example, modelsingle parfactor ({X 6= }, F (X, )F (Y, X)) shattered shownswitching set makes polar, made polar switchingatoms. finer partitioning parfactors one given shattering may allowrepresent desired switching set. However, find partitioning generalcase unknown.3.4 Non-tractable Casefollowing discuss optimization pseudo-Boolean functions fallwithin described tractable classes. observed multiple linear programming relaxations quadratic pseudo-Boolean problems optimum (Boros &695fide Nijs, Landsiedel, Wollherr, & BussHammer, 2002), known roof dual bound. generally optimistic boundtrue optimum problem, linear programming relaxes integrality constraintvariables. roof dual bound also obtained efficiently solvingmaximum flow problem specially constructed network. minimization setting,network represents tightest submodular relaxation original quadratic function (Kahl & Strandmark, 2012). addition lower bound, solution relaxedproblem gives persistencies. Persistencies assignments subset variablesform part least one optimal solution. persistencies found full setvariables, form minimizer problem. Otherwise, problem simplifiedfixing persistencies value produce smaller problem preservesminimum.preprocessing step reduces size problem solves completely,combined optimization method. approach known Quadratic Pseudo-Boolean Optimization (QPBO) (Kolmogorov & Rother, 2007) computervision literature.Furthermore, network used compute initial roof dual boundpersistencies used search additional persistencies approximateconfiguration remaining variables means probing (Boros, Hammer, &Tavares, 2006) improving (Rother, Kolmogorov, Lempitsky, & Szummer, 2007) techniques. Probing heuristically chooses variable x residual problem recomputesmaximum flow assumptions x = 0 x = 1. Analyzing valueremaining variables assumptions, may possible find additionalpersistencies improve lower bound. improve method fixes subset variables given assignment efficiently searches configurations remainingvariables improve cost respect original assignment. procedureexecuted multiple times guaranteed decrease quality approximatesolution.described techniques applicable quadratic problems. next sectionconcerned models described purely pairwise interactionsvariables. models, use quadratization techniques employed,enables use QPBO algorithm extensions problems. Also,connection approach alternative based generalized roof dualityshown Section 4.2.4. Quadratizationsection quadratization pseudo-Boolean functions reviewedshown methods applied parfactor models. quadratization procedurealso known order reduction.4.1 Quadratization Pseudo-Boolean Functionquadratization pseudo-Boolean function (x) new function (x, w)(x) = min (x, w),w696(5)fiQuadratization Roof Duality Markov Logic Networks(x, w) quadratic pseudo-Boolean function defined auxiliary slack variablesw.(x), always exists (x, w) satisfying (5) (Rosenberg, 1975; Ishikawa,2011). Furthermore, (x) belongs expressible set submodular functions describedZivny et al. (2009), submodular (x, w) also found. quadratization,problem finding minx (x) replaced quadratic problem minx,w (x, w).Example 4. function f (x1 , x2 , x3 ) = x1 x2 x3 quadratizationmin (x1 , x2 , x3 , w) = min x1 w x2 w x3 w + 2w.wwverified minimizing (x1 , x2 , x3 , w) respect w assignment(x1 , x2 , x3 ) B3 verifying evaluates 1 (1, 1, 1) 0 everywhere else.minx1 ,x2 ,x3 f (x1 , x2 , x3 ) conveniently computed quadratic optimization minx1 ,x2 ,x3 ,w (x1 , x2 , x3 , w). quadratization obtained ISH techniquediscussed following.general, quadratization necessarily determined whole function once;instead, single multiple terms specific algebraic form replaced stepequivalent quadratic representation. substitutions introducesminimization independent slack variables, minimizations distributedwhole expression, final quadratization joint optimizationslack variables.given (x) many possible ways obtain quadratization. practice,quadratizations a) slack variables, b) positive quadraticthus non-submodular terms c) easily computable. Condition a) importantquadratization needs applied many times, e.g., many higher order terms;b) number magnitude experimentally known correlate complexityresulting optimization (Kolmogorov & Rother, 2007; Gallagher, Batra, & Parikh, 2011);c) satisfy conditions, finding quadratization may become complex.instance, finding quadratization smallest number slack variables NPcomplete problem approaches (Boros & Hammer, 2002).4.2 Quadratization TechniquesQuadratization techniques pseudo-Boolean functions rely identifying subexpressionsmatch template quadratic form known. Potentials knownexpressible quadratized using submodularity-preserving functions presentedZivny Jeavons (2008). following review existing techniques validpseudo-Boolean function introduce new quadratization.4.2.1 General Quadratization Individual Terms (ISH)technique used quadratize individual higher-order term. generalformulas given Ishikawa (2011), restricted cases given KolmogorovZabin (2004) Freedman Drineas (2005).697fide Nijs, Landsiedel, Wollherr, & Bussbinary variables x1 , . . . , xd negative coefficient (a < 0),(1 )ax1(d )xd()= min aw{S1wB(d 1)}.Example 4 illustrates case cubic function. positive coefficient (a > 0)quadratization( )( )ax1 1 xd=andXminw1 ,...,wnd B()wi (ci,d (S1()+ 2i) 1) + aS2i=1definitions()S1=X( )xi ,()S2i=1=d1 Xd1X( ) ( )xi xj ji=1 j=i+1(1,d1, ci,d =nd =22,()()(S1 1)= 12odd = ndotherwisereduction negative term always performed single slack variable,results quadratic submodular function. positive terms, number slackvariables nd proportional 21 d. general, resulting function submodularquadratization creates positive quadratic terms. However, termsinvolve slack variables, may canceled existing quadratic terms oppositesign. fact, seen quadratization always submodularity-preservingcubic functions (Zivny & Jeavons, 2008). hand, original functionsubmodular, quadratization may produce non-submodular terms worseexperimental results methods.4.2.2 Asymmetric Quadratization (ASM)(Gallagher et al., 2011) asymmetric quadratization term ax1 x2 x3 > 0givenax1 x2 x3 = min a(w x2 w x3 w + x1 w + x2 x3 ).wobtained transforming ax1 x2 x3 ax2 x3 ax1 x2 x3 using ISHmethod. Observe left-hand side equality symmetric, right-handside not. Reordering variables thus creates three different quadratizations. Also,right-hand side two non-submodular terms (compared three ISHmethod), one involve slack variable w. existing termnegative coefficient variables non-submodular term without w,combined may cancel out, leaving single non-submodular term. Asymmetricquadratizations created terms order using similar procedure.possibility eliminating non-submodular terms motivates search combination quadratizations optimizes certain cost representing qualityquadratization. shown total magnitude non-submodular termsgood cost function (Gallagher et al., 2011). call ASM procedure minimizing698fiQuadratization Roof Duality Markov Logic Networksquantity finding quadratization expression, term selectone asymmetric ISH quadratic forms. Note that, propositionallevel, number terms may large, resulting large optimization problemchoice quadratization per term.4.2.3 Preprocessing Positive Terms (FIX)(Fix, Gruber, Boros, & Zabih, 2011) approach seen preprocessing methodavoid quadratization ISH positive terms. transforms multiple positive higher-orderterms common subset variables negative higher-order terms. summarizeprocedure case common subset variables consists single variable.Consider set terms contain common variable x1 , Hpositive coefficient H > 0. Then, holds assignment binary variablesx1 , . . . , x n!XHSHjHxj = minXw{0,1}HXx1 w +HSHSHxjjH\{1}XHSH wxj .jH\{1}Observe last sum terms order left-hand sidenegative sign, positive terms lower order. transformationrepeatedly applied positive higher-order terms eliminated.point ISH quadratization negative terms applied, introducing one additional slackvariable per term. method decide common variable performtransformation, x1 thus selected arbitrarily.4.2.4 Generalized Roof Duality (GRD)new quadratization method based generalized roof duality theory (Kolmogorov, 2012) practical implementation (Kahl & Strandmark, 2012). nonsubmodular, higher-order function (x), approach finds submodular relaxation (x, y)(x) = (x, x)(x, y) = (y, x)(symmetry)submodular expressible.(6)constraints, solution relaxed problem provides lower boundpersistent assignments variables, similar roof dual relaxation quadraticfunction. restricting search relaxations expressible functions, resultoptimized solving maximum flow problem. following proposition presentslink submodular relaxations computed GRD quadratizations.proposition used compute quadratization function roof dualbound equivalent GRD bound function.Proposition 2. Let (x, y) relaxation (x) satisfies (6). existsquadratic (x, w) s.t. a) roofdual((x, w)) minx,y (x, y), b) minw (x, w) = (x).699fide Nijs, Landsiedel, Wollherr, & BussProof. Let (x, y, w) submodular quadratization (x, y). Define10 (x, y, w, v) = ((x, y, w) + (y, x, v)).2easy check minw,v 0 (x, y, w, v) = (x, y). Also, (x, y, w) quadratic submodular, easy see (y, x, v) must also submodular (negativequadratic terms multi-linear form). Consequently, 0 (x, y, w, v) submodular.a) Let (x, w) = 0 (x, x, w, w) verify0 (x, y, w, v) = 0 (y, x, v, w)Thus, 0 (x, y, w, v) relaxation (x, w) conditions (6). However, roof dualbound known larger submodular relaxations (Kahl & Strandmark,2012),roofdual((x, w)) min 0 (x, y, w, v) = min (x, y).x,y,w,vx,yb) follows1min (x, w) = min 0 (x, x, w, w) = min ((x, x, w) + (x, x, w)) = (x, x) = (x).w 2wwCondition b) Proposition 2 ensures (x) quadratization (x), conditiona) ensures roof dual bound quadratization least relaxation(x, y).Example 5. cubic term x1 x2 x3 compute third-order submodular relaxation using (Kahl & Strandmark, 2012). ISH quadratization submodularitypreserving cubic functions, method used make relaxation quadratic,procedure Proposition 2 results1(w1 x1 + w1 x2 w1 x3 w1 w2 x1 w2 x2 + w2 x32+ w2 + 2x1 x2 x1 x2 + x3 + 1).x1 x2 x3 = minw1 ,w2relaxation obtained using generalized roof duality theory expressedquadratization function. practice, method allows us use GRD approachdesign quadratizations specially suited potentials hand.4.3 Quadratization MLNsquadratization MLN expresses probability distribution using parfactorsquadratic pseudo-Boolean potentials optimization slack atoms.Definition 3 (First-order quadratization). Let (a) pseudo-Boolean potential appliedfirst-order atoms a, construct l = (l1 , l2 , . . . , l|LV (a)| ), li LV (a) arbitraryordering. quadratization , (x) = minw (x, w1 , w2 , . . . , wk )700fiQuadratization Roof Duality Markov Logic Networksdefine first-order quadratization (a) = minb (a, b1 , b2 , . . . , bk ), bi , =1, 2, . . . , k slack atoms. Slack atoms created using new predicate Bi arity|LV (a)| every slack variable wi bi = Bi (l1 , l2 , . . . , l|LV (a)| ), = 1, 2, . . . , k.definition allows compactly represent individual quadratizations manyground potentials similar structure. Also, particular form originalquadratic representation assumed, quadratization technique used. instance, applying ISH quadratization first-order potential P (X)Q(X, Z)Q(Y, Z),Example 4, results quadratization P (X)B1 (Y, X, Z) Q(X, Z)B1 (Y, X, Z)Q(Y, Z)B1 (Y, X, Z) + 2B1 (Y, X, Z).obtain convenient quadratization whole model following remarksuseful.Remark 3. quadratization performed new first-order slack predicates,different parfactors never grounded expressions ground slack atom.Remark 4. Slack predicates applied logical variables appearing expressionquadratized. Consequently, two ground substitutions , 0 create groundslack atoms also define minimizations function: b = b0 (a, b) =(a, b)0 .definition first-order quadratizations, express quadratizationpseudo-Boolean probabilistic first-order logic model parfactors GXX1P (x) = exp(ag )ZgG gr(Lg :Cg )XX1= exp(ag )ZgG gr(Lg :Cg )XX1= expmin g (ag , bg )bgZgG gr(Lg :Cg )XX1= expmin g (ag , bg )(7)bgZgG gr(Lg :Cg )XX1= exp ming (ag , bg ) ,(8)wZgG gr(Lg :Cg )optionally expressed maximizationP (x) =1exp maxwZXXgG gr(Lg :Cg )701g (ag , bg ) ,fide Nijs, Landsiedel, Wollherr, & Bussw contains ground slack variables {bg | gr(Lg : Cg ), g G}. step(7) (8) follows sets variables produced different grounding substitutions either identical disjoint. different parfactors, minimizationsalways independent, implied Remark 3. single parfactor g two groundings minbg (ag , bg ) minbg 0 (ag 0 , bg 0 ), either bg 6= bg 0 bg = bg 0 .first case, combine two independent minimizations minbg ,bg 0 (ag , bg ) +(ag 0 , bg 0 ). second case, Remark 4 implies minbg 2(ag , bg ).4.4 Relation Pairwise MLNsconcept pairwise MLNs (Fierens et al., 2013) one presentedproduce new model quadratic parfactors, cost introducing additionaloptimization slack variables. pairwise MLN approach shows MLNtransformed max-equivalent MLN quadratic clauses, conceptmax-equivalence similar (5). review approach, first assumeclauses three literals. case, clause three literalsrewritten new clauses positive normal form, conjunctions threepositive literals. form, transformation suggested splits conjunctionthree positive literals new rules involve two atoms use newauxiliary slack atom. third-order case, procedure described specialcase approach. Namely, show equivalent using ISH quadratizationSection 4.2.1 MLN pseudo-Boolean form.Consider MLN rule weight > 0 parfactor (, (, P (t1 )Q(t2 )R(t3 ))).pairwise MLN approach would transform parfactors quadratic potential functions{(, P (t1 ) B(t4 )), (, Q(t2 ) B(t4 )), (, R(t3 ) B(t4 )), (2, B(t4 ))}.(9)parfactor also represented pseudo-Boolean form (, (P (t1 )Q(t2 )R(t3 ))).allows use ISH quadratization presented Example 4P (t1 )Q(t2 )R(t3 ) = (P (t1 )Q(t2 )R(t3 ))= min (P (t1 )B(t4 ) R(t3 )B(t4 ) Q(t2 )B(t4 ) + 2B(t4 ))B(t4 )= max (P (t1 )B(t4 ) + R(t3 )B(t4 ) + Q(t2 )B(t4 ) 2B(t4 )).B(t4 )observed terms potential function correspond one-toone ones created pairwise MLN approach (9). fact, decomposingparfactor new ones quadratic terms would give formpairwise MLN. analogous procedure case < 0 showsspecial case three literals, pairwise MLNs replicated pseudo-Boolean formexpressing formulas multi-linear form using ISH quadratizationthird-order case.Although ISH quadratization often recommended computer vision literaturecase > 0 (in maximization setting), known leadtightest roof dual relaxations < 0 (Fix et al., 2011; Gallagher et al., 2011).702fiQuadratization Roof Duality Markov Logic Networksexperimental section extend observations problems domain probabilisticlogic verify quadratization methods lead improvements inference quality.model contains clauses n > 3 literals, pairwise MLN approach suggestsuse auxiliary atoms recursively transform clauses new ones n 1 literals,clause three literals. instance, clause n = 4 literalstransformed two clauses three literals, one infinite weight.point, procedure applied third-order clauses, creatingtotal 3 slack atoms.contrast, approach works pseudo-Boolean representation potential.case, additional preprocessing first transform clause third-orderrequired, quadratizations pseudo-Boolean functions order. Recallclauses compactly represented pseudo-Boolean form using (3). Consequently,ISH method used quadratize clauses n literals either one n2 slackatoms, depending sign whether optimization formulated maximizationminimization. particular case clause four literals singleslack atom required. slack atom many propositionalizations,reducing number slack atoms creates large difference number propositionalslack variables. Finally, approach create hard rules, lead badconditioning optimization algorithms.4.5 Normalization Parfactors QuadratizationConsider parfactor (, P (X)P (Y )Q(Z) + P (Z)Q(Z)) true evidence predicateQ, QT 6= . Although potential cubic general, particular groundsubstitutions quadratic. case groundingsQ(Z) QT , cases potential simplifies P (X)P (Y ) + P (Z). potential also quadratic ground substitutions X = ,becomes P (X)Q(Z) + P (Z)P (Z). Finally, groundings = Z cubicpotential P (X)P (Z)Q(Z) + P (Z)Q(Z), factorizes (P (X) + 1)P (Z)Q(Z).groundings, quadratization exploiting structure potential could computed.examples highlight structure potentials change particulargroundings, might even require slack variables become quadratic.motivates putting parfactors form potentials structuregroundings. effect normalization reduction number slackvariables possibility compute quadratizations tailored different formpotentials. achieved combining two types splittings parfactors,creating equivalent first-order representation makes different forms potentialsexplicit first-order level.4.5.1 Splitting Atomsfirst preprocessing method simplifies parfactor incorporating evidence atomP (t). proceeds replacing parfactor three new ones, additionalconstraint C (t, PT ), C (t, PF ), C (t, PU ). together constraints cover possible ground substitutions atom, new parfactors produce groundingsoriginal one, ground model changed. Importantly, potentials703fide Nijs, Landsiedel, Wollherr, & Bussparfactors constraints C (t, PT ), C (t, PF ) simplified replacing atomP (t) True False, respectively.Definition 4 (Fixed atoms). atom predicate P fixed C (t, PT ), C (t, PF )C (t, PU ) constraint set C.parfactor n atoms, fixing atoms create 3n new parfactors.practice, number smaller a) potential may become 0 constantsimplification (Shavlik & Natarajan, 2009) b) sets PT , PF , PU mayempty groundings associated it.4.5.2 Splitting Logical VariablesAtoms constructed predicate may ground ground atomspecific grounding substitutions. example, parfactor (, P (X)P (Ann)) representsquadratic potential ground substitutions X except = {X Ann},becomes linear potential. simplifications identified groundingobserving form atoms predicate may unify, splittingparfactor accordingly. example creates parfactors (X 6= Ann, P (X)P (Ann))(, P (Ann)). Splitting logical variables repeated unificationatoms longer possible. ensures valid ground substitution parfactormaps atoms parfactor distinct ground atoms.Example 6. Consider parfactor (, P (X, )P (X, Z)Q(X) S(Z)) assumepredicates arguments domain D. knowledge available P , i.e., (PT = ,PF = , PU = D), evidence groundings Q, i.e., (QT D,QF = ,QU = \ QT ), full knowledge groundings S, i.e., (ST D, SF D,SU = ).start splitting parfactor atom S(Z), fully observed. caseS(Z) constrained True cancels potential left out. SU = ,case constraint C (X, SU ) also ignored. S(X) constrainedFalse, potential simplifies P (X, )P (X, Z)Q(X). Proceeding similarlyatoms, remains one parfactor simplified potential additional constraintsC (Z, SF ), C (X, QU ), C ((X, ), PU ), C ((X, Z), PU ).fixing atoms, observe two atoms constructed predicate Pproduce ground substitutions = Z. Splitting condition produces twonew parfactors, potential simplifies P (X, )Q(X) case = Z.steps, original parfactor transformed two new parfactors:(a) =P (X, )P (X, Z)Q(X)C ={C (Z, SF ), C (X, QU ), C ((X, ), PU ), C ((X, Z), PU ), 6= Z}(a) =P (X, )Q(X)C ={C (Y, SF ), C (X, QU ), C ((X, ), PU )}704fiQuadratization Roof Duality Markov Logic NetworksNote preprocessing step, potential function gone quarticcubic function. Computing quadratization potential original formsubstituting possible groundings produces |D|3 ground slack atoms. contrast,splitting variables results fewer ground substitutions parfactor, sincecurtailed constraints (and number ground slack variables).4.6 Benefits Limitations First-Order QuadratizationExcept ISH method, introduced methods compute quadratization consideringinteractions terms expression. considering interactions,methods create quadratization whose roof dual relaxation tighter, producing betterbounds persistencies.However, methods also need additional processing problem.processing performed propositional model, may become large. instance,Gallagher et al. (2011) suggested implement ASM method constructing secondMarkov Random Field variables represent possible quadratizationsterm. Also, approach based generalized roof duality needs solve auxiliarylinear program find best submodular relaxation. FIX approach solveoptimization, needs perform multiple transformation steps model,clear order perform them.Instead, MLNs, general procedure presented Section 4 used obtainquadratic MLN applying quadratization techniques individual parfactors. advantage procedure computational cost obtaining quadratizationindependent size ground problem. Furthermore, parfactors providecompact description structure ground model, exploitedsophisticated quadratization methods. auxiliary problems requiredquadratization methods become extremely small solved individual parfactors,still produce good quadratic potential groundings parfactor.hand, parfactor representation model unique, thusresulting quadratic MLN may depend manipulations performed parfactors.fact, shattering model may give representative template structureground problem. work, merge parfactors constraintspreprocessing step, take steps partition combineparfactors. Section 5 evaluate quadratization methods separately.However, practice methods may combined restrictions, possiblecombination different methods may give best results.4.7 Quadratize-Solve-Simplify-Repeat (QSSR)QPBO algorithm applied general quadratic minimization problem,variables persistency found. discussed Section 3.4,cases probe improve procedures used increase number persistenciescompute approximate solution. However, computational cost makeinefficient problems large number unsolved variables. quadratized models,mitigated observing many slack variables longer necessarycomputing persistencies given roof duality. fixing variable705fide Nijs, Landsiedel, Wollherr, & Bussx persistent value higher-order problem simplifies cancels terms containing x. Thus, constructing new problem incorporates informationsolved variables much smaller number slack variables, used obtainadditional persistencies. procedure quadratizing model first-order level,solving problem partially QPBO algorithm simplifying higher-orderproblem obtained persistencies repeated additional persistenciesfound. denote iterative computational procedure Quadratize-Solve-SimplifyRepeat (QSSR). using algorithm experiments, stop problemsolved first simplification, additional iterations generally produceadditional persistencies comparison probing. Kahl Strandmark (2012) usedsimilar procedure iteratively solving fixing variables context computinggeneralized roof dual bound.5. Computational Experimentspresented quadratization methods evaluated impact performanceQPBO algorithm extensions. performance QPBO-based inferencealso evaluated problems quadratization required. Finally compareoverall pipeline existing inference engines.5.1 Datasetsevaluate approach various standard MLNs datasets well additionalproblems. characteristics problems summarized Table 1. first setdatasets similar ones employed evaluation state-of-the-art enginesTuffy (Niu et al., 2011) RockIt (Noessner et al., 2013). link prediction problemUWCSE dataset (LP) tries find relations faculty members students.relational classification (RC) Cora dataset determines category researchpapers. information extraction (IE) problem models obtain dataset recordsparsed sources. webKB dataset used predict university departmentwebsite belongs, given hyperlink relations contained words (KB). entity resolution (ER) problem Cora dataset obtained Alchemy website.goal problem identify citations referring paper. trainedmodel available problem, trained Alchemy using first fiveavailable splits evaluation (Singla & Domingos, 2006a). Friends smokers (F&S)common test model social network friendship relations, smoking habitscancer occurrences. Evidence generated described Singla Domingos (2008)domain size 200 persons. F&S problem relatively simple, additionalproblem weights formulas negated also considered (-F&S).order gain broader insight performance inference algorithmshigher-order problems, created two additional third-order problems. first onebased KB problem webKB dataset mentioned (KB3).original KB inference problem uses words contained page contents well linkstructure infer page categories, third-order problem webKB dataset createdquerying class page, also jointly inferring linkspage, solely word tokens appearing page. Learning performed706fiQuadratization Roof Duality Markov Logic NetworksAlchemy, size problem reduced inference performedatoms True ground truth number randomly sampledatoms.second new third-order problem image denoising (ID) model, triesrestore noisy binary image. rules indicating observed valuepixel correspond denoised value two rules indicating groups threehorizontally vertically neighbouring pixels take value. easy seeassociated MAP problem rules fall within described cases MLNswhose rules converted polar pseudo-Boolean functions described Section 3.3.2,thus solved exactly. unary rules given weight 1.0. ensureterms smoothing rules cancel out, rule pixels given weight0.35 rule pixels 0.3. 90 90 pixels random binary image usedevidence, pixel 50% chance off.5.2 Enginescompare approach MAP-inference solvers Alchemy, Tuffy RockIt.Alchemy original solver MLNs and, contrast engines,use relational database ground model, lead long grounding times.Alchemy Tuffy optimize ground model using MaxWalkSAT, stochastic searchtechnique made scale well large problems. RockIt uses ILP solverexploits symmetries model reduce number constraints.number constraints may large, takes iterative approachconstraints violated current solution added solver.make sure problem solved implementations,preprocessing required. First, formulas existential clauses ignored formulas converted conjunctive normal form. Then, Tuffy internally transformsformulas negative weight approximate formula positive weight, apply transformation. Unfortunately, transformation appliedhigher-order problems, reduces order formula. Lastly, ER KB3problems use method compactly specify ground atoms query, assumesquery atoms False. query variables, also known canopies (Singla& Domingos, 2006a), used eliminate large number uninteresting variables,created using cheap distance metric (McCallum, Nigam, & Ungar, 2000). neither Tuffy RockIt support input format, given extensivelist False evidence atoms instead.5.3 Results Quadratic Problemsquadratic problems literature, analyze performance QPBO additional persistencies computed probing extension described Section 3.4. Table 2shows QPBO algorithm gives persistent solution variables, evenprovides exact solution KB problem. probe procedure also solves IEproblem exactly, still leaves unsolved variables RC LP problems.general, inference times problems extremely short.707fide Nijs, Landsiedel, Wollherr, & Buss5.4 Comparison Quadratization Methodshigher-order problems performance described quadratizationmethods evaluated. includes pairwise MLN approach, equivalentISH quadratization problems cubic potentials. potentials parfactorsexpressed multi-linear polynomial quadratizing model.First evaluate number persistencies obtained differentproblems Table 3. expected, submodularity, ID problem completelysolved methods. Friends Smokers creates non-submodular terms,also solved exactly methods. remaining problems solvedmethods, cases small number variables fixed. general,observed methods aware terms potentialproduce better results ISH, applies fixed transformation. final probingstep computationally expensive, may significantly increase numbersolved variables, even solve problems exactly. noted stepimportant even approximate solution variables subsequently obtainedusing improve method. Otherwise, number persistencies small, improvemethod needs operate model potentially many variables, also needsoptimize slack variables stemming remaining higher-order terms.Table 1: Summary characteristics described datasets associated groundnetworks grounded higher-order form multi-linear representation. Trivially satisfied dissatisfied factors ignored.IEKBRCLPIDF&S -F&S KB3ERFormulas10241061524466661331Domains433811135Query Predicates211113324Observed Predicates16232120016Ground atoms336670 9079 96504624 8100 40180 40180 8190 10948Factors351001 31283 58485 161806 55800 127982 127982 22627 910670Higher order factors0000 15840 32220 32220 6736 424580708fiQuadratization Roof Duality Markov Logic NetworksTable 3: Percentage variables solved. Step 1) Initial QPBO result 2) QPBO resultQSSR simplification 3) Probe. Inference time seconds step parentheses. () completeStepISHFIXASMGRDID1100.0 (0.01)100.0 (0.01)100.0 (0.01)100.0 (0.01)F&S12100.0 (0.02)100.0 (0.82)100.0 (0.86)99.6 (1.0)100.0 (0.0)-F&S12319.4 (1.79)19.4 (1.61)19.419.4(0.9)(0.78)99.6 (0.42)99.6 (0.02)99.6 (2.66)99.6 (1.11)99.6 (0.02)99.6 (2.54)KB312355.0 (0.06)56.5 (0.04)65.8 (19.42)82.4 (0.03)86.8 (0.01)100.0 (0.1)82.3 (0.02)86.6 (0.01)100.0 (0.05)60.6 (0.08)62.7 (0.04)96.7 (5.59)ER12392.1 (0.46)92.3 (0.03)92.8 (7.6)91.9 (1.04)92.3 (0.04)93.0 (162.36)95.0 (0.47)95.0 (0.02)95.3 (5.57)95.4 (1.49)96.1 (0.03)96.6 (7.08)5.5 Approximate Inferencealso compared quality approximate solutions enginestotal running times. problems formulated minimizations, solutionsengines evaluated ground model. Table 4 observedquadratic problems, engines achieve optimal costs, knownoptimality guarantee given QPBO Table 2 small MIP gapused RockIt. exception LP problem, solver used RockItproblems obtaining tight bound, Tuffy QPBO+I provide better solutions.higher-order problems, ASM quadratization achieves best costlowest computation time cases. Using GRD reduction performs slightly worse,possibly quadratization improve step needs executedTable 2: Percentage persistencies given QPBO algorithm usingprobing technique different quadratic problems.PersistenciesPersistencies (probe)Qpbo time (s)Probe time (s)IEKBRCLP99.871000.0300.15010090.3090.300.0060.02185.5886.220.0694.8007090.002fide Nijs, Landsiedel, Wollherr, & Bussvariables. Tuffy perform well higher order problems, possiblyinternal transformation uses approximation original formula.noted computation times affected multiple factors. WhereasAlchemy, Tuffy approach make clear distinction grounding inference, RockIt uses cutting plane algorithm incrementally grounds factorssatisfied current solution, leads large speedups many factorseasily satisfied solution largely homogeneous. hand, IDproblem example approach produces considerably longer running times.Another factor ability specify evidence form canopies, allowsrelational database execute queries grounding efficiently.Table 4: Resulting cost different engines various quadratic higher-order problems. Alchemy Tuffy run increasing number flipssignificant advances made. RockIt run relative gaps 1 10n ,n = 9, 8, . . . convergence achieved within hour. comparedmethod using ASM GRD quadratization higher-orderproblems, using improve residual problem advances made20 iterations. Total running times seconds parenthesis. (*) Guaranteedoptimal cost persistencies () ground within 1 hour.AlchemyTuffyRockItQPBO+IIEKBRCLP-111113.5(162)-480.8(119)-4511.6(17)-111274.1 (115)-4031.7(17)-686.3(424)-4511.6-111312.4*-4031.8-507.7IDF&S-F&SKB3ER1772.7 (442)-3.8(159)-182338.7 (47)21.1(543)-10739.5 (551)1784.2(25)-4.2*(3)-191856.9(3230)-1045.3(308)-14128.9 (433)-1003.8* (244)-4.2*(5)-185267.3(8)-1492.8(256)-15271.3 (1902)(19)(27)(11)(13)-4511.6(22)-111312.4*(6)-4031.8(9)-732.6(9)ASM+QPBO+I-1003.8*(5)-4.2*(6)-193715.3(12)-1484.4(57)-15430.7(101)GRD+QPBO+I-1003.8*(6)-4.2*(9)-193715.3(14)-1476.9(101)-15430.5(113)Figure 2, evolution cost ER problem running timeimprove shown different quadratizations. problem, methods convergesolution similar costs, convergence much faster cases ISHGRD quadratizations used.710fiQuadratization Roof Duality Markov Logic Networks15100ASMFIXGRDISH1515015200Cost15250153001535015400154501234Improve (s)Figure 2: Cost higher-order ER model function time spent improve method, using different quadratization techniques. Improve startssolving one iteration original problem removing redundant slacks,described Section 4.7.6. Conclusions Future Workarticle discussed use pseudo-Boolean functions, quadratization techniques, MAP inference methods based roof duality MLNs. shownquadratization method pseudo-Boolean functions employed MLNs firstorder level, generalizing previously existing approach. enables use quadratization methods exploit structure problem without need solve possiblylarge auxiliary optimization problem. Various quadratization approaches adaptedwork first-order models, including novel approach leverages connectiongeneralized roof duality theory.Additionally, best knowledge, first work discussesrecognition (super-)submodularity expressibility MLNs. particular, allowsguarantee expressibility restricted set MLNs. Although class potentialsguaranteed expressible using submodular quadratic functions limited,potentials seen wide applications computer vision.presented techniques extensively evaluated problems literaturewell various additional problems. higher-order problems, choice quadratizationapproach shown important factor quality results. methodsexploit first-order representation problem often enabled QPBO solvelarger part problem perform better approximate inference. cases, largeparts problems could solved exactly, approximate solutions matchedimproved quality solvers. optimization times problemsobserved small, often much shorter rest pipeline,optimize. total, timings show competitive results relationstate-of-the-art inference engines.711fide Nijs, Landsiedel, Wollherr, & Bussvarious paths future work. work explored performanceimproved partitioning model presented shattering techniques,using manipulations parfactors. Also, possibility obtain betterapproximate solutions using move making algorithms (Lempitsky, Rother, Roth, & Blake,2010) tested, algorithms may leverage first-order representationfinding good moves. additional interesting aspect consider integrate approach cutting plane techniques lifting techniques, expected givesignificant performance benefit problems large number factors.Acknowledgmentsauthors also wish thank anonymous reviewers valuable commentshelpful suggestions. research leading results partly received funding European Research Council European Unions Seventh FrameworkProgramme (FP/2007-2013) / ERC Grant Agreement no. 26787, Project SHRINE,Technische Universitat Munchen - Institute Advanced Study (www.tum-ias.de), fundedGerman Excellence Initiative.ReferencesApsel, U., & Brafman, R. I. (2012). Exploiting uniform assignments first-order MPE.Proc. Conf. Uncertainty Artificial Intelligence, pp. 7483.Beedkar, K., Del Corro, L., & Gemulla, R. (2013). Fully parallel inference markov logicnetworks. 15th GI-Symposium Database Systems Business, TechnologyWeb, Magdeburg, Germany. Bonner Kollen.Billionnet, A., & Minoux, M. (1985). Maximizing supermodular pseudoboolean function:polynomial algorithm supermodular cubic functions. Discrete Applied Mathematics, 12 (1), 111.Boros, E., Hammer, P. L., & Tavares, G. (2006). Preprocessing unconstrained quadraticbinary optimization. Tech. rep., Rutgers Center Operations Research.Boros, E., & Hammer, P. (2002). Pseudo-boolean optimization. Discrete Applied Mathematics, 123 (1), 155225.Crama, Y. (1989). Recognition problems special classes polynomials 01 variables.Mathematical Programming, 44 (1-3), 139155.Darwiche, A. (2003). differential approach inference bayesian networks. J.ACM, 50 (3), 280305.de Salvo Braz, R. (2007). Lifted First-Order Probabilistic Inference. Ph.D. thesis, UniversityIllinois Urbana-Champaign.Domingos, P., & Webb, W. A. (2012). tractable first-order probabilistic logic.. Proc.Conf. Artificial Intelligence. AAAI.Eaton, F., & Ghahramani, Z. (2013). Model reductions inference: Generality pairwise,binary, planar factor graphs. Neural computation, 25 (5), 12131260.712fiQuadratization Roof Duality Markov Logic NetworksFierens, D., Kersting, K., Davis, J., Chen, J., & Mladenov, M. (2013). Pairwise markovlogic. Inductive Logic Programming, pp. 5873. Springer.Fix, A., Gruber, A., Boros, E., & Zabih, R. (2011). graph cut algorithm higher-orderMarkov random fields. Int. Conf. Computer Vision, pp. 10201027. IEEE.Freedman, D., & Drineas, P. (2005). Energy minimization via graph cuts: Settlingpossible. Conf. Computer Vision Pattern Recognition, pp. 939946. IEEEComputer Society.Gallagher, A. C., Batra, D., & Parikh, D. (2011). Inference order reduction markovrandom fields. Proc. Int. Conf. Computer Vision Pattern Recognition, pp.18571864. IEEE.Gallo, G., & Simeone, B. (1989). supermodular knapsack problem. MathematicalProgramming, 45 (1-3), 295309.Huang, J., Chavira, M., & Darwiche, A. (2006). Solving MAP exactly searchingcompiled arithmetic circuits. Proc. Conf. Artificial Intelligence, Vol. 6, pp. 37.AAAI.Ishikawa, H. (2011). Transformation general binary MRF minimization first-ordercase. Trans. Pattern Analysis Machine Intelligence, 33 (6), 12341249.Kahl, F., & Strandmark, P. (2012). Generalized roof duality. Discrete Applied Mathematics,160 (16-17), 24192434.Kolmogorov, V., & Rother, C. (2007). Minimizing nonsubmodular functions graphcuts-a review. Trans. Pattern Analysis Machine Intelligence, 29 (7), 12741279.Kolmogorov, V. (2012). Generalized roof duality bisubmodular functions. DiscreteApplied Mathematics, 160 (4-5), 416426.Kolmogorov, V., & Zabin, R. (2004). energy functions minimized via graphcuts?. Trans. Pattern Analysis Machine Intelligence, 26 (2), 147159.Lempitsky, V., Rother, C., Roth, S., & Blake, A. (2010). Fusion moves markov randomfield optimization. Trans. Pattern Analysis Machine Intelligence, 32 (8), 13921405.McCallum, A., Nigam, K., & Ungar, L. H. (2000). Efficient clustering high-dimensionaldata sets application reference matching. Proc. Int. Conf. KnowledgeDiscovery Data Mining, pp. 169178. ACM.Mittal, H., Goyal, P., Gogate, V. G., & Singla, P. (2014). New rules domain independentlifted MAP inference. Proc. Advances Neural Information Processing Systems,pp. 649657.Niu, F., Re, C., Doan, A., & Shavlik, J. (2011). Tuffy: Scaling statistical inferencemarkov logic networks using RDBMS. Proc. VLDB Endowment, 4 (6), 373384.Noessner, J., Niepert, M., & Stuckenschmidt, H. (2013). Rockit: Exploiting parallelismsymmetry MAP inference statistical relational models.. AAAI Workshop:Statistical Relational Artificial Intelligence.713fide Nijs, Landsiedel, Wollherr, & BussOrlin, J. B. (2009). faster strongly polynomial time algorithm submodular functionminimization. Mathematical Programming, 118 (2), 237251.Poole, D. (2003). First-order probabilistic inference. Gottlob, G., & Walsh, T. (Eds.),Int. Joint Conf. Artificial Intelligence, pp. 985991. Morgan Kaufmann.Ramalingam, S., Russell, C., Ladicky, L., & Torr, P. H. S. (2011). Efficient minimizationhigher order submodular functions using monotonic boolean functions. CoRR,abs/1109.2304.Richardson, M., & Domingos, P. (2006). Markov logic networks. Machine learning, 62 (1),107136.Riedel, S. (2009). Cutting plane map inference markov logic. Int. Workshop StatisticalRelational Learning.Rosenberg, I. (1975). Reduction bivalent maximization quadratic case. Cahiersdu Centre detudes de Recherche Operationnelle, 17, 7174.Rother, C., Kolmogorov, V., Lempitsky, V., & Szummer, M. (2007). Optimizing binaryMRFs via extended roof duality. Proc. Conf. Computer Vision Pattern Recognition, pp. 18. IEEE.Sarkhel, S., Venugopal, D., Singla, P., & Gogate, V. (2014). Lifted MAP inferencemarkov logic networks. Proc. Int. Conf. Artificial Intelligence Statistics, pp.859867.Schlesinger, D. (2007). Exact solution permuted submodular minsum problems.Energy Minimization Methods Computer Vision Pattern Recognition, pp. 2838. Springer.Shavlik, J. W., & Natarajan, S. (2009). Speeding inference markov logic networkspreprocessing reduce size resulting grounded network. Proc. Int.Joint Conf. Artificial Intelligence, pp. 19511956.Singla, P., & Domingos, P. (2008). Lifted first-order belief propagation. Proc. NationalConf. Artificial Intelligence, Vol. 2, pp. 10941099.Singla, P., & Domingos, P. (2006a). Entity resolution markov logic. Proc. Int. Conf.Data Mining, pp. 572582. IEEE.Singla, P., & Domingos, P. (2006b). Memory-efficient inference relational domains.Proc. National Conf. Artificial Intelligence, Vol. 21, pp. 488493.Zivny, S., Cohen, D. A., & Jeavons, P. G. (2009). expressive power binary submodularfunctions. Discrete Applied Mathematics, 157 (15), 33473358.Zivny, S., & Jeavons, P. G. (2008). submodular functions expressible usingbinary submodular functions?. Tech. rep. CS-RR-08-08, University Oxford.714fiJournal Artificial Intelligence Research 55 (2016) 565-602Submitted 08/15; published 03/16Finding Strategyproof Social Choice Functionsvia SAT SolvingFelix Brandtbrandtf@in.tum.deTechnical University Munich (TUM)Munich, GermanyChristian Geistgeist@in.tum.deTechnical University Munich (TUM)Munich, GermanyAbstractpromising direction computational social choice address research problemsusing computer-aided proving techniques. particular SAT solvers, approachshown viable proving classic impossibility theoremsArrows Theorem also finding new impossibilities context preference extensions. paper, demonstrate computer-aided techniques alsoapplied improve understanding strategyproof irresolute social choice functions.functions, however, require evolved encoding otherwise search spacerapidly becomes much large. contribution two-fold: present efficientencoding translating problems SAT leverage encoding prove newresults strategyproofness respect Kellys Fishburns preference extensions. example, show Pareto-optimal majoritarian social choice functionsatisfies Fishburn-strategyproofness. Furthermore, explain human-readable proofsresults extracted minimal unsatisfiable cores corresponding SATformulas.1. IntroductionEver since famous Four Color Problem solved using computer-assisted approach,clear computers contribute significantly verifying existingalso finding proving new results. Due rigorous axiomatic foundation, socialchoice theory appears field computer-aided theorem proving particularlypromising line research. Perhaps best known result context stems TangLin (2009), reduce well-known impossibility results Arrows theoremfinite instances, checked satisfiability (SAT) solver (see, e.g., Biere,Heule, van Maaren, & Walsh, 2009). Geist Endriss (2011) able extendmethod fully-automatic search algorithm impossibility theorems contextpreference relations sets alternatives. paper, apply techniquesimprove understanding strategyproofness context set-valued, so-calledirresolute, social choice functions. types problems, however, complexrequire evolved encoding otherwise search space rapidly becomes large.Table 1 illustrates quickly number involved objects grows that, result,exhaustive search doomed fail.c2016AI Access Foundation. rights reserved.fiBrandt & GeistAlternativesChoice setsTournamentsCanonical tournamentsMajoritarian SCFs45671564450, 625311,0241210186332,76856101011272 10645610959Table 1: Number objects involved problems irresolute majoritarian SCFscontribution two-fold. one hand, provide extended frameworkSAT-based computer-aided theorem proving techniques statements social choicetheory related research areas. Despite complexity, framework allowsextraction human-readable proofs, eliminates need extensive (and difficult)verification underlying techniques. hand, rather reproducingexisting results, solve open problems, independent interest,context irresolute strategyproof social choice functions. results unlikelyfound without help computers, strengthens importanceapproach.results obtained computer-aided theorem proving already found attentionsocial choice community (Chatterjee & Sen, 2014) similar techniques provenquite effective problems economics, too. Examples ongoing workFrechette, Newman, Leyton-Brown (2016) SAT solvers useddevelopment execution FCCs upcoming reverse spectrum auction, recent resultsDrummond, Perrault, Bacchus (2015) solve stable matching problems via SATsolving, well work Tang Lin (2011) apply SAT solving discover classestwo-player games unique pure Nash equilibrium payoffs. another recent paper,Caminati, Kerber, Lange, Rowat (2015) verified combinatorial Vickrey auctions viahigher-order theorem provers. respect, approach bears similarities automatedmechanism design (see, e.g., Conitzer & Sandholm, 2002), desirable propertiesencoded mechanisms computed fit specific problem instances. alsobody work logical formalizations important theorems social choice theory,prominently, Arrows Theorem (see, e.g., Nipkow, 2009; Grandi & Endriss, 2013; Cina &Endriss, 2015), directed towards formalizing verifying existingresults.Given universality SAT-based method ease adaptation (e.g., testing similar conjectures minimal effort simply replacing altering axioms),expect similar techniques applicable open problems socialchoice theory related research areas future. Results different variantsno-show paradox (Brandl, Brandt, Geist, & Hofbauer, 2015; Brandt, Geist, & Peters,2016c) support hypothesis. noted, however, thatat least currentlyanexpert user programmer required operate systems. interesting questionremains whether possible develop automatic proof assistant allows researchers quickly test hypotheses small domains without giving much generalityefficiency.566fiFinding Strategyproof Social Choice Functions via SAT SolvingLet us turn towards social choice theoretic results. Formally, social choicefunction (SCF) defined function maps individual preferences set alternatives set socially most-preferred alternatives. SCF strategyproof agentobtain preferred outcome misrepresenting preferences. well-knownGibbard-Satterthwaite theorem that, restricting attention SCFs always return single alternative, trivial SCFs strategyproof. assumptionsingle-valuedness, however, criticized unreasonably restrictive (see,e.g., Gardenfors, 1976; Kelly, 1977; Taylor, 2005; Barbera, 2010). proper definitionstrategyproofness general setting irresolute SCFs requires specificationpreferences sets alternatives. Rather asking agents specifypreferences sets (which requires exponential space would bound variousrationality constraints), typically assumed preferences single alternativesextended preferences sets. course, various ways extendpreferences sets (see, e.g., Gardenfors, 1979; Duggan & Schwartz, 2000; Taylor, 2005),leads different class strategyproof SCFs. function yieldspreference relation subsets alternatives given preference relation singlealternatives called set extension preference extension. paper, focus twoset extensions attributed Kelly (1977) Fishburn (1972),1 shownarise uniquely natural assumptions (Gardenfors, 1979; Erdamar & Sanver, 2009;see also Section 2.2 paper).strategyproofness Kellys extension (henceforth Kelly-strategyproofness)known rather restrictive condition (Kelly, 1977; Barbera, 1977; Nehring, 2000),SCFs Pareto rule, omninomination rule, top cycle, uncoveredset, minimal covering set, bipartisan set shown Kelly-strategyproof(Brandt, 2015). Interestingly, prominent SCFs majoritarian, i.e.,based pairwise majority relation ordered respect set inclusion. results suggest bipartisan set may finest Kelly-strategyproofmajoritarian SCF. paper, show case automatically generating Kelly-strategyproof SCF strictly contained bipartisan set. Brandt(2015) furthermore showed that, mild condition, Kelly-strategyproofness carriescoarsenings SCF. Thus, finding inclusion-minimal Kelly-strategyproof SCFsparticular interest. address problem automating search functionssmall domains report findings.Existing results suggest demanding notion Fishburn-strategyproofnessmay satisfied rather indiscriminating SCFs top cycle (Feldman, 1979;Brandt & Brill, 2011; Sanver & Zwicker, 2012).2 Using computer-aided proving technique, able confirm suspicion proving that, within domain majoritarian SCFs, Fishburn-strategyproofness incompatible Pareto-optimality. orderachieve impossibility, manually prove novel characterization Pareto-optimal ma1. Gardenfors (1979) attributed extension Fishburn weakest extension satisfiescertain set axioms proposed Fishburn (1972). authors, however, refer Gardenforsextension, term reserve extension due Gardenfors (1976) himself.2. negative result Ching Zhou (2002) uses Fishburns extension much stronger notionstrategyproofness.567fiBrandt & Geistjoritarian SCFs induction step, allows us generalize computer-generatedimpossibility larger numbers alternatives.paper structured follows. Section 2, present general mathematical framework use throughout paper introduce new conditiontournament-strategyproofness, show equivalent standard strategyproofness majoritarian SCFs. Section 3, describe computer-aided proving methodexplain encode main questions paper SAT problems. alsodescribe optimization techniques features approach. Section 4, report main findingsan impossibility possibility resultand discuss possibleextensions limits. Section 5, novel approach proof extractioncomputer-generated results presented. provide human-readable proof mainresult verified without help computers. Finally, Section 6 wrapwork give outlook research directions.2. Mathematical Framework Strategyproofnesssection, provide terminology notation required results introduce notions strategyproofness majoritarian SCFs allow us abstract awayreference preference profiles.2.1 Social Choice FunctionsLet N = {1, . . . , n} set least three voters preferences finite setalternatives. convenience, assume n odd, entails pairwisemajority relation antisymmetric. preferences voter N representedcomplete, antisymmetric, transitive preference relation Ri A. interpretation(x, y) Ri , usually denoted x Ri y, voter values alternative x least muchalternative y. set preference relations denoted R(A).set preference profiles, i.e., finite vectors preference relations, given R (A).typical element R (A) R = (R1 , . . . , Rn ). accordance conventionalnotation, write Pi strict part Ri , i.e., x Pi x Ri Ri x. Notedifference Ri Pi Ri reflexive Pi not. orderimprove readability, write Ri : x, y, z shorthand x Pi Pi z. preferenceprofile, weight ordered pair alternatives wR (x, y) defined majoritymargin |{i N | x Ri y}| |{i N | Ri x}|.central objects study social choice functions, i.e., functions mapindividual preferences voters nonempty set socially preferred alternatives.Definition 1. social choice function (SCF) function f : R (A) 2A \ .SCF resolute |f (R)| = 1 R R (A), otherwise irresolute.restrict attention majoritarian SCFs, tournament solutions,defined using majority relation. majority relation RM preference profile Rrelation defined(x, y) RM wR (x, y) 0,568fiFinding Strategyproof Social Choice Functions via SAT Solvingalternatives x, A. SCF f said majoritarian neutral30 .outcome depends majority relation, i.e., f (R) = f (R0 ) whenever RM = RMbefore, write PM strict part RM , i.e., PM b RM b b RM a.alternative x called Condorcet winner R x PM A\{x}.words, Condorcet winner best alternative respect majority relationseems natural majoritarian SCFs select Condorcet winner. Unfortunately,clear-cut winners exist general variety so-called Condorcet extensions,i.e., SCFs uniquely return Condorcet winner whenever one exists differtreatment remaining cases, proposed literature. paper,consider following majoritarian Condorcet extensions (see, e.g., Laslier, 1997; Brandt,Brill, & Harrenstein, 2016a, information).Top Cycle Define dominant set non-empty set alternativesalternative x \ x PM y. top cycle TC (alsoknown weak closure maximality, GETCHA, Smith set) defined (unique)inclusion-minimal dominant subset A.4Uncovered Set Let C denote covering relation A, i.e., x C (x covers y)x PM and, z A, PM z implies PM z. uncovered set UCcontains alternatives covered according C, i.e., UC (R) = {x | Cx A}.Bipartisan Set Consider symmetric two-player zero-sum game setactions players given payoffs defined follows. Suppose firstplayer chooses second player chooses b. payoff first player 1PM b, 1 b PM a, 0 otherwise. bipartisan set BP contains alternativesplayed positive probability unique Nash equilibrium game.SCF f called refinement another SCF g f (R) g(R) preferenceprofiles R R (A). short, write f g case. shownBP UC TC (see, e.g., Laslier, 1997).main result, define well-known notion Pareto-optimality: SCF fPareto-optimal never selects Pareto-dominated alternative x A, i.e., x/ f (R)whenever exists Pi x N .2.2 StrategyproofnessAlthough investigation strategyproof SCFs universal senseapplied set extension, paper concentrate two well-known setextensions attributed Kelly (1977) Fishburn (1972).5 two set extensions3. Neutrality postulates permutation alternatives SCF producesoutcome (modulo permutation). See also Section 3.1.1.4. easily seen set dominant sets ordered respect set inclusion thereforeadmits unique minimal element. Assume contradiction two dominant sets X,contained other. Then, exists x X \ \ X. definition dominant setsrequires x PM PM x, contradiction.5. Another natural well-known set extension Gardenfors leads even stronger notion strategyproofness, cannot satisfied interesting majoritarian SCF (Brandt & Brill, 2011). Note569fiBrandt & Geistdefined follows: Let Ri preference relation X, two nonemptysubsets A.X RiK x Ri x X .(Kelly, 1977)One interpretation extension voters completely unaware mechanism(e.g., lottery) used pick winning alternative (Gardenfors, 1979; Erdamar& Sanver, 2009). words, contains exactly pairwise comparisons votersmake without knowledge mechanism (e.g., {a, b} RiK {c} Pi b Pi c).X RiF following three conditions satisfied:x Ri x X \ X ,Ri z X z \ X,(Fishburn, 1972)x Ri z x X \ z \ X.extension one may assume winning alternative picked lottery according underlying priori distribution voters aware (Ching & Zhou,2002). Alternatively, existence chairman breaks ties according linear,unknown, preference relation also rationalizes preference extension (Erdamar & Sanver,2009). interpretations, extension describes exactly conclusionsvoter aware tie-breaking method draw (e.g., {a, b} RiF {b, c} Pi b Pi c,hold Kellys extension RiK ).easy see X RiK implies X RiF pair sets X, A.plan prove results entire classes set extensions, call setextension E independent irrelevant alternatives (IIA) comparison two sets Xdepends restriction individual preferences X . Formally, E satisfiesIIA pairs preference relations Ri , Ri0 nonempty sets X,Ri |XY = Ri0 |XY holdsX RiE X Ri0 E .mild natural condition, satisfied previously mentionedset extensions major set extension literature aware of.Based set extension E, state corresponding notion P E strategyproofness irresolute SCFs. Note contrast related papers (e.g.,Ching & Zhou, 2002; Sato, 2008), interpret preference extensions fully specified (incomplete) preference relations rather minimal conditions set preferences.Again, write PiE asymmetric part RiE , set extension E.Definition 2. Let E set extension. SCF f P E -manipulable voterexist preference profiles R R0 Rj = Rj0 j 6= f (R0 ) E-preferredf (R) voter i, i.e.,f (R0 ) PiE f (R).SCF called P E -strategyproof P E -manipulable.negative result Fishburn-strategyproofness trivially carries demanding setextensions.570fiFinding Strategyproof Social Choice Functions via SAT Solving12, 34567ecbebcebccebbcebce(a) preference profile Rbbccee(b) corresponding (strict)majority relation PM(c) manipulated (strict) ma0jority relation PMfirstagent submits b, a, c, d, epreferences. edgesimpacted changedepicted bold.Figure 1: Let choice sets indicated shaded nodes; example takenproof Theorem 3 (cf. Section 5.1.3). first agent R P F -manipulate submitting b, a, c, d, e preferences (since f (R0 ) = {a, c, d, e} P1F {a, b, c, d} = f (R)),constitute P K -manipulation (since {a, b, c, d} {a, c, d, e} incomparableaccording Kelly-extension).follows observation set extensions P F -strategyproofness impliesexample illustrating notions strategyproofness shownFigure 1.SCFs, TC P F -strategyproof, BP P K - P F -strategyproof,whereas UC known satisfy P K -strategyproofness (Brandt & Brill, 2011; Brandt,2015).P K -strategyproofness.2.3 Tournament-Strategyproofnessorder allow efficient encoding, would like omit references preferenceprofiles replace succinct representation expressivepower. majoritarian SCFs, natural choice use (strict) majority relation,which, odd number voters, represented tournament:tournament asymmetric complete binary relation set alternatives A.6 thus view majoritarian SCFs functions defined tournaments ratherpreference profiles, and, slight abuse notation,7 write f (T ) instead f (R)= PM strict part majority relation R. We, furthermore, denote\ 0 := {e : e/ 0 } edge difference two tournaments 0 .encoding efficient, important formalize notion strategyproofness using references tournaments rather preference profiles.6. Note tournaments defined edge set only. Since exactly one edgepair vertices, vertex set derived edge set.7. may noted that, majoritarian SCFs map profiles (with arbitrary, fixed numbervoters) sets alternatives, interpretation via tournaments abstracts away referenceindividual voters. implications Theorems 1 3, depend upon presencesufficient number voters. discuss required number voters Section 5.2.571fiBrandt & Geistfollowing definition serves purpose shown equivalent standardnotion strategyproofness majoritarian SCFs.Definition 3. majoritarian SCF f said P E -tournament-manipulable existtournaments T, 0 preference relation R \ 0f (T 0 ) PE f (T ).majoritarian SCF called P E -tournament-strategyproof P E -tournamentmanipulable.Theorem 1. majoritarian SCF P E -strategyproof P E -tournamentstrategyproof.Proof. show majoritarian SCF P E -manipulable P E tournament-manipulable.direction left right, let f P E -manipulable majoritarian SCF.exist preference profiles R, R0 integer j Ri = Ri0 6= j0 strict majority relationsf (R0 ) PjE f (R). Define tournaments := PM 0 := PMR R0 , respectively. Since R R0 differ voter j, follows \ 0 Rj ,i.e., edges reversed 0 must Rj . Thus, R := Rj ,get f tournament-manipulable.converse, let f P E -tournament-manipulable majoritarian SCF. SCF fadmits manipulation instance, i.e., two tournaments T, 0 preferencerelation R \ 0 f (T 0 ) PE f (T ).proof McGarveys Theorem (McGarvey, 1953), construct preferencemajority relation:profile R = (R1 , . . . , Rn1 ) 0 strict part PMstart empty profile and, strict edge (a, b) 0 , add two voters ia,bja,b preferencesRia,b : a, b, x1 , . . . , xm2 Rja,b : xm2 , . . . , x1 , a, b, respectively.x1 , . . . , xm2 denotes arbitrary enumeration 2 alternatives \ {a, b}.holds weights wR (a, b) edges (a, b)(2 (a, b) 0wR (a, b) =0 (a, b) \ 0 .Note number voters n 1 R even (and m2 2).adding R n-th voter, get profile R := (R , R ) odd number votersrequired. wR (a, b) 1 edges (a, b) and, thus, R (strict)majority relation. second profile R0 defined contain first n 1voters R reversed preference R n-th voter (i.e., R0 := (R , R )).8profile R0 0 (strict) majority relation (since wR0 (a, b) = 18. Immunity manipulation reversing preferences considered Sanver Zwicker (2012)name half-way monotonicity. proof entails (weak) half-way monotonicity equivalent strategyproofness majoritarian SCFs.572fiFinding Strategyproof Social Choice Functions via SAT SolvingSetting axiomsLPsolvernautyResultsTournamentsolverCNF encoderModel decoderSAT solverFigure 2: High-level system architectureedges (a, b) \ 0 weights edges 0 least 1 again),completes manipulation instance. I.e., found preference profiles R, R0differ voter n (who truthful preferences R ) holdsf (R0 ) = f (T 0 ) PE f (T ) = f (R).3. Methodologymethod applied paper similar yet powerful ones presentedTang Lin (2009) Geist Endriss (2011). Rather translating wholeproblem navely SAT, evolved approach, resolves large degree freedomalready encoding problem, employed. approach comparableway SMT (satisfiability modulo theories) solving works: core SATsolver; certain aspects problem, however, dealt separate theory solvingunit accepts richer language makes use specific domain knowledge (Biereet al., 2009, ch. 26). general idea, however, remains encode problemlanguage suitable SAT solving apply SAT solver efficient universalproblem solving machine.desirable, using existing tools higher-order formalizations directly ratherspecific approach, unfortunately, option. instance, formalization strategyproof majoritarian SCFs higher-order logic (HOL) accepted Nitpick (Blanchette& Nipkow, 2010) straightforward, highly flexible, well-readable, successfulproofs counterexamples involving three alternatives search spaceexceeded.9 optimized formalization, derived together authorNitpick (at cost reduced readability flexibility), extends performancefour alternatives, turns low results.9. hand, strict formalization required Nitpick helped identify formally inaccurate definition Fishburn-strategyproofness Gardenfors (1979) (which later repeatedauthors).573fiBrandt & GeistConcretely, approach following (see also high-level architecture Figure 2): given domain size n want check whether exists majoritarianSCF f satisfies set axioms (e.g., P F -strategyproofness Pareto-optimality).encode setting well given axioms propositional formula (SATinstance) let SAT solver decide whether formula satisfying assignment.satisfying assignment, decode concrete instance majoritarianSCF f satisfies required properties. formula unsatisfiable, knowfunction f exists.see, depending problem, preparatory tasks solvedactual encoding: (i ) sets, tournaments, preference relations enumerated;(ii ) isomorphisms tournaments determined using tool nauty (McKay &Piperno, 2013); (iii ) choice sets specific SCFs computed (e.g., via matrix multiplication UC linear programming BP ).following, describe detail general setting majoritarianSCFs well desirable properties, strategyproofness, encoded SATproblem CNF (conjunctive normal form).10 First, describe initial encoding,expressive enough encode required properties, allows small domain sizes(depending axioms) four five alternatives only. Second, explainencoding optimized increase overall performance orders magnitudelarger instances seven alternatives solvable.3.1 Initial Encodingdesign, SAT solvers operate propositional logic. direct nave propositionalencoding problem would, however, require huge number propositional variablessince many higher-order concepts involved (e.g., sets alternatives, preference relationssets well alternatives, functions tuples relations sets).approach, use one type variable encode SCFs. variablesform cT,X tournament X set alternatives.11 semanticsvariables cT,X f (T ) = X, i.e., majoritarian SCF f selectsset alternatives X choice set preference profile (strict) majoritym(m1)m(m+1)relation . total, gives us high manageable number 2 2 2m = 2 2variables initial encoding.encoding variables cT,x alternatives x rather sets would require lessvariable symbols. encoding, however, leads much complexity generatedclauses, offsets savings. imbalance best exhibitedencoding strategyproofness statements always made forVpairs outcomes(i.e.,Vsets alternatives). occurrence cT,X could replaced xX cT,x yXcT,y ./since formula contains conjunction within disjunction, possible10. Converting arbitrary propositional formula navely CNF lead exponential blow-uplength formula. are, however, well-known efficient techniques (e.g., Tseitins encoding, seeTseitin, 1983) avoid cost introducing linearly many auxiliary variables. applytechniques manually needed.11. algorithms, subroutine c(T, X) take care compact enumeration variables. Sinceknow advance many tournaments non-empty subsets are, simply use standardenumeration method pairs objects.574fiFinding Strategyproof Social Choice Functions via SAT SolvingCNF, either expansionV(and therefore exponential blow-up) replacement (e.g.,helper variable cT,X xX cT,x ) would required.following two subsections demonstrate initial encoding contextualwell explicit axioms CNF.3.1.1 Context AxiomsApart explicit axioms, going describe next subsection,axioms need considered order fully model contextmajoritarian SCFs. purpose, arbitrary function maps tournamentsnon-empty sets vertices called tournament choice function. Usinginitial encoding three axioms introduced, ensure functionalitytournament choice function neutrality respected (making tournament solution):(1) functionality, (2) canonical isomorphism equality, (3) orbit condition.first axiom ensures relational encoding f variables cT,X indeed modelsfunction rather arbitrary relation, i.e., tournament exactly oneset X variable cT,X set true. formal terms written(T ) ((X) cT,X (Y, Z) 6= Z (cT,Y cT,Z ))!^_^cT,X(cT,Y cT,Z ) .X(1)6=Zillustrative example, corresponding simple pseudo-code generating CNFfile found Appendix B.second third axiom together constitute neutrality tournament choicefunction f , which, formally, written(f (T )) = f ((T )) tournaments permutations : A.direct encoding neutrality axiom, however, would tedious due quantification permutations. addition, reformulation canonical isomorphismequality orbit condition enables substantial optimization encodingsee Section 3.2. require observations order precisely state twoaxioms.use well-known fact graph isomorphisms define equivalence relationset tournaments.12 equivalence class, pick representativecanonical tournament class. tournament , unique canonicalrepresentation (denoted Tc ). also pick one potentially many isomorphismsTc canonical isomorphism denote .13 allows usformulate axiom canonical isomorphism equality.Definition 4. tournament choice function f satisfies canonical isomorphism equalityf (T ) = (f (Tc )) tournaments .(2)12. Two tournaments 0 isomorphic permutation : (T ) = 0 .13. practice, tool nauty automatically compute canonical representations tournamentsisomorphisms.575fiBrandt & GeistbceFigure 3: orbits tournamentOT = {{a, b, c}, {d}, {e}}. correspondingb c eautomorphism would =. C := {a, b, c} represents componentb c esense elements x C holds x PM e PM x.last three context axioms, definition orbit clarified.orbits tournament equivalence classes alternatives according followingequivalence relation: two alternatives a, b considered equivalentautomorphism : maps b, i.e., (a) = b. set orbitstournament denoted OT . example found Figure 3.Definition 5. tournament choice function f satisfies orbit conditionf (Tc ) f (Tc ) =(3)canonical tournaments Tc orbits OTc .shown tournament choice function, neutrality equivalentconjunction orbit condition canonical isomorphism equality, equivalently,class tournament choice functions satisfying orbit condition canonicalisomorphism equality equal class tournament solutions. formalizestatement Lemma 1. proof Lemma 1 based standard argumentscategory theory presented Appendix A.Lemma 1. tournament choice function, neutrality equivalent conjunctionorbit condition canonical isomorphism equality.3.1.2 Explicit AxiomsMany axioms efficiently encoded proposed encoding language. sectionpresent main conditions required achieve results Section 4. Clearly,important one strategyproofness. formal terms, P E -tournament-strategyproofnesswritten(T, 0 , R \ 0 ) f (T 0 ) PE f (T )^^ ^^(4)(cT,X cT,Y )0 R \T 0 PE XT, 0 tournaments, R preference relation, X, non-empty subsetsA. algorithmic encoding strategyproofness omitted since presentoptimized version Section 3.2.576fiFinding Strategyproof Social Choice Functions via SAT SolvingAnother property SCFs play important role results onerefinement another (known) SCF g. Fortunately, easily encoded usingframework:(T )(X g(T )) f (T ) = X^ _cT,X .(5)Xg(T )desire resulting SCF f different g (for instance, obtain strictrefinement conjunction Axiom (5)), encode additional clause:(T ) f (T ) 6= g(T )_cT,g(T ) .(6)Finally, even properties regarding cardinalities choice sets encoded.following axiomstating |f (T )| < |g(T )| least one tournament will,instance, useful Section 4.1.1 searching SCFs return small choice sets:(T )(X) |X| < |g(T )| f (T ) = X_ _cT,X .(7)X|X|<|g(T )|3.2 Optimized Encoding Improved Performanceorder efficiently solve instances four alternatives, need streamlineinitial encoding without weakening logical expressiv power. section,present three optimization techniques found effective.3.2.1 Obvious Redundancy Eliminationstraightforward first step reduce obvious redundancy within axioms.example, consider axiom strategyproofness, wherein order determine whetheroutcome = f (T 0 ) preferred outcome X = f (T )we consider preferencerelations R \ 0 . suffices, however, stop finding first preferencerelation PE X already know = f (T 0 ) X = f (T )true.Similarly, many axioms, exclude considering symmetric pairs objects (e.g.,functionality tournament choice function, need consider pairssets (X, ) (Y, X)).3.2.2 Canonical Tournamentsmain efficiency gain achieved making use canonical isomorphismequality (see Section 3.1.1) encoding. Recall condition statestournament choice set f (T ) determined choice set f (Tc ) corresponding canonical tournament Tc applying respective canonical isomorphism .577fiBrandt & Geistforeach Canonical tournament Tcforeach Tournament 0RTc \T 0 {R | R preference relation R Tc \ 0 };foreach Set Xforeach Setboolean found false;foreach R RTc \T 0found setExt(R , E).prefers(Y, X)variable not(c(Tc , X));variable not(c(Tc0 , T10 (Y )));newClause();found true;Algorithm 1: P E -tournament-strategyproofness (optimized)Therefore, suffices formulate axioms single representative equivalenceclass tournaments, case, canonical tournament. magnitudes Table 1illustrate formulation dramatically reduces required number variables,size CNF formula, time required encoding it.particular, axioms replace outer quantifier quantifier Tcranges canonical tournaments only.14 case strategyproofness, however,second tournament 0 restriction canonical tournaments potentiallystrong enough capture full power axiom. therefore keep 0arbitrary tournament make sure need variable symbols cTc0 ,Y canonicaltournaments CNF encoding. achieved canonical isomorphism0 since Condition (2), f (T 0 ) = f (Tc0 ) = T10 (Y ). optimizedencoding shown Algorithm 1.Furthermore, since longer make statements within CNF formulanon-canonical tournaments, canonical isomorphism equality condition becomesempty condition and, thus, dropped encoding.3.2.3 Approximation Logically Related PropertiesApproximation standard tool SAT/SMT speed solving process.instance, over-approximation help find unsatisfiable instances faster solvingparts full problem description CNF. partial CNF formula foundunsatisfiable, superset also trivially unsatisfiable. Since common manipulationinstances literature require one edge tournament reversed, one can,instance, use over-approximation form single-edge-strategyproofness, slightlyweaker variant (tournament-)strategyproofness |T \ 0 | = 1.1514. tool nauty capable enumerating non-isomorphic (i.e., canonical) tournaments.15. obvious whether condition actually strictly weaker tournamentstrategyproofness, identified Pareto-optimal SCFs Kelly-single-edge-strategyproofKelly-tournament-strategyproof (cf. Section 4.1.1).578fiFinding Strategyproof Social Choice Functions via SAT Solvingsolver returns single-edge-strategyproof SCF satisfiesset properties , know immediately also strategyproof SCFsatisfies . used form approximation prove results Remark 2.16similar fashion, one also apply logically simpler conditions, onesBrandt Brill (2011), slightly stronger weaker P E -strategyproofnessspecific set extensions E order logically under- over-approximate problems,respectively. logically simpler conditions help improve encodingsolving times, none required obtain results presented paper.Another way over-approximate problems restrict domain SCF(e.g., random sampling), explore somewhat detail extractingsmall proofs Section 5.1.1.3.3 Finding Refinements Incremental Solvingorder obtain results refined (i.e., inclusion-minimal) otherwise minimalSCFs, important also produce property SAT solver satisfactoryway. Generally, since task SAT solver generate one satisfying assignment, necessarily output finest SCF satisfy given set properties.iterated incremental solving, however, force SAT solver generateprogressively finer simply different SCFs satisfy set desired properties.17refinements, achieved adding clauses encode desired SCFmust (strictly) finer previously found solution (see, e.g., formulation Section 3.1.2). finest SCF desired properties found, addingclauses leads unsatisfiable formula, SAT solver detects therefore verifiesminimality solution.final solving step, main tools hand required results,significant ones describe next section.4. Results Discussionpresent two main findings:exists strict refinement BP P K -strategyproof (Theorem 2).majoritarian SCFs 5, P F -strategyproofness Pareto-optimalityincompatible (Theorem 3). < 5, UC satisfies P F -strategyproofness Paretooptimality.minor results mentioned discussions proceeding proofs Section 4.2.1.16. = 7 approximation required reach result, also enabled speed-up smallerinstances: running time = 6, example, reduced almost five hours three minutes.17. Note finding refinement SCF equivalent finding smaller/minimal modelSAT sense; encoding assignments number satisfied variables.579fiBrandt & Geist4.1 Minimal Kelly-Strategyproof SCFsBrandt (2015) showed every coarsening f P K -strategyproof SCF f 0 P K strategyproof f (R) = f 0 (R) whenever |f 0 (R)| = 1. Thus, interesting questionidentify finest (or inclusion-minimal ) P K -strategyproof SCFs.previous results suggested BP could aor even thefinest majoritarianSCF satisfies P K -strategyproofness, first provide counterexample assertions using = 5 alternatives, second show also larger domain sizesexist majoritarian refinements BP still P K -strategyproof return significantlysmaller choice sets BP .Theorem 2. exists majoritarian Condorcet extension refines BP stillP K -strategyproof. consequence, BP even finest majoritarian Condorcet extension satisfying P K -strategyproofness.Proof. Within seconds implementation finds satisfying assignment = 5encoding explicit axioms refinement BP (implies Condorcet extension) P K strategyproofness. corresponding majoritarian SCF decoded assignmentdefined like BP exception depicted Figure 4.bceFigure 4: Tournament P K -strategyproof refinement BP possible. C :={a, b, c} represents component sense elements x C holdsx PM e PM x. BP chooses whole set tournament, refinedsolution selects {a, b, c, d} only.Using technique described Section 3.3, furthermore confirmed obtained SCF refinement BP five alternatives still P K -strategyproof.Note, however, satisfy (natural, strong) property compositionconsistency (see, e.g., Laslier, 1997). Thus, remains open whether BP might characterized anor even theinclusion-minimal, P K -strategyproof, composition-consistentmajoritarian SCF.18able resolve open problem completely, proved following statements extending approach also cover composition-consistency. BPinclusion-minimal, P K -strategyproof, composition-consistent majoritarian SCF18. Although already domain five alternatives inclusion-minimal, P K strategyproof, composition-consistent Condorcet extensions, could find using computeraided method, counterexamples might extend larger domains.580fiFinding Strategyproof Social Choice Functions via SAT Solving5.19 7, BP inclusion-minimal majoritarian SCF satisfying setmonotonicity 20 composition-consistency. result might extend larger instances, holds 5 alternatives properties uniquely characterizeBP .we, however, drop composition-consistency again, find multiple inclusionminimal majoritarian SCFs refinements BP still P K -strategyproof. Interestingly, SCFs turn discriminating others senseaverage yield significantly smaller choice sets. following sectiongoing search discriminating SCFs analyze average size respectivechoice sets.4.1.1 Finding Discriminating Kelly-Strategyproof SCFsMany P K -strategyproof tournament solutions criticized discriminating enough. known, instance, large random tournaments, TCUC select alternatives probability approaching 1 (Scott & Fey, 2012), BPselects exactly half alternatives average fixed number alternatives(Fisher & Reeves, 1995). discriminating tournament solutions, hand,Copeland, Markov, Slater rules violate P K -strategyproofness. Usingcomputer-aided approach, search discriminating majoritarian SCFssatisfy P K -strategyproofness. Though spirit automated mechanism design (see, e.g., Conitzer & Sandholm, 2002), apply techniques mostly improveunderstanding P K strategyproofness related axioms rather proposegenerated tournament solutions actual use.measure discriminating power majoritarian SCFs, use averagerelative size avg(f ) choice sets returned SCF f . Formally defineavg(f ) :=X1|f (T )|,|A| |T|set labeled tournaments |A| = alternatives. call SCF fdiscriminating another SCF g avg(f ) < avg(g). Given set axioms ,try find discriminating SCF f (i.e., minimal value avg(f ))f satisfies axioms .theory would possible encode relevant axioms enumerate SCFs required properties incrementally applying Axiom (6),number SCFs usually much large. instead refine initial solutionapplying Axioms (5) (6) indicated Section 3.3,find inclusion-minimal SCF, necessarily discriminating SCF f . thusproceed via Algorithm 2, guaranteed find discriminating SCF f withoutenumerating candidates SCFs. algorithm starts constructing initial candidate SCF satisfies required axioms, iteratively refines much possible(via conjunction Axioms (5) (6)), encodes additional axiom stating19. = 6 already find refinement properties.20. Set-monotonicity postulates choice set invariant weakening unchosen alternatives;implies P K -strategyproofness (Brandt, 2015).581fiBrandt & Geistfuture solutions must yield choice set strictly smaller cardinality leastone tournament (Axiom (7)). algorithm repeats refinement encodingprocess solution found. Since Axiom (7) necessary conditionavg(f ) < avg(g), sure finest SCF f returned.SCF smallestSolution null;CNF minimalRequirements encodeAxioms();minimalRequirements preprocess(minimalRequirements); // optionalisSatisfiable(minimalRequirements)CNF currentRequirements minimalRequirements;SCF currentSolution solve(currentRequirements);canBeRefined(currentSolution)Append Axioms (5) (6) currentRequirements g = currentSolution;currentSolution solve(currentRequirements);// inclusion-minimal solution foundavgSize(currentSolution) < avgSize(smallestSolution)smallestSolution currentSolution;Append Axiom (7) minimalRequirements g = currentSolution;return smallestSolution;Algorithm 2: search algorithm find cardinality-minimal SCF f (i.e., minimalvalue avg(f )) satisfies given set axioms. reminder, Axioms (5) (6)encode strict refinement g; Axiom (7) encodes |f (T )| < |g(T )| tournamentT.Preprocessing generally optional Algorithm 2; = 6 we, however, useunit propagation order reduce size resulting SAT instance.21 Noteoptimization techniques described Section 3.2 (in particular, canonical tournaments)also applied here.results analysis exhibited Figure 5. four alternativesaxioms consideration lead minimal size avg(f ), larger domains,P K -strategyproofness allows smaller choice sets BP (e.g., 45% instead 50%alternatives = 6). Interestingly, gap BP discriminating SCFs satisfy P K -strategyproofness extraordinarily large; particular,moving P K -strategyproofness P K -single-edge-strategyproofness allowssizable reduction avg(f ). related property Kelly-participation, Brandl et al.(2015) remarked average size choice sets reduced almost 50% compared BP , supports intuition participation weaker propertystrategyproofness (even though logically two independent).BP set-monotonicity yield exact values avg(f ) 6,somewhat surprising found SCFs coarsenings BP yet setmonotonic domain size. SCFs, however, set-monotonic refinementsdiscriminating BP . Interestingly, generalize larger21. case Kelly-strategyproofness, unit propagation deletion duplicate clauses reducedCNF formula 600 million three million clauses.582fiFinding Strategyproof Social Choice Functions via SAT Solving60 %56%50%50 %45%minf (avg(f ))40 %38%Uncovered set UCBipartisan set BPSet-monotonicityP K -strategyproofnessP K -single-edgestrategyproofness30 %20 %18%restriction10 %23456Number alternatives |A| =Figure 5: comparison minimal values (rounded) avg(f ) majoritarian, Paretooptimal SCFs f satisfy given axioms (e.g., P K -strategyproofness). Interestingly,values set-monotonicity identical ones BP . Non-solid dots representupper bounds, i.e., cases could compute SCF f value avg(f )guarantee indeed minimal.583fiBrandt & Geistdomains since found discriminating majoritarian SCF f = 7 satisfiesset-monotonicity Pareto optimality selecting 49.73% alternativesaverage.demanding axioms usually lead larger choice sets (for instance, SCFalways returns alternatives trivially satisfies many axioms), one might view minimalvalue avg(f ) attempt quantify strength axiom. leavedetailed study quantification future work.4.2 Incompatibility Fishburn-Strategyproofness Pareto-Optimalityorder prove main result incompatibility Pareto-optimality P F strategyproofness first show following lemma, establishes that, majoritarianSCFs, notion Pareto-optimality equivalent refinement uncoveredset (UC ).22Lemma 2. majoritarian SCF f Pareto-optimal refinementUC .Proof. well-known, already observed Fishburn (1977), UC Paretooptimal, implies refinements also Pareto-optimal.direction left right, let f Pareto-optimal majoritarian SCFarbitrary tournament. suffices show f (T ) never contain covered alternative(since f (T ) UC (T ) contains uncovered alternatives only). let b alternativecovered another alternative a. going construct preference profileR (strict) majority relation b Pareto-dominated a.Together Pareto-optimality f implies b/ f (T ). use variantwell-known construction McGarvey (1953), triples rather pairsalternatives. Note voter need ensure strictly prefers border obtain desired Pareto-dominance b. Starting empty profile,alternative x/ {a, b} add two voters Rx1 , Rx2 profile. two votersdefined depending x ranked relative b order establish edgesa, x b, x. Note since x implies x b (because C b), edge (a, b)cannot contained three-cycle x and, thus, forms transitive triple x.Case 1: x (implies x b)Rx1 : x, a, b, v1 , . . . , vm3 ; Rx2 :vm3 , . . . , v1 , x, a, bCase 2a: x x bRx1 : a, x, b, v1 , . . . , vm3 ;Rx2 :vm3 , . . . , v1 , a, x, bCase 2b: x b xRx1 : a, b, x, v1 , . . . , vm3 ;Rx2 :vm3 , . . . , v1 , a, b, xv1 , . . . , vm3 denotes arbitrary enumeration m3 alternatives A\{a, b, x}.cases, two voters cancel pairwise comparisons(a, b), (x, a) (x, b). remaining edges (y, z) (with {y, z} {a, b} = )22. stronger version lemma shown Brandt, Geist, Harrenstein (2016b).584fiFinding Strategyproof Social Choice Functions via SAT Solvingadd two voters (now even closer construction McGarvey.)R(y,z)1 : y, z, a, b, v1 , . . . , vm4R(y,z)2 : vm4 , . . . , v1 , a, b, y, z,together establish edge (y, z), reinforce (a, b) cancel otherwise. Note orderachieve odd number voters, arbitrary voter added without changingmajority relation (as edges weight least two far). completesconstruction preference profile R (strict) majority relationb Pareto-dominated a.establish full result (which admit proof counterexampleTheorem 2) wesimilarly previous approachesmake use inductive argument.Lemma 3. set extension E satisfies IIA, exists majoritarian SCF f+ 1 alternatives P E -strategyproof Pareto-optimal, also existsmajoritarian SCF f 0 alternatives satisfies two properties.Proof. Let f UC majoritarian SCF + 1 2 alternatives P E strategyproof. define fe restriction f alternatives basedtournaments alternative e Condorcet loser, i.e., alternative x(y, x) \ {x}. formal terms, definefe (T ) := f (T +e ),+e tournament obtained adding alternative e Condorcetloser. restriction f well-defined SCF since alternative e cannot containedf (T +e ) UC (T +e ) = UC (T ), last equation follows simple observationcovering relation unaffected deleting Condorcet losers.need show alternative e restriction fe majoritarianSCF P E -strategyproof Pareto-optimal. Since holds e A,pick e arbitrarily.Majoritarian: fact fe majoritarian SCF carries trivially f .P E -strategyproofness: Assume contradiction fe P E -strategyproof.Then, Theorem 1 exist tournaments 0 alternativesfe (T 0 ) PE fe (T ) R \ 0 . since fe (T 0 ) = f (T 0+e ) fe (T ) = f (T +e )(and fact E satisfies IIA), getf (T 0+e ) PE f (T +e ),contradicts P E -tournament-strategyproofness f (as two tournaments 0+e+e form manipulation instance), thus P E -strategyproofness.Pareto-optimality: Lemma 2, equivalent refinement UC .Thus, let arbitrary tournament alternatives consider followingchain set inclusions, proves fe UC :fe (T ) = f (T +e ) UC (T +e ) = UC (T ).585fiBrandt & Geistvirtue Lemma 3 suffices check claim restricted domain= 5, following lemma.Lemma 4. exactly five alternatives (i.e., = 5) majoritarian SCF fsatisfies P F -strategyproofness Pareto-optimality.Proof. base case = 5 alternatives verified using computer-aided approach,i.e., checked that, |A| = 5 alternatives, satisfying assignmentencoding P F -tournament-strategyproofness (cf. Theorem 1) refinementUC (cf. Lemma 2), SAT solver confirmed within seconds. human-readableproof claim extracted computer-aided approach presentedSection 5.1.2.Finally, papers main result regarding P F -strategyproofness follows directlyLemmas 3 4.Theorem 3. number alternatives 5 majoritarian SCF fsatisfies P F -strategyproofness Pareto-optimality.Proof. prove statement inductively. base case = 5 covered Lemma 4.induction step, apply contrapositive Lemma 3 E := F, directlyyields desired results.number voters required impossibility kept implicit far,upper bound m2 1 = 19 voters derived constructionproof Theorem 1. Section 5 see, however, human-readable proofTheorem 3 extracted, requires seven voters.consequence Theorem 3, virtually common tournament solutionsexcepttop cycle (see Remark 2)fail P F -strategyproof.4.2.1 Remarksturn towards technique proof extraction, let us discuss insightsregarding Theorem 3, been, large extent, enabled universalitypresented method.Remark 1 (Strengthenings). shown computer-aided methodTheorem 3 holds even without assumption neutrality. Since then, however,optimizations based canonical tournaments longer used, extracted proofs(cf. Section 5) much complex therefore decided present resultneutrality here.23theorem strengthened additionally requiring P F -single-edgestrategyproofness (cf. Section 3.2) even weaker variant P F -strategyproofnessmanipulator allowed swap two adjacent alternatives (see, e.g., Sato, 2013).23. addition, running times much longer, which, however, major concern given manyconjectures tested result.586fiFinding Strategyproof Social Choice Functions via SAT SolvingRemark 2 (The Top Cycle TC ). Note Theorem 3 conflict factTC P F -strategyproof, as, 4 alternatives, TC strictly coarser UCtherefore Pareto-optimal. Possibly, TC even finest majoritarian Condorcetextension satisfies P F -strategyproofness 5. able verify5 7 using computer program. case four alternatives, UC strictrefinement TC (as method shows) still P F -strategyproof. = 8 timespace requirements appear prohibitive; already = 7 (despite optimizationsapproximations) encoding solving problem takes almost 24 hours,= 6 runs three minutes. obvious whether inductive argumentextend verified instances larger numbers alternatives (as, instance,induction step would require least five alternatives).Remark 3 (Other Preference Extensions). advantage computer-aided approach universality. can, instance, easily adapt implementationcheck set extensions ones Kelly Fishburn.Interestingly, main result relies small fraction power Fishburnextension: suffices compare disjoint sets sets contained one another.formal terms, following set extension suffices impossibility:KX Ri X = ,X RiF X RiF X X,otherwise.Actually, would even suffice compare sets X |X | 3.also checked strengthening Fishburn extension: voter prefers set Xset X better optimistic pessimistic expectations.Formally, X RiOPx Ri x X ,Ri x x X.extension weakening optimistic pessimistic notions strategyproofness Duggan-Schwartz Theorem (Duggan & Schwartz, 2000). majoritarian setting, P OP -strategyproofness leads analogous impossibility Theorem 34 already.Remark 4 (Generality Lemma 3). Note proofs individual propertieswithin inductive proof Lemma 3 rely definition fe standindependently other. Furthermore, may noted Lemma 3 evenshown refinements arbitrary majoritarian SCFs g whose choice set g(T )shrink Condorcet losers removed (rather Pareto-optimal majoritarianSCFs).5. Proof Extractionmajor concern regarding computer-aided proofs difficulty checking correctness. implementation correctly confirmed number existing results587fiBrandt & Geistconsidered testing, doubts correctness new results naturallyremain. SAT solvers offer kind proof trace, checked thirdparty-software. This, however, guarantee correctness encodingconfirms unsatisfiability corresponding CNF formula.section, show human-readable proofs desired statementsextracted approach, verified manual mathematicalproof. general idea proof extraction technique lies finding analyzingminimal unsatisfiable core (also referred minimal unsatisfiable set (MUS))SAT instance. unsatisfiable core CNF formula subset clauses alreadyunsatisfiable itself. subset clauses unsatisfiable core satisfiable,core called minimal. case, minimal unsatisfiable core contains informationconcrete instances axioms employed obtain impossibility(e.g., manipulation instances, applications Pareto optimality, etc). informationextracted straightforward way reveals structure arguments proof.exemplify technique Section 5.1, extract human-readable proofmain result (Theorem 3). Section 5.2 additionally enrich proof setminimal corresponding preference profiles, shows result Theorem 3holds setting least seven voters.general, extracting human-readable proofs serves two separate purposes.one hand, human-readable proof significantly raise confidence correctnessresults, basically making verification approach obsolete since resultsdirectly verifiable. hand, extracted proofs sometimesprovide additional insight problems via arguments structure. case,number voters required impossibility would (easily) accessibledirectly.5.1 Human-Readable Proof Theorem 3order extract human-readable proof Theorem 3, actually main ingredientLemma 4, follow series three steps:1. Obtain suitable MUS CNF formula encodes P F -tournamentstrategyproof refinement UC five alternatives2. Decode MUS human-readable format3. Interpret human-readable MUS obtain human-readable prooffirst two steps computer-aided largely automated, step threerequires manual effort.5.1.1 Obtaining Suitable MUS CNF FormulaExtracting minimal unsatisfiable core feature offered range SAT solvers.paper, use PicoMUS (part PicoSAT, Biere, 2008) job.2424. Compiled trace support order use core extraction addition clause selector variables.significantly improves size resulting MUS.588fiFinding Strategyproof Social Choice Functions via SAT Solvingnoted, however, MUS inclusion-minimal, necessarily representsmallest unsatisfiable set (i.e., minimal number clauses variables).25number clauses turned good proxy proof complexity length,tried find MUS small number clauses. run complete,optimized SAT encoding described Section 3.2, PicoMUS returns MUS55 clauses. already massive reduction compared three million clausesoriginal problem instance, found even smaller MUS 16 clausesrandomly sampling sets tournaments used instead full domaintournaments generating problem files. Another heuristic approach consideringneighborhoods single tournaments (for instance, tournaments reachedchanging two edges transitive tournament) yielded less significantimprovement total 25 clauses.seems natural larger domains generally better lead required impossibility often smaller domains, larger domains actually tend towardslarger proofs even miss small proofs. instance, domain size = 200(consisting labeled tournaments) proof smaller 18 clauses found,number runs = 50 produced four proofs 16 clauses each.26Therefore, setting, medium-sized domain (s = 50 = 100 experiments)appears best suited. complete results running time proof size analysis givendifferent domain sizes obtained Figures 8 9 Appendix C.5.1.2 Decoding MUS Human-Readable Formatnext step make obtained MUS accessible humans. end, first(automatically) add comments original CNF manipulation clausecreation, select comments belong clauses MUS. commentscontain witnesses manipulation instances found, i.e., information originaltournament , manipulated tournament 0 , respective choice sets f (T ) f (T 0 ),original preferences manipulator R (compare Definition 3). Furthermore,variable symbol easily decoded tournament choice set represents,helpful particular non-manipulation clauses (orbit condition Paretooptimality).result step presented Figure 6, tournament representedlower triangular representation adjacency matrix (see proof Lemma 4Section 5.1.3 graphical representations).5.1.3 Interpreting MUS Obtaining Human-Readable Proofwitnessed MUS small step textual, human-readable proof.bit practice, one quickly understand structure proof: startsorbit condition first line refinement condition last line,25. tool CAMUS Liffiton Sakallah (2008) theoretically capable finding smallestMUS (with minimal number clauses), terminate reasonable amount timelarge CNF instances.26. addition, medium-sized domains efficient regarding running time per generated proof,admittedly plays minor, still important role given total running time largedomains 20 hours.589fiBrandt & Geistp c n f 341 16218 231 232 233202 330 0c : 1111111111233 202 0c : 1101100111234 202 0c : 1101100111218 218 0c : 1101100111232 232 0c : 1101100111248 338 0c : 1101100111231 202 0c : 1101100111247 202 0c : 1101100111314 314 0c : 1100101110c : 1100101110318 318 0c : 1100101110c : 1100101110322 322 0c : 1100101110326 326 0c : 1100101110334 202 0c : 1100101110202 0314 318 322 326234 247 248 0> [ e ] ; : 1011100111 > [ , e ] ; P : b , , c , e ,> [ e ] ; : 0010100111 > [ ] ; P : b , c , , , e> [ , e ] ; : 0010100111 > [ ] ; P : b , c , , , e> [ ] ; : 1001000100 > [ e ] ; P : e , c , , , b> [ , b , c , ] ; : 1001000100 > [ , c , , e ] ; P : e , c , , , b> [ , b , c , , e ] ; : 1100100101 > [ b , c , e ] ; P : b , e , c , ,> [ b , c , ] ; : 1111111111 > [ e ] ; P : , e , b , c ,> [ b , c , , e ] ; : 1111111111 > [ e ] ; P : , e , b , c ,> [ c ] ; : 1100100101 > [ e ] ; P : b , , e , , c> [ c ] ; : 1100110110 > [ b ] ; P : b , c , , e ,> [ ] ; : 1100100101 > [ b ] ; P : b , , e , , c> [ ] ; : 1100110110 > [ ] ; P : b , c , e , ,> [ c , ] ; : 1100110110 > [ , b ] ; P : b , e , , c ,> [ e ] ; : 1100110110 > [ ] ; P : b , c , , e ,> [ , e ] ; : 1001111010 > [ ] ; P : c , , , e , b330 334 338 0Figure 6: version extracted MUS, manipulation instances (here: binaryclauses) decoded human-readable format: two mappings tournaments(original manipulated 0 ) choice sets truthful preferences manipulator P . information covers variables thus suffices also decode remainingclauses.590fiFinding Strategyproof Social Choice Functions via SAT SolvingTruthful choice{e}{a} {e}{b, c, d}f (T1 ) ={b, c, d} {e}{a}{a} {b, c, d}{c}{d}f (T2 ) = {c, d}{e}{d, e}f (Te0 ) UC (Te0 ) = {e}Manipulated choiceManipulatorspreferencesf (Ta ) UC (Ta ) = {a}b, c, d, a, ef (Te ) UC (Te ) = {e}a, e, b, c,({e}T10 isof (T10 ) ={a, c, d, e} morphic27 T1{b}{a}T20 isof (T20 ) ={a, b} morphic27 T2{d}f (Td ) UC (Td ) = {d}f (T2 ) = {c, e}28e, c, a, d, bb, e, a, c,b, c, d, e,c, a, d, e, ba, c, b, e,Table 2: Set manipulation instances (one per line) conclude f (T1 ) = ={a, b, c, d, e} f (T2 ) = {c, d, e}. truthful choices considered leadsP F -tournament-manipulation instance (a contradiction assumption P F tournament-strategyproofness). tournaments defined Figure 7.leave (limited) possibilities respective choice sets, excludes possiblechoices one another suitable manipulation instances. full proof runs follows.Proof Lemma 4. contradiction, let f majoritarian SCF = {a, b, c, d, e}satisfies P F -strategyproofness Pareto-optimality. Recall that, Theorem 1, fP F -tournament-strategyproof, too, Lemma 2 refinement UC(i.e., f UC ). Let furthermore T1 T2 tournaments depicted Figure 7.proceed three steps: first, show f (T1 ) = UC (T1 ) = A. Second, arguef (T2 ) = UC (T2 ) = {c, d, e}. last, prove two insights actually formsbasis manipulation instance, leads desired contradiction.Let us start f (T1 ) = UC (T1 ) = A. First, note since alternatives {b, c, d}form orbit know either {b, c, d} f (T1 ) {b, c, d} f (T1 ) = (cf. Definition 5).going exclude remaining choice sets P F -tournament-manipulation instances. first example, suppose f (T1 ) = {e}. voter individual preferencesP : b, c, d, a, e could reverse edges (b, a) (b, c) T1 transitive tournament Ta Condorcet winner results (which needs uniquely selected f sincef UC ). Since, however, {a} PF {e}, contradicts P F -tournament-strategyproofness.example also works exclude f (T1 ) = {a, e}. Note arguments correspond lines 5 8 extracted MUS Figure 6. (analogous) manipulationinstances possible choice sets = {a, b, c, d, e} given Table 2Figure 7.591fiBrandt & Geistbbbccceee(a) T1(b) T2(c) Tabbbcee(d) Te(e)bceT10c(f)bT20bccceee(g) Td(h) T200(i) Te0Figure 7: Tournaments required proof Lemma 4. uncovered setsmarked grey; edges (for Te0 : be) reversed manipulating voter(cf. Table 2) depicted thick edges. Note proof would also succeed less edgereversals Ta , Te , Td , Te0 (such tournaments Condorcet winnersrather transitive). transitive tournaments isomorphic, however,thus succinctly represented single clause 202 extracted MUS.f (T2 ) = UC (T2 ) = {c, d, e}, first observe f (T2 ) UC (T2 ) = {c, d, e} henceneed exclude strict subset {c, d, e}. proceed giving possiblemanipulation instance subsets. complete list found Table 2Figure 7. Observe last line Table 2 excludes f (T2 ) = {c, e} consideringmanipulated choice (known) truthful choice f (Te0 ) UC (Te0 ) = {e}.last step, provide manipulation instance based f (T1 ) = f (T2 ) ={c, d, e}. this, first observe renaming alternatives get f (T200 ) = {b, c, e}manipulation instance results voter preferences P0 : b, e, c, d, a.!!b c eb c e27. isomorphisms 1 =2 =, respectively.b e cc e b28. SAT solver actually returned isomorphic copy instance, restructured improvereadability.592fiFinding Strategyproof Social Choice Functions via SAT Solvingvoter reverse edges (d, a) (e, c) T1 create T200 obtain P F -preferredoutcome {b, c, e}, contradiction P F -strategyproofness f .Note actually manipulation instance f (T1 ) = {a} {b, c, d}f (T10 ) = {a, c, d, e} requires Fishburn-extension; instances Kellyextension suffices.5.2 Number Voters Requiredprevious parts paper taken advantage fact conditiontournament-strategyproofness abstracted away reference voters. interestingask, however, many voters least required obtained impossibilityTheorem 3 hold. construction proof Theorem 1 gives implicit upperbound m2 1 = 19 voters, improved seven voters.slightly modifying techniques described Brandt, Geist, Seedig (2014),able (automatically) construct minimal preference profiles steps Proof 5.1.3.Brandt et al. (2014) provided SAT-formulation whether given majority relationinduced given number voters, extended framework include axiomsmanipulation instances. detail, re-used axioms linear preferencesmajority implications, added axioms truthful preferences manipulatormajority implications manipulated profile.profiles generated steps proof Lemma 4 Section 5.1.3given Appendix D. largest profiles contains seven voters,profiles easily extended seven voters adding pairs voters oppositepreferences. observation shows seven smallest number votersachieved extracted proof, remains open whether, another proof,number voters reduced seven.6. Conclusionextended applied computer-aided theorem proving based SAT solvingextensively analyze Kelly- Fishburn-strategyproof majoritarian SCFs. ledrange results, positive negative. important novel contribution workability extract human-readable proof negative SAT instances. eliminatesneed verify computer-aided method since impossibility results directlychecked based human-readable proofs. Based ease adaptationproposed method, anticipate insights spring overall approachfuture. Apart simply applying system investigate strategyproofness,potential applications related line work include:Unrestricted SCFs order reduce complexity, studied majoritarian SCFsonly. framework, however, applicable way general SCFs,operate full preference profiles (rather majority relations). challengefind suitable representation preference profiles potentially correspondinginductive arguments number voters.593fiBrandt & Geistaxioms preliminary experiments suggest technique easilyapplied range properties strategyproofness, deserveinvestigation. many cases suffices formalize implement additionalaxioms. particular interest could properties link behavior SCFsdifferent domain sizes. initial steps direction, able extend approachcover property participation (Brandl et al., 2015; Brandt et al., 2016c) wellweak version composition-consistency (cf. Section 4.1).Smallest number voters required mentioned Section 5.2, Theorem 3 holdsnumber voters n 7, known whether number minimal. Onecould adapt proof extraction presented Section 5 search smallest proofnumber voters, rather number clauses, settle question.Generalization inductive argumentappears reasonable investigatewhether inductive argument Lemma 3 generalized whole classproperties/axioms, ideally based logical form. work Geist Endriss(2011), would enable automated search theorems SCFs.Apart concrete ideas, applications general approach envisionedmany areas theoretical economics.Acknowledgmentsmaterial based upon work supported Deutsche Forschungsgemeinschaftgrants BR 2312/7-2 BR 2312/9-1. paper benefitted discussions COSTAction Meeting IC1205 Computational Social Choice (Maastricht, 2014), 13th International Conference Autonomous Agents Multiagent Systems (Paris, 2014),5th International Workshop Computational Social Choice (Pittsburgh, 2014),Dagstuhl Seminar Computational Social Choice: Theory Applications (Dagstuhl,2015). authors particular thank Jasmin Christian Blanchette, Markus Brill, HansGeorg Seedig, Bill Zwicker helpful discussions support, three anonymous reviewers valuable comments suggestions improve paper.Appendix A. Proof Lemma 1first show orbit condition equivalent statement automorphisms:Lemma 5. Let f tournament choice function. following statement equivalent orbit condition:(f (Tc )) = f (Tc ) canonical tournaments Tc automorphisms .(8)Proof. Let f tournament choice function Tc canonical tournament. direction left right, let furthermore OTc orbit Tc . pick two alternativesa, b O. show either alternatives chosen f neither one is. Sinceb orbit, must automorphism Tc (a) = b.Observe f (Tc ) b (f (Tc )) b f (Tc ), laststep application Condition (8).594fiFinding Strategyproof Social Choice Functions via SAT Solvingconverse, let automorphism Tc , pick arbitrary alternativeconsider inverse image 1 (a) =: b. Since b orbit, holdsorbit condition f (Tc ) b f (Tc ). Furthermore, (b) =get f (Tc ) (f (Tc )). Thus, f (Tc ) = (f (Tc )),wanted prove.Next prove general statement split isomorphism canonicalisomorphism automorphism.Lemma 6. isomorphism : Tc decomposed canonical isomorphismautomorphism : Tc Tc . I.e., isomorphism : Tcautomorphism : Tc Tc = .Proof. Define : Tc Tc setting := T1 . Since inverses compositionsisomorphisms isomorphisms,followsdirectly automorphism.Furthermore, = T1 = T1 = .Lemmas 5 6 together used prove Lemma 1:Lemma 1. tournament choice function, neutrality equivalent conjunctionorbit condition canonical isomorphism equality.Proof. Let f tournament choice function first note Lemma 5 might useCondition (8) rather orbit condition. Therefore, direction left righttrivially true.direction right left, first show canonical isomorphismequality (2) together Condition (8) implies neutrality canonical tournaments:let Tc canonical tournament, permutation define 0 := (Tc ). Lemma 6,decompose isomorphism : Tc 0 = T0 automorphismTc . following chain equalities holds, proves claim canonicaltournaments:(2)(8)f ((Tc )) = f (T 0 ) = 0 (f (Tc0 )) = 0 (f (Tc )) = 0 ((f (Tc ))) = (f (Tc )). (9)arbitrary tournaments permutations , write (Tc ) obtainf ((T )) = f ((T (Tc ))) = f (( )(Tc )),which, since Tc canonical, equal(2)( )(f (Tc ))) = (T (f (Tc ))) = (f (T ))Condition (9). finishes proof.595fiBrandt & GeistAppendix B. Pseudo-Code Encodingpresent (as illustrative example) simple pseudo-code Algorithm 3 generate CNF form Axiom 1 (functionality tournament choice function; cf. Section 3.1.1).foreach Tournamentforeach Set Xvariable(c(T, X));newClause();foreach Setforeach Set Z 6=variable not(c(T, ));variable not(c(T, Z));newClause();Algorithm 3: Functionality tournament choice functionAppendix C. MUS Search Analysis (Running Time Size MUS)appendix, present complete results running time (Figure 8) MUSsize (measured number clauses; Figure 9) analyses given different sizes randomlysampled domains. setting, sizes = 50 = 100 appear offer good resultsterms running time actually finding small proofs.sample size10112064.4785018.23591008.26992008.695119.51,0005000200 400 600 800 1,00070.5020406080Average running time per proof(in seconds)Number proofs found(out 1000 runs)Figure 8: Number unsatisfiable instances (i.e., proofs found) running time resultsheuristics different numbers sampled tournaments (labeled, 1000 runs).596fiFinding Strategyproof Social Choice Functions via SAT Solvings=102(16,1)10s=205(16,1)s=5006040(16,4)200s=10010050s=200Number proofs founds (in 1000 runs)100200150100500(16,1)(18,29)s=500300200(19,3)1000020406080100120Size proofs (number clauses)140160Figure 9: sizes MUSes (proofs) heuristics different numbers sampledtournaments (labeled). size MUS obtained running full domainindicated red line. improved readability, size multiplicity smallestMUS explicitly listed.597fiBrandt & GeistAppendix D. Profiles Extracted Proof Theorem 3display MUS Figure 6 enriched minimal preference profilesstep proof Theorem 3. profiles generated checked minimalitycomputer (and using SAT solver) less second each.p c n f 341 16218 231 232 233 234 247 248 0Agent 0 : b , c , , , eAgent 1 : , e , c , , bAgent 2 : e , , b , c ,202 330 0c : 1111111111 > [ e ] ; : 1011100111 > [ , e ] ; P : b , , c , e ,Agent 0 : b , , c , e ,Agent 1 : c , , , e , bAgent 2 : e , c , , b ,Agent 3 : , e , , c , bAgent 4 : e , , b , , cManipulated p r e f e r e n c e f g e n 0 : b , , c , , e233 202 0c : 1101100111 > [ e ] ; : 0010100111 > [ ] ; P : b , c , , , e234 202 0c : 1101100111 > [ , e ] ; : 0010100111 > [ ] ; P : b , c , , , eAgent 0 : b , c , , , eAgent 1 : , e , , b , cAgent 2 : e , c , , b ,Manipulated p r e f e r e n c e f g e n 0 : , c , b , , e218 218 0c : 1101100111 > [ ] ; : 1001000100 > [ e ] ; P : e , c , , , b232 232 0c : 1101100111 > [ , b , c , ] ; : 1001000100 > [ , c , , e ] ; P : e , c , , , bAgent 0 : e , c , , , bAgent 1 : , , e , b , cAgent 2 : , , e , b , cAgent 3 : , e , b , c ,Agent 4 : c , e , b , ,Agent 5 : b , c , , e ,Agent 6 : b , , c , e ,Manipulated p r e f e r e n c e f g e n 0 : b , , c , , e248 338 0c : 1101100111 > [ , b , c , , e ] ; : 1100100101 > [ b , c , e ] ; P : 1 > 4 >2 > 3 > 0Agent 0 : 1 > 4 > 2 > 3 > 0Agent 1 : 2 > 3 > 0 > 4 > 1Agent 2 : 2 > 4 > 3 > 1 > 0Agent 3 : 0 > 4 > 3 > 1 > 2Agent 4 : 1 > 0 > 4 > 2 > 3Manipulated p r e f e r e n c e f g e n 0 :1 > 2 > 0 > 4 > 3231 202 0c : 1101100111 > [ b , c , ] ; : 1111111111 > [ e ] ; P : 0 > 4 > 1 > 2 > 3598fiFinding Strategyproof Social Choice Functions via SAT Solving247 202 0c : 1101100111 > [ b , c , , e ] ; : 1111111111 > [ e ] ; P : 0 > 4 > 1 > 2 > 3Agent 0 : 0 > 4 > 1 > 2 > 3Agent 1 : 3 > 1 > 2 > 0 > 4Agent 2 : 4 > 2 > 3 > 1 > 0Manipulated p r e f e r e n c e f g e n 0 :4 > 0 > 3 > 2 > 1314 314 0c : 1100101110 > [ c ] ; :Agent 0 : 1 > 3 > 4 > 0 > 2Agent 1 : 4 > 3 > 1 > 2 > 0Agent 2 : 4 > 1 > 2 > 0 > 3Agent 3 : 2 > 0 > 3 > 4 > 1Agent 4 : 2 > 0 > 3 > 4 > 1Manipulated p r e f e r e n c e f1 > 2 > 0 > 4 > 3c : 1100101110 > [ c ] ; :Agent 0 : 1 > 2 > 3 > 4 > 0Agent 1 : 0 > 3 > 4 > 2 > 1Agent 2 : 0 > 4 > 2 > 3 > 1Agent 3 : 4 > 1 > 2 > 0 > 3Agent 4 : 3 > 4 > 1 > 2 > 0Manipulated p r e f e r e n c e f3 > 1 > 2 > 0 > 41100100101 > [ e ] ; P : 1 > 3 > 4 > 0 > 2agent 0 :1100110110 > [ b ] ; P : 1 > 2 > 3 > 4 > 0agent 0 :318 318 0c : 1100101110 > [ ] ; : 1100100101 > [ b ] ; P : 1 > 3 > 4 > 0 > 2c : 1100101110 > [ ] ; : 1100110110 > [ ] ; P : 1 > 2 > 4 > 0 > 3Agent 0 : 1 > 2 > 4 > 0 > 3Agent 1 : 3 > 4 > 1 > 2 > 0Agent 2 : 4 > 0 > 2 > 3 > 1Agent 3 : 2 > 0 > 3 > 4 > 1Agent 4 : 1 > 0 > 3 > 4 > 2Manipulated p r e f e r e n c e f g e n 0 :3 > 1 > 2 > 0 > 4322 322 0c : 1100101110 > [ c , ] ; : 1100110110 > [ , b ] ; P : 1 > 4 > 0 > 2 > 3Agent 0 : 1 > 4 > 0 > 2 > 3Agent 1 : 2 > 0 > 3 > 4 > 1Agent 2 : 3 > 4 > 1 > 2 > 0Manipulated p r e f e r e n c e f g e n 0 :1 > 0 > 3 > 4 > 2326 326 0c : 1100101110 > [ e ] ; : 1100110110 > [ ] ; P : 1 > 2 > 3 > 4 > 0Agent 0 : 1 > 2 > 3 > 4 > 0Agent 1 : 0 > 3 > 4 > 2 > 1Agent 2 : 0 > 4 > 2 > 3 > 1Agent 3 : 4 > 1 > 2 > 0 > 3Agent 4 : 3 > 4 > 1 > 2 > 0Manipulated p r e f e r e n c e f g e n 0 :3 > 1 > 2 > 0 > 4599fiBrandt & Geist334 202 0c : 1100101110 > [ , e ] ; : 1001111010 > [ ] ; P : 2 > 0 > 3 > 4 > 1Agent 0 : 2 > 0 > 3 > 4 > 1Agent 1 : 1 > 4 > 0 > 2 > 3Agent 2 : 3 > 4 > 1 > 2 > 0Manipulated p r e f e r e n c e f g e n 0 :3 > 1 > 0 > 4 > 2202 0Agent 0 : 4 > 3 > 2 > 1 > 0314 318 322 326 330 334 338 0Agent 0 : 2 > 0 > 3 > 4 > 1Agent 1 : 3 > 4 > 1 > 2 > 0Agent 2 : 4 > 1 > 2 > 0 > 3ReferencesBarbera, S. (1977). Manipulation social decision functions. Journal Economic Theory,15 (2), 266278.Barbera, S. (2010). Strategy-proof social choice. Arrow, K. J., Sen, A. K., & Suzumura,K. (Eds.), Handbook Social Choice Welfare, Vol. 2, chap. 25, pp. 731832.Elsevier.Biere, A. (2008). PicoSAT essentials. Journal Satisfiability, Boolean ModelingComputation (JSAT), 4, 7579.Biere, A., Heule, M., van Maaren, H., & Walsh, T. (Eds.). (2009). Handbook Satisfiability,Vol. 185 Frontiers Artificial Intelligence Applications. IOS Press.Blanchette, J. C., & Nipkow, T. (2010). Nitpick: counterexample generator higherorder logic based relational model finder. Proceedings First InternationalConference Interactive Theorem Proving, pp. 131146. Springer.Brandl, F., Brandt, F., Geist, C., & Hofbauer, J. (2015). Strategic abstention basedpreference extensions: Positive results computer-generated impossibilities. Proceedings 24th International Joint Conference Artificial Intelligence (IJCAI),pp. 1824. AAAI Press.Brandt, F. (2015). Set-monotonicity implies Kelly-strategyproofness. Social ChoiceWelfare, 45 (4), 793804.Brandt, F., & Brill, M. (2011). Necessary sufficient conditions strategyproofness irresolute social choice functions. Proceedings 13th ConferenceTheoretical Aspects Rationality Knowledge (TARK), pp. 136142. ACM Press.Brandt, F., Brill, M., & Harrenstein, P. (2016a). Tournament solutions. Brandt, F.,Conitzer, V., Endriss, U., Lang, J., & Procaccia, A. D. (Eds.), Handbook Computational Social Choice, chap. 3. Cambridge University Press.Brandt, F., Geist, C., & Harrenstein, P. (2016b). note McKelvey uncovered setPareto optimality. Social Choice Welfare, 46 (1), 8191.600fiFinding Strategyproof Social Choice Functions via SAT SolvingBrandt, F., Geist, C., & Peters, D. (2016c). Optimal bounds no-show paradox viaSAT solving. Proceedings 15th International Conference AutonomousAgents Multi-Agent Systems (AAMAS), pp. 314322. IFAAMAS.Brandt, F., Geist, C., & Seedig, H. G. (2014). Identifying k-majority digraphs via SATsolving. Proceedings 1st AAMAS Workshop Exploring Beyond WorstCase Computational Social Choice (EXPLORE).Caminati, M. B., Kerber, M., Lange, C., & Rowat, C. (2015). Sound auction specificationimplementation. Proceedings 16th ACM Conference EconomicsComputation (ACM-EC), pp. 547564. ACM Press.Chatterjee, S., & Sen, A. (2014). Automated reasoning social choice theoryremarks. Mathematics Computer Science, 8 (1), 510.Ching, S., & Zhou, L. (2002). Multi-valued strategy-proof social choice rules. Social ChoiceWelfare, 19 (3), 569580.Cina, G., & Endriss, U. (2015). syntactic proof Arrows theorem modal logicsocial choice functions. Proceedings 14th International ConferenceAutonomous Agents Multi-Agent Systems (AAMAS), pp. 10091017. IFAAMAS.Conitzer, V., & Sandholm, T. (2002). Complexity mechanism design. Proceedings18th Annual Conference Uncertainty Artificial Intelligence (UAI), pp.103110.Drummond, J., Perrault, A., & Bacchus, F. (2015). SAT effective completemethod solving stable matching problems couples. Proceedings 24thInternational Joint Conference Artificial Intelligence (IJCAI), pp. 518525. AAAIPress.Duggan, J., & Schwartz, T. (2000). Strategic manipulability without resoluteness sharedbeliefs: Gibbard-Satterthwaite generalized. Social Choice Welfare, 17 (1), 8593.Erdamar, B., & Sanver, M. R. (2009). Choosers extension axioms. Theory Decision,67 (4), 375384.Feldman, A. (1979). Manipulation Pareto rule. Journal Economic Theory, 21,473482.Fishburn, P. C. (1972). Even-chance lotteries social choice theory. Theory Decision,3 (1), 1840.Fishburn, P. C. (1977). Condorcet social choice functions. SIAM Journal Applied Mathematics, 33 (3), 469489.Fisher, D. C., & Reeves, R. B. (1995). Optimal strategies random tournament games.Linear Algebra Applications, 217, 8385.Frechette, A., Newman, N., & Leyton-Brown, K. (2016). Solving station repacking problem. Proceedings 30th AAAI Conference Artificial Intelligence (AAAI).AAAI Press.Gardenfors, P. (1976). Manipulation social choice functions. Journal Economic Theory,13 (2), 217228.601fiBrandt & GeistGardenfors, P. (1979). definitions manipulation social choice functions. Laffont,J. J. (Ed.), Aggregation Revelation Preferences. North-Holland.Geist, C., & Endriss, U. (2011). Automated search impossibility theorems socialchoice theory: Ranking sets objects. Journal Artificial Intelligence Research, 40,143174.Grandi, U., & Endriss, U. (2013). First-order logic formalisation impossibility theoremspreference aggregation. Journal Philosophical Logic, 42 (4), 595618.Kelly, J. S. (1977). Strategy-proofness social choice functions without single-valuedness.Econometrica, 45 (2), 439446.Laslier, J.-F. (1997). Tournament Solutions Majority Voting. Springer-Verlag.Liffiton, M. H., & Sakallah, K. A. (2008). Algorithms computing minimal unsatisfiablesubsets constraints. Journal Automated Reasoning, 40 (1), 133.McGarvey, D. C. (1953). theorem construction voting paradoxes. Econometrica,21 (4), 608610.McKay, B. D., & Piperno, A. (2013). Practical graph isomorphism, II. Journal SymbolicComputation.Nehring, K. (2000). Monotonicity implies generalized strategy-proofness correspondences. Social Choice Welfare, 17 (2), 367375.Nipkow, T. (2009). Social choice theory HOL: Arrow Gibbard-Satterthwaite. JournalAutomatated Reasoning, 43, 289304.Sanver, M. R., & Zwicker, W. S. (2012). Monotonicity properties adaptionirresolute social choice rules. Social Choice Welfare, 39 (23), 371398.Sato, S. (2008). strategy-proof social choice correspondences. Social Choice Welfare,31, 331343.Sato, S. (2013). sufficient condition equivalence strategy-proofness nonmanipulability preferences adjacent sincere one. Journal Economic Theory, 148, 259278.Scott, A., & Fey, M. (2012). minimal covering set large tournaments. Social ChoiceWelfare, 38 (1), 19.Tang, P., & Lin, F. (2009). Computer-aided proofs Arrows impossibilitytheorems. Artificial Intelligence, 173 (11), 10411053.Tang, P., & Lin, F. (2011). Discovering theorems game theory: Two-person gamesunique pure nash equilibrium payoffs. Artificial Intelligence, 175 (1415), 20102020.Taylor, A. D. (2005). Social Choice Mathematics Manipulation. CambridgeUniversity Press.Tseitin, G. S. (1983). complexity derivation propositional calculus. Automation Reasoning, pp. 466483. Springer.602fiJournal Artificial Intelligence Research 55 (2016) 995-1023Submitted 08/15; published 04/16Distributed Representation-Based FrameworkCross-Lingual Transfer ParsingJiang GuoWanxiang CheJGUO @ IR . HIT. EDU . CNCAR @ IR . HIT. EDU . CNResearch Center Social Computing Information RetrievalHarbin Institute TechnologyHarbin, Heilongjiang, ChinaDavid YarowskyYAROWSKY @ JHU . EDUCenter Language Speech ProcessingJohns Hopkins UniversityBaltimore, MD, USAHaifeng WangWANGHAIFENG @ BAIDU . COMBaidu Inc., Beijing, ChinaTing LiuTLIU @ IR . HIT. EDU . CNResearch Center Social Computing Information RetrievalHarbin Institute TechnologyHarbin, Heilongjiang, ChinaAbstractpaper investigates problem cross-lingual transfer parsing, aiming inducing dependency parsers low-resource languages using training data resource-richlanguage (e.g., English). Existing model transfer approaches typically dont include lexical features, transferable across languages. paper, bridge lexical feature gapusing distributed feature representations composition. provide two algorithmsinducing cross-lingual distributed representations words, map vocabularies two different languages common vector space. Consequently, lexical features non-lexicalfeatures used model cross-lingual transfer. Furthermore, framework flexibleenough incorporate additional useful features cross-lingual word clusters. combinedcontributions achieve average relative error reduction 10.9% labeled attachment scorecompared delexicalized parser, trained English universal treebank transferredthree languages. also significantly outperforms state-of-the-art delexicalized models augmented projected cluster features identical data. Finally, demonstrate modelsboosted minimal supervision (e.g., 100 annotated sentences) target languages, great significance practical usage.1. IntroductionDependency Parsing one long-standing central problems natural language processing (NLP). goal dependency parsing induce implicit tree structures natural languagesentence following dependency grammar, highly beneficial various downstreamtasks, question answering, machine translation knowledge mining/representation.majority work dependency parsing dedicated resource-rich languages, English Chinese. languages, exists large-scale annotated treebanks used2016 AI Access Foundation. rights reserved.fiG UO , C , YAROWSKY, WANG & L IUsupervised training dependency parsers, Penn Treebank (Marcus, Marcinkiewicz,& Santorini, 1993; Xue, Xia, Chiou, & Palmer, 2005). However, languagesworld, even labeled training data parsing, labor intensivetime consuming manually annotate treebanks languages. fact given riserange research unsupervised methods (Klein & Manning, 2004), transfer methods (Hwa, Resnik, Weinberg, Cabezas, & Kolak, 2005; McDonald, Petrov, & Hall, 2011) linguisticstructure prediction.Considering unsupervised methods fall far behind transfer methods termsaccuracy, well difficulty evaluation, focus transfer methods study.attempt build parsers low-resource languages exploiting treebanks resource-richlanguages. two approaches linguistic transfer general, namely data transfermodel transfer. Data transfer methods emphasizes creation artificial training dataused supervised training target language side. appealing propertylearn language-specific linguistic structures effectively. major drawbacksrequirement parallel data noise automatically created training data introducedword alignment-based projection. hand, model transfer methods build modelssource language side, used directly parsing target languages without needcreating annotated data target languages.paper falls latter category. major obstacle transferring parsing systemone language another lexical features (e.g., words) directly transferableacross languages. address challenge, McDonald et al. (2011) built delexicalized parser parser non-lexical features. delexicalized parser makes sense POStag features significantly predictive unlabeled dependency parsing. However, labeleddependency parsing, especially semantic-oriented dependencies like Stanford typed dependencies (De Marneffe et al., 2006; De Marneffe & Manning, 2008), non-lexical featurespredictive enough. Tackstrom, McDonald, Uszkoreit (2012) proposed learn cross-lingualword clusters multilingual paralleled unlabeled data word alignments, applyclusters features semi-supervised delexicalized parsing. Word clusters thoughtkind coarse-grained representations words. Thus, approach partially fills gap lexicalfeatures cross-lingual learning dependency parsing.paper proposes novel approach cross-lingual dependency parsing basedpure distributed feature representations. contrast discrete feature representations usedtraditional dependency parsers, distributed representations map symbolic features continuousrepresentation space, shared across languages. Therefore, model abilityutilize lexical non-lexical features naturally. Specifically, framework contains twoprimary components:neural network-based dependency parser. expect non-linear model dependencyparsing study, distributed feature representations shown effective non-linear architectures linear architectures (Wang & Manning, 2013). ChenManning (2014) proposed transition-based dependency parser using neural networkarchitecture, simple works well benchmark datasets. Briefly, model simply replaces predictor transition-based dependency parser well-designed neuralnetwork classifier. provide explanations merits model Section 3,well adapt cross-lingual task.996fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSINGCross-lingual word representation learning. key filling lexical feature gapproject representations features different languages common vectorspace, preserving translational equivalence. study compare two approacheslearning cross-lingual word representations Section 4. first approach namedrobust projection, second approach based canonical correlation analysis.approaches simple implement scalable large data.Another drawback model transfer methods focus universal structures across various languages, thus lack ability recovering target language-specificstructures. Therefore, necessary conduct target language adaptation top transferred models. introduce practical straightforward solution incorporating minimalsupervision target languages (Section 6).evaluate models universal multilingual treebanks v2.0 (McDonald et al., 2013).Case studies include transferring English (EN) German (DE), Spanish (ES) French(FR). Experiments show incorporating lexical features, performance cross-lingualdependency parsing improved significantly. embedding cross-lingual cluster features (Tackstrom et al., 2012), achieve average relative error reduction 10.9% labeledattachment score (LAS), compared delexicalized parsers. also significantly outperforms delexicalized models McDonald et al. augmented cluster features identicaldata. addition, show using small amount labeled training data (e.g., 100 sentences) target language side parameter adaptation (minimal supervision), performancecross-lingual transfer system boosted, recalls language-specific dependencystructures improved dramatically.1original major contributions paper include:propose novel flexible cross-lingual learning framework dependency parsingbased distributed representations, effectively incorporate lexical nonlexical features.present two novel effective approaches inducing cross-lingual word representationbridge lexical feature gap cross-lingual dependency parsing transfer.show cross-lingual word cluster features effectively embedded model,leading significant additive improvements.show cross-lingual transfer systems easily effectively adaptedtarget languages minimal supervision, demonstrating great potential practical usage.2. Backgroundsection describes necessary background crucial understanding transferparsing framework.1. article thoroughly revised extended version work Guo, Che, Yarowsky, Wang, Liu (2015).provide detailed linguistic methodological background cross-lingual parsing. Additional extensionsprimarily include experiments analysis target language adaptation minimal supervision. systemmade publicly available at: https://github.com/jiangfeng1124/acl15-clnndep.997fiG UO , C , YAROWSKY, WANG & L IUpunctrootdobjnsubjROOTPRONamodVERBgoodADJcontrolNOUN..Figure 1: example labeled dependency tree.2.1 Dependency ParsingGiven input sentence x = w0 w1 ...wn wi ith word x, goal dependencyparsing build dependency tree, denoted = {(h, m, l) 0 h n; 0 <n, l L}, (h, m, l) indicates directed arc head word wh modifier wmdependency label l, L label set (Figure 1).mainstream models proposed dependency parsing describedeither graph-based models transition-based models (McDonald & Nivre, 2007). Graph-basedmodels (Eisner, 1996; McDonald, Crammer, & Pereira, 2005) view parsing problem findinghighest scoring tree directed graph. score dependency tree typically factoredscores small independent structures. way factorization defines ordermodel also complexity inference process (McDonald & Pereira, 2006; Carreras,2007; Koo & Collins, 2010). instance, first-order models factored dependency arcs,thus also known arc-factored models. Higher-order models would consider expressivesubstructures sibling grandchild structures. Transition-based models instead aimpredict transition sequence initial parser state terminal states, conditionedparsing history (Yamada & Matsumoto, 2003; Nivre, 2003; Nivre, Hall, & Nilsson, 2004).approach lot interest since fast (linear time projective parsing) incorporaterich non-local features (Zhang & Nivre, 2011).considered past simple transition-based parsing using greedy decodinglocal training accurate graph-based parsers globally trained use exactinference algorithms. However, Chen Manning (2014) showed greedy transition-basedparsers significantly improved well-designed neural network architecture. approach considered new paradigm parsing, based pure distributedfeature representations. recently, architecture improved different ways.example, Weiss, Alberti, Collins, Petrov (2015) combined neural network structuredperceptron, use beam-search decoding, achieving new state-of-the-art performance. Dyer, Ballesteros, Ling, Matthews, Smith (2015) instead explored novel techniques learningbetter representations parser states utilizing long short-term memory networks (LSTM).work also includes Zhou, Zhang, Huang, Chen (2015) applied structured learningbeam-search decoding neural network model. study, choose originalChen & Mannings architecture, without losing generality, build basic dependency parsingmodels cross-lingual transfer.998fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING2.2 Distributed Representations NLPRecent years seen numerous attempts learning distributed representations different natural language objects, morphemes, words phrases, sentences documents. Usingdistributed representations, symbolic units embedded dense, continuous lowdimensional vector space, thus often referred embeddings.2Distributed representation attractive NLP several reasons. First, provides straightforward way measuring similarities natural language objects. distributedrepresentations, easily tell two words/phrases/documents similar semanticeven aspects simply measuring cosine distance vectors.Second, learned large-scale unannotated data general, thus highly beneficial various downstream applications source alleviate data sparsity.straightforward way applying distributed representations NLP tasks fed distributedfeature representations existing supervised NLP systems augmented features, semisupervised fashion (Turian, Ratinov, & Bengio, 2010). Despite simplicity effectiveness,shown potential distributed representations cannot fully exploited generalized linear models adopted traditional NLP systems (Wang & Manning,2013). One remedy discretize distributed feature representations, convert continuous, dense low-dimensional vectors traditional discrete, sparse high-dimensionalspace, studied Guo, Che, Wang, Liu (2014). However, believe non-linear system(e.g., neural network) powerful promising solution. decent progress alreadymade paradigm NLP various tasks, neural sequence labeling (Collobertet al., 2011), dependency parsing (Chen & Manning, 2014), sentence classification (Kim, 2014)machine translation (Sutskever, Vinyals, & Le, 2014).Third, provides kind representation shared across languages, taskseven diverse modalities data resources. property motivated lines research multilingual representation learning (Klementiev et al., 2012; Chandar P et al., 2014; Hermann &Blunsom, 2014), multi-task learning (Collobert & Weston, 2008) multi-modal learning (Srivastava & Salakhutdinov, 2012). also primary motivation work facilitatescross-lingual transfer parsing via multilingual distributed representation learning words.3. Cross-Lingual Dependency Parsingsection, first describe primary transition-based dependency parsing model utilizingneural networks, details cross-lingual transfer.3.1 Neural Network Architecture Transition-Based Dependency Parsingsection, first briefly describe transition-based dependency parsing arc-standardparsing algorithm. revisit neural network architecture transition-based dependencyparsing proposed Chen Manning (2014).discussed Section 2.1, transition-based parsing generates dependency tree predicting transition sequence initial parser state terminal state. Several transition-basedparsing algorithms presented literature, arc-standard arc-eager algorithms projective parsing (Nivre, 2003, 2004), list-based algorithm (Nivre, 2008)2. paper, two terminologies used interchangeably.999fiG UO , C , YAROWSKY, WANG & L IUswap-based algorithm (Nivre, 2009) non-projective parsing. Different algorithms differenttransition actions. Take arc-standard algorithm example, parsing state (typically knownconfiguration) represented tuple consisting stack S, buffer B, partially derived forest (i.e., set dependency arcs) A. Given input word sequence x = w1 w2 , ..., wn ,initial configuration represented as: [w0 ]S , [w1 w2 , ..., wn ]B , , terminal configuration [w0 ]S , []B , A, w0 pseudo word indicating root whole dependencytree. Denoting Si (i = 0, 1, ...) ith element stack, Bi (i = 0, 1, ...) ith element buffer,3 arc-standard system defines three types transition actions: L EFT-A RC(r),R IGHT-A RC(r), HIFT, r dependency relation.rL EFT-A RC(r): extend new arc (S1S0 ) (S0 head S1 modifier)remove S1 stack.rR IGHT-A RC(r): extend new arc (S1S0 ) (S1 head S0 modifier)pop S0 stack.HIFT: move B0 buffer stack. Precondition B empty.typical approach greedy arc-standard parsing build multi-class classifier (e.g.,support vector machines, maximum entropy models) predicting transition action given feature vector extracted specific configuration. conventional feature engineering suffersproblem sparsity, incompleteness expensive feature computation (Chen & Manning,2014), neural network model provides effective solution.architecture neural network based dependency parsing model illustrated Figure 2. Unlike high-dimensional, sparse discrete features used traditional parsing models,neural network model, apply distributed feature representations. Primarily, three typesinformation extracted configuration Chen & Mannings model: word features, POSfeatures relation features respectively. study, add non-local features including distance features indicating distance two items, valency features indicatingnumber children given item (Zhang & Nivre, 2011). distance valency featuresdiscretized buckets. features projected embedding layer via corresponding lookup tables (i.e., embedding matrices), estimated trainingprocess. complete feature templates used system shown Table 1.Then, feature compositions performed hidden layer via cube activation function:h = g(x) = (W1 [xw , xt , xr , xd , xv ] + b1 )3W1 weight matrix input layer hidden layer, b1 bias vector.Feature compositions important dependency parsing NLP general.Researchers used cost-intensive manual feature engineering design large set featuretemplates. However, approach cannot cover potentially useful features. Lei, Xin, Zhang,Barzilay, Jaakkola (2014) showed full feature representation derivedKronecker product multiple views features, results tensor model. representingtensor low-rank form using C ANDECOMP /PARAFAC (CP) tensor decomposition (Kolda &Bader, 2009), number parameters effectively reduced, thus suitable taskslimited training data (Cao & Khudanpur, 2014).3. S0 /B0 top/head element stack/buffer.1000fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSINGSoftmax Layer:= ( )Hidden Layer:= = ( + )3Transition ActionsHidden RepresentationInput Layer:= [ , , , , , ]Feature ExtractionWordsClustersLexical featuresROOTParsing ConfigurationsStackhas_VERBLookup Tables1POS tags,RelationsDistance,ValencyNon-lexical featuresgood_ADJBufferControl_NOUN._.nsubjHe_PRONFigure 2: Neural network model dependency parsing. Cluster features introducedSection 5.2 5.3.TypeFeature Templatesw, = 0, 1, 2ESwi , EBWordwwwwElc1(S, Erc1(S, Elc2(S, Erc2(S, = 0, 1i)i)i)i)wwElc1(lc1(S, Erc1(rc1(S, = 0, 1)))), = 0, 1, 2ESt , EBPOSElc1(S, Erc1(S, Elc2(S, Erc2(S, = 0, 1i)i)i)i)Elc1(lc1(S, Erc1(rc1(S, = 0, 1))))RelationrrrrElc1(S, Erc1(S, Elc2(S, Erc2(S, = 0, 1i)i)i)i)rrElc1(lc1(S, Erc1(rc1(S, = 0, 1))))DistanceES, ES0 ,S10 ,B0ValencyESlv0 , ESlv1 , ESrv1Table 1: Feature templates neural network model transition-based dependency parsing.{w,c,t,r,d,lv,rv}Epindicates various feature embeddings element position p. lc1(rc1) first child left (right) lc2 (rc2) second child left (right).indicates lexical features, indicates non-lexical features.suggest cube activation function g(x) = x3 viewed special caselow-rank tensor. verification, g(x) expanded as:g(w1 x1 + ... + wm xm + b) =(wi wj wk )xi xj xk + b(wi wj )xi xj + ...i,j,ki,j1001fiG UO , C , YAROWSKY, WANG & L IUtreat bias term b x0 x0 = 1, weight corresponding featurecombination xi xj xk wrote wi wj wk , exactly rank-1 component tensor low-rank form using CP tensor decomposition. Consequently, cube activation functionimplicitly derives full feature combinations. fact, add many features possibleinput layer improve parsing accuracy. show Section 5.2 Brown-clusterfeatures readily incorporated model.composed features propagated output layer, generating probabilistic distribution output labels (i.e., transition actions) via softmax activation function: =sof tmax(W2 h). use following objective function train model:J () =1 N2CrossEnt(di , yi ) +N i=02CrossEnt(p, q) cross-entropy two distributions p q:CrossEnt(p, q) = pk ln qkkparameters trained using back-propagation. model, typically consistsembedding matrices weights network. However, cases, may excludeword embedding matrix E w , indicates word embeddings constrained fixed(i.e., without updating) training.3.2 Cross-Lingual Transferidea cross-lingual transfer using parser examined straightforward. contrasttraditional approaches discard rich lexical features (delexicalizing) transferringmodels one language another, model transferred using full model trainedsource language side (i.e., English).Since non-lexical feature (POS, relation, distance, valency) embeddings directly transferable languages, key component framework cross-lingual learninglexical feature embeddings (i.e., word embeddings). cross-lingual word embeddingsinduced, first learn dependency parser source language side. that, parserdirectly used parsing target language data.3.2.1 U NIVERSAL EPENDENCIESdiscussed previously, cross-lingual model transfer assumes universal grammatical structuresidentified multiple languages. Therefore, evaluated test set target languageeither unlabeled attachment score (UAS) labeled attachment score (LAS), performancetransfer parsing rely heavily multilingual consistency annotation schemes. Generallysyntactic annotation schemes differ head-finding rules (e.g., choice lexical versus functional head) dependency relation labels (i.e., syntactic tagset). challenging taskconstruct multilingual treebanks consistent annotations. initial cross-lingual parsing studies, CoNLL shared task datasets (Buchholz & Marsi, 2006) broadly used. However,inconsistencies occur head-finding rules syntactic tagset across languages,made difficult evaluate cross-lingual parsers.1002fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSINGorder overcome difficulties, new collection multilingual treebanks homogeneous syntactic dependency annotation presented recently, namely Universal Dependency Treebanks (UDT) (McDonald et al., 2013). universal annotation scheme createdharmonizing available treebanks slightly different variants Stanford typed dependencies (De Marneffe et al., 2006), along universal Part-of-Speech tags (Petrov, Das, & McDonald, 2012). dataset greatly facilitates research multilingual syntactic analysis, alsomakes possible use LAS evaluation. fact, UDT already used standarddataset benchmarking research cross-lingual transfer parsing (Ma & Xia, 2014; Tiedemann,2014; Zhang & Barzilay, 2015; Duong, Cohn, Bird, & Cook, 2015a, 2015b; Rasooli & Collins,2015). efforts towards universal dependencies include recent Universal Dependencies project (UD) 4 HamleDT (Zeman et al., 2014). paper, conduct experimentsUDT (v2.0) 5 dataset without losing generality.3.2.2 P ROJECTIVE VS . N - PROJECTIVE PARSINGNon-projectivity common phenomenon multilingual dependency parsing. term nonprojectivity indicates dependency tree crossing-arcs, often appear morphologically rich languages. Various algorithms proposed graph-based transitionbased parsing algorithms produce non-projective trees. example, arc-standard algorithm(Section 3.1) readily extended adding swap action handle non-projectivity,gives expected linear worst-case O(n2 ) complexity (Nivre, 2009). strategies includelist-based algorithm (Nivre, 2008) adapted Covington algorithm (Covington, 2001), combination list-based swap-based algorithm (Choi &McCallum, 2013). Unfortunately, systematically comparison differentalgorithms literature far.study, however, focus projective parsing non-projectivetrees source language (English) training data. Consequently, non-projectivities target languages handled moment.64. Cross-Lingual Word Representation LearningPrior introducing approaches cross-lingual word representation learning, briefly reviewbasic model learning monolingual word embeddings, constitutes subprocedurecross-lingual approaches.4.1 Continuous Bag-of-Words Modelrecent years, various approaches studied learning word embeddings largescale plain texts. approaches generally derived so-called distributional hypothesis (Firth, 1957): shall know word company keeps. study, considerContinuous Bag-of-Words (CBOW) model (Mikolov, Chen, Corrado, & Dean, 2013) imple4. https://universaldependencies.github.io/docs/5. https://github.com/ryanmcd/uni-dep-tb6. Note target languages address paper, non-projectivity pervasive. Specifically, proportion projective trees presented training corpus respectively 91% DE, 94% ES, 88%FR.1003fiG UO , C , YAROWSKY, WANG & L IUmented open-source toolkit word2vec.7 basic principle CBOW model predictindividual word sequence given bag context words within fixed window sizeinput, using log-linear classifier. model avoids non-linear transformation hiddenlayers, hence trained high efficiency.large window size, grouped words using resulting word embeddings topically similar; whereas small window size, grouped words syntactically similar (Bansal, Gimpel, & Livescu, 2014). set window size 1 parsing task.Next, introduce approach inducing bilingual word embeddings. general, expectbilingual word embeddings preserve translational equivalences. example, cooking (English) close translation: kochen (German) embedding space.4.2 Robust Alignment-Based Projectionfirst method inducing cross-lingual word embeddings two stages. First, learn wordembeddings source language (S) corpora monolingual case, projectmonolingual word embeddings target language (T), based word alignments.Given sentence-aligned parallel corpus D, first conduct unsupervised bidirectional wordalignment, collect alignment dictionary. Specifically, word-aligned sentence pairD, keep alignments conditional alignment probability exceeding threshold = 0.95discard others. Specifically, let = {(wiT , wjS , ci,j ), = 1, 2, ..., NT ; j = 1, 2, ..., NS }alignment dictionary, ci,j number times ith target word wiT alignedj th source word wjS . NS NT vocabulary sizes. use shorthand (i, j)denote word pair . projection formalized weighted averageembeddings translation words:ci,jv(wiT ) =v(wjS )(1)ci,(i,j)ATci, = j ci,j , v(w) embedding w.Obviously, simple projection method one drawback: assigns word embeddingstarget language words occur word aligned data, typically smallermonolingual datasets. Therefore, order improve robustness projection, utilizemorphology-inspired mechanism, propagate embeddings in-vocabulary words out-ofTvocabulary (OOV) words. Specifically, OOV word woov, extract list candidatewords similar terms edit distance (Levenshtein distance), set averagedvector embedding woov. formally,v(woov) = Avg (v(w ))w CC = {ww EditDist(woov, w) }(2)reduce noise, choose small edit distance threshold = 1.process robust projection viewed two-stage graph-propagation algorithm,illustrated Figure 3 (left panel). Embeddings first propagated source language wordstarget language words appear bilingual lexicons. Next, monolingual propagationperformed obtain OOV word embeddings target language, using edit distance metric.7. http://code.google.com/p/word2vec/1004fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING1Source Language1Bilingual Lexicon(weighted)Target LanguageParallel dataWiktionaryPanLex21222CCA1In-Vocabulary wordsOut-of-Vocabulary wordsFigure 3: Illustration robust projection (left) CCA (right) inducing cross-lingual wordembeddings.4.3 Canonical Correlation Analysissecond approach consider similar Faruqui Dyer (2014), uses CCAimprove monolingual word embeddings multilingual correlation. CCA way measuringlinear relationship multidimensional variables. two multidimensional variables,CCA aims find two projection matrices map original variables new basis (lowerdimensional), correlation two variables maximized.refer readers work Hardoon, Szedmak, Shawe-Taylor (2004) theoreticalfoundations algorithm specifics CCA. lets treat CCA black box, see CCAapplied inducing bilingual word embeddings. Suppose already two pre-trainedmonolingual word embeddings (e.g., English German): Rn1 d1 Rn2 d2 .first step, extract one-to-one alignment dictionary alignment dictionaryAST .8 Here, , indicating every word translated one word , viceversa.process illustrated Figure 3 (right panel). Denoting dimension resulting wordembeddings min(d1 , d2 ). First, derive two projection matrices V Rd1 , W Rd2respectively using CCA:V, W = CCA( , )(3)Then, V W used project entire vocabulary := V,= W(4)Rn1 Rn2 resulting word embeddings cross-lingual task.8. also worth trying, observed slight performance degradation experimental setting.1005fiG UO , C , YAROWSKY, WANG & L IU4.4 Pros ConsContrary robust projection approach, CCA assigns embeddings every word monolingual vocabulary. However, one potential limitation CCA assumes linear transformationword embeddings, difficult satisfy. mean time, training source languageparser using CCA cross-lingual word embeddings, constrained E w fixed,mentioned Section 3.1, otherwise, translational equivalence broken. robust projection approach, however, doesnt limitation. discussion experimentspresented Section 5.3.2.Note approaches generalized lower-resource languages parallel bitextsavailable. way, dictionary readily obtained either using bilingual lexiconinduction approaches (Mann & Yarowsky, 2001; Koehn & Knight, 2002; Haghighi, Liang, BergKirkpatrick, & Klein, 2008), online-resources like Wiktionary9 Panlex.105. Experimentssection describes experiments. first describe data settings used experiments, results.5.1 Data Settingspre-training word embeddings, use WMT-2011 monolingual news corporaEnglish, German Spanish.11 French, combined WMT-2011 WMT-2012 monolingual news corpora.12 got word alignment counts using fast-align toolkit cdec (Dyeret al., 2010) parallel news commentary corpora (WMT 2006-10) combined Europarl corpus English{German, Spanish, French}.13training neural network dependency parser, set number hidden units400. dimension embeddings different features shown Table 2.Dim.Word50POS50Label50Distance5Valency5Cluster8Table 2: Dimensions various types feature embeddings.Mini-batch adaptive stochastic gradient descent (AdaGrad) (Duchi, Hazan, & Singer, 2011)used optimization. CCA approach, use implementation Faruqui Dyer(2014).employ universal dependency treebanks (UDT v2.0) reliable evaluationapproach cross-lingual dependency parsing. universal multilingual treebanks annotatedusing universal POS tagset (Petrov et al., 2012) contains 12 POS tags, welluniversal dependencies defines 40 dependency relations. follow standard splittreebanks languages.9.10.11.12.13.https://www.wiktionary.org/http://panlex.org/http://www.statmt.org/wmt11/http://www.statmt.org/wmt12/http://www.statmt.org/europarl/1006fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING5.2 Baseline Systemscompare approach following systems.first baseline, evaluate delexicalized transfer neural network-based parser[D ELEX], use non-lexical features (Figure 2). investigate effectnon-local features (distance, valency). delexicalized systems includenon-local features referred [D ELEX (basic)].also compare approach delexicalized parser presented McDonald et al.(2013) [M C D13], used perceptron-based transition-based parser beam size 8,along richer non-local features (Zhang & Nivre, 2011). re-implementation approachframework Zpar (Zhang & Clark, 2011) referred [M C D13 ].Furthermore, consider strong baseline system proposed Tackstrom et al. (2012),utilized cross-lingual word cluster features enhance perceptron-based delexicalizedparser [M C D13 +Cluster]. use alignment dictionary described Section 4.2induce cross-lingual word clusters. re-implement P ROJECTED clustering approach described work Tackstrom et al., assigns target word clusteroften aligned:c(wiT ) = arg max ci,j 1[c(wjS ) = k]k(i,j)ATObviously, method also drawback words occur alignment dictionary (OOV) cannot assigned cluster. Therefore, use strategy describedSection 4.2 find likely clusters OOV words. Instead computing averageembeddings, solve argmax problem:) = arg maxc(woovk1[c(w ) = k]w C(5), w) }C = {wEditDist(woovset 1 constantly. Instead clustering model Uszkoreit Brants (2008), useBrown clustering (1992) induce hierarchical word clusters, word representedbit-string. use word cluster feature templates Tackstrom et al. (2012), setnumber Brown clusters 256.5.3 Experimental Resultsparsing models trained using development data English early-stopping.Table 3 lists results cross-lingual transfer experiments dependency parsing. Table 4summarizes experimental gains detailed Table 3.first examine benefit brought non-local distance valency features. observedcomparison ELEX (basic) ELEX, marginal improvements obtained DEFR, significant improvements ES. Therefore, adopted featuresfollowing experiments.delexicalized system obtains slightly lower performance reported McDonaldet al. (2013) (M C D13), used greedy decoding local training. re-implementationMcDonald et al.s work attains comparable performance C D13. languages consider study, using cross-lingual word embeddings either alignment-based projectionCCA, obtain statistically significant improvements delexicalized system,1007fiG UO , C , YAROWSKY, WANG & L IUELEX (basic)ELEXP ROJP ROJ+ClusterCCACCA+ClusterUnlabeled Attachment Score (UAS)ENDEESFRAVG83.63 56.85 67.28 68.70 64.2883.67 57.01 68.05 68.85 64.6491.96 60.07 71.42 71.36 67.6292.33 60.35 71.90 72.93 68.3990.62 59.42 68.87 69.58 65.9692.03 60.66 71.33 70.87 67.62Labeled Attachment Score (LAS)ENDEESFRAVG79.37 47.06 56.43 57.73 53.7479.42 47.12 56.99 57.78 53.9690.48 49.94 61.76 61.55 57.7590.91 51.54 62.28 63.12 58.9888.88 49.32 59.65 59.50 56.1690.49 51.29 61.69 61.50 58.16C D1383.3358.5068.0770.1465.5778.5448.1156.8658.2054.39C D13C D13 +Cluster84.4490.2157.3060.5568.1570.4369.9172.0165.1267.6680.3088.2847.3450.2057.1260.9658.8061.9654.4257.71Table 3: Cross-lingual transfer dependency parsing English test dataset 4 universal multilingual treebanks. Results measured unlabeled attachment score (UAS)labeled attachment score (LAS). ELEX (basic) delexicalized model without nonlocal features (distance, valency). denotes re-implementation C D13. Sincemodel varies different target languages CCA-based approach, indicatesaveraged UAS/LAS.Experimental ContributionP ROJvs. ELEXCCAvs. ELEXP ROJvs. C D13CCAvs. C D13P ROJ+Clustervs. P ROJCCA+Clustervs. CCAC D13 +Cluster vs. C D13P ROJ+Clustervs. ELEXCCA+Clustervs. ELEXP ROJ+Clustervs. C D13CCA+Clustervs. C D13P ROJ+Clustervs. C D13 +ClusterCCA+Clustervs. C D13 +ClusterDE/ES/FR Avg. (Relative)+3.79 (8.2%)+2.19 (4.8%)+3.33 (7.3%)+1.74 (3.8%)+1.23 (2.9%)+2.00 (4.6%)+3.29 (7.2%)+5.02 (10.9%)+4.20 (9.1%)+4.46 (9.8%)+3.74 (8.2%)+1.27 (3.0%)+0.45 (1.1%)Table 4: Summary experimental gains detailed Table 3, absolute LAS gainrelative error reduction. gains statistically significant using MaltEval (Nilsson& Nivre, 2008) p < 0.01.UAS LAS. Interestingly, notice P ROJ consistently outperforms CCA significantmargin, comparable C D13 +Cluster. analysis observation conducted Section 5.3.1 5.3.2.1008fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSINGTypeClusterFeature TemplatescESc , EB, = 0, 1, 2ccccElc1(Si ) , Erc1(S, Elc2(S, Erc2(S, = 0, 1i)i)i)ccElc1(lc1(S, Erc1(rc1(S, = 0, 1))))Table 5: Word cluster feature templates.framework flexible incorporating richer features simply embeddingcontinuous vectors. Thus embed cross-lingual word cluster features model,together proposed cross-lingual word embeddings. cluster feature templates shownTable 5, similar POS tag feature templates. shown Table 3, significantadditive improvements obtained P ROJ CCA embedding cluster features.Compared delexicalized system, relative error reduced 13.1% UAS,12.6% LAS. combined system outperforms C D13 +Cluster significantly .5.3.1 E FFECT ROBUST P ROJECTIONSince P ROJ induction cross-lingual word clusters, use edit distance measureOOV words, would like see affects performance parsing.Intuitively, higher coverage projected words test dataset promote parsingperformance more. verify this, conduct experiments settings usingP ROJ+Cluster model. robust projection, examine effect edit distances ranging1 3. Results shown Table 6. Improvements observed languages usingrobust projection edit distance measure, especially FR, highest coverage gainobtained robust projection. also observe slightly improvements DE ES usingedit distance 2. performance starts degrade gets larger. reasonable, sincelarger edit distance increases word coverage, also introduces noise.SimpleDEESFRcoverageUASLAScoverageUASLAScoverageUASLAS91.3759.7450.8494.5170.9761.3490.8371.1761.72=194.7060.3551.5496.6771.9062.2897.6072.9363.12Robust=296.5060.5351.7097.7572.0062.3498.3372.7963.02Table 6: Effect robust projection.1009=397.4760.5351.6998.4771.9362.2798.5872.7062.94fiG UO , C , YAROWSKY, WANG & L IU5.3.2 E FFECT F INE -T UNING W ORD E MBEDDINGSAnother reason effectiveness P ROJ CCA lies fine-tuning word embeddingstraining parser.CCA viewed joint method inducing cross-lingual word embeddings.training source language dependency parser cross-lingual word embeddings derivedCCA, EN word embeddings fixed. Otherwise, translational equivalencebroken. However, P ROJ, limitation. Word embeddings updatednon-lexical feature embeddings, order obtain accurate dependency parser. referprocedure fine-tuning process word embeddings. verify benefits fine-tuning,conduct experiments see relative loss word embeddings fixed training. Resultsshown Table 7, indicates fine-tuning indeed offers considerable help.DEESFRUASLASUASLASUASLASFixed59.7449.4470.1061.3170.6560.69Fine-tuning60.0749.9471.4261.7671.3661.50+0.33+0.50+1.32+0.45+0.71+0.81Table 7: Effect fine-tuning word embeddings.5.4 Compare Existing Bilingual Word Embeddingssection, compare bilingual embeddings several previous approaches context dependency parsing. best knowledge, first work evaluationbilingual word embeddings syntactic tasks.approaches consider include multi-task learning approach (Klementiev et al., 2012)[MTL], bilingual auto-encoder approach (Chandar P et al., 2014) [B IAE], bilingual compositional vector model (Hermann & Blunsom, 2014) [B ICVM], bilingual bag-of-wordsapproach (Gouws et al., 2015) [B ILBOWA].MTL B IAE, adopt released word embeddings directly due inefficiencytraining.14 B ICVM B ILBOWA, re-run systems dataset previousexperiments.15 Results summarized Table 8.CCA P ROJ consistently outperforms approaches languages, P ROJ performs best. inferior performance MTL B IAE partly due low word coverage.example, cover 31% words universal DE test treebank, whereas CCAP ROJ covers 70%. Moreover, B IAE, B ICVM B ILBOWA introduce sentence-level translational equivalence objectives regularizers learning bilingual word embeddings.approaches advantageous dont assume/require word alignment. However, word-toword translational equivalence cannot well preserved way.14. MTL embeddings normalized training.15. B ICVM uses bilingual parallel dataset.1010fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSINGMTL (Klementiev et al., 2012)B IAE (Chandar P et al., 2014)B ICVM (Hermann & Blunsom, 2014)B ILBOWA (Gouws et al., 2015)CCAP ROJDEUASLAS56.93 46.2253.74 43.6856.30 46.9954.51 44.9559.42 49.3260.07 49.94ESUAS67.7158.8167.7867.2368.8771.42LAS58.4346.6658.0856.1659.6561.76FRUASLAS67.51 57.2760.10 49.4769.13 58.1364.82 52.7369.58 59.5071.36 61.55Table 8: Comparison existing bilingual word embeddings. MTL B IAE, usereleased bilingual word embeddings.Target Word (ES)china(china)problemas(problems)septiembre(september)P ROJindiarussiataiwanchineseproblemdifficultiestroublesissuesoctoberaugustjanuarydecemberCCArussiaindonesiabeijingchineseproblemswoestroublesdilemmasdecemberjulyoctoberjuneNeighboring Words (EN)MTLB IAEchinakoreaindependent indiasumitomochinesemalaysianbrazileventsproblemsanctionsgreatlyconditionshighlightedlawsscaledecembermonthfebruaryapriljulyscheduledmarchnovemberB ICVMchinesechinoissino33.55problematicproblematicaldifficultiestroubles11th11.0011eleventhB ILBOWAhelsinkibulgariansconstitutingmarketdeficienciessituationsomissionsattentivelya.mp.mtwelve1998-1999Table 9: Target words Spanish 4 similar words English, induced variousapproaches.verify assumption, taking EN/ES case study. manually inspect 4similar words (by cosine similarity) English given set words Spanish (Table 9).observe semantic syntactic shifting k-nearest neighbors prediction B IAE,B ICVM B ILBOWA, whereas P ROJ CCA give translational equivalent predictions.example, B ICVM yields adjective like problematical target noun problemas; B ILBOWA yieldssemantic-related word market china. general, P ROJ robust approach, behavingconsistently well sampled words.worth noting dont assume/require bilingual parallel data CCA P ROJ.need practice bilingual lexicon paired languages. especially importantgeneralizing approaches lower-resource languages, parallel texts available.1011fiG UO , C , YAROWSKY, WANG & L IU6. Target-Language Adaptation Minimal Supervisionimportant us distinguish linguistic structures learned via cross-lingual transferversus learned basis monolingual information languageparsed. Intuitively, cross-lingual approaches learn common dependency structuresshared source target language. However, many languages,specialized (language-specific) syntactic characteristics learned datatarget language.Take adjective-noun order example, Spanish French, adjectives often appearsnouns, thus forming right-directed arc labeled amod, whereas English, amod(adjectival modifier) arcs mostly left-directed, illustrated Figure 4. Another examplesubject-verb-object order. German, verbs often appear end sentence V2 position,causes much left-directed dobj (direct object) arcs English (Figure 5).differences clearly observed universal treebanks. Table 10 shows significantdistribution divergence left-directed right-directed arcs dobj amod relationstreebanks different languages.Relation: dobj; Language: EN vs. DEdobjdobjratioEN38,39576450.3 : 1DE4,2773,4571.2 : 1Relation: amod; Language: EN vs. ES, FRamod amodratioEN1,66757,8641 : 34.7ES14,8765,2052.9 : 1FR12,9194,9102.6 : 1Table 10: Distribution divergences left-directed right-directed arcs dobj relation ENDE (top), amod relation EN ES/FR (bottom).amodamodNOUNADJNOUNADJSpanish:ConsejoSuperiorconflictossocialesADJNOUNADJNOUNEnglish:SuperiorCouncilsocialconflictsamodamodFigure 4: Reverse direction amod relation Spanish English. French also adjectives following nouns.1012fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSINGrootadvmoddetdobjADVDETNOUNVERBDE:endlichdenrichtigengefundenEN:finallyfoundright manADVVERBDETNOUNadvmoddetdobjrootFigure 5: Reverse direction dobj relation German English.Therefore, section, investigate much cross-lingual transfer model improved annotating small amount labeled training data target language side. Even thoughbuilding large-scale treebanks low-resource languages supervised learning costly, annotating dependency structures small amount sentences (e.g., 100) difficult.still conduct experiments universal dependency treebanks, provide labeledtraining data multiple languages. language studied (DE, ES, FR), incrementallyaugment amount labeled sentences 100 1,000 step 100, adapt parameters cross-lingual transfer model specific target language. Theoretically, since targetlanguage treebanks contain non-projective trees, would make sense apply non-projectivealgorithms (e.g., swap-based) target language adaptation. way, however, W2re-trained scratch, doesnt show good performance experiments since minimally supervised data small. Consequently, still rely arc-standard algorithmadaption. process almost training source language parser describedSection 3, except word embedding matrix E w fixed, rest parameters(E {t,l,d,v,c} , W1 , W2 , b1 ) optimized using augmented labeled data target language,taking Equation 3.1 objective function. development data used process, thussimply perform parameter updating 2,000 iterations.addition, built another strong baseline system employs augmented labeledtraining data supervised learning. system, utilize word embeddings Brownclusters features, derived separately language.shown Figure 6, results really promising. P ROJ+Cluster CCA+Clustersystems consistently outperform delexicalized system supervised system significant margin. P ROJ+Cluster CCA+Cluster general achieve comparable performances,CCA+Cluster slightly better.worthy noting performances P ROJ+Cluster CCA+Cluster boostedaugmenting 100 sentences. Take DE example, UAS increased 60.35% 68.91%,LAS 51.54% 61.54%, nearly equal effect using 1,000 sentencessupervised learning. observation demonstrates great potential cross-lingual transfersystem practical usage.1013fi85857580G UO , C , YAROWSKY, WANG & L IU80UAS75UAS65UAS6075708070PROJ+ClusterCCA+ClusterDelexicalizedSupervised020040060080010000200400600800100020060080010007575607070LAS5065LAS6555LAS40080PROJ+ClusterCCA+ClusterDelexicalizedSupervisedLabeled training data (FR)650Labeled training data (ES)8070Labeled training data (DE)6565PROJ+ClusterCCA+ClusterDelexicalizedSupervised455070550200400600800Labeled training data (DE)100060PROJ+ClusterCCA+ClusterDelexicalizedSupervisedPROJ+ClusterCCA+ClusterDelexicalizedSupervised556055PROJ+ClusterCCA+ClusterDelexicalizedSupervised40450200400600800Labeled training data (ES)100002004006008001000Labeled training data (FR)Figure 6: Target-language adaptation incrementally augmenting labeled training data (sentences) fine-tune cross-lingual transfer model. Performances evaluated usingUAS (top) LAS (bottom). Note points whose x coordinates 0 representcross-lingual transfer performance, labeled training data used.Analysis. primary hypothesis incorporating data target language, modelable learn special syntactic patterns consistent source language.verify this, study influence target-language adaptation two special relations:dobj (DE) amod (ES, FR), measuring precision recall changes use100 target language sentences. Results shown respectively Table 11 Table 12.observe great improvements recall relations, indicates model indeed gainsability learning target-language-specific dependency structures supervision100 sentences.7. Related Studiescross-lingual annotation projection method pioneered Yarowsky, Ngai, Wicentowski (2001) shallow NLP tasks (POS tagging, NER, etc.), later applied dependencyparsing (Hwa et al., 2005; Smith & Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann,2014). work along line dedicated improving robustness syntactic pro1014fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSINGRelation: dobj; Language: DEPrecision RecallPROJ+Cluster41.4531.09+10041.9051.400.45 20.31CCA+Cluster39.4731.74+10043.5957.574.12 25.83Table 11: Effect minimal supervision (100 sentences) dobj.Relation: amod; Language: ES, FRESFRPrecision Recall Precision RecallPROJ+Cluster94.9780.0592.9481.70+10091.6092.5293.6195.753.37 12.470.67 14.05CCA+Cluster93.3777.3192.0872.22+10091.8592.7792.7796.411.52 15.460.69 24.19Table 12: Effect minimal supervision (100 sentences) amod.jection alleviating noise errors introduced word alignment-based projection. Typicalapproaches include soft projection (Li, Zhang, & Chen, 2014), treebank translation (Tiedemann, Agic, & Nivre, 2014), distribution transfer (Ma & Xia, 2014), recently proposeddensity-driven projection (Rasooli & Collins, 2015). worth mentioning remarkable resultsachieved annotation projection methods (Tiedemann, 2015; Rasooli & Collins,2015), due large part parsers trained target language side.cross-lingual model transfer, learning cross-lingual feature representations promising direction. Typical approaches include cross-lingual word clustering (Tackstrom et al., 2012)employed paper baseline system, projection features (Durrett, Pauls, & Klein, 2012). Kozhevnikov Titov (2014) derived linear projection maps target instancessource-side feature representations, extent similar CCA approach. XiaoGuo (2014) learned cross-lingual word embeddings applied MSTParser linguistictransfer, inspired work. Sgaard et al. (2015) obtained multi-source unified word embeddings via inverted indexing Wikipedia, applied various NLP tasks. However,results didnt show significant improvements parsing. Nevertheless, idea utilizing multisource information learning cross-lingual word embeddings makes great sense. recently,Duong et al. (2015a, 2015b) also utilized neural network architecture parameter sharingparsers different languages. However, approach requires annotated treebankstarget language side, makes distinct transfer parsing framework. additionrepresentation learning, attempts also made integrate monolingual linguistic features parsing models, manually constructed universal dependency parsing rules (Naseem,1015fiG UO , C , YAROWSKY, WANG & L IUChen, Barzilay, & Johnson, 2010) manually specified typological features (Naseem, Barzilay,& Globerson, 2012; Zhang & Barzilay, 2015).Using neural networks dependency parsing new approach. best knowledge, Mayberry Miikkulainen (1999) presented first work explored neural networksshift-reduce constituent-based parsing. used one-hot feature representations. Henderson(2004) used simple synchrony network predict parse decisions constituency parser,first use neural networks broad-coverage Penn Treebank parser. Titov Henderson (2007) applied Incremental Sigmoid Belief Networks constituent-based parsing. GargHenderson (2011) later extended work transition-based dependency parsing using Temporal Restricted Boltzman Machine. parsers, however, much less scalable practice.Earlier progress made using deep learning parsing includes work Collobert (2011)Socher et al. (2013) constituent-based parsing, Stenetorp (2013) built recursive neuralnetworks transition-based dependency parsing.8. Conclusionpaper proposes novel framework based distributed representations cross-lingual dependency parsing. Two algorithms proposed induction cross-lingual word representations,namely robust projection CCA, bridge lexical feature gap.Experiments show using cross-lingual word embeddings derived either approach,transferred parsing performance improved significantly delexicalized system.notable observation projection method performs significantly better CCA. Additionally, framework flexibly able incorporate cross-lingual word cluster features,significant gains use. combined system significantly outperforms delexicalized systems languages, average 10.9% error reduction LAS,significantly outperforms models McDonald et al. (2013) augmented projected wordcluster features.Furthermore, show performance cross-lingual transfer system specific target language boosted minimal supervision language, greatsignificance practical usage.Acknowledgmentsgrateful Manaal Faruqui providing bilingual resources. thank Ryan McDonaldpointing evaluation issue experiment. also thank Sharon Buschingproofreading anonymous reviewers insightful comments suggestions. worksupported National Key Basic Research Program China via grant 2014CB340503National Natural Science Foundation China (NSFC) via grant 61133012 61370164.Corresponding author: Wanxiang Che, E-mail: car@ir.hit.edu.cn.ReferencesBansal, M., Gimpel, K., & Livescu, K. (2014). Tailoring continuous word representations dependency parsing. Proceedings 52nd Annual Meeting Association Computa1016fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSINGtional Linguistics (Volume 2: Short Papers), pp. 809815, Baltimore, Maryland. AssociationComputational Linguistics.Brown, P. F., Desouza, P. V., Mercer, R. L., Pietra, V. J. D., & Lai, J. C. (1992). Class-based n-grammodels natural language. Computational linguistics, 18(4), 467479.Buchholz, S., & Marsi, E. (2006). Conll-x shared task multilingual dependency parsing.Proceedings Tenth Conference Computational Natural Language Learning (CoNLLX), pp. 149164, New York City. Association Computational Linguistics.Cao, Y., & Khudanpur, S. (2014). Online learning tensor space. Proceedings 52ndAnnual Meeting Association Computational Linguistics (Volume 1: Long Papers),pp. 666675, Baltimore, Maryland. Association Computational Linguistics.Carreras, X. (2007). Experiments higher-order projective dependency parser. ProceedingsCoNLL Shared Task Session EMNLP-CoNLL 2007, pp. 957961, Prague, CzechRepublic. Association Computational Linguistics.Chandar P, S., Lauly, S., Larochelle, H., Khapra, M., Ravindran, B., Raykar, V. C., & Saha, A.(2014). autoencoder approach learning bilingual word representations. AdvancesNeural Information Processing Systems 27, pp. 18531861. Curran Associates, Inc.Chen, D., & Manning, C. (2014). fast accurate dependency parser using neural networks.Proceedings 2014 Conference Empirical Methods Natural Language Processing(EMNLP), pp. 740750, Doha, Qatar. Association Computational Linguistics.Choi, J. D., & McCallum, A. (2013). Transition-based dependency parsing selectional branching. Proceedings 51st Annual Meeting Association Computational Linguistics (Volume 1: Long Papers), pp. 10521062, Sofia, Bulgaria. Association ComputationalLinguistics.Collobert, R. (2011). Deep learning efficient discriminative parsing. Proceedings 14thInternational Conference Artificial Intelligence Statistics (AISTATS), pp. 224232,Fort Lauderdale, FL, USA. JMLR.org.Collobert, R., & Weston, J. (2008). unified architecture natural language processing: Deepneural networks multitask learning. Proceedings 25th International ConferenceMachine Learning, ICML 08, pp. 160167, Helsinki, Finland. ACM.Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011). Naturallanguage processing (almost) scratch. Journal Machine Learning Research, 12, 24932537.Covington, M. A. (2001). fundamental algorithm dependency parsing. Proceedings39th annual ACM southeast conference, pp. 95102.De Marneffe, M.-C., MacCartney, B., Manning, C. D., et al. (2006). Generating typed dependencyparses phrase structure parses. Proceedings Fifth International ConferenceLanguage Resources Evaluation (LREC06), pp. 449454, Genoa, Italy. EuropeanLanguage Resources Association (ELRA).De Marneffe, M.-C., & Manning, C. D. (2008). stanford typed dependencies representation.COLING 2008: Proceedings workshop Cross-Framework Cross-Domain ParserEvaluation, pp. 18, Manchester, UK. Association Computational Linguistics.1017fiG UO , C , YAROWSKY, WANG & L IUDuchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods online learningstochastic optimization. Journal Machine Learning Research, 12, 21212159.Duong, L., Cohn, T., Bird, S., & Cook, P. (2015a). Low resource dependency parsing: Cross-lingualparameter sharing neural network parser. Proceedings 53rd Annual MeetingAssociation Computational Linguistics 7th International Joint ConferenceNatural Language Processing (Volume 2: Short Papers), pp. 845850, Beijing, China.Association Computational Linguistics.Duong, L., Cohn, T., Bird, S., & Cook, P. (2015b). neural network model low-resource universal dependency parsing. Proceedings 2015 Conference Empirical MethodsNatural Language Processing, pp. 339348, Lisbon, Portugal. Association ComputationalLinguistics.Durrett, G., Pauls, A., & Klein, D. (2012). Syntactic transfer using bilingual lexicon. Proceedings 2012 Joint Conference Empirical Methods Natural Language ProcessingComputational Natural Language Learning, pp. 111, Jeju Island, Korea. AssociationComputational Linguistics.Dyer, C., Ballesteros, M., Ling, W., Matthews, A., & Smith, N. A. (2015). Transition-based dependency parsing stack long short-term memory. Proceedings 53rd Annual MeetingAssociation Computational Linguistics 7th International Joint ConferenceNatural Language Processing (Volume 1: Long Papers), pp. 334343, Beijing, China. Association Computational Linguistics.Dyer, C., Lopez, A., Ganitkevitch, J., Weese, J., Ture, F., Blunsom, P., Setiawan, H., Eidelman, V.,& Resnik, P. (2010). cdec: decoder, alignment, learning framework finite-statecontext-free translation models. Proceedings ACL 2010 System Demonstrations, pp.712, Uppsala, Sweden. Association Computational Linguistics.Eisner, J. M. (1996). Three new probabilistic models dependency parsing: exploration.Proceedings 16th conference Computational linguistics-Volume 1, pp. 340345,Copenhagen, Denmark. Association Computational Linguistics.Faruqui, M., & Dyer, C. (2014). Improving vector space word representations using multilingualcorrelation. Proceedings 14th Conference European Chapter Association Computational Linguistics, pp. 462471, Gothenburg, Sweden. AssociationComputational Linguistics.Firth, J. R. (1957). synopsis linguistic theory 19301955. Studies linguistic analysis, pp.132. Blackwell.Garg, N., & Henderson, J. (2011). Temporal restricted boltzmann machines dependency parsing.Proceedings 49th Annual Meeting Association Computational Linguistics:Human Language Technologies, pp. 1117, Portland, Oregon, USA. Association Computational Linguistics.Gouws, S., Bengio, Y., & Corrado, G. (2015). Bilbowa: Fast bilingual distributed representationswithout word alignments. Proceedings 32nd International Conference MachineLearning (ICML), pp. 748756, Lille, France.Guo, J., Che, W., Wang, H., & Liu, T. (2014). Revisiting embedding features simple semisupervised learning. Proceedings 2014 Conference Empirical Methods Natural1018fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSINGLanguage Processing (EMNLP), pp. 110120, Doha, Qatar. Association ComputationalLinguistics.Guo, J., Che, W., Yarowsky, D., Wang, H., & Liu, T. (2015). Cross-lingual dependency parsingbased distributed representations. Proceedings 53rd Annual Meeting Association Computational Linguistics 7th International Joint Conference NaturalLanguage Processing (Volume 1: Long Papers), pp. 12341244, Beijing, China. AssociationComputational Linguistics.Haghighi, A., Liang, P., Berg-Kirkpatrick, T., & Klein, D. (2008). Learning bilingual lexiconsmonolingual corpora. Proceedings ACL-08: HLT, pp. 771779, Columbus, Ohio.Association Computational Linguistics.Hardoon, D. R., Szedmak, S., & Shawe-Taylor, J. (2004). Canonical correlation analysis:overview application learning methods. Neural computation, 16(12), 26392664.Henderson, J. (2004). Discriminative training neural network statistical parser. Proceedings 42nd Meeting Association Computational Linguistics (ACL04), MainVolume, pp. 95102, Barcelona, Spain.Hermann, K. M., & Blunsom, P. (2014). Multilingual models compositional distributed semantics. Proceedings 52nd Annual Meeting Association ComputationalLinguistics (Volume 1: Long Papers), pp. 5868, Baltimore, Maryland. Association Computational Linguistics.Hwa, R., Resnik, P., Weinberg, A., Cabezas, C., & Kolak, O. (2005). Bootstrapping parsers viasyntactic projection across parallel texts. Natural language engineering, 11(03), 311325.Jiang, W., Liu, Q., & Lv, Y. (2011). Relaxed cross-lingual projection constituent syntax.Proceedings 2011 Conference Empirical Methods Natural Language Processing,pp. 11921201, Edinburgh, Scotland, UK. Association Computational Linguistics.Kim, Y. (2014). Convolutional neural networks sentence classification. Proceedings2014 Conference Empirical Methods Natural Language Processing (EMNLP), pp. 17461751, Doha, Qatar. Association Computational Linguistics.Klein, D., & Manning, C. (2004). Corpus-based induction syntactic structure: Models dependency constituency. Proceedings 42nd Meeting Association Computational Linguistics (ACL04), Main Volume, pp. 478485, Barcelona, Spain.Klementiev, A., Titov, I., & Bhattarai, B. (2012). Inducing crosslingual distributed representationswords. Proceedings COLING 2012, pp. 14591474, Mumbai, India. COLING2012 Organizing Committee.Koehn, P., & Knight, K. (2002). Learning translation lexicon monolingual corpora. Proceedings ACL-02 Workshop Unsupervised Lexical Acquisition, pp. 916, Philadelphia, Pennsylvania, USA. Association Computational Linguistics.Kolda, T. G., & Bader, B. W. (2009). Tensor decompositions applications. SIAM review, 51(3),455500.Koo, T., & Collins, M. (2010). Efficient third-order dependency parsers. Proceedings48th Annual Meeting Association Computational Linguistics, pp. 111, Uppsala,Sweden. Association Computational Linguistics.1019fiG UO , C , YAROWSKY, WANG & L IUKozhevnikov, M., & Titov, I. (2014). Cross-lingual model transfer using feature representationprojection. Proceedings 52nd Annual Meeting Association ComputationalLinguistics (Volume 2: Short Papers), pp. 579585, Baltimore, Maryland. AssociationComputational Linguistics.Lei, T., Xin, Y., Zhang, Y., Barzilay, R., & Jaakkola, T. (2014). Low-rank tensors scoringdependency structures. Proceedings 52nd Annual Meeting AssociationComputational Linguistics (Volume 1: Long Papers), pp. 13811391, Baltimore, Maryland.Association Computational Linguistics.Li, Z., Zhang, M., & Chen, W. (2014). Soft cross-lingual syntax projection dependency parsing. Proceedings COLING 2014, 25th International Conference ComputationalLinguistics: Technical Papers, pp. 783793, Dublin, Ireland. Dublin City University Association Computational Linguistics.Ma, X., & Xia, F. (2014). Unsupervised dependency parsing transferring distribution viaparallel guidance entropy regularization. Proceedings 52nd Annual MeetingAssociation Computational Linguistics (Volume 1: Long Papers), pp. 13371348,Baltimore, Maryland. Association Computational Linguistics.Mann, G. S., & Yarowsky, D. (2001). Multipath translation lexicon induction via bridge languages.Proceedings Second Meeting North American Chapter AssociationComputational Linguistics Language Technologies, NAACL 01, pp. 18, Pittsburgh,Pennsylvania. Association Computational Linguistics.Marcus, M. P., Marcinkiewicz, M. A., & Santorini, B. (1993). Building large annotated corpusenglish: penn treebank. Computational linguistics, 19(2), 313330.Mayberry, M. R., & Miikkulainen, R. (1999). Sardsrn: neural network shift-reduce parser.Proceedings Sixteenth International Joint Conference Artificial Intelligence, pp.820827. Morgan Kaufmann Publishers Inc.McDonald, R., Crammer, K., & Pereira, F. (2005). Online large-margin training dependencyparsers. Proceedings 43rd Annual Meeting Association ComputationalLinguistics (ACL05), pp. 9198, Ann Arbor, Michigan. Association Computational Linguistics.McDonald, R., & Nivre, J. (2007). Characterizing errors data-driven dependency parsingmodels. Proceedings 2007 Joint Conference Empirical Methods NaturalLanguage Processing Computational Natural Language Learning (EMNLP-CoNLL), pp.122131, Prague, Czech Republic. Association Computational Linguistics.McDonald, R., Nivre, J., Quirmbach-Brundage, Y., Goldberg, Y., Das, D., Ganchev, K., Hall, K.,Petrov, S., Zhang, H., Tackstrom, O., Bedini, C., Bertomeu Castello, N., & Lee, J. (2013).Universal dependency annotation multilingual parsing. Proceedings 51st AnnualMeeting Association Computational Linguistics (Volume 2: Short Papers), pp. 9297, Sofia, Bulgaria. Association Computational Linguistics.McDonald, R., Petrov, S., & Hall, K. (2011). Multi-source transfer delexicalized dependencyparsers. Proceedings 2011 Conference Empirical Methods Natural LanguageProcessing, pp. 6272, Edinburgh, Scotland, UK. Association Computational Linguistics.1020fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSINGMcDonald, R. T., & Pereira, F. C. (2006). Online learning approximate dependency parsingalgorithms. Proceedings 11st Conference European Chapter Association Computational Linguistics, pp. 8188, Trento, Italy. Association ComputerLinguistics.Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation word representationsvector space. International Conference Learning Representations (ICLR) Workshop.Naseem, T., Barzilay, R., & Globerson, A. (2012). Selective sharing multilingual dependencyparsing. Proceedings 50th Annual Meeting Association ComputationalLinguistics (Volume 1: Long Papers), pp. 629637, Jeju Island, Korea. Association Computational Linguistics.Naseem, T., Chen, H., Barzilay, R., & Johnson, M. (2010). Using universal linguistic knowledgeguide grammar induction. Proceedings 2010 Conference Empirical MethodsNatural Language Processing, pp. 12341244, Cambridge, MA. Association Computational Linguistics.Nilsson, J., & Nivre, J. (2008). Malteval: evaluation visualization tool dependencyparsing.. Proceedings Sixth International Language Resources Evaluation(LREC08), pp. 161166, Marrakech, Morocco. European Language Resources Association(ELRA).Nivre, J. (2003). efficient algorithm projective dependency parsing. Proceedings8th International Workshop Parsing Technologies (IWPT), pp. 149160, Nancy, France.Association Computational Linguistics.Nivre, J. (2004). Incrementality deterministic dependency parsing. Proceedings Workshop Incremental Parsing: Bringing Engineering Cognition Together, pp. 5057,Barcelona, Spain. Association Computational Linguistics.Nivre, J. (2008). Algorithms deterministic incremental dependency parsing. ComputationalLinguistics, 34(4), 513553.Nivre, J. (2009). Non-projective dependency parsing expected linear time. ProceedingsJoint Conference 47th Annual Meeting ACL 4th International JointConference Natural Language Processing AFNLP, pp. 351359, Suntec, Singapore.Association Computational Linguistics.Nivre, J., Hall, J., & Nilsson, J. (2004). Memory-based dependency parsing. HLT-NAACL 2004Workshop: Eighth Conference Computational Natural Language Learning (CoNLL-2004),pp. 4956, Boston, Massachusetts, USA. Association Computational Linguistics.Petrov, S., Das, D., & McDonald, R. (2012). universal part-of-speech tagset. ProceedingsEighth International Conference Language Resources Evaluation (LREC-2012),pp. 20892096, Istanbul, Turkey. European Language Resources Association (ELRA).Rasooli, M. S., & Collins, M. (2015). Density-driven cross-lingual transfer dependency parsers.Proceedings 2015 Conference Empirical Methods Natural Language Processing, pp. 328338, Lisbon, Portugal. Association Computational Linguistics.Smith, D. A., & Eisner, J. (2009). Parser adaptation projection quasi-synchronous grammarfeatures. Proceedings 2009 Conference Empirical Methods Natural LanguageProcessing, pp. 822831, Singapore. Association Computational Linguistics.1021fiG UO , C , YAROWSKY, WANG & L IUSocher, R., Bauer, J., Manning, C. D., & Andrew Y., N. (2013). Parsing compositional vectorgrammars. Proceedings 51st Annual Meeting Association ComputationalLinguistics (Volume 1: Long Papers), pp. 455465, Sofia, Bulgaria. Association Computational Linguistics.Sgaard, A., Agic, v., Martnez Alonso, H., Plank, B., Bohnet, B., & Johannsen, A. (2015). Invertedindexing cross-lingual nlp. Proceedings 53rd Annual Meeting AssociationComputational Linguistics 7th International Joint Conference Natural Language Processing (Volume 1: Long Papers), pp. 17131722, Beijing, China. AssociationComputational Linguistics.Srivastava, N., & Salakhutdinov, R. R. (2012). Multimodal learning deep boltzmann machines. Advances Neural Information Processing Systems 25, pp. 22222230. CurranAssociates, Inc.Stenetorp, P. (2013). Transition-based dependency parsing using recursive neural networks. DeepLearning Workshop NIPS, Lake Tahoe, Nevada, USA.Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence sequence learning neural networks. Advances Neural Information Processing Systems 27, pp. 31043112. CurranAssociates, Inc.Tackstrom, O., McDonald, R., & Uszkoreit, J. (2012). Cross-lingual word clusters direct transferlinguistic structure. Proceedings 2012 Conference North American ChapterAssociation Computational Linguistics: Human Language Technologies, pp. 477487, Montreal, Canada. Association Computational Linguistics.Tiedemann, J. (2014). Rediscovering annotation projection cross-lingual parser induction.Proceedings COLING 2014, 25th International Conference Computational Linguistics: Technical Papers, pp. 18541864, Dublin, Ireland. Dublin City University Association Computational Linguistics.Tiedemann, J. (2015). Cross-lingual dependency parsing universal dependencies predictedPoS labels., 340349.Tiedemann, J., Agic, v., & Nivre, J. (2014). Treebank translation cross-lingual parser induction.,130140.Titov, I., & Henderson, J. (2007). Fast robust multilingual dependency parsing generativelatent variable model. Proceedings CoNLL Shared Task Session EMNLP-CoNLL2007, pp. 947951, Prague, Czech Republic. Association Computational Linguistics.Turian, J., Ratinov, L.-A., & Bengio, Y. (2010). Word representations: simple general methodsemi-supervised learning. Proceedings 48th Annual Meeting AssociationComputational Linguistics, pp. 384394, Uppsala, Sweden. Association Computational Linguistics.Uszkoreit, J., & Brants, T. (2008). Distributed word clustering large scale class-based languagemodeling machine translation. Proceedings ACL-08: HLT, pp. 755762, Columbus,Ohio. Association Computational Linguistics.Wang, M., & Manning, C. D. (2013). Effect non-linear deep architecture sequence labeling.Proceedings Sixth International Joint Conference Natural Language Processing,pp. 12851291, Nagoya, Japan. Asian Federation Natural Language Processing.1022fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSINGWeiss, D., Alberti, C., Collins, M., & Petrov, S. (2015). Structured training neural networktransition-based parsing. Proceedings 53rd Annual Meeting AssociationComputational Linguistics 7th International Joint Conference Natural LanguageProcessing (Volume 1: Long Papers), pp. 323333, Beijing, China. Association Computational Linguistics.Xiao, M., & Guo, Y. (2014). Distributed word representation learning cross-lingual dependencyparsing. Proceedings Eighteenth Conference Computational Natural LanguageLearning, pp. 119129, Ann Arbor, Michigan. Association Computational Linguistics.Xue, N., Xia, F., Chiou, F.-D., & Palmer, M. (2005). penn chinese treebank: Phrase structureannotation large corpus. Natural language engineering, 11(02), 207238.Yamada, H., & Matsumoto, Y. (2003). Statistical dependency analysis support vector machines.Proceedings 8th International Workshop Parsing Technologies (IWPT), pp. 195206, Nancy, France. Association Computational Linguistics.Yarowsky, D., Ngai, G., & Wicentowski, R. (2001). Inducing multilingual text analysis tools viarobust projection across aligned corpora. Proceedings first international conferenceHuman language technology research, pp. 18, San Diego, CA, USA. AssociationComputational Linguistics.Zeman, D., Dusek, O., Marecek, D., Popel, M., Ramasamy, L., Stepanek, J., Zabokrtsky, Z., &Hajic, J. (2014). Hamledt: Harmonized multi-language dependency treebank. LanguageResources Evaluation, 48(4), 601637.Zhang, Y., & Barzilay, R. (2015). Hierarchical low-rank tensors multilingual transfer parsing.Proceedings 2015 Conference Empirical Methods Natural Language Processing,pp. 18571867, Lisbon, Portugal. Association Computational Linguistics.Zhang, Y., & Clark, S. (2011). Syntactic processing using generalized perceptron beamsearch. Computational Linguistics, 37(1), 105151.Zhang, Y., & Nivre, J. (2011). Transition-based dependency parsing rich non-local features.Proceedings 49th Annual Meeting Association Computational Linguistics: Human Language Technologies, pp. 188193, Portland, Oregon, USA. AssociationComputational Linguistics.Zhao, H., Song, Y., Kit, C., & Zhou, G. (2009). Cross language dependency parsing using bilinguallexicon. Proceedings Joint Conference 47th Annual Meeting ACL4th International Joint Conference Natural Language Processing AFNLP, pp.5563, Suntec, Singapore. Association Computational Linguistics.Zhou, H., Zhang, Y., Huang, S., & Chen, J. (2015). neural probabilistic structured-predictionmodel transition-based dependency parsing. Proceedings 53rd Annual MeetingAssociation Computational Linguistics 7th International Joint ConferenceNatural Language Processing (Volume 1: Long Papers), pp. 12131222, Beijing, China.Association Computational Linguistics.1023fiJournal Articial Intelligence Research 55 (2016) 1091-1133Submitted 09/2015; published 04/2016Semantic VisualizationNeighborhood Graph RegularizationTuan M. V. LeHady W. Lauwvmtle.2012@phdis.smu.edu.sghadywlauw@smu.edu.sgSchool Information SystemsSingapore Management University80 Stamford Road, Singapore 178902AbstractVisualization high-dimensional data, text documents, useful mapsimilarities among various data points. high-dimensional space, documentscommonly represented bags words, dimensionality equal vocabularysize. Classical approaches document visualization directly reduce visualizabletwo three dimensions. Recent approaches consider intermediate representationtopic space, word space visualization space, preserves semanticstopic modeling. aiming good model parametersobserved data, previous approaches considered local consistency among datainstances. consider problem semantic visualization jointly modeling topicsvisualization intrinsic document manifold, modeled using neighborhood graph.document topic distribution visualization coordinate. Specically,propose unsupervised probabilistic model, called Semafore, aims preservemanifold lower-dimensional spaces neighborhood regularization frameworkdesigned semantic visualization task. validate ecacy Semafore,comprehensive experiments number real-life text datasets news articlesWeb pages show proposed methods outperform state-of-the-art baselinesobjective evaluation metrics.1. IntroductionText documents come various avors, Web pages, news articles, blog posts, emails,messages social media Twitter. much English, increasingamounts content various languages well. backdrop growth volume, diversity, complexity various corpora, need useful tools analyzewealth text content. One form analysis look paper visualization. dierent types visualizations, temporal longitudinal,networked, natures. interested form visualizationrepresent collection documents coordinates low-dimensional space,learn similarities dierences among documents based distancesvisualization space.Visualization high-dimensional data important exploratory data analysis task,actively studied various academic communities. HCI communityinterested presentation information, well interface aspects (Chi, 2000),machine learning community interested quality dimensionality reductionc2016AI Access Foundation. rights reserved.fiLe & Lauw(Van der Maaten & Hinton, 2008), i.e., transform high-dimensional representation lower-dimensional representation shown scatterplot.visualization form simple, widely applicable across various domains.Consider therefore problem visualizing documents scatterplot. Commonly,document represented bag words, i.e., vector word counts. highdimensional representation would reduced coordinates visualizable 2D (or 3D)space. One pioneering technique Multidimensional Scaling (MDS) (Kruskal, 1964).goal preserve distances high-dimensional space low-dimensional embedding. applied documents, visualization technique generic high-dimensionaldata, e.g., MDS, may necessarily preserve topical semantics. Words often ambiguous, issues polysemy, word carries multiple senses,synonymy, dierent words carry sense. dimensions original representation (which words) may accurately capture ambiguity, aectsquality reduced representation (which visualization space) well.model semantics documents way resolve ambiguity,current popular approach topic modeling, PLSA (Hofmann, 1999) LDA(Blei, Ng, & Jordan, 2003). document associated probability distributionset topics. topic probability distribution words vocabulary.way, polysemous words separated dierent topics, synonymous wordsgrouped topic.Topic modeling another form dimensionality reduction: word spacetopic space. word space refers documents original representation, usuallybag words. topic space refers simplex topic distributions. documentsprobability distribution topics eectively representation documenttopic space. However, topic model designed visualization.one possible visualization plot documents topic distributions simplex, 2Dvisualization space could express three topics, limiting.Given success modeling semantics documents, therefore ask questionwhether best forms dimensionality reductions (visualizationtopic modeling) documents. end goal arrive visualization documentsconsistent semantic representation (topics), well originalrepresentation (words). coupling distinct task topic modeling visualizationrespectively, enables novel capabilities. one thing, topic modeling helps createricher visualization, associate coordinate visualization spacetopic word distributions, providing semantics visualization space.another, tight integration potentially allows visualization serve wayexplore tune topic models, allowing users introduce feedback (Hu, Boyd-Graber,Satino, & Smith, 2014) model visual interface (Choo, Lee, Reddy, & Park,2013). capabilities support several use case scenarios. One potential use casedocument organizer system. visualization could potentially help assigning categoriesdocuments, showing closely related documents labeled. Anotheraugmented retrieval system. Given query, results may include relevantdocuments, also similar documents (neighbors visualization).1092fiSemantic Visualization Neighborhood Graph Regularization1.1 Problem Statementrefer task jointly modeling topics visualization semantic visualization.input set documents D. specied number topics Z visualizationdimensionality (assumed 2D, without losing generality), goal derive,every document D, latent coordinate visualization space, probabilitydistribution Z topics. focus documents description,approach would apply visualization data types latent factor modeling,i.e., topic model, makes sense.straightforward way undergo two-step reductions. rst reduction,original representation documents reduced topic distributions using topic modeling. second reduction, documents topic distributions reducedvisualization coordinates. approach may value compared direct reduction word space visualization space. However, ideal, disjointreductions could mean errors may propagate rst second reduction,resulting visualization may faithfully capture original representation.better way solve problem join two reductions single, jointprocess produces topic distributions visualization coordinates. approachrst pioneered PLSV (Iwata, Yamada, & Ueda, 2008), also showedjoint approach outperformed disjoint approach. PLSV derives latent parametersmaximizing likelihood observing documents. goal concernederror model observation.literature, found algorithms ensure smoothness tend performbetter learning tasks (Zhou, Bousquet, Lal, Weston, & Scholkopf, 2004). Smoothnessconcerns preserving observed proximity documents. objective arises naturally assumption intrinsic geometry data low-rank, non-linearsubspace within high-dimensional space. Therefore, preserving neighborhood structureimportant learning tasks. assumption well-accepted machine learningcommunity (Laerty & Wasserman, 2007), nds application supervisedunsupervised learning (Belkin & Niyogi, 2003; Zhou et al., 2004; Zhu, Ghahramani, Lafferty, et al., 2003). Recently, preponderance evidence assumption alsoapplies text data particular (Cai, Mei, Han, & Zhai, 2008; Cai, Wang, & He, 2009;Huh & Fienberg, 2012). therefore propose incorporate assumption newunsupervised, semantic visualization model.1.2 Overviewpropose unsupervised probabilistic model jointly derives topic distributionsvisualization coordinates intrinsic geometry data. proposed model calledSemafore, stands SEmantic visualization MAniFOld REgularization.build neighborhood regularization framework semantic visualization model.framework involves new issues resolve, including regularization function,space regularization take place.model evaluated series real-life, publicly available datasets,also benchmark datasets used document classication task. advantage statisticalmethod, ours, dependent specic language. Two datasets1093fiLe & LauwEnglish, one Brazilian Portuguese. model unsupervised (classlabel neither required used learning), objectively quantify visualization quality, leverage class label information. common assumption documentsclass expected neighbors original space (Belkin, Niyogi, &Sindhwani, 2006; Zhou et al., 2004; Zhu et al., 2003), suggests alsoclose visualization space. investigate eectiveness Semafore placingdocuments class nearby visualization space, systematically compareexisting baselines without one properties, namely: joint modelingtopic visualization, neighborhood regularization.1.3 Contributionsvisualization topic modeling are, separately, well-studied problems, interfacetwo, semantic visualization, relatively new problem, previouswork. work, make following contributions.propose incorporating neighborhood structure semantic visualization.respect, propose probabilistic model Semafore, two integrated components. One kernelized semantic visualization model, enabling substitutionkernel functions relate visualization coordinates topic distributions (seeSection 3.3). neighborhood graph regularization framework semanticvisualization described Section 4.1.Realizing neighborhood graph regularization involves explorationincorporate appropriate forms neighborhood structure. respect,investigate eects neighborhood graph construction techniques knearest neighbors (k-NN), -ball, disjoint minimum spanning trees (DMST),well dierent edge weight estimations heat-kernel (see Section 4.2)context semantic visualization.Section 5, describe requisite learning algorithms based maximumposteriori (MAP) estimation using expectation-maximization (EM), orderparameters various regularization functions kernels propose.nal contribution evaluation Semafores eectiveness series reallife, public datasets described Section 6, shows Semafore outperformsexisting baselines well-established objective visualization metric.prior work (Le & Lauw, 2014b), proposed problem described preliminary model. extended article, signicant technical changes providesignicantly comprehensive discussion model. instance, discussStudent-t kernel, addition previously introduced Gaussian kernel. Furthermore, investigate ecacies dierent neighborhood graph constructions, including-ball DMST graphs, addition previously introduced kNN graph.graph weights also enhanced investigation heat kernel, additionsimple-minded binary scheme previously. discussed Section 6.3, enhancementscollectively result statistically signicant improvements previous model. Beyond1094fiSemantic Visualization Neighborhood Graph Regularizationtechnical enhancements, also provide comprehensive model analysis empirical validation, including richer quantitative qualitative discussions visualizationsresulting topic models, well metric measure topic interpretability basedpairwise mutual information.2. Related Worksection, discuss dierent aspects work, identify related papersliterature, point key conceptual dierences.2.1 Visualization Dimensionality ReductionOne way perform visualization using generic dimensionality reduction technique.techniques come several avors, depending objective. Principal componentanalysis (PCA) (Jollie, 2005) identies components explain variancedata. Related PCA singular value decomposition (SVD) (Golub & Van Loan,2012). Comparatively, independent component analysis (ICA) (Comon, 1994) identiescomponents independent one another, whereas linear discriminant analysis(Fishers LDA) (Fisher, 1936) identies components discriminateknown class labels. generic, techniques frequently applied featureextraction, optimized visualization. focus propertiescomponents (e.g., orthogonality, independence) rather intrinsic relationshipamong data instances. Furthermore, based linear projections, maycapture non-linearities data well.Another category techniques, directly related visualization,embedding approach. aims preserve high-dimensional similarities dierenceslow-dimensional embedding. One pioneering work multidimensional scaling(MDS) (Kruskal, 1964). Given set pairwise distances ij data points j,MDS determines coordinates xi xj respectively, embedded visualizationdistance ||xi xj || approximates ij much possible. MDS, distancepreserved ij frequently linear distance, measuring distance along straight linetwo points input space. Instead linear distance, Isomap (Tenenbaum,De Silva, & Langford, 2000) seeks preserve geodesic distance, nding shortest pathsgraph edges connecting neighboring data points. LLE (Roweis & Saul, 2000) seekspreserve linear distances, among neighboring points avoiding needestimate pairwise distances widely separated data points. Recently alsoworks applying similar concept embedding using probabilistic modeling,PE (Iwata, Saito, Ueda, Stromsten, Griths, & Tenenbaum, 2007), SNE (Hinton & Roweis,2002), t-SNE (Van der Maaten & Hinton, 2008), GTM (Bishop, Svensen, & Williams,1998). Yet others based semi-denite programming (Shaw & Jebara, 2007, 2009).Alternatively, several embedding techniques aim preserve relationship amongdata instances, rather properties local minima (Kim & Torre, 2010).Importantly, techniques optimized semantic visualization,model topics all. coordinates reect semantic meaning,reecting optimization objective.1095fiLe & Lauwrelated works far seek address semantic visualizationtask directly. closest previous work topic modeling visualizationsingle generative process Probabilistic Latent Semantic Visualization (PLSV) (Iwataet al., 2008), also shows joint approach outperforms separate approach.PLSV builds upon foundation topic modeling technique Probabilistic LatentSemantic Analysis (PLSA) (Hofmann, 1999) incorporating visualization coordinates,build upon foundation PLSV incorporating RBF kernels (Section 3.3)neighborhood structure (Section 4).also related works share similar objective, shareparadigm visualization topic modeling. instance, LDA-SOM (Millar, Peterson, &Mendenhall, 2009) rst conducts topic modeling using Latent Dirichlet Allocation (LDA)(Blei et al., 2003), separately embeds documents topic distributionsSelf-Organizing Map (SOM) (Kohonen, 1990). However, joint model,SOM uses dierent visualization space Euclidean space interestedin. another instance, SSE (Le & Lauw, 2014a) builds Spherical AdmixtureModel (SAM) (Reisinger, Waters, Silverthorn, & Mooney, 2010) belonging classspherical topic models targeted spherical (unit vector) reprepresentations topicsdocuments, directly comparable equivalent simplex representationmultinomial modeling (probability distribution words) adopted work wellPLSV.semantic visualization, refer task joining visualization topic modeling. related, dierent, task topic visualization, objective visualizetopics, terms keywords dominant topic (Chaney & Blei, 2012;Chuang, Manning, & Heer, 2012), topics dominant corpus (Wei, Liu, Song,Pan, Zhou, Qian, Shi, Tan, & Zhang, 2010), topics related one another(Gretarsson, Odonovan, Bostandjiev, Hollerer, Asuncion, Newman, & Smyth, 2012).2.2 Topic ModelingTopic model involves statistical modeling text (documents words) order discoverabstract concepts topics occur corpus. Beginning latent semanticindexing (Dumais, Furnas, Landauer, Deerwester, Deerwester, et al., 1995), topic modelevolves modern probabilistic treatments, Probabilistic Latent SemanticAnalysis (PLSA) (Hofmann, 1999) Latent Dirichlet Allocation (LDA) (Blei et al.,2003). Intuitively, topic captures collection words tend co-occurdescribe concept. appeal producing highly interpretablestatistical models let users make semantic sense corpus. text-onlydocument corpora, topic models also applied cases links observedaddition text (McCallum, Wang, & Corrada-Emmanuel, 2007).Meanwhile, assumption intrinsic geometry data non-linear lowdimensional subspace within high-dimensional space nds application supervisedunsupervised (Belkin & Niyogi, 2003) learning algorithms. especially prevalentsemi-supervised learning (Zhou et al., 2004; Zhu et al., 2003) way bridge labeledunlabeled data. Regularization technique realize assumption longhistory (Belkin et al., 2006). specic form regularization function varies among1096fiSemantic Visualization Neighborhood Graph Regularizationapplications. study assumption unsupervised topic models beginsLapPLSI (Cai et al., 2008), introduces regularization PLSA (Hofmann, 1999),minimizing Euclidean distance neighboring documents topic distributions.Follow-up work introduce distance functions (Cai et al., 2009; Wu, Bu, Chen, Zhu,Zhang, Liu, Wang, & Cai, 2012). previous work focus maintaining proximitysimilar documents, DTM (Huh & Fienberg, 2012) adds new criterion also maintaindistance among dierent documents. work dierent also needcontend visualization aspects, topic modeling.2.3 Semantic Similaritytopic models, alternative mechanisms learn semantic relationshipdocuments. One way measuring semantic similarity among documentswords. instance, vector space model, documents may represented termvector, similarity may expressed terms cosine similarity (Turney, Pantel,et al., 2010). word occurrences alone, could also additional signalssemantic similarity. instance, working Wikipedia corpus, categorieslinks also took account determine similarity among articles (Gabrilovich& Markovitch, 2009; Ponzetto & Strube, 2007). work diers severalimportant respects. First, objective similarity value per se, ratherdetermining lower-dimensional embedding coordinates, would allow visualizationone application. Second, method based probabilistic modeling latent variables,akin topic modeling, instead operating vector space model representationdocuments.3. Semantic Visualizationintroduce problem formulation semantic visualization Section 3.1. focuspaper eects neighborhood graph structure semantic visualizationtask. gure clearest way showcase eects design neighborhoodpreservation framework existing generative process, PLSV (Iwataet al., 2008), review Section 3.2. Section 3.3, describe innovationsemantic visualization model, abstraction mappingtopic space visualization space using radial basis function (RBF) kernels.allows exploration various kernels, identify two exploration.ease following discussion, include table notations Table 1.3.1 Problemtask semantic visualization, input corpus documents = {d1 , . . . , dN }.Every dn bag words, wnm denotes mth word dn . total numberwords dn Mn . objective learn, dn , latent distribution Ztopics {P(z|dn )}Zz=1 . topic z associated parameter z , probabilitydistribution {P(w|z )}wW words vocabulary W . words highestprobabilities given topic capture semantic topic.1097fiLe & LauwNotationdnxnMnzzzWNZDescriptionspecic documentlatent coordinate dn visualization spacenumber words document dnspecic topiccoordinate topic z visualization spaceword distribution topic zvocabulary (the set words lexicon)total number documents corpustotal number topics (user-dened)collection xn documentscollection z topicscollection z topicscollective set parameters {, , }Table 1: Notations.semantic visualization, additional objective semantic visualization,learn, document dn , latent coordinate xn low-dimensionalityvisualization space. Similarly, topic z associated latent coordinate zvisualization space. document dn topic distribution expressed termsEuclidean distance coordinate xn dierent topic coordinates= {z }Zz=1 . Intuitively, closer xn topics z , higher P(z|dn )probability topic z document dn .following sections, systematically describe various componentssolution. generative process links latent variables (coordinates) wordsdocuments described Section 3.2. specic relationship documentstopics coordinates constitutes specic mapping function, model RBFkernel Section 3.3. following Section 4, discuss incorporate neighborhoodstructure semantic visualization.3.2 Generative Processdescribe generative process documents based topics visualizationcoordinates. review PLSV whose graphical model shown Figure 1.eventual complete model generalization model, involving enhancementskernelization (Section 3.3) neighborhood structure preservation (Section 4).generative process follows:1. topic z = 1, . . . , Z:(a) Draw zs word distribution: z Dirichlet()(b) Draw zs coordinate: z Normal(0, 1 I)2. document dn , n = 1, . . . , N :(a) Draw dn coordinate: xn Normal(0, 1 I)1098fiSemantic Visualization Neighborhood Graph RegularizationNMn wxZzFigure 1: Graphical model PLSV.(b) word wnm dn :i. Draw topic: z Multi({P(z|xn , )}Zz=1 )ii. Draw word: wnm Multi(z )Here, Dirichlet prior, identity matrix, control varianceZZNormal distributions. parameters = {xn }Nn=1 , = {z }z=1 , = {z }z=1 , collectivelydenoted = , , , learned documents based maximum posterioriestimation. log likelihood function shown Equation 1.L(|D) =MnNn=1 m=1logZP(z|xn , )P(wnm |z )(1)z=1reiterate focus incorporating neighborhood graph structuresemantic visualization. building neighborhood graph regularization frameworkexisting generative process, i.e., PLSV, clearly observe improvementPLSV arises neighborhood graph regularization. sense, worktradition introducing neighborhood graph regularization probabilistic topicmodeling (Huh & Fienberg, 2012; Cai et al., 2008, 2009), contributions relateneighborhood graph regularization, rather generative process. said,one signicant dierence PLSV, exibility allowing various kernelfunctions, discuss next.3.3 RBF KernelsStep 2(b)i generative process, topic z word drawndistribution {P(z|xn , )}Zz=1 . distribution relates coordinates topicsvisualization space = {z }Zz=1 coordinate xn document dndocuments topic distribution {P(z|dn )}Zz=1 .relationship formulated mapping problem want ndfunction G maps point visualization space point topic space. However,form G cannot known exactly visualization space topic spacelatent spaces G may dierent across dierent domains. Therefore, computetopic distributions, need way approximate G.build function approximation unknown function G, use abstractionRadial Basis Function (RBF) neural networks (Bishop, 1995) feedforward multilayered RBF neural networks one hidden layer serve universal approximator1099fiLe & LauwK nzZZ/xnFigure 2: Topic distribution expressed function visualization coordinates usingRadial Basis Function (RBF) network.arbitrary continuous functions (Park & Sandberg, 1991). property providescondence model would ability approximate existing relationshipvisualization space topic space arbitrary precision. Unlike PLSV (Iwataet al., 2008) dened specic mapping function, approach generalizes semantic visualization model dening mapping problem terms kernelization,admits several mapping functions within family RBF kernels.context, Radial Basis Function (Buhmann, 2000) relate coordinate variablesbased distances denes kernel function (||xn z ||) terms far datapoint (e.g., xn ) center (e.g., z ). kernel function may take various forms,e.g., Gaussian, multi-quadric, inverse quadratic, polyharmonic spline. express P(z|dn )function xn , consider normalized architecture RBF network, threelayers shown Figure 2. input layer consists one input node (xn ). hiddenlayer consists Z number normalized RBF activation functions. centeredz computes Z (||xn z ||) . linear output layer consists Z output nodes.z =1(||xn z ||)output node yz (xn ) corresponds P(z|dn ), linear combination RBFfunctions, shown Equation 2. Here, wz,z weight inuence RBF functionz P(z|dn ), constraint Zz =1 wz,z = 1.ZP(z|dn ) = yz (xn ) =z =1 wz,z (||xn z ||)Zz =1 (||xn z ||)(2)Equation 2 general form, instantiate specic mapping function,need determine assignment wz,z form function . wz,z ,experiment special case wz,z = 1 z = z 0 otherwise.kernel function , one variation consider Gaussian, yields function Equation 3, refers collective set z s. Note set varianceGaussian 1. However, true value really important dierent variancevalue produces re-scaled visualization scaling factor equal variance.1100fiSemantic Visualization Neighborhood Graph Regularizationexp( 12 ||xn z ||2 )P(z|dn )Gaussian = P(z|xn , )Gaussian = Z12z =1 exp( 2 ||xn z || )(3)Another variation considered Student-t. distribution also usedt-SNE (Van der Maaten & Hinton, 2008) context non-semantic, direct embedding mitigate eects crowding. Due mismatched dimensionalities, pointscrunched together center visualization, prevents gaps formingclusters. Therefore, hypothesize using Student-t radial basis function, yields function Equation 4, help improve performancemodel crowding becomes issue. Note Student-t distribution one degreefreedom yields radial basis function form similar inverse quadratic.(1 + ||xn z ||2 )1P(z|dn )Studentt = P(z|xn , )Studentt = Z2 1z =1 (1 + ||xn z || )(4)Gaussian function (Equation 3) also used previously baseline PLSV(Iwata et al., 2008) compare to. inclusion helps establish paritycomparative purposes, investigate eectiveness alternative Student-tkernel (described above), well neighborhood regularization (describednext section).4. Neighborhood Graph Regularization Frameworkrecent works (Cai et al., 2008, 2009; Huh & Fienberg, 2012) trying preservelocal neighborhood structure learning low-dimensional topic representations documents. works assume documents sampled nonlinear low-dimensionalsubspace embedded high-dimensional space. Therefore, local neighborhoodstructure important revealing hidden topics documents preservedlearning topic representations documents (Bai, Guo, Lan, & Cheng, 2014).generative process semantic visualization described Section 3, document parameters sampled independently, may necessarily reect underlying local neighborhood structure. therefore seek realize assumption semantic visualization.particular, assume two documents di dj close original space,parameters j low-rank representation similar well. Coupledkernelized semantic visualization model described Section 3, neighborhoodpreservation approach described section constitutes proposed model, Semafore,stands SEmantic visualization MAniFOld REgularization.4.1 Neighborhood Regularizationneighborhood structure represented neighborhood graph. Given setdata points Euclidean space, neighborhood graph constructed inputdata points vertices. denition, edges symmetric, i.e., ij = ji , weighted.collection edge weights collectively denoted = {ij }.moment, assume neighborhood graph, addressissue neighborhood graph may incorporated semantic visualiza1101fiLe & Lauwtion framework. actuality, neighborhood graph construction importantcomponent, whose construction described detail Section 4.2.One eective means incorporate neighborhood structure learning modelregularization framework (Belkin et al., 2006). leads re-designlog-likelihood function Equation 1 new regularized function L (Equation 5),consists parameters (visualization coordinates topic distributions),documents neighborhood structure.L(|D, ) = L(|D) + R(|)(5)rst component L log-likelihood function Equation 1, reectslatent parameters observation D. second component Rregularization function, reects consistency latent parametersneighboring documents neighborhood structure . regularization parameter,commonly found neighborhood based algorithms (Belkin et al., 2006; Cai et al., 2008,2009), controls extent regularization (we experiment dierentexperiments).4.1.1 Proposed Regularization Functionturn denition R function. intuition data pointsclose high-dimensional space, also close low-rank representations, i.e., local consistency, also known smoothness. One function satisesR+ Equation 6. Here, F distance function operates low-rank space.Minimizing R+ leads minimizing distance F(i , j ) neighbors (ij = 1).R+ (|) =Nij F(i , j )(6)i,j=1;i=jlevel local consistency still insucient, regulatenon-neighbors (i.e., ij = 0) behave. instance, prevent non-neighborssimilar low-rank representations. Another valid objective visualization keepnon-neighbors apart, satised another objective function R Equation 7. Rminimized two non-neighbors di dj (i.e., ij = 0) distant low-rankrepresentations. addition 1 F prevent division-by-zero error.R (|) =Ni,j=1;i=j;ij =01 ijF(i , j ) + 1(7)hypothesize neither objective eective own. complete objectivewould capture spirits keeping neighbors close, keeping non-neighbors apart.Therefore, put Equation 6 Equation 7 together using summation maximizeobjective function shown Equation 8. Note coecient 12 Equation 8simplifying formula derivative R (|).1R (|) = (R+ (|) + R (|))21102(8)fiSemantic Visualization Neighborhood Graph Regularization2d11d2I1I20-2-10-11234d3-2Figure 3: Example topic distribution may dierent visualization coordinates. points red line topic distributions.Summation preserves absolute magnitude distance, helps improvevisualization task keeping non-neighbors separated visualizable Euclidean space.Taking product unsuitable, constrains ratio distances neighbors distances non-neighbors. may result crowding eect,many documents clustered together, relative ratio may maintained,absolute distances visualization space could small.proposed regularization function above, also possible considerregularization functions. instance, also experimented modifyingregularization function adapted Discriminative Topic Model (DTM) (Huh & Fienberg,2012), addressed topic modeling semantic visualization. Noteoriginal DTM formulation, distance function F(i , j ) operates topic space,adapt semantic visualization redening distance function F(i , j )operate visualization space instead. modied DTM formulation shownunderperform proposed regularization function (Le & Lauw, 2014b).4.1.2 Enforcing Neighborhood Structure: Visualization vs. Topic Spaceturn denition F(1 , 2 ). neighborhood-based models (Belkin et al.,2006; Cai et al., 2008, 2009), one low-rank representative space. semanticvisualization, two: topic visualization spaces. lookenforce neighborhood graph structure.rst glance, seem equivalent. all, representationsdocuments. However, necessarily case. Consider simple example twotopics z1 z2 visualization coordinates 1 = (0, 0) 2 = (2, 0) respectively.Meanwhile, three documents {d1 , d2 , d3 } coordinates x1 = (1, 1), x2 = (1, 1),x3 = (1, 1). two documents coordinates, alsotopic distributions. example, x1 x2 equidistant 1 2 ,therefore according Equation 3, topic distribution P(z1 |d1 ) =P(z1 |d2 ) = 0.5, P(z2 |d1 ) = P(z2 |d2 ) = 0.5. two documents topicdistributions, may necessarily coordinates. d3 also1103fiLe & Lauwtopic distribution d1 d2 , dierent coordinate. fact, coordinateform (1, ?) topic distribution. example illustrated Figure 3.suggests enforcing neighborhood structure topic space may necessarily lead data points closer visualization space. postulateregularizing visualization space eective. also advantages computational eciency so, describe shortly. Therefore,dene F(i , j ) squared Euclidean distance ||xi xj ||2 correspondingvisualization coordinates.4.2 Neighborhood Graphdiscuss neighborhood graph may approximated, concerns twoissues graph edges dened, well weighted. neighborhood graph constructed original data space represent documenttf-idf vector (Manning, Raghavan, Schutze, et al., 2008). also experiment dierentvector representations, including word counts term frequencies, nd tf-idf givebest results. distance two document vectors measured using Euclideandistance.4.2.1 Graph Constructionresearch studies properties methods construction neighborhood graphs (Zemel & Carreira-Perpinan, 2004; Carey & Mahadevan, 2014). Sinceconstruction neighborhood graph critical step may aect performancevarious graph-based algorithms, problem research issue independent interest. scope exploring well-established graph construction techniquesmay apply case semantic visualization. investigate various graphconstruction methods empirically Section 6.following, briey review two categories graph construction methods.1. Neighborhood-based Graphs. formulation, edges formed data pointsdeemed suciently close other. admits dierent denitionssucient closeness. common denitions found literature includetwo below.(a) -ball: neighborhood graph contains edge connecting two documents didj , di dj distance less threshold .(b) k-nearest neighbors (k-NN) graph: neighborhood graph contains edgeconnecting two documents di dj , di set Nk (dj ) knearestneighbors dj , dj set Nk (di ).-ball k-NN strongly data-dependent parameters (i.e., k)straightforward choose best value parameters. Neither guaranteesgraph would connected. also need carefully selected tuned,extent also aect balance contribution neighborsR+ non-neighbors R neighborhood regularization R Equation 8.1104fiSemantic Visualization Neighborhood Graph RegularizationAppendix A, explore empirically graph parameters help maintainbalance within neighborhood regularization function.-ball suers another issue tends produce many edges pointslocated high-density regions, thus little restriction maximum degreevertex. k-NN suer problem one commonlyused types graphs.subsequent development experiments, experiment -ballk-NN graph may variance performance dierent graphconstruction techniques dierent datasets (Hein, Audibert, & Luxburg, 2007; Ting,Huang, & Jordan, 2010; Coifman & Lafon, 2006).2. Minimum Spanning Tree-based Graphs. -ball k-NN quite sensitivenoise sparsity, graph construction based combining multiple minimumspanning trees help reduce sensitivity noise output graph (Zemel& Carreira-Perpinan, 2004). two variations based approach.(a) Perturbed Minimum Spanning Trees (PMST): PMST builds neighborhoodgraph generating > 1 perturbed copies whole dataset accordinglocal noise model tting MST perturbed copy. weighteij [0, 1] assigned edge points xi xj equalaverage number times edge appears trees.(b) Disjoint Minimum Spanning Trees (DMST): DMST produces neighborhoodgraph nding deterministic collection r minimum spanning treessatises property tree collection uses edge trees.neighborhood graph union edges trees contains r(N 1)edges.representative category, use DMST, deterministic easierconstruct PMST showing similar ecacies.4.2.2 Graph Weightingnext issue assign weights edges neighborhood graph.respect, consider two variations edge weights.1. Simple Minded :ij =1,0,di dj connected,otherwise.(9)simplest approach use binary weighting assign weightsedges. However, approach assign uniform weights edgessensitive errors, cli eect 1 immediately 0. Moreover,since weights smoothed, could result loss information.hypothesize among connected nodes, may still dierencesterms degrees similarity, expressed mutual distances.motivates second approach below.1105fiLe & Lauw2. Heat Kernel :ij =exp(0,||di dj ||2),di dj connected,otherwise.(10)alternative approach using Heat Kernel function (Belkin & Niyogi, 2001;Jebara, Wang, & Chang, 2009). Heat Kernel advantage Simple Mindedallowing smoother weights edges, helps address issues sensitivityloss information. However, Simple Minded parameterized, HeatKernel one parameter needs determined (i.e., ). Note = ,Heat Kernel degenerates Simple Minded, i.e., former generalformulation. exact value important model wouldeectively absorbed regularization parameter. simplicity, set = 2.5. Model Fittingdiscuss parameters model described Sections 3 4learned. One well-accepted framework learn model parameters using maximum posteriori (MAP) estimation Expectation-Maximization EM algorithm (Dempster,Laird, & Rubin, 1977).model, regularized conditional expectation complete-data log likelihood MAP estimation priors is:Q(|) =+MnNZP(z|n, m, ) log P(z|xn , )P(wnm |z )n=1 m=1 z=1NZn=1z=1log(P(xn )) +log(P(z )) +Zlog(P(z ))z=1+ R(|),current estimate. P(z|n, m, ) class posterior probability nthdocument mth word current estimate. P(z ) symmetric Dirichlet priorparameter word probability z . P(xn ) P(z ) Gaussian priors zeromean spherical covariance document coordinates xn topic coordinates z .set hyper-parameters = 0.01, = 0.1N = 0.1Z following PLSV (Iwataet al., 2008).E-step, P(z|n, m, ) updated follows:P(z|n, m, ) = ZP(z|xn , )P(wnm |z )z =1 P(z|xn , )P(wnm |z ).M-step, maximizing Q(|) w.r.t zw , next estimate word probabilityzw follows:N Mnm=1 I(wnm = w)P(z|n, m, ) +n=1zw = W,N Mnm=1 I(wnm = w )P(z|n, m, ) + Ww =1n=11106fiSemantic Visualization Neighborhood Graph RegularizationI(.) indicator function. z xn cannot solved closed form,estimated maximizing Q(|) using quasi-Newton (Liu & Nocedal, 1989).computation fo gradients Q(|) w.r.t z xn depend specickernel used (see Section 3.3).Gaussian kernel, following gradients:nQ(|)=P(z|xn , ) P(z|n, m, ) (z xn ) z ,zNQ(|)=xnn=1 m=1MnZm=1 z=1R(|).P(z|xn , ) P(z|n, m, ) (xn z ) xn +xnStudent-t kernel, following gradients:N Mn2 P(z|xn , ) P(z|n, m, ) (z xn )Q(|)=z ,z1 + ||xn z ||2n=1 m=1MnZ2 P(z|xn , ) P(z|n, m, ) (xn z )Q(|)R(|)=xn +.2xn1 + ||xn z ||xnm=1 z=1gradient R(|) w.r.t. xn computed depending form regularization function R(|). use proposed regularization function R (|)described Section 4.1.1, following gradient:R (|)R(|)=xnxn(xn xj )1=4nj (xn xj )4(1 nj ).22(F(n , j ) + 1)j=1;j=nj=1;j=nmentioned earlier, eciency advantage regularizing visualization space. R(|) contain variable z regularization visualizationO(N 2 ). contrast, regularizaspace. complexity computing R(|)xntion topic space, take gradient R(|) w.r.t z . contributes. Therefore, regularizatowards greater complexity O(Z 2 N 2 ) compute R(|)ztion topic space would run much slower visualization space.6. Experimentsmain objective experiments evaluate eectiveness neighborhood regularization semantic visualization model. describing experimental setup,rst examine dierent design choices model relating kernel, graph construction, regularization function. Thereafter, compare Semafore baselinemethods also aim address visualization topic modeling, quantitativelyqualitatively, rst terms visualization terms topic modeling.1107fiLe & Lauw6.1 Experimental Setupsection, give description benchmark datasets well suitable metricsused evaluation.6.1.1 Datasetsuse three real-life, publicly available datasets (Cardoso-Cachopo, 2007) evaluation.20N ews contains newsgroup articles (in English) 20 classes.Reuters8 contains newswire articles (in English) 8 classes.Cade12 contains web pages (in Brazilian Portuguese) classied 12 classes.benchmark datasets used document classication. task fullyunsupervised, ground-truth class labels useful objective evaluation.create balanced classes sampling fty documents class, following practicePLSV (Iwata et al., 2008). results in, one sample, 1000 documents 20N ews,400 Reuters8, 600 Cade12. vocabulary sizes 5.4K 20N ews, 1.9KReuters8, 7.6K Cade12. algorithms probabilistic, generate samplesdataset. sample, conduct independent runs. Therefore, resultreported setting average total 25 runs.6.1.2 Metricssuitable metric, return fundamental principle good visualizationpreserve relationship documents (in high-dimensional space)lower-dimensional visualization space. User studies, even well-designed, couldoverly subjective may repeatable across dierent users reliably. Therefore,objective evaluation, rely two types quantitative analysis:Classication: evaluation relies ground-truth class labels founddatasets. well-established practice many clustering visualizationworks machine learning. basis evaluation reasonable assumptiondocuments class related documents dierent classes.Therefore good visualization would place documents class neighborsvisualization.document dn , hide true class cn , generate predictionclass Ct (n) taking majority class among t-nearest neighbors, determined Euclidean distance visualization space. Classication accuracyClassif ication Acc(t) dened fraction documents whose predicted classCt (n) matches true class cn . specically, have:N1(Ct (n) = cn ),Classif ication Acc(t) =Nn=11108fiSemantic Visualization Neighborhood Graph Regularizationdelta function equals 1 prediction matches 0 otherwise.metric used PLSV (Iwata et al., 2008). accuracy computedbased documents coordinates, trends produced computed basedtopic distributions (due coupling kernels described Section 3.3).Neighborhood Preservation: evaluation rely ground-truth classlabels local neighborhood structure input data. assumptiongood visualization would able preserve local structure inputdata much possible. two documents neighbors input data,still neighbors visualization space.every document dn , compute sets t-nearest neighbors Yt (n) Xt (n)document dn input data visualization respectively. neighborhoodpreservation accuracy P reservation Acc(t) dened average fractionoverlap size Yt (n) Xt (n) size Yt (n) (i.e. t), n = 1, . . . , N .specically, have:P reservation Acc(t) =N1 |Yt (n) Xt (n)|,Nn=1|Yt (n) Xt (n)| size overlap set Yt (n) Xt (n).similar measure found literature (Akkucuk & Carroll, 2006),called rate agreement local structure agreement rate usedmeasure well local structure preserved input datalow dimensional embedding. also used tuning parameters non-lineardimensionality reduction method (Chen & Buja, 2009).subsequent experiments, let vary range [5, 50] step size5 report accuracies. Since dierent methods may behave dierently dierentts, choosing specic comparison may unfair methods. Moreover,method consistently well dierent ts would also smoother localstructure. Therefore, comparing various methods, present preservationclassication accuracies averaged across [5, 50], denoted P reservation Acc(Avg)Classif ication Acc(Avg) respectively.6.2 Parameter Studysection, study eects graph parameters model. Specically,parameters concern graph construction, including number neighbors k k-NNgraph, distance threshold -ball graph, number minimum spanning treesr DMST. type graph, use Simple Minded weight. followinggures, regularization function R = 10 number topics Z = 20.use neighborhood preservation accuracy P reservation Acc(t) show eectsgraph parameters metric need ground-truth class labels,always available tuning graph parameters.1109fiLe & LauwWWWEZFigure 4: Preservation accuracy Semafore using k-NN graph dierent neighborhood size k (a) 20N ews, (b) Reuters8, (c) Cade12.WWWEZFigure 5: Preservation accuracy Semafore using DMST graph dierent number minimum spanning trees r (a) 20N ews, (b) Reuters8, (c) Cade12.EWWWZFigure 6: Preservation accuracy Semafore using -ball graph dierent valuesdistance threshold (a) 20N ews, (b) Reuters8, (c) Cade12.1110fiSemantic Visualization Neighborhood Graph RegularizationFigure 4, show performance model dierent neighborhood size kk-NN graph dierent datasets. every k, vary plot P reservation Acc(t).Figure 4 shows optimum k 20N ews, Reuters8, Cade12 10, 10, 5respectively. compute average accuracy P reservation Acc(Avg) conrmsoptima indeed k values. on, use k=10 20N ewsReuters8, k=5 Cade12 k-NN graph used.DMST graph, plot P reservation Acc(t) dierent number minimumspanning trees r dierent datasets Figure 5. dicult see r bestgure dierences much. P reservation Acc(Avg)computed shows three datasets, optimum r=5,6,7.Subsequently, use r=6 DMST graphs three datasets.-ball graph, Figure 6 plot P reservation Acc(t) dierent valuesrange [1.32, 1.40]. choose range =1.32 =1.40 roughly giveaverage number neighbors 5 100 respectively. P reservation Acc(Avg) showsoptimum 20N ews, Reuters8, Cade12 1.34, 1.35, 1.33 respectively.6.3 Model Analysissection, study various design choices involved designing Semaforemodel, nally concluding eventual synthesis design choices usedcomparison baselines. keep discussion focused organized,following sub-section, vary single design choice, order isolate eects.unvaried, model following setup default: number topics Z = 20,graph construction method k-NN, graph weighting method simple minded,RBF kernel Gaussian, regularization function R = 10.6.3.1 Neighborhood Graph Constructioninvestigate three graph construction methods: k-NN, -ball DMST, representatives neighborhood-based minimum spanning tree-based methods respectively.graph, parameter tuned shown Section 6.2. regularizationparameter , try dierent settings dataset. happens = 10performs best graph construction methods across three datasets.Figure 7, run Semafore dierent types graph three datasetsreport P reservation Acc(Avg) dierent number topics Z. results showdierent types graph behave dierently dierent datasets. 20N ews, -ballDMST give model highest performance. Since dierence twostatistically signicant, choose use DMST subsequent experiments 20N ews.Reuters8, since -ball outperforms others (signicant 0.05 level), goingdefault choice subsequent experiments. Cade12, choice DMST,slightly better k-NN (statistically signicant Z = 10, 40, 50).6.3.2 Neighborhood Graph Weightingcompare two variations graph weighting methods, namely: Simple MindedHeat Kernel methods. experiment, use k-NN graph specic ks dierent1111fiLe & LauwD^dWWWEEEdEEdZEdFigure 7: eects dierent graph construction methods models performance.,<tWWW^DtEdEEdZEdFigure 8: eects dierent graph weighting schemes models performance.graph used experiment k-NN graph specic ks dierentdatasets studied Section 6.2.EdE^<WWW'<EdZEdFigure 9: eects Gaussian Student-t RBF kernels models performance.1112fiSemantic Visualization Neighborhood Graph RegularizationRegularization functionGraph constructionGraph weightingRBF kernel20N ewsRDMSTHeat KernelStudent-tReuters8R-ballHeat KernelStudent-tCade12RDMSTSimple MindedStudent-tTable 2: Synthesized Model Dataset.datasets studied Section 6.2. regularization parameter set 10 tryingvarious settings picking best one.Figure 8, compare Simple Minded method Heat Kernel method seeinuences model dierent number topics Z. observe Heat Kernelsignicantly consistently better Simple Minded method across cases20N ews Reuters8. dierence statistically signicant 0.01 level. Oneexplanation Heat Kernel assigns smoother weights graph edges, thusrobust Simple Minded. Cade12, Simple Minded slightly better, thoughdierences statistically signicant 0.05 level Z = 40. Subsequently,use Heat Kernel 20N ews Reuters8, Simple Minded Cade12 partnal synthesis.6.3.3 RBF Kerneldescribed Section 3.3, express topic distributions function visualizationcoordinates using RBF network abstraction. section, show dierentRBF kernels aect models performance. two kernels exploring Gaussian(Equation 3) Student-t (Equation 4). tune regularization term kernelsee best one two kernels = 10.Figure 9 shows results dierent number topics Z. Student-t kernel slightedge Gaussian kernel consistently across dierent number topics. dierencesmall, statistically signicant (at 0.05 level) majority cases (for 20N ewsZ = 10, 20, 30, 50, Reuters8 Z = 30, Cade12 Z = 10, 30, 50). slightimprovement could sign crowding problem exist model. Student-tkernel would even useful extreme crowding issues,number documents visualized even larger. Subsequently, dueslight edge, use Student-t part nal synthesis. see shortly, usingStudent-t within synthesized model results signicant improvement overall.6.3.4 Synthesised Semafore ModelBased model analysis preceding paragraphs, combine design choicesnal synthesis model called Semafore. synthesized model slightly dierentdierent datasets, listed Table 2.conduct another set experiments verify synthesized modelswould produce noticeable improvement earlier version (kNN + Simple Minded+ Gaussian Kernel) appeared earlier work (Le & Lauw, 2014b), underlining1113fiLe & LauwEE^D'<,<^<EdEEdZEE^D'<D^d^D^<WWWEE^D'<D^d,<^<EdFigure 10: synthesized models dierent properties compared earlier version(kNN + Simple Minded + Gaussian Kernel) appeared earlier work(Le & Lauw, 2014b).utility subsequent enhancements. Figure 10 shows indeed case.Based standard deviations shown gures, improvements clear20N ews Reuters8 clear Cade12. Paired samples t-test indicateimprovement signicant 0.05 level lower cases, except casesZ = 10, 20 Cade12. use synthesized models comparisonsbaseline methods following section.6.4 Comparison Visualizationscompare proposed model several baselines. First, outline setcomparative methods. Thereafter, discuss quantitative evaluation (in terms accuracy),well qualitative evaluation (in terms example visualizations). Finally, showgains visualization quality come expense topic modeling.semantic visualization seeks ensure consistency topic model visualization, comparison focuses methods producing topics visualization coordinateslisted Table 3.Semafore proposed method incorporates neighborhood structuresemantic visualization.PLSV (Iwata et al., 2008) state-of-the-art, representing joint approachwithout neighborhood structure preservation.PE (LDA) represents pipeline approach involving topic modeling LDA (Bleiet al., 2003), followed visualizing documents topic distributions PE (Iwataet al., 2007). pipeline better LDA/MDS appeared earlierwork (Le & Lauw, 2014b). pipeline methods, shown inferior PLSV(Iwata et al., 2008), reproduced avoid duplication.1114fiSemantic Visualization Neighborhood Graph RegularizationVisualizationTopic modelJoint modelNeighborhoodSemaforePLSVPE (LDA)t-SNE (LDA)Table 3: Comparative Methods.t-SNE (LDA) another pipeline approach rst uses LDA (Blei et al., 2003)learn topic model use t-SNE (Van der Maaten & Hinton, 2008) visualizedocuments topic distributions.completeness, also conduct experiments comparing method t-SNELaplacian EigenMaps (LE) (Belkin & Niyogi, 2003) (direct visualization, without topicmodeling). keep discussion focused, show Appendix B,consider t-SNE LE comparative baselines two methods modelvisualization, topics.6.4.1 Accuracysection, compare model several baselines terms classicationaccuracy (Figure 11) neighborhood preservation accuracy (Figure 12). twogures, standard deviations Semafore shown.Classcation Accuracy. Figure 11(a), 11(c) 11(e) show Classf ication Acc(t)dierent ts Z = 20 20N ews, Reuters8, Cade12 respectively. t,comparison shows outperformance Semafore baselines consistently. fourmethods show behavior performances decrease increases.increases, may lose accuracy predicting labels documents near bordercluster.Now, vary number topics Z. Figure 11(b), show performanceClassf ication Acc(Avg) 20N ews. Figure 11(d) 11(f) show Reuters8Cade12 respectively. gures, draw following observationscomparative methods:Semafore performs best datasets across various numbers topics (Z).Semafore beats PLSV 25% 51% 20N ews, 613% Reuters8,2232% Cade12. margins performance respect PLSV statistically signicant 0.01 signicant level lower cases. eectively showcasesutility neighborhood regularization enhancing quality visualization.preserving local consistency, Semafore achieves good accuracy even smallnumber topics (e.g., 10).PLSV performs better PE (LDA) t-SNE (LDA), showsutility joint, instead separate, modeling topics visualization.PE (LDA) t-SNE (LDA) worse PLSV embeds documentsusing two-step reductions optimize separately two dierent objective functions.1115fiLe & LauwTherefore, errors previous step may propagate next, withoutopportunity correction. may cause distortions visualization.cases, PLSV, PE (LDA) t-SNE (LDA) tend decreasing accuracies number topics increases. may number topicincreases, topic distributions word probabilities may overt datathus accuracy reduced. contrast, Semafore shows quite stable performance across dierent numbers topics. may explained utilityneighborhood regularization, helps prevent overtting numbertopics increases.Neighborhood Preservation Accuracy. better classication accuracy,Semafore also preserves well local structure input data visualization space.P reservation Acc(t) results Figure 12(a), 12(c) 12(e) show Semaforeconsistently better baselines terms neighborhood preservation acrossdierent ts dierent datasets. Figure 12(b), 12(d) 12(f), vary numbertopics Z report P reservation Acc(Avg) results. Semafore beats PLSV41% 76% 20N ews, 2436% Reuters8, 2945% Cade12 termsneighborhood preservation accuracy. improvements Semafore PLSVstatistically signicant 0.01 signicant level lower cases.accuracy results based visualization coordinates. also computed accuracies based topic distributions, similar trends.6.4.2 Visualizationsprovide intuitive appreciation, briey describe qualitative comparison visualizations. method dataset, visualization shown scatterplot (bestseen color). document coordinate, assigned shape color basedclass. topic also coordinate, drawn black, hollow circle. legendprovided, mapping symbol corresponding class label.Note illustrative, rather comparative discussion, objectiveevaluation rely eyeballing alone. However, shown quantitativeresults preceding section, section, focus qualitative studyoutput visualizations.20News. Figure 13 shows visualization 20N ews dataset. Semafores Figure 13(a)shows dierent classes well separated. distinct clusters blue squarespurple diamonds top hockey baseball classes respectively, clustersorange triangles pink asterisks bottom cryptography medicine, etc.Beyond individual classes, visualization also places related classes nearby. Computerrelated classes found lower left. Politics religion lower right.Comparatively, Figure 13(b) PLSV shows crowding center. instance,motorcycle (green dashes) autos (red dashes) mixed center without goodseparation. Figure 13(c) PE (LDA) worse. PE (LDA) give good separationsimilar classes. mixes autos (red dashes) space (green circles) togethercenter. Medicine (pink asterisks) also mixed classes PE (LDA)Semafore PLSV give good separation it. Figure 13(d) visualization tSNE (LDA). Although t-SNE (LDA) separate well hockey (blue squares) baseball1116fiSemantic Visualization Neighborhood Graph RegularizationW>^sW>^EsW>ZsZs^W>^s^E>EdW>^sEdEsW>^^E>^E>EdFigure 11: Classication Accuracy Comparison.1117fiLe & LauwW>^sW>WW^EsWW>ZsZs^W>^s^E>EdWW>^sEdEsW>WW^^E>^E>EdFigure 12: Preservation Accuracy Comparison.1118fiSemantic Visualization Neighborhood Graph Regularization(purple diamonds) classes, able detect semantic similarities (as baseballhockey sports). addition, still mixes documents dierent classestogether center upper right.Reuters8. Figure 14 shows visualization outputs Reuters8 dataset. SemaforeFigure 14(a) better separating eight classes distinct clusters. anticlockwise direction top, navy blue diamonds (money-fx ), red dashes(interest), red squares (crude), light blue pluses (earn), green triangles (acq), purple crosses(ship), blue asterisks (grain), nally orange circles (trade).comparison, PLSV Figure 14(b) shows several classes intermixedcenter, including red dashes (interest), orange circles (trade), navy blue diamonds(money-fx ). PE (LDA) Figure 14(c) also worse mixes dierentiated classesred dashes (interest) navy blue diamonds (money-fx ) together. t-SNE (LDA)Figure 14(d) seems better cluster separation still mix documents dierentclasses together red squares (crude) green triangles (acq) upper right.Green triangles (acq) also mix light blue pluses (earn) left visualizationt-SNE (LDA).Cade12. Figure 15 shows visualization outputs Cade12.challenging dataset. Even so, Semafore Figure 15(a) still achieves better separationclasses, compared PLSV Figure 15(b). Particularly, Semafore givesbetter separation esportes (green triangles) well compras-on-line (orange circles)PLSV PE (LDA). t-SNE (LDA) shows quite good clusters esportes (greentriangles) well compras-on-line (orange circles) also merges many dierentclasses together clusters right upper right.6.5 Comparison Topic ModelsOne question whether Semafores gain visualization quality closest baselinePLSV expense quality topic model. investigate this,compare topic models Semafore PLSV, share core generative process.parity, comparison, include joint models, whereby visualizationcoordinates aect topic models well.metric use measure quality topic models pairwise mutual informationPMI. measures topic interpretability, based coocurrence frequencies top wordstopic large external corpus. Although metrics perplexity heldout likelihood show generalization ability learned topic model unseen testdata, traditional metrics capture whether topics coherent (Chang, Gerrish,Wang, Boyd-Graber, & Blei, 2009). Therefore, comparison, rely PMI,measure quality topic words terms interpretability human.human subjects, interpretability closely related coherence (Newman, Lau, Grieser, &Baldwin, 2010), i.e., much top keywords topic associatedother. extensive study evaluation methods coherence, Newman et al. (2010)identify Pointwise Mutual Information (PMI) best measure, termsgreatest correlation human judgments.PMI based term cooccurrences. pair words wi wj , PMI denedp(wi ,wj )log p(wi )p(w. topic, average pairwise PMIs among top 10 wordsj)1119fiLe & Lauw^W>^sW>^E>>Figure 13: Visualization documents 20N ews number topics Z = 20. pointrepresents document shape color represent document class.topic drawn black, hollow circle.1120fiSemantic Visualization Neighborhood Graph Regularization^W>^sW>^E>>Figure 14: Visualization documents Reuters8 number topics Z = 20.point represents document shape color represent document class.topic drawn black, hollow circle.1121fiLe & Lauw^W>^sW>^E>>Figure 15: Visualization documents Cade12 number topics Z = 20. pointrepresents document shape color represent document class.topic drawn black, hollow circle.1122fiSemantic Visualization Neighborhood Graph RegularizationW>^s^WD/^WD/^^W>^sEdEdZEFigure 16: Topic Interpretability Semafore PLSV terms PMI Score (higherbetter).topic. topic model, average PMI across topics. Intuitively, PMI higher(better), topic features words highly correlated one another.Key PMI use external corpus estimate p(wi , wj ) p(wi ). FollowingNewman et al. (2009), use Google Web 1T 5-gram Version 1 (Brants & Franz, 2006),huge corpus n-grams generated 1 trillion word tokens. p(wi ) estimatedfrequencies 1-grams. recommended Newman et al., p(wi , wj ) estimatedfrequencies 5-grams. obtain PMI English-based 20N ews Reuters8,Cade12 possess large-scale n-gram corpus specicallyBrazilian Portuguese.Figure 16, plot PMI score various number topics Z. Semafore performsbetter PLSV across topics settings. Figure 16(a) 20News, exceptcase Z = 10, cases Semafores outperformance signicant 0.05 levellower. Figure 16(b) Reuters8, cases Semafores outperformance signicant0.05 level lower except Z = 30. results show Semafore improvesvisualization sacricing topic interpretability learned topics.greater appreciation quality output topic models, Appendix C,show several examples topic models Z = 20, Semafore PLSV, termstop keywords highest probabilities topic.7. Conclusionpaper, address semantic visualization problem, jointly conducts topicmodeling visualization documents. propose new framework incorporateneighborhood structure within probabilistic semantic visualization model called Semafore.model carefully designed reect context semantic visualization, leadingnumber design choices related RBF kernel mapping topic visualiza1123fiLe & Lauwtion spaces, approximation neighborhood graph construction weighting,well appropriate regularization functions spaces. Experiments real-lifedatasets show Semafore signicantly outperforms baselines terms visualization quality accuracy, similar, slightly better topic model.provides evidence neighborhood structure, together joint modeling topicsvisualization, important semantic visualization.Appendix A. Balancing Contributions Neighbors Non-neighborsRegularizationmentioned Section 4.2, balance contribution neighbors R+non-neighbors R neighborhood regularization R Equation 8 may require carefultuning graph parameters (i.e., k). example, case using k-NN graphN total number documents, would kN terms neighbor regularizationR+ , (N k)N terms non-neighbor regularization R . Supposing N increasessignicantly, might imbalance k remain unchanged. Therefore, Nchanges, k also tuned accordingly maintain balance. simplisticpoint, ratio kN (N k)N would remain roughly Nk grow similar factors. practice, recommend tuning k carefully.run additional experiments validate argument 20N ews dataset.basic point N changes, k tuned still show signicant improvementdue neighborhood graph regularization. closest baseline PLSV, empirically terms classication accuracy, well conceptually PLSV shares similargenerative process dierent kernel without neighborhood regularization.Hence, compare performance method Semafore (with k-NN graph, heatkernel weighting, Student-t kernel) PLSV various data sizes Z = 20 topics.Figure 17(a) dataset size N = 500, Semafore runs k = 10.Figure 17(b) dataset size N = 1000, Semafore runs k = 10.Figure 17(c) dataset size N = 5000, Semafore runs k = 50.note 10X dierence smallest largest datasets.Yet relative outperformance Semafore PLSV around 15% 20% evidentacross three datasets. supports case k tuned produce positiveeect using neighborhood graph regularization.Appendix B. Additional Comparisonsmentioned Section 6.4, completeness, include additional comparisonsvisualization methods also aim topic modeling. particular, include twomethods. First, include t-SNE (Van der Maaten & Hinton, 2008), also usedcomposite t-SNE (LDA). Second, include Laplacian EigenMaps (LE) (Belkin &Niyogi, 2003), takes input neighborhood graph. Figure 18 Figure 19show classication accuracy preservation accuracy Semafore , t-SNE LE1124fiSemantic Visualization Neighborhood Graph Regularization^W>^s^W>^s^EEW>^sEFigure 17: Classication accuracy comparison 20N ews various data sizes (Z = 20).^^E>EZFigure 18: Classication accuracy comparison.^^EWWW>EZFigure 19: Preservation accuracy comparison.1125fiLe & Lauwvarying t. Semafore outperforms LE cases. t-SNE, Semafore outperforms t-SNE Reuters8. However, 20N ews Cade12, dicult tellwhether Semafore t-SNE better. t-SNE tends decreasing accuracy increases. expected t-SNE known focus preserving local structure(Van der Maaten & Hinton, 2008). small, basically consider localstructure visualization. increases, consider global structurevisualization Semafore outperforms t-SNE signicantly. Overall, Semaforestable t-SNE changes, indicates Semafore tries balance preservinglocal global structure better t-SNE. emphasize comparisoninformation purpose only, regard t-SNE LE comparative baselines.Appendix C. Topic Model Examplesshowcase topic models derived Semafore PLSV. 20N ews, Table 4 showstopics Semafore, Table 5 shows topics PLSV. Reuters8, Table 6shows topics Semafore, Table 7 shows topics PLSV. Cade12, Table 8shows topics Semafore, Table 9 shows topics PLSV.method, show list twenty topics. topic, producetop ten words highest probabilities. shown top words, topicscorrespond strongly classes. example, topic s19 Table 4 20N ewsChristianity, corresponds soc.religion.christian class. Topic s4cars motorcycles, corresponding rec.autos rec.motorcycles. Topic s12 probablyconcerning categories rec.sport.baseball and/or rec.sport.hockey.Overall, observe quality topic words comparable across comparative methods. Note direct correspondence topics dierentmethods (e.g., rst topic Semafore may correspond rst topic PLSV).manual inspection, see related topics, e.g., s4 p6 ,s12 p7 . However, sets topics set keywords topicidentical. borne slight dierence terms PMI scores.qualitative study helps show Semafore improves visualization quality,still maintaining least quality topic words, better. supportsconclusion reached quantitative comparisons main manuscript.1126fiSemantic Visualization Neighborhood Graph RegularizationTable 4: Semafores Topic Model 20N ews (for 20 topics)Topic IDs0s1s2s3s4s5s6s7s8s9s10s11s12s13s14s15s16s17s18s19Top 10 Wordsspace, system, -rcb-, book, computer, university, list, post, price, sciencearticle, year, good, write, guy, well, time, head, question, leavegun, law, kuwait, people, death, fbus, article, control, weapon, childwindow, le, program, widget, application, type, will, resource, call, functioncar, bike, speed, engine, drive, lock, turn, mile, front, changewill, power, place, work, rate, write, sound, lead, good, interestedwrite, article, thing, time, people, better, start, problem, will, goodwrite, time, people, friend, pay, public, article, tax, opinion, moneypeople, claim, write, system, person, moral, evidence, objective, read, stateimage, datum, graphic, send, le, format, package, software, mail, includearmenian, re, jew, child, kill, start, people, turkish, door, israelsystem, board, will, datum, time, work, tape, test, copy, commandgame, team, year, player, win, play, will, hit, season, hockeywill, post, space, good, time, include, cost, option, launch, peopledrive, card, window, appear, disk, ram, driver, memory, work, colormr., president, stephanopoulo, state, group, consider, party, question, issue, presswrite, article, well, will, thing, work, point, include, time, helpkey, article, chip, food, write, people, government, encryption, thing, algorithmprice, buy, apple, computer, dealer, t, model, problem, sell, monitorgod, jesus, will, christian, religion, faith, truth, bible, belief, churchTable 5: PLSVs Topic Model 20N ews (for 20 topics)Topic IDp0p1p2p3p4p5p6p7p8p9p10p11p12p13p14p15p16p17p18p19Top 10 Wordswrite, people, christian, belief, time, faith, god, religion, life,god, will, jesus, kuwait, atheist, church, christian, man, religion, sinarmenian, appear, art, turkish, tartar, 1st, village, armenia, 1.40, genocidewill, key, write, time, article, government, system, thing, chip, hitmr., stephanopoulo, president, will, party, state, door, time, meeting, openwrite, re, article, gun, system, -rcb-, start, people, fbus, claimcar, will, bike, engine, drive, well, dealer, battery, change, frontgame, win, year, will, team, play, season, good, goal, playoplayer, team, write, hockey, game, fan, article, year, will, guyspace, system, datum, will, april, nasa, security, university, computer, listgraphic, image, le, ftp, send, format, package, system, datum, objectimage, datum, program, window, version, le, software, tool, support, userdrive, jumper, master, ndet loop, slave, rate, gun, function, crime, setwindow, le, card, will, program, color, driver, support, disk, bitpeople, write, state, article, law, government, country, rights, jew,write, article, thing, people, good, will, time, lot, year, daywork, drive, tape, scsus, problem, simm, controller, write, memory, articlewidget, -rcb-, window, -lcb-, application, resource, set, visual, type, leprice, will, write, system, computer, article, apple, chip, monitor, boardwill, vote, comp, newsgroup, suit, problem, os2, sco, post, mail1127fiLe & LauwTable 6: Semafores Topic Model Reuters8 (for 20 topics)Topic IDs0s1s2s3s4s5s6s7s8s9s10s11s12s13s14s15s16s17s18s19Top 10 Wordscompany, pipeline, raise, crude, march, spokesman, renery, capacity, corp, postpct, bank, day, stg, today, reuter, money, market, mln, billoer, share, company, board, group, acquire, stock, dlr, acquisition, receiveexchange, currency, dollar, west, nance, baker, monetary, germany, continue, interestshare, reuter, dlr, mln, buy, company, corp, pay, stock, groupprice, opec, market, bpd, ocial, february, month, output, saudus, januaryrate, bank, pct, cut, fund, prime, point, reserve, issue, lowerbillion, foreign, import, increase, dlr, trade, economic, export, will, countrybank, billion, market, government, fall, stock, economy, rise, surplus, decitwill, company, sell, pct, vessel, operation, week, billion, shipping, unitstrike, port, union, spokesman, cargo, employer, worker, sector, redundancy, courtoil, export, dlr, industry, year, pct, future, company, report, pricereuter, pct, report, national, week, brazil, today, increase, pay, apriltrade, japan, japanese, reagan, state, tari, unite, market, washington, ocialgrain, mln, soviet, crop, tonne, year, usda, production, fall, analysttrade, talk, gulf, gatt, bill, yeutter, round, reuter, call, negotiationcerticate, reuter, cost, government, program, agreement, agriculture, will, study, loanyear, ocial, import, will, state, price, government, china, land, risemln, ct, loss, net, shr, dlr, prot, qtr, reuter, yearoil, mln, will, barrel, dlr, crude, source, level, petroleum, dayTable 7: PLSVs Topic Model Reuters8 (for 20 topics)Topic IDp0p1p2p3p4p5p6p7p8p9p10p11p12p13p14p15p16p17p18p19Top 10 Wordswill, oil, company, reuter, industry, canada, price, shell, raise, sellrate, currency, dollar, exchange, baker, west, will, bank, reuter, treasurybank, pct, day, import, year, rate, export, february, expect, reutershare, company, corp, oer, stock, board, will, reuter, dlr, buyrate, bank, pct, prime, cut, point, interest, market, lower, savingsmarket, bank, stock, price, japan, ministry, rise, ocial, gulf, bondreuter, pct, week, report, year, march, mark, american, commission, guremln, ct, loss, net, dlr, shr, year, prot, qtr, reutermln, pct, billion, stg, dlr, reuter, market, january, revise, risebillion, dlr, rate, market, surplus, currency, reserve, trading, dollar, foreignoil, opec, price, bpd, pipeline, mln, crude, ocial, dlr, outputcrude, dlr, barrel, corp, capacity, renery, oil, company, oer, groupreuter, ocial, state, cut, gulf, government, today, action, force, telloil, government, indonesium, price, foreign, bank, billion, reserve, company, industrycerticate, company, mln, year, grain, cooperative, program, dlr, government, costyear, trade, agriculture, reuter, grain, agreement, gatt, yeutter, nancial, agriculturalstrike, port, union, spokesman, employer, brazil, cargo, worker, redundancy, sectortrade, japan, japanese, reagan, tari, unite, washington, state, nakasone, semiconductorgrain, mln, crop, tonne, soviet, year, ocial, china, pct, oertrade, country, minister, talk, state, meeting, economic, exchange, issue, baldrige1128fiSemantic Visualization Neighborhood Graph RegularizationTable 8: Semafores Topic Model Cade12 (for 20 topics)Topic IDs0s1s2s3s4s5s6s7s8s9s10s11s12s13s14s15s16s17s18s19Top 10 Wordssp, aulas, tecnologia, rj, sao, area, janeiro, particulares, areas, sicaterra, jun, gif, busca, virtual, brasil, forum, tempo, noticias, revistastrabalho, seguranca, saude, medicina, ocupacional, prevencao, ppra, pcmso, imagem, imagenspeixes, cade, lazer, pesca, agua, rio, praia, hotel, sao, doceagar, vida, personal, sica, base, tratamento, tem, pode, sistema, trainersao, br, rio, sul, criancas, www, escola, mail, http, atendimentolinks, page, home, fotos, pagina, dicas, download, tenis, informacoes, jogosinternet, informatica, acesso, mg, br, servicos, provedor, mail, revista, horizonteservicos, sao, paulo, entregas, entrega, sp, cesta, express, empresa, servicopesca, sp, grupo, brasil, eventos, video, mg, informacoes, turismo, dancaastronomia, pagina, jose, foi, bem, espaco, tem, veja, losoa, correiomp, banda, musicas, rock, musica, page, letras, bandas, pagina, sitehistoria, cultura, mundo, site, page, brasil, informacoes, rs, livro, artenoticias, jornal, cidade, sp, sao, regiao, demolay, ordem, rio, capituloempresas, informacoes, informacao, dados, atraves, textos, mail, equipe, unicamp, centroengenharia, servicos, projetos, empresa, consultoria, quimica, instituto, pesquisa, rio, manutencaosite, informacoes, brasil, associacao, educacao, pagina, organizacao, centro, brasileira, direitossoftware, web, empresa, sistemas, sistema, br, marketing, desenvolvimento, windows, dadosvirtual, online, venda, produtos, cade, shopping, internet, loja, compras, cursosfutebol, informacoes, fotos, clube, historia, paulo, sao, quake, pagina, cadeTopic IDp0p1p2p3p4p5p6p7p8p9p10p11p12p13p14p15p16p17p18p19Top 10 Wordsengenharia, projetos, servicos, trabalho, empresa, consultoria, seguranca, sp, medicina, saosao, ong, rio, instituto, personal, educacao, organizacao, sp, paulo, nssao, br, desenvolvimento, sistema, tratamento, mail, sistemas, clientes, informacoes, empresaaulas, formula, quimica, particulares, informacoes, matematica, pilotos, fotos, sica, sitejornal, tenis, noticias, esportes, sp, informacoes, sao, esporte, fotos, linksmusica, page, rock, bandas, links, home, pagina, musicas, music, fotospesca, demolay, sp, peixes, sao, fotos, ordem, capitulo, paulo, jitsump, musicas, nacionais, agar, internacionais, rock, formato, site, page, paginapesquisa, tecnologia, informacoes, cade, ciencia, geograa, pesquisas, area, instituto, paginasite, pagina, internet, mail, clique, veja, br, pode, foi, linksastronomia, informacoes, cultura, site, pagina, brasil, home, page, fotos, historiabanda, fotos, rock, letras, page, musicas, pagina, site, home, mpinternet, provedor, acesso, mg, informatica, software, servicos, belo, horizonte, manutencaofutebol, clube, sao, paulo, campeonato, historia, informacoes, pagina, turismo, tricolornoticias, terra, internet, brasil, informatica, online, jornal, virtual, servicos, buscalinks, page, quake, home, pagina, fotos, dicas, mp, download, informacoesgrupo, banda, karate, pagina, page, informacoes, fotos, home, rio, historiaprodutos, virtual, shopping, cade, venda, online, sao, rio, loja, comprasbr, sao, informacoes, marketing, mail, empresa, internet, www, fax, sitevida, dia, sao, foi, terra, panico, jose, tem, planetas, grandeTable 9: PLSVs Topic Model Cade12 (for 20 topics)1129fiLe & LauwReferencesAkkucuk, U., & Carroll, J. D. (2006). PARAMAP vs. Isomap: comparison two nonlinearmapping algorithms. Journal Classication, 23 (2), 221254.Bai, L., Guo, J., Lan, Y., & Cheng, X. (2014). Local Linear Matrix FactorizationDocument Modeling. Advances Information Retrieval, pp. 398411. Springer.Belkin, M., & Niyogi, P. (2001). Laplacian eigenmaps spectral techniques embedding clustering. Advances Neural Information Processing Systems (NIPS),Vol. 14, pp. 585591.Belkin, M., & Niyogi, P. (2003). Laplacian eigenmaps dimensionality reduction datarepresentation. Neural Computation, 15 (6), 13731396.Belkin, M., Niyogi, P., & Sindhwani, V. (2006). Manifold regularization: geometric framework learning labeled unlabeled examples. Journal Machine LearningResearch (JMLR), 7, 23992434.Bishop, C. M. (1995). Neural Networks Pattern Recognition. Oxford University Press.Bishop, C. M., Svensen, M., & Williams, C. K. (1998). GTM: generative topographicmapping. Neural Computation, 10 (1), 215234.Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. JournalMachine Learning Research (JMLR), 3, 9931022.Brants, T., & Franz, A. (2006). Web 1T 5-gram Version 1. Linguistic Data Consortium,Philadelphia.Buhmann, M. D. (2000). Radial basis functions. Acta Numerica 2000, 9.Cai, D., Mei, Q., Han, J., & Zhai, C. (2008). Modeling hidden topics document manifold.Proceedings ACM Conference Information Knowledge Management(CIKM).Cai, D., Wang, X., & He, X. (2009). Probabilistic dyadic data analysis local globalconsistency. Proceedings International Conference Machine Learning(ICML).Cardoso-Cachopo, A. (2007). Improving Methods Single-label Text Categorization. PhDThesis, Instituto Superior Tecnico, Universidade Tecnica de Lisboa.Carey, C., & Mahadevan, S. (2014). Manifold Spanning Graphs. Twenty-Eighth AAAIConference Articial Intelligence.Chaney, A. J.-B., & Blei, D. M. (2012). Visualizing Topic Models. ProceedingsInternational AAAI Conference Web Social Media (ICWSM).Chang, J., Gerrish, S., Wang, C., Boyd-Graber, J. L., & Blei, D. M. (2009). Readingtea leaves: humans interpret topic models. Advances Neural InformationProcessing Systems, pp. 288296.Chen, L., & Buja, A. (2009). Local multidimensional scaling nonlinear dimension reduction, graph drawing, proximity analysis. Journal American StatisticalAssociation, 104 (485), 209219.1130fiSemantic Visualization Neighborhood Graph RegularizationChi, E. H.-h. (2000). taxonomy visualization techniques using data state reference model. Proceedings IEEE Symposium Information Visualization(InfoVis), pp. 6975.Choo, J., Lee, C., Reddy, C. K., & Park, H. (2013). UTOPIAN: User-driven topic modeling based interactive nonnegative matrix factorization. IEEE TransactionsVisualization Computer Graphics, 19 (12), 19922001.Chuang, J., Manning, C. D., & Heer, J. (2012). Termite: visualization techniques assessing textual topic models. Proceedings International Working ConferenceAdvanced Visual Interfaces (AVI), pp. 7477.Coifman, R. R., & Lafon, S. (2006). Diusion maps. Applied Computational HarmonicAnalysis, 21 (1), 5 30.Comon, P. (1994). Independent component analysis, new concept?. Signal Processing,36 (3), 287314.Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood incompletedata via EM algorithm. Journal Royal Statistical Society, Series B, 39 (1),138.Dumais, S., Furnas, G., Landauer, T., Deerwester, S., Deerwester, S., et al. (1995). Latentsemantic indexing. Proceedings Text Retrieval Conference.Fisher, R. A. (1936). use multiple measurements taxonomic problems. AnnalsEugenics, 7 (2), 179188.Gabrilovich, E., & Markovitch, S. (2009). Wikipedia-based semantic interpretationnatural language processing. Journal Articial Intelligence Research (JAIR), 34 (2),443.Golub, G. H., & Van Loan, C. F. (2012). Matrix Computations, Vol. 3. JHU Press.Gretarsson, B., Odonovan, J., Bostandjiev, S., Hollerer, T., Asuncion, A., Newman, D., &Smyth, P. (2012). TopicNets: Visual analysis large text corpora topic modeling.ACM Transactions Intelligent Systems Technology (TIST), 3 (2), 23.Hein, M., Audibert, J.-y., & Luxburg, U. V. (2007). Graph Laplacians ConvergenceRandom Neighborhood Graphs. Journal Machine Learning Research, pp.13251368.Hinton, G. E., & Roweis, S. T. (2002). Stochastic neighbor embedding. AdvancesNeural Information Processing Systems (NIPS), pp. 833840.Hofmann, T. (1999). Probabilistic latent semantic indexing. Proceedings International ACM SIGIR Conference Research Development InformationRetrieval (SIGIR), pp. 5057.Hu, Y., Boyd-Graber, J., Satino, B., & Smith, A. (2014). Interactive topic modeling.Machine Learning, 95 (3), 423469.Huh, S., & Fienberg, S. E. (2012). Discriminative topic modeling based manifold learning.ACM Transactions Knowledge Discovery Data (TKDD), 5 (4), 20.1131fiLe & LauwIwata, T., Saito, K., Ueda, N., Stromsten, S., Griths, T. L., & Tenenbaum, J. B. (2007).Parametric embedding class visualization. Neural Computation, 19 (9), 25362556.Iwata, T., Yamada, T., & Ueda, N. (2008). Probabilistic latent semantic visualization: topicmodel visualizing documents. Proceedings ACM SIGKDD InternationalConference Knowledge Discovery Data Mining (KDD), pp. 363371.Jebara, T., Wang, J., & Chang, S.-F. (2009). Graph construction b-matching semisupervised learning. Proceedings 26th Annual International ConferenceMachine Learning, pp. 441448. ACM.Jollie, I. (2005). Principal Component Analysis. Wiley Online Library.Kim, M., & Torre, F. (2010). Local minima embedding. Proceedings InternationalConference Machine Learning (ICML), pp. 527534.Kohonen, T. (1990). self-organizing map. Proceedings IEEE, 78 (9), 14641480.Kruskal, J. B. (1964). Multidimensional scaling optimizing goodness nonmetrichypothesis. Psychometrika, 29 (1), 127.Laerty, J. D., & Wasserman, L. (2007). Statistical Analysis Semi-Supervised Regression.Advances Neural Information Processing Systems (NIPS), pp. 801808.Le, T., & Lauw, H. W. (2014a). Semantic visualization spherical representation.Proceedings ACM SIGKDD International Conference Knowledge DiscoveryData Mining, pp. 10071016. ACM.Le, T. M., & Lauw, H. W. (2014b). Manifold learning jointly modeling topicvisualization. Proceedings AAAI Conference Articial Intelligence.Liu, D. C., & Nocedal, J. (1989). limited memory BFGS method large scaleoptimization. Mathematical Programming, 45, 503528.Manning, C. D., Raghavan, P., Schutze, H., et al. (2008). Introduction InformationRetrieval, Vol. 1. Cambridge University Press Cambridge.McCallum, A., Wang, X., & Corrada-Emmanuel, A. (2007). Topic role discoverysocial networks experiments enron academic email.. Journal ArticialIntelligence Research (JAIR), 30, 249272.Millar, J. R., Peterson, G. L., & Mendenhall, M. J. (2009). Document ClusteringVisualization Latent Dirichlet Allocation Self-Organizing Maps. FLAIRSConference, Vol. 21, pp. 6974.Newman, D., Karimi, S., & Cavedon, L. (2009). External evaluation topic models.Australasian Document Computing Symposium (ADCS).Newman, D., Lau, J. H., Grieser, K., & Baldwin, T. (2010). Automatic evaluation topiccoherence. Human Language Technologies: 2010 Annual ConferenceNorth American Chapter Association Computational Linguistics, pp. 100108.Park, J., & Sandberg, I. W. (1991). Universal approximation using radial-basis-functionnetworks. Neural Computation, 3 (2), 246257.1132fiSemantic Visualization Neighborhood Graph RegularizationPonzetto, S. P., & Strube, M. (2007). Knowledge derived Wikipedia computingsemantic relatedness.. Journal Articial Intelligence Research (JAIR), 30, 181212.Reisinger, J., Waters, A., Silverthorn, B., & Mooney, R. J. (2010). Spherical topic models.Proceedings International Conference Machine Learning (ICML), pp.903910.Roweis, S. T., & Saul, L. K. (2000). Nonlinear dimensionality reduction locally linearembedding. Science, 290 (5500), 23232326.Shaw, B., & Jebara, T. (2007). Minimum volume embedding. Proceedings International Conference Articial Intelligence Statistics (AISTATS), pp. 460467.Shaw, B., & Jebara, T. (2009). Structure preserving embedding. ProceedingsInternational Conference Machine Learning (ICML), pp. 937944. ACM.Tenenbaum, J. B., De Silva, V., & Langford, J. C. (2000). global geometric frameworknonlinear dimensionality reduction. Science, 290 (5500), 23192323.Ting, D., Huang, L., & Jordan, M. I. (2010). Analysis Convergence Graph Laplacians. Proceedings International Conference Machine Learning (ICML).Turney, P. D., Pantel, P., et al. (2010). frequency meaning: Vector space modelssemantics. Journal Articial Intelligence Research (JAIR), 37 (1), 141188.Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. Journal MachineLearning Research (JMLR), 9 (2579-2605), 85.Wei, F., Liu, S., Song, Y., Pan, S., Zhou, M. X., Qian, W., Shi, L., Tan, L., & Zhang,Q. (2010). Tiara: visual exploratory text analytic system. ProceedingsACM SIGKDD International Conference Knowledge Discovery Data Mining(KDD), pp. 153162.Wu, H., Bu, J., Chen, C., Zhu, J., Zhang, L., Liu, H., Wang, C., & Cai, D. (2012). Locallydiscriminative topic modeling. Pattern Recognition, 45 (1), 617625.Zemel, R. S., & Carreira-Perpinan, M. A. (2004). Proximity graphs clusteringmanifold learning. Advances Neural Information Processing Systems (NIPS),pp. 225232.Zhou, D., Bousquet, O., Lal, T. N., Weston, J., & Scholkopf, B. (2004). Learning localglobal consistency. Advances Neural Information Processing Systems (NIPS),16 (16).Zhu, X., Ghahramani, Z., Laerty, J., et al. (2003). Semi-supervised learning using Gaussianelds harmonic functions. Proceedings International ConferenceMachine Learning (ICML), Vol. 3, pp. 912919.1133fiJournal Artificial Intelligence Research 55 (2016) 835-887Submitted 10/2015; published 04/2016Parallel Model-Based Diagnosis Multi-Core ComputersDietmar JannachThomas Schmitzdietmar.jannach@tu-dortmund.dethomas.schmitz@tu-dortmund.deTU Dortmund, GermanyKostyantyn Shchekotykhinkostyantyn.shchekotykhin@aau.atAlpen-Adria University Klagenfurt, AustriaAbstractModel-Based Diagnosis (MBD) principled domain-independent way analyzing system examination behaving expected. Given abstractdescription (model) systems components behavior functioning normally, MBD techniques rely observations actual system behavior reasonpossible causes discrepancies expected observed behavior. Due generality, MBD successfully applied variety applicationdomains last decades.many application domains MBD, testing different hypotheses reasonsfailure computationally costly, e.g., complex simulations system behavior performed. work, therefore propose different schemesparallelizing diagnostic reasoning process order better exploit capabilitiesmodern multi-core computers. propose systematically evaluate parallelizationschemes Reiters hitting set algorithm finding leading minimal diagnoses using two different conflict detection techniques. Furthermore, perform initialexperiments basic depth-first search strategy assess potential parallelizationsearching one single diagnosis. Finally, test effects parallelizing directencodings diagnosis problem constraint solver.1. IntroductionModel-Based Diagnosis (MBD) subfield Artificial Intelligence concernedautomated determination possible causes system behaving expected.early days MBD, diagnosed systems typically hardware artifacts likeelectronic circuits. contrast earlier heuristic diagnosis approaches connectedsymptoms possible causes, e.g., expert rules (Buchanan & Shortliffe, 1984),MBD techniques rely abstract explicit representation (model) examinedsystem. models contain information systems structure, i.e., listcomponents connected, well information behaviorcomponents functioning correctly. model available, expected behavior (outputs) system given inputs thus calculated. diagnosis problemarises whenever expected behavior conflicts observed system behavior. MBDtechniques core construct test hypotheses faultiness individualcomponents system. Finally, diagnosis considered subset componentsthat, assumed faulty, explain observed behavior system.Reiter (1987) suggests formal logical characterization diagnosis problemfirst principles proposed breadth-first tree construction algorithm determinec2016AI Access Foundation. rights reserved.fiJannach, Schmitz, & Shchekotykhindiagnoses given problem. Due generality used knowledge-representationlanguage suggested algorithms computation diagnoses, MBDlater applied variety application problems hardware. applicationfields MBD, example, include diagnosis knowledge bases ontologies, processspecifications, feature models, user interface specifications user preference statements,various types software artifacts including functional logic programs wellVHDL, Java spreadsheet programs (Felfernig, Friedrich, Jannach, & Stumptner, 2004;Mateis, Stumptner, Wieland, & Wotawa, 2000; Jannach & Schmitz, 2014; Wotawa, 2001b;Felfernig, Friedrich, Isak, Shchekotykhin, Teppan, & Jannach, 2009; Console, Friedrich,& Dupre, 1993; Friedrich & Shchekotykhin, 2005; Stumptner & Wotawa, 1999; Friedrich,Stumptner, & Wotawa, 1999; White, Benavides, Schmidt, Trinidad, Dougherty, & Cortes,2010; Friedrich, Fugini, Mussi, Pernici, & Tagni, 2010).several application fields, search diagnoses requires repeated computations based modified versions original model test different hypothesesfaultiness individual components. several works original problemconverted Constraint Satisfaction Problem (CSP) number relaxed versionsoriginal CSP solved construct new node search tree (Felferniget al., 2004; Jannach & Schmitz, 2014; White et al., 2010). Depending application domain, computation CSP solutions check consistency can, however,computationally intensive actually represents costly operationconstruction search tree. Similar problems arise underlying reasoningtechniques, e.g., ontology debugging (Friedrich & Shchekotykhin, 2005), used.Current MBD algorithms sequential nature generate one node time.Therefore, exploit capabilities todays multi-core computer processors,nowadays found even mobile devices. paper, propose new schemesparallelize diagnostic reasoning process better exploit available computingresources modern computer hardware. particular, work comprises followingalgorithmic contributions insights based experimental evaluations:propose two parallel versions Reiters (1987) sound complete Hitting Set(HS) algorithm speed process finding diagnoses, commonproblem setting above-described MBD applications. approachesconsidered window-based parallelization schemes, means limited number search nodes processed parallel point time.evaluate two different conflict detection techniques multi-core setting,goal find leading diagnoses. set experiments, multiple conflicts computed construction tree node using novelMergeXplain method (MXP) (Shchekotykhin, Jannach, & Schmitz, 2015)processing time therefore implicitly allocated conflict generation.demonstrate speedups also achieved parallelization scenarios search one single diagnosis, e.g., using basic paralleldepth-first strategy.measure improvements achieved parallel constraint solvingusing direct CSP-based encoding diagnosis problem. experiment836fiParallel Model-Based Diagnosis Multi-Core Computersillustrates parallelization underlying solvers, particular usingdirect encoding, advantageous.evaluate proposed parallelization schemes extensive set experiments. following problem settings analyzed.(i) Standard benchmark problems diagnosis research community;(ii) Mutated CSPs Constraint Programming competition domainCSP-based spreadsheet debugging (Jannach & Schmitz, 2014);(iii) Faulty OWL ontologies used evaluation MBD-based debugging techniquesexpressive ontologies (Shchekotykhin, Friedrich, Fleiss, & Rodler, 2012);(iv) Synthetically generated problems allow us vary characteristicsunderlying diagnosis problem.results show using parallelization techniques help achieve substantialspeedups diagnosis process (a) across variety application scenarios, (b) withoutexploiting specific knowledge structure underlying diagnosis problem,(c) across different problem encodings, (d) also application problems like ontologydebugging cannot efficiently encoded SAT problems.outline paper follows. next section, define main conceptsMBD introduce algorithm used compute diagnoses. Section 3, presentsystematically evaluate parallelization schemes Reiters HS-tree methodgoal find minimal diagnoses. Section 4, report results evaluationsimplicitly allocate processing time conflict generation using MXPconflict detection. Section 5 assess potential gains comparably simplerandomized depth-first strategy hybrid technique problem finding onesingle diagnosis. results experiments direct CSP encoding reportedSection 6. Section 7 discuss previous works. paper ends summaryoutlook Section 8.2. Reiters Diagnosis Frameworksection summarizes Reiters (1987) diagnosis framework use basiswork.2.1 DefinitionsReiter (1987) formally characterized Model-Based Diagnosis using first-order logic.main definitions summarized follows.Definition 2.1. (Diagnosable System) diagnosable system described pair (SD,Comps) SD system description (a set logical sentences) Comps representssystems components (a finite set constants).connections components normal behavior componentsdescribed terms logical sentences. normal behavior system components837fiJannach, Schmitz, & Shchekotykhinusually described SD help distinguished negated unary predicate ab(.),meaning abnormal.diagnosis problem arises observation P Obs systems input-outputbehavior (again expressed first-order sentences) deviates expected system behavior. diagnosis corresponds subset systems componentsassume behave abnormally (be faulty) assumptions must consistentobservations. words, malfunctioning componentspossible reason observations.Definition 2.2. (Diagnosis) Given diagnosis problem (SD, Comps, Obs), diagnosissubset minimal set Comps SD Obs tab(c)|c P u abpcq|c PCompszu consistent.According Definition 2.2, interested minimal diagnoses, i.e., diagnosescontain superfluous elements thus supersets diagnoses. Whenever use term diagnosis remainder paper, therefore mean minimaldiagnosis. Whenever refer non-minimal diagnoses, explicitly mention fact.Finding diagnoses theory done simply trying possible subsetsComps checking consistency observations. Reiter (1987), however,proposes efficient procedure based concept conflicts.Definition 2.3. (Conflict) conflict (SD, Comps, Obs) set tc1 , ..., ck u CompsSD Obs abpc1 q, ..., abpck qu inconsistent.conflict corresponds subset components which, assumed behave normally,consistent observations. conflict c considered minimal,proper subset c exists also conflict.2.2 Hitting Set AlgorithmReiter (1987) discusses relationship conflicts diagnoses claimsTheorem 4.4 set diagnoses collection (minimal) conflicts Fequivalent set H minimal hitting sets 1 F .determine minimal hitting sets therefore diagnoses, Reiter proposesbreadth-first search procedure construction hitting set tree (HS-tree), whoseconstruction guided conflicts. logic-based definition MBD problem(Reiter, 1987), conflicts computed calls Theorem Prover (TP). TPcomponent considered black box assumptions madeconflicts determined. Depending application scenario problem encoding,one can, however, also use specific algorithms like QuickXplain (Junker, 2004), Progression (Marques-Silva, Janota, & Belov, 2013) MergeXplain (Shchekotykhin et al., 2015),guarantee computed conflict sets minimal.main principle HS-tree algorithm create search tree nodeeither labeled conflict represents diagnosis. latter case nodeexpanded. Otherwise, child node generated element nodes1. Given collection C subsets finite set S, hitting set C subset containsleast one element subset C. corresponds set cover problem.838fiParallel Model-Based Diagnosis Multi-Core Computersconflict outgoing edge labeled one component nodes conflict.subsequent expansions node components used label edgespath root tree current node assumed faulty. newlygenerated child node either diagnosis labeled conflictcontain component already assumed faulty stage. conflictfound node, path labels represent diagnosis sense Definition 2.2.2.2.1 Examplefollowing example show HS-tree algorithm QuickXplain(QXP) conflict detection technique combined locate fault specificationCSP. CSP instance defined tuple pV, D, Cq, V tv1 , . . . , vn uset variables, tD1 , . . . , Dn u set domains variables V ,C tC1 , . . . , Ck u set constraints. assignment subset X V setpairs txv1 , d1 y, . . . , xvk , dm yu vi P X variable dj P Di valuedomain variable. assignment comprises exactly one variable-value pairvariable X. constraint Ci P C defined list variables S, called scope,forbids allows certain simultaneous assignments variables scope.assignment satisfies constraint Ci comprises assignment allowed Ci .assignment solution satisfies constraints C.Consider CSP instance variables V ta, b, cu variabledomain t1, 2, 3u following set constraints defined:C1 : b,C2 : b c,C3 : c a,C4 : b cObviously, solution exists diagnosis problem consists finding subsetsconstraints whose definition faulty. engineer modeled CSP could,example, made mistake writing C2, b c.Eventually, C4 added later correct problem, engineer forgot removeC2. Given faulty definition I, two minimal conflicts exist, namely ttC1, C2, C3u,tC2, C4uu, determined help QXP. Given two conflicts,HS-tree algorithm finally determine three minimal hitting sets ttC2u, tC1, C4u,tC3, C4uu, diagnoses problem instance. set diagnoses also containstrue cause error, definition C2.Let us review detail HS-tree/QXP combination works example problem. illustrate tree construction Figure 1. logic-based definitionReiter, HS-tree algorithm starts check observations Obs consistentsystem description SD components Comps. application settingcorresponds check exists solution CSP instance.2 Sincecase, QXP-call made, returns conflict tC1, C2, C3u, usedlabel root node ( 1 ) tree. element conflict, child nodecreated conflict element used path label. tree node, consistency SD, Obs, Comps tested; time, however, elements appear2. Comps constraints tC1...C4u SD corresponds semantics/logic constraintsworking correctly, e.g., ABpC1q _ pa bq. Obs empty example could partial valueassignment (test case) another scenario.839fiJannach, Schmitz, & Shchekotykhin1{C1,C2, C3}C12C2C33{C2,C4}4C2{C2,C4}C4C4C25Figure 1: Example HS-tree construction.labels path root node current node considered abnormal.CSP diagnosis setting, means check solution modifiedversion original CSP remove constraints appear labelspath root current node.node 2 , C1 correspondingly considered abnormal. removing C1CSP is, however, sufficient solution exists relaxed problem, another callQXP made, returns conflict tC2, C4u. tC1u therefore diagnosisnew conflict used label node 2 . algorithm proceeds breadth-firststyle tests assuming tC2u tC3u individually faulty consistentobservations, case means solution relaxed CSP exists. Since tC2udiagnosis least one solution exists C2 removed CSP definitionnode marked 3 expanded. node 3 , corresponddiagnosis, already known conflict tC2, C4u reused overlapnodes path label call P (QXP) required. last tree level,nodes 4 5 expanded (closed marked 7) tC2ualready identified diagnosis previous level resulting diagnoses wouldsupersets tC2u. Finally, sets tC1, C4u tC3, C4u identified additionaldiagnoses.2.2.2 DiscussionSoundness Completeness According Reiter (1987), breadth-first construction scheme node closing rule ensure minimal diagnoses computed.end HS-tree construction process, set edge labels pathroot tree node marked 3 corresponds diagnosis.3Greiner, Smith, Wilkerson (1989), later on, identified potential problem Reitersalgorithm cases conflicts returned P guaranteed minimal.extension algorithm based HS-DAG (directed acyclic graph) structureproposed solve problem.context work, use methods return conflicts guaranteed minimal. example, according Theorem 1 work Junker (2004),given set formulas sound complete consistency checker, QXP always returns3. Reiter (1987) states Theorem 4.8 given set conflict sets F , HS-tree algorithm outputspruned tree set tHpnq|n node labeled 3u corresponds set Hminimal hitting sets F Hpnq set arc labels path node n root.840fiParallel Model-Based Diagnosis Multi-Core Computerseither minimal conflict conflict. minimality guarantee turn meanscombination HS-tree algorithm QXP sound complete, i.e., returnedsolutions actually (minimal) diagnoses diagnosis given set conflictsmissed. holds computing multiple conflicts time MXP(Shchekotykhin et al., 2015).simplify presentation parallelization approaches, therefore relyReiters original HS-tree formulation; extension deal HS-DAG structure(Greiner et al., 1989) possible.On-Demand Conflict Generation Complexity many above-mentionedapplications MBD practical problems, conflicts computed on-demand,i.e., tree construction, cannot generally assume set minimalconflicts given advance. Depending problem setting, finding conflictstherefore computationally intensive part diagnosis process.Generally, finding hitting sets collection sets known NP-hard problem(Garey & Johnson, 1979). Moreover, deciding additional diagnosis exists conflictscomputed demand NP-complete even propositional Horn theories (Eiter &Gottlob, 1995). Therefore, number heuristics-based, approximate thus incomplete,well problem-specific diagnosis algorithms proposed years.discuss approaches later sections. next section, we, however, focus(worst-case) application scenarios goal find minimal diagnoses givenproblem, i.e., focus complete algorithms.Consider, example, problem debugging program specifications (e.g., constraintprograms, knowledge bases, ontologies, spreadsheets) MBD techniques mentionedabove. application domains, typically sufficient find one minimal diagnosis. work Jannach Schmitz (2014), example, spreadsheet developerpresented ranked list sets formulas (diagnoses) represent possiblereasons certain test case failed. developer either inspectindividually provide additional information (e.g., test cases) narrow setcandidates. one diagnosis computed presented, developer wouldguarantee true cause problem, lead limited acceptancediagnosis tool.3. Parallel HS-Tree Constructionsection present two sound complete parallelization strategies ReitersHS-tree method determine minimal diagnoses.3.1 Non-recursive HS-Tree Algorithmuse non-recursive version Reiters sequential HS-tree algorithm basisimplementation two parallelization strategies. Algorithm 1 shows main loopbreadth-first procedure, uses list open nodes expanded central datastructure.algorithm takes diagnosis problem (DP) instance input returns setdiagnoses. DP given tuple (SD, Comps, Obs), SD system841fiJannach, Schmitz, & ShchekotykhinAlgorithm 1: diagnose: Main algorithm loop.Input: diagnosis problem (SD, Comps, Obs)Result: set diagnoses123456789H; paths H; conflicts H;nodesToExpand = xgenerateRootNode(SD, Comps, Obs)y;nodesToExpand xnewNodes = x y;node = head(nodesToExpand) ;foreach c P node.conflictgenerateNode(node, c, , paths, conflicts, newNodes);nodesToExpand = tail(nodesToExpand) newNodes;return ;Algorithm 2: generateNode: Node generation logic.Input: existingNode expand, conflict element c P Comps,sets , paths, conflicts, newNodes12345678910111213newPathLabel = existingNode.pathLabel {c};pE l P : l newPathLabelq ^ checkAndAddPathppaths, newPathLabelqnode = new Node(newPathLabel);P conflicts : X newPathLabel Hnode.conflict = S;elsenewConflicts = checkConsistency(SD, Comps, Obs, node.pathLabel);node.conflict = head(newConflicts);node.conflict HnewNodes = newNodes xnodey;conflicts = conflicts newConflicts;else{node.pathLabel};description, Comps set components potentially faulty Obs setobservations. method generateRootNode creates initial node, labeledconflict empty path label. Within loop, first element firstin-first-out (FIFO) list open nodes nodesToExpand taken current element.function generateNode (Algorithm 2) called element nodes conflictadds new leaf nodes, still explored, global list. newnodes appended () remaining list open nodes main loop,842fiParallel Model-Based Diagnosis Multi-Core Computerscontinues elements remain expansion.4 Algorithm 2 (generateNode)implements node generation logic, includes Reiters proposals conflict re-use,tree pruning, management lists known conflicts, paths diagnoses.method determines path label new node checks new path labelsuperset already found diagnosis.Algorithm 3: checkAndAddPath: Adding new path label redundancycheck.Input: previously explored paths, newPathLabel exploredResult: Boolean stating newPathLabel added paths3E l P paths : l newPathLabelpaths = paths newPathLabel;return true;4return false;12function checkAndAddPath (Algorithm 3) used check nodealready explored elsewhere tree. function returns true new path labelsuccessfully inserted list known paths. Otherwise, list known pathsremains unchanged node closed.new nodes, either existing conflict reused new one created callconsistency checker (Theorem Prover), tests new node diagnosisreturns set minimal conflicts otherwise. Depending outcome, new nodeadded list nodesToExpand diagnosis stored. Note Algorithm 2return value instead modifies sets , paths, conflicts, newNodes,passed parameters.3.2 Level-Wise Parallelizationfirst parallelization scheme examines nodes one tree level parallel proceedsnext level elements level processed. example shownFigure 1, would mean computations (consistency checks theorem provercalls) required three first-level nodes labeled tC1u, tC2u, tC3u donethree parallel threads. nodes next level explored threadsprevious level finished.Using Level-Wise Parallelization (LWP) scheme, breadth-first character maintained. parallelization computations generally feasible consistencychecks node done independently done nodeslevel. Synchronization required make sure thread starts exploringpath already examination another thread.Algorithm 4 shows sequential Algorithm 1 adapted supportparallelization approach. Again, maintain list open nodes expanded.difference run expansion nodes parallel collect4. limitation regarding search depth number diagnoses find easily integratedscheme.843fiJannach, Schmitz, & ShchekotykhinAlgorithm 4: diagnoseLW: Level-Wise Parallelization.Input: diagnosis problem (SD, Comps, Obs)Result: set diagnoses12345678910H; conf licts H; paths = H;nodesToExpand = xgenerateRootNode(SD, Comps, Obs)y;nodesToExpand xnewNodes = x y;foreach node P nodesToExpandforeach c P node.conflict// computations parallelthreads.execute(generateNode(node, c, , paths, conflicts, newNodes));threads.await();nodesToExpand = newNodes;// Wait current level complete// Prepare next levelreturn ;nodes next level variable newNodes. current level finished,overwrite list nodesToExpand list containing nodes next level.Java-like API calls used pseudo-code Algorithm 4 interpretedfollows. statement threads.execute() takes function parameter schedulesexecution pool threads given size. thread pool of, e.g., size 2,generation first two nodes would done parallel next ones wouldqueued one threads finished. mechanism, ensurenumber threads executed parallel less equal number hardwarethreads CPUs.statement threads.await() used synchronization blocks executionsubsequent code scheduled threads finished. guaranteepath explored twice, make sure two threads parallel add nodepath label list known paths. achieved declaringfunction checkAndAddPath critical section (Dijkstra, 1968), meanstwo threads execute function parallel. Furthermore, make accessglobal data structures (e.g., already known conflicts diagnoses) thread-safe,i.e., ensure two threads simultanuously manipulate them.53.3 Full ParallelizationLWP, situations computation conflict specific nodetakes particularly long. This, however, means even nodes currentlevel finished many threads idle, expansion HS-tree cannot proceedlevel completed. Algorithm 5 shows proposed Full Parallelization (FP)algorithm variant, immediately schedules every expandable node executionthereby avoids potential CPU idle times end level.5. Controlling concurrency aspects comparably simple modern programming languages like Java,e.g., using synchronized keyword.844fiParallel Model-Based Diagnosis Multi-Core ComputersAlgorithm 5: diagnoseFP: Full Parallelization.Input: diagnosis problem (SD, Comps, Obs)Result: set diagnoses123456789101112H; paths H; conflicts H;nodesToExpand = xgenerateRootNode(SD, Comps, Obs)y;size = 1; lastSize = 0;psizelastSizeq _ pthreads.activeThreads 0q1 size lastSizenode = nodesToExpand.get[lastSize + i];foreach c P node.conflictthreads.execute(generateNodeFP(node, c, , paths, conflicts,nodesToExpand));lastSize = size;wait();size = nodesToExpand.length();return ;main loop algorithm slightly different basically monitors listnodes expand. Whenever new entries list observed, i.e., last observedlist size different current one, retrieves recently added elements addsthread queue execution. algorithm returns diagnoses newelements added since last check threads active.6FP, search necessarily follow breadth-first strategy anymorenon-minimal diagnoses found process. Therefore, whenever find newdiagnosis d, check set known diagnoses contains supersetsremove .updated generateNode method listed Algorithm 6. updating shareddata structures (nodesToExpand, conflicts, ), make sure threadsinterfere other. mutual exclusive section marked synchronizedkeyword.compared LWP, FP wait end level specificnode takes particularly long generate. hand, FP needs synchronizationthreads, cases last nodes level finishedtime, LWP could also advantageous. evaluate aspect Section 3.5.3.4 Properties AlgorithmsAlgorithm 1 together Algorithms 2 3 corresponds implementationHS-tree algorithm (Reiter, 1987). Algorithm 1 implements breadth-first search strategypoint (1) Reiters HS-tree algorithm since nodes stored list nodesToExpand6. functions wait() notify() implement semantics pausing thread awaking pausedthread Java programming language used avoid active waiting loops.845fiJannach, Schmitz, & ShchekotykhinAlgorithm 6: generateNodeFP: Extended node generation logic.Input: existingNode expand, c P Comps,sets , paths, conflicts, nodesToExpand1234567891011121314151617newPathLabel = existingNode.pathLabel {c};pE l P : l newPathLabelq ^ checkAndAddPathppaths, newPathLabelqnode = new Node(newPathLabel);P conflicts : X newPathLabel Hnode.conflict = S;elsenewConflicts = checkConsistency(SD, Comps, Obs, node.pathLabel);node.conflict = head(newConflicts);synchronizednode.conflict HnodesToExpand = nodesToExpand xnodey;conflicts = conflicts newConflicts;else E P : newPathLabel{node.pathLabel};P : newPathLabelz d;notify();processed iteratively first-in-first-out order (see lines 5 8). Algorithm 2 firstchecks pruning rules (i) (ii) Reiter applied line 2. rules statenode pruned (i) exists diagnosis (ii) set labelscorresponding path tree subset set labelspath node. Pruning rule (ii) implemented Algorithm 3. Pruning rule (iii)Reiters algorithm necessary since settings TP -call guarantees returnminimal conflicts.Finally, point (2) Reiters HS-tree algorithm description implemented lines4-8 Algorithm 2. Here, algorithm checks conflict reusednode label. case reuse possible, algorithm calls theorem prover TP findanother minimal conflict. conflict found, node added list open nodesnodesToExpand . Otherwise, set node path labels added set diagnoses.corresponds situation Reiters algorithm would mark nodeHS-tree 3 symbol. Note label nodes 7 done Reitersalgorithms since simply store nodes expansion list.Overall, conclude HS-tree algorithm implementation (Algorithm 13) properties Reiters original HS-tree algorithm. Namely, hitting setreturned algorithm minimal (soundness) existing minimal hitting setsfound (completeness).846fiParallel Model-Based Diagnosis Multi-Core Computers3.4.1 Level-Wise Parallelization (LWP)Theorem 3.1. Level-Wise Parallelization sound complete.Proof. proof based fact LWP uses expansion pruningtechniques sequential algorithm (Algorithms 2 3). main loop line 3 appliesprocedure original algorithm difference executionsAlgorithm 2 done parallel level tree. Therefore, differencesequential algorithm LWP lies order nodes one levellabeled generated.Let us assume two nodes n1 n2 tree sequentialHS-tree algorithm process n1 n2 . Assuming neither n1 n2 corresponddiagnoses, sequential Algorithm 1 would correspondingly first add child nodesn1 queue open nodes later append child nodes n2 .parallelize computations needed generation n1 n2 LWP,happen computations n1 need longer n2 . casechild nodes n2 placed queue first. order nodessubsequently processed is, however, irrelevant computation minimal hittingsets, since neither labeling pruning rules influenced it. fact,labeling node n depends whether minimal conflict set f existsHpnq X f H, nodes level. pruning rulesstate node n pruned exists node n1 labeled 3Hpn1 q Hpnq, i.e., supersets already found diagnoses pruned. n n1level, |Hpnq| |Hpn1 q|. Consequently, pruning rule appliedHpnq Hpn1 q. Therefore, order nodes, i.e., nodes pruned, irrelevantminimal hitting set lost. Consequently, LWP complete.Soundness algorithm follows fact LWP constructs hitting setsalways order increasing cardinality. Therefore, LWP always return minimal hitting sets even scenarios stop k diagnoses found,1 k N predefined constant N total number diagnosesproblem.3.4.2 Full Parallelization (FP)minimality hitting sets encountered search guaranteed FP,since algorithm schedules node processing immediately generation (line 8Algorithm 5). special treatment generateNodeFP function ensuressupersets already found hitting sets added supersets newly foundhitting set removed thread-safe manner (lines 13 16 Algorithm 6). Duechange generateNodeFP, analysis soundness completenessdone two distinct cases.Theorem 3.2. Full Parallelization sound complete, applied find diagnosescardinality.Proof. FP stops either (i) hitting set exists, i.e., leaf nodes treelabeled either 3 7, (ii) predefined cardinality (tree-depth) reached.latter case, every leaf node tree labeled either 3, 7, minimal conflict847fiJannach, Schmitz, & Shchekotykhinset. Case (ii) reduced (i) removing branches tree labeledminimal conflict. branches irrelevant since contributeminimal hitting sets higher cardinality. Therefore, without loss generality, limitdiscussion case (i).According definition generateNodeFP, tree built using pruningrule done sequential HS-tree algorithm. consequence, tree generatedFP must comprise least nodes tree generated sequential HStree procedure. Therefore, according Theorem 4.8 work Reiter (1987)tree generated FP must comprise set leaf nodes labeled 3set tHpnq|n node labeled 3u corresponds set H minimal hittingsets. Moreover, result returned FP comprises minimal hitting sets,generateNodeFP removes hitting sets H supersets hitting sets.Consequently, FP sound complete, applied find diagnoses.Theorem 3.3. Full Parallelization cannot guarantee completeness soundnessapplied find first k diagnoses, i.e. 1 k N , N total numberdiagnoses problem.Proof. proof done constructing example FP returns leastone non-minimal hitting set set , thus violating Definition 2.2. instance,situation might occur FP applied find one single diagnosis example problempresented Section 2.2.1. Let us assume generation node correspondingpath C2 delayed, e.g., operating system scheduled another threadexecution first, node 4 correspondingly generated first. case, algorithmwould return non-minimal hitting set tC1, C2u diagnosis.Note elements set returned FP case turneddiagnoses applying minimization algorithm like Inv-QuickXplain (Shchekotykhin,Friedrich, Rodler, & Fleiss, 2014), algorithm adopts principles QuickXplainapplies divide-and-conquer strategy find one minimal diagnosis given setinconsistent constraints.Given hitting set H diagnosis problem, algorithm capable computingminimal hitting set H 1 H requiring Op|H 1 |`|H 1 | logp|H|{|H 1 |qqq calls theoremprover TP. first part, |H 1 |, reflects computational costs determining whetherH 1 minimal. second part represents number subproblems mustconsidered divide-and-conquer algorithm order find minimal hitting set H 1 .3.5 Evaluationdetermine performance improvements achieved various formsparallelization proposed paper, conducted series experiments diagnosisproblems number different application domains. Specifically, used electroniccircuit benchmarks DX Competition 2011 Synthetic Track, faulty descriptionsConstraint Satisfaction Problems (CSPs), well problems domain ontologydebugging. addition, ran experiments synthetically created diagnosis problemsanalyze impact varying different problem characteristics. diagnosis algorithms848fiParallel Model-Based Diagnosis Multi-Core Computersevaluated paper implemented Java unless noted otherwise. Generally,use wall clock times performance measure.main part paper, focus results DX Competitionproblems widely used benchmark. results problemsetups presented discussed appendix paper. cases,results DX Competition problems follow similar trend achievedexperiments.section compare HS-tree parallelization schemes LWP FPsequential version algorithm, goal find diagnoses.3.5.1 Dataset Procedureset experiments, selected first five systems DX Competition 2011Synthetic Track (see Table 1) (Kurtoglu & Feldman, 2011). system, competition specifies 20 scenarios injected faults resulting different faulty output values.used system description given input output values diagnosisprocess. additional information injected faults course ignored.problems converted Constraint Satisfaction Problems. experiments usedChoco (Prudhomme, Fages, & Lorca, 2015) constraint solver QXP conflictdetection, returns one minimal conflict called node construction.computation times required conflict identification strongly dependorder possibly faulty constraints, shuffled constraints test repeatedtests 100 times. report wall clock times actual diagnosis task; timesrequired input output independent HS-tree construction schemerelevant benchmarks. parallel approaches, used thread pool sizefour.7Table 1 shows characteristics systems terms number constraints(#C) problem variables (#V).8 numbers injected faults (#F)numbers calculated diagnoses (#D) vary strongly different scenariossystem. columns show ranges values scenarios.columns #D |D| indicate average number diagnoses average cardinality.seen, search tree diagnosis become extremely broad6,944 diagnoses average diagnosis size 3.38 system c432.3.5.2 ResultsTable 2 shows averaged results searching minimal diagnoses. first listrunning times milliseconds sequential version (Seq.) improvementsLWP FP terms speedup efficiency respect sequential version.Speedup Sp computed Sp T1 {Tp , T1 wall time using 1 thread (thesequential algorithm) Tp wall time p parallel threads used. speedup7. four hardware threads reasonable assumption standard desktop computers also mobiledevices. hardware used evaluation chapter laptop Intel i7-3632QMCPU, 16GB RAM, running Windows 8 also four cores hyperthreading. resultsevaluation server hardware 12 cores reported later Section.8. systems marked *, search depth limited actual number faults ensuresequential algorithm terminates within reasonable time frame.849fiJannach, Schmitz, & ShchekotykhinSystem7418274L8574283*74181*c432*#C21353867162#V28444579196#F4-51-32-43-62-5#D30 - 3001 - 215180 - 4,99110 - 3,8281 - 6,944#D139.066.41,232.7877.81,069.3|D|4.663.134.424.533.38Table 1: Characteristics selected DXC benchmarks.2 would therefore mean needed computation times halved; speedup 4,theoretical optimum using 4 threads, means time reducedone quarter. efficiency Ep defined Sp {p compares speeduptheoretical optimum. fastest algorithm system highlighted bold.System7418274L8574283*74181*c432*Seq.(QXP)[ms]6520937121,69585,024LWP(QXP)S4E42.230.562.550.642.530.631.220.311.470.37FP(QXP)S4E42.28 0.572.77 0.692.66 0.673.19 0.803.75 0.94Table 2: Observed performance gains DXC benchmarks searching diagnoses.tests, parallelization approaches outperform sequential algorithm. Furthermore, differences sequential algorithm one parallel approaches statistically significant (p 0.05) 95 100 tested scenarios.systems, FP efficient LWP speedups range 2.28 3.75(i.e., reduction running times 70%). 59 100 scenariosdifferences LWP FP statistically significant. trendobserved efficiency FP higher complex problems. reasonproblems time needed node generation much larger absolutenumbers additional overhead times required thread synchronization.3.5.3 Adding Threadsuse cases diagnosis process done powerful server architecturesoften even CPU cores modern desktop computers. order assessextent 4 threads help speed diagnosis process, testeddifferent benchmarks server machine 12 CPU cores. test comparedFP 4, 8, 10, 12 threads sequential algorithm.results DXC benchmark problems shown Table 3. tested systemsdiagnosis process faster using 8 instead 4 threads substantial speedups5.20 could achieved compared sequential diagnosis, corresponds850fiParallel Model-Based Diagnosis Multi-Core Computersruntime reduction 81%. one system, utilization 10 threads ledadditional speedups. Using 12 threads fastest 3 5 tested systems.efficiency, however, degrades threads used, time neededsynchronization threads. Using threads hardware actually coresresult additional speedups tested systems. reasontime threads busy conflict detection, e.g., finding solutions CSPs,use almost 100% processing power assigned them.System7418274L857428374181*c432*Seq.(QXP)[ms]5818451,31413,84743,916S42.092.533.043.453.43E40.520.630.760.860.86S82.433.294.385.204.77FP(QXP)E8S10E100.30 2.52 0.250.41 3.35 0.340.55 4.42 0.440.65 5.11 0.510.60 5.00 0.50S122.543.384.505.194.74E120.210.280.370.430.39Table 3: Observed performance gains DXC benchmarks server 12 hardwarethreads.3.5.4 Additional Experimentsdetails additional experiments conducted compare proposed parallelization schemes sequential HS-Tree algorithm presented Section A.1appendix. results show significant speedups also achieved Constraint Satisfaction Problems (Section A.1.1) ontologies (Section A.1.2). appendixfurthermore contains analysis effects adding threads benchmarksCSPs ontologies (Section A.1.3) presents results simulation experimentsystematically varied different problem characteristics (Section A.1.4).3.5.5 DiscussionOverall, results evaluations show parallelization approaches help improve performance diagnosis process, tested scenarios approachesachieved speedups. cases FP faster LWP. However, dependingspecifics given problem setting, using LWP advantageous situations,e.g., time needed generate node small conflict generation time vary strongly. cases synchronization overhead neededFP higher cost waiting threads finish. tested ontologiesSection A.1.2, case four tested scenarios.Although FP average faster LWP significantly better sequentialHS-tree construction approach, tested scenarios efficiency still faroptimum 1. explained different effects. example, effectfalse sharing happen memory two threads allocated block(Bolosky & Scott, 1993). every access memory block synchronized althoughtwo threads really share memory. Another possible effect called cache851fiJannach, Schmitz, & Shchekotykhincontention (Chandra, Guo, Kim, & Solihin, 2005). threads work different computingcores share memory, cache misses occur often dependingproblem characteristics thus theoretical optimum cannot reached cases.4. Parallel HS-Tree Construction Multiple Conflicts Per Nodesequential parallel version HS-tree algorithm, TheoremProver TP call corresponds invocation QXP. Whenever new node HS-treecreated, QXP searches exactly one new conflict case none already knownconflicts reused. strategy advantage call TP immediatelyreturns one conflict determined. turn means parallelexecution threads immediately see new conflict shared data structurescan, best case, reuse constructing new nodes.disadvantage computing one conflict time QXP searchconflicts restarted invocation. recently proposed new conflict detectiontechnique called MergeXplain (MXP) (Shchekotykhin et al., 2015), capablecomputing multiple conflicts one call. general idea MXP continue searchidentification first conflict look additional conflicts remainingconstraints (or logical sentences) divide-and-conquer approach.combined sequential HS-tree algorithm, effect tree construction time initially spent conflict detection construction continuesnext node. exchange, chances conflict available reuse increasenext nodes. time, identification conflicts less timeintensive smaller sets constraints investigated due divide-and-conquerapproach MXP. experimental evaluation various benchmark problems showssubstantial performance improvements possible sequential HS-tree scenariogoal find leading diagnoses (Shchekotykhin et al., 2015).section, explore benefits using MXP parallel HS-tree construction schemes proposed previous section. using MXP combinationmultiple threads, implicit effect CPU processing power devoted conflict generation individual threads need time complete constructionnew node. contrast sequential version, threads continuework parallel.next section, briefly review MXP algorithm reportresults empirical evaluation benchmark datasets (Section 4.2).4.1 Background QuickXplain MergeXplainAlgorithm 7 shows QXP conflict detection technique Junker (2004) appliedproblem finding conflict diagnosis problem HS-tree construction.QXP operates two sets constraints9 modified recursive calls.background theory B comprises constraints considered anymorepart conflict current stage. beginning, set contains SD, Obs,9. use term constraints original formulation. QXP independentunderlying reasoning technique, elements sets could general logical sentences well.852fiParallel Model-Based Diagnosis Multi-Core ComputersAlgorithm 7: QuickXplain (QXP)Input: diagnosis problem (SD, Comps, Obs), set visitedNodes elementsOutput: set containing one minimal conflict CS C1 B SD Obs {ab(c)|c P visitedNodes}; C abpcq|c P CompszvisitedNodesu;2 isConsistent(B C) return conflict;3 else C H return H;4 return tc| abpcq P getConflictpB, B, Cqu;5678910function getConflict (B, D, C)H ^ isConsistent(B) return H;|C| 1 return C;Split C disjoint, non-empty sets C1 C2D2 getConflict (B C1 , C1 , C2 )D1 getConflict (B D2 , D2 , C1 )return D1 D2 ;set nodes path current node HS-tree (visited nodes).set C represents set constraints search conflict.conflict C empty, algorithm immediately returns. Otherwise getConflict called, corresponds Junkers QXP method minor differencegetConflict require strict partial order set constraints C.introduce variant QXP since cannot always assume prior fault informationavailable would allow us generate order.rough idea QXP relax input set faulty constraints C partitioningtwo sets C1 C2 . C1 conflict, algorithm continues partitioning C1next recursive call. Otherwise, i.e., last partitioning split conflicts Cconflicts left C1 , algorithm extracts conflict sets C1C2 . way, QXP finally identifies individual constraints inconsistentremaining consistent set constraints background theory.MXP builds ideas QXP computes multiple conflicts one call (ifexist). general procedure shown Algorithm 8. initial consistency checks,method findConflicts called, returns tuple xC 1 , y, C 1 setremaining consistent constraints set found conflicts. function recursivelysplits set C constraints two halves. parts individually checkedconsistency, allows us exclude larger consistent subsets C search process.Besides potentially identified conflicts, calls findConflicts also return two setsconstraints consistent (C11 C21 q. union two sets consistent,look conflict within C11 C11 (and background theory) style QXP.details found earlier work, also results in-depthexperimental analysis reported (Shchekotykhin et al., 2015).853fiJannach, Schmitz, & ShchekotykhinAlgorithm 8: MergeXplain (MXP)Input: diagnosis problem (SD, Comps, Obs), set visitedNodes elementsOutput: , set minimal conflicts1 B SD Obs {ab(c)|c P visitedNodes}; C abpcq|c P Compszu;2isConsistentpBq return solution;3 isConsistentpB Cq return H;4 x , findConflictspB, Cq5 return tc| abpcq P };67891011121314151617function findConflicts (B, C) returns tuple xC 1 ,isConsistent(B C) return xC, Hy;|C| 1 return xH, tCuy;Split C disjoint, non-empty sets C1 C2xC11 , 1 findConflictspB, C1 qxC21 , 2 findConflictspB, C2 q1 2 ;isConsistentpC11 C21 BqX getConflictpB C21 , C21 , C11 qCS X getConflictpB X, X, C21 qC11 C11 z tu P XtCS ureturn xC11 C21 , y;4.2 Evaluationsection evaluate effects parallelizing diagnosis process useMXP instead QXP calculate conflicts. (Shchekotykhin et al., 2015)focus finding limited set (five) minimal diagnoses.4.2.1 Implementation VariantsUsing MXP parallel tree construction implicitly means time allocatedconflict generation using QXP proceeding next node.analyze extent use MXP beneficial tested three different strategiesusing MXP within full parallelization method FP.Strategy (1): configuration simply called MXP instead QXP nodegeneration. Whenever MXP finds conflict, added global list known conflicts(re-)used parallel threads. thread executes MXP nodegeneration continues next node MXP returns.Strategy (2): strategy implements variant MXP slightly complex.MXP finds first conflict, method immediately returns conflictcalling thread continue exploring additional nodes. time, new background thread started continues search additional conflicts, i.e., completeswork MXP call. addition, whenever MXP finds new conflict checksalready running node generation thread could reused conflict854fiParallel Model-Based Diagnosis Multi-Core Computersavailable beforehand. case, search conflicts threadstopped new conflict needed anymore. Strategy (2) could theory resultbetter CPU utilization, wait MXP call finishcontinue building HS-tree. However, strategy also leads higher synchronizationcosts threads, e.g., notify working threads newly identified conflicts.Strategy (3): Finally, parallelized conflict detection procedure itself. Wheneverset C constraints split two parts, first recursive call findConflictsqueued execution thread pool second call executed current thread.calls finished, algorithm continues.experimentally evaluated three configurations benchmark datasets.results showed Strategy (2) lead measurable performance improvementscompared Strategy (1). additional communication costs seem highersaved executing conflict detection process backgroundthread. Strategy (3) applied combination strategies, similarexperiments reported sequential HS-tree construction (Shchekotykhin et al.,2015), additional performance gains could observed due higher synchronizationcosts. limited effectiveness Strategies (2) (3) principle causednature benchmark problems strategies might advantageousdifferent problem settings. following, therefore report resultsapplying Strategy (1).4.2.2 Results DXC Benchmark Problemsresults DXC benchmarks shown Table 4. left side table showsresults using QXP right hand side shows results MXP.speedups shown FP columns refer respective sequential algorithms usingconflict detection technique.Using MXP instead QXP favorable using sequential HS-tree algorithmalso reported work MXP (Shchekotykhin et al., 2015). reductionrunning times ranges 17% 44%. speedups obtained FP usingMXP comparable FP using QXP range 1.33 2.10, i.e., leadreduction running times 52%. speedups achieved additionspeedups sequential algorithm using MXP could already achieve QXP.best results printed bold face Table 4 using MXP combinationFP consistently performs best. Overall, using FP combination MXP38% 76% faster sequential algorithm using QXP. tests indicateparallelization method works well also conflict detection techniquescomplex QXP and, case, return one conflict call.addition, investing time conflict detection situations goal findleading diagnoses proves promising strategy.4.2.3 Additional Experiments Discussionran additional experiments constraint problems ontology debugging problems. detailed results provided Section A.2.855fiJannach, Schmitz, & ShchekotykhinSystem7418274L857428374181c432Seq.(QXP)[ms]1215496993,714FP(QXP)S4E41.26 0.321.36 0.341.58 0.391.99 0.551.77 0.44Seq.(MXP)[ms]1012353942,888FP(MXP)S4E41.52 0.381.33 0.331.48 0.372.10 0.531.72 0.43Table 4: Observed performance gains DXC benchmarks (QXP vs MXP).Overall, results obtained embedding MXP sequential algorithm confirmresults Shchekotykhin et al. (2015) using MXP favorable QXPsmall problem instances. However, also observe allocatingtime conflict detection MXP parallel processing setup helpspeedup diagnosis process search number leading diagnoses. bestperforming configuration across experiments using Full Parallelization methodcombination MXP setup led shortest computation times 2025 tested scenarios (DX benchmarks, CSPs, ontologies).5. Parallelized Depth-First Hybrid Searchapplication domains MBD, finding minimal diagnoses either requiredsimply possible computational complexity application-specific constraints allowed response times. settings, number algorithmsproposed years, example try find one minimal diagnosesquickly find diagnoses certain cardinality (Metodi, Stern, Kalech, & Codish, 2014;Feldman, Provan, & van Gemund, 2010b; de Kleer, 2011). cases, algorithmsprinciple extended used find diagnoses. are, however, optimizedtask.Instead analyzing various heuristic, stochastic approximative algorithms proposed literature individually respect potential parallelization,analyze next section parallelization helpful already simple classdepth-first algorithms. context, also investigate measurable improvementsachieved without using (domain-specific) heuristic. Finally, proposehybrid strategy combines depth-first full-parallel HS-tree constructionconduct additional experiments assess strategy advantageous taskquickly finding one minimal diagnosis.5.1 Parallel Random Depth-First Searchsection introduces parallelized depth-first search algorithm quickly find one singlediagnosis. different threads explore tree partially randomized form, callscheme Parallel Random Depth-First Search (PRDFS).856fiParallel Model-Based Diagnosis Multi-Core Computers5.1.1 Algorithm DescriptionAlgorithm 9 shows main program recursive implementation PRDFS. SimilarHS-tree algorithm, search diagnoses guided conflicts. time, however,algorithm greedily searches depth-first manner. diagnosis found,checked minimality diagnosis contain redundant elements.minimization non-minimal diagnosis achieved calling method likeInv-QuickXplain (Shchekotykhin et al., 2014) simply trying remove one elementdiagnosis checking resulting set still diagnosis.Algorithm 9: diagnosePRDFS: Parallelized random depth-first search.Input: diagnosis problem (SD, Comps, Obs),number minDiags diagnoses findResult: set diagnoses12345678H; conflicts H;rootNode = getRootNode(SD, Comps, Obs);1 nbThreadsthreads.execute(expandPRDFS(rootNode, minDiags, , conflicts));|| minDiagswait();threads.shutdownNow();return ;idea parallelization approach algorithm start multiple threadsroot node. threads perform depth-first search parallel, picknext conflict element explore randomized manner.logic expanding node shown Algorithm 10. First, conflictgiven node copied, changes set constraints affectthreads. Then, long enough diagnoses found, randomly chosen constraintcurrent nodes conflict used generate new node. expansion functionimmediately called recursively new node, thereby implementing depth-firststrategy. identified diagnosis minimized added list knowndiagnoses. Similar previous parallelization schemes, access global listsknown conflicts made thread-safe. specified number diagnosesfound threads finished, statement threads.shutdownNow() immediately stopsexecution threads still running results returned. semanticsthreads.execute(), wait(), notify() Section 3.5.1.2 ExampleLet us apply depth-first method example Section 2.2.1. Remembertwo conflicts problem ttC1, C2, C3u, tC2, C4uu. partially expanded treeproblem seen Figure 2.857fiJannach, Schmitz, & ShchekotykhinAlgorithm 10: expandPRDFS: Parallel random depth-first node expansion.Input: existingNode expand, number minDiags diagnoses find,sets conflicts123456789101112131415161718C = existingNode.conflict.clone();// Copy existingNodes conflict|| minDiags ^|C| 0Randomly pick constraint c CC Cztcu;newPathLabel = existingNode.pathLabel {c};node = new Node(newPathlabel);P conflicts : X newPathLabel Hnode.conflict = S;elsenode.conflict = checkConsistency(SD, Comps, Obs, node.pathLabel);node.conflict H// New conflict foundconflicts = conflicts node.conflict;// Recursive call implements depth-first search strategyexpandPRDFS(node, minDiags, , conflicts);else// Diagnosis founddiagnosis = minimize(node.pathLabel);{diagnosis};|| minDiagsnotify();example, first root node 1 created conflict tC1, C2, C3ufound. Next, random expansion would, example, pick conflict element C1generate node 2 . node, conflict tC2, C4u computed tC1ualone diagnosis. Since algorithm continues depth-first manner,pick one label elements node 2 , e.g., C2 generate node 3 .node, consistency check succeeds, conflict computed algorithmfound diagnosis. found diagnosis tC1, C2u is, however, minimal containsredundant element C1. function Minimize, called end Algorithm10, therefore remove redundant element obtain correct diagnosis tC2u.used one thread example, one parallel threads wouldprobably started expanding root node using conflict element C2 (node 4 ).case, single element diagnosis tC2u would identified already firstlevel. Adding parallel threads therefore help increase chances find onehitting set faster different parts HS-tree explored parallel.Instead random selection strategy, elaborate schemes pick next nodespossible, e.g., based application-specific heuristics fault probabilities. One couldalso better synchronize search efforts different threads avoid duplicate calculations. conducted experiments algorithm variant used shared858fiParallel Model-Based Diagnosis Multi-Core Computers1{C1,C2, C3}C124C2C3{C2,C4}3C2{C2,C4}C4C2C4Figure 2: Example HS-tree construction PRDFS.synchronized list open nodes avoid two threads generate identical sub-treeparallel. did, however, observe significantly better results methodshown Algorithm 9 probably due synchronization overhead.5.1.3 Discussion Soundness CompletenessEvery single thread depth-first algorithm systematically explores full search spacebased conflicts returned Theorem Prover. Therefore, existing diagnosesfound parameter minDiags equal higher number actuallyexisting diagnoses.Whenever (potentially non-minimal) diagnosis encountered, minimization process ensures minimal diagnoses stored list diagnoses. duplicateaddition diagnosis one threads last lines algorithmprevented consider diagnoses equal contain set elementsset definition cannot contain element twice.Overall, algorithm designed find one diagnoses quickly. computation minimal diagnoses possible algorithm highly inefficient, e.g., duecomputational costs minimizing diagnoses.5.2 Hybrid StrategyLet us consider problem finding one minimal diagnosis. One easily imaginechoice best parallelization strategy, i.e., breadth-first depth-first,depend specifics given problem setting actual size existingdiagnoses. single-element diagnosis exists, exploring first level HS-treebreadth-first approach might best choice (see Figure 3(a)). depth-first strategymight eventually include element non-minimal diagnosis, wouldnumber additional calculations ensure minimality diagnosis.If, contrast, smallest actually existing diagnosis cardinality of, e.g., five,breadth-first scheme would fully explore first four HS-tree levelsfinding five-element diagnosis. depth-first scheme, contrast, might quickly find859fiDiagnosis detectedJannach, Schmitz, & Shchekotykhinsuperset five-element diagnosis, e.g., six elements, needs sixadditional consistency checks remove redundant element diagnosis (Figure3(b)).Diagnosis detectedDiagnosis detected(a) Breadth-first strategy advantageous.(b) Depth-first strategy advantageous.Figure 3: Two problem configurations different search strategies favorable.Since cannot know cardinality diagnoses advance, propose hybridstrategy, half threads adopt depth-first strategy half usesfully parallelized breadth-first regime. implement strategy, Algorithms 5(FP) 9 (PRDFS) started parallel algorithm allowed use onehalf defined share available threads. coordinationtwo algorithms done help shared data structures contain knownconflicts diagnoses. enough diagnoses (e.g. one) found, running threadsterminatedresults returned.Diagnosis detected5.3 Evaluationevaluated different strategies efficiently finding one minimal diagnosisset benchmark problems used previous sections. experimentsetup identical except goal find one arbitrary diagnosisincluded additional depth-first algorithms. order measure potential benefitsparallelizing depth-first search, ran benchmarks PRDFS 4 threads1 thread, latter setup corresponds Random Depth First Search(RDFS) without parallelization.5.3.1 Results DXC Benchmark Problemsresults DXC benchmark problems shown Table 5. Overall, testedsystems, approaches proposed paper help speed processfinding one single diagnosis. 88 100 evaluated scenarios least onetested approaches statistically significantly faster sequential algorithm.12 scenarios, finding one single diagnosis simple modestsignificant speedups compared sequential algorithm obtained.comparing individual parallel algorithms, following observationsmade:860fiParallel Model-Based Diagnosis Multi-Core Computersexamples, PRDFS method faster breadth-first searchimplemented FP technique. one benchmark system, PRDFS approacheven achieve speedup 11 compared sequential algorithm, corresponds runtime reduction 91%.compared non-parallel RDFS, PRDFS could achieve higher speedupstested systems except simple one, took 16 ms evensequential algorithm. Overall, parallelization therefore advantageous alsodepth-first strategies.performance Hybrid strategy lies performances components PRDFS FP 4 5 tested systems. systems, closerfaster one two. Adopting hybrid strategy therefore represent goodchoice structure problem known advance, combinesideas breadth-first depth-first search able quickly find diagnosisproblem settings unknown characteristics.System7418274L857428374181c432Seq.[ms]1613546912,789FPS4E41.37 0.341.34 0.331.67 0.422.08 0.521.89 0.47RDFS[ms]91125741,435PRDFSS4E40.84 0.211.06 0.271.22 0.311.23 0.312.96 0.74HybridS4E40.84 0.211.05 0.261.06 0.261.04 0.261.81 0.45Table 5: Observed performance gains DXC benchmarks finding one diagnosis.5.3.2 Additional Experimentsdetailed results obtained additional experiments providedappendix. measurements include results CSPs (Section A.3.1) ontologies(Section A.3.2), well results obtained systematically varying characteristics synthetic diagnosis problems (Section A.3.3). results indicate applyingdepth-first parallelization strategy many cases advantageous CSP problems.tests ontology problems simulation results however reveal depending problem structure cases breadth-first strategybeneficial.5.3.3 Discussionexperiments show parallelization depth-first search strategy (PRDFS)help reduce computation times search one single diagnosis.evaluated cases, PRDFS faster sequential counterpart.cases, however, obtained improvements quite small virtually non-existent,explained follows.861fiJannach, Schmitz, & Shchekotykhinsmall scenarios, parallel depth-first search cannot significantlyfaster non-parallel variant creation first node parallelized. Therefore major fraction tree construction process parallelizedall.problem settings existing diagnoses size.parallel depth-first searching threads therefore explore tree certaindepth none threads immediately return diagnosis much smallerone determined another thread. E.g., given diagnosis problem,diagnoses size 5, threads explore tree least level 5 finddiagnosis also likely find diagnosis level. Therefore,setting thread much faster others.Finally, suspect problems cache contention correspondingly increased number cache misses, leads general performance deteriorationoverhead caused multiple threads.Overall, obtained speedups depend problem structure. hybridtechnique represents good compromise cases faster sequentialbreadth first search approach tested scenarios (including CSPs, ontologies, synthetically created diagnosis problems presented Section A.3). Also,efficient PRDFS cases breadth first search betterdepth first search.6. Parallel Direct CSP Encodingsalternative conflict-guided diagnosis approaches like Reiters hitting set technique,so-called direct encodings become popular research community recentyears (Feldman, Provan, de Kleer, Robert, & van Gemund, 2010a; Stern, Kalech, Feldman,& Provan, 2012; Metodi et al., 2014; Mencia & Marques-Silva, 2014; Menca, Previti, &Marques-Silva, 2015; Marques-Silva, Janota, Ignatiev, & Morgado, 2015).10general idea direct encodings generate specific representation diagnosisproblem instance knowledge representation language use theoremprover (e.g., SAT solver constraint engine) compute diagnoses directly.methods support generation one multiple diagnoses calling theorem proveronce. Nica, Pill, Quaritsch, Wotawa (2013) made number experimentscompared conflict-directed search direct encodings showedseveral problem settings, using direct encoding advantageous.part paper, goal evaluate whether parallelization searchprocess case inside constraint engine help improve efficiencydiagnostic reasoning process. goal chapter therefore rather quantifyextent internal parallelization solver useful present newalgorithmic contribution.10. direct encodings may always possible MBD settings discussed above.862fiParallel Model-Based Diagnosis Multi-Core Computers6.1 Using Gecode Solver Direct Encodingsevaluation use Gecode constraint solver (Schulte, Lagerkvist, & Tack, 2016).particular, use parallelization option Gecode test effects diagnosisrunning times.11 chosen problem encoding similar one used NicaWotawa (2012). allows us make results comparable obtainedprevious works. addition, provided encoding represented languagesupported multiple solvers.6.1.1 ExampleLet us first show general idea small example. Consider following CSP12consisting integer variables a1, a2, b1, b2, c1 constraints X1 , X2 , X3defined as:X1 : b1 a1 2, X2 : b2 a2 3, X3 : c1 b1 b2.Let us assume programmer made mistake X3 actually c1b1 ` b2. Given set expected observations (a test case) a1 1, a2 6, d1 20, MBDapplied considering constraints possibly faulty components.direct encoding given CSP extended definition array ABrab1 , ab2 , ab3 boolean (0/1) variables encode whether corresponding constraintconsidered faulty not. constraints rewritten follows:X11 : ab1 _ pb1 a1 2q,X21 : ab2 _ pb2 a2 3q,X31 : ab3 _ pc1 b1 b2q.observations encoded equality constraints bind valuesobserved variables. example, constraints would be:O1 : a1 1,O2 : a2 6,O3 : d1 20order find diagnosis cardinality 1, additionally add constraintab1 ` ab2 ` ab3 1let solver search solution. case, X3 would identifiedpossible diagnosis, i.e., ab3 would set 1 solver.6.1.2 Parallelization Approach Gecodeusing direct encoding, parallelization diagnosis process, shownReiters approach, cannot done embedded underlying searchprocedure. However, modern constraint solvers, Gecode, or-tools manysolvers participated MiniZinc Challenge (Stuckey, Feydy, Schutt, Tack,& Fischer, 2014), internally implement parallelization strategies better utilize todaysmulti-core computer architectures (Michel, See, & Van Hentenryck, 2007; Chu, Schulte, &11. state-of-the-art SAT solver capable parallelization could used analysis well.12. Adapted earlier work (Jannach & Schmitz, 2014).863fiJannach, Schmitz, & ShchekotykhinStuckey, 2009). following, therefore evaluate set experiments,solver-internal parallelization techniques help speed diagnosis processdirect encoding used.13Gecode implements adaptive work stealing strategy (Chu et al., 2009) parallelization. general idea summarized follows. soon thread finishesprocessing nodes search tree, steals nodes non-idle threads.order decide thread work stolen, adaptive strategy usesbalancing heuristics estimate density solutions particular partsearch tree. higher likelihood containing solution given branch,work stolen branch.6.2 Problem Encodingevaluation use MiniZinc constraint modeling language. languageprocessed different solvers allows us model diagnosis problems CSPsshown above.6.2.1 Finding One Diagnosisfind single diagnosis given diagnosis problem (SD, Comps, Obs), generatedirect encoding MiniZinc follows.(1) set components Comps generate array ab = [ab1 , . . . , abn ]boolean variables.(2) formula sdi P SD add constraint formconstraint abris _ psdi q;observation oj P Obs model extended constraintconstraint oj ;(3) Finally, add search goal output statement:solve minimize sumpi 1..nqpbool2intpabrisqq;output[show(ab)];first statement last part (solve minimize), instructs solver search(single) solution minimal number abnormal components, i.e., diagnosisminimum cardinality. second statement (output) projects assignments setabnormal variables, interested knowing componentsfaulty. assignments problem variables irrelevant.6.2.2 Finding Diagnosesproblem encoding shown used quickly find one/all diagnoses minimum cardinality. is, however, sufficient scenarios goal finddiagnoses problem. therefore propose following sound complete algorithmrepeatedly modifies constraint problem systematically identify diagnoses.13. contrast parallelization approaches presented previous sections, proposenew parallelization schemes rather rely existing ones implemented solver.864fiParallel Model-Based Diagnosis Multi-Core ComputersTechnically, algorithm first searches diagnoses size 1 increasesdesired cardinality diagnoses step step.Algorithm 11: directDiag: Computation diagnoses using direct encoding.Input: diagnosis problem (SD, Comps, Obs), maximum cardinality kResult: set diagnoses12345678910H; C H; card 1;k |Comps| k |Comps|;= generateModel (SD, Comps, Obs);card k= updateModel (M, card , C);1 computeDiagnosespMq;C C generateConstraintsp1 q;1 ;card card ` 1;return ;Procedure Algorithm 10 shows main components direct diagnosis method usedconnection parallel constraint solver find diagnoses. algorithm startsgeneration MiniZinc model (generateModel) described above.difference search solutions given cardinality;details encoding search goals given below.iteration, algorithm modifies model updating cardinalitysearched diagnoses furthermore adds new constraints corresponding alreadyfound diagnoses (updateModel). updated model provided MiniZincinterpreter (constraint solver), returns set solutions 1 . element P 1corresponds diagnosis cardinality card .order exclude supersets already found diagnoses 1 future iterations,generate constraint P 1 formulas j l (generateConstraints):constraint abrjs false _ _ abrls false;constraints ensure already found diagnosis supersets cannot foundagain. added model next iteration main loop. algorithmcontinues diagnoses cardinalities k computed.Changes Encoding calculate diagnoses given size, first instructsolver search possible solutions provided constraint problem.14addition, keeping steps (1) (2) Section 6.2.1 replace lines step (3)14. achieved calling MiniZinc --all-solutions flag.865fiJannach, Schmitz, & Shchekotykhinfollowing statements:constraint sumpi 1..nqpbool2intpabrisqq card ;solve satisfy;output[show(ab)];first statement constrains number abnormal variables truecertain value, i.e., given cardinality card. second statement tells solver findvariable assignments satisfy constraints. last statement guaranteessolver considers solutions different differentrespect assignments abnormal variables.Soundness Completeness Algorithm 10 implements iterative deepening approach guarantees minimality diagnoses . Specifically, algorithmconstructs diagnoses order increasing cardinality limiting number abvariables set true model. computation starts card 1,means one ab variable true. Therefore, diagnoses cardinality 1,i.e., comprising one abnormal variable, returned solver. founddiagnosis add constraint requires least one abnormal variablesdiagnosis false. Therefore, neither diagnosis supersets foundsubsequent iterations. constraints implement pruning rule HS-treealgorithm. Finally, Algorithm 10 repeatedly increases cardinality parameter cardone continues next iteration. algorithm continues increment cardinality card becomes greater number components, correspondslargest possible cardinality diagnosis. Consequently, given diagnosis problemwell sound complete constraint solver, Algorithm 10 returns diagnosesproblem.6.3 Evaluationevaluate speedups achieved parallelization also direct encoding,used first five systems DXC Synthetic Track tested scenariosusing Gecode solver without parallelization 2 4 parallel threads.6.3.1 Resultsevaluated two different configurations. setup (A), task find one singlediagnosis minimum cardinality. setup (B), iterative deepening procedureSection 6.2.2 used find diagnoses size actual error.results setup (A) shown Table 6. observe using parallelconstraint solver pays except tiny problems overall search timeless 200 ms. Furthermore, adding worker threads also beneficial largerproblem sizes speedup 1.25 achieved complex test casetook 1.5 seconds solve.pattern observed setup (B). detailed results listed Table7. tiny problems, internal parallelization Gecode solver leadperformance improvements slightly slows whole process. soon866fiParallel Model-Based Diagnosis Multi-Core Computersproblems become complex, parallelization pays observe speedup1.55 complex tested cases, corresponds runtime reduction35%.System7418274L857428374181c432Direct EncodingAbs. [ms]S2E2S427 0.85 0.42 0.7930 0.89 0.44 0.7932 0.85 0.43 0.79200 1.04 0.52 1.151,399 1.17 0.58 1.25E40.200.200.200.290.31Table 6: Observed performance gains DXC benchmarks finding one diagnosisdirect encoding using one (column Abs.), two, four threads.System7418274L857428374181c432Direct EncodingAbs. [ms]S2E2S4136 0.84 0.42 0.8060 0.83 0.41 0.77158 0.93 0.47 0.921,670 1.19 0.59 1.33229,869 1.22 0.61 1.55E40.200.190.230.330.39Table 7: Observed performance gains DXC benchmarks finding diagnosesdirect encoding using one (column Abs.), two, four threads.6.3.2 Summary RemarksOverall, experiments show parallelization beneficial direct encodingdiagnosis problem employed, particular problems non-trivial.Comparing absolute running times Java implementation using open sourcesolver Choco optimized C++ implementation Gecode generally appropriate benchmark problems, Gecode works faster absolute scale.Note, however, true cases. particular searching diagnoses size actual error complex system c432, even Reitersnon-parallelized Hitting Set algorithm much faster (85 seconds) using directencoding based iterative deepening (230 seconds). line observationNica et al. (2013) direct encodings always best choice searchingdiagnoses.first analysis run-time behavior Gecode shows larger problem is,time spent solver iteration reconstruct internal structures,lead measurable performance degradation. Note work reliedMiniZinc encoding diagnosis problem independent specifics867fiJannach, Schmitz, & Shchekotykhinunderlying constraint engine. implementation relies direct use APIspecific CSP solver might help address certain performance issues. Nevertheless,implementation must solver-specific allow us switch solvers easilypossible MiniZinc..7. Relation Previous Workssection explore works related approach. First examine differentapproaches computation diagnoses. focus general methodsparallelizing search algorithms.7.1 Computation DiagnosesComputing minimal hitting sets given set conflicts computationally hard problemalready discussed Section 2.2.2 several approaches proposed yearsdeal issue. approaches divided exhaustive approximate ones.former perform sound complete search minimal diagnoses, whereaslatter improve computational efficiency exchange completeness, e.g., searchone small set diagnoses.Approximate approaches example based stochastic search techniques likegenetic algorithms (Li & Yunfei, 2002) greedy stochastic search (Feldman et al., 2010b).greedy method proposed Feldman et al. (2010b), example, uses two-stepapproach. first phase, random possibly non-minimal diagnosis determinedmodified DPLL15 algorithm. algorithm always finds one random diagnosisinvocation due random selection propositional variables assignments.second step, algorithm minimizes diagnosis returned DPLL techniquerepeatedly applying random modifications. randomly chooses negative literaldenotes corresponding component faulty flips value positive.obtained candidate well diagnosis problem provided DPLL algorithmcheck whether candidate diagnosis not. case success obtaineddiagnosis kept another random flip done. Otherwise, negative literal labeledfailure another negative literal randomly selected. algorithm stopsnumber failures greater predefined constant returns best diagnosisfound far.approach Li Yunfei (2002) genetic algorithm takes number conflictsets input generates set bit-vectors (chromosomes), every bit encodestruth value atom ab(.) predicate. iteration algorithm appliesgenetic operations, mutation, crossover, etc., obtain new chromosomes. Subsequently, obtained bit-vectors evaluated hitting set fitting functioneliminates bad candidates. algorithm stops predefined number iterationsreturns best diagnosis.general, approximate approaches directly comparable LWPFP techniques, since incomplete guarantee minimality returned15. Davis-Putnam-Logemann-Loveland.868fiParallel Model-Based Diagnosis Multi-Core Computershitting sets. goal contrast improve performance timemaintaining completeness soundness property.Another way finding approximate solutions use heuristic search approaches.example, Abreu van Gemund (2009) proposed Staccato algorithm appliesnumber heuristics pruning search space. aggressive pruning techniquesresult better performance search algorithms. However, also increase probability diagnoses found. approach aggressivenessheuristics varied input parameters depending application goals.recently, Cardoso Abreu (2013) suggested distributed version Staccato algorithm, based Map-Reduce scheme (Dean & Ghemawat, 2008)therefore executed cluster servers. recent algorithms focusefficient computation one minimum cardinality (minc) diagnoses (de Kleer,2011). distributed approach minimum cardinality scenario, assumption (possibly incomplete) set conflicts already available inputbeginning hitting-set construction process. application scenariosaddress work, finding conflicts considered computationallyexpensive part assume know minimal conflicts advancecompute on-demand also done works (Felfernig, Friedrich, Jannach,Stumptner, et al., 2000; Friedrich & Shchekotykhin, 2005; Williams & Ragno, 2007); see alsowork Pill, Quaritsch, Wotawa (2011) comparison conflict computationapproaches.Exhaustive approaches often based HS-trees like work Wotawa (2001a)tree construction algorithm reduces number pruning steps presence nonminimal conflicts. Alternatively, one use methods compute diagnoses withoutexplicit computation conflict sets, i.e., solving problem dual minimal hitting sets(Satoh & Uno, 2005). Stern et al. (2012), example, suggest method exploresduality conflicts diagnoses uses symmetry guide search.approaches exploit structure underlying problem, hierarchical (Autio& Reiter, 1998), tree-structured (Stumptner & Wotawa, 2001), distributed (Wotawa &Pill, 2013). algorithms similar HS-tree algorithm and, consequently,parallelized similar way. example, consider Set-Enumeration Tree(SE-tree) algorithm (Rymon, 1994). algorithm, similarly Reiters HS-tree approach,uses breadth-first search specific expansion procedure implements pruningnode selection strategies. LWP FP parallelization variantused SE-tree algorithm comparable speedups expected.7.2 Parallelization Search AlgorithmsHistorically, parallelization search algorithms approached three different ways(Burns, Lemons, Ruml, & Zhou, 2010):(i) Parallelization node processing: applying type parallelization, treeexpanded one single process, computation labels evaluationheuristics done parallel.869fiJannach, Schmitz, & Shchekotykhin(ii) Window-based processing: approach, sets nodes, called windows, processed different threads parallel. windows formed search algorithmaccording predefined criteria.(iii) Tree decomposition approaches: Here, different sub-trees search tree assigned different processes (Ferguson & Korf, 1988; Brungger, Marzetta, Fukuda, &Nievergelt, 1999).principle, three types parallelization applied form HS-treegeneration problem.Applying strategy (i) MBD problem setting would mean parallelize processconflict computation, e.g., parallel variant QXP MXP. testedpartially parallelized version MXP, however lead performanceimprovements compared single-threaded approach evaluated benchmarkproblems (Shchekotykhin et al., 2015). experiments Section 4 however showusing MXP combination LWP FP thereby implicitly allocating CPU timecomputation multiple conflicts construction single node advantageous. well-known conflict prime implicate computation algorithms (Junker,2004; Marques-Silva et al., 2013; Previti, Ignatiev, Morgado, & Marques-Silva, 2015)contrast designed parallel execution computation multiple conflicts.Strategy (ii) computing sets nodes (windows) parallel example appliedPowley Korf (1991). work windows determined different thresholdsheuristic function Iterative Deepening A*. Applying strategy HS-treeconstruction problem would mean categorize nodes expanded accordingcriterion, e.g., probability finding diagnosis, allocate differentgroups individual threads. absence window criteria, LWP FP couldseen extreme cases window size one, open node allocated one threadprocessor. experiments done throughout paper suggest independentparallelization strategy (LWP FP) number parallel threads (windows)exceed number physically available computing threads obtain best performance.Finally (iii), strategy exploring different sub-trees search differentprocesses can, example, applied context MBD techniques using BinaryHS-Tree (BHS) algorithms (Pill & Quaritsch, 2012). Given set conflict sets, BHSmethod generates root node labels input set conflicts. Then, selectsone components occurring conflicts generates two child nodes,left node labeled conflicts comprising selected component rightnode remaining ones. Consequently, diagnosis tree decomposed two subtrees processed parallel. main problem kind parallelizationconflicts often known advance computed search.Anglano Portinale (1996) suggested another approach ultimatelyparallelized diagnosis problem based structural problem characteristics.work, first map given diagnosis problem Behavioral Petri Net (BPN). Then,obtained BPN manually partitioned subnets every subnet provideddifferent Parallel Virtual Machine (PVM) parallel processing. relationshipwork LWP FP parallelization schemes limited approaches alsorequire manual problem decomposition step.870fiParallel Model-Based Diagnosis Multi-Core Computersgeneral, parallelized versions domain-independent search algorithms likeapplied MBD settings. However, MBD problem specifics makeapplication algorithms difficult. instance, PRA methodvariant HDA discussed work Burns et al. (2010) use mechanism minimizememory requirements retracting parts search tree. forgotten partslater re-generated required. MBD setting, generation nodes howevercostly part, applicability HDA seems limited. Similarly,duplicate detection algorithms like PBNF (Burns et al., 2010) require existenceabstraction function partitions original search space blocks. general MBDsettings, however cannot assume function given.order improve performance therefore avoid parallel generationduplicate nodes different threads, plan investigate future work.promising starting point research could work Phillips, Likhachev,Koenig (2014). authors suggest variant A* algorithm generatesindependent nodes order reduce costs node generation. Two nodes consideredindependent generation one node lead change heuristicfunction node. generation independent nodes done parallelwithout risk repeated generation already known state. main difficultyadopting algorithm MBD formulation admissible heuristic requiredevaluate independence nodes arbitrary diagnosis problems. However,specific problems encoded CSPs, Williams Ragno (2007) presentheuristic depends number unassigned variables particular search node.Finally, parallelization also used literature speed processinglarge search trees fit memory. Korf Schultze (2005), instance, suggestextension hash-based delayed duplicate detection algorithm allows searchalgorithm continue search parts search tree written readhard drive. methods theory used combination LWP FPparallelization schemes case complex diagnosis problems. plan explore use(externally) saved search states context MBD part future works.8. Summarywork, propose systematically evaluate various parallelization strategiesModel-Based Diagnosis better exploit capabilities multi-core computers. showparallelization advantageous various problem settings diagnosis approaches. approaches include conflict-driven search minimaldiagnoses different conflict detection techniques (heuristic) depth-first searchorder quickly determine single diagnosis. main benefits parallelizationapproaches applied independent underlying reasoning enginevariety diagnostic problems cannot efficiently represented SAT CSPproblems. addition HS-tree based parallelization approaches, also showparallelization beneficial settings direct problem encoding possiblemodern parallel solver engines available.evaluations furthermore shown speedups proposed parallelization methods vary according characteristics underlying diagnosis problem.871fiJannach, Schmitz, & Shchekotykhinfuture work, plan explore techniques analyze characteristics orderpredict advance parallelization method best suited find one singlediagnoses given problem.Regarding algorithmic enhancements, furthermore plan investigate information underlying problem structure exploited achieve better distribution work parallel threads thereby avoid duplicate computations.Furthermore, plan explore usage parallel solving schemes dual algorithms, i.e., algorithms compute diagnoses directly without computation minimal conflicts (Satoh & Uno, 2005; Felfernig, Schubert, & Zehentner, 2012; Stern et al.,2012; Shchekotykhin et al., 2014).presented algorithms designed use modern multi-core computerstoday usually less dozen cores. results show additional performance improvements obtain proposed techniques become smalleradding CPUs. part future works therefore plan developalgorithms utilize specialized environments support massive parallelization.context, future topic research could adaption parallel HS-treeconstruction GPU architectures. GPUs, thousands computing cores,proved superior tasks parallelized suitable way. Campeotto,Palu, Dovier, Fioretto, Pontelli (2014) example used GPU parallelize constraint solver. However, yet fully clear whether tree construction techniquesefficiently parallelized GPU, many data structures shared acrossnodes access synchronized.Acknowledgementspaper significantly extends combines previous work (Jannach, Schmitz, &Shchekotykhin, 2015; Shchekotykhin et al., 2015).would like thank Hakan Kjellerstrand Gecode team support.also thankful various helpful comments suggestions made anonymousreviewers JAIR, DX14, DX15, AAAI15, IJCAI15.work supported Carinthian Science Fund (KWF) contract KWF3520/26767/38701, Austrian Science Fund (FWF) German Research Foundation (DFG) contract numbers 2144 N-15 JA 2095/4-1 (Project DebuggingSpreadsheet Programs).Appendix A.appendix report results additional experiments made differentbenchmark problems well results simulation experiments artificially createdproblem instances.Section A.1 contains results LWP FP parallelization schemes proposedSection 3.Section A.2 reports additional measurements regarding use MergeXplainwithin parallel diagnosis process, see Section 4.872fiParallel Model-Based Diagnosis Multi-Core ComputersSection A.3 finally provides additional results parallelization depth-firststrategies discussed Section 5.A.1 Additional Experiments LWP FP Parallelization Strategiesaddition experiments DXC benchmark systems reported Section 3.5,made additional experiments Constraint Satisfaction Problems, ontologies,artificial Hitting Set construction problems. Furthermore, examined effectsincreasing number available threads benchmarks CSPs ontologies.A.1.1 Diagnosing Constraint Satisfaction ProblemsData Sets Procedure set experiments used number CSP instances2008 CP solver competition (Lecoutre, Roussel, & van Dongen, 2008)injected faults.16 diagnosis problems created follows. first generatedrandom solution using original CSP formulations. solution, randomlypicked 10% variables stored value assignments, servedtest cases. stored variable assignments correspond expected outcomesconstraints formulated correctly. Next, manually inserted errors (mutations)constraint problem formulations17 , e.g., changing less operatoroperator, corresponds mutation-based approach software testing.diagnosis task consists identifying possibly faulty constraints using partialtest cases. addition benchmark CSPs converted number spreadsheetdiagnosis problems (Jannach & Schmitz, 2014) CSPs test performance gainsrealistic application settings.Table 8 shows problem characteristics including number injected faults (#F),number diagnoses (#D), average diagnosis size (|D|). general, selectedCSPs quite diverse respect size.Results measurement results using 4 threads searching diagnoses givenTable 9. Improvements could achieved problem instances. exceptionsmallest problem mknap-1-5 speedups achieved LWP FP statisticallysignificant. problems, improvements strong (with running timereduction 50%), whereas others improvements modest. average, FPalso faster LWP. However, FP consistently better LWP oftendifferences small.observed results indicate performance gains depend number factorsincluding size conflicts, computation times conflict detection,problem structure itself. average FP faster LWP, characteristicsproblem settings seem considerable impact speedups obtaineddifferent parallelization strategies.16. able sufficient number repetitions, picked instances comparably small runningtimes.17. mutated CSPs downloaded http://ls13-www.cs.tu-dortmund.de/homepage/hp_downloads/jair/csps.zip.873fiJannach, Schmitz, & ShchekotykhinScenarioc8costasArray-13domino-100-100gracefulK3-P2mknap-1-5queens-8hospital paymentprofit calculationcourse planningpreservation modelrevenue calculation#C5238710060728382845770193#V239881001539875140583803154#F823411545214#D428111729120423024221452|D|6.252.522.94110.93.84.24213Table 8: Characteristics selected problem settings.Scenarioc8costasArray-13domino-100-100gracefulK3-P2mknap-1-5queens-8hospital paymentprofit calculationcourse planningpreservation modelrevenue calculationSeq.(QXP)[ms]5594,0131,3861,96531414112,66019722,130167778LWP(QXP)S4E41.10 0.272.160.543.08 0.772.750.691.03 0.261.570.391.640.411.710.432.580.651.460.372.81 0.70FP(QXP)S4E41.07 0.272.58 0.653.05 0.762.99 0.751.02 0.251.65 0.411.73 0.432.00 0.502.61 0.651.48 0.372.58 0.64Table 9: Results CSP benchmarks spreadsheets searching diagnoses.A.1.2 Diagnosing OntologiesData Sets Procedure recent works, MBD techniques used locate faultsdescription logic ontologies (Friedrich & Shchekotykhin, 2005; Shchekotykhin et al., 2012;Shchekotykhin & Friedrich, 2010), represented Web Ontology Language(OWL) (Grau, Horrocks, Motik, Parsia, Patel-Schneider, & Sattler, 2008). testingontology, developer similarly earlier approach (Felfernig, Friedrich,Jannach, Stumptner, & Zanker, 2001) specify set positive negative test cases.test cases sets logical sentences must entailed ontology (positive)entailed ontology (negative). addition, ontology itself, setlogical sentences, consistent coherent (Baader, Calvanese, McGuinness,Nardi, & Patel-Schneider, 2010). diagnosis (debugging) problem context arises,one requirements fulfilled.work Shchekotykhin et al. (2012), two interactive debugging approachestested set faulty real-world ontologies (Kalyanpur, Parsia, Horridge, & Sirin, 2007)874fiParallel Model-Based Diagnosis Multi-Core Computerstwo randomly modified large real-world ontologies. use dataset evaluateperformance gains applying parallelization schemes ontology debugging problem. details different tested ontologies given Table 10.characteristics problems described terms description logic (DL) usedformulate ontology, number axioms (#A), concepts (#C), properties (#P),individuals (#I). terms first-order logic, concepts properties correspondunary binary predicates, whereas individuals correspond constants. Every letterDL name, ALCHF pDq , corresponds syntactic feature language. E.g.,ALCHF pDq Attributive concept Language Complement, properties Hierarchy,Functional properties Datatypes. underlying description logic reasoner, usedPellet (Sirin, Parsia, Grau, Kalyanpur, & Katz, 2007). manipulation knowledge bases diagnosis process accomplished OWL-API (Horridge &Bechhofer, 2011).Note considered ontology debugging problem different diagnosis settings discussed far cannot efficiently encoded CSP SAT problem.reason decision problems, checking consistency conceptsatisfiability, ontologies given Table 10 ExpTime-complete (Baader et al.,2010). set experiments therefore helps us explore benefits parallelizationproblem settings computation conflict sets hard. Furthermore,application parallelization approaches ontology debugging problem demonstrates generality methods, i.e., show methods applicablewide range diagnosis problems require existence sound completeconsistency checking procedure.Due generality Reiters general approach and, correspondingly, implementation diagnosis procedures, technical integration OWL-DL reasonersoftware framework relatively simple. difference CSP-based problemsinstead calling Chocos solve() method inside Theorem Prover, make callPellet reasoner via OWL-API check consistency ontology.OntologyChemicalKoalaSweet-JPLminiTambisUniversityEconomyTransportationCtonOpengalen-no-propchainsDLALCHF pDqALCON pDqALCHOF pDqALCNSOIN pDqALCHpDqALCHpDqSHFALCHIF pDq#A144442,579173491,7811,30033,2039,664#C/#P/#I48/20/021/5/61,537/121/50183/44/030/12/4339/53/482445/93/18317,033/43/04,713/924/0#D6101348908641,78215110|D|1.672.3133.677.17844.13Table 10: Characteristics tested ontologies.Results obtained results using thread pool size four shown Table11. Again, every case parallelization advantageous compared sequentialversion cases obtained speedups substantial. Regarding comparison875fiJannach, Schmitz, & ShchekotykhinLWP FP variants, clear winner across test cases. LWP seemsadvantageous problems complex respectcomputation times. problems easily solved, FP sometimes slightlybetter. clear correlation problem characteristics like complexityknowledge base terms size could identified within set benchmarkproblems.OntologyChemicalKoalaSweet-JPLminiTambisUniversityEconomyTransportationCtonOpengalen-no-propchainsSeq.(QXP)[ms]237167135853551,69620311,044LWP(QXP)S4E41.44 0.361.42 0.361.470.371.430.361.660.412.20 0.552.72 0.681.27 0.321.590.40FP(QXP)S4E41.33 0.331.27 0.321.55 0.391.46 0.371.68 0.421.90 0.482.33 0.581.22 0.301.86 0.47Table 11: Results ontologies searching diagnoses.A.1.3 Adding ThreadsConstraint Satisfaction Problems Table 12 shows results CSP benchmarksspreadsheets using 12 threads. test utilizing 4 threadsadvantageous one small scenario. However, 7 11 tested scenarioscomputations 8 threads pay off. indicateschoosing right degree parallelization depend characteristics diagnosisproblem. diagnosis mknap-1-5 problem, example, cannot spedparallelization contains one single conflict found root node.contrast, graceful-K3-P2 problem benefits use 12 threadscould achieve speedup 4.21 scenario, corresponds runtime reduction76%.Ontologies results diagnosing ontologies 12 threads shownTable 13. tested ontologies, comparably simple debugging cases, using4 threads payed 3 7 cases. best results diagnosing3 ontologies obtained 8 threads used. one ontology using4 threads even slower sequential algorithm. indicateseffectiveness parallelization depends characteristics diagnosis problemadding threads even slightly counterproductive.A.1.4 Systematic Variation Problem CharacteristicsProcedure better understand way problem characteristics influenceperformance gains, used suite artificially created hitting set construction problems876fiParallel Model-Based Diagnosis Multi-Core ComputersScenarioSeq.(QXP)[ms] S4E4c8444 1.05 0.26costasArray-133,854 2.69 0.67domino-100-100213 2.04 0.51gracefulK3-P21,743 3.03 0.76mknap-1-54,141 1.00 0.25queens-886 1.18 0.30hospital payment11,728 1.60 0.40profit calculation81 1.53 0.38course planning15,323 2.31 0.58preservation model127 1.34 0.34revenue calculation460 2.39 0.60S81.072.882.304.121.001.301.701.592.851.412.17FP(QXP)E8S10E10S12E120.13 1.08 0.11 1.07 0.090.36 2.84 0.28 2.80 0.230.29 2.22 0.22 2.00 0.170.51 4.18 0.42 4.21 0.350.13 1.00 0.10 1.00 0.080.16 1.24 0.12 1.19 0.100.21 1.51 0.15 1.36 0.110.20 1.51 0.15 1.44 0.120.36 2.84 0.28 2.73 0.230.18 1.41 0.14 1.43 0.120.27 1.96 0.20 1.85 0.15Table 12: Observed performance gains CSP benchmarks spreadsheetsserver 12 hardware threads.OntologyChemicalKoalaSweet-JPLminiTambisUniversityEconomyTransportationSeq.(QXP)[ms]246216134883521,448S41.371.071.091.471.531.481.74E40.340.270.270.370.380.370.43S81.291.021.131.491.640.901.23FP(QXP)E8S100.16 1.300.13 1.030.14 1.080.19 1.470.21 1.560.11 0.760.15 1.07E100.130.100.110.150.160.080.11S121.320.991.021.451.560.711.09E120.110.080.090.120.130.060.09Table 13: Observed performance gains ontologies server 12 hardwarethreads.following varying parameters: number components (#Cp), number conflicts(#Cf), average size conflicts (|Cf|). Given parameters, used problem generatorproduces set minimal conflicts desired characteristics. generatorfirst creates given number components uses components generaterequested number conflicts.obtain realistic settings, generated conflicts equal size rathervaried according Gaussian distribution desired size mean. Similarly,components equally likely part conflict used Gaussiandistribution assign component failure probabilities. probability distributions couldused generation process well, e.g., reflect specifics certain applicationdomain.Since experiment conflicts known advance, conflict detection algorithm within consistency check return one suitable conflict upon request.zero computation times unrealistic assumption conflict877fiJannach, Schmitz, & Shchekotykhindetection actually costly part diagnosis process, varied assumedconflict computation times analyze effect relative performance gains.computation times simulated adding artificial active waiting times (Wt) insideconsistency check (shown ms Table 14). Note consistency check calledconflict reused current node; artificial waiting time appliescases new conflict determined.experiment repeated 100 times different variations problem settingfactor random effects. number diagnoses #D thus average well.algorithms had, however, solve identical sets problems thus returned identicalsets diagnoses. limited search depth 4 experiments speedbenchmark process. average running times reported Table 14.Results Varying Computation Times First, varied assumed conflict computation times quite small diagnosis problem using 4 parallel threads (Table 14).first row assumed zero computation times shows long HS-tree constructionalone needs. improvements parallelization smaller caseoverhead thread creation synchronization. However, soon add averagerunning time 10ms consistency check, parallelization approaches resultspeedup 3, corresponds runtime reduction 67%. increasingassumed computation time lead better relative improvements using pool4 threads.Results Varying Conflict Sizes average conflict size impacts breadthHS-tree. Next, therefore varied average conflict size. hypothesis largerconflicts correspondingly broader HS-trees better suited parallel processing.results shown Table 14 confirm assumption. FP always slightly efficientLWP. Average conflict sizes larger 9 did, however, lead strong additionalimprovements using 4 threads.Results Adding Threads larger conflicts, adding additional threads leadsimprovements. Using 8 threads results improvements 7.27 (corresponding running time reduction 85%) larger conflict sizescases even higher levels parallelization achieved.Results Adding Components Finally, varied problem complexityadding components potentially faulty. Since left numbersize conflicts unchanged, adding components led diagnoses includeddifferent components. limited search depth 4 experiment, fewerdiagnoses found level search trees narrower. result,relative performance gains lower fewer components (constraints).Discussion simulation experiments demonstrate advantages parallelization.tests, speedups LWP FP statistically significant. results alsoconfirm performance gains depend different characteristics underlyingproblem. additional gains waiting end search level workerthreads finished typically led small improvements.Redundant calculations can, however, still occur, particular conflictsnew nodes determined parallel two worker threads return conflict.878fiParallel Model-Based Diagnosis Multi-Core Computers#Cp, #Cf, #D Wt Seq.LWP|Cf|[ms] [ms]S4E4Varying computation times Wt50, 5, 4250232.26 0.5650, 5, 425104832.98 0.7550, 5, 425 100 3,223 2.83 0.71Varying conflict sizes50, 5, 69910 1,672 3.62 0.9150, 5, 921410 3,531 3.80 0.9550, 5, 1227810 4,605 3.83 0.96Varying numbers components50, 10, 920110 3,516 3.79 0.9575, 10, 910510 2,223 3.52 0.88100, 10, 99710 2,419 3.13 0.78#Cp, #Cf, #D Wt Seq.LWPI|Cf|[ms] [ms]S8E8Adding threads (8 instead 4)50, 5, 69910 1,672 6.40 0.8050, 5, 921410 3,531 7.10 0.8950, 5, 1227810 4,605 7.25 0.91FPS4E42.583.102.830.640.770.713.683.833.880.920.960.973.77 0.943.29 0.823.45 0.86FPS8E86.507.157.270.810.890.91Table 14: Simulation results.Although without parallelization computing resources would left unusedanyway, redundant calculations lead overall longer computation times smallproblems thread synchronization overheads.A.2 Additional Experiments Using MXP Conflict Detectionsection report additional results obtained using MergeXplaininstead QuickXplain conflict detection strategy described Section 4.2.different experiments made using set CSPs ontology debugging problems. Remember set experiments goal identify set leadingdiagnoses.A.2.1 Diagnosing Constraint Satisfaction ProblemsTable 15 shows results searching five diagnoses using CSP spreadsheetbenchmarks. MXP could help reduce running times testedscenarios except smaller ones. tiny scenario mknap-1-5, simplesequential algorithm using QXP fastest alternative. scenarios,however, parallelization pays faster sequentially expanding searchtree. best result could achieved scenario costasArray-13, FP usingMXP reduced running times 83% compared sequential algorithm using QXP,879fiJannach, Schmitz, & Shchekotykhincorresponds speedup 6. results indicate FP works wellQXP MXP.Scenarioc8costasArray-13domino-100-100gracefulK3-P2mknap-1-5queens-8hospital paymentprofit calculationcourse planningpreservation modelrevenue calculationSeq.(QXP)[ms]4552,6015352819751,885331,52241148FP(QXP)S4E41.03 0.263.66 0.911.26 0.322.67 0.670.99 0.251.55 0.391.17 0.291.92 0.480.99 0.251.50 0.371.21 0.30Seq.(MXP)[ms]2512,1285041921631,426401,18843042FP(MXP)S4E41.06 0.264.92 1.231.43 0.362.48 0.621.01 0.251.67 0.421.28 0.321.86 0.461.42 0.351.50 0.371.48 0.37Table 15: Results CSP benchmarks spreadsheets (QXP vs MXP).Note one case (costasArray-13) see efficiency value larger one,means obtained speedup super-linear. happen special situationssearch limited number diagnoses use FP method (see alsoSection A.3.1). Assume generating one specific node takes particularly long, i.e.,computation conflict set requires considerable amount time. case,sequential algorithm stuck node time, FP methodcontinue generating nodes. nodes sufficient find (limited)required number diagnoses, lead efficiency value greatertheoretical optimum.A.2.2 Diagnosing Ontologiesresults shown Table 16. Similar previous experiment, using MXPcombination FP pays cases except simple benchmark problems.A.3 Additional Experiments Parallel Depth-First Searchsection, report results additional experiments made assesseffects parallelizing depth-first search strategy described Section 5.3. setexperiments goal find one single minimal diagnosis. report resultsobtained constraint problems ontology debugging problems discussfindings simulation experiment systematically varied problemcharacteristics.A.3.1 Diagnosing Constraint Satisfaction Problemsresults searching single diagnosis CSPs spreadsheets shownTable 17. Again, parallelization generally shows good strategy speed880fiParallel Model-Based Diagnosis Multi-Core ComputersOntologyChemicalKoalaSweet-JPLminiTambisUniversityEconomyTransportationCtonOpengalen-no-propchainsSeq.(QXP)[ms]187155683319711742,145FP(QXP)S4E42.10 0.531.49 0.371.27 0.321.04 0.261.05 0.261.10 0.271.08 0.271.36 0.341.22 0.30Seq.(MXP)[ms]144134562614531541,748FP(MXP)S4E41.94 0.481.27 0.321.05 0.261.08 0.271.02 0.261.00 0.251.10 0.271.33 0.331.35 0.34Table 16: Results Ontologies (QXP vs MXP).diagnosis process. measured speedups except speedup RDFS first scenarioc8 statistically significant. specific problem setting, FP strategymeasurable effect strategies even modest performance deteriorationobserved compared Reiters sequential algorithm. reason lies resultingstructure HS-tree narrow conflicts size one.following detailed observations made comparing algorithms.tested CSPs, FP advantageous compared RDFS PRDFS.spreadsheets, contrast, RDFS PRDFS better breadth-firstapproach FP three five cases.comparing RDFS PRDFS, observe parallelizationadvantageous also depth-first strategies.Again, however, improvements seem depend underlying problem structure. case hospital payment scenario, speedup PRDFS high3.1 compared sequential algorithm, corresponds runtime reduction67%. parallel strategy is, however, consistently bettertest cases.performance Hybrid method lies performancestwo components many, all, tested scenarios.A.3.2 Diagnosing OntologiesNext, evaluated search one diagnosis real-world ontologies (Table 18).tested scenarios, applying depth-first strategy often pay comparedbreadth-first methods. reason tested examples ontology debugging domain many cases single-element diagnoses exist, quicklydetected breadth-first strategy. Furthermore absolute running times often comparably small. Parallelizing depth-first strategy leads significant speedupscases.881fiJannach, Schmitz, & ShchekotykhinScenarioc8costasArray-13domino-100-100gracefulK3-P2mknap-1-5queens-8hospital paymentprofit calculationcourse planningpreservation modelrevenue calculationSeq.[ms]4621,9965737216672263993,072182152FPS4E41.09 0.274.78 1.191.22 0.302.86 0.712.18 0.551.38 0.341.83 0.461.67 0.421.11 0.281.78 0.441.11 0.28RDFS[ms]4543,7294530511455182702,496104121PRDFSS4E40.89 0.223.42 0.851.17 0.292.01 0.501.02 0.261.02 0.262.14 0.541.15 0.290.90 0.230.99 0.250.92 0.23HybridS4E40.92 0.235.90 1.471.05 0.261.89 0.471.35 0.330.95 0.241.72 0.431.10 0.280.87 0.220.95 0.240.90 0.22Table 17: Results CSP benchmarks spreadsheets finding one diagnosis.OntologyChemicalKoalaSweet-JPLminiTambisUniversityEconomyTransportationSeq.[ms]7310358291765FPS4E42.18 0.542.20 0.550.92 0.230.95 0.241.06 0.271.10 0.271.03 0.26RDFS[ms]579462301861PRDFSS4E41.62 0.411.93 0.480.97 0.240.92 0.231.03 0.261.16 0.291.03 0.26HybridS4E41.47 0.371.39 0.350.92 0.230.93 0.231.03 0.261.10 0.270.98 0.24Table 18: Observed performance gains ontologies finding one diagnosis.A.3.3 Systematic Variation Problem CharacteristicsTable 19 finally shows simulation results searching one single diagnosis.experiment used uniform probability distribution selecting componentsconflicts obtain complex diagnosis problems. results summarizedfollows.FP expected better sequential version HS-tree algorithmtested configurations.small problems contain comparably small conflicts,depth-first strategy work well. parallel sequential versionseven slower Reiters original proposal, except cases zero conflictcomputation times assumed. indicates costs hitting set minimization high.larger problem instances, relying depth-first strategy find one singlediagnosis advantageous also better FP. additional test even882fiParallel Model-Based Diagnosis Multi-Core Computers#Cp, #Cf, I|D| WtSeq.I|Cf|[ms][ms]Varying computation times Wt50, 5, 43.4001150, 5, 43.40108950, 5, 43.40 100572Varying conflict sizes50, 5, 62.86109050, 5, 92.36108650, 5, 122.111083Varying numbers components50, 10, 93.471022975, 10, 93.9710570100, 10, 9 4.34101,467conflicts100, 12, 95.0010 26,870FPRDFS[ms]PRDFSS4E4HybridS4E4S4E42.611.501.500.650.370.3721551,0521.011.281.300.250.320.330.852.242.260.210.560.561.571.551.610.390.390.401431381241.261.341.230.310.330.312.122.041.950.530.510.492.363.092.370.590.770.592022282401.351.371.340.340.340.331.651.421.260.410.360.311.280.322801.390.351.240.31Table 19: Simulation results finding one diagnosis.larger problem shown last line Table 19 reveals potential depth-firstsearch approach.problems larger, PRDFS help obtain runtimeimprovements compared RDFS.Hybrid method works well single case zero computation times.Again, represents good choice problem structure known.Overall, simulation experiments show speedups achieveddifferent methods depend underlying problem structure also searchone single diagnosis.ReferencesAbreu, R., & van Gemund, A. J. C. (2009). Low-Cost Approximate Minimal Hitting SetAlgorithm Application Model-Based Diagnosis. SARA09, pp. 29.Anglano, C., & Portinale, L. (1996). Parallel model-based diagnosis using PVM. EuroPVM96, pp. 331334.Autio, K., & Reiter, R. (1998). Structural Abstraction Model-Based Diagnosis.ECAI98, pp. 269273.Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. (2010).Description Logic Handbook: Theory, Implementation Applications, Vol. 32.Bolosky, W. J., & Scott, M. L. (1993). False Sharing Effect Shared MemoryPerformance. SEDMS93, pp. 5771.883fiJannach, Schmitz, & ShchekotykhinBrungger, A., Marzetta, A., Fukuda, K., & Nievergelt, J. (1999). parallel search benchZRAM applications. Annals Operations Research, 90 (0), 4563.Buchanan, B., & Shortliffe, E. (Eds.). (1984). Rule-based Expert Systems: MYCIN Experiments Stanford Heuristic Programming Project. Addison-Wesley, Reading,MA.Burns, E., Lemons, S., Ruml, W., & Zhou, R. (2010). Best-First Heuristic SearchMulticore Machines. Journal Artificial Intelligence Research, 39, 689743.Campeotto, F., Palu, A. D., Dovier, A., Fioretto, F., & Pontelli, E. (2014). ExploringUse GPUs Constraint Solving. PADL14, pp. 152167.Cardoso, N., & Abreu, R. (2013). Distributed Approach Diagnosis Candidate Generation. EPIA13, pp. 175186.Chandra, D., Guo, F., Kim, S., & Solihin, Y. (2005). Predicting Inter-Thread Cache Contention Chip Multi-Processor Architecture. HPCA11, pp. 340351.Chu, G., Schulte, C., & Stuckey, P. J. (2009). Confidence-Based Work Stealing ParallelConstraint Programming. CP09, pp. 226241.Console, L., Friedrich, G., & Dupre, D. T. (1993). Model-Based Diagnosis Meets ErrorDiagnosis Logic Programs. IJCAI93, pp. 14941501.de Kleer, J. (2011). Hitting set algorithms model-based diagnosis. DX11, pp. 100105.Dean, J., & Ghemawat, S. (2008). MapReduce: Simplified Data Processing Large Clusters. Communications ACM, 51 (1), 107113.Dijkstra, E. W. (1968). Structure THE-Multiprogramming System. Communications ACM, 11 (5), 341346.Eiter, T., & Gottlob, G. (1995). Complexity Logic-Based Abduction. JournalACM, 42 (1), 342.Feldman, A., Provan, G., de Kleer, J., Robert, S., & van Gemund, A. (2010a). Solvingmodel-based diagnosis problems max-sat solvers vice versa. DX10, pp.185192.Feldman, A., Provan, G., & van Gemund, A. (2010b). Approximate Model-Based DiagnosisUsing Greedy Stochastic Search. Journal Artifcial Intelligence Research, 38, 371413.Felfernig, A., Friedrich, G., Isak, K., Shchekotykhin, K. M., Teppan, E., & Jannach, D.(2009). Automated debugging recommender user interface descriptions. AppliedIntelligence, 31 (1), 114.Felfernig, A., Friedrich, G., Jannach, D., & Stumptner, M. (2004). Consistency-based diagnosis configuration knowledge bases. Artificial Intelligence, 152 (2), 213234.Felfernig, A., Friedrich, G., Jannach, D., Stumptner, M., & Zanker, M. (2001). Hierarchicaldiagnosis large configurator knowledge bases. KI01, pp. 185197.Felfernig, A., Schubert, M., & Zehentner, C. (2012). efficient diagnosis algorithminconsistent constraint sets. Artificial Intelligence Engineering Design, AnalysisManufacturing, 26 (1), 5362.884fiParallel Model-Based Diagnosis Multi-Core ComputersFelfernig, A., Friedrich, G., Jannach, D., Stumptner, M., et al. (2000). Consistency-baseddiagnosis configuration knowledge bases. ECAI00, pp. 146150.Ferguson, C., & Korf, R. E. (1988). Distributed tree search application alpha-betapruning. AAAI88, pp. 128132.Friedrich, G., & Shchekotykhin, K. M. (2005). General Diagnosis Method Ontologies.ISWC05, pp. 232246.Friedrich, G., Stumptner, M., & Wotawa, F. (1999). Model-Based Diagnosis HardwareDesigns. Artificial Intelligence, 111 (1-2), 339.Friedrich, G., Fugini, M., Mussi, E., Pernici, B., & Tagni, G. (2010). Exception handlingrepair service-based processes. IEEE Transactions Software Engineering, 36 (2),198215.Friedrich, G., & Shchekotykhin, K. (2005). General Diagnosis Method Ontologies.ISWC05, pp. 232246.Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide TheoryNP-Completeness. W. H. Freeman & Co.Grau, B. C., Horrocks, I., Motik, B., Parsia, B., Patel-Schneider, P., & Sattler, U. (2008).OWL 2: next step OWL. Web Semantics: Science, Services AgentsWorld Wide Web, 6 (4), 309322.Greiner, R., Smith, B. A., & Wilkerson, R. W. (1989). Correction AlgorithmReiters Theory Diagnosis. Artificial Intelligence, 41 (1), 7988.Horridge, M., & Bechhofer, S. (2011). OWL API: Java API OWL Ontologies.Semantic Web Journal, 2 (1), 1121.Jannach, D., & Schmitz, T. (2014). Model-based diagnosis spreadsheet programs:constraint-based debugging approach. Automated Software Engineering, February2014 (published online).Jannach, D., Schmitz, T., & Shchekotykhin, K. (2015). Parallelized Hitting Set ComputationModel-Based Diagnosis. AAAI15, pp. 15031510.Junker, U. (2004). QUICKXPLAIN: Preferred Explanations Relaxations OverConstrained Problems. AAAI04, pp. 167172.Kalyanpur, A., Parsia, B., Horridge, M., & Sirin, E. (2007). Finding justificationsowl dl entailments. Semantic Web, Vol. 4825 Lecture Notes ComputerScience, pp. 267280.Korf, R. E., & Schultze, P. (2005). Large-scale parallel breadth-first search. AAAI05,pp. 13801385.Kurtoglu, T., & Feldman, A. (2011). Third International Diagnostic Competition (DXC11). https://sites.google.com/site/dxcompetition2011. Accessed: 2016-03-15.Lecoutre, C., Roussel, O., & van Dongen, M. R. C. (2008). CPAI08 competition. http://www.cril.univ-artois.fr/CPAI08/. Accessed: 2016-03-15.Li, L., & Yunfei, J. (2002). Computing Minimal Hitting Sets Genetic Algorithm.DX02, pp. 14.885fiJannach, Schmitz, & ShchekotykhinMarques-Silva, J., Janota, M., Ignatiev, A., & Morgado, A. (2015). Efficient Model BasedDiagnosis Maximum Satisfiability. IJCAI15, pp. 19661972.Marques-Silva, J., Janota, M., & Belov, A. (2013). Minimal Sets Monotone PredicatesBoolean Formulae. Computer Aided Verification, pp. 592607.Mateis, C., Stumptner, M., Wieland, D., & Wotawa, F. (2000). Model-Based DebuggingJava Programs. AADEBUG00.Mencia, C., & Marques-Silva, J. (2014). Efficient Relaxations Over-constrained CSPs.ICTAI14, pp. 725732.Menca, C., Previti, A., & Marques-Silva, J. (2015). Literal-based MCS extraction.IJCAI15, pp. 19731979.Metodi, A., Stern, R., Kalech, M., & Codish, M. (2014). novel sat-based approachmodel based diagnosis. Journal Artificial Intelligence Research, 51, 377411.Michel, L., See, A., & Van Hentenryck, P. (2007). Parallelizing constraint programs transparently. CP07, pp. 514528.Nica, I., Pill, I., Quaritsch, T., & Wotawa, F. (2013). route success: performancecomparison diagnosis algorithms. IJCAI13, pp. 10391045.Nica, I., & Wotawa, F. (2012). ConDiag - computing minimal diagnoses using constraintsolver. DX12, pp. 185191.Phillips, M., Likhachev, M., & Koenig, S. (2014). PA*SE: Parallel A* Slow Expansions.ICAPS14.Pill, I., Quaritsch, T., & Wotawa, F. (2011). conflicts diagnoses: empiricalevaluation minimal hitting set algorithms. DX11, pp. 203211.Pill, I., & Quaritsch, T. (2012). Optimizations Boolean Approach ComputingMinimal Hitting Sets. ECAI12, pp. 648653.Powley, C., & Korf, R. E. (1991). Single-agent parallel window search. IEEE TransactionsPattern Analysis Machine Intelligence, 13 (5), 466477.Previti, A., Ignatiev, A., Morgado, A., & Marques-Silva, J. (2015). Prime CompilationNon-Clausal Formulae. IJCAI15, pp. 19801987.Prudhomme, C., Fages, J.-G., & Lorca, X. (2015). Choco Documentation. TASC, INRIARennes, LINA CNRS UMR 6241, COSLING S.A.S. http://www.choco-solver.org.Reiter, R. (1987). Theory Diagnosis First Principles. Artificial Intelligence, 32 (1),5795.Rymon, R. (1994). SE-tree-based prime implicant generation algorithm. AnnalsMathematics Artificial Intelligence, 11 (1-4), 351365.Satoh, K., & Uno, T. (2005). Enumerating Minimally Revised Specifications Using Dualization. JSAI05, pp. 182189.Schulte, C., Lagerkvist, M., & Tack, G. (2016). GECODE - open, free, efficient constraintsolving toolkit. http://www.gecode.org. Accessed: 2016-03-15.886fiParallel Model-Based Diagnosis Multi-Core ComputersShchekotykhin, K., Friedrich, G., Fleiss, P., & Rodler, P. (2012). Interactive ontology debugging: Two query strategies efficient fault localization. Journal Web Semantics,1213, 88103.Shchekotykhin, K. M., & Friedrich, G. (2010). Query strategy sequential ontologydebugging. ISWC10, pp. 696712.Shchekotykhin, K., Jannach, D., & Schmitz, T. (2015). MergeXplain: Fast ComputationMultiple Conflicts Diagnosis. IJCAI15, pp. 32213228.Shchekotykhin, K. M., Friedrich, G., Rodler, P., & Fleiss, P. (2014). Sequential diagnosishigh cardinality faults knowledge-bases direct diagnosis generation. ECAI14,pp. 813818.Sirin, E., Parsia, B., Grau, B. C., Kalyanpur, A., & Katz, Y. (2007). Pellet: PracticalOWL-DL Reasoner. Web Semantics: Science, Services Agents World WideWeb, 5 (2), 51 53.Stern, R., Kalech, M., Feldman, A., & Provan, G. (2012). Exploring Duality ConflictDirected Model-Based Diagnosis. AAAI12, pp. 828834.Stuckey, P. J., Feydy, T., Schutt, A., Tack, G., & Fischer, J. (2014). MiniZinc Challenge2008-2013. AI Magazine, 35 (2), 5560.Stumptner, M., & Wotawa, F. (1999). Debugging functional programs. IJCAI99, pp.10741079.Stumptner, M., & Wotawa, F. (2001). Diagnosing Tree-Structured Systems. ArtificialIntelligence, 127 (1), 129.White, J., Benavides, D., Schmidt, D. C., Trinidad, P., Dougherty, B., & Cortes, A. R.(2010). Automated diagnosis feature model configurations. Journal SystemsSoftware, 83 (7), 10941107.Williams, B. C., & Ragno, R. J. (2007). Conflict-directed A* role model-basedembedded systems. Discrete Applied Mathematics, 155 (12), 15621595.Wotawa, F. (2001a). variant Reiters hitting-set algorithm. Information ProcessingLetters, 79 (1), 4551.Wotawa, F. (2001b). Debugging Hardware Designs Using Value-Based Model. AppliedIntelligence, 16 (1), 7192.Wotawa, F., & Pill, I. (2013). classification modeling issues distributed modelbased diagnosis. AI Communications, 26 (1), 133143.887fiJournal Artificial Intelligence Research 55 (2016) 715-742Submitted 10/15; published 3/16Combining Two Three-Way Embedding Models LinkPrediction Knowledge BasesAlberto Garca-Duranalberto.garcia-duran@utc.frSorbonne universites, Universite de technologie de CompiegneCNRS, Heudiasyc UMR 7253CS 60 319, 60 203 Compiegne cedex, FranceAntoine Bordesabordes@fb.comFacebook AI Research770 Broadway, New York, NY 10003. USANicolas Usunierusunier@fb.comFacebook AI Research112 Avenue de Wagram, 75017 Paris, FranceYves Grandvaletyves.grandvalet@utc.frSorbonne universites, Universite de technologie de CompiegneCNRS, Heudiasyc UMR 7253CS 60 319, 60 203 Compiegne cedex, FranceAbstractpaper tackles problem endogenous link prediction knowledge base completion. Knowledge bases represented directed graphs whose nodes correspondentities edges relationships. Previous attempts either consist powerful systemshigh capacity model complex connectivity patterns, unfortunately usuallyend overfitting rare relationships, approaches trade capacity simplicity order fairly model relationships, frequent not. paper, proposeTatec, happy medium obtained complementing high-capacity model simpler one, pre-trained separately combined. present several variantsmodel different kinds regularization combination strategies showapproach outperforms existing methods different types relationships achievingstate-of-the-art results four benchmarks literature.1. IntroductionKnowledge bases (KBs) crucial tools deal rise data, since provideways organize, manage retrieve digital knowledge. repositories coverkind area, specific domains like biological processes, example GeneOntology (Ashburner, Ball, Blake, Botstein, Butler, Cherry, Davis, Dolinski, Dwight, Eppig,et al., 2000), generic purposes. Freebase (Bollacker, Evans, Paritosh, Sturge, &Taylor, 2008), huge collaborative KB belongs Google Knowledge Graph,example latter kind provides expert/common-level knowledge capabilitiesusers. example knowledge engine WolframAlpha (Wolfram Research,2009), engine answers natural language question, like far Saturnsun?, human-readable answers (1,492 109 km) using internal KB.KBs used question answering, also natural language process2016 AI Access Foundation. rights reserved.fiGarca-Duran, Bordes, Usunier & Grandvaleting tasks like word-sense disambiguation (Navigli & Velardi, 2005), co-reference resolution(Ponzetto & Strube, 2006) even machine translation (Knight & Luk, 1994).KBs formalized directed multi-relational graphs, whose nodes correspondentities connected edges encoding various kinds relationship. Hence, onealso refer multi-relational data. following denote connections amongentities via triples facts (head, label, tail ), entities head tail connectedrelationship label. information KB represented via tripleconcatenation several ones. Note multi-relational data present KBsalso recommender systems, nodes would correspond users productsedges different relationships them, social networks instance.main issue KBs far complete. Freebase currentlycontains thousands relationships 80 millions entities, leading billionsfacts, remains small portion human knowledge, obviously. since question answering engines based KBs like WolframAlphacapable generalizing acquired knowledge fill missing facts,de facto limited: search matches question/query internal KBinformation missing provide correct answer, even correctlyinterpreted question. Consequently, huge efforts nowadays devoted towardsKB construction completion (Lao, Mitchell, & Cohen, 2011; Bordes, Glorot, Weston, &Bengio, 2013a; Dong, Gabrilovich, Heitz, Horn, Lao, Murphy, Strohmann, Sun, & Zhang,2014), via manual automatic processes, mix both. mainly divided twotasks: entity creation extraction, consists adding new entities KBlink prediction, attempts add connections entities. paper focuseslatter case. Performing link prediction formalized filling incomplete tripleslike (head, label, ?) (?, label, tail), predicting missing argument tripletriple exist KB, yet. instance, given small example KBFigure 1, made 6 entities 2 different relationships, containing facts like (JaredLeto, influenced by, Bono) (Michael Buble, profession, singer), would likeable predict new links (Frank Sinatra, profession, singer), usingfact influenced singer Michael Buble instance.Link prediction KBs complex due several issues. entities homogeneously connected: lot links entities, whereas othersrarely connected. illustrate diverse characteristics present relationshipstake look FB15k, subset Freebase introduced Bordes, Usunier, GarcaDuran, Weston, Yakhnenko (2013b). data set 14k entities 1k typesrelationships, entities mean number triples 400, median 21 indicatinglarge number appear triples. Besides, roughly 25% connections type 1-to-1, is, head connected one tail, around 25%type Many-to-Many, is, multiple heads linked tail vice versa.result, diverse problems coexist database. Another property relationshipsbig impact performance typing arguments. FB15k,relationships strongly typed like /sports/sports team/location, onealways expects football team head location tail, far less precise/common/webpage/category one expects web page addresses tail716fiEmbedding Models Link Prediction KBsionfesprActorSingerJaredLetoBonoMichaelBubleFrankSinatraFigure 1: Example (incomplete) Knowledge Base 6 entities, 2 relationships7 facts.pretty much everything else head. link prediction algorithm able adaptdifferent settings.Though exists (pseudo-) symbolic approaches link prediction based Markovlogic networks (Kok & Domingos, 2007) random walks (Lao et al., 2011), learning latentfeatures representations KB constituents - so-called embedding methods - recentlyproved efficient performing link prediction KBs, (e.g. Bordes et al., 2013b;Wang, Zhang, Feng, & Chen, 2014b; Lin, Liu, Sun, Liu, & Zhu, 2015; Chang, Yih, Yang,& Meek, 2014; Wang, Zhang, Feng, & Chen, 2014a; Zhang, Salwen, Glass, & Gliozzo,2014; Yang, Duan, Zhou, & Rim, 2014b). works, entities representedlow-dimensional vectors - embeddings - relationships act operators them:embeddings operators define scoring function learned triplesobserved KBs higher scores unobserved ones. embeddings meantcapture underlying features eventually allow create new links successfully.scoring function used predict new links: higher score, likelytriple true. Representations relationships usually specific (except LFM(Jenatton, Le Roux, Bordes, & Obozinski, 2012) sharing parametersacross relationships), embeddings entities shared relationships allowtransfer information across them. learning process considered multi-task,one task concerns relationship, entities shared across tasks.Embedding models classified according interactions use encodevalidity triple scoring function. joint interaction head,label tail used dealing 3-way model; binaryinteractions head tail, head label, labeltail core model, 2-way model. kinds models represententities vectors, differ way model relationships: 3-way modelsgenerally use matrices, whereas 2-way models use vectors. difference capacityleads difference expressiveness models. larger capacity 3-way models(due large number free parameters matrices) beneficial relationships717fiGarca-Duran, Bordes, Usunier & Grandvaletappearing lot triples, detrimental rare ones even regularization applied.Capacity difference 2- 3-way models, information encodedtwo models also different: show Sections 3 5.3.2 kindsmodels assess validity triple using different data patterns.paper introduce Tatec encompass previous works combining wellcontrolled 2-way interactions high-capacity 3-way ones. aim capturing datapatterns approaches separately pre-training embeddings 2-way 3-waymodels using different embedding spaces two them. demonstratefollowing otherwise pre-training and/or use different embeddingspaces features cannot conveniently captured embeddings. Eventually,pre-trained weights combined second stage, leading combination modeloutperforms previous works conditions four benchmarksliterature, UMLS, Kinships, FB15k SVO. Tatec also carefully regularized sincesystematically compared two different regularization schemes: adding penalty termsloss function hard-normalizing embedding vectors constraining norms.paper extension previous work (Garca-Duran, Bordes, & Usunier,2014): added much thorough study regularization combination strategies Tatec. Besides propose experiments several new benchmarkscomplete comparison proposed method w.r.t. state-of-the-art. also give examples predictions projections 2D obtained embeddings provideinsights behavior Tatec. paper organized follows. Section 2 discussesprevious works. Section 3 presents model justifies choices. Detailed explanations training procedure regularization schemes given Section 4.Finally, present experimental results four benchmarks Section 5.2. Related Worksection, discuss state-of-the-art modeling large multi-relational databases,particular focus embedding methods knowledge base completion.One simplest successful 2-way models TransE (Bordes et al., 2013b).model, relationships represented translations embedding space:(h, `, t) holds, embedding tail close embedding headh plus vector depends label `. natural approach modelhierarchical asymmetric relationships, common knowledge basesFreebase. Several modifications TransE proposed recently, TransH (Wanget al., 2014b) TransR (Lin et al., 2015). TransH, embeddings entitiesprojected onto hyperplane depends ` translation. secondalgorithm, TransR, follows idea, except projection operator matrixgeneral orthogonal projection hyperplane. shall seenext section, TransE corresponds Bigram model additional constraintsparameters.2-way models shown good performances KB datasets,limited expressiveness fail dramatically harder datasets.contrast, 3-way models perform form low-rank tensor factorization,respect extremely high expressiveness depending rank constraints.718fiEmbedding Models Link Prediction KBscontext link prediction multi-relational data, RESCAL (Nickel, Tresp, & Kriegel,2011) follows natural modeling assumptions. Similarly TransE, RESCAL learns onelow-dimensional embedding entity. However, relationships representedbilinear operator embedding space RESCAL, i.e. relationship correspondsmatrix, whereas TransE also represented vectors. Besides differentinteractions across arguments triple explain validity, models also differtraining objective. training objective RESCAL Frobenius normoriginal data tensor low-rank reconstruction, whereas Tatec uses marginranking criterion TransE. Another related 3-way model SME(bilinear) (Bordes et al.,2013a). parameterization SME(bilinear) constrained version RESCAL,also uses ranking criterion training objective.Latent Factor Model (LFM) (Jenatton et al., 2012) Neural Tensor Networks(NTN) (Socher, Chen, Manning, & Ng, 2013) use combinations 3-way modelconstrained 2-way model, sense closer algorithm Tatec.important differences algorithms Tatec, though. First, LFMNTN share entity embeddings 2-way 3-way models, learn differententity embeddings. use different embeddings 2-way 3-way modelsincrease model expressiveness, equivalent combinationshared embeddings higher dimensional embedding space, additional constrainsrelation parameters. show experiments however, additionalconstraints lead significant improvements. second main differenceapproach LFM parameters relationships 2-way3-way interaction terms also shared, case Tatec. Indeed, jointparameterization might reduce expressiveness 2-way interaction terms which,argue Section 3.3, left maximum degrees freedom. Lastly, LFM seeksmaximize likelihood function given set positive negative facts. NTNgeneral parameterization LFM, still uses entity embeddings2-way 3-way interaction terms. Also, NTN two layers non-linearityfirst layer, model add nonlinearity embedding step.order precise overview differences approaches, giveSection 3 (Table 1) formulas scoring functions related works.works ignore type-constraints present relationships (i.e.entities legitimate arguments given relationship), approaches presentextensions making use side information. Hence, Chang et al. (2014) proposemodification RESCAL avoid incompatible entity-relation triples participateloss function Krompass, Baier, Tresp (2015) propose similar frameworkmodels optimized iterating small batches. Krompass et al. (2015) also proposelocal closed-world assumption approximates type-information availabletriples without requiring kind side information KB. Thoughconsider use side information, type-constraints could easily introducedmodel either taking KB (when available), using approachKrompass et al. (2015).lot focus recently algorithms purely based learningembeddings entities and/or relationships, many earlier alternatives proposed.discuss works carried Bayesian clustering framework, well approaches719fiGarca-Duran, Bordes, Usunier & GrandvaletFigure 2: Example RDF file Freebaseexplicitly use graph structure data. Infinite Relational Model (Kemp,Tenenbaum, Griffiths, Yamada, & Ueda, 2006), nonparametric extensionStochastic Block Model (Wang & Wong, 1987), Bayesian clustering approach learnsclusters entities kind, i.e. groups entities similar relationshipsentities. work followed Sutskever, Salakhutdinov, Tenenbaum(2009), propose 3-way tensor factorization model based Bayesian clusteringentities within cluster share distribution embeddings.Symbolic approaches, aforementioned Kok Domingos (2007),also worth mentioning context. Though rule-based inference new links maylead great expressiveness, usually limited quality coveragehandcrafted rules. Still, Path Ranking Algorithm (PRA) (Lao et al., 2011) presentedmodel able discover rules automatically performing random walks trainingdata, limitation connectivity nodes; i.e. shortenough path connecting two nodes, model able infer relationthem. Recently, Gardner, Talukdar, Krishnamurthy, Mitchell (2014) cover gapcombining model pre-trained embeddings. PRA used KnowledgeVault project (Dong et al., 2014) conjunction embedding approach. Thus, eventhough consider symbolic approaches here, could also combinedembedding model.Even though present evaluate algorithms context knowledge bases,work also applies broader context RDF data. RDF standard modeldata interchange Web. core Linked Data initiative (Bizer, Heath,Idehen, & Berners-Lee, 2008) aims extend linking structure Web useURIs name relationship things well two ends link.linking structure forms directed, labeled graph, edges represent named linktwo objects. Thus data essentially made triples. RDF-terminologytriple defined (subject, predicate, object). Freebase dump availableformat. Figure 2 shows example RDF file Freebase, m.02mjmr identifier resource representing Barack Obama. identifier several predicatesns:influence.influence node.influenced ns:people.person.religion, whoseobjects ns:m.01d1n (Reinhold Niebuhr) ns:m.01lp8 (Christianity), respectively.3. TATECdescribe model motivations underlying parameterization. dataset relations entities fixed set entities E = {e1 , ..., eE }. Relationsrepresented triples (h, `, t) head h tail indexes entities (i.e.720fiEmbedding Models Link Prediction KBsh, [[E]] = {1, ..., E}), label ` index relationship L = {l1 , ..., },defines type relation entities eh et .3.1 Scoring Functiongoal learn discriminant scoring function set possible triples E LEtriples represent likely relations receive higher scores triplesrepresent unlikely ones. proposed model, Tatec, learns embeddings entitieslow dimensional vector space, say Rd , parameters operators Rd Rd ,operators associated single relationship. precisely, score givenTatec triple (h, `, t), denoted s(h, `, t), defined as:s(h, `, t) = s1 (h, `, t) + s2 (h, `, t)(1)s1 s2 following form:(B) Bigram 2-way interaction term:fi fffi fffi fi ffs1 (h, `, t) = r`1 fieh1 + r`2 fiet1 + eh1 fiDfiet1 ,(2)eh1 , et1 embeddings Rd1 head tail entities (h, `, t) respectively,r`1 r`2 vectors Rd1 depend relationship `, diagonalmatrix depend input triple.fifffifi ff throughout section, . . canonical dot product,ffnotationfi figeneralfififix = x Ay x two vectors spacesquare matrix appropriate dimensions.use two different relation vectors subject object order modelasymmetric relationships; instance, r1` = r2` , (Paris, capital of, France)would score (France, capital of, Paris).(T) Trigram 3-way interaction term:fi fi ffs2 (h, `, t) = eh2 fiR` fiet2 ,(3)R` matrix dimensions (d2 , d2 ), eh2 et2 embeddings Rd2head tail entities respectively. embeddings entities term2-way term; even different dimensions.embedding dimensions d1 d2 hyperparameters model. vectorsmatrices learned without additional parameter sharing.2-way interaction term model similar Bordes et al. (2013a),slightly general contain constraint relation-dependentvectors r`1 r`2 . also seen relaxation translation model Bordeset al. (2013b), special case r`1 = r`2 , identity matrix,entity embeddings constrained lie unit sphere.3-way term corresponds exactly model used collective factorizationmethod RESCAL (Nickel et al., 2011), chose high expressivenesscomplex relationships. Indeed, said earlier introduction, 3-way models721fiGarca-Duran, Bordes, Usunier & Grandvaletbasically represent kind interaction among entities. combination 2- 3way terms already used Jenatton et al. (2012), Socher et al. (2013), but, besidesdifferent parameterization, Tatec contrasts additional freedom broughtusing different embeddings two interaction terms. LFM (Jenatton et al., 2012),constraints imposed relation-dependent matrix 3-way terms (low ranklimited basis rank-one matrices), relation vectors r`1 r`2 constrainedimage matrix (D = 0 work). global constraints severelylimited expressiveness 3-way model, act stringent regularizationreduces expressiveness 2-way model, which, explain Section 3.3,left maximum degrees freedom. similar NTN (Socher et al., 2013)respect share parameter relations. overall scoringfunction similar model single layer, fundamental differenceuse different embedding spaces use non-linear transfer function,results facilitated training (for instance, gradients larger magnitude). Table1 details scoring function aforementioned models.3.2 Term Combinationstudy two strategies combining bigram trigram scores indicated Equation (1). cases, s1 s2 first trained separately detail Section 4combined. difference two strategies depends whetherjointly update (or fine-tune) parameters s1 s2 second phase not.3.2.1 Fine Tuningfirst strategy, denoted Tatec-ft, simply consists summing scores followingEquation (1).fi fffi fffi fi fffi fi ffsF (h, `, t) = r`1 fieh1 + r`2 fiet1 + eh1 fiDfiet1 + eh2 fiR` fiet2parameters s1 s2 (and hence s) fine-tuned second training phaseaccommodate combination. version could trained directly withoutpre-training s1 s2 separately show experiments detrimental.3.2.2 Linear Combinationsecond strategy combines bigram trigram terms using linear combination,without jointly fine-tuning parameters remain unchanged pre-training.score hence defined follows:fi fffi fffi fi fffi fi ffsLC (h, `, t) = 1` r`1 fieh1 + 2` r`2 fiet1 + 3` eh1 fiDfiet1 + 4` eh2 fiR` fiet2combination weights i` depend relationship learned optimizingranking loss (defined later (6)) using L-BFGS, additional quadratic penalizationP ||` ||22term, ` P, ` contains combination weights relation `, con` +strained ` ` = ( hyperparameter). version Tatec denoted Tatec-lcfollowing.722fiEmbedding Models Link Prediction KBsh-th relationt-th entityl-th entityFigure 3: entry (h,l,t) tensor indicates relation l holds entitiesh t.3.3 Interpretation Motivation Modelsection discusses motivations underlying parameterization Tatec,particular choice 2-way model complement 3-way term.3.3.1 2-Way Interactions Fiber Biasesfirst motivation 2-way 3-way model, use analogymatrix factorization. common matrix factorization techniques collaborativefiltering add biases (also called offsets intercepts) model. instance, criticalstep best-performing techniques Netflix prize add user item biases,i.e. approximate user-rating Rui according (see e.g. Koren, Bell, & Volinsky, 2009):fi ffRui Pu fiQi + u + +(4)P RU k , row Pu containing k-dimensional embedding user(U number users), Q RIk containing embeddings items, u Rbias depending user R bias depending item ( constantconsider on).2-way + 3-way interaction model propose seen 3-mode tensor version biased version matrix factorization: trigram term (T) collectivematrix factorization parameterizationoffiff RESCAL algorithm (Nickel et al., 2011)fiplays role analogous term Pu Qi matrix factorization model collaborative filtering (4).bigram term (B) plays role biases fiber tensor,1 i.e.123s1 (h, `, t) Bl,h+ Bl,t+ Bh,t(5)thus analogue tensors term u + matrix factorization model(4). exact form s1 (h, `, t) given (B) corresponds specific form collective1. Fibers higher order analogue matrix rows columns tensors defined fixingevery index one.723fiGarca-Duran, Bordes, Usunier & Grandvaleth1factorization fiber-wise bias matrices B1 = Bl,hl[[L]],h[[E]], B2 B3 Equation(5). exactly learn one bias fiber many fibers littledata, while, argue following, specific form collective factorizationpropose (B) allow share relevant information different biases. Notewhereas tensor dimensions n n (in general, problem setentities considered head tail) 2mn + n2 biases, Tatec computesbiases means linear combinations n + 2m embeddings, allows learningtransfer across them.3.3.2 Need Multiple Embeddingskey feature Tatec use different embedding spaces 2-way 3-way terms,existing approaches types interactions use embeddingspace (Jenatton et al., 2012; Socher et al., 2013). motivate choice section.important notice biases matrix factorization model (4), bigram term overall scoring function (1) affect model expressiveness,particular affect main modeling assumption embeddings lowrank. user/item-biases (4) boil adding two rank-1 matrices 1T1 factorization model. Since rank matrix hyperparameter, one maysimply add 2 hyperparameter get slightly larger expressiveness before,reasonably little impact since increase rank would remain small comparedoriginal value (which usually 50 100 large collaborative filtering data sets).critical feature biases collaborative filtering interfere capacitycontrol terms rank, namely 2-norm regularization: instance, Korenet al. (2009) adjustterms (4) using squared error measure fit regularization term kPu k22 + kQi k22 + u2 + i2 , > 0 regularization parameter.kind regularization weighted trace norm regularization PQT (Salakhutdinov& Srebro, 2010).Leaving asideweighted part, idea convergence,PP22quantitykQi k2 equal 2 times sum singular valuesu kPu k2 +matrix PQT . However, kk22 , regularization applied userbiases,2 times singular value rank-one matrix 1 , equal Ikk2 ,much larger kk22 . Thus, pattern user+item biases exists data,weakly hidden stronger factors, less regularized othersmodel able capture it. Biases, allowed fit datafactors, offer opportunity relaxing control capacity partsmodel translates gains patterns capture indeed usefulpatterns generalization. Otherwise, ends relaxing capacity leadoverfitting.fi fffi ffbigram terms closely related trigram term: terms r`1 fieh1 r`2 fiet1fi fi ffto trigram term adding constant features entities embeddings,addedeh1 fiDfiet1 directly appropriate quadratic form. Thus, way gainaddition bigram terms ensure capture useful patterns,also capacity control terms less strict trigram terms. tensorfactorization models, especially 3-way interaction models parameterizations724fiEmbedding Models Link Prediction KBs(T), capacity control regularization individual parameters still wellunderstood, sometimes turns detrimental effective experiments.effective parameter admissible rank embeddings, leadsconclusion bigram term really useful addition trigram termhigher-dimensional embeddings used. Hence, absence clear concrete wayeffectively controlling capacity trigram term, believe different embeddingspaces used.3.3.3 2-Way Interactions Entity Types+Similaritypart model expressive, less regularized (see Subsection4.2) part useful patterns learn meaningfulprediction task hand. section, give motivation 2-way interactionterm task modeling multi-relational data.relationships multi-relational data, knowledge bases like FB15k particular, strongly typed, sense well-defined specific subsets entities either heads tails selected relationships. instance, relationship likecapital expects (big) city head country tail valid relation. Largeknowledge bases huge amounts entities, belong many different types.Identifying expected types head tail entities relationships, appropriategranularity types (e.g. person artist writer), likely filterff 95%fi ffthefientity set prediction. exact form first two terms r`1 fieh1 + r`2 fiet12-way interaction model (B), corresponds low-rank factorization perbias matrices (head, label) (tail, label) head tail entitiesembeddings, based assumption types entities predicted based(learned) features, features predicting head-typespredicting tail-types. such, natural share entities embeddings first twoterms (B).fi fi fflast term, eh1 fiDfiet1 , intended account global similarity entities.instance, capital France easily predicted looking citystrongest overall connections France knowledge base. country city maystrongly linked geographical positions, independent respectivetypes. diagonal matrix allows re-weight features embedding spaceaccount fact features used describe types maydescribe similarity objects different types. use diagonalmatrix strictly equivalent using general symmetric matrix place D.2reason using symmetric matrix comes intuition direction manyrelationships arbitrary (i.e. choice triples Paris capital Francerather France capital Paris), model invariant arbitraryinversions directions relationships (in case inversion direction,relations vectors r`1 r`2 swapped, parameters unaffected).2. see equivalence taking eigenvalue decomposition symmetricchangefi applyfi D:ffbasis embeddings keep diagonal part term eh1 fiDfiet1 , applyreverse transformation vectors r`1 r`2 . Note since rotations preserve Euclidean distances,equivalence still holds 2-norm regularization embeddings.725fiGarca-Duran, Bordes, Usunier & GrandvaletTable 1: Scoring function several models literature. Capitalized lettersdenote matrices lower cased ones, vectors.ModelTransETransHTransRRESCALLFMScore (s(h, `, t))h`` fi h ||eff + r ` e ||t 2` fi ` ff 2h`fifi||(e we fiw ff) + r(e fi wff e w )||2hfi`tfi|| e M`+firfi ffe M` ||2hfi `fieRfi ` fi 0 ffh fi ` fi ffefi ` fi ffh fi ` fi fffi R fi + e fi R fi z + zfi R fi e + e fi R fi etasks invariance desirable, diagonal matrix could replacedarbitrary matrix.4. TrainingTraining Tatec carried using stochastic gradient descent objective functioncomprising data-fitting term regularization term. two terms decribeddetails section.4.1 Ranking Objectiveuse ranking objective function, designed give higher scores positivetriples (facts express true verified information KB) negativeones (facts supposed express false information). negative triplesprovided KB, often not, need process turn positive triplescorrupted ones carry discriminative training. simple approach consistscreating negative examples replacing one argument positive triple randomelement. way simple efficient practice may introduce noise creatingwrong negatives.Let set positive triples provided KB, optimize following rankingloss function:XXs(h, `, t) + s(h0 , `0 , t0 ) +(6)(h,`,t)S (h0 ,`0 ,t0 )C(h,`,t)[z]+ = max(z, 0) C(h, `, t) set corrupted triples. Dependingapplication, set defined 3 different ways:1. C(h, `, t) = {(h0 , `0 , t0 ) [[E]] L [[E]]|h0 6= h `0 6= ` t0 6= t}2. C(h, `, t) = {(h0 , `, t0 ) [[E]] L [[E]]|h0 6= h t0 6= t}3. C(h, `, t) = {(h, `0 , t) [[E]] L [[E]]|`0 6= `}margin hyperparameter defines minimum gap scorepositive triple negative ones. stochastic gradient descent performedminibatch setting. epoch data set shuffled split disjoint minibatchestriples 1 2 (see next section) negative triples created every positive one.use two different learning rates 1 2 , one Bigram one Trigrammodel; kept fixed whole training.726fiEmbedding Models Link Prediction KBsAlgorithm 1 Learning unregularized Tatec.input Training set = {(h, l, t)}, margin , learning rates 1 21: initialization2:- Bigram: e1 uniform( 6d , 6d ) entity e113:r1 , r2 uniform( 6d , 6d ) `114:uniform( 6d , 6d )115:- Trigram: e2 uniform( 6d , 6d ) entity e226:R uniform( 6d , 6d ) `227:- Tatec-ft: pre-trained weights Bigram Trigram8: embeddings normalized 2- Frobenius-norm equal 1.9: loop10:Sbatch sample(S, m) // sample training minibatch size11:Tbatch // initialize set pairs examples12:(h, `, t) Sbatch13:(h0 , `0 , t0 ) sampleaccording selected strategy C(h, `, t)negative triple14:Tbatch Tbatch (h, `, t), (h0 , `0 , t0 ) // record pairs examples15:endX16:Update parameters using gradientss(h, `, t) + s(h0 , `0 , t0 ) + :(h,`,t),(h0 ,`0 ,t0 ) Tbatch17:18:19:20: end- Bigram (Eq. 2): = s1- Trigram (Eq. 3): = s2- Tatec-ft (Eq. 1): = s1 + s2loopinterested Bigram Trigram terms Tatec capture differentdata patterns, using random initialization weights may lead bad local minimathus poor solution. Hence, first pre-train separately s1 (h, `, t) s2 (h, `, t),use learned weights initialize full model. Training Tatechence carried two phases: (disjoint) pre-training either (joint) fine-tuningTatec-ft learning combination weights Tatec-lc. pre-trainingfine-tuning stopped using early stopping validation set, follow trainingprocedure summarized Algorithm 1, unregularized case. Traininglinear combination weights Tatec-lc stopped convergence L-BFGS.4.2 RegularizationPrevious work embedding models used two different regularization strategies: eitherconstraining entity embeddings have, most, 2-norm value e (Garca-Duranet al., 2014) adding 2-norm penalty weights (Wang et al., 2014b; Lin et al.,2015) objective function (6). former, denote hard regularization,regularization performed projecting entity embeddings minibatch onto2-norm ball radius e . latter, denote soft regularization, penalization term form [||e||22 2e ]+ entity embeddings e added. soft schemeallows 2-norm embeddings grow e , penalty.control large capacity relation matrices Trigram model,adapted two regularization schemes: hard scheme, force relation matrices727fiGarca-Duran, Bordes, Usunier & GrandvaletTable 2: Statistics data sets used paper extracted four knowledgebases: FB15k, SVO, Kinships UMLS.Data setEntitiesRelationshipsTraining examplesValidation examplesTest examplesFB15k14,9511,345483,14250,00059,071SVO30,6054,5471,000,00050,000250,000Kinships10426224,97328,12228,121UMLS13549102,61289,30289,302have, most, Frobenius norm value l , soft one, include penalizationterm form [||R||2F 2l ]+ loss function (6) . result, soft schemefollowing regularizationterm added loss function (6): C1 [||e1 ||22 2e ]+ +C2 [||e2 ||222e ]+ + [||R||2F 2l ]+ , C1 C2 hyperparameters weight importancesoft constraint. terms practicality, bigger flexibility soft version comesone hyperparameter. following, suffixes soft hard used refereither regularization scheme. Tatec also implicit regularizationfactor since using entity representation entity regardless rolehead tail.sum up, hard regularization case, optimization problem Tatec-ft is:XXmins(h, `, t) + s(h0 , `0 , t0 ) +(h,`,t)S (h0 ,`0 ,t0 )C(h,`,t)s.t.||ei1 ||2 e[[E]]||ei2 ||2 e`||R ||F l[[E]]` [[L]]soft regularization case is:XXXmin[ s(h, `, t) + s(h0 , `0 , t0 )]+ + C1[||ei1 ||22 2e ]+(h,`,t)S (h0 ,`0 ,t0 )C(h,`,t)+ C2X[||ei2 ||22 2e ]+ +i[[E]]i[[E]]X[||R` ||2F 2l ]+`[[L]]fi fffi fffi fi fffi fi ffs(h, `, t) = r`1 fieh1 + r`2 fiet1 + eh1 fiDfiet1 + eh2 fiR` fiet2 cases.5. Experimentssection presents various experiments illustrate competitive Tatec respect several state-of-the-art models 4 benchmarks literature: UMLS, Kinships, FB15k SVO. statistics data sets given Table 2. versionsTatec components Bigram Trigram compared state-ofthe-art models database.5.1 Experimental Settingsection details protocols used various experiments.728fiEmbedding Models Link Prediction KBs5.1.1 Datasets Metricsexperimental settings evaluation metrics borrowed previous works,allow result comparisons.UMLS/Kinships Kinships (Denham, 1973) KB expressing relational structurekinship system Australian tribe Alyawarra, UMLS (McCray, 2003)KB biomedical high-level concepts like diseases symptoms connected verbs likecomplicates, affects causes. data sets, whole set possible triples,positive negative, observed. used area precision-recall curve metric.dataset split 10-folds cross-validation: 8 training, 1 validationlast one test. Since number available negative triples much biggernumber positive triples, positive ones fold replicated match numbernegative ones.3 negative triples correspond first setting negative examplesSection 4.1. number training epochs fixed 100. Bigram, TrigramTatec models validated every 10 epochs using AUC precision-recallcurve validation criterion 1,000 randomly chosen validation triples - keepingproportion negative positive triples. TransE, ran baseline,validated every 10 epochs well.FB15k Introduced Bordes et al. (2013b), data set subset Freebase,large database generic facts gathering 1.2 billion triples 80 million entities.evaluation it, used ranking metric. head test triple replacedentities dictionary turn, score computed them.scores sorted descending order rank correct entity stored.procedure repeated removing tail instead head. meanranks mean rank, proportion correct entities ranked top 10hits@10. called raw setting. setting correct positive triples rankedhigher target one hence counted errors. order reduce noisemeasure, thus granting clearer view ranking performance, removepositive triples found either training, validation testing set, excepttarget one, ranking. setting called filtered (Garca-Duran et al., 2014).Since FB15k made positive triples, negative ones generated.that, epoch generate two negative triples per positive replacing singleunit positive triple random entity (once head tail).corruption approach implements prior knowledge unobserved triples likelyinvalid, widely used previous work learning embeddings knowledgebases words context language models. negative triples correspondsecond setting negative examples Section 4.1. ran 500 training epochsTransE, Bigram, Trigram Tatec, using final filtered mean rankvalidation criterion. several models statistically similar filtered mean ranks, takehits@10 secondary validation criterion.4 Since dataset, training, validation3. replication process carried training.4. Garca-Duran et al. (2014) previously reported results FB15k SVO TransETatec. However, preliminary work, hyperparameters validated smaller validationset wide enough grid search, led suboptimal results. hence decided re-runalgorithms got major improvements.729fiGarca-Duran, Bordes, Usunier & Grandvalettest sets fixed, give confidence interval results, randomly splittest set 4 subsets computing evaluation metrics. 5 times,finally compute mean standard deviation 20 values mean rankhits@10.SVO SVO database nouns connected verbs subject-verb-direct objectrelations extracted Wikipedia articles. introduced Jenatton et al.(2012). database perform verb prediction task, one assigncorrect verb given two nouns acting subject direct object; words,present results ranking label given head tail. FB15k, two ranking metricscomputed, mean rank hits@5%, proportion predictionscorrect verb ranked top 5% total number verbs, withintop 5% 4,547 227. use raw setting SVO. Due different kindtask (predicting label instead predicting head /tail ), negative triplesgenerated replacing label random verb. negative triples correspondthird setting negative examples Section 4.1. TransE, Bigram Trigramnumber epochs fixed 500 validated every 10 epochs.Tatec ran 10 epochs, validated each. mean rank chosenvalidation criterion 1,000 random validation triples.5.1.2 Implementationpre-train Bigram Trigram models validated learning ratestochastic gradient descent among {0.1, 0.01, 0.001, 0.0001} margin among {0.1, 0.25,0.5, 1}. radius e determining value L2 -norm entity embeddings penalized fixed 1, radius l Trigram modelvalidated among {0, 1, 5, 10, 20}. Due different size KBs, embedding dimension validated different ranges. SVO selectedamong {25, 50}, among {50, 75, 100} FB15k among {10, 20, 40} UMLSKinships. soft regularization applied, regularization parametervalidated among {0, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100}. fine-tuning Tatec, learningrates selected among values learning Bigram Trigram modelsisolation, independent values chosen pre-training, marginpenalization terms C1 C2 soft regularization used. configurationsmodel selected using performance validation set given Appendix A.Training combination weights Tatec-lc carried iterative way,alternating optimization parameters via L-BFGS, update parameters using`` = P||||||k2|| , stopping criterion reached. parameters initializedk21 value validated among {0.1, 1, 10, 50, 100, 200, 500, 1000}.5.1.3 BaselinesVariants performed breakdown experiments 2 different versions Tatecassess impact various aspects. variants are:Tatec-ft-no-pretrain: Tatec-ft without pre-training s1 (h, l, t) s2 (h, l, t).730fiEmbedding Models Link Prediction KBsTable 3: Test AUC precision-recall curve UMLS Kinshipsmodels literature (top) Tatec (bottom). Best performing methods bold.ModelSME(linear)RESCALLFMTransE-softTransE-hardBigram-hardTrigram-hardTatec-ft-hardBigram-softTrigram-softTatec-ft-softTatec-lc-softUMLS0.983 0.0030.980.990 0.0030.734 0.0330.706 0.0340.936 0.0200.980 0.0060.984 0.0040.936 0.0180.983 0.0040.985 0.0040.985 0.004Kinships0.907 0.0080.950.946 0.0050.135 0.0050.134 0.0050.140 0.0040.943 0.0090.876 0.0120.141 0.0030.948 0.0080.919 0.0080.941 0.009Tatec-ft-shared: Tatec-ft sharing entities embeddings s1 (h, l, t)s2 (h, l, t) without pre-training.experiments 3 versions Tatec performed soft regularization setting. hyperparameters chosen using grid above.Previous Models retrained TransE hyperparameter gridTatec used running baseline datasets, using either soft hardregularization. addition, display results best performing methodsliterature dataset, values extracted original papers.UMLS Kinships, also report performance 3-way models RESCAL,LFM 2-way SME(linear). FB15k, recent variants TransE, TransH,TransR cTransR (Lin et al., 2015) chosen main baselines.TransH TransR/cTransR, optimal values hyperparameters dimension, margin learning rate selected within similar rangesTatec. SVO, compare Tatec three different approaches: Counts,2-way model SME(linear) 3-way LFM. Counts based direct estimationprobabilities triples (head, label, tail) using number occurrences pairs (head,label) (label, tail) training set. results models extracted(Jenatton et al., 2012), followed experimental setting. Since resultspaper available raw setting, restricted experimentsconfiguration SVO well.5.2 Resultsrecall suffixes soft hard refer regularization scheme used, suffixesft lc combination strategy Tatec.731fiGarca-Duran, Bordes, Usunier & GrandvaletTable 4: Test results FB15k SVO models literature (top), Tatec(middle) variants (bottom). Best performing methods bold. filtered setting usedFB15k raw setting SVO.ModelCountsSME(linear)LFMTransHTransRcTransRTransE-softTransE-hardTatec-no-pretrainTatec-sharedBigram-hardTrigram-hardTatec-ft-hardBigram-softTrigram-softTatec-ft-softTatec-lc-softFB15kMean RankHits@108764.47768.77570.250.7 2.071.5 0.350.6 2.071.5 0.397.1 3.965.7 0.294.8 3.263.4 0.394.5 2.967.5 0.4137.7 7.156.1 0.459.8 2.677.3 0.387.7 4.170.0 0.2121.0 7.258.0 0.357.8 2.376.7 0.368.5 3.272.8 0.2SVOMean Rank Hits@5%517.472199.67719578282.5 1.770.6 0.2282.8 2.370.6 0.2219.2 1.977.6 0.1187.9 1.279.5 0.1188.5 1.979.8 0.1211.9 1.877.8 0.1189.2 2.179.5 0.2185.4 1.580.0 0.1182.6 1.280.1 0.15.2.1 UMLS Kinshipsresults two knowledge bases provided Table 3. UMLS, modelsperforming well. combination Bigram Trigram models slightlybetter Trigram alone significant. seems constituentsTatec, Bigram Trigram, encode complementary informationcombination bring much improvement. Basically, dataset, manymethods somewhat efficient best one, LFM. difference TransEBigram dataset illustrates potential impact diagonal matrix D,constrain embeddings head tail entities triple similar.Regarding Kinships, big gap 2-way models like TransE 3way models like RESCAL. cause deterioration comes peculiaritypositive triples KB: entity appears 104 times number entitiesKB head connected 104 entities even once.words, conditional probabilities P (head|tail) P (tail|head) totally uninformative.important consequence 2-way models since highly relyinformation: Kinships, interaction head-tail is, best, irrelevant, though practiceinteraction may even introduce noise.Due poor performance Bigram model, combinedTrigram model combination turn detrimental w.r.t. performanceTrigram isolation: 2-way models quite noisy KB cannot takeadvantage them. side Trigram model logically reaches similar732fiEmbedding Models Link Prediction KBsTable 5: Test results FB15k. Proportion entities ranked Top 1.ModelTransE-softBigram-softTrigram-softTatec-ft-softHits@128.127.224.937.8performance RESCAL, similar LFM well. Performance Tatec versionsbased fine-tuning parameters (Tatec-ft) worse TrigramBigram degrades model. Tatec-lc, using potentially sparse linear combinationmodels, drawback since completely cancel influencebigram model. conclusion experiments KB, onecomponents Tatec quite noisy, directly remove Tatec-lcautomatically. soft regularization setting seems slightly better also.5.2.2 FB15kTable 4 (left) displays results FB15k. Unlike Kinships, 2-way modelsoutperform 3-way models mean rank hits@10. simplicity 2-waymodels seems advantage FB15k: something already observedYang, Yih, He, Gao, Deng (2014a). combination Bigram Trigrammodels Tatec leads impressive improvement performance, meansKB information encoded 2 models complementary. Tatecoutperforms existing methods except TransE mean rank wide marginhits@10. Bigram-soft performs roughly like cTransR, better counterpartBigram-hard. Though Trigram-soft better Trigram-hard well, Tatec-ft-softTatec-ft-hard converge similar performances. Fine-tuning parametersbetter simply using linear combination even Tatec-lc still performs well.Tatec-ft outperforms variants Tatec-shared Tatec-no-pretrainwide margin, confirms pre-training use different embeddingsspaces essential properly collect different data patterns Bigram Trigram models: sharing embeddings constrain much model, withoutpre-training Tatec able encode complementary information constituents.performance Tatec cases in-between performances soft versionBigram Trigram models, indicates converge solutioneven able reach best performance constituent models. Table 5displays Hits@1 several models. Whereas differences performanceBigram Trigram large ones shown hits@10, Tatec stillbest model wide margin.also broke results type relation, classifying relationship accordingcardinality head tail arguments. relationship considered 1-to-1,1-to-M, M-to-1 M-M regarding variety arguments head given tail vice versa.average number different heads whole set unique pairs (label, tail) givenrelationship 1.5 considered 1, way around.number relations classified 1-to-1, 1-to-M, M-to-1 M-M 353, 305, 380733fiGarca-Duran, Bordes, Usunier & GrandvaletTable 6: Detailed results category relationship. compare Bigrams,Trigram Tatec models terms Hits@10 (in %) FB15k filtered settingmodels literature. (M. stands Many).TaskRel. categoryTransE-softTransHTransRcTransRBigram-softTrigram-softTatec-ft-soft1-to-176.266.878.881.576.256.479.3Predicting head1-to-M. M.-to-1 M.-to-M.93.647.570.287.628.764.589.234.169.28934.771.290.337.470.179.630.25793.242.377.21-to-176.765.579.280.875.953.178.5Predicting tail1-to-M. M.-to-1 M.-to-M.50.993.172.939.883.367.237.490.472.138.690.173.844.489.872.828.881.660.851.592.780.7307, respectively. results displayed Table 6. Bigram Trigram modelscooperate constructive way types relationship predictinghead tail. Tatec-ft remarkably better M-to-M relationships.5.2.3 SVOTatec achieves also good performance task since outperforms previousmethods metrics. before, regularization strategies lead similarperformances, soft setting slightly better. terms hits@5%, Tatec outperforms constituents, however terms mean rank Bigram model considerablyworse Trigram Tatec. performance LFM TrigramBigram models, confirms fact sharing embeddings 2-3-way terms actually prevent make best use types interaction.Kinships, since performance Bigram much worseTrigram, Tatec-lc competitive. seems Bigram Trigramperform well different types relationships (such FB15k), combiningvia fine-tuning (i.e. Tatec-ft) allows get best both; however, oneconsistently performing worse relationships seems happen KinshipsSVO, Tatec-lc good choice since cancel influence badmodel. Table 7 depicts training times various models FB15k, presenting relativetime w.r.t. TransE one training epoch. speedup training, could follow oneseveral following strategies:use adaptive learning rates order make convergence faster;train model GPUs, quite usual deep learning communityworking large datasets;parallellize training Hogwild (Recht, Re, Wright, & Niu, 2011).could also speed validation. example, since entities relationshipsusually strongly typed (i.e. given relationship, subset entities real candidatessubject object), might consider entities suitable typegiven relationship role. Nevertheless, given scalability major issuedatasets used paper look speed optimization here.734fiEmbedding Models Link Prediction KBsTable 7: Relative training times respect TransE FB15k running oneepoch single core.ModelBigram-softTrigram-softTatec-ft-softRelative train. time1.43.64.05.3 Illustrative Experimentslast experimental section provides illustrations insights performanceTatec TransE.5.3.1 TransE Symmetrical RelationshipsTransE peculiar behavior: performs well FB15k quite poorlydatasets. Looking detail FB15k, noticed databasemade lot pairs symmetrical relationships /film/film/subjects/film/film subject/films, /music/album/genre /music/genre/albums.simplicity translation model TransE works well when, predicting validityunknown triple, model make use symmetrical counterpartpresent training set. Specifically, 45,817 59,071 test triples FB15ksymmetrical triple training set. split test triples two subsets, onecontaining test triples symmetrical triple used learningstage containing ones symmetrical triple existtraining set, overall mean rank TransE 50.7 decomposed meanrank 17.5 165.7, overall hits@10 71.5 decomposed 76.6 53.7,respectively. TransE makes adequate use particular feature. originalTransE paper (Bordes et al., 2013b), algorithm shown perform well FB15kdataset extracted KB WordNet (Miller, 1995): suspectWordNet dataset also contains symmetrical counterparts test triples training set(such hyperonym vs hyponym, meronym vs holonym).Tatec also make use information is, expected, much better relationssymmetrical counterparts train: FB15k, mean rank Tatec-ft-soft17.5 relations symmetrical counterparts 197.4 instead hits@10 84.4%instead 50%. Yet, results datasets show, Tatec also able generalizecomplex information needs taken account.5.3.2 Anecdotal Examplesexamples predictions Tatec FB15k displayed Table 8. firstrow, want know answer question location polishnational football team?; among possible answers find locations,specifically countries, makes sense national team. questiontopic film Remember titans? top-10 candidates may potential film topics. answers question religion Noam Chomskybelong to? typed religions. examples, sides re735fiGarca-Duran, Bordes, Usunier & GrandvaletTable 8: Examples predictions FB15k. Given entity relation typetest triple, Tatec fills missing slot. bold expected correct answer.Triple(poland national football team, /sports team/location, ?)(?, /film/film subject/films , remember titans)(noam chomsky, /people/person/religion, ?)(?, /webpage/category, official website)Top-10 predictionsMexico, South Africa, Republic PolandBelgium, Puerto Rico, Austria, GeorgiaUruguay, Colombia, Hong Kongracism, vietnam war, aviation, capital punishmenttelevision, filmmaking, Christmasfemale, english language, korean waratheism, agnosticism, catholicism, ashkenazi jewsbuddhism, islam, protestantismbaptist, episcopal church, Hinduismsupreme court canada, butch hartman, robyn hitchcoc, mercer universityclancy brown, dana delany, hornetsgrambling state university, dnipropetrovsk, juanes60SingersJapanese singersBritish MPNSGlee castingAttorneys USA40SingersJapanese singersBritish MPNSGlee castingAttorneys USA6040202000202040406060402002040608040(a) Embeddings Trigram30201001020304050(b) Embeddings BigramFigure 4: Embeddings obtained Trigram Bigram models projected2-D using t-SNE. MPNS stands Main Profession Singer.lationship clearly typed: certain type entity expected head tail (country,religion, person, movie, etc.). operators Tatec may operate specific regionsembedding space. contrary, relationship /webpage/category example non-typed relationship. one, could actually seen attribute ratherrelationship, indicates entity head topic website official website.Since many types entities webpage little correlation amongrelationships, predicting left-hand side argument nearly impossible.Figures 4a 4b show 2D projections embeddings selected entities Trigram Bigram models trained FB15k, respectively, obtained projectingusing t-SNE (Van der Maaten & Hinton, 2008). projection carriedFreebase entities whose profession either singer attorney USA.observe Figure 4a attorneys clustered separated singers, exceptone, corresponds multifaceted Fred Thompson5 . However, embeddingssingers clearly clustered: since singers appear multitude triples,layout result compendium (sometimes heterogeneous) categories. illustrategraphically different data patterns Bigram Trigram respond, focussmall cluster made Japanese singers seen Figure 4a (Trigram).Figure 4b (Bigram) however, entities diluted whole set5. Apart attorney, actor, radio personality, lawyer politician736fiEmbedding Models Link Prediction KBsTable 9: Examples predictions SVO. Given two nouns acting subject directobject test triple, Tatec predicts best fitting verb. bold expected correct answer.Triple(bus, ?, service)(emigrant, ?, country)(minister, ?, protest)(vessel, ?, coal)(tv channel, ?, video)(great britain, ?, north america)Top-10 predictionsuse, provide, run, have, includecarry, offer, enter, make, takeflee, become, enter, leave, formdominate, establish, make, move, joinlead, organize, join, involve, makeparticipate, conduct, stag, begin, attenduse, transport, carry, convert, sendmake, provide, supply, sell, containfeature, make, release, use, producehave, include, call, base, showinclude, become, found, establish, dominatename, have, enter, form, runsingers. Looking neighboring embeddings Japanese singers entities Figure4b, find entities highly connected japan like yoko ono born Japan, vic mignogna,greg ayres, chris patton laura bailey worked dubbing industryJapanese anime movies television series. shows impact interactionheads tails Bigram model: tends push together entities connectedtriples whatever relation. case, forms Japanese cluster.Table 9 shows examples predictions SVO. first example, though runtarget verb pair (bus, service), verbs like provide offer goodmatches well. Similarly, non-target verbs like establish join, lead, participateattend good matches second third examples ((emigrant, country)(minister, protest)) respectively. fourth fifth instances show exampleheterogeneous performance relationship (the target verb transportcases) easily explained semantic point view: transportgood fit given pair (vessel, coal), whereas TV channel transports videonatural way express one watch videos TV channel, henceleads poor performance target verb ranked #696. sixth exampleparticularly interesting, since even target verb, colonize, ranked farlist (#344), good candidates pair (Great Britain, North America) foundtop-10. similar representation colonize,almost synonyms, ranked much higher. effect verb frequency.illustrated Figure 5a, frequent relationship is, higher Frobeniusnorm is; hence, verbs similar meanings unbalanced frequencies rankeddifferently, explains rare verb, colonize, ranked much worsesemantically similar words. consequence relation Frobeniusnorm appearance frequency usual verbs tend highly ranked even thoughsometimes good matches, due influence norm score.figure, see Frobenius norm relation matrices largerregularized (soft) case unregularized case. happens fixedlarge value C2 l regularized case (e fixed 1). imposesstrong constraint norm entities relationship matricesmakes Frobenius norm matrices absorb whole impact norm737fiGarca-Duran, Bordes, Usunier & Grandvalet2505000Regularized (soft)UnregularizedRegularized (hard)4000Filtered Mean Rank200||Rl||2F1501005030002000100000 0101210103104101000 010510110210310410510Number triplesNumber triples(a) Frobenius norm rel. matrices according (b) Test mean rank according numbertraining triples relationship.number training triples rel.Figure 5: Indicators behavior Tatec-ft FB15k according numbertraining triples relationship.Table 10: Examples predictions SVO regularized unregularizedTrigram. bold expected correct answer.Triple(bus, ?, service)(emigrant, ?, country)(minister, ?, protest)(vessel, ?, coal)(tv channel, ?, video)(great britain, ?, north america)Top-10 predictionsUnregularizedRegularized (soft)use, operate, offer, call, build,provide, use, have, include, make,include, have, know, make, createoffer, take, carry, serve, runuse, represent, save, flee, visit,flee, become, come, enter, found,come, make, leave, create, knowinclude, form, make, leave, joinbring, lead, reach, have, become, lead, organize, conduct, participate, joinsay, include, help, leave, appointmake, involve, support, suppress, raisetake, use, have, carry, make,use, transport, make, carry, deliver,hold, move, become, fill, servesend, contain, supply, leave, providemake, include, write, know, have,release, make, feature, produce, have,produce, use, play, give, becomeinclude, use, take, show, basehave, use, include, make, leave,include, found, become, run,name,become, know, take, call, buildmove, annex, form, establish, dominatescore, and, thus, impact verb frequency. could down-weight importanceverb frequency tuning parameters l C2 enforce stronger constraint.Figure 10 shows effect verb frequency two models predictingmissing verb Table 9.Breaking performance relationship, translated strong relationperformance relationship frequency (see Figure 5b). However,relation 2-norm entities embeddings frequencyobserved, explained given entity appear left rightargument unbalanced way.6. Conclusionpaper presents Tatec, tensor factorization method satisfactorily combines 2and 3-way interaction terms obtain performance better best either constituent. Different data patterns properly encoded thanks use different embedding spaces two-phase training (pre-training fine-tuning/linear-combination).Experiments four benchmarks different tasks different quality measures738fiEmbedding Models Link Prediction KBsprove strength versatility model, whose scoring function, argue Section 3.1, tightly connected energy-based model literature TransE,RESCAL LFM. experiments also allow us draw conclusionstwo usual regularization schemes used far embedding-based models:achieve similar performances, even soft regularization appears slightly efficientone extra-hyperparameter.work uses FB15k main testbed model tied KB schema;long data formatted triples, Tatec applied. Hence, could directlyapplied many Linked Data settings especially RDF format.Acknowledgmentswork carried framework Labex MS2T (ANR-11-IDEX-0004-02),funded French National Agency Research (EVEREST-12-JS02-005-01).Part work done Nicolas Usunier Sorbonne universites, Universitede technologie de Compiegne, CNRS, Heudiasyc UMR 7253.Appendix A. Optimal Hyperparametersoptimal configurations UMLS are:- TransE-soft: = 40, = 0.01, = 0.5, C = 0;- Bigram-soft: d1 = 40, 1 = 0.01, = 0.5, C = 0.1;- Trigram-soft: d2 = 40, 2 = 0.01, = 1, C = 0.1, l = 5;- Tatec-soft: d1 = 40, d2 = 40, 1 = 2 = 0.001, = 1, C1 = C2 = 0.01, l = 5;- TransE-hard: = 40, = 0.01, = 0.1;- Bigram-hard: d1 = 40, 1 = 0.01, = 0.5;- Trigram-hard: d2 = 40, 2 = 0.01, = 1, l = 10;- Tatec-hard: d1 = 40, d2 = 40, 1 = 2 = 0.001, = 1, l = 10.- Tatec-linear-comb: d1 = 40, d2 = 40, = 0.5, = 50.optimal configurations Kinships are:- TransE-soft: = 40, = 0.01, = 1, C = 0;- Bigram-soft: d1 = 40, 1 = 0.01, = 1, C = 1;- Trigram-soft: d2 = 40, 2 = 0.01, = 0.5, C = 0.1, l = 5;- Tatec-soft: d1 = 40, d2 = 40, 1 = 2 = 0.001, = 1, C1 = 100, C2 = 0.0001, l = 10;- TransE-hard: = 40, = 0.01, = 1;- Bigram-hard: d1 = 40, 1 = 0.01, = 1;- Trigram-hard: d2 = 40, 2 = 0.01, = 0.5, l = 10;- Tatec-hard d1 = 40, d2 = 40, 1 = 2 = 0.001, = 1, l = 10.- Tatec-linear-comb: d1 = 40, d2 = 40, = 1, = 10.optimal configurations FB15k are:- TransE-soft: = 100, = 0.01, = 0.25, C = 0.1;- Bigram-soft: d1 = 100, 1 = 0.01, = 1, C = 0;739fiGarca-Duran, Bordes, Usunier & Grandvalet-Trigram-soft: d2 = 50, 2 = 0.01, = 0.25, C = 0.001, l = 1;Tatec-soft: d1 = 100, d2 = 50, 1 = 2 = 0.001, = 0.5, C1 = C2 = 0;TransE-hard: = 100, = 0.01, = 0.25;Bigram-hard: d1 = 100, 1 = 0.01, = 0.25;Trigram-hard: d2 = 50, 2 = 0.01, = 0.25, l = 5;Tatec-hard: d1 = 100, d2 = 50, 1 = 2 = 0.001, = 0.25, l = 5;Tatec-no-pret: d1 = 100, d2 = 50, 1 = 2 = 0.01, = 0.25, C1 = 0, C2 = 0.001, l = 1;Tatec-shared: d1 = d2 = 75, 1 = 2 = 0.01, = 0.25, C1 = C2 = 0.001, l = 5;Tatec-linear-comb: d1 = 100, d2 = 50, = 0.25, = 200.optimal configurations SVO are:- TransE-soft: = 50, = 0.01, = 0.5, C = 1;- Bigram-soft: d1 = 50, 1 = 0.01, = 1, C = 0.1;- Trigram-soft: d2 = 50, 2 = 0.01, = 1, , C = 10, l = 20;- Tatec-soft: d1 = 50, d2 = 50, 1 = 2 = 0.0001, = 1, C1 = 0.1, C2 = 1, l = 20;- TransE-hard: = 50, = 0.01, = 0.5;- Bigram-hard: d1 = 50, 1 = 0.01, = 1;- Trigram-hard: d2 = 50, 2 = 0.01, = 1, l = 20;- Tatec-hard: d1 = 50, d2 = 50, 1 = 2 = 0.0001, = 1, l = 20.- Tatec-linear-comb: d1 = 50, d2 = 50, = 1, = 50.ReferencesAshburner, M., Ball, C. A., Blake, J. A., Botstein, D., Butler, H., Cherry, J. M., Davis,A. P., Dolinski, K., Dwight, S. S., Eppig, J. T., et al. (2000). Gene ontology: toolunification biology. http://geneontology.org/.Bizer, C., Heath, T., Idehen, K., & Berners-Lee, T. (2008). Linked data web(ldow2008). http://linkeddata.org/.Bollacker, K., Evans, C., Paritosh, P., Sturge, T., & Taylor, J. (2008). Freebase:collaboratively created graph database structuring human knowledge. http://www.freebase.com.Bordes, A., Glorot, X., Weston, J., & Bengio, Y. (2013a). semantic matching energyfunction learning multi-relational data. Machine Learning, 94, 233259.Bordes, A., Usunier, N., Garca-Duran, A., Weston, J., & Yakhnenko, O. (2013b). Translating embeddings modeling multi-relational data. Advances Neural InformationProcessing Systems, pp. 27872795.Chang, K.-W., Yih, W.-t., Yang, B., & Meek, C. (2014). Typed tensor decompositionknowledge bases relation extraction. Proceedings 2014 ConferenceEmpirical Methods Natural Language Processing (EMNLP), pp. 15681579.Denham, W. (1973). detection patterns Alyawarra nonverbal behavior. Ph.D.thesis, University Washington.Dong, X., Gabrilovich, E., Heitz, G., Horn, W., Lao, N., Murphy, K., Strohmann, T., Sun,S., & Zhang, W. (2014). Knowledge vault: web-scale approach probabilistic740fiEmbedding Models Link Prediction KBsknowledge fusion. Proceedings 20th ACM SIGKDD international conferenceKnowledge discovery data mining, pp. 601610. ACM.Garca-Duran, A., Bordes, A., & Usunier, N. (2014). Effective blending two threeway interactions modeling multi-relational data. ECML PKDD 2014. SpringerBerlin Heidelberg.Gardner, M., Talukdar, P. P., Krishnamurthy, J., & Mitchell, T. (2014). Incorporatingvector space similarity random walk inference knowledge bases. ConferenceEmpirical Methods Natural Language Processing, EMNLP 2014, pp. 397406.Jenatton, R., Le Roux, N., Bordes, A., & Obozinski, G. (2012). latent factor modelhighly multi-relational data. NIPS 25.Kemp, C., Tenenbaum, J. B., Griffiths, T. L., Yamada, T., & Ueda, N. (2006). Learningsystems concepts infinite relational model. Proc. 21st nationalconf. Artif. Intel. (AAAI), pp. 381388.Knight, K., & Luk, S. K. (1994). Building large-scale knowledge base machine translation. AAAI, Vol. 94, pp. 773778.Kok, S., & Domingos, P. (2007). Statistical predicate invention. Proceedings 24thinternational conference Machine learning, ICML 07, pp. 433440.Koren, Y., Bell, R., & Volinsky, C. (2009). Matrix factorization techniques recommendersystems. Computer, 42 (8), 3037.Krompass, D., Baier, S., & Tresp, V. (2015). Type-constrained representation learningknowledge graphs. arXiv preprint arXiv:1508.02593.Lao, N., Mitchell, T., & Cohen, W. W. (2011). Random walk inference learninglarge scale knowledge base. Proceedings Conference Empirical MethodsNatural Language Processing, pp. 529539. Association Computational Linguistics.Lin, Y., Liu, Z., Sun, M., Liu, Y., & Zhu, X. (2015). Learning entity relation embeddingsknowledge graph completion. Proceedings AAAI15.McCray, A. T. (2003). upper level ontology biomedical domain. ComparativeFunctional Genomics, 4, 8088.Miller, G. (1995). WordNet: Lexical Database English. Communications ACM,38 (11), 3941.Navigli, R., & Velardi, P. (2005). Structural semantic interconnections: knowledge-basedapproach word sense disambiguation. Pattern Analysis Machine Intelligence,IEEE Transactions on, 27 (7), 10751086.Nickel, M., Tresp, V., & Kriegel, H.-P. (2011). three-way model collective learningmulti-relational data. Proceedings 28th International Conference MachineLearning (ICML-11), pp. 809816.Ponzetto, S. P., & Strube, M. (2006). Exploiting semantic role labeling, wordnetwikipedia coreference resolution. Proceedings main conference HumanLanguage Technology Conference North American Chapter AssociationComputational Linguistics, pp. 192199. Association Computational Linguistics.741fiGarca-Duran, Bordes, Usunier & GrandvaletRecht, B., Re, C., Wright, S., & Niu, F. (2011). Hogwild: lock-free approach parallelizing stochastic gradient descent. Advances Neural Information ProcessingSystems 24, pp. 693701.Salakhutdinov, R., & Srebro, N. (2010). Collaborative filtering non-uniform world:Learning weighted trace norm. tc (X), 10, 2.Socher, R., Chen, D., Manning, C. D., & Ng, A. Y. (2013). Reasoning Neural Tensor Networks Knowledge Base Completion. Advances Neural InformationProcessing Systems 26.Sutskever, I., Salakhutdinov, R., & Tenenbaum, J. (2009). Modelling relational data usingbayesian clustered tensor factorization. Adv. Neur. Inf. Proc. Syst. 22.Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-sne. Journal MachineLearning Research, 9 (2579-2605), 85.Wang, Y. J., & Wong, G. Y. (1987). Stochastic blockmodels directed graphs. JournalAmerican Statistical Association, 82 (397).Wang, Z., Zhang, J., Feng, J., & Chen, Z. (2014a). Knowledge graph text jointlyembedding. Proceedings 2014 Conference Empirical Methods NaturalLanguage Processing (EMNLP).Wang, Z., Zhang, J., Feng, J., & Chen, Z. (2014b). Knowledge graph embedding translating hyperplanes. Proceedings Twenty-Eighth AAAI ConferenceArtificial Intelligence, pp. 11121119.Wolfram Research, I. (2009). Wolfram alpha. http://www.wolframalpha.com.Yang, B., Yih, W.-t., He, X., Gao, J., & Deng, L. (2014a). Learning multi-relational semantics using neural-embedding models. CoRR, abs/1411.4072.Yang, M.-C., Duan, N., Zhou, M., & Rim, H.-C. (2014b). Joint relational embeddingsknowledge-based question answering. Proceedings 2014 ConferenceEmpirical Methods Natural Language Processing (EMNLP), pp. 645650.Zhang, J., Salwen, J., Glass, M., & Gliozzo, A. (2014). Word semantic representations usingbayesian probabilistic tensor factorization. Proceedings 2014 ConferenceEmpirical Methods Natural Language Processing (EMNLP).742fiJournal Artificial Intelligence Research 55 (2016) 11351178Submitted 12/15; published 04/16Exploiting Causality Selective Belief FilteringDynamic Bayesian NetworksStefano V. Albrechtsvalb@cs.utexas.eduDepartment Computer ScienceUniversity Texas AustinAustin, TX 78712, USASubramanian Ramamoorthys.ramamoorthy@ed.ac.ukSchool InformaticsUniversity EdinburghEdinburgh, EH8 9AB, UKAbstractDynamic Bayesian networks (DBNs) general model stochastic processespartially observed states. Belief filtering DBNs task inferring belief state (i.e.probability distribution process states) based incomplete noisy observations.hard problem complex processes large state spaces. article,explore idea accelerating filtering task automatically exploiting causalityprocess. consider specific type causal relation, called passivity, pertainsstate variables cause changes variables. present Passivity-basedSelective Belief Filtering (PSBF) method, maintains factored belief representationexploits passivity perform selective updates belief factors. PSBF producesexact belief states certain assumptions approximate belief states otherwise,approximation error bounded degree uncertainty process. showempirically, synthetic processes varying sizes degrees passivity, PSBFfaster several alternative methods achieving competitive accuracy. Furthermore,demonstrate passivity occurs naturally complex system multi-robotwarehouse, PSBF exploit accelerate filtering task.1. IntroductionDynamic Bayesian networks (DBNs) (Dean & Kanazawa, 1989) general modelstochastic processes partially observed states. topology DBN compactspecification variables process interact transitions (cf. Figure 1). Givenpossible incompleteness noise observations, may generally possibleinfer state process absolute certainty. Instead, may infer beliefsprocess state based history observations, form probability distributionstate space process. often called belief state task calculatingbelief states commonly referred belief filtering.number exact approximate inference methods exist Bayesian networks (see,e.g., Koller & Friedman, 2009; Pearl, 1988) used filtering DBNs,applying unrolled DBN + 1 slice repeated observedtime step, via successive update current posterior (belief state) usedc2016AI Access Foundation. rights reserved.fiAlbrecht & Ramamoorthyxt1xt+11y1t+1xt2xt+12y2t+1t+1Figure 1: Example dynamic Bayesian network (DBN) two state variables twoobservation variables. xti xt+1variables represent process states time+ 1, respectively, yit+1 variables (shaded) represent observation time + 1.arrows describe variables interact.prior next time step (see also Murphy, 2002). However, clearunrolled variant becomes intractable network grows unboundedly time. Evensuccessive update, exact methods become intractable high-dimensional processstates approximate methods may propagate growing errors time. Therefore, filteringmethods developed utilise special structure DBNs maintain errorspropagated time. (We defer detailed discussion methods Section 2.)Often, key developing efficient filtering methods identify structureprocess leveraged inference. article, interested applicationDBNs representations actions partially observed decision processes,POMDPs (Kaelbling, Littman, & Cassandra, 1998; Sondik, 1971) many variants.DBNs used represent effects actions decision process, specifyingvariables interact information decision maker observes. many cases,decision processes exhibit high degrees causal structure (Pearl, 2000), meanchange one part process may cause change another part. experienceprocesses causal structure may used make filtering tasktractable, tell us beliefs need revised certain aspectsprocess state. example, variable x2 Figure 1 changes value variable x1changed value (i.e. change x1 causes change x2 ), seems intuitive usecausal relation deciding whether revise ones belief x2 . Unfortunately,current filtering methods take causal structure account.refer type causal relation (between x1 x2 ) passivity. Intuitively,say state variable xi passive given action if, executing action,subset state variables directly affect xi (i.e. xi parents DBN)xi may change value least one variables subset changedvalue. worth pointing passivity occurs naturally frequently manyplanning domains, especially robotic physical systems (Mainzer, 2010).following example1 illustrates simple robot arm:1. mark end example solid black square.1136fiExploiting Causality Selective Belief Filtering DBNs32213XABXB1(a) Robot arm gripperC(b) Holding blocks BFigure 2: Robot arm three rotational joints gripper. variables representabsolute orientations corresponding joints.Example 1 (Robot arm). Consider robot arm three rotational joints gripper,shown Figure 2a. joints denoted 1 , 2 , 3 may take valuesdiscrete set {0 , 1 , ..., 359 } indicate absolute orientations (e.g. = 0 meansjoint points exactly right, = 180 means points left).joint i, let two actions CWi CCWi rotate joint 1 clockwisecounter-clockwise, respectively. uncertainty system could due stochasticjoint movements unreliable sensor readings joint orientations.action CWi CCWi , variable passive value directlymodified action. However, variables j6=i passive changevalues corresponding preceding variable j1 changed value, since changedorientation joint j 1 causes changed orientation joint j (recall orientationsabsolute). Note also accounts chains causal effects, indicatedarrows: orientation joint 3 changes orientation joint 1 changes, since joint1 causes joint 2 change, turn causes joint 3 change.examples passivity seen context object manipulation,blocks planning domain (e.g. Pasula, Zettlemoyer, & Kaelbling, 2007). Figure 2bshows arm holding blocks B A, top B. Here, position B (XB )passive respect joint orientations since change orientationschanged. Furthermore, causal chain joint orientations positionblock (XA ), since position change Bs position changes.passivity exploited accelerate filtering task example?fact state variables passive means aspects state may remainunchanged, depending action choose. example, choose rotate joint3, fact joints 1 2 passive means unaffected action.Thus, seems redundant revise beliefs orientations joints 1 2. However,precisely current filtering methods (cf. Section 2).concretely, assume use factored belief representation P (1 , 2 , 3 ) = P (1 , 2 )P (2 , 3 ) choose rotate 3 direction. Then, easy see needupdate factor P (2 , 3 ), since 3 changes value, factor P (1 , 2 ), sincevariables 1 , 2 passive. Since parents 1 , 2 (if any) changevalues, know 1 , 2 change values either. show later, skipping1137fiAlbrecht & RamamoorthyP (1 , 2 ) result loss information cases, similarly chainscausal connections (cf. Example 1). complex example planning domaininvolving passivity, exploited, discussed Section 6.2.addition guiding belief revision, several features make passivityinteresting example causal relation: First all, passivity latent causal relation,meaning readily extracted process dynamics without additionalannotation expert. (In Section 4, give procedure identifies passive variablesbased conditional probability tables.) Furthermore, passivity deterministicrelation since passive variables may stochastic behaviour changingvalues. Finally, passivity relatively simple example causal relation, ideaexploiting passivity order accelerate filtering task intuitive. Yet, bestknowledge, formalised explored rigorously before.purpose present article formalise evaluate idea automaticallyexploiting causal structure efficient belief filtering DBNs, using passivity concreteexample causal relation. Specifically, hypothesis large processeshigh degrees passivity, structure exploited accelerate filtering task.discussing related work Section 2 technical preliminaries Section 3,contributions grouped following parts:Section 4, give formally concise definition passivity discuss variousaspects definition. definition assumes decision process specifiedset dynamic Bayesian networks (one action). also discuss nonexample passivity, mean variables appear passive reallypassive. Finally, give simple procedure detect passive variablesbased conditional probability tables.Section 5, present Passivity-based Selective Belief Filtering (PSBF) method.Following idea outlined above, PSBF uses factored belief representationbelief factors defined clusters correlated state variables. PSBF follows2-step update procedure wherein belief state first propagatedprocess dynamics (the transition step) conditioned observation (theobservation step). interesting novelty PSBF way performstransition step: rather updating belief factors, PSBF updatesfactors whose variables suspects changed, possible exploitingpassivity (to made precise shortly). Similarly, observation step, PSBF updatesbelief factors determines structurally connectedobservation, uses parts observation relevantbelief factor, thus allowing efficient incorporation observations. PSBFproduces exact belief states certain assumptions approximate belief statesotherwise. also discuss computational complexity error bounds PSBF.Section 6, evaluate PSBF two experimental domains: first evaluate PSBFsynthetic (i.e. randomly generated) processes varying sizes degrees passivity.process sizes vary one thousand one trillion states, passivitydegrees vary 25% 100% passivity. results show PSBF fasterseveral alternative methods maintaining competitive accuracy. particular,1138fiExploiting Causality Selective Belief Filtering DBNsresults indicate computational gains grow significantly degreepassivity size process. evaluate PSBF complex simulationmulti-robot warehouse system style Kiva (Wurman, DAndrea, & Mountz,2008). show passivity occurs system PSBF exploitaccelerate filtering task, outperforming alternative methods.Finally, discuss strengths weaknesses PSBF Section 7, concludework Section 8. proofs found appendix.2. Related Workexists substantial body work belief filtering partially observed stochasticprocesses. section, review filtering methods utilise special structureDBNs situate work within related literature.2.1 Approximate Belief Filtering DBNsSeveral authors proposed filtering methods wherein belief state represented setstate samples. Specifically, probability process state normalisedfrequency state samples correspond s. methods commonlyreferred particle filters (PF); see work Doucet, de Freitas, Gordon (2001)survey. common variant PF (Gordon, Salmond, & Smith, 1993), filteringtask consists propagating current state samples process dynamicssubsequent resampling step based probabilities new state sampleswould produced observation. Two interesting features PFapplied processes discrete continuous variables, approximationerror converges zero increase number state samples.known problem PF fact number samples needed acceptableapproximations grow drastically variance process dynamics (as shownexperiments; cf. Section 6). Rao-Blackwellised PF (RBPF) (Doucet, De Freitas,Murphy, & Russell, 2000) developed address problem. RBPF assumesstate variables grouped sets R X distribution Xefficiently calculated R filtering. Hence, sample RBPF consistssample R corresponding marginal distribution X. RBPF usefulvariance R relatively low variance X high, since reduces numbersamples needed acceptable approximations.Boyen Koller (1999, 1998) recognised process consists several independentweakly interacting subcomponents, belief state represented efficientlyproduct smaller beliefs individual subcomponents. seminal contribution show approximation error due factored representation essentiallybounded degree uncertainty (or mixing rates) process. precisely,prove relative entropy (or KL divergence; Kullback & Leibler, 1951) two belief states contracts exponential rate propagated stochastic transitionprocess. Based observation, propose filtering method (BK) wherein beliefstate represented factored form belief factors updated using exact inference method, junction tree algorithm (Lauritzen & Spiegelhalter, 1988). Since1139fiAlbrecht & Ramamoorthyinternal cliques used junction tree algorithm may correspond beliefstate representation BK, final projection step typically performedoriginal factorisation restored. performance method depends crucially whether relevant correlations state variables captured smallclusters, whether projection step performed efficiently.Factored particle filtering (FP) (Ng, Peshkin, & Pfeffer, 2002) addresses main drawbacks PF (many samples needed) BK (small clusters required) approximatingbelief factors using set factored state samples. samples factored senseassign values variables corresponding factor. allows FPrepresent belief factors large BK, reduces number samplesneeded due smaller number variables factor. authors provide different methods updating factored state samples, generic idea first performjoin operation full state samples reconstructed factored samples,updated standard PF. updated samples projectedfactored form using project operation. main drawback FP joinproject operations essentially correspond standard relational database operations,expensive.Murphy Weiss (2001) propose filtering method called factored frontier (FF). FFuses fully factored representation belief states; is, belief state productmarginals individual state variable. allows compact representationbeliefs. algorithm works moving set state variables (the frontier) forwardbackward DBN topology. requires certain variable ordering,difficult attain intra-correlations state variables (i.e. edges within + 1slice DBN) allowed. authors show method equivalent singleiteration loopy belief propagation (LBP) (Pearl, 1988). Thus, similar LBP, FFapplied successive iterations improve approximation accuracy.None works discussed explicitly address question causal relationsstate variables exploited accelerate filtering task, or, alternatively,filtering methods proposed therein implicitly benefit causal structure. method,PSBF, related BK FP PSBF, too, uses factored belief representation,belief factors defined clusters correlated state variables. Therefore,analysis approximation errors Boyen Koller (1998) also applies PSBF,show Section 5 well experiments. However, contrast BK FP, PSBFperform inference complete factorisation, rather individualfactors. consequence, PSBF require join project operation, onemain disadvantages BK FP.2.2 Belief Filtering Decision Processesmethods discussed preceding subsection used belief filtering decisionprocesses, including POMDPs (Kaelbling et al., 1998; Sondik, 1971). regard,methods viewed pure filters concerned belief filteringcontrol decision process. contrast combined filteringmethods, interleave filtering control tasks decision processes makespecific assumptions regarding solutions thereof. exists large body literature1140fiExploiting Causality Selective Belief Filtering DBNscombined methods, including reachability-based methods (Hauskrecht, 2000; Washington,1997), grid-based methods (Zhou & Hansen, 2001; Brafman, 1997; Lovejoy, 1991), pointbased methods (Smith & Simmons, 2005; Pineau, Gordon, & Thrun, 2003), compressionmethods (Roy, Gordon, & Thrun, 2005; Poupart & Boutilier, 2002).potential advantage combined methods access additionalstructure may, therefore, utilise synergies filtering control tasks. Onesynergy use decision quality guide belief filtering, rather metricsrelative entropy. Poupart Boutilier (2001, 2000) propose filtering method, calledvalue-directed approximation, chooses different approximation schemes differentdecisions minimise expected loss decision quality (i.e. accumulated rewards).method assumes POMDP solved exactly value functionprovided form -vectors represent available actions POMDP.Based value function, algorithm computes switching set alternativeplans determine error bounds approximation schemes. used searchoptimal approximation scheme tree-based manner, search traversesapproximate exact schemes.idea using decision quality guide belief filtering appealing, methodinvolves series optimisation problems exhaustive tree search,costly complex systems. advantage pure filtering methods, including proposedmethod PSBF, filter processes complex combined methods,multi-robot warehouse system studied Section 6. actual control taskdone via domain-specific solutions (cf. Section 6.2.1).2.3 Substructure ParameterisationBayesian networks, hence DBNs, allow compact parameterisation (i.e. specificationprobabilities) efficient inference via conditional independence relations. addition,considerable work identifying substructure parameterisationsimplify knowledge acquisition enhance inference (Koller & Friedman, 2009;Boutilier, Dean, & Hanks, 1999). property studied work, passivity, one examplesubstructure parameterisation. notable examples include causal independence (e.g. Heckerman & Breese, 1994; Heckerman, 1993) context-specific independence(Boutilier, Friedman, Goldszmidt, & Koller, 1996).Causal independence assumption effects individual causes commonvariable (i.e. parents variable) independent one another. allowscompact parameterisation via operators noisy-or (Srinivas, 1993; Pearl, 1988),used enhance inference (Zhang & Poole, 1996). Note passivityconceptually much simpler property causal independence, passivity neitherconcerned strength individual causes extent dependother. Moreover, passivity read directly parameterisation (cf. Section 4.3)whereas causal independence usually imposed designer.Context-specific independence (CSI) property states variable independent parents given certain assignment values (i.e. context)parents. Non-local CSI statements follow similarly d-separation (Geiger, Verma,& Pearl, 1989). allow reduction parameters (Boutilier et al., 1996)1141fiAlbrecht & Ramamoorthyenhancement inference (Poole & Zhang, 2003). discuss Section 4, passivity viewed special kind CSI applied DBNs, parents respectvariable passive provide context CSI. However, contrast CSI,passivity assume context actually observed.3. Technical Preliminariessection introduces basic concepts notation used work. beginbrief discussion decision processes provide context work, followeddiscussion dynamic Bayesian networks model perform inference.3.1 Decision Processes, Belief States, Exact Updatesconsider stochastic decision process wherein, time t, process statest decision maker, agent, choosing action . executing st ,process transitions state st+1 probability (st , st+1 ) agent receivesobservation ot+1 probability (st+1 , ot+1 ). assume factored representationsstate space observation space O, = X1 ... Xn = Y1 ... Ym ,domains Xi , Yj finite. notation si used denote value Xistate S, analogously oj O. Moreover, assume processtime-invariant, meaning independent t. framework compatiblemany decision models used artificial intelligence literature, including POMDPs(Kaelbling et al., 1998; Sondik, 1971) many variants.agent chooses action based belief state bt (also known information state),represents agents beliefs likelihood states time t. Formally,belief state probability distribution state space process. Belief filteringtask calculating belief state based history observations. Ideally,resulting belief state exact retains relevant information pastobservations (this sometimes referred sufficient statistic; cf. Astrom, 1965).exact update rule simple procedure produces exact belief states:Definition 1 (Exact update rule). exact update rule defined follows: takingaction observing ot+1 , belief state bt updated bt+1 viabt+1 (s0 ) =Xbt (s) (s, s0 )(1)sSbt+1 (s0 ) = bt+1 (s0 ) (s0 , ot+1 )(2)normalisation constant.sometimes refer step bt bt+1 transition step step bt+1 bt+1observation step. Unfortunately, space complexity storing exact belief statestime complexity updating using exact update rule exponentialnumber state variables, making infeasible complex systems large statespaces. Hence, efficient approximate methods required.1142fiExploiting Causality Selective Belief Filtering DBNs3.2 Dynamic Bayesian Networksdynamic Bayesian network (DBN) (Dean & Kanazawa, 1989) Bayesian networkspecial temporal semantics specifies stochastic process transitions onestate another. DBNs used model effects actions stochastic decisionprocess. Specifically, compact representation transition functionobservation function Oa action a:Definition 2 (DBN). dynamic Bayesian network action a, denoted , acyclicdirected graph consisting of:t+1 xt , xt+1 X ,State variables X = xt1 , ..., xtn X t+1 = xt+11 , ..., xnrepresenting states process time + 1, respectively.t+1 t+1 , representing obser Observation variables t+1 = y1t+1 , ..., ymjjvation received time + 1.Directed edges Ea X X t+1 X t+1 X t+1 X t+1 t+1 t+1 t+1 ,specifying network topology dependencies variables.Conditional probability distributions Pa (z | paa (z)) variable z X t+1 t+1 ,specifying probability z assumes certain value given specific assignmentparents paa (z) = {z 0 | (z 0 , z) Ea }. convenience, also define pata (Z) =t+1 pa (Z), pa (Z) =X paa (Z) pat+1zZ paa (z).(Z) = Xedges Ea distributions Pa define functions0(s, ) =n0Pa xt+1= s0i | paa (xt+1) - (s, )(3)i=1(s0 , o) =Pa yjt+1 = oj | paa (yjt+1 ) - (s0 , o)(4)j=1t+10use notation paa (xt+1) - (s, ) specify parents xit+10X , respectively, assume corresponding values . Formally,t+1t+1xtl pata (xit+1 ) xt+1pat+1= s0l0 . Similarly, use(xi ), xl = sl xl0l0t+10notation paa (yj ) - (s , o) specify parents yjt+1 X t+1 t+1 ,respectively, assume corresponding values s0 o.XtExample 2 (DBN representation robot arm). represent robot arm Example 1 set DBNs, one DBN action{CW, CCWi }.at+1t+1state observationvariables= 1 , 2t+1 , 3t+1 ,nDBNs X = 1 , 2 , 3 , Xt+1 = 1t+1 , 2t+1 , 3t+1 . make example realistic, let us assumejoint orientations bounded relative orientation immediately preceding joint(e.g. form cone), first joint bounded relative ground.means joint movement depends well preceding joint orientation, shown Figure 3. Moreover, joint orientations correlated (i.e. edges within1143fiAlbrecht & Ramamoorthy1t1t+11t+12t2t+12t+13t3t+13t+1XtX t+1t+1Figure 3: DBN representation robot arm.X t+1 ) joint exceed bound given preceding joint. Finally, observation variables depend solely corresponding joint variable. actionsexample would differ variable distributions Pa .3.3 Additional Definitionsuseful define following:xt+1binary order defined X X t+1 xti xtj xt+1jt+11 < j n, xi xj 1 i, j n.Given set Z X X t+1 , write Z denote tuple contains variablesZ, ordered .Given ordered tuple Z = (zi1 , ..., zi|Z| ), define set S(Z) = Xi1 .... Xi|Z|contain value tuples variables Z.Given value tuple sZ = (si1 , ..., si|Z| ) S(Z), use notation Z - sZabbreviation zil = sil zil Z (i.e. variables Z assumecorresponding values sZ ).4. Passivitysection introduces formal definition passivity, used basisremainder article. also provide simple procedure detect passivevariables process dynamics.4.1 Formal Definitionoutlined Section 1, state variable xt+1called passive action exists(in DBN ) xt+1 may change valuesubset xt+1parentsX1144fiExploiting Causality Selective Belief Filtering DBNsleast one variables subset changed value. Conversely, xt+1change variables subset change. Formally, define passivity follows:t+1Definition 3 (Passivity). Let action given DBN. state variable xit+1called passive existsset a,i paa (xi ) \ xi that:t+1(i) xtj a,i : xt+1Eaj , xi(ii) two states st st+1 (st , st+1 ) > 0 :sti = st+1xtj a,i : stj = st+1j(5)state variable passive called active.set a,i corresponds subset variables described above: containsvariables directly affect xit+1 (i.e. parents xt+1X ) xt+1maychange value variables a,i changed value. sometimes sayvariable xt+1passive respect another variable xtj casextj a,i . Furthermore, omit obvious context.Clause (i) Definition 3 requires xt+1intra-correlated variables a,i ;specifically, edge xt+1xt+1xtj a,i . example, seejFigure 1 assumed variable xt+1passive respect variable2xt1 . (We discuss purpose clause next subsection.) Clause (ii) definescore semantics passivity requiring xt+1remains unchanged variablesa,i remain unchanged. Note means distribution Pa xt+1may specifydeterministic stochastic behaviour variables a,i change values.includes xt+1may change value all.state variable xit+1 passive even parents X , nonexti . case, set a,i would empty clause (i) well premise (5)would trivially hold true. However, variable passive changevalue circumstances. words, would constant.case, one consider removing variable state description order reducecomputational costs.noted Section 2.3, passivity shown special kind context-specificindependence (CSI) (Boutilier et al., 1996) applied DBNs. Here, associated set a,ipassive variable xt+1provides context: given assignment values xtj a,i (i.e.t+1context) xtj = xt+1independent xtk , xt+1xtk pata (xt+1j , xi) \ a,ikk 6= i. However, besides similarity, important difference passivityCSI, passivity actually assume context observed. Thus,passivity viewed kind CSI unobserved contexts. become clearSection 5, describe filtering method exploits passivity.4.2 Non-Example Passivitypurpose clause (i) definition passivity? all, discussedpreviously, clause (ii) captures core idea passivity, variable maychange value variables respect passive changed value.1145fiAlbrecht & Ramamoorthyxt1xt+11xt2xt+12Figure 4: Example process clause (ii) insufficient.However, may seem intuitive clause (ii) sufficient passivity,fact processes clause (ii) alone suffice. words, clause (ii)necessary sufficient passivity. illustrate following example:Example 3 (Non-example passivity). Consider process two binary state variables,x1 , x2 , single action, a, shown Figure 4. (We omit observation variablesclarity.) dynamics process xt+1takes value xt2 xt+1takes12value xt1 (i.e. x1 x2 swap values time step). process,state variables satisfy clause (ii) Definition 3: set x01 = x02 (i.e. initial values),(st , st+1 ) positive states st = st+1 , hence (5) true. set x01 6= x02 ,(st , st+1 ) positive states st , st+1 sti 6= st+1, {1, 2}, hence (5)trivially true since premise false.Despite satisfying clause (ii), state variables xt+1xt+1Example 312fact passive, following two reasons: Firstly, passivity causal relationmust imply causal order (Pearl, 2000). However, causal orderx1 x2 , edge xt+1xt+112 . Secondly, passivity meansvariable may change value another variable respect passive (avariable a,i ) changed value. words, whether passive variable xt+1may change value depends past values a,i (at time t) new valuesa,i (at time + 1). However, variables Example 3 depend valuestime t, hence values time + 1 predetermined depend whethervariables a,i change values.first issue, namely causal order, addressed adding corresponding edges X t+1 . instance, Example 3 could add edge xt+1xt+112establish causal order. However, generally solve second issue,every passive variable xit+1 must depend past new values variables a,i . words, xit+1 must inter-correlated well intra-correlatedvariables a,i . former given definition (since every variable a,iparent xt+1) latter precisely required clause (i) Definition 3.Therefore, clauses (i) (ii) together define formal meaning passivity.4.3 Detecting Passive Variablesmentioned Section 1, passivity latent causal property senseextracted process dynamics without additional information, additionalassumptions regarding representation variable distributions. order determine1146fiExploiting Causality Selective Belief Filtering DBNsAlgorithm 1 Passive(xt+1, )1:Input: state variable xt+1, DBNOutput: a,i xt+1passive , else falset+13: Q OrderedQueue P pata (xi ) \ xti// ascending order |a,i |2:4:Q 6=5:a,i NextElement(Q)6:Q Q \ {a,i }7:xtj a,it+1xt+16 Eaj , xi8:9:10:11:12:13:14:15:16:Go line 4 // clause (i) violateda,i paa (xt+1) \ a,i xtint+1t+1x|xa,ija,ijS(a,i ), S(a,i ), si Xit+1=s ,Pa xt+1=|x,,a,ia,i< 1a,iGo line 4 // clause (ii) violatedreturn a,ireturn falsevariable xit+1 passive , one find set a,i clauses Definition 3satisfied. simple procedure representation variabledistributions given Algorithm 1. algorithm takes inputs variable xt+1t+1DBN , checks whether xi passive searching set a,i satisfiesclauses Definition 3. Note power set P line 3 includes empty set ,hence also accounts a,i = . Lines 7 9 check clause (i) satisfied lines10 14 check clause (ii) satisfied. Line 13 essentially checks (5) holds true.clauses satisfied, xt+1passive respect variables a,i ,algorithm returns set a,i . Otherwise, algorithm returns logical false.2time complexity Algorithm 1 exponential worst case, xt+1passive. Specifically, time requirements line 4 grow exponentially numberparents xt+1X , time requirements line 12 grow exponentiallycardinality a,i a,i . However, time requirements reduced significantlycommitting specific representations variable distributions Pa . example,distributions represented tabular form, one utilise arrays indicesperform sweeping tests (5), i.e. line 13. Moreover, important realisealgorithm needs performed state variable, prior start2. Strictly speaking, Algorithm 1 checks property stronger passivitycheck (st , st+1 ) > 0 (cf. clause (ii)) line 12. However, algorithm modified includecheck. omit exposition order highlight core ideas behind algorithm.1147fiAlbrecht & Ramamoorthyprocess demand. since passivity invariant process states.words, variable passive , always passive . Therefore, sufficescheck advance passivity.Note set a,i necessarily unique. example, considervariable xt+11passive respect variables xt2 xt3 , i.e. a,1 = xt2 , xt3 , assumext+1changesx3t+1 changes(i.e.2change time). Then,000easy verify a,1 = x2 a,1 = x3 also satisfy clauses (i) (ii), hencea,1 , 0a,1 , 00a,1 valid sets definition passivity. guiding principlecases Occams razor, which, intuitively speaking, states simplest explanationsuffices. case, means suffices use smallest set a,i termscardinality |a,i |. (Hence, line 3 Algorithm 1 sorts queue Q ascending order|a,i |.) rationale exist multiple causal explanations passive variablext+1, one involving fewest key variables favoured since reduces(compared alternative explanations) number cases wouldrevise beliefs xit+1 . earlier example, accept a,1 causal explanationt+1xt+1every time xt+1xt+1may1 , would revise beliefs x1230changed values. However, accept a,1 causal explanation, wouldrevise belief x1t+1 xt+1may changed value. difference2become obvious Section 5.2, explains passivity exploitedreduce computational costs.5. Passivity-based Selective Belief Filteringsection presents Passivity-based Selective Belief Filtering (PSBF) method,exploits passivity efficient filtering. discussed Section 3, assume processspecified set dynamic Bayesian networks contains one DBNaction A. Therefore, whenever refer action (e.g. , , Pa , paa ),assumed context .PSBF follows general two-step update procedure belief state firstpropagated process dynamics (transition step) conditionedobservation (observation step). Thus, natural divide exposition PSBFthree parts: (1) belief state representation, (2) transition step, (3) observationstep. discussed Sections 5.1, 5.2, 5.3, respectively. summary PSBFgiven Section 5.4. also discuss computational complexity error boundsPSBF Sections 5.5 5.6, respectively.5.1 Belief State RepresentationRecall Section 1 principal idea behind PSBF maintain separate beliefsindividual aspects process, exploit passivity order perform selectiveupdates separate beliefs. union individual aspects constitutes completestate description process. Therefore, belief state represented productseparate beliefs individual aspects.capture informal notion individual aspects formally form clusters,defined follows:1148fiExploiting Causality Selective Belief Filtering DBNsC11t1t+1C1C11t1t+11t+11t+11t1t+12t+12t2t+12t+12t2t+12t+13t+13t3t+13t+13t3t+13t+11t+1C22t2t+1C33t3t+1C2(a) C1 , C2 , C3(b) C1(c) C1 , C2Figure 5: Three clusterings robot arm DBN.Definition 4 (Cluster). clustering X t+1 set C = {C1 , ..., CK } satisfiesk : Ck X t+1 C1 ... CK = X t+1 . refer elements Ck C clusters.underlying idea behind concept clusters variables cluster Ckconnected important sense. Specifically, two variables commoncluster, exists relation variables regarding likelihoodvalues may assume. words, variables correlated X t+1 .number K concrete choice clusters Ck specified usergenerated automatically. example, may specified manually domain expertfamiliar structure modelled system, generated automatically usingmethods ones described Section 6.1. stressed, however,order reduce computational costs, advisable follow general rule smallpossible, large necessary choosing clusters (see Section 5.5 discussioncomputational complexity). Therefore, two variables strongly correlated,presumably common cluster, whereas weaklycorrelated (weakly meaning correlation ignored safely),separate clusters order reduce computational costs. illustratedfollowing example:Example 4 (Clusters robot DBN). Recall robot arm DBN Example 2, specifit+1 given three clusterscally Figureclusterstatet+1 3. One wayt+1t+1variables XC1 = 1, C2 = 2, C3 = 3, shown Figure 5a. clusteringefficient since minimises size cluster. However, clusters fail captureimportant correlation joint orientation restricted preceding jointorientationi1 . Anotherway cluster state variables given single clusterC1 = 1t+1 , 2t+1 , 3t+1 , shown Figure 5b. clustering captures correlationsvariables. However, largest possible clusterand, therefore,least efficient one. compromise given two clusters C1 = 1t+1 , 2t+1 , C2 = 2t+1 , 3t+1 ,shown Figure 5c. clustering captures correlation joint orientations immediately preceding joint orientations, efficientprevious clustering since smaller clusters.1149fiAlbrecht & RamamoorthyGiven definition clusters, capture informal notion separate beliefsform belief factors:Definition 5 (Belief factor). Given cluster Ck , corresponding belief factor bkprobability distribution set S(Ck ).Intuitively, belief factor bk represents agents beliefs likelihood valuesvariables corresponding cluster Ck . analogy view belief factorsmaller belief state, view b full belief state combinationsmaller belief states. However, distinguish two, refer b simply beliefstate bk belief factor.Finally, given clusters Ck corresponding belief factors bk , belief state brepresented factored formb(s) =Kbk (sk )k=1use notation sk refer tuple (si )xt+1 Ck . (E.g., Ck = xt+1, xt+123= (s1 , s2 , s3 , s4 ), sk = (s2 , s3 ).)5.2 Exploiting Passivity Transition Steporder perform selective updates belief factors bk , require procedureperforms transition step independently factor.3 obtain procedureintroducing two assumptions allow us modify transition step (1) exactupdate rule. assumptions guarantee transition step performed exactly,sense (1). However, discuss shortly, assumptions violated obtainapproximate belief states.first assumption, (A1), states clusters must uncorrelated (i.e.edges X t+1 clusters), second assumption, (A2), states clustersmust disjoint. Formally, defined follows:t+1(A1) : xt+1Ck pat+1(xi ) Ck(A2) k 6= k 0 : Ck Ck0 =Note neither assumption implies other. is, may case (A1)satisfied (A2) violated, vice versa. Assuming (A1) (A2),reformulate (1)X0bt+1Tka (s, s0k )btk0 (sk0 )(6)k (sk ) = 1S(pat (Ck ))k0 :[xt+1Ck0 : xti pat (Ck )]1 normalisation constant0Tka (s, s0k ) =Pa xt+1= (s0k )i | paa (xt+1) - (s, sk ) .xt+1Ck3. also advantage belief factors updated parallel, useful featureconsidering many platforms use parallel processing techniques.1150fiExploiting Causality Selective Belief Filtering DBNsprocedure performs transition step independently belief factor bk , henceupdated order parallel.Assumption (A1) allows us bring (1) form updates belieffactors bk independently other. Specifically, (A1) allows us define cluster-basedtransition function Tka , turn enables summation (6). Assumption (A2),hand, guarantees product (6) correct. particular, maycase |sk0 | < |Ck0 | (i.e. fewer elements sk0 Ck0 ) variablesCk0 patat (Ck ) (i.e. xt+1Ck0 xti/ patat (Ck )). cases, btk0t+1taken marginal distribution variables xi Ck0 xti patat (Ck ),(A2) guarantees marginalisation introduces errors.mentioned previously, assumption may violated obtain approximate beliefstates. However, important distinction (A1) (A2) regard:(A2) violated, (6) still well-defined sense still executed,except product (6) may degrade accuracy results. contrast(A1), structural requirement Tka sense Tka ill-defined without (A1).since, (A1) violated, variables Ck may parents X t+10Ck , case paa (xt+1) - (s, sk ) would ill-defined. Thus, (A1) violated,enforce modifying distributions Pa xt+1Ck marginaliset+1variables pat+1(x)C,clustersC. meanskkvariable separate distribution every cluster contains variable, therebypossibly introducing approximation error.Given modified transition step (6), exploit passivity perform selectiveupdates belief factors bk . Recall Section 4.1 variable xt+1passivet+1exists set a,i variables xi may change valuevariables a,i changed value. causal connection used decidewhether values variables cluster Ck may changed, casecorresponding belief factor bk updated. Theorem 1 provides formal foundation:Theorem 1. (A1) (A2) hold, xt+1Ck passive ,: bt+1k (sk ) = bk (sk ).Proof. Proof Appendix A.Theorem 1 states clusters C1 , ..., CK disjoint uncorrelated,variables cluster Ck passive , transition step correspondingbelief factor btk bt+1omitted without loss information.kTheorem 1 translate situations (A1) (A2), both, violated?key assumption (A1), states clusters must uncorrelated.discussed earlier, enforce modifying variable distributions Pa cluster.However, passive variable xit+1 Ck correlated (passive active) variablet+1t+1xt+1Ck0 , xt+1pat+1distribution Pa(xi ), marginalising xjjjt+1t+1xi typically cause xi lose passivity, sense would longer satisfyclauses Definition 3. Consequently, would always perform transitionstep Ck , even unmodified variables Ck passive. problematicunnecessary computations, also modified distributionsintroduce error every time transition step performed.1151fiAlbrecht & Ramamoorthy1t1t+11t+1C12t2t+12t+1C23t3t+13t+1XtX t+1t+1Figure 6: Robot arm DBN implementing action CW3 . Dashed circles mark passive statevariables. coloured ellipses represent clusters C1 C2 .alleviate effect, one check chance unmodified variablescluster would change values. shown case whenevercausal path active variable variable cluster:Definition 6 (Causal path). causal path , active variable xt+1anothert+1t+1 (Q)t+1(1)(2)(Q)(1)variable xj , sequence hx , x , ..., x x = xi , x= xj ,1 q < Q :(i) x(q) X t+1(ii) x(q) , x(q+1) Ea(iii) x(q+1) passive respect x(q)Intuitively, causal path defines chain causal effects (such joints 1 3Example 1): since active variable x(1) may changed value x(2) passiverespect x(1) , x(2) may also changed value; since x(2) may changedvalue x(3) passive respect x(2) , x(3) may also changed value, etc.Hence, absence observing changes, mere existence causal pathx(1) x(Q) reason revise beliefs x(Q) . Therefore, general update rule,omit transition step btk bt+1unmodified variables cluster Ck passivek, causal path active variable variable Ck .demonstrated following example:Example 5 (PSBF update rule robot arm DBN). Let us consider robot armprevious examples. Figure 6 shows DBN implements action CW3 .action rotates joint 3 robot arm 1 clock-wise (i.e. joint orientation 3t+1direct target action). Therefore, variable 3t+1 active variables1t+1 2t+1 passive (shown dashed circles).use clustering C1 = 1t+1 , 2t+1 , C2 = 2t+1 , 3t+1 reasons given Example 4. Since 1t+1 parent 2t+1 , PSBF enforce assumption (A1)1152fiExploiting Causality Selective Belief Filtering DBNsAlgorithm 2 SkippableClusters(C, )1:Input: clustering C = {C1 , ..., CK }, DBN2:Output: set clusters C C skipped transition step3:C C4:Q OrderedQueue(X t+1 )5:C 6= Q 6=6:7:8:9:10:11:12:13:14:xt+1NextElement(Q)Q Q \ xt+1Passive(xt+1, )C C \ Ck C | xt+1Ckxt+1Qjt+1CausalPath(xt+1, xj , )nC C \ Ck C | xt+1CkjnQ Q \ xt+1jreturn Cmarginalising 1t+1 variable distribution Pa 2t+1 cluster C2 . modified variable distribution loses passivity property (both clauses Definition 3violated), unmodified distribution 1t+1 still passive.performing transition step, PSBF update belief factor b2corresponding cluster C2 contains active variable 3t+1 . However, since variablescluster C1 passive (there modified variables C1 ), since causalpath 3t+1 variable C1 , PSBF omit update belief factor b1 .Intuitively, makes sense since change orientation joint 3 cannot causechange orientations preceding joints. Note corresponds saving50% transition step.Algorithm 2 defines procedure utilises rule find clusterstransition step skipped. algorithm takes inputs clustering C DBN, returns set C skippable clusters. essentially searches activevariables xt+1removes clusters Ck C contain variablescausal path xit+1 . function OrderedQueue(X t+1 ) returns orderedqueue Q variables X t+1 . performance Algorithm 2 depends orderqueue. experiments, obtained good performance ordering variablesdescending order number outgoing edges. function NextElement(Q) returnsnext element queue; function Passive(xt+1, ) defined Algorithm 1;t+1 t+1function CausalPath(xi , xj , ) returns logical true1153fiAlbrecht & Ramamoorthycausal path xt+1xjt+1 .4 Note that, given invariance passivity processstates (cf. Section 4.1), suffices call Algorithm 2 (in advance needed)determine clusters omit transition step.5.3 Efficient Incorporation ObservationsPSBF perform observation step similarly exact update rule (2),conditions propagated belief state bt+1 observation ot+1 obtain fully updatedbelief state bt+1 . However, given factored belief state representation used PSBF,require procedure respects factorisation observation step. Assuming(A1) (A2) hold, bring (2) form updates belief factors bkindependentlyXt+1 000bt+1(s, ot+1 )bt+1(7)k (sk ) = 2 bk (sk )k0 (sk )t+1 )) : = s0 k 0 6= k : C 0 pat+1 (Y t+1 ) 6=S(pat+1kk(Yk2 normalisation constant. Note that, analogously (6), variablest+1 ), bt+1 taken marginal distributionCk0 pat+1k0(Yt+1t+1Ck0 paat (Y). Assumption (A2) guarantees marginalisation introduceserrors. (A1) (A2) hold, transition step (6) observation step (7)produce exact belief states sense (1) (2), regardless many clustersskipped transition step (cf. Theorem 1).observation step (7) updates belief states uses observation variablesprocess. words, ignores internal structure observation variables.However, clear variables cluster Ck marginally independentobservation variables t+1 (this determined using d-separation (Geiger et al., 1989),simply checking directed path Ck t+1 ), needperform observation step corresponding belief factor bk . expressedformally Theorem 2:Theorem 2. xt+1Ck marginally independent yjt+1 t+1 ,t+1: bt+1k (sk ) = bk (sk ).Proof. Proof Appendix B.Theorem 2 states variables Ck independent t+1 ,observation step bk skipped. However, even Ck independent t+1 , maycase variables Ck depend subset Yk t+1 observationvariables. Clearly, cases, suffices use Yk rather t+1 observationstep. account this, first note variables t+1 may correlatedother. preserve correlations, subdivide t+1 clusters Cl t+1introduce following assumptions:(A3) : yjt+1 Cl paa (yjt+1 ) t+1 Cl(A4) l 6= l0 : Cl Cl0 =4. simple way implement function modify standard graph search method (such breath-firstsearch) check (iii) Definition 6, apply variables X t+1 edges Ea .1154fiExploiting Causality Selective Belief Filtering DBNsAssumptions (A3) (A4) analogous (A1) (A2), respectively, essentiallyserve purposes observation step. distinguish clusters Ck Cl ,sometimes refer former state cluster latter observation cluster.Assuming (A3) (A4) hold, redefine observation stepXt+1 00t+10bt+1(s)=b(s)(s,)bt+1(8)2kklkkk0 (sk )t+100l: Cl Yk 6= S(pat+1(Cl )) : sk = sk k 6= k : Ck0 pa (Cl ) 6=al (s, ot+1l ) =t+1t+1Pa yjt+1 =(ot+1)|pa(y)(s,)jjllyjt+1 ClYk t+1 set observation variables marginally independentvariables Ck .Given Theorem 2, one see (8) equivalent (7) observation variablesclustered (or, equivalently, single observation cluster Cl = t+1 ). However,important note observation variables clustered (i.e. multipleobservation clusters Cl ), (8) notQnecessarilyequivalentPP(7).QmTo see this, helpfulcompare abstract formulations(o)bj=1jj=1 (oj ) bs ,former corresponds (8) latter (7). Therein, (o1 , ..., om ) observation, bsprobability state S, (oj ) probability observing yj = ojs. abstract formulations equivalent = 1 bs = 1 s,cases may equivalent. Nonetheless, fix number observationvariables m, (8) approximates (7) closely increase number state variablesn. experiments indicate often suffices use state variablesobservation variables order obtain good approximations.Finally, show suffices perform observation step bk usingclusters Cl whose variables independent variables Ck , observe (8)fact repeated application (7) every Cl , updated belief factor bt+1kt+1used place bk subsequent application. Since every applicationform (7) (with t+1 = Cl ), conclude Theorem 2 holds, hence observationstep skipped clusters Cl independent Ck .5.4 Summary PSBFpreceding sections summarised follows:Representation: belief state bt represented product K belief factors btk ,Qbt (s) = Kk=1 bk (s). belief factor bk probability distributionset S(Ck ), Ck X t+1 cluster correlated state variables.Transition step: transition step btk bt+1performed using (6), clusterskCk include active variables , causal pathactive variable . clusters skipped.Observation step: observation step bt+1bt+1performed using (8),kkclusters Ck dependent observation variables t+1 , usingobservation clusters Cl relevant Ck . clusters skipped.1155fiAlbrecht & RamamoorthyAlgorithm 3 PSBF(at , ot+1 , (btk )Ck C | C, C, (a )aA )1:Input: action , observation ot+1 , belief factors (btk )Ck C2:Parameters: state clustering C, observation clustering C, DBNs (a )aA3:Output: updated belief factors (bt+1k )Ck C4:// Transition step5:C SkippableClusters(C, )6:Ck C7:8:9:10:11:Ck Cbt+1btkkelses0k S(Ck )X0bt+1Tka (s, s0k )btk0 (sk0 )k (sk ) 1S(patat (Ck ))12:k0 :[xt+1Ck0 : xti patat (Ck )]// Observation stepCk Cn14:Yk yjt+1 t+1 | directed path Ck yjt+113:15:Yk =16:bt+1bt+1kk17:18:19:elses0k S(Ck )t+1 00bt+1k (sk ) 2 bk (sk )Xal (s, ot+1 )0bt+1k0 (sk )(Cl ) 6=Cl C : Cl Yk 6= S(pat+1(Cl )) : sk = s0k k0 6= k : Ck0 pat+120:return(bt+1k )Ck CAlgorithm 3 provides procedural specification PSBF. algorithm takes inputsaction time t, , subsequent observation time + 1, ot+1 , belief factorstime t, btk . internal parameters state clustering C, observation clusteringC, set DBNs (a )aA define process. Lines 4 11 implementtransition step lines 12 19 implement observation step. Note sufficesexecute lines 5 14 advance (or demand) remember resultsfuture reference. algorithm returns updated belief factors bt+1k .1156fiExploiting Causality Selective Belief Filtering DBNs5.5 Space Time Complexitybelief factor bk one elementPbk (sk ) sk S(Ck ).5 Thus, total space requiredmaintain K belief factors bk Kk=1 |S(Ck )|. Furthermore, size set S(Ck ) growsexponentially number variables Ck , hence dominant growth factorspace requirement given largest cluster Ck |Ck | = maxk0 |Ck0 |. Therefore,space complexity PSBF O(exp maxk |Ck |), hence representation feasiblereasonably small clusters Ck .Similarly, numberPof operations required perform transition observationsteps order 2 Kk=1 |S(Ck )| worst case (i.e. clusters need updatedsteps). Specifically, line 11 line 19 Algorithm 3 executedevery sk Ck . dominant growth factor given largest cluster Ck , hencetime complexity PSBF O(2 exp maxk |Ck |) = O(exp maxk |Ck |). Noteassumes analysis performed lines 5 14 Algorithm 3 done advance.time complexity worst case, clusters need updatedtransition observation steps. difficult derive time complexityaverage case unclear average case terms passivity. Evenstipulate certain average degree passivity (e.g. 50% variables passive), wouldstill difficult make general statement time requirements since dependscrucially passive variables distributed across clusters. example, evenprocess average 90% passivity, one active variable clusterevery cluster would need updated transition step. Thus, generalstatement make regards passivity time complexity PSBFrefined O(exp maxCk CT CO |Ck |), CT CO include clustersneed updated transition observation step, respectively.5.6 Error Boundsfive possible sources approximation errors PSBF:clusters correlated (i.e. (A1) (A3) violated)clusters overlapping (i.e. (A2) (A4) violated)Generally (8) multiple observation clusters Cl usedfirst two cases, approximation error depends amount correlationoverlap. little correlation overlap clusters,approximation error expected small. Conversely, clusters stronglycorrelated overlapping, approximation error expected large.Boyen Koller (1998) provide useful analysis error bound filteringmethod uses factored belief state representation. Since PSBF uses factoredrepresentation, analysis applies directly PSBF. purpose sectionrestate main result analysis context work.analysis uses concept relative entropy (Kullback & Leibler, 1951)measure similarity belief states:5. practice, suffices store |S(Ck )| 1 elements, irrelevant analysis.1157fiAlbrecht & RamamoorthyDefinition 7 (Relative entropy). Let two probability distributions definedset X. relative entropy definedKL(||) =XxX(x) ln(x)(x)(x) > 0 (x) > 0.Similar Boyen Koller (1998), define approximation error incurred PSBFrelative exact belief state. However, since consider decision process multipleactions (represented DBNs ), define error action respectively:Definition 8 (Approximation error). Let b exact belief state b approximation PSBF. taking action a, let b0 exact update b (using (1) (2))b0 PSBF-update b (using (6) (8)). Furthermore, let b0 exact updateb (using (1) (2)). say PSBF incurs error relative b0KL(b0 ||b0 ) KL(b0 ||b0 ) .analysis also relies concept mixing rates. Intuitively, mixing rateDBN quantifies degree stochasticity . depends mixing rates kaindividual clusters Ck :Definition 9 (Mixing rate). mixing rate cluster Ck X t+1 definedka = 0min00,sXmin Tka (s0 , s), Tka (s00 , s) .sS(Ck )Ck satisfy (A1) (A2), observation variables t+1 one observationcluster, mixing rate given = (mink ka /r)q cluster Ckdepends r influences q clusters Ck0 6=k (Boyen & Koller, 1998).worst case (that is, (A1A4) violated), minimal mixing rate given kasingle cluster Ck = X t+1 .Finally, main result work Boyen Koller (1998), restatedcontext work Theorem 3, essentially states approximation error PSBF(measured terms relative entropy) bounded mixing rates process:Theorem 3 (Boyen & Koller, 1998). Let bt exact belief state bt approximation PSBF using clusters Ck . Then, states ~s = (s0 , ..., st ) actions~a = (a0 , ..., at1 ),hmax~aEo1 ,...,ot KL(bt ||bt )mina ~aexpectation E Qtaken possible sequences observations o1 , ..., ot1+1 , +1 ), defined above.probabilities P (o , ..., ) = t1=0 (s1158fiExploiting Causality Selective Belief Filtering DBNsProcess size# x vars (n)# vars (m)# states (|S|)# obs. (|O|)103> one thousand8206> one million64L309> one billion512XL4012> one trillion4096Table 1: Synthetic process sizes. variables binary.6. Experimental Evaluationevaluated PSBF two experimental domains: Section 6.1, evaluated PSBFsynthetic (i.e. randomly generated) processes varying sizes degrees passivity.Section 6.2, evaluated PSBF simulation multi-robot warehouse system. briefsummary experimental results given Section 6.3.6.1 Synthetic Processesfirst evaluated PSBF series synthetic processes. PSBF compared selectionalternative methods, including PF (Gordon et al., 1993), RBPF (Doucet et al., 2000), BK(Boyen & Koller, 1998), FF (Murphy & Weiss, 2001); see Section 2 discussionmethods. algorithms implemented Matlab 7.13, used Matlabtoolbox BNT (Murphy, 2001) implement BK FF.6.1.1 Specification Synthetic Processesgenerated synthetic processes four different sizes specified Table 1.process generated follows:First, variable xit+1 chosen passive probability p, casealso add edge (xti , xt+1). refer p degree passivity. samplet+1edges X /XX t+1 , generate mixture Gaussians G using Algorithm 4 (seeAppendix C). Figure 7 shows example G generated process size M. setG used produce areas correlated variables (i.e. Gaussians),constitute natural candidates state clusters.Let vector maximum densities Gaussian G, letvector densities value N. Then, every combination j, edge (xti , xt+1j )2added probability equal maximum element j / , operatorspoint-wise. xt+1chosen passive, edge (xti , xt+1j ) addedt+1 t+1t+1 t+1< j. case, also add edge (xi , xj ). Edges (xi , xj ) added similarlyt+1< j,6 also add edge (xti , xt+1j ) passive xj . ensure everyvariable effect generated process, xti connected least one xt+1jt+1X t+1 (adding(adding (xti , xt+1)necessary)xleastoneparentXj6. condition < j cases ensure resulting DBN acyclic.1159fiAlbrecht & Ramamoorthy0.350.3Density0.250.20.150.10.0502468101214161820i, jFigure 7: Example mixture Gaussians generated process size consistingt/t+1three Gaussians. closer two variables xixt+1peak commonjGaussian, higher probability edge added them.t+1 t+1(xtj , xt+1j ) necessary). Finally, edges (xi , yj ) added probability 0.1,i, j, ensuring yjt+1 least one parent X t+1 .variables process binary. Passive variables assumed passiverespect parents X . distributions Pa xt+1X t+1 generatedt+1uniformly randomly without bias. passive variables xi , modify Pa satisfy clause(ii) Definition 3. distributions Pa yjt+1 t+1 generated probabilitysampled uniformly either [0.0, 0.2] [0.8, 1.0], obtain meaningful observations.Finally, every process consists two actions. obtained randomly choosingone three variables xt+1whose distributions Pa resamplededges X added probability 0.1 (passive variables chosen way longerpassive). simulations, actions chosen uniformly randomly.process starts random initial state, algorithms testedsequence processes, initial states, chosen actions, random numbers.6.1.2 Clustering Methodsused three different clustering methods, denoted hpci, hmorali, hmodisi. methodsapplied variables X t+1 without edges involving X t+1 :hpci drops directions edges (i.e. edge xt+1xt+1ads reversejt+1t+1edge xj xi ) puts variables (undirected) pathone cluster. definition, resulting clusters satisfy assumptions (A1A4).hmorali connects parents variable drops directions (it moralisesvariables) extracts clusters fully connected variables (maximum cliques).resulting clusters may satisfy assumptions (A1A4).hmodisi similar hmorali truncates resulting clusters make disjoint(clusters removed become subset another cluster). definition,resulting clusters satisfy (A2/A4), necessarily (A1/A3).example, consider Figure 5 Section 5.1. Here, hpci would produce clusterC1 Figure 5b, since variables connected undirected path. Furthermore,1160fiExploiting Causality Selective Belief Filtering DBNshmorali would produce two clusters C1 C2 Figure 5c, correspondtwo maximum cliques moralising variables X t+1 . Finally, hmodisi would producecluster C1 Figure 5c cluster C3 Figure 5a.PSBF used clustering method generate clusters state variables (Ck )observation variables (Cl ). Moreover, PSBF enforced (A1/A3) whenever necessarymodifying variable distributions described Section 5.1.6.1.3 Accuracyorder compare accuracy tested algorithms, computed relative entropy(cf. Definition 7) exact belief states obtained using exact update rule (cf. Definition 1)approximate belief states produced tested algorithms. However, since exactbelief states relative entropy hard compute large processes, ablecompare accuracy algorithms processes size only. algorithms initialiseduniform belief states, uniformly sampled particles.first compared accuracy PSBF BK, since use factorisationbelief state representations. Figure 8 shows relative entropy PSBF BK averaged 1000 processes 0%, 20%, 40%, 60%, 80%, 100% passivity, respectively.results show PSBF hpc/modisi produced lower relative entropy (i.e. higher accuracy) BK hpc/modisi, PSBF hmorali produced relative entropy comparableBK hmorali. indicates violations (A2/A4) introduce smaller errorsviolations (A1/A3). Note PSBF BK convergent behaviourrelative entropy, shows approximation error due factorisationbounded, discussed Section 5.6. interesting since PSBF BK obtain approximation errors factorisation different ways: PSBF loses accuracy modifyingvariable distributions ensure state clusters independent (cf. Section 5.2),BK loses accuracy marginalising original factorisation inference (i.e.projection step; cf. Section 2.1). Nevertheless, shown results, resultingapproximation errors bounded cases, similar convergence.Note relative entropy methods increased degree passivityprocess. explained fact higher passivity implies higher determinacyand, therefore, lower mixing rates (cf. Definition 9), crucial factor errorbounds PSBF BK (cf. Theorem 3). Finally, note PSBF produce exactbelief states (i.e. zero relative entropy) using hpci clustering, despite factclusters generated hpci satisfy assumptions (A1A4). However, discussed detailSections 5.3 5.6, another possible source approximation errors multiple observationclusters used, often case using hpci produce observation clusters.compare accuracy PF/RBPF PSBF/BK, number samples usedPF/RBPF chosen automatically process required approximatelymuch time per belief update PSBF hmorali BK hmorali, respectively.experiments, meant PF (RBPF) able process 100 300 (2050) samples. However, since process 1000 states, nearly enoughrepresent uniform belief state. Hence, PF/RBPF produced much higher relative entropyPSBF/BK. Moreover, fact processes high variance meansPF/RBPF would require many samples achieve accuracy PSBF/BK (as1161fiAlbrecht & Ramamoorthy20.60.4BK (pc)PSBF (pc)BK (moral)PSBF (moral)BK (modis)PSBF (modis)Relative entropyRelative entropy0.80.200500100015002000Transition25001.510.5030000500(a) 0% passivity100015002000Transition250030002500300025003000(b) 20% passivity5Relative entropyRelative entropy321432100500100015002000Transition25000300005008642015002000Transition(d) 60% passivityRelative entropyRelative entropy(c) 40% passivity10000500100015002000Transition25003000(e) 80% passivity64200500100015002000Transition(f) 100% passivityFigure 8: Accuracy results PSBF BK. Plots show relative entropy exactalgorithms belief states (lower better). Results averaged 1000 processes size(n = 10, = 3), average 0%100% non-target variables passive (cf.Section 6.1.1). PSBF/BK used clustering methods hpci, hmorali, hmodisi.shown next section). One would expect latter issue alleviated useexact inference RBPF (cf. Section 2.1). However, case muchvariance process captured marginal distributions used particlesRBPF. contrast, synthetic processes exhibit high variance across variables,1162fiExploiting Causality Selective Belief Filtering DBNsautomatic grouping7 state variables sampled exact variables still containedmuch variance sampled variables. Hence, RBPF required significantly samplesnumber could process time provided.Finally, order compare accuracy FF PSBF/BK, number iterationsused FF (more precisely, number iterations loopy belief propagation; cf. Murphy &Weiss, 2001) chosen automatically process FF required approximatelymuch time per belief update PSBF hmorali BK hmorali, respectively. However,FF often able perform several iterations provided time, resultingrelative entropy substantially higher PSBF/BK. problemFF designed specific class DBN topologies, namely containingedges within X t+1 (called regular DBNs Murphy & Weiss, 2001). allowsFF use fully factored representation belief states, variablebelief factor. However, processes used experiments high intra-correlationstate variables (i.e. many edges X t+1 ), especially increasing passivity.correlations cannot captured belief state representation FF, resultingsignificantly higher relative entropy PSBF/BK.6.1.4 Timingmeasured computation times processes sizes S, M, L, XL passivities 25%,50%, 75%, 100%, respectively. PSBF BK used hmorali clustering, seemedappropriate fair comparison since produced consistently similar accuracyalgorithms. number samples used PF chosen automatically processPF achieved average accuracy approximately good PSBFBK, respectively, final 20% process. involved computing exact beliefstates relative entropies, able use PF processes size only. omitRBPF FF section shown previous section unsuitableprocesses consider. PSBF tested 1, 2, 4 parallel processes,allocated approximately number belief factors.Figures 9a 9d show times 1000 transitions averaged 1000 processes,Figure 9e shows average percentage belief factors updated transitionobservation steps PSBF. timing reported PSBF includes time takenmodify variable distributions (in case overlapping clusters) detect skippable clusterstransition observation steps, done advanceaction. results show PSBF able minimise time requirements significantlyexploiting passivity. First, note marginal gains 25% 50%passivity, despite fact PSBF updated 14% fewer clusters transition step.clusters mostly small. However, significant gains50% 75% passivity average speed-ups 11% (S), 14% (M), 15% (L), 18% (XL),7. open question group state variables sampled exact variables (Doucet et al.,2000). used simple heuristic whereby set sampled variables contained variables xt+1parents X t/t+1 none xti . remaining variables X t+1 constituted setexact variables. ensure resulting grouping valid actions (i.e. DBNs) process,considered edges involved DBNs; is, performed grouping union Eaa. Moreover, improve efficiency, subdivided set exact variables clustersvariables connected undirected edges X t+1 without edges involving sampled variables.1163fi100PF:BKPF:PSBFBK50025%50%75%Passivity400200150PSBF1 100PSBF2PSBF450100%(a) (n=10, m=3)025%50%75%PassivitySeconds 1000 transitions150Seconds 1000 transitionsSeconds 1000 transitionsSeconds 1000 transitionsAlbrecht & Ramamoorthy35030025020015010050100%0(b) (n=20, m=6)25%100%(c) L (n=30, m=9)100% updated belief factors50%75%Passivity700600500400300200100025%50%75%Passivity100%(d) XL (n=40, m=12)(trans)(obs)(trans)(obs)806040L (trans)L (obs)XL (trans)XL (obs)20025%50%75%Passivity100%(e) Updated belief factorsFigure 9: Timing results. (ad) Average number seconds required 1000 transitionsUNIX dual-core machine 2.4 GHz, sizes S, M, L, XL. Passivity p% meansaverage p% non-target variables passive (cf. Section 6.1.1). PSBF BKused hmorali clustering. PF optimised binary variables used number samplesachieve accuracy PSBF BK, respectively. PSBF run 1 (PSBF-1), 2(PSBF-2), 4 (PSBF-4) parallel processes. (e) Average percentage belief factorsupdated transition observation steps, respectively.75% 100% passivity average speed-ups 11% (S), 33% (M), 46% (L),49% (XL). shows computational gains grow significantlydegree passivity size process.results show PSBF consistently outperformed BK process sizes.two main computational savings PSBF relative BK: firstly, skipping belieffactors transition observation steps, secondly, performpotentially expensive projection step restore original factorisation inference.However, times algorithms grew exponentially size process,note relative difference PSBF BK decreased significantly lowerdegrees passivity. instance Free Lunch (see Section 7 discussion),means PSBF performs best processes high passivity sufferperformance processes lack passivity. Specifically, computational overheadmodifying variable distributions detecting skippable belief factors amortise1164fiExploiting Causality Selective Belief Filtering DBNseffectively large processes low passivity. Furthermore, low passivity, PSBFoften perform full transition observation steps (i.e. update belief factorsstep), costly large processes.BK PF affected passivity? surprisingly, performance BKnearly unaffected increasing degrees passivity. junction tree algorithm usedBK benefited marginally increased sparsity process, computationalgains minimal. first unable use PF required many samples(between 10k 200k) achieve comparable accuracy PSBF/BK, duehigh variance processes. order investigate effect passivity PF,implemented version PF strictly optimised binary variables. Interestingly,found passivity adverse effect performance PF, requiring useexponentially samples increased passivity (see Figure 9a). makes senseview PF factored approximation method (such PSBF BK) meansanalysis Section 5.6 applies. However, PF puts variables single cluster(since actually factored method), mixing rate process much lowerPSBF BK (as discussed Section 5.6) and, thus, error bounds lesstight. compensate this, PF requires significantly samples increased passivity.6.2 Multi-robot Warehouse Systemsection, demonstrate passivity occur naturally complex systemPSBF exploit accelerate filtering task. end, considermulti-robot warehouse system style Kiva (Wurman et al., 2008),robots task transport goods within warehouse (cf. Figure 10a).6.2.1 Specification Warehouse SystemFigure 10b shows initial state warehouse simulation. warehouse consists2 workstations (W1, W2), 4 robots (R1R4), 16 inventory pods (I1I16). robotmove forward backward, turn left right, load unload inventory pod (ifpositioned pod), nothing. Kiva, robots move inventory podsunless carrying pod, case pods become obstacles. moveturn operations stochastic robot may move/turn far (3% chance)nothing (2% chance). robot possesses two sensors, one telling inventorypod loaded (if any) one direction facing. direction sensor noisyrandom direction may reported (3% chance).robot maintains list tasks form Bring inventory pod workstationW (yellow area around W) Bring inventory pod position (x,y). tasksexecuted depends control mode, use two simulations:88. control modes ad hoc often make suboptimal decisions. However, found currentsolution techniques (DEC-)POMDPs, including approximate methods, infeasible setting.Nonetheless, quality decisions made control modes largely depends accuracybelief states, hence important belief states updated accurately. Therefore,control modes sufficient purposes.1165fiAlbrecht & Ramamoorthy(a) Kiva warehouse system(b) Initial state simulationFigure 10: (a) Kiva warehouse system (image reproduced DAndrea & Wurman, 2008).Robots (orange coloured) transport shelfs goods workstations. (b) Initialstate warehouse simulation. warehouse consists 2 workstations (W1, W2), 4robots (R1R4), 16 inventory pods (I1I16).Centralised mode: central controller maintains belief state bt statewarehouse system. time t, samples 100 states bt removesduplicate states, resulting setP t= {s1 , s2 , ...}. resamples stateprobabilities w(s ) = b (s )/ q b (sq ). Based current taskrobot, performs search (Hart, Nilsson, & Raphael, 1968) (with Manhattandistance) space joint actions find optimal action robot.executing actions, robots send sensor readings controller,controller updates belief state using sensor readings.Decentralised mode: robot maintains belief state communication robots. knowledge robotscurrent tasks, communicated task allocation module. time t,robot samples set state done centralised mode. Treatingrobots static obstacles, performs search based currenttask find action . repeated robot r states sq S,resulting actions ar,q usedP obtain distributions r : [0, 1] (Aset actions) r (a) = q : ar,q =a w(sq ). robot executes action updates belief state using sensor readings distributions raverage robots actions.tasks generated external scheduler time intervals sampled U [1, 10].generated task assigned one robots sequential auction (Dias, Zlot,Kalra, & Stentz, 2006). robots bids calculated total number steps neededsolve current tasks auctioned task (in simplified modelrobots removed), averaged states S. robot lowest bidassigned task.1166fiExploiting Causality Selective Belief Filtering DBNsFigure 11: Example DBN smaller warehouse system consisting one inventorypod (I1) two robots (R1, R2). DBN implements joint action R1 movesR2 turns. Dashed circles mark passive state variables. coloured areas representstate clusters C1 C8 .6.2.2 DBN Topology ClusteringFigure 11 shows example DBN smaller warehouse one inventory pod tworobots. inventory pod represented two variables, I.x I.y, correspondx position inventory pod. robot R represented four variables:R.x/R.y x/y position, R.d direction, R.s status. statusrobot R either R.s=0 (unloaded) R.s=I (loaded inventory pod I). Constantssize warehouse positions workstations omitted DBN.four types clusters: I-clusters (C1C4) preserve correlationR loaded I, must always position R (there two I-clusters(I,R) pair); R-clusters (C5) S-clusters (C6), respectively, preservecorrelation two robots position carry inventory pod(there one R/S-cluster (Ra,Rb) pair > b); And, finally, D-clusters (C7,C8). PSBF uses singleton observation clusters (i.e. one cluster observation variable).differences DBNs centralised decentralised modes(Figure 11 uses centralised mode). centralised mode, one DBNaction combination robots. Since controller observes R.s noise-free, addedges R.x/R.y I.x/I.y R.s=I remove otherwise simplify inference(thus, Figure 11, R1 loaded I1 R2 unloaded). decentralised mode,robot observes sensor readings, hence add remove edgesitself, edges robots must permanently added. also meansrobots status variables (R.s) must linked I.x/I.y and, therefore, includedI-clusters (to preserve correlation must position R Rloaded I). Moreover, since robot knows action, one DBN1167fiSeconds per transitionAlbrecht & RamamoorthyCentralised180 Decentralised1601401201008060BKPSBFPFFigure 12: Results warehouse simulation, using centralised decentralisedcontrol modes. Timing measured UNIX dual-core machine 2.4 GHz averaged20 different simulations 100 transitions each.actions, variables associated robots active (thedistributions r defined previous section used average actions).6.2.3 Resultsimplemented PSBF, BK, PF C#, using framework Infer.NET (Minka, Winn,Guiver, & Knowles, 2012) implement BK. allowed BK exploit sparsityprocess offered improved memory handling. PSBF optimised sparsity (6)(8), respectively, summing states btk0 / bt+1k0 positive. PF naturallybenefits sparsity allows concentrate samples fewer states. numbersamples used PF set way controller decisions invariantrandom numbers used sampling process PF. done ensureresults repeatable. Finally, maintain sparsity process, probabilitybelief states lower 0.01 set 0. tested algorithms initialised exactbelief state, shown Figure 10b.Figure 12 shows time per transition averaged 20 different simulations 100transitions each. timing reported PSBF includes time needed modify variabledistributions (for overlapping clusters) detect skippable belief factors transitionobservation steps, done demand every previously unseenDBN. centralised mode, PSBF able outperform BK average 49%PF 36%. PF needed 20,000 samples produce consistent (i.e. repeatable) results.decentralised mode, PSBF outperformed BK average 17% PF 32%. PFneeded 45,000 samples produce consistent results, due increased varianceprocess. differences statistically significant, based paired t-tests 5%significance level. Note PSBF BK slower decentralised mode sincecorresponding DBNs much higher inter-connectivity. addition, PSBF updatedbelief factors since active variables.expected, PSBF able exploit high degree passivity processaccelerate filtering task. many cases, meant PSBF needed update lesshalf belief factors. Precisely many belief factors updated depends1168fiExploiting Causality Selective Belief Filtering DBNsperformed action. illustrate this, consider smaller warehouse DBN shown Figure 11(for centralised mode), R1 moving R2 turning. Here, R1.x, R1.y,R2.d active variables variables passive (dashed circles), correspondingpassivity 70%. DBN, PSBF updates belief factors corresponding clustersC1, C2, C5, C8, since contain active variables, also updates belieffactors C3 C4, since directed paths active variables (R1.x R1.y)them. Therefore, factors updated C6 C7.consider full warehouse experiment, contains 16 inventory pods 4 robots,resulting 48 variables 128 I-clusters, 6 R-clusters, 6 S-clusters, 4 D-clusters.Assume similar situation one robot moves inventory pod, say R4 I1,R13 turn. case, PSBF updates 3 6 R-clusters (those containingR4), 0 6 S-clusters (since status change), 3 4 D-clusters (for R13), 38 128I-clusters (32 I-clusters containing R4 plus 6 I-clusters R13 I1), amountingtotal saving 69.44% belief factors need updated.number states warehouse system (including invalid states) exceeded 1045states. Therefore, unable compare accuracy tested algorithms termsrelative entropy. Instead, compared accuracy based results taskauctions number completed tasks end simulation. givesgood indication algorithms accuracy, since outcome auctionnumber completed tasks depend accuracy belief states. centralisedmode, algorithms generated 95% identical task auctions completed 15.7 (BK),15.5 (PSBF), 15.2 (PF) tasks average. decentralised mode, generated93% identical auctions completed 12.1 (BK), 12.2 (PSBF), 11.7 (PF) tasksaverage. modes, none differences statistically significant. Therefore,indicates PSBF achieved accuracy similar BK PF.6.3 Summary Experimental Evaluationexperimental results show PSBF produces belief states competitive accuracy:synthetic processes, PSBF achieved accuracy average bettercomparable accuracy alternative methods. warehouse system, PSBFable complete statistically equivalent number tasks comparedmethods, indicates accuracy equivalent comparable.Furthermore, experimental results show PSBF performed belief updatessignificantly faster alternative methods: synthetic processes, PSBF usingparallel processes outperformed BK 64% largest process (XL), PF tookmuch time achieve accuracy comparable PSBF. particular, results showcomputational gains grow significantly degree passivitysize process. warehouse system, PSBF outperformed alternative methods49%, substantial saving considering size state space (more1045 states). Furthermore, computational gains much higher centralisedcontrol mode decentralised control mode, since latter significantlylower degree passivity. Therefore, shows high degrees passivity beargreat potential filtering task.1169fiAlbrecht & Ramamoorthy7. Free Lunch PSBFview belief filtering method generally suited types processes.Instead, method assumes certain structure process (explicitly implicitly)attempts exploit order render filtering task tractable. Typically,methods tailored way respect structure perform wellstructure present process, suffer significant loss performance structureabsent. instance, PF works best processes low degrees uncertainty, sincemeans fewer state samples needed acceptable approximations.hand, number samples needed acceptable approximations grow substantiallydegree uncertainty process (as shown experiments). anotherexample, BK works best processes little correlation state variables, sincemeans belief factors small processed efficiently. However,many variables strongly correlated, BK typically becomes infeasible.Therefore, structural assumptions taken account choosingfiltering method specific process.formal account view given Free Lunch theorems (Wolpert& Macready, 1997, 1995) state that, intuitively speaking, two algorithmsequivalent performance averaged possible instances problem.words, classes problem instances algorithm better performancealgorithm B, must classes problem instancesworse performance B. Then, question is: class problem instances (thatis, processes) PSBF expected achieve good performance? class essentiallydescribed following three criteria:Degree passivity PSBF attempts accelerate filtering task omittingtransition step many belief factors possible. depends passivityvariables state clusters. ideal case, process exhibits high degreepassivity PSBF omit transition step many belief factors.worst case, process passive variables all, PSBF updatebelief factors transition step. However, discussed Section 5.5, high degreepassivity necessarily sufficient infer many clusters skippedtransition step, since passive variables could distributed waycluster skipped (e.g. passive variables distributed uniformlyamongst state clusters). Therefore, optimal case, passivity concentratedcorrelated state variables passive variables end clusters.Size state clusters space time complexity belief state representationPSBF exponential size largest state cluster (cf. Section 5.5). Therefore,ideal case, relevant variable correlations captured small stateclusters cost storing belief factors performing update proceduressmall. worst case, large state clusters required retain variablecorrelations cost storing updating belief factors large. Another reasonstate clusters small way PSBF performstransition step. One pre-requisite omitting transition step belief factorvariables corresponding cluster passive. many variables1170fiExploiting Causality Selective Belief Filtering DBNsone cluster, less likely variables cluster passive, and,therefore, less likely cluster skipped.Structure observations third criterion, though arguably less importantcriteria, structure observations (i.e. way observationvariables depend state variables) size observation clusters (Cl ).PSBF attempts accelerate observation step skipping stateclusters whose variables structurally independent observation, and,cluster cannot skipped, incorporating observation clustersrelevant update. Therefore, ideal case, fraction state clustersdepend observation, relevant correlations observation variablescaptured small observation clusters. worst case, state clustersdepend observation sense, structure observationallow efficient clustering.Thus, summary, PSBF suitable processes high degrees passivityrelevant variable correlations captured small state observationclusters. hand, PSBF may suitable low degreespassivity, large state observation clusters necessary retain relevantvariable correlations process.addition identifying class processes filtering method suitable,also important justify practical relevance class. work, interestedrobotic physical decision processes (as shown examples experiments).systems typically exhibit number features: First all, robotic systems usuallycausal structure (e.g. Mainzer, 2010; Pearl, 2000). Passivity, specific typecausality, observed many robotic systems, including robot arm usedexamples multi-robot warehouse system Section 6.2. Furthermore, robotic systemstypically modular structure, module responsible specificsubtask may interact modules. modular structure often allowsefficient clustering, sense module corresponds cluster correlated statevariables. Finally, sensors used robotic systems typically provide informationcertain aspects system, components system may benefitsensor information. words, independencies stateobservation variables. features correspond criteria (above) specifyclass processes PSBF suitable filtering method. Therefore, believeclass practically justified.8. ConclusionInferring state stochastic process difficult technical challenge complexsystems large state spaces. key developing efficient solutions identify specialstructure process, e.g. topology parameterisation dynamic Bayesiannetworks, leveraged render filtering task tractable.end, present article explored idea automatically detecting exploitingcausal structure order accelerate belief filtering task. considered specific typecausal relation, termed passivity, pertains state variables cause changes1171fiAlbrecht & Ramamoorthystate variables. demonstrate potential exploiting passivity, developed novelfiltering method, PSBF, uses factored belief state representation exploits passivityperform selective updates belief factors. PSBF produces exact belief statescertain assumptions approximate belief states otherwise. showed empirically,synthetic processes varying sizes degrees passivity well examplecomplex multi-robot system, PSBF faster several alternative methodsachieving competitive accuracy. particular, results showed computationalgains grow significantly size process degree passivity.work demonstrates system exhibits much causal structure,great potential exploiting structure render filtering task tractable.particular, experiments support initial hypothesis factored beliefs passivityuseful combination large processes. insight relevant complex processeshigh degrees causality, robots used homes, offices, industrial factories,filtering task may constitute major impediment due often large statespace system.several potential directions future work. example, would usefulknow definition passivity could relaxed variables falldefinition, principal idea behind PSBF still applicable. Onerelaxation could form approximate passivity, allows small probabilitiespassive variables change values even relevant parents remain unchanged.addition, would interesting know idea performing selective updatesbelief factors (via passivity) could also applied existing methods usefactored belief state representation (cf. Section 2.1). Finally, another useful avenue futurework would formulate additional types causal relations exploitedways similar PSBF exploits passivity, perhaps ways that.Acknowledgementsarticle result long debate presented topic, process benefitednumber discussions suggestions. particular, authors wish thankanonymous reviewers NIPS12 UAI13 conferences well Journal AIResearch; attendees workshop Advances Causal Inference held UAI15;colleagues School Informatics University Edinburgh. Furthermore,authors acknowledge financial support German National Academic Foundation,UK Engineering Physical Sciences Research Council (grant number EP/H012338/1),European Commission (TOMSY Grant Agreement 270436).1172fiExploiting Causality Selective Belief Filtering DBNsAppendix A. Proof Theorem 1prove Theorem 1, useful first establish following lemma:Lemma 1. (A1) holds xt+1Ck passive ,s, s0 : Tka (s, s0k ) = 1 sk = s0k .Proof.: fact (A1) means a,i Ck xt+1Ck . Since xt+1Ckpassive , follows xtj a,i passive , a,i . Therefore, givenTka (s, s0k ) = 1 clause (ii) Definition 3, follows sk = s0k .: Follows directly (A1) fact xt+1Ck passive .Using Lemma 1, give compact proof Theorem 1:Theorem 2. (A1) (A2) hold, xt+1Ck passive ,: bt+1k (sk ) = bk (sk ).Proof.0bt+1k (sk )=1XTka (s, s0k )S(pat (Ck ))=1Xbtk0 (sk0 )k0 :[xt+1Ck0 : xti pat (Ck )]Lem1Tka (s, s0k )btk0 (sk0 )S(pat (Ck )):sk =s0k k0 :[xt+1Ck0 : xti pat (Ck )]=1 btk (sk )XTka (s, s0k )btk0 (sk0 )S(pat (Ck )):sk =s0k k0 6= k:[xt+1Ck0 : xti pat (Ck )]{z|(A1)= 1=1 btk (sk )=btk (sk ). (1 = 1 since btk normalised)1173}fiAlbrecht & RamamoorthyAppendix B. Proof Theorem 2prove Theorem 2, first note following proposition:Proposition 1. xt+1Ck marginally independent yjt+1 t+1 ,s, s0 : k0 6=k sk0 = s0k0 (s, ot ) = (s0 , ot ).proposition follows directly definition.Using Proposition 1, give compact proof Theorem 2:Theorem 2. xt+1Ck marginally independent yjt+1 t+1 ,t+1: bt+1k (sk ) = bk (sk ).Proof.Xt+1 00bt+1k (sk ) = 2 bk (sk )(s, ot+1 )0bt+1k0 (sk )t+1 )) : = s0 k 0 6= k : C 0 pat+1 (Y t+1 ) 6=S(pat+1kk(Yk|Prop1{z= constant , independent=bt+1 (s0k )P k t+1 00s00 bk (sk )k=bt+1 (s0k )P k t+1 00s00 bk (sk )k0= bt+1k (sk ).1174}s0kfiExploiting Causality Selective Belief Filtering DBNsAppendix C. Mixture GaussiansAlgorithm 4 provides simple procedure randomly generates mixture Gaussians(i.e. set normal distributions) synthetic processes Section 6.1. algorithmtakes input number n state variables returns set G Gaussians whose meansset {1, ..., n}. number Gaussians, means, varianceschosen automatically achieve good coverage state variables minimising(visual) overlap Gaussians. See Figure 7 example.Algorithm 4 MixtureOfGaussians(n)1:Input: number state variables n2:Parameters: 4, min 5 , max3:Output: mixture Gaussians G4:G5:R {(1, ..., n)}6:R 6=n107:R next element R8:R R \ {R}9:R(drand |R|e) // rand returns random number (0, 1)10:1 min[ R(1), R(|R|) ]11:12:min[max , max[min , rand ]]G G (, 2 ) // mean variance Gaussian13:R (R(1), R(2), ..., R(p)) R(p) <14:R+ (R(q), R(q + 1), ..., R(|R|)) R(q) > +15:R 6=16:17:18:19:R R {R }R+ 6=R R {R+ }return G1175fiAlbrecht & RamamoorthyReferencesAstrom, K. (1965). Optimal control Markov processes incomplete state information.Journal Mathematical Analysis Applications, 10, 174205.Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: structural assumptions computational leverage. Journal Artificial Intelligence Research, 11 (1),194.Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-specific independence Bayesian networks. Proceedings 12th Conference UncertaintyArtificial Intelligence, pp. 115123.Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes.Proceedings 14th Conference Uncertainty Artificial Intelligence, pp. 3342.Boyen, X., & Koller, D. (1999). Exploiting architecture dynamic systems. Proceedings16th National Conference Artificial Intelligence, pp. 313320.Brafman, R. (1997). heuristic variable grid solution method POMDPs. Proceedings14th National Conference Artificial Intelligence, pp. 727733.DAndrea, R., & Wurman, P. (2008). Future challenges coordinating hundreds autonomous vehicles distribution facilities. Proceedings IEEE InternationalConference Technologies Practical Robot Applications, pp. 8083.Dean, T., & Kanazawa, K. (1989). model reasoning persistence causation.Computational Intelligence, 5, 142150.Dias, M., Zlot, R., Kalra, N., & Stentz, A. (2006). Market-based multirobot coordination:survey analysis. Proceedings IEEE, 94 (7), 12571270.Doucet, A., de Freitas, N., & Gordon, N. (2001). Sequential Monte Carlo Methods Practice.Springer Science & Business Media.Doucet, A., De Freitas, N., Murphy, K., & Russell, S. (2000). Rao-Blackwellised particlefiltering dynamic Bayesian networks. Proceedings 16th ConferenceUncertainty Artificial Intelligence, pp. 176183.Geiger, D., Verma, T., & Pearl, J. (1989). d-separation: theorems algorithms.Proceedings 5th Conference Uncertainty Artificial Intelligence, pp. 139148.Gordon, N., Salmond, D., & Smith, A. (1993). Novel approach nonlinear/non-GaussianBayesian state estimation. IEE Proceedings F (Radar Signal Processing), Vol.140, pp. 107113.Hart, P., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determinationminimum cost paths. IEEE Transactions Systems Science Cybernetics,Vol. 4, pp. 100107.Hauskrecht, M. (2000). Value-function approximations partially observable Markovdecision processes. Journal Artificial Intelligence Research, 13, 3394.1176fiExploiting Causality Selective Belief Filtering DBNsHeckerman, D. (1993). Causal independence knowledge acquisition inference.Proceedings 9th Conference Uncertainty Artificial Intelligence, pp. 122127.Heckerman, D., & Breese, J. (1994). new look causal independence. Proceedings10th Conference Uncertainty Artificial Intelligence, pp. 286292.Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning acting partiallyobservable stochastic domains. Artificial intelligence, 101 (1), 99134.Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles Techniques.MIT Press.Kullback, S., & Leibler, R. (1951). information sufficiency. Annals Mathematical Statistics, 22 (1), 7986.Lauritzen, S., & Spiegelhalter, D. (1988). Local computations probabilities graphicalstructures application expert systems. Journal Royal StatisticalSociety. Series B (Methodological), 50 (2), 157224.Lovejoy, W. (1991). Computationally feasible bounds partially observed Markov decisionprocesses. Operations Research, 39, 162175.Mainzer, K. (2010). Causality natural, technical, social systems. European Review,18, 433454.Minka, T., Winn, J., Guiver, J., & Knowles, D. (2012). Infer.NET 2.5.. Microsoft ResearchCambridge. http://research.microsoft.com/infernet.Murphy, K. (2001). Bayes net toolbox Matlab. Computing Science Statistics,33 (2), 10241034. https://code.google.com/p/bnt/.Murphy, K., & Weiss, Y. (2001). factored frontier algorithm approximate inferenceDBNs. Proceedings 17th Conference Uncertainty Artificial Intelligence,pp. 378385.Murphy, K. (2002). Dynamic Bayesian Networks: Representation, Inference Learning.Ph.D. thesis, University California, Berkeley.Ng, B., Peshkin, L., & Pfeffer, A. (2002). Factored particles scalable monitoring.Proceedings 18th Conference Uncertainty Artificial Intelligence, pp. 370377.Pasula, H., Zettlemoyer, L., & Kaelbling, L. (2007). Learning symbolic models stochasticdomains. Journal Artificial Intelligence Research, 29, 309352.Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann.Pearl, J. (2000). Causality: Models, Reasoning, Inference. Cambridge University Press.Pineau, J., Gordon, G., & Thrun, S. (2003). Point-based value iteration: anytime algorithmPOMDPs. Proceedings 18th International Joint Conference ArtificialIntelligence, Vol. 18, pp. 10251032.Poole, D., & Zhang, N. (2003). Exploiting contextual independence probabilistic inference.Journal Artificial Intelligence Research, 18, 263313.1177fiAlbrecht & RamamoorthyPoupart, P., & Boutilier, C. (2000). Value-directed belief state approximation POMDPs.Proceedings 16th Conference Uncertainty Artificial Intelligence, pp.497506.Poupart, P., & Boutilier, C. (2001). Vector-space analysis belief-state approximationPOMDPs. Proceedings 17th Conference Uncertainty ArtificialIntelligence, pp. 445452.Poupart, P., & Boutilier, C. (2002). Value-directed compression POMDPs. AdvancesNeural Information Processing Systems, pp. 15471554.Roy, N., Gordon, G., & Thrun, S. (2005). Finding approximate POMDP solutionsbelief compression. Journal Artificial Intelligence Research, 23, 140.Smith, T., & Simmons, R. (2005). Point-based POMDP algorithms: improved analysisimplementation. Proceedings 21st Conference Uncertainty ArtificialIntelligence, pp. 542549.Sondik, E. (1971). Optimal Control Partially Observable Markov Processes. Ph.D.thesis, Stanford University.Srinivas, S. (1993). generalization noisy-or model. Proceedings 9th ConferenceUncertainty Artificial Intelligence, pp. 208215.Washington, R. (1997). BI-POMDP: bounded, incremental partially-observable Markovmodel planning. Recent Advances AI Planning, pp. 440451. Springer.Wolpert, D., & Macready, W. (1995). free lunch theorems search. Tech. rep. SFI-TR95-02-010, Santa Fe Institute.Wolpert, D., & Macready, W. (1997). free lunch theorems optimization. IEEETransactions Evolutionary Computation, 1 (1), 6782.Wurman, P., DAndrea, R., & Mountz, M. (2008). Coordinating hundreds cooperative,autonomous vehicles warehouses. AI Magazine, 29 (1), 9.Zhang, N., & Poole, D. (1996). Exploiting causal independence Bayesian network inference.Journal Artificial Intelligence Research, 5, 301328.Zhou, R., & Hansen, E. (2001). improved grid-based approximation algorithmPOMDPs. Proceedings 17th International Joint Conference ArtificialIntelligence, pp. 707716.1178fiJournal Artificial Intelligence Research 55 (2016) 653-683Submitted 06/15; published 03/16Exact Algorithms MRE InferenceXiaoyuan ZhuChanghe YuanXIAOYUAN . ZHU @ QC . CUNY. EDUCHANGHE . YUAN @ QC . CUNY. EDUQueens College, City University New York65-30 Kissena Blvd., Queens, NY 11367AbstractRelevant Explanation (MRE) inference task Bayesian networks findsrelevant partial instantiation target variables explanation given evidence maximizingGeneralized Bayes Factor (GBF). exact MRE algorithm developed previouslyexcept exhaustive search. paper fills void introducing two Breadth-First Branch-andBound (BFBnB) algorithms solving MRE based novel upper bounds GBF. One upperbound created decomposing computation GBF using target blanket decompositionevidence variables. upper bound improves first bound two ways. Onesplit target blankets large converting auxiliary nodes pseudo-targetsscale large problems. perform summations instead maximizationstarget variables target blanket. empirical evaluations show proposedBFBnB algorithms make exact MRE inference tractable Bayesian networks couldsolved previously.1. IntroductionBayesian networks probabilistic models capture conditional independenciesrandom variables directed acyclic graphs, provide principled approaches scientific explanation. Explanation tasks Bayesian networks classified three categories: explanationreasoning, explanation model, explanation evidence (Lacave & Diez, 2002). goalexplanation reasoning Bayesian networks explain reasoning process used produce results credibility results established. goal explanationmodel present knowledge encoded Bayesian network easily understandable formsvisual aids experts users examine even update knowledge. goalexplanation evidence explain observed variables particular statesusing variables domain.research focuses developing algorithms solving one methods explainingevidence Bayesian networks, Relevant Explanation (MRE) (Yuan & Lu, 2007; Yuan,Lim, & Lu, 2011b). idea MRE find partial instantiation target variablesmaximizes Generalized Bayes Factor (GBF) (Fitelson, 2007; Good, 1985) explanationevidence. GBF rational function probabilities suitable comparing explanationsdifferent cardinalities. MRE shown theoretically empirically able pruneaway independent less relevant variables final explanation (Yuan et al., 2011b; Yuan,Liu, Lu, & Lim, 2009; Pacer, Lombrozo, Griffiths, Williams, & Chen, 2013).Due difficulty finding meaningful upper bound GBF, exact algorithmsdeveloped solve MRE except exhaustive search. local search Markov chain MonteCarlo methods proposed previously (Yuan et al., 2009; Yuan, Lim, & Littman, 2011a).c2016AI Access Foundation. rights reserved.fiZ HU & UANpaper, introduce first non-trivial exact MRE algorithms based Breadth-First Branchand-Bound (BFBnB) search. key idea proposed methods decompose wholeBayesian network set overlapping subnetworks using target blanket decompositionevidence variables. subnetwork characterized subset evidence variables targetblanket d-separates evidence variables target evidence variables. upperbound GBF derived solving independent optimization problems subnetworks.also show bound tightened merging target blankets share target variables.propose another improved upper bound based two novel ideas. First,decomposition may lead large target blankets prevent branch-and-bound algorithmscaling large MRE problems. address problem, propose split large target blanketsconverting auxiliary nodes pseudo-targets introduce additional decomposition. Second,find upper bound tightened identifying summing enclosed-targetstarget blanket. proposed upper bounds used BFBnB algorithms pruningsearch space. evaluated algorithms set benchmark diagnostic Bayesian networks.Experimental results show proposed algorithms make exact MRE inference tractableBayesian networks could solved previously.rest paper organized follows. basics Bayesian networks MREproblem introduced Section 2. Section 3, novel target blanket upper bound proposed.improved upper bound discussed Section 4. proposed BFBnB algorithms introduced Section 5. Section 6, experimental results presented. Finally, discussionsconclusions provided Section 7.2. Backgroundsection, introduce basics Bayesian networks, explanation Bayesian networks,MRE problem.2.1 Bayesian Networks Moral GraphBayesian network (Pearl, 1988; Darwiche, 2009; Koller & Friedman, 2009) representeddirected acyclic graph (DAG). nodes DAG represent random variables. lack arcsDAG define conditional independence relations among nodes. arcnode X, say parent X, X child . use upper-case lettersdenote variables X variable sets X, lower-case letters values scalars x vectorsx. node ancestor node X directed path X. Let (X)denote ancestors X, smallest ancestral set (X) node set X defined(X) = X (Xi X (Xi )). directed graphs, d-separation describes conditionalindependence relation two sets nodes X Y, given third set nodes Z, i.e.,p(X|Z, Y) = p(X|Z). Markov blanket X smallest node set d-separates XremainingQ nodes network. network whole represents joint probabilitydistribution X p(X|PA(X)), PA(X) set parents X.moral graph Gm DAG G undirected graph set nodes.edge X Gm edge Gparents node G. undirected graph, Z separates X Y, Z interceptspaths X Y. Moral graphs powerful construction explain d-separation.654fiE XACT LGORITHMS MRE NFERENCELemma 1 (Lauritzen, Dawid, Larsen, & Leimer., 1990) links d-separation DAG separationundirected graphs.Lemma 1. Let X, Y, Z disjoint subsets nodes DAG G. Z d-separates XZ separates X (GAN (XYZ) )m , (GAN (XYZ) )m moral graphsubgraph G node set (X Z).2.2 Explanation Evidence Bayesian NetworksNumerous methods developed explain evidence Bayesian networks.methods make simplifying assumptions focus singleton explanations (Heckerman, Breese,& Rommelse, 1995; Jensen & Liang, 1994; Kalagnanam & Henrion, 1988). However, singletonexplanations may underspecified unable fully explain given evidence evidencecompound effect multiple causes.domain multiple interdependent target variables, multivariate explanations oftenappropriate explaining given evidence. Maximum Posteriori assignment (MAP) findscomplete instantiation set target variables maximizes joint posterior probabilitygiven partial evidence variables. Recently Kwishthout (2013) extended MAP findset joint assignments inforbable explanations. Probable Explanation (MPE) (Pearl,1988) similar MAP except MPE defines target variables unobserved variables.common drawback methods often produce hypotheses overspecifiedmay contain irrelevant variables explaining given evidence.Everyday explanations necessarily partial explanations (Leake, 1995). Various pruning techniques used avoid overly complex explanations. methods groupedtwo categories: pre-pruning post-pruning. Pre-pruning methods use context-specific independence relations represented Bayesian networks prune irrelevant variables (Pearl, 1988;Shimony, 1993; van der Gaag & Wessels, 1993, 1995) applying methods MAPgenerate explanations. contrast, post-pruning methods first generate explanations using methodsMAP MPE prune variables important. example methodproposed de Campos et al. (2001).Several methods aim directly find appropriate explanations. likelihood evidenceused measure explanatory power explanation (Gardenfors, 1988). ChajewskaHalpern (1997) extend approach use value pair <likelihood, prior probability>order explanations, forcing users make decisions clear order twoexplanations. Henrion Druzdzel (1991) assume system set pre-defined explanation scenarios organized tree; use scenario highest posterior probabilityexplanation. Flores et al. (2005) propose automatically create explanation tree greedilybranching informative variable step maintaining probabilitybranch tree certain threshold. Nielsen et al. developed another method usescausal information flow (Ay & Polani, 2008) select variables expand explanation tree.2.3 Relevant Explanationexplanation evidence Bayesian networks, often classify nodes three categories:target, evidence, auxiliary. target set represents variables interest inference.evidence set E represents observed information. auxiliary set represents variables655fiZ HU & UANinterest inference. MRE, finds partial instantiation explanationgiven evidence e Bayesian network, formally defined follows (Yuan et al., 2011b).Definition 1. Let set targets, e given evidence Bayesian network.Relevant Explanation problem finding partial instantiation x maximumgeneralized Bayes factor score GBF (x; e) explanation e, i.e.,MRE (M; e) = argmax GBF (x; e),(1)x,XMGBF definedGBF (x; e) =p(e|x) 1,p(e|x)(2)x joint value assignment (instantiation) subset X M, x representsalternative explanations x.commonly used measure selecting explanatory hypothesis probability explanation given evidence, used MAP MPE find likely configuration settarget variables. Probability-based methods, however, intrinsic capability pruneless relevant facts. Moreover, probability measure quite sensitive modeling choices,e.g., simply refining model dramatically change best explanation. contrast, MRE maximizes rational function probabilities GBF Equation 2. makes possible compareexplanations different cardinalities prune less relevant variables automaticallyprincipled way. search space MRE exponential number targets, complexity MRE conjectured N P P P -hard (Yuan et al., 2011a), makes naive brute forcealgorithm impractical.study properties generalized Bayes factor, reformulate GBF follows.GBF (x; e) ==p(e|x)p(x|e)p(x)=p(e|x)p(x)p(x|e)p(x|e)(1 p(x)).p(x)(1 p(x|e))(3)Therefore, need prior posterior probabilities hypothesis order computeGBF. GBF hence able overcome drawback Bayes factor (Jeffreys, 1961)pairwise comparisons multiple hypotheses.Belief update ratio useful concept. belief update ratio X given e definedfollows (Yuan et al., 2011b).p(X|e).(4)r(X; e) =p(X)GBF calculated belief update ratio follows.p(x|e)(1 p(x))r(x; e) p(x|e)=p(x)(1 p(x|e))1 p(x|e)r(x; e) 1= 1+.1 p(x|e)GBF (x; e) =1. use p(x) shorthand p(X = x) paper.656(5)fiE XACT LGORITHMS MRE NFERENCE2.4 Existing Methods Solving MRELocal search methods tabu search (Glover, 1990) applied solve MRE (Yuanet al., 2011a). Tabu search starts empty solution set. step, generates neighborscurrent solution adding, changing, deleting one target variable. tabu search selectsbest neighbor highest GBF score visited before. tabu search,best neighbor worse current solution. stop tabu search properly, upper boundsset total number search steps number search steps since lastimprovement L stopping criteria. Another Markov Chain Monte Carlo algorithm integratesreversible-jump MCMC algorithm (Green, 1995) simulated annealing (Kirkpatrick, Gelatt,& Vecchi, 1983) find solution simulating non-homogeneous Markov chain eventuallyconcentrates mass mode distribution GBF scores solutions. methodsprovide approximate solutions whose quality unknown. Furthermore, accuracyefficiency methods typically highly sensitive tunable parameters.2.5 Branch Bound Algorithms Solving MPE/MAPBranch-and-bound algorithms developed solving MAP MPE using upperbounds derived based property optimization criterion structure Bayesian networks. mini-bucket upper bound used AND/OR tree search solving MPE (Dechter& Rish, 2003; Marinescu & Dechter, 2009). Recently, improved mini-bucket upper bound (Marinescu, Dechter, & Ihler, 2014) proposed guide AND/OR search exact MAP inference.work (Choi, Chavira, & Darwiche, 2007) showed mini-bucket upper boundderived node splitting scheme. solve MAP exactly, upper bound proposed commuting order max sum operations MAP calculation (Park & Darwiche, 2003).exact algorithm proposed solving MAP computing upper bounds arithmetic circuitcompiled Bayesian network (Huang, Chavira, & Darwiche, 2006).3. Novel Upper Bound Based Target Blanket Decomposition Solving MREdifficult solve MRE problems exactly exponential search spaceneed probabilistic inference search step. naive brute-force search method scaleBayesian networks 15 targets. work, develop breadth-first branch-and-boundalgorithms use suite new upper bounds based target blanket decomposition prunesearch space. algorithm makes possible solve MRE problems targets exactly.3.1 Search Space FormulationAssuming n targets, target states, search space MRE contains (d +1)n 1 possible states (or solutions). organize search space search tree instantiatingtargets according total order targets. state tree contains valuessubset targets V. V defined expanded set targets. set variables Uyet considered expansion defined unexpanded set; i.e., U = {Ui |Ui M; VjV, Vj < Ui }; set variables P considered used definedpruned set; i.e., P = \ {V U}. Figure 1 demonstrates different target sets state{X1 , X4 , X6 } 9-target MRE problem.657fiZ HU & UANPruned (P)X1X3X2X5X4X6 X7Expanded (V)X8X9Unexpanded (U)Figure 1: Different types targets search state MRE, i.e., expanded (gray), pruned (white),unexpanded (black).ababacbacabbabaccacbccbcbcbcabc abc abc abc abc abc abc abcFigure 2: search tree MRE problem three targets, i.e., A, B, C. examplesub-tree rooted {b} marked gray.search tree empty state root. non-leaf state tree numberchildren states instantiate one unexpanded target. Figure 2 shows example search treethree targets = {a, a}, B = {b, b}, C = {c, c} order. Differentbranches search tree may different numbers layers, statescardinality appear layer.possible use dynamic ordering expand targets improve GBF scorefirst. However, shown static ordering actually make computing upper bounds muchfaster ultimately make search efficient (Yuan & Hansen, 2009). therefore simplyordered targets according indices work.3.2 Upper Bound Based Target Blanket DecompositionMRE inference, upper bound state greater GBF score descendant states S. search, keep track highest-scoring state prunewhole subtree upper bound less GBF current best solution. followingintroduce novel upper bound MRE inference.first define concept called target blanket decomposition evidence variables.Definition 2. target blanket decomposition evidence variables tuple < EvdList, TBList >satisfies following properties: 1) EvdList set exclusive exhaustive subsets,i.e.,EvdList = {Ei }, E = Ei Ei Ej = 6= j. 2) TBList set target blankets,one blanket TB (Ei ) evidence subset Ei , TB (Ei ) minimal set targetsd-separates Ei targets evidence variables.658fiE XACT LGORITHMS MRE NFERENCEtarget blanket decomposition naturally decomposes whole network overlappingsubnetworks, subnetwork containing evidence subset Ei target blanket TB (Ei ).Target blankets named due resemblance Markov blankets, strictly speaking,really Markov blankets evidence variables, may auxiliaryvariables them.given target set evidence set E, may exist multiple target blanket decompositions satisfy Definition 2. see that, note simply merge two evidencesubsets corresponding target blankets decomposition obtain another decomposition. However, among possible decompositions, minimal target blanket decomposition,define follows.Definition 3. minimal target blanket decomposition evidence variables target blanketdecomposition < EvdList, TBList > proper subset Ei EvdList validtarget blanket.minimal target blanket decomposition must unique according following theorem.Theorem 1. minimal target blanket decomposition evidence set E unique.Proof. Proof contradiction: assume two minimal target blanket decompositions D1D2 . Based property minimal target blanket decomposition, evidence subset D1proper subset evidence subset D2 , vice versa. must exist two distinct1evidence subsets EDD1 EjD2 D2 whose overlap Eij empty. musttrue Eij d-separated target evidence variables given subset2TB (EiD1 ) TB (EDj ), contradicting definition.immediately follows non-minimal target blanket decompositions generatedperforming merging minimal decomposition.upper bound GBF derived multiplying upper bounds belief update ratioscalculated subnetworks. first derive upper bound belief update ratiofollowing theorem.Theorem 2. Let = {X1 , X2 , . . . , Xn } set targets, e evidence, < EvdList,TBList > target blanket decomposition E, EvdList = {Ei } TBList ={TB (Ei )}. Then, subset X M, belief update ratio r(x; e) upper boundedfollows:!maxx,XMC =Qp(ei )r(x; e)maxz,Z=TB (Ei )p(e).659r(z; ei )C,(6)fiZ HU & UANProof.formulation r(M; e),r(M; e) = p(M|e) p(M)= p(e|M) p(e)=p(ei |TB (Ei ))/p(e)p(TB (Ei )|ei )p(ei )=p(TB (Ei ))/p(e)!=r(TB (Ei ); ei )p(ei ) /p(e).(7)third equality based property target blankets. Thus,!r(M; e) =r(TB (Ei ); ei ) C,(8)C =r(m; e),Qp(ei ) p(e). Equation 8, immediately following upper bound!max r(m; e)maxz,Z=TB (Ei )C.r(z; ei )(9)X M, X = M\X, let Si = TB (Ei ) X denote subset targetssummed ith subnetwork, Si = TB (Ei ) X. Thus, separate TB (Ei )two parts, i.e., related unrelated summation X, indexed = {i : Si 6= }J = {j : Sj = } respectively. Based definition, have:Xp(e|X) =p(e|M)p(X|X)X!X=iIXp(ei |TB (Ei )) p(X|X)p(ej |TB (Ej ))jJ!max p(ei |Si si )iIsip(ej |TB (Ej )) .(10)jJlast inequality derived using maximization instead averaging X. Since p(e|X) =r(X; e)p(e), following upper bound r(x; e) performing maximization accordingX sides Equation 10:max r(x; e) = maxxxiI=iIp(e|x)p(e)max maxsisimaxp(ei |si si )p(ei )p(ei )!r(z; ei )z,Z=TB (Ei )jJ660!jJmaxp(ej |z)p(ej ) 1p(ej )p(e)z,Z=TB (Ej )maxz,Z=TB (Ej )r(z; ej ) C.(11)fiE XACT LGORITHMS MRE NFERENCEThus, X M, obtain final upper bound combining Equations 9 11.!max r(x; e)max r(z; ei ) C.x,XMz,Z=TB (Ei )MRE inference, evidence e given, thus C constant. Theorem 2 assumes V = ,true beginning search. search V 6= , followingcorollary derive upper bound belief update ratio.Corollary 3. Let = {X1 , X2 , . . . , Xn } set targets, e evidence, < EvdList,TBList > target blanket decomposition E, EvdList = {Ei } TBList ={TB (Ei )}. Let U V unexpanded expanded target sets state S. Let Ti =TB (Ei ) V, Ti = TB (Ei )\Ti . Then, subset X U, belief update ratior(x v; e) upper bounded follows.!max r(x v; e)max r(z ti ; ei ) C,(12)x,XUC =Qp(ei )z,Z=Tip(e).Based Corollary 3, derive upper bound GBF Theorem 4:Theorem 4. Let = {X1 , X2 , . . . , Xn } set targets, e evidence, < EvdList,TBList > target blanket decomposition E, EvdList = {Ei } TBList ={TB (Ei )}. Let U V unexpanded expanded target sets state S. Let Ti =TB (Ei ) V, Ti = TB (Ei )\Ti . Then, subset X U, generalized Bayesianfactor score GBF (x v; e) upper bounded follows.max r(z ti ; ei ) C 1GBF (x v; e) 1 +maxz,Z=Ti,1 p(v|e)x,XU(13)QC = p(ei ) p(e).Proof. First, formulate GBF using belief update ratio Equation 5.GBF (m; e) = 1 +r(m; e) 1.1 p(m|e)subset X U, p(x v|e) = p(x|v, e) p(v|e) p(v|e). Thusr(x v; e) 1maxmaxGBF (x v; e) 1 +x,XUx,XU.1 p(v|e)(14)using Corollary 3, obtain following upper bound GBF:!maxGBF (x v; e) 1 +x,XUC =Qp(ei )p(e).661max r(z ti ; ei )z,Z=Ti1 p(v|e)C 1,fiZ HU & UANMoral graphDirected subgraphSplitted graphTB(E2)TB(E1,E2)GHJE1E2FE3K(A)BGCHLJE1E2FE3KBGCHLJTB(E1)BE1E2FE3KTB(E3)TB(E3)(B)(C)CLFigure 3: example compiling splitting target blanket decomposition. E1 , E2 , E3evidence nodes. Gray nodes targets. Others auxiliary nodes. original targetblanket TB (E1 , E2 ) split TB (E1 ) TB (E2 ) converting auxiliary nodepseudo-target.Using Equation 13, bound descendant states current state S. Equation 13shows MRE problem (left), need search subsets targets findbest solution. However, calculate upper bound (right), need search fixed targetset TB (Ei ) subnetwork, usually small size easy compute.instantiation z TB (Ei ), calculate r(z; ei ) store table called belief ratio table.one table subnetwork. tables computed preprocessing steplooked step search compute upper bound state.3.3 Compiling Minimal Target Blanket DecompositionTheorems 2- 4 based factorizingconditional joint distribution p(e|M) productQset conditional distributions p(ei |TB (Ei )). Thus finding target blanket decomposition,including evidence subsets target blankets, key part proposed methods.proposed method, compile minimal target blanket decomposition based Lemma 1first compiling moral graph. moral graph used prune irrelevant parts networkset network separation tests. compile moral graph, first generate smallestancestral set containing target set evidence set E, i.e., (ME). compilemoral graph (GAN (ME) )m . Figure 3(B) illustrates example moral graph compiledBayesian network three evidence nodes Figure 3(A). Using Lemma 1, minimal target blanket decomposition evidence variables achieved depth first graph traversal startingunvisited evidence node moral graph (GAN (ME) )m . three scenariosnode visited.Case 1: evidence node visited, add evidence current evidence subsetEi , mark visited, continue visit unmarked neighbors.Case 2: target visited, add target TB (Ei ) mark visited.662fiE XACT LGORITHMS MRE NFERENCEAlgorithm 1 Compiling Minimal Target Blanket DecompositionInput: target set; E evidence set; MGraph moral graph (GAN (ME) )m .Output: Target blanket decomposition < EvdList, TBList >, EvdList = {Ei },TBList = {TB (Ei )}.1: function C OMPILE INIMALTARGET B LANKETS(M, E, MGraph)2:EvdList ;3:TBList ;4:node Nd MGraph5:Nd E Nd visited6:EvdSeti ;7:TBSeti ;8:SearchStack Nd ;. initialize stack Depth-first search9:SearchStack empty10:SearchNode SearchStack .pop();11:SearchNode visited12:continue;13:end14:mark SearchNode visited;15:SearchNode E. Case 116:EvdSeti .push(SearchNode);17:SearchStack .push(SearchNodes unvisited neighbors);18:else SearchNode. Case 219:TBSeti .push(SearchNode);20:else SearchNode auxiliary node. Case 321:SearchStack .push(SearchNodes unvisited neighbors);22:end23:end24:EvdList.push(EvdSeti );25:TBList.push(TBSeti );26:end27:mark targets auxiliary nodes MGraph unvisited;28:end29: end functionCase 3: auxiliary node visited, mark visited continue visitunmarked neighbors.restarting search new evidence node, unmark targets auxiliary nodes,targets may occur different target blankets, e.g., node F shared TB (E1 , E2 )TB (E3 ) shown Figure 3(B). algorithm stops evidence nodes visited. algorithm summarized Algorithm 1. Furthermore, show Algorithm 1guaranteed find minimal target blanket decomposition evidence set E.Theorem 5. Algorithm 1 guaranteed find minimal target blanket decomposition evidencevariables.663fiZ HU & UANProof. evidence E, Algorithm 1 stops search path target encountered.evidence variables belong evidence subset Ei E minimal target blanketdecomposition must visited search paths terminated. Furthermore, since startsearch one evidence E Ei , must true search stops minimaltarget blanket TB (Ei ) fully visited.3.4 Merging Target Blanketscomputing upper bound, maximize belief update ratio r(TB (Ei ); ei )TB (Ei ) independently. Thus common targets two different target blankets TB (Ei )TB (Ej ) may set inconsistent values. much inconsistency may result loose bound.tighten upper bound merging target blankets share targets. hand,number targets individual target blanket large, make calculatingbelief update ratio tables inefficient even infeasible due excessive memory consumption.propose merge target blankets share targets constraint number targetsresulting target blanket cannot exceed constant K.use undirected graph represent problem merging target blankets. nodesgraph denote target blankets. two target blankets share targets, edgetwo corresponding nodes. weight edge number targets shared two targetblankets. formulation translates problem merging target blankets graph partitionproblem. specifically, merging problem addressed recursively solving minimum bisection problem (Feige & Krauthgamer, 2002) undirected graph, partitionsvertices two equal halves minimize sum weights edges twopartitions.minimum bisection problem NP-hard problem, however. cannot afford spendmuch time computing upper bound. therefore use hierarchical clustering-like greedyalgorithm merging target blankets. first merge pair target blankets onecovers other. remaining target blankets, repeatedly merge two target blanketsshare highest number targets long number targets resulting targetblanket exceed K. algorithm iterates two steps target blanketsmerged. merge algorithm summarized Algorithm 2.4. Improved Target Blanket Boundtarget blanket upper bound two potential difficulties scaling large Bayesian networks many target variables. First, minimal target blanket decomposition (in Section 3.3)lead large subnetworks belief ratio tables large build. Second, upperbound still loose even merging target blankets. section, propose another upper bound based two new techniques improving scalability tightness previousbound.4.1 Splitting Large Target Blanketsdecomposition method Section 3.3 may lead large target blankets prevent application BFBnB large scale MRE problems. address problem, notice MREproblems, subnetwork contains three types nodes, i.e., targets TB (Ei ), evidence Ei ,664fiE XACT LGORITHMS MRE NFERENCEAlgorithm 2 Merging Target BlanketsInput: target blanket decomposition < EvdList, TBList >, EvdList = {Ei },TBList = {TB (Ei )}; K maximum number targets target blanket.Output: updated decomposition < EvdList, TBList >.1: function ERGE TARGET B LANKETS(EvdList, TBList, K)2:MergeFlag True;3:MergeFlag4:TBListOld TBList;. TBi denotes TB (Ei )5:EvdListOld EvdList;6:(TBi , TBj ) pair, TBi , TBj TBListOld7:TBi TBj TBList. merge target blankets subset relation8:TBi TBj9:TBj .merge(TBi );10:TBList.remove(TBi );11:Ej .merge(Ei );12:EvdList.remove(Ei );13:else TBj TBi14:TBi .merge(TBj );15:TBList.remove(TBj );16:Ei .merge(Ej );17:EvdList.remove(Ej );18:end19:end20:end21:NumTargets 0;22:(TBi , TBj ) pair, TBi , TBj TBList23:(TBi TBj ).size() > NumTargets (TBi TBj ).size() < K24:TBPair (TBi , TBj );. find TB pair merge25:NumTargets (TBi TBj ).size();26:EvdPair (Ei , Ej );27:end28:end29:NumTargets 030:MergeFlag False;31:else32:TBPair .TBi .merge(TBPair .TBj );33:TBList.remove(TBPair .TBj );34:EvdPair .Ei .merge(EvdPair .Ej );35:EvdList.remove(EvdPair .Ej );36:end37:end38: end function665fiZ HU & UANauxiliary nodes Ai , TB (Ei ) marks boundary subnetworks, Ei Ai conditionally independent outside nodes given TB (Ei ). idea split large targetblankets converting auxiliary nodes pseudo-targets act targets compilingtarget blanket decomposition calculating belief ratio tables. pseudo-targets add additional d-separation Bayesian network, thus split large target blankets smaller ones.Theorems 4 6 guarantee that, split, still find upper bound GBF.Theorem 6. Let TB (Ei ) target blanket ith subset evidence Ei , Ai setauxiliary nodes ith subnetwork target blanket decomposition. Assumingconverting subset Ai pseudo-targets, ith subnetwork decomposedset target blankets TB (Eij ), TB (Ei ) Y=j TB (Eij ), Ei =j Eij , Eij Eik =j 6= k. Then, belief update ratio r(x; ei ) upper bounded follows.maxr(x; ei )maxr(z; eij ) C,(15)x,X=TB (Ei )C =Qjjz,Z=TB (Eij )p(eij ) p(ei ).Proof. Let X = TB (Ei ), Ai have,Xp(ei |X) =p(ei |X Y)p(Y|X)=XYjp(eij |TB (Eij ))p(Y|X)jmaxz,Z=TB (Eij )Yp(eij |TB (Eij )\Z, z).(16)second equality based property target blankets.Since p(ei |X) = r(X; ei )p(ei ), following upper bound r(x; ei ),maxr(x; ei )maxr(z; eij ) C,x,X=TB (Ei )C =Qjjz,Z=TB (Eij )p(eij ) p(ei ).introduce greedy algorithm convert auxiliary nodes pseudo-targets incrementallysize resulting target blanket exceed K. algorithm, split targetblankets whose sizes exceed K using following steps.Step 1: Calculate degree (i.e., number neighboring nodes) auxiliary nodeAij moral graph, sort according degrees descending order.Step 2: Convert one auxiliary node pseudo-target according order, performdepth-first search Section 3.3 compile minimal target blankets decompositionsubnetwork.Step 3: Stop size resulting target blanket exceed K, repeat Step 2auxiliary nodes converted.666fiE XACT LGORITHMS MRE NFERENCEAlgorithm 3 Splitting Target BlanketsInput: target blanket decomposition < EvdList, TBList >, EvdList = {Ei },TBList = {TB (Ei )}; K maximum number targets target blanket. MGraphmoral graph (GAN (ME) )m .Output: updated decomposition < EvdList, TBList >.1: function PLIT TARGET B LANKETS(EvdList, TBList, K, MGraph)2:TBi TBList3:TBi .size() > K4:Aij Ai descending degree . Ai auxiliary node set TBi5:convert Aij pseudo-target;6:< EvdList , TBList > CompileMinimalTargetBlankets(TBi ,Ei ,MGraph);7:TBij .size() < K, TBij TBList8:break;9:end10:end11:end12:end13: end functionalgorithm summarized Algorithm 3. example splitting target blanketillustrated Figure 3(C). original target blanket TB ({E1 , E2 }) = {I, F, B, D} splitTB (E1 ) = {A, B, D} TB (E2 ) = {A, F, I} converting auxiliary node pseudo-target.4.2 Tightening Upper Boundcombining Theorems 4 6, see splitting process makes upper bound loosemaximizations pseudo-targets. order maintain even improve searchefficiency, need re-tighten bound. key idea comes Theorem 4, Ticontains two types variables, i.e., pruned-targets unexpanded targets. Equation 13,pruned-targets unexpanded targets state maximized derive upper bound. Sincepruned-targets occur subsequently generated states, maximizing pruned-targetsmakes upper bound loose. However, directly summing pruned-targets subnetworkschange structure target blanket decomposition, thus makes upper bound invalid.notice certain targets summed instead maximized withoutaffecting decomposition. First, need define another type targets called enclosed-targetset H target blanket following.Definition 4. MRE inference, enclosed-targets H target blanket targetsconditionally independent variables outside target blanket given remaining variables target blanket.words, enclosed-targets targets blocked (d-separated) outsiderest target blanket. Definition 4, see enclosed-targetoccurs one target blanket. example Figure 3(B), J L enclosed-targetsTB (E3 )={F, J, L}.667fiZ HU & UANidea sum enclosed-targets individual subnetworks TB (Ei ) orderproduce tighter upper bound. following theorem.Theorem 7. Let = {X1 , X2 , . . . , Xn } set targets, e evidence, < EvdList,TBList > target blanket decomposition E, EvdList = {Ei } TBList ={TB (Ei )}. Let U, V, P unexpanded, expanded, pruned target sets state S.Let Ti = TB (Ei ) V, Ti = TB (Ei )\Ti . Let Hi enclosed-target set TB (Ei )= Hi P. Then, subset X U, belief update ratio r(x v; e) upperbounded follows.max r(x v; e)max r(z ti ; ei ) C,(17)x,XUC =Qp(ei )z,Z=Ti \Hisp(e).Proof. X U, let W = X V W = M\W. Similar Theorem 2, letSi = TB (Ei ) W, = {i : Si 6= }, J = {j : Sj = }, have,Xp(e|W) =p(e|M)p(W|W)W=XYWp(ei |TB (Ei )) p(W|W)p(ej |TB (Ej )).iI(18)jJEquation 18 based property target blankets.Assuming X = U\X, W = P X Si W. Since occursTB (Ei ), perform summation non-empty Equation 18 follows.Xp(ei |TB (Ei ))p(His |M\His )=X p(ei , |TB (Ei )\His )=Xp(His |TB (Ei )\His )p(His |M\His )p(ei , |TB (Ei )\His )= p(ei |TB (Ei )\His ),(19)p(His |TB (Ei )\His ) = p(His |M\His ), since conditionally independenttargets given TB (Ei )\His .Based Equation 19, obtain upper bound summing Equation 18replacing summation (Si \His ) maximization.p(e|W)max p(ei |TB (Ei )\Si , z)p(ej |TB (Ej )).(20)iIz,Z=Si \HisjJSince p(e|W) = r(W; e)p(e), following upper bound r(x v; e) re-organizingpartition TB (Ei ), i.e., Si based partition Ti based partition.max r(x v; e)max r(z ti ; ei )max r(z tj ; ej ) C,(21)x,XUiIz,Z=Ti \HisjJ668z,Z=TjfiE XACT LGORITHMS MRE NFERENCEHiBBCCCCFigure 4: Subproblem graph compiling belief ratio tables incremental algorithm.QC = p(ei ) p(e).Thus, X U, obtain tighter upper bound r(x v; e).max r(x v; e)max r(z ti ; ei ) C.x,XUz,Z=Ti \Hissubstituting Equation 17 14, tightened upper bound GBF Theorem 8.Theorem 8. Let = {X1 , X2 , . . . , Xn } set targets, e evidence, < EvdList,TBList > target blanket decomposition E, EvdList = {Ei } TBList ={TB (Ei )}. Let U, V, P unexpanded, expanded, pruned target sets state S.Let Ti = TB (Ei ) V, Ti = TB (Ei )\Ti . Let Hi enclosed-target set TB (Ei )= Hi P. Then, subset X U, generalized Bayesian factor scoreGBF (x v; e) upper bounded follows.max r(z ti ; ei ) C 1maxGBF (x v; e) 1 +x,XUC =Qp(ei )z,Z=Ti \His1 p(v|e),(22)p(e).main difference Theorems 4 8 Theorem 4 maximizes upperbound Theorem 8 sums results tighter upper bound.4.3 Compiling Belief Ratio Tablesbelief ratio tables contain belief update ratios configurations series targetsets generated based TB (Ei ), used calculate upper bounds MRE inference.compile belief ratio tables, find set enclosed-targets Hi converting targetTB (Ei ) auxiliary node individually. add new targets evidence nodesoriginal subnetwork, add target Hi . subsets Hi used build2|Hi | belief ratio tables TB (Ei ), |Hi | denotes size Hi . Let Hij subset Hi ,669fiZ HU & UANtarget set belief ratio table TB (Ei )\Hij . Since number belief ratio tablesincreases exponentially according |Hi |, limit |Hi | larger N timespace reasons. Different enclosed-targets contribute different amount tightening upper bound,thus select top N enclosed-targets Hi sorting enclosed-targets Hij accordingbelief update ratios maxhij r(hij ; ei ) descending order.compile belief ratio tables, need calculate belief update ratios configurations target set TB (Ei )\Hij . straightforward method compile 2|Hi | belief ratiotables independently. slow redundant computation. work, proposeincremental algorithm compiles belief ratio tables gradually smallest target setHi =TB (Ei )\Hi . Figure 4, assuming four enclosed-targets, i.e., Hi = {A, B, C, D},organized alphabetical order, node subproblem graph represents target. firstcompile belief ratio table Hi , traverse subproblem graph breadth-first order.state (node), compile belief ratio table adding target complied belief ratiotable parent state. traversing subproblem graph, algorithm compiles beliefratio tables. algorithm summarized Algorithm 4.proposed algorithm, calculate hash key HK target blanket based Hi .belief ratio tables target blanket stored hash table hash keys calculatedbased corresponding subset enclosed-targets Hij .5. Breadth-First Branch-and-Bound AlgorithmsMRE inference, search nodes potential solutions except root node.choose variety search methods explore search tree, e.g., depth-first search, best-firstsearch, breadth-first search. Since MRE prunes away independent less relevant targets,usually number targets optimal solution large. Thus breadth-first search mayreach optimal solutions faster search strategies. choose breadthfirst search MRE inference. breadth-first search may require memory storeunexpanded states layer, however.order utilize proposed upper bounds, BFBnB algorithms two major steps:preprocessing search. preprocessing step includes compiling minimal target blanket decomposition, merging target blankets sharing targets, splitting large target blankets, creatingbelief ratio tables TB (Ei ). suite upper bounds derived including different combinations preprocessing modules. empirical results show targetblanket upper bounds Theorems 4 8 show excellent performance.search step BFBnB explores search tree layer layer keeping trackhighest-scoring state, prunes state upper bound less current best GBF.proposed upper bounds lead two versions BFBnB algorithms, MPBnd SPBnd. MPBndbased upper bound Theorem 4, maximizes unexpanded targetspruned-targets. Thus, MPBnd, target blanket TB (Ei ) one belief ratio tablecomputed calculating belief update ratios configurations TB (Ei ). state S,search configuration belief ratio table consistent expanded targetsti highest belief update ratio. calculate upper bound using Theorem 4.SPBnd based upper bound Theorem 8, sums part pruned-targetsbased structure Bayesian networks. Thus, SPBnd, target blanket TB (Ei )set belief ratio tables derived calculating belief update ratios670fiE XACT LGORITHMS MRE NFERENCEAlgorithm 4 Compile Belief Ratio TablesInput: target set; E evidence set; e given evidence; MGraph moral graph(GAN (ME) )m ; target blanket decomposition < EvdList, TBList >, EvdList ={Ei }, TBList = {TB (Ei )}; N maximum number enclosed-targets targetblanket.Output: BeliefRatioTable belief ratio tables target blanket.1: function C OMPILE B ELIEF R ATIOTABLE(M, E, e, MGraph, EvdList, TBList, N )2:BeliefRatioTable ;3:TBi TBList4:Hi ;5:X TBi. generate enclosed-target set Hi6:change X auxiliary node;7:NewTBList CompileMinimalTargetBlankets(M, E, MGraph);8:NewTBi .push(X);9:NewTBi == TBi NewE == Ei10:Hi .push(X);11:end12:reset X target node;13:end14:Hi Hi .top(N );. select top N nodes according maxhij r(hij ; ei )15:SearchQueue ;16:BeliefRatioTablei ;17:Hi = TB \Hi ;18:instantiation hi Hi19:BeliefRatioTablei [Hash()].push(GBF (hi ; e));20:end21:SearchQueue.push();. contain Hij22:SearchQueue empty. compile belief ratio table TB23:Hij SearchQueue.pop();24:X > Hij , X Hi. order defined Hi25:instantiation x (X Hij Hi ). calculate incrementally26:BeliefRatioTablei [Hash(X Hij )].push(GBF (x; e));27:end28:SearchQueue.push(Hij X);29:end30:end31:BeliefRatioTable.push(BeliefRatioTablei );32:end33: end functionconfigurations TB (Ei )\Hij . algorithm calculating belief ratio tables SPBndsummarized Algorithm 4, includes algorithm used MPBnd special case.state S, calculate hash key SK based pruned-targets use SK &HK indexbelief ratio table target blanket. Then, search configuration selected671fiZ HU & UANAlgorithm 5 BFBnB Algorithm Based Target Blanket Upper BoundsInput: target set; E evidence set; e given evidence; MGraph moral graph(GAN (ME) )m ; K maximum number targets target blanket; N maximumnumber enclosed-targets target blanket.Output: BestExplanation arg maxx,XM GBF (x; e).1: function BFB N BS EARCH(M, E, e, MGraph, K, N )2:< EvdList, TBList > CompileMinimalTargetBlankets(M, E, MGraph);3:MergeTargetBlankets(EvdList, TBList, K);4:SplitTargetBlankets(EvdList, TBList, K, MGraph);5:TBi .size() < K, TBi TBList6:return None;. stop algorithm size TBi larger K7:end8:BeliefRatioTable CompileBeliefRatioTable(M,E,e,MGraph,EvdList,TBList,N );9:openList state x X M;. initialize openList;10:maxGBF openList.top();11:openList empty12:x openList.pop();13:state U. U unexpanded set current state x14:sucState {y} x;15:UpperBound CalcUpperBound (sucState, BeliefRatioTable);16:UpperBound maxGBF17:continue;. use UpperBound prune expanded state sucState18:end19:maxGBF < GBF (sucState; e)20:maxGBF GBF (sucState; e);21:BestExplanation {y} x;22:end23:openList.push(sucState);24:end25:end26: end function27: function C ALC U PPER B OUND(sucState, BeliefRatioTable)28:BeliefRatioTable BeliefRatioTable29:BeliefRatioTable Hij BeliefRatioTable [Hash(P)&Hash(Hi )];30:MaxBeliefRatio maximum belief ratio consistent v sucState;31:endQMaxBeliefRatio C132:UpperBound 1 +;1p(v|e)33: end functionbelief ratio table consistent expanded targets ti highest belief updateratio. Finally, calculate upper bound using Theorem 8.BFBnB algorithm summarized Algorithm 5. main difference MPBndSPBnd methods used generate belief ratio table, i.e., Line 8 Algorithm 5.speed search process, sort belief ratio table target blanket descending order,672fiE XACT LGORITHMS MRE NFERENCENetworksAlarmCarpoHeparInsuranceEmdec6hCPCS179Nodes37617027168179Leaves1143416117151States2.842.232.313.302.002.29Arcs467412352261239Table 1: Benchmark diagnostic Bayesian networks used evaluate proposed algorithms.i.e., higher belief update ratios closer front table. Furthermore, search treeMRE, expanded targets state guaranteed included descedant states. Thusproposed method, record indices best belief update ratios, one beliefratio table, expanded state. calculate upper bound current state, needsearch best belief update ratio recorded indices parent.6. Experimentsproposed algorithms evaluated six benchmark diagnostic Bayesian networks listed Table 1, i.e., Alarm (Ala), Carpo (Car), Hepar (Hep), Insurance (Ins), Emdec6h (Emd), CPCS179(Cpc) (Beinlich, Suermondt, Chavez, & Cooper, 1989; Binder, Koller, Russell, & Kanazawa, 1997;Onisko, 2003; Pradhan, Provan, Middleton, & Henrion, 1994). Among them, Alarm, Carpo, Hepar,Insurance networks fewer 100 nodes. Emdec6h CPCS179 larger networks 100 nodes. listed number nodes (Nodes), number leaf nodes(Leaves), average number node states (States), number arcs (Arcs) Bayesiannetworks Table 1. experiments performed 2.67GHz Intel Xeon CPU E7 512GRAM running 3.7.10 Linux kernel.6.1 Experimental DesignSince proposed algorithms, MPBnd SPBnd, first nontrivial exact MRE algorithms,use naive Breadth-First Brute-Force search algorithm (BFBF) baseline; basicallyBFBF BFBnB bound set infinity. also included results tabu searchindicate difficulty MRE problems. MPBnd SPBnd, set maximum numbertargets target blanket K 18. SPBnd, set maximum number enclosed-targetstarget blanket N 7. BFBF search solve test cases fewer 15 targets.compare performance among BFBF, MPBnd, SPBnd, perform experimentstwo test settings, one exactly 12 targets (12-target setting) around 20 targets(difficult-target setting). 12-target setting, randomly generated five test settingsnetwork, setting consisting leaf nodes evidence, 12 remaining nodes targets,others auxiliary nodes. setting, randomly generated 20 configurationsevidence (test cases) sampling prior distributions networks.difficult-target setting, randomly generated five test settings network,setting consisting leaf nodes evidence, around 20 remaining nodes targets,others auxiliary nodes. number targets selected test cases challenging673fiZ HU & UAN#Cases/TimeBFBFMPBndSPBndT6400T3200T1600T800T400Ala1.7e317.02.69217.3859.1794.7762.4721.2Car66.13.51.510025.410013.21006.9983.6981.8Hep270.014.69.09344.89122.68611.5815.9813.0Ins1.6e51.6e372.78126.37713.7747.0733.6731.8Emd212.015.113.19589.39545.09523.29511.8956.1Cpc1.3e4815.0377.090108.09055.19028.09014.1907.0Table 2: Comparison SPBnd, MPBnd, BFBF, tabu running time seconds (sec) wellaccuracy tabu searches Bayesian networks 12-target setting.BFBF still solvable MPBnd SPBnd. setting, randomly generated20 configurations evidence (test cases) sampling prior distributions networks.tabu search, set number search steps since last improvement L maximumnumber search steps according different network settings. 12-target setting, set L20 {400, 800, 1600, 3200, 6400}. difficult-target setting, set L 80{12800, 25600, 51200}. evaluate search performance, compared solutionstabu search SPBnd, counted number test cases tabu search achievedoptimal solutions.6.2 Evaluation MPBnd SPBnd 12-Target SettingTable 2, compared proposed MPBnd SPBnd algorithms BFBF tabu searchtest cases 12-target setting. tabu search, listed number test cases solvedoptimally (top) running time seconds (bottom) using different limits numbersearch steps. running time MPBnd SPBnd includes preprocessing time searchtime. MPBnd, SPBnd BFBF able solve test cases exactly. MPBnd shownsignificantly faster BFBF pruning upper bound. running timeSPBnd reduced using tightened upper bound. T400 fastest algorithmworst accuracy. increase , running time tabu search increased significantly.However, networks, tabu search could solve test cases optimally, evenusing running time MPBnd SPBnd.compare MPBnd, SPBnd BFBF detail, computed average running timeindividual test settings MPBnd, SPBnd BFBF. illustrate logarithmic runningtime pairs, BFBF vs MPBnd MPBnd vs SPBnd, points Figures 5 6 respectively.also draw contour lines mark difference logarithmic running timeFigures 5 6. example, contour line marked -3 Figure 5 contains points674fiE XACT LGORITHMS MRE NFERENCE7AlaCarHepInsEmdCpcMPBnd (log10ms)60-15-24-3356789BFBF (log10ms)Figure 5: Distributions logarithmic running time pairs milliseconds) MPBnd BFBFBayesian networks 12-target setting.6AlaCarHepInsEmdCpcSPBnd (log10ms)50-14-23-3234567MPBnd (log10ms)Figure 6: Distributions logarithmic running time pairs milliseconds SPBnd MPBndBayesian networks 12-target setting.MPBnd 1000 times faster BFBF. results show although average runningtime may change significantly, ratios running times BFBF MPBnd,MPBnd SPBnd relatively stable. MPBnd roughly 10 100 times faster BFBF.SPBnd roughly 3 4 times faster MPBnd.6.3 Evaluation MPBnd SPBnd Difficult-Target SettingTable 3, compared proposed SPBnd algorithm MPBnd tabu search testcases difficult-target setting. list number targets network first row675fiZ HU & UAN#Cases/TimeMPBndSPBndT51200T25600T12800Ala2017363604.06602.40601.42Car150.980.131005.551002.891001.50Hep224781067217.12659.38635.15Ins1743.9158.72760.96760.76760.59Emd20225968634.148617.48868.93Cpc201,569.2419826.11824.2822.7Table 3: Comparison SPBnd, MPBnd, tabu running time minutes (min) wellaccuracy tabu searches Bayesian networks difficult-target setting.table. tabu search, list number test cases solved optimally (top) runningtime minutes (bottom) network. Increasing 12800 51200 helpfulpreventing tabu search getting stuck local optima. Moreover, performance tabusearch varies greatly different test networks. SPBnd shown significantly fasterMPBnd networks due tightened upper bound. SPBnd, need compileseries belief ratio tables target blanket, may consume significant amountrunning time. example, although running time 419 minutes CPCS179, search took182.4 minutes. Also results, see running time MPBnd SPBnddepends number targets, also tightness upper bound Bayesiannetwork structures, control number pruned states size belief tabletarget blanket TB (Ei ) respectively.compare SPBnd MPBnd detail, computed average running time individual test settings them, illustrated running time pair logarithm pointFigure 7. also drew contour lines mark difference SPBnd MPBnd.results showed data points network form cluster. SPBnd roughly 3 4times faster MPBnd.results Figures 6 7 showed test cases running times MPBndSPBnd close. two possible reasons behind observation. First,network structures, enclosed-target sets may exist individual target blankets. Thuscases SPBnd degenerate MPBnd. Second, running time test case consiststwo parts, compiling belief ratio tables performing search. test cases, compilingbelief ratio tables may take significant amount time SPBnd. therefore make tradeoffcompiling time search time adjusting maximum number enclosed-targets Ntarget blanket.6.4 Scalability SPBndalso evaluated MPBnd SPBnd test cases increasing number targets 1725 increment 2 three Bayesian networks, Hepar, Emdec6h, CPCS179.target number i, randomly generated four test settings 5 test cases setting.676fiE XACT LGORITHMS MRE NFERENCE8AlaCarHepInsEmdCpcSPBnd (log10ms)760-0.5-15-1.5435678MPBnd (log10ms)Figure 7: Distributions logarithmic running time (ms) pairs SPBnd MPBnd test casesdifficult-target setting.TimeMPBndSPBndHepEmdCpcHepEmdCpc1764.763.6483.724.036.3130.719101.0182.415.0100.894.1Targets2123411.3643.466.9 243.1444.2499.6-25711.9-Table 4: Comparison SPBnd MPBnd running time (min) test cases increasingnumber targets. dash indicates time (800m).Bayesian networks, set leaf nodes evidence, remaining nodes targets, othersauxiliary nodes. set time limit running 800 minutes. results reportedTable 4. results show pruning upper bound slowed exponential growthrunning time significantly. SPBnd handle complex MRE problemsreach MPBnd. results, also see running time SPBnd affectedtightness upper bound structures Bayesian networks, longer heavily dependsnumber targets.6.5 Importance Splitting Large Target Blanketsshow importance splitting large target blankets, calculated percentage test casesneed splitting operation various target settings three Bayesian networks, Hepar, Emdec6h,CPCS179, increasing number targets. experiment, performed preprocessing step, handle 50 targets. target blankets test cases677fiZ HU & UANEmdCpcHep70Ratio (%)60504030201001520253035404550Number tagetsFigure 8: Percentage cases using splitting operation different target settings.Figure 8 split smaller ones size less K=18 using proposed splittingalgorithm. results Figure 8 show ratio test cases needs splitting increases,decreases increasing numbers targets. reason initially increasenumber targets leads large target blankets. number targets keeps increasing,densely distributed targets tend introduce d-separation Bayesian network resultsmaller target blankets. Figure 8, peaks curves located 22 approximately.networks large number non-leaf nodes, e.g., Emdec6h, ratio higher 0.5significant number target settings. means without splitting large target blanketsMRE problems cannot solved networks target settings.6.6 Effect Summing Enclosed-Targetssection, take Hepar, Emdec6h, CPCS179 examples illustrate summingenclosed-targets tightens target blanket upper bound. calculated log differenceupper bound UpBnd maximum GBF MaxGBF subtree rooted search statetest cases 12-target setting. Normalized histograms differences usingusing tightening plotted Figure 9. x-axis shows log10 (UpBnd MaxGBF ),y-axis shows normalized percentages. Thus smaller log10 (UpBnd MaxGBF ) indicatestighter upper bound. clear graph summing enclosed-targets makesbound much tighter.6.7 Convergence Upper Boundgain better perspective tightened upper bound Theorem 8 improves time,calculated maximum upper bound MaxBound layer search treecurrent maximum GBF CurrMax . also recorded running time finished searchlayer. Figure 10 shows convergence curves MaxBound (dotted line) CurrMax (circledline) search time one typical test case three networks, i.e., Hepar,Emdec6h, CPCS179, 17-target setting Table 4. Figure 10(A) shows upperbound dropped sharply beginning search decreased gradually search.678fiE XACT LGORITHMS MRE NFERENCE0.250.350.16TighteningTighteningProbabilityTighteningTighteningTighteningTightening0.120.250.150.080.150.040.050.056225651525015Log10(UpBnd-MaxGBF)Log10(UpBnd-MaxGBF)(A)551525Log10(UpBnd-MaxGBF)(B)(C)Figure 9: Tightness upper bounds. evaluate effect summing enclosed-targets usingnormalized histogram different Bayesian networks, i.e., Hepar (A), Emdec6h (B),CPCS179 (C).12307BoundCurrMaxBoundCurrMaxBoundCurrMax6Log10GBF1020584106312345Running time (log10ms)(A)1234Running time (log10ms)(B)512345Running time (log10ms)(C)Figure 10: Convergence upper bound current maximum GBF different Bayesian networks, i.e., Hepar (A), Emdec6h (B), CPCS179 (C).Figure 10(B) shows upper bound dropped sharply end search became tightquickly. Figure 10(C) shows upper bound dropped sharply beginningend search. results demonstrate different behaviors bound tighteningsearch.7. Discussions Conclusionsmain contributions paper two BFBnB algorithms, i.e., MPBnd SPBnd, solving MRE exactly using upper bounds based target blanket decomposition evidence variables.first non-trivial exact algorithms solving MRE Bayesian networks. keyidea proposed method decompose conditional joint probability p(E|M) setmarginal probabilities p(Ei |TB (Ei )). marginal probability related subnetwork characterized target blanket subset evidence variables. upper bound GBF derived679fiZ HU & UANmaximizing belief update ratio fixed target set TB (Ei ) subnetwork separately.upper bound tightened merging target blankets sharing set targets.bound improved based two ideas. First, proposed split largetarget blankets converting auxiliary nodes pseudo-targets, scales MRE inferencelarger Bayesian networks targets. Second, tightened upper bound GBFidentifying summing enclosed-targets target blanket TB (Ei ). new upperbound reduces search space dramatically. experimental results show SPBnd significantly faster MPBnd BFBF algorithms. proposed SPBnd MPBnd solveMRE inference exactly Bayesian networks could solved previously.proposed upper bounds calculated efficiently two reasons. First, targetblanket TB (Ei ) usually much smaller whole target set M. Second, original MREproblem, need search subsets target set find best solution. However,calculate upper bound, need search fixed target set TB (Ei ) subnetwork.proposed MRE upper bound consists four sources relaxations, i.e., (1) relaxationp(x, v|e) p(v|e) Equation 14, (2) bounding descendant states, (3) inconsistentvalues sharing nodes different target blankets, (4) maximization prunedtargets individual search states. optimal upper bound achieved state equalmaximum GBF subtree rooted state. size expanded target set increase,p(x, v|e) p(v|e) become much smaller 1, relaxation (1) becomestight. Thus, achieve optimal upper bound need minimize impact (3)(4). work, minimized effect (3) merging target blankets sharing settargets, minimized effect (4) identifying summing enclosed-target setstarget blanket.Different brute-force algorithm, search time MPBnd SPBnd longermainly dependent size search space (i.e., number targets number statestarget), also tightness upper bound structure Bayesian networks.Bayesian networks large number targets, upper bound efficiently generatedlong number targets target blanket small. SPBnd, tightnessupper bound depends number enclosed-targets quality enclosed-target Hijmeasured belief update ratio maxhij r(hij ; ei ). enclosed-targets, proposedmethod SPBnd degenerates MPBnd.work, splitting algorithm designed split target blankets, whose sizes largerK, based separation property undirected graphs converting auxiliary nodespseudo-targets. evidence node connected directly many targets, may decomposeusing methods node splitting (Choi et al., 2007). also one future researchdirections.Acknowledgementswork supported NSF grants IIS-0953723, IIS-1219114, PSC-CUNY enhancement award. Part research previously presented AAAI-15 (Zhu & Yuan, 2015).680fiE XACT LGORITHMS MRE NFERENCEReferencesAy, N., & Polani, D. (2008). Information flows causal networks. Advances Complex Systems(ACS), 11(01), 1741.Beinlich, I., Suermondt, G., Chavez, R., & Cooper, G. (1989). alarm monitoring system:case study two probabilistic inference techniques belief networks. Proceedings2nd European Conference AI Medicine, pp. 247256.Binder, J., Koller, D., Russell, S., & Kanazawa, K. (1997). Adaptive probabilistic networkshidden variables. Machine Learning, 29, 213244.Chajewska, U., & Halpern, J. Y. (1997). Defining explanation probabilistic systems. Proceedings Thirteenth Annual Conference Uncertainty Artificial Intelligence (UAI97),pp. 6271, San Francisco, CA. Morgan Kaufmann Publishers.Choi, A., Chavira, M., & Darwiche, A. (2007). Node splitting: scheme generating upperbounds Bayesian networks. Proceedings 23rd Annual Conference UncertaintyArtificial Intelligence (UAI-07), pp. 5766.Darwiche, A. (2009). Modeling Reasoning Bayesian Networks. Cambridge UniversityPress.de Campos, L. M., Gamez, J. A., & Moral, S. (2001). Simplifying explanations Bayesian beliefnetworks. International Journal Uncertainty, Fuzziness Knowledge-Based Systems, 9(4),461489.Dechter, R., & Rish, I. (2003). Mini-buckets: general scheme bounded inference. J. ACM,50(2), 107153.Feige, U., & Krauthgamer, R. (2002). polylogarithmic approximation minimum bisection.SIAM J. Comput., 31(4), 10901118.Fitelson, B. (2007). Likelihoodism, bayesianism, relational confirmation. Synthese, 156, 473489.Flores, J., Gamez, J. A., & Moral, S. (2005). Abductive inference Bayesian networks: findingpartition explanation space. Eighth European Conference Symbolic Quantitative Approaches Reasoning Uncertainty, ECSQARU05, pp. 6375. Springer Verlag.Gardenfors, P. (1988). Knowledge Flux: Modeling Dynamics Epistemic States. MIT Press.Glover, F. (1990). Tabu search: tutorial. Interfaces, 20, 7494.Good, I. J. (1985). Weight evidence: brief survey. Bayesian statistics, 2, 249270.Green, P. (1995). Reversible jump Markov chain Monte Carlo computation Bayesian modeldetermination. Biometrica, 82, 711732.Heckerman, D., Breese, J., & Rommelse, K. (1995). Decision-theoretic troubleshooting. Communications ACM, 38, 4957.Henrion, M., & Druzdzel, M. J. (1991). Qualitative propagation scenario-based schemesexplaining probabilistic reasoning. Bonissone, P., Henrion, M., Kanal, L., & Lemmer, J.(Eds.), Uncertainty Artificial Intelligence 6, pp. 1732. Elsevier Science Publishing Company, Inc., New York, N. Y.681fiZ HU & UANHuang, J., Chavira, M., & Darwiche, A. (2006). Solving map exactly searching compiledarithmetic circuits. Proceedings 21st National Conference Artificial intelligence,Vol. 2, pp. 11431148. AAAI Press.Jeffreys, H. (1961). Theory Probability. Oxford University Press.Jensen, F. V., & Liang, J. (1994). drHugin: system value information Bayesian networks. Proceedings 1994 Conference Information Processing ManagementUncertainty Knowledge-Based Systems, pp. 178183.Kalagnanam, J., & Henrion, M. (1988). comparison decision analysis expert rulessequential diagnosis. Proceedings 4th Annual Conference Uncertainty ArtificialIntelligence (UAI-88), pp. 253270, New York, NY. Elsevier Science.Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization simulated annealing. Science,pp. 671680.Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models - Principles Techniques.MIT Press.Kwisthout, J. (2013). Inforbable Explanations: Finding explanations Bayesian networksprobable informative. van der Gaag, L. (Ed.), Symbolic QuantitativeApproaches Reasoning Uncertainty, Vol. 7958 Lecture Notes Computer Science,pp. 328339. Springer Berlin Heidelberg.Lacave, C., & Diez, F. (2002). review explanation methods Bayesian networks.Knowledge Engineering Review, 17, 107127.Lauritzen, S. L., Dawid, A. P., Larsen, B. N., & Leimer., H.-G. (1990). Independence propertiesdirected markov fields. Networks, 20(5), 491505.Leake, D. B. (1995). Abduction, experience, goals: model everyday abductive explanation.Journal Experimental Theoretical Artificial Intelligence, 7, 407428.Marinescu, R., Dechter, R., & Ihler, A. (2014). AND/OR search marginal MAP. Proceedings30th Annual Conference Uncertainty Artificial Intelligence (UAI-14), pp. 563572.Marinescu, R., & Dechter, R. (2009). AND/OR branch-and-bound search combinatorial optimization graphical models. Artif. Intell., 173(16-17), 14571491.Onisko, A. (2003). Probabilistic Causal Models Medicine: Application Diagnosis LiverDisorders. Ph.D. thesis, Institute Biocybernetics Biomedical Engineering, PolishAcademy Science.Pacer, M., Lombrozo, T., Griffiths, T., Williams, J., & Chen, X. (2013). Evaluating computationalmodels explanation using human judgments. Proceedings 29th Annual ConferenceUncertainty Artificial Intelligence (UAI-13), pp. 498507.Park, J. D., & Darwiche, A. (2003). Solving map exactly using systematic search. Proceedings19th Annual Conference Uncertainty Artificial Intelligence (UAI-03), pp. 459468.Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference.Morgan Kaufmann.682fiE XACT LGORITHMS MRE NFERENCEPradhan, M., Provan, G., Middleton, B., & Henrion, M. (1994). Knowledge engineering largebelief networks. Proceedings Tenth Annual Conference Uncertainty ArtificialIntelligence, UAI-94, p. 484490. Morgan Kaufmann Publishers, Inc.Shimony, S. (1993). role relevance explanation I: Irrelevance statistical independence.International Journal Approximate Reasoning, 8(4), 281324.van der Gaag, L., & Wessels, M. (1993). Selective evidence gathering diagnostic belief networks.AISB Quarterly, pp. 2334.van der Gaag, L., & Wessels, M. (1995). Efficient multiple-disorder diagnosis strategic focusing,pp. 187204. UCL Press, London.Yuan, C., & Hansen, E. A. (2009). Efficient computation jointree bounds systematic MAPsearch. Proceedings 21st International Joint Conference Artificial Intelligence(IJCAI-09), pp. 19821989.Yuan, C., Lim, H., & Littman, M. L. (2011a). relevant explanation: computational complexityapproximation methods. Ann. Math. Artif. Intell., 61, 159183.Yuan, C., Lim, H., & Lu, T.-C. (2011b). relevant explanation Bayesian networks. J. Artif.Intell. Res., 42, 309352.Yuan, C., Liu, X., Lu, T.-C., & Lim, H. (2009). relevant explanation: Properties, algorithms,evaluations. Proceedings 25th Annual Conference Uncertainty Artificial Intelligence (UAI-09), pp. 631638.Yuan, C., & Lu, T.-C. (2007). Finding explanations Bayesian networks. Proceedings18th International Workshop Principles Diagnosis (DX-07), pp. 414419.Zhu, X., & Yuan, C. (2015). exact algorithm solving relevant explanation Bayesiannetworks. Proceedings 29th National Conference Artificial intelligence (AAAI15), pp. 36493655.683fiJournal Artificial Intelligence Research 55 (2016) 799-833Submitted 08/15; published 03/16Exact Algorithm Based MaxSAT Reasoning MaximumWeight Clique ProblemZhiwen FangZHIWENF @ GMAIL . COMState Key Lab. Software Development EnvironmentBeihang University, Beijing, 100083, P.R. ChinaChu-Min LiCHU - MIN . LI @ U - PICARDIE . FRMIS, Universite de Picardie Jules VerneAmiens 80039, FranceKe XuKEXU @ NLSDE . BUAA . EDU . CNState Key Lab. Software Development EnvironmentBeihang University, Beijing, 100083, P.R. ChinaAbstractRecently, MaxSAT reasoning shown effective computing tight upper boundMaximum Clique (MC) (unweighted) graph. paper, apply MaxSAT reasoningcompute tight upper bound Maximum Weight Clique (MWC) wighted graph. firststudy three usual encodings MWC weighted partial MaxSAT dealing hard clauses,must satisfied solutions, soft clauses, weighted falsified.drawbacks encodings motivate us propose encoding MWC specialweighted partial MaxSAT formalism, called LW (Literal-Weighted) encoding dedicatedupper bounding MWC, soft clauses literals soft clauses weighted.optimal solution LW MaxSAT instance gives upper bound MWC, insteadoptimal solution MWC. introduce two notions called Top-k literal failed clauseTop-k empty clause extend classical MaxSAT reasoning techniques, well two soundtransformation rules transform LW MaxSAT instance. Successive transformations LWMaxSAT instance driven MaxSAT reasoning give tight upper bound encoded MWC.approach implemented branch-and-bound algorithm called MWCLQ. Experimentalevaluations broadly used DIMACS benchmark, BHOSLIB benchmark, random graphsbenchmark winner determination problem show approach allows MWCLQreduce search space significantly solve MWC instances effectively. Consequently,MWCLQ outperforms state-of-the-art exact algorithms vast majority instances. Moreover,surprisingly effective solving hard dense instances.1. IntroductionConsider undirected graph G = (V ,E), V set n vertices {v1 , v2 , ..., vn } Eset edges. density G computed 2m/(n(n 1)). clique G subset C Vevery pair vertices adjacent. contrary, independent set G subsetV every pair vertices disconnected. vertex cover G subset Vevery edge G least one endpoint S. maximum clique (MC) problem asks findclique largest cardinality. MC problem prominent combinatorial optimizationproblem tightly related two well-known graph problems, namely maximumc2016AI Access Foundation. rights reserved.fiFANG , L , & XUindependent set (MIS) problem minimum vertex cover (MVC) problem. Concretely,maximum clique C G maximum independent set complement graph G G, V \Cminimum vertex cover G. Therefore, algorithms three problems directlyapplied solve others practice. addition, exact MC solvers take advantagefollowing relation size maximum clique number independent sets. Gpartitioned k independent sets, G cannot contain clique larger k,independent set contribute one vertex clique.MC problem NP-hard decision problem NP-complete (Karp, 1972), appears many applications social network analysis (e.g., Zhang, Nie, Jiang, Chen, & Liu,2014b; Kibanov, Atzmueller, Scholz, & Stumme, 2014). fixed-parameter intractable (Downey& Fellows, 1995). Moreover, proved approximating MC within |V |1 given > 0NP-hard (Zuckerman, 2006). best polynomial-time approximation algorithm achievesapproximation ratio O(n(log log n)2 /(log n)3 ) (Feige, 2004). theoreticalpractical importance MC problem, huge amount effort devoted solvedesigning two types algorithms (also called solvers). One type heuristic algorithms mainlyincluding stochastic local search (e.g., Pullan & Hoos, 2006; Cai, Su, & Sattar, 2011; Cai, Su,Luo, & Sattar, 2013; Fang, Chu, Qiao, Feng, & Xu, 2014a). Another exact algorithms includingbranch-and-bound (BnB) search (e.g., Ostergard, 2002; Regin, 2003; Tomita & Seki, 2003; Konc &Janezic, 2007; Li & Quan, 2010b; Tomita & Kameda, 2007; Li, Fang, & Xu, 2013). Heuristic algorithms able solve large-scale instances cannot guarantee optimality solutions.Exact algorithms guarantee optimality solutions, worst-case time complexityexponential unless P = N P .tight upper bound size maximum clique graph essential BnB algorithmsolve MC problem efficiently. However, challenging obtain upper boundreasonable time. state-of-the-art BnB algorithms apply approximation coloring independent set partition algorithms compute upper bound MC. instance, Fahle (2002) usesconstructive heuristic DSATUR color vertices one one according degrees. KoncJanezic (2007), Tomita Kameda (2007), Li Quan (2010b) apply greedy strategy proposed Tomita Seki (2003) partition graph independent sets, use numberindependent sets partition upper bound MC. MaxCLQ (Li & Quan, 2010b, 2010a)encodes MC instance partial MaxSAT instance improves upper bound basedindependent set partition making use MaxSAT reasoning. excellent performance MaxCLQ shows MaxSAT reasoning technologies allows compute tight upper bound MCwithin reasonable time. IncMaxCLQ (Li et al., 2013) combines incremental upper boundMaxSAT reasoning compute tight upper bound efficiently. addition independentset partition MaxSAT reasoning, approaches, graph matching (Regin, 2003),also used upper bounding MC.One generalization MC problem associate vertex positive weight.weight clique defined total weight vertices it. maximum weight clique(MWC) problem consists finding clique largest weight. computationally equivalentproblems like weighted set packing problem. MWC problem appears variety realworld applications, protein structure predictions (Mascia, Cilia, Brunato, & Passerini, 2010),coding theory (Zhian, Sabaei, Javan, & Tavallaie, 2013), combinatorial auctions (Wu & Hao, 2015),computer vision (Ma & Latecki, 2012; Zhang, Javed, & Shah, 2014a), etc. example,video object segmentation problem, challenging task select region high objectness800fiA N E XACT LGORITHMAXIMUM W EIGHT C LIQUEscore sharing similar appearance, solved MWC algorithm (Ma & Latecki,2012).Compared MC problem, less work done solve MWC problem. Cliquer (Ostergard, 2002, 2001) one state-of-the-art exact solvers MC MWC,deals MC MWC using similar methods. preprocessing, Cliquer partitionsgraph independent sets determining one independent set time. longvertices added current independent set, one largest degree added.purpose preprocessing define vertex ordering v1 <v2 <. . .<vn , vi (1in)vertex inserted independent set time i. Cliquer searches MC MWCsubgraph induced {vi , vi+1 , . . ., vn } successively i=n, n 1, . . ., 1 (in ordering).MC MWC subgraph induced {vi , vi+1 , . . ., vn } associated viused prune subtrees subsequent search. Kumlander (2004, 2008b) inherits search strategyCliquer. addition MWC associated vi , Kumlander also partitions currentsubgraph independent sets uses sum maximum weight independent setupper bound. VCTable (Shimizu, Yamaguchi, Saitoh, & Masuda, 2012) improves Kumlanders algorithm new initial vertex order better implementation using bitwise operations.Yamaguchi Masuda (2008) propose new upper bound based longest path directedacyclic graph constructed original graph, improves bound based independent set partition. OTClique (Shimizu, Yamaguchi, Saitoh, & Masuda, 2013), also basedCliquer, uses dynamic programming strategy calculate upper bounds small subproblems preprocessing, stores results table. stored upper bounds usedsearch. MinSatz (Li, Zhu, Manya, & Simon, 2012) exact solver MinSAT problem. important application MinSatz solve combinatorial optimization problemsMC MWC. MinSatz constructs weighted graph MinSAT instance uses cliquepartition combined MaxSAT reasoning compute tight bound MinSAT instancesolve. addition exact algorithms, heuristic algorithms also proposed solveMWC problem (Pullan, 2008; Wu, Hao, & Glover, 2012; Benlic & Hao, 2013).paper, apply MaxSAT reasoning solve MWC. first study three usual encodings MWC MaxSAT. encodings intrinsic difficulties dealing vertexweights. motivates us propose dedicated encoding MWC MaxSAT called LW(Literal-Weighted) encoding, soft clauses literals soft clauses weighted.optimal solution MaxSAT instance usual encodings MWC MaxSAT givesMWC, optimal solution LW MaxSAT instance LW encoding gives upperbound MWC. So, makes little sense run MaxSAT solver find optimal solutionLW MaxSAT instance. interest LW encoding transform LW MaxSATinstance reduce optimal solution, tighter upper bound encoded MWCobtained.order transform LW MaxSAT instance, introduce two notions called Top-k literalfailed clause Top-k empty clause, two sound transformation rules. Then, implementBnB algorithm MWC called MWCLQ. every search tree node, MWCLQ first encodescurrent subgraph LW MaxSAT instance. Then, driven MaxSAT reasoning, MWCLQrepeatedly transforms LW MaxSAT instance obtain tighter upper bound encodedMWC. best knowledge, first time MaxSAT reasoning techniquesused specifically compute tight upper bound BnB algorithm MWC. Experimental results widely used DIMACS benchmark, BHOSLIB benchmark, random graphs realistic801fiFANG , L , & XUbenchmark winner determination problem show MWCLQ reduces search spacesignificantly outperforms state-of-the-art exact algorithms vast majority instancesbenchmarks.paper extended work Fang, Li, Qiao, Feng, Xu (2014b) by:clearly motivating LW encoding illustrating weakness three classical encodingsMWC MaxSAT dealing vertex weights graph;introducing notion Top-k empty clause exploiting MWCLQ;formally proving transformation rules;adding experimental results show effectiveness approach. algorithmMWCLQ compared integer programming solver CPLEX, MinSATsolver MinSatz, especially realistic instances winner determination problem.State-of-the-art MaxSAT solvers using different encodings MWC MaxSAT alsocompared MWCLQ. order evaluate LW encoding, state-ofthe-art MaxSAT solvers cannot use, compare different versions MWCLQ,difference encoding MWC MaxSAT used compute upper bound.paper organized follows. next section, introduce necessary notationsbackground knowledge. Section 3, present MWCLQ different encodings MWCMaxSAT, extending MaxSAT reasoning weighted literals introducing notionsTop-k literal failed clause Top-k empty clause, two transformation rules. Experimentalresults shown Section 4. Section 5 concludes paper.2. Preliminariessubgraph G induced subset V V G = (V , E ), E = {{vi ,vj } | vi ,vj V{vi , vj } E}. vertex v, (v) = {u| {u,v}E} set neighbors v cardinality|(v)| called degree v. use Gv denote subgraph induced (v) {v} G\vdenote subgraph induced V \{v}. maximal clique clique cannot extendedmore. maximum clique maximal clique largest possible size. cardinalitymaximum clique G usually denoted (G) called clique number G.vertex v G, maximum clique G either Gv G\v. chromatic numberG, denoted (G), minimum number colors needed color vertices Gtwo adjacent vertices share color. vertices sharing color constituteindependent set. Therefore, graph coloring problem equivalent partitioning Vminimum number independent sets. Note (G) greater equal (G).graph edge-weighted vertex-weighted. focus vertex-weighted graphpaper. Formally, vertex-weighted undirected graph G = (V, E, w) undirected graph G =(V, E) combined weighting function w: V R+ every vertex v associatedpositive weight w(v). sequel, use term weighted graph insteadPof vertex-weightedgraph simplicity. weight clique C G defined w(C) = vC w(v). Givenweighted graph G, maximum weight clique problem asks find clique largest weightG (Ostergard, 2001; Pullan, 2008; Wu et al., 2012) largest weight often denoted802fiA N E XACT LGORITHMAXIMUM W EIGHT C LIQUEv (G) literature. Note maximum weight clique necessarily clique containingmaximum number vertices, must maximal clique.MC problem encoded partial MaxSAT problem. MaxSAT, variable x maytake value 0 (f alse) 1 (true). literal variable x negation x. clause c = 1 2... l disjunction literals, also expressed set {1 , 2 , . . . , l }. clausesatisfied clause least one literal assigned true. length clausec number literals contains, denoted length(c). unit clause clause containingone literal. empty clause, denoted , contains literals cannot satisfied.conjunctive normal form (CNF) formula = c1 c2 ... cm conjunction clauses. GivenCNF formula set variables {x1 , x2 , ..., xn }, satisfiability (SAT) problem testassignment satisfying clauses , maximum satisfiability (MaxSAT)problem find assignment satisfying maximum number clauses (Li & Manya, 2009).minimum satisfiability (MinSAT) problem, contrary, find assignment minimizingnumber satisfied clauses (Li et al., 2012). cases, clauses declaredhard must satisfied solutions, clauses soft falsified. partialMaxSAT problem asks find assignment maximize number satisfied soft clausessatisfying hard clauses. Note MaxSAT problem particular partial MaxSAT problemwithout hard clauses. weighted clause pair (c, w), c soft clause w, positivenumber, weight. weighted partial MaxSAT problem find assignment maximizingtotal weight satisfied soft clauses satisfying hard clauses. weighted MaxSAT problemweighted partial MaxSAT problem without hard clauses. optimal solution (weighted)(partial) MaxSAT instance denoted opt() paper.Two main types exact algorithms developed MaxSAT: SAT-based MaxSAT solvers (e.g.,Morgado, Heras, Liffiton, Planes, & Marques-Silva, 2013; Ansotegui, Bonet, & Levy, 2013; Davies& Bacchus, 2013a; Ansotegui & Gabas, 2013; Morgado, Dodaro, & Marques-Silva, 2014; Martins,Joshi, Manquinho, & Lynce, 2014) solve MaxSAT instance repeatedly calling CDCL(Conflict-Driven Clause Learning) based SAT solver solve sequence SAT problems,BnB MaxSAT solvers (e.g., Li, Manya, & Planes, 2007; Kugel, 2010). SAT-based MaxSATsolvers particularly efficient solve industrial MaxSAT problems, BnB MaxSATsolvers particular efficient solve random MaxSAT problems. SAT-based solversMaxHS (Davies & Bacchus, 2013b) also exploit MIP (Mixed Integer Programming)solving MaxSAT.maximize number satisfied clauses equals minimize number falsified clauses.Many algorithms based BnB scheme MaxSAT compute lower bound numberfalsified clauses (Li, Manya, & Planes, 2006; Li et al., 2007; Larrosa, Heras, & de Givry, 2008;Kugel, 2010) prune search space solving MaxSAT instance . Detecting disjoint inconsistent subsets soft clauses proved powerful computing bound,subset soft clauses said inconsistent subset together hard clauses unsatisfiable.Unit propagation effective technique widely used SAT MaxSAT solvers (Li & Anbulagan, 1997; Li, Manya, & Planes, 2005). pseudo-code allowing find inconsistent subsetsoft clauses based unit propagation given Algorithm 1 (Li et al., 2005). algorithmworks follows. uses stack store unit clauses performs unit propagationempty clause produced empty. empty clause produced, set clausesused deriving empty clause, excluding hard clauses, returned. algorithm callediteratively find many disjoint inconsistent subsets soft clauses possible. Note soft803fiFANG , L , & XUclauses involved inconsistent subset removed detecting inconsistent subsetsensure subsets disjoint. Also note solution empty setreturned, empty set means set hard clauses unsatisfiable.Algorithm 1: ConflictDetectionByUP(, S), detect inconsistent subset soft clauses.Input: MaxSAT instance stack storing unit clauses .Output: Return inconsistent subset soft clauses unit propagation results emptyclause, otherwise return false.1 begin2empty3pop unit clause u S;4literal u, record u reason ;5foreach clause c contains6satisfy c;78910111213141516171819foreach clause c containsremove c;c unit clausepush c S;c emptypush c empty queue Q, = {c};Q emptypop clause c Qforeach removed literal creason r literalpush r Q, insert r I;return set soft clauses I;return false;Failed literal detection (Freeman, 1995) used enhance unit propagation MaxSAT solving (Li et al., 2006). literal CNF formula called failed literal , unit propagation{} results empty clause. Let IS() set soft clauses used unit propagationderive empty clause assigning true . literals soft clause c = {1 , 2 , . . ., l }failed, {c} IS(1 ) IS(2 ) IS(l ) inconsistent subset soft clauses (Li &Quan, 2010b).3. MWCLQ: Exact Algorithm MWCsection, propose exact algorithm based BnB scheme, namely MWCLQ,MWC. Subsection 3.1, describe basic BnB algorithm. Subsection 3.2, introducenovel encoding called LW (Literal-Weighted) encoding MWC MaxSAT discussingthree usual encodings. Given weighted graph G, LW MaxSAT encoding gives LW MaxSATinstance lw represents upper bound v (G). Subsection 3.3, propose two transfor804fiA N E XACT LGORITHMAXIMUM W EIGHT C LIQUEmation rules lw . Successive transformations lw driven MaxSAT reasoning give tightupper bound v (G), presented Subsection 3.4.3.1 Branch-and-Bound Search MWCAlgorithm 2 depicts pseudocode MWCLQ.Algorithm 2: MWCLQ(G, C, LB), branch-and-bound algorithm MWC.Input: weighted graph G=(V , E, w), clique C construction, lower boundLB.Output: clique weight greater LB, clique found.1 begin2|V | = 03return C;4567891011121314U B overestimate(G)+w(C);U B LBreturn ;v select(V );C1 MWCLQ(Gv , C{v}, LB);LB max(LB, w(C1 ));C2 MWCLQ(G\v, C, LB);w(C1 ) w(C2 )return C2 ;elsereturn C1 ;MWCLQ searches clique, extended C, weight larger LB G = (V , E,w). first computes upper bound UB calling overestimate(G) compares UB LBtest whether search necessary G. possible find better solution, MWCLQselects vertex calling select(V ). vertex v G, maximum weight clique Geither clique Gv containing v clique G\v containing v. Thus, MWCLQ searchesmaximum weight clique Gv G\v successively.MWCLQ form similar MaxCLQ, BnB algorithm MC (Li & Quan, 2010b),overestimate function computing upper bound, select function choosing branching vertex, significantly different. two functions essential MWCLQMaxCLQ. high-quality upper bound allows solvers prune useless search, good vertexordering promises efficient search process. MaxCLQ computes base upper bound partitioning G independent sets, improves base upper bound MaxSAT reasoning.Meanwhile, MaxCLQ orders vertices selecting first vertex minimum degree.paper, use simple vertex ordering select v largest weight, breaking ties favorvertex higher degree, focus efficiently compute tight upper bound usingMaxSAT reasoning.805fiFANG , L , & XU3.2 Encoding MWC MaxSATMC MWC instance encoded MaxSAT follows.Boolean variables: boolean variable xi added vertex vi , assignedvalue true vi maximum clique construction;Hard clauses: set hard clauses added require pair unconnected verticesbelong clique. Concretely, hard clause xi xj added pairunconnected vertices vi vj ;Soft clauses: exist different ways define set soft clauses. Given graph,encodings MC MWC MaxSAT presented paper use set hardclauses. differ soft clauses used, analyzed discussedsubsection.MC instance also encoded MinSAT instance without hard clauses adoptingapproach proposed Ignatiev, Morgado, Marques-Silva (2014). obtained MinSATinstance turn encoded MaxSAT instance without hard clauses using approacheswork Kugel (2012), Zhu, Li, Manya and, Argelich (2012). encoding without hardclauses provides new angle view MC MWC solving, awaits future research.section, focus encodings MWC MaxSAT hard clauses different soft clauses used. Subsection 3.2.1 defines direct encoding MWC MaxSAT.Subsection 3.2.2 defines split encoding iterative split encoding MWC MaxSAT. Subsection 3.2.3 proposes novel literal-weighted encoding MWC MaxSAT illustratesadvantages compared direct encoding, split encoding iterative split encodingcomputing upper bound MWC.3.2.1 IRECT E NCODINGMWCAX SAT1 v1v2 24 v3v4 5Figure 1: weighted graph 4 vertices 1 edge. Numbers indicate vertex weights.straightforward way define soft clauses associate unit soft clause xi vertexvi , giving direct encoding MC MWC MaxSAT. example, MC instancegraph Fig. 1 (without considering vertex weights) encoded following partial MaxSATinstance: (1) set variables {x1 , x2 , x3 , x4 }; (2) set hard clauses {x1 x3 , x1 x4 ,x2 x3 , x2 x4 , x3 x4 }; (3) set soft clauses {x1 , x2 , x3 , x4 }.assignment satisfying hard clauses gives rise clique, since variables assignedvalue true correspond pairwise connected vertices. non-weighted case, assignment satisfying hard clauses maximizing number satisfied soft clauses gives risemaximum clique.806fiA N E XACT LGORITHMAXIMUM W EIGHT C LIQUEweighted case, unit soft clause associated weight correspondingvertex. example, MWC instance weighted graph Fig. 1 encodedweighted partial MaxSAT instance boolean variables hard clausesMC instance. set weighted soft clauses is: {(x1 ,1), (x2 ,2), (x3 ,4), (x4 ,5)}. assignmentsatisfying hard clauses maximizing total weight satisfied soft clauses gives risemaximum weight clique.encoded MaxSAT, MaxSAT reasoning applied solve MC MWCproblem. Many MaxSAT algorithms obtain upper bound satisfied soft clauses computinglower bound number falsified soft clauses. Inconsistent subsets soft clauses oftenused computing lower bound. Recall subset soft clauses said inconsistentsubset together hard clauses satisfiable. non-weighted MaxSAT instancesoft clauses, r disjoint inconsistent subsets soft clauses detected, mr upperbound opt() (Li et al., 2005, 2006).weighted case, define weight subset soft clauses minimumweight clauses S, namely,w(S) = min w(c).cSSimilar non-weighted case, following proposition.Proposition 1. (Li et al., 2007) Given weighted partialPif disjoint inconsisPMaxSAT instance ,tent subsets S1 , S1 , ..., Ss detected, opt() soft clause c w(c) 1is w(Si ).Observe unit soft clauses direct encoding capture connectionvertices, set soft clauses used every graph n vertices, mattervertices connected other. MC instance, number soft clausesused base upper bound, MaxSAT reasoning applied improve base upperbound detecting inconsistent soft clause subsets. Unfortunately, direct encodinggive trivial base upper bound, number vertices graph. Moreover, sincesoft clauses direct encoding unit, inconsistent subset contains exactly two soft clauses,limiting improvement upper bound half number vertices graph.weighted case similar. base upper bound total weight vertices. usefollowing example illustrate direct encoding cannot compute tight upper bound evensimple MWC instance.Example 1. total weight soft clauses direct encoding graph Fig. 1 12.Using unit propagation, {(x3 , 4), (x4 , 5)} found inconsistent subset hardclause x3 x4 , upper bound improved 4. left soft clauses are: (x1 , 1),(x2 , 2), (x4 , 1), unit propagation detects {(x1 , 1), (x4 , 1)} also inconsistent,improve upper bound 1. Finally, upper bound improved 6, largeroptimal solution 5.drawbacks, direct encoding perform well, shown experimental results presented Section 4.3. One might want add at-most-one constraintindependent set remedy drawbacks direct encoding. is, independent set= {v1 , v2 , . . . , vl }, add at-most-one constraint x1 +x2 +. . .+xl 1. However, at-most-oneconstraint add anything new, subset hard clauses {xi xj | 1i<jl}encoding already enforces at-most-one constraint (Chen, 2010). addition, encoding807fiFANG , L , & XUat-most-one constraint using hard binary clauses efficient enough independent setextremely large, usually case solving hard MC MWC instances. Therefore,at-most-one constraint presumably useless encoding MC MWC MaxSAT.3.2.2 PLIT E NCODINGTERATIVEPLIT E NCODINGMWCAX SATAnother encoding introduced MC MaxCLQ (Li & Quan, 2010b) defining set softclauses based independent set partition G. Concretely, MaxCLQ first partitions G setindependent sets, creates soft clause independent set, disjunctionvariables corresponding vertices independent set. independent set based encodingshown substantially efficient direct encoding solve MC.natural way extend independent set based MaxSAT encoding MWC split vertexweights. independent set = {v1 , v2 , . . . , vl }, w(v1 ) w(v2 ) . . . w(vl ),split vertex weights minimum weight w(vl ), thus obtain weighted soft clause(x1 x2 . . . xl , w(vl )) l unit soft clauses: (x1 , w(v1 ) w(vl )), (x2 , w(v2 ) w(vl )), . . .,(xl , w(vl ) w(vl )), l largest integer w(vl ) > w(vl ) I. encodingcalled split encoding. total weight soft clauses split encoding lessdirect encoding, giving better base upper bound, may still large.Example 2. possible independent set partition graph Fig. 1 {v4 , v3 , v1 } {v2 }.soft clauses split encoding based partition (x4 x3 x1 , 1), (x4 , 4), (x3 , 3)(x2 , 2). total weight soft clauses 10, better direct encoding.Using unit propagation, {(x4 , 4), (x3 , 3)} found inconsistent subset hardclause x3 x4 , allows us improve upper bound 3. way, {(x4 , 1),(x2 , 2)} found inconsistent, improves upper bound 1. Finally, remainingsoft clauses (x4 x3 x1 , 1) (x2 , 1) consistent. Thus, upper bound computed usingsplit encoding still 6.split encoding improved so-called iterative split encoding, usedMinSatz (Li et al., 2012). idea iterative split encoding split vertex weights repeatedlyvertices independent set weight. Concretely, independentset = {v1 , v2 , . . . , vl } w(v1 ) w(v2 ) . . . w(vl ), add soft clause (x1 x2. . . xl , w(xl )), repeatedly find largest l w(xl ) > w(xl ) add soft clause(x1 x2 . . . xl , w(xl ) w(xl )), l 1.Example 3. possible independent set partition graph Fig. 1 {v4 , v3 , v1 }, {v2 }.soft clauses iterative split encoding based partition (x4 x3 x1 , 1), (x4 x3 , 3),(x4 , 1) (x2 , 2). total weight soft clauses 7. Starting unit propagation setting x4 = 1,find {(x4 , 1), (x2 , 2)} inconsistent subset, improve upper bound1 get new soft clause (x2 , 1). Then, find {(x4 x3 , 3), (x2 , 1)} inconsistent,upper bound improved 5, tightest upper bound.iterative split encoding gives non-trivial base upper bound, betterobtained direct encoding split encoding. key point iterative split encodingvertex weights split advance, generating numerous soft clauses, variablemay appear several soft clauses clauses may differ one variable (e.g. clauses(x4 x3 x1 , 1) (x4 x3 , 3) Example 3). Many splittings may useful upper808fiA N E XACT LGORITHMAXIMUM W EIGHT C LIQUEbound computation, MaxSAT reasoning may complicated instancenumerous soft clauses.3.2.3 L ITERAL -W EIGHTED E NCODINGMWCAX SATRecall MaxSAT encoding MC MaxCLQ guarantees variable appearssoft clauses number soft clauses equals number independent sets. MaxSATreasoning instance much simpler. order extend advantage MWCsplit vertex weights advance, introduce Literal-Weighted encoding (LW encoding) MWC LW MaxSAT, literals soft clause also weighted. Concretely, weighted literal pair(, w), literal w weight. literal-weightedsoft clause (LW soft clause) disjunction weighted literals. Formally, given MWC instanceG = (V, E, w), encode G LW MaxSAT instance lw follows:Associate vertex vi weighted literal (xi , w(xi )), w(xi ) = w(vi );Add hard clause xi xj pair unconnected vertices vi vj ;independent set = {v1 , v2 , ..., vl }, add LW soft clause c = (x1 , w(v1 )) (x2 , w(v2 )). . . (xl , w(vl )).LW soft clause c also presented set {(x1 , w(x1 )), (x2 , w(x2 )),. . ., (xl , w(xl ))}.weight c definedw(c) = max (w(xj )).1jlLW clause c ordered w(x1 ) w(x2 ) ... w(xl ). sequel, LW soft clausesalways ordered. Therefore, weight LW soft clause always w(x1 ), representing costnone vertices corresponding independent set included clique construction.show optimal solution LW MaxSAT instance lw , denoted opt(lw ),maximizes total weight satisfied soft clauses satisfies hard clauses, givesupper bound v (G), differently usual encodings optimal solutionMaxSAT instance gives v (G).MWC given assignment lw satisfies hard clauses maximizestotal weight satisfied literals. Example 4 suggests opt(lw ) v (G) differentMWC instance G.1v17v24v52 v36v6v4 3Figure 2: weighted graph 6 vertices 6 edges. Numbers indicate vertex weights.809fiFANG , L , & XUExample 4. possible independent set partition graph Fig. 2 {{v1 , v3 , v6 }, {v2 , v4 }{v5 }}. set soft clauses LW MaxSAT instance lw based partition {{(x6 ,6), (x3 , 2), (x1 , 1)}, {(x2 , 7), (x4 , 3)}, {(x5 , 4)}}. MWC graph {v5 , v6 }v (G)=10. hand, {x1 =1, x2 =1} optimal solution lw opt(lw )=13.Moreover, optimal solution lw correspond maximum weight cliquegraph.following proposition states relationship optimal solution LW MaxSATinstance MWC encoded graph.Proposition 2. Consider weighted graph G = (V, E, w), let lw LW instance basedindependent set partition G, v (G) opt(lw ).Proof. Suppose {vi1 , vi2 , . . . , vip } maximum weight clique G. Let cij LW clausecontaining xij .v (G) =pXj=1w(xij )pXw(cij ) opt(lw ).j=1Since opt(lw ) upper bound v (G), makes little sense apply MaxSAT solverfind opt(lw ). However, transform lw , optimal solution new instancetighter upper bound v (G), upper bound new optimal solution alsotightened. Example 5 illustrates this.Example 5. possible independent set partition graph Fig. 1 {v4 , v3 , v1 } {v2 }.LW soft clauses {(x4 , 5), (x3 , 4), (x1 , 1)} {(x2 , 2)}. total weight soft clauses 7,giving base upper bound iterative split encoding. Unit propagation setting x2 = 1makes x4 x3 false hard clauses. find split clause {(x4 , 5),(x3 , 4), (x1 , 1)} {(x4 , 2), (x3 , 2)} {(x4 , 3), (x3 , 2), (x1 , 1)}, keep base upperbound 7, splitting allows derive inconsistent subset soft clauses {{(x4 , 2),(x3 , 2)}, {(x2 , 2)}} improve base upper bound 5, tightest possible upper bound.Note unit propagation need derive empty clause improve upper boundexample. Furthermore, order obtain tightest upper bound, one unit propagation suffices, two inconsistent subsets need derived iterative split encoding Example 3.key point Example 5 original LW MaxSAT instance transformed newone, upper bound optimal solution improved 5 using MaxSAT reasoning.tightest upper bound new LW MaxSAT instance also tightest upper boundv (G).next subsection, define two sound transformation rules transform LWMaxSAT instance, allowing derive tight upper bound MWC general case. effective, application two rules driven MaxSAT reasoning,presented Subsection 3.4.810fiA N E XACT LGORITHMAXIMUM W EIGHT C LIQUE3.3 Transformation Rules Literal-Weighted MaxSATpropose two transformation rules split LW soft clause LW MaxSAT instance encodingMWC instance G. soundness rules based Proposition 3, ensuringsplitting, optimal solution new LW MaxSAT instance tighter upper bound v (G).Proposition 3. Let lw LW MaxSAT instance encoding MWC instance G.LW soft clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . ., (xl , w(xl ))} lw 0<w(x1 ), split cc = {(x1 , ), (x2 , min(w(x2 ), )), . . ., (xl , min(w(xl ), ))} c = {(x1 , w(x1 )), (x2 ,max(w(x2 ) , 0)), . . ., (xl , max(w(xl ), 0))}, literals weight 0 removed,get lw = (lw \{c}) {c ,c }, v (G) opt(lw ) opt(lw ).Proof. First all, note c c ordered c. prove v (G) opt(lw ), firstshow min(w(xj ), ) + max(w(xj ), 0) = w(xj ) j 1<jl. fact,1. w(xj ) , min(w(xj ), ) + max(w(xj ), 0) = + w(xj ) = w(xj );2. w(xj ) < , min(w(xj ), ) + max(w(xj ), 0) = w(xj ) + 0 = w(xj ).words, total weight literal changed splitting c c c . Let C ={vi1 , vi2 , . . . , vip } maximum weight clique G. {xi1 = 1, xi2 = 1, . . ., xip = 1}variables 0, assignment lw satisfying hard clauses. Let S(xij ) = {c | xijc} set soft clauses containing xij lw 1jp. clauses S(xij ) satisfiedxij . time,Xw(c) =cS(xij )Xmax w(x) w(xij ).cS(xij )xcHence, have,opt(lw )pXXj=1 cS(xij )w(c)pXj=1w(xij ) =pXw(vij ) = v (G).j=1prove opt(lw ) opt(lw ), let assignment w(lw , A) (w(lw , A)) totalweight satisfied soft clauses lw (lw ).1. c satisfied lw A, c c also satisfied lw A;2. c satisfied A, c also satisfied since c literals c, c maysatisfied satisfied literal c may weight 0 c may thus removedc .note w(c) = w(c )+w(c ), w(lw , A) w(lw , A) assignment A,implying opt(lw ) opt(lw ).Given weighted graph G = (V, E, w), let lw LW MaxSAT instance basedindependent set partition G, propose following two rules transform lw .811fiFANG , L , & XU1. -Rule Given LW soft clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . ., (xl , w(xl ))} lwweight 0<w(x1 ), split c c = {(x1 , ), (x2 , min(w(x2 ), )), . . ., (xl ,min (w(xl ), ))} c = {(x1 , w(x1 )), (x2 , max(w(x2 ), 0)), . . ., (xl ,max(w(xl ),0))}, i.e., lw = (lw \ {c}) {c , c }.2. (k, )-Rule Given LW clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . . , (xl , w(xl ))} lw ,integer 1k<l weight 0<w(x1 )w(xk+1 ), split c c = {(x1 , ), (x2 ,max(w(x2 )+w(x1 ), 0)), . . ., (xk , max(w(xk )+w(x1 ), 0))}, c = {(x1 , w(x1 )),(x2 , min(w(x2 ), w(x1 ))), . . ., (xk , min(w(xk ), w(x1 ))), (xk+1 , w(xk+1 )), . . ., (xl ,w(xl ))}, i.e., lw = (lw \ {c}) {c , c }.purpose -Rule (k, )-Rule split c clause c weightclause c weight w(x1 ) without changing total weight literal. constraintw(x1 )w(xk+1 ) ensure c remains ordered (i.e., w(x1 )min(w(x2 ), w(x1 )). . .w(xk+1 )). . .w(xl )). use example illustrate (k, )-Rule. Let c LW softclause c = {(x1 , 5), (x2 , 3), (x3 , 2)}, (1) k=1 =2, c = {(x1 , 2)} c = {(x1 , 3), (x2 ,3), (x3 , 2)}; (2) k=2 =3, c = {(x1 , 3), (x2 , 1)} c = {(x1 , 2), (x2 , 2), (x2 , 2)};(3) k=2 =2, c = {(x1 , 2), (x2 , 0)} = {(x1 , 2)} c = {(x1 , 3), (x2 , 3), (x3 , 2)}.soundness -Rule (k, )-Rule easily proved using Proposition 3.two rules applied many possible ways generate many possible clauses. example,special case, =w(xl ) -Rule, c = {(x1 , )), (x2 , )), . . . , (xl , ))} leastxl removed c since weight 0 c . words, transform lwclassical weighted MaxSAT instance repeatedly applying -Rule =w(xl ) clausec containing literals different weights. way, obtain classical weighted partialMaxSAT instance , literals soft clause weight whose optimalsolution gives maximum weight clique. Moreover, let MaxSAT instance obtainedapplications -Rule, opt(lw ) opt(1 ) opt(2 ) . . . opt() = v (G).fact, iterative split encoding G.Observe application -Rule (k, )-Rule gives MaxSAT instance whoseoptimal solution gives tighter upper bound v (G). Since unrestricted applications tworules may effective, restrict applications cases inconsistent subsetsoft clauses weight derived. Concretely, (k, )-Rule always applied clausec k weighted literals failed falsified, allowing obtain cinconsistent subset soft clauses derived, -Rule always applied clausec inconsistent subset soft clauses. application allows improve upper boundv (G) positive weight . Note rules strictly greater 0,= 0, rules generate new empty clauses weight 0, cannot improve upperbound. call extended MaxSAT reasoning application -Rule (k, )-Rule drivenMaxSAT reasoning LW MaxSAT instance. Details approach given afterwards.3.4 Upper Bound Based MaxSAT Reasoningsection, show transformation LW MaxSAT instance driven MaxSAT reasoning. Subsection 3.4.1, apply -Rule transform inconsistent subset soft clauses.Subsection 3.4.2 Subsection 3.4.3, introduce two notions, namely Top-k failed literal clause Top-k empty clause, deduce inconsistent subset soft clauses using812fiA N E XACT LGORITHMAXIMUM W EIGHT C LIQUE(k, )-Rule, transformed applying -Rule. Subsection 3.4.4, presentoverestimating algorithm computes tight upper bound MWC successively transformingLW MaxSAT instance.3.4.1 RANSFORMINGNCONSISTENTUBSETOFT C LAUSESFirst all, detect inconsistent subsets using unit propagation presented Algorithm 1.Example 6. possible independent set partition graph Fig. 2 {{v1 , v4 , v6 }, {v2 ,v5 }, {v3 }}, LW soft clauses based partition c1 = {(x6 , 6), (x4 , 3), (x1 , 1)}, c2 ={(x2 , 7), (x5 , 4)} c3 = {(x3 , 2)}. set x3 =1 satisfy unit clause c3 , x3removed hard clauses x1 x3 , x3 x5 , x3 x6 . Unit clauses x1 , x5 x6 implyx1 =0, x5 =0 x6 =0, respectively. So, c1 c2 become unit clauses. Accordingly,set x4 =1 x2 =1 satisfy them, falsifying hard clause x2 x4 .Consequently, inconsistent soft clause subset = {c1 , c2 , c3 } detected weightsubset 2. Proposition 1 allows decrease upper bound 2, thus improved upperbound 13. Since soft clauses involved inconsistent subset, cannot improveupper bound more. However, split soft clauses using -Rule basedfollowing proposition.Proposition 4. Let = {c1 , c2 , . . ., ci } inconsistent subset LW soft clauses, -Ruleapplied split every cj cj cj =w(S), = {c1 , c2 , ..., ci } stillinconsistent soft clause subset weight .Proof. clearly inconsistent, clauses literals clausesS. addition, every clause weight . Hence, inconsistent subset soft clausesweight .purpose splitting obtain = {c1 , c2 , . . ., ci } improve upperbound.Example 7. presented Example 6, {c1 , c2 , c3 } inconsistent subset weight 2. Applying -Rule =2, c1 = {(x6 , 2), (x4 , 2), (x1 , 1)}, c1 = {(x6 , 4), (x4 , 1)}, c2 = {(x2 , 2),(x5 , 2)}, c2 = {(x2 , 5), (x5 , 2)}, c3 = {(x3 , 2)} c3 = {(x3 , 0)} (c3 removed).easy see {c1 , c2 , c3 } inconsistent subset soft clauses weight 2,remaining clauses {{(x6 , 4), (x4 , 1)}, {(x2 , 5), (x5 , 2)}} used improveupper bound. However, unit propagation cannot used improve upper boundunit clause exists. Moreover, failed literal detection work either everysoft clause contains least one literal failed. propose apply (k, )-Rule splitTop-k literal failed clause defined afterwards improve upper bound.3.4.2 OP - K FAILED L ITERAL ETECTIONDefinition 1. LW soft clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . ., (xl , w(xl ))} Top-k literalfailed x1 , x2 , . . ., xk failed literals, 1k<l.define Top-k weight LW soft clause c wk (c) = w(x1 )w(xk+1 ), 1k < length(c).813fiFANG , L , & XUProposition 5. Consider LW soft clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . ., (xl , w(xl ))}. cTop-k literal failed k<l, IS(xj ) (1jk) set soft clauses making xj failed, splitc c c using (k, )-Rule = min(wk (c), w(IS(x1 )), w(IS(x2 )), ..., w(IS(xk ))),{c } IS(x1 ) IS(x2 ) . . . IS(xk ) inconsistent subset soft clauses weight .Proof. set clearly inconsistent, satisfaction literal c results emptyclause set. minimum weight clause set . inconsistent subsetsoft clauses weight .soon determine c Top-k literal failed, apply (k, )-Rule split c use-Rule split clauses IS(x1 ) IS(x2 ) . . . IS(xk ). inconsistent subset weightobtained way. Observe k=l, {c} IS(x1 ) IS(x2 ) . . . IS(xl ) classicalinconsistent subset soft clauses weight min(w(c), w(IS(x1 )), w(IS(x2 )), ..., w(IS(xl )))clause split using -Rule.Example 8. Consider soft clauses produced Example 7, c1 = {(x6 , 4), (x4 , 1)} c2 ={(x2 , 5), (x5 , 2)}. test x2 c2 setting x2 =1. satisfy hard clauses x2 x6 x2 x4 ,need set x6 =0 x4 =0. Then, c1 = {(x6 , 4), (x4 , 1)} becomes falsified, making x2 failed.However, x5 failed. Therefore c2 Top-k literal failed k=1. Applying (k, )-Rulek=1 =3 c2 , c2 = {(x2 , 3)} c2 = {(x2 , 2), (x5 , 2)}. Applying -Rule=3 c1 , get c1 = {(x6 , 3), (x4 , 1)} c1 = {(x6 , 1)}. Consequently, get inconsistentsubset {c1 , c2 } weight 3.result, detection Top-k literal failed clause allows improve upper bound3, giving tightest upper bound 10.3.4.3 OP - K E MPTY C LAUSE ETECTIONnoteworthy literal declared failed, unit propagation literal falsifies literals clause. Sometimes, propagation literal makes weightedliterals soft clause c falsified, literals c. case, cannot declaredfailed, cannot improve upper bound using approaches presented above. However,split c using (k, )-Rule obtain falsified clause, declared failed.1v17v24v52 v36v6v4 3Figure 3: weighted graph 6 vertices 9 edges. Numbers indicate vertex weightExample 9. Consider weighted graph G Fig. 3, possible independent set partition G{{v6 , v3 }, {v2 ,v5 }, {v4 , v1 }} LW soft clauses c1 ={(x6 , 6), (x3 , 2)}, c2 ={(x2 , 7),814fiA N E XACT LGORITHMAXIMUM W EIGHT C LIQUE(x5 , 4)} c3 ={(x4 , 3), (x1 , 1)}. total weight soft clauses LW MaxSAT instancebased partition 16. one hand, literals largest weight softclause failed. set x6 =1, need set x2 =0 satisfy hard clause x2 x6 .literal x6 failed, unit propagation falsify clause. However,weighted literal x2 c2 falsified. split c2 c2 = {(x2 , 3)} c2 = {(x2 , 4), (x5 , 4)}.c2 falsified propagating x6 . So, x6 failed literal splitting c2 , c1 Top-k(k=1) literal failed clause split c1 = {(x6 , 3)} c1 = {(x6 , 3), (x3 , 2)} using(k, )-Rule. thus obtain inconsistent subset {c1 , c2 } weight =3. Consequently,upper bound improved 3.Example 9 suggests us define Top-k empty clause notion.Definition 2. LW soft clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . ., (xl , w(xl ))} Top-k empty,1k<l, literal assigned true, unit propagation falsifiesliterals x1 , x2 , . . ., xk c.straightforward show following proposition.Proposition 6. literal declared failed splitting Top-k empty clause using(k, )-Rule.Proof. Splitting Top-k empty clause gives clause c = x1 x2 . . . xk ,literal satisfaction makes c empty via unit propagation.3.4.4 OVERESTIMATING LGORITHMMWCLQAlgorithm 3 formally describes successively transform LW MaxSAT instance applying-Rule and/or (k, )-Rule obtain disjoint inconsistent subsets soft clauses every searchtree node MWCLQ. upper bound computed Algorithm 3 way tight,shown experimental results.Given weighted graph G, Algorithm 3 first encodes MWC instance LW MaxSATinstance based independent set partition G. partitioning procedure works follows.Vertices sorted decreasing order weights (ties broken favor verticeshigher degree) successively inserted independent set. Suppose currentindependent sets I1 , I2 , . . ., Ii (in order, 0 beginning partitioning process).current first vertex v inserted first Ij v unconnected verticesalready Ij . Ij exist, new independent set Ii+1 opened v insertedIi+1 . independent set encoded LW soft clause. total weights soft clausesinitial upper bound MWC G improved detecting disjoint inconsistentsubsets soft clauses using extended MaxSAT reasoning.detection inconsistent subset soft clauses performed detecting failed literalsshortest available soft clause c. literal failed either unit propagation falsifiesliterals another clause weighted literals another soft clause. caseliterals c failed unit propagation falsifies literals another clause, usualinconsistent subset soft clauses obtained. case k weighted literals cfalsified and/or unit propagation literal c falsifies weighted literals anothersoft clause t, (k, )-Rule applied split c and/or obtain inconsistent subsetsoft clauses. reason Top-k empty clauses Top-k literal failed clauses815fiFANG , L , & XUAlgorithm 3: overestimate(G), computing upper bound MWC MaxSAT reasoning1234567Input: weighted graph G=(V , E, w).Output: upper bound maximum weight clique G.beginpartition G independent sets I1 , I2 ,...,Ii ;encode PG LW MaxSAT instance lw mark soft clauses available;/* base upper bound */UB clw w(c);lw contains available soft clausec shortest available soft clause lw ;mark c unavailable;/* Let set soft clauses involved inconsistentsubset Stopk set top-k literal failed top-kempty clauses */8910111213141516171819202122232425262728293031323334, Stopk , k 0;k < length(c)k+1 failed empty clauseIS(k+1 );else k+1 failed Top-kt empty clause wkt (t) > 0(IS(k+1 ) \ {t});Stopk Stopk {t};else break;k k + 1;k > 0k = length(c){c};/* literals c failed */else wk (c) > 0Stopk Stopk {c};/* c top-k literal failed */else continue;min(w(S), mintStopk (wkt (t)));UB UB;/* improve upper bound > 0 */, Stopk;foreach clause clapply -Rule cl;= {cl };foreach Top-k literal failed clause Top-k empty clause Stopkapply (, kt )-Rule t;Stopk= Stopk{t };;lw (lw \ (S Stopk )) Stopkmark clauses lw available;return UB;/* update soft clauses *//* improved upper bound MaxSAT reasoning */816fiA N E XACT LGORITHMAXIMUM W EIGHT C LIQUEcollected Stopk . Meanwhile, soft clauses involved inconsistent subset collectedS. case, computed = min(w(S), mintStopk (wkt (t))) (Line 23), w(S)minimum clause weight S. Note Stopk empty usual inconsistent subset.-Rule (k, )-Rule applied split clauses subset upper bound improvedweight subset.Observe Algorithm 3 called every search tree node scratch graphdifferent search tree node different. Also observe detected inconsistent subsets softclauses disjoint, soft clause one subset used detect subset. fact,inconsistent subset soft clauses removed Line 32 detecting another inconsistentset. initial upper bound improved total weight inconsistent subsets.approach might improved using MaxSAT resolution defined work Bonet, Levyand, Manya (2006) Larrosa, Heras, Givry (2008), adapting approach AbrameHabet (2014) transforms inconsistent subset clauses weighted empty clauseset new clauses used detection inconsistent subsets soft clauses.Nevertheless, application MaxSAT resolution carefully driven approachcompetitive (Abrame & Habet, 2015), MaxSAT resolution produces many intermediateclauses.note clauses produced using -Rule (k, )-Rule cannot obtained simplyapplying MaxSAT resolution, MaxSAT resolution transforms MaxSAT instanceequivalent one, -Rule (k, )-Rule may change optimal solution LWMaxSAT instance Proposition 3.4. Empirical Evaluationsection, empirically evaluate MWCLQ extended MaxSAT reasoning using standard benchmarks, namely DIMACS benchmark, BHOSLIB benchmark, random graphs,benchmark winner determination problem (WDP). conducted four experimentsstudy. first experiment evaluate performance MWCLQ comparingstate-of-the-art exact algorithms. second experiment compare different encodingsMWC MaxSAT. third experiment investigate impact splitting soft clausesextended MaxSAT reasoning. forth experiment, applied MWC algorithms solveWDP instances compare performances.first introduce benchmarks used experiments describe experimental environment. present discuss experimental results detail.4.1 Benchmarks Experimental EnvironmentThree types benchmarks used first three experiments.1. DIMACS DIMACS benchmark taken Second DIMACS ImplementationChallenge, used widely benchmarking purposes literature algorithms MC, MWC, MVC, MIS on. 80 DIMACS instances generatedreal-world applications coding theory, fault diagnosis, Kellers conjectureSteiner Triple Problem, well random graphs generated different properties,DSJC, brock p hat families. size instances ranges less 50817fiFANG , L , & XUvertices 1,000 edges 4,000 vertices 5,000,000 edges. downloadedinstances website (ThanhVu & Thang, 2014).2. BHOSLIB BHOSLIB (Xu, 2004)(Benchmarks Hidden Optimum Solutions GraphProblems) instances based CSP model named RB (Xu & Li, 2000; Xu, Boussemart,Hemery, & Lecoutre, 2007). Phase transitions exist model RB transition pointslocated exactly. BHOSLIB instances generated phase transition regionmodel RB appear extremely hard solve various algorithms, evengraph size small (Liu, Lin, Wang, Su, & Xu, 2011; Xu & Li, 2006). firstly usedSAT competition 2004, widely used evaluate algorithms MC,MVC MIS.3. Random random graph n vertices density p generated randomly selectingedge probability p complete graph n vertices. experiments, nranges 150 700 p 0.5 0.95. Random graphs allow show asymptoticbehavior algorithm.convert non-weighted graph weighted graph associating weight w(vi ) = mod200+1 vertex vi . method initially proposed Pullan (2008) usedstandard converting approach generate weighted graphs non-weighted instances (Wuet al., 2012; Benlic & Hao, 2013).solvers used experiments implemented C/C++. compile usinggcc/g++ 4.7.2 option -O3. experiments running machine Intel(R) Xeon(R)CPU E5-2680 @ 2.70GHz, 8 cores 16G RAM Debian GNU/Linux 7.4. cut-off timesolver solve instance one hour (3600 seconds).4.2 Comparison MWCLQ Algorithmscompare MWCLQ two state-of-the-art exact solvers specific MWC, MinSAT solverCPLEX.1. Cliquer state-of-the-art solver MC problem MWC problem. bestknowledge, almost recent exact algorithms MWC (e.g., Kumlander, 2004; Shimizuet al., 2013, 2012) based Cliquer. used latest version Cliquer released2010, available homepage (Ostergard, 2010).2. DKum (Kumlander, 2004, 2008b) implemented VB. source code foundauthors homepage (Kumlander, 2008a). execute experimental environment,translated C.3. MinSatz (Li et al., 2012) exact weighted partial MinSAT solver, achievesstate-of-the-art performance solving clique problems combinatorial auction problems.4. CPLEX high-performance mathematical programming solver linear programming,mixed integer programming, quadratic programming, quadratically constrained programming problems. Let G = (V, E, w) weighted graph, V = {v1 , v2 , . . . , vn },818fiA N E XACT LGORITHMAXIMUM W EIGHT C LIQUEinteger programming formulation MWC instance G usually defined follows.maxnXxi w(vi )i=1subjectxi + xj 1,{vi , vj }/Exi {0, 1},= 1, 2, . . . , n.Observe at-most-one constraint enforced every independent set G. solutioninteger programming problem corresponds maximum weight clique G. CPLEX12.6 used experiments solve MWC encoding integer programmingproblem way.Table 1 shows runtimes different solvers DIMACS BHOSLIB benchmarks. 80instances DIMACS benchmark used experiment. MWCLQ solves 61 instanceswithin cut-off time, Cliquer, DKum, MinSatz CPLEX solves 52, 48, 58 44 instances, respectively. simplicity, exclude instances solved within 100 secondssolvers solved solver within 3600 seconds. 39 instances displayed Table 1,MWCLQ outperforms Cliquer 32 instances comparable Cliquer instances.MWCLQ significantly outperforms DKum instances except hamming10-2,solved within 20 seconds. MWCLQ dominates MinSatz 32 instances. particular,8 instances brock family, MWCLQ 20X faster MinSatz instances 400 vertices solves four instances 800 vertices, cannot solvedMinSatz. MWCLQ outperforms CPLEX 28 instances. Particularly, CPLEX cannot solveinstance brock p hat families, MWCLQ solve efficiently.interesting MinSatz CPLEX solve MANN a27 MANN a45 within cut-offtime, cannot solved specific MWC solver. Note MANN a27 MANN a45also hard heuristic algorithms. instance, BLS (Benlic & Hao, 2013) findapproximating solution 12281 success rate 16% within 396.58 seconds MANN a27,PLS (Pullan, 2008) find approximating solution 12264.also used 40 instances BHOSLIB benchmark. instances extremelyhard exact MWC solvers. evaluated solvers solve 5 smallest instances450 vertices. MWCLQ DKum solve 5 instances, CPLEX, Cliquer MinSatz solves 3,1 0 instances, respectively. Moreover, MWCLQ achieves best performance 45 instances.Table 2 shows mean runtimes random graphs. generate 50 graphs point. MWCLQ algorithm solves 700 graphs experiment within cutoff time,Cliquer, Dkum, MinSatz CPLEX solves 595, 548, 548 387 instances, respectively.MWCLQ significantly outperforms Dkum MinSatz instances dominates CPLEXpoints except (200, 0.95) algorithms solve instances within 30 seconds. MWCLQdominates Cliquer completely random graphs density D0.7, comparable Cliquer D<0.7. MWCLQ solver solve instances point (700, 0.7).experimental results suggest particular MWCLQ effective solving graphs819fiFANG , L , & XUTable 1: Runtimes (in seconds) DIMACS BHOSLIB benchmarks. cut-off time 3600seconds. |V | stands number vertices, density graph voptimal solution MWC. solver cannot solve instance within 3600seconds, runtime marked -. Instances solved within 100 seconds solverssolved solver within cut-off time omitted.Graph|V |brock400 1400brock400 2400brock400 3400brock400 4400brock800 1800brock800 2800brock800 3800brock800 4800C250.9250DSJC1000.51000DSJC500.5500gen200 p0.9 44200gen200 p0.9 55200gen400 p0.9 75400hamming10-21024johnson32-2-4496MANN a27378MANN a451035p hat1000-11000p hat1000-21000p hat1500-11500p hat500-1500p hat500-2500p hat500-3500p hat700-1700p hat700-2700san10001000san200 0.7 2200san200 0.9 1200san200 0.9 2200san200 0.9 3200san400 0.7 1400san400 0.7 2400san400 0.7 3400san400 0.9 1400sanr200 0.7200sanr200 0.9200sanr400 0.5400sanr400 0.7400Total: 80rb30-15-1450frb30-15-2450frb30-15-3450frb30-15-4450frb30-15-5450Total: 40747574756565656589505089899099889899244925255075255050698989897070709069895070v342233503471362631213043307629715092218617255043541680065051220331228334265151457771619123139205375144152901716242268256082474839413110277197762325512618352992828282828229903006299530323011Cliquer260.2366.1290.7287.4154816031702199032.500.97678.6171814690.121.030.025.591.03169.5403.9223.323290.2011500.1529.985210651820DKum607.2524.5573.6606.3221591.880.97119.8362.27.470.352.720.023.860.17213.423.28166.915.11470.3215.374.4848177.744.98423.4287.5154.05MinSatz2046273723631609640.6352732.8072.4035.194.45348721.751628252.31.5210.4110094.4239.9346.900.110.1016.37118.212.6234.7496.1218492.7368.979.44429.0580CPLEX30.412.441.81238.70.060.432.0732.610.700.310.833.5424.4587.7543.5313.3010.8111.0144193.5258.2118.33MWCLQ129.3127.7107.281.81142720021545203939.9981.300.927.002.7615.460.5325014.120.012.34916.70.1347.88183.800.241.6416.193.444.986.3712570.136.400.3125.5761244.930.14131.7181.857.315fiA N E XACT LGORITHMAXIMUM W EIGHT C LIQUETable 2: Mean runtimes seconds random graphs, obtained solving 50 graphs point.cut-off time 3600 seconds. |V | stands number vertices, densitygraph v weight optimal solution, averaged solved instances. mean runtime marked - instance solved point withincut-off time. # stands number instances solved solver within cut-offtime|V |150150200200200300300300500500600600700700Benchmarkv0.9033940.9547660.8032490.9050950.9573720.7024410.8033340.9053510.6022850.7029690.6024960.7032930.6025100.703283Total 700CliquerTime#21.06501006504.0250974.95001.765081.455003.9450119.95023.515012565046.0150300645595DKumTime#7.925035.42505.0550373.0502464193.0150107.15009.4550302.15062.4650288429132.8500548MinSatzTime#3.57502.215012.655094.6350111.15032.3650379.6500208.55029944810065002757500548CPLEXTime#2.58501.995054.835023.45503.9150353.550303.747263.540000000387MWCLQTime#0.58500.42500.995014.305028.77501.135014.8750845.4506.035093.365034.7950869.95078.2450243450700Table 3: Lower bounds given different solvers DIMACS instances cannot solvedsolver within cut-off time.InstanceC1000.9C2000.5C2000.9C4000.5C500.9MANN a81gen400 p0.9 55gen400 p0.9 65hamming10-4keller5keller6p hat1000-3p hat1500-2p hat1500-3p hat700-3|V |10002000200040005003321400400102477633611000150015007009050905090100909083758274517575Cliquer11812466534128418681952501285573828605112417289715212822DKum1846118678286320701201303731791798213916372893312720675156821MinSatz63852198674722635594100970598861804062331755956779610070507340CPLEX80661358736218546520111386661167205042331769377258595490587400MWCLQ847124661003426986672111033667668324614331763167588710484497565fiFANG , L , & XU104Time103102101MWCLQCliquerDKumsan200_0.9_2gen200_p0.9_44san400_0.7_3san400_0.7_2san200_0.9_3san400_0.7_1brock400_3p_hat700-2c250.9hamming10-2san1000brock800_3brock400_4p_hat500-3brock400_1brock800_1brock800_2p_hat1000-2brock800_4san400_0.9_1100Figure 4: Runtimes find optimal solution. Instances solvers find optimalsolution within 100 seconds MWCLQ finds optimal solution less one secondomittedhigh density. CPLEX also effective dense graphs, cannot solve random instance500 vertices.15 DIMACS instances cannot solved algorithm within cut-off time,report largest weight clique found solver algorithm terminates,lower bound optimal solution. Table 3 shows MWCLQ computes best lower bounds11 instances, CPLEX gives best lower bounds 5 instances. MWCLQ givesignificantly better lower bound MWC solvers instances except C2000.5,MWCLQ Cliquer share bound. result suggests MWCLQ often computebetter approximate solution exact solvers within given time.exact algorithm MC MWC usually solves instance two phases. first phase,algorithm finds optimal solution. second phase, algorithm provessolution indeed optimal showing better solution exists. Fig. 4, compareruntimes Cliquer, Dkum MWCLQ need find optimal solution DIMACS instance.822fiA N E XACT LGORITHMAXIMUM W EIGHT C LIQUEsimplicity, instances solvers find optimal solution within 100 secondsMWCLQ finds optimal solution less one second omitted. MWCLQ always findsoptimal solution much faster solvers instances except hamming10-2.instances brock800 2 brock800 4, although MWCLQ spends time Cliquerexactly solving (see Table 1), finds optimal solution 10 times faster Cliquer.summary, Table 3 Fig. 4 show MWCLQ generally finds optimal solutionmuch faster solvers, although sometimes may spend time prove optimality.4.3 Comparison Different Encodings MWC MaxSATpresented four encodings MWC MaxSAT, namely direct encoding, split encoding, iterative split encoding LW encoding. MWC instance encodedMaxSAT instance using first three encodings, MaxSAT solver used searchoptimal solution MWC instance. However, MWC instance encoded MaxSATinstance using LW encoding, optimal solution MaxSAT instance MWC,upper bound MWC. Therefore, MaxSAT solver used solve MWC instance,first three encodings used.Different MaxSAT solvers, MWCLQ uses MaxSAT reasoning upper bounding procedure search tree node, current subgraph dynamically encodedMaxSAT instance. So, four encodings could used MWCLQ encode currentsubgraph.Recall MWCLQ based LW encoding. implemented three versionsMWCLQ, i.e., MWCLQdir , MWCLQsp MWCLQit, identical MWCLQ,based direct encoding, split encoding iterative split encoding, respectively.MaxSAT instance obtained using direct encoding, inconsistent subsets contain two softclauses, soft clauses unit. MWCLQdir detects inconsistent subset{(x1 , w(x1 )), (x2 , w(x2 ))}, weighted clause subset, say, (x2 , w(x2 )), split(x2 , w(x1 )) (x2 , w(x2 )w(x1 )). upper bound improved w(x1 ),clause (x2 , w(x2 )w(x1 )) used detection. MaxSAT instance obtained usingsplit iterative split encoding, vertices soft clause weight.Observe Top-k literal failed clause Top-k empty clause make sense,(k, )-Rule needed three encoding.experiment, ran following four different kinds state-of-the-art MaxSAT solverssolve MWC instances encoded MaxSAT using direct encoding, split encodingiterative split encoding.1. akMaxSat (Kugel, 2010) branch-and-bound MaxSAT solver computes lowerbound combination MaxSAT resolution detection disjoint inconsistent subsets. One authors sent us source code submitted MaxSAT evaluation 2012.2. MaxSatz (Li et al., 2007) branch-and-bound MaxSAT solver incorporates SATtechnologies. used latest version MaxSatz2013, one best solversMaxSAT evaluation 2013.3. WPM1-2013 (Ansotegui, Bonet, & Levy, 2009) MaxSAT solver based successive callsSAT solvers. One authors provided us executable file WPM usedMaxSAT evaluation 2013.823fiFANG , L , & XU4. MaxHS (Davies & Bacchus, 2013b) hybrid Maxsat solver exploits SATinteger programming technologies.report number instances solved MWCLQdir , MWCLQsp, MWCLQit, MWCLQ,akMaxSat, MaxSatz MaxHS within 3600 seconds. results WPM reported, solves 3 DIMACS instances solve BHOSLIB instance. addition,always runs memory solving random instances.Table 4: Number instances solved within 3600 seconds MaxSAT solvers using three differentencodings four versions MWCLQ. Direct stands direct encoding, Splitsplit encoding Iter iterative split encoding.InstanceBenchmark#brock12c-fat7C7DSJC2gen5hamming6johnson4keller3MANN4p hat15san15DIMACS: 80BHOSLIB: 40(150,0.9)50(150,0.95)50(200,0.8)50(200,0.9)50(200,0.95)50(300,0.7)50(300,0.8)50(300,0.9)50(500,0.6)50(500,0.7)50(600,0.6)50(600,0.7)50(700,0.6)50(700,0.7)50RAND: 700akMaxSatDirectSplitIter4447772220002225553331111115457993738390014350504650503850504450504450503048501479888000000000000000000267313317Direct47212531278420504949494950472000001346MaxSatzSplit47212531258400505050505050190000000319Iter47212531278425505050505050180000000318Direct4720354134134635050505050504135000000376MaxHSSplit07100430214220750001400000000071Iter07100320203180750001400000000071MWCLQdir471203311863605005000505005049500500399MWCLQsp8722243118104805050504125505005041500500507MWCLQit872224311914535505050505050502950505039500618MWCLQ127222531111156155050505050505050505050505050700Experimental results Table 4 suggest encoding approaches affect performancesMaxSAT solvers solve MWC instance. However, effectiveness direct encoding,split encoding iterative encoding different MaxSAT solvers clear. Concretely,iterative split encoding makes akMaxSat little faster encodings do. MaxHS usingdirect encoding dominates MaxHS using encodings. iterative split encoding betterchoice MaxSatz solve BHOSLIB instances, direct encoding better solve randomgraphs. results also show MWCLQ significantly outperforms MaxSAT solvers solveMWC instance. observe although MaxSAT reasoning powerful improving upperbound BnB algorithm MC MWC, using MaxSAT solver solve MWC instanceeffective.824fiA N E XACT LGORITHMAXIMUM W EIGHT C LIQUEThanks extended MaxSAT reasoning, MWCLQ significantly outperforms MWCLQdir ,MWCLQsp MWCLQit . MWCLQdir MWCLQsp slow solving MWC instancescannot capture graph structure well. Although iterative split encodingMWCLQit capture graph structure, MWCLQit exploit power MaxSATreasoning efficiently, clause splittings MWCLQit driven MaxSAT reasoning.advantages MWCLQ described follows: (1) BnB algorithm MWCable exploit graph structure, especially ordering vertices partitioning graphindependent sets search tree node, MaxSAT solver do; (2) exploitspower MaxSAT reasoning upper bounding procedure derive tight upper bound,classical BnB algorithm MWC do. Nevertheless, make MaxSAT reasoningbeneficial, relevant encoding MWC MaxSAT necessary, shown Table 4.encoding obvious impact performance MWCLQ.4.4 Effectiveness Upper Bound Based Extended MaxSAT Reasoningstudy impact extended MaxSAT reasoning soft clause splitting, implementedtwo derived versions MWCLQ, namely, MWCLQ-- MWCLQ-. MWCLQ-- identicalMWCLQ except uses trivial bound based independent partition graph,equal sum largest weight independent set. MWCLQ- identicalMWCLQ except detects inconsistent subsets soft clauses, use -Rule(k, )-Rule split clauses subset.Table 5 shows runtimes (in seconds) search tree sizes (in thousands) MWCLQ--, MWCLQand MWCLQ solving DIMACS BHOSLIB instances, well gain ratioMWCLQ- MWCLQ compared MWCLQ-- terms runtime search tree size computed MWCLQ--/MWCLQ- MWCLQ--/MWCLQ, respectively. MaxSAT reasoning withoutsoft clause splitting MWCLQ- makes MWCLQ- better MWCLQ-- terms search treesize. reduction search tree size enough general compensate overheadMaxSAT reasoning, gain MWCLQ- compared MWCLQ-- terms runtimeclear. However, soft clause splitting using -Rule (k, )-Rule allows MaxSAT reasoning prune much search space, making MWCLQ substantially (from 1.08 4.05 times)faster MWCLQ--. fact, MWCLQ solves 3 instances MWCLQ-- MWCLQ-,significantly outperforms instances except san1000. Observe MWCLQ--MWCLQ- solve number instances Minsatz (58) DIMACS benchmark,solve instances Cliquer (52), Dkum (48) CPLEX (44).Table 6 shows mean runtimes (in seconds) mean search tree sizes (in thousands) differentversions MWCLQ random instances, averaged solved instances point,well gain ratio MWCLQ MWCLQ- compared MWCLQ--. MWCLQ solvesinstances. However, neither MWCLQ-- MWCLQ- solve graphs points (300, 0.90)(700, 0.7). MaxSAT reasoning allows MWCLQ- prune search space MWCLQ-on instances, overhead makes MWCLQ- slower instances 300 vertices. However, similarly DIMACS BHOSLIB cases, MWCLQ 1.2 14.3 timesfaster MWCLQ--, soft clause splittings using -Rule (k, )-Rule allowMWCLQ substantially reduce search space instances. Furthermore, gain termsruntime search tree size increases density graph.825fiFANG , L , & XUTable 5: Runtimes (in seconds) search tree sizes (in thousands) different versions MWCLQ solving DIMACS BHOSLIB instances, well gain ratio MWCLQ MWCLQ- compared MWCLQ--. cut-off time 3600 seconds. -means solver cannot solve instance within cut-off time. Instances solvedwithin 10 seconds solvers solved within cut-off time omitted.BenchmarkGraphbrock400 1brock400 2brock400 3brock400 4brock800 1brock800 2brock800 3brock800 4C250.9DSJC1000.5gen200 p0.9 44gen200 p0.9 55hamming10-2p hat1000-2p hat500-3p hat700-2san1000san200 0.9 3san400 0.7 3san400 0.9 1sanr200 0.9sanr400 0.7frb30-15-1frb30-15-2frb30-15-3frb30-15-4frb30-15-5MWCLQ-TimeSize209.063153203.558770173.450079131.5374201813421259256364848619684705942623675790159.43154287.732615427.83756010.4125303009457787113.417715135.93117356.31159559.79293625.90669035.7112228411.315631142.2813416171.867246267.98313985.2722362Time234.3227.7194.2146.31990269522012931160.591.8723.128.702813117.5137.248.4710.5320.1039.60394.839.17167.0257.382.23MWCLQGainSize0.89545050.89512440.89436160.90328530.913964250.956054330.894425180.896333670.99201140.95252071.2036921.2013101.072389490.97105710.99306801.1679460.9324031.2930010.90107971.041084761.0892871.03484011.04634111.0417705Gain1.161.151.151.141.061.071.061.071.571.042.051.931.921.681.022.011.222.231.131.441.441.391.311.26Time129.3127.7107.281.81142720021545203939.9981.307.002.7615.462501916.747.88183.816.196.3712576.4025.57244.930.14131.7181.857.31MWCLQGainSize1.62253671.59237881.62198301.61151331.272237151.283399101.272482181.293527003.9944721.08170323.989793.77344417581768923.28771152.3741860.74216943.4821801.541213631064.058501.4057051.68279281.4025971.30145891.47163361.495575Gain2.492.472.532.471.881.911.901.927.051.547.727.345.944.231.447.322.427.872.145.605.174.615.094.01Table 6: Runtimes (in seconds) search tree sizes (in thousands) different versions MWCLQ random instances, averaged solved instances point, wellgain ratio MWCLQ MWCLQ- compared MWCLQ--. cut-off time3600 seconds solver solve one instance.Graph|V |1500.901500.952000.802000.902000.953000.703000.803000.905000.605000.706000.606000.707000.607000.70#5050505050505031505050505031MWCLQ-TimeSize1.857155.2314401.7172549.4114030405.8817721.5855227.09765522613998327.532310133.83484842.8513483122932060794.10248973177682683#5050505050505033505050505015Time1.341.281.8940.13117.11.8030.9822618.12149.546.401390104.83343MWCLQGainSize1.382554.091230.905391.2361093.4785830.884930.8762651.002450040.9321760.89319050.92126610.882926270.90236240.95575774826Gain2.8111.691.342.309.531.121.221.631.061.091.061.101.051.19#5050505050505050505050505050Time0.580.420.9914.3028.771.1314.87845.46.0393.3634.79869.978.242434MWCLQGainSize3.1910812.45461.732453.46203114.1024541.402631.8226112.67856761.2513141.43163261.2376671.411495221.20142191.31351894Gain6.5830.692.966.9133.322.102.934.671.762.131.762.141.751.94fiA N E XACT LGORITHMAXIMUM W EIGHT C LIQUE4.5 Application MWCLQ Winner Determination Problemimportant application MWC solve winner determination problem (WDP) combinatorial auctions. auctioneer set items, = {1, 2, . . . , m}, sell, buyerssubmit set n bids, B = {B1 , B2 , . . . , Bn }. bid pair Bi = (Si , Pi ), Sisubset items Pi 0 price items Si . WDP problem determinebids winning losing maximum auctioneers revenue, itemallocated one bidder. define MWC instance G = (V, E, w) WDP instancefollows.bid Bi B, define vertex vi weight w(vi ) = Pi , i.e., V = {v1 , v2 , . . . , vn }w(vi ) = Pi ;Add edge {vi , vj } G Bi Bj share common items, i.e., E ={{vi , vj } | Si Sj = , 1 < j n}.maximum weight clique G corresponds feasible subset bids maximum revenue.experiment, compared MWCLQ Cliquer, Dkum, MinSatz CPLEX realistic WDP instances. also selected MaxHS using direct encoding,effective MaxSAT solver solve MWC instance, comparison. used benchmarkprovided Lau Goh (2002), widely used benchmark purpose test WDPalgorithms (Guo, Lim, Rodrigues, & Zhu, 2006; Sghir, Hao, Jaafar, & Ghedira, 2014; Wu & Hao,2015). Instances benchmark generated incorporating following factors, i.e., pricing factor models bidders acceptable price range bid, preference factortakes account bidders preferences among bids, fairness factor measures fairness distributing items among bidders. benchmark contains 500 instances 1500items 1500 bids, divided 5 groups item number bid number.group contains 100 instances labeled REL-m-n, number items nnumber bids.Table 7: Mean runtimes seconds WDP instances, obtained solving 100 graphsgroup. cut-off time 3600 seconds. mean runtime marked - instancesolved group within cut-off time. # stands number instances solvedsolver.BenchmarkGroupREL-500-1000REL-1000-1000REL-1000-500REL-1000-1500REL-1500-1500#100100100100100CliquerTime#809.91003.731000.031002.841003.38100DKumTime#628.31004.521000.031003.551005.30100MinSatzTime#552.510025.391000.8110067.9010078.19100MaxHSTime#001003.710000CPLEXTime#00266.710000MWCLQTime#55.611001.451000.051001.561002.42100Table 7 summarizes mean runtimes numbers instances solved within cut-off timegroup. Results show MWCLQ outperforms solvers groups except REL-1000500, Cliquer, Dkum, MinSatz MWCLQ comparable. MWCLQ achieves least10X speedup instances hardest group REL-500-1000. MaxHS CPLEXeffective solving instances. experiment, transformed WDP MWC first827fiFANG , L , & XUformulated integer programming. Another method used Wu Hao (2015)formulate WDP directly integer programming. method appears slightlyeffective transformation experiment, affect comparison,instances CPLEX able solve within 3600 seconds also REL-1000-500transformation.Table 8 reports detailed comparative results first 10 instances group. MWCLQ outperforms solvers instances except 10 instances easiest group(in401, in402, ..., in410), solved Cliquer, DKum, MinSatz MWCLQ withinless one second. results suggest MWCLQ effective solving MWC instances WDP specific MWC algorithms, MinSatz, MaxHS CPLEX. Moreover,MWCLQ even efficient state-of-the-art heuristic algorithms relativelyhard instances. example, MWCLQ solves in108 101.04 seconds, heuristic algorithm (Wu & Hao, 2015), based tabu search takes, 113.53 seconds find solutionprobability 0.73. Ignoring difference running environments, MWCLQ faster tabusearch algorithm in108. Note heuristic algorithms give feasible solution,cannot guarantee optimality.5. ConclusionMaxSAT reasoning proved effective MC problem, based partitiongraph independent sets. However, MaxSAT reasoning cannot naturally extended solveMWC problem literal weights, shown relatively poor performanceMWCLQ- , MWCLQdir , MWCLQsp MWCLQit. MWCLQ- exploits MaxSAT reasoningdeal literal weights. encodings MWC MaxSAT used MWCLQdirMWCLQsp capture well graph structure. Although MWCLQit exploit graphstructure, many useless splits difficulties take advantage MaxSAT reasoningefficiently. thus propose encode MWC instance literal-weighted MaxSAT instance,soft clauses literals soft clauses weighted. optimal solutionLW MaxSAT instance MWC, upper bound MWC. interest LWencoding transform LW MaxSAT instance optimal solution newinstance tighter upper bound MWC.Concretely, every search tree node BnB algorithm MWC, partition currentsubgraph independent sets obtain LW MaxSAT instance, soft clausecorresponds independent set. successively transform LW MaxSAT instanceidentifying Top-k literal failed clause Top-k empty clause using -Rule(k, )-Rule. Consequently, obtain tight upper bound MWC prune searchspace. approach implemented MWCLQ, substantially better MWCLQ-,MWCLQdir , MWCLQsp MWCLQit, confirming effectiveness approach. MWCLQalso favorably compared state-of-the-art MWC solvers Cliquer, DKum, MinSatzCPLEX, well several state-of-the-art MaxSAT solvers using different encodings, standardbenchmarks instances realistic applications.future, plan study impact vertex ordering unit clause ordering MWCLQ. also interesting use MaxSAT reasoning solve combinatorial optimizationproblems, especially weighted version, using dedicated encoding.828fiA N E XACT LGORITHMAXIMUM W EIGHT C LIQUETable 8: Runtimes seconds first 10 instances group benchmark Table 7.cut-off time 3600 seconds. |V | stands number vertices, densitygraph transforming WDP MWC v optimal solution MWC.solver cannot solve instance within 3600 seconds, runtime marked -Graphin101in102in103in104in105in106in107in108in109in110in201in202in203in204in205in206in207in208in209in210in401in402in403in404in405in406in407in408in409in410in501in502in503in504in505in506in507in508in509in510in601in602in603in604in605in606in607in608in609in610|V |10001000100010001000100010001000100010001000100010001000100010001000100010001000500500500500500500500500500500150015001500150015001500150015001500150015001500150015001500150015001500150015000.310.300.310.300.300.300.300.310.300.300.150.160.160.170.160.160.170.160.160.160.150.150.160.170.170.140.170.160.140.170.090.080.080.080.080.080.080.070.080.080.090.090.090.100.100.100.100.100.100.11v72724.61772518.22272129.50072709.64675646.12771258.61369713.40375813.20569475.89568295.28981557.74290708.12786239.21487075.42886515.95191518.96493129.24894904.67987268.96589962.39677417.48276273.33674843.95778761.69075915.90072863.32476365.71777018.83373188.61973791.65888656.95886236.91187812.37785600.00184860.16584623.41490288.47286853.50088316.08789014.137108800.445105611.476105121.021107733.805109840.984107113.067113180.284105266.107109472.332113716.965Cliquer819.6414.9551.4330.8840.3272.7595.91321247.3308.51.803.733.636.133.112.373.954.072.133.540.020.020.030.050.050.020.050.040.010.044.641.693.442.372.161.412.821.243.591.294.522.011.843.863.722.724.202.212.776.14DKum616.1289.0455.5218.1547.3346.4460.11121238.8307.72.474.986.657.204.853.375.695.183.384.780.030.020.040.080.080.020.070.040.020.065.682.304.423.802.822.323.731.604.372.195.512.982.565.715.754.376.653.723.7810.03829MinSatz610.8308.8479.6353.5367.7270.7736.9924.9305.0411.722.0925.9026.6630.4727.4222.8528.1822.0926.6622.090.690.720.760.890.880.750.790.820.710.8581.2565.1672.0661.3262.0965.9268.9958.2664.3964.3976.0470.6165.9684.5875.2777.6078.3777.6069.8494.67MaxHS923.011107.2891.231082.91300.6992.341569.11187.2846.561132.1-CPLEX181.70198.12229.0208.97224.98175.15364.58199.39217.31243.42-MWCLQ53.8734.4044.9437.0037.3323.5668.07101.0429.1043.161.011.371.572.011.641.111.691.051.541.210.040.040.040.060.060.050.060.060.040.062.301.451.721.561.561.341.321.111.801.152.391.751.353.092.222.132.382.171.863.70fiFANG , L , & XU6. Acknowledgmentswould like thank anonymous reviewers helpful comments suggestions. alsothank Jichang Zhao, Qiao Kan, Xu Feng Shaowei Cai proofreads suggestions. Partwork done first author joint Ph.D. student Universite de Picardie JulesVerne. research partly supported NSFC (Grant No. 61421003), fund StateKey Lab Software Development Environment (Grant No. SKLSDE-2015ZX-05), ChineseState Key Laboratory Software Development Environment Open Fund (Grant No. SKLSDE2012KF-07), MeCS platform Universite de Picardie Jules Verne.ReferencesAbrame, A., & Habet, D. (2014). Efficient application max-sat resolution inconsistent subsets.Proc. CP-2014, pp. 92107. Springer.Abrame, A., & Habet, D. (2015). resiliency unit propagation max-resolution. Proc.AAAI-2015, pp. 268274. AAAI Press.Ansotegui, C., Bonet, M. L., & Levy, J. (2009). Solving (weighted) partial maxsat satisfiability testing. Theory Applications Satisfiability Testing-SAT 2009, pp. 427440.Ansotegui, C., Bonet, M. L., & Levy, J. (2013). Sat-based maxsat algorithms. Artificial Intelligence,196, 77105.Ansotegui, C., & Gabas, J. (2013). Solving (weighted) partial maxsat ILP. Proc. CPAIOR2013, pp. 403409.Benlic, U., & Hao, J. K. (2013). Breakout local search maximum clique problems. Computers& Operations Research, 40, 192206.Bonet, M. L., Levy, J., & Manya, F. (2006). complete calculus max-sat. TheoryApplications Satisfiability Testing-SAT 2006, pp. 240251. Springer.Cai, S., Su, K., Luo, C., & Sattar, A. (2013). NuMVC: efficient local search algorithmminimum vertex cover. Journal Artificial Intelligence Research, 46, 687716.Cai, S., Su, K., & Sattar, A. (2011). Local search edge weighting configuration checkingheuristics minimum vertex cover. Artificial Intelligence, 175(9), 16721696.Chen, J. (2010). new SAT encoding at-most-one constraint. International WorkshopModelling Reformulating Constraint Satisfaction Problems.Davies, J., & Bacchus, F. (2013a). Exploiting power mip solvers maxsat. TheoryApplications Satisfiability TestingSAT 2013, pp. 166181. Springer.Davies, J., & Bacchus, F. (2013b). Postponing optimization speed MAXSAT solving. Proc.CP-2013, pp. 247262. Springer.Downey, R. G., & Fellows, M. R. (1995). Fixed-parameter tractability completeness I: Basicresults. SIAM Journal Computing, 24(4), 873921.Fahle, T. (2002). Simple fast: Improving branch-and-bound algorithm maximum clique.Proc. ESA-2002, pp. 485498.Fang, Z., Chu, Y., Qiao, K., Feng, X., & Xu, K. (2014a). Combining edge weight vertex weightminimum vertex cover problem. Frontiers Algorithmics, pp. 7181. Springer.830fiA N E XACT LGORITHMAXIMUM W EIGHT C LIQUEFang, Z., Li, C. M., Qiao, K., Feng, X., & Xu, K. (2014b). Solving maximum weight clique usingmaximum satisfiability reasoning. Proc. ECAI-2014, Vol. 263, pp. 303 308.Feige, U. (2004). Approximating maximum clique removing subgraphs. SIAM JournalDiscrete Mathematics, 18(2), 219225.Freeman, J. W. (1995). Improvements propositional satisfiability search algorithms. Ph.D. thesis.Guo, Y., Lim, A., Rodrigues, B., & Zhu, Y. (2006). Heuristics bidding problem. Computers &operations research, 33(8), 21792188.Ignatiev, A., Morgado, A., & Marques-Silva, J. (2014). reducing maximum independent setminimum satisfiability. Theory Applications Satisfiability TestingSAT 2014, pp.103120. Springer.Karp, R. M. (1972). Reducibility among combinatorial problems. Complexity ComputerComputations, pp. pp 85103. Springer.Kibanov, M., Atzmueller, M., Scholz, C., & Stumme, G. (2014). Temporal evolution contactscommunities networks face-to-face human interactions. Science China InformationSciences, 57(3), 117.Konc, J., & Janezic, D. (2007). improved branch bound algorithm maximum cliqueproblem. Communications Mathematical Computer Chemistry, 58, 569590.Kugel, A. (2010). Improved exact solver weighted Max-SAT problem. Workshop Pragmatics SAT, Vol. 436.Kugel, A. (2012). Natural Max-SAT encoding Min-SAT. Learning Intelligent Optimization, pp. 431436. Springer.Kumlander, D. (2004). new exact algorithm maximum-weight clique problem basedheuristic vertex-coloring backtrack search. Proc. MOC-2004, pp. 202208.Kumlander, D. (2008a) http://www.kumlander.eu/graph/index.html.Kumlander, D. (2008b). importance special sorting maximum-weight clique algorithmbased colour classes. Modelling, computation optimization information systemsmanagement sciences, pp. 165174. Springer.Larrosa, J., Heras, F., & de Givry, S. (2008). logical approach efficient max-sat solving.Artificial Intelligence, 172(2), 204233.Lau, H. C., & Goh, Y. G. (2002). intelligent brokering system support multi-agent web-based4 th-party logistics. Proc. ICTAI-2002, pp. 154161. IEEE.Li, C. M., & Anbulagan, A. (1997). Heuristics based unit propagation satisfiability problems.Proc. IJCAI-1997, pp. 366371. Morgan Kaufmann Publishers Inc.Li, C. M., Fang, Z., & Xu, K. (2013). Combining MaxSAT reasoning incremental upper boundmaximum clique problem. Proc. ICTAI-2013, pp. 939946. IEEE.Li, C. M., Manya, F., & Planes, J. (2005). Exploiting unit propagation compute lower boundsbranch bound max-sat solvers. Proc. CP-2005, Vol. 3709, pp. 403414. Springer.Li, C. M., Manya, F., & Planes, J. (2007). New inference rules Max-SAT. Journal ArtificialIntelligence Research, 30, 321359.831fiFANG , L , & XULi, C. M., & Manya, F. (2009). Maxsat, hard soft constraints.. Handbook satisfiability, 185,613631.Li, C. M., Manya, F., & Planes, J. (2006). Detecting disjoint inconsistent subformulas computinglower bounds max-sat. Proc. AAAI-2006, Vol. 6, pp. 8691.Li, C. M., & Quan, Z. (2010a). Combining graph structure exploitation propositional reasoningmaximum clique problem. Proc. ICTAI-2010, Vol. 1, pp. 344351. IEEE.Li, C. M., & Quan, Z. (2010b). efficient branch-and-bound algorithm based MaxSATmaximum clique problem. Proc. AAAI-2010, pp. 128133.Li, C. M., Zhu, Z., Manya, F., & Simon, L. (2012). Optimizing minimum satisfiability. Artificial Intelligence, 190, 3244.Liu, T., Lin, X., Wang, C., Su, K., & Xu, K. (2011). Large hinge width sparse random hypergraphs. Proc. IJCAI-2011, Vol. 2011, pp. 611616.Ma, T., & Latecki, L. J. (2012). Maximum weight cliques mutex constraints video objectsegmentation. Proc. CVPR-2012, pp. 670677. IEEE.Martins, R., Joshi, S., Manquinho, V., & Lynce, I. (2014). Incremental cardinality constraintsmaxsat. Proc. CP-2014, pp. 531548. Springer.Mascia, F., Cilia, E., Brunato, M., & Passerini, A. (2010). Predicting structural functional sitesproteins searching maximum-weight cliques. Proc. AAAI-2010, pp. 12741279.AAAI.Morgado, A., Dodaro, C., & Marques-Silva, J. (2014). Core-guided maxsat soft cardinalityconstraints. Proc. CP-2014, pp. 564573. Springer.Morgado, A., Heras, F., Liffiton, M., Planes, J., & Marques-Silva, J. (2013). Iterative coreguided maxsat solving: survey assessment. Constraints, 18(4), 478534.Ostergard, P. (2001). new algorithm maximum-weight clique problem. Nordic JournalComputing, 8, 424436.Ostergard, P. (2002). fast algorithm maximum clique problem. Discrete Applied Mathematics, 120, 197207.Ostergard, P. (2010). Cliquer source code. http://users.tkk.fi/pat/cliquer.html.Pullan, W. (2008). Approximating maximum vertex/edge weighted clique using local search.Journal Heuristics, 19, 117134.Pullan, W., & Hoos, H. H. (2006). Dynamic local search maximum clique problem. JournalArtificial Intelligence Research, 25, 159185.Regin, J. C. (2003). Solving maximum clique problem constraint programming. Proc.CPAIOR-2003, pp. 634648.Sghir, I., Hao, J.-K., Jaafar, I. B., & Ghedira, K. (2014). recombination-based tabu search algorithm winner determination problem. Artificial Evolution, pp. 157167. Springer.Shimizu, S., Yamaguchi, K., Saitoh, T., & Masuda, S. (2012). improvements Kumlandersmaximum weight clique extraction algorithm. Proc. International ConferenceElectrical, Computer, Electronics Communication Engineering, pp. 307311.832fiA N E XACT LGORITHMAXIMUM W EIGHT C LIQUEShimizu, S., Yamaguchi, K., Saitoh, T., & Masuda, S. (2013). Optimal table method findingmaximum weight clique. Proc. 13th International Conference Applied ComputerScience, No. 12. WSEAS.ThanhVu, H. N., & Thang, B. (2014). DIMACS benchmark. https://turing.cs.hbg.psu.edu/txn131/clique.html.Tomita, E., & Kameda, T. (2007). efficient branch-and-bound algorithm finding maximumclique computational experiments. Journal Global Optimization, 37, 95111.Tomita, E., & Seki, T. (2003). efficient branch-and-bound algorithm finding maximumclique. Proc. Discrete Mathematics Theoretical Computer Science, Vol. 2731, pp.278289.Wu, Q., Hao, J. K., & Glover, F. (2012). Multi-neighborhood tabu search maximum weightclique problem. Annals Operations Research, 196, 611634.Wu, Q., & Hao, J.-K. (2015). Solving winner determination problem via weighted maximumclique heuristic. Expert Systems Applications, 42(1), 355365.Xu, K., Boussemart, F., Hemery, F., & Lecoutre, C. (2007). Random constraint satisfaction: Easygeneration hard (satisfiable) instances. Artificial Intelligence, 171, 514534.Xu, K., & Li, W. (2000). Exact phase transitions random constraint satisfaction problems. JournalArtificial Intelligence Research, 12, 93103.Xu, K. (2004). BHOSLIB: Benchmarks hidden optimum solutions graph problems. http://www.nlsde.buaa.edu.cn/kexu/benchmarks/graph-benchmarks.htm.Xu, K., & Li, W. (2006). Many hard examples exact phase transitions. Theoretical ComputerScience, 355(3), 291302.Yamaguchi, K., & Masuda, S. (2008). new exact algorithm maximum weight cliqueproblem. Proc. ITC-CSCC-2008, pp. 317320.Zhang, D., Javed, O., & Shah, M. (2014a). Video object co-segmentation regulated maximumweight cliques. Proc. ECCV-2014, pp. 551566. Springer.Zhang, W., Nie, L., Jiang, H., Chen, Z., & Liu, J. (2014b). Developer social networks softwareengineering: construction, analysis, applications. Science China Information Sciences,57(12), 123.Zhian, H., Sabaei, M., Javan, N. T., & Tavallaie, O. (2013). Increasing coding opportunities using maximum-weight clique. Proc. Computer Science Electronic EngineeringConference-2013, pp. 168173. IEEE.Zhu, Z., Li, C. M., Manya, F., & Argelich, J. (2012). new encoding MinSAT MaxSAT.Proc. CP-2012, pp. 455463. Springer.Zuckerman, D. (2006). Linear degree extractors inapproximability max clique chromatic number. Proceedings 38th annual ACM symposium Theory computing,pp. 681690. ACM.833fiJournal Artificial Intelligence Research 55 (2016) 743-798Submitted 06/15; published 03/16Knowledge Representation ProbabilisticSpatio-Temporal Knowledge BasesFrancesco ParisiFPARISI @ DIMES . UNICAL .Department Informatics, Modeling,Electronics System EngineeringUniversity Calabria, Rende, ItalyJohn GrantGRANT @ CS . UMD . EDUDepartment Computer Science UMIACSUniversity Maryland, College Park, USAAbstractrepresent knowledge integrity constraints formalization probabilistic spatiotemporal knowledge bases. start defining syntax semantics formalization calledPST knowledge bases. definition generalizes earlier version, called SPOT,declarative framework representation processing probabilistic spatio-temporal dataprobability represented interval exact value unknown. augmentprevious definition adding type non-atomic formula expresses integrity constraints.result highly expressive formalism knowledge representation dealing probabilistic spatio-temporal data. obtain complexity results checking consistency PSTknowledge bases answering queries PST knowledge bases, also specify tractablecases. domains PST framework finite, extend results also arbitrarilylarge finite domains.1. IntroductionRecent years seen great deal interest tracking moving objects. fundamentalissue many applications providing location-based context-aware services, emergency call-out assistance, live traffic reports, food drink finder, location-based advertising, mobile tourist guidance, pervasive healthcare, analysis animal behavior (Ahson & Ilyas, 2010;Petrova & Wang, 2011; Karimi, 2013). innovative services becoming widely diffusedMarketsandMarkets forecasts location-based services market grow $8.12billion 2014 $39.87 billion 2019 (MarketsandMarkets, 2014).important aspect systems providing location-based context-aware servicesneed manage spatial temporal data together. reason, researchers investigated detail representation processing spatio-temporal data, AI (Cohn &Hazarika, 2001; Gabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev, 2005; Yaman, Nau,& Subrahmanian, 2004, 2005a; Knapp, Merz, Wirsing, & Zappe, 2006) databases (Agarwal,Arge, & Erickson, 2003; Pelanis, Saltenis, & Jensen, 2006). However, many cases locationobjects uncertain: cases handled using probabilities (Parker, Yaman, Nau, &Subrahmanian, 2007b; Tao, Cheng, Xiao, Ngai, Kao, & Prabhakar, 2005). Sometimes probabilities known exactly. Indeed, position object given time estimatedmeans location estimation method proximity (where location objectderived vicinity one antennas), fingerprinting (where radio signal strength meac2016AI Access Foundation. rights reserved.fiPARISI & G RANTsurements produced moving object matched radio map builtsystem working), dead reckoning (where position object derived lastknown position, assuming direction motion either speed travelled distanceknown) (Ahson & Ilyas, 2010; Karimi, 2013). However, since location estimation methodslimited accuracy precision, asserted object given positiongiven time probability whose value belongs interval. SPOT (Spatial PrObabilistic Temporal) framework introduced Parker, Subrahmanian, Grant (2007a) providedeclarative framework representation processing probabilistic spatio-temporal dataprobabilities known exactly.SPOT framework able represent atomic statements form object id is/was/willinside region r time probability interval [`, u]. allows representationinformation concerning moving objects several application domains. cell phone providerinterested knowing cell phones range towers given timeprobability (Bayir, Demirbas, & Eagle, 2010). transportation company interestedpredicting vehicles given road given time (and probability)order avoid congestion (Karbassi & Barth, 2003). Finally, retailer interested knowingpositions shoppers moving shopping mall order offer suitable customized couponsdiscounts (Kurkovsky & Harihar, 2006).framework introduced Parker et al. (2007a) extended Parker, Infantes, Subrahmanian, Grant (2008) Grant, Parisi, Parker, Subrahmanian (2010) includespecific integrity constraint that, given moving object, points reachablegiven starting point one time unit. captures scenario objects speed limitspoints reachable objects depending distance points. However,even extended SPOT framework yet general enough represent additional knowledge concerning movements objects. Examples knowledge may aware cannotrepresent SPOT framework are, instance, fact(i) cannot two distinct objects given region given time interval (as happensairport passenger screening);(ii) object cannot reach given region starting given location less givenamount time (as happens vehicles whose route options well speedlimited);(iii) object go away given region stayed least given amounttime (as happens production lines assembling several parts requires given amounttime).overcome limitation allow kind knowledge represented, define probabilistic spatio-temporal (PST) knowledge bases (KBs) consisting atomic statements,representable SPOT framework spatio-temporal denial (abbreviated std) formulas, general class formulas account three cases above, many (includingreachability constraint Parker et al., 2008; Grant et al., 2010).focus paper systematic study knowledge representation probabilisticspatio-temporal data. start defining concept PST KB provide formal semantics, given terms worlds, interpretations, models (Section 2). defineconcept consistent PST KB, characterize complexity checking consistency, showing744fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESNP-complete general, even presence binary std-formulas (Section 3).present sufficient condition checking consistency relies feasibilityset mixed-binary linear inequalities (Section 3.2), necessary condition using instead setlinear inequalities (Section 3.3). showing special case unary std-formulastractable (Section 3.4), deal restricted expressive class binary std-formulasidentify cases consistency checking problem tractable (Section 3.5). addressproblem answering selection queries PST KBs optimistic cautioussemantics (Section 4). show checking consistency exploited answer kindsqueries PST KBs, characterize complexity query answering problem (Section 4.2).that, derive several sets linear inequalities answering queries (Section 4.3). Finally,extend framework case time, space, number objects increasedarbitrarily large finite domains, show PST KB either eventually consistent eventually inconsistent (Section 5). discuss related work (Section 6). Section 7 summarizespaper. also suggest research projects Section 8.2. PST Frameworksection introduces syntax semantics PST KBs generalizing SPOT framework introduced Parker et al. (2007a) extended Parker et al. (2008) Grant et al. (2010).Basically, define PST KB augmenting previous framework non-atomic formulas(i.e., spatio-temporal denial formulas) represent integrity constraints. way makestatements whose meaning certain object trajectories cannot occur.2.1 Syntaxassume existence three types constant symbols: object symbols, time value symbols,spatial region symbols. constants ID = {id1 , . . . , idm }, = [0, 1, . . . , tmax](where tmax integer), set r Space = {p1 , . . . , pn }. r region Space.apply unique name assumption; so, instance idi idj 6= j different objects;similarly, pi pj 6= j different points. also use variables type: objectvariables, time variables, spatial variables.spatio-temporal atom (st-atom, short) expression form loc(X, Y, Z), where:(i) X object variable constant id ID,(ii) space 1 variable constant r Space,(iii) Z time variable constant .say st-atom loc(X, Y, Z) ground arguments X, Y, Z constants. instance, loc(id, r, t), id ID, r Space, ground st-atom. intuitivemeaning loc(id, r, t) object id is/was/will inside region r time t.Definition 1 (PST atom). PST atom ground st-atom loc(id, r, t) annotated probabilityinterval [`, u] [0, 1] (with ` u rational numbers), denoted loc(id, r, t)[`, u].1. write Space refer set points used PST KB. write space refer spatial aspectprobabilistic spatio-temporal knowledge.745fiPARISI & G RANT7loc(id1 , c, 9)[.9, 1]loc(id1 , a, 1)[.4, .7]loc(id1 , b, 1)[.4, .9]loc(id1 , d, 15)[.6, 1]loc(id1 , e, 18)[.7, 1]loc(id2 , b, 2)[.5, .9]loc(id2 , c, 12)[.9, 1]loc(id2 , d, 18)[.6, .9]loc(id2 , d, 20)[.2, .9]65ec432b1001234567(a)(b)Figure 1: (a) map airport area (names regions bottom-right corner);PST atoms.(b)Intuitively, PST atom loc(id, r, t)[`, u] says object id is/was/will inside region rtime probability interval [`, u]. Hence, PST atoms represent informationpast present, also information future, methods predictingdestination moving objects (Mittu & Ross, 2003; Hammel, Rogers, & Yetso, 2003; Southey, Loh,& Wilkinson, 2007), querying predictive databases (Akdere, Cetintemel, Riondato, Upfal,& Zdonik, 2011; Parisi, Sliva, & Subrahmanian, 2013).original SPOT definition, ease implementation, Space grid withinrectangular regions considered; however, general framework, Space arbitraryregion nonempty subset Space. Still, convenience use rectangularregions running example.Example 1. Consider airport security system collects data biometric sensorswell Bluetooth WiFi enabled devices. Biometric data faces recognized sensors (Li & Jain, 2011) matched given profiles (such checked-in passports,wanted criminals). Similarly, device identifiers (e.g., MAC addresses) recognized areascovered network antennas matched profiles collected airport hotspots (suchlogins, possibly associated passport numbers). simplified plan airport area reportedFigure 1(a), regions a, b, c, d, e covered sensors and/or antennas highlighted.entered area, passengers typically move path delimited queue dividers (represented dotted lines figure, overlapping regions b), reach roomupper-half right side security checks performed (region c included room).Next, passengers spend time hall room (overlapping region d), finally gotowards exit (near region e).Suppose security system uses SPOT framework represent informationevery PST atom consists profile id resulting matching phase, regionsensor/antenna recognizing profile operating, time point profile recognized,lower upper probability bounds recognizing process. instance, PST atomloc(id1 , c, 9)[.9, 1] says profile id id1 region c time 9 probability746fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESinterval [.9, 1] (the high-accuracy sensors used security check points located region c entailnarrow probability interval upper bound equal 1). Atom loc(id1 , a, 1)[.4, .7] saysid1 recognized region earlier time 1 probability [.4, .7]. Assumeinformation represented set atoms Figure 1(b), includes two atoms above.2PST atoms used represent output process aimed tracking objects basissensor measurements. Generally, sensors characterized likelihood function providingconditional probability obtaining measurement given value parameter ,distance tracked object sensor. instance, likelihood function l()represent probability detecting object distance meters sensorposition. However, likelihood function generally probability distribution viewedfunction . may l(1 ) = .9, l(2 ) = .4, l(3 ) = .1, l( 3 ) = 0, distances1 < 2 < 3 . information encoded using PST atoms loc(id, ri , t)[l(i ), l(i )]region ri determined distance (more general probability intervals usedlikelihood values know exactly).2 However, several object tracking techniques combineinformation likelihood function prior position distribution obtain probabilitydistribution Space. PST atoms represent kind information defining PST atomsingle probability point Space. 3 refer reader Related Work sectiondetailed discussion object tracking techniques relationship framework.Although PST atoms express much useful information, cannot express additional knowledge integrity constraints provide. paper add integrity constraints originalPST framework form PST KBs. integrity constraints form spatio-temporal denial formulas (std formulas short). soon see formulas expressive enoughcapture large set conditions. Basically, std formula universally quantified negationconjunctions st-atoms built-in predicates. note std formulas related subclassfirst-order formulas introduced Doder, Grant, Ognjanovic (2013), except(the std formulas) allow built-in predicates well. case, focus Doder et al.axiomatization various probabilistic spatio-temporal logics.Definition 2 (Std-formula). std-formula expression formk^loc(Xi , Yi , Zi ) (X) (Y) (Z)X, Y, Zi=1where:X set object variables, set space variables, Z set time variables;loc(Xi , Yi , Zi ), [1..k], st-atoms, Xi , Yi , Zi may variables constants appropriate type, that, Xi (resp., Yi , Zi ) variable, occursX (resp, Y, Z). Moreover, variable X, Y, Z occurs least one st-atomloc(Xi , Yi , Zi ), [1..k];2. note PST KBs resulting PST atoms encoding information provided likelihood function mayinconsistent fact likelihood function need probability distribution. turnclearer introducing formal semantics PST KBs Section 2.2.3. PST KBs resulting PST atoms encoding output tracking turn consistent.747fiPARISI & G RANT(X) conjunction built-in predicates form Xi Xj , Xi Xj eithervariables occurring X ids ID, operator {=, 6=};(Y) conjunction built-in predicates form Yi Yj , Yi Yj eithervariables occurring regions (i.e., non-empty subsets Space), comparisonoperator {=, 6=, ov, nov} (where ov stands overlaps nov standsoverlap);(Z) conjunction built-in predicates form Zi Zj Zi Zj eithertime value variable Z may followed +n n positive integeroperator {=, 6=, <, }.Example 2. running example, region c security checks one individual timeperformed. constraint cannot two distinct objects region c time 120 expressed following std-formula:f1 = X1 , X2 , Z1 [loc(X1 , c, Z1 ) loc(X2 , c, Z1 ) X1 6= X2 Z1 1 20 Z1 ].Due distance several obstacles entrance exit, alsoconstraint object reach region e starting region less 10 time units,expressed as:f2 = X1 , Z1 , Z2 [loc(X1 , a, Z1 ) loc(X1 , e, Z2 ) Z1 < Z2 Z2 < Z1 + 10].Moreover, security check individual takes least 2 time units, knowobject id go away region c stayed least 2 time units,expressed as:f3 = Y1 , Y2 , Z1 , Z2 , Z3 [loc(id, Y1 , Z1 ) loc(id, c, Z2 ) loc(id, Y2 , Z3 ) Y1 nov c Y2 nov cZ2 = Z1 + 1 Z2 < Z3 Z2 + 2 Z3 ].2work later useful distinguish std-formulas based number (k) statoms them. particular, unary std-formulas k = 1 binary std-formulas k = 2.Example 2 f1 f2 binary std-formulas f3 ternary std-formula.initial SPOT framework (Parker et al., 2007a) PST atoms considered. Moreover, assumed points Space reachable points objects.overcome limitation, Grant et al. (2010) extended SPOT framework introducing reachability definitions. reachability atom written reachableid (p, q) id ID objectid, p, q Space. Intuitively, reachability atom says possible object idreach location q location p one unit time. Hence, reachable one time unitdepends locations p q, also object id. show, reachabilityexpressed formalism integrity constraint. However, order formulate reachabilityframework denial formulas, need deal reachable, ratherreachable.Example 3. Let r region consisting points q reachable p one timeunit. corresponding std-formula is:X1 , Z1 , Z2 [loc(X1 , {p}, Z1 ) loc(X1 , r, Z2 ) Z2 = Z1 + 1].2integrity constraint used points reachable p one time unit.also express points reached p number time units, 1,changing Z1 + 1 Z1 + i.748fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESExample 4. running example, following std-formula states points regionr = {(x, y)|0 x 5 = 3} (i.e., close upper-side wall dividing hallroom one queue dividers) reachable less 3 time unitspoint r0 = {(x, y)|0 x 5 = 2} (i.e., points close side wall):f4 = X1 , Z1 , Z2 [loc(X1 , r0 , Z1 ) loc(X1 , r, Z2 ) Z1 < Z2 Z2 < Z1 + 3].2ready formally define PST KBs.Definition 3 (PST KB). Given sets ID, , Space, PST KB K pair hA, Fi,finite set PST atoms F finite set std-formulas using object symbols ID, timevalues , spatial regions consisting sets points Space.Example 5. running example, ID = {id1 , id2 }, = [0, 20], Space set points (x, y)0 x 7 0 7, PST KB Kex pair hAex , Fex i, Aexset consisting PST atoms Figure 1(b), Fex set {f1 , f2 , f3 , f4 } std-formulasdefined Examples 2 4.22.2 Semanticssemantics PST KB defined concept worlds. introducingconcept, define ground std-formulas.Given std-formula f form Definition 2, denote f set substitutions variables X, Y, Z constants ID, S, , respectively,set subsets Space contain single point. 4 Moreover, given substitutionf , denote (f ) ground std-formula resulting applying f : (f ) =Vki=1 loc((Xi ), (Yi ), (Zi )) ((X)) ((Y)) ((Z)) . ground conjunctionbuilt-in predicates ((X)) ((Y)) ((Z)) evaluates either true false. trueomit it. (f ) either negation conjunction ground st-atoms truth value true(when conjunction built-in predicates evaluates false).Example 6. Consider formula f1 = X1 , X2 , Z1 [loc(X1 , c, Z1 )loc(X2 , c, Z1 )X1 6= X2Z1 1 20 Z1 ] introduced Example 2, substitution = {X1 /id1 , X2 /id2 , Z1 /6},id1 , id2 ID 6 tmax. Thus, (f1 ) = [loc(id1 , c, 6) loc(id2 , c, 6)],conjunction ground built-in predicates id1 6= id2 6 1 6 20, evaluating true,reported (f1 ).2Definition 4 (World). world w function, w : ID Space.Basically, world w specifies trajectory id ID. is, id ID, w saysSpace object id was/is/will time . particular, means objectone location time.5 However, location may contain multiple objects. easysee world w represented set {loc(id, {p}, t)| w(id, t) = p} ground st-atoms.4. use singleton subsets Space order reduce number possible instantiations variablesexponential linear size Space, without serious effect meanings std-formulas.5. examples may useful allow objects enter leave space consideration.accomplished, instance, one external points outside space objects may located.simplify matters assume Space contains points.749fiPARISI & G RANTExample 7. World w1 describing trajectories id1 id2 time units [0, 20]w1 (id1 , t) = (4, 1) [0, 5], w1 (id1 , t) = (7, 2) [6, 7], w1 (id1 , t) = (7, 4)[8, 10], w1 (id1 , t) = (4, 4) [11, 16], w1 (id1 , t) = (1, 6) [17, 20], w1 (id2 , t) =(4, 1) [0, 11], w1 (id2 , t) = (7, 5) [12, 15], w1 (id2 , t) = (7, 7) [16, 16],w1 (id2 , t) = (4, 5) [17, 20].2Definition 5 (Satisfaction). Given world w ground st-atom = loc(id, r, t), say wsatisfies (denotedr. Moreover, say w satisfies conjunctionVk w |= a) iff w(id, t) Vground st-atoms i=1 ai (denoted w |= ki=1 ai ) iff w |= ai [1..k]. Finally, world w satisfiesstd-formula f (denoted w |= f ) iff substitution f , w |= (f ).Note that, negation front f , w |= (f ) iff w satisfy ground st-atom(f ) conjunction ground built-in predicates (f ) evaluates false.Example 8. World w1 Example 7 satisfies st-atom loc(id1 , b, 0), w1 (id1 , 0) = (4, 1)belongs region b (see Figure 1(a)). Moreover, w1 |= [loc(id1 , b, 0) loc(id1 , e, 15)] w1 6|=loc(id1 , e, 15), since w1 (id1 , 15) = (4, 4) 6 e.2following, denote W(K) set worlds PST KB K. Moreover,order simplify formulas, assume w ranges W(K).interpretation PST KB K probability distribution function (PDF) W(K),is, function assigning probability value world W(K). I(w) probabilityw describes actual trajectories objects.6 interpretations models Kcase write instead I.Definition 6 (Model). model PST KB K = hA, Fi interpretation K that:P(w) [`, u];loc(id, r, t)[`, u] A,w | w|=loc(id,r,t)f F,P(w) = 0.w | w6|=ffirst condition definition means that, atom = loc(id, r, t)[`, u] A,sum probabilities assigned worlds satisfying st-atom loc(id, r, t)belong probability interval [`, u] specified a. second condition means every worldsatisfying formula f F must assigned probability equal 0.Example 9. Let w1 world introduced Example 7. Let w2 w1 except w2 (id1 , 1) =(3, 2), let w3 w2 except w3 (id2 , 2) = (2, 2), w3 (id2 , t) = (0, 3) [18..20]. Let(w1 ) = .7 (w2 ) = .2 (w3 ) = .1, (w) = 0 worldsW(Kex ). checked satisfies conditions DefinitionP6 PST KB Kexrunning example. instance, atom loc(id1 , a, 1)[.4, .7] Aex , w|w|=loc(id1 ,a,1) (w) =(w1 ) = .7 [.4, .7] (note that, time 1, w2 (id1 , 1) = w3 (id1 , 1) = (3, 2) region a).Moreover, easy check w1 , w2 , w3 satisfy every std-formula Fex . Thus, modelKex .2say PST KB K consistent iff model it. set models Kdenoted M(K).6. PDF, I(w) non-negative sums 1 worlds.750fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESDefinition 7 (Consistency). PST KB K consistent iff M(K) 6= .Example 10. PST KB Kex running example consistent, exists modelExample 9 it.23. Checking Consistency PST KBssection, address fundamental problem checking consistency PST KBs.Given PST KB K = hA, Fi, consistency checking problem deciding whether M(K) 6= ,is, whether model K.Section 3.1 show consistency checking problem NP-complete. goalrest section find efficient ways determine consistency. Section 3.2 findsufficient condition using set mixed-binary linear inequalities. Then, Section 3.3 findnecessary condition using different set linear inequalities. deal special casestd-formulas unary Section 3.4. Finally investigate detail casestd-formulas either unary binary Section 3.5.complexity analysis take size PST KB K = hA, Fi, whose PST atomsstd-formulas built constants ID, , Space, number PST atomsstd-formulas K plus number items ID, , Space, is, |K| = |A| + |F| + |ID| +|T | + |Space|.3.1 Checking Consistency NP-Completeconsidering case general PST KBs, first note consistency checking problem addressed initial SPOT framework Parker et al. (2007a) PST atomsconsidered. special case PST KB concept F = . shownconsistency PST KB K = hA, (using notation) checked polynomialtime w.r.t. size K solving set linear inequalities whose variables vid,t,p representprobability object id point p time t.reason presenting result Parker et al. twofold: first, compare complexityconsistency checking problem general PST KBs initial SPOT framework;second, use prove tractability results PST KBs.Fact 1 (Parker et al.). Let K = hA, PST KB (where set std-formulas empty).K consistent iff feasible solution CC(K), CC(K) consists following(in)equalities:P(1) loc(id, r, t)[`, u] A: `vid,t,p u;pr(2) id ID, :Pvid,t,p = 1;pSpace(3) id ID, T, p Space: vid,t,p 0.Basically, inequalities (1) ensure solution CC(K) places object r probability ` u, required atom (id, r, t, [`, u]). Inequalities (2) (3) ensureid t, vid,t,p variables jointly represent probability distribution. Fact 1 correctevery model K corresponds solution CC(K) sum751fiPARISI & G RANTprobabilities assigned worlds K satisfying st-atom loc(id, {p}, t) equalvalue assigned variable vid,t,p .state first result: consistency checking problem NP-complete.Theorem 1. Given PST KB K = hA, Fi, deciding whether K consistent NP-complete.Proof. (Membership). show checking consistency K reduced decidinginstance K (an extension to) Probabilistic Satisfiability (PSAT) problem (Hailperin, 1984;Nilsson, 1986), NP (Georgakopoulos, Kavvadias, & Papadimitriou, 1988). Given setclauses C1 , . . . , Cm , consisting disjunction one literals constructedpropositional variables x1 , . . . , xn , probability values Pr(C1 ), . . . , Pr(Cm )clause, PSAT problem deciding whether probability distribution set2n truth assignments propositional variables x1 , . . . , xn clause Ci ,sum probabilities assigned truth assignments satisfying Ci equal Pr(Ci ),[1..m]. PSAT generalization SAT, obtained PSAT assigning probabilityequal one clause. Georgakopoulos et al. formulated PSAT terms feasibilitysystem + 1 linear equations using 2n variables corresponding probabilities assignedtruth assignments. show existence polynomial-size witness, following resultlinear programming theory exploited Georgakopoulos et al.: system linearequalities feasible solution, admits least one feasible solution nonzero variables (Papadimitriou & Steiglitz, 1982). follows consider extensionPSAT clause Ci associated probability interval [Pr` (Ci ), Pru (Ci )], insteadsingle value. membership NP extension straightforwardly followsmembership proof provided Georgakopoulos et al. PSAT, using probability intervalsstill formulated linear system introduced Georgakopoulos et al. reducing doublesided inequalities equalities single bounded slack variables (Jaumard, Hansen, & de Aragao,1991).Given PST KB K = hA, Fi, define instance K PSAT clause associated probability interval. Let U set propositional variables xid,p,tid ID, p Space, (i.e., st-atom loc(id, {p}, t) corresponds propositionalvariable xid,p,t U ). conjunction K clauses associated probability intervals definedfollows:WPST atom = loc(id, r, t)[`, u] A, K consists clause Ca =xid,p,t .prprobability interval [Pr` (Ca ), Pru (Ca )] equal [`, u].f F f (f ) = [kVloc((Xi ), (Yi ), (Zi ))], K consistsi=1clause C(f ) =kWi=1x(Xi ),(Yi ),(Zi ) , whose probability interval [1, 1].id ID , K consists clause Cid,t =Wxid,p,t , whosepSpaceprobability interval [1, 1].id ID, , pi , pj Space, pi 6= pj , K consists clauseCid,t,pi ,pj = xid,pi ,t xid,pj ,t whose probability interval [1, 1].752fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESeasy see world w W(K), truth assignment w variablesU w (xid,p,t ) true iff w(id, t) = p. However, truth assignmentscorrespond world W(K) (for instance, (xid,pi ,t ) (xid,pj ,t ),pi 6= pj , true). show K consistent iff K satisfiable.() Given model K, show PDF set truth assignmentsclause C K , sum probabilities assigned truth assignmentssatisfying C belongs [Pr` (C), Pru (C)]. Let truth assignment w corresponding world w W(K), (w ) = (w) ( ) = 0 truth assignmentscorresponding world. easy check conditions Definition 6 entail clausesform Ca C(f ) satisfied , clauses form Cid,t Cid,t,pi ,pj satisfiedwell since w1 , . . . w|W(K)| may assigned probability different 0every world w W(K) definition assigns exactly one point Space id, pair.() Let PDF set truth assignments clause C K , sumprobabilities assigned truth assignments satisfying C belongs [Pr` (C), Pru (C)].model K defined (w) = (w ) truth assignment corresponding world w W(K). Since clauses form Cid,t (resp., Cid,t,pi ,pj ) satisfied, truth assignments (xid,p,t )=false p Space (resp., (xid,pi ,t )=true(xid,pj ,t )=true, pi , pj Space, pi 6= pj ) assigned probability 0. Hence,truth assignments correspond world, fact clauses form Ca C(f )satisfied entails conditions Definition 6 hold.(Hardness). show reduction problem NP-hard Hamiltonian path problem (Papadimitriou, 1994), is, problem checking whether path directed graph Gvisits vertex G exactly once.Given directed graph G = hV, Ei, V = {v0 , . . . , vk } set vertices, Eset pairs (vi , vj ) vi , vj V , construct instance problem follows. LetID = {id}, Space = V , = [0, . . . , k]. K pair hA, Fi consists PSTatom loc(id, {v0 }, 0)[1, 1] F consists std-formulas f1i (with [0..k]) f2 that:f1i = Z1 , Z2 [loc(id, {vi }, Z1 ) loc(id, Space\V 0 , Z2 ) Z2 = Z1 + 1] V 0set vertices vj s.t. (vi , vj ) E. formula says points id reach startingvi one time step V 0 . (f1i exist V 0 = Space.)f2 = Y1 , Z1 , Z2 [loc(id, Y1 , Z1 ) loc(id, Y1 , Z2 ) Z1 6= Z2 ], saying idlocation distinct time values.show K consistent iff Hamiltonian path G.() one id A, every world w W(K) w places id vertexV time value . K consistent, model M(K)assigns probability greater zero worlds w f F, w |= f . particular,let w one world. fact w |= f1i entails [0, k 1], w(id, t) = viw(id, + 1) = vj iff (vi , vj ) E. Moreover, fact w |= f2 entails t, t0 [0, k], 6= t0 ,w(id, t) 6= w(id, t0 ), meaning id never placed w vertex different timeunits. Since loc(id, v0 , 0)[1, 1] A, every world assigned probability greater zerow(id, 0) = v0 . follows every world w W(K) assignedM(K) probability greater zero encodes Hamiltonian path G whose first vertexv0 . fact, w W(K) (w) > 0 following properties hold: (i) w(id, 0) = v0 ,753fiPARISI & G RANT(ii) [0, k 1], w(id, t) = vi , w(id, + 1) = vj iff (vi , vj ) E. (iii) t, t0 [0, k], 6= t0 ,w(id, t) 6= w(id, t0 ). Conditions (i) (ii) entail = w(id, 0), w(id, 1), . . . , w(id, k)path G starting vertex v0 , condition (iii) entails vertex v V occurs exactly.() Let Hamiltonian path G. denote [i] (with [0..k]) i-th vertex .W.l.o.g. assume first vertex v0 , is, [0] = v0 . show Kconsistent finding model it. Let function W worlds w W,(w) = 0, except world w that: w (id, 0) = [0] = v0 , [1, k],w (id, t) = [t]. easy see w |= F. fact, [0..k], f1i satisfied w , sincefact path G entails [0, k 1], w (id, t) = vi w (id, t+1) = vjedge (vi , vj ) edge G. Moreover, f2 satisfied w , since fact Hamiltonianpath entails w places id different locations (i.e., vertices G) different times. SincewaPprobability different 0. Let (w ) = 1. Therefore,P |= F, assignedw|w|=loc(id,v0 ,0) (w) = (w )+ w|w6=w w|=loc(id,v0 ,0) (w) = 1, condition requiredatom loc(id, v0 , 0)[1, 1] holds too. Thus, model K.note NP-hardness holds already binary std-formulas. Section 3.5 findconditions make consistency checking problem tractable binary std-formulas. reduction shown membership proof, consistency checking problem PSAT, wouldallow us define additional tractable cases PSAT instances resulting reductiontractable. However, discussed Section 6, tractable cases identified PSAT (Georgakopoulos et al., 1988; Andersen & Pretolani, 2001) carryframework.3.2 Sufficient Condition Checking Consistencypresent set mixed-binary linear inequalities whose feasibility entails consistency PSTKB K = hA, Fi. explained Section 3.1, Parker et al. (2007a) showed consistencyPST KB K = hA, (using notation) checked polynomial time w.r.t. sizeK solving set linear inequalities whose variables vid,t,p represent probability objectid point p time t. Here, start set linear inequalities augmentinequalities ensuring so-obtained set linear inequalities feasible solutionevery ground std-formula derived F satisfied. achieve this, need introducebinary variables , thus obtaining set mixed-binary linear inequalities.Definition 8 (MBL(K)). Let K = hA, Fi. MBL(K) consists following (in)equalities:P(1) loc(id, r, t)[`, u] A: `vid,t,p u;pr(2) id ID, :Pvid,t,p = 1;pSpace(3) id ID, T, p Space: vid,t,p 0;(4) f F f (f ) = [kVi=1number st-atoms f , (in)equalities:754loc((Xi ), (Yi ), (Zi ))], kfiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESP(a) [1..k] :p(Yi ) v(Xi ),(Zi ),p ;Pk(b)i=1 = k 1;(c) [1..k] : {0, 1}.Basically, inequalities (1) ensure solution MBL(K) places object r probability ` u, required atom (id, r, t, [`, u]). Inequalities (2) (3) ensureid t, vid,t,p variables jointly represent probability distribution. Moreover,ground st-atom loc((Xi ), (Yi ), (Zi )) ground std-formula (f ), inequalities (4)(a)(4)(c) entail probability v(Xi ),(Zi ),p object (Xi ) point p region (Yi )time (Zi ) either constrained 0 free take value greater 1. Intuitivelyenough, v(Xi ),(Zi ),p enforced zero (i.e., = 0), object (Xi ) region(Yi ) time (Zi ). hand, v(Xi ),(Zi ),p left free take value lessequal one (i.e., = 1), (Xi ) may may region (Yi ) time (Zi ). Finally,equality (4)(b) entails least one k ground st-atoms loc((Xi ), (Yi ), (Zi ))(f ) (Xi ) placed point (Yi ) time (Zi ).Example 11. Consider ground std-formula (f1 ) = [loc(id1 , c, 6) loc(id2 , c, 6)] Example 6. Then, inequalities MBL(K) corresponding (f1 ) are:PP(4b) 1 + 2 = 1;(4c) 1 , 2 {0, 1}.2(4a) pc vid1 ,6,p 1 ;pc vid2 ,6,p 2 ;following theorem states MBL(K) used check K consistent.Theorem 2. feasible solution MBL(K) K consistent.Proof. Let solution MBL(K), (vid,t,p ) value assigned variable vid,t,p.Q define function W(K) that, world w W(K), (w) =idID,tT,w(id,t)=p (vid,t,p ), (w) product values assigned solutionvariables vid,t,p w(id, t) = p. shown that, (in)equalities (2) (3)definitionMBL(K) entail PDF W(K). Moreover,since (vid,t,p )PP(w),atomloc(id,r,t)[`,u]A,equalw|w|=loc(id,r,t) (w) =P P w|w|=loc(id,t,p)Pprw|w|=loc(id,t,p) (w) =pr (vid,t,p ) [`, u]. Given f F f (f )Vklogically equivalent negation conjunction st-atomsi=1 loc((Xi ), (Yi ),P(Zi )), inequalities (4)(a-c) entail [1..k] p(Yi ) (v(Xi ),(Zi ),p ) =0. Thus p (Yi ), (v(Xi ),(Zi ),p ) = 0. Hence, world w W(K) w((Xi ),(Zi )) = p, (w) = 0 due presence factorP (v(Xi ),(Zi ),p ) = 0 product defining (w). Therefore, std-formula f F, w | w6|=f (w) = 0; hence modelK K consistent.consequence Theorem 2 well-known techniques solving linear optimizationproblems adopted address consistency checking problem, thus taking advantageresults 50 years research integer linear programming (Junger et al., 2010).following example shows converse Theorem 2 hold (K may consistent even feasible solution MBL(K)).Example 12. Let ID = {id}, = [0, 1], Space = {p0 , p1 }, K = hA, Fi where:= {loc(id, {p0 }, 0)[0.5, 0.5], loc(id, {p1 }, 1)[0.5, 0.5]}F = {[loc(id, {p0 }, 0) loc(id, {p1 }, 1)}.755fiPARISI & G RANTThus, W = {w1 , w2 , w3 , w4 } where: w1 (id, 0) = p0 , w1 (id, 1) = p0 , w2 (id, 0) = p0 , w2 (id, 1) =p1 , w3 (id, 0) = p1 , w3 (id, 1) = p0 , w4 (id, 0) = p1 , w4 (id, 1) = p1 .easy check (w1 ) = 0.5, (w2 ) = 0, (w3 ) = 0, (w4 ) = 0.5model K. MBL(K) includes following inequalities:0.5 vid,0,p0 0.5; 0.5 vid,1,p1 0.5; vid,0,p0 + vid,0,p1 = 1; vid,1,p0 + vid,1,p1 = 1;vid,0,p0 1 ; vid,1,p1 2 ; 1 + 2 = 1; 1 , 2 {0, 1};vid,0,p0 0, vid,0,p1 0, vid,1,p0 0, vid,1,p1 0.first two inequalities force vid,0,p0 vid,1,p1 0.5. second line 12 must equal 1. contradicts 1 + 2 = 1. Hence MBL(K) feasible solution.23.3 Necessary Condition Checking Consistencyfollowing, given PST KB K, introduce set NC(K) linear inequalities Kconsistent feasible solution NC(K). is, existence feasible solutionNC(K) turns necessary condition consistency K.MBL(K) (see Definition 8), NC(K) uses rational variables vid,t,p representing probabilityobject id point p time t. kinds variables used definitionNC(K), case obtain pure system linear inequalities.Definition 9 (NC(K)). Let K = hA, Fi PST KB. NC(K) consists following (in)equalities:Pvid,t,p u;(1) loc(id, r, t)[`, u] A: `pr(2) id ID, :Pvid,t,p = 1;pSpace(3) id ID, T, p Space: vid,t,p 0;V(4) f F (with k conjuncts) f s.t. (f ) = [ ki=1 loc((Xi ), (Yi ), (Zi ))],inequalities:Pkp1 (Y1 ), p2 (Y2 ), . . . , pk (Yk ),i=1 v(Xi ),(Zi ),pi k 1.Herein, (in)equalities (1)-(3) MBL(K) Definition 8,meaning. addition,V NC(K) contains inequalities (4) impose that, ground stdformula form ki=1 loc(idi , ri , ti ) k-tuple points hp1 , p2 , . . . , pk belongingrespectively regions hr1 , r2 , . . . , rk i, sum probabilities vidi ,pi ,ti object idipoint pi time ti , [1..k], greater k 1. stated following theorem,set consisting inequalities (4) along inequalities (1)-(3) turns feasible solutioncorresponding PST KB inconsistent.Theorem 3. feasible solution NC(K), K consistent.Proof. Suppose NC(K) feasible solution. due fact feasiblesolution CC(hA, (introduced Fact 1), PST KB hA, consistent, thus K =hA, Fi consistent well (since set M(hA, Fi) models K = hA, Fi subsetset M(hA, i) models K0 = hA, i).Otherwise, feasible solution CC(hA, i) thus NC(K) feasible solutiondue fact least one inequalities item (4) Definition 9 satisfied every756fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESsolutionV CC(hA, i). is, solution CC(hA, i), ground std- formula(f ) = ki=1 loc((Xi ), (Yi ), (Zi )) exist p1 (Y1 ), p2 (Y2 ), . . . , pkP(Yk ), ki=1 (v(Xi ),(Zi ),pi ) > k 1.Since every model 0 K0 = hA, corresponds solution CC(hA, i)sum probabilities assigned0 worlds satisfying st-atom loc(id, {p}, t) equalPk(vid,t,p ), fact i=1 (v(Xi ),(Zi ),pi ) > k 1 holds entails that, model 0K0 , sum probabilities assigned 0 worlds satisfying least one st-atoms(f ) greater k 1, is,0 M(K0 ),kXX0 (w) > k 1(1)i=1 w|=loc((Xi ),(Yi ),(Zi ))recall use following well-known result minimum probability conjunction probabilistic events among correlation known. Given n probabilistic eventse1 , . . . , en whose (marginal)probabilities Pr(e1 ), . . . , Pr(en ) respectively, Pr(e1 en )Pmax (0, 1 n + ni=1 Pr(ei )). one Frechet inequalities (the one providesupper bound maximum probability) implicitly reported already Booles work (1854).setting, viewing st-atoms probabilistic events, Frechet inequality entailsmodel 0 K0 , probability set st-atoms together satisfy world greaterequal maximum zero 1 |S| plus sum probabilitiesst-atom according 0 . is,XW(K0 )wS, w |=X0 (w) max 0, 1 |S| +XW(K0 ),ww |=0 (w)(2)Equation (1) entails model 0 K0 , term right-hand side Equation (2) evaluates value greater zero set st-atoms (f ) considered.Therefore, model 0 K0 , sum probabilities worlds satisfyingst-atoms (f ) greater zero. Since set M(hA, Fi) models K = hA, Fisubset set M(hA, i) models K0 = hA, i, property also holds modelK0 . Thus, 0 model K. Hence K inconsistent.example usage Theorem 3 given below, checking NC(K)feasible solution conclude PST KB K consistent.Example 13. Let ID = {id}, = [0, 1, 2], Space = {p0 , p1 , p2 }. Let K = hA, Ficonsists PST atoms loc(id, {p0 }, 0)[0.4, 1], loc(id, {p1 }, 0)[0.5, 1], loc(id, {p0 }, 1)[0.8, 1], loc(id, {p0 }, 2) [0.8, 1], meaning that, id p0 p1 time 0 probability greater equal 0.4 0.5, respectively, p0 times 1 2probability greater equal 0.8.F consists std-formula: [loc(id, {p0 , p1 }, 0) loc(id, {p0 }, 1) loc(id, {p0 }, 2)],saying id cannot p0 times 1 2 region consistingpoints p0 p1 time point 0.757fiPARISI & G RANTeasy see NC(K) contains, among others, following inequalities:0.4 vid,0,p0 10.5 vid,0,p1 10.8 vid,1,p0 10.8 vid,2,p0 1vid,0,p0 + vid,1,p0 + vid,2,p0 2vid,0,p1 + vid,1,p0 + vid,2,p0 2last two inequalities derive item (4) Definition 9. Clearly, inequalitiescannot satisfied given 0.5 vid,0,p1 , 0.8 vid,1,p0 , 0.8 vid,2,p0 . Thus, since NC(K)feasible solution, conclude K inconsistent.2However, cannot say anything consistency K feasible solutionNC(K). following examples show case NC(K) feasible solution Kconsistent, case NC(K) feasible solution K inconsistent.Example 14. Consider PST KB Example 13 modified probabilityid p0 times 1 2 greater equal 0.5, instead 0.8. easy seefeasible solution NC(K) case. Theorem 3 cannot used decide whether Kconsistent not. matter fact, K consistent shown follows. Let w1 , w2 w3worlds Kw1 (id, 0) = p1 , w1 (id, 1) = p1 , w1 (id, 2) = p0 ,w2 (id, 0) = p0 , w2 (id, 1) = p0 , w2 (id, 2) = p1 ,w3 (id, 0) = p2 , w3 (id, 1) = p0 , w3 (id, 2) = p2 ,let PDF W(K) (w1 ) = 0.5, (w2 ) = 0.4, (w3 ) = 0.1,(w) = 0 world w W(K). straightforward check modelK.2Example 15. Again, let ID = {id}, = [0, 1, 2], Space = {p0 , p1 , p2 }. Let K = hA, Fi= {loc(id, {p0 }, 0)[0.5, 1], loc(id, {p0 }, 1)[0.5, 1], loc(id, {p0 }, 2)[0.5, 1]}, i.e., idp0 time probability greater 0.5, F consists std-formulas:[loc(id, {p0 }, 0) loc(id, {p0 }, 1)],[loc(id, {p0 }, 1) loc(id, {p0 }, 2)],[loc(id, {p0 }, 0) loc(id, {p0 }, 2)],saying id cannot p0 time 1 2 already previous time value.solution NC(K) obtained assigning variables vid,t,p0 vid,t,p1 value 0.5(where [0, 2]), variables value 0. NC(K) feasible solution, Theorem 3says nothing fact K consistent not. However, checked Kconsistent. Let Pt set worlds W(K) placing id p0 time t, .std-formulas F entail every world belonging two three sets must assignedprobability equal 0 model K. is, F entails P0 , P1 , P2 pairwise disjointsets consider worlds assigned probability greater zero model.observe PST atoms require sum probabilities worldsthree sets least equal 0.5. Therefore, overall sum probabilities assignedworlds three sets would greater equal 1.5, entailsmodel K.2Theorem 3 shows consistency K implies existence feasible solutionNC(K) previous examples show relationship two758fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESconcepts. However, make stronger statement special case. PST KBs Fconsists single ground std-formula (of arity) constructed st-atoms referring singlepoint regions, NC(K) feasible solution iff KB consistent.Theorem 4. Let K = hA, Fi F = { loc(id1 , {p1 }, t1 ) loc(idn , {pn }, tn )]}. Then,K consistent iff feasible solution NC(K).Proof. Using Theorem 3 need prove feasible solution NC(K) Kconsistent.Let f std-formula F. first observe f contains pair st-atoms referringid time value different points Space, K consistent. Indeed, case,f satisfied every world, thus impose restriction interpretationsK (see Definition 4). Given this, following w.l.o.g. assume every distinct pairst-atoms loc(idi , {pi }, ti ), loc(idj , {pj }, tj ) f , idi = idj ti 6= tj .Let solution NC(K), K0 = hA, i. Then, solution CC(K0 ) (from Fact1) corresponds model 0 K0 (i) sum probabilities assigned 0worlds satisfying st-atomloc(id, {p}, t) (i.e., marginal probability loc(id, {p}, t))Pequal (vid,t,p ), (ii) ki=1 (vidi ,ti ,pi ) k 1. Viewing st-atom loc(idi , {pi }, ti ) fprobabilistic event whose (marginal) probability (vidi ,ti ,pi ), Frechet inequality (recalledproof Theorem 3) entails thatPthe minimum probability st-atoms f occurtogether equal max(0, 1 k + ki=1 (vidi ,ti ,pi )). equal zero since secondargument function max greater zero due fact (ii) holds. factminimum probability st-atoms f simultaneously occur equal zero suffices ensureleast one model 00 K0 00 assigns probability equal zeroworlds K0 satisfy f . f std-formula F, every world satisfyingf assigned probability equal zero 00 , follows 00 model K too.following example shows considering even binary std-formula containing st-atomreferring region consisting two points, may happen feasible solutionNC(K) even K consistent.Example 16. Let ID = {id}, = [0, 1], Space = {p0 , p1 , p2 }. Let K = hA, Fi= {loc(id, {p0 }, 0)[0.4, 1], loc(id, {p1 }, 0)[0.4, 1], loc(id, {p0 }, 1)[0.4, 1], }, F stdformula [loc(id, {p0 , p1 }, 0) loc(id, {p2 }, 1)] saying id cannot move point p2 time 1either p0 p1 time 0. easy see NC(K) feasible K consistent. 2Section 3.5, present method deciding polynomial time consistencyPST KBs binary std-formulas satisfying acyclicity conditions used. turnsconsistency PST KBs Examples 15 16 decided polynomialtime using approach.3.4 Unary Std-Formulas Tractablestart identifying tractable case consistency checking problem: std-formulasunary, is, formula F consists one st-atom possibly conjunctionbuilt-in predicates (i.e., Definition 2, k = 1).759fiPARISI & G RANTExample 17. constraint object region r time 5 10expressed following unary std-formula: X1 , Z1 [loc(X1 , r, Z1 ) Z1 5 10 Z1 ].constraint object id always region r expressed as:Y1 , Z1 [loc(id, Y1 , Z1 ) Y1 nov r]. 72following theorem states checking consistency tractable unary std-formulasconsidered.Theorem 5. Let K = hA, Fi PST KB F consists unary std-formulas only. Then,deciding whether K consistent P IM E.Proof. statement follows fact F consists unary std-formulas only, K =hA, Fi equivalent (i.e., exactly set models as) K0 = hA0 , i, A0consists atoms plus atom loc((Xi ), (Yi ), (Zi ))[0, 0]ground std-formula(f ) = [loc((Xi ), (Yi ), (Zi ))], f F f . Since, f F f polynomialw.r.t. size K, size A0 (and thus K0 ) increases polynomial number atoms.Hence, apply Fact 1, entails consistency PST KBs F =decided P IM E.3.5 Tractable Binary Std-Formulasfollowing focus PST KBs std-formulas binary. restrictedexpressive class std-formulas allow us impose several practical constraints manyapplication contexts. matter fact, f1 f2 Example 2 well f4 Example 4binary std-formulas. Furthermore, using approach suggested proof Theorem 5,assume unary std-formulas encoded PST atoms. Thus, results statedsection straightforwardly apply case unary binary std-formulas PSTKB.start noting consistency checking proved feasible Grant et al. (2010)case reachability definitions (but integrity constraints) allowed.showed Example 3 reachability definition expressed binary std-formula. Hence,special case binary std-formulas represent reachability definitions, consistencychecking tractable.general case PST KB K = hA, Fi F consists binary std-formulas,define undirected graph, called std-graph, maximal independent set representsworld K satisfying std-formulas F. 8 later use graph characterize binarystd-formulas consistency checking problem turns tractable.Definition 10 (std-graph). Given PST KB K = hA, Fi F consists binary stdformulas, std-graph G(K) undirected graph hV, Ei whose sets vertices V edgesE that:1) V consists set ground st-atoms form loc(id, {p}, t) id ID, pSpace, ;7. Recall every substitution Y1 must region containing single point.8. maximal independent set std-graph maximal independent set undirected graph whose verticesst-atoms. formal relationship maximal independent sets std-graph worlds PST KBgiven Proposition 1.760fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES2) E consistsi) edge every pair ground st-atoms V referring objecttime value, is, p1 , p2 Space, p1 6= p2 , id ID, , hloc(id, {p1 }, t),loc(id, {p2 }, t)i E;ii) edge hloc(id1 , {p1 }, t1 ), loc(id2 , {p2 }, t2 )i every pair ground st-atomsV loc(id1 , r1 , t1 ) loc(id2 , r2 , t2 )] p1 r1 p2 r2 belongsset ground std-formulas derived F.write G instead G(K) K known.Basically, edge G connects pair st-atoms cannot belong together worldsatisfying std-formulas F. particular, edge type i) connects two st-atoms representingfact object two places time admitted accordingdefinition world (see Definition 4). edge type ii) connects two st-atoms representing fact(i.e., object id1 point p1 time t1 object id2 point p2 time t2 ) consistentground std-formula entailed F.structure G follows. hid, ti pair, G contains clique 9 size |Space|consists vertex loc(id, {p}, t) point Space edges type i)following, referclique clique hid, ti pair hid, ti clique. groundstd-formula f = loc(id1 , r1 , t1 ) loc(id2 , r2 , t2 )] derived F, G containsclique size |r1 | + |r2 | consists vertex loc(id1 , {p1 }, t1 ) point r1 , vertexloc(id2 , {p2 }, t2 ) point r2 , edges types i) ii) refer cliqueclique std-formula f .Example 18. Let ID = {id1 , id2 }, = [0, 1, 2], Space = {p1 , p2 , p3 , p4 }. Assume Fconsists following std-formulas:f1 = X1 , X2 , Z1 [loc(X1 , {p2 , p4 }, Z1 ) loc(X2 , {p2 , p4 }, Z1 ) X1 6= X2 0 Z1Z1 1], saying cannot two distinct objects region consisting points{p2 , p4 } times 0 1;f2 = Z1 [loc(id1 , {p3 , p4 }, 0) loc(id1 , {p1 }, Z1 ) 1 Z1 Z1 2], saying objectid1 cannot reach point p1 starting region {p3 , p4 } time 0 1 2 time units.std-graph G shown Figure 2(a), where, sake readability, vertices labellednames points Space refer, id time value vertexreported column row belongs (for instance, vertex loc(id1 , {p1 }, 0)represented circle upper-left corner). Observe G consists 10 (maximal) cliques,one 6 hid, ti pairs, one 4 ground std-formulas derived F.Specifically, hid, ti-clique, id {id1 , id2 } [1..2], consists four verticesloc(id, {pk }, t) k [1..4], cliques ground std-formulas following sets:{loc(id1 , {p2 }, 0), loc(id2 , {p2 }, 0), loc(id1 , {p4 }, 0), loc(id2 , {p4 }, 0)},{loc(id1 , {p2 }, 1), loc(id2 , {p2 }, 1), loc(id1 , {p4 }, 1), loc(id2 , {p4 }, 1)},{loc(id1 , {p3 }, 0), loc(id1 , {p4 }, 0), loc(id1 , {p1 }, 1)},{loc(id1 , {p3 }, 0), loc(id1 , {p4 }, 0), loc(id1 , {p1 }, 2)}.2761fiPARISI & G RANTid1id2p1p2p2p1p3p4p4p3p1p2p2p1p3p4p4p3id1,0id2,0p1p2p2p1id1,1id2,1p3p4p4p3id1,2id2,2t=0t=1t=2(a)(b)Figure 2: (a) Std-graph G; (b) Auxiliary-graph AG.worth noting cliques hid, ti pairs well std-formulasmaximal cliques general, shown following.Example 19. Continuing Example 18, assume F augmented following (ground)std-formulas:f3 = [loc(id2 , {p3 }, 1) loc(id2 , {p2 }, 2)]f4 = [loc(id2 , {p4 }, 1) loc(id2 , {p2 }, 2)]Thus, clique std-formula f3 consists set {loc(id2 , {p3 }, 1), loc(id2 , {p2 }, 2)},f4 consists set {loc(id2 , {p4 }, 1), loc(id2 , {p2 }, 2)}. cliques maximal ones, included clique consisting set vertices{loc(id2 , {p3 }, 1), loc(id2 , {p4 }, 1), loc(id2 , {p2 }, 2)}. basically due factconstraint imposed f3 f4 expressed succinctly, [loc(id2 , {p3 , p4 }, 1)loc(id2 , {p2 }, 2)] whose associated clique maximal.2following proposition follows definition std-graph fact every objectmust unique point time value.Proposition 1. Let K = hA, Fi PST KB F consists binary std-formulas only. Everymaximal independent set G consisting vertex hid, ti clique one-to-onecorrespondence world w W w |= F.9. Note use terminology clique complete subgraph G, maximal clique clique containedclique. point since maximal cliques often called simply cliques.762fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESObserve maximal independent set G property stated Proposition 1, G must contain maximal clique including least two hid, ti cliques (this happensinstance F contains std-formula [loc(id, Space, t1 ) loc(id, Space, t2 )]). case,world satisfies F PST KB trivially inconsistent. general case, may exponential number maximal independent sets G property stated Proposition 1,fact PST KB consistent decided using G explained follows.3.5.1 C LIQUE -ACYCLIC TD -G RAPHScharacterization tractable cases consistency checking problem focuses KBsbinary std-formulas std-graphs property, call clique-acyclic. startpreliminary definitions.Definition 11 (Binary maximal clique). std-graph G call clique binary maximal iffcontains vertices two hid, ti pairs properly included clique containsvertices hid, ti pairs.particular, std-graph G Figure 2(a) 4 binary maximal cliques involving id1 .Definition 12 (Clique-subgraph). call subgraph std-graph G clique subgraph iff containsvertices G, one edge hid, ti clique well one new distinct edgebinary maximal clique.clique-subgraph std-graph graph G Figure 2(a) 10 edges, one6 hid, ti pairs one binary maximal clique.Definition 13 (Clique-acyclic std-graph). Std-graph G said clique-acyclic iff cliquesubgraphs acyclic graphs (that is, forests). G called clique-cyclic clique-acyclic.Basically, clique-acyclicity means cycle found std-graph compressingbinary maximal cliques single edges using one edge hid, ti-clique. easysee std-graph shown Figure 2(a) clique-acyclic. stated next proposition,clique-acyclicity checked using following auxiliary graph basically compressesclique-subgraph essential structure. clear definition auxiliarygraph obtained clique-subgraphs graph G.Definition 14 (Auxiliary graph). auxiliary graph G undirected graph AG = hV 0 , E 0that:V 0 consists vertex hid, ti pair, id ID ;E 0 consists edges binary maximal cliques clique-subgraphprevious vertex loc(id, {p}, t) replaced corresponding hid, ti pair. denoteC(e) binary maximal clique C edge e originated.auxiliary graph std-graph Figure 2(a) shown Figure 2(b). example,C(e) edge e = h(id1 , 0), (id1 , 1)i consists following set vertices{loc(id1 , {p3 }, 0), loc(id1 , {p4 }, 0), loc(id1 , {p1 }, 1)}.following proposition follows Definitions 13 14.763fiPARISI & G RANTProposition 2. Std-graph G clique-acyclic iff auxiliary graph AG acyclic (that is, AGforest).following, introduce set linear inequalities used check consistency PST KBs std-formulas binary auxiliary graph acyclic.next subsection special case cyclic auxiliary graph. casesworking single connected component auxiliary graph. Suppose AGn connected components C1 , . . . , Cn let Ki PST KB corresponding Cii. K model show obtain model Mi Ki . Let wi world appropriatehid, ti pairs Ki .PLet Wi worlds K extend wi hid, ti valuesKi . Define Mi (wi ) = wWi (w). Going direction, suppose M1 , . . . , Mnmodels PST KBs corresponding Ci , . . . , Cn respectively. world w K, letw1 , . . . , wn restrictions w C1 , . . . , Cn respectively. Defining (w) = ni=1 (wi )model K. shown following result.Proposition 3. K consistent iff PST KBs corresponding connected componentsG (and hence AG) consistent.Hence proofs suffices assume AG single connected component.10Definition 15 (BC(K)). Let K = hA, Fi PST KB F consists binary std-formulasonly. BC(K) consists following (in)equalities:Pvid,t,p u;(1) loc(id, r, t)[`, u] A: `pr(2) id ID, :Pvid,t,p = 1;pSpace(3) id ID, T, p Space: vid,t,p 0;Pvid,t,p 1.(4) edge e AG:loc(id,{p},t)C(e)following theorem states checking whether BC(K) feasible solution equivalentdeciding consistency PST KBs K std-formulas binary generate acyclicauxiliary graph. intuition behind result follows. feasible solutionBC(K), inequalities (1)-(3) entail hid, ti pair, events object id timepoint p Space arranged fit whole probability space without overlapping. Roughlyspeaking would suffice define model PST KB without std-formulas combiningdistributions obtained hid, ti pair. Considering std-formulas means eventscannot occur together, is, cannot coexist portion probability space.Intuitively, fact inequality (4) BC(K) satisfied edge e entails eventscorresponding st-atoms C(e) arranged distinct portions probability space(avoiding overlaps). auxiliary graph AG acyclic, reasoning inductively repeatededge AG without ever reconsidering arrangement events correspondingst-atoms considered previously. satisfiability inequality (4) BC(K) also necessary10. indicate statement following theorem AG several connected components,proof works one Proposition 3 applied.764fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASEScondition consistency K fact satisfied edge e AG intuitivelymeans events corresponding st-atoms C(e), even taken alone, cannot arrangedprobability space without overlapping.Theorem 6. Let K = hA, Fi PST KB F consists binary std-formulas only. AGacyclic, K consistent iff feasible solution BC(K).Proof. use notation K0 K F removed, is, K0 = hA, i. Then, M(K)subset M(K0 ).() prove contrapositive: feasible solution BC(K), K inconsistent. simplest case BC(K) feasible solution due fact BC(K0 )feasible solution. inequalities (4), hence K0 consistent, thus Kconsistent well.Otherwise, feasible solution BC(K0 ) thus BC(K) feasible solution duefact least one inequalitiesitem (4) Definition 15 satisfied. Therefore,Pedge e AG loc(id,{p},t)C(e) vid,t,p > 1. show K inconsistentshow interpretation model K. suppose interpretationmodel K. Since pair A1 , A2 st-atoms belong C(e) iff (a) [A1 A2 ] belongs setground formulas derived F, (b) A1 = loc(id, {p1 }, t) A2 = loc(id, {p2 }, t)p2 6= p1 , world w W(K) (w) > 0, w |= A1 , w |= A2 (inparticular, case (b) world w W(K) w |= A1 , w |= A2 , whether(w) > 0). is, every world w K assigned non-zero probabilitysatisfies one st-atom C(e). Hence, W(K) partitioned |C(e)| + 2pairwise disjoint sets follows:(i) set worlds w W(K) (w) = 0 (this set includes worlds satisfyingleast two st-atoms C(e))(ii) set worlds w W(K) (w) > 0 w satisfy st-atom C(e);(iii) st-atom Ai C(e), set worlds w W(K) (w) > 0, w |= Ai ,w 6|= Aj Aj C(e) (and different Ai ).Therefore, sum probabilities worlds satisfying least one st-atom C(e) equalsum, st-atom C(e), sum probabilities worlds satisfying atomC(e) st-atoms C(e),Xw W(K)w |= loc(id, {p}, t)loc(id, {p}, t) C(e)(w) =XXloc(id,{p},t)C(e)w W(K)w |= loc(id, {p}, t)w 6|= loc(id0 , {p0 }, t0 )loc(id0 , {p0 }, t0 ) C(e)(w).recall model K also model K0 . above-mentioned partitioningW(K) also holds W(K0 ). Therefore, solution BC(K0 ), one-to-one correspondingmodel K0 , sum probabilities assigned worlds K0 satisfying st-atom loc(id, {p}, t) equal value assigned variable vid,t,p . particular,765fiPARISI & G RANTgiven st-atom loc(id, {p}, t) C(e),XX(w) =W(K0 )(w) = vid,t,p .W(K0 )ww |= loc(id, {p}, t)ww |= loc(id, {p}, t)loc(id, {p}, t) C(e)w 6|= loc(id0 , {p0 }, t0 )loc(id0 , {p0 }, t0 ) C(e)Considering st-atoms C(e) obtainXX(w) =W(K0 )Xloc(id,{p},t)C(e)ww |= loc(id, {p}, t)loc(id, {p}, t) C(e)=X(w) =W(K0 )ww |= loc(id, {p}, t)w 6|= loc(id0 , {p0 }, t0 )loc(id0 , {p0 }, t0 ) C(e)vid,t,p > 1.loc(id,{p},t)C(e)inequality holds since inequality (4) Definition 15 satisfied edge e.Finally, since following inequality holds due definition modelXX(w)(w),wW(K0 )w W(K0 )w |= loc(id, {p}, t)latter strictly greater one, follows PDF W(K0 ), meaningmodel K0 . Hence interpretation model K K inconsistent.() prove auxiliary graph AG acyclic BC(K) feasible solution,K consistent. prove mathematical induction number edges AG.Base case: number edges 0, AG consists set isolated vertices. factAG contains edges means set ground std-formulas derived Fempty. Thus feasible solution BC(K) iff feasible solution CC(K0 ). UsingFact 1, obtain K consistent.Inductive step: prove statement holds edges AG, holds + 1edges. given acyclic graph AG + 1 edges, write AG +1 .acyclicity choose subgraph, AG edges new edge connects vertexisolated vertex AG AG clearly acyclic. Let K = hA, F PST KBF consists subset ground std-formulas derived F std-graph Gauxiliary graph AG . induction hypothesis feasible solution BC(K ),K consistent.write K+1 = hA, F +1 F +1 consists ground std-formulas F pluscorresponding new edge e, AG +1 turns auxiliary graph G(K+1 ).Assuming induction hypothesis, show feasible solution BC(K+1 ),K+1 consistent.Let solution BC(K+1 ). Obviously, also solution BC(K ), consistssubset (in)equalities BC(K+1 ). fact solution BC(K ) meansmodel K sum probabilities assigned worlds766fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESK satisfying st-atom loc(id, {p}, t) equal value (vid,t,p ) assigned variable vid,t,p. Since solution also BC(K+1 ), thus inequality (4) Definition 15 holds, followsXXX(vid,t,p ) 1.(w) =loc(id,{p},t)C(e)w W(K ),w |= loc(id, {p}, t)loc(id,{p},t)C(e)is, model K property sum probabilities worldssatisfying least one st-atom C(e) less equal one.assigns probability equal zero every world W(K ) containingleast two st-atoms C(e), done, since turns model K+1 too.Otherwise, starting , show model +1 K+1 constructedreasoning follows.recall new edge e added AG obtain AG +1 form h(id1 , t1 ),(id2 , t2 )i. Let C1 (e) C2 (e) sets st-atoms C(e) form loc(id1 , {p}, t1 )loc(id2 , {p}, t2 ), respectively. Hence, C(e) = C1 (e) C2 (e) C1 (e) C2 (e) = . Let= hw1 , w2 , . . . , wn permutation worlds w W(K ) assignedprobability greater zero (i.e., (w) > 0) first k worlds satisfy statom C1 (e). Note that, W(K ) = W(K+1 ), world W(K ) consists oneatom C1 (e), function whose domain ID (see Definition 4) st-atomsC1 (e) refer id andPtime value. Let us denote sum probabilitiesfirst worlds , is, = ij=1 (wj ), wj . Thus, k equal sumprobabilities assigned worlds satisfying st-atom C1 (e), i.e.,Xk =X(w) =w W(K ),w |= loc(id, {p}, t),loc(id, {p}, t) C1 (e)(vid,t,p ).loc(id,{p},t)C1 (e)Let subset Space consisting points p variables vid2 ,t2 ,p assignedvalue greater zero , is, = {p | p Space, (vid2 ,t2 ,p ) > 0}. Observevariables correspond st-atoms referring endpoint (id2 , t2 ) edge e. Let = hp1 , p2 , . . . , pmpermutation points first h points p correspond variables vid2 ,t2 ,ploc(id2 , {p}, t2 ) 6 C2 (e), subsequent points (with index [h + 1, .., m])correspond variables vid2 ,t2 ,p loc(id2 , {p}, t2 ) C2 (e). Let sumPivalues assigned first points , is, =j=1 (vid2 ,t2 ,pj ) pi .Observe thatPm = 1 since equality (2) Definition 15 holds BC(K+1 ), h =1(vid,t,p ). fact inequality (4) Definition 15 holds BC(K+1 )loc(id2 ,{p},t2 )C2 (e)entails that,1 h + k =X(vid,t,p ) +loc(id,{p},t)C2 (e)X(vid,t,p ) 1.loc(id,{p},t)C1 (e)Therefore, obtain k h . Intuitively, means possible define PDFworlds K+1 world satisfies two st-atoms C(e), keeping satisfiedPST atoms std-formulas satisfied . formally described below.767fiPARISI & G RANTdefine model +1 K+1 follows. Let U = hu1 , u2 , . . . , uz sequenceconsisting values {1 , . . . , n } {1 , . . . , } ordered ascending order. definefollowing z non-zero probability worlds K+1 . ui U , let wi world that:id ID \ {id2 }, \ {t2 }, wi (id, t) = wj (id, t), wj W(K ) jsmallest subscript j ui .wi (id2 , t2 ) = pj , pj j smallest subscript j ui .define +1 PDF W(K+1 ) (i) +1 (w1 ) = v1 , (ii) +1 (wi ) =vi vi1 [1..z], (iii) worlds w W(K+1 ), +1 (w) = 0.show +1 model K+1 . First, observe +1 model K .fact, id ID \ {id2 }, id \ {t2 }, p Space, probability object idtime value point p according +1 equal probability id time valuepoint p according ,XXid ID \ {id2 }, \ {t2 }, p Space,+1 (w) =(w).w W(K ),w(id, t) = pw W(K+1 ),w(id, t) = pMoreover, hid2 , t2 pair,Xp Space,+1 (w) = (vid2 ,t2 ,p ) =X(w).w W(K ),w(id, t) = pw W(K+1 ),w(id, t) = pensures that, PST atom = loc(id, r, t)[`, u] A, sum probabilitiesassigned +1 worlds satisfying st-atom loc(id, r, t) belong probability interval[`, u] specified a, since hold . Moreover, none worlds assignedprobability equal zero assigned probability greater zero +1 ,consists worlds assigned non-zero probability . Therefore, +1 modelK .Finally, show +1 model K+1 , suffices observe k h entailsworld wi K+1 (with [1..z]) assigned probability greater zerosatisfies two atoms C(e). completes proof.following example shows usage result Theorem 7 check consistencyPST KB.Example 20. Continuing Example 18, assume K PST KB F set stdformulas given Example 18 consist following set PST atoms:loc(id1 , {p3 , p4 }, 0)[.7, 1], loc(id1 , {p1 }, 1)[.2, .5], loc(id1 , {p1 }, 2)[.3, .8], loc(id2 , {p2 }, t)[.7, 1][0..2]. checked PST KB corresponding set linear inequalities BC(K) feasible solution. Thus, since auxiliary graph AG(K) shown Figure 2(b)acyclic, follows K consistent.Consider PST KB K0 obtained K replacing atom loc(id1 , {p1 }, 2)[.3, .8]loc(id1 , {p1 }, 2)[.8, .8], lower probability .8 instead .3. case, BC(K0 )includes inequalities 0.7 vid1 ,0,p3 + vid1 ,0,p4 1 0.8 vid1 ,2,p1 0.8 (due PSTatoms), vid1 ,0,p3 + vid1 ,0,p4 + vid1 ,1,p1 1 (due edge h(id1 , 0), (id1 , 2)i AG(K)).Clearly, feasible solution BC(K0 ), thus K0 consistent.2768fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESnote fact acyclicity AG used left-to-right proof Theorem 6.Therefore, whether AG(F) acyclic, PST KBs std-formulas binary,necessary condition stated following theorem used checking consistency.Theorem 7. Let K = hA, Fi PST KB F consists binary std-formulas only.feasible solution BC(K), K consistent.condition BC(K) feasible solution stronger NC(K) feasiblesolution, used Theorem 3 general std-formulas, since easily checked everysolution BC(K) solution NC(K), converse hold general. Thus,PST KBs K, using Theorem 3 cannot conclude anything consistencyK, NC(K) feasible solution, show K inconsistent using resultTheorem 7.Example 21. already observed fact PST KB K Example 16consistent cannot concluded checking whether feasible solution NC(K). However,easy see BC(K) feasible solution case, thus concludePST KB Example 16 consistent applying result Theorem 7.2Although necessary condition Theorem 6 still used check consistencyauxiliary graph cyclic (see Theorem 7), sufficient condition Theorem 6 entailsPST KB K consistent corresponding set BC(K) linear inequalities feasiblesolution auxiliary graph acyclic. following, identify class std-graphsconsistency checking problem remains tractable even std-graph clique-cyclic (andthus auxiliary graph cyclic).3.5.2 IMPLE C LIQUE -C YCLIC TD -G RAPHSsection provide set linear inequalities checking consistency PST KBwhose std-graph satisfies following property.Definition 16 (Simple clique-cyclic std-graph). Std-graph G said simple clique-cyclic iffauxiliary graph AG simple graph 11 cyclic connected component G containssingle maximal clique hid, ti clique.following example simple clique-cyclic std-graph. provide std-graphauxiliary graph.Example 22. PST KB K = hA, Fi Example 15, obtain std-graph G shownFigure 3(a). auxiliary graph AG shown Figure 3(b). G consists single connected component one maximal clique Cl hid, ti clique, namelyCl = {loc(id, {p0 }, 0), loc(id, {p0 }, 1), loc(id, {p0 }, 2)}. easy see G simple cliquecyclic.2following theorem states checking consistency PST KB whose std-graphsimple clique-cyclic accomplished checking whether feasible solution setlinear inequalities.start defining new linear system: CL(K).11. loops multiple edges vertices.769fiPARISI & G RANTidp1t=0p2p0p1id1,0t=1p2p0id1,1p1idt=2id1,2p2p0(a)(b)Figure 3: (a) Std-graph (b) auxiliary graph PST KB Example 15.Definition 17 (CL(K)). Let K = hA, Fi PST KB F consists binary std-formulasonly. CL(K) consists following (in)equalities:(1) loc(id, r, t)[`, u] A: `Pvid,t,p u;pr(2) id ID, :Pvid,t,p = 1;pSpace(3) p Space, id ID, : vid,t,p 0;(4) (a)P acyclic connected component G (and hence AG), edge e AG:vid,t,p 1.loc(id,{p},t)C(e)(b) cyclic connected componentG (and hence AG), maximal clique ClPhid, ti clique:vid,t,p 1.loc(id,{p},t)Clintuition behind result stated following theorem similar giventractable case Theorem 6. Indeed, G simple clique-cyclic CL(K) feasible solution,events corresponding st-atoms edge e acyclic connected component AG,well events corresponding st-atoms maximal clique connected component AG, arranged probability space avoiding overlaps. basically sufficesdefine model K.Theorem 8. Let K = hA, Fi PST KB F consists binary std-formulas only. Gsimple clique-cyclic, K consistent iff feasible solution CL(K).770fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESProof. explained earlier deal single cyclic connected component G.() proof follows reasoning left-to-right proof Theorem 6.() Let solution CL(K). Obviously, alsoPsolution CC(K0 ) K0 =000hA,P i, model K (vid,t,p ) = wW(K0 ),w|=loc(id,{p},t) (w)loc(id,{p},t)Cl (vid,t,p ) 1. Given this, show starting define new modelK0 also model K.hid, ti pair, define S(id, t) subset Space consisting points pvid,t,p assigned value greater zero , i.e., S(id, t) = {p | p Space, (vid,t,p ) > 0}.distinguish following two sets hid, ti pairs: IDT1 (resp. IDT2 ) sethid, ti pairs (resp. is) point p S(id, t) loc(id, {p}, t) Cl.separately consider pairs IDT1 IDT2 , hid, ti pair belonging onesets, define sequence points S(id, t) along sequence cumulative probabilityvalues used build model .start set IDT1 . Let hidi , ti i, [1..|IDT1 |] i-th pair IDT1 (afterordering pairs IDT1 according fixed order). Let = hp1i , . . . , pmpermutationpoints S(idi , tP), let (k) sum values assigned first k points, is, (k) = kj=1 (vidi ,ti ,pj ). Thus, hi (1), . . . , (mi )i sequence (cumulative)probability values associated sequence points hp1i , . . . , pmi-th pair IDT1 .consider pairs IDT2 , i.e, occurring st-atom Cl. Denotehidi , ti i, [1..|IDT2 |], i-th pair IDT2 (according fixed order). first1pair IDT2 , let 1 = hp11 , . . . , ph1 1 , ph1 1 +1 , . . . , pm1 permutation points S(id1 , t1 )first h1 points p correspond variables vid1 ,t1 ,p loc(id1 , {p}, t1 ) Cl1(consequently, points p {ph1 1 +1 , . . . , pm1 } correspond variables vid1 ,t1 ,ploc(id1 , {p}, t1 ) 6 Cl).hidi , ti [2..|IDT2 |], let = hp1i , . . . , pgi , pigi +1 , . . . , pigi +hi , pigi +hi +1 , . . . , pmg+h+1gsequence points S(idi , ti ) (i) point p {p1i , . . . , pi , pi , . . . , pi }(resp. p {pgi +1 , . . . , pgi +hi }) corresponds variable vidi ,ti ,p loc(idi , {p}, ti ) 6 Cl(resp. loc(idi , {p}, ti ) Cl); (ii) points sequence distinct except pgipgi +hi +1 may refer point S(idi , ti ). Denoting (k) sum valuesPassigned first k points (i.e., (k) = kj=1 (vidi ,ti ,pj )), choose sequences2 , . . . , |IDT2 | that, [2..|IDT2 |], (gi ) = i1 (gi1 +hi1 ) (assume g1 = 0).Note make choice since following holds:|IDT2 | hiX X|IDT2 |(vidi ,ti ,pk ) =Xi=1 k=1|IDT2 |(gi + hi ) (gi ) =i=1XX(vidi ,ti ,p ) 1.i=1 loc(idi ,{p},ti )Clway sequences h1 , . . . , |IDT2 | hi (1), . . . , (mi )i defined allow us buildmodel K0 world assigned probability greater zero satisfies twodistinct st-atoms Cl. ensure model K too.define model . LetSV = hv1 , v2 , . . . , vz sequence consistingvalues iIDT1 {i (1), . . . , (mi )} iIDT2 {i (1), . . . , (mi )} ordered ascending order.define following z non-zero probability worlds K0 (note world K0 alsoworld K). vj V , let wj world that:771fiPARISI & G RANThidi , ti IDT1 , define wj (idi , ti ) = pki , pki left-most valuehi (1), . . . , (mi )i (k) vj .hidi , ti IDT2 , define wj (idi , ti ) = pki , pki left-most valuehi (1), . . . , (mi )i (k) vj .Finally, define PDF W(K0 ) (i) (w1 ) = v1 , (ii) (wj ) = vj vj1j [1..z], (iii) (w) = 0 worlds w W(K0 ). easy check00modelP K corresponds solution CC(K ), is,(vid,t,p ) = wW(K0 ),w|=loc(id,{p},t) (w). Moreover, construction shown ensuresnon-zero probability world K0 satisfies two distinct st-atoms Cl, followsmodel K.Theorem 8 used, instance, decide PST KB Example 15 consistent.Example 23. PST KB K = hA, Fi Example 15, CL(K) linear system obtainedCC(hA, i) augmenting inequality vid,0,p0 + vid,1,p0 + vid,2,p0 1. Since CL(K)also contains inequality 0.5 vid,t,p0 [0..2] (due presence PST atomloc(id, {p0 }, t)[0.5, 1] A), follows CL(K) feasible solution, concludeK consistent (here G clique-cyclic std-graph shown Figure 3(a)).2conclude section following theorem stating checking consistencyPST KBs identified Section 3.5 tractable.Theorem 9. Let K = hA, Fi PST KB F consists binary std-formulas only. Gclique-acyclic simple clique-cyclic, checking consistency K P IM E.Proof. G consists one connected component, statement follows facts buildingG, well checking whether G acyclic (resp. simple clique-cyclic), accomplishedpolynomial time w.r.t. size K checking whether feasible solution BC(K)(resp., CL(K)) polynomial too. G consists one connected component, statement follows facts finding connected components G done polynomialtime, and, using Proposition 3, need check consistency PST KBs corresponding connected components order decide consistency whole PSTKB.4. Querying PST Knowledge Basessection investigates complexity checking answer queries. Section 4.1 containsbasic definitions. Section 4.2 contains major result complexity checking answersqueries. Finally, Section 4.3 gives sufficient necessary conditions answering queriestractable cases.4.1 Optimistic Cautious Answersproblem querying SPOT data investigated Parker et al. (2007a, 2009), Parisi,Parker, Grant, Subrahmanian (2010), Grant, Molinaro, Parisi (2013) specific772fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESframeworks corresponding PST KBs (of form hA, i), PST atoms considered. section, address problem answering selection queries general (consistent) PST KBs. kinds queries considered Parker et al. (2007a, 2009), Parisiet al. (2010), Grant et al. (2013) focused count queries.selection query expression form (?id, q, ?t, [`, u]), q region [`, u]probability interval. Intuitively, selection query says: Given region q probability interval[`, u], find objects id times id inside q time probability interval[`, u]. two semantics interpreting statement, leading two types answersselection queries. Optimistic answers objects time values may query regionprobability specified interval, whereas cautious answers consist objectstime values guaranteed region probability given interval. Thus,cautious answers subset optimistic ones.following definition extends original definition optimistic cautious selectionquery answers general case consistent PST knowledge bases.Definition 18 (Optimistic/Cautious Query Answers). Let K consistent PST KB, Q =(?id, q, ?t, [`, u]) selection query. Then,hid, tiPis optimistic answer Q w.r.t. K iff exists model M(K) s.t.(w) [`, u].w|=loc(id,q,t)hid, tiPis cautious answer Q w.r.t. K iff every model M(K),(w) [`, u].w|=loc(id,q,t)Example 24. Let q1 = {(7, 3), (7, 4)} (q1 overlaps region c, see Figure 1). ModelExample 9 entails hid1 , 9i optimistic answer Q = (?id, q1 , ?t, [.7, 1]), w1 (id1 , 9) =w2 (id1 , 9) = w3 (id1 , 9) = (7, 4) q1 (w1 ) + (w2 ) + (w3 ) = 1 [.7, 1].Let q2 region including region c. hid1 , 9i cautious answer Q0 = (?id, q2 , ?t, [.7, 1]),according model Kex , id1 must region c (and thus q2 ) time 9 probability[.9, 1] (due loc(id1 , c, 9)[.9, 1] Aex ). Clearly, hid1 , 9i also optimistic answer Q0 . 24.2 Complexity Query Answeringfollowing theorem shows consistency checking used answer selection queriesoptimistic cautious semantics.Theorem 10. Let K = hA, Fi consistent PST KB, Q = (?id, q, ?t, [`, u]). Then,1) hid, ti optimistic answer Q w.r.t. K iff hA {loc(id, q, t)[`, u]}, Fi consistent.2) hid, ti cautious answer Q w.r.t. K iff hA {loc(id, q, t)[0, ` ]}, Fi hA{loc(id, q, t)[u + , 1]}, Fi consistent, smallest rational numberprecision m3 , = |A| + |F| |ID| |T | |Space| + |ID| |T | + |ID| |T | |Space|.Proof. Statement 1) easily follows Definitions 6 18.Statement 2) follows fact hid, ti cautious answer Q w.r.t. K iff i) hid, tioptimistic answer Q0 = (?id, q, ?t, [0, ` ]) w.r.t. K, ii) hid, ti optimistic773fiPARISI & G RANTanswer Q00 = (?id, q, ?t, [u + , 1]) w.r.t. K, small non-zero constant whose sizepolynomial w.r.t. size K. show existence , observe (i) checkingconsistency K reduced deciding PSAT instance K consisting clauses,= |A| + |F| |ID| |T | |Space| + |ID| |T | + |ID| |T | |Space|, shown membershipproof Theorem 1; (ii) satisfying probability distribution instance PSATclauses exists, one + 1 non-zero probabilities, entriesconsisting rational numbers precision O(m2 ) (Georgakopoulos et al., 1988; Papadimitriou& Steiglitz, 1982); Then, choosing equal smallest rational number precision m3 sufficesobtain sufficiently small non-zero constant, whose size polynomial w.r.t. size K,hA {loc(id, q, t)[0, ` ]}, Fi (resp., hA {loc(id, q, t)[u + , 1]}, Fi) consistent iffhA {loc(id, q, t)[0, ` ]}, Fi (resp., hA {loc(id, q, t)[u + , 1]}, Fi) consistent,infinitely small value greater zero.following corollary follows Theorems 1 10.Corollary 1. Let K = hA, Fi consistent PST KB. Given query Q = (?id, q, ?t, [`, u]),1) deciding whether hid, ti optimistic answer Q w.r.t. K NP.2) deciding whether hid, ti cautious answer Q w.r.t. K coNP.problem deciding whether pair hid, ti optimistic/cautious answer selectionquery Q solved polynomial time PST KBs whose F component empty (Parker et al.,2007a, 2009; Parisi et al., 2010). However, presence integrity constraints problembecomes hard.Theorem 11. Let K = hA, Fi consistent PST KB. Given query Q = (?id, q, ?t, [`, u]),1) deciding whether hid, ti optimistic answer Q w.r.t. K NP-hard.2) deciding whether hid, ti cautious answer Q w.r.t. K coNP-hard.Proof. first prove item 1). show reduction problem NP-hard Hamiltonianpath problem (Papadimitriou, 1994). Given directed graph G = hV, Ei, V = {v0 , . . . , vk }set k +1 vertexes G E set edges, construct instance problemfollows. Let ID = {id}, Space = V {p0 , . . . , pk1 } {pT , pF }, = [0, . . . , 2k + 1].K pair hA, Fi consist PST atom loc(id, v0 , 0)[1, 1] F consistsfollowing formulas:[0..k 1], f1i = Z1 , Z2 [loc(id, {vi }, Z1 ) loc(id, Space \ V 0 , Z2 ) Z2 = Z1 + 1]V 0 = {vj |(vi , vj ) E}{pi , pT , pF }. is, locations id reach startingvi one time step locations vj s.t. (vi , vj ) E point {pi , pT , pF }.f1k = Z1 , Z2 [loc(id, {vk }, Z1 ) loc(id, Space \ V 00 , Z2 ) Z2 = Z1 + 1] V 00 ={vj |(vk , vj ) E} {pT , pF }. formula says locations id reach startingvk one time step locations vj (vk , vj ) E {pT , pF }.[0..k1], f2i = Z1 , Z2 [loc(id, {pi }, Z1 )loc(id, Space\{vi+1 }, Z2 )Z2 = Z1 +1],saying location id reach starting pi one time step vi+1 .774fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESf3 = Y1 , Z1 , Z2 [loc(id, Y1 , Z1 ) loc(id, Y1 , Z2 ) Z1 6= Z2 Y1 6= {pT } Y1 6= {pF }],saying id location distinct time values, location differentpT pF .f4 = Z1 , Z2 [loc(id, {p0 , . . . , pk1 }, Z1 ) loc(id, {pT }, Z2 )], saying idpT was/is/will location {p0 , . . . , pk1 }.f5 = Y2 , Z1 , Z2 [loc(id, {pT }, Z1 ) loc(id, Y2 , Z2 ) Y2 6= {pT } Z1 < Z2 ], sayingid go away pT reaching location.f6 = Y2 , Z1 , Z2 [loc(id, {pF }, Z1 ) loc(id, Y2 , Z2 ) Y2 6= {pF } Z1 < Z2 ], sayingid go away pF reaching location.Finally, let Q = (?id, {pT }, ?t, [1, 1]), let pair checked optimistic answerQ hid, 2k + 1i.First all, observe M(K) 6= . fact, consider world w W(K) w(id, 0) =v0 , w(id, 1) = p0 , w(id, 2) = v1 , w(id, 3) = p1 , . . . , w(id, t) = vt/2 , w(id, + 1) = pt/2+1 , (incases even) . . . , w(id, 2k2) = vk1 , w(id, 2k1) = pk , w(id, 2k) = vk , w(id, 2k+1) =pF . easy see w |= F. fact, [0..2k], id moves towards reachable locationsone time point: id vi , [0..k 1], moves pi (that is, f1i satisfied w);id pi , [0..k 1], moves vi+1 (that is, f2i satisfied w); vkmoves pF (that is, f1k satisfied w). Moreover, id placed w locationdifferent times (that is, f3 satisfied), neither location {p1 , . . . , pk1 } pT (thatis, f4 satisfied too). Moreover, id move pF reaching location (that is, f6satisfied), reach location pT (thus, also f5 satisfied w). Now, consider PDFW(K) assigning probability equal 1 w, probability equal 0 every worldW(K). Clearly, model K: assigns non-zero probability worlds satisfying F,ensures atom loc(id, v0 , 0)[1, 1] satisfied w assigned probability equal1.show hid, 2k + 1i optimistic answer Q w.r.t. K iff Hamiltonianpath G.()P hid, 2k + 1i optimistic answer Q w.r.t. K, model M(K) s.t.(w) = 1. Let W W(K) set worldsPw w(id, 2k + 1) =w|w|=loc(id,pT ,2k+1) PpT (w) > 0. wW (w) = 1 model (thus, w|w|=loc(id,v0 ,0) (w) = 1due loc(id, v0 , 0)[1, 1]) A), follows every world w W w(id, 0) = v0 .Moreover, since model, thus f4 satisfied every world w W , holds[0..2k + 1], w(id, t) 6 {p0 , . . . , pk 1}, meaning id never placed location{p0 , . . . , pk1 } w. Moreover, since w W , w(id, 0) = v0 , since f1i f2i(with [1..k]), f3 satisfied every world W , follows (i) [0, k],w(id, t) V (i.e., id placed vertex G time point [0, k]), (ii)t, t0 [0, k], w(id, t) 6= w(id, t0 ) (id placed vertex G different times[0, k]). Given this, time = k + 1, f1k f3 entail id placed world W locationpT pF . f5 (resp., f6 ) entails id go away pT (resp., pF ) reachinglocation, since w(id, 2k + 1) = pT , follows every world w W[k + 1, 2k + 1], w(id, t) = pT . Hence, w W (w) > 0, w(id, 0) = v0 ,[1, k], w(id, t) V , t, t0 [0, k], w(id, t) 6= w(id, t0 ), [k + 1, 2k + 1], w(id, t) = pT .Therefore, w(id, 0), w(id, 1), . . . , w(id, k) Hamiltonian path G.775fiPARISI & G RANT() Let Hamiltonian path G. denote [i] (with [0..k]) i-th vertex. W.l.o.g. assume firstPvertex v0 , is, [0] = v0 . showmodel M(K) w|w|=loc(id,pT ,2k+1) (w) = 1, is, hid, 2k + 1ioptimistic answer Q w.r.t. K. Let function W worlds w W,(w) = 0 except world w that: w (id, 0) = [0] = v0 , [1, k],w (id, t) = [t], [k + 1, 2k + 1], w (id, t) = pT . easy see w |= F. fact,[1..k], f1i well f2i satisfied w , since fact path G entails[0, k 1], w (id, t) = vi w (id, + 1) = vj edge (vi , vj ) edge Gw (id, k + 1) = pT reachable location V . Moreover, f3 satisfied w ,since fact Hamiltonian path entails w places id different locations differenttimes, except location pT . Formula f4 satisfied w , since w place idlocation {p0 , . . . , pk1 }. Formula f5 satisfied w , since w place id locationdifferent pT time point k + 1, id reaches pT . Finally, f6 satisfied w well,since w place id location pF . Since wP|= F, assigned probabilitydifferent 0. Let (w ) = 1. Moreover, w|w|=loc(id,v0 ,0) (w) = (w ) = 1,condition required loc(id, v0 , 0)[1, 1] holds too, proving model K.prove item 2). show reduction problem complementHamiltonian path problem. Given directed graph G, construct instance problemproof 1), except Q = (?id, {pT }, ?t, [0, 1 ]), > 0 statedTheorem 10. shown proof 1), K consistent. Moreover, Hamiltonianpath G iff pair hid, 2k + 1i optimistic answer Q0 = (?id, {pT }, ?t, [1, 1]).observe hid, 2k + 1i optimistic answer Q0 iff hid, 2k + 1i cautiousanswer Q = (?id, {pT }, ?t, [0, 1 ]) > 0 sufficiently small. reasoningproof Theorem 10, chooseP = , ensures M(K)P(w)<1,w|w|=loc(id,pT ,t) (w) 1 .w|w|=loc(id,pT ,t)query answering problem related decision version entailment problemprobabilistic logic (Nilsson, 1986; Georgakopoulos et al., 1988), follows. Given conjunction clauses, associated probability, additional clause Cgiven lower upper bounds admissible values probability, decide whetherprobability distribution satisfying probability clauses lower upperbounds C. probabilistic logic, entailment problem reduced PSAT (Nilsson, 1986; Georgakopoulos et al., 1988), shown query answering problem PSTKBs reduced consistency checking problem. However, important differencequery answering problem probabilistic entailment problem: assumeinput PST KBs consistent, entailment problem defined (evensatisfiable). consequence, coNP-hardness probabilistic entailment problem straightforwardly follows NP-hardness PSAT ( unsatisfiable iff contradiction Clower upper bounds entailed ). is, satisfiability source complexityprobabilistic entailment problem. However, setting, given consistent PST KBthus checking consistency cannot source complexity query answering problem.fact, Theorem 11 viewed strengthening coNP-hardness probabilistic entailmentproblem, proved specific setting corresponding case input formulaknown satisfiable.776fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES4.3 Sets Linear Inequalities Answering Queriesfollowing corollary Theorems 2 10 states set mixed-binary linear inequalitiesMBL() introduced Definition 8 also exploited answering queries.Corollary 2. Let K = hA, Fi consistent PST KB, Q = (?id, q, ?t, [`, u]) query. Then,1) feasible solution MBL(hA{loc(id, q, t)[`, u]}, Fi), hid, ti optimisticanswer Q w.r.t. K.2) MBL(hA {loc(id, q, t)[0, ` ]}, Fi) MBL(hA {loc(id, q, t)[u + , 1]}, Fi)feasible solution, hid, ti cautious answer Q w.r.t. K (where givenTheorem 10).Similarly, Theorems 3 10 following corollary follows, stating linear systemNC() introduced Definition 9 also used answering queries.Corollary 3. Let K = hA, Fi consistent PST KB, Q = (?id, q, ?t, [`, u]) query. Then,1) feasible solution NC(hA {loc(id, q, t)[`, u]}, Fi), hid, tioptimistic answer Q w.r.t. K.2) NC(hA {loc(id, q, t)[0, ` ]}, Fi) NC(hA {loc(id, q, t)[u + , 1]}, Fi)feasible solution, hid, ti cautious answer Q w.r.t. K (where givenTheorem 10).Moreover, obtain following corollary Theorems 4 10.Corollary 4. Let K = hA, Fi consistent PST KB F = { loc(id1 , {p1 }, t1 )loc(id2 , {p2 }, t2 ) loc(idn , {pn }, tn )]}, Q = (?id, q, ?t, [`, u]) query. Then,1) hid, ti optimistic answer Q w.r.t. K iff feasible solution NC(hA{loc(id, q, t)[`, u]}, Fi).2) hid, ti cautious answer Q w.r.t. K iff NC(hA {loc(id, q, t)[0, ` ]}, Fi)NC(hA {loc(id, q, t)[u + , 1]}, Fi) feasible solution, given Theorem 10.Theorems 6 10 obtain following result.Corollary 5. Let K = hA, Fi consistent PST KB F consists binary std-formulasonly, Q = (?id, q, ?t, [`, u]) query. AG acyclic,1) hid, ti optimistic answer Q w.r.t. K iff feasible solution BC(hA{loc(id, q, t)[`, u]}, Fi).2) hid, ti cautious answer Q w.r.t. K iff BC(hA {loc(id, q, t)[0, ` ]}, Fi)BC(hA {loc(id, q, t)[u + , 1]}, Fi) feasible solution, given Theorem 10.Moreover, results Theorem 7, shape auxiliary graph considered,Theorem 10 entail following.777fiPARISI & G RANTCorollary 6. Let K = hA, Fi consistent PST KB F consists binary std-formulasonly, Q = (?id, q, ?t, [`, u]) query.1) feasible solution BC(hA {loc(id, q, t)[`, u]}, Fi), hid, tioptimistic answer Q w.r.t. K2) BC(hA {loc(id, q, t)[0, ` ]}, Fi) BC(hA {loc(id, q, t)[u + , 1]}, Fi)feasible solution, hid, ti cautious answer Q w.r.t. K (where givenTheorem 10).Still, Theorems 8 10 obtain following corollary.Corollary 7. Let K = hA, Fi consistent PST KB F consists binary std-formulasonly, Q = (?id, q, ?t, [`, u]) query. AG simple-clique cyclic,1) hid, ti optimistic answer Q w.r.t. K iff feasible solution CL(hA{loc(id, q, t)[`, u]}, Fi).2) hid, ti cautious answer Q w.r.t. K iff CL(hA {loc(id, q, t)[0, ` ]}, Fi)CL(hA {loc(id, q, t)[u + , 1]}, Fi) feasible solution, given Theorem 10.Finally, state following corollary Theorems 9 10 concerning cases computing selection queries tractable. Although result stated assuming binary stdformulas PST KB, holds general case unary binary stdformulas PST KB since, using approach suggested proof Theorem 5,assume unary std-formulas encoded PST atoms.Corollary 8. Let K = hA, Fi PST KB F consists binary std-formulas only.G clique-acyclic simple clique-cyclic, deciding whether hid, ti optimistic/cautiousanswer selection query Q = (?id, q, ?t, [`, u]) PTIME.5. Domain Enlargementpoint assumed three domains ID = {id1 , . . . , idm }, = [0, 1, . . . ,tmax], Space = {p1 , . . . , pn } fixed. context investigated consistencyPST KB. section investigate happens PST KB one domainsmodified. modification consists possibly adding new time values, spatial points,objects. fact, interested happens add arbitrarily many (but finite numberof) entities. Section 5.1 consider case deal longer time period addadditional time values beyond tmax. Section 5.2 deal time caseallow finer division time values. Section 5.3 deals case Space enlarged,is, new points added. Finally, Section 5.4 deal additional objects wellcombinations domain enlargement.5.1 Extending Time Beyond tmaxstart PST KB K using ID, , Space. Suppose extend beyond tmax;write 0 = [0, 1, . . . , tmax, . . . , t0 ]. means syntax must also extended778fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESconstants tmax + 1, . . . , t0 ; however, K use new constants. semanticsdifferent world w0 must function w0 : ID 0 Space. write Wset worlds using W 0 set worlds using 0 . connection WW 0 every w W extended many worlds W 0 choosing point Spacenew time value object. one world w0 W 0 unique restrictionworld W . Satisfaction worlds W 0 defined way satisfactionworlds W (with W 0 substituted W ).interest semantics due concept interpretation interpretationassigns world probability. Let W = {w1 , . . . , wz } z = nm(tmax+1) let W 0 =0{w10 , . . . , wy0 } = nm(t +1) . every wi W , define set worlds Wi0 ={wj0 |wj0 W 0 wj0 extension wi }. means restriction every wj0 Wi0wi . Next, letPI1 interpretation W . call interpretation I10 extension I1 i,1 z, w0 W 0 I10 (w0 ) = I1 (wi ). is, probability value I1 (wi ) distributed amongextensions wi i. also go direction. I20 interpretationP00W , define restriction W , I2 , using formula, is, I2 (wi ) = w0 W 0 I2 (w0 )i, 1 z.framework allows us investigate happens consistency inconsistencyK go 0 . start inconsistency. state next theorem two differentforms useful various contexts.Theorem 12. Let K contain time values .a) K inconsistent time , K remains inconsistent expanded 0 .b) 0 expansion K consistent 0 consistent .Proof. second part contrapositive first, suffices prove one part.prove second one. assume K, uses time values , consistentexpanded 0 . means interpretation 0 K (using W 0 ) modelK. WePobtain interpretation (for W ) 0 using formula given above, is,(wi ) = w0 W 0 0 (w0 ) i, 1 z. need show model K.Consider first atomic formulaswi , wi |= loc(id, r, t) iff w0 |=PA. Clearly, worldP0000loc(id, r, t) w Wi . Hence, w|w|=loc(id,r,t) (w) =w0 |w0 |=loc(id,r,t) (w ).takes care A.PPNext, let f F. w0 |w0 6|=f 0 (w0 ) = 0. need show also w|w6|=f (w) = 0.Let wi W world wi 6|= f . means substitution using timevalues wi 6|= (f ). remains substitutionPwhen 0 used time.wj0 Wi0 , wj0 6|= (f ). 0 model K hence F, w0 |w0 6|=f 0 (w0 ) = 0 resultfollows definition .Next consider case PST KB defined using consistent. turnsconsistency need preserved expanded 0 . Consider case followingsingle integrity constraint:X1 X2 Y1 Z1 [loc(X1 , Y1 , Z1 ) loc(X2 , Y1 , Z1 ) X1 6= X2 tmax < Z1 ] suppose|ID| > |Space|. std-formula states two different objects cannot locationtime tmax. condition tmax < Z1 false substitutions Z1 ; hence integrityconstraint automatically true. enlarged 0 (> ), say t0 = tmax + 1,779fiPARISI & G RANTinconsistency enough points Space objects occupy distinctpoints. show given consistent K always extend = [0, . . . , tmax]big = [0, . . . , tmax, . . . , tbig] K consistent big K remains consistent0 0 = [0, . . . , tmax, . . . , t0 ]. Essentially, must make sure every time variablesubstitution makes conjunct true f . example, previous examplemodifiedX1 X2 Y1 Z1 Z2 [loc(X1 , Y1 , Z1 ) loc(X2 , Y1 , Z2 ) X1 6= X2 tmax < Z1 Z2 = Z1 + 4]must enough time values include tmax + 5. show systematicallyproof next theorem.Theorem 13. consistent PST KB K defined time values = [0, . . . , tmax], always finite time value tbig tmax computed linear time, K consistentusing big = [0, . . . , tmax, . . . , tbig] K consistent 0 = [0, . . . , tmax, . . . , t0 ].Proof. obtain time value tf integrity constraint f F. f initializetf = tmax + 1. f contains one time variable Z done f . suppose fleast one conjunct two distinct time variables. must one following fourforms (where +0 omitted): 1) Zi + = Zj + n, 2) Zi + 6= Zj + n, 3) Zi + < Zj + n,4) Zi + Zj + n. process involves adding certain number tf conjunct.type 1) add |m n|, type 2) add 1, type 3) add max(0, n + 1), type 4) addmax(0, n m). Then, adding numbers conjuncts obtain tf valuef F. Let tbig = maxf F {tf }. process linear size F.must show K consistent big consistent 0 = [0, . . . , tmax,. . . , t0 ]. t0 tbig consistency follows Theorem 12b). assume tbig t0K consistent big . means interpretation big model K.proof Theorem 12, issue concerns constraints F. write W bigworlds using big write wb arbitrary world W big . f F,P set big(wb ) = 0. obtain 0 big arbitrary way long formulabbPw |w 6|=f 0 0(w ) = big (wib ) satisfied. show 0 model K 0 , needw0 Wi0Pshow w0 |w0 6|=f (w0 ) = 0. let wi0 W 0 wi0 6|= f . substitution0 wi0 6|= 0 (f ). 0 may include time values 0 . construction assuresalready substitution b using values big world wibrestriction wi0 big wib 6|= b (f ). Hence 0 model K 0 .results allow us consider case arbitrarily large (finite) time.Definition 19. call K eventually consistent (resp. inconsistent) time integerL K consistent (resp. inconsistent) = [0, . . . , L, . . . , ].Corollary 9. Every K either eventually consistent time eventually inconsistent time.Proof. K inconsistent time Theorem 12a) eventually inconsistent time.K consistent time two cases. K still consistent time bigcomputed proof Theorem 13 eventually consistent time. Otherwiseconclude Theorem 12a) eventually inconsistent time.Consider results used query answering expand time= [0, . . . , tmax] 0 = [0, . . . , tmax, . . . , t0 ]. order avoid confusion write K780fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASEStime K0 time 0 . First, K inconsistent, according Theorem 12a),K0 also inconsistent. consider case K consistent (recall queries evaluatedconsistent PST KBs). Note check pair hid, ti answer Qexpansion 0 , considers also time values tmax < t0 . Using Theorem 10obtain:hid, ti optimistic answer Q w.r.t. K0 iff hA {loc(id, q, t)[`, u]}, Fi consistentusing 0 . need check separately K0 consistent K0 hA{loc(id, q, t)[`, u]}, Fi.check cautious answer, first check K0 consistent, so,hid, ti cautious answer Q w.r.t. K0 iff hA {loc(id, q, t)[0, ` ]}, Fi hA{loc(id, q, t)[u + , 1]}, Fi consistent using 0 , given Theorem 10.similar vein, Theorem 11 carries also case extended time.5.2 Extending Time Frequent Time Valuessecond type time extension consider frequent time values,tmax + 1 time values 0 tmax. illustration, use 0 = [0, 0.5, 1, 1.5,. . . , tmax]. Again, syntax must changed well include new constants 0.5, 1.5,on; however, original K defined using = [0, 1, . . . , tmax] contain newconstant. situation worlds similar happened previous subsection: everyw : ID Space unique restriction many worlds, ws, w0 : ID 0 Space. Also,satisfaction worlds W 0 using 0 defined way worlds W using .first result type time extension previous one.Theorem 14. Let K contain time values .a) K inconsistent time , K remains inconsistent expanded 0 .b) 0 expansion K consistent 0 consistent .Proof. Analogous proof Theorem 12.Next consider case K consistent (using ). Again, consistency needpreserved going 0 . Consider case following std-formula:Y1 Z1 [loc(id1 , Y1 , Z1 ) Z1 > 0 Z1 < 1).formula states id1 cannot location 0 1. condition 0 < Z1 Z1 <1 false substitutions , hence integrity constraint true. expand 0substitution 0.5 Z1 makes formula state id1 cannot location time 0.5,making inconsistent. However, extend frequent time values f r Kconsistent f r consistent subdivision original time values.Theorem 15. consistent PST KB K defined time values = [0, . . . , tmax],always integer value computed linear time, K consistent usingf r = [0, 1 , 2 , . . . , 1, +1, . . . , tmax] K remains consistent every subdivision .781fiPARISI & G RANTProof. proof analogous proof Theorem 13. calculate integer fintegrity constraint f F let = maxf F {f }. idea make sureenough time values allow (Z) become true subdivision time intervalsbecome true substitution. worst case every time variable may require new subdivisiontime values. Hence choose f = 2k k number time variables f .rest proof analogous proof Theorem 13.previous section wanted consider arbitrarily large time values,consider dividing time intervals arbitrarily many times.Definition 20. call K divisionally consistent (resp. inconsistent) time integerL K consistent (resp. inconsistent) = [0, 1 , 2 , . . . , tmax] > L.Corollary 10. Every K either divisionally consistent time divisionally inconsistent time.Proof. Analogous proof Corollary 9.Results analogous time expansion done Section 5.1 hold also query answeringcomplexity.5.3 Space Enlargementsubsection consider happens consistency inconsistency K Spaceenlarged, say Space = {p1 , . . . , pn } Space0 = {p1 , . . . , pn , . . . , pv }. changesemantics different case expanded . w : ID Space world(for Space), remains world Space0 . writing W set worlds using SpaceW 0 set worlds using Space0 , obtain W W 0 . change definitionsatisfaction, number interpretations becomes greatly enlarged. Still, every interpretationusing Space extended unique 0 using Space0 assigning 0 (w0 ) = 0 w0W0 \ W.case time, start case K inconsistent. unlike time,Space enlarged K may become consistent. Consider simple example K consistssingle atom: = loc(id1 , {p1 , . . . , pn }, 1)[.2, .7]. K inconsistent. add single pointSpace Space0 = {p1 , . . . , pn , pn+1 } K becomes consistent: instance w10 worldw10 (id1 , 1) = p1 values arbitrary, w20 world w20 (id1 , 1) =pn+1 values arbitrary, assigning 0 (w10 ) = 0.5 0 (w20 ) = 0.5 0 (w0 ) =0 worlds makes 0 model. similar situation may occur integrity constraints.Consider K contains single std-formula: f : X1 Y1 Z1 [loc(X1 , Y1 , Z1 )Y1 ov {p1 , . . . , pn }]Since every region overlaps Space, f , itself, inconsistent. again, enlarge Spaceone point pn+1 Space0 find model 0 follows. Let 0 (w0 ) = 0 every world0w0 w0 (id, t)PSpace for0 hid, ti-pair, (w0 ) = 1 w0 (id, t) = pn+1hid, ti pairs. w0 |w0 6|=f (w) = 0; hence inconsistency removed. However, supposeaddition f , K also contains atom = loc(id1 , {p1 }, 0)[1, 1] A, is, = {a}F = {f }. PST atom states id1 must p1 time 0. case cannot makeK consistent adding number points Space f conflict. Hencegeneral statement happens inconsistent PST KB Space extendedSpace0 .782fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESNext consider case K consistent (using Space). case showremains consistent Space expanded Space0 .Theorem 16. K consistent Space, K remains consistent Space enlargedSpace0 .Proof. Let model K (using Space). assigns probability (w) world w.Space extended Space0 , many new worlds added. define 0 (w) = (w) wworld Space used 0 (w) = 0 otherwise. Thus basically excludingworlds using points Space sense given probability 0, 0 modelK using Space0 . Hence consistency preserved.Again, case time, interested happens arbitrarily large finitespaces.Definition 21. call K eventually consistent (resp. inconsistent) space integerL K consistent (resp. inconsistent) Space = {p1 , . . . , pn , . . . , pL , . . . , p }.Corollary 11. Every K either eventually consistent space eventually inconsistent space.Proof. two cases. start first case K consistent Space. Then,Theorem 16 remains consistent larger Space0 . Hence eventually consistentspace. Consider second case K inconsistent Space. two subcases.First, suppose K becomes consistent larger Space0 . Then, first case,Theorem 16 K eventually consistent space. second subcase K never becomesconsistent matter Space extended Space0 . means K eventually inconsistentspace.showed earlier examples inconsistent K (with Space) may become consistentSpace enlarged. calculate bound size needed spacebound reached consistency inconsistency change spatial enlargement.Theorem 17. every inconsistent K using Space = {p1 , . . . , pn }, explicit bound Ltractable compute K using Space0 = {p1 , . . . , pL } inconsistent, remainsinconsistent enlargement Space0 .Proof. already know corollary L exists. showcompute it. First all, enlargement Space resolves inconsistency K Keventually inconsistent space choose L = n. Thus need deal detailcase enlargement Space resolves inconsistency K, is, Keventually consistent space. consider adding points Space make K consistent.three cases:1. inconsistency due alone,2. inconsistency due combination elements F,3. inconsistency due F only.783fiPARISI & G RANTConsider first Case 1 inconsistency must due atoms fixed idvalues. assume dealing atoms using specific pair values: idt. Adding point points Space resolve inconsistency atoms using idgive Space probability less 1. instance, let = {loc(id, r1 , t, [0, 0.4]), loc(id, r2 , t, [0.1,0.3])} r1 r2 = Space. Then, enlarging Space Space0 = Space {pn+1 } resolvesinconsistency. true general, example, pn+1 addedSpace, {p1 , . . . , pn } becomes proper subset relevant Space0 may consistentlyprobability less 1. Now, consider may inconsistencies involving multiple pairsid values. matter addition single point pn+1 Spaceresolves inconsistencies. Case 1 choose L = n + 1.Case 2 inconsistency K due combination elements F. everyatom refers specific region r specific object id time value t, inconsistencyoccur f F, substitution f must act atom (with probability interval[0, 0]). instance, let = loc(id, r1 , t, [0, 0.4]) f = X1 [loc(X1 , r2 , t)] r1 r2 =Space. instance f causes inconsistency [loc(id, r2 , t)] effectsemantics atom loc(id, r2 , t, [0, 0]). true general, example.way Case 2 reduces Case 1 choose L = n + 1.last case, Case 3, inconsistency due F. problem casedue fact F requires points Space. need considerexpressed number spatial points std-formulas. cannot done writing manyspatial variables formula f spatial variables, say Y1 , . . . , Ym , wayexpress must refer different points. However, express Spaceenough points writing f0 = X1 Y1 Z1 [loc(X1 , Y1 , Z1 ) Y1 ov Space]. Again, addingsingle point Space resolves inconsistency. also write std-formulas require leastcertain number points. express constraint point occupiedone object one time using following 3 std-formulas:f1 = X1 Y1 Z1 Z2 [loc(X1 , Y1 , Z1 ) loc(X1 , Y1 , Z2 ) Z1 6= Z2 ]f2 = X1 X2 Y1 Z1 [loc(X1 , Y1 , Z1 ) loc(X2 , Y1 , Z1 ) X1 6= X2 ]f3 = X1 X2 Y1 Z1 Z2 [loc(X1 , Y1 , Z1 ) loc(X2 , Y1 , Z2 ) X1 6= X2 Z1 6= Z2 ]f1 states object point two different times; f2 states timetwo objects point; f3 states two different objects cannot pointtwo different times. Hence using std-formulas reference number objects timesnumber time points. Let L = n + |ID| (tmax + 1). Thus Space0 = {p1 , . . . , pL } consistspoint object time point addition original n points Space,place objects Space0 even none points Space used objectneeds new point time point. Clearly, L > n + 1, L works Cases 1 2well. value L choose tractable explicit bound. Thus K usingSpace0 = {p1 , . . . , pL } still inconsistent, points added Space0 make consistent.5.4 Extending Number Objects Several Entitieslast case number constants may increased ID. Since world wfunction w : ID Space, expansion objects similar expansion time,784fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESexpansion space. extension restriction interpretations going IDlarger ID0 analogous addition time values . Without getting detailsanalogous cases time given earlier, state K inconsistent ID, remainsinconsistent ID expanded ID0 . show consistency need preserved, consider f =X1 X2 . . . Xm+1 [loc(X1 , Y1 , Z1 )loc(X2 , Y2 , Z2 ). . .loc(Xm+1 , Ym+1 , Zm+1 )X1 6= X2X1 6= X3 . . . Xi 6= Xj . . . Xm 6= Xm+1 ] 6= j. Recalling ID = {id1 , . . . , idm }find K = {f } consistent ID states cannot distinctobjects. becomes inconsistent ID enlarged ID0 = {id1 , . . . , idm , idm+1 }.always find q linear time K consistent ID0 = {id1 , . . . , idm , . . . , idq }K consistent enlargement ID0 . Obtaining value q easier obtainingtbig time. matter counting number X variables f Ftaking maximum value, bigger m. Hence define eventually consistent (resp.inconsistent) objects way time, obtain result every K eithereventually consistent objects eventually inconsistent objects.far considered individual enlargements either time values points spaceobjects. may interested also combining several types extensions. expansiontime objects similar, start combining them.Definition 22. call K using ID, , Space eventually consistent (resp. inconsistent)objects general time integers L1 , L2 , L3 K consistent (resp.inconsistent) ID = {id1 , . . . , idm , . . . , idL1 , . . . , id }, Space = {p1 , . . . , pn }, =tmax+1[0, 1 , 2 , . . . , 1, +1, . . . , . . . , ] > L2 > L3 ., . . . tmax,combination expansion objects time magnitude divisionally worksessentially way expansion one items. is, setworlds original K W set worlds expansion ID W 0 ,every world W set extensions W 0 every world W 0unique world W restriction. key issue Space remains unchanged.let K contain time values , regions Space objects ID. Supposeexpanded 0 expansion may magnitude division ID expandedID0 find K, context 0 ID0 Space unchanged, consistent.proof Theorem 12 start model 0 K using 0 ID0 obtain correspondingmodel K using starting ID. Thus, contrapositive, K inconsistent, Space ID, remains inconsistent 0 , Space ID0 . shows inconsistentK eventually inconsistent objects general time. case K consistent , SpaceID becomes inconsistent expansion 0 , Space ID0 , eventuallyinconsistent objects general time. alternative K remain consistentmatter ID expanded. Hence earlier results put together followingresult.Theorem 18. Every PST KB K either eventually consistent objects general timeeventually inconsistent objects general time.However, situation different cases combine space time objectsshow using example.1212. example suggested one reviewers.785fiPARISI & G RANTExample 25. Consider PST KB K = h, {f }i defined using ID = {id1 , . . . , idm }, =[0, 1, . . . , tmax], Space = {p1 , . . . , pn } n f std-formula f2 usedproof Theorem 17 states two different objects cannot pointtime. condition n, K consistent. Expand ID ID0 ={id1 , . . . , idm , . . . , idn+1 } leaving Space unchanged call new KB K0 .many objects number points; hence K0 inconsistent. Next, expand SpaceSpace00 = {p1 , . . . , pn , pn+1 } leaving ID0 unchanged call new KB K00 . K00consistent.2example shows cannot claim eventual (in)consistency number objectsnumber points may increase continue process indefinitely. analogous result holds case number time values number points may increase.6. Related Workfirst discuss related works classical probabilistic logic explicit spatialtemporal components. discuss relationship work spatio-temporalapproaches. Finally, relate framework object tracking.6.1 Probabilistic Logicdiscussed Section 3, PST KB expressed classical propositional logic (Hailperin,1984; Nilsson, 1986; Paris, 1994), particular consistency checking problemformulated terms Probabilistic Satisfiability (PSAT), whose first formulation attributedBoole (1854). presentation AI community Nilsson (1986), study PSATpoint view efficient algorithms computational complexity first addressedGeorgakopoulos et al. (1988), showed PSAT NP NP-hard even binaryclauses. tractable results identified Georgakopoulos et al. concern special caseclause involves two literals (2PSAT) graph clauses outerplanar,13graph clauses contains vertex literal two kinds edges: i) edgepair literals built propositional variable, (ii) edge pairliterals appearing clause. note PSAT formula K encoding PST KB K(see proof Theorem 1) contains two literals per clause even focus binarystd-formulas (K becomes 2PSAT formula assume Space consists twopoints).result provided Georgakopoulos et al. relies reducing 2PSAT tractable instance2MAXSAT (weighted maximum satisfiability problem two literals per clause). Using reduction result Conforti Cornuejols (1992) tractability problemsformulated integer program whose matrix balanced, following general resultprovided Conforti Cornuejols: PSAT tractable balanced set clauses, is,set clauses whose corresponding {0, 1} clause-variable matrix balanced. However,result also doesnt help finding tractable cases PST KBs K reduction PSAT K ,considering three points Space suffices make matrix corresponding K balanced (itentails presence odd cycle graph clauses that, observed Andersen & Pre13. graph said outerplanar embedded plane vertices lie face.is, drawn plane without crossings way vertex totally surrounded edges.786fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASEStolani, 2001, characterizes non-balanced matrices). Motivated fact tractable casesPSAT identified Georgakopoulos et al. Conforti Cornuejols rely using polynomialtime algorithm whose complexity characterized high polynomial degree (specifically,complexity O(n6 log n) n number propositional variables), Andersen Pretolani identified efficient algorithms classes balanced sets clausesrepresented either hypertrees (where hyperarc corresponds set literals clause)co-occurrence graph partial k-tree (Bodlaender, 1998).worth noting none tractable cases identified paper derivedresults PSAT described reducing K PSAT formula K . fact, tractablecases derive specific structure PST KBs. hand, tractability resultsentail tractability PSAT instances reduced tractable instance consistencychecking problem. particular, decide polynomial time PSAT instances Kstructure specified reduction consistency checking problem PSAT,consistency checking problem corresponding PST KB K turns tractable.principle, fact reduction consistency checking problem PSATenables well-known techniques solving (general) instances PSAT based column generation (Kavvadias & Papadimitriou, 1990; Jaumard et al., 1991) used addressing consistency PST KBs. holds problem answering selection queries PSTKBs that, shown Section 4, addressed solving suitable instances consistencychecking problem. Recent approaches solve PSAT using SAT (Finger & Bona, 2011) IntegerLinear Programming (Cozman & di Ianni, 2015) column selection report experiments showingphase transition behaviour (first observed Finger Bona PSAT) depending fraction number clauses propositional variables well number probabilityassignments. Using techniques PSAT instances hundreds propositional variablesclauses solved reasonable time. However, believe reducing consistency checking (or query answering) problem PSAT applying techniques wouldsuccessful approach, number propositional variables clauses wouldgenerated would huge even small-size PST KBs. avoid problem, conjecturespecific structure (PSAT formulas encoding) PST KBs could exploited deviseefficient techniques solving consistency checking query answering problems.regard, would interesting investigate connection framework emerging field lifted probabilistic inference (Kersting, 2012), structure FOL-constructs(such indistinguishable individuals) exploited speed reasoning process, seeresults carry PST KBs.use integrity constraints encode domain knowledge studied Lukasiewicz(1999, 2001) Flesca, Furfaro, Parisi (2014), probabilistic frameworks howeverexplicitly deal space time. problem probabilistic deduction presenceconditional constraints basic events addressed Lukasiewicz (1999), identifiedtractable instances probabilistic KBs, whose conditional constraints define conditional constrainttrees, support deduction paths premise/conclusions basic events. problem checkingconsistency relational probabilistic databases (where tuples viewed basic events)presence denial constraints addressed Flesca et al., provided tractability results constraints whose conflict hypergraph (Chomicki, Marcinkowski, & Staworko, 2004) acyclic (Fagin, 1983) well special kind cyclic hypergraphs, encode neitherclique-acyclic std-graphs simple clique-cyclic std-graphs. important probabilistic logic pro787fiPARISI & G RANTgramming approach conditional constraints proposed Lukasiewicz (2001), studiedcomplexity satisfiability entailment problems several types formulaswithout identifying tractable cases. Differently above-cited papers, atomic informationframework structure involving objects, space, time, thus atoms may alsointrinsically related object, space, time value.6.2 Spatio-Temporal ApproachesSubstantial work done spatio-temporal logics (Gabelaia et al., 2005; Knapp et al.,2006) combine spatial temporal formalisms. includes important contributionsqualitative spatio-temporal representation reasoning (Muller, 1998; Wolter & Zakharyaschev,2000; Cohn & Hazarika, 2001), focus describing entities qualitative relationshipsdealing discrete time. Cohn, Li, Liu, Renz (2014) provided upto-date overview work done field qualitative spatial reasoning, recentlyimportant problem combining topological directional information extended spatial objectsaddressed. However, works intended reasoning moving objectswhose location given time uncertain (they put probabilities mix). Yamanet al. (2004, 2005a, 2005b) focused spatio-temporal logical theories describe known plansmoving objects sets go atoms, stating object go locationL1 L2 , leaving L1 reaching L2 time intervals, travelling speedgiven interval. Later, Parker et al. (2007b) extended logic include probabilisticinformation plans. SPOT framework Parker et al. (2007a) extendedwork uncertainty objects might given time.Past work SPOT framework investigated efficient algorithms computing optimisticcautious answers selection queries (Parker et al., 2009; Parisi et al., 2010). initial SPOTframework build adding integrity constraints implemented testedreal US Navy databases containing ship location data (Parker et al., 2009; Parisi et al., 2010). Aggregate queries recently investigated Grant et al. (2013), proposed three semanticsalong computational methods evaluating them. SPOT databases provide informationmoving objects, one important aspect addressed Parker et al. (2008) investigated Grant et al. (2010) revising SPOT data information objects maychanged objects move. Grant et al. (2010) proposed several strategies revising SPOT datafinding maximal consistent subsets, minimally modifying spatial, temporal, object,probability components PST atoms. full logic including negation, disjunction quantifiersmanaging SPOT data recently proposed Doder et al. (2013), focused findingsound complete sets axioms several fragments logic. Grant, Parisi, Subrahmanian (2013) provided comprehensive survey SPOT framework related researchalso reviewed.much work spatio-temporal databases (Agarwal et al., 2003; Pelanis et al.,2006) probabilistic spatio-temporal databases (Tao et al., 2005; Zhang, Chen, Jensen, Ooi, &Zhang, 2009; Zheng, Trajcevski, Zhou, & Scheuermann, 2011), works mainly focus devising indexing mechanisms scaling query computation, instead representing knowledgedeclarative fashion. particular, Chung, Lee, Chen (2009) use indexing speed computation range queries derive PDF location object moving one-dimensionalspace using past moving behavior moving velocity distribution. Zhang et al. (2009)788fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESprovide B x -tree index variant B + -tree applicable moving objects whose locationvelocity uncertain. Two types pruning introduced Yang, Lu, Jensen (2010)efficiently solve queries asking sets k objects least threshold probabilitycontaining k nearest objects given object. Dealing similar problem, Chen, Qin,Liu (2010) propose TPR-tree indexing. Finally, Zheng et al. (2011) deal primarilyobjects moving along road networks, introduce indexing mechanism efficiently processing probabilistic range queries. However, none works systematically addresses issueconsidering integrity constraints probabilistic spatio-temporal data.6.3 Object TrackingObject tracking one important problems computer vision (Szeliski, 2010)consecutive positions tracked object estimated moves different framesvideo. Numerous approaches object tracking proposed, mainly differingtype object representation used (e.g., centroid, primitive geometric shapes), imagefeatures selected (e.g., colour, optical flow), object detection method adopted (e.g., backgroundsubtraction, segmentation). However, tracking algorithm chosen given applicationstrongly depends application domain (Yilmaz, Javed, & Shah, 2006). Moreover, objecttracking algorithms may incur errors, due instance loss information caused projection3D world 2D image, noise images, partial full object occlusions, estimationposition tracked moving objects inherently uncertain (even camera focuses fixed,specific area). Several important statistical methods object tracking computer vision (e.g., seeBroida & Chellappa, 1986; Beymer & Konolige, 1999; Rosales & Sclaroff, 1999) basedwell-known Kalman filter (1960) extensions deal non-linear case, wellparticle filtering (Kitagawa, 1987).Filtering techniques extensively used object tracking presence sensors,cameras. matter fact, object tracking extensively addressedgeneral setting position (i.e., state) one objects estimated recursiveBayesian filter given measurements time coming different kinds sensors(including, instance, radar, sonar, infrared, types sensors possibly along visualsensors) (Stone, Corwin, & Barlow, 1999). Basically, observed time point t, outputfilter probability distribution (i.e., posterior) position target object,computed combining motion updated time prior likelihoodobservation received time t, likelihood represents probability sensormeasurement conditioned object position (Bar-Shalom, Kirubarajan, & Li, 2002).Kalman filter used discrete-time estimation continuous spatial positionsobjects whose movement equations assumed linear Gaussian noise. alsoused successfully non-linear systems applying linearization unscented transformation (Julier, Jeffrey, & Uhlmann, 2004). discrete space non-linear systems, particle filteringsuccessfully used, providing solution applied state-space modelgeneralizes traditional Kalman filtering methods (Arulampalam, Maskell, Gordon, &Clapp, 2002). general framework particle filtering based Sequential Importance Sampling Resampling proposed Liu Chen (1998), though number different typesparticle filters exist shown outperform others used particularapplications (Arulampalam et al., 2002).789fiPARISI & G RANTDifferently Kalman filtering, estimated position object observedtime point represented continuous distribution, particle filters based histogram representation probability density, approximated finite number particles (i.e.,samples): particle represents position space, weights associated particles (orproportion number particles) define histogram probability distribution space.fits representation paradigm PST KBs: PST KBs allow us represent,object time point, PDF Space defining PST atom single valued probabilityinterval (that is, ` = u) point Space, used easily represent output object tracking techniques based particle filtering. filtering techniques returning continuousdistribution Space, discretization step applied.output object tracking techniques represented using PST KBs,important aspects techniques deal PST KBs cannot do. particular, filtering techniques use conditional independence represent PDF objects positions conditionalpositions previous time. PST KBs encode output inference process,lack expressive power kind inference. instance, tracking techniquesrepresent knowledge object region r1 time t1 probably regionr2 time t2 probability depending time elapsed t1 t2 . Indeed trackersrely motion model according distribution objects location spreadselapsed time since last measurement: distribution objects locationsnarrowly focused locations near measured position t1 t2 close t1 , diffuset2 faraway t1 . tracking techniques sort things quite naturally, PST KBscapture aspects behavior. instance, express fact objectprobably region r1 t1 , use integrity constraint imposing traveldistance units 1 time point. would increase probability finding objecttime t2 region less units away r1 , would decrease probability findingobject region farther away units. However, different inferredtracking techniques using conditional independence.hand, tracking techniques combine well interval probabilities.fact typically return PDF objects position observed time point. contrast, using general PST atoms (with probability intervals), object time point,(possibly infinite number) PDFs compatible probability intervals specifiedatoms succinctly represented. instance, assuming Space = {p1 , p2 } PST atomloc(id, {p1 }, t)[0, 0.5], PDFs f Space assigning probability f (p1 ) [0, 0.5]f (p2 ) = 1 f (p1 ) represented. However, set PDFs represented restricted usingsingle valued probability intervals adding integrity constraints using std-formulas.note PST formalism allows us impose integrity constraints KBsobtained integrating position data coming different sources. Consider instanceintegration several PST KBs, consisting PST atoms encoding outputautonomous tracking system. so-obtained integrated KB still PST KB, integrityconstraints used express knowledge overall system, could expressedconsidering tracking systems separately. instance, suppose integrated PST KBconsisting position data monitored cars collected using black-box tracking systemsinstalled cars insurance companies. std-formulas used express correlationsamong monitored cars. instance, knowing region r licensed inspection stationable inspect k cars time, impose constraint cannot790fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESk cars r time. constraint would meaningless considered KBcar tracking system separately, useful restricting set consistent interpretationsPST KB obtained integrating several sources.7. Summarybelieve first comprehensive paper focuses systematically knowledge representation form integrity constraints probabilistic spatio-temporal data.14 knowledgerepresented form spatio-temporal atoms describing location objects timeprobability interval well spatio-temporal denial formulas describing integrity constraints system must satisfy. Within framework investigated consistency checkingproblem well problem answering selection queries consistent PST KBs. Althoughproblems turned hard general case, devised several sets linearinequalities allow us decide consistency well answer queries checking feasibility. addition, identified different classes spatio-temporal denial formulaschecking consistency answering queries tractable. Finally, discussed extensionframework arbitrarily large finite numbers objects, time values, points spaceshowed behavior consistency inconsistency uniform.8. Future Work Conclusionissues investigated. Following Parisi Grant (2014b),studied problem restoring consistency PST KBs form KhA, (where setstd-formulas empty), consider problem repairing inconsistent PST KBinconsistency due presence std-formulas satisfied. regard wouldinteresting devise methods answering queries inconsistent PST KBs. Recentlyresearch probabilistic reasoning inconsistency (Picado-Muino, 2011; Thimm,2013; Potyka & Thimm, 2014) help regard. would also interesting lookpossibility semantic query optimization PST KBs, study use previousknowledge efficiently check consistency process queries updates.Another direction future work investigation probabilistic std-formulas expressingconstraints hold probability given interval. Intuitively, would allow us stateinstance two objects region time probability greatergiven threshold, instead stating cannot situation. kindprobabilistic constraint could expressed using pstd-formulas form f [`, 1] fstd-formula (i.e., f [1, 1] captures meaning std-formula). change semanticsmeans changingdefinition model. second part Definition 6 modifiedPf F, w|w|=f (w) [`, 1]. Clearly, lower bounds complexity consistency checkingquery answering problems still hold extension. easy check upper boundprovided Theorem 1 holds since reduction PSAT still provided mapping pstdformulas clauses associated probability interval. regards tractable cases, conjectureresults Theorems 6 8 still hold pstd-formulas form f [`, 1] right-handside inequality (4) Definitions 15 17, respectively, replaced lower probabilitybound ` ground pstd-formula f [`, 1] generating inequality. However, allowing general14. substantially revised expanded version work Parisi Grant (2014a).791fiPARISI & G RANTprobability intervals associated std-formulas introduces new issues semantics: f [`, u]would entail f [1 u, 1 `] holds. Providing clear intuitive semantics kindstd-formula, well probabilistic std-formulas allowing probability intervals associatedconjunct (instead whole formula), deferred future work.PST formalism propositional even though atoms substantial content.added std-formulas integrity constraints; special class first-order logic formulas. would interesting consider works general first-order probabilistic logics (Halpern, 1990; Lukasiewicz & Kern-Isberner, 1999; Kern-Isberner & Thimm, 2010).logics developed different purpose attempt could made represent spatiotemporal information them. works well could enhanced spatiotemporal information, Markov Logic (Richardson & Domingos, 2006), Bayesian LogicPrograms (Kersting & Raedt, 2007). particular, Milch et al. (2005) introduces first-order language called BLOG (Bayesian LOGic) defining probability models worlds unknownobjects identity uncertainty, finds natural application object tracking unknownobjects. may possible find generalization PST formalism includesconcepts. so, aspect need take care fact Markov logicBayesian logic programs deal unique probability distribution, dealprobability distributions compatible PST atoms std-formulas.Researchers AI studying spatial temporal reasoning many years (Allen,1984; Randell, Cui, & Cohn, 1992; Galton, 2009). interesting project incorporationconcepts PST framework. new syntax semantics include adding ruleslanguage. additions allow adding types information well newintegrity constraints. instance, using concepts qualitative direction orientation proposed spatial reasoning (Galton, 2009) would allow us explicitly represent knowledgeregion toward object moving. Another important concept needed many applications explicit representation qualitative quantitative distance objects wellinformation speed (i.e., maximum average) objects. Additional structured information objects type (e.g., vessel, vehicle, person, etc.) would general usefulexhaustively reason moving objects. However, depending addition made increaseexpressive power extended framework, important consequences complexityconsistency checking problem may arise. Spatial temporal aspects formalisms qualitativespatial temporal reasoning expressive framework (Gabelaiaet al., 2005; Knapp et al., 2006). trade-off expressiveness complexity withinhierarchy formalisms obtained combining well-known spatial temporal logics analyzedGabelaia et al. (2005), shown complexity satisfiability problemspatio-temporal logics (not dealing probabilities) vary NP-complete undecidable.Using formalisms PST framework may drastically increase computational complexity problems studied paper. Nevertheless, believe attemptlater made include even simpler concepts qualitative spatio-temporal reasoning PSTframework, particularly trying exploit restrictions recently studied Huang, Li, Renz (2013)identify tractable fragments.paper, proposed framework four features moving objects takenaccount: spatial component, temporal component, inherent uncertainty acquired data,integrity constraints application domain. expressiveness features couldimproved represent additional knowledge may interest practical applications, partic792fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESularly using ideas papers dealing spatio-temporal reasoning. endeavor concreteimplementation framework, ideas PostGIS spatial database system extendsPostgreSQL would useful. believe worthwhile later incorporate conceptsPST framework. took first step extending PST framework additionintegrity constraints, hope researchers use work starting point investigationsimportant role integrity constraints probabilistic spatio-temporal knowledge bases.Acknowledgmentswish thank referees numerous helpful comments helped us substantially improving paper.ReferencesAgarwal, P. K., Arge, L., & Erickson, J. (2003). Indexing moving points. J. Comput. Syst. Sci.,66(1), 207243.Ahson, S. A., & Ilyas, M. (2010). Location-Based Services Handbook: Applications, Technologies,Security. CRC Press, Hoboken, NJ.Akdere, M., Cetintemel, U., Riondato, M., Upfal, E., & Zdonik, S. B. (2011). case predictive database systems: Opportunities challenges. Proceedings 5th BiennialConference Innovative Data Systems Research (CIDR), pp. 167174.Allen, J. F. (1984). Towards general theory action time. Artif. Intell., 23(2), 123154.Andersen, K. A., & Pretolani, D. (2001). Easy cases probabilistic satisfiability. Ann. Math. Artif.Intell., 33(1), 6991.Arulampalam, M. S., Maskell, S., Gordon, N. J., & Clapp, T. (2002). tutorial particle filtersonline nonlinear/non-gaussian bayesian tracking. IEEE Transactions Signal Processing,50(2), 174188.Bar-Shalom, Y., Kirubarajan, T., & Li, X.-R. (2002). Estimation Applications TrackingNavigation. John Wiley & Sons, Inc., New York, NY, USA.Bayir, M. A., Demirbas, M., & Eagle, N. (2010). Mobility profiler: framework discoveringmobility profiles cell phone users. Pervasive Mobile Computing, 6(4), 435 454.Beymer, D., & Konolige, K. (1999). Real-time tracking multiple people using continuous detection. Proceedings Workshop Frame-rate Applications, Methods ExperiencesRegularly Available Technology Equipment (FRAME-RATE), conjunction7th IEEE International Conference Computer Vision (ICCV).Bodlaender, H. L. (1998). partial k-arboretum graphs bounded treewidth. Theor. Comput.Sci., 209(1-2), 145.Boole, G. (1854). Investigation Laws Thought Founded MathematicalTheories Logic Probabilities. Macmillan, London.Broida, T. J., & Chellappa, R. (1986). Estimation object motion parameters noisy images.IEEE Trans. Pattern Anal. Mach. Intell., 8(1), 9099.793fiPARISI & G RANTChen, Y.-F., Qin, X.-L., & Liu, L. (2010). Uncertain distance-based range queries uncertainmoving objects. J. Comput. Sci. Technol., 25(5), 982998.Chomicki, J., Marcinkowski, J., & Staworko, S. (2004). Computing consistent query answers usingconflict hypergraphs. Proceedings 2004 ACM CIKM International ConferenceInformation Knowledge Management (CIKM), pp. 417426.Chung, B. S. E., Lee, W.-C., & Chen, A. L. P. (2009). Processing probabilistic spatio-temporalrange queries moving objects uncertainty. Proceedings 12th InternationalConference Extending Database Technology (EDBT), pp. 6071.Cohn, A. G., & Hazarika, S. M. (2001). Qualitative spatial representation reasoning:overview. Fundam. Inform., 46(1-2), 129.Cohn, A. G., Li, S., Liu, W., & Renz, J. (2014). Reasoning topological cardinal directionrelations 2-dimensional spatial objects. J. Artif. Intell. Res. (JAIR), 51, 493532.Conforti, M., & Cornuejols, G. (1992). class logic problems solvable linear programming.Proceedings 33rd Annual Symposium Foundations Computer Science (FOCS),pp. 670675.Cozman, F. G., & di Ianni, L. F. (2015). Probabilistic satisfiability coherence checkinginteger programming. Int. J. Approx. Reasoning, 58, 5770.Doder, D., Grant, J., & Ognjanovic, Z. (2013). Probabilistic logics objects located spacetime. J. Logic Computation, 23(3), 487515.Fagin, R. (1983). Degrees acyclicity hypergraphs relational database schemes. JournalACM, 30(3).Finger, M., & Bona, G. D. (2011). Probabilistic satisfiability: Logic-based algorithms phasetransition. Proceedings 22nd International Joint Conference Artificial Intelligence(IJCAI), pp. 528533.Flesca, S., Furfaro, F., & Parisi, F. (2014). Consistency checking querying probabilisticdatabases integrity constraints. J. Comput. Syst. Sci., 80(7), 14481489.Gabelaia, D., Kontchakov, R., Kurucz, A., Wolter, F., & Zakharyaschev, M. (2005). Combiningspatial temporal logics: Expressiveness vs. complexity. J. Artif. Intell. Res., 23, 167243.Galton, A. (2009). Spatial temporal knowledge representation. Earth Science Informatics, 2(3),169187.Georgakopoulos, G. F., Kavvadias, D. J., & Papadimitriou, C. H. (1988). Probabilistic satisfiability.J. Complexity, 4(1), 111.Grant, J., Molinaro, C., & Parisi, F. (2013). Aggregate count queries probabilistic spatio-temporaldatabases. Proceedings 7th International Conference Scalable Uncertainty Management (SUM), pp. 255268.Grant, J., Parisi, F., Parker, A., & Subrahmanian, V. S. (2010). agm-style belief revision mechanism probabilistic spatio-temporal logics. Artif. Intell., 174(1), 72104.Grant, J., Parisi, F., & Subrahmanian, V. S. (2013). Research probabilistic spatiotemporaldatabases: SPOT framework. Advances Probabilistic Databases UncertainInformation Management, Vol. 304 Studies Fuzziness Soft Computing, pp. 122.Springer.794fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESHailperin, T. (1984). Probability logic. Notre Dame Journal Formal Logic, 25(3), 198212.Halpern, J. Y. (1990). analysis first-order logics probability. Artif. Intell., 46(3), 311350.Hammel, T., Rogers, T. J., & Yetso, B. (2003). Fusing live sensor data situational multimediaviews. Proceedings 9th International Workshop Multimedia Information Systems(MIS), pp. 145156.Huang, J., Li, J. J., & Renz, J. (2013). Decomposition tractability qualitative spatialtemporal reasoning. Artif. Intell., 195, 140164.Jaumard, B., Hansen, P., & de Aragao, M. P. (1991). Column generation methods probabilisticlogic. ORSA Journal Computing, 3(2), 135148.Julier, S. J., Jeffrey, & Uhlmann, K. (2004). Unscented filtering nonlinear estimation. Proceedings IEEE, 92, 401422.Junger, M., Liebling, T., Naddef, D., Nemhauser, G., Pulleyblank, W., Reinelt, G., Rinaldi, G., &Wolsey, L. (Eds.). (2010). 50 Years Integer Programming 1958-2008: Early YearsState-of-the-Art. Springer, Heidelberg.Kalman, R. E. (1960). new approach linear filtering prediction problems. TransactionsASMEJournal Basic Engineering, 82(Series D), 3545.Karbassi, A., & Barth, M. (2003). Vehicle route prediction time arrival estimation techniquesimproved transportation system management. Proceedings 2013 IEEE IntelligentVehicles Symposium, pp. 511516.Karimi, H. A. (2013). Advanced location-based technologies services. CRC Press, Hoboken,NJ.Kavvadias, D. J., & Papadimitriou, C. H. (1990). linear programming approach reasoningprobabilities. Ann. Math. Artif. Intell., 1, 189205.Kern-Isberner, G., & Thimm, M. (2010). Novel semantical approaches relational probabilisticconditionals. Proceedings 12th International Conference Principles KnowledgeRepresentation Reasoning (KR).Kersting, K., & Raedt, L. D. (2007). Bayesian logic programming: Theory tool. Getoor, L.,& Taskar, B. (Eds.), Introduction Statistical Relational Learning. MIT Press.Kersting, K. (2012). Lifted probabilistic inference. Proceedings 20th European ConferenceArtificial Intelligence (ECAI), pp. 3338.Kitagawa, G. (1987). Non-gaussian state-space modeling nonstationary time series. JournalAmerican Statistical Association, 82(400), 10321041.Knapp, A., Merz, S., Wirsing, M., & Zappe, J. (2006). Specification refinement mobilesystems mtla mobile uml. Theor. Comput. Sci., 351(2), 184202.Kurkovsky, S., & Harihar, K. (2006). Using ubiquitous computing interactive mobile marketing.Personal Ubiquitous Comput., 10(4), 227240.Li, S. Z., & Jain, A. K. (Eds.). (2011). Handbook Face Recognition, 2nd Edition. Springer.Liu, J. S., & Chen, R. (1998). Sequential monte carlo methods dynamic systems. JournalAmerican Statistical Association, 93, 10321044.795fiPARISI & G RANTLukasiewicz, T. (2001). Probabilistic logic programming conditional constraints. ACM Trans.Computational Logic, 2(3), 289339.Lukasiewicz, T. (1999). Probabilistic deduction conditional constraints basic events. J.Artif. Intell. Res. (JAIR), 10, 199241.Lukasiewicz, T., & Kern-Isberner, G. (1999). Probalilistic logic programming maximumentropy. Proceedings 5th European Conference Symbolic Quantitative Approaches Reasoning Uncertainty (ECSQARU), pp. 279292.MarketsandMarkets (2014). Location Based Services (LBS) Market (Mapping, DiscoveryInfotainment, Location Analytics, Leisure Social Networking, Location Based Advertising, Augmented Reality Gaming, Tracking) - Worldwide Forecasts Analysis (2014 - 2019). http://www.marketsandmarkets.com/Market-Reports/location-based-service-market-96994431.html.Milch, B., Marthi, B., Russell, S. J., Sontag, D., Ong, D. L., & Kolobov, A. (2005). BLOG: probabilistic models unknown objects. Proceedings 19th International Joint Conference Artificial Intelligence (IJCAI), pp. 13521359.Mittu, R., & Ross, R. (2003). Building upon coalitions agent experiment (coax) - integrationmultimedia information gccs-m using impact. Proceedings 9th InternationalWorkshop Multimedia Information Systems (MIS), pp. 3544.Muller, P. (1998). qualitative theory motion based spatio-temporal primitives. Proceedings 6th International Conference Principles Knowledge RepresentationReasoning (KR), pp. 131143.Nilsson, N. J. (1986). Probabilistic logic. Artif. Intell., 28(1), 7187.Papadimitriou, C. H., & Steiglitz, K. (1982). Combinatorial optimization: algorithms complexity. Prentice-Hall, Inc., Upper Saddle River, NJ, USA.Papadimitriou, C. M. (1994). Computational complexity. Addison-Wesley, Reading, Massachusetts.Paris, J. (1994). Uncertain Reasoners Companion: Mathematical Perspective. CambridgeUniversity Press.Parisi, F., & Grant, J. (2014a). Integrity constraints probabilistic spatio-temporal knowledgebases. Proceedings 8th International Conference Scalable Uncertainty Management (SUM), pp. 251264.Parisi, F., & Grant, J. (2014b). Repairs consistent answers inconsistent probabilistic spatiotemporal databases. Proceedings 8th International Conference Scalable Uncertainty Management (SUM), pp. 265279.Parisi, F., Parker, A., Grant, J., & Subrahmanian, V. S. (2010). Scaling cautious selection spatialprobabilistic temporal databases. Methods Handling Imperfect Spatial Information,Vol. 256 Studies Fuzziness Soft Computing, pp. 307340. Springer.Parisi, F., Sliva, A., & Subrahmanian, V. S. (2013). temporal database forecasting algebra. Int. J.Approximate Reasoning, 54(7), 827860.Parker, A., Infantes, G., Grant, J., & Subrahmanian, V. S. (2009). SPOT databases: Efficient consistency checking optimistic selection probabilistic spatial databases. IEEE TransactionsKnowledge Data Engineering (TKDE), 21(1), 92107.796fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASESParker, A., Infantes, G., Subrahmanian, V. S., & Grant, J. (2008). AGM-based belief revisionmechanism probabilistic spatio-temporal logics. Proceedings 23rd AAAI Conference Artificial Intelligence (AAAI), pp. 511516.Parker, A., Subrahmanian, V. S., & Grant, J. (2007a). logical formulation probabilistic spatialdatabases. IEEE Transactions Knowledge Data Engineering (TKDE), 19(11), 15411556.Parker, A., Yaman, F., Nau, D. S., & Subrahmanian, V. S. (2007b). Probabilistic go theories.Proceedings 20th International Joint Conference Artificial Intelligence (IJCAI), pp.501506.Pelanis, M., Saltenis, S., & Jensen, C. S. (2006). Indexing past, present, anticipated futurepositions moving objects. ACM Trans. Database Syst., 31(1), 255298.Petrova, K., & Wang, B. (2011). Location-based services deployment demand: aroadmapmodel. Electronic Commerce Research, 11(1), 529.Picado-Muino, D. (2011). Measuring repairing inconsistency probabilistic knowledge bases.Int. J. Approx. Reasoning, 52(6), 828840.Potyka, N., & Thimm, M. (2014). Consolidation probabilistic knowledge bases inconsistencyminimization. Proceedings 21st European Conference Artificial Intelligence(ECAI), pp. 729734.Randell, D. A., Cui, Z., & Cohn, A. G. (1992). spatial logic based regions connection.Proceedings 3rd International Conference Principles Knowledge RepresentationReasoning (KR), pp. 165176.Richardson, M., & Domingos, P. (2006). Markov logic networks. Mach. Learn., 62(1-2), 107136.Rosales, R., & Sclaroff, S. (1999). 3D trajectory recovery tracking multiple objects trajectoryguided recognition actions. Proceedings 6th Conference Computer VisionPattern Recognition (CVPR, pp. 21172123.Southey, F., Loh, W., & Wilkinson, D. F. (2007). Inferring complex agent motions partial trajectory observations. Proceedings 20th International Joint Conference ArtificialIntelligence (IJCAI), pp. 26312637.Stone, L. D., Corwin, T. L., & Barlow, C. A. (1999). Bayesian Multiple Target Tracking (1st edition).Artech House, Inc., Norwood, MA, USA.Szeliski, R. (2010). Computer Vision: Algorithms Applications. Springer-Verlag New York,Inc., New York, NY, USA.Tao, Y., Cheng, R., Xiao, X., Ngai, W. K., Kao, B., & Prabhakar, S. (2005). Indexing multidimensional uncertain data arbitrary probability density functions. Proceedings31st International Conference Large Data Bases (VLDB), pp. 922933.Thimm, M. (2013). Inconsistency measures probabilistic logics. Artif. Intell., 197, 124.Wolter, F., & Zakharyaschev, M. (2000). Spatio-temporal representation reasoning basedrcc-8. Proceedings 7th International Conference Principles Knowledge Representation Reasoning (KR), pp. 314.797fiPARISI & G RANTYaman, F., Nau, D. S., & Subrahmanian, V. S. (2004). logic motion. Proceedings 9thInternational Conference Principles Knowledge Representation Reasoning (KR),pp. 8594.Yaman, F., Nau, D. S., & Subrahmanian, V. S. (2005a). Going far, logically. Proceedings19th International Joint Conference Artificial Intelligence (IJCAI), pp. 615620.Yaman, F., Nau, D. S., & Subrahmanian, V. (2005b). motion closed world assumption. Proceedings 19th International Joint Conference Artificial Intelligence (IJCAI), pp.621626.Yang, B., Lu, H., & Jensen, C. S. (2010). Probabilistic threshold k nearest neighbor queriesmoving objects symbolic indoor space. Proceedings 13th International Conference Extending Database Technology (EDBT), pp. 335346.Yilmaz, A., Javed, O., & Shah, M. (2006). Object tracking: survey. ACM Comput. Surv., 38(4).Zhang, M., Chen, S., Jensen, C. S., Ooi, B. C., & Zhang, Z. (2009). Effectively indexing uncertainmoving objects predictive queries. Proceedings VLDB Endowment (PVLDB), 2(1),11981209.Zheng, K., Trajcevski, G., Zhou, X., & Scheuermann, P. (2011). Probabilistic range queriesuncertain trajectories road networks. Proceedings 14th International ConferenceExtending Database Technology (EDBT), pp. 283294.798fiJournal Artificial Intelligence Research 55 (2016) 283-316Submitted 03/24; published 01/16News Across Languages - Cross-Lingual DocumentSimilarity Event TrackingJan RupnikAndrej MuhicGregor LebanPrimoz SkrabaBlaz FortunaMarko Grobelnikjan.rupnik@ijs.siandrej.muhic@ijs.sigregor.leban@ijs.siprimoz.skraba@ijs.siblaz.fortuna@ijs.simarko.grobelnik@ijs.siArtificial Intelligence Laboratory, Jozef Stefan Institute,Jamova cesta 39, 1000 Ljubljana, SloveniaAbstracttodays world, follow news distributed globally. Significant eventsreported different sources different languages. work, addressproblem tracking events large multilingual stream. Within recently developedsystem Event Registry examine two aspects problem: compare articlesdifferent languages link collections articles different languages referevent. Taking multilingual stream clusters articles language,compare different cross-lingual document similarity measures based Wikipedia.allows us compute similarity two articles regardless language. Buildingprevious work, show methods scale well compute meaningfulsimilarity articles languages little direct overlap trainingdata. Using capability, propose approach link clusters articles acrosslanguages represent event. provide extensive evaluationsystem whole, well evaluation quality robustness similaritymeasure linking algorithm.1. IntroductionContent Internet becoming increasingly multilingual. prime example Wikipedia. 2001, majority pages written English, 2015, percentageEnglish articles dropped 14%. time, online news begun dominate reporting current events. However, machine translation remains relatively rudimentary. allows people understand simple phrases web pages, remains inadequateadvanced understanding text. paper consider intersectiondevelopments: track events reported multiple languages.term event vague ambiguous, practical purposes, definesignificant happening reported media. Examples eventswould include shooting Malaysia Airlines plane Ukraine July 18th, 2014(see Figure 1) HSBCs admittance aiding clients tax evasion February9th, 2015. Events covered many articles question findarticles different languages describing single event. Generally, eventsc2016AI Access Foundation. rights reserved.fiRupnik, Muhic, Leban, Fortuna, Skraba & GrobelnikFigure 1: Events represented collections articles event, caseMalaysian airliner shot Ukraine. results shownfigure obtained using query http://eventregistry.org/event/997350#?lang=eng&tab=articles. content presented part EventRegistry system, developed authors.specific general themes time component plays important roleexample, two wars Iraq would considered separate events.input, consider stream articles different languages list events.goal assign articles corresponding events. priori, knowcoverage articles, is, events may covered knowarticles necessarily fit one events. task divided two parts:detecting events within language linking events across languages.paper address second step.consider high volume articles different languages. using languagedetector, stream split separate monolingual streams. Within monolingual284fiCross-Lingual Document Similarity Event Trackingstream, online clustering approach employed, tracked clusters corresponddefinition events - based Event Registry system (Leban, Fortuna,Brank, & Grobelnik, 2014b, 2014a). main goal paper connect clusters(representations events) across languages, is, detect set articleslanguage reports event set articles language B.approach link clusters across languages combines two ingredients: cross-lingualdocument similarity measure, interpreted language independent topicmodel, semantic annotation documents, enables alternative way comparing documents. Since work represents complicated pipeline, concentratetwo specific elements. Overall, approach considered systemsperspective (considering system whole) rather considering problemsisolation.first ingredient approach link clusters across languages represents continuation previous work (Rupnik, Muhic, & Skraba, 2011a, 2012, 2011b; Muhic, Rupnik,& Skraba, 2012) explored representations documents validmultiple languages. representations could interpreted multilingual topics,used proxies compute cross-lingual similarities documents. learnrepresentations, use Wikipedia training corpus. Significantly,consider major hub languages English, German, French, etc.significant overlap article coverage, also smaller languages (in terms numberWikipedia articles) Slovenian Hindi, may negligible overlaparticle coverage. define similarity two articles regardless language, allows us cluster articles according topic. underlying assumptionarticles describing event similar therefore putcluster.Using similarity function, propose novel algorithm linking events/clustersacross languages. pose task classification problem based several setsfeatures. addition features, cross-lingual similarity also used quickly identifysmall list potential linking candidates cluster. greatly increasesscalability system.paper organized follows: first provide overview system wholeSection 2, includes subsection summarizes main system requirements.present related work Section 3. related work covers work cross-lingualdocument similarity well work cross-lingual cluster linking. Section 4, introduce problem cross-lingual document similarity computation describe severalapproaches problem, notably new approach based hub languages. Section 5, introduce central problem cross-lingual linking clusters news articlesapproach combines cross-lingual similarity functions knowledge extraction based techniques. Finally, present interpret experimental resultsSection 6 discuss conclusions point several promising future directions.2. Pipelinebase techniques cross-lingual event linking online system detectionworld events, called Event Registry (Leban et al., 2014b, 2014a). Event Registry285fiRupnik, Muhic, Leban, Fortuna, Skraba & GrobelnikDatacollec-onMain-streamnewsAr-cle-levelprocessingEventconstruc-onSeman&cannota&onAr&cle clusteringExtrac&ondate referencesEvent forma&onManual eventadministra&onEvent info.extrac&onDetec&onar&cle loca&onCross-lingualclustermatchingCross-lingualar-cle matchingDetec&onar&cle duplicatesEvent storage &maintenanceFilling eventtemplateIden&fyingrelated eventsFrontendinterfaceAPIaccessFigure 2: Event Registry pipeline. new articles collected, first analyzed individually (Article-level processing). next step, groups articlesevent identified relevant information eventextracted (Event construction phase). Although pipeline contains severalcomponents, focus two highlighted image.repository events, events automatically identified analyzing news articlescollected numerous news outlets world. important componentspipeline Event Registry shown Figure 2. briefly describemain components.collection news articles performed using Newsfeed service (Trampus& Novak, 2012). service monitors RSS feeds around 100,000 mainstream newsoutlets available globally. Whenever new article detected RSS feed, servicedownloads available information article sends articlepipeline. Newsfeed downloads daily average around 200,000 news articles variouslanguages, English, Spanish German common.Collected articles first semantically annotated identifying mentions relevantconcepts either entities important keywords. disambiguation entity linkingconcepts done using Wikipedia main knowledge base. algorithmsemantic annotation uses machine learning detect significant terms within unstructuredtext link appropriate Wikipedia articles. approach models link probability combines prior word sense distributions context based sense distributions.286fiCross-Lingual Document Similarity Event Trackingdetails reported Milne Witten (2008) Zhang Rettinger (2014a).part semantic annotation also analyze dateline article identifylocation described event well extract dates mentioned article usingset regular expressions. Since articles frequently revised also detect collectedarticle revision previous one use information next phasespipeline. last important processing step document level efficientlyidentify articles available languages similar article.methodology task one main contributions paper explaineddetails Section 4.next step, online clustering algorithm (Brank, Leban, & Grobelnik, 2014)applied articles order identify groups articles discussingevent. new article, clustering algorithm determines articleassigned existing cluster new cluster. underlying assumptionarticles describing event similar enough therefore putcluster. clustering, new article first tokenized, stop wordsremoved remaining words stemmed. remaining tokens representedvector-space model normalized using TF-IDF1 (see Section 4.1 definition).Cosine similarity used find similar existing cluster, comparing documents vector centroid vector cluster. user-defined threshold useddetermine article similar enough existing clusters (0.4 usedexperiments). highest similarity threshold, article assignedcorresponding cluster, otherwise new cluster created, initially containing singlearticle. Whenever article assigned cluster, clusters centroid vector alsoupdated. Since articles event commonly written short periodtime, remove clusters oldest article cluster becomes 4 daysold. housekeeping mechanism prevents clustering becoming slow alsoensures articles assigned obsolete clusters.number articles cluster reaches threshold (which language dependent parameter), assume articles cluster describing event.point, new event unique ID created Event Registry, clusterarticles assigned it. analyzing articles, extract main informationevent, event location, date, relevant entities keywords, etc.Since articles cluster single language, also want identifyexisting clusters report event languages join clustersevent. task performed using classification approachsecond major contribution paper. described detail Section 5.cluster identified information event extracted, availabledata stored custom-built database system. data accessibleAPI web interface (http://eventregistry.org/), provide numerous searchvisualization options.1. IDF weights dynamically computed new article news articles within 10 daywindow.287fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik2.1 System Requirementsgoal build system monitors global media analyzes eventsreported on. approach consists two steps: tracking events separately language(based language detection online clustering approach) connecting them.pipeline must able process millions articles per day perform billionssimilarity computations day. steps rely heavily similarity computation,therefore highly scalable.Therefore, focus implementations run single shared memory machine,opposed clusters machines. simplifies implementation system maintenance.summarize, following properties desirable:Training - training (building cross-lingual models) scale many languages robust quality training resources. systemable take advantage comparable corpora (as opposed parallel translationbased corpora), missing data.Operation efficiency - similarity computation fast - system mustable handle billions similarity computations per day. Computing similaritynew document set known documents efficient (the mainapplication linking documents two monolingual streams).Operation cost - system run strong shared machine serverrely paid services.Implementation - system straightforward implement, parameterstune.believe cross-lingual similarity component meets requirementsdesirable commercial setting, several different costs taken consideration.3. Related Worksection, describe previous work described literature. Since twodistinctive tasks tackle paper (computing cross-lingual document similaritycross-lingual cluster linking), separated related work two correspondingparts.3.1 Cross-Lingual Document Similarityfour main families approaches cross-lingual similarity.3.1.1 Translation Dictionary Based Approachesobvious way compare documents written different languages use machinetranslation perform monolingual similarity (see Peters & Braschler, 2012; Potthast,Barron-Cedeno, Stein, & Rosso, 2011). One use free tools Moses (Koehn et al.,2007) translation services, Google Translate (https://translate.google.com/).288fiCross-Lingual Document Similarity Event Trackingtwo issues approaches: solve harder problem needssolved less robust training resource quality - large sets translated sentences typically needed. Training Moses languages scarce linguistic resourcesthus problematic. issue using online services Google TranslateAPIs limited free. operation efficiency cost requirements maketranslation-based approaches less suited system. Closely related works CrossLingual Vector Space Model (CL-VSM) (Potthast et al., 2011) approach presentedPouliquen, Steinberger, Deguernel (2008) compare documents usingdictionaries, cases EuroVoc dictionaries (Rodrguez, Azcona, & Paredes,2008). generality approaches limited quality available linguisticresources, may scarce non-existent certain language pairs.3.1.2 Probabilistic Topic Model Based Approachesexist many variants modelling documents language independent way using probabilistic graphical models. models include: Joint Probabilistic Latent Semantic Analysis (JPLSA) (Platt, Toutanova, & Yih, 2010), Coupled Probabilistic LSA(CPLSA) (Platt et al., 2010), Probabilistic Cross-Lingual LSA (PCLLSA) (Zhang, Mei, &Zhai, 2010) Polylingual Topic Models (PLTM) (Mimno, Wallach, Naradowsky, Smith,& McCallum, 2009) Bayesian version PCLLSA. methods (exceptCPLSA) describe multilingual document collections samples generative probabilistic models, variations assumptions model structure. topicsrepresent latent variables used generate observed variables (words), processspecific language. parameter estimation posed inference problemtypically intractable one usually solves using approximate techniques. variants solutions based Gibbs sampling Variational Inference, nontrivialimplement may require experienced practitioner applied. Furthermore,representing new document mixture topics another potentially hard inferenceproblem must solved.3.1.3 Matrix Factorization Base ApproachesSeveral matrix factorization based approaches exist literature. models include:Non-negative matrix factorization based (Xiao & Guo, 2013), Cross-Lingual Latent Semantic Indexing CL-LSI (Dumais, Letsche, Littman, & Landauer, 1997; Peters & Braschler,2012), Canonical Correlation Analysis (CCA) (Hotelling, 1935), Oriented Principal Component Analysis (OPCA) (Platt et al., 2010). quadratic time space dependencyOPCA method makes impractical large scale purposes. addition, OPCA forcesvocabulary sizes languages same, less intuitive. setting, method Xiao Guo (2013) prohibitively high computational costbuilding models (it uses dense matrices whose dimensions product training setsize vocabulary size). proposed approach combines CCA CL-LSI. Anotherclosely related method Cross-Lingual Explicit Semantic Analysis (CL-ESA) (Potthast,Stein, & Anderka, 2008), uses Wikipedia (as current work) comparedocuments. interpreted using sample covariance matrix featurestwo languages define dot product used compute similarities. authors289fiRupnik, Muhic, Leban, Fortuna, Skraba & GrobelnikCL-ESA compare CL-LSI find CL-LSI outperform CL-ESAinformation retrieval, costlier optimize large corpus (CL-ESA requirestraining). find scalability argument apply case: based advances numerical linear algebra solve large CL-LSI problems involve millionsdocuments opposed 10,000 document limit reported Potthast et al. (2008).addition, CL-ESA less suited computing similarities two large monolingualstreams. example, day compute similarities 500,000 English500,000 German news articles. Comparing German news article 500,000English news articles either prohibitively slow (involves projecting English articlesWikipedia) consumes much memory (involves storing projected English articles,Wikipedia size 1,000,000 500,000 1,000,000 non-sparse matrix).3.1.4 Monolingual ApproachesFinally, related work includes monolingual approaches treat document written different languages monolingual fashion. intuition named entities (for example, Obama) cognate words (for example, tsunami) writtensimilar fashion many languages. example, Cross-Language Character n-GramModel (CL-CNG) (Potthast et al., 2011) represents documents bags character ngrams. Another approach use language dependent keyword lists based cognatewords (Pouliquen et al., 2008). approaches may suitable comparing documents written languages share writing system, apply caseglobal news tracking.Based requirements Section 2.1, chose focus methods based vectorspace models linear embeddings. propose method efficientpopular alternatives (a clustering-based approach latent semantic indexing), stillsimple optimize use.3.2 Cross-Lingual Cluster LinkingAlthough number services aggregate news identifying clusterssimilar articles, almost services provide linking clusters differentlanguages. Google News well Yahoo! News able identify clusters articlesevent, offer linking clusters across languages. servicefound, provides cross-lingual cluster linking, European Media Monitor(EMM) (Pouliquen et al., 2008; Steinberger, Pouliquen, & Ignat, 2005). EMM clustersarticles 60 languages tries determine clusters articles differentlanguages describe event. achieve cluster linking, EMM uses three differentlanguage independent vector representations cluster. first vector containsweighted list references countries mentioned articles, secondvector contains weighted list mentioned people organizations. last vectorcontains weighted list Eurovoc subject domain descriptors. descriptorstopics, air transport, EC agreement, competition pollution controlarticles automatically categorized (Pouliquen, Steinberger, & Ignat, 2006). Similarityclusters computed using linear combination cosine similaritiescomputed three vectors. similarity threshold, clusters290fiCross-Lingual Document Similarity Event Trackinglinked. Compared EMM, approach uses document similarities obtain small setpotentially equivalent clusters. Additionally, decide two clusters equivalentbased hand-set threshold similarity value instead use classification modeluses larger set features related tested pair clusters.system, significantly different worth mentioning, GDELTproject (Leetaru & Schrodt, 2013). GDELT, events also extracted articles,case, event specified form triple containing two actorsrelation. project contains extensive vocabulary possible relations, mostly related political events. order identify events, GDELT collects articles65 languages uses machine translation translate English. informationextraction done translated article.4. Cross-Lingual Document SimilarityDocument similarity important component techniques text mining naturallanguage processing. Many techniques use similarity black box, e.g., kernelSupport Vector Machines. Comparison documents (or types text snippets)monolingual setting well-studied problem field information retrieval (Salton& Buckley, 1988). first formally introduce problem followed descriptionapproach.4.1 Problem Definitionfirst describe documents represented vectors compare documents mono-lingual setting. define way measure cross-lingual similaritynatural models consider.4.1.1 Document Representationstandard vector space model (Salton & Buckley, 1988) represents documents vectors, term corresponds word phrase fixed vocabulary. Formally,document represented vector x Rn , n corresponds sizevocabulary, vector elements xk correspond number times term k occurreddocument, also called term frequency Fk (d).also used term re-weighting scheme adjusts fact wordsoccur frequently general. term weight correspond importanceterm given corpus. common weighting scheme called Term FrequencyInverse Document Frequency (T F IDF ) weighting.Document Frequency (IDF )InverseNweight dictionary term k defined log DFk , DFk numberdocuments corpus contain term k N total number documentscorpus. building cross-lingual models, IDF scores computedrespect Wikipedia corpus. part system, computed TFIDFvectors streams news articles multiple languages. IDF scoreslanguage changed dynamically - new document computed IDF newsarticles within 10 day window.291fiRupnik, Muhic, Leban, Fortuna, Skraba & GrobelnikTherefore define documents F IDFxij :=term frequency document.inverse document frequency term jF IDF weighted vector space model document representation corresponds map: text Rn defined by:N.(d)k = F k (d) logDF k4.1.2 Mono-Lingual Similaritycommon way computing similarity documents cosine similarity,sim(d1 , d2 ) =h(d1 ), (d2 )i,k(d1 )kk(d2 )kh, kk standard inner product Euclidean norm. dealing twolanguages, one could ignore language information build vector space usingunion tokens languages. cosine similarity function spaceuseful extent, example Internet Obama may appear SpanishEnglish texts presence terms English Spanish document would contribute similarity. general however, large parts vocabulariesmay intersect. means given language pair, many words languagescannot contribute similarity score. cases make similarity functioninsensitive data.4.1.3 Cross-Lingual SimilarityProcessing multilingual dataset results several vector spaces varying dimensionality, one language. dimensionality vector space corresponding i-thlanguage denoted ni vector space model mapping denoted : text Rni .similarity documents language language j defined bilinear operator represented matrix Si,j Rni nj :simi,j (d1 , d2 ) =hi (d1 ), Si,j j (d2 )i,ki (d1 )kkj (d2 )kd1 d2 documents written i-th j-th language respectively.maximal singular value Si,j bounded 1, similarity scores lieinterval [1, 1]. provide overview models Section 4.2 introduceadditional notation 4.3. Starting Section 4.4 ending Section 4.7describe approaches compute Si,j given training data.4.2 Cross-Lingual Modelssection, describe several approaches problem computing multilingual similarities introduced Section 4.1. present four approaches: simple approachbased k-means clustering Section 4.4, standard approach based singular value decomposition Section 4.5, related approach called Canonical Correlation Analysis (CCA)292fiCross-Lingual Document Similarity Event TrackingSection 4.6 finally new method, extension CCA two languages Section 4.7. CCA used find correlated patterns pair languages,whereas extended method optimizes Sum Squared Correlations (SSCOR)several language pairs, introduced Kettenring (1971). SSCOR problemdifficult solve setting (hundreds thousands features, hundreds thousandsexamples). tackle this, propose method consists two ingredients.first one based observation certain datasets (such Wikipedia) biasedtowards one language (English Wikipedia), exploited reformulatedifficult optimization problem eigenvector problem. second ingredient dimensionality reduction using CL-LSI, makes eigenvector problem computationallynumerically tractable.concentrate approaches based linear maps rather alternatives,machine translation probabilistic models, discussed section relatedwork. start introducing notation.4.3 Notationcross-lingual similarity models presented paper based comparable corpora.comparable corpus collection documents multiple languages, alignmentdocuments topic, even rough translation other.Wikipedia example comparable corpus, specific entry describedmultiple languages (e.g., Berlin currently described 222 languages). News articlesrepresent another example, event described newspapers severallanguages.formally, multilingual document = (u1 , . . . um ) tuple documentstopic (comparable), ui document written language i. Noteindividual document ui empty document (missing resource)must contain least two nonempty documents. means analysisdiscard strictly monolingual documents cross-lingual information available.comparable corpus = d1 , . . . , ds collection multilingual documents. usingvector space model, represent set matrices X1 , . . . , Xm ,Xi Rni matrix corresponding language ni vocabulary sizelanguage i. Furthermore, let Xi` denote `-th column matrix Xi matricesrespect document alignment - vector Xi` corresponds TFIDF vectori-th component Pmultilingual document d` . use N denote total row dimensionX, i.e., N :=i=1 ni . See Figure 3 illustration introduced notation.describe four models cross-lingual similarity computation nextsub-sections.4.4 k-Meansk-means algorithm perhaps well-known widely-used clustering algorithm. Here, present application compute cross-lingual similarities. ideabased concatenating corpus matrices, running standard k-means clustering obtainmatrix centroids, reversing concatenation step obtain set aligned bases,finally used compute cross-lingual similarities. See Figure 4 overview293fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik= {d1 , d2 , . . . , ds }d2d3d1X{{XX{XX {XX111X12 X13X1s212X22 X23X2s313X32 X33X3s{Xm{n{n{n1{23{nN{12 X3Xm XmXmdsFigure 3: Multilingual corpora matrix representations using vector spacemodel.294fiCross-Lingual Document Similarity Event Trackingk-means:X1X2X3=C1QC2C3new vector i-th language,x 2 Rni mapped newcoordinates minimize:||Xi Ci ||solution= (CiT Ci )1CiT x{Columns Qindicator vectorsPiFigure 4: k-means algorithm coordinate change.procedure. left side Figure 4 illustrates decomposition right sidesummarizes coordinate change.order apply algorithm, first merge term-document matricessingle matrix X stacking individual term-document matrices (as seen Figure 3):,X := X1T , X2T , , Xmcolumns respect alignment documents (here MATLAB notationconcatenating matrices used). Therefore, document represented long vectorindexed terms languages.run k-means algorithm (Hartigan, 1975) obtain centroid matrixC RN k , k columns represent centroid vectors. centroid matrixsplit vertically blocks:C = [C1T Cm] ,according number dimensions language, i.e., Ci Rni k . reiterate,matrices Ci computed using multilingual corpus matrix X (based Wikipediaexample).compute cross-lingual document similarities new documents, note matrixCi represents vector space basis used map points Rni k-dimensionalspace, new coordinates vector x Rni expressed as:(CiT Ci )1 CiT xi .resulting matrix similarity computation language language jdefined scaling factor as:Ci (CiT Ci )1 (CjT Cj )1 Cj .matrix result mapping documents language independent space usingpseudo-inverses centroid matrices Pi = (CiT Ci )1 Ci comparing usingstandard inner product, results matrix PiT Pj . sake presentation,assumed centroid vectors linearly independent. (An independent subspacecould obtained using additional Gram-Schmidt step (Golub & Van Loan, 2012)matrix C, case.)295fiRupnik, Muhic, Leban, Fortuna, Skraba & GrobelnikLSI:X1X2X3=U1VTU2U3X = U SVUT U =U 2 RN kV TV =V 2 RskFigure 5: LSI multilingual corpus matrix decomposition.4.5 Cross-Lingual Latent Semantic Indexingsecond approach consider Cross-Lingual Latent Semantic Indexing (CL-LSI) (Dumais et al., 1997) variant LSI (Deerwester, Dumais, Landauer, Furnas, &Harshman, 1990) one language. approach similar k-means,first concatenate corpus matrices, compute decomposition, caseCL-LSI truncated Singular Value Decomposition (SVD), decouple column space matrix use blocks compute linear maps common vector space, standardcosine similarity used compare documents.method based computing truncated singular value decompositionconcatenated corpus matrix X U SV . See Figure 5 decomposition. Representingdocuments topic coordinates done way k-means case (seeFigure 4), describe compute coordinate change functions.cross-lingual similarity functions based rank-k truncated SVD: X U V ,U RN k basis vectors interest Rkk truncated diagonal matrixsingular eigenvalues. aligned basis obtained first splitting U vertically according]T . Then,number dimensions language: U = [U1T Umk-means clustering, compute pseudoinverses Pi = (UiT Ui )1 UiT . matrices Piused change basis standard basis Rni basis spannedcolumns Ui .4.5.1 Implementation NoteSince matrix X large could use iterative method like Lanczos algorithm reorthogonalization (Golub & Van Loan, 2012) find left singular vectors(columns U ) corresponding largest singular values. turns Lanczosmethod converges slowly gap leading singular values small. Moreover,Lanczos method hard parallelize. Instead, use randomized versionSVD (Halko, Martinsson, & Tropp, 2011) viewed block Lanczos method.enables us use parallelization speeds computation considerably.compute matrices Pi used QR algorithm (Golub & Van Loan, 2012)factorize Ui Ui = Qi Ri , QTi Qi = Ri triangular matrix. Piobtained solving Ri Pi = Qi .296fiCross-Lingual Document Similarity Event Tracking4.6 Canonical Correlation Analysispresent statistical technique analyze data two sources, extensionpresented next section.Canonical Correlation Analysis (CCA) (Hotelling, 1935) dimensionality reductiontechnique similar Principal Component Analysis (PCA) (Pearson, 1901), additional assumption data consists feature vectors arose two sources (twoviews) share information. Examples include: bilingual document collection (Fortuna, Cristianini, & Shawe-Taylor, 2006) collection images captions (Hardoon,Mourao-Miranda, Brammer, & Shawe-Taylor, 2008). Instead looking linear combinations features maximize variance (PCA) look linear combinationfeature vectors first view linear combination second view,maximally correlated.Interpreting columns Xi observation vectors sampled underlying distribution Xi Rni , idea find two weight vectors wi Rni wj Rnjrandom variables wiT Xi wjT Xj maximally correlated (wi wj usedmap random vectors random variables, computing weighted sums vector components). Let (x, y) denote sample-based correlation coefficient two vectorsobservations x y. using sample matrix notation Xi Xj (assumingdata missing simplify exposition), problem formulated followingoptimization problem:maximizenwi Rni ,wj RjwiT Ci,j wj(wiT Xi , wjT Xj ) = q,qwiT Ci,i wi wjT Cj,j wjCi,i Cj,j empirical estimates variances Xi Xj respectively Ci,jestimate covariance matrix. Assuming observation vectors centered(only purposes presentation), matrices computed following way:1Xi XjT , similarly Ci,i Cj,j . optimization problem reducedCi,j = n1eigenvalue problem includes inverting variance matrices Ci,i Cj,j .matrices invertible, one use regularization technique replacing Ci,i(1 )Ci,i + I, [0, 1] regularization coefficient identitymatrix. (The applied Cj,j .) single canonical variable usually inadequaterepresenting original random vector typically one looks k projection pairs(wi1 , wj1 ), . . . , (wik , wjk ), (wiu )T Xi (wju )T Xj highly correlated (wiu )T Xiuncorrelated (wiv )T Xi u 6= v analogously wju vectors.Note method original form applicable two languagesaligned set observations available. next section describe scalable extensionCCA two languages.4.7 Hub Language Based CCA ExtensionBuilding cross-lingual similarity models based comparable corpora challenging twomain reasons. first problem related missing alignment data: numberlanguages large, dataset documents cover languages small (or may evenempty). Even two languages considered, set aligned documents297fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelniksmall (an extreme example given Piedmontese Hindi Wikipediasinter-language links available), case none methods presented farapplicable. second challenge scale - data high-dimensional (many languageshundreds thousands features per language) number multilingual documents may large (over one million case Wikipedia). optimization problemposed CCA trivial solve: covariance matrices prohibitivelylarge fit memory (even storing 100,000 100,000 element matrix requires 80GBmemory) iterative matrix-multiplication based approaches solving generalized eigenvalue problems required (the covariance matrices expressed products sparsematrices, means fast matrix-vector multiplication).describe extension CCA two languages, trainedlarge comparable corpora handle missing data. extension considerbased generalization CCA two views, introduced Kettenring (1971),namely Sum Squared Correlations SSCOR, state formally latersection. approach exploits certain characteristic data, namely hub languagecharacteristic (see below) two ways: reduce dimensionality datasimplify optimization problem.4.7.1 Hub Language Characteristiccase Wikipedia, observed even though training resources scarcecertain language pairs, often exists indirect training data. considering third language, training data languages pair, use compositionlearned maps proxy. refer third language hub language.hub language language high proportion non-empty documents ={d1 , ..., d` }. mentioned, focus multilingual documents includeleast two languages. prototypical example case Wikipedia English.notion hub language could interpreted following way. non-EnglishWikipedia page contains one links variants page languages,English likely one them. makes English hub language.use following notation define subsets multilingual comparable corpus:let a(i, j) denote index set multilingual documents non-missing datai-th j-th language:a(i, j) = {k | dk = (u1 , ..., um ), ui 6= , uj 6= } ,let a(i) denote index set multilingual documents non missing datai-th language.describe two step approach building cross-lingual similarity matrix.first part related LSI reduces dimensionality data. second steprefines linear mappings optimizes linear dependence data.4.7.2 Step 1: Hub Language Based Dimensionality Reductionfirst step method project X1 , . . . , Xm lower-dimensional spaces withoutdestroying cross-lingual structure. Treating nonzero columns Xi observationvectors sampled underlying distribution Xi Vi = Rni , analyze empirical298fiCross-Lingual Document Similarity Event Trackingcross-covariance matrices:Ci,j =X1(Xi` ci ) (Xj` cj )T ,|a(i, j)| 1`a(i,j)ci = a1i `a(i) Xi` . finding low-rank approximations Ci,j identifysubspaces Vi Vj relevant extracting linear patterns XiXj . Let X1 represent hub language corpus matrix. LSI approach findingsubspaces perform singular value decomposition full N N covariancematrix composed blocks Ci,j . |a(i, j)| small many language pairs (ascase Wikipedia), many empirical estimates Ci,j unreliable, resultoverfitting. reason, perform truncated singular value decompositionPmmatrix C = [C1,2 C1,m ] U SV , U Rn1 k , Rkk , V R( i=2 ni )k .split matrix V vertically blocks n2 , . . . , nm rows: V = [V2T VmT ]T . Notecolumns U orthogonal columns Vi (columns V orthogonal).Let V1 := U . proceed reducing dimensionality Xi setting: Yi = ViT Xi ,Yi RkN . summarize, first step reduces dimensionality databased CL-LSI, optimizes hub language related cross-covariance blocks.P4.7.3 Step 2: Simplifying Solving SSCOR.second step involves solving generalized version canonical correlation analysismatrices Yi order find mappings Pi . approach based sum squarescorrelations formulation Kettenring (1971), consider correlationspairs (Y1 , Yi ), > 1 due hub language problem characteristic. presentoriginal unconstrained optimization problem, constrained formulation basedhub language problem characteristic. simplify constraints reformulateproblem eigenvalue problem using Lagrange multipliers.original sum squared correlations formulated unconstrained problem:maximizewi RkX(wiT Yi , wjT Yj )2 .i<jsolve similar problem restricting = 1 omitting optimization non-hublanguage pairs. Let Di,i Rkk denote empirical covariance Yi Di,j denoteempirical cross-covariance computed based Yi Yj . solve following constrained(unit variance constraints) optimization problem:maximizewiRkXw1T D1,i wi2subject wiT Di,i wi = 1,= 1, . . . , m.(1)i=2constraints wiT Di,i wi simplified using Cholesky decomposition Di,i =KiT Ki substitution: yi := Ki wi . inverting Ki matrices defining Gi :=K1T D1,i Ki1 , problem reformulated:maximizeyi RkXy1T Gi yi2subject yiT yi = 1,i=2299= 1, . . . , m.(2)fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelniknecessary condition optimality derivatives Lagrangian vanish.Lagrangian (2) expressed as:L(y1 , . . . , ym , 1 , . . . , ) =Xy1T Gi yi2+i=2XyiT yi 1 .i=1Stationarity conditions give us:XL=0y1T Gi yi Gi yi + 1 y1 = 0,y1(3)L = 0 y1T Gi yi GTi y1 + yi = 0, > 1.yi(4)i=2Multiplying equations (4) yiT applying constraints, eliminategives us:GTi y1 = y1T Gi yi yi , > 1.(5)Plugging (3), obtain eigenvalue problem:!XGi GTi y1 + 1 y1 = 0.i=2eigenvectorsPmi=2 Gi Giyi obtained (5): yi :=wi :=Ki1 yi .solve problem first language. solutionsGTy1.kGTy1 kNote solution (1) recovered by:linear transformation w variables thus expressed as:Y1 := eigenvectorsXGi GTi ,i=2N diagonal matrixW1 = K11 Y1Wi = Ki1 GTi Y1 N,normalizes GTi Y1 ,N (j, j) :=1kG(i Y1 (:,j)k .4.7.4 Remarktechnique related Generalization Canonical Correlation Analysis (GCCA)Carroll (1968), unknown group configuration variable defined objectivemaximize sum squared correlations group variable others.problem reformulated eigenvalue problem. difference lies factset unknown group configuration variable hub language, simplifiessolution. complexity method O(k 3 ), k reduced dimensionLSI preprocessing step, whereas solving GCCA method scales O(s3 ),number samples (see Gifi, 1990). Another issue GCCA cannotdirectly applied case missing documents.summarize, first reduced dimensionality data k-dimensional featuresfound new representation (via linear transformation) maximizes directionslinear dependence languages. final projections enable mappingscommon space defined as: Pi (x) = WiT ViT x.300fiCross-Lingual Document Similarity Event TrackingEnglish articlesSpanish articlescandidateclustersFigure 6: Clusters composed English Spanish news articles. Arrows link Englisharticles Spanish k-nearest neighbor matches according crosslingual similarity.5. Cross-Lingual Event Linkingmain application test cross-lingual similarity cross-lingual eventlinking. online media streams particularly news articles often duplicationreporting, different viewpoints opinions, centering around single event.events covered many articles question address findarticles different languages describing single event. paper considerproblem matching events different languages. address problemdetection events instead base evaluation online system detectionworld events, Event Registry. events represented clusters articlesultimately problem reduces finding suitable matchings clusters articlesdifferent languages.5.1 Problem Definitionproblem cross-lingual event linking match monolingual clusters news articlesdescribe event across languages. example, want match clusterSpanish news articles cluster English news articles describeearthquake.article written language `, ` L = {`1 , `2 , ..., `m }.language `, obtain set monolingual clusters C` . precisely, articlescorresponding cluster c C` written language `. Given pair languages`a L, `b L `a 6= `b , would like identify cluster pairs (ci , cj ) C`a C`bci cj describe event.Matching clusters generalized matching problem. cannot assumeone cluster per language per event, assume complete coverage i.e.,exists least one cluster per event every language. partly due newscoverage might granular languages, partly due noise errors301fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnikevent detection process. implies cannot make assumptions matching(e.g., one-to-one complete matching) excludes use standard weighted bipartitematching type algorithms problem. example shown Figure 6,cluster may contain articles closely matched many clusters differentlanguage.also seek algorithm exhaustive comparison clusters,since become prohibitively expensive working real-time setting.specifically, wish avoid testing cluster ci clusterslanguages. Performing exhaustive comparison would result O(|C|2 ) tests, |C|number clusters (over languages), feasible numberclusters order tens thousands. address testing clustersconnected least one k-nearest neighbor (marked candidate clustersFigure 6).5.2 Algorithmorder identify clusters equivalent cluster ci , developed two-stagealgorithm. cluster ci , first efficiently identify small set candidate clustersfind clusters among candidates, equivalent ci . exampleshown Figure 6.details first step described Algorithm 1. algorithm beginsindividually inspecting news article ai cluster ci . Using chosen methodcomputing cross-lingual document similarity (see Section 4.2), identifies 10similar news articles ai language ` L. similar article aj , identifycorresponding cluster cj add set candidates. set candidate clustersobtained way several orders magnitude smaller number clusters,linear respect number news articles cluster ci . practice,clusters contain highly related articles similar articles languagesmostly fall candidate clusters.Although computed document similarities approximate, assumption articles different languages describing event generally higher similarityarticles different events. assumption always hold, redundancy data mitigates false positives. Since compute 10 similararticles article ci , likely identify relevant candidates clusterci .second stage algorithm determines (if any) candidate clustersequivalent ci . treat task supervised learning problem. candidatecluster cj C, compute vector learning features indicative whetherci cj equivalent apply binary classification model predictsclusters equivalent not. classification algorithm used train modellinear Support Vector Machine (SVM) method (Shawe-Taylor & Cristianini, 2004).use three groups features describe cluster pair (ci , cj ). first group basedcross-lingual article links, derived using cross-lingual similarity: newsarticle ai linked 10-nearest neighbors articles languages (10 perlanguage). group contains following features:302fiCross-Lingual Document Similarity Event Trackinginput: test cluster ci , set clusters C` language ` Loutput: set clusters C potentially equivalent ciC {};article ai cilanguage ` L/* use hub CCA find 10 similar articles article ailanguage `*/SimilarArticles = getCCASimilarArticles(ai , `);article aj SimilarArticles/* find cluster cj article aj assigned*/cj c, c C` aj c;/* add cluster cj set candidates C*/C C {cj };endendendAlgorithm 1: Algorithm identifying candidate clusters C potentially equivalent cilinkCount number times news articles cj among 10-nearestneighbors articles ci . words, number times articleci similar article (i.e., among 10 similar) cj .avgSimScore average similarity score links, identified linkCount,two clusters.second group concept-related features. Articles imported EventRegistry annotated disambiguating mentioned entities keywords corresponding Wikipedia pages (Zhang & Rettinger, 2014b). Whenever Barack Obama is,example, mentioned article, article annotated link Wikipedia page.way, mentions entities (people, locations, organizations) ordinary keywords (e.g., bank, tax, ebola, plane, company) annotated. Although Spanish articleObama annotated Spanish version Wikipedia page, manycases link Wikipedia pages English versions. done sinceWikipedia provides information regarding pages different languages representconcept/entity. Using approach, word avion Spanish articleannotated concept word plane English article. Althougharticles different languages, annotations therefore provide languageindependent vocabulary used compare articles/clusters. analyzingarticles clusters ci cj , identify relevant entities keywordscluster. Additionally, also assign weights concepts basedfrequently occur articles cluster. list relevant conceptscorresponding weights, consider following features:entityCosSim cosine similarity vectors entities clusters cicj .303fiRupnik, Muhic, Leban, Fortuna, Skraba & GrobelnikkeywordCosSim cosine similarity vectors keywords clusters cicj .entityJaccardSim Jaccard similarity coefficient (Levandowsky & Winter, 1971)sets entities clusters ci cj .keywordJaccardSim Jaccard similarity coefficient sets keywordsclusters ci cj .last group features contains three miscellaneous features seem discriminative unrelated previous two groups:hasSameLocation feature boolean variable true locationevent clusters same. location events estimated consideringlocations mentioned articles form cluster provided EventRegistry.timeDiff absolute difference hours two events. publicationtime date events computed average publication time datearticles provided Event Registry.sharedDates determined Jaccard similarity coefficient sets datementions extracted articles. use extracted mentions dates providedEvent Registry, uses extensive set regular expressions detect normalize mentions dates different forms.6. Evaluationdescribe main dataset building cross-lingual models basedWikipedia present two sets experiments. first set experiments establisheshub based approach deal language pairs little training dataavailable. second set experiments compares main approaches presentedtask mate retrieval task event linking. mate retrieval taskgiven test set document pairs, pair consists documenttranslation. Given query document test set, goal retrieve translationlanguage, also referred mate document. Finally, examinedifferent choices features impact event linking performance.6.1 Wikipedia Comparable Corpusinvestigate empirical performance low-rank approximations test algorithms large-scale, real-world multilingual dataset extracted Wikipediausing inter-language links alignment. results large number weakly comparable documents 200 languages. Wikipedia large source multilingualdata especially important languages translation tools, multilingual dictionaries Eurovoc (Rodrguez et al., 2008), strongly aligned multilingualcorpora Europarl (Koehn, 2005) available. Documents different languagesrelated inter-language links found left Wikipedia page.304fiCross-Lingual Document Similarity Event TrackingWikipedia constantly growing. currently 12 Wikipedias 1 million articles, 52 100k articles, 129 10k articles, 2361, 000 articles.Wikipedia page embedded page tag. First, check titlepage starts Wikipedia namespace (which includes categories discussion pages)process page does. Then, check redirection pagestore redirect link inter-language links point redirection links also.none applies, extract text parse Wikipedia markup. Currently,markup removed.get inter-language link matrix using previously stored redirection links interlanguage links. inter-language link points redirection replaceredirection target link. turns obtain matrix symmetric,consequently underlying graph symmetric. means existenceinter-language link one way (i.e., English German) guaranteeinter-language link reverse direction (German English). correcttransform matrix symmetric computing + obtaining undirectedgraph. rare case symmetrization multiple links pointingdocument, pick first one encountered. matrix enables us buildalignment across Wikipedia2 languages.6.2 Experiments Missing Alignment Datasubsection, investigate empirical performance hub CCA approach.demonstrate approach successfully applied even casefully missing alignment information. purpose, select subset Wikipedialanguages containing three major languages, English (4,212k articles)en (hub language),Spanish (9,686k articles)es, Russian (9,662k articles)ru, five minority (in termsWikipedia sizes) languages, Slovenian (136k articles)sl, Piedmontese (59k articles)pms,Waray-Waray (112k articles)war (all 2 million native speakers), Creole (54karticles)ht (8 million native speakers), Hindi (97k articles)hi (180 million nativespeakers). preprocessing, remove documents contain less 20 differentwords (referred stubs3 ) remove words occurring less 50 documentswell top 100 frequent words (in language separately). representdocuments normalized TFIDF (Salton & Buckley, 1988) weighted vectors. IDFscores computed language based aligned documents EnglishWikipedia. English language IDF scores based English documentsaligned Spanish documents exist.evaluation based splitting data training test sets. selecttest set documents multilingual documents least one nonempty alignmentlist: (hi, ht), (hi, pms), (war, ht), (war, pms). guarantees coverlanguages. Moreover test set suitable testing retrievalhub chosen pairs empty alignments. remaining documents used2. dataset based Wikipedia dumps available 2013.3. documents typically low value linguistic resource. Examples include titlescolumns table, remains parsing process, Wikipedia articles little information contained one two sentences.305fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelniktraining. Table 1, display corresponding sizes training test documentslanguage pair.training set, perform two step procedure obtain common document representation set mappings Pi . test set language pair, testi,j ={(x` , y` )|` = 1 : n(i, j)}, consists comparable document pairs (linked Wikipedia pages),n(i, j) test set size. evaluate representation measuring mate retrievalquality test sets: `, rank projected documents Pj (y1 ), . . . , Pj (yn(i,j) )according similarity Pi (x` ) compute rank mate documentr(`) = rank(Pj (y` )). finalretrieval score (between -100 100) computed as:Pn(i,j) n(i,j)r(`)100`=1n(i,j)n(i,j)1 0.5 . score less 0 means method performs worse random retrieval score 100 indicates perfect mate retrieval.mate retrieval results included Table 2.observe method performs well pairs languages, least 50,000training documents available(en, es, ru, sl ). note taking k = 500 k = 1, 000multilingual topics usually results similar performance, notable exceptions:case (ht, war ) additional topics result increase performance, opposed(ht, pms) performance drops, suggests overfitting. languagesmethod performs poorly ht war, explained qualitydata (see Table 3 explanation follows). case pms, demonstrate solidperformance achieved language pairs (pms, sl ) (pms, hi ), 2,000training documents shared pms sl training documents availablepms hi. Also observe case (pms, ht) method still obtainsscore 62, even though training set intersection zero ht data corrupted,show next paragraph.Table 1: Training test sizes (in thousands). first row represents size training sets used construct mappings low-dimensional language independentspace using en hub. diagonal elements represent number uniquetraining documents test documents language.enesruslen 671 - 4.6 463 - 4.3 369 - 3.2 50.3 - 2.0es463 - 4.3 187 - 2.9 28.2 - 2.0ru369 - 3.2 29.6 - 1.9sl50.3 - 2hiwarhtpmshi14.4 - 2.88.7 - 2.59.2 - 2.73.8 - 1.614.4 - 2.8war8.58 - 2.46.9 - 2.42.9 - 1.11.2 - 0.990.58 - 0.88.6 - 2.4ht17 - 2.313.2 - 23.2 - 2.20.95 - 1.20.0 - 2.10.04 - 0.517 - 2.3pms16.6 - 2.713.8 - 2.610.2 - 1.31.8 - 1.00.0 - 0.80.0 - 2.00.0 - 0.416.6 - 2.7inspect properties training sets roughly estimating fractionrank(A)min(rows(A), cols(A)) English training matrix corresponding mate matrix,rows(A) cols(A) denote number rows columns respectively.denominator represents theoretically highest possible rank matrix could have.306fiCross-Lingual Document Similarity Event TrackingTable 2: Pairwise retrieval, 500 topics left 1,000enesruslhiwaren98 - 98 95 - 97 97 - 98 82 - 84 76 - 74es 97 - 9894 - 96 97 - 98 85 - 84 76 - 77ru 96 - 97 94 - 9597 - 97 81 - 82 73 - 74sl 96 - 97 95 - 95 95 - 9591 - 91 68 - 68hi 81 - 82 82 - 81 80 - 80 91 - 9168 - 67war 68 - 63 71 - 68 72 - 71 68 - 68 66 - 62ht 52 - 58 63 - 66 66 - 62 61 - 71 44 - 55 16 - 50pms 95 - 96 96 - 96 94 - 94 93 - 93 85 - 85 23 - 26topics righthtpms53 - 55 96 - 9756 - 57 96 - 9655 - 56 96 - 9659 - 69 93 - 9350 - 55 87 - 8628 - 48 24 - 2162 - 4966 - 54Ideally, two fractions approximately - aligned spacesreasonably similar dimensionality. display numbers pairs Table 3.Table 3: Dimensionality drift. column corresponds pair aligned corpus matricesEnglish another language. numbers represent rationumerical rank highest possible rank. example, column enht tells us English-Creole pairwise-aligned corpus matrix pair,English counterpart full rank, Creole counterpart far fullrank.en es0.81 0.89en ru0.8 0.89en sl0.98 0.96en hi11en war0.74 0.56en ht1 0.22en pms0.89 0.38clear case Creole language 22% documentsunique suitable training. Though removed stub documents, manyremaining documents nearly same, quality smaller Wikipedias low.confirmed Creole, Waray-Waray, Piedmontese languages manualinspection. low quality documents correspond templates year, person,town, etc. contain unique words.also problem quality test data. example, looktest pair (war, ht) 386/534 Waray-Waray test documents uniqueside almost Creole test documents (523/534) unique. indicates pooralignment leads poor performance.6.3 Evaluation Cross-Lingual Event Linkingorder determine accurately predict cluster equivalence, performed twoexperiments multilingual setting using English, German Spanish languageslabelled data evaluate linking performance. first experiment,tested well individual approaches cross-lingual article linking performused linking clusters event. second experiment tested307fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnikaccurate prediction model trained different subsets learning features.evaluate prediction accuracy given dataset used 10-fold cross validation.created manually labelled dataset order evaluate cross-lingual event linkingusing two human annotators. annotators provided interface listingarticles, content top concepts pair clusters. task determineclusters equivalent (i.e., discuss event). obtain pair clusters(ci , cj ) annotate, first randomly chose cluster ci , used Algorithm 1 computeset potentially equivalent clusters C randomly chose cluster cj C. datasetprovided annotators contains 808 examples, 402 equivalent clusterspairs 406 not. Clusters learning example either English, SpanishGerman. Although Event Registry imports articles languages well, restrictedexperiments three languages. chose three languages sincelarge number articles clusters per day makes cluster linkingproblem hard due large number possible links.Section 4.2, described three main algorithms identifying similar articlesdifferent languages. algorithms k-means, LSI hub CCA. trainingset, used common Wikipedia alignment three languages. testalgorithms performed best, made following test. three algorithms,analyzed articles Event Registry article computed similararticles languages. test informative identified similar articlescluster linking trained three classifiers described Section 5.2 onealgorithm. classifier allowed use learning features cross-lingualarticle linking features values determined based selected algorithm(k-means, LSI hub CCA). results trained models shown Table 4.also show number topics (the dimensions latent space) influencesquality, except case k-means algorithm, performance 500topic vectors reported, due higher computational cost.observe that, task cluster linking, LSI hub CCA perform comparablyoutperform k-means.also compared proposed approaches task Wikipedia mate retrieval(the task Section 6.2). computed Average (over language pairs) MeanReciprocal Rank (AMRR) (Voorhees et al., 1999) performance different approachesWikipedia data holding 15, 000 aligned test documents using 300, 000aligned documents training set. Figure 7 shows AMRR score functionnumber feature vectors. clear hub CCA outperforms LSI approachk-means lags far behind testing Wikipedia data. hub CCA approach 500topic vectors manages perform comparably LSI-based approach 1, 000 topicvectors, shows CCA method improve model memory footprintwell similarity computation time.Furthermore, inspected number topics influences accuracy clusterlinking. see Table 4 choosing number features larger 500 barelyaffects linking performance, contrast fact additional topics helpedimprove AMMR, see Figure 7. differences may arisen due different domainstraining testing (Wikipedia pages versus news articles).308fiCross-Lingual Document Similarity Event Tracking0.90.8AMRR0.70.60.5Hub CCALSIk-means0.40.30.2100200300400500600700Number feature vectors8009001,000Figure 7: Average mean reciprocal ranksalso analyzed cluster size influences accuracy cluster linking. intuitionlinking large cluster pairs easier linking clusters articles.reasoning large clusters would provide document linking information (morearticles mean links similar articles) well accurately aggregatedsemantic information. case smaller clusters, errors similarity modelsgreater impact decrease performance classifier, too. validatehypothesis split learning examples two datasets one containing clusterpairs combined number articles clusters 20 one datasetcombined number 20 more. results experiment seenTable 5. seen, results confirm expectations: smaller clustersindeed harder correctly predict cluster pair merged not.hub CCA attains higher precision classification accuracy task linkingsmall cluster pairs methods, LSI slightly better linking largecluster pairs. gain precision LSI hub CCA linking large clusters muchsmaller gain precision hub CCA LSI linking small clusters.reason decided use hub CCA similarity computation component system.second experiment, evaluate relevant individual groups featurescorrectly determine cluster equivalence. purpose, computed accuracy usingindividual groups features, well using different combination groups. Since hubCCA best performance three algorithms, used compute valuescross-lingual article linking features. results evaluation shown Table 6.see using single group features, highest prediction accuracyachieved using concept-related features. classification accuracy case 88.5%.additionally including also cross-lingual article linking features, classification309fiRupnik, Muhic, Leban, Fortuna, Skraba & GrobelnikTable 4: Accuracy cluster linking 500/800/1,000 topic vectors obtained different cross-lingual similarity algorithms. table shows algorithmsobtained classification accuracy, precision recall.Modelshub CCALSIk-meansAccuracy %78.2/79.6/80.378.9/78.7/80.673.9/-/-Precision %76.3/78.0/80.576.8/77.0/78.769.5/-/-Recall %81.6/82.1/79.983.3/80.6/83.684.6/-/-F1 %78.9/80.0/80.279.9/78.8/81.176.3/-/-Table 5: Accuracy cluster linking using 500 topic vectors two datasets containinglarge (left number) small (right number) clusters. dataset smallclusters contained subset learning examples combined numberarticles clusters cluster pair 20. remaininglearning examples put dataset large clusters.Modelshub CCALSIk-meansAccuracy %81.2 - 77.882.8 - 76.475.5 - 71.2Precision %80.5 - 74.581.3 - 70.972.8 - 70.8Recall %91.3 - 57.593.1 - 57.595.3 - 36.2F1 %85.6 - 64.986.8 - 63.582.5 - 47.9accuracy rises slightly 89.4%. Using three groups features, achieved accuracy89.2%.test accuracy predictions language dependent also performedevaluations separately individual language pairs. experiment splitannotated learning examples three datasets, dataset containedexamples one language pair. training classifier three groups featuresavailable. results shown Table 7. see performance clusterlinking English-German dataset highest terms accuracy, precision, recallF1 . performance English-Spanish dataset comparable performanceEnglish-German dataset, former achieves higher recall (and slightly higherF1 score), latter achieves higher precision. possible explanation resultshigher quantity quality English-German language resources leadsaccurate cross-lingual article similarity measure well extensive semanticannotation articles.Based performed experiments, make following conclusions.cross-lingual similarity algorithms provide valuable information used identifyclusters describe event different languages. task cluster linking,cross-lingual article linking features however significantly less informative comparedconcept-related features extracted semantic annotations. Never310fiCross-Lingual Document Similarity Event Trackingtheless, cross-lingual article similarity features important two reasons.first allow us identify given cluster limited set candidate clusterspotentially equivalent. important feature since reduces searchspace several orders magnitude. second reason features importantconcept annotations available articles annotation news articlescomputationally intensive done subset collected articles.prediction accuracies individual language pairs comparable although seemsachievable accuracy correlates amount available language resources.Table 6: accuracy classifier story linking using different sets learningfeatures.Featureshub CCAConceptsMischub CCA + Conceptshub CCA + MiscConcepts + MiscAccuracy %78.3 5.988.5 2.754.8 6.789.4 2.578.8 5.088.7 2.689.2 2.6Precision %78.2 7.088.6 4.861.8 16.589.4 4.678.9 7.188.8 4.688.8 4.9Recall %78.9 5.288.6 2.258.2 30.289.6 2.479.4 4.688.8 2.290.1 1.9F1 %78.4 5.588.5 2.452.4 13.089.4 2.379.0 4.588.7 2.389.3 2.3Table 7: accuracy classifier story linking training data languagepair separately using learning features.Language pairen, deen, eses, deAccuracy %91.8 5.587.7 5.488.6 4.3Precision %91.7 6.387.7 7.489.7 9.1Recall %93.7 6.388.5 9.884.3 11.9F1 %92.5 5.187.6 5.985.9 6.06.4 Remarks Scalability ImplementationOne main advantages approach highly scalable. fast,robust quality training data, easily extendable, simple implement relativelysmall hardware requirements. similarity pipeline computationally intensivepart currently runs machine two Intel Xeon E5-2667 v2, 3.30GHz processors256GB RAM. sufficient similarity computation large numberlanguages needed. currently uses Wikipedia freely available knowledge baseexperiments show similarity pipeline dramatically reduces search spacelinking clusters.311fiRupnik, Muhic, Leban, Fortuna, Skraba & GrobelnikCurrently, compute similarities 24 languages tags: eng, spa, deu, zho, ita,fra, rus, swe, nld, tur, jpn, por, ara, fin, ron, kor, hrv, tam, hun, slv, pol, srp, cat, ukrsupport language top 100 Wikipedia languages. data stream Newsfeed(http://newsfeed.ijs.si/) provides 430k unique articles per day. system currentlycomputes 2 million similarities per second, means compute 16 1010 similaritiesper day. store one day buffer language requires 1.5 GB memorydocuments stored 500-dimensional vectors. note time complexitysimilarity computations scales linearly dimension feature spacedepend number languages. article, compute top 10 similar onesevery language.linear algebra matrix vector operations, use high performance numericallinear algebra libraries BLAS, OPENBLAS Intel MKL, currently allows usprocess one million articles per day. current implementation, usevariation hub approach. projector matrices size 500 300, 000, everyprojector takes 1.1 GB RAM. Moreover, need proxy matrices size 500 500every language pair. 0.5 GB 24 languages 9.2 GB 100 languages.together need around 135 GB RAM system 100 languages. Usageproxy matrices enables projection input documents common spacehandling language pairs missing low alignment. enables us block-wisesimilarity computations improving system efficiency. code thereforeeasily parallelized using matrix multiplication rather performing matrix - vectormultiplications. speeds code factor around 4. way, obtaincaching gains ability use vectorization. system also easily extendable.Adding new language requires computation projector matrix proxy matricesalready available languages.6.5 Remarks Reproducibility Experimentsmade code data used experiments publicly availablehttps://github.com/rupnikj/jair_paper.git. manually labelled dataset usedevaluation event linking available dataset subfolder githubrepository. included archive contains two folders: positive negative,first folder includes examples cluster pairs two languages representevent second folder contains pairs clusters two languages representdifferent events. example JSON file contains top level informationpair clusters (including text articles) well set meta attributes,correspond features described Section 5.2.code folder includes MATLAB scripts building cross-lingual similarity modelsintroduced 4.2, used publicly available Wikipedia corpus reproduce cross-lingual similarity evaluation. also made available similaritycomputation 100 languages service xling.ijs.si.addition, Event Registry system (http://eventregistry.org/) comesAPI, documented https://github.com/gregorleban/event-registry-python,used download events articles.312fiCross-Lingual Document Similarity Event Tracking7. Discussion Future Workpaper presented cross-lingual system linking events different languages. Building existing system, Event Registry, present evaluate severalapproaches compute cross-lingual similarity function. also present approachlink events evaluate effectiveness various features. final pipeline scalableterms number articles number languages, accurately linking events.task mate retrieval, observe refining LSI-based projectionshub CCA leads improved retrieval precision, methods perform comparablytask event linking. inspection showed CCA-based approach reachedhigher precision smaller clusters. interpretation linking featureshighly aggregated large clusters, compensates lower per-document precisionLSI. Another possible reason advantage show Wikipedia lostnews domain. hypothesis could validated testing approach documentsdifferent domain.experiments show hub CCA-based features present good baseline,greatly benefit additional semantic-based features. Even though experiments addition CCA-based features semantic features lead great performance improvements, two important benefits approach. First, linkingprocess sped using smaller set candidate clusters. Second, approachrobust languages semantic extraction available, due scarce linguisticresources.7.1 Future WorkCurrently system loosely-coupled language component built independentlyrest system, particular linking component. possible betterembeddings obtained methods jointly optimize classification taskembedding.Another point interest evaluate system languages scarce linguisticresources, semantic annotation might available. purpose, labelleddataset linked clusters extended first. mate retrieval evaluation showedeven language pairs training set overlap, hub CCA recovers signal.order improve performance classifier cluster linking, additionalfeatures also extracted articles clusters checked increaseaccuracy classification. Since amount linguistic resources vary significantlylanguage language would also make sense build separate classifierlanguage pair. Intuitively, improve performance since weights individuallearning features could adapted tested pair languages.Acknowledgmentsauthors gratefully acknowledge funding work providedprojects X-LIKE (ICT-257790-STREP), MultilingualWeb (PSP-2009.5.2 Agr.# 250500),313fiRupnik, Muhic, Leban, Fortuna, Skraba & GrobelnikTransLectures (FP7-ICT-2011-7), PlanetData (ICT-257641-NoE), RENDER (ICT-257790STREP), XLime (FP7-ICT-611346), META-NET (ICT-249119-NoE).ReferencesBrank, J., Leban, G., & Grobelnik, M. (2014). high-performance multithreaded approachclustering stream documents. Proceedings 17th International Multiconference Information Society 2014, Volume E, Ljubljana, Slovenia, pp. 58.Carroll, J. D. (1968). Generalization canonical correlation analysis three setsvariables. Proceedings American Psychological Association, 227228.Deerwester, S., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A. (1990).Indexing latent semantic analysis. Journal American Society InformationScience, 41(6), 391407.Dumais, S., Letsche, T., Littman, M., & Landauer, T. (1997). Automatic Cross-LanguageRetrieval Using Latent Semantic Indexing. AAAI spring symposium crosslanguage text speech retrieval. American Association Artificial Intelligence,vol. 16. 1997, p. 21.Fortuna, B., Cristianini, N., & Shawe-Taylor, J. (2006). Kernel methods bioengineering,communications image processing, chap. Kernel Canonical Correlation AnalysisLearning Semantics Text, pp. 263282. Idea Group Publishing.Gifi, A. (1990). Nonlinear Multivariate Analysis. Wiley Series Probability Statistics.Golub, G. H., & Van Loan, C. F. (2012). Matrix computations, Vol. 3. Johns HopkinsUniversity Press.Halko, N., Martinsson, P. G., & Tropp, J. A. (2011). Finding Structure Randomness:Probabilistic Algorithms Constructing Approximate Matrix Decompositions. Society Industrial Applied Mathematics Review, 53 (2), 217288.Hardoon, D. R., Mourao-Miranda, J., Brammer, M., & Shawe-Taylor, J. (2008). Usingimage stimuli drive fMRI analysis. Neural Information Processing, pp. 477486.Springer.Hartigan, J. (1975). Clustering algorithms. John Wiley & Sons Inc, New York.Hotelling, H. (1935). predictable criterion.. Journal educational Psychology,26 (2), 139.Kettenring, J. R. (1971). Canonical analysis several sets variables. Biometrika, 58,43345.Koehn, P. (2005). Europarl: Parallel Corpus Statistical Machine Translation.Tenth Machine Translation Summit, Vol. 5, pp. 7986 Phuket, Thailand.Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan,B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., & Herbst, E.(2007). Moses: Open Source Toolkit Statistical Machine Translation. Proceedings45th Annual Meeting ACL Interactive Poster DemonstrationSessions, ACL 07, pp. 177180 Stroudsburg, PA, USA. Association ComputationalLinguistics.314fiCross-Lingual Document Similarity Event TrackingLeban, G., Fortuna, B., Brank, J., & Grobelnik, M. (2014a). Cross-lingual detectionworld events news articles. Proceedings 13th International SemanticWeb Conference, pp. 2124 Riva del Garda - Trentino, Italy.Leban, G., Fortuna, B., Brank, J., & Grobelnik, M. (2014b). Event Registry: LearningWorld Events News. Proceedings Companion Publication23rd International Conference World Wide Web Companion, WWW Companion14, pp. 107110 Seoul, Republic Korea. International World Wide Web ConferencesSteering Committee.Leetaru, K., & Schrodt, P. A. (2013). GDELT: Global data events, location, tone,19792012. International Studies Association (ISA) Annual Convention, Vol. 2,p. 4 San Francisco, California, USA.Levandowsky, M., & Winter, D. (1971). Distance Sets. Nature, 234 (5323), 3435.Milne, D., & Witten, I. H. (2008). Learning Link Wikipedia. Proceedings17th ACM Conference Information Knowledge Management, CIKM 08, pp.509518 New York, NY, USA. ACM.Mimno, D., Wallach, H. M., Naradowsky, J., Smith, D. A., & McCallum, A. (2009). Polylingual Topic Models. Proceedings 2009 Conference Empirical MethodsNatural Language Processing: Volume 2 - Volume 2, EMNLP 09, pp. 880889 Singapore. Association Computational Linguistics.Muhic, A., Rupnik, J., & Skraba, P. (2012). Cross-lingual document similarity. Information Technology Interfaces (ITI), Proceedings ITI 2012 34th InternationalConference on, pp. 387392 Cavtat / Dubrovnik, Croatia. IEEE.Pearson, K. (1901). lines planes closest fit systems points space. Philosophical Magazine, 2 (6), 559572.Peters, C., & Braschler, M. (2012). Multilingual Information Retrieval. Springer BerlinHeidelberg, Berlin, Heidelberg.Platt, J. C., Toutanova, K., & Yih, W.-t. (2010). Translingual document representationsdiscriminative projections. Proceedings 2010 Conference EmpiricalMethods Natural Language Processing, pp. 251261 Massachusetts, USA. Association Computational Linguistics.Potthast, M., Barron-Cedeno, A., Stein, B., & Rosso, P. (2011). Cross-language PlagiarismDetection. Language Resources Evaluation, 45 (1), 4562.Potthast, M., Stein, B., & Anderka, M. (2008). Wikipedia-Based Multilingual RetrievalModel. Advances Information Retrieval , 30th European Conference Information Retrieval Research (ECIR), pp. 522530 Glasgow, UK.Pouliquen, B., Steinberger, R., & Deguernel, O. (2008). Story tracking: linking similar newstime across languages. Proceedings Workshop Multi-source Multilingual Information Extraction Summarization, pp. 4956 Manchester, UnitedKingdom. Association Computational Linguistics.Pouliquen, B., Steinberger, R., & Ignat, C. (2006). Automatic annotation multilingualtext collections conceptual thesaurus. arXiv preprint cs/0609059.315fiRupnik, Muhic, Leban, Fortuna, Skraba & GrobelnikRodrguez, J. M. A., Azcona, E. R., & Paredes, L. P. (2008). Promoting GovernmentControlled Vocabularies Semantic Web: EUROVOC ThesaurusCPV Product Classification System. Semantic Interoperability European DigitalLibrary, 111.Rupnik, J., Muhic, A., & Skraba, P. (2011a). Low-rank approximations large, multilingual data. Low Rank Approximation Sparse Representation, Neural Information Processing Systems 2011 Workshop.Rupnik, J., Muhic, A., & Skraba, P. (2011b). Spanning Spaces: Learning Cross-Lingual Similarities. Beyond Mahalanobis: Supervised Large-Scale Learning Similarity, NeuralInformation Processing Systems 2011 Workshop.Rupnik, J., Muhic, A., & Skraba, P. (2012). Multilingual Document Retrieval HubLanguages. Proceedings 15th Multiconference Information Society 2012(IS-2012), pp. 201204 Ljubljana, Slovenia.Salton, G., & Buckley, C. (1988). Term-weighting approaches automatic text retrieval..Vol. 24, pp. 513523. Elsevier.Shawe-Taylor, J., & Cristianini, N. (2004). Kernel Methods Pattern Analysis. CambridgeUniversity Press.Steinberger, R., Pouliquen, B., & Ignat, C. (2005). NewsExplorer: Multilingual News Analysis Cross-Lingual Linking. Information Technology Interfaces.Trampus, M., & Novak, B. (2012). Internals Aggregated Web News Feed.Proceedings 15th Multiconference Information Society 2012 (IS-2012), pp.221224 Ljubljana, Slovenia.Voorhees, E. M., et al. (1999). TREC-8 Question Answering Track Report. Proceedings 8th Text Retrieval Conference (TREC-8), Vol. 99, pp. 7782 Gaithersburg,MD, USA.Xiao, M., & Guo, Y. (2013). novel two-step method cross language representationlearning. Advances Neural Information Processing Systems, pp. 12591267 Sateline, NV, USA.Zhang, D., Mei, Q., & Zhai, C. (2010). Cross-lingual latent topic extraction. Proceedings48th Annual Meeting Association Computational Linguistics, pp.11281137 Uppsala, Sweden. Association Computational Linguistics.Zhang, L., & Rettinger, A. (2014a). Semantic Annotation, Analysis Comparison:Multilingual Cross-lingual Text Analytics Toolkit. Proceedings Demonstrations 14th Conference European Chapter Association Computational Linguistics (EACL 2014), pp. 1316 Gothenburg, Sweden. AssociationComputational Linguistics.Zhang, L., & Rettinger, A. (2014b). X-LiSA: Cross-lingual Semantic Annotation. Proceedings Large Data Bases (VLDB) Endowment, 7 (13), 16931696.316fiJournal Artificial Intelligence Research 55 (2016) 63-93Submitted 03/15; published 01/16Cross-Lingual Bridges Models Lexical BorrowingYulia TsvetkovChris Dyerytsvetko@cs.cmu.educdyer@cs.cmu.eduLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USAAbstractLinguistic borrowing phenomenon transferring linguistic constructions (lexical,phonological, morphological, syntactic) donor language recipient language result contacts communities speaking different languages. Borrowedwords found languages, andin contrast cognate relationshipsborrowingrelationships may exist across unrelated languages (for example, 40% Swahilisvocabulary borrowed unrelated language Arabic). work, developmodel morpho-phonological transformations across languages. features baseduniversal constraints Optimality Theory (OT), show comparedseveral standardbut linguistically navebaselines, OT-inspired model obtainsgood performance predicting donor forms borrowed forms dozentraining examples, making cost-effective strategy sharing lexical informationacross languages. demonstrate applications lexical borrowing model machinetranslation, using resource-rich donor language obtain translations out-of-vocabularyloanwords lower resource language. framework obtains substantial improvements(up 1.6 BLEU) standard baselines.1. IntroductionState-of-the-art natural language processing (NLP) tools, text parsing, speechrecognition synthesis, text speech translation, semantic analysis inference, relyavailability language-specific data resources exist resource-richlanguages. make NLP tools available languages, techniques developedprojecting resources resource-rich languages using parallel (translated) databridge cross-lingual part-of-speech tagging (Yarowsky, Ngai, & Wicentowski, 2001;Das & Petrov, 2011; Li, Graa, & Taskar, 2012; Tckstrm, Das, Petrov, McDonald, &Nivre, 2013), syntactic parsing (Wu, 1997; Kuhn, 2004; Smith & Smith, 2004; Hwa, Resnik,Weinberg, Cabezas, & Kolak, 2005; Xi & Hwa, 2005; Burkett & Klein, 2008; Snyder, Naseem,& Barzilay, 2009; Ganchev, Gillenwater, & Taskar, 2009; Tiedemann, 2014), word sensetagging (Diab & Resnik, 2002), semantic role labeling (Pad & Lapata, 2009; Kozhevnikov &Titov, 2013), metaphor identification (Tsvetkov, Boytsov, Gershman, Nyberg, & Dyer, 2014),others. limiting reagent methods parallel data. small parallelcorpora exist many languages (Smith, Saint-Amand, Plamada, Koehn, Callison-Burch,& Lopez, 2013), suitably large parallel corpora expensive, typically existEnglish geopolitically economically important language pairs.Furthermore, English high-resource language, linguistically typologicaloutlier number respects (e.g., relatively simple morphology, complex system verbalc2016AI Access Foundation. rights reserved.fiTsvetkov & DyerpippalfalafelparpaareHebrewGawwadaSanskritpilpilfalAfilpilipiliPersianArabicSwahiliFigure 1: example multilingual borrowing Sanskrit typologically diverse, lowand high-resource languages (Haspelmath & Tadmor, 2009).auxiliaries, large lexicon, etc.), assumption construction-level parallelismprojection techniques depend thus questionable. Given state affairs,urgent need methods establishing lexical links across languages relylarge-scale parallel corpora. Without new strategies, 7,000+ languagesworldmany millions speakerswill remain resource-poor standpointNLP.advocate novel approach automatically constructing language-specific resources,even languages resources raw text corpora. main motivationresearch linguistic borrowingthe phenomenon transferring linguistic constructions(lexical, phonological, morphological, syntactic) donor language recipientlanguage result contacts communities speaking different languages (Thomason& Kaufman, 2001). Borrowed words (also called loanwords, e.g., Figure 1) lexicalitems adopted another language integrated (nativized) recipient language.Borrowing occurs typically part minority language speakers, languagewider communication minority language (Sankoff, 2002); one reasondonor languages often bridge resource-rich resource-limited languages.Borrowing distinctive pervasive phenomenon: languages borrowedlanguages point lifetime, borrowed words constitute large fraction(1070%) language lexicons (Haspelmath, 2009).Loanword nativization primarily phonological process. Donor words undergo phonological repairs adapt foreign word segmental, phonotactic, suprasegmentalmorpho-phonological constraints recipient language (Holden, 1976; Van Coetsem, 1988;Ahn & Iverson, 2004; Kawahara, 2008; Hock & Joseph, 2009; Calabrese & Wetzels, 2009;Kang, 2011, inter alia). Common phonological repair strategies include feature/phonemeepenthesis, elision, degemination, assimilation. speakers encounter foreignword (either lemma inflected form), analyze morphologically stem,morphological loanword integration thus amounts selecting appropriate donorsurface form (out existing inflections lemma), applying recipientlanguage morphology (Repetti, 2006). Adapted loanwords freely undergo recipientlanguage inflectional derivational processes. Nouns borrowed preferentially,parts speech, affixes, inflections, phonemes (Whitney, 1881; Moravcsik,1978; Myers-Scotton, 2002, p. 240).Although borrowing pervasive topic enduring interest historicaltheoretical linguists (Haugen, 1950; Weinreich, 1979), limited work computational64fiCross-Lingual Bridges Models Lexical Borrowingmodeling addressed phenomenon. However, topic well-suited computationalmodels (e.g., systematic phonological changes occur borrowing modeledusing established computational primitives finite state transducers), modelsborrowing useful applications. work summarized developmentcomputational model lexical borrowing exploration applicationsaugment language resources computational approaches NLP resource-limitedlanguages. Specifically, demonstrate multilingual dictionaries extracted using modelsborrowing improve resource-limited statistical machine translation (MT), using pivotingparadigm borrowing pair translation pair single languagecommon.problem address identification plausible donor words (in donorlanguage) given loanword (in recipient language), vice versa. example, givenSwahili loanword safari journey, model identifies Arabic donor KQ (sfryh)1journey (3). Although high level, instance well-known problemmodeling string transductions, interest able identify correspondences acrosslanguages minimal supervision, make technique applicable low-resourcesettings. reduce supervision burden, model includes awareness morphophonological repair strategies native speakers language subconsciously employadapt loanword phonological constraints recipient language (3.3). end,use constraint-based theories phonology, exemplified Optimality Theory (OT)(Prince & Smolensky, 2008; McCarthy, 2009), non-computational linguistic workdemonstrated particularly well suited account phonologically complex borrowingprocesses (Kang, 2011). operationalize OT constraints features borrowingmodel (3.4). conduct case study Arabic Swahili, two phylogenetically unrelatedlanguages long history contact; apply model additional languagepairs (3.5). employ models lexical borrowing obtain cross-lingual bridgesloanwords low-resource language donors resource-rich language.donor language used pivot obtain translations via triangulation out-of-vocabularyloanwords (4). conduct translation experiments three resource-poor setups:SwahiliEnglish pivoting via Arabic, MalteseEnglish pivoting via Italic, RomanianEnglish2 pivoting via French. intrinsic evaluation, ArabicSwahili, ItalianMaltese,FrenchRomanian borrowing models significantly outperform transliteration cognatediscovery models (5.1). provide systematic quantitative qualitative analysiscontribution integrated translations, relative baselines oracles, corporavarying sizes (5.2). proposed pivoting approach yields substantial improvements(up +1.6 BLEU) SwahiliArabicEnglish translation, moderate improvement (up+0.8 BLEU) MalteseItalianEnglish translation, small (+0.2 BLEU) statisticallysignificant improvements RomanianFrenchEnglish.contributions twofold. software implementationsOT (Hayes, Tesar, & Zuraw, 2013), used chiefly facilitate linguisticanalysis; show use OT formulate model learned lesssupervision linguistically nave models. best knowledge, first1. use Buckwalter notation write Arabic glosses.2. Romanian resource-poor MT perspective, work simulate resource-poorscenario.65fiTsvetkov & Dyercomputational model lexical borrowing used downstream NLP task. Second,show lexical correspondences induced using model project resourcesnamely,translationsleading improved performance downstream translation system.32. Motivationtask modeling borrowing under-explored computational linguistics, althoughimportant practical applications lends modeling varietyestablished computational techniques. section first situate task respecttwo closely related research directions: modeling transliteration modeling cognateforms. motivate new line research proposed work: modeling borrowing.2.1 Borrowing vs. TransliterationBorrowing transliteration. Transliteration refers writing different orthography,whereas borrowing refers expanding language include words adapted anotherlanguage. Unlike borrowing, transliteration amenable orthographicrathermorpho-phonologicalfeatures, although transliteration also prone phoneticadaptation (Knight & Graehl, 1998). Borrowed words might begun transliterations,characteristic borrowed words become assimilated linguistic systemrecipient language, became regular content words, example, orangesugar English words borrowed Arabic l. ' PAK (nArnj) Q@ (Alskr), respectively.Whatever historical origins, synchronically, words indistinguishablespeakers words native ancestral forms language. Thus, morphophonological processes must accounted borrowing models complexrequired transliteration models.2.2 Borrowing vs. InheritanceCognates words related languages inherited single word commonancestral language (the proto-language). Loanwords, hand, occurlanguages, either related not, historically came contact. modelingperspective, cognates borrowed words require separate investigation loanwordslikely display marginal phonotactic (and phonological) patterns inheritedlexical items. Theoretical analysis cognates tended concerned diachronicpoint view, modeling word changes across time. immense scientificinterest, language processing applications arguably better served models synchronicprocesses, peculiar loanword analysis.3. article thoroughly revised extended version work Tsvetkov, Ammar, Dyer(2015), Tsvetkov Dyer (2015). provide detailed linguistic background lexicalborrowing OT. demonstrate results new language, Maltese, emphasize generalitymethod. Additional extensions include detailed error analysis complete literaturesurvey.66fiCross-Lingual Bridges Models Lexical Borrowing2.3 Borrowing?Borrowing distinctive pervasive phenomenon: languages borrowedlanguages point lifetime, borrowed words constitute large fractionlanguage lexicons. Another important property borrowing adaptationborrowed items, changes words systematic, knowledge morphologicalphonological patterns language used predict borrowings realizedlanguage, without list all. Therefore, modeling borrowing taskwell-suited computational approaches.suggestion work identify borrowing relations resourcerich donor languages (such English, French, Spanish, Arabic, Chinese, Russian)resource-limited recipient languages. example, 3070% vocabulary Vietnamese,Cantonese, Thairelatively resource-limited languages spoken hundreds millionspeopleare borrowed Chinese English, languages numerous dataresources created. Similarly, African languages greatly influencedArabic, Spanish, English, Frenchwidely spoken languages Swahili, Zulu,Malagasy, Hausa, Tarifit, Yoruba contain 40% loanwords. Indo-Iranian languagesHindustani, Hindi, Urdu, Bengali, Persian, Pashtospoken 860 million, also extensivelyborrowed Arabic English (Haspelmath & Tadmor, 2009). short, leastbillion people speaking resource-scarce languages whose lexicons heavily borrowedresource-rich languages.important? Lexical translations alignments extracted large parallelcorpora widely used project annotations high- low-resource languages(Hwa et al., 2005; Tckstrm et al., 2013; Ganchev et al., 2009, inter alia). Unfortunately,large-scale parallel resources unavailable majority resource-limited languages.Loanwords used source cross-lingual links complementary lexical alignmentsobtained parallel data bilingual lexicons. holds promise applying existingcross-lingual methods bootstrapping linguistic resources languages paralleldata available.3. Constraint-Based Models Lexical Borrowingtask identify plausible donorloan word pairs language pair. modelingstring transductions well-studied problem NLP, wish able learn crosslingual patterns minimal training data. therefore propose model whose featuresmotivated linguistic knowledgerather overparameterized numerous weaklycorrelated features practical large amounts training data available.features scoring model inspired Optimality Theory (OT; 3.1),borrowing candidates ranked universal constraints posited underly humanfaculty language, candidates determined transduction processes articulatedprior studies contact linguistics.illustrated Figure 2, model conceptually divided three main parts: (1)mapping orthographic word forms two languages common phonetic space; (2)generation loanword pronunciation candidates donor word; (3) rankinggenerated loanword candidates, based linguistic constraints donor recipient67fiTsvetkov & Dyer1donorword IPA22Syllabification2Phonologicaladaptation13MorphologicaladaptationGEN - generate loanword candidatesRankingOT constraintsIPAloanwordEVAL - rank candidatesFigure 2: morpho-phonological borrowing model conceptually three main parts: (1)conversion orthographic word forms pronunciations International Phonetic Alphabet format;(2) generation loanword pronunciation candidates; (3) ranking generated candidates usingOptimality-Theoretic constraints. Part (1) (2) rule-based, (1) uses pronunciation dictionaries,(2) based prior linguistic studies; part (3) learned. (3) learn OT constraint weightsdozen automatically extracted training examples.languages. proposed system, parts (1) (2) rule-based; whereas (3) learned.component model discussed detail rest section.model implemented cascade finite-state transducers. Parts (1) (2)amount unweighted string transformation operations. (1), convert orthographicword forms pronunciations International Phonetic Alphabet (IPA),pronunciation transducers. (2) syllabify donor pronunciations, perform insertion,deletion, substitution phonemes morphemes (affixes), generate multipleloanword candidates donor word. Although string transformation transducers (2)generate loanword candidates found recipient language vocabulary,candidates filtered due composition recipient language lexicon acceptor.model performs string transformations donor recipient (recapitulatinghistorical process). However, resulting relation (i.e., final composed transducer)bidirectional model well used reason underlying donor formsgiven recipient forms. probabilistic cascade, Bayes rule could used reversedirection infer underlying donor forms given loanword. However, instead opttrain model discriminatively find likely underlying form, given loanword.part (3), candidates evaluated (i.e., scored) weighted sum universal constraintviolations. non-negative weights, call cost vector, constitute modelparameters learned using small training set donorrecipient pairs. useshortest path algorithm find path minimal cost.3.1 OT: Constraint-Based EvaluationBorrowing relations may result quite complex transformations surface.decision evaluate borrowing candidates weighting counts constraint violationsbased Optimality Theory, shown complex surface phenomenawell-explained interaction constraints form outputs relationshipsinputs outputs (Kager, 1999).OT posits surface phonetic words language emerge underlying phonologicalforms according two-stage process: first, various candidates surface formenumerated consideration (the generation gen phase); then, candidatesweighed one another see closely conforms toor equivalently, leastegregiously violatesthe phonological preferences language. preferences68fiCross-Lingual Bridges Models Lexical Borrowing/Eg/a.+dep-iomax-ioonsetno-codaEgb.Eg@c.Ed.Eg!!!Table 1: constraint tableau. dep-io max-io onset no-coda ranked OT constraintsaccording phonological system English. /Eg/ underlying phonological form, (a),(b) (c), (d) output candidates consideration. actual surface form (a),incurs lower ranked violations candidates.[Sarr]a.Sarrb.Sar.ric.d.+*complexno-coda!!!dep-ioSa.riSa.rrimax-ioTable 2: example OT analysis adapted account borrowing. OT constraints rankedaccording phonological system recipient language (here, Swahili). donor (Arabic)($r~) evil considered underlying form. winning surface form (c) Swahiliword Qloanword shari evil.correctly characterized, actual surface form selected optimalrealization underlying form. preferences expressed violable constraints(violable many cases may candidate satisfies them).two types OT constraints: markedness faithfulness constraints. Markedness constraints (McCarthy & Prince, 1995) describe unnatural (dispreferred) patternslanguage. Faithfulness constraints (Prince & Smolensky, 2008) reward correspondencesunderlying form surface candidates. clarify distinctionfaithfulness markedness constraint groups NLP readership, drawfollowing analogy components machine translation speech recognition: faithfulness constraints analogical translation model acoustic model (reflectingwell output candidate appropriate input), markedness constraintsanalogical language model (requiring well-formedness output candidate).Without faithfulness constraints, optimal surface form could differ arbitrarilyunderlying form. originally proposed, OT holds set constraints universal,ranking language-specific.OT, then, grammar set universal constraints language-specificranking, derivation surface form consists underlying form, surfacecandidates, constraint violations candidates (under surface formcorrectly chosen). example OT analysis shown Table 1; OT constraintsexplained later, Tables 3 4.OT adapted account borrowing treating donor language wordunderlying form recipient language; is, phonological system69fiTsvetkov & Dyerrecipient language encoded system constraints, constraints accountdonor word adapted borrowed. show example Table 2.substantial prior work linguistics borrowing OT paradigm (Yip, 1993;Davidson & Noyer, 1997; Jacobs & Gussenhoven, 2000; Kang, 2003; Broselow, 2004; Adler,2006; Rose & Demuth, 2006; Kenstowicz & Suchato, 2006; Kenstowicz, 2007; Mwita, 2009),none led computational realizations.OT assumes ordinal constraint ranking strict dominance rather constraintweighting. that, OT-inspired model departs OTs standard evaluation assumptions: following Goldwater Johnson (2003), use linear scoring scheme.3.2 Case Study: ArabicSwahili Borrowingsection, use ArabicSwahili4 language-pair describe prototypicallinguistic adaptation processes words undergo borrowed. Then, describemodel processes general terms.Swahili lexicon influenced Arabic result prolonged periodlanguage contact due Indian Ocean trading (800 ce1920), well influenceIslam (Rothman, 2002). According several independent studies, Arabic loanwordsconstitute 18% (Hurskainen, 2004b) 40% (Johnson, 1939) Swahili word types.Despite strong susceptibility Swahili borrowing large fraction Swahiliwords originating Arabic, two languages typologically distinct profoundlydissimilar phonological morpho-syntactic systems. survey systems brieflysince illustrate Arabic loanwords substantially adapted conformSwahili phonotactics. First, Arabic five syllable patterns:5 CV, CVV, CVC, CVCC,CVVC (McCarthy, 1985, pp. 2328), whereas Swahili (like Bantu languages)open syllables form CV V. segment level, Swahili loanword adaptation thusinvolves extensive vowel epenthesis consonant clusters syllable final positionsyllable ends consonant, example, H. AJ (ktAb) kitabu book (Polom, 1967;Schadeberg, 2009; Mwita, 2009). Second, phonological adaptation Swahili loanwordsincludes shortening vowels (unlike Arabic, Swahili phonemic length);substitution consonants found Arabic Swahili (e.g., emphatic(pharyngealized) /tQ //t/, voiceless velar fricative /x//k/, dental fricatives /T//s/,/D//z/, voiced velar fricative /G//g/); adoption Arabic phonemesoriginally present Swahili /T/, /D/, /G/ (e.g., QKYm' (tH*yr) tahadhari warning);($r~) shari evil). Finally, adapteddegemination Arabic geminate consonants (e.g., Qloanwords freely undergo Swahili inflectional derivational processes, example,(Alwzyr) waziri minister, mawaziri ministers, kiuwaziri ministerial (Zawawi,QKP@1979; Schadeberg, 2009).4. simplicity, subsume Omani Arabic historical dialects Arabic label Arabic;data examples Modern Standard Arabic. Similarly, subsume Swahili, dialectsprotolanguages Swahili.5. C stands consonant, V vowel.70fiCross-Lingual Bridges Models Lexical Borrowing3.3 ArabicSwahili Borrowing Transducersuse unweighted transducers pronunciation, syllabification, morphologicalphonological adaptation describe below. example illustratespossible string transformations individual components model shown Figure 3.goal transducers minimally overgenerate Swahili adapted forms Arabicwords, based adaptations described above.Arabic wordIPAkuttabakitaba...Syllabificationku.tta.ba.ku.t.ta.ba....ki.ta.ba.ki.ta.b....PhonologicaladaptationMorphologicaladaptationku.ta.ba. [degemination]ku.tata.ba. [epenthesis]ku.ta.bu. [final vowel subst.]ki.ta.bu. [final vowel subst.]ki.ta.bu. [epenthesis]...Figure 3: example Arabic wordSwahili loanword kitabu.ku.tata.ba.li.ku.tata.ba.vi.ki.ta.bu.ki.ta.bu.ki.ta.bu....RankingOT constraintsIPASwahili wordku.ta<DEP-V>ta<PEAK>.ba.li<DEP-MORPH>.ku.ta<DEP-V>ta<PEAK>.ba.li.kitabuku.tta<*COMPLEX>.ba.ki.ta.bu<IDENT-IO-V>.ki.ta.bu<DEP-V>.vi<DEP-MORPH>.ki.ta.bu<IDENT-IO-V>.AK. AJ (ktAbA) book.sg.indef transformed model3.3.1 PronunciationBased IPA, assign shared symbols sounds exist sound systemsArabic Swahili (e.g., nasals /n/, /m/; voiced stops /b/, /d/), language-specificunique symbols sounds unique phonemic inventory Arabic (e.g.,pharyngeal voiced voiceless fricatives //, /Q/) Swahili (e.g., velar nasal /N/).Swahili, construct pronunciation dictionary based Omniglot grapheme-to-IPAmapping.6 Arabic, use CMU Arabic vowelized pronunciation dictionary containing700K types average four pronunciations per unvowelized input wordtype (Metze, Hsiao, Jin, Nallasamy, & Schultz, 2010).7 design four transducersArabic Swahili word-to-IPA IPA-to-word transducerseach union linearchain transducers, well one acceptor per pronunciation dictionary listing.3.3.2 SyllabificationArabic words borrowed Swahili undergo repair violations Swahili segmentalphonotactic constraints, example via vowel epenthesis consonant cluster. Importantly,repair depends upon syllabification. simulate plausible phonological repair processes,generate multiple syllabification variants input pronunciations. syllabificationtransducer optionally inserts syllable separators phones. example,input phonetic sequence /kuttAbA/, output strings include /ku.t.tA.bA/, /kut.tA.bA/,/ku.ttA.bA/ syllabification variants; variant violates different constraintsconsequently triggers different phonological adaptations.6. www.omniglot.com7. Since working level word types context, cannot disambiguateintended form, include options. example, input word AK. AJ (ktAbA) book.sg.indef,use pronunciations /kitAbA/ /kuttAbA/.71fiTsvetkov & Dyer3.3.3 Phonological AdaptationPhonological adaptation syllabified phone sequences crux loanword adaptationprocess. implement phonological adaptation transducers composition plausiblecontext-dependent insertions, deletions, substitutions phone subsets, based priorstudies summarized 3.2. follows, list phonological adaptation componentsorder transducer composition borrowing model. vowel deletion transducershortens Arabic long vowels vowel clusters. consonant degemination transducershortens Arabic geminate consonants, example, degeminates /tt/ /ku.ttA.bA/,outputting /ku.tA.bA/. substitution similar phonemes transducer substitutessimilar phonemes phonemes found Arabic Swahili (Polom,1967, p. 45). example, emphatic /tQ /, /dQ /, /sQ / replaced correspondingnon-emphatic segments [t], [d], [s]. vowel epenthesis transducer inserts vowelpairs consonants (/ku.ttA.bA/ /ku.tatA.bA/), end syllable,syllable ends consonant (/ku.t.tA.bA/ /ku.ta.tA.bA/). Sometimes possiblepredict final vowel word, depending word-final coda consonantArabic counterpart: /u/ /o/ added Arabic donor ends labial, /i//e/ added coronals dorsals (Mwita, 2009). Following rules, final vowelsubstitution transducer complements inventory final vowels loanword candidates.3.3.4 Morphological AdaptationArabic Swahili significant morphological processes alter appearancelemmas. deal morphological variants, construct morphological adaptationtransducers optionally strip Arabic concatenative affixes clitics, optionallyappend Swahili affixes, generating superset possible loanword hypotheses.obtain list Arabic affixes Arabic morphological analyzer SAMA (Maamouri,Graff, Bouziri, Krouna, & Kulick, 2010); Swahili affixes taken hand-craftedSwahili morphological analyzer (Littell, Price, & Levin, 2014). sake simplicityimplementation, strip one Arabic prefix one suffix perword; Swahili concatenate two Swahili prefixes one suffix.3.4 Learning Constraint WeightsDue computational problems working OT (Eisner, 1997, 2002), makesimplifying assumptions (1) bounding theoretically infinite set underlying formssmall linguistically-motivated subset allowed transformations donor pronunciations,described 3.3; (2) imposing priori restrictions set surface realizationsintersecting candidate set recipient pronunciation lexicon; (3) assumingset constraints finite regular (Ellison, 1994); (4) assigning linear weightsconstraints, rather learning ordinal constraint ranking strict dominance(Boersma & Hayes, 2001; Goldwater & Johnson, 2003).discussed 3.1, OT distinguishes markedness constraints detect dispreferredphonetic patterns language, faithfulness constraints, ensure correspondencesunderlying form surface candidates. implemented constraintslisted Tables 3 4. Faithfulness constraints integrated phonological transformationcomponents transitions following insertion, deletion, substitution. Markedness72fiCross-Lingual Bridges Models Lexical BorrowingFaithfulness constraintsmax-io-morph (donor) affix deletionmax-io-cconsonant deletionmax-io-vvowel deletiondep-io-morph (recipient) affix epenthesisdep-io-vvowel epenthesisident-io-cconsonant substitutionident-io-c-msubstitution manner pronunciationident-io-c-asubstitution place articulationident-io-c-ssubstitution sonorityident-io-c-ppharyngeal consonant substitutionident-io-c-gglottal consonant substitutionident-io-c-eemphatic consonant substitutionident-io-vvowel substitutionident-io-v-osubstitution vowel opennessident-io-v-rsubstitution vowel roundnessident-io-v-fsubstitution vowel frontnessident-io-v-fin final vowel substitutionTable 3: Faithfulness constraints prefer pronounced realizations completely congruentunderlying forms.Markedness constraintsno-codasyllables must codaonsetsyllables must onsetspeakone syllabic peaksspcomplex onsets rise sonority,complex codas fall sonority*complex-s consonant clusters syllable margins*complex-c consonant clusters within syllable*complex-v vowel clustersTable 4: Markedness constraints impose language-specific structural well-formedness surfacerealizations.constraints implemented standalone identity transducers: inputs equal outputs,path weights representing candidate evaluation respect violated constraintsdifferent.final loanword transducer composition transducers described 3.3OT constraint transducers. path transducer represents syllabified phonemicsequence along (weighted) OT constraints violates, shortest path outputsthose, whose cumulative weight violated constraints minimal.OT constraints realized features linear model, feature weightslearned discriminative training maximize accuracy obtained loanwordtransducer small development set donorrecipient pairs. parameter estimation,employ NelderMead algorithm (Nelder & Mead, 1965), heuristic derivative-free73fiTsvetkov & Dyermethod iteratively optimizes, based objective function evaluation, convexhull n + 1 simplex vertices.8 objective function used work soft accuracydevelopment set, defined proportion correctly identified donor wordstotal set 1-best outputs.3.5 Adapting Model New LanguageArabicSwahili case study shows that, principle, borrowing model constructed.reasonable question ask is: much work required build similar systemnew language pair? claim design permits rapid development newlanguage pairs. First, string transformation operations, well OT constraintslanguage-universal. adaptation required linguistic analysis identify plausiblemorpho-phonological repair strategies new language pair (i.e., subset allowedinsertions, deletions, substitutions phonemes morphemes). Since needovergenerate candidates (the OT constraints filter bad outputs), effortminimal relative many grammar engineering exercises. second language-specificcomponent grapheme-to-IPA converter. non-trivial problemcases, problem well studied, many under-resourced languages (e.g., Swahili),phonographic systems orthography corresponds phonology. tendencyexplained fact that, many cases, lower-resource languages developedorthography relatively recently, rather organically evolved written formspreserve archaic idiosyncratic spellings distantly related currentphonology language see English.illustrate ease language pair engineered, appliedborrowing model ItalianMaltese FrenchRomanian language pairs. MalteseRomanian, like Swahili, large number borrowed words lexicons (Tadmor,2009). Maltese (a phylogenetically Semitic language) 35.1%30.3% loanwords Romance(Italian/Sicilian) origin (Comrie & Spagnol, 2015). Although French Romaniansister languages (both descending Latin), 12% Romanian types Frenchborrowings came language past centuries (Schulte, 2009).language pairs manually define set allowed insertions, deletions, substitutionsphonemes morphemes, based training sets. set Maltese affixesdefined based linguistic survey Fabri, Gasser, Habash, Kiraz, Wintner (2014).employ GlobalPhone pronunciation dictionary French (Schultz & Schlippe,2014), converted IPA, automatically constructed Italian, Romanian, Maltesepronunciation dictionaries using Omniglot grapheme-to-IPA conversion ruleslanguages.4. Models Lexical Borrowing Statistical Machine Translationturning experimental verification analysis borrowing model,introduce external application borrowing model used componentmachine translation. rely borrowing model project translation information8. decision use NelderMead rather conventional gradient-based optimization algorithmsmotivated purely practical limitations finite-state toolkit used made computingderivatives latent structure impractical engineering standpoint.74fiCross-Lingual Bridges Models Lexical Borrowinghigh-resource donor language low-resource recipient language, thus mitigatingdeleterious effects out-of-vocabulary (OOV) words.OOVs ubiquitous difficult problem MT. translation system encountersOOVa word observed training data, trained system thuslacks translation variantsit usually outputs word source language,producing erroneous disfluent translations. MT systems, even trained billionsentence-size parallel corpora, encounter OOVs test time. Often, namedentities neologisms. However, OOV problem much acute morphologicallyrich low-resource scenarios: there, OOVs primarily lexicon-peripheral itemsnames specialized/technical terms, also regular content words. Since borrowedwords component regular lexical content language, projecting translationsonto recipient language identifying borrowed lexical material plausible strategysolving problem.Procuring translations OOVs subject active research decades.Translation named entities usually generated using transliteration techniques (AlOnaizan & Knight, 2002; Hermjakob, Knight, & Daum III, 2008; Habash, 2008). Extractingtranslation lexicon recovering OOV content words phrases done miningbi-lingual monolingual resources (Rapp, 1995; Callison-Burch, Koehn, & Osborne, 2006;Haghighi, Liang, Berg-Kirkpatrick, & Klein, 2008; Marton, Callison-Burch, & Resnik, 2009;Razmara, Siahbani, Haffari, & Sarkar, 2013; Saluja, Hassan, Toutanova, & Quirk, 2014).addition, OOV content words recovered exploiting cognates, transliteratingpivoting via closely-related resource-richer language, languageexists (Haji, Hric, & Kubo, 2000; Mann & Yarowsky, 2001; Kondrak, Marcu, & Knight,2003; De Gispert & Marino, 2006; Habash & Hu, 2009; Durrani, Sajjad, Fraser, & Schmid,2010; Wang, Nakov, & Ng, 2012; Nakov & Ng, 2012; Dholakia & Sarkar, 2014). worksimilar spirit latter pivoting approach, show obtain translationsOOV content words pivoting via unrelated, often typologically distant resource-richlanguage.solution depicted, high level, Figure 4. Given OOV word resource-poorMT, use borrowing system identify list likely donor words donorlanguage. Then, using MT system resource-rich language, translate donorwords target language resource-poor MT (here, English). Finally,integrate translation candidates resource-poor system.discuss integrating translation candidates acquired via borrowing plus resourcerich translation.Briefly, phrase-based translation works follows. set candidate translationsinput sentence created matching contiguous spans input inventoryphrasal translations, reordering target-language appropriate order, choosingbest one according model combines features phrases used, reorderingpatterns, target language model (Koehn, Och, & Marcu, 2003). limitationapproach generate input/output phrase pairs directly observedtraining corpus. resource-limited languages, standard phrasal inventorygenerally incomplete due limited parallel data. Thus, decoders hopeproducing good output find fluent, meaning-preserving translation using incompletetranslation lexicons. Synthetic phrases strategy integrating translated phrases75fiSWAHILIENGLISHsafari |||kituruki |||*OOV**OOV*ARABICENGLISH( ysAfr) ||| travelTRANSLATIONCANDIDATESARABIC -to- SWAHILIBORROWINGTsvetkov & Dyer( trky) ||| turkishFigure 4: improve resource-poor SwahiliEnglish MT system, extract translation candidatesOOV Swahili words borrowed Arabic using Swahili-to-Arabic borrowing systemArabicEnglish resource-rich MT.directly MT translation model, rather via pre- post-processing MT inputsoutputs (Tsvetkov, Dyer, Levin, & Bhatia, 2013; Chahuneau, Schlinger, Smith, & Dyer, 2013;Schlinger, Chahuneau, & Dyer, 2013; Ammar, Chahuneau, Denkowski, Hanneman, Ling,Matthews, Murray, Segall, Tsvetkov, Lavie, & Dyer, 2013; Tsvetkov, Metze, & Dyer, 2014;Tsvetkov & Dyer, 2015). Synthetic phrases phrasal translations directlyextractable training data, generated auxiliary translation posteditingprocesses (for example, extracted borrowing model). important advantagesynthetic phrases process often benefits phrase synthesizers highrecall (relative precision) since global translation model still final saywhether synthesized phrase used.OOV, borrowing system produces n-best list plausible donors;donor extract k-best list translations.9 Then, pair OOVresulting n k translation candidates. translation candidates noisy:generated donors may erroneous, errors propagated translation.10allow low-resource translation system leverage good translations missingdefault phrase inventory, able learn trustworthy are,integrate borrowing-model acquired translation candidates synthetic phrases.let translation model learn whether trust phrases, translation optionsobtained borrowing model augmented indicator feature indicatingphrase generated externally (i.e., rather extracted paralleldata). Additional features assess properties donorloan words relation; goalprovide indication plausibility pair (to mark possible errors outputsborrowing system). employ two types features: phonetic semantic. Since borrowingprimarily phonological phenomenon, phonetic features provide indication9. set n k 5; experiment values.10. give input borrowing system OOV words, although, clearly, OOVs loanwords,loanword OOVs borrowed donor language. However, important propertyborrowing model operations general, specific language-pair reducedsmall set plausible changes donor word undergo process assimilationrecipient language. Thus, borrowing system minimally overgenerates set outputcandidates given input. borrowing system encounters input word borrowedtarget donor language, usually (but always) produces empty output.76fiCross-Lingual Bridges Models Lexical Borrowingtypical (or atypical) pronunciation word language; loanwords expectedless typical core vocabulary words. goal semantic features measuresemantic similarity donor loan words: erroneous candidates borrowedwords changed meaning time expected different meaningOOV.4.1 Phonetic Featurescompute phonetic features first train (5-gram) language model (LM) IPA pronunciations donor/recipient language vocabulary (p ). Then, re-score pronunciationsdonor loanword candidates using LMs. hypothesize donorloanwordpairs donor loanword phone LM score high. capture intuitionthree features: f1 = p (donor), f2 = p (loanword), harmonic mean two1 f2scores f3 = f2f1 +f(the harmonic mean set values high values2high).4.2 Semantic Featurescompute semantic similarity feature candidate donor OOVloanword follows. first train, using large monolingual corpora, 100-dimensional wordvector representations donor recipient language vocabularies.11 Then, employcanonical correlation analysis (CCA) small donorloanword dictionaries (training setsborrowing models) project word embeddings 50-dimensional vectorsmaximized correlation dimensions. semantic feature annotatingsynthetic translation candidates cosine distance resulting donor loanwordvectors. use word2vec Skip-gram model (Mikolov, Sutskever, Chen, Corrado, &Dean, 2013) train monolingual vectors,12 CCA-based tool (Faruqui & Dyer, 2014)projecting word vectors.135. Experimentsturn problem empirically validating model proposed.evaluation consists two parts. First, perform intrinsic assessment modelsability learn borrowing correspondences compare similar approachesuse less linguistic knowledge used solve similar string mappingproblems. Second, show effect borrowing-augmented translations translationsystems, exploring effects features proposed above.5.1 Intrinsic Evaluation Models Lexical Borrowingexperimental setup defined follows. input borrowing modelloanword candidate Swahili/Maltese/Romanian, outputs plausible donor wordsArabic/Italian/French monolingual lexicon (i.e., word pronunciation dictionary).11. assume parallel data limited recipient language, monolingual data available.12. code.google.com/p/word2vec13. github.com/mfaruqui/eacl14-cca77fiTsvetkov & Dyertrain borrowing model using small set training examples, evaluateusing held-out test set. rest section describe detail datasets, tools,experimental results.5.1.1 Resourcesemploy ArabicEnglish SwahiliEnglish bitexts extract training set (corporasizes 5.4M 14K sentence pairs, respectively), using cognate discovery technique(Kondrak, 2001). Phonetically semantically similar strings classified cognates;phonetic similarity string similarity phonetic representations, semanticsimilarly approximated translation.14 thereby extract Arabic Swahili pairs(a,s)ha, si phonetically similar ( min(|a|,|s|)< 0.5) (a, s) Levenshteindistance aligned English word e. FastAlign (Dyer,Chahuneau, & Smith, 2013) used word alignments. Given extracted word pairha, si, also extract word pairs {ha0 , si} proper Arabic words a0 sharelemma producing average 33 Arabic types per Swahili type. use MADA(Habash, Rambow, & Roth, 2009) Arabic morphological expansion.resulting dataset 490 extracted ArabicSwahili borrowing examples,15set aside randomly sampled 73 examples (15%) evaluation,16 use remaining 417examples model parameter optimization. ItalianMaltese language pair, usetechnique extract 425 training 75 (15%) randomly sampled test examples.FrenchRomanian language pair, use existing small annotated set borrowingexamples,17 282 training 50 (15%) randomly sampled test examples.use pyfsta Python interface OpenFst (Allauzen, Riley, Schalkwyk, Skut, &Mohri, 2007)for borrowing model implementation.185.1.2 Baselinescompare model several baselines. Levenshtein distance baselines choseclosest word (either surface pronunciation-based). cognates baselines,evaluate variant Levenshtein distance tuned identify cognates (Mann & Yarowsky,2001; Kondrak & Sherif, 2006); method identified Kondrak Sherif (2006)among top three cognate identification methods. transliteration baselinesgenerate plausible transliterations input Swahili (or Romanian) words donorlexicon using model Ammar, Dyer, Smith (2012), multiple referenceslattice without reranking. CRF transliteration model linear-chain CRFlabel source character sequence target characters. features labelunigrams, label bigrams, label conjoined moving window source characters.OT-uniform baselines, evaluate accuracy borrowing model uniform14. cognate discovery technique sufficient extract small training set, generally applicable,requires parallel corpora manually constructed dictionaries measure semantic similarity. Largeparallel corpora unavailable language pairs, including SwahiliEnglish.15. training/test example one Swahili word corresponds extracted Arabic donor words.16. manually verified test set contains clear ArabicSwahili borrowings. example, extractSwahili kusafiri, safari Arabic Q@, Q, Q (Alsfr, ysAfr, sfr) aligned travel.17. http://wold.clld.org/vocabulary/818. https://github.com/vchahun/pyfst78fiCross-Lingual Bridges Models Lexical BorrowingReachabilityAmbiguityarsw87.7%857itmt92.711frro82.0%12Table 5: evaluation borrowing model design. Reachability percentage donorrecipient pairs reachable donor recipient language. Ambiguity averagenumber outputs model generates per one input.weights, thus shortest path loanwords transducer forms violatefewest constraints.5.1.3 Evaluationaddition predictive accuracy models (if model produces multiple hypotheses1-best weight, count proportion correct outputs set),evaluate two particular aspects proposed model: (1) appropriateness modelfamily, (2) quality learned OT constraint weights. first aspect designedevaluate whether morpho-phonological transformations implemented modelrequired sufficient generate loanwords donor inputs. report twoevaluation measures: model reachability ambiguity. Reachability percentagetest samples reachable (i.e., path input test example correctoutput) loanword transducer. nave model generates possible stringswould score 100% reachability; however, inference may expensive discriminativecomponent greater burden. order capture trade-off, also reportinherent ambiguity model, average number outputs potentiallygenerated per input. generic ArabicSwahili transducer, example, ambiguity786,998the size Arabic pronunciation lexicon.195.1.4 Resultsreachability ambiguity borrowing model listed Table 5. Briefly,model obtains high reachability, significantly reducing average number possibleoutputs per input: Arabic 787K 857 words, Maltese 129K 11, French62K 12. result shows loanword transducer design, based priorlinguistic analysis, plausible model word borrowing. Yet, average 33correct Arabic words possible 857 outputs, thus second part modelOTconstraint weights optimizationis crucial.accuracy results Table 6 show challenging task modeling lexicalborrowing two distinct languages is, importantly, orthographicphonetic baselines including state-of-the-art generative model transliterationsuitable task. Phonetic baselines ArabicSwahili perform better orthographicones, substantially worse OT-based models, even OT constraints weighted.Crucially, performance borrowing model learned OT weights corroborates19. measure ambiguity equivalent perplexity assuming uniform distribution output forms.79fiTsvetkov & DyerOrthographic baselinesPhonetic baselinesOTLevenshtein-orthographicTransliterationLevenshtein-pronunciationCognatesOT-uniform constraint weightsOT-learned constraint weightsAccuracy (%)arsw itmt frro8.961.538.016.461.336.019.864.426.319.763.730.729.365.658.548.483.375.6Table 6: evaluation borrowing model accuracy. compare following setups:orthographic (surface) phonetic (based pronunciation lexicon) Levenshtein distance, cognateidentification model uses heuristic Levenshtein distance lower penalty vowel updatessimilar letter/phone substitutions, CRF transliteration model, model uniformlearned OT constraint weights assignment.assumption made numerous linguistic accounts OT adequate analysislexical borrowing phenomenon.5.1.5 Qualitative Evaluationconstraint ranking learned borrowing model (constraints listed Tables 3, 4)line prior linguistic analysis. Swahili no-coda dominates markednessconstraints. *complex-s *complex-c, restricting consonant clusters, dominate*complex-v, confirming Swahili permissive vowel clusters. sspsonority-basedconstraintcaptures common pattern consonant clustering, found across languages,also learned model undominated competitors Swahili,dominating markedness constraint Romanian. Morphologically-motivated constraintsalso comply tendencies discussed linguistic literature: donor words may remainunmodified treated stem, reinfected according recipientmorphology, thus dep-io-morph dominated easily max-io-morph. Finally,vowel epenthesis dep-io-v common strategy Arabic loanword adaptation,ranked lower according model; however, ranked highly FrenchRomanianmodel, vowel insertion rare.second interesting by-product model inferred syllabification.conduct systematic quantitative evaluation, higher-ranked Swahili outputs tendcontain linguistically plausible syllabifications, although syllabification transducer insertsoptional syllable boundaries every pair phones. result attestsplausible constraint ranking learned model. Example Swahili syllabifications20 alongconstraint violations produced borrowing model depicted Table 7.5.2 Extrinsic Evaluation Pivoting via Borrowing MTturn extrinsic evaluation, looking two low-resource translation tasks: SwahiliEnglish translation (resource-rich donor language: Arabic), MalteseEnglish translation20. chose examples ArabicSwahili system challenging case duelinguistic discrepancies.80fiCross-Lingual Bridges Models Lexical Borrowingenbookpalacear orth.ktAbAlqSrar pron.kitAbAlqaSrsw syl.ki.ta.bu.ka.sriwageAjrhAujrahu.ji.ra.Violated constraintsident-io-c-ghA, ai, dep-io-vh, uimax-io-morphhAl, i, ident-io-c-shq, ki,ident-io-c-ehS, si, *complex-chsri, dep-io-vh, iimax-io-vhA, i, onsethui ,dep-io-vh, ii, max-io-chh,Table 7: Examples inferred syllabification corresponding constraint violations producedborrowing model.(resource-rich donor language: Italian), RomanianEnglish translation (resource-richdonor language: French). begin reviewing datasets used, discuss twooracle experiments attempt quantify much value could obtain perfectborrowing model (since mistakes made MT systems involve borrowed words).Armed understanding, explore much improvement obtainedusing system.5.2.1 Datasets SoftwareSwahiliEnglish parallel corpus crawled Global Voices project website21 .MalteseEnglish language pair, sample parallel corpus sizeEUbookshop corpus OPUS collection (Tiedemann, 2012). Similarly, simulateresource-poor scenario RomanianEnglish language pair, sample corpustranscribed TED talks (Cettolo, Girardi, & Federico, 2012). evaluate translationimprovement corpora different sizes conduct experiments sub-sampled 4,000,8,000, 14,000 parallel sentences training corpora (the smaller trainingcorpus, OOVs has). Corpora sizes along statistics source-side OOVtokens types given Table 8. Statistics held-out dev test sets usedtranslation experiments given Table 9.ArabicEnglish pivot translation system trained parallel corpus 5.4million sentences available Linguistic Data Consortium (LDC), optimizedstandard NIST MTEval dataset year 2005 (MT05). ItalianEnglish systemtrained 11 million sentences OPUS corpus. FrenchEnglish pivot systemtrained 400,000 sentences transcribed TED talks, optimizeddev talks RomanianEnglish system; test talks RomanianEnglishsystem removed FrenchEnglish training corpus.MT experiments, use cdec22 translation toolkit (Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, & Resnik, 2010), optimize parametersMERT (Och, 2003). English 4-gram language models Kneser-Ney smoothing(Kneser & Ney, 1995) trained using KenLM (Heafield, 2011) target sideparallel training corpora Gigaword corpus (Parker, Graff, Kong, Chen, & Maeda,2009). Results reported using case-insensitive BLEU single reference (Papineni,Roukos, Ward, & Zhu, 2002). verify improvements consistent21. sw.globalvoicesonline.org22. www.cdec-decoder.org81fiTsvetkov & DyerswenmtenroenTokensTypesOOV tokensOOV typesTokensTypesOOV tokensOOV typesTokensTypesOOV tokensOOV types4K84,76414,5544,465 (12.7%)3,610 (50.3%)104,18114,6054,735(8.7%)4,171 (44.0%)35,9787,2103,268 (16.6%)2,382 (55.0%)8K170,49323,1343,509 (10.0%)2,950 (41.1%)206,78122,4073,497 (6.4%)3,236 (34.2%)71,58411,1442,585 (13.1%)1,922 (44.4%)14K300,64833,2882,965 (8.4%)2,523 (35.1%)358,37331,1762,840 (5.2%)2,673 (28.2%)121,71815,1122,177 (11.1%)1,649 (38.1%)Table 8: Statistics SwahiliEnglish, MalteseEnglish, RomanianEnglish corporasource-side OOV rates 4K, 8K, 14K parallel training sentences.SentencesTokensTypesswendevtest1,5521,73233,446 35,0577,0087,180mtendevtest2,0002,00054,628 54,2729,5089,471roendevtest2,6872,26524,754 19,6595,1414,328Table 9: Dev test corpora sizes.effect optimizer instability, train three systems MT setup; reportedBLEU scores averaged systems.5.2.2 Upper Boundsgoal experiments evaluate contribution OOV dictionariesextract pivoting via borrowing, also understand potential contribution exploiting borrowing. overall improvement would achievedcould correctly translate OOVs borrowed another language?overall improvement achieved correctly translate OOVs? answerquestion defining upper bound experiments. upper bound experimentsword-align available parallel corpora, including dev test sets, extractalignments oracle translations OOV words. Then, append extracted OOVdictionaries training corpora re-train SMT setups without OOVs. Translationscores resulting system provide upper bound improvement correctlytranslating OOVs. append oracle translations subset OOV dictionaries,particular translations OOVs output borrowing systemempty, obtain upper bound achieved using method (if borrowingsystem provided perfect outputs relative reference translations). Understanding82fiCross-Lingual Bridges Models Lexical Borrowing4K5,05010,138347Loan OOVs swenLoan OOVs mtenLoan OOVs roen8K4,2196,45627114K3,5774,883216Table 10: size dictionaries extracted using pivoting via borrowing integrated translationmodels.Transliteration OOVs swenTransliteration OOVs mtenTransliteration OOVs roen4K4926,7349068K3219,04971414K2215,008578Table 11: size translated lexicons extracted using pivoting via transliteration integratedtranslation models.upper bounds relevant experiments, experiments involveaugmenting translation dictionaries; however, aware prior work providingsimilar analysis upper bounds, recommend calibrating procedurefuture work OOV mitigation strategies.5.2.3 Borrowing-Augmented Setupsdescribed 4, integrate translations OOV loanwords translation model usingsynthetic phrase paradigm. Due data sparsity, conjecture non-OOVsoccur times training corpus also lack appropriate translation candidates,target-language OOVs. therefore plug borrowing system OOVsnon-OOV words occur less 3 times training corpus. list Table 10size resulting borrowed lexicons integrate translation tables.235.2.4 Transliteration-Augmented Setupsaddition standard baselines, evaluate transliteration baselines, replaceborrowing model baselines described 5.1. borrowing system,transliteration outputs filtered contain target language lexicons. listTable 11 size obtained translated lexicons.5.2.5 ResultsTranslation results shown Table 12. evaluate separately contributionintegrated OOV translations, translations annotated phoneticsemantic features. also provide upper bound scores integrated loanword dictionarieswell recovering OOVs.23. Differences statistics stem differences types corpora, genre, domain, morphologicalrichness source language.83fiTsvetkov & DyerswenmtenroenBaseline+ Transliteration OOVs+ Loan OOVs+ FeaturesUpper bound loanUpper bound OOVsBaseline+ Transliteration OOVs+ Loan OOVs+ FeaturesUpper bound loanUpper bound OOVsBaseline+ Transliteration OOVs+ Loan OOVs+ FeaturesUpper bound loanUpper bound OOVs4K13.213.414.314.818.919.226.426.527.226.928.531.615.815.816.016.016.628.08K15.115.315.716.419.120.431.430.831.731.932.235.618.518.718.718.619.428.814K17.117.218.218.420.721.135.234.935.334.535.738.020.720.820.720.620.930.4Table 12: BLUE scores SwahiliEnglish, MalteseEnglish, RomanianEnglish MTexperiments.SwahiliEnglish MT performance improved +1.6 BLEU augmenttranslated OOV loanwords leveraged ArabicSwahili borrowingArabicEnglish MT. contribution borrowing dictionaries +0.61.1 BLEU,phonetic semantic features contribute additional half BLEU. importantly,upper bound results show system improved substantially betterdictionaries OOV loanwords. result confirms OOV borrowed wordsimportant type OOVs, proper modeling potential improve translationlarge margin. MalteseEnglish system also improved substantially, +0.8BLEU, contribution additional features less pronounced. RomanianEnglishsystems obtain small significant improvement 4K 8K, p < .01 (Clark, Dyer,Lavie, & Smith, 2011). However, expected rate borrowing FrenchRomanian smaller, and, result, integrated loanword dictionaries small.Transliteration baseline, conversely, effective RomanianFrench language pair,languages related typologically, common cognates addition loanwords.Still, even dictionaries translations pivoting via borrowing/transliterationimprove, even almost approach upper bounds results.5.2.6 Error Analysisaugmented MT systems combine three main components: translation system itself,borrowing system, pivot translation system. step application errors mayoccur lead erroneous translations. identify main sources errors Swahili84fiCross-Lingual Bridges Models Lexical BorrowingError sourceReachability borrowing systemLoanword production errorsArabicEnglish translation errorsSwahiliEnglish translation errors#1131912029%32.054.15.78.2Table 13: Sources errors.English end-to-end system, conducted manual analysis errors translations OOVtypes produced SwahiliEnglish 4K translation systems. gold standard corpususe Helsinki Corpus Swahili24 (Hurskainen, 2004a, HCS). HCS morphologically,syntactically, semantically annotated corpus 580K sentences (12.7M tokens).corpus 52,351 surface forms (1.5M tokens) marked Arabic loanwords.3,610 OOV types SwahiliEnglish 4K translation systems, 481 word typesannotated HCS. manually annotated 481 words identified 353 errors;remaining 128 words translated correctly end-to-end system. analysisreveals error sources detailed below. Table 13 summarize statistics errorsources.1. Reachability borrowing system.368 481 input words produced loanword candidates. main reasonunreachable paths complex morphology Swahili OOVs, takenaccount borrowing system. example, atakayehusika involved,lemma husika involve.2. Loanword production errors.half errors due incorrect outputs borrowing system.line ArabicSwahili borrowing system accuracy reported Table 6.example, morphological variants lemma wahi never (hayajawahi, halijawahi,hazijawahi), incorrectly produced Arabic donor word Ag. (jAwh) java. Additional examples include variants lemma saidia help (isaidie, kimewasaidia)produced Arabic donor candidates variants proper name Saidia.3. ArabicEnglish translation errors.frequent source errors ArabicEnglish MT system, identified OOV Arabic words. example, although Swahili loanword awashukuruthank borrowing system correctly produced plausible donor word P(w$kwr) thank (rarely used), translation variant producedArabicEnglish MT kochkor.4. SwahiliEnglish translation errors.cases, although borrowing system produced correct donor candidate,ArabicEnglish translation also correct, translation variants differentreference translations SwahiliEnglish MT system. example,24. www.aakkl.helsinki.fi/cameel/corpus/intro.htm85fiTsvetkov & Dyerword alihuzunika grieved correctly produced Arabic donor Qm '@ (AlHzn) grief.Translation variants produced ArabicEnglish MT sadness, grief, saddened,sorrow, sad, mourning, grieved, saddening, mourn, distressed, whereas expectedtranslation SwahiliEnglish reference translations disappointed. Anothersource errors occurred despite correct outputs borrowing translationsystems historical meaning change words. interesting example semanticshift word sakafu floor, borrowed Arabic word (sqf )ceiling.Complex morphology Swahili Arabic frequent source errorssteps application. Concatenation several prefixes Swahili affects reachabilityborrowing system. Swahili prefixes flip meaning words, examplekutoadhibiwa impunity, produces lemma adhibiwa punishment, consequentlytranslations torture, torturing, tortured. Finally, derivational processes languageshandled system, example, verb aliyorithi inherited, producesArabic noun KP@@ (AlwArvp) heiress, English translations heiress. Jointlyreasoning morphological processes donor recipient languages suggestspossible avenue remedying issues.6. Additional Related Workexception study conducted Blair Ingram (2003) generationborrowed phonemes EnglishJapanese language pair (the method generalizeborrowed phonemes borrowed words, rely linguistic insights),aware prior work computational modeling lexical borrowing.papers mention tangentially address borrowing, briefly list here. Daum III(2009) focuses areal effects linguistic typology, broader phenomenon includesborrowing genetic relations across languages. study aimed discovering languageareas based typological features languages. Garley Hockenmaier (2012) trainmaximum entropy classifier character n-gram morphological features identifyanglicisms (which compare loanwords) online community German hip hopfans. Finally, List Moran (2013) published toolkit computational taskshistorical linguistics remark Automatic approaches borrowing detectionstill infancy historical linguistics.7. ConclusionGiven loanword, model identifies plausible donor words contact language.show discriminative model Optimality Theoretic features effectively modelssystematic phonological changes ArabicSwahili loanwords. also found modelmethodology generally applicable language pairs minimal engineeringeffort. translation results substantially improve baseline confirm OOVloanwords important merit investigation.numerous research questions would like explore further. possiblemonolingually identify borrowed words language? automatically identifydonor language (or phonological properties) borrowed word? Since languages may86fiCross-Lingual Bridges Models Lexical Borrowingborrow many sources, jointly modeling process lead better performance?reduce amount language-specific engineering required deploy model?integrate knowledge borrowing additional downstream NLP applications?intend address questions future work.Acknowledgmentsthank Nathan Schneider, Shuly Wintner, Llus Mrquez, anonymous reviewershelp constructive feedback. also thank Waleed Ammar helpArabic. work supported part U.S. Army Research LaboratoryU.S. Army Research Office contract/grant number W911NF-10-1-0533, partNational Science Foundation award IIS-1526745. Computational resourcesprovided Google form Google Cloud Computing grant NSFXSEDE program TG-CCR110017.ReferencesAdler, A. N. (2006). Faithfulness perception loanword adaptation: case studyHawaiian. Lingua, 116 (7), 10241045.Ahn, S.-C., & Iverson, G. K. (2004). Dimensions Korean laryngeal phonology. JournalEast Asian Linguistics, 13 (4), 345379.Al-Onaizan, Y., & Knight, K. (2002). Machine transliteration names Arabic text.Proc. ACL workshop Computational Approaches Semitic Languages, pp. 113.Association Computational Linguistics.Allauzen, C., Riley, M., Schalkwyk, J., Skut, W., & Mohri, M. (2007). OpenFst: generalefficient weighted finite-state transducer library. Implementation ApplicationAutomata, pp. 1123. Springer.Ammar, W., Chahuneau, V., Denkowski, M., Hanneman, G., Ling, W., Matthews, A., Murray,K., Segall, N., Tsvetkov, Y., Lavie, A., & Dyer, C. (2013). cmu machine translationsystems WMT 2013: Syntax, synthetic translation options, pseudo-references.Proc. WMT.Ammar, W., Dyer, C., & Smith, N. A. (2012). Transliteration sequence labelinglattice encodings reranking. Proc. NEWS workshop ACL.Blair, A. D., & Ingram, J. (2003). Learning predict phonological structure Englishloanwords Japanese. Applied Intelligence, 19 (1-2), 101108.Boersma, P., & Hayes, B. (2001). Empirical tests gradual learning algorithm. Linguisticinquiry, 32 (1), 4586.Broselow, E. (2004). Language contact phonology: richness stimulus, povertybase. Proc. NELS, Vol. 34, pp. 122.Burkett, D., & Klein, D. (2008). Two languages better one (for syntactic parsing).Proc. EMNLP, pp. 877886.87fiTsvetkov & DyerCalabrese, A., & Wetzels, W. L. (2009). Loan phonology, Vol. 307. John BenjaminsPublishing.Callison-Burch, C., Koehn, P., & Osborne, M. (2006). Improved statistical machine translation using paraphrases. Proc. ACL.Cettolo, M., Girardi, C., & Federico, M. (2012). WIT3 : Web inventory transcribedtranslated talks. Proc. EAMT, pp. 261268.Chahuneau, V., Schlinger, E., Smith, N. A., & Dyer, C. (2013). Translating morphologically rich languages synthetic phrases. Proc. EMNLP, pp. 16771687.Clark, J. H., Dyer, C., Lavie, A., & Smith, N. A. (2011). Better hypothesis testingstatistical machine translation: Controlling optimizer instability. Proc. ACL, pp.176181.Comrie, B., & Spagnol, M. (2015). Maltese loanword typology. Submitted.Das, D., & Petrov, S. (2011). Unsupervised part-of-speech tagging bilingual graph-basedprojections. Proc. ACL, pp. 600609. Association Computational Linguistics.Daum III, H. (2009). Non-parametric Bayesian areal linguistics. Proc. NAACL, pp.593601. Association Computational Linguistics.Davidson, L., & Noyer, R. (1997). Loan phonology Huave: nativization rankingfaithfulness constraints. Proc. WCCFL, Vol. 15, pp. 6579.De Gispert, A., & Marino, J. B. (2006). Catalan-English statistical machine translationwithout parallel corpus: bridging Spanish. Proc. LREC, pp. 6568.Dholakia, R., & Sarkar, A. (2014). Pivot-based triangulation low-resource languages.Proc. AMTA.Diab, M., & Resnik, P. (2002). unsupervised method word sense tagging using parallelcorpora. Proc. ACL.Durrani, N., Sajjad, H., Fraser, A., & Schmid, H. (2010). Hindi-to-Urdu machine translationtransliteration. Pro. ACL, pp. 465474.Dyer, C., Chahuneau, V., & Smith, N. A. (2013). simple, fast, effective reparameterization IBM Model 2. Proc. NAACL.Dyer, C., Lopez, A., Ganitkevitch, J., Weese, J., Ture, F., Blunsom, P., Setiawan, H.,Eidelman, V., & Resnik, P. (2010). cdec: decoder, alignment, learning frameworkfinite-state context-free translation models. Proc. ACL.Eisner, J. (1997). Efficient generation primitive Optimality Theory. Proc. EACL, pp.313320.Eisner, J. (2002). Comprehension compilation Optimality Theory. Proc. ACL, pp.5663.Ellison, T. M. (1994). Phonological derivation Optimality Theory. Proc. CICLing, pp.10071013.Fabri, R., Gasser, M., Habash, N., Kiraz, G., & Wintner, S. (2014). Linguistic introduction:orthography, morphology syntax Semitic languages. Natural LanguageProcessing Semitic Languages, pp. 341. Springer.88fiCross-Lingual Bridges Models Lexical BorrowingFaruqui, M., & Dyer, C. (2014). Improving vector space word representations using multilingual correlation. Proc. EACL.Ganchev, K., Gillenwater, J., & Taskar, B. (2009). Dependency grammar induction via bitextprojection constraints. Proc. ACL, pp. 369377. Association ComputationalLinguistics.Garley, M., & Hockenmaier, J. (2012). Beefmoves: dissemination, diversity, dynamicsEnglish borrowings German hip hop forum. Proc. ACL, pp. 135139.Goldwater, S., & Johnson, M. (2003). Learning OT constraint rankings using maximumentropy model. Proc. Stockholm workshop variation within Optimality Theory,pp. 111120.Habash, N. (2008). Four techniques online handling out-of-vocabulary wordsArabic-English statistical machine translation. Proc. ACL, pp. 5760.Habash, N., & Hu, J. (2009). Improving Arabic-Chinese statistical machine translation usingEnglish pivot language. Proc. WMT, pp. 173181.Habash, N., Rambow, O., & Roth, R. (2009). MADA+TOKAN: toolkit Arabictokenization, diacritization, morphological disambiguation, POS tagging, stemminglemmatization. Proc. MEDAR, pp. 102109.Haghighi, A., Liang, P., Berg-Kirkpatrick, T., & Klein, D. (2008). Learning bilingual lexiconsmonolingual corpora. Proc. ACL, pp. 771779.Haji, J., Hric, J., & Kubo, V. (2000). Machine translation close languages.Proc. ANLP, pp. 712.Haspelmath, M. (2009). Lexical borrowing: concepts issues. Loanwords WorldsLanguages: comparative handbook, 3554.Haspelmath, M., & Tadmor, U. (Eds.). (2009). Loanwords Worlds Languages:Comparative Handbook. Max Planck Institute Evolutionary Anthropology, Leipzig.Haugen, E. (1950). analysis linguistic borrowing. Language, 210231.Hayes, B., Tesar, B., & Zuraw, K. (2013). OTSoft 2.3.2..Heafield, K. (2011). KenLM: Faster smaller language model queries. Proc. WMT.Hermjakob, U., Knight, K., & Daum III, H. (2008). Name translation statistical machinetranslation-learning transliterate. Proc. ACL, pp. 389397.Hock, H. H., & Joseph, B. D. (2009). Language history, language change, languagerelationship: introduction historical comparative linguistics, Vol. 218. Walterde Gruyter.Holden, K. (1976). Assimilation rates borrowings phonological productivity. Language,131147.Hurskainen, A. (2004a). HCS 2004Helsinki corpus Swahili. Tech. rep., Compilers:Institute Asian African Studies (University Helsinki) CSC.Hurskainen, A. (2004b). Loan words Swahili. Bromber, K., & Smieja, B. (Eds.),Globalisation African Languages, pp. 199218. Walter de Gruyter.89fiTsvetkov & DyerHwa, R., Resnik, P., Weinberg, A., Cabezas, C., & Kolak, O. (2005). Bootstrapping parsersvia syntactic projection across parallel texts. Natural Language Engineering, 11 (3).Jacobs, H., & Gussenhoven, C. (2000). Loan phonology: perception, salience, lexiconOT. Optimality Theory: Phonology, syntax, acquisition, 193209.Johnson, F. (1939). Standard Swahili-English dictionary. Oxford University Press.Kager, R. (1999). Optimality Theory. Cambridge University Press.Kang, Y. (2003). Perceptual similarity loanword adaptation: English postvocalic word-finalstops Korean. Phonology, 20 (2), 219274.Kang, Y. (2011). Loanword phonology. van Oostendorp, M., Ewen, C., Hume, E., & Rice,K. (Eds.), Companion Phonology. WileyBlackwell.Kawahara, S. (2008). Phonetic naturalness unnaturalness Japanese loanword phonology. Journal East Asian Linguistics, 17 (4), 317330.Kenstowicz, M. (2007). Salience similarity loanword adaptation: case studyFijian. Language Sciences, 29 (2), 316340.Kenstowicz, M., & Suchato, A. (2006). Issues loanword adaptation: case studyThai. Lingua, 116 (7), 921949.Kneser, R., & Ney, H. (1995). Improved backing-off m-gram language modeling. Proc.ICASSP, Vol. 1, pp. 181184. IEEE.Knight, K., & Graehl, J. (1998). Machine transliteration. Computational Linguistics, 24 (4),599612.Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation. Proc.NAACL-HLT, pp. 4854.Kondrak, G. (2001). Identifying cognates phonetic semantic similarity. Proc.NAACL, pp. 18. Association Computational Linguistics.Kondrak, G., Marcu, D., & Knight, K. (2003). Cognates improve statistical translationmodels. Proc. HLT-NAACL, pp. 4648. Association Computational Linguistics.Kondrak, G., & Sherif, T. (2006). Evaluation several phonetic similarity algorithmstask cognate identification. Proc. Workshop Linguistic Distances, pp.4350. Association Computational Linguistics.Kozhevnikov, M., & Titov, I. (2013). Cross-lingual transfer semantic role labeling models.Proc. ACL, pp. 11901200.Kuhn, J. (2004). Experiments parallel-text based grammar induction. Proc. ACL, p.470.Li, S., Graa, J. V., & Taskar, B. (2012). Wiki-ly supervised part-of-speech tagging. Proc.EMNLP, pp. 13891398.List, J.-M., & Moran, S. (2013). open source toolkit quantitative historical linguistics.Proc. ACL (System Demonstrations), pp. 1318.Littell, P., Price, K., & Levin, L. (2014). Morphological parsing Swahili using crowdsourcedlexical resources. Proc. LREC.90fiCross-Lingual Bridges Models Lexical BorrowingMaamouri, M., Graff, D., Bouziri, B., Krouna, S., & Kulick, S. (2010). LDC Standard Arabicmorphological analyzer (SAMA) v. 3.1..Mann, G. S., & Yarowsky, D. (2001). Multipath translation lexicon induction via bridgelanguages. Proc. HLT-NAACL, pp. 18.Marton, Y., Callison-Burch, C., & Resnik, P. (2009). Improved statistical machine translationusing monolingually-derived paraphrases. Proc. EMNLP, pp. 381390.McCarthy, J. J. (1985). Formal problems Semitic phonology morphology. Ph.D. thesis,MIT.McCarthy, J. J. (2009). Optimality Theory: Applying theory data. John Wiley &Sons.McCarthy, J. J., & Prince, A. (1995). Faithfulness reduplicative identity. Beckman etal. (Eds.), 249384.Metze, F., Hsiao, R., Jin, Q., Nallasamy, U., & Schultz, T. (2010). 2010 CMU GALEspeech-to-text system. Proc. INTERSPEECH, pp. 15011504.Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributedrepresentations words phrases compositionality. Proc. NIPS, pp.31113119.Moravcsik, E. (1978). Language contact. Universals human language, 1, 93122.Mwita, L. C. (2009). adaptation Swahili loanwords Arabic: constraint-basedanalysis. Journal Pan African Studies.Myers-Scotton, C. (2002). Contact linguistics: Bilingual encounters grammatical outcomes. Oxford University Press Oxford.Nakov, P., & Ng, H. T. (2012). Improving statistical machine translation resourcepoor language using related resource-rich languages. Journal Artificial IntelligenceResearch, 179222.Nelder, J. A., & Mead, R. (1965). simplex method function minimization. Computerjournal, 7 (4), 308313.Och, F. J. (2003). Minimum error rate training statistical machine translation. Proc.ACL, pp. 160167.Pad, S., & Lapata, M. (2009). Cross-lingual annotation projection semantic roles.Journal Artificial Intelligence Research, 36 (1), 307340.Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). bleu: method automaticevaluation machine translation. Proc. ACL, pp. 311318.Parker, R., Graff, D., Kong, J., Chen, K., & Maeda, K. (2009). English Gigaword fourthedition..Polom, E. C. (1967). Swahili Language Handbook. ERIC.Prince, A., & Smolensky, P. (2008). Optimality Theory: Constraint interaction generativegrammar. John Wiley & Sons.91fiTsvetkov & DyerRapp, R. (1995). Identifying word translations non-parallel texts. Pro. ACL, pp.320322.Razmara, M., Siahbani, M., Haffari, R., & Sarkar, A. (2013). Graph propagationparaphrasing out-of-vocabulary words statistical machine translation. Proc. ACL,pp. 11051115.Repetti, L. (2006). emergence marked structures integration loans Italian.Amsterdam Studies Theory History Linguistic Science Series 4, 274, 209.Rose, Y., & Demuth, K. (2006). Vowel epenthesis loanword adaptation: Representationalphonetic considerations. Lingua, 116 (7), 11121139.Rothman, N. C. (2002). Indian Ocean trading links: Swahili experience. ComparativeCivilizations Review, 46, 7990.Saluja, A., Hassan, H., Toutanova, K., & Quirk, C. (2014). Graph-based semi-supervisedlearning translation models monolingual data. Proc. ACL, pp. 676686.Sankoff, G. (2002). Linguistic outcomes language contact. Chambers, J., Trudgill, P.,& Schilling-Estes, N. (Eds.), Handbook Sociolinguistics, pp. 638668. Blackwell.Schadeberg, T. C. (2009). Loanwords Swahili. Haspelmath, M., & Tadmor, U. (Eds.),Loanwords Worlds Languages: Comparative Handbook, pp. 76102. MaxPlanck Institute Evolutionary Anthropology.Schlinger, E., Chahuneau, V., & Dyer, C. (2013). morphogen: Translation morphologicallyrich languages synthetic phrases. Prague Bulletin Mathematical Linguistics,100, 5162.Schulte, K. (2009). Loanwords Romanian. Haspelmath, M., & Tadmor, U. (Eds.),Loanwords Worlds Languages: Comparative Handbook, pp. 230259. MaxPlanck Institute Evolutionary Anthropology.Schultz, T., & Schlippe, T. (2014). GlobalPhone: Pronunciation dictionaries 20 languages.Proc. LREC.Smith, D. A., & Smith, N. A. (2004). Bilingual parsing factored estimation: Usingenglish parse korean.. Proc. EMNLP, pp. 4956.Smith, J. R., Saint-Amand, H., Plamada, M., Koehn, P., Callison-Burch, C., & Lopez, A.(2013). Dirt cheap web-scale parallel text Common Crawl. Proc. ACL, pp.13741383.Snyder, B., Naseem, T., & Barzilay, R. (2009). Unsupervised multilingual grammar induction.Proc. ACL/AFNLP, pp. 7381.Tckstrm, O., Das, D., Petrov, S., McDonald, R., & Nivre, J. (2013). Token typeconstraints cross-lingual part-of-speech tagging.. Transactions AssociationComputational Linguistics, 1, 112.Tadmor, U. (2009). Loanwords worlds languages: Findings results. Haspelmath,M., & Tadmor, U. (Eds.), Loanwords Worlds Languages: ComparativeHandbook, pp. 5575. Max Planck Institute Evolutionary Anthropology.92fiCross-Lingual Bridges Models Lexical BorrowingThomason, S. G., & Kaufman, T. (2001). Language contact. Edinburgh University PressEdinburgh.Tiedemann, J. (2012). Parallel data, tools interfaces OPUS. Proc. LREC, pp.22142218.Tiedemann, J. (2014). Rediscovering annotation projection cross-lingual parser induction.Proc. COLING.Tsvetkov, Y., Ammar, W., & Dyer, C. (2015). Constraint-based models lexical borrowing.Proc. NAACL, pp. 598608.Tsvetkov, Y., Boytsov, L., Gershman, A., Nyberg, E., & Dyer, C. (2014). Metaphor detectioncross-lingual model transfer. Proc. ACL, pp. 248258.Tsvetkov, Y., & Dyer, C. (2015). Lexicon stratification translating out-of-vocabularywords. Proc. ACL.Tsvetkov, Y., Dyer, C., Levin, L., & Bhatia, A. (2013). Generating English determinersphrase-based translation synthetic translation options. Proc. WMT.Tsvetkov, Y., Metze, F., & Dyer, C. (2014). Augmenting translation models simulatedacoustic confusions improved spoken language translation. Proc. EACL, pp.616625.Van Coetsem, F. (1988). Loan phonology two transfer types language contact.Walter de Gruyter.Wang, P., Nakov, P., & Ng, H. T. (2012). Source language adaptation resource-poormachine translation. Proc. EMNLP, pp. 286296.Weinreich, U. (1979). Languages contact: Findings problems. Walter de Gruyter.Whitney, W. D. (1881). mixture language. Transactions American PhilologicalAssociation (1870), 526.Wu, D. (1997). Stochastic inversion transduction grammars bilingual parsing parallelcorpora. Computational linguistics, 23 (3), 377403.Xi, C., & Hwa, R. (2005). backoff model bootstrapping resources non-Englishlanguages. Proc. EMNLP, pp. 851858.Yarowsky, D., Ngai, G., & Wicentowski, R. (2001). Inducing multilingual text analysis toolsvia robust projection across aligned corpora. Proc. HLT, pp. 18.Yip, M. (1993). Cantonese loanword phonology Optimality Theory. Journal EastAsian Linguistics, 2 (3), 261291.Zawawi, S. (1979). Loan words effect classification Swahili nominals.Leiden: E.J. Brill.93fiJournal Artificial Intelligence Research 55 (2016) 889-952Submitted 9/15; published 4/16Searching Best Solutions Graphical ModelsNatalia FlerovaNFLEROVA @ UCI . EDUUniversity California, IrvineIrvine, CA 92697, USARadu MarinescuRADU . MARINESCU @ IE . IBM . COMIBM Research IrelandRina DechterDECHTER @ UCI . EDUUniversity California, IrvineIrvine, CA 92697, USAAbstractpaper focuses finding best solutions combinatorial optimization problemsusing best-first depth-first branch bound search. Specifically, present new algorithm mA*, extending well-known A* m-best task, first time prove desirableproperties, including soundness, completeness optimal efficiency, maintained. Since bestfirst algorithms require extensive memory, also extend memory-efficient depth-first branchbound m-best task.adapt algorithms optimization tasks graphical models (e.g., Weighted CSPMPE Bayesian networks), provide complexity analysis empirical evaluation. experiments confirm theory best-first approach largely superior memory available,depth-first branch bound robust. also show algorithms competitiverelated schemes recently developed m-best task.1. Introductionusual aim combinatorial optimization find optimal solution, minimum maximum,objective function. However, many applications desirable obtain singleoptimal solution, set first best solutions integer m. motivatedmany real-life domains, task arises. instance, problem finding likelyhaplotype pedigree presented finding probable assignment Bayesiannetwork encodes genetic information (Fishelson, Dovgolevsky, & Geiger, 2005). practice data often corrupted missing, makes single optimal solution unreliable.possible increase confidence answer finding set best solutionschoosing final solution expert help obtaining additional genetic data.examples m-best tasks arise procurement auction problems probabilistic expert systems, certain constraints often cannot directly incorporated model, eithermake problem infeasibly complex vague formalize (e.g. idiosyncraticpreferences human user). Thus domains may practical first find severalgood solutions relaxed problem pick one satisfies additional constraintspost-processing manner. Additionally, sometimes set diverse assignments approximatelycost required, reliable communication network design. Finally, contextsummation problem graphical models, probability evidence partition function,approximation derived summing likely tuples.c2016AI Access Foundation. rights reserved.fiF LEROVA , ARINESCU , & ECHTERproblem finding best solutions well studied. One earliestinfluential works belongs Lawler (1972). provided general scheme extendsoptimization algorithm m-best task. idea compute next best solution successivelyfinding single optimal solution slightly different reformulation original problemexcludes solutions generated far. approach extended improvedyears still one primary strategies finding best solutions. approachesdirect, trying avoid repeated computation inherent Lawlers scheme. Two earlierworks relevant provide highest challenge work Nilsson (1998)Aljazzar Leue (2011).Nilsson proposed junction-tree based message-passing scheme iteratively findsbest solutions. claimed best runtime complexity among m-best schemesgraphical models. analysis (Section 6) shows indeed Nilssons scheme secondbest worst case time complexity algorithm BE+m-BF (Section 5.3). However,practice scheme feasible problems large induced width.recent work Aljazzar Leue proposed algorithm called K*, A* search-stylescheme finding k shortest paths interleaved breadth-first search. usedspecialized data structure unclear approach straightforwardly extendedgraphical models, point leave future work.One popular approximate approaches solving optimization problems basedLP-relaxation problem (Wainwright & Jordan, 2003). m-best extension approach(Fromer & Globerson, 2009) guarantee exact solutions, quite efficient practice.discuss previous works Section 6.main focus lies optimization context graphical models, Bayesian networks, Markov networks constraint networks. However, algorithms developedused general purpose tasks, finding shortest paths graph. Variousgraph-exploiting algorithms solving optimization tasks graphical models developedpast decades. algorithms often characterized either inference type(e.g., message-passing schemes, variable elimination) search type (e.g., AND/OR searchrecursive-conditioning). earlier works, (e.g., Flerova, Dechter, & Rollon, 2011), extendedinference schemes represented bucket elimination algorithm (BE) (Dechter, 1999, 2013)task finding best solutions. However, due large memory requirements, variableelimination algorithms, including bucket elimination, cannot used practice finding exactsolutions combinatorial optimization tasks problems graph dense. Depth-first branchbound (DFBnB) best-first search (BFS) flexible trade space time.work explores question solving best solutions task using heuristic search schemes.contribution lies extending heuristic algorithms best solutions task. describe general purpose m-best variants depth-first branch bound best-first search,specifically A*, yielding algorithms m-BB m-A* respectively, analyze properties. show m-A* inherits A*s desirable properties (Dechter & Pearl, 1985),significantly optimally efficient compared alternative exact search-based scheme.also discuss size search space explored m-BB. extend new m-best algorithms graphical models exploring AND/OR search space.evaluate resulting algorithms 6 benchmarks 300 instances total,examine impact number solutions algorithms behaviour. particular,890fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSobserve runtime schemes (except depth-first branch boundexploring AND/OR tree) scales much better worst case theoretical analysissuggests.also show m-A* search using exact bucket elimination heuristic (a schemecall BE+m-BF) highly efficient easier problems suffers severely memory issuesdenser graphs, far A*-based schemes using approximate mini-bucket heuristics. Finally, compare schemes efficient algorithms basedLP-relaxation (Fromer & Globerson, 2009; Batra, 2012), showing competitiveness even superiority large values (m 10), providing optimality guarantees.paper organized follows. Section 2 provide relevant background. Section 3presents extension best-first search m-best task. particular, define m-A*,extension A* algorithm finding best solutions (3.1), prove main properties (3.2).Section 4 describes algorithm m-BB, extension depth-first branch bound algorithmsolving m-best task. Section 5 discuss adaptation two newly proposed m-bestsearch algorithms AND/OR search spaces graphical models, including hybrid methodBE+m-BF incorporates variable elimination heuristic search. Section 6 elaboratesrelated work contrasts methods. Section 7 presents empirical evaluationm-best schemes Section 8 concludes.2. Backgroundbegin formally defining graphical models framework providing backgroundheuristic search.2.1 Graphical Modelsdenote variables upper-case letters (e.g., X, Y, Z) values variables lower-caseletters (e.g., x, y, z). Sets variables denoted upper-case letters bold (e.g. X, Y, Z).assignment (X1 = x1 , . . . , Xn = xn ) abbreviated x = (x1 , . . . , xn ).denote functions letters f, g, h etc., set functions byPF. function fscope S1 = {X1 , . . . , Xr } denoted fS1 . PsummationP operator xX defines sumpossible values variables X, namely x1 X1 , . . . , xn Xn . Minimization minxXmaximization maxxX operators defined similar manner. Note use terms eliminationPmarginalization interchangeably.convenience sometimes use minx (maxx , x )Pdenote minxX (maxxX , xX ).graphical model collection local functions subsets variables conveys probabilistic, deterministic, preferential information, whose structure described graph.graph captures independencies irrelevance information inherent model, usefulinterpreting modeled data and, significantly, exploited reasoning algorithms.set local functions combined variety ways generate global function, whosescope set variables.NEFINITION 1 (Graphical model). graphical model 4-tuple = hX, D, F, i:1. X = {X1 , . . . , Xn } finite set variables;2. = {D1 , . . . , Dn } set respective finite domains values;891fiF LEROVA , ARINESCU , & ECHTER3. F = {f1 , . . . , fr } set non-negative real-valued discrete functions, defined scopesvariables Si X. called local functions.NNQ P4.combination operator, e.g.,{ , } (product, sum)graphical model representsglobal function, whose scope X combinationNlocal functions: rj=1 fj .NPN= Qand fi : DSi N Weighted Constraint Satisfaction Problems (WCSPs).=fi = Pi (Xi | pai ) Bayesian network. probabilities Pdefined relative directed acyclic graph G X, set Xi1 , . . . , Xik parentspai Xi , i.e. Xij edge pointing Xij Xi . illustration, considerBayesian network 5 variables whose directed acyclic graph (DAG) given Figure 1(a).common optimization task Bayesian network probable explanation(MPE) also known maximum posteriori hypothesis (MAP),1 goal computeoptimal valuerC = maxfj (xSj )xj=1optimizing configurationx = argmaxxrfj (xSj )j=1related task, typical WCSP,min-sum, namely computing minimal costPP assignment (min-sum): C = minx j fj (x) optimizing configuration x = argminx j fj (x).Historically task also sometimes referred energy Qminimization. equivalentMPE/MAP task following sense: Cmax= maxx j fj (x) solution MPEproblem,P Cmax = exp (Cmin ), Cmin solution min-sum problem Cmin =minx j gj (x) j, gj (x) = log (fj (x)).graphical model defines primal graph captures dependencies problemsvariables. variables vertices. edge connects two vertices whose variables appearscope function. important property graphical model, characterizingcomplexity reasoning tasks induced width. ordered graph pair (G, o)G undirected graph, = (X1 , . . . , Xn ) ordering nodes. width nodenumber nodes neighbors precede ordering. width graph alongordering maximum width nodes. induced ordered graph obtainedordered graph follows: nodes processed last first based o; node Xjprocessed, preceding neighbors connected. width ordered induced graph alongordering called induced width along denoted w (o). induced widthgraph, denoted w , minimal induced width orderings. Abusing notationsometimes use w denote induced width along particular ordering, meaningclear context.Figure 1(b) depicts primal graph Bayesian network Figure 1(a). Figures 1(c)1(d) show induced graphs primal graph Figure 1(a) respectively along orderings1. communities MAP also refers task optimizing partial assignment variables. However,paper use MAP MPE interchangeable, referring optimal full variable assignment.892fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSP (A)BECCEB(c)(d)P (B|A)BCBCP (C|A)EEP (E|B, C)P (D|A, B)(a)(b)Figure 1: (a) DAG Bayesian network, (b) primal graph (also called moral graph),(c) induced graph along = (A, E, D, C, B), (d) induced graph along= (A, B, C, D, E), example Gogate (2009).= (A, E, D, C, B) o0 = (A, B, C, D, E). dashed lines Figure 1(c) representinduced edges, namely edges absent moral graph, introducedinduced graph. see induced width along ordering w (o) = 4 inducedwidth along ordering o0 w (o0 ) = 2, respectively.2.2 Heuristic Searchanalysis focuses best-first search (BFS), whose behaviour task finding singleoptimal solution well understood. Assuming minimization task, best-first search always expandsnode best (i.e., smallest) value heuristic evaluation function. maintains graphexplored paths, list CLOSED expanded nodes frontier OPEN nodes. BFS choosesOPEN node n smallest value heuristic evaluation function f (n), expandsgenerating successors succ(n), places CLOSED, places succ(n) OPEN.popular variant best-first search, A*, uses heuristic evaluation function f (n) = g(n) + h(n),g(n) cost path root n, h(n) heuristic function estimatesoptimal cost go h (n) n goal node. heuristic function called admissiblenever overestimates (for minimization) true minimal cost reach goal h (n). Namely,n h(n) h (n). heuristic called consistent monotonic, every node n everysuccessor n0 n following inequality holds: h(n) c(n, n0 ) + h(n0 ). h(n) consistent,values evaluation function f (n) along path non-decreasing. knownregardless tie-breaking rule A* expands node n reachable strictly C -bounded pathroot, node referred surely expanded A* (Dechter & Pearl, 1985).path C -bounded relative f , n : f (n) < C , C cost optimalsolution.A* search number attractive properties (Nillson, 1980; Pearl, 1984; Dechter & Pearl,1985):893fiF LEROVA , ARINESCU , & ECHTERSoundness completeness: A* terminates optimal solution.h consistent, A* explores set nodes = {n|f (n) C } surelyexpands nodes = {n|f (n) < C }.Optimal efficiency consistent heuristic: h consistent, node surely expanded A* must expanded sound complete search algorithm usingheuristic information.Optimal efficiency node expansions: heuristic function consistent, A*,searching graph, expands node once, time nodes expansionA* found shortest path it.Dominance: Given two heuristic functions h1 h2 , s.t. n h1 (n) < h2 (n), A1 expandevery node surely expanded A2 , Ai uses heuristic hi .Although best-first search known best algorithm terms number nodes expanded (Dechter & Pearl, 1985), requires exponential memory worst-case.popular alternative depth-first branch bound (DFBnB), whose attractive feature, compared best-first search, executed linear memory. Yet,search space graph, exploit memory improve performance flexibly trading spacetime. Depth-first branch bound expands nodes depth-first manner, maintaining costbest solution found far upper bound U B cost optimal solution.heuristic evaluation function current node n greater equal upper bound,node pruned subtree never explored. worst case depth-first branchbound explores entire search space. best case first solution found optimal,case performance good BFS. However, solution depth unbound, depth-firstsearch might follow infinite branch never terminate. Also, search space graph,DFBnB may expand nodes numerous time, unless uses caching checks duplicates.2.3 Search Graphical ModelsSearch algorithms provide way systematically enumerate possible assignments givengraphical model. Optimization problems graphical models naturally presentedtask finding optimal cost path appropriate search space.simplest variant search space so-called search tree. level correspondsvariable original problem. nodes correspond partial variable assignmentsarc weights derived problems input functions. size search tree boundedO(k n ), n number variables k maximum domain size.Throughout section going illustrate concepts using example problemsix variables {A, B, C, D, E, F } six pairwise functions. primal graph shown Figure2(a). Figure 2(b) displays search tree corresponding lexicographical ordering.2.3.1 AND/OR EARCH PACESsearch trees blind problem decomposition encoded graphical models therefore inefficient. exploit independencies model. AND/OR search spaces894fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS(a) Primalgraph(b) search tree along ordering A, B, C, D, E, FFigure 2: example problem 6 variables {A, B, C, D, E, F } 5 pairwise functions.graphical models introduced better capture problem structure (Dechter & Mateescu, 2007). AND/OR search space defined relative pseudo tree primal graphcaptures problem decomposition. Figure 3(a) shows pseudo tree example problem.EFINITION 2. pseudo tree undirected graph G = (V, E) directed rooted tree =(V, E 0 ), every arc G included E 0 back-arc , namely connects nodeancestor . arcs E 0 may included E.Given graphical model = hX, D, Fi primal graph G pseudo tree G,AND/OR search tree ST contains alternating levels nodes. structure basedunderlying pseudo tree . root node ST node labelled variableroot . children node Xi nodes labelled value assignments hXi , xi(or simply hxi i). children node hXi , xi nodes labelled childrenXi , representing conditionally independent subproblems. AND/OR tree correspondingpseudo tree Figure 3(a) shown Figure 3(b). arcs nodes Xi hXi , xiAND/OR search tree annotated weights derived cost functions F:EFINITION 3 (arc weight). weight w(Xi , xi ) arc (Xi , hXi , xi i) combination (i.e.sum WCSP product MPE) functions, whose scope includes Xi fullyassigned along path root node corresponding hXi , xi i, evaluated valuesalong path.identical subproblems identified context (namely, partial instantiationancestors separates subproblem rest problem graph), merged,yielding AND/OR search graph (Dechter & Mateescu, 2007). Merging context-mergeablenodes yields context-minimal AND/OR search graph, denoted CT . example seenFigure 3(c). size context-minimal AND/OR search graph shown exponentialinduced width G along pseudo tree (Dechter & Mateescu, 2007).solution tree CT subtree that: (1) contains root node CT ; (2)internal node n , children ; (3) internal node n ,exactly one children ; (4) every tip node (i.e., nodes children)terminal node. cost solution tree product, MPE sum WCSP, weightsassociated arcs.node n CT associated value v(n) capturing optimal solution costconditioned subproblem rooted n. Assuming MPE/MAP problem, shown v(n)895fiF LEROVA , ARINESCU , & ECHTER01BB0101CBCEF0CE101FF00 1 0 1 0 1 0 1EC101FF00 1 0 1 0 1 0 1(a) Pseudo tree101FF0E101FF0 1 0 1 0 1 0 10 1 0 1 0 1 0 1(b) AND/OR search tree01BB01CCCE0100CC10101E10E10EE10101FFFF0 10 10 10 10 10 10 10 1(c) Context-minimal AND/OR search graphFigure 3: AND/OR search spaces graphical models.computed recursively based values ns successors: nodes maximization,nodes multiplication. WCSPs, v(n) nodes updated minimizationsummation, respectively (Dechter & Mateescu, 2007).next provide overview depth-first branch bound best-first search algorithms,explore AND/OR search spaces (Marinescu & Dechter, 2009b, 2009a; Otten & Dechter, 2011).schemes use heuristics generated either mini-bucket elimination scheme (2.3.4)soft arc-consistency schemes (Marinescu & Dechter, 2009a, 2009b; Schiex, 2000; Darwiche, Dechter, Choi, Gogate, & Otten, 2008) composite (Ihler, Flerova, Dechter, & Otten,2012). customary heuristic search literature, defining search algorithmsassume without loss generality minimization task (i.e., min-sum optimization problem).896fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSAlgorithm 1: AOBF exploring AND/OR search tree (Marinescu & Dechter, 2009b)1234567891011121314151617181920212223242526272829303132Input: graphical model = hX, D, Fi, pseudo tree rooted X1 , heuristic function h()Output: Optimal solutionCreate root node labelled X1 let G (explored search space) = {s};Initialize v(s) = h(s) best partial solution tree G;SOLVEDSelect non-terminal tip node n . node break;n node labeled Xiforall xi D(Xi )Create child n0 = hXi , xi i;n TERMINALMark n0 SOLVED;succ(n) succ(n) n0 ;else n node labeled hXi , xiforall successor Xj XiCreate child n0 = Xj ;succ(n) succ(n) n0 ;Initialize v(n0 ) = h(n0 ) new nodes;Add new nodes explores search space G G {succ(n)};Let {n};6=Let p node descendants G still S;{p};p nodev(p) = minksucc(p) (w(p, k) + v(k));Mark best successor k ancestors p, k = arg minksucc(p) (w(p, k) + v(k))(maintaining previously marked successor still best);Mark p SOLVED best marked successor solved;else pP nodev(p) = ksucc(p) v(k);Mark arcs successors;Mark p SOLVED children SOLVED;p changes value p marked SOLVEDAdd parents p p one successors marked arc;Recompute following marked arcs root s;return hv(s), i;2.3.2 B EST-F IRST AND/OR EARCHstate-of-the-art version best-first search AND/OR search spaces graphical modelsBest-First AND/OR search algorithm (AOBF) (Marinescu & Dechter, 2009b). AOBFvariant AO* (Nillson, 1980) explores context-minimal AND/OR search graph.AOBF described Algorithm 1. simplicity, present algorithm traversingAND/OR search tree. AOBF maintains explicated part search space G also keepstrack current best partial solution tree . interleaves iteratively top-down node expansionstep (lines 4-16), selects non-terminal tip node generates children G,bottom-up cost revision step (lines 17-30), updates values internal nodes basedchildrens values. newly generated child node terminal marked solved (line 9).897fiF LEROVA , ARINESCU , & ECHTERbottom-up phase, nodes least one solved child nodeschildren solved also marked solved. algorithm also marks arc bestchild node minimum achieved (line 23). Following backwardstep, new best partial solution tree recomputed (line 31). AOBF terminates rootnode marked solved. heuristic used admissible, point terminationoptimal solution cost v(s), root node search space.Extending algorithm explore context-minimal AND/OR search graph straightforward done follows. expanding non-terminal node lines 11-14, AOBFgenerate corresponding children already present explicated searchspace G rather links them. identical nodes G easily recognized basedcontexts (Marinescu & Dechter, 2009b).HEOREM 1 (complexity, Marinescu & Dechter, 2009b). Algorithm AOBF traversing contextminimal AND/OR graph time space complexity O(n k w ), n numbervariable problem, w induced width pseudo tree k bounds domain size.2.3.3 EPTH -F IRST AND/OR B RANCH B OUNDdepth-first AND/OR Branch Bound (AOBB) (Marinescu & Dechter, 2009a) algorithmtraverses AND/OR search space depth-first rather best-first manner, keepingtrack current upper bound minimal solution cost.simplicity, present variant algorithm explores AND/ORsearch tree. AOBB described Algorithm 2 interleaves forward node expansion (lines 4-17)backward cost revision (or propagation) step (lines 19-29) updates node values (capturingcurrent best solution subproblem rooted node), search terminatesoptimal solution found. node n pruned (lines 12-13) current upper boundhigher nodes heuristic lower bound, computed recursively using procedure describedAlgorithm 3.worst case, AOBB explores entire search space, namely O(n k w ) nodes (assumingcontext-minimal AND/OR search graph). practice, however, AOBB likely expandnodes AOBF using heuristic, empirical performance AOBB depends heavilyorder solutions encountered, namely quickly algorithm findsclose optimal solution use upper bound pruning.2.3.4 INI -B UCKET H EURISTICSAND/OR search algorithms presented (AOBF AOBB) often use mini-bucket(also known MBE) heuristic h(n). Mini-Bucket Elimination MBE (Dechter & Rish, 2003)approximate version exact variable elimination algorithm called bucket elimination (BE)(Dechter, 1999). MBE (Algorithm 4) bounds space time complexity full bucket elimination (which exponential induced width w ). Given variable ordering, algorithmassociates variable Xi bucket contains functions defined variable,higher index variables. Large buckets partitioned smaller subsets, called minibuckets, containing distinct variables. parameter called i-bound.algorithm processes buckets last first (lines 2-10 Algorithm 4). mini-bucketsvariable processed separately. Assuming min-sum problem, MBE calculates sum898fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSAlgorithm 2: AOBB exploring AND/OR search tree (Marinescu & Dechter, 2009b)12345678910111213Input: graphical model = hX, D, Fi, pseudo tree rooted X1 , heuristic function h();Output: Optimal solutionCreate root node labelled X1 let stack created expanded nodes OP EN {s};Initialize v(s) best partial solution tree rooted (s) ; U B ;OP EN 6=Select top node n OPEN;n node labeled Xiforeach xi D(Xi )Add child n0 labeled hXi , xi list successors n;Initialize v(n0 ) = 0, best partial solution tree rooted n (n0 ) = ;n node labelled hXi , xiforeach ancestor k nRecursively evaluate cost partial solution tree rooted k, based heuristic functionh(), assign cost f (k); // see Algorithm 3evaluated partial solution better current upper bound k (e.g. f (k) v(k)Prune subtree current tip node n;else14foreach successor Xj XiAdd child n0 labeled Xj list successors n;Initialize v(n0 ) , best partial solution tree rooted n, (n0 ) ;151617181920212223242526272829Add successors n top OPEN;list successors node n emptynode n root nodereturn solution: v(n), (n) ;elsep nodev(p) v(p) + v(n), (p) (p) (n);else p nodenew value better old one, e.g. v(p) > (c(p, n) + v(n)) minimizationv(p) w(p, n) + v(n), (p) (p) hxi , Xi i;Remove n list successors p;Move one level up: n p;functions mini-bucket eliminates variable using min operator (line 9).new function placed appropriate lower bucket (line 10). MBE generates bound (lowerminimization upper maximization) optimal value. Higher values take computational resources, yield accurate bounds. large enough (i.e., w ), MBEcoincides full Bucket Elimination.HEOREM 2 (complexity, Dechter & Rish, 2003). Given graphical model variable orderinginduced width w (o) i-bound parameter i, time mini-bucket algorithmMBE(i) O(nk min(i,w (o))+1 ) space complexity O(nk min(i,w (o)) ), n numberproblem variables k maximum domains size.Mini-bucket elimination viewed message passing leaves root along minibucket tree. mini-bucket tree graphical model mini-buckets nodes. BucketX899fiF LEROVA , ARINESCU , & ECHTERAlgorithm 3: Recursive computation heuristic evaluation function1234function evalPartialSolutionTree(T (n), h(n))Input: Partial solution subtree (n) rooted node n, heuristic function h(n);Output: Heuristic evaluation function f (T (n));succ(n) ==n nodereturn 0;else5return h(n);678910111213elsen nodelet k1 , . . . , kl children n;Preturn li=1 evalPartialSolutionTree(T (ki ), h(ki ));else n nodelet k child n;return w(n, k) + evalPartialSolutionTree(T (k), h(k));Algorithm 4: Mini-Bucket Elimination123456789101112Input: model = hX, D, Fi, ordering o, parameterOutput: Approximate solution M, ordered augmented bucketsInitialize: Partition functions F Bucket1 , . . . , Bucketn , Bucketi contains functionswhose highest variable Xi .// Backward passp n downto 1Let h1 , . . . , hj functions (original intermediate) Bucketp ; let S1 , . . . , Sj scopes;Xp instantiated (Xp = xp )Assign Xp = xp hi put resulting function appropriate bucket;elseGenerate i-partitioning;foreach Qk Q0PGenerate message function hkb : hkb = minxp Xp ji=1 hi ;Add hkb bucket Xb , largest-index variable scope(hkb );// Forward passAssign value variable ordering combination functions bucketminimal;return function computed bucket first variable corresponding assignment;child BucketY function hXY generated BucketX variable X eliminated,placed BucketY . Therefore, every vertex root one parent possiblyseveral child vertices. Note mini-bucket tree corresponds pseudo tree, minibuckets variables combined form call augmented buckets, correspondingvariable nodes (Dechter & Mateescu, 2007).Mini-bucket elimination often used generate heuristics search algorithms graphicalmodels, formulated search spaces Kask Dechter (1999a, 1999b) extendedAND/OR search Marinescu Dechter (2005).900fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSEFINITION 4 (MBE heuristic AND/OR search space, Marinescu & Dechter, 2005). Givenordered set augmented buckets {B(X1 ), . . . , B(Xn )} generated Mini-Bucket algorithmBE(i) along bucket tree , given node n AND/OR search tree, static minibucket heuristic function h(n) computed follows:1. n node, labeled hXp , xp i, then:Xh(n) =hkjhkj {B(Xp )B(Xp1 ...Xpq )}Namely, sum intermediate functions hkj satisfy following two properties:generated buckets B(Xk ), Xk descendant Xp bucket treereside bucket B(Xp ) bucket B(Xp1 . . . Xpq ) = {B(Xp1 ), . . . , B(Xpq )} correspond ancestors {Xp1 ), . . . , Xpq } Xp2. n node, labeled Xp , then:h(n) =minmsucc(p)(w(n, m) + h(m))children n labeled values xp Xp .established necessary background, turn main part paper,presenting contributions, beginning extension best-first search m-best task.customary heuristic search literature without loss generality, assumeremaining paper min-sum optimization problem.3. Best-First Search Finding Best SolutionsExtending best-first search (Section 2.2) particular popular version, A*, mbest task fairly straightforward suggested, example, Charniak Shimony (1994).Instead stopping finding optimal solution, algorithm continues exploring searchspace, reporting next discovered solutions obtained. showsolutions indeed best found decreasing order optimality.particular, second solution reported second best solution and, general, ith solutiondiscovered ith best.3.1 m-A*: Definitionm-best tree-search variant A* denoted m-A* (Algorithm 5, assumes consistent heuristic)solves m-best optimization problem general search graph. show laterextended general admissible heuristics.scheme expands nodes order increasing value f usual A* manner.keeps lists created nodes OPEN expanded nodes CLOSED, usual, maintainingsearch tree, denoted r. Beginning start node s, m-A* picks node smallestevaluation function f (n) OPEN puts CLOSED (line 7). node goal, newsolution reported (lines 8-13). Otherwise, node expanded children created (lines15-23). algorithm may encounter node multiple times maintain901fiF LEROVA , ARINESCU , & ECHTERAlgorithm 5: m-A* exploring graph, assuming consistent heuristic1234567891011Input: implicit directed search graph G = (N, E), start node set goal nodes Goals,consistent heuristic evaluation function h(n), parameterOutput: best solutionsInitialize: OPEN=, CLOSED=, tree r = , = 1 (i counts current solution searched for)OPEN {s}; f (s) = h(s);Make root r;OPEN emptyreturn solutions found far;Remove node, denoted n, OPEN minimum f (break ties arbitrarily, favour goal nodesdeeper nodes) put CLOSED;n goal nodeOutput current solution obtained tracing back pointers n (pointers assigned step22); denote solution Soli ;=return;else12+ 1;131415161718192021222324elseExpand node n, generating children Ch ;foreach n0 Chn0 already appears OPEN CLOSED timesDiscard node n0 ;elseCompute current path cost g(n0 ) = g(n) + c(n, n0 );Compute evaluation function f (n0 ) = g(n0 ) + h(n0 ) ;Attach pointer n0 back n r;Insert n0 right place OPEN based f (n0 );return set best solutions foundcopies OPEN CLOSED lists combined (line 17), separate paths copyexplored search tree (lines 22-23). Nodes encountered beyond times discarded (line 18).denote Ci ith best solution cost, fi (n) cost ith best solution going noden, fi (n) heuristic evaluation function estimating fi (n) gi (n) hi (n) estimatesith best costs n n goal, respectively.heuristic consistent, whenever algorithm reaches node seen (ifsearch space graph tree), exists possibility new path improvingpreviously discovered ones. Therefore, lines 17-18 revised following wayaccount possibility better path n0 discovered:17n0 appears already times union OPEN CLOSED18g(n0 ) strictly smaller gm (n0 ), current m-best path n019Keep n0 pointer n put n back OPEN20Discard earlier subtree rooted nFigure 4 shows example m-A* finding = 3 shortest paths toy problem.left hand side Figure 4 shows problem graph 7 variables 8 edges, together902fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS3h(A) = 52h(B) = 4ordernodes expanded#1h(C) = 2B#5CB f =6h(D) = 3C2C312#8D24#2m=3C1 = 8C f =4= 10= 10#3D1 f = 6f =82h(F ) = 1h(E) = 1EF3#10 f = 9 #9E2F2 f = 83Gh(G) = 0G4f = 12#12G3#7 f = 6 #4F1f = 8 E1#11G2f = 10f = 10#6G1f =8nodes CLOSED(a) Problem graph(b) Trace m-A*Figure 4: Example problem. left: problem graph heuristic values h(n) node.right: trace m-A* finding = 3 best solutions evaluation function f (n)node. White nodes CLOSED, grey one created, discarded.admissible heuristic functions node. Note heuristic consistent. example,h(A) > h(C) + c(A, C). start node, G goal node. right side Figurepresent trace m-A*, evaluation function copy nodes createdtime 3rd solution found. white nodes CLOSED, grey one (node G4 )created, never put OPEN. algorithm expands nodes OPEN increasing orderevaluation functions. assume ties broken favour deeper nodes. First, m-A*discovers solution C F G cost C1 = 8, next solution C E Gcost C1 = 10 found. third solutions B F G cost C1 = 10. Notetwo copies node D, E F four copies G created. goal node G4discarded, bound total number copies particular node = 3.NHEOREM 3. Given graphical model = hX, D, F, n variables whose domain sizebounded k, worst case time space complexity m-A* exploring search treeO(k n ).Proof. worst case m-A* would explore entire search tree, whose size O(k n ) (Section 2.3). Since underlying search space tree, algorithm never encounternodes once, thus nodes duplicated.903fiF LEROVA , ARINESCU , & ECHTER3.2 Properties m-A*section extend desirable properties A*, listed Section 2.2, m-best case.simplicity without loss generality, assume throughout search graph accommodatesleast distinct solutions.HEOREM 4. Given optimization task, implicit directed search graph G integerparameter 1, m-A* guided admissible heuristic following properties:1. Soundness completeness: m-A* terminates best solutions generated ordercosts.2. Optimal efficiency consistent heuristic: node surely expanded2 m-A*must expanded search algorithm traversing G guaranteed findbest solutions heuristic information.3. Optimal efficiency node expansions: m-A* expands node timesheuristic consistent. ith path found node ith best path.4. Dominance: Given two heuristic functions h1 h2 , every n h1 (n) < h2 (n),m-A*1 expand every node surely expanded m-A*2 , m-A*i using heuristic hi .prove properties m-A* Sections 3.2.1-3.2.2.3.2.1 OUNDNESS C OMPLETENESSAlgorithm m-A* maintains copies node discards rest. next showrestriction compromise completeness.P ROPOSITION 1. node discarded m-A* lead m-best solutions.Proof. Consider consistent heuristic first (as described Algorithm 5). momentm-A* discovered node n (m + 1)th time, copies n reside OPEN CLOSEDalgorithm maintains distinct paths each. Let (m + 1)th path.prove Theorem 10, node n discovered (m + 1)th time, cost Cnew newlydiscovered path new (m + 1)th best, namely better costs already discovered:Cnew Cm . Therefore, eliminated (m + 1)th path node n guaranteed worseremaining ones thus part potential m-best optimal solutionsmight passing node n.heuristic consistent, m-A* modified replace worst previouslydiscovered paths newly found new , cost latter better place newcopy OPEN. Thus, again, safe bound number copies m.clear along particular solution path evaluation function nodesbounded paths cost C(), heuristic admissible.P ROPOSITION 2. following true regarding m-A*:1. solution path , forall n , f (n) C().2. precisely defined Section 3.2.3904fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS2. Unless already discovered m-A*, always node n residesOPEN.3. Therefore, long m-A* discover must node OPEN f (n)C().Proof. 1. f (n) = g (n) + h(n) since h(n) c (n, t) due admissibility, c (n, t)actual cost n goal node along , conclude f (n) g (n) + h(n) = C().2. reachable path root always leaf OPEN unless nodes along pathexpanded CLOSED.3. Follows easily 1 2.follows immediately Proposition 2 (stated similarly Nilsson, 1982) that:P ROPOSITION 3. [Necessary condition node expansion] node n expanded m-A*searching ith best solution (1 m) satisfies f (n) Ci .also clearP ROPOSITION 4. [Sufficient condition node expansion] Every node n OPEN,f (n) < Ci , must expanded m-A* ith best solution found.Soundness completeness m-A* follows quite immediately.HEOREM 5 (soundness completeness). Algorithm m-A* generates m-best solutionsorder, namely, ith solution generated ith best solution.Proof. Let us assume contradiction case. Let ith generated solution pathfirst one generated according best-first order. Namely ith solutiongenerated cost C C > Ci . However, algorithm selected goal ti along, evaluation function f (ti ) = gi (ti ) = C, while, based Proposition 2,node n0 OPEN whose evaluation function Ci . Thus n0 selectedexpansion instead ti . contradiction therefore result follows.3.2.2 MPACT H EURISTIC TRENGTHLike A*, performance m-A* improves accurate heuristic.P ROPOSITION 5. Consider two heuristic functions h1 h2 . Let us denote m-A*1 algorithm uses heuristic h1 m-A*2 one using heuristic h2 . heuristic h1informed h2 , namely every node n, h2 (n) < h1 (n), algorithm m-A*2 expandevery node expanded algorithm m-A*1 finding j th solutionj {1, 2, . . . , m}, assuming tie-breaking rule.Proof. Since h1 informed h2 , h1 (n) > h2 (n) every non-goal node n. Let usassume m-A*1 expands non-terminal node n finding j th best solutioncost Cj . node n expanded, means (a) point OPEN (b) evaluationfunction satisfies f1 (n) = g(n) + h1 (n) Cj (Proposition 3). Consider current pathstart node n. node n0 path selected point expansion thus905fiF LEROVA , ARINESCU , & ECHTERm-A search spacesearch spaceC1 CmalgorithmCi|CmC1 |search spaceexplored m-AcomparedFigure 5: schematic representation search spaces explored m-A* algorithm, depending cost Cmevaluation functions nodes also bounded cost j th best solution:f1 (n0 ) Cj . Since h1 (n0 ) > h2 (n0 ) every node n0 along path , evaluation functionsaccording heuristic h2 (n) obey:f2 (n0 ) = g(n0 ) + h2 (n0 ) < g(n0 ) + h1 (n0 ) < Cj(1)thus node n0 must also expanded m-A*2 .Consider case exact heuristic. easy show that:HEOREM 6. h = h exact heuristic, m-A* generates solutions j-optimalpaths 1 j m.Proof. Since h exact, f values OPEN expanded sequence values C1 C2. generated nodes evaluation function f = C definition. . . Ci . . . Cm1optimal paths (since h = h ), f = C2 must paths secondbest on. Notice solutions costs.h = h , m-A*s complexity clearly linear number nodes evaluation. However, cost function small range values, mayfunction f Cm. avoid exponential frontierexponential number solution paths cost Cmchose tie-breaking rule expanding deeper nodes first, yielding number node expansionsbounded n, n bounds solution length. Clearly:HEOREM 7. m-A* accessfavour deeperP h = h , then, using tie-breaking rulethnodes, expands #N = #Ni nodes, #Ni length optimal solutionpath. Clearly, #N n.906fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSFigure 6: graph G0 represents new problem instance constructed appending branch leading new goal node node n.3.2.3 -A* C ONSISTENT H EURISTICm-A* uses consistent heuristic, several useful properties.Optimal efficiency consistent heuristic. Algorithm A* known optimally efficientconsistent heuristic (Dechter & Pearl, 1985). Namely, algorithm extends searchpaths root uses heuristic information A* expand every nodesurely expanded A*, i.e., expand every n, f (n) < C . extend notionnodes surely expanded A* m-best case:-boundedP ROPOSITION 6. Algorithm m-A* expand node n reachable strictly Cmpath root, regardless tie-breaking rule. set nodes referred surelyexpanded m-A*.-bounded path = {s, n , n , . . . n}. start nodeProof. Let us consider strictly Cm1 2clearly expanded beginning search children, including node n1 , placed, node n must expanded m-A* finding mth bestOPEN. Since f (n1 ) < Cm1solution (Proposition 4), children, including n2 , turn placed OPEN. truenodes , including n.HEOREM 8 (m-optimal efficiency). search algorithm, guaranteed find m-bestsolutions explores search space m-A* consistent heuristic,expand node surely expanded m-A*. Namely expand every node, i.e. f (n0 ) < C , n0 .lies path dominated Cmproof idea similar work Dechter Pearl (1985). Namely showalgorithm expand node n, surely expanded m-A*, miss one m-bestsolutions, applied slightly modified problem:Proof. Let us consider problem search graph G consistent heuristic h. Assumenode n surely expanded m-A* finding j th best solution. Let B algorithmuses heuristic h guaranteed find best solutions. Let also assumenode n expanded B. consistency heuristic also allows us better characterizenodes expanded m-A*.create new problem graph G0 (see Figure 6) adding new goal nodeh(t) = 0, connecting n edge cost c = h(n) + , = 0.5(Cj D)907fiF LEROVA , ARINESCU , & ECHTER= maxn0 Sj f (n0 ), Sj set nodes surely expanded m-A* findingj th solution. possible show heuristic h admissible graph G0 (Dechter &Pearl, 1985). Since = 0.5(Cj D), C = 2. construction, evaluation functionnew goal node is:f (t) = g(t) + h(t) = g(n) + c = g(n) + h(n) + = f (n) + + = Cj < Cj(2)means reachable path whose cost strictly bounded Cj .guarantees m-A* expand (Proposition 6), discovering solution cost Cj .hand, algorithm B, expand node n original problem, still expandthus reach node discover solution cost Cj , returning trueset best solutions modified problem. contradiction theorem follows.P ROPOSITION 7. heuristic function employed m-A* consistent, values evaluation function f sequence expanded nodes non-decreasing.proof straightforward extension result Nilsson (1980).Proof. Let node n2 expanded immediately n1 . n2 already OPEN timen1 expanded, node selection rule follows f (n1 ) f (n2 ). n2OPEN, must added result expansion n1 , i.e., child n1 .case cost getting n2 start node g(n2 ) = g(n1 ) + c(n1 , n2 )evaluation function node n2 f (n2 ) = g(n2 ) + h(n2 ) = g(n1 ) + c(n1 , n2 ) + h(n2 ). Since h(n)consistent, h(n1 ) c(n1 , n2 )+h(n2 ) f (n2 ) g(n1 )+h(n1 ). Namely, f (n2 ) f (n1 ).heuristic function consistent, stronger condition Proposition 4:HEOREM 9. Algorithm m-A* using consistent heuristic function:;1. expands nodes n f (n) < Cm;2. never expands nodes evaluation function f (n) > Cm3. expands nodes f (n) = Cm , subject tie-breaking rule.node n never expandedProof. 1. Assume exists node n f (n) < Cmm-A*. situation arise node n never OPEN list, otherwise wouldexpanded, according Proposition 4. implies parent node n searchspace (let us denote node p) never expanded. However, similarly done. Thusproof Proposition 7, easy show f (p) f (n) and, consequently f (p) < Cmnode p must also never OPEN, otherwise would expanded. Clearly, trueancestors n, start node s. Since node clearly OPEN beginningsearch, initial assumption incorrect property follows.2. 3. Follow directly Proposition 3.Figure 7 provides schematic summary search space explored m-A* consistent heuristic.908fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS{n|f(n) < Cm}search spaceexplored m-A**{n|f (n) > Cm}{n|f (n) = Cm}Figure 7: nodes explored m-A* algorithm consistent heuristic.Optimal efficiency node expansions. Whenever node n selected expansionfirst time m-A*, algorithm already found shortest path node. extendproperty follows:HEOREM 10. Given consistent heuristic h, m-A* selects node n expansionith time, g(n) = gi (n), namely found ith best path start node n.Proof. induction. = 1 (basic step) theorem holds (Nillson, 1980). Assume alsoholds (i 1)th expansion node n. Let us consider ith case, > 1 (inductive step).already expanded node n (i 1) times due inductive hypothesis alreadyfound (i 1) distinct best paths node n. Let us assume cost newly foundsolution path greater ith optimal one, i.e. gi (n) > gi (n). Then, exists different,undiscovered path n cost g (n) = gi (n) < gi (n). Proposition 2 existsOPEN node n0 . Obviously, node n0 must located start node noden. Denoting C (n0 , n) = c(n0 , n1 ) + + c(nk , n), heuristic consistency easilyfollows h(n0 ) < C (n0 , n) + h(n) evaluation function node n0 along pathf (n0 ) = g (n0 ) + h(n0 ) < g (n0 ) + C (n0 , n) + h(n). Seeing cost pathn g (n) = g (n0 ) + C , conclude f (n0 ) < f (n). However, contradictsassumption node n expanded ith time node n0 . theorem follows.3.2.4 MPACT R EQUIRED B EST OLUTIONSsequence sizes search spaces explored m-A* function obviously monotonically increasing m. Denoting j-A* i-A* versions m-A* algorithmsearch respectively j best solutions, make following straightforward characterization:P ROPOSITION 8. Given search graph consistent heuristic,1. node expanded i-A* expanded j-A* < j use tie-breakingrule.909fiF LEROVA , ARINESCU , & ECHTER2. set S(i, j) nodes defined S(i, j) = {n|Ci < f (n) < Cj } surely expandedj-A* surely expanded i-A*.3. Cj = Ci , difference number nodes expanded i-A* j-A* determinedtie-breaking rule.proof follows trivially Theorem 9. result, larger discrepancy respective costs Cj Ci yields larger difference search spaces explored j-A* i-A*.difference, however, also depends granularity values sequenceobserved evaluation functions increase, related arc costs (or weights) searchgraph. Ci = Cj = C, search space explored i-A* j-A* differfrontier nodes satisfying f (n) = C. Figure 5 represents schematically search spaces exploredi-A* algorithm.4. Depth-First Branch Bound Finding Best SolutionsAlong valuable properties, m-A* inherits also disadvantages A*: exponential spacecomplexity, makes algorithm infeasible many applications. alternative approachsearching using depth-first branch bound (DFBnB), implemented linearspace necessary therefore often practical. DFBnB finds optimal solutionexploring search space depth-first manner. algorithm maintains cost U bestsolution encountered far prunes search nodes whose lower-bounding evaluation functionf (n) = g(n) + h(n) larger U . Extending DFBnB m-best task straightforward,describe next.4.1 m-BB AlgorithmAlgorithm m-BB, depth-first branch bound extension m-best task, exploressearch tree presented Algorithm 6. usual, algorithm maintains lists OPENCLOSED nodes. also maintains sorted list CANDIDATE nodes contains bestsolutions found far. Nodes OPEN organized last - first manner orderfacilitate depth-first exploration search space (i.e., OPEN stack). step, m-BBexpands next node n OPEN (line 5). goal node, new complete solution found(line 6) stored CANDIDATE list (line 7-9), re-sorted (line 10).best solutions maintained (lines 11-13).main modification depth-first branch bound, extended m-best task,pruning condition. Let U1 U2 . . . Um denote costs best solutionsencountered thus far. Um upper bound used pruning. solutions discovered, pruning takes place. Algorithm m-BB expands current node n, generates children(lines 15-17) computes evaluation function (line 18-19). prunes subproblem niff f (n) Um (lines 20-23). easy see algorithm terminates, outputsm-best solutions problem.HEOREM 11. Algorithm m-BB sound complete m-best solutions task.Proof. Algorithm m-BB explores search space systematically. solutions, Cskipped ones satisfying f (n) Um (see step 22). Since Um Cm910fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSAlgorithm 6: m-BB exploring graph, assuming consistent heuristic123456789101112131415161718192021222324Input: implicit directed search graph G = (N, E) start node n0 set goal nodes Goals.heuristic function h(n). Parameter (the number desired solutions).Output: best solutionsInitialize: OPEN=, CLOSED=, tree r = , sorted list CANDIDATE = , UpperBound = , = 1 (icounts current solution searched for);Put start node n0 OPEN, g(n0 ) = 0, f (n0 ) = h(n0 );Assign n0 root r;OPEN emptyRemove top node OPEN, denoted n, put CLOSED;n goal nodesoli solution obtained tracing back pointers n n0 (pointers assigned step 17);Ci cost soli ;Place solution soli CANDIDATE;Sort CANDIDATE increasing order solution costs;size CANDIDATE listUm cost mth element CANDIDATE;Keep first elements CANDIDATE, discard rest;elseExpand node n, generating children succ(n);forall n0 succ(n)Attach pointer n0 back n r;g(n0 ) = g(n) + c(n, n0 );f (n0 ) = g(n0 ) + h(n0 );f (n0 ) < UmPlace n0 OPEN;elseDiscard n0 ;return solutions CANDIDATE listtherefore path cannot lead newly discoveredbest solution cost, implies f (n) Cmm-best cost.NHEOREM 12. Given graphical model = hX, D, F, i, worst case time complexitym-BB explores search tree O(k n + log m), n number variables,k domain size number required solutions. Space complexity O(n).Proof. worst case m-BB would explore entire search tree size O(k n ). maintainingCANDIDATE list introduces additional time overhead O(log m). Since search treeyields caching, m-BB uses space linear number variables.4.2 Characterization Search Space Explored m-BBalready shown m-A* superior exact search algorithm finding mbest solutions heuristic consistent (Theorem 8). particular, m-BB must expand}.nodes surely expanded m-A*, namely set nodes {n|f (n) < CmTheorem 8 pruning condition clear that:911fiF LEROVA , ARINESCU , & ECHTERP ROPOSITION 9. Given consistent heuristic m-BB must expand node set {n|f (n) <}. Also, instances m-BB expands nodes satisfying f (n) > C .CmSeveral sources overhead m-BB discussed next.4.2.1 -BB VS . BBPruning m-BB occur upper bound current mth best solution assignedvalid value, i.e., solutions found. absence determinism, solutionsconsistent, time takes find arbitrary solutions depth-first manner O(m n),n length solution (for graphical models n coincides number variables).problem contains determinism may difficult find even single solution. meansm-BB search may exhaustive quite time.4.2.2 MPACT OLUTION RDERdifference number nodes expanded BB m-BB depends greatly variancesolution costs. solutions cost, U1 = Um . However,situation unlikely therefore conditions m-BBs node expansions impacted1 , . . . , U j } non-increasing sequenceorder solutions discovered. Let {Umthupper bounds best solution, point m-BB uncovered j th solution.jInitially Um= , j {1, . . . , 1}.P ROPOSITION 10. discovery (j 1)th j th solutions set nodesj1U j U j1 .expanded m-BB included Sj = {n | f (n) Um}, CmProof. discovering (j 1)th j th solutions m-BB expands nodes satisfyingj1j1{n | f (n) Um}, hence j : Cj Um. j th solution found, either replacesjthprevious bound solution Um = Cj k th upper bound, k {1, . . . , 1}, yieldingj1jU j U j1 .. Either way, Cm= Um1Um4.2.3 RDERING OVERHEADneed keep list sorted solutions (the CANDIDATE list) implies O(log m) overheadnew solution discovered. total number solutions encountered terminationhard characterize.4.2.4 C ACHING OVERHEADoverhead related caching arises m-BB explores search graph uses caching.version algorithm (not explicitly presented) stores best partial solutionsfully explored subproblems (and subset partial set discovered) re-usesresults whenever subproblem encountered again. order implement caching, mBB requires store list length node cached. Moreover, cached partialsolutions need sorted, yields O(m log m) time overhead per cached node.912fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS5. Adapting m-A* m-BB Graphical Modelsmain task find best solutions optimization tasks graphical models. Therefore,adapt m-best search algorithms m-A* m-BB explore AND/OR search spacegraphical models, yielding algorithms m-AOBF m-AOBB, respectively. also describehybrid algorithm BE+m-BF, combining Bucket Elimination m-A*.5.1 m-AOBF: Best-First AND/OR Search Best Solutions Graphical Modelsextension algorithm AOBF (Section 2.3.2) m-best task seems fairly straightforward,principle. m-AOBF AOBF continues searching discovering first solution,required number best solutions obtained. actual implementation requires severalmodifications discuss next.easy extend AOBFs bottom-up node values updates corresponding arc markingmechanism m-best task. Therefore, order keep track current best partial solutiontree searching ith best solution adopt naive approach maintains explicitlylist OPEN containing entire partial solution trees (not nodes), sorted ascending orderheuristic evaluation costs. Algorithm 7 presents pseudo-code simple scheme exploresAND/OR search tree generates solutions one one order costs. step,algorithm removes next partial solution tree 0 OPEN (line 4). 0 completesolution, added list solutions along cost (lines 5-8), otherwise algorithmexpands tip node n 0 , generating successors (line 10-17). newly generated noden0 added 0 separately, yielding new partial solution tree 00 (lines 19-23), whose costrecursively evaluated using Algorithm 3, AOBB (line 28). new partial treesplaced OPEN (line 29). Search stops solutions found.note maintenance OPEN list containing explicit partial solution subtreessource significant additional overhead become apparent empirical evaluationSection 7. Thus, question whether performance m-AOBF improved opentherefore rich topic future work.m-A* properties (Section 3.2) extended m-AOBF. particular, algorithm mAOBF admissible heuristic sound complete, terminating best solutionsgenerated order costs. m-AOBF also optimal terms number nodes expandedcompared algorithm explores AND/OR search spaceconsistent heuristic function.HEOREM 13 (m-AOBF complexity). complexity algorithm m-AOBF traversing eitherh1AND/OR search tree context minimal AND/OR search graph time space O(k deg ),h depth underlying pseudo tree, k maximum domain size, deg boundsdegree nodes pseudo tree. pseudo tree balanced (each internal nodeexactly deg child nodes), time space complexity O(k n ), n numbervariables.real complexity bound m-AOBF comes cost function. appears howevermaintaining OPEN list brute force manner lend easily effective wayenumerating partial solution subtrees therefore search space partial solutionsubtrees actually exponential n. detailed proof Theorem 13 given Appendix.913fiF LEROVA , ARINESCU , & ECHTERAlgorithm 7: m-AOBF exploring AND/OR search tree123456789101112131415161718192021222324252627282930Input: graphical model = hX, D, Fi, pseudo tree rooted X1 , heuristic function h(), parameter m;Output: best solutionsCreate root node labelled X1 , let G = {s} (explored search space) = {s} (partial solution tree);Initialize ; OP EN {T }; = 1; (i counts current solution searched for);OP EN 6=Select top partial solution tree 0 remove OPEN;0 complete solution{hf (T 0 ), 0 i};+ 1;continue;Select non-terminal tip node n 0 ;// Expand node nn node labeled Xiforall xi D(Xi )Create child n0 labeled hXi , xi i;succ(n) succ(n) {n0 };else n node labeled hXi , xiforall successor Xj XiCreate child n0 labeled Xj ;succ(n) succ(n) {n0 };G G {succ(n)};// Generate new partial solution treesL ;forall n0 succ(n) Initialize v(n0 ) = h(n0 );n nodeforall n0 succ(n)Create new partial solution tree 00 0 {n0 };L L {T 00 };else n nodeCreate new partial solution tree 00 0 {succ(n)};forall 00 LRecursively evaluate assign f (T 00 ) cost partial solution tree 00 , based heuristicfunction h(); // see Algorithm 3Place 00 OPEN, keeping sorted ascending order costs f (T 00 );return best solutions found S;5.2 m-AOBB: AND/OR Branch Bound Best Solutions Graphical ModelsAlgorithm m-AOBB extends AND/OR Branch Bound search (AOBB, Section 2.3.3)m-best task. main difference AOBB m-AOBB value function computednode. m-AOBB tracks costs best partial solutions solved subproblem. Thus extends node value v(n) solution tree (n) rooted n AOBB orderedsets length m, denoted v(n) (n), respectively, v(n) = {v1 (n), . . . , vm (n)}ordered set costs best solutions subproblem rooted n, (n) =(n)} set corresponding solution trees. extension arises due{T1 (n), . . . , Tmdepth-first manner search space exploration m-AOBB conjunction AND/OR decomposition. Therefore, due AND/OR decomposition m-AOBB needs completely solve914fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSAlgorithm 8: m-AOBB exploring AND/OR search tree1234567891011121314Input: graphical model = hX, D, Fi, pseudo tree rooted X1 , heuristic function h(), parameter m;Output: best solutions// INITIALIZECreate root node labeled X1 let stack created expanded nodes OP EN = {s};Initialize v(s) = (a set bounds best solutions s) set best partial solution trees rooted(s) = ; U B = , sorted list DIDAT E = ;OP EN 6=Select top node n OPEN;// EXPANDn node labeled Xiforeach xi D(Xi )Add child n0 labeled hXi , xi list succ(n) containing successors n;Initialize v(n0 ) = 0, set best partial solution trees rooted n (n0 ) = ;n node labeled hXi , xiLet p ancestor n;Recursively evaluate assign f (p) cost partial solution tree rooted p, based heuristic h(); //see Algorithm 3vm (p) < f (p) vm (p)Prune subtree current tip node n;elseforeach successor Xj XiAdd child n0 labeled Xj list succ(n) containing successors n;Initialize v(n0 ) = , set best partial solution trees rooted n (n0 ) = ;1516171819202122232425262728293031323334Remove n OPEN add succ(n) top OPEN;// PROPAGATElist successors node n emptyn root nodereturn set solutions rooted n costs: (n), v(n) ;elseUpdate ancestors n, nodes p, bottom up:p nodeCombine set partial solution trees subproblem rooted p (p) set partialsolution trees rooted n (n) costs v(p) v(n); // see Algorithm 9Assign resulting set costs set best partial solution trees respectively v(p)(p);else p nodeforeach solution cost vi (n) set v(n)Update cost weight arc, creating new set costs v 0 (n):vi0 (n) = c(p, n) + vi (n);Merge sets partial solutions v(n) v(p) sets partial solution trees rooted p n:(p) (n), keeping best elements; //Algorithm 10Assign results merging respectively v(p) (p);Remove n list successors p;Move one level up: n p;return v(s) (s)subproblems rooted children n0 node n, even single solutionsubproblem n acquired (unlike m-BB case). Consequently, bottom-up phasesets costs propagated updated. m-AOBF hand, maintainsset partial solution trees.915fiF LEROVA , ARINESCU , & ECHTERAlgorithm 9: Combining sets costs partial solution trees1234567891011function Combine(v(n), v(p), (n),T (n))Input: Input sorted sets costs v(n), v(p), corresponding partial solution trees (n), (p), numberrequired solutionsOutput: set costs best combined solutions v 0 (p), corresponding partial solution trees 0 (p)// INITIALIZESorted list OPEN, initially empty; //contains potential cost combinationsv 0 (p) ; 0 (p) ;k = 1; //number partial solutions already assembled, total// Search possible combinationsOPEN v1 (n) + v1 (p);k < OP EN emptyRemove top node V OPEN, V = Svi (n) + vj (p);vk0 (p) V ;0(p) Ti (n) Tj (p);vi+1 (n) + vj (p) OP ENPut vi+1 (n) + vj (p) OP EN ;13vi (n) + vj+1 (p) OP ENPut vi (n) + vj+1 (p) OP EN ;14k k + 1;15return v 0 (p), (p);12Unlike m-AOBF discovers solutions one one order costs, m-AOBB (pseudocode Algorithm 8) reports entire set solutions once, termination. m-AOBB interleaves forward node expansion (lines 5-18) backward propagation (or cost revision) step(lines 19-33) updates node values search terminates. node n pruned (lines 12-13)current upper bound mth solution n, vm (n), lower nodes evaluationfunctions f (n), computed recursively AOBB (Algorithm 3). bottom-uppropagation phase node partial solutions subproblems rooted nodeschildren combined (line 24-26, Algorithm 9). parent node p v(p) (p)updated incorporate new possibly better partial solutions rooted child node n (lines27-31, Algorithm 10).5.2.1 C HARACTERIZING N ODE P ROCESSING OVERHEADaddition increase explored search space m-BB experiences compared BBdue reduced pruning (Section 4.2), AND/OR search introduces additional overhead mAOBB. propagation set costs partial solution trees leads increasememory factor per node. Processing partial solutions nodesintroduces additional overhead.HEOREM 14. Algorithm m-AOBB exploring AND/OR search tree time overheadO(m deg log m) per node O(m k) per node, deg bounds degreepseudo tree k largest domain size. Assuming k < deg log(m), total worst case timecomplexity O(n k h deg log(m)) space complexity O(mn). time complexitym-AOBB exploring AND/OR search graph O(n k w deg log(m)), space complexityO(mn k w ).916fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSAlgorithm 10: Merging sets costs partial solution trees1234567891011121314151617function Merge(v(n),v(p),T (n),T (p))Input: Input sorted cost sets v(n) v(p), sets corresponding partial solution trees (n) (p),number required solutionsOutput: v 0 (p), merged set best solution costs, 0 (p) set corresponding partial solution trees// INITIALIZEv 0 (p) ;0 (p) ;i, j 1; //indices cost setsk 1; //index resulting array// Merge two sorted setskvi (p) vj (n)vk0 (p) vi (p);0Tk (p) Ti (p);+ 1;k k + 1;elsevk0 (p) vj (n);0Tk (p) Tj (n);j j + 1;k k + 1;return v 0 (p) 0 (p);Proof. Combining sets current m-best partial solutions (Algorithm 9) introduces overheardO(m log(m)). resulting time overhead per node O(deg log(m)). Merging twosorted sets costs (Algorithm 10) done O(m) steps. node O(k) children,resulting overhead O(m k). Assuming k < deg log(m), complexity dominatedprocessing nodes. worst case, tree version m-AOBB, called m-AOBB-tree,would explore complete search space size O(n k h ), h bounds depth pseudotree, graph version, called m-AOBB-graph, would visit space size O(n k w ),w induced width pseudo tree. space complexity m-AOBB-tree followsneed propagate sets O(m) partial solutions length O(n). time overheadm-AOBB AND/OR trees AND/OR graphs. space complexity m-AOBBgraph explained need store partial solutions cached node.5.3 Algorithm BE+m-BFknown exact heuristics graphical models generated Bucket Elimination(BE) algorithm described Section 2.3.4. therefore first compile exact heuristics alongordering using apply m-A* (or m-AOBF, work point),using exact heuristics. resulting algorithm called BE+m-BF. Worst-case analysisalgorithm show yields best worst-case complexity compared knownm-best algorithm graphical models.917fiF LEROVA , ARINESCU , & ECHTERHEOREM 15. time complexity BE+m-BF O(nk w+1 + nm) n numbervariables, k largest domain size, w induced width problem desirednumber solutions. space complexity O(nk w + nm).Proof. BEs time complexity O(nk w+1 ) space complexity O(nk w ) (Dechter, 1999).Since compiles exact heuristic function, m-A* exact heuristic expands nodesf (n) = Cj searching ith solution. algorithm breaks ties favourdeeper nodes, expand nodes solution paths. path length n, yielding totaltime space complexity step algorithm equal O(n m).6. Related Workdistinguish several primary approaches employed earlier m-best exact algorithms,mentioned already Introduction. Note original works include spacecomplexity analysis bounds provided often own.first influential approach introduced Lawler (1972). aimed use of-theshelf optimization schemes best solutions. Lawler showed extend given optimizationalgorithm m-best task. step, algorithm seeks best solution re-formulationoriginal problem excludes solutions already discovered. scheme improved years still one primary strategies finding m-best solutions.time space complexity Lawlers scheme O(nmT (n)) O(S(n)) respectively,(n) S(n) time space complexity finding single best solution. example,use AOBF underlying optimization algorithm, use Lawlers method yields timecomplexity O(n2 mk w log n ) space complexity O(nk w log n ).Hamacher Queyranne (1985) built upon Lawlers work used building blocks algorithms find first second best solutions. two best solutions generated,new problem formulated second best solution best solution new problem.Then, second best solution new problem becomes overall third best solutionprocedure repeated. algorithm time complexity O(m T2 (n)) space complexityO(S2 (n)), T2 (n) S2 (n) respectively time space finding second bestsolution. complexity method always bounded Lawler, seeingLawlers scheme used algorithm finding second best task. Using m-AOBFfind two best solutions, obtain time complexity O(2mnk w log n ) space complexityO(2nk w log n ).Nilsson (1998) applied Lawlers method using join-tree algorithm. top algorithm reuses computations previous iterations. scheme, called max-flow algorithm, usesmessage-passing junction tree calculate initial max-marginal functions cluster(e.g. probability values probable assignments task) yielding best solution. Notestep equivalent running bucket-elimination algorithm. Subsequent solutions recovered conditioning search consult generated function. time complexity analysisNilsson (1998) O(2p|C| + 2mp|R| + pm log (pm)), p number cliques jointtree, |C| size largest clique |R| size largest residual (i.e. numbervariables cluster neighbouring clusters). space complexity boundedO(p|C| + p(|S|)), |S| size separator clusters. applied buckettree, Nilssons scheme time space complexity O(2nk w+1 + mn(2k + log(mn))O(nk w +1 + nk w ) respectively, since bucket tree p = n cliques, whose size bounded918fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS|C| = k w +1 residual cluster |R| = k (the domain single variable).Thus algorithm better time complexity schemes mentioned far, exceptBE+m-BF.Two works de Campos et al. build upon Nilssons approach, extending solving mbest MAP task using genetic algorithms (de Campos, Gamez, & Moral, 1999) probability trees(de Campos, Gamez, & Moral, 2004), respectively. schemes approximate authorsprovide theoretical analysis complexity.Fairly recently, Yanover Weiss (2004) developed iterative scheme based belief propagation, called BMMF. iteration BMMF uses Loopy Belief Propagation solve two newproblems obtained restricting values certain variables. applied junction treeinduced width w (whose largest cluster size bounded k w +1 ) exact algorithmtime complexity O(2mnk w+1 ) space complexity O(nk w + mnk). appliedloopy graph, BMMF guaranteed find exact solutions.Another approach based Lawlers idea uses optimization via LP-relaxation (Wainwright& Jordan, 2003), formulated Fromer Globerson (2009). method, called Spanning TReeInequalities Partitioning Enumerating Solutions, STRIPES, also partitions searchspace, systematically excluding previously determined assignments. step newconstraints added LP optimization problem, solved via off-the-shelf LP-solver.general, algorithm approximate. However, trees junction-trees exactunderlying LP solver reports solutions within time limit. PESTEELARS extensionscheme Batra (2012) solves LP relaxation using message-passing approach that,unlike conventional LP solvers, exploits structure problems graph. complexityLP-based algorithm hard characterize using usual graph parameters.Another approach extends variable elimination (or dynamic programming) schemes directlyobtain best solutions. recent paper (Flerova et al., 2011) extended bucket elimination mini-bucket elimination m-best solutions task, yielding exact scheme calledelim-m-opt approximate version called mbe-m-opt, respectively. work also embedsm-best optimization task within semi-ring framework. time space complexitiesalgorithm elim-m-opt bounded O(m log mnk w +1 ) O(mnk w ), respectively.Two related dynamic programming based ideas Seroussi Golmard (1994) Elliot(2007). Seroussi Golmard extract solutions directly, propagating best partialsolutions along junction tree. Given junction tree p cliques, largest cluster size |C|, separatorsize bounded |S| branching degree deg, time complexity algorithm O(m2 p|C| deg) space complexity O(m p |S|). Adapted bucket tree, algorithmtime complexity O(m2 nk w +1 deg) space complexity O(mnk w ). Elliot propagatesbest partial solutions along representation called Valued And-Or Acyclic Graph, also knownsmooth deterministic decomposable negation normal form (sd-DNNF) (Darwiche, 2001).time complexity Elliots algorithm O(nk w +1 log (m deg)) space complexityO(mnk w +1 ).Several methods focus search schemes obtaining multiple optimal solution k shortestpaths task (KSP). survey see paper Eppstein (1994). majority algorithmsassume entire search graph available memory thus directly applicable.recent exception Aljazzar Leue (2011), whose K algorithm finds k shortest pathssearch on-the-fly thus potentially useful graphical models. algorithminterleaves A* search problems implicit graph G Dijkstras algorithm (1959) spe919fiF LEROVA , ARINESCU , & ECHTERBE+m-BFO(nk wm-AOBFO(nk wlog n+1+ mn)O(2nk w)Nilsson 1998+1+ mn(2 log(mn) + 2k))elim-m-optGosh et al. 2012Elliot2007Yanover Weiss 2004m-AOBBHamacher QueyranneO(mnk wO(mnk w+1+1O(mnk w )log m)log (m deg))O(mnk wO(mnk w deg log m)O(2mnk w+1log nAljazzar Leue 2011O(nk w w log (nk) + m)))Seroussi Golmard 1994O(m2 nk w+1 deg)Lawler 1972O(n2 mk w )m-A*-treeO(k n )m-BB-treeO(k n + log m)Figure 8: Time complexity comparison exact m-best algorithms specified bucket tree.parent node graph better complexity children. Problem parameters:n - number variables, k - largest domain size, w - induced width, deg - degreejoin (bucket) tree. algorithms highlighted.cific path graph structure denoted P (G). P (G) directed graph, vertices correspondedges problem graph G. Given consistent heuristic, K , applied AND/ORsearch graph time space O(nk w w log(n k) + m).recently, Gosh, et al., (2012) introduced best-first search algorithm generating ordered solutions explicit AND/OR trees graphs. time complexity algorithmbounded O(mnk w ), applied context-minimal AND/OR search graph. spacecomplexity bounded O(s nk w+1 ), number candidate solutions generatedstored algorithm, hard quantify using usual graph parameters. However, approach,explores space complete solutions, seem practical graphical modelsrequires entire AND/OR search space fully explicated memory attempting generate even second best solution. contrast, algorithms generate bestsolutions traversing space partial solutions.920fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSFigure 8 provides visual comparison worst-case time complexity boundsdiscussed schemes form directed graph, node corresponds algorithmparent graph better complexity child. assume analysisn >> > k > 2.see best emerging scheme, far worst-case performance goes BE+m-BF.However, since requires compiling exact heuristics, often infeasible. see alsoalgorithm elim-m-opt appears relatively good time complexity superior, example, mAOBB search. However, showed previous work (Flerova et al., 2011), quite limitedempirically. Note worst-case analysis often fails capture practical performance, either algorithms good worst-case performance require much memory,ignores power cost function bounding performance.7. Experimental Evaluationexperiments consist two parts: evaluation m-best search algorithms benchmarksrecent UAI Pascal2 competitions comparison schemes previously developed algorithms randomly generated networks, whose parameters structurerestricted due limitations available implementations competing schemes.defer discussion second part experiments Section 7.5, concentratingevaluation m-best search schemes only.7.1 Overview Methodologyused 6 benchmarks, all, except binary grids, came real-world domains:1. Pedigrees2. Binary grids3. WCSP4. Promedas5. Proteins6. Segmentationpedigrees benchmark (pedigree*) used UAI 2008 competition.3 arisedomain genetic linkage analysis associated task haplotyping.haplotype sequence alleles different loci inherited individual one parent,two haplotypes (maternal paternal) individual constitute individuals genotype.genotypes measured standard procedures, result list unordered pairsalleles, one pair locus. maximum likelihood haplotype problem consists findingjoint haplotype configuration members pedigree maximizes probabilitydata. shown that, given pedigree data, haplotyping problem equivalentcomputing probable explanation Bayesian network represents pedigree (seepaper Fishelson Geiger (2002) details).3. http://graphmod.ics.uci.edu/group/Repository921fiF LEROVA , ARINESCU , & ECHTERBenchmarkPedigreesGridsWCSPPromedasProteinsSegmentation# inst133261867247n581-1006144-250025-1057197-211315-242222-234k3-722-100218-812-21w16-3915-905-2875-1205-1615-18hT52-10448-28311-33734-1877-4447-67Table 1: Benchmark parameters: # inst - number instances, n - number variables, k - domainsize, w - induced width, hT - pseudo tree height.binary grid networks (50-*, 75-* 90-*)4 nodes correspondingbinary variables arranged N N square functions defined pairsvariables generated uniformly randomly.WCSP (*.wcsp) benchmark includes random binary WCSPs, scheduling problemsSPOT5 benchmark, radio link frequency assignment problems, providing large varietyproblem parameters.Protein side-chain prediction (pdb*) networks correspond side-chain conformation prediction tasks protein folding problem (Yanover, Schueler-Furman, & Weiss, 2008).resulting instances relatively nodes, large variable domains, generally renderinginstances complex.Promedas (or chain *) segmentation (* s.binary) probabilistic networkscome set problems used 2011 Probabilistic Inference Challenge.5 Promedas instances based Bayesian network model developed expert systems medical diagnosis(Wemmenhove, Mooij, Wiegerinck, Leisink, Kappen, & Neijt, 2007). Segmentation commonbenchmark used computer vision, modeling task image segmentation MPE problem,namely assigning label every pixel image, pixels label sharecertain characteristics.Table 1 describes benchmark parameters: # inst - number instances, n - number variables, k - maximum domain size, w - induced width ordering used, hT - pseudo-tree height.induced width one crucial parameters indicating difficulty problem,difference induced width mini-bucket i-bound signifies strengthheuristic. i-bound considerably smaller induced width, heuristicweak, i-bound equal greater induced width yields exact heuristic,turn yields much faster search. Clearly, large number variables, high domain size largepseudo tree height suggest harder problems.7.1.1 LGORITHMSdistinguish 6 algorithms: BE+m-BF, m-A*-tree m-BB-tree exploring regularsearch tree modifications explore AND/OR search tree, denoted m-AOBF-treem-AOBB-tree. also consider variant m-AOBF explores AND/OR search graphm-AOBF-graph. implement m-AOBB AND/OR search graph,4. http://graphmod.ics.uci.edu/repos/mpe/grids/5. http://www.cs.huji.ac.il/project/PASCAL/archive/mpe.tgz922fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSoverhead due book-keeping looked prohibitive. used m-AOBF representative AND/ORgraph search, see proved indeed cost-effective. algorithmsguided pre-compiled mini-bucket heuristics, described Section 2.3.4. used 10 i-bounds,ranging 2 22. However, hard problems computing mini-bucket heuristiclarger i-bounds proved infeasible, actual range i-bounds varies among benchmarksamong instances within benchmark. algorithms restricted static variable orderingcomputed using min-fill heuristic (Kjrulff, 1990). AND/OR schemes used pseudotree. implementation algorithms m-BB, m-BF m-AOBF break ties lexicographically,algorithm m-AOBB solves independent subproblems rooted node increasingorder lower bound heuristic estimates.algorithms implemented C++ (32-bit) experiments run 2.6GHzquad-core processor. memory limit set 4 GB per problem, time limit 3 hours.report CPU time (in seconds) number nodes expanded search. uniformityconsider task throughout maximization-product problem, also knownProbable Explanation task (MPE MAP). focus complete exact solutions thusreport results algorithm found less solutions (for best-first schemes)optimality solutions proved (for branch bound schemes).7.1.2 G OALS E MPIRICAL E VALUATIONaddress following aspects:1. Comparing best-first depth-first branch bound approaches2. impact AND/OR decomposition search performance3. Scalability algorithms number required solutions4. Comparison earlier proposed algorithms7.2 Main Trends Behavior AlgorithmsTables 2, 4, 6, 8, 10, 12 present algorithms raw results form runtimeseconds number expanded nodes select instances benchmark, selectedbest illustrate prevailing trends. benchmark show results two valuesi-bound, corresponding, cases, relatively weak strong heuristics. Note ibound impact BE+m-BF, since always calculates exact heuristic. show threevalues number solutions m, equal 1 (ordinary optimization problem), 10 100.order see bigger picture, Figures 9-14 show bar charts representingbenchmark median runtime number instances solved algorithm particularstrength heuristic (i-bound) {1, 2, 5, 10, 100}. y-axis logarithmic scale.numbers bars indicate actual values median time seconds numbersolved instances, respectively. important note figures account harderinstances, i-bound yield exact heuristic. acknowledge mediantimes strictly comparable since calculated varied number instances solvedalgorithm. However, metric robust outliers gives us intuitionalgorithms relative success. addition, Tables 3, 5, 7, 9, 11 13 show benchmarknumber instances, given algorithm best terms runtime termsnumber expanded nodes. several algorithms show best result, counts towardsscore them.923fiF LEROVA , ARINESCU , & ECHTERinstance(n,k,w ,h)i-boundm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFOOMOOMOOMTimeoutTimeout0.056647OOMOOMOOM1269.034864882522.9923202230.056647number solutionsm=10nodesOOMOOMOOMTimeoutTimeout0.066671OOMOOMOOM1275.65348648869164.72121105590.066671m-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFOOMOOMOOMOOT1461.7646419482OOMOOMOOMOOM151.4933563300107.034274313OOMOOMOOMOOMOOT2389.3274629839OOMOOMOOMOOM152.2733609110185.667245553OOMOOMOOMOOMOOT3321.4783802828OOMOOMOOMOOM148.0836255491251.988319419OOMm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFOOMOOMOOMTimeout5760.2514260410OOMOOMOOM793.562579416Timeout484.691530768OOMOOMOOMOOMTimeoutOOTOOMOOMOOMOOMTimeout551.671995114OOMOOMOOMOOMTimeoutOOTOOMOOMOOMOOMTimeout858.293507104OOM10.2210.29algorithmm=1time503.wcsp4(144, 4, 9, 44)8myciel5g 3.wcsp4(47,2, 19, 46)8satellite01ac.wcsp4(79, 8, 19, 56)829.wcsp4(83, 4, 18, 58)8m-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BF10.2311.5912.961.810.00.050.090.020.170.020.0nodes464134OOM938812224361987717111234714012098376291577111time11.5812.892.630.00.050.090.020.170.330.0464182OOM93886922451371478511682395144721553846324239168m=100timenodesOOMOOMOOMTimeoutTimeout0.066984OOMOOMOOM1255.463486517758010.425681483860.06698411.5712.77115.30.010.080.130.020.2579.380.01464698OOM93950822795879189667739289914822724551256731546739Table 2: WCSP: CPU time (in seconds) number nodes expanded. Timeout standsexceeding time limit 3 hours. OOM indicates 4GB memory. boldhighlight best time number nodes m. Parameters: n - numbervariables, k - domain size, w - induced width, h - pseudo tree height.next provide elaboration interpretation results.7.2.1 WCSPTable 2 shows results two values i-bound select instances chosen best illustratecommon trends seen across WCSP benchmark. Figure 9 presents median timenumber solved instances algorithm i=16. Table 3 shows i-bound (i=16)number instances schemes best runtime best numberexpanded nodes. many problem instances benchmark mini-bucket eliminationlarge i-bounds infeasible, thus present results small medium i-bounds.924fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSBE+m-BFm-AOBF treem-AOBF graphm-A* tree1.940.010.040.080.051.711010063323323323322Solved instancesWCSPs, i=16334566m-BB treem-AOBB tree555556m-AOBF graphm-A* tree6BE+m-BFm-AOBF tree60.00.010.030.051.70.00.010.020.055255110-12.82.792.711.710.00.010.020.041.710.10.00.010.020.05Median timeWCSPs, i=16100m-BB treem-AOBB tree6.3310110012510100Figure 9: Median time number solved instances (out 49) select values WCSPs,i-bound=16. Numbers bars - actual values time (sec) # instances. Totalinstances benchmark: 61, discarded instances due exact heuristic: 12.BE+m-BF. suggested theory, whenever BE+m-BF run memory,efficient scheme. See example Table 2, 503.wcsp 29.wcsp. However, calculationexact heuristic feasible easier instances and, Figure 9 shows, solve925fiF LEROVA , ARINESCU , & ECHTERalgorithmsolvedm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFWCSPs: # inst=61, n=14-1058k=2-100, w =6-287, hT =8-585, i-bound=16m=1m=2m=5m=10m=100#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN43434343431/21/10/11/10/01/20/20/20/20/35/14/35/34/35/21/02/01/01/00/01/22/00/00/00/02/12/12/12/12/1Table 3: Number instances, algorithm best runtime (#BT) best numberexpanded nodes (#BN), WCSPs. 61 instances 12 exact heuristics. tableaccounts remaining 49, i-bound= 16 .2 WCSP instances. seen Table 3, two instances BE+m-BF demonstrated bestruntime among schemes.m-AOBB-tree. number problems small values m, m-AOBB-tree superior mBB-tree terms runtime number expanded nodes. example, 29.wcsp,i=4, m=10 m-AOBB-tree requires 2.63 seconds solve problem expands 147851 nodesruntime m-BB-tree 12.89 seconds expands 2245137 nodes. However,majority instances m-AOBB-tree slower schemes, seen Figure 9. Moreover,m-AOBB-tree scales poorly number solutions. = 100 oftenworst runtime largest explored search space among schemes, e.g. i=8, 503.wcsp.striking decrease performance grows consistent across various benchmarksexplained need combine sets partial solutions nodes describedearlier. overhead connected AND/OR decomposition also accounts larger time pernode ratio m-AOBB-tree, compared schemes. example, Table 2 instancemyciel5g 3.wcsp, i=8, m=10 m=100 m-AOBB-tree expands less nodes m-BB-tree,runtime larger. Nevertheless, m-AOBB-tree benefits. Since space efficientalgorithms, often scheme able find solutions harder instances,especially heuristic weak, see myciel5g 3.wcsp i=4 satellite01ac.wcspi=4 i=8, respectively.m-BB-tree. Figure 9 see m-BB-tree solves almost number problemsm-AOBB-tree considerably better median time.m-AOBF-tree m-AOBF-graph. Unsurprisingly, best-first search algorithms often runspace problems feasible branch bound, 503.wcsp myciel5g 3.wcsp i=8.m-AOBF-based schemes overall inferior algorithms, solving, Figure 9 shows,least number problems. schemes run memory much often m-A*-tree.believe due overhead maintaining OPEN list partial solution trees, opposedOPEN list individual nodes m-A*-tree does. Whenever m-AOBF schemes managefind solutions, example instance 29.wcsp, i=8, m-AOBF-graph explores smallest926fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSsearch space among schemes, except BE+m-BF. time m-AOBF-tree sometimesexpands nodes compared m-AOBF-graph, also m-A*-tree,would normally expect, since m-A*-tree traverses search space, inherently largerAND/OR one. However, important remember though better space efficiencyAND/OR schemes often observed, guaranteed. Many factors, tie breakingnodes value evaluation function, impact performance mAOBF-tree. m-AOBF-tree m-AOBF-graph almost median time numbersolved problems, seen Figure 9.m-A*-tree. three best first algorithms m-A*-tree overall best. Figure 9 seesolves instances schemes values median runtimeclose BE+m-BF. Table 3 proves i-bound=16 scheme fastest amongschemes largest number instances, showing best runtime 4-5 instances, dependingm. explained part relatively reduced overhead maintaining search spaceOPEN list memory, compared example m-AOBF schemes.7.2.2 P EDIGREESTable 4 displays results select instances Pedigree benchmark two i-bounds each.Overall, difference results algorithms greatly diminishes heuristicstrength increases. Figure 10 shows median time number solved instances selectvalues i=16. number instances schemes best runtimebest number expanded nodes i-bound presented Table 5.BE+m-BF. BE+m-BF often superior algorithms, especiallyschemes use lower values i-bound, e.g. pedigree23, i=12, ms. large i-bounds thusaccurate heuristics difference much smaller. Moreover, sometimes BE+m-BFslower schemes, due time required calculate exact heuristic, e.g. pedigree23,i=16. Table 5 shows BE+m-BF overall fastest. see Pedigree benchmarkalgorithm quite successful, evident many instances solved (see Figure 10).m-AOBB-tree. low values m-AOBB-tree slightly superior algorithms,solving number instances, (see Figure 10). hand, median timelargest. fails solve instances m=100. Table 4 see m-AOBB-treeslowest, (e.g., pedigree23, i=16, ms). Yet, instance pedigree33, i=12, m=1, schemeone find solution.m-BB-tree. expected, m-BB-tree inferior best-first search schemes unless latterrun memory. case WCSP, scheme often faster m-AOBB-tree,example, pedigree30, i=16, values m. bar charts show m-BB-tree secondworst median time values m, solves number problems m=100.m-AOBF schemes. m-AOBF algorithms unsuccessful Pedigree benchmark.often run memory even = 1 (e.g. pedigree33, i=22). instancesreport solution m-AOBF-tree faster m-AOBF-graph, though difference usuallylarge.m-A*-tree. saw WCSPs, pedigree instances m-A*-tree faster twom-AOBF schemes, seen Figure 10, values m. Moreover, superior harder instances927fiF LEROVA , ARINESCU , & ECHTERinstance(n,k,w ,h)i-boundpedigree3312algorithmm=1time(798, 4, 24, 132)22pedigree3012(1290, 5, 20, 105)16pedigree2312(403, 5, 21, 64)16pedigree2012(438, 5, 20, 65)16nodesnumber solutionsm=10timenodesOOMOOMOOMTimeoutTimeoutOOMOOMOOM1.55771384.15177397TimeoutOOMm=100nodesOOMOOMOOMTimeoutTimeoutOOMOOMOOM3.7611242221.48655141TimeoutOOMtimem-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFOOMOOMOOMTimeout7814.77145203641OOMOOMOOM1.32736252.981457172.8870644OOMm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFOOMOOMOOMTimeout2510.59334539957.906423OOMOOM65.43486638884.2812243789594.3669073997.906423OOMOOMOOMTimeoutOOT7.997611OOMOOM65.86486755185.7212298570Timeout7.997611OOMOOMOOMTimeoutOOT9.2223028OOMOOM67.014882985127.2513027245Timeout9.2223028m-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFOOMOOM8.4471366423.0444622432.118310967.11630OOMOOM0.52538624.583728813.393461457.11630OOMOOM7157294676953753559012482OOMOOM55927959931502521072482OOMOOM11.1590480235.466179124Timeout7.6819297OOMOOM1.178530014.141641751OOM7.6819297m-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BF27.034.2388.8524.9524.9427.3254.4624.95OOMOOM232198672393794365855491OOMOOM2279192585730125313224918.4624.561077.97.240.585.61713.737.2426.6637.631019.7624.9924.9329.14970.6124.99OOMOOM23247017434961639405153482OOMOOM22819075946423593338283482OOMOOM23539279155747Timeout26.1632643OOMOOM25.97231113340.26617200Timeout26.163264327.4766.81Table 4: Pedigrees: CPU time (in seconds) number nodes expanded. Timeout standsexceeding time limit 3 hours. OOM indicates 4GB memory. boldhighlight best time number nodes m.Parameters: n - numbervariables, k - domain size, w - induced width, h - pseudo tree height.infeasible m-AOBF schemes BE+m-BF, e.g. pedigree23, i=16. shown Figure 10,solves 5 instances i=16, ms, best second best results, dependingnumber solutions. However, median time m-A*-tree considerably largerBE+m-BF, i-bound latter solves single instance less.7.2.3 B INARY G RIDSTable 6 shows results select instances grid networks domain. Figure 11 showsmedian runtime number solved instances i=18, Table 7 presents numberinstances, algorithm best, i-bound. trends algorithms928fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSm-AOBF graphm-A* treem-BB treem-AOBB tree842.17BE+m-BFm-AOBF tree6.665.625.6528.1329.1440.298.29m-BB treem-AOBB tree5545524555645560.00.00.0100m-AOBF graphm-A* tree55410BE+m-BFm-AOBF tree101000.050.00.020.00.010.00.010-10.00.01000.00.0Solved instancesPedigrees, i=160.320.00.052410.270.00.00.230.00.01000.230.00.01.395.61015.5827.3210227.4162.91Median timePedigrees, i=16280.27103Figure 10: Median time number solved instances (out 12) select values Pedigrees, i-bound=16. Numbers bars - actual values time (sec) # instances.Total instances benchmark: 13 , discarded instances due exact heuristic: 1.behavior observed WCSP Pedigree benchmarks also noticed Grid benchmarkwell. particular, m-AOBB-tree successful small, even solvinginstances, seen Figure 11. shows worse results m=100 numbersolutions largest median time. m-BB-tree smaller median time ms, still929fiF LEROVA , ARINESCU , & ECHTERalgorithmsolvedm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFPedigrees: # inst=13, n=335-1290k=3-7, w =15-47, hT =52-204, i-bound=16m=1m=2m=5m=10m=100#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN666660/00/00/00/00/00/00/00/00/00/01/11/11/11/11/10/00/01/11/11/11/11/10/00/00/04/44/44/44/44/4Table 5: Number instances, algorithm best runtime (#BT) best numberexpanded nodes (#BN), Pedigrees. 13 instances 1 exact heuristics. tableaccounts remaining 12, i-bound= 16 .considerably slower best-first schemes. m-A*-tree presents best compromisesmall medium running time relatively large number solved instances. Table 7shows majority grid instances fastest algorithm. two m-AOBF schemesresults quite similar other, solving almost number instances ms littledifference median runtimes, shown Figure 11. consistently inferiorschemes except BE+m-BF, often runs memory. main differenceGrid benchmark compared previously discussed domains lies behaviour BE+mBF i-bound high. Even though expands less nodes, many problems BE+m-BFslower schemes due large time required compute exact heuristic.example, grid 75-19-5, i=18, m=10 runtime BE+m-BF 143.11 seconds, evenm-AOBB-tree, known slow, terminates 94.0 seconds. time, instanceBE+m-BF explores smallest search space values m.7.2.4 P ROMEDASTable 8 shows results Promedas benchmark. Figure 12 presents median time number solved instances benchmark i=16. Table 9 shows i-bound numberinstances schemes best runtime best number expanded nodes.significant fraction instances solved algorithms, especially lowmedium i-bounds. Unlike benchmarks, m-AOBB-tree solves instancessmall ms, also quite successful m=100, solving one instance less bestscheme value m, m-BB-tree. Moreover, sometimes m-AOBB-tree schemereport solutions, especially weak heuristic, e.g. chain 50.fg chain 212.fg, i=12.BE+m-BF runs memory instances, seen Table 8. Overall, variancealgorithms performance significant Promedas previously discussed benchmarks. example, see Figure 12, i=16 m-A*-tree, m-BB-tree m-AOBB-tree solve25 33 instances [1, 10], BE+m-BF m-AOBF-based schemessolve 4 8 instances. Table 9 demonstrates m-A*-tree often fastestalgorithms.930fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSinstance(n,k,w ,h)50-15-5i-boundm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFOOMOOM8.838668658.7519671523.29251502OOMOOMOOMOOMOOMTimeout347.2417332742OOMOOMnumber solutionsm=10timenodesOOMOOM11.97117754911.91264739334.282485393OOMOOMOOMOOMOOMTimeout692.5928676212OOMOOMm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFOOMOOMOOMTimeout3290939289OOMOOM5120535570085289289OOMOOMOOMTimeout368.181643170718.471220OOMOOM0.39557831.83421798116.77750531018.471220OOMOOMOOMTimeoutTimeout18.679534OOMOOM0.891046214.92892065Timeout18.679534m-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFOOMOOMOOMTimeout347.2417332742OOMOOMOOMOOM1.315339974.831496868346.532450725OOMOOMOOMOOMOOMTimeout692.5928676212OOMOOMOOMOOM1.8821154776.9315403354118.264940247OOMOOMOOMOOMOOMTimeout2277.9275442102OOMOOMOOMOOM3.5336234485.4216631321563.2218306275OOMOOMm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFOOMOOMOOMTimeout3591.1119431966143.11361OOMOOM14.3160950616.27400508239.661367955143.11361OOMOOMOOMTimeoutTimeout143.112330OOMOOM18.76202984422.28532057394.03480629143.112330OOMOOMOOMTimeoutTimeout144.1116897OOMOOM28.04299543737.268191215Timeout144.1116897algorithmm=1time10(400, 2, 27, 99)1850-17-510(289, 2, 22, 84)1890-20-510(400, 2, 27, 99)1875-19-510(361, 2, 25, 89)1882.9518.450.351.391.7918.45nodesm=100timenodesOOMOOM20.22193103922.064708311TimeoutOOMOOMOOMOOMOOMTimeout2277.9275442102OOMOOMTable 6: Grids: CPU time (in seconds) number nodes expanded. Timeout standsexceeding time limit 3 hours. OOM indicates 4GB memory. boldhighlight best time number nodes m. Parameters: n - numbervariables, k - domain size, w - induced width, h - pseudo tree height.7.2.5 P ROTEINTable 10 shows select Protein instances i=4 i=8, respectively. Figure 13 Table 11 showsummary results i=4. benchmark fairly difficult due large domain size(up 81). heuristic calculation feasible higher i-bounds. particular, BE+m-BFconsiderable problems calculating exact heuristic. Even low i-bounds relatively easyinstances solved. Note instances pdb1ctk pdb1dlw i-bound=8 yields exact heuristic.m-AOBF-tree m-AOBF-graph fail find solutions within memory limitmajority instances, e.g., pdb1b2v pdb1cxy, i=4. much differenceruntimes algorithms, exception m-AOBB-tree. example, pdb1b2v, i=8,931fiF LEROVA , ARINESCU , & ECHTERm-AOBF graphm-A* treem-BB treem-AOBB tree440.64BE+m-BFm-AOBF tree37.260.1610100m-BB treem-AOBB tree13151315181315131519m-AOBF graphm-A* tree19BE+m-BFm-AOBF tree0.391.041.230.020.380.91.30.01522.143.231.022.2820.1645.3517.4928.040.36113151810-10.00.350.011000.830.781011.430.8816.2720.68Median timeGrids, i=1810291.79103653345555456455Solved instancesGrids, i=18610110012510100Figure 11: Median time number solved instances (out 31) select valuesGrids, i-bound=18. Numbers bars - actual values time (sec) # instances.Total instances benchmark: 32, discarded instances due exact heuristic: 1.m-AOBB-tree requires 6.46 seconds find m=10 solutions, runtimes algorithmsrange 0.03 0.09 seconds (except BE+m-BF runs memory). However,slow performance m-AOBB-tree easier problems feasible algorithmscompensated fact many instances scheme report solution, solving932fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSalgorithmsolvedm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFGrids: # inst=32, n=144-2500k=2-2, w =15-74, hT =48-312, i-bound=18m=1m=2m=5m=10m=100#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN12121313140/00/00/00/00/00/00/10/20/10/38/28/28/69/67/61/01/00/01/02/27 / 107 / 105/55/52/25/75/66/56/66/4Table 7: Number instances, algorithm best runtime (#BT) best numberexpanded nodes (#BN), Grids. 32 instances 1 exact heuristics. tableaccounts remaining 31, i-bound= 18 .instances considerable amount [1, 10] (Figure 13). Table 11 shows m-AOBBtree best terms time space overwhelming majority problemsvalues except = 100.7.2.6 EGMENTATIONTable 12 shows results select instances Segmentation benchmark two i-bounds,namely i=4 i=12, Figure 14 Table 13 present summary results i=12.Unlike WCSP, benchmark chose display relatively low i-bounds calculating heuristic larger infeasible, problems low induced widthwished avoid displaying results obtained exact heuristics. main peculiaritybenchmark striking success BE+m-BF. Overall solves many instances usuallysuperior m-A*-tree m-BB-tree, seen Figure 14. Moreover, runtime superiorschemes, true instances Table 12 also illustrated resultsTable 13. heuristic weak, m-AOBB-tree fairly successful, example, findingsolutions values 12 4 s.binary, i=4, infeasible scheme exceptBE+m-BF. However, usual, m-AOBB-tree overall slowest schemes.7.3 Best-first vs Depth-First Branch Bound Best SolutionsLet us consider data presented Tables 2-13 Figures 9-14 order summarizeobservations contrast performance best-first depth-first branch bound schemes.Among best-first search schemes m-A*-tree successful. often effective,armed good heuristic, requires less space best-first schemes.already noted, BE+m-BF shows good results Segmentation benchmark, bestalgorithm terms median runtime, solving least number problemsschemes. However, benchmarks calculation exact heuristic ofteninfeasible.933fiF LEROVA , ARINESCU , & ECHTERinstance(n,k,w ,h)i-bound16(620, 2, 30, 64)22(676, 2, 30, 70)chain 212.fg121612(773, 2, 33, 79)22chain 50.fgm=1timechain 107.fgchain 141.fgalgorithm12661, 2, 36, 76)22nodesm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFOOMOOM7.8991986514.58313971167.951398364OOMOOMOOM9.2109356417.03861414122.013214924OOMm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFOOMOOMOOM9553.911276222668272.09878480OOMOOMOOM14.161261489279.6156821714140.96490042OOMm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFnumber solutionsm=10timenodesOOMOOM18.89210812235.157051974229.495134280OOMOOMOOM20.83246536442.369205755418.4611123810OOMm=100time44.61102.6627.9456.59100.45855.84nodesOOMOOM46416271849463013594667OOMOOMOOM63568711921742721388619OOMOOMOOMOOMOOT25481595OOMOOMOOM33799268794780214103095OOMOOMOOMOOMOOT2091.2664400241OOMOOMOOMOOM885.72160581726909.4833842266OOMOOMOOMOOMTimeout49808550OOMOOMOOM11187921592280611336657OOMOOMOOMOOMTimeout4206.07111853485OOMOOMOOM33.873669711141.66276150331239.8824717964OOMOOMOOMOOMTimeoutTimeoutOOMOOMOOM78.378186757342.51582461015032.1186444575OOMOOMOOMOOMTimeout1404.2733495406OOMOOMOOM53.87567394891.1518515503TimeoutOOMOOMOOMOOMTimeout3748.8593992107OOMOOMOOMOOM176.1434915510TimeoutOOMOOMOOMOOMTimeout10070.0245628104OOMOOMOOMOOM447.4685945673TimeoutOOM1772.89.9178.08584.83721.6738.48460.15315.2Table 8: Promedas: CPU time (in seconds) number nodes expanded. Timeout standsexceeding time limit 3 hours. OOM indicates 4GB memory. boldhighlight best time number nodes m. Parameters: n - numbervariables, k - domain size, w - induced width, h - pseudo tree height.two m-AOBF-based schemes overall inferior due prohibitively large memory, solvingfewer instances algorithms. believe non-trivial extension AOBFsingle solution m-best task straightforward, hard represent multiplepartial solution trees efficient manner. order efficient m-AOBF implementation,one needs quickly identify partial solution subtree select extend next, searching (k + 1)th solution finding k th best solution. AOBF (for 1 solution) usesarc-marking mechanism efficiently represent current best partial solution subtreesearch, easy extend case searching best solutions. Therefore,shown Section 5.1, m-AOBF implements naive mechanism par934fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSBE+m-BFm-AOBF tree105m-AOBF graphm-A* treem-BB treem-AOBB tree10100m-AOBF graphm-A* treem-BB treem-AOBB tree272688882344566777Solved instancesPromedas, i=16101817262730282732282733BE+m-BFm-AOBF tree1.362.974.453.6260.17.470.151.552.03.431.957.080.0852185.53325.85267.6951.79146.850.03125273110-12.031.375.190.010.480.621011007.6433.6810245.83109.68Median timePromedas, i=161031268.8810410012510100Figure 12: Median time number solved instances (out 86) summary select valuesPromedas, i-bound=16. Numbers bars - actual values time (sec) #instances. Total instances benchmark: 75, discarded instances due exact heuristic:11.tial solution trees represented explicitly memory. simple representation, however, incursconsiderable computational overhead searching best solutions, indeedrevealed experiments. efficient implementation m-AOBF left future work.935fiF LEROVA , ARINESCU , & ECHTERalgorithmsolvedm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFPromedas: # inst=86, n=197-2113k=2-2, w =5-120, hT =34-187, i-bound=16m=1m=2m=5m=10m=100#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN42424544460/00/00/00/00/00/10/20/20/30/222 / 1721 / 1718 / 1718 / 159/91/01/01/02/010 / 54/84/83/54/82/78/78/68/68/58/6Table 9: Number instances, algorithm best runtime (#BT) best numberexpanded nodes (#BN), Promedas. 86 instances 11 exact heuristics.table accounts remaining 75, i-bound= 16 .Unsurprisingly, branch bound algorithms robust terms memory alsodominate m-A*-tree best-first schemes many benchmarks terms numberinstances solved. However, tend considerably larger median time expandnodes. particular, m-AOBB-tree scale well number solutions largevalues runtime increases drastically. Unlike m-AOBF, whose inferior performanceattributed specifics implementation, depth-first m-AOBB suffers issues inherentsolving m-best problem depth-first manner. Algorithm 10 describes, m-AOBB needsmerge best partial solution internal node, hurts performance significantly,cannot avoided, unless algorithmic approach fundamentally changed.see way overcome limitation.Overall, whenever calculation exact heuristic feasible, BE+m-BFalgorithm choice. Otherwise, m-A*-tree superior relatively easy problems, mAOBB-tree best scheme hard memory intensive instances. superiority best-firstapproach, whenever memory available, expected, based, one hand, intuition derivedknowledge task finding single solution, hand, theoreticalresults Section 3.2.7.4 Scalability Algorithms Number Required SolutionsFigures 15-17 present plots showing runtime seconds number expanded nodesfunction number solutions (on log scale) two instances benchmark.Figure 15 displays results WCSP Pedigree benchmarks, Figure 16 - Grids Promedas,Figure 17 - Proteins Segmentation. Lower values (on y-axis) preferable. rowcontains two instances benchmarks specific value i-bound, runtime plotsshown ones containing expanded nodes. examples chosen bestillustrate prevailing tendencies.Note theoretical analysis suggests runtime BE+m-BF, best amongalgorithms, scale since worst case complexity O(nk w + mn). theoreticalcomplexity best-first search schemes m-AOBF-tree m-A*-tree linear number936fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSinstance(n,k,w ,h)pdb1b2v(133, 36, 13, 33)pdb1cxy(70, 81, 9, 19)i-boundm=1time4848pdb1ctj4(62, 81, 8, 21)8pdb1dlw4(84, 81, 8, 29)algorithm8m-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFnodesOOMOOM0.1210.236.510.020.030.00.010.292508948337100584OOM959513924016563OOMnumber solutionsm=10nodesOOMOOM0.17318611.09103464535.14827365OOM0.062940.091080.035970.0788616.46256588OOMtimeOOMOOM0.380.4370851020OOMOOMOOM0.011210.0357910.667029OOM10.43844.085.640.010.00.020.010.010.220.0146.1747.27187.380.010.141.0618.530.01OOMOOM354007660926074833624945621118309862OOMOOM57910863803021451906294OOMOOM6900162913154850294m=100timenodesOOMOOM0.3414.44462.00.420.640.150.562491404370230849005OOM1956135305167330TimeoutOOMOOMOOM0.480.6OOMOOM443473849OOM10854191203OOMOOMOOM0.128700.275370244.281335157OOMOOMOOMOOMOOMOOMOOMOOM0.040.072.0413.231039.9618.290.020.070.110.030.032.320.0246.2647.33544.550.050.181.09157.010.054801142934567435389442261430605426530274265538554324265OOMOOM579405639110712759004635OOMOOM724016703786321146350.941.4519.741325.84157.430.070.420.750.080.1571.860.0746.4950.720.390.521.860.39653401207868335307198105018259510573106632733241050OOMOOM5823756762911OOT4265OOMOOM10855280189OOT4265Table 10: Protein: CPU time (in seconds) number nodes expanded. Timeout standsexceeding time limit 3 hours. OOM indicates 4GB memory. boldhighlight best time number nodes m. Parameters: n - numbervariables, k - domain size, w - induced width, h - pseudo tree height.solutions, m-BB-tree overhead due m-best task factor (m log m)m-AOBB-tree (m log deg), deg degree pseudo tree. observedcompared schemes runtime BE+m-BF indeed rises quite slowly numbersolutions increases, even reaches 100. runtime m-A*-tree also scales well m.behaviour m-BB-tree depends lot benchmarks. Pedigrees Protein runtimechanges little instances number solutions grows, benchmarks,runtime m=100 tends significantly larger m=1. m-AOBF-tree m-AOBFgraph often provide solutions even m=1 or, alternatively, run memoryslightly increases (m [2, 10]). algorithms clearly successful practice.937fiF LEROVA , ARINESCU , & ECHTERBE+m-BFm-AOBF tree105m-AOBF graphm-A* treem-BB treem-AOBB tree1457.6145.2841.91190.2839.45177.337.46102220.6410350.72597.38Median timeProtein, i=4104521010010101313131313131313Solved instancesProtein, i=42025354235253542m-BB treem-AOBB tree253525253544m-AOBF graphm-A* tree44BE+m-BFm-AOBF tree0.140.380.672.952.890.030.190.3910.010.170.322.870.010.140.272.840.010.130.251003.391011001256666610110100Figure 13: Median time number solved instances (out 72) select valuesProtein, i-bound=4. Numbers bars - actual values time (sec) # instances.Total instances benchmark: 72, discarded instances due exact heuristic: 0.discussed before, runtime number expanded nodes m-AOBB-tree increasedrastically gets larger.938fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSalgorithmsolvedm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm=1#BT / #BN268/31 / 1011 / 98/121 / 226/4Protein: # inst=72, n=15-242k=18-81, w =5-16, hT =7-44, i-bound=4m=2m=5m=10#BT / #BN #BT / #BN #BT / #BN2627277/17/17/02 / 122 / 121 / 139/912 / 914 / 109/28/36/321 / 2118 / 1917 / 176/25/25/2m=100#BT / #BN345/01 / 1017 / 1311 / 103/36/2Table 11: Number instances, algorithm best runtime (#BT) bestnumber expanded nodes (#BN), Protein. 72 instances 0 exact heuristics.table accounts remaining 72, i-bound= 4 .7.5 Comparison Competing Algorithmscompare methods number previously developed schemes described detailsSection 6: STRIPES, PESTEELARS Nilssons algorithm. implementationsschemes provided Dhruv Batra. first two approaches based ideas LP relaxations approximate, known often find exact solutions, though provideguarantees optimality. Nilssons algorithm exact message-passing scheme operatingjunction tree. first set experiments (on tree benchmark) also show resultsSTILARS algorithm, older version PESTEELARS algorithm. However, schemeconsistently inferior two LP-based schemes considered twobenchmarks. following, collectively refer 4 algorithms competing schemes.7.5.1 R ANDOMLY G ENERATED B ENCHMARKSavailable us code LP-based Nilssons approaches developed run restricted inputs only, could applied benchmarks used bulk evaluation described above. concluded re-implementing competing codes work generalinput would time consuming would provide additional insights. Thus chosecompare algorithms competitors using benchmarks acceptablecompeting schemes.Specifically, comparison performed following three benchmarks: random trees,random binary grids random graphs submodular potentials, call submodulargraphs remainder section. Table 14 shows parameters benchmarks.instances generated following manner. First, vector 12 logarithmically spaced integers 10 103.5 generated, serving number variables instances.binary grids benchmarks value used generate two problems numbervariables. edges variables generated uniformly randomly, makingsure end graph tree, grid loopy graph, depending benchmark. edgedefine binary potential vertex unary potential exponential form: f = e ,939fiF LEROVA , ARINESCU , & ECHTERinstance(n,k,w ,h)i-bound4(225, 2, 16, 48)12(227, 2, 16, 57)7 9 s.binary(234, 2, 16, 53)OOMOOMOOMTimeout164.9156533120.02257.3110332710.4718430.0337540.0482510.0841580.0225m-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFOOMOOMOOMTimeout71.7127337030.012270.2333380.337990.015850.05106870.21110760.01227OOMOOMOOMTimeout360.14149062120.0213653.75461215.7218270.0991030.193011914.2810546280.021365m-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFOOMOOMOOMTimeout127.1733379490.012348.85122663OOM0.0219780.0344150.0527500.01234OOMOOMOOMTimeout505.08179762000.031337OOMOOM0.0641700.111335710.548064900.031337m-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFOOMOOMOOMTimeout110.1942274370.01231OOMOOM1.021026712.074287910.75391700.01231OOMOOMOOMTimeout555.6233021650.031615OOMOOM1.171154072.952796711.998094030.031615m=1time12 4 s.binary16 16 s.binarym-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFnumber solutionsm=10nodesOOMOOMOOMTimeout505.82188883210.02161910.36143333OOM0.0656920.21243491.621180740.021619algorithm41241211 4 s.binary4(231, 2, 16, 57)12nodestimem=100timenodesOOMOOMOOMTimeout4371.051891797260.2111194OOMOOM0.3186161.32131571489.57409610800.21111940.190.381.20.190.210.280.950.21OOMOOMOOMTimeoutOOT11157OOMOOM30542141591OOT11157OOMOOMOOMTimeoutOOT10212OOMOOM1580789675OOT10212OOMOOMOOMTimeoutOOT0.2814241OOMOOM1.861679837.110101558497.936172278540.2814241Table 12: Segmentation: CPU time (in seconds) number nodes expanded. Timeoutstands exceeding time limit 3 hours. OOM indicates 4GB memory.bold highlight best time number nodes m. Parameters: n - numbervariables, k - domain size, w - induced width, h-pseudo tree height.real number sampled uniform distribution. third benchmark potentials modified submodular. random trees m-best optimization LPproblem guaranteed tight, graphs submodular potentials LP optimizationproblem tight, m-best extension not, arbitrary loopy graphs, including grids,algorithms provide guarantees.7.5.2 C OMPETING LGORITHMS P ERFORMANCETable 15 shows runtimes select instances random tree benchmark 5 mbest search schemes competing LP schemes STILARS, PESTEELARS STRIPES.940fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSBE+m-BFm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB tree1051949.3103510013141418192024242424m-BB treem-AOBB tree24242424242024242424m-AOBF graphm-A* tree20202120Solved instancesSegmentation, i=1224242424BE+m-BFm-AOBF tree10110242420.020.310.40.030.08110-10.010.220.260.020.040.70.010.150.20.010.030.041003.941010.171.11.550.170.651020.00.080.10.010.010.03Median timeSegmentation, i=1210412510100Figure 14: Median time number solved instances (out 47) select valuesSegmentation, i-bound=12. Numbers bars - actual values time (sec) #instances. Total instances benchmark: 47, discarded due exact heuristic: 0.observed benchmarks STILARS always inferior two schemestherefore excluded remainder evaluation. Instead, Tables 16 17,show results random binary grids submodular graphs benchmarks, addedcomparison Nilssons max-flow algorithm. Table 15-17 time limit set 1 hour,memory limit 3 GB. schemes behavior quite consistent across instances.941fiF LEROVA , ARINESCU , & ECHTERTime vs m. WCSPs: bwt3ac.wcspTime vs m. WCSPs: queen5_5_3.wcsp(45, 11, 16, 27), i=4(25, 3, 18, 21), i=167156Time, secTime, sec51054321001.010.0Number solutionsm-AOBF-treem-AOBF-graph2.01e6m-A*-treem-BB-tree100.01.0m-AOBB-treeBE+m-BF10.0m-AOBF-treem-AOBF-graphNodes vs m. WCSPs: bwt3ac.wcsp(45, 11, 16, 27), i=41.41e6100.0Number solutionsm-A*-treem-BB-treem-AOBB-treeBE+m-BFNodes vs m. WCSPs: queen5_5_3.wcsp(25, 3, 18, 21), i=161.21.51.0NodesNodes0.81.00.60.50.40.00.00.21.010.0Number solutionsm-AOBF-treem-AOBF-graphm-A*-treem-BB-tree100.01.0m-AOBB-treeBE+m-BFm-AOBF-treem-AOBF-graphm-A*-treem-BB-tree100.0m-AOBB-treeBE+m-BFTime vs m. Pedigrees: pedigree9Time vs m. Pedigrees: pedigree30(1290, 5, 20, 105), i=16(1119, 7, 25, 123), i=221400200012001500Time, sec1000Time, sec10.0Number solutions8001000600400200050001.010.0Number solutionsm-AOBF-treem-AOBF-graphm-A*-treem-BB-tree100.01.0m-AOBB-treeBE+m-BFm-AOBF-treem-AOBF-graphNodes vs m. Pedigrees: pedigree301e710.0Number solutionsm-A*-treem-BB-tree100.0m-AOBB-treeBE+m-BFNodes vs m. Pedigrees: pedigree9(1290, 5, 20, 105), i=1653(1119, 7, 25, 123), i=22Nodes41.5Nodes2.01e71.020.510.001.010.0Number solutionsm-AOBF-treem-AOBF-graphm-A*-treem-BB-tree100.01.0m-AOBB-treeBE+m-BF10.0Number solutionsm-AOBF-treem-AOBF-graphm-A*-treem-BB-tree100.0m-AOBB-treeBE+m-BFFigure 15: CPU time seconds number expanded nodes function number solutions. WCSP Pedigrees, 4 GB, 3 hours.942fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSTime vs m. Grids: 50-19-5Time vs m. Grids: 75-18-5(361, 2, 25, 93), i=18(324, 2, 24, 85), i=18403500300025002000150010005000Time, secTime, sec30201001.010.0100.0Number solutionsm-AOBF-treem-AOBF-graphm-A*-treem-BB-tree1.0m-AOBB-treeBE+m-BFm-AOBF-treem-AOBF-graphNodes vs m. Grids: 50-19-5676543210m-A*-treem-BB-treem-AOBB-treeBE+m-BF(324, 2, 24, 85), i=181e55NodesNodes432101.010.0100.0Number solutionsm-AOBF-treem-AOBF-graphm-A*-treem-BB-tree1.0m-AOBB-treeBE+m-BF10.0100.0Number solutionsm-AOBF-treem-AOBF-graphTime vs m. Promedas: or_chain_17.fgm-A*-treem-BB-treem-AOBB-treeBE+m-BFTime vs m. Promedas: or_chain_212.fg(773, 2, 33, 79), i=22(531, 2, 19, 51), i=1650001040008Time, secTime, sec100.0Nodes vs m. Grids: 75-18-5(361, 2, 25, 93), i=188 1e810.0Number solutions300062000410002001.010.0m-AOBF-treem-AOBF-graph1e5100.0Number solutionsm-A*-treem-BB-tree1.0m-AOBB-treeBE+m-BF10.0m-AOBF-treem-AOBF-graphNodes vs m. Promedas: or_chain_17.fg(531, 2, 19, 51), i=161e80.6m-A*-treem-BB-treem-AOBB-treeBE+m-BFNodes vs m. Promedas: or_chain_212.fg(773, 2, 33, 79), i=22Nodes0.83Nodes4100.0Number solutions0.4210.200.01.010.0Number solutionsm-AOBF-treem-AOBF-graphm-A*-treem-BB-tree100.01.0m-AOBB-treeBE+m-BF10.0Number solutionsm-AOBF-treem-AOBF-graphm-A*-treem-BB-tree100.0m-AOBB-treeBE+m-BFFigure 16: CPU time seconds number expanded nodes function number solutions. Grids Promedas, 4 GB, 3 hours.943fiF LEROVA , ARINESCU , & ECHTERTime vs m. Protein: pdb1at0Time vs m. Protein: pdb1b2v(122, 81, 8, 25), i=4(133, 36, 13, 33), i=8800700600500400300200100076Time, secTime, sec5432101.010.0100.0Number solutionsm-AOBF-treem-AOBF-graphm-A*-treem-BB-tree1.0m-AOBB-treeBE+m-BFm-AOBF-treem-AOBF-graphNodes vs m. Protein: pdb1at0m-AOBB-treeBE+m-BF2.52.00.6NodesNodes1.50.41.00.20.50.00.01.010.0100.0Number solutionsm-AOBF-treem-AOBF-graphm-A*-treem-BB-tree1.0m-AOBB-treeBE+m-BF10.0100.0Number solutionsm-AOBF-treem-AOBF-graphm-A*-treem-BB-treem-AOBB-treeBE+m-BFTime vs m. Segmentation: 10_16_s.binaryTime vs m. Segmentation: 7_29_s.binary(230, 2, 15, 52), i=4(234, 2, 15, 63), i=1210000880006Time, secTime, secm-A*-treem-BB-tree(133, 36, 13, 33), i=81e50.860004400022000001.010.0100.0Number solutionsm-AOBF-treem-AOBF-graphm-A*-treem-BB-tree1.0m-AOBB-treeBE+m-BF10.0100.0Number solutionsm-AOBF-treem-AOBF-graphm-A*-treem-BB-treem-AOBB-treeBE+m-BFTime vs m. Segmentation: 10_16_s.binaryTime vs m. Segmentation: 7_29_s.binary(230, 2, 15, 52), i=4(234, 2, 15, 63), i=1210000880006Time, secTime, sec100.0Nodes vs m. Protein: pdb1b2v(122, 81, 8, 25), i=41e710.0Number solutions60004400020002001.010.0Number solutionsm-AOBF-treem-AOBF-graphm-A*-treem-BB-tree100.01.0m-AOBB-treeBE+m-BF10.0Number solutionsm-AOBF-treem-AOBF-graphm-A*-treem-BB-tree100.0m-AOBB-treeBE+m-BFFigure 17: CPU time seconds number expanded nodes function number solutions. Protein Segmentation, 4 GB, 3 hours.944fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSalgorithmsolvedm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeBE+m-BFSegmentation: # inst=47, n=222-234k=2-21, w =15-18, hT =47-67, i-bound=12m=1m=2m=5m=10m=100#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN23232323230/50/00/00/00/00/50/70 / 110 / 150 / 1415 / 011 / 012 / 011 / 03/07/05/03/02/00/00/03/00/00/00/021 / 1920 / 1722 / 1324 / 924 / 10Table 13: Number instances, algorithm best runtime (#BT) best number expanded nodes (#BN), Segmentation. 47 instances 0 exact heuristics.table accounts remaining 47, i-bound= 12 .BenchmarkRandom treesRandom Binary GridsRandom submodular graphs# inst122412n10-599416-319216-3192k2-422w16-794-74hT5-1329-2219-208Table 14: Benchmark parameters: # inst - number instances, n - number variables, k - domainsize, w - induced width, hT - pseudo tree height.STILARS Nilssons schemes always dominated two competing schemesterms runtime. STRIPES PESTEELARS sometimes faster schemesm=1, e.g. tree nnodes880 ps1 k4, however, three benchmark scale rather poorlym. 5 almost always inferior algorithms, provided latter reportresults, occasional exception m-AOBB-tree, also tends slow large m.problems PESTEELARS STRIPES superior search schemeslargest networks 1000 variables, grid nnodes3192 ps2 k2,infeasible algorithms. Overall, five m-best algorithms proved superiorityconsidered competing schemes majority instances, often better runtime, especially> 2, guaranteeing solution optimality.8. Conclusionwork finding best solutions graphical models focused either iterativeschemes based Lawlers idea dynamic programming (e.g., variable-elimination treeclustering). showed first time combinatorial optimization defined graphicalmodels traditional heuristic search paradigms directly applicable, often superior.Specifically, extended best-first depth-first branch bound search algorithms solvem-best optimization task, presenting m-A* m-BB, respectively. showed properties A* extend m-A* algorithm and, particular, proved m-A* superior945fiF LEROVA , ARINESCU , & ECHTERinstancetree nnodes245 ps1 k2(245, 2, 2, 32)tree nnodes880 ps1 k4(880, 4, 2, 52)tree nnodes5994 ps1 k4(5994, 4, 2, 189)algorithmi-bound=4. k=2m=5m=10timetime0.050.090.080.130.010.020.020.030.0614.147.9333.30.40.880.511.32m-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeSTILARSSTRIPESPESTEELARSm=1time0.020.020.00.00.020.00.090.0m=2time0.020.030.00.00.020.040.170.13m=100time0.610.950.120.373045.251757.4113.8847.32m-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeSTILARSSTRIPESPESTEELARS0.30.480.080.11.170.05.670.00.460.760.170.31.370.1111.260.871.061.80.240.5452.3628.1928.096.131.93.280.481.23927.1281.2156.419.26OOMOOM3.6714.38Timeout2440.22607.0179.0m-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeSTILARSSTRIPESPESTEELARSOOMOOM5.445.77851.480.05248.530.05OOMOOM10.6829.04922.192.72506.2518.28OOMOOM18.3136.49Timeout64.481279.8791.17OOMOOM37.2197.73Timeout250.362576.87169.39OOMOOM206.261112.2Timeout7325.4TimeoutTimeoutTable 15: Random trees, i-bound=4. Timeout - time, OOM - memory. 3 GB, 1 hour.search scheme m-best task. also analyzed overhead algorithms causedneed find multiple solutions. introduced BE+m-BF, hybrid variable eliminationbest-first search scheme showed best worst-case time complexity amongm-best algorithms graphical models known us.evaluated schemes empirically. observed AND/OR decompositionsearch space, significantly boosts performance traditional heuristic search schemes,cost-effective m-best search algorithms, least current implementation.expected, best-first schemes dominate branch bound algorithms whenever sufficientspace available, fail memory-intensive problems. compared schemes 4previously developed algorithms: three approximate schemes based LP-relaxation problem algorithm performing message passing junction tree. showed schemesoften dominate competing schemes, known efficient, terms runtime, especiallyrequired number solutions large. Moreover, scheme guarantee solution optimality.Acknowledgementwork sponsored part NSF grants IIS-1065618 IIS-1254071, UnitedStates Air Force Contract No. FA8750-14-C-0011 DARPA PPAML program.946fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSinstancegrid nnodes380 ps1 k2(380, 2, 25, 379)grid nnodes380 ps2 k2(380, 2, 25, 61)grid nnodes3192 ps2 k2(3192, 2, 75, 217)algorithmm=2timeRandom binary gridi-bound=20m=5m=10timetimem=25timem-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeNilssonSTRIPESPESTEELARSOOMOOM0.490.5655.93112.55.064.63OOMOOM0.530.64106.34772.4946.5713.4OOMOOM0.580.71202.651860.46172.9528.95OOMOOM0.670.912027.665026.68361.0475.28m-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeNilssonSTRIPESPESTEELARSOOMOOM0.20.327.62110.42.233.98OOMOOM0.230.3612.82757.1419.4111.5OOMOOM0.260.5867.591820.4538.5424.34OOMOOM0.360.951964.184985.0TimeoutTimeoutm-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeNilssonSTRIPESPESTEELARSOOMOOMOOMTimeoutTimeoutOOM123.4526.86OOMOOMOOMTimeoutTimeoutOOM658.0581.27OOMOOMOOMTimeoutTimeoutOOM3035.29172.35OOMOOMOOMTimeoutTimeoutOOMTimeoutTimeoutTable 16: Random binary grids, i-bound=20. Timeout - time, OOM - memory. 3 GB,1 hour.947fiF LEROVA , ARINESCU , & ECHTERinstancegen nnodes132 ps1 k2(132, 2, 13, 34)gen nnodes380 ps1 k2(380, 2, 25, 61)gen nnodes1122 ps1 k2(1122, 2, 43, 112)algorithmi-bound=20m=5m=10timetime0.020.030.030.060.010.020.00.030.095.4460.81144.931.323.138.5218.56m=25time0.050.090.020.05120.67394.2613.2448.76m-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeNilssonSTRIPESPESTEELARSm=2time0.010.010.00.00.039.340.52.9m-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeNilssonSTRIPESPESTEELARSOOMOOM0.470.5451.77105.582.074.38OOMOOM0.510.61110.96728.06.214.09OOMOOM0.570.73141.681753.9813.2129.96OOMOOM0.721.032027.054817.0976.075.04m-AOBF treem-AOBF graphm-A* treem-BB treem-AOBB treeNilssonSTRIPESPESTEELARSOOMOOMOOMTimeoutTimeoutOOM16.469.69OOMOOMOOMTimeoutTimeoutOOM57.9628.84OOMOOMOOMTimeoutTimeoutOOM107.7361.04OOMOOMOOMTimeoutTimeoutOOM282.4158.7Table 17: Random loopy graphs submodular potentials, i-bound=20. Timeout - time,OOM - memory. 3 GB, 1 hour.948fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSFigure 18: Example AND/OR search tree 3 layers nodes.Appendix A. Proof Theorem 13Let ST AND/OR search tree relative pseudo tree depth h, n numbervariables, k maximum domain size, deg maximum degree nodes .Define partial solution subtree 0 subtree ST that: (1) 0 contains rootST ; (2) non-terminal node n 0 , 0 contains exactly one child node n0n; (3) non-terminal node n 0 0 contains child nodes n01 , . . . , n0j n;(4) leaf tip node 0 doesnt successors 0 .nodes ST grouped layers. h layers ith layer, denotedLi , 1 h, contains nodes whose variables depth , togetherchildren. assume root depth 1. illustration, Figure 18 depictsAND/OR search tree 3 layers, example L1 = {A, hA, 0i, hA, 1i}.denote TiOR set partial solution subtrees whose leaf nodes nodes Li .Similarly, TiAN set partial solution subtrees whose leaf nodes nodes Li .partial solution subtree 0 T2OR whose leaf nodes nodes belonging 2nd layerhighlighted Figure 18, namely 0 = {A, hA, 0i, B, C}.L EMMA 1. Given 0 TiOR 00 TiAN 00 extension 0 , 0 00number leaf nodes.Proof. Let number leaf nodes 0 . definition, nodesextended exactly one child node 00 . follows 00 also leaf nodes.L EMMA 2. Given 0 TiOR , number leaf nodes 0 , denoted mi , deg i1 .Proof. show induction mi = deg i1 . = 1 m1 = 1. Assume = p 1,. first extend 0 00 . Lemma 1, 00 0mp1 = deg p2 , let 0 Tp1p1number leaf nodes, namely mp1 . Next, extend 00 000 TpOR . Sincemp1 leaf nodes 00 deg child nodes 000 , follows mp ,number leaf nodes 000 mp = mp1 deg = deg p2 deg = deg p1 .Proof Theorem 13 Consider number partial solution subtrees N contained ST :949fiF LEROVA , ARINESCU , & ECHTERN=hX(NiOR + NiAN )(3)i=1NiOR= |TiOR | NiAN = |TiAN |, respectively.0, easy see 0 extended single partial solution subtreeGiven Ti100Ti leaf nodes 0 deg child nodes 00 . Therefore:NiOR = Ni1(4)Given 0 TiOR , 0 extended k partial solution subtrees 00 TiANleaf nodes 0 exactly one child node 00 kbounds domain size. Lemmas 1 2, that:NiAN = NiOR k degi1(5)Using Equations 4 5, well N1OR = 1, rewrite Equation 3 follows:N = (1 + k)+ (k + k deg+1 )+ (k deg+1 + k deg2 +deg+1)(6)+ ...+ (k degO(kh2 +deg h3 +...+1deg h 1deg1+ k degh1 +deg h2 +...+1))Thus, worst-case number partial solution subtrees need stored OPENh1h1N O(k deg ). Therefore, time space complexity m-AOBF follows O(k deg ).pseudo tree balanced, namely internal node exactly deg child nodes,time space complexity bound O(k n ), since n O(deg h1 ).ReferencesAljazzar, H., & Leue, S. (2011). K : heuristic search algorithm finding k shortest paths.Artificial Intelligence, 175(18), 21292154.Batra, D. (2012). efficient message-passing algorithm M-best MAP problem. UncertaintyArtificial Intelligence.Charniak, E., & Shimony, S. (1994). Cost-based abduction MAP explanation. Artificial Intelligence, 66(2), 345374.Darwiche, A. (2001). Decomposable negation normal form. Journal ACM (JACM), 48(4),608647.Darwiche, A., Dechter, R., Choi, A., Gogate, V., & Otten, L. (2008).Results probablistic inference evaluation UAI08, web-reporthttp://graphmod.ics.uci.edu/uai08/Evaluation/Report.In: Uncertainty Artificial Intelligence applications workshop.950fiS EARCHING B EST OLUTIONS G RAPHICAL ODELSde Campos, L. M., Gamez, J. A., & Moral, S. (1999). Partial abductive inference bayesian beliefnetworks using genetic algorithm. Pattern Recognition Letters, 20(11), 12111217.de Campos, L. M., Gamez, J. A., & Moral, S. (2004). Partial abductive inference bayesian networks using probability trees. Enterprise Information Systems V, pp. 146154. Springer.Dechter, R. (1999). Bucket elimination: unifying framework reasoning. Artificial Intelligence,113(1), 4185.Dechter, R., & Mateescu, R. (2007). AND/OR search spaces graphical models. Artificial Intelligence, 171(2-3), 73106.Dechter, R., & Rish, I. (2003). Mini-buckets: general scheme bounded inference. JournalACM, 50(2), 107153.Dechter, R. (2013). Reasoning probabilistic deterministic graphical models: Exact algorithms. Synthesis Lectures Artificial Intelligence Machine Learning, 7(3), 1191.Dechter, R., & Pearl, J. (1985). Generalized best-first search strategies optimality A*.Journal ACM (JACM), 32(3), 505536.Dijkstra, E. W. (1959). note two problems connexion graphs. Numerische mathematik,1(1), 269271.Elliott, P. (2007). Extracting K Best Solutions Valued And-Or Acyclic Graph. Mastersthesis, Massachusetts Institute Technology.Eppstein, D. (1994). Finding k shortest paths. Proceedings 35th Symposium Foundations Computer Science, pp. 154165. IEEE Comput. Soc. Press.Fishelson, M., & Geiger, D. (2002). Exact genetic linkage computations general pedigrees.International Conference Intelligent Systems Molecular Biology (ISMB), pp. 189198.Fishelson, M. a., Dovgolevsky, N., & Geiger, D. (2005). Maximum likelihood haplotypinggeneral pedigrees. Human Heredity, 59(1), 4160.Flerova, N., Dechter, R., & Rollon, E. (2011). Bucket mini-bucket schemes best solutionsgraphical models. Graph structures knowledge representation reasoningworkshop.Fromer, M., & Globerson, A. (2009). lp view m-best map problem. Advances NeuralInformation Processing Systems, 22, 567575.Ghosh, P., Sharma, A., Chakrabarti, P., & Dasgupta, P. (2012). Algorithms generating orderedsolutions explicit AND/OR structures. Journal Artificial Intelligence (JAIR), 44(1),275333.Gogate, V. G. (2009). Sampling Algorithms Probabilistic Graphical Models DeterminismDISSERTATION. Ph.D. thesis, University California, Irvine.Hamacher, H., & Queyranne, M. (1985). K best solutions combinatorial optimization problems.Annals Operations Research, 4(1), 123143.Ihler, A. T., Flerova, N., Dechter, R., & Otten, L. (2012). Join-graph based cost-shifting schemes.arXiv preprint arXiv:1210.4878.Kask, K., & Dechter, R. (1999a). Branch bound mini-bucket heuristics. IJCAI, Vol. 99,pp. 426433.951fiF LEROVA , ARINESCU , & ECHTERKask, K., & Dechter, R. (1999b). Mini-bucket heuristics improved search. ProceedingsFifteenth conference Uncertainty artificial intelligence, pp. 314323. MorganKaufmann Publishers Inc.Kjrulff, U. (1990). Triangulation graphsalgorithms giving small total state space. Tech. ReportR-90-09.Lawler, E. (1972). procedure computing k best solutions discrete optimization problemsapplication shortest path problem. Management Science, 18(7), 401405.Marinescu, R., & Dechter, R. (2009a). AND/OR Branch-and-Bound search combinatorial optimization graphical models. Artificial Intelligence, 173(16-17), 14571491.Marinescu, R., & Dechter, R. (2009b). Memory intensive AND/OR search combinatorial optimization graphical models. Artificial Intelligence, 173(16-17), 14921524.Marinescu, R., & Dechter, R. (2005). AND/OR branch-and-bound graphical models. International Joint Conference Artificial Intelligence, Vol. 19, p. 224. Lawrence ErlbaumAssociates Ltd.Nillson, N. J. (1980). Principles Artificial Intelligence. Tioga, Palo Alto, CA.Nilsson, D. (1998). efficient algorithm finding probable configurations probabilistic expert systems. Statistics Computing, 8(2), 159173.Nilsson, N. (1982). Principles artificial intelligence. Springer Verlag.Otten, L., & Dechter, R. (2011). Anytime AND/OR depth first search combinatorial optimization. SOCS.Pearl, J. (1984). Heuristics: Intelligent Search Strategies. Addison-Wesley.Schiex, T. (2000). Arc consistency soft constraints. International Conference PrinciplesPractice Constraint Programming (CP), 411424.Seroussi, B., & Golmard, J. (1994). algorithm directly finding K probable configurations Bayesian networks. International Journal Approximate Reasoning, 11(3), 205233.Wainwright, M. J., & Jordan, M. I. (2003). Variational inference graphical models: viewmarginal polytope. Proceedings Annual Allerton congerence communicationcontrol computing, Vol. 41, pp. 961971. Citeseer.Wemmenhove, B., Mooij, J. M., Wiegerinck, W., Leisink, M., Kappen, H. J., & Neijt, J. P. (2007).Inference promedas medical expert system. Artificial intelligence medicine, pp.456460. Springer.Yanover, C., & Weiss, Y. (2004). Finding Probable Configurations Using Loopy BeliefPropagation. Advances Neural Information Processing Systems 16. MIT Press.Yanover, C., Schueler-Furman, O., & Weiss, Y. (2008). Minimizing learning energy functionsside-chain prediction. Journal Computational Biology, 15(7), 899911.952fiJournal Artificial Intelligence Research 55 (2016) 165-208Submitted 03/15; published 01/16Effectiveness Automatic TranslationsCross-Lingual Ontology MappingMamoun Abu Heloumamoun.abuhelou@disco.unimib.itDepartment Informatics,Systems CommunicationUniversity Milan-BicoccaMatteo Palmonarimatteo.palmonari@disco.unimib.itDepartment Informatics,Systems CommunicationUniversity Milan-BicoccaMustafa Jarrarmjarrar@birzeit.eduDepartment Computer ScienceBirzeit UniversityAbstractAccessing integrating data lexicalized different languages challenge. Multilingual lexical resources play fundamental role reducing language barriers mapconcepts lexicalized different languages. paper present large-scale studyeffectiveness automatic translations support two key cross-lingual ontologymapping tasks: retrieval candidate matches selection correct matchesinclusion final alignment. conduct experiments using four different largegold standards, one consisting pair mapped wordnets, cover four differentfamilies languages. categorize concepts based lexicalization (type words,synonym richness, position subconcept graph) analyze distributionsgold standards. Leveraging categorization, measure several aspects translationeffectiveness, word-translation correctness, word sense coverage, synset synonym coverage. Finally, thoroughly discuss several findings study,believe helpful design sophisticated cross-lingual mapping algorithms.1. IntroductionDifferent ontology representation models proposed ease data exchangeintegration across applications. Axiomatic ontologies represented logic-based languages, e.g., OWL (2004), define concepts means logical axioms. Lexical ontologies define meaning concepts taking account words usedexpress (Hirst, 2004): concept defined one synonym words (Miller,1995), refer lexicalization concept, connected conceptssemantic relations. Several hybridizations two approaches also proposed (Vossen et al., 2010).data sources using different ontologies integrated, mappingsconcepts described ontologies established. task also calledontology mapping. Automatic ontology mapping methods introduced ease taskfinding potential mappings determining ones included finalalignment. Ontology mapping methods perform two main sub tasks: candidate matchc2016AI Access Foundation. rights reserved.fiAbu Helou, Palmonari, & Jarrarretrieval, first set potential matches found; mapping selection, subsetpotential matches included final alignment.aspects concept modelling common lexical logical ontologies, despitedifferences: concepts lexicalization organized subconcept graphs.Synonymful lexicalizations, i.e., lexicalizations contain one synonym words,frequently found lexical ontologies axiomatic ontologies. However,enriching lexicalization concepts axiomatic ontologies set synonymswell-established practice ontology mapping (Sorrentino, Bergamaschi, Gawinecki, & Po,2010; Shvaiko & Euzenat, 2013; Faria, Martins, Nanavaty, Taheri, Pesquita, Santos, Cruz,& Couto, 2014).Cross-lingual ontology mapping task establishing mappings conceptssource ontology lexicalized language concepts target ontology lexicalized different language (Spohr, Hollink, & Cimiano, 2011). considermillion datasets published online linked open data 24 different languages (LOGD, 2015), cross-lingual ontology mapping currently considered importantchallenge (Gracia, Montiel-Ponsoda, Cimiano, Gomez-Perez, Buitelaar, & McCrae, 2012).instance, COMSODE project (2015), several tables lexicalized different languages published RDF (2014) annotated using domain ontologies.Data publishers would like annotate data using ontologies lexicalized native language well English. Annotations native language publishersfacilitate access local citizens, annotations English support integration data published different countries large amount data publishedEnglish. cross-lingual ontology mapping system may help facilitating bilingualdata annotation.Cross-lingual ontology mapping methods also helpful construction multilingual large lexical ontologies (De Melo & Weikum, 2009; Abu Helou, Palmonari, Jarrar, &Fellbaum, 2014). example, Arabic Ontology project (Birzeit, 2011; Jarrar, 2011,2006; Jarrar et al., 2014), kernel core concepts could extended mapping newconcepts defined synsets glosses English WordNet (Miller, 1995; Fellbaum,1998) derive novel semantic relations.cross-lingual ontology mapping methods include step conceptslexicalizations one ontology automatically translated languageontology (Pianta, Bentivogli, & Girardi, 2002; Vossen, 2004). frequently adoptedapproach obtain automatic translations use multilingual lexical resources,machine translation tools bilingual dictionaries. quality translations usedmapping method major impact performance. However, foundsystematic large-scale analysis effectiveness automatic translationscontext cross-lingual mapping missing. study presented paper aimsproviding significant contribution fill gap.study, use two multilingual lexical resources sources translations: GoogleTranslate (2015) BabelNet 1 (Navigli & Ponzetto, 2012). Google Translate machinetranslation tool frequently used cross-lingual ontology mapping (Shvaiko,Euzenat, Mao, Jimenez-Ruiz, Li, & Ngonga, 2014). Previous work suggested1. used BabelNet version 2.5. Recent versions released writing paper (BabelNet, 2012).166fiEffectiveness Automatic Translations Cross-Lingual Ontology MappingGoogle Translate performs better Web translation services context concept mapping (Al-Kabi, Hailat, Al-Shawakfa, & Alsmadi, 2013; Oliver & Climent, 2012).addition, service configured obtain reverse translations,increase number words automatically translated. BabelNet largestmultilingual knowledge resource available today. concepts derivedfusion English WordNet - largest lexical ontology - large source encyclopedic knowledge Wikipedia (2015b). Multilingual lexicalizations createdusing inter-lingual links Wikipedia, different translation strategies several bilingualdictionaries collaboratively created Web, explain detail Sections 3.35. one hand, expect translations obtained BabelNet cover largenumber words. hand, evaluating translations obtained BabelNetindirectly evaluating different sources translations, usedindividually several cross-lingual mapping approaches. Another reason choosingresources study large number languages covered Google TranslateBabelNet (respectively, 90 272), compared resources kind.study organized follows. focusing concepts lexicalizations, considerconcepts synsets, i.e., sets words equivalent meaning given context (Miller,1995). definition used classify concepts (synsets) different categories, baseddifferent characteristics: word ambiguity (e.g., monosemous vs. polysemous), numbersynonyms (e.g., synonymful vs. synonymless), position concept hierarchy (e.g.,leaves vs. intermediate concepts). Using classifications, evaluate effectivenesstranslations obtained multilingual lexical resources studying performancecross-lingual mapping tasks executed using automatic translations different categoriessynsets. first analyze coverage translations impact candidate matchretrieval task. analyze difficulty mapping selection task using baselinemapping selection method. analyses based different measures introducedevaluate translation effectiveness terms coverage correctness, basedcomparison translations considered perfect according gold standard, i.e.,set cross-lingual mappings deemed correct.gold standards, use cross-lingual mappings manually established (or validated)lexicographers four wordnets (Arabic, Italian, Slovene Spanish)English WordNet. Using gold standards based wordnets two main advantages.contain large number mapped concepts, much larger, e.g., gold standards used evaluate cross-lingual ontology mapping systems Ontology AlignmentEvaluation Initiative (OAEI) (Shvaiko et al., 2014; OAEI, 2015), leveragelexical characterization concepts different categories provide in-depth analysis. wordnets used experiments also representative different familieslanguages different ontology sizes.best knowledge, first attempt carry systematic largescale study effectiveness multilingual lexical resources sources translationscontext cross-lingual ontology mapping. previous work, resourcesmostly evaluated context specific algorithms (Fu, Brennan, & OSullivan,2012; Spohr et al., 2011), limited number gold standards, limitednumber languages. experiments lead interesting findings, discussed(numbered) list observations summarized lessons learned section. Overall,167fiAbu Helou, Palmonari, & Jarrarbelieve findings study useful definition accurateflexible mapping algorithms, based characterization concepts lexicalization.paper structured follows. Section 2, introduce preliminary definitions used rest paper. Section 3, overview related work, goaldiscussing role automatic translations cross-lingual ontology mapping relatedfields. evaluation measures multilingual lexical resources used studyobtain translations presented respectively sections 4 5. section 6, presentexperiments. Conclusions future work end paper.2. Preliminariessection introduce definitions used study, cover concept lexicalizations, cross-lingual mapping translation tasks.2.1 Lexicalization Conceptsconsider general definition ontologies, focusing lexical characterizationconcepts, relations natural language words used concepts.choice motivated observation even ontology matching systems looksemantics axiomatic ontologies, e.g., LogMap (Jimenez-Ruiz & Grau, 2011), useconcept lexicalizations retrieve candidate matches concepts source ontology.reason, borrow several definitions lexical ontologies like WordNet (Miller, 1995)use terminology throughout paper.Slightly abusing terminology (but coherently WordNet), words lexemesassociated concept. word called simple contains one token, e.g, table,called collection 2 contains tokens, e.g., tabular array.Wordnet organizes natural language words synonym sets, so-called synsets.synset represents one underlying concept, i.e., set words (synonyms) sharemeaning given context. W set words represented wordnet,synset P(W ) set words = {w1 , ..., wn }.synset contain one word (synonymless) many words (synonymful ). useconcept synset interchangeably rest paper. Depending specificcase, use two notations concepts: set notation {w1 , ..., wn } used needmake explicit reference words contained synset, symbol notationused reference needed. also use set notation w stateword w contained synset s. set words contained concept also calledlexicalization. use superscript specify natural language used conceptlexicalizations needed, i.e., wL , sL , W N L represent word, synset wordnetrespectively lexicalized language L.addition lexical relations, link individual words (e.g., synonymy, antonymy),wordnets support semantic relations, link concepts. Hypernymy hyponymy important semantic relations wordnets. defined one2. alternative name used instead collection multiword expression (MWE), frequently usedparticular literature machine translation tool evaluation (Sag, Baldwin, Bond, Copestake,& Flickinger, 2002); use collection coherent WordNet terminology used throughoutpaper.168fiEffectiveness Automatic Translations Cross-Lingual Ontology Mappinginverse one determine subconcept graph wordnets (see Section 3.1). example, synset {table, tabular array} hyponym synset {array},synset {array} hypernym synset {table, tabular array}.word polysemous, i.e., many meanings (or senses) member manysynsets. paper superscript + right-hand corner word, e.g., board+ ,indicates polysemous word. word monosemous, i.e. one meaningmember one synset. example, English WordNet3 eight sensesword table+ ; one senses means set data arranged rows columns,word tabular array synonym word. Another sense means foodmeals general, word board+ synonym word.Given set words W set synsets defined wordnet W N ,function senses : W 7 P(S) returns set synsets word belong to, definedsenses(w) = {s|w s}. define set word senses wordnet,W = {< w, > |s senses(w)}, i.e., set couples < w, >, sensew (in given context). Observe number word senses higher numbersynsets consider associations words synsets.Example 1. word table+ eight senses English WordNet, senseEn (table)= {{ table+ , tabular array },{table+ },{table+ },{mesa+ , table+ },{table+ },{board+ , table+ },{postpone, prorogue+ , hold over+ , put over+ , table+ , shelve+ , set back+ , defer+ , remit+ ,put off+ }, {table+ , tabularize, tabularise, tabulate+ }}4 .2.2 Cross-Lingual Mappingontology matching field, cross-lingual mapping defined task findingestablishing mappings concepts source ontology lexicalized language L1concepts target ontology lexicalized language L2 (Spohr et al., 2011). Mappingsrepresent different relations source target concepts. consider specificmapping relation R, source concept target concept t, output mappingtask set couples < s, >, also called alignment. cross-lingual mapping taskmapping relation R composed two main steps (or, sub tasks):candidate match retrieval: find, source concept lexicalized L1 , settarget concepts lexicalized L2 . call concepts found task candidatematches.mapping selection: given set candidate matches = {t1 , ..., tn } (lexicalizedL2 ) source concept (lexicalized L1 ), select set concepts 0that, 0 , R(s, t) holds. R(s, t) holds, say correctmatch s, < s, > correct mapping.depth analysis semantics cross-lingual mappings, referprevious work (Abu Helou et al., 2014). gold standard alignment (or, gold standardshort), denoted gs, alignment synsets (concepts) two wordnetsmappings alignment believed correct. paper, considerequivalence mappings, i.e., mappings specify source target concepts3. following use WordNet version 3.0.4. senses definitions found online http://wordnetweb.princeton.edu/perl/webwn?s=table169fiAbu Helou, Palmonari, & Jarrarequivalent meaning. often assumed cardinality equivalence mappings1:1, meaning source concept one correct match.gold standard alignment cardinality 1:1, synset source languageone equivalent synset target language. use predicate symbolindicate two synsets equivalent (express meaning) gold standard.Using synset mappings gold standard gs, define possible senses word wL1L12target language L2 , denoted sensesLgs (w ), senses L2 equivalentsenses wL1 native language L1 :L2sensesgs(wL1 ) = {sL2 | sL1 (wL1 sL1 sL1 sL2 )}(1)Observe candidate match retrieval step define upper bound mappingselection step: correct mapping selected target mappingretrieved candidate match. addition, mapping selection form disambiguationtask : correct meaning concept (the lexicalization concept), targetlanguage chosen among different possible meanings. larger number candidatematches little evidence preferring one candidate another likely makeselection problem difficult.2.3 Translation TasksTranslating words one language words another language crucial contextcross-lingual concept mapping, and, particular, candidate match retrieval step.seek clarity consider two translation tasks: translations single wordstranslations synsets. Translations based external multilingual lexical resources,e.g., machine translation tool dictionary built using multilingual lexical resources.define word-translation word wL1 target language L2 resourceL1 7 P(W L2 ) maps word w L1 sets words2D, function wT ransL: Wtarget language L2 .define synset-translation synset sL1 target language L2L1 7 P(P(W L2 )) maps synset sets2resource D, function sT ransL:sets words, output word-translation w s.synset-translation function defined follows:L2L1L1L12sT ransL(s ) = {wT ransD (w ) | w }(2)Example 2. synset-translation Italian synset {tavola+ , tabella}It+En+,It ),English given follow: sTransEn({tavola , tabella} ) = {wTransD (tavolaEnwTransD (tabella )} = {{table, board, plank, panel, diner, slab}, {table, list}}.Observe definition synset-translation function make setunion outputs every word-translation applied words synset. Instead,using Eq.2, write output synset-translation function multiset unionsets returned every word-translation. instance, Example 2, sTransEn({tavola+ , tabella}It ) = {table(2) , board(1) , plank(1) , panel(1) , diner(1) , slab(1) }, superscriptnumbers brackets indicate frequency count words translation set.Similarly, table(2) means word table appears two subsets, i.e., wordtable resulted translation two synonym words source synset,170fiEffectiveness Automatic Translations Cross-Lingual Ontology Mappingtavola tabella. way count number word-translationsproduced one word target language given synset-translation. countshelpful use results synset-translation perform mapping selectionstep. example, counts used weigh candidate matches majorityvoting approach, like one used experiments Section 6.3.2.3. Automatic Translations Cross-Lingual Mapping Taskssection review use automatically generated translations cross-lingualontology mapping related tasks enrichment multilingual knowledgeresources cross-lingual word-sense disambiguation.enrichment multilingual knowledge resources related cross-lingual ontologymapping findings study several reasons. First, multilingual knowledge resources used sources translations cross-lingual ontology mappingapproaches. Second, wordnets mapped English WordNet use goldstandards multilingual knowledge resources, mappings represent interlingual links concepts. Third, two frequently adopted approachesenrich multilingual knowledge resources based either mapping concepts lexicalizeddifferent languages translating concepts lexicalizations. Since evaluatecorrectness coverage translations ontology concepts, findings relevant alsoapproaches intend use translations ontology mapping methods enrichmultilingual knowledge resources.Cross-lingual word sense disambiguation another research field translationsused solve mapping problem, related to, also quite differentfrom, mapping tasks considered study.discussing related work different research areas discuss usageterm concept lexical axiomatic ontologies. conclude section presentingcontributions study evaluation automatically generated translationscross-lingual ontology mapping related tasks.3.1 Concepts Lexical Axiomatic OntologiesConcepts constituents thoughts (Margolis & Laurence, 2014). relation natural language thought much debated. example, maintainconcepts independent language (Fodor, 1975; Pinker, 1994) others believeconcepts require natural language exist (Carruthers, 2002; Spelke, 2003). However,natural language plays major role expressing concepts many computational knowledge representation systems proposed support natural language processing, informationretrieval data integration tasks. Ontologies among computational knowledgerepresentation systems. distinguish two different kinds ontologies.lexical ontologies, meaning concepts primarily defined relationwords used express them. example, order represent concepttable, reference object used eat meal, set words used referconcept specified. Lexical ontologies include domain thesauri, wordnets,popular English WordNet (Miller, 1995; Fellbaum, 1998). axiomaticontologies (or, logical ontologies) meaning concepts defined axioms specified171fiAbu Helou, Palmonari, & Jarrarlogical language, e.g., First Order Logic, interpreted constraintsmathematical structures support automated reasoning (Horrocks, 2008). Exampleslogical ontologies include web ontologies defined RDFS (2014) OWL, annotateddatabase schema spreadsheet also considered ontology based broaddefinition (Zhuge, Xing, & Shi, 2008; Po & Sorrentino, 2011; Mulwad, Finin, & Joshi, 2013;Zhang, 2014). example, represent afore-mentioned concept table, definepiece furniture smooth flat top usually supported onevertical legs logical language. intended interpretation concept everytable ever existed world, or, specifically, list products typetable described spreadsheet (Mulwad et al., 2013; Zhang, 2014).Many hybrid approaches also exist. example, efforts assure certain logical properties relations represented lexical ontologies found KYOTO (Vossen et al.,2010). YAGO logical ontology integrates many concepts English WordNet (Suchanek, Kasneci, & Weikum, 2008). WordNet concepts used annotate databaseschema given formal interpretation used support database integration (Sorrentino et al., 2010).matter fact, despite several differences, concepts modelled lexical, axiomatic,hybrid ontologies share two important features. First, concepts organized subconceptgraphs, i.e., hierarchies, partially ordered sets, lattices define relationsconcepts based generality. relations referred subconcept relationsaxiomatic ontologies, different relations represented lexical ontologies,e.g., hyponymy/hypernymy. Second, every ontology concepts lexical descriptionsmay include set synonym words. course, synonyms first class citizenslexical ontologies available large number concepts, availabilitylimited axiomatic ontologies. However, step enrich concept lexicalizationslogical ontologies synonyms extracted dictionaries lexical resourcesintroduced many ontology mapping approaches exploit lexical matching algorithms (Shvaiko & Euzenat, 2013; Otero-Cerdeira, Rodrguez-Martnez, & Gmez-Rodrguez,2015; Sorrentino et al., 2010; Faria et al., 2014).3.2 Translations Cross-Lingual Ontology Mappingmajority ontology mapping methods proposed literature addressedproblem mapping ontological resources lexicalized natural language, calledmono-lingual ontology mapping. Since mono-lingual matching systems cannot directly access semantic information ontologies lexicalized different natural languages (Fuet al., 2012), techniques reconcile ontologies lexicalized different natural languagesproposed (Gracia et al., 2012; Trojahn, Fu, Zamazal, & Ritze, 2014).Translation-driven approaches used overcome natural language barrierstransforming cross-lingual mapping problem mono-lingual one (Fu et al., 2012).Different multilingual lexical resources used perform translation tasks,including manual translations, machine translation tools, bilingual dictionaries builtWeb-based multilingual resources. rich classification comparison crosslingual mapping systems refer work Trojahn et al. (2014).172fiEffectiveness Automatic Translations Cross-Lingual Ontology MappingLiang Sini (2006) manually mapped English thesaurus AGROVOC (2014)Chinese thesaurus CAT (2014). mappings generated approaches likelyaccurate reliable. However, time resource consuming processspecially maintaining large complex ontologies.Machine translation tools widely adopted cross-lingual ontology mapping. Spohret al. (2011) translate ontology labels pivot language (English) using machinetranslation tool (Bing, 2016). Then, define feature vector based combinationstring-based structural-based similarity metrics learn matching function usingsupport vector machine. Like approaches based supervised machine learning algorithms, approach disadvantage requiring significant number trainingsamples well-designed features achieve good performance. Fu et al. (2012) translateontology labels using Google Translate, match translated labels combining different similarity measures. approach leverages structural informationontology concepts considering neighbours matching process. approaches proposed also apply string-based, lexical structural matchingmethods ontology labels translated machine translation tools, like Google TranslateBing (Faria et al., 2014; Jimenez-Ruiz, Grau, Xia, Solimando, Chen, Cross, Gong, Zhang,& Chennai-Thiagarajan, 2014; Djeddi & Khadir, 2014).Multilingual knowledge resources available web also exploited translate concepts labels (Hovy, Navigli, & Ponzetto, 2012). Wiktionary (2015) usedgenerate translations match English French ontologies (Lin & Krizhanovsky, 2011).First, bilingual English-French lexicon built using Wiktionary used translatelabels ontologies. Then, monolingual ontology matching system COMSused (Lin, Butters, Sandkuhl, & Ciravegna, 2010). COMS uses set string-based, lexicalstructural matching techniques find appropriate mappings. similar approachuses Wikipedia inter-lingual links retrieve candidate matches source concepts (Bouma,2010; Hertling & Paulheim, 2012). However, used alone, Wiktionary Wikipediainter-lingual links may limited coverage, particular resource-poor languages.spite efforts, cross-lingual mapping systems still perform significantly worsemono-lingual mapping systems according recent results OAEI contest (Shvaikoet al., 2014), suggest cross-lingual ontology mapping still challengingproblem (Trojahn et al., 2014). datasets used evaluate cross-lingual mappingOAEI, i.e., datasets multifarm track (Meilicke et al., 2012), consist alignmentsestablished axiomatic ontologies relatively small size specific domainconference organization. Since study want investigate translations obtaineddifferent multilingual lexical resources large scale specific domain,decided use different larger gold standards experiments.3.3 Translations Enrichment Multilingual Knowledge ResourcesSeveral multilingual wordnets (lexical ontologies) developed manually automatically translating concepts English WordNet new languages (Pianta et al., 2002;Vossen, 2004; Tufis, Cristea, & Stamou, 2004; Gonzalez-Agirre, Laparra, & Rigau, 2012;Tomaz & Fiser, 2006). expand merge models (Vossen, 2004) main approaches used development multilingual wordnets. merge model, synsets173fiAbu Helou, Palmonari, & Jarrarpre-existing resource one language (e.g., thesaurus, even unstructured lexical resource like dictionary) aligned equivalent synset English. expandmodel, English synsets translated respective languages. main advantagetwo approaches avoid expensive manual elaboration semantic hierarchynew languages. English WordNet (Fellbaum, 1998) hierarchy used referencewordnets. Moreover, ontology built following approaches alsoautomatically mapped English WordNet.several wordnets, English concepts manually translated human lexicographers using external lexical resources dictionaries, thesauri taxonomies.approach applied build example Arabic wordnet (Rodrguez et al.,2008), Italian wordnet (Pianta et al., 2002), Spanish wordnet (Gonzalez-Agirreet al., 2012) core Slovene wordnet, used experiments. However,manual approach construct ontologies aim cover natural languages lexiconsoften effort-intensive time-consuming task (De Melo & Weikum, 2009). Automaticapproaches therefore proposed reduce lexicographers workload.Parallel corpora used building wordnets languages English.basic assumption underlying methods translations words real textsoffer insights semantics (Resnik & Yarowsky, 1999). Slovene wordnetenriched using word alignments generated sentence-aligned multilingual corpus (Fiser,2007). wordnet extended using bilingual dictionaries inter-linguallinks Wikipedia. similar approach also followed building French wordnet (Sagot& Fiser, 2008). monosemous words English WordNet automatically translated using bilingual French-English dictionaries built various multilingual resources,Wikipedia inter-lingual links, Wiktionary, Wikispecies (2015), EUROVOCthesaurus (2015).Sentence-aligned parallel corpora may available pair natural languages. addition, specific tools needed perform sentence and/or word alignment across corpora, bilingual dictionaries extracted corporabiased towards domains cover. overcome limitations, Macedonianwordnet (Saveski & Trajkovski, 2010), machine translation tool used create parallel corpora. Monosemous English words directly translated using bilingualEnglish-Macedonian dictionary. polysemous words, English WordNet sense-taggedglosses (WordNet-Princeton, 2015) automatically translated Macedonian usingGoogle Translate.supervised method automatically enrich English synsets lexicalizationslanguages also proposed (De Melo & Weikum, 2012). method learns determine best translation English synsets taking account bilingual dictionaries,structural information English WordNet, corpus frequency information.approaches enrich multilingual knowledge resources proposedbuild Universal WordNet (UWN, De Melo & Weikum, 2009), WikiNet (Nastase, Strube,Boerschinger, Zirn, & Elghafari, 2010), BabelNet (Navigli & Ponzetto, 2012),integrate multilingual encyclopedic knowledge Wikipedia English WordNet.paper, focus BabelNet, largest multilingual knowledge resource today,use study build bilingual dictionaries use translation (explained174fiEffectiveness Automatic Translations Cross-Lingual Ontology MappingSection 5). comprehensive comparison amongst afore-mentioned three resourcesfound work Navigli Ponzetto (2012).BabelNet (Navigli & Ponzetto, 2012) built integrating English WordNetWikipedia. two resources mapped using unsupervised approach.result, BabelNet covers approximately 83% WordNets nominal synsets. SynsetsEnglish WordNet cover particular (but only) classes objects, e.g., University5 , Wikipedia entries cover particular (but only) named entities, e.g.,University Milano-Bicocca6,7 . Synsets English WordNet BabelNetentries enriched lexicalizations languages using variety lexical resources. first set lexicalizations languages English obtained usinginter-lingual links Wikipedia. Synsets Wikipedia entries cannot foundenriched using automatic translations English senses-tagged sentences, extractedWikipedia SemCor corpus (Miller, Leacock, Tengi, & Bunker, 1993).frequent translation given language detected included variant lexicalization language; approach named context-translation. Translationsmonosemous English words collected using Google Translate directly included expanded lexicalizations; approach named contextless-translation.Observe contextless translations based heuristics, i.e., monosemouswords correctly translated (also referred monosemous word heuristics). coreBabelNet consists lexicalizations obtained approaches, also named BabelNet synsets. Later, BabelNet synsets lexicalizations expanded multilinguallexical resources: Wiktionary, WikiData (2015), OmegaWiki (2015), several wordnetsmapped English wordnet, available Open MultilingualWordnet (OMWN, 2015; Bond & Foster, 2013).BabelNet lexicalizations (synsets) evaluated manually mapped wordnets, also use experiments gold standards. also performed manualevaluation randomly sampled set concepts. limit evaluation consistsmaking explicit sampled senses uniformly cover polysemous monosemoussenses. Otherwise distinction important evaluate different translations, also vast number translations obtained using contextless approach,based monosemous word heuristics. experiments (Section 6)specifically analyze effectiveness monosemous word heuristics contextontology mapping.observe expand model used substantially merge modelapproaches automate enrichment multilingual wordnets knowledge resources.One may attempt enrich existing wordnet via merge approach mappingunstructured weakly structured lexicon, e.g., dictionary, structured referenceontology, e.g., English WordNet. example, Arabic Ontology Project (Jarrar, 2011; Abu Helou et al., 2014), authors plan use approach extend coreontology manually created mapped English WordNet. However, mappingtask incorporated approach particularly challenging (Abu Helou, 2014): lacksemantic relations synsets unstructured lexicon makes difficult5. http://wordnetweb.princeton.edu/perl/webwn?s=university6. https://en.wikipedia.org/wiki/University Milano-Bicocca7. http://babelnet.org/synset?word=University Milano-Bicocca175fiAbu Helou, Palmonari, & Jarrardisambiguate meaning translation matching steps (Shvaiko & Euzenat, 2013; Trojahn et al., 2014). effective cross-lingual ontology mapping methodsupport application merge model large scale, thus supporting construction enrichment multilingual knowledge resources. example, recent worksuggests approach, despite difficulty task, return multilingual concept lexicalizations richer ones obtained automatically translatingconcepts labels (Abu Helou & Palmonari, 2015).3.4 Translations Cross-Lingual Word Sense DisambiguationCross-lingual ontology mapping also related Cross-lingual Word Sense Disambiguation problem (CL-WSD), studied recent past addressedSemEval 2010 2013 challenges (Els & Veronique, 2010; Lefever & Hoste, 2013).goal CL-WSD predict semantically correct translations ambiguous wordscontext (Resnik & Yarowsky, 1999).CL-WSD, lexical disambiguation task performed word translation task,called lexical substitution task (McCarthy & Navigli, 2009). Given source wordsentence (e.g., Italian word), system tries translate word differentlanguage (e.g., English). translation considered correct preserves senseword context also target language.CL-WSD systems rely parallel corpora (Gale, Church, & Yarowsky, 1992;Resnik & Yarowsky, 1999; Apidianaki, 2009), including exploit existing multilingual wordnets (Ide, Erjavec, & Tufis, 2002). However, success coveragemethods highly depends nature parallel corpora wayextracted information used select appropriate senses. Corpora knowndomain-orientated coverage, i.e., fine-grained senses different domains mightfound specific parallel corpora (Navigli, 2009). importantly, parallel corpora mayavailable language couples specific domains (Apidianaki, 2009; Saveski &Trajkovski, 2010).One fundamental difference CL-WSD task cross-lingual ontology mapping CL-WSD context always available defined sentence wordoccurs in. cross-lingual ontology mapping context defined neighbours translated concept, may limited (Mulwad et al., 2013; Zhang, 2014), mayeven available, e.g., unstructured lexicon matched structuredontology (Abu Helou et al., 2014).3.5 Scope Contribution StudyEven approaches cross-lingual ontology mapping based transformingcross-lingual mapping problem monolingual one leveraging translations obtainedmachine translations tool multilingual lexical resources (Trojahn et al., 2014),efforts dedicated systematically study effectiveness translationscross-lingual ontology mapping.Fu, Brennan, OSullivan (2009) studied limitations translation-based ontology mapping approaches, particular, extent inadequate translations introduce noise subsequent mapping step fail cover adequate number176fiEffectiveness Automatic Translations Cross-Lingual Ontology Mappingconcepts. performed two experiments, examined mappings independent,domain specific, small-scale ontologies labeled English Chinese:Semantic Web Research Communities ontology ISWC ontology. ontologieslexically enriched. Fu colleagues classified translation errorsintroduced machine translation tools three main categories. Inadequate translation,translation source concept returns word, belongs concepttarget ontology specific/generic equivalent concept; synonymictranslation, translation source concept returns word, synonymword used target ontology denote equivalent concept (but differentone used target ontology); incorrect translation, translationsimply wrong. addition, study showed translating ontology labels isolationleads poorly translated ontologies yields low-quality matching results, thus,label translations conducted within context. context characterizedsurrounding ontology concepts.Spohr et al. (2011) observed that, target ontology lexicalizedone language, convenient translate source concepts languagesmerge evidence provided translations. However, applicablemultilingual labels available target ontology, caseseveral cross-lingual mapping scenarios. addition, obtain better translations studysuggested translating source target ontologies labels pivot languageimprove, extent, quality translation. However, authors statedevidence experiments several language pairs needed supportclaim, quality machine translations depends significantly pair consideredlanguages.paper analyze effectiveness automatic translations cross-lingual concept mapping using large scale, general domain, lexically rich ontologies (wordnets).ontologies used studies cover four different families languages besides English.study effectiveness translations conducting large number experimentsaddress candidate match retrieval mapping selection steps ontologymapping process. Overall, believe none previous work cross-lingual ontologymapping provided systematic study, compared terms scale (size considered concepts), number considered languages, level detail analysis (conceptcategorization).analyses discussed paper also related studies automatictranslation strategies conducted evaluate BabelNet, one two multilinguallexical resources used source translation study. work, quantitativelyevaluate correctness coverage translation strategies used BabelNetmeans support cross-lingual mapping tasks (using mappings wordnets comparison; see Section 6.3.1). studies conducted evaluate BabelNet aimed, instead,evaluating translation strategies means enrich multilingual lexicalizationsconcepts. introduce two new measures evaluate correctness coveragetranslations obtained multilingual resources, i.e., translation correctness synonymcoverage (see Section 4.3.2). addition, coverage correctness automatic translations study evaluated considering different categories synsets (defined177fiAbu Helou, Palmonari, & Jarrar{tavolo+, tavola+} {asse+, tavola+}{tavola+}axle treeboardaxisplanktabledesk{plate+}{table+}boardplank{tavola+, tabella}tablepanellisttableItalian SynsetswTransDEn(wiIt)diner slab{board+, plank+} {table+, tabular array}English Synsets: translation.{tavola+, tabella}: mapped synsets.: synset.Figure 1: Example: Synset-translationSection 6.2). Finally, analyze effectiveness monosemous word heuristic,used several mapping systems BabelNet (contextless translation).4. Evaluation Measuresstudy, want estimate effectiveness translations obtained multilinguallexical resources (hereafter referred resources) finding candidate matches largeset concepts. also want estimate difficulty selecting one correct mappingamong set candidate matches, based information provided translations.first objective, define four measures use evaluate translationcorrectness coverage. first two measures, translation correctness wordsense coverage, used evaluate effectiveness word-translations givenword independently meaning, i.e., sense word given.two measures, synset coverage synonym coverage, used evaluateeffectiveness synset-translation given synset focusing lexicalizationsynsets target language. Word sense coverage synset coverage two measuresproposed previous work Navigli Ponzetto (2012), rewrite definitionsaccording notation introduced Section 2.3. Translation correctness synonymcoverage introduced study. facilitate definition measures firstintroduce definition perfect translations respect gold standard.measures derive several measures, e.g., averaging values across one wordnet,present results experiments. second objective, use measurestraightforwardly derived well-known Precision measure (Shvaiko & Euzenat,2013) explained, directly, Section 6.3.2.178fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping4.1 Perfect Translations Gold Standardperfect word-translation word wL1 target language L2 w.r.t goldstandard gs set every synonym words possible senses wL1 targetlanguage L2 :[nL2L1L2L1L2 L2L2L1L2wT ransgs (w ) =wi | (s sensesgs (w ) wi )(3)i=1Example 3. Figure 1 illustrates synset-translation tasks four Italian synsetsEnglish. synset mapped equivalent synset English specified goldstandard gs. translations also obtained mappings ItalianEnglish wordnets represented gs. instance, four (Italian English)synsets mappings are: {tavola+ , tabella+ } {table+ , tabular array}, {asse+ , tavola+ }{board+ , plank+ }, {tavolo+ , tavola+ } {table+ }, {tavola+ } {plate+ }. Figure 1,perfect word-translation Italian word tavola English given follow:+,It ) = {table+ , tabular array, board+ , plank+ , plate+ }En .wTransEngs (tavolaObserve perfect word-translation function returns every word every possiblesense target language, i.e., word translation perfect returns completelexicalization every possible senses input word target language. definitionmotivated scope analysis, evaluates effectiveness automatictranslations settings domain determined a-priori. individualinput word considered outside specific context, e.g., specific sentence, specialiseddomain concept hierarchy, meaning word cannot disambiguated, unlessword monosemous. Otherwise, observe domain-specific machine translationsystem, e.g., specialised financial domain, could determine correct meaning (andtranslation) word, even word considered individually, implicitinterpretation context system. Thus, consideration polysemous wordsabsence context specification, defined translation word (i.e., setwords returned source translation) perfect contains, every possibleusage word, possible lexicalizations target language. one considers wordtranslations specialized domain, he/she may need adapt definition perfectword-translation consequently.perfect synset-translation synset sL1 target language L2 w.r.t goldstandard gs defined set every synonym words synset L2 mapped sL1gs. perfect synset-translations defined follows:[nL2L2L2L2L1L2L2 L1(4)sT ransgs (s ) =wi | (wi )i=1Example 4. Figure 1, perfect synset-translation Italian synset {tavola+ ,++En .tabella} given follow: sTransEngs ({tavola , tabella} ) = {table , tabular array}4.2 Evaluation Word-Translationssection introduce translation correctness word sense coverage measures.179fiAbu Helou, Palmonari, & Jarrar4.2.1 Translation Correctnessinput word, measure evaluates extent resource returns precisecomplete translations compared perfect word translations defined gold standard, consider every possible sense word target language.define measure, need specify word returned resourcecorrect. word wL2 correct-translation word wL1 w.r.t gold standardgs, wL2 belongs set perfect word-translations wL1 w.r.t gs (denotedL1L22wT ransLgs (w )). principle captured function correctTwL1 ,D (w ) definedfollowing equation:L121wL2 {wT ransLL2gs (w )}.correctTwL1 ,D (w ) =0otherwise.Example 5. Figure 1 English words table, board, plank correcttranslations Italian word tavola, e.g., correctTtavoalIt (tableEn ) =1. Englishwords diner, panel, slab incorrect translations Italian word tavola,e.g., correctTtavoalIt (slabEn )=0.measure correctness translations returned resource word wL1translation-correctness defined Eq. 5. measure computed harmonicmean, i.e., F1 -measure, two measures: 1) Precision (Pr), defined number correcttranslations returned resource total number translations returnedD; 2) Recall (R) 8 , defined number correct translations returned translationresource total number perfect word translations. use Recall, PrecisionF1 -measure (computed standard range), normalized range [0..100].translation returned resource D, Precision set zero.Pr =|{wL2 |correctTwL1 ,D (wL2 )}|2L1|{wT ransL(w )}|100, R =|{wL2 |correctTwL1 ,D (wL2 )}|2L|{wT ransLgs (w 1 )}|100Pr R100(5)Pr + RExample 6. example shown Figure 1, correctness English translationItalian word tavola computed follows: recall R = 60.0, precision P r = 50.0,translation-correctness TransCorrectnessEn (tavolaIt ) = 55.0.L2ransCorrectnessD(wL1 ) = F1 (P r, R) 100 = 24.2.2 Word Sense Coverageinput word, measure evaluates many possible word senses targetlanguage covered least word translation (as defined Navigli & Ponzetto, 2012).translation covers sense sL2 input word wL1 different languageresource returns least one word sL2 . use binary predicate cov(x, y) stateword-translation x covers sense y. Word senses coverage tells extentpolysemy word covered translation resource. Ideally, resource effective8. remark Recall also named translation accuracy W SD literature (Navigli, 2009).180fiEffectiveness Automatic Translations Cross-Lingual Ontology Mappingtranslating word wL1 able return correct-translations every possiblesense wL1 L2 .Given word wL1 translated target language L2 resource D, wordsenses coverage wL1 defined follows:L2wsCoverageD(wL1 )L2L1L1L22| sL2 | sL2 sensesLgs (w ) cov(wT ransD (w ), ) |=2L| sL2 | sL2 sensesLgs (w 1 ) |(6)Example 7. Figure 1, polysemous Italian word tavola four senses,one mapped equivalent synset English. Using translation resourcethree four senses covered (Eq.6). instance, senses {table+ }+covered, cov(wT ransEn(tavola), {table}) = 1, sense {plate } covered,cov(wTransEn(tavola), {plate}) =0.4.3 Evaluation Synset-Translationssection introduce synset synonym coverage measures.4.3.1 Synset Coveragemeasure defined boolean function applied input synset (Navigli & Ponzetto,2012). synset sL1 covered translation, i.e., multi set union translationconstituent words, returns least one word equivalent synset targetlanguage. measure useful computed set source synsets presentedNavigli Ponzetto. example, computing percentage source synsetsmapped gold standard covered translations obtained lexical resource,evaluate number mappings discovered using translationresource.formally define synset coverage compact way, use concept perfectL12synset translation synset sL1 target language LL2 , denoted sT ransLgs (s ).L1 synset translated target language L2 resource D, synset coveragedefined follows:L1L2 sT ransL2 (sL1 ))21wL2 (wL2 sT ransLL2 L1gs(s ) wsCoverageD (s ) =(7)0otherwise.Example 8. Consider Italian equivalent English synsets depicted Figure 1. Three four Italian synsets covered translation returns leastone word equivalent English synsets. instance, mapping {tavolo+ , tavola+ }{table+ } covered, mapping {tavola+ } {plate+ } covered.4.3.2 Synonyms Coverageinput synset sL1 , measure evaluates number words sL1word-translation covers equivalent synset target language. measure tellsmany synonyms concept lexicalization covered correct translations.181fiAbu Helou, Palmonari, & JarrarGive synset sL1 equivalent synset sL2 target language; sL1 translatedusing resource D, synonym coverage sL1 defined follows:L2 L1synonymsCoverageD(s )L1L22| wL1 | wL1 sL1 cov(wT ransL(w ), ) |=|sL1 |(8)Example 9. Figure 1 Italian synsets {tavola+ , tabella}, {asse+ , tavola+ },{tavolo+ , tavola+ } full synonym words coverage (Eq.8). Whereas, synset {tavola+ }covered word covered.Synonym coverage valuable measure evaluate translations obtained lexicalresources field cross-lingual concept mapping. Consider, example, inputsynset sL1 translation resource returns many (synonym) wordsequivalent synsets sL2 target language. one hand, synonym wordsuseful increase probability finding sL2 among candidate matches sL1 .hand, synonym words used evidence selecting sL2 best matchsL1 , e.g., compared candidate matches little evidence collectedvia translation9 . Finally, observe synonym words coverage complementaryindication word senses coverage measure effectives translation resource,i.e., coverage synonym words tool disambiguate polysemy translationsreturned translation resource.Throughout paper, order quantify overall coverage measures correctness word-translation tasks across dataset (wordnet), compute Macroaverage measure (Vincent, 2013). reported coverage measures normalizedrange [0..100].5. Multilingual Lexical Resources TranslationAutomatic translations obtained using different kinds multilingual machinereadable lexical resources. selection resources depends level information encode, instance, quality (accuracy) translations provide, lexical domains cover. resources include: dictionaries, thesauri, wordnets, machinetranslation tools, Web-based collaborative multilingual knowledge resources (resourceslexical knowledge manually collaboratively generated, e.g., Wikipedia).study two multilingual lexical resources used sources translations: GoogleTranslate BabelNet. Google Translate statistical machine translation tool. Different machine translation systems exist could used; instance, rule-based systems,e.g., Apertium (Apertium, 2015), statistical-based systems, e.g., UPC (Marioo et al.,2006). used Google Translate previous work suggested performs betterWeb translation services context concept mapping (Al-Kabi et al., 2013;Oliver & Climent, 2012), adopted several matching systems includingones evaluated OAEI (Shvaiko et al., 2014). Moreover, Google Translate genericstatistical machine translation, domain-independent system, covers large numberlanguages, including ones considered study. common evaluation measure9. intuition used, example, cross-lingual similarity measure proposed supportmatching lexical ontologies lexicalized different languages (Abu Helou & Palmonari, 2015).182fiEffectiveness Automatic Translations Cross-Lingual Ontology MappingFigure 2: Google Translate response Italian word tavola translated Englishmachine translation quality BLEU (Bilingual Evaluation Understudy) (Papineni,Roukos, Ward, & Zhu, 2002), based n-gram precision model. Thus,measure fit context word-to-word translation, case considering. comparison different machine translation tools scope study.rich comprehensive comparison different machine translation tools referwork Costa-jussa, Farrus, Marino, Fonollosa (2012).BabelNet arguably largest state-of-the-art multilingual knowledge resource. BabelNet integrated several Web-based collaborative multilingual knowledge resources(see Section 3.3). addition, makes different translation strategies available,want evaluate indirectly study: sense-tagged sentence translation, direct machinetranslation monosemous words, translations obtained Wikipedia Wordnetmappings.used Google Translate BabelNet construct bilingual dictionaries everypairs non-English English languages considered study (see Section 6.2).Google Translate service accessible API used translatesentence-to-sentence word-to-word many pairs languages. Figure 2 shows Googlesword-to-word translation response JSON (2015) format Italian word tavolatranslated English. Google returns preferable (common) translation transitem. list possible translations also given dict item, part-of-speech(P oS) tagged. translation word dict item reverse translation setscore. reverse translation set potential synonym words input word.score estimates translation usage (e.g., common, uncommon, rare translations).translation directions (e.g., It-to-En, En-to-It) machine translation toolssaid different performance applied cross-lingual information retrievaltasks (McCarley, 1999). ensure largest possible coverage compiled three bilingualdictionaries using Google Translate taking account translation direction.also collect translations provided reverse translation sets. bestknowledge available matching systems consider translations returned transitem. pair non-English English languages considered gold standardbuild following bilingual dictionaries: f romEn uses translations collectedEnglish non-English words; toEn uses translations collected nonEnglish English words; merges translations collected two dictionariesensure largest possible coverage (with Google Translate). Observe f romEntoEn subsets .183fiAbu Helou, Palmonari, & JarrarTable 1: Translation settingsBilingual Dictionaryf romEntoEnMTBNBNcore&BNcore&BNDescriptiontranslations English non-English words using Google Translatetranslations non-English English words using Google Translateunion f romEn toEntranslations encoded BabelNet except translation Open Multilingual WordNetBabelNet core synsets translationsunion BNcoreunion BNBabelNet structured graph nodes. Nodes, called BabelNet synsets, representconcepts named entities, lexicalized several languages. instance,Italian lexicalizations node represent Italian synset, represents equivalentsynset corresponding English lexicalization, synset English WordNet.translation given word source language (e.g., It) target language (e.g.,En) BabelNet given every word target language, localizesnodes lexicalized input word. example, Italian word tavolalexicalization 15 nodes10 (14 concepts, 1 named entity). nodes provide 25possible translations (lexicalization) English: {board, correlation table, place setting,plank, setting, table, tablet, gang plank, wood plank, plate, table setting, stretcher bar,Panel, Panel cartoon, Oil panel, ..., etc}. word lexicalization may deriveone many different lexical resources integrated BabelNet.analyze impact different lexical resources integrated BabelNet,extracted, every pair non-English English languages used study (seeSection 6.2), two bilingual dictionaries BabelNet synsets. first dictionaryextracted BabelNet core synsets (called BNcore ), contain multilingual lexicalizations built from: sense-tagged sentences, monosemous word translation using GoogleTranslate (monosemous words heuristic), Wikipedia inter-lingual links. second dictionary extracted BabelNet synsets (called BN ), synsets BabelNet, containmultilingual lexicalizations built from: BNcore , lexicalization obtained WikiData,Wikitionary, OmegaWiki, Wikipedia redirection links. Observe BNcore subset BN . excluded translations obtained Open Multilingual WordNet(Bond & Foster, 2013), adopt gold standards study.also merged translations BNcore dictionaries (called &BNcore ),translations BN dictionaries (called &BN ). waycompare evaluate impact different Web-based multilingual resources, BabelNetcore synsets, machine translation tools cross-lingual mapping tasks.bilingual dictionaries use study summarized Table 1.6. ExperimentsThree experiments conducted study coverage, correctness, impact twomultilingual lexical resources used sources translation mapping conceptslexicalized different languages. Four non-English wordnets, mappedEnglish WordNet, used gold standards.10. http://babelnet.org/search?word=tavola&lang=IT184fiEffectiveness Automatic Translations Cross-Lingual Ontology MappingTable 2: Size wordnets (gold standards) used experimentsWordsWord sensesSynsetsEnglish147306206941117659Arabic138662348110349Italian401786158833731Slovene399857094742583Spanish368805798938702First, Section 6.2 describe details wordnets profile conceptsbased lexicalizations. Then, Section 6.2, move perform experiments.organize discussion experiments follows. Section 6.3.1 evaluate coverage correctness translations obtained different lexical resourcesdiscuss impact retrieving candidate matches concept mapping tasks.Section 6.3.2, evidence collected translations used baseline mapping selectionapproach, i.e., majority voting, evaluate difficulty mapping selection task.Section 6.3.3, analyze coverage translations relation positionconcepts semantic hierarchies.Finally, Section 6.4 summarize observations draw potential futuredirections.6.1 Experimental SetupEnglish, Arabic, Italian, Slovene, Spanish wordnets imported database.wordnets database includes words, synsets, semantic relations, mappingsnon-English wordnet English Wordnet. compiled different bilingual dictionaries (Table 1) Google Translate API BabelNet described Section5. stored dictionaries database efficiently execute experiments.6.2 Mapped Wordnets Used Gold Standardsstudy use wordnets English (Miller, 1995; Fellbaum, 1998), Arabic (Rodrguezet al., 2008), Italian (Pianta et al., 2002), Slovene (Fiser, 2007) Spanish (GonzalezAgirre et al., 2012). wordnets provide high quality cross-lingual mappings containlarge inventories concepts. size terms words, word senses synsetsreported Table 211 . wordnets built using different approachescover different families languages: Germanic languages (e.g., English), Romancelanguages (e.g., Italian Spanish), Slavic languages (e.g., Slovene), Semiticlanguages (e.g., Arabic). Spanish, English, Arabic also among top five spokenlanguages world (Wikipedia, 2015a), processing gathered significantinterest research community. Italian Slovene represent two minority languages.Table 3 show distribution words wordnet disaggregated severalcategories: considering word ambiguity, distinguish Monosemous words (M ),words one sense (meaning), Polysemous words (P ), wordstwo senses. considering word complexity, distinguish Single words(S), strings (lexemes) spaces hyphens, Collection words (C), stringsconsist two simple words, connected spaces hyphens. also11. Arabic, Italian, Slovene wordnets obtained OMWN (2015), Spanish wordnetobtained MCR (2012). lexical gaps (synsets lexicalization) (Vossen, 2004) excluded.185fiAbu Helou, Palmonari, & JarrarTable 3: Word distribution gold standards category: quantity (percentage)Wordsonosemous(M )P olysemous(P )Simple(S)Collection(C)&S&CP &SP &CEnglish120433(81.8)26873(18.2)83118(56.4)64188(43.6)59021(40.1)61412(41.6)24097(16.4)2776(01.9)Arabic10025(72.3)3841(27.7)8953(64.6)4913(35.4)5361(38.5)4664(33.6)3592(26.0)249(01.8)Italian29816(74.2)10362(25.8)33133(82.5)7045(17.5)22987(57.2)6827(17.0)10146(25.3)218(00.5)Slovene28635(71.6)11350(28.4)29943(74.9)10042(25.1)19223(48.1)9412(23.5)10720(26.8)630(01.6)Spanish30106(81.6)6774(18.4)22630(61.4)14250(38.6)16212(44.0)13894(37.7)6418(17.4)356(00.9)Table 4: Synsets categoriesCategoryPOWMW&OW&M WIXP &OWP &M WSynset namewords Monosemouswords PolysemousOne-WordMany-WordsMonosemous OWMonosemous WMIXedPolysemous OWPolysemous WDefinition synsets have...monosemous wordspolysemous wordsone word (synonymless synset)two synonym words (synonymful synset)one word, also monosemous wordtwo synonym words, monosemous wordsmonosemous polysemous synonym wordsone word, also polysemous wordtwo synonym words, polysemous wordsconsider four categories derived combining word ambiguity complexitycategories. example, tourism monosemous simple word (M &S), tabulararray monosemous collection word (M &C), table+ polysmouse simpleword (P &S), break up+ polysemous collection word (P &C).Observation 1. vast majority collection words monosemous words:average 1.3% words polysemous collection words across wordnets. meansword used concept label less likely ambiguous composite wordlikely ambiguous simple word.classify synsets based ambiguity number words (respectively first second, third fourth categories synsets described upper partTable 4). combining orthogonal classifications, consider five categoriessynsets described lower part Table 4. One observe &OW&M W subsets . P &OW P &M W subsetsP , IX subsets W S. Examples synsets EnglishWordNet category shown Table 5. Table 6 describes, every wordnet,total number percentage synsets grouped category.Table 5: Synset examples categories EnglishCategory&OWExample{desk}&M WIXP &OWP &OWP &OW{tourism, touristry}{table+ , tabular array}{cocktail+ }{cocktail+ }{table+ }P &M W{board+ , table+ }Definitionpiece furniture writing surface usually drawerscompartmentsbusiness providing services touristsset data arranged rows columnsshort mixed drinkappetizer served first course mealpiece furniture smooth flat top usually supportedone vertical legsfood meals general186fiEffectiveness Automatic Translations Cross-Lingual Ontology MappingTable 6: Synset category-wise distribution gold standards: quantity (percentage)SynsetsPMWSOWSM&OWSM&MWSMIXP&OWSP&MWSEnglish57415(48.8)41568(35.3)53784(45.7)63875(54.3)33596(28.6)23819(20.2)18676(15.9)30279(25.7)11289(9.60)Arabic3381(32.7)4409(42.6)6162(59.5)4197(40.5)1995(19.3)1386(13.4)2559(24.7)2194(21.2)2215(21.4)Italian14393(42.7)14641(43.4)13644(40.4)21084(59.6)10492(31.1)3901(11.6)5691(16.9)9609(28.5)4046(12.0)Slovene17615(41.4)19609(46.0)14994(35.2)27589(64.8)14848(34.9)2767(06.5)5359(12.6)12741(29.9)6868(16.1)Spanish19020(49.1)16269(42.1)14994(38.7)27589(71.3)14120(36.5)4900(12.7)3413(08.8)12005(31.0)4264(11.0)Table 7: Examples mappings Italian English synsets categorySynsetsM&OWS{artschool}M&MWSM&OWS{scuoladarte}{radiostazione,stazioneradio}{radiostation}M&MWS{turismo} {tourism,touristry}{accoppiata,{exacta,abperbinata}fecta}MIX{minorit} {minority+, nonage}{biforcarsi,ramificarsi,diramarsi}P&OWS{forchetta}{fork+ }{stretto,vicino}P&MWS{chiudersi}{close+ ,shut+ }{inquietarsi, {care+ ,allarworry+ }marsi}{branch,fork+ ,furcate,ramify,separate}{close+ }MIXP&OWSP&MWS{tavolino,banco+ ,scrivania}{docente+ ,cattedratico,professore}{tavola+ ,tabella}{desk}{ordinario+ {full}professor}{entita+ , {entity}cosa+ }{prof,professor}{viaggiatore+{traveler,}traveller}{classe+ ,aula+ }{table+ ,tabulararray}{contribuire+{conduce,}contribute,lead+ }{cibo+ ,{repast,pasto+ ,meal+ }+mangiare}{poltrona+ ,seggiola,sedia}{segnare+ ,scalfire}{chair +}{cosa+ }{tavola+ ,tavolo+ }{score+ ,mark+ ,nock+ }{moderare+ {chair+ ,{cibo+ ,}moderate+ , vitto+ }lead+ }{thing + }{classroom,schoolroom}{table+ }{board+ ,table+ }Observation 2. Wordnets synonymless synsets (OW S) synonymfulsynsets (MWS), 58.1% synsets being, average, synonymless. Arabic,less OW W S, represents exception among considered wordnets.particular, Arabic polysemous synsets (all P ) equally distributed OWW S.gold standards exist mappings synsets every category. Examplesmappings couple categories synsets Italian English shownTable 7. percentage mapped synsets non-English wordnetsEnglish WordNet, grouped category, reported Table 8.results confirm languages cover number words noticedHirst (2004), and, hence, concepts shared different languages different waysexpress meanings (i.e., belong different lexical categories). instance, 57%Italian &OW synsets mapped monosemous synsets English (M &OW&M W S). hand, 25% Italian &OW mapped polysemoussynsets English (P &OW P &M W S). percentage monosemous non-Englishsynsets mapped polysemous English synsets ranges 10% (Slovene)30% (Arabic). percentage monosemous English synsets mappedpolysemous non-English synsets ranges 6% (Arabic) 14% (Italian). instance,&OW Italian synsets {fotografare} {azioni ordinarie} mapped {shoot+ ,snap+ , photograph+ } {common shares, common stock, ordinary shares}, respectivelyP &M W &M W English synset.187fiAbu Helou, Palmonari, & JarrarTable 8: Distribution mapping category: percentageM&OWS M&MWS MIXEnglishArabicM&OWS 32.919.25.1M&MWS 15.128.65.1MIX17.228.737.7P&OWS 27.414.821.7P&MWS 7.38.730.4SloveneM&OWS 23.425.214.2M&MWS 47.839.713.0MIX18.127.548.7P&OWS 7.14.08.4P&MWS 3.53.715.7P&OWS P&MWS M&OWS M&MWS MIXItalian5.42.336.220.910.62.51.521.234.910.315.522.617.827.238.757.329.517.910.718.419.444.26.96.322.0Spanish9.06.842.610.77.84.44.322.263.17.720.227.114.517.144.145.325.717.85.415.121.136.12.93.825.3P&OWS P&MWS9.44.622.543.020.54.12.826.829.037.48.43.319.448.520.43.11.924.225.944.9Observation 3. Synsets different languages, equivalent meaning,fall different synset categories. example, Italian monosemous synonymless synset{forchetta} mapped polysemous synomymless synset {fork+ } English.indicates monosemous word heuristic, adopted approachesconcept mapping multilingual knowledge construction, e.g., work presented NavigliPonzetto (2012), successful large number concepts fails still relevantnumber concepts. average 19.3% non-English monosemous synsets mappedEnglish polysemous synsets gold standards, average 8.9% English monosemous synsets mapped non-English polysemous synsets gold standards.details impact monosemous word heuristics provided Section 6.3.1,translation correctness analyzed.6.3 Results Discussionsection describe details three experiments presented study.6.3.1 Experiment 1: Coverage Correctness TranslationsCandidate Match Retrievalorder evaluate coverage translations obtained different lexical resources, use two measures. compute average word sense coverage acrosswords wordnet, word sense coverage defined individual word Eq.6.compute average synset coverage across synsets wordnet, synset coverage (a boolean value) defined individual word Eq. 7. values normalizedrange [0..100]. sake clarity simply refer measures word sensesynset coverage (at wordnet level).Table 9 reports, wordnet, word sense synset coverage different translation settings. Synsets higher coverage word senses translation settings.explained observation synset covered translation returnsleast one word lexicalization equivalent synset target language (seeEq.7).observe machine translations non-English English (M toEn) achievehigher word sense synset coverage machine translation English non-English(M f romEn). instance, word sense coverage toEn 5.2 (Italian) 10.9188fiEffectiveness Automatic Translations Cross-Lingual Ontology MappingTable 9: Word sense synset coverages different translation settingsTranslationBNcoreBNf romEntoEnMT&BNcore&BNArabicSensesSynsets19.937.430.851.351.369.957.976.159.277.760.879.262.580.2ItalianSensesSynsets40.062.551.772.860.281.965.483.968.187.669.889.072.289.9SloveneSensesSynsets28.844.235.952.040.260.049.667.253.872.455.874.257.575.2SpanishSensesSynsets33.944.739.849.056.167.867.077.069.479.771.581.372.381.7(Spanish) percentage points higher f romEn, synset coverage toEn2.0 (Italian) 9.2 (Spanish) percentage points higher f romEn.Observation 4. Machine translation tools perform asymmetrically: toEn achieveshigher word sense synset coverage f romEn.machine translation bilingual dictionary (M ), built unionmachine translation directions (see Section 5), performs better dictionariesbuilt considering direction alone (i.e., f romEn toEn). Wordsense coverage average 2.7 8.2 percentage points higher toEnf romEn, respectively. Synset coverage average 3.5 7.0 percentagepoints higher toEn f romEn, respectively.BNcore BN translation settings, based BabelNet, obtain lower coverage every machine translation setting wordnets. explainedlimited coverage words occur non-English wordnets Wikipedia concepts(which mostly cover named entities), incompleteness mappings used construct BabelNet (Navigli & Ponzetto, 2012). However, remarked that,several languages, BabelNet also includes lexicalizations Open MultilingualWordNet excluded study part gold standard (seeSection 6.2). means several well-known languages French, Germany,Spanish, Italian12 expect much higher translation coverage BabelNet.Still, best results obtained combining available translations, i.e.,machine translation tool BabelNet, &BN . instance, &BN word sensecoverage average 3.5 percentage points higher . &BN synset coverageaverage 2.4 percentage points higher .also observe BN achieves considerably higher coverage BNcore ,average difference word sense synset coverage 10.4 10.1 percentage pointsrespectively (BNcore subset BN - see Section 5). However, additionalcoverage lost combining BNcore BN translations: &BN wordsense coverage average 1.7 percentage points higher &BNcore ,&BN synset coverage average 0.8 percentage points higher &BNcore .Observation 5. results highlight machine translation tools achieve highercoverage BabelNet, integrates several Web-based multilingual resources (i.e.,Wikitionary, OmegaWiki, WikiData, Wikipeida redirection links). However, integrating BabelNet machine translation tools still yields significant gain coverage, mostly12. See Languages Coverage Statistics BabelNet (2012).189fiAbu Helou, Palmonari, & JarrarTable 10: Average synset coverage categorySynsetsBN35.4P58.8OW44.5MW56.1&OW 32.2&M W 40.3IX59.8P &OW 55.8P &M W 61.9ArabicMT MT&BN64.267.382.985.467.070.585.086.858.061.573.476.086.988.675.479.090.592.0BN68.669.064.181.063.881.680.771.080.8ItalianMT MT&BN86.088.480.583.079.582.293.695.283.586.192.694.594.395.883.386.493.795.1BN59.844.852.850.760.854.353.143.447.3SloveneMT MT&BN78.880.865.569.170.173.076.679.178.380.381.483.276.579.060.564.574.777.6BN34.462.544.159.032.440.165.557.975.5SpanishMT MT&BN78.179.580.283.075.878.087.789.374.776.288.089.085.887.677.180.288.890.9Table 11: Average number candidate matchesSynsetssynonymless (OW S)synonymful (M W S)Arabic48124Italian1749Slovene1121Spanish2775BNcore (Wikipedia inter-lingual links, context based translations).Table 10 reports average coverage synset category using BN, MT&BN translation settings (the settings achieving highest coverage). results showsynonymful synsets (M W S) covered synonymless synsets (OW S)every wordnet almost every translation setting. confirms intuition richerconcept lexicalizations help find least one correct translation using machine translation tools. Polysemous synsets (all P ) covered monosemous synsets(all ) Arabic Spanish, less monosemous synsets (all ) ItalianSlovene. explained distribution polysemous monosemous synsetssynonymless synonymful synsets: monosemous synsets (all )synonymless synsets, polysemous synsets (all P ) synonymful synsets.IXed synsets covered synsets, since synonymful synsets,combine monosemous polysemous words.Observation 6. Synonymful synsets (M W S) covered synonymlesssynsets (OW S) (see Table 10). However, higher coverage comes price largernumber candidate matches, thus making mapping selection task challenging(see Table 11).Observation 6 supported figures shown Table 11, reports averagenumber candidate matches synonymless vs. synonymful synsets. addition,synonymful synsets contain least one polysemous word (see Table 6 ). Thus, oneexpect sets candidate matches returned translations synonymful synsetslarger size, also noisier, translation polysemous words.in-depth analysis difficulty mapping selection task different synsetcategories provided Section 6.3.2. analysis confirm mapping selectionproblem difficult synsets contain polysemous words, representmajority synonymful synsets. time, joint translation synonym wordssupport mapping selection many synsets (e.g., synsets contain190fiEffectiveness Automatic Translations Cross-Lingual Ontology MappingTable 12: Average recall word-translation correctness categoryWordsPCM&SM&CP&SP&CWords20.253.138.113.326.712.755.025.329.3BN(63.6)(38.0)(48.3)(63.4)(63.0)(65.2)(37.7)(46.6)(50.8)PCM&SM&CP&SP&C45.242.643.746.743.748.343.722.944.5BN(66.1)(39.6)(56.3)(66.0)(65.8)(66.5)(39.2)(50.3)(58.9)ArabicMT45.1(36.9)83.3(22.8)67.0(27.3)35.1(43.9)54.8(32.6)34.0(44.9)85.2(22.3)55.4(32.0)55.7(31.0)SloveneMT63.6(47.8)73.0(30.1)67.0(41.4)64.3(45.0)62.5(49.0)66.0(45.5)75.0(30.0)38.9(31.3)66.3(42.3)MT&BN48.0(56.4)85.2(40.9)70.0(49.1)37.0(53.8)58.6(57.2)35.8(55.0)87.0(40.9)58.2(40.3)58.3(50.2)49.071.554.456.946.756.971.856.054.8BN(65.6)(44.9)(57.0)(65.8)(65.3)(66.3)(44.8)(48.6)(58.6)MT&BN66.6(60.8)75.4(33.7)69.7(49.9)67.3(60.3)65.4(60.7)69.1(61.1)77.4(33.5)41.1(39.9)69.1(52.4)28.174.848.917.437.617.077.330.636.7BN(61.6)(38.9)(51.0)(62.3)(61.1)(63.0)(38.7)(47.5)(53.1)ItalianMT65.8(47.0)89.8(31.9)73.0(41.0)67.3(47.5)65.4(46.7)66.9(47.9)90.0(31.7)80.3(39.0)72.0(42.1)SpanishMT68.4(48.1)92.1(28.4)78.8(41.0)63.1(48.4)73.2(47.6)62.8(48.6)93.0(27.9)75.6(38.8)72.8(43.5)MT&BN69.8(62.1)91.3(45.1)75.9(55.5)72.6(62.8)69.1(61.6)72.2(63.5)91.4(45.1)84.9(44.1)75.3(56.8)MT&BN70.8(56.9)93.7(41.4)81.0(53.3)65.7(53.5)75.5(59.1)65.4(53.9)94.7(41.5)76.7(40.3)75.0(53.3)polysemous words, e.g., IXed synsets), means collect evidence decidingmapping.order evaluate correctness translations obtained different resources, use two measures. compute average word-translation correctness acrosswords wordnet; word-translation correctness defined individual wordEq.5. addition, report average word-translation recall (recall, short), usingsubformula Eq.513 .Average recall word-translation correctness BN , &BN dictionaries, disaggregated word category, reported Table 12.results show word-translation correct monosemous collectionwords polysemous simple words. contrast, recall word-translationhigher polysemous words (P ) monosemous words (M ) every sourcetranslation every wordnets, exception BN dictionary Slovenewordnet. Recall word-translation also higher simple words (S) collectionwords (C) every setting. observations explained monosemous wordsusually less frequent domain-specific polysemous words. addition,collection words also monosemous words - remarked Observation 1 -,polysemous words simple words: recall correctness translationssimple words affected translation simple polysemous words. Translationspolysemous simple words return average larger word sets. sets likelycontain richer lexicalizations target language, also contain wordsbelong sense input words target language.Observation 7. translation monosemous collection words averagecorrect translation polysemous simple words, achieves lower recall.13. Word-translation correctness defined using formula based F1 -measure.191fiAbu Helou, Palmonari, & JarrarFocusing performance different sources translation, notice recallhigher BN , correctness BN higher . &BNcombines strengths dictionaries, i.e., higher recall word-translation, higher correctness BN . instance, Table 12 noticecorrectness word-translation improved 9.8 19.2 percentage pointsSpanish Arabic respectively, add translations derived BN . recallword-translation improved much 20.4 38.3 percentage points ItalianSpanish respectively, add BN translations derived . best resultsthus obtained &BN , obtain recall (correctness) scores range58.3%(50.2%) (Arabic) 75.3% (56.8%)(Italian). low recall Arabicexplained low recall translations monosemous collection words.Observation 8. combination machine translation tools Web-based multilingual resources context-based sentence translations, like ones incorporatedBabelNet, improves recall, also correctness word-translations.6.3.2 Experiment 2: Mapping Selection Difficultyone hand, translations returned given synset used evidenceselect mapping synset target language. hand, translationsmany, polysemous words synset return several candidate matches,incorrect, thus making mapping selection task difficult solve. experimentanalyzes difficulty mapping selection task performed candidate matchesretrieved translations obtained different lexical resources.experiment use translations returned machine translation tool,sake simplicity (with exception analysis synonym word coverage,also include BN ). focus because, shown previous section,higher coverage BN , widely used previous work ontology matching. addition, slight increase coverage obtained &BN ,compared , ignored particular experiment.perform analysis use greedy baseline method candidate mapping selection compare quality alignment computed method goldstandard alignment. baseline mapping selection method use majority voting topevidence collected synset translations.Mapping Selection Majority Voting. Every source synset translated usingsynset translation function defined Eq.2. output represented multi setunion returned translations. word w(i) multi set, (i) wordfrequency count, represents votes candidate matches contain w. Therefore,candidate match source synset s, contains many words returnedtranslation s, receive votes likely targetselected mapping. Candidate matches ranked votes mapping containingtop-voted match selected.happen several candidate matches receive equal number votes,results tie. case, source synset mapping selection task undecidable;contrast say mapping decidable unique candidate match receiveshighest number votes. However, tie occurs among set top-voted candidate192fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping100TopOne90100TopSetArabicSlovene90808070706060505040403030202010100ItalianSpanish0ArabicItalianSloveneSpanish(a) correct mappings found opOneopSet settingsM-MWSM-OWSMIXP-MWSP-OWS(b) correct mappings distinguishable candidate matches categoryFigure 3: Correct mappings found baseline selection strategymatches, valuable know set contains also correct mapping (w.r.t goldstandard) number candidate matches tie. fact, set top-votedcandidate matches also contains correct match, correct mapping could foundvia user interaction relatively low effort. reasons use two settingsexperiments majority voting candidate selection approach:TopOne: exists unique top-voted candidate match source synset,mapping containing match selected included alignment. tieoccurs, mapping selected.TopSet: correct mapping selected oracle set top-votedmatches (no matter cardinality) included alignment.quantify quality alignment compute (selection) correctnesspercentage correct mappings returned selection setting set coveredmappings, i.e., mappings set candidate matches contains correctmapping14 . words, TopOne setting, mapping considered correct sourcesynset, correct match synset (according gold standard)unique top-voted candidate match; TopSet setting, mapping considered correctsource synset, whenever correct match synset included settop-voted candidate matches. Observe every mapping counted correctTopOne setting, also counted correct TopSet setting.comparison performance terms correct mappings returnedopOne opSet selection settings wordnet shown Figure 3(a). averagecorrect mappings obtained opOne opSet settings 28% 50% respectively.Based performance simple baseline methods, suggest translationshelpful mapping selection, although sophisticated methods make useevidence devised. addition, number correct mappings increasedaverage 30 points case assume user select correctmapping among set top-voted matches returned mapping selection method,14. equivalent compute relative precision measure: Precision interpreted usual ontologymatching (Shvaiko & Euzenat, 2013) normalized range [0..100], evaluatedrestricted subset gold standard. restricted subset consists mappings containingsource concepts covered translations193fiAbu Helou, Palmonari, & Jarrar1009080706050403020100ArabicSloveneM-MWSM-OWSMIX1009080706050403020100ItalianSpanishP-MWSArabicSloveneM-MWSP-OWS(a)M-OWSMIXItalianSpanishP-MWSP-OWS(b)Figure 4: Percentage correct mappings synset category (a) opOne selection(b) opSet selectione.g., interactive ontology matching approach (Cruz, Loprete, Palmonari, Stroe, &Taheri, 2014; Sarasua, Simperl, & Noy, 2012). However, average cardinality setstop-voted matches (T opSet) high 49 synsets, makes difficult usersmake decision.Figure 3(b) shows, every wordnet every category source synset, percentagecorrect mappings found using opOne selection total synset decidablemappings. baseline opOne mapping selection strategy achieves remarkable performance monosemous synsets (i.e., &OW &M W S) poor performancepolysemous synsets. average, opOne selection capable select correct matchesmuch 88.2% monosemous synsets.Figure 4(a) 4(b) show, every wordnet every category target synset,percentage correct mappings found respectively opOne opSet selectionsettings. figured mappings synsets polysemous words, particularpolysemous synonymless synsets (P &OW S), much likely undecidable, i.e.,set many top-voted candidate matches found. fact, target synsetsP &OW S, mapping almost always undecidable opOne selection.Observation 8. Evidence provided machine translation tools valuable successfully decide upon correct mappings monosemous synsets, fails supportdecision polysemous synsets.Observation 9. Mappings polysemous synonymless target synsets (P &OW S)cannot successfully selected leveraging evidence translationssimple selection strategy like majority voting translations assign equal numbervotes several candidate matches.Observation 10. set top-voted candidate matches validated, e.g.,opSet selection settings, possible find correct mapping vast majoritymonosemous synsets (on average, 85%).want investigate correct mappings likely found largersmall number top-voted mappings selected (with opSet selection). end,analyze distribution correct mappings found opSet selection among topvoted candidate matches different size every wordnet. Correct mappings found194fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping40ArabicCorrect synsets (%)35ItalianSloveneSpanish30252015105ArabicItalian0TopOne2345678910Number candidate matchesSlovakianSpanishFigure 5: Percentage correct mappings vs. size top-voted candidate matchesopSet selectionTable 13: Synonym words coverage (%) synonymful synsets (MWS)TranslationBNMTMT&BNArabic51.968.971.3Italian59.868.572.6Slovene56.761.565.4Spanish61.274.477.5sets top-voted candidate matches size ranges 1 238 candidates.distribution plotted Figure 5: x-axis represents number selected top-votedcandidate matches (up size equal ten), y-axis represents percentagefound correct mappings. average, 28% correct mappings found uniquetop-voted candidate match exists, i.e., like opOne selection settings (see Figure 3(a)).instance, 4% correct mappings found sets top-voted mappingscontain four candidate matches, percentage represents absolute number317, 1455, 991, 1328 synsets Arabic, Italian, Slovene, Spanish wordnets,respectively.Observation 11. Synsets occur targets mappings found opOne selection (decidable mappings) safely filtered candidate matchessource synsets, error estimated low 0.2% removing correct match.Finally, analyze impact synonyms mapping selection task. Synonymfulsynsets (i.e., &M W S, MIX, P &M W S) likely correctly mappedopOne selection (Figure 4(a)) synonymless synsets (i.e., &OW P &OW S),even average number candidate matches greater synonymful synsetssynonymless synsets (see Table 11). results confirm synonyms helpfulretrieving candidate matches - previously observed Observation 6 - alsoselecting correct mappings: translation different words expressconcept provide evidence decide best mapping concept.Table 13 reports, every wordnet, synonym words coverage synonymful synsets(M W S) using BN , &BN dictionaries (synonym words coverage definedEq.8). best results obtained &BN , synonym words coverage ranging 65.4% (Slovene) 77.5% (Spanish). Thus, average, two synonymstranslated correctly synonymful synsets.195fiAbu Helou, Palmonari, & Jarrar50BN45MT40MT&BNMT3540MT&BN30353025252020151510105500ArabicItalianSloveneArabicSpanish(a) Percentage W synsets fully covereddifferent translation settingsItalianSloveneSpanish(b) Percentage correct mappings Wsynsets found opOne selectionFigure 6: Synonymful synsets (M W S) whose synonym words fully coveredFigure 6(a) shows percentage synonymful synsets fully covered, i.e.,synsets contain words correctly translated. average, dictionary fully covers greater percentage synonymful synsets BN , gain 18points. best results obtained &BN average gain 6 pointsrespect . Although BN dictionary limited impact overall synsets coverage(with gain 2.4 points, shown Experiment 2 ), BN improves synonym words coverage average 6 points, significant impact mapping selectionmajority voting. instance, compared , &BN dictionary improvespercentage correct mappings opOne selection synonymful synsetsfully covered 4.6 points, shown Figure 6(b). Covering synonym wordsbelonging synonymful synset, improves synsets coverage, also makesmapping selection step easier. Thus, integrating lexical resources translationadvantageous mapping selection tasks well.Observation 12. synonymful synsets, larger number synonym wordscovered translations, easier mapping selection task is.6.3.3 Experiment 3: Coverage Correctness Translations vs.Concept Specializationrecall synset covered none words equivalent synsettarget language returned translation. words, synsetcovered, correct match cannot found among set candidate matches foundtranslation. analysis helps exploration problem synset coverageinvestigating 1) impact domain specificity synset coverage, 2) possibilityimproving coverage expanding set found candidate matches synsetssimilar ones retrieved translations.investigate non covered synsets characterized extent basedspecificity, use two different methods characterize specificity: domain labelsassociated synsets WordNet Domains (Bentivogli, Forner, Magnini, & Pianta, 2004),e.g., biology, animals, on; position synsets occupy semantic hierarchies,e.g., synsets occur leaf nodes hypernym hierarchies.196fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping7060Percentage50403020100ArabicItalianSloveneSpanishFigure 7: Percentage domain specific synsets coveredconsider synset associated domain label Wordnet domainsdomain-specific, i.e., every label different Factoum (i.e., general, non specifieddomain). every wordnet, percentage domain specific synsets covereddictionary shown Figure 7. example, found that, average, 36%non covered synsets dictionary labeled Factoum. restnon covered synsets (64%) distributed different domains (with biology, animals, person, plants, geography frequent ones). findings consolidateones discussed Experiment 1 : monosemous words, often express specificconcepts, found less covered polysemous words, often expressgeneral concepts.Observation 13. Domain-specific concepts less coverage, machine translationtools, general concepts.intent, consider synsets covered translations distributed semantic hierarchy defined hypernym/hyponym relation.context, consider leaf synsets (called Lsynsets) specific synsets, intermediate synsets (called Isynsets), i.e., synsets occurring positions hierarchy,considered generic. consider subset synsets, i.e., nominalsynsets, whose hierarchical structure well-established English wordnet. particular, determine position source synset consider position equivalentsynset English WordNet, using mappings existing wordnets.Figure 8(a) reports percentage Lsynsets Isynsets every wordnet.notice wordnets leaf synsets intermediate synsets,exception Arabic wordnet. exception explained strategy usedconstruction wordnet relatively small size. constructionArabic wordnet (Rodrguez et al., 2008), based expand model paradigmintroduced EuroWordNet project (Vossen, 2004), initiated translationcore concepts English WordNet (Boyd-Graber, Osherson, & Schapire, 2006),was, thereafter, extended concepts. core concepts (over 5 thousands) oftenassumed common across different cultures languages, often intermediatesynsets.Figure 8(b) reports percentage Lsynsets Isynsets covereddictionary wordnet. average percentage nominal Lsynsets Isynsetscovered dictionary 21.1% 16.6%, respectively. Table 14 reports,every wordnet, distribution nominal Lsynsets vs. Isynsets, grouped synset197fiAbu Helou, Palmonari, & Jarrar100.030.025.0PercentagePercentage80.060.0L synsetssynsets40.020.020.0ArabicItalian15.0Slovene10.0Spanish5.00.00.0EnglishArabicItalianSloveneL synsetsSpanish(a)synsets(b)Figure 8: Percentage Leaf synsets (Lsynsets) Intermediate synsets (Isynsets) (a)gold standards (b) non-covered synsets30.0Percentage25.020.015.010.05.00.0ArabicItalianSloveneSpanishFigure 9: Neighbour synset coverage non-covered synsetcategory. notice Lsynsets likely covered Isynsets,large number non-covered synsets consists synonymless synsets.Moreover, would like evaluate if, non covered synsets, translations returncandidate matches least semantically similar equivalent synsetstarget language. Neighbor synsets (i.e., hypernyms, hyponyms, siblings) usuallyconsidered similar many wordnet graph based similarity measures (Navigli, 2009). Inspired work presented Resnik Yarowsky (1999), one could consider establishingweighted mapping synset source language synsets target language,weight represents degree similarity source targetsynset. experiment define similar synsets one either hyponym/hypernymsiblings one. shown Figure 9, average percentage synsetscovered least one synset similar equivalent synset foundamong candidate matches 20.1%. consistent intuition machinetranslation systems provide translations (implicitly) capture coarse-grainedsense specification fine-grained sense specification encoded wordnets.fact, observed WordNet sometimes fine-grained even human judgesagree (Hovy, Marcus, Palmer, Ramshaw, & Weischede, 2006).Observation 14. significant percentage non covered synsets (20.1%, average), machine translation tools return synsets least similar equivalentsynsets target language.198fiEffectiveness Automatic Translations Cross-Lingual Ontology MappingTable 14: Distribution leaf intermediate (non-)covered synsets categorySynsetsM-MWSM-OWSMIXP-MWSP-OWSSynsetsM-MWSM-OWSMIXP-MWSP-OWSArabicNon-coveredCoveredLeafInterLeafInter9.116.035.039.914.327.127.031.63.78.519.668.22.75.217.974.28.813.119.758.47.814.423.554.3SloveneNon-coveredCoveredLeafInterLeafInter10.94.760.024.313.24.670.411.911.37.138.143.513.08.328.150.622.310.235.332.215.26.752.425.7ItalianNon-CoveredCoveredLeafInterLeafInter5.31.571.621.511.44.060.324.34.12.144.649.34.02.634.459.011.35.544.139.08.63.652.135.7SpanishNon-CoveredCoveredLeafInterLeafInter8.01.182.58.417.84.264.913.19.62.448.739.37.13.328.361.314.17.037.041.813.84.256.925.0Based observation, candidate match retrieval step modifiedinclude among candidate matches also synsets similar ones retrieved translation. approach followed several cross-lingual ontology matching systems (Fu et al., 2012; Cruz, Palmonari, Caimi, & Stroe, 2013; Faria et al., 2014). However,expanding set considered candidate matches disadvantage increasingdifficulty mapping selection task. results analyses suggest expansioncandidate matches set technique could applied particular categories source synsets, e.g., synonymless leaf synsets. could provide system (oruser, interactive matching settings) greater ability map synsets lesslikely covered translations, without increasing number candidate matchesevery source synset, e.g., synsets distinguishable monosemous candidatematches (see Observation 8).6.4 Lessons Learned & Future Workssection summarize main results findings study highlightpotential future directions.general conclusion draw study machine translation toolsmultilingual knowledge resources return useful translations large numberconcepts. Thus, translations provide valuable support candidate match retrievalcross-lingual ontology matching, covering minimum 75.2% maximum 89.9%synsets four languages English considered study. considerBabelNet also incorporates translations derived mappings Open MultilingualWordnet (Bond & Foster, 2013) (which excluded studyused gold standards), coverage expected even increase severalresource-rich languages covered wordnet. addition, experiments suggesttranslations helpful, limited extent, selected categories synsets,also mapping selection task.Concisely, main results experiments suggest that:199fiAbu Helou, Palmonari, & Jarrarmonosemous concepts (i.e., concepts monosemous words) considered domain-specific;combining lexical resources improves quality results;machine translation tools perform poorer domain-specific concepts domainindependent ones;synonymful synsets higher coverage synomymless synsets;most, all, monosemous concepts mapped confidently even simpleselection methods (e.g., translation-based majority voting);mappings involving polysemous synonymless synsets harder filter withinmapping selection task;coverage synonym words (in synonymful synsets), easiermapping selection task.Compared previous systems, used machine translation tools consideringone translation direction, study built dictionaries cover translationdirections including reverse translations. technique shown significantlyimprove coverage translations. practice, candidate matches foundlarger number input concepts, thus increasing upper-bound recall cross-lingualontology matching systems. promising future research direction, one may tryimprove coverage considering additional information available machine translationtools like Google Translate (e.g., reverse translation synonym-like sets, part-of-speech taggedtranslations, translation scores). additional information increaseupper-bound recall, also precision, adequately used matching selection step.example, one may compare words returned reverse translations inputsource synset, e.g., using translation-correctness measure (Eq.5). translationhigher translation-correctness could given higher weight selection step.selection correct mapping set candidate matches still remains difficulttask, particular contextual knowledge cannot used disambiguate meaningconcepts. However, findings paper suggest several research directionsmitigate problem.one hand, simple baseline selection method based majority voting usedexperiments overcome sophisticated methods. example, recentwork, define lexical similarity measure based evidence collected translationsrun local similarity optimization algorithm improve assignmentssource target concepts (Abu Helou & Palmonari, 2015). future work, wouldlike leverage analysis mapping selection difficulty dependent lexicalcharacterisation source target concepts (e.g., polysemous vs. monosemous concepts,synonymless vs. synonymful synsets) discussed paper. plan investigatematching algorithms could adapt behavior based category sourcesynset candidate matches.hand, cross-lingual mappings may still hard decide upon usingfully automatic approach. Thus, would like investigate cross-lingual ontology200fiEffectiveness Automatic Translations Cross-Lingual Ontology Mappingmatching domain, adoption semi-automatic matching methods. web applicationcould used solve difficult cross-lingual matching tasks, one proposed matchshort service descriptions different languages (Narducci, Palmonari, & Semeraro, 2013).Beyond this, interactive matching processes aggregate inputs given multiplicityusers, either experts (Cruz et al., 2014) crowd workers (Sarasua et al., 2012) seemparticularly promising large cross-lingual matching tasks. findings paperparticularly useful similar approaches help decide mappingsuser inputs valuable (e.g., polysemous synonymless concepts). Overallplan follow latter research directions use map model ease constructionlexical-semantic ontology context Arabic Ontology Project (Abu Helouet al., 2014), also motivated study presented paper.7. Conclusionsstudy investigated effectiveness automatic translations derivedstate-of-the-art machine translation tool (Google Translate) state-of-the-art multilingual knowledge resource (BabelNet) support cross-lingual ontology mapping. performanalysis used four large repositories cross-lingual mappings, includemappings wordnets four different languages English WordNet. Effectivenessautomatic translations analyzed terms coverage correctness. One key contribution study, besides scale experiments, analysis effectivenessautomatic translations specific categories synsets.example, found automatic translations achieve lower coverage domainspecific concepts. another example, found amount monosemous wordscorrectly translated polysemous words another language negligible:cross-lingual ontology mapping methods use monosemous word heuristic may leadinclude several wrong mappings alignment. coarse grain, analysessuggest automatic translations capable covering large number word senses,particular multilingual lexical resources (e.g., Google Translate BabelNet)translation strategies (i.e., reverse translations Google Translate) integrated.hand, automatic translations correct limited extent, leastcompared translations derived manually mapped wordnets.analyses discussed paper inspired definition cross-lingual similaritymeasure lexical ontologies (Abu Helou & Palmonari, 2015). natural subsequent steputilize study outcomes cross-lingual mapping systems. One promisingresearch direction define adaptive mapping methods different strategiesused depending lexical characterization source concepts. example, onecould integrate interactive mapping methods crowdsourcing approaches decidesubset mappings, estimated particularly difficult map. Anotherresearch direction plan investigate method estimate concept ambiguitysmall ontologies explicitly contain synonyms, e.g., matchingwordnets. method would help us use adaptive cross-lingual mapping methodsaxiomatic ontologies lexically-poor data sources, e.g., web tables.201fiAbu Helou, Palmonari, & JarrarAcknowledgmentsauthors would like thank anonymous reviewers helpful commentsvaluable suggestions, Pikakshi Manchanda help proofreading. worksupported part COMSODE project (FP7-ICT-611358) SIERA project (FP7INCO-295006). Corresponding author: Mamoun Abu Helou, E-mail: mamoun.abuhelou@disco.unimib.it.ReferencesAbu Helou, M. (2014). Towards constructing linguistic ontologies: Mapping frameworkpreliminary experimental analysis. Proceedings Second Doctoral WorkshopArtificial Intelligence, Pisa, Italy. CEUR-WS.Abu Helou, M., & Palmonari, M. (2015). Cross-lingual lexical matching word translation local similarity optimization. Proceedings 10th InternationalConference Semantic Systems, SEMANTiCS 2015, Vienna, Austria, September.Abu Helou, M., Palmonari, M., Jarrar, M., & Fellbaum, C. (2014). Towards building linguistic ontology via cross-language matching. Proceedings 7th InternationalConference Global WordNet.AGROVOC (2014).Multilingual agricultural thesaurus.vest-registry/vocabularies/agrovoc.http://aims.fao.org/Al-Kabi, M. N., Hailat, T. M., Al-Shawakfa, E. M., & Alsmadi, I. M. (2013). Evaluating English Arabic Machine Translation Using BLEU. International Journal AdvancedComputer Science Applications(IJACSA), 4 (1).Apertium (2015). Open-source machine translation platform. http://www.apertium.org.Apidianaki, M. (2009). Data-driven semantic analysis multilingual wsd lexical selection translation. Proceedings 12th Conference European ChapterAssociation Computational Linguistics (ACL), EACL 09, pp. 7785, Stroudsburg, PA, USA. Association Computational Linguistics (ACL).BabelNet (2012). large multilingual semantic network. http://babelnet.org.Bentivogli, L., Forner, P., Magnini, B., & Pianta, E. (2004). Revising wordnet domainshierarchy: Semantics, coverage balancing. Proceedings WorkshopMultilingual Linguistic Ressources, MLR 04, pp. 101108, Stroudsburg, PA, USA.Association Computational Linguistics (ACL).Bing (2016). Bing translate. http://www.bing.com/translator.Birzeit (2011). Arabic Ontology. http://sina.birzeit.edu/ArabicOntology/.Bond, F., & Foster, R. (2013). Linking extending open multilingual wordnet.ACL (1), pp. 13521362. Association Computer Linguistics (ACL).Bouma, G. (2010). Cross-lingual ontology alignment using eurowordnet wikipedia.LREC. European Language Resources Association (ACL).202fiEffectiveness Automatic Translations Cross-Lingual Ontology MappingBoyd-Graber, J., Osherson, & Schapire, R. (2006). Adding dense, weighted connectionsWordNet connections wordnet. Proceedings Third Global WordNetMeeting.Carruthers, P. (2002). cognitive functions language. Behavioral Brain Sciences,25 (6), 657674.CAT (2014).Chinese Agricultural Thesaurus.http://www.ciard.net/partners/labof-chinese-agricultural-ontology-services.COMSODE (2015). Components supporting open data exploitation. http://www.comsode.eu/.Costa-jussa, M. R., Farrus, M., Marino, J. B., & Fonollosa, J. A. R. (2012). Studycomparison rule-based statistical catalan-spanish machine translation systems.Computing informatics, 31 (2), 245270.Cruz, I. F., Palmonari, M., Caimi, F., & Stroe, C. (2013). Building linked ontologieshigh precision using subclass mapping discovery. Artif. Intell. Rev., 40 (2), 127145.Cruz, I., Loprete, F., Palmonari, M., Stroe, C., & Taheri, A. (2014). Pay-as-you-go multiuser feedback model ontology matching. Knowledge Engineering KnowledgeManagement, Vol. 8876 Lecture Notes Computer Science, pp. 8096. Springer.De Melo, G., & Weikum, G. (2009). Towards universal wordnet learning combinedevidence. Proceedings 18th ACM Conference Information KnowledgeManagement, CIKM 2009, Hong Kong, China, November, pp. 513522. ACM.De Melo, G., & Weikum, G. (2012). Constructing utilizing wordnets using statisticalmethods. Language Resources Evaluation, 46 (2), 287311.Djeddi, W. E., & Khadir, M. T. (2014). Xmap++: Results oaei 2014. Proceedings9th International Workshop Ontology Matching co-located 13thInternational Semantic Web Conference (ISWC 2014), October, Vol. 20, pp. 163169.Els, L., & Veronique, H. (2010). Semeval-2010 task 3: Cross-lingual word sense disambiguation. Proceedings 5th International Workshop Semantic Evaluation.Association Computational Linguistics (ACL).EUROVOC (2015). EUs multilingual thesaurus. http://europa.eu/eurovoc.Faria, D., Martins, C., Nanavaty, A., Taheri, A., Pesquita, C., Santos, E., Cruz, I. F., &Couto, F. M. (2014). Agreementmakerlight results oaei 2014. Proceedings9th International Workshop Ontology Matching co-located 13thInternational Semantic Web Conference (ISWC 2014), October, Vol. 20, pp. 126134.Fellbaum, C. (Ed.). (1998). WordNet Electronic Lexical Database. MIT Press,Cambridge, MA; London.Fiser, D. (2007). Leveraging parallel corpora existing wordnets automatic construction slovene wordnet. LTC, Vol. 5603 Lecture Notes Computer Science,pp. 359368. Springer.Fodor, J. (1975). Language Thought. Cambridge, MA: Harvard University Press.203fiAbu Helou, Palmonari, & JarrarFu, B., Brennan, R., & OSullivan, D. (2009). Cross-lingual ontology mapping - investigation impact machine translation. ASWC, pp. 115.Fu, B., Brennan, R., & OSullivan, D. (2012). configurable translation-based cross-lingualontology mapping system adjust mapping outcomes. J. Web Semantic, 15, 1536.Gale, W. A., Church, K. W., & Yarowsky, D. (1992). Using bilingual materials developword sense disambiguation methods. Proceedings International ConferenceTheoretical Methodological Issues Machine Translation.Gonzalez-Agirre, A., Laparra, E., & Rigau, G. (2012). Multilingual central repositoryversion 3.0. LREC, pp. 25252529. European Language Resources Association(ELRA).Google (2015). Google Translate. https://translate.google.com/.Gracia, J., Montiel-Ponsoda, E., Cimiano, P., Gomez-Perez, A., Buitelaar, P., & McCrae,J. (2012). Challenges multilingual web data. Web Semantic, 11, 6371.Hertling, S., & Paulheim, H. (2012). Wikimatch - using wikipedia ontology matching.Ontology Matching, Vol. 946 CEUR Workshop Proceedings. CEUR-WS.org.Hirst, G. (2004). Ontology lexicon. Handbook Ontologies InformationSystems. Springer.Horrocks, I. (2008). Ontologies semantic web. Communications ACM, 51 (12),5867.Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., & Weischede, R. (2006). Ontonotes:90% solution. Proceedings Human Language Technology ConferenceNAACL. Companion Volume, Short Papers XX, NAACL 06. AssociationComputational Linguistics (ACL), Morristown, NJ, USA.Hovy, E., Navigli, R., & Ponzetto, S. P. (2012). Collaboratively built semi-structured contentartificial intelligence: story far. Artificial Intelligence.Ide, N., Erjavec, T., & Tufis, D. (2002). Sense discrimination parallel corpora. Proceedings ACL-02 Workshop Word Sense Disambiguation: Recent SuccessesFuture Directions - Volume 8, WSD 02, pp. 6166, Stroudsburg, PA, USA. Association Computational Linguistics (ACL).Jarrar, M. (2006). Position paper: Towards notion gloss, adoption linguisticresources formal ontology engineering. Proceedings 15th InternationalConference World Wide Web, pp. 497503, NY, USA. ACM.Jarrar, M. (2011). Building formal arabic ontology (invited paper). ProceedingsExperts Meeting Arabic Ontologies Semantic Networks. Alecso, Arab League.26-28 July.Tunis.Jarrar, M., Yahya, A., Salhi, A., Abu Helou, M., Sayrafi, B., Arar, M., Daher, J., Hicks, A.,Fellbaum, C., Bortoli, S., Bouquet, P., Costa, R., Roche, C., & Palmonari, M. (2014).Arabization multilingual knowledge sharing- final report research setup.SIERA Project 2.3 Deliverable.204fiEffectiveness Automatic Translations Cross-Lingual Ontology MappingJimenez-Ruiz, E., Grau, B. C., Xia, W., Solimando, A., Chen, X., Cross, V., Gong, Y.,Zhang, S., & Chennai-Thiagarajan, A. (2014). Logmap family results oaei 2014?.Proceedings 9th International Workshop Ontology Matching co-located13th International Semantic Web Conference (ISWC 2014), October, Vol. 20,pp. 126134.Jimenez-Ruiz, E., & Grau, B. C. (2011). Logmap: Logic-based scalable ontology matching. Semantic WebISWC 2011, pp. 273288. Springer.Json-W3schools (2015). Javascript object notation. http://www.w3schools.com/json/.Lefever, E., & Hoste, V. (2013). Semeval-2013 task 10: Cross-lingual word sense disambiguation. Proc. SemEval, 158166.Liang, A. C., & Sini, M. (2006). Mapping agrovoc chinese agricultural thesaurus:Definitions, tools, procedures. New Review Hypermedia Multimedia, 12 (1),5162.Lin, F., Butters, J., Sandkuhl, K., & Ciravegna, F. (2010). Context-based ontology matching: Concept application cases. 10th IEEE International Conference Computer Information Technology, CIT 2010, Bradford, West Yorkshire, UK, June29-July 1, 2010, pp. 12921298.Lin, F., & Krizhanovsky, A. (2011). Multilingual ontology matching based wiktionarydata accessible via sparql endpoint. CoRR, abs/1109.0732.LOGD (2015).Linking Open Government Data.http://logd.tw.rpi.edu/iogds-data-analytics. [Online; accessed March-2015].Margolis, E., & Laurence, S. (2014). Concepts. Zalta, E. N. (Ed.), Stanford Encyclopedia Philosophy (Spring edition).Marioo, J. B., Banchs, R. E., Crego, J. M., de Gispert, A., Lambert, P., Fonollosa, J. A. R.,& Costa-jussa, M. R. (2006). N-gram-based machine translation. Comput. Linguist.,32 (4), 527549.McCarley, J. S. (1999). translate documents queries cross-languageinformation retrieval?. Proceedings 37th Annual Meeting AssociationComputational Linguistics Computational Linguistics (ACL), ACL 99, pp.208214, Stroudsburg, PA, USA. Association Computational Linguistics (ACL).McCarthy, D., & Navigli, R. (2009). english lexical substitution task. Language Resources Evaluation, 43 (2), 139159.MCR (2012). Multilingual Central Repository. http://adimen.si.ehu.es/web/MCR.Meilicke, C., Garca-Castro, R., Freitas, F., van Hage, W. R., Montiel-Ponsoda, E.,de Azevedo, R. R., Stuckenschmidt, H., Svab-Zamazal, O., Svatek, V., Tamilin, A.,Trojahn, C., & Wang, S. (2012). Multifarm: benchmark multilingual ontologymatching. Web Semantics: Science, Services Agents World Wide Web,15 (3).Miller, G. A. (1995). Wordnet: lexical database english. Commun. ACM, 38 (11),3941.205fiAbu Helou, Palmonari, & JarrarMiller, G. A., Leacock, C., Tengi, R., & Bunker, R. T. (1993). semantic concordance.Proceedings workshop Human Language Technology, HLT 93, pp. 303308,Stroudsburg, PA, USA. Association Computational Linguistics (ACL).Mulwad, V., Finin, T., & Joshi, A. (2013). Semantic message passing generating linkeddata tables. International Semantic Web Conference (1), Vol. 8218 LectureNotes Computer Science, pp. 363378. Springer.Narducci, F., Palmonari, M., & Semeraro, G. (2013). Cross-language semantic matchingdiscovering links e-gov services lod cloud. KNOW@LOD, Vol. 992CEUR Workshop Proceedings, pp. 2132. CEUR-WS.org.Nastase, V., Strube, M., Boerschinger, B., Zirn, C., & Elghafari, A. (2010). Wikinet:large scale multi-lingual concept network. LREC. European Language ResourcesAssociation (ACL).Navigli, R. (2009). Word sense disambiguation: survey. ACM Comput. Surv., 41 (2),10:110:69.Navigli, R., & Ponzetto, S. P. (2012). Babelnet: automatic construction, evaluationapplication wide-coverage multilingual semantic network. Artificial Intelligence,193 (0), 217 250.OAEI (2015).ontology alignment evaluation initiative.ontologymatching.org.http://oaei.Oliver, A., & Climent, S. (2012). Building wordnets machine translation sense taggedcorpora. Proceedings 6th International Conference Global WordNet.Omegawiki (2015). Wikimedia. http://www.omegawiki.org.OMWN (2015). open multilingual wordnet. http://compling.hss.ntu.edu.sg/omw/.Otero-Cerdeira, L., Rodrguez-Martnez, F. J., & Gmez-Rodrguez, A. (2015). Ontologymatching: literature review. Expert Systems Applications, 42 (2), 949 971.OWL (2004). Web ontology language. http://www.w3.org/TR/owl-features/.Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). Bleu: method automaticevaluation machine translation. Proceedings 40th Annual MeetingAssociation Computational Linguistics (ACL), ACL 02, pp. 311318, Stroudsburg,PA, USA. Association Computational Linguistics (ACL).Pianta, E., Bentivogli, L., & Girardi, C. (2002). Multiwordnet: developing aligned multilingual database. Proceedings First International Conference GlobalWordNet.Pinker, S. (1994). Language Instinct. Harper Perennial Modern Classics, New York.Po, L., & Sorrentino, S. (2011). Automatic generation probabilistic relationshipsimproving schema matching. Information Systems, 36 (2), 192 208. Special Issue:Semantic Integration Data, Multimedia, Services.RDF (2014). Resource Description Framework. http://www.w3.org/RDF/.RDFS (2014). RDF Schema. http://www.w3.org/TR/rdf-schema.206fiEffectiveness Automatic Translations Cross-Lingual Ontology MappingResnik, P., & Yarowsky, D. (1999). Distinguishing systems distinguishing senses: newevaluation methods word sense disambiguation. Natural Language Engineering,5 (2), 113133.Rodrguez, H., Farwell, D., Farreres, J., Bertran, M., Mart, M. A., Black, W., Elkateb, S.,Kirk, J., Vossen, P., & Fellbaum, C. (2008). Arabic wordnet: Current state futureextensions. Proceedings Forth International Conference Global WordNet.Sag, I., Baldwin, T., Bond, F., Copestake, A., & Flickinger, D. (2002). Multiword expressions: pain neck nlp. Computational Linguistics Intelligent TextProcessing, Vol. 2276 Lecture Notes Computer Science, pp. 115. Springer.Sagot, B., & Fiser, D. (2008). Building free french wordnet multilingual resources.Proceedings Sixth International Language Resources Evaluation (LREC),Marrakech, Maroc.Sarasua, C., Simperl, E., & Noy, N. (2012). Crowdmap: Crowdsourcing ontology alignmentmicrotasks. Semantic Web ISWC 2012, Vol. 7649 Lecture NotesComputer Science, pp. 525541. Springer.Saveski, M., & Trajkovski, I. (2010). Automatic construction wordnets using machinetranslation language modeling. Proceedings Seventh Language Technologies Conference, 13th International Multiconference Information Society, volume C.Shvaiko, P., & Euzenat, J. (2013). Ontology matching: State art future challenges.IEEE Trans. Knowl. Data Eng., 25 (1), 158176.Shvaiko, P., Euzenat, J., Mao, M., Jimenez-Ruiz, E., Li, J., & Ngonga, A. (Eds.). (2014).Proceedings 9th International Workshop Ontology Matching collocated13th International Semantic Web Conference (ISWC 2014), Riva del Garda,Trentino, Italy, October 20, 2014, Vol. 1317 CEUR Workshop Proceedings. CEURWS.org.Sorrentino, S., Bergamaschi, S., Gawinecki, M., & Po, L. (2010). Schema label normalizationimproving schema matching. Data & Knowledge Engineering, 69 (12), 1254 1273.Special issue 28th International Conference Conceptual Modeling (ER 2009).Spelke, S. E. (2003). makes us smart? core knowledge natural language.Language Mind: Advances Study Language Thought, pp. 277311.Mit Press.Spohr, D., Hollink, L., & Cimiano, P. (2011). machine learning approach multilingual cross-lingual ontology matching. Proceedings 10th InternationalConference Semantic Web - Volume Part I, ISWC11, pp. 665680. Springer.Suchanek, F. M., Kasneci, G., & Weikum, G. (2008). Yago: large ontology wikipediawordnet. Web Semantics: Science, Services Agents World Wide Web,6 (3), 203217.Tomaz, E., & Fiser, D. (2006). Building slovene wordnet. Proceedings 5th International Conference Language Resources Evaluation (LREC06), Genoa,Italy.207fiAbu Helou, Palmonari, & JarrarTrojahn, C., Fu, B., Zamazal, O., & Ritze, D. (2014). State-of-the-art multilingualcross-lingual ontology matching. Towards Multilingual Semantic Web, pp. 119135. Springer.Tufis, D., Cristea, D., & Stamou, S. (2004). Balkanet: Aims, methods, results perspectives. general overview. Special Issue BalkaNet. Romanian Journal ScienceTechnology Information.Vincent, V. A. (2013). Macro- micro-averaged evaluation measures [[basic draft]].Technical report.Vossen, P. (2004). Eurowordnet: multilingual database autonomous languagespecific wordnets connected via inter-lingualindex. International Journal Lexicography, 17 (2), 161173.Vossen, P., Eneko, A., Francis, B., Wauter, B., Axel, H., Amanda, H., Shu-Kai, H., Hitoshi,I., Chu-Ren, H., Kyoko, K., Andrea, M., German, R., Francesco, R., Roxane, S., &Maurizio, T. (2010). KYOTO: Wiki Establishing Semantic InteroperabilityKnowledge Sharing Across Languages Cultures, pp. 265294. Springer.Wikidata (2015). Wikimedia. http://www.wikidata.org.Wikipedia (2015a). List languages number native speakers. http://en.wikipedia.org/wiki/List-of-languages-by-number-of-native-speakers.Wikipedia (2015b). Wikipedia. https://www.wikipedia.org.Wikispecies (2015). Wikimedia. https://species.wikimedia.org.Wiktionary (2015). Wikimedia. https://www.wiktionary.org.WordNet-Princeton (2015). Semantically tagged glosses. http://wordnet.princeton.edu/glosstag.shtml.Zhang, Z. (2014). Towards efficient effective semantic table interpretation.Semantic Web - ISWC 2014 - 13th International Semantic Web Conference, Riva delGarda, Italy, October 19-23, 2014. Proceedings, Part I, pp. 487502.Zhuge, H., Xing, Y., & Shi, P. (2008). Resource space model, owl database: Mappingintegration. ACM Transactions Internet Technology (TOIT), 8 (4), 20.208fiJournal Artificial Intelligence Research 55 (2016) 389-408Submitted 07/15; published 02/16Predicting Twitter User Demographics using DistantSupervision Website Traffic DataAron CulottaNirmal Kumar Raviaculotta@iit.edunravi@hawk.iit.eduDepartment Computer Science, Illinois Institute TechnologyChicago, IL 60616Jennifer Cutlerjcutler2@stuart.iit.eduStuart School Business, Illinois Institute TechnologyChicago, IL 60616AbstractUnderstanding demographics users online social networks important applications health, marketing, public messaging. Whereas prior approachesrely supervised learning approach, individual users labeled demographics training, instead create distantly labeled dataset collecting audiencemeasurement data 1,500 websites (e.g., 50% visitors gizmodo.com estimatedbachelors degree). fit regression model predict demographicsinformation followers website Twitter. Using patterns derivedtextual content social network user, final model producesaverage held-out correlation .77 across seven different variables (age, gender, education,ethnicity, income, parental status, political preference). apply modelclassify individual Twitter users ethnicity, gender, political preference, findingperformance surprisingly competitive fully supervised approach.1. IntroductionSocial media increasingly used make inferences real world, application politics (OConnor, Balasubramanyan, Routledge, & Smith, 2010), health (Dredze,2012), marketing (Gopinath, Thomas, & Krishnamurthi, 2014). Understanding demographic makeup sample social media users critical progressarea, allows researchers overcome considerable selection bias uncontrolleddata. Additionally, capability help public messaging campaigns ensuretarget demographic reached.common approach demographic inference supervised classificationtraining set annotated users, model fit predict user attributes contentwritings (Argamon, Dhawle, Koppel, & Pennebaker, 2005; Schler, Koppel, Argamon,& Pennebaker, 2006; Rao, Yarowsky, Shreevats, & Gupta, 2010; Pennacchiotti & Popescu,2011; Burger, Henderson, Kim, & Zarrella, 2011; Rao, Paul, Fink, Yarowsky, Oates, &Coppersmith, 2011; Al Zamal, Liu, & Ruths, 2012). approach number limitations: collecting human annotations costly error-prone; many demographic variablesinterest cannot easily labeled inspecting profile (e.g., income, education level);restricting learning small set labeled profiles, generalizability classifierc2016AI Access Foundation. rights reserved.fiCulotta, Ravi, & Cutlerlimited. Additionally, past work focused text primary source evidence,making little use network evidence.paper, fit regression models predict seven demographic variables Twitterusers (age, gender, education, ethnicity, income, parental status, political preference)based follow content tweets. Rather usingstandard supervised approach, construct distantly labeled dataset consisting webtraffic demographic data Quantcast.com. pairing web traffic demographicssite followers site Twitter.com, fit regression model setTwitter users expected demographic profile. evaluate accuracyrecovering Quantcast statistics well classifying individual Twitter users.experiments investigate several research questions:RQ1. demographics set Twitter users inferred networkinformation alone? find across seven demographic variables average heldout correlation .73 web traffic demographics websitepredicted regression model based sites Twitter followers. learn,example, high-income users likely follow Economist youngusers likely follow PlayStation.RQ2. revealing demographics: social network features linguistic features? Overall, find text-based features slightly predictivesocial network features (.79 vs. .73), particularly income age variables.RQ3. regression model extended classify individual users? Usinghand-labeled validation set users annotated gender, ethnicity, politicalpreference, find distantly trained regression model provides classificationaccuracy competitive fully-supervised approach. Averaged across threeclassification tasks, approaches obtain F1 score 81%.RQ4. much follower linguistic information needed prediction?find identities 10 friends per user, chosen random, sufficientachieve 90% accuracy obtained using 200 friends. Accuracy using linguisticfeatures begins plateau 2,000 unique terms observed per user.remainder paper, first review related work, describe datacollected Twitter QuantCast feature representation used task; next,present regression classification results; finally, conclude outline directionsfuture work.12. Related WorkPredicting attributes social media users growing area interest, recent workfocusing age (Schler et al., 2006; Rosenthal & McKeown, 2011; Nguyen, Smith, & Ros,2011; Al Zamal et al., 2012), gender (Rao et al., 2010; Burger et al., 2011; Liu & Ruths,1. Replication code data available here: https://github.com/tapilab/jair-2016-demographics.390fiPredicting Twitter User Demographics2013), race/ethnicity (Pennacchiotti & Popescu, 2011; Rao et al., 2011), personality (Argamon et al., 2005; Schwartz, Eichstaedt, Kern, Dziurzynski, Ramones, Agrawal, Shah, Kosinski, Stillwell, Seligman, et al., 2013), political affiliation (Conover, Goncalves, Ratkiewicz,Flammini, & Menczer, 2011; Barbera, 2013; Volkova & Van Durme, 2015), occupation (Preotiuc-Pietro, Lampos, & Aletras, 2015). work predicts demographicsweb browsing histories (Goel, Hofman, & Sirer, 2012). majority approachesrely hand-annotated training data, require explicit self-identification user,limited coarse attribute values (e.g., 25-years-old).Distantly supervised learning (also called lightly weakly supervised learning) provides alternative standard supervised learning relies less individual annotatedexamples, instead bootstrapping models declarative constraints. Previous workdeveloped methods train classifiers prior knowledge label proportions (Jin & Liu,2005; Musicant, Christensen, & Olson, 2007; Quadrianto, Petterson, & Smola, 2009a; Liang,Jordan, & Klein, 2009; Ganchev, Graca, Gillenwater, & Taskar, 2010; Mann & McCallum,2010; Zhu, Chen, & Xing, 2014) prior knowledge features-label associations (Schapire,Rochery, Rahim, & Gupta, 2002; Druck, Mann, & McCallum, 2008; Melville, Gryc, &Lawrence, 2009). addition standard document categorization tasks, lightly supervised approaches applied named-entity recognition (Mann & McCallum, 2010;Ganchev & Das, 2013; Wang & Manning, 2014), dependency parsing (Druck, Mann, &McCallum, 2009; Ganchev, Gillenwater, & Taskar, 2009), language identification (King &Abney, 2013), sentiment analysis (Melville et al., 2009).Chang, Rosenn, Backstrom, Marlow (2010) propose related distantly supervisedapproach demographic inference, inferring user-level ethnicity using name/ethnicity distributions provided U.S. Census; however, approach uses evidence firstlast names, often available, thus appropriate populationlevel estimates. Oktay, Firat, Ertem (2014) extend work Chang et al. (2010)also include statistics first names. Rao et al. (2011) take similar approach, alsoincluding evidence linguistic features infer gender ethnicity Facebookusers; evaluate fine-grained ethnicity classes Nigeria use limitedtraining data. recently, Mohammady Culotta (2014) trained ethnicity modelTwitter using county-level supervision, which, like approach, uses distant sourcesupervision build model individual demographics.several studies predicting population-level statistics social media.Eisenstein, Smith, Xing (2011) use geolocated tweets predict zip-code statisticsrace/ethnicity, income, variables using Census data; Schwartz et al. (2013)Culotta (2014) similarly predict county health statistics Twitter. However, noneprior work attempts predict evaluate user level.primary methodological novelties present work use web traffic dataform weak supervision use follower information primary sourceevidence. Additionally, work considers larger set demographic variablesprior work, predicts much fine-grained set categories (e.g., six different agebrackets instead two three used previously).paper extends initial version work (Culotta, Kumar, & Cutler, 2015).novel contributions include following: (1) added additional demographic variable (political preference); (2) added additional labeled dataset391fiCulotta, Ravi, & Cutlerevaluation (also political preference); (3) whereas initial version used friendfeatures, introduced textual features 9M tweets; (4) includednew analysis accuracy varies number terms collected per user.also reproduced prior results using newly collected data QuantCast.com;since QuantCast demographic statistics changed time, overall correlationsreported deviate slightly reported original version, trendsqualitative conclusions remain same.3. Datasection describe various data collected experiments.3.1 QuantcastQuantcast.com audience measurement company tracks demographics visitors millions websites. accomplished part using cookies trackbrowsing activity large panel respondents (Kamerer, 2013). writing,estimated demographics large number websites publicly accessiblesearchable web interface.sampled 1,532 websites Quantcast downloaded statistics seven demographic variables:Gender: Male, FemaleAge: 18-24, 25-34, 35-44, 45-54, 55-64, 65+Income: $0-50k, $50-100k, $100-150k, $150k+Education: College, College, Grad SchoolChildren: Kids, KidsEthnicity: Caucasian, Hispanic, African American, AsianPolitical Preference: Democrat, Republicanvariable, Quantcast reports estimated percentage visitors websitegiven demographic.3.2 Twitterwebsite collected previous step, executed script search Twitteraccount, manually verified it; 1,066 accounts original set 1,532 found.assumption work demographic profiles followers websiteTwitter correlated demographic profiles visitors website.undoubtedly biases introduced (e.g., Twitter users may skew younger webtraffic panel), aggregate differences limited impact final model.represent 1,066 Twitter accounts feature vectors derivedinformation followers. Below, describe features based socialnetwork linguistic content followers tweets.392fiPredicting Twitter User Demographicsh+p://www.lifehacker.com*73%*Male*19%*Graduate*School*30%*$50A100k*12%*Hispanic*lifehacker*A*B*lifehackers*Neighbor*Vector*C*D*2*/*2*=*1.0*1*/*2*=*0.5*C*D*Neighbors*Figure 1: Data model. collect QuantCast demographic data website,construct Neighbor Vector Twitter connections website,based proportion websites followers friendsneighbor.3.2.1 Friend FeaturesRecall Twitter users allowed follow accounts, introduces asymmetric relationship users. X follows , say friend X (thoughreverse may true). account, queried Twitter REST API sample 300 followers, using followers/ids request. sample necessarilyuniform. Twitter API documentation states time, results orderedrecent following first however, ordering subject unannouncedchange eventual consistency issues.followers, collected 5,000 accounts follow,called friends, using friends/ids API request. Thus, original accountsQuantcast, (300 5K = 1.5M ) additional accounts two hopsoriginal account (the friend follower). refer discovered accountsneighbors original Quantcast account. course, many accountsduplicates, two different followers follow many accounts (i.e., triadicclosure) indeed, core assumption number duplicates representsstrength similarity neighbors.393fiCulotta, Ravi, & Cutler105number unique neighbors107number neighbor linkscountcount106104105104103 010101rank102103103 010101rank102103Figure 2: Rank-order frequency plots number neighbors per account number links neighbors per account.original accounts, compute fraction followers friendsneighbors store neighbor vector. Figure 1 shows example.Suppose Quantcast account LifeHacker two followers B; followsC, B follows C D. neighbor vector LifeHacker {(C, 1), (D, .5)}.suggests LifeHacker stronger relationship C D.2 example,data top neighbors LifeHacker EA (the video game developer), GTASeries (avideo game fan site), PlayStation.resulting dataset consists 1.7M unique neighbors original 1,066 accounts.reduce dimensionality, removed neighbors fewer 100 followers, leaving46,649 unique neighbors total 178M incoming links. Figure 2 plots numberunique neighbors per account well number neighbor links per account.3.2.2 Text Featuresaddition neighbor vector, created analogous vector based tweetsfollowers account. collected recent 200 tweets 300followers 1,066 accounts using statuses/user timeline API request.tweet, perform standard tokenization, removing non-internal punctuation,converting lower case, maintaining hashtag mentions. URLs collapsedsingle feature type, digits (e.g., 12 mapped 99; 123 mapped 999).Characters repeated twice converted single occurrence. Terms usedfewer 20 different users removed.resulting dataset consists 9,427,489 tweets containing 112,642 unique terms written 59,431 users. original 1,066 accounts, create text vector similarprevious section. value represents proportion followers accountuse term. E.g., xij = .1 indicates 10% 300 followers account useterm j.2. Note use friends rather followers, since friend links created thuslikely indicate interests.394fiPredicting Twitter User Demographics4. Analysissection, report results predicting demographics aggregate level (i.e.,demographics accounts followers) user level (i.e., individual Twitterusers demographic profile).4.1 RegressionQuantcast site, pair demographic variables friend text featurevectors construct regression problem. Thus, attempt predict demographicprofile followers Twitter account based friends followerscontent tweets.Due high dimensionality (46,649 friend features 112,642 text features)small number examples (1,066), use elastic net regularization (Zou & Hastie, 2005),combines L1 L2 penalties. Furthermore, since output variable consistsdependent categories (e.g., age brackets), use multi-task variant elastic netensure features selected L1 regularizer category. useimplementation MultiTaskElasticNet scikit-learn (Pedregosa et al., 2011).Recall standard linear regression selects coefficients minimize squared errorlist training instances {xi , yi }Ni=1 , feature vector xi expected output yi .argminN1 X(yi xi )2N i=1Lasso imposes L1 regularizer , ridge regression imposes L2 regularizer. Elastic net combines penalties:argminN1 X(yi xi )2 + 1 ||||1 + 2 ||||22N i=11 2 control strength L1 L2 regularizers, respectively. L1regularizer encourages sparsity (i.e., many 0 values ), L2 regularizer preventsvalues becoming large.Multi-task elastic net extends standard elastic net groups related regression problems (Obozinski & Taskar, 2006). E.g., case, would like account factregressions College, College, Grad School related; thus,would like sparse solutions similar across tasks (that is, L1 selectfeatures task).(1)(M )Let (j) coefficients task j, let k = (k . . . k )T vectorcoefficients formed concatenating coefficients kth feature across tasks.multi-task elastic net objective enforces similar features selected across tasks:argminNjX1 XNj=1 j(j)(j)(yi (j)T xi )2 +i=11pXk=1395||k ||1 + 2 ||||22fiCulotta, Ravi, & CutlerDemocrat60Politicsr =0.84r =0.854040Predicted Value (%)20 40 6010 20 30 40 5018-24201010 20 30 40Caucasianr =0.8620 40 60 80020 40 60 8035-4420Ager =0.6120HispanicAfrican Americanr =0.8920 40604020045-5455-64151050r =0.6800 10 20 30 40True Value (%)r =0.7420101010r =0.78r =0.691510500010 20502040r =0.6430 40 50 60r =0.7610 20 30 40Kids7060504030r =0.7220 40 60College60$150k+FamilyEducation4020 40 602065+College6030200 5 10 15 20Asian201020 40 60r =0.7710 20Ethnicityr =0.7920 30 40r =0.77$100-150kr =0.6920 40 60 801010 20 30 4040202040Income$50-100k30402520154030201080604020r =0.8060r =0.7540302010$0-50kr =0.9110025-34r =0.8130Male502020GenderRepublicanGrad School40302010r =0.7810 20 30 40 50Figure 3: Scatter plots true demographic variables Quantcast versus predicted using elastic net regression fit friend text features Twitter.predictions computed using five fold cross-validation; panel alsoreports held-out correlation coefficient (r).Nj number instances task j p number features.fit three versions model using three different feature sets: Friends, Text,Friends+Text. tuned regularization parameters held-out set 200 accountsGender prediction, setting scikit-learn parameters l1 ratio=0.5 model,alpha=1e5 Friends model, alpha=1e2 Text Friends+Text models.4.1.1 Regression Resultsperform five-fold cross-validation report held-out correlation coefficient (r)predicted true demographic variables. Figure 3 displays resultingscatter plots 21 categories 7 demographic variables.see overall correlation strong: .77 average, ranging .6135-44 age bracket .91 Male. correlation coefficients significantusing two-tailed t-test (p < 0.01), Bonferroni adjustment 21 comparisons.results indicate neighbor text vectors provides reliable signaldemographics group Twitter users. put correlation value context,indirect comparison, Eisenstein et al. (2011) predict ethnicity proportions ZIPCode using Twitter data obtain maximum correlation .337 (though dataexperimental setup differ considerably).396fiPredicting Twitter User DemographicsCategoryGenderAgeIncomePoliticsValueMaleFemale18-2425-3435-4445-5455-6465+$0-50k$50-100k$100-150k$150k+DemocratRepublicanEducationChildrenEthnicityCollegeCollegeGrad SchoolKidsKidsCaucasianHispanicAfr. Amer.AsianTop AccountsAdamSchefter, SportsCenter, espn, mortreport, WIREDTheEllenShow, Oprah, MarthaStewart, Pinterest, EtsyIGN, PlayStation, RockstarGames, Ubisoft, steam gamesazizansari, lenadunham, mindykaling, WIREDcnnbrk, BarackObama, AP, TMZ, espnFoxNews, cnnbrk, AP, WSJ, CNNFoxNews, cnnbrk, AP, ABC, WSJFoxNews, AP, WSJ, cnnbrk, DRUDGE REPORTYouTube, PlayStation, IGN, RockstarGames, DrakeAdamSchefter, cnnbrk, SportsCenter, espn, ErinAndrewsWSJ, espn, AdamSchefter, SportsCenter, ErinAndrewsWSJ, TheEconomist, Forbes, nytimes, businessBarackObama,Oprah,NewYorker,UncleRUSH,MichelleObamaFoxNews,michellemalkin,seanhannity,megynkelly,DRUDGE REPORTYouTube, PlayStation, RockstarGames, Xbox, IGNStephenAtHome, WIRED, ConanOBrien, mashablenytimes, WSJ, NewYorker, TheEconomist, washingtonpostNewYorker, StephenAtHome, nytimes, maddow, pitchforkparenting,parentsmagazine,HuffPostParents,TheEllenShow, thepioneerwomanjimmyfallon,FoxNews,blakeshelton,TheEllenShow,TheOnionlatimes, Lakers, ABC7, Dodgers, KTLAKevinHart4real, Drake, Tip, iamdiddy, UncleRUSHTechCrunch, WIRED, BillGates, TheEconomist, SFGateTable 1: Accounts highest estimated coefficients category.examine results, Table 1 displays features 5 largest coefficients per class according regression model fit using friend features. Manyresults match common stereotypes: sports accounts predictive men (AdamShefterMortReport ESPN reporters), video game accounts predictive younger users(IGN video gaming media company), financial news accounts predictive greaterincome, parenting magazines predictive users children. alsoappear geographic effects, California-related accounts highly weightedHispanic Asian categories. seems good city-level resolutionLos Angeles accounts (latimes, Lakers) strongly correlated Hispanic users,whereas San Francisco accounts (SFGate, SFist, SFWeekly) strongly correlatedAsian users. seem selection bias, one must use cautioninterpreting results. example, BillGates predictive Asian users,part California many Asian-Americans part Californiastrong technology sector.397fiCulotta, Ravi, & CutlerCategoryGenderAgeValueMaleFemale18-2425-3435-4445-5455-6465+Income$0-50k$50-100k$100-150kPolitics$150k+DemocratRepublicanEducationCollegeCollegeGrad SchoolChildrenKidsKidsEthnicityCaucasianHispanicAfr. Amer.AsianTop Termsfilm, guy, gay, man, fuck, game, team, internet, review, guyshair, her, omg, family, girl, she, girls, cute, beautiful, thinkingd, haha, album, x, xd, :, actually, stream, wanna, imsuper, dc, baby, definitely, nba, pregnancy, wedding, even,entire, nycstar, fans, kids, tv, bike, mind, store, awesome, screen, sonwow, vote, american, comes, ca, santa, county, boys, nice,highvote, golf, red, american, country, north, county, holiday,smile, 99,999vote, golf, @foxnews, holiday, may, american, he, family,north, nationallol, games, @youtube, damn, black, ps9, side, d, community,godgreat, seattle, he, performance, lose, usa, kansas, iphone,wow, coldsanta, flight, nice, looks, practice, congrats, bike, dc, retweet,ridedc, nyc, market, @wsj, congrats, beach, san, york, ca, lookswomen, u, aint, nyc, equality, la, voice, seattle, dc, @nytimes@foxnews, christmas, #tcot, football, county, morning,family, christians, country, obamaslol, games, put, @youtube, county, made, ps9, xbox, videos,foundour, youre, seattle, photo, @mashable, la, apple, fashion,probably, sandc, @nytimes, market, which, review, excellent, boston, also,congrats, @washingtonpostcare, street, gay, years, health, drink, dc, white, ht, albumkids, school, child, family, kid, daughter, children, utah,moms, parentschristmas, fun, dog, country, st, could, luck, guy, florida, johnla, los, san, el, angeles, california, ca, lol, l.a, lakersblack, lol, bout, aint, brown, lil, african, blessed, smh, atlantachinese, la, sf, san, china, korea, india, bay, vs, hiTable 2: Terms highest estimated coefficients category.Additionally, Table 2 shows top 10 terms category text-only model.text features follow many trends friend features, perhapsgreater level granularity. terms self-evident, highlight here:xd emoticon laughing used among young users, often separated spaces(thus tokens x appearing separately 18-24 bracket); smh stands398fiPredicting Twitter User DemographicsModelmulti-task elastic netelastic netridgeFriends.73.72.62Text.79.78.79Friends + Text.77.76.78Average.76.75.73Table 3: Average held-out correlation across demographic variables three competingregression models.shaking head, expression disbelief predictive African American users;hi abbreviation state Hawaii, large Asian population.Finally, compare multi-task elastic net single-task variant elastic netridge regression (with regularization parameters tuned before). Table 3 showsthree methods mostly produce comparable accuracy, exception friendsfeatures, ridge regression performs substantially worse others.4.2 Classificationregression results suggest proposed model accurately characterizedemographics group Twitter accounts. section, provide additional validation manually annotated Twitter accounts investigate whether modelaccurately predict demographics individual users.4.2.1 Labeled DataMany demographic variables difficult label individual level e.g., income education level rarely explicitly mentioned either profile tweet. Indeed,advantage approach aggregate statistics readily availablemany demographics interest difficult label individual level. validation purposes, focus three variables fairly reliably labeled individuals:gender, ethnicity, political preference.gender ethnicity data originally collected Mohammady Culotta(2014) follows: First, used Twitter Streaming API obtain random sampleusers, filtered United States (using time zone place country codeprofile). six days worth data (December 6-12, 2013), sampled 1,000 profilesrandom categorized analyzing profile, tweets, profile imageuser. categorized 770 Twitter profiles one four ethnicities (Asian, AfricanAmerican, Hispanic, Caucasian). ethnicity could determineddiscarded (230/1,000; 23%).3 category frequency Asian (22), African American(263), Hispanic (158), Caucasian (327). estimate inter-annotator agreement, secondannotator sampled categorized 120 users. Among users annotatorsselected one four categories, 74/76 labels agreed (97%). disagreementcategory could determined: 21/120 labels (17.5%), one annotatorindicated category could determined, selected category. Gender3. introduces bias towards accounts identifiable ethnicity; leave investigationfuture work.399fiCulotta, Ravi, & CutlerGender# friends104EthnicityPolitics103102# unique terms101104103102101 010101102103 100101102user rank103 100101102103Figure 4: Rank-order frequency plots number friends per user numberunique terms per user labeled datasets (gender, ethnicity, politics).friends terms restricted one 46,649 accounts 112,642terms used regression experiments.annotation done automatically comparing first name provided user profileU.S. Census list names gender (Census, 1990). Ambiguous namesremoved.user, collected 200 friends using Twitter API. removedaccounts restricted access friend information; also removed Asian users duesmall sample size, leaving total 615 users. classification, user representedidentity friends (up 200). friend accounts contained46,649 accounts used regression experiments retained. additionally collected3,200 tweets user constructed binary term vector, usingtokenization regression model.political preference data comes Volkova, Coppersmith, Van Durme (2014),turn builds labeled data Pennacchiotti Popescu (2011) Al Zamalet al. (2012). Volkova (2014) provides detailed description data. usegeo-centric portion data, contains Twitter users Maryland, Virginia,Delaware report political affiliation Twitter profile description (e.g., Imfather, husband, Republican). Note feature representation considertokens user profile. contains 183 Republican users 230 Democratic users.(We consider related political datasets annotated based userfollows, may give unfair advantage friend features.) user5,000 friends 200 tweets. Figure 4 shows number friends numberunique terms per user dataset.400fiPredicting Twitter User DemographicsGenderEthnicityPoliticsAverageFriendsdistant full.75.66.60.68.80.83.72.72Textdistant.86.86.56.76full.84.86.73.81Friends + Textdistantfull.87.84.81.86.74.73.81.81Table 4: F1 results Twitter user classification manually annotated data. consider three different feature sets (Friends, Text, Friends+Text), well twoclassification models: full fully-supervised logistic regression classifier fitmanually labeled Twitter users; distant proposed distantly-supervised regression model, fit QuantCast data, using manually annotated Twitterusers. largest values row bold.4.2.2 Classification Modelsmodel initially trained regression task, make modificationsapply classification task. represent user labeled data binaryvector friend text features, using tokenization regression results.example, user follows accounts B, feature values 1corresponding accounts; similarly, user mentions terms X Y, featurevalues 1. repurpose regression model perform classification, must modifycoefficients returned regression. first compute z-score coefficientrespect coefficients category value. E.g., coefficientsMale class adjusted mean 0 unit variance. makes coefficientscomparable across labels. classify user, compute dot productcoefficients binary feature vector, selecting class maximum value.regression model fit combined Friend+Text feature set performed poorlyinitial classification experiments. Upon investigation, determined coefficientstwo types feature tended differ order magnitude. Rather usemodel directly, instead adopted ensemble approach combining outputstwo models trained separately text friend features. classify set users,computed feature-coefficient dot product separately text friend models,computed z-score resulting values class label (e.g., dot-products producedtext model Male class standardized zero mean unit variance).put predicted values model range. finally summedoutputs models returned class maximum value user.also compared fully-supervised baseline. trained logistic regressionclassifier L2 regularization, using feature representation above. performthree-fold cross-validation compare accuracy distantly supervised approach.401fiCulotta, Ravi, & CutlerFriendsEthnicityPoliticsMacro F1GenderTextFriends+Text0.90.80.70.60.50.40.90.80.70.60.50.40.90.80.70.6distant supervision0.5full supervision0.410 20 30 40 50 60 70 80 90 100 10 20 30 40 50 60 70 80 90 100 10 20 30 40 50 60 70 80 90 100% labeled training dataFigure 5: Twitter user classification results comparing standard logistic regression classifier (full supervision), trained using cross-validation, versus proposedapproach (distant supervision), fit solely statistics Quantcast, individually labeled data. Distant supervision comparable fullsupervision, even 100% available training data provided.4.2.3 Classification ResultsTable 4 compares F1 scores distantly supervised approach (distant) wellfully supervised baseline (full). report results using three feature setsthree labeled datasets.Overall, distantly supervised approach comparable fully supervised approach. two three tasks, F1 score distant meets exceeds full.third task (politics), best distant method within 3% best full method (.80vs .83). Averaging three tasks, best distant method indistinguishablebest full method (both produce F1 scores 81%).primary result distant supervision performs poorly political classificationusing text features. speculate part due small number tweets peruser available dataset (at 200 tweets per user). Additionally, text featuresused distantly supervised approach collected several years tweetscontained political dataset. Given rapid topical changes political dialogue,likely many highly-weighted terms distant text model less relevantolder data. results suggest friend features may less susceptibledata drift friend-based distant model performs much better text-basedmodel (.80 vs .56).number labeled data relatively small (fewer 1,000 users), examined accuracy fully supervised approach number labeled data increase(Figure 5). appears supervised classification accuracy mostly plateaued402fiPredicting Twitter User DemographicsMacro F1GenderEthnicityPolitics0.90.80.70.60.50.400.90.80.70.60.50.400.90.80.70.60.50.40FriendsText10 20 30 40 5002000 4000 6000 800010 20 30 40 5002000 4000 6000 800010 20 30 40 500friends per user5001000terms per userFigure 6: Classification F1 scores distantly supervised approach numberfriends number unique terms collected per user increase (with standarddeviations computed five random trials).task (with possible exception Friends+Text features gender classification).ethnicity, distant outperforms full half labeled data used fitclassification approach, full dominates. Thus, appears distantlysupervised approach comparable fully supervised learning across range sizeslabeled data.4.2.4 Sensitivity Number FeaturesFinally, investigate much information need user makeaccurate prediction demographics. so, perform experimentrandomly sample subset friends terms user, reportF1 number selected features increases. friends, consider subsets size{1, 3, 5, 10, 20, 30, 40, 50} (values greater 50 significantly increase accuracy).terms, consider subsets size {10, 100, 1000, 2000, 8029} (8,029 maximumnumber unique terms used single user labeled data).Figure 6 displays results. see accuracy plateaus quickly using friendfeatures: three tasks, F1 score using 10 friends within 5% score using200 friends. text features, accuracy begins plateau around 2K unique termsGender Ethnicity tasks. lower accuracy using Text features Politicslikely due part simple fact Politics data fewer tweets per user (amaximum 200 tweets per user, compared 3,200 Gender Ethnicitytasks).results implications scalability Twitter API rate limits make difficultcollect complete social graph tweets set users. Additionally,403fiCulotta, Ravi, & Cutlerimportant privacy implications; revealing even small amount social information mayalso reveal considerable amount demographic information. Twitter users concernedprivacy may wish disable setting makes friend identity information public.5. Conclusions Future Workpaper, shown pairing web traffic demographic data Twitter dataprovides simple effective way train demographic inference model without annotation individual profiles. validated approach aggregate (by comparing Quantcast data) individual level (by comparing hand-labeledannotations), finding high accuracy cases. Somewhat surprisingly, approachoutperforms fully-supervised approach gender classification, competitiveethnicity political classification.short-term future work, test generalizability approach newgroups Twitter users. example, collect users city county comparepredictions Census demographics geographic location. Additionally, investigate ways combine labeled unlabeled data using semi-supervisedlearning (Quadrianto, Smola, Caetano, & Le, 2009b; Ganchev et al., 2010; Mann & McCallum, 2010). Finally, fully validate across demographic variables, consideradministering surveys Twitter users compare predictions self-reported surveyresponses.Additional future work may investigate sophisticated types distant supervision.example, homophily constraints imposed encourage neighbors similar demographics; location constraints use used learn county demographicdata. Also, multi-task model captures interaction demographic variables training time, also use collective inference reflect correlations amongdemographic variables. Finally, considered simple bag-of-words featurerepresentations; future work may investigate low-dimensional embeddings non-linearmodels.Acknowledgmentsresearch funded part support IIT Educational Research Initiative Fund. Culotta supported part National Science Foundation grant#IIS-1526674. opinions, findings conclusions recommendations expressedmaterial authors necessarily reflect sponsor.ReferencesAl Zamal, F., Liu, W., & Ruths, D. (2012). Homophily latent attribute inference:Inferring latent attributes twitter users neighbors. ICWSM.Argamon, S., Dhawle, S., Koppel, M., & Pennebaker, J. W. (2005). Lexical predictorspersonality type. proceedings Joint Annual Meeting InterfaceClassification Society North America.404fiPredicting Twitter User DemographicsBarbera, P. (2013). Birds feather tweet together. bayesian ideal point estimationusing twitter data. Proceedings Social Media Political Participation,Florence, Italy, pp. 1011.Burger, J. D., Henderson, J., Kim, G., & Zarrella, G. (2011). Discriminating gendertwitter. Proceedings Conference Empirical Methods Natural LanguageProcessing, EMNLP 11, pp. 13011309, Stroudsburg, PA, USA. Association Computational Linguistics.Census (1990).List surnames.http://www2.census.gov/topics/genealogy/1990surnames. Accessed: 2015-06-01.Chang, J., Rosenn, I., Backstrom, L., & Marlow, C. (2010). ePluribus: ethnicity socialnetworks. Fourth International AAAI Conference Weblogs Social Media.Conover, M. D., Goncalves, B., Ratkiewicz, J., Flammini, A., & Menczer, F. (2011). Predicting political alignment twitter users. IEEE Third international conferencesocial computing (SOCIALCOM), pp. 192199. IEEE.Culotta, A. (2014). Estimating county health statistics twitter. CHI.Culotta, A., Kumar, N. R., & Cutler, J. (2015). Predicting demographics twitterusers website traffic data. Twenty-ninth National Conference ArtificialIntelligence (AAAI).Dredze, M. (2012). social media change public health. IEEE Intelligent Systems,27 (4), 8184.Druck, G., Mann, G., & McCallum, A. (2008). Learning labeled features using generalized expectation criteria. Proceedings 31st Annual International ACM SIGIRConference Research Development Information Retrieval, pp. 595602.Druck, G., Mann, G., & McCallum, A. (2009). Semi-supervised learning dependencyparsers using generalized expectation criteria. ACL.Eisenstein, J., Smith, N. A., & Xing, E. P. (2011). Discovering sociolinguistic associationsstructured sparsity. Proceedings 49th Annual Meeting AssociationComputational Linguistics: Human Language Technologies - Volume 1, HLT 11,pp. 13651374, Stroudsburg, PA, USA. Association Computational Linguistics.Ganchev, K., & Das, D. (2013). Cross-lingual discriminative learning sequence modelsposterior regularization.. EMNLP, pp. 19962006.Ganchev, K., Gillenwater, J., & Taskar, B. (2009). Dependency grammar induction viabitext projection constraints. Proceedings Joint Conference 47thAnnual Meeting ACL 4th International Joint Conference NaturalLanguage Processing AFNLP: Volume 1-Volume 1, pp. 369377. AssociationComputational Linguistics.Ganchev, K., Graca, J., Gillenwater, J., & Taskar, B. (2010). Posterior regularizationstructured latent variable models. J. Mach. Learn. Res., 11, 20012049.Goel, S., Hofman, J. M., & Sirer, M. I. (2012). web: large-scalestudy browsing behavior.. ICWSM.405fiCulotta, Ravi, & CutlerGopinath, S., Thomas, J. S., & Krishnamurthi, L. (2014). Investigating relationshipcontent online word mouth, advertising, brand performance.Marketing Science, 33 (2), 241258.Jin, R., & Liu, Y. (2005). framework incorporating class priors discriminativeclassification. PAKDD.Kamerer, D. (2013). Estimating online audiences: Understanding limitations competitive intelligence services. First Monday, 18 (5).King, B., & Abney, S. (2013). Labeling languages words mixed-language documentsusing weakly supervised methods. Proceedings NAACL-HLT, pp. 11101119.Liang, P., Jordan, M. I., & Klein, D. (2009). Learning measurements exponentialfamilies. Proceedings 26th Annual International Conference MachineLearning, ICML 09, p. 641648, New York, NY, USA. ACM.Liu, W., & Ruths, D. (2013). Whats name? using first names features genderinference twitter. AAAI Spring Symposium Analyzing Microtext.Mann, G. S., & McCallum, A. (2010). Generalized expectation criteria semi-supervisedlearning weakly labeled data. J. Mach. Learn. Res., 11, 955984.Melville, P., Gryc, W., & Lawrence, R. D. (2009). Sentiment analysis blogs combininglexical knowledge text classification. Proceedings 15th ACM SIGKDDInternational Conference Knowledge Discovery Data Mining, KDD 09, p.12751284, New York, NY, USA. ACM.Mohammady, E., & Culotta, A. (2014). Using county demographics infer attributestwitter users. ACL Joint Workshop Social Dynamics Personal AttributesSocial Media.Musicant, D., Christensen, J., & Olson, J. (2007). Supervised learning trainingaggregate outputs. Seventh IEEE International Conference Data Mining, 2007.ICDM 2007, pp. 252261.Nguyen, D., Smith, N. A., & Ros, C. P. (2011). Author age prediction text using linearregression. Proceedings 5th ACL-HLT Workshop Language TechnologyCultural Heritage, Social Scie nces, Humanities, LaTeCH 11, pp. 115123,Stroudsburg, PA, USA. Association Computational Linguistics.Obozinski, G., & Taskar, B. (2006). Multi-task feature selection. workshop structural Knowledge Transfer Machine Learning 23rd International ConferenceMachine Learning (ICML.OConnor, B., Balasubramanyan, R., Routledge, B. R., & Smith, N. A. (2010). tweetspolls: Linking text sentiment public opinion time series.. ICWSM, 11, 122129.Oktay, H., Firat, A., & Ertem, Z. (2014). Demographic breakdown twitter users:analysis based names. Academy Science Engineering (ASE).Pedregosa, F., et al. (2011). Scikit-learn: Machine learning Python. Machine LearningResearch, 12, 28252830.406fiPredicting Twitter User DemographicsPennacchiotti, M., & Popescu, A.-M. (2011). machine learning approach twitter userclassification.. Adamic, L. A., Baeza-Yates, R. A., & Counts, S. (Eds.), ICWSM.AAAI Press.Preotiuc-Pietro, D., Lampos, V., & Aletras, N. (2015). analysis user occupationalclass twitter content. ACL.Quadrianto, N., Petterson, J., & Smola, A. J. (2009a). Distribution matching transduction. Advances Neural Information Processing Systems 22, p. 15001508. MITPress.Quadrianto, N., Smola, A. J., Caetano, T. S., & Le, Q. V. (2009b). Estimating labelslabel proportions. J. Mach. Learn. Res., 10, 23492374.Rao, D., Paul, M. J., Fink, C., Yarowsky, D., Oates, T., & Coppersmith, G. (2011). Hierarchical bayesian models latent attribute detection social media. ICWSM.Rao, D., Yarowsky, D., Shreevats, A., & Gupta, M. (2010). Classifying latent user attributestwitter. Proceedings 2nd International Workshop Search MiningUser-generated Contents, SMUC 10, pp. 3744, New York, NY, USA. ACM.Rosenthal, S., & McKeown, K. (2011). Age prediction blogs: study style, content,online behavior pre- post-social media generations. Proceedings 49thAnnual Meeting Association Computational Linguistics: Human LanguageTechnologies - Volume 1, HLT 11, pp. 763772, Stroudsburg, PA, USA. AssociationComputational Linguistics.Schapire, R. E., Rochery, M., Rahim, M. G., & Gupta, N. K. (2002). Incorporating priorknowledge boosting. Proceedings Nineteenth International Conference,pp. 538545.Schler, J., Koppel, M., Argamon, S., & Pennebaker, J. W. (2006). Effects agegender blogging. AAAI 2006 Spring Symposium Computational ApproachesAnalysing Weblogs (AAAI-CAAW), pp. 0603.Schwartz, H. A., Eichstaedt, J. C., Kern, M. L., Dziurzynski, L., Ramones, S. M., Agrawal,M., Shah, A., Kosinski, M., Stillwell, D., Seligman, M. E., et al. (2013). Characterizing geographic variation well-being using tweets. Seventh International AAAIConference Weblogs Social Media.Volkova, S. (2014). Twitter data collection: Crawling users, neighbors communication personal attribute prediction social media. Tech. rep., Johns HopkinsUniversity.Volkova, S., Coppersmith, G., & Van Durme, B. (2014). Inferring user political preferencesstreaming communications. Proceedings Association ComputationalLinguistics (ACL).Volkova, S., & Van Durme, B. (2015). Online bayesian models personal analyticssocial media. Proceedings Twenty-Ninth Conference Artificial Intelligence(AAAI), Austin, TX.Wang, M., & Manning, C. D. (2014). Cross-lingual projected expectation regularizationweakly supervised learning. TACL, 2, 5566.407fiCulotta, Ravi, & CutlerZhu, J., Chen, N., & Xing, E. P. (2014). Bayesian inference posterior regularizationapplications infinite latent svms. Journal Machine Learning Research, 15,17991847.Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net.Journal Royal Statistical Society: Series B (Statistical Methodology), 67 (2),301320.408fiJournal Artificial Intelligence Research 55 (2016) 249-281Submitted 03/15; published 01/16Utilisation Metadata Fields Query ExpansionCross-Lingual Search User-Generated Internet VideoAhmad KhwilehDebasis GangulyGareth J. F. Jonesahmad.khwileh2@mail.dcu.iedganguly@computing.dcu.iegjones@computing.dcu.ieADAPT Centre, School ComputingDublin City UniversityDublin 9, IrelandAbstractRecent years seen significant efforts area Cross Language InformationRetrieval (CLIR) text retrieval. work initially focused formally publishedcontent, recently research begun concentrate CLIR informal socialmedia content. However, despite current expansion online multimedia archives,little work CLIR content. limited workCross-Language Video Retrieval (CLVR) professional videos, documentariesTV news broadcasts, date, significant investigation CLVRrapidly growing archives informal user generated (UGC) content. Key differencesUGC professionally produced content nature structuretextual UGC metadata associated it, well form qualitycontent itself. setting, retrieval effectiveness may suffer translationerrors common CLIR tasks, also recognition errors associated automaticspeech recognition (ASR) systems used transcribe spoken content videoinformality inconsistency associated user-created metadatavideo. work proposes evaluates techniques improve CLIR effectivenessnoisy UGC content. experimental investigation shows different sourcesevidence, e.g. content different fields structured metadata, significantlyaffect CLIR effectiveness. Results experiments also show metadata fieldvarying robustness query expansion (QE) hence negative impactCLIR effectiveness. work proposes novel adaptive QE technique predictsreliable source expansion shows technique effectiveimproving CLIR effectiveness UGC content.1. IntroductionIncreasing amounts user generated multilingual video content (UGC) uploadedsocial video-sharing websites Youtube (2015), Facebook (Facebook video, 2015),BlipTv (2015) many others. 2015, YouTube, predominant online video sharingsite, reported 300 hours video content uploaded every minute 61different languages (YouTube Press, 2015). ease flexibility video contentproduction, coupled low cost publishing wide potential reach, resultingexponential growth number videos available Web.time, due increasing user demands accessing viewing content,important manage way facilitate effective efficient accessc2016AI Access Foundation. rights reserved.fiKhwileh, Ganguly, & Jonesit. large amounts content creating need developmentsophisticated video retrieval systems, presenting new challenges exciting opportunitiesInformation Retrieval (IR) research (Bendersky, Garcia-Pueyo, Harmsen, Josifovski, &Lepikhin, 2014; Naaman, 2012).One key challenges effective exploitation UGC content multilingual setting effective search languages user queries contentmetadata. multilingual perspective, quality UGC depends solelycharacteristics individuals actually produce upload videos language.lack formal editorial control means uploaded videos typically varied across languages terms audio, visual metadata quality. Moreover, quantitytopical coverage content across different languages uneven, often meaning satisfying information need user one language achievedproviding relevant content another language. example, bilingual Arabic speakersfrequently enter Arabic queries relevant content English,even likely video material little UGC Arabic content currently availablecertain topics cultural historical topics. fact classical use-caseCross-Language Information Retrieval (CLIR) seeks enable users enter searchqueries one language retrieve relevant content another one. Translation technologieskey successfully bridging language gap users query relevantcontent (Oard & Diekema, 1998; Herbert, Szarvas, & Gurevych, 2011).quality monolingual search UGC content dependent effective utilization available metadata. CLIR search effectiveness depend translationquality query content languages. many potential choicesdesign robust CLIR framework Internet video search task, current lackdetailed investigation means lack understanding specific challengesrepresents thus little guidance available choices madedeveloping framework.paper, investigate CLIR search effectiveness archive user-generatedInternet video content originally used MediaEval 2012 Search Hyperlinkingtask (Eskevich, Jones, Chen, Aly, Ordelman, & Larson, 2012a), extendCLIR task. examine retrieval effectiveness using title description metadataprovided video uploader automatic speech recognition (ASR) transcriptscontent. investigate application automatic query expansion sourceimproving CLIR retrieval performance. Retrieval query expansion carriedusing Divergence Randomness (DFR) IR model, automatic translationcarried using Google Translate (2015). understand task better, undertakedetailed performance analysis examining impact different source metadata information CLIR behaviour. However, current investigation limited applicationstate-of-the-art Machine Translation (MT) information retrieval (IR) methodstask, order establish basis investigations.remainder paper structured follows: Section 2 gives generalbackground CLIR, Section 3 reviews related work, Section 4 describes test setused experiments evaluation metric, Section 5 describes initial retrievalexperiments examining relative CLIR effectiveness source evidence (ASR,Title Description), Section 6 describes approach improving CLIR effectiveness250fiCross-Lingual Search User-Generated Internet Videousing careful adjustment retrieval algorithm setting, Section7 describes approachimproving CLIR effectiveness using automatic query expansion techniques, Section 8concludes paper provides directions work.2. Cross Language Information Retrievalstated previously, goal CLIR satisfy user information need expressedquery one language using content another language. CLIR techniques usetranslation bridge language barrier query indexed content.techniques differ mainly translation module placed, eitherquery processing document indexing stage. Figure 1 shows CLIR techniquesutilise translation technologies bridge barrier query language (L2)document language (L1). Query Translation approach (QT CLIR) commonFigure 1: Document Query based CLIR Techniques.CLIR technique (Oard & Diekema, 1998; Herbert et al., 2011; Sokolov, Hieber, & Riezler,2014); query translated match index language (L1). techniqueknown low cost (per translated query) easy implement, since translationtool used online retrieval time translate query document language.Yet approach dependent sensitive quality query translationretrieval. queries may lack context semantic content, makesharder interpret translate. Previous literature explored multiple techniquesovercome issues either improving translation quality using various translationtechniques (Chen, Hueng, Ding, & Tsai, 1998; Gao, Nie, Xun, Zhang, Zhou, & Huang,2001; Varshney & Bajpai, 2014; Lee, Chen, Kao, & Cheng, 2010) improving queryusing query reformulation techniques Query Expansion (QE) RelevanceFeedback (RF) process (Carpineto & Romano, 2012).251fiKhwileh, Ganguly, & JonesQE RF operates selecting terms documents markedrelevant user adding original query. absence user createdrelevance data, top ranked documents assumed relevant process often referredpseudo relevance feedback (PRF). Although noisy, PRF shown effectiveimproving overall retrieval effectiveness (Bellaachia & Amor-Tijani, 2008). crosslingual settings, query expanded translation provide effectivequery translation using pre-translation QE technique (Ballesteros & Croft, 1997). QEexpansion also applied translation (post-translation QE), using combinationpre-translation post-translation QE shown Figure 1, order alleviateimpact IR effectiveness arising translation problems (Ballesteros & Croft, 1998;Rogati & Yang, 2002).alternative QT CLIR Document Translation (DT CLIR) documentscollection translated query language (Oard & Hackett, 1998; Lee & Croft,2014). Several arguments suggest DT competitive superior QT CLIRtasks, due fact less sensitive translation errors. DTadvantage translation carried offline prior retrieval, allowspossibility tuned accurate translation. Another advantage DT CLIRrequire result translation shown Figure 1, since documentsalready translated index time. However, DT CLIR shown effectiveseveral tasks, application CLIR settings often impractical duelarge amount time resources required document translation. Particularly,document collection large search carried across multiple languagepairs. less common CLIR technique shown effective, Hybrid CLIRapproach utilises document query translation approaches, thus allowingrelative advantages approaches complement (McCarley, 1999; Kishida& Kando, 2006; Parton, McKeown, Allan, & Henestroza, 2008).Several approaches proposed carry translation process CLIRframework. commonly used ones bilingual dictionaries machine translation (MT) (Zhou, Truran, Brailsford, Wade, & Ashman, 2012). Bilingual dictionariesperform word-by-word translation using machine-readable dictionary setsentries words possible translations language (Pirkola, Hedlund,Keskustalo, & Jarvelin, 2001). approach suffer issues coverage, sincewords may contained machine-readable dictionary, ambiguity sincerelies dictionary many words multiple possible translations selecting correct translation among non-trivial task. Machine translation (MT)techniques use trained system perform automatic translation free-text onenatural language another (Nikoulina, Kovachev, Lagos, & Monz, 2012; Magdy & Jones,2014). MT also similar dictionary coverage problems, creationsingle best translation addresses translation ambiguity issues. recent years, MTbecome commonly used technique CLIR due increasing availabilityhigh quality off-the-shelf MT systems. CLIR research dealt translationmodule black-box without control translation process, rather usedone freely available online translation tools Google Translate (2015), Bingtranslate (2015) others, proven effective. example, CLEFevaluation campaigns 2009 (CLEF, 2015a), best performing non-Google MT system252fiCross-Lingual Search User-Generated Internet Videoachieved 70% performance achieved Google Translate tool (Leveling, Zhou,Jones, & Wade, 2010; Zhou et al., 2012).experimental investigation, choose default common CLIRsettings, QT CLIR technique utilises Google MT translate tool.nevertheless, plan explore CLIR settings (e.g. DT-CLIR open-box MT system)future work.3. Related WorkSeveral CLIR tasks explored across different domains document types (Peters,Braschler, & Clough, 2012). closely related CLIR work examinedresearch carried tasks within CLEF evaluation campaigns professionallygenerated video content (2015b).2002-2004 Cross-Language Spoken Document Retrieval (CL-SDR) task investigated news story document retrieval using data NIST TREC 8-9 SpokenDocument Retrieval (SR) manually translated queries (Federico & Jones, 2004; Federico, Bertoldi, Levow, & Jones, 2005). aim tasks evaluate CLIRsystems noisy automatic transcripts spoken documents known story boundariesinvolved retrieval American English news broadcasts unsegmentedsegmented transcripts taken radio TV news. CLIR tasks done usingtopics several European languages. metadata provided tasks,interesting findings indicate even manually translated queries, best CLIRperformance resulted 15% reduction monolingual ones (Federico & Jones, 2004),using dictionary term-by-term translation, reduction increased40% 60%, highlights challenge CLIR video collections (Federico et al.,2005).ambitious Cross-Language Speech Retrieval (CL-SR) task ran within CLEF2005-2007 (White, Oard, Jones, Soergel, & Huang, 2006; Oard, Wang, Jones, White,Pecina, Soergel, Huang, & Shafran, 2007; Pecina, Hoffmannova, Jones, Zhang, & Oard,2008). examined CLIR spontaneous conversational speech oral history collectioncontent English Czech. tasks provided ASR transcripts, automaticallymanually generated metadata interviews. goal Czech Englishtasks create systems could help monolingual cross lingual searchers identifysections interview wish listen Czech English interviews.reported results tasks showed use manual metadata yielded substantialstatistically significant improvement retrieval effectiveness ASR transcriptsautomatically created metadata. investigation carried Inkpen, Alzghool, Jones,Oard (2006) CL-SR standard collection showed retrieval effectiveness couldimproved careful selection term weighting scheme ASRmanual metadata. Alzghool Inkpen (2008) also used test collection CLEF2007 CL-SR task present method combining results different retrieval modelsorder improve overall retrieval effectiveness. also provided comparisonASR manual metadata, indicating superiority manual metadatamaintaining retrieval effectiveness. Another interesting follow study, reportedJones, Zhang, Newman, Lam-Adesina (2007), examined compared CLIR253fiKhwileh, Ganguly, & Joneseffectiveness source evidence included collection. Results workindicate searching manually generated metadata gives higher performance termsrecall precision search noisy ASR transcripts.VideoCLEF task introduced CLEF 2008 CLEF 2009. taskprovided Dutch TV content featuring English-speaking experts studio guests. VideoCLEF piloted tasks involved performing classification, translation keyword extractiondual language video using either machine learning techniques treating IRtask. Participants provided Dutch archival metadata, Dutch speech transcripts,English speech transcripts (Larson, Newman, & Jones, 2009, 2010).previous work CLVR focused running CLIR tasks professional videobroadcast whether documentaries, TV shows interviews high quality recordingconsistency length, visual audio quality across collections. collectionsincluded manually automatically created metadata. example, domain experts following carefully prescribed format wrote manually created metadata CLEF2005-2007 consistent speech quality word error rate 25% across collectionsused (White et al., 2006; Oard et al., 2007; Pecina et al., 2008).CLEF tasks followed establishment MediaEval benchmarkingcampaign 2010 (MediaEval, 2015). Activities MediaEval focused variousmultimedia search tasks, included CLIR elements.emergence user-generated video content web introduced new searchopportunities challenges exploitation user-generated metadata (Eickhoff, Li,& de Vries, 2013; Filippova & Hall, 2011; Toderici, Aradhye, Pasca, Sbaiz, & Yagnik, 2010).CLIR published text explored wide variety language pairsmany years, recent research begun explore CLIR user-generated informal text.One example work one done Bagdouri, Oard, Castelli (2014)explored retrieval questions posed formal English across user generated documentsArabic collected forum posts. employed DT CLIR approachtranslated Arabic informal text English. results show retrieval precisionenhanced applying informal text classifier help translation informalcontent. Lee Croft (2014) also experimented CLIR task informal documents.developed CLIR task large collection Chinese forum posts demonstratedtranslation noise increased informal text used discussion forums.retrieval approach proposed use PRF approach improve retrieval effectiveness.results showed PRF approaches useful reducing impact translationerrors retrieval effectiveness tasks.UGC begun attract considerable research interest video retrieval indexing recent years. none work far included element CLIR,much addressed main issues user-generated content video retrieval.example, work focused quality user-generated metadata video retrieval (Eickhoff et al., 2013; Filippova & Hall, 2011; Toderici et al., 2010), workfocused quality visual/audio features within scale dynamics UGCcontent (Bendersky et al., 2014; Chelba, Bikel, Shugrina, Nguyen, & Kumar, 2012; Langlois, Chambel, Oliveira, Carvalho, Marques, & Falcao, 2010). Moreover, 2010,TREC Video Retrieval Evaluation (TRECVID) (2015), main video retrieval benchmarkmultimedia community, provided collection Internet videos used several254fiCross-Lingual Search User-Generated Internet VideoTable 1: Length statistics indexed blip10000 fields.Stan.DevAvg.LengthMedianMaxMinTitle3.05.35.022.00.0Desc106.947.724.03197.01.0ASR2399.5703.01674.820451.00.0tasks. However, design TRECVID tasks mainly focused exploiting visual information applications shot level (concept detection), short video clips (eventdetection) others. One task relevant work known-item searchtask (KIS) (Over, Awad, Fiscus, Antonishek, Michel, Smeaton, Kraaij, & Quenot, 2011)TRECVID, task aimed explore retrieval visual queries includedTRECVID annually 2010 2012. Results participants rather inconsistent year year terms retrieval effectiveness different search approaches,one conclusion difficulty actually setting evaluation task Internetcollections.work, focus studying retrieval challenges Internet-based UGC multimedia collections audio data highly variable many aspects including audioconditions recording, microphones used, fluency informality language used speaker. challenges produce ASR errors affectretrieval effectiveness monolingual retrieval, reported Eskevich, Jones,Wartena, Larson, Aly, Verschoor, Ordelman (2012b) Eskevich (2014), alsocross-lingual settings combined query translation.best knowledge, work first effort explore issues CLIRvideo collected user-contributed source Internet. Thus creators variedbackgrounds differing motivations interests generated content withoutcentral editorial control style, format quality. makes uploaded videosvaried terms amount quality manually added metadata descriptions,thus challenging multiple retrieval perspectives. particular relevanceinvestigation following aspects data:Distribution document lengths: restriction document lengthfound highly variable. length variability poses challengeretrieval task, particularly significant within CLIR due presencetranslation errors. breakdown details various fields blip10000test collection shown Table 1.High variability ASR quality video transcripts: Even ASR systemused, variation audio quality, speaking styles speakers, generallyleads significant variability accuracy transcripts.Inconsistencies sparseness associated user contributed metadata: titlesmay short one two terms, descriptions generic,informal sometimes incomplete, making utility retrieval varied.255fiKhwileh, Ganguly, & Jones4. Experimental Test Set Evaluationblip10000 collection used experiments crawl Internet video sharingplatform Blip.tv (Schmiedeke, Xu, Ferrane, Eskevich, Kofler, Larson, Esteve, Lamel, Jones,& Sikora, 2013). originally used content dataset MediaEval 2012Search Hyperlinking task (Eskevich et al., 2012a). blip10000 collection containscrawled videos together associated metadata. metadata composedtitles descriptions video provided video uploader.addition, associated ASR transcripts also provided videos. collectionconsists 14,838 videos total running time ca. 3,288 hours, total size862 GB1 .length statistics fields shown Table 1. notedhuge variation length distributions across different fields. Table 1 alsohighlights variations individual fields videos. example, onevideo may ASR, another may contain 20K terms. experimentsindexed metadata fields separately, combination shown Figure 2.Figure 2: Example combined-field document.1. Blip10000 Data Collection obtained from:http://skuld.cs.umass.edu/traces/mmsys/2013/blip/Blip10000.html256fiCross-Lingual Search User-Generated Internet VideoTable 2: Monolingual English query vs Arabic-English translated query example.Monolingual (MN) Query :<top><num>37 </num><Mn-Lg>the video features recent USA sanctionedclean energy Act.</Mn-Lg><Mn-Sh>clean energy legislation USA</Mn-Sh></top>Machine Translated (CL) Query :<top><num>37</num><CL-AR-Lg>Video displays Identify measures UnitedStates toward alternative Energy</CL-AR-Lg><CL-AR-Sh>Rules United Statestoward alternative energy</CL-AR-Sh></top>4.1 Query Construction CLIR TaskMediaEval 2012 Search Hyperlinking task (Eskevich et al., 2012a) knownitem search task, search single previously seen relevant video (the known-item),provided 60 English queries collected using Amazon Mechanical Turk (MTurk)crowd-sourcing platform (2015). query contains full query statement (long query)terse web type search query (short query). investigation, exploredshort long queries give better understanding query-length independentretrieval behaviour monolingual CLIR tasks. create CLIR test set,extended original monolingual English queries giving Arabic, ItalianFrench native speakers, asking translate natural queriesnative language. short long queries translated Arabic. orderexplore CLIR effectiveness across multiple language pairs, short query setalso expressed Italian, long query set constructed French.types queries expressed two languages (long queries expressed ArabicFrench, short queries expressed Arabic Italian) allowed usdraw better conclusions CLIR performance task. used Googletranslate API2 translate query sets back English. would expected.MT translation produced different version original monolingual ones; additionexpected deletion/insertion edits shown example Table 2, alsoNamed Entity Errors (NEEs) Out-Of-Vocabulary (OOV) items Google translationcould translate correctly. translation edits errors pose challengeretrieval effectiveness MT translated queries compared monolingual ones.monolingual English query sets originally provided MediaEval2012 Search Hyperlinking task. investigation query sets labelledfollows:2. https://developers.google.com/translate257fiKhwileh, Ganguly, & JonesMn-Sh: 60 EN short queries (monolingual)Mn-Lg: 60 EN long queries (monolingual)CLIR query sets labelled follows:CL-AR-Sh: 60 AR short queries translated ENCL-AR-Lg: 60 AR long queries translated ENCL-IT-Sh: 60 long queries translated ENCL-FR-Lg: 60 FR long queries translated EN4.2 Mean Reciprocal Rank (MRR) Evaluation MetricSince retrieval problem addressing known-item searchseeking retrieve single known relevant item, evaluate investigations usingstandard metric task Mean Reciprocal Rank (MRR) metric computedshown Equation 1 ranki indicates rank ground truth known itemith query intended find.RR =n11Xn i=1 ranki(1)Similar known-item experiments, also chose define recall numbertimes relevant-item found across set queries (Buttcher, Clarke, & Cormack,2010). Moreover, recall reported default standard TREC1000 results cutoff, also report cutoff points 10, 50, 100 documents experiments.5. Single Field Retrievalfirst part investigation examines behaviour separate document information fields CLIR framework. particularly interested impacttranslation errors inconsistencies retrieval effectiveness given noise ASRtranscripts, shortness title field, inconsistencies description field.examine question evaluating CLIR robustness field measureretrieval effectiveness behaves CLIR framework. Throughout investigationpaper, define CLIR Robustness well field source evidence performsCLIR framework. observe computing significance changeCLIR monolingual performance using setting across query sets.run CLIR robustness evaluation experiment, compare CLIR effectivenessfield monolingual baseline:ASR index contains ASR transcript fields.Title index contains title fields.Desc index contains description fields.258fiCross-Lingual Search User-Generated Internet VideoTable 3: Mono vs. CLIR performance per indexTitle indexASR indexDesc indexMn-Sh0.2390.42750.2154CL-AR-Sh0.22880.27480.1943CL-IT-Sh0.23830.38730.2102Mn-Lg0.28270.45130.2432CL-AR-Lg0.22440.34870.2285CL-FR-Lg0.22390.38330.23165.1 Retrieval Modelsingle field retrieval experiments carried using Terrier retrieval engine3 .Terrier standard open source IR toolkit providing many best established retrievalalgorithms widely used IR research community. Stop-words removed basedstandard Terrier list, stemming performed using Terrier implementationPorter stemming. used PL2 model4 , probabilistic retrieval modelDivergence Randomness (DFR) framework (Gianni, 2003). reason selectedmodel available retrieval models characteristics data collection.Previous studies as(Amati & Van Rijsbergen, 2002) shown PL2 lesssensitivity length distribution compared retrieval models works betterexperiments seek early precision, aligns known-item experiment. PL2thus suitable since Internet based data collection huge variation lengths, whetherfield level document level, shown Table 1. PL2 document scoringmodel defined shown Equation 2, Score(d, Q) retrieval matching scoredocument query term Poisson distribution F/N , F queryterm frequency whole collection N total number documentscollection. qtw query term weight given qtf /qtf max; qtf query termfrequency qtf max maximum query term frequency among query terms.tfn normalized term frequency defined Equation 3, l lengthdocument d. avgl average length documents, c free parameternormalization. set parameter c, followed empirically determined standardsettings recommended Amati Van Rijsbergen (2002) Ounis (2007b),c = 1 short queries c = 7 long queries.Score(d, Q) =XtQqtw .1tfn(tfn log2+ ( tfn ). log2 e + 0.5 log2 (2.tfn ))1 + tfntfn =X(tf. log2 (1 + c.avgl)), (c > 0)l(2)(3)5.2 Experimental Results Discussionresults index shown Table 3, show MRR lowercases CLIR task. Thus retrieval effectiveness fields negatively impacted3. http://www.terrier.org/4. Terrier implementation model found :http://terrier.org/docs/v4.0/javadoc/org/terrier/matching/models/PL2.html259fiKhwileh, Ganguly, & JonesTable 4: AR CLIR - t-values according % MRR reduction indexCL-AR-Sh CL-AR-LgTitle index-1.69-1.73ASR index-1.94*-2.50*Desc index-0.829-0.44*Statistically significant values p-value < 0.05.Table 5: FR CLIR - t-values according % MRR reduction indexCL-IT-Sh CL-FR-LgTitle index-0.05-1.77ASR index-1.58-2.04*Desc index-0.32-0.47*Statistically significant values p-value < 0.05.CLIR. confirms expected additional retrieval challenge arisesimperfect query translation. MRR Arabic queries reduced higher degreeFrench Italian queries. likely due relative difficulty ArabicMT (Alqudsi, Omar, & Shaker, 2012). One significant challenge Arabic EnglishMT relates named entities. instance, query including word dreamweaver (theproprietary web development tool) expressed dreamweaver FR IT,AR, represented QP Y@ resulted OOV termGoogle Translate transliterated completely different word Aldirimovruseful retrieval using English language metadata.Further, looking reduction MRR index indicates different responses query translation; notably impact greatest index ASRtranscript field across languages using short long queries.better understand significance CLIR reductions MRR, computedstatistical significance reduction. calculated t-value difference95% confidence level representing monolingual CLIR MRRs pairs everyquery level. significance test results terms t-values indexes searchedArabic CLIR queries shown Table 4 French Italian CLIR queriesTable 5. Looking t-values, observe queries less challengingothers since performance significantly different monolingual.Furthermore, Tables 4 5 indicate ASR transcripts indeedlowest robustness CLIR setting. searching single-field indexes,long short queries, ASR index least robustness statistically significantnegative reduction Arabic French (p<0.05). Italian short queries,MRR reduction rates ASR index (ASR index) statistically significant,still highest negative impact fields.conclude experiment even incomplete, short and/or sometimes unreliable, user-uploaded titles meta descriptions robust CLIRsetting ASR fields. noted earlier, degree ASR recognition errors mayvary one video another Internet, due wide variation audio quality.260fiCross-Lingual Search User-Generated Internet Videointeraction recognition error rate, document length retrieval behaviourhighly complex, observed Eskevich Jones (2014), plan exploreeffect detail future work view improving CLIR robustnessASR transcript field.6. Retrieval Combined Metadata Fieldsexamined effectiveness three separate fields monolingual retrievalCLIR, section explore potential combining improving retrievaleffectiveness. investigation, carried another set experiments combinedevidence individual fields. this, first combine fields pairs,shown Figure 1, integrate three fields varied field weighting.6.1 Retrieval Modelcombined field experiments use DFR PL2F model5 (Macdonald, Plachouras,He, Lioma, & Ounis, 2006). modified version PL2 model usedprevious section. PL2F model designed adopt per-field weighting combiningmultiple evidence fields single index search. term frequencies documentfields normalised separately combined weighted sum. PL2F usesdocument scoring function PL2, shown Equation 2, tf n weighted sumnormalised term frequencies normalised term frequencies tfX field x,case x (ASR, title, desc) indicated Equation 4. lx lengthfield x document d. avglx average length field x across documents,cx , wx per-field normalization parameters. per-field normalization featurePL2 modifies standard PL2 document scoring function include weighted sumnormalised term frequencies tfx .tfn =X(wx .tfx . log2 (1 + cx .xavglx)), (cx > 0)lx(4)tfx also needs two parameters wx , cx set. Hence, scoring indexed documentneed set parameters:Cx set per-field length normalization parameters cx need setevery field Cx ={ c asr, c title, c desc}, Wx set per-field boost factors wxneed set field Wx ={ w asr, w title, w desc}.6.2 Two Field CombinationsTable 6 shows MRR values fields combined pairs indexed usingPL2F retrieval model. interested potential improved retrieval usingfields combination. Comparing results Table 6 earlier results shownTable 3, see field combination effective monolingual CLIRtasks. improvement could probably obtained weighting fields differently.5. Terrier implementation model found http://terrier.org/docs/v4.0/javadoc/org/terrier/matching/models/PL2F.html261fiKhwileh, Ganguly, & JonesTable 6: Mono vs. CLIR performance field pair combinationsTitleDesc indexASRDesc indexASRTitle indexMn-Sh0.25030.43940.4295CL-AR-Sh0.24210.36240.3676CL-IT-Sh0.24630.39510.3820Mn-Lg0.30200.52450.4527CL-AR-Lg0.27950.39050.3451CL-FR-Lg0.26140.43260.3768Table 7: Weighting scheme Wx single-weighted retrieval modelsPL2ASRPL2TitlePL2DescASRwx11Title1wx1Desc11wxHowever, main goal investigation potential combining three fields,explore detail next section.6.3 Three Field Combinationssection describe investigation retrieval effectiveness combinationthree fields. explore giving higher weight specific field others.set values proposed single-weighted retrieval models adoptedfollowing steps:Construct model based using PL2F document scoring targets single fieldx (ASR, title, desc): PL2FASR, PL2Title, PL2Desc.Assign equal cx value fields allow full-length normalization termfrequency field Cx = {1,1,1} short queries, Cx ={7,7,7} longqueries. also followed empirically standard settings recommended AmatiVan Rijsbergen (2002), Ounis (2007b).Wx , set wx value targeted field, rest fixed 1,give priority field x others, Wx = {wx ,1,1}. reasonchose 1 allow presence term frequencies,normal (is boosted) weights.combination weighting schemes shown Table 7, case one fieldweight boost wx . examine retrieval behaviour, vary wx boost parametersmodel range 1 60 using increments 1. first weighting iterationweighting point wx = 1 models Wx = {1,1,1}.6.3.1 Experimental Results DiscussionFigure 3 shows MRR performance weighting point long queries (CL-ARLg CL-FR-Lg query sets), short queries (CL-AR-Sh CL-IT-Sh query sets).seen Figure 3, fields behave differently weight boosting. best CLIR262fiCross-Lingual Search User-Generated Internet VideoFigure 3: MRR CLIR performance single weighted models across weighting points(wx) using short long query sets.precision performance always achieved giving higher weight title fieldAR, FR query sets. Across weighting points languages pairs,PL2Title model shows higher performance fields short longquery sets.Moreover, also seen figures, get lower performancegive progressively higher weights ASR Desc fields. strong CLIR performancePL2Title model indicates stability title fields Internet videosfields. Also, fact titles may written video uploaderattention descriptions could referred following reasons:uploader thought important high quality title video sincewould help promoting video-sharing site.uploader believed importance since shown headervideo, description generally shown video mayexamined viewer.263fiKhwileh, Ganguly, & Jonescould also case known-item queries, users wrote queriesviewed videos might likely include titles videosquery find intended video, believe would easier findusing title video. However, noted MTurk taskused create queries Search Hyperlinking Mediaeval task displayvideo title user writing query created intentionsuitable re-find known-item video.Table 8: Single index Mono vs. CLIR Recall performance represented numberfound documents cut-off values 10, 50, 100.Title indexASR indexDesc indexTitleDesc indexASRDesc indexASRTitle indexIndexMn-Sh19351720353537Title indexASR indexDesc indexTitleDesc indexASRDesc indexASRTitle indexIndexMn-Sh25422933474247Title indexASR indexDesc indexTitleDesc indexASRDesc indexASRTitle indexIndex2546343850465010 - cutCL-AR-Sh CL-IT-Sh161931341515171930353134333750 - cutCL-AR-Sh CL-IT-Sh2323384223272830414438424047100 - cut2324414527313235474941454550Mn-Lg23342325403440CL-AR-Lg18292121322833CL-FR-Lg19292022333834Mn-Lg30433137464348CL-AR-Lg28372729403641CL-FR-Lg26392831434045324734425046522940323543394329433238474249Comparing MRR PL2Title values shown Table 3, also seenperformance PL2Title almost double one obtained independentTitle field (Title index). MRR values ASR Desc fields similartwo experiments. wx increases Title field, seeimprovement, optimal weight depending query lengthlanguage pair. attempt better understand field combination improvesretrieval effectiveness, examined Recall individual fields combinations.264fiCross-Lingual Search User-Generated Internet VideoTable 8 shows total number known-items retrieved top 10, 50 100field set. seen Title field lowest recall isolation,boost Recall fields used combination. results Figure 3suggest title field brings additional evidence without bringing noise,case Desc ASR fields degrade effectiveness weight increased.7. Query Expansion Using Combined Metadata FieldsSections 5 6, analyzed effectiveness data source (ASR, Titledescription) CLIR framework showed overall performance robustfields combined together optimal way. also showed adjustingretrieval settings give higher weight reliable data source, i.e. Title,comparison less reliable fields, benefit overall CLIR performance.section, seek modify query using field information improveretrieval effectiveness. query modification strategy explore based queryexpansion (QE) techniques (Carpineto & Romano, 2012), use different sourcesevidence (fields) enrich original query. underlying motivation behind applyingQE expanding query include important terms make effectiveidentifying relevant items. Ideally, QE alleviate impact translationerrors adding terms top ranked videos either relevant relatedquery. QE effectiveness relies quality informativeness top rankeddocuments (Amati, Carpineto, & Romano, 2004). Top ranked documents takenexternal resources WordNet Wikipedia (Pal, Mitra, & Datta, 2013)local document collection considering top ranking documents relevantquery. consider interesting/challenging approach here, localcollection expansion approach since aim investigate noisy collection webvideos utilized improve query effectiveness. We, nevertheless, plan consideruse external collections QE future investigation.Existing research shown QE techniques useful improving monolingual CLIR effectiveness many languages (Bellaachia & Amor-Tijani, 2008). However, research focused primarily collections professionally writtenformal text none minimum amount noise. some,limited, work applying QE techniques noisy data, work OCRData (Tong, Zhai, Milic-Frayling, & Evans, 1996; Lam-Adesina & Jones, 2006),recently, user-generated informal text (Lee & Croft, 2014). section,interested taking challenge applying QE CLIR settings focus avoidingproblems may arise noise presented source evidence; particulartask, translation errors query, transcription errors ASR wellinconsistency errors user generated textual metadata. explore expansionqueries based top ranked documents. Expansion terms intended makequery reliable robust find intended relevant item. interestsection summarized help following research questions:expand query using top ranked documents, might affectoverall CLIR effectiveness? effective QE setting noisy datacollected Internet videos be?265fiKhwileh, Ganguly, & Jonesdifferent UGC information sources useful QE? Sincedifferent sources (fields) different relative characteristics behaviour retrieval, observed empirically Sections 5 6.describe approach addressing questions following sections.investigate reliability single field QE challenges Section 7.1.propose adaptive approach improve overall QE robustness6 selecting bestsource expansion Section 7.2.7.1 Query Expansion Fieldsemploy Divergence Randomness (DFR) QE mechanism proposed Gianni(2003). technique computes weight rank terms top ranking documents. DFR QE generalizes Rocchios method (Salton & Buckley, 1997) implementseveral term weighting models measure informativeness term pseudorelevant set.DFR QE two stages. First, applies DFR term weighting model measureinformativeness top terms top ranking document. main conceptDFR term weighting model infer informativeness term divergencedistribution top documents random distribution. use DFR weightingmodel called Bo1, parameter-free DFR model uses BoseEinstein statistics weightterm based informativeness. parameter free model widely usedproven effective (He & Ounis, 2007a; Plachouras, He, & Ounis, 2004; Gianni,2003). weight w term top ranked documents using DFR Bo1 modelshown Equation 5, tfx frequency term pseudo-relevant set (topn ranked documents). Pn given F/N ; F term frequency query termwhole collection N number documents whole collection.w(t) = tfx . log2 (1 + Pn) + log2 (1 + Pn )Pn(5)Secondly, query term weight qtw , obtained single-pass retrieval (asdescribed Equation 2), adjusted according newly obtained weightingvalues w(t) newly extracted terms original ones using Equation 6,wmax (t) indicated maximum w(t) values among expanded query terms.qtw = qtw +w(t)wmax (t)(6)illustrate QE approach used experiments, provide QE exampleCLIR-AR query :EEE PC 900 Troubleshooting laptopterms pc, laptop, mac, us, classrooms generated running DFR QEtake top 5 terms top-5 documents. Note two expansion terms pclaptop also appear original query, therefore, weight terms6. QE robustness interpreted context likely improve retrieval performancebaseline, baseline single-pass retrieval using original query.266fiCross-Lingual Search User-Generated Internet Videoboosted greater 17 . new expansion terms (mac, us classrooms)added original query weights adjusted based informativenessuniqueness top n documents versus whole collection. term macpredicted informative unique gets weight greater 0 since appearstop n documents terms (classrooms, us) assigned low weightsclose 0 since also appear documents (non top-n). Using method,final expanding reweighing query explained follows :eee1.000, pc1.9211, 9001.0000 ,troubleshoot1.0000, laptop1.2988, mac0.2195,us0.0000, classroom0.0000.original query terms may appear top-terms (900, EEEtroubleshoot), formula Equation 6 would giveweight single-pass retrieval settings. cases might commonCLIR settings due presence named entity translation errors, exampleAldirimovr described Section 5. Since NEEs produced translation,never appear top ranked documents since presentcollection, therefore weight always remain expansions.pose extra challenge overall QE effectiveness try handlefollowing sections designing post-translation QE tuned task. mainreasons us pick post-translation QE approach experimental investigation:study impact translation errors/translation quality QE effectiveness.investigated running post-translation analysis query expansionperformance.investigate whether adding new informative terms query reduceimpact translation errors improve retrieval effectiveness.use default QE parameter settings experiment setextract 10 informative terms top 3 returned documents. settingssuggested Gianni (2003) conducting extensive experiments several testcollections. However, case, task much challenging,one relevant document find, also extend settings explore possibleparameter combinations. parameter settings proposed QE runs tunedexplore top (3, 5, 10) terms top (3, 5, 7) documents. QE runsparameters combinations follows:Taking top 3 terms, includes QE runs take top 3 terms top3 documents, top 5 top 10 documents.Taking top 5 terms, includes QE runs take top 5 terms top3 documents, top 5 top 10 documents.Taking top 10 terms, includes QE runs take top 10 termstop 3 documents, top 5 top 10 documents.7. 1 normal weight term appears original query267fiKhwileh, Ganguly, & JonesTable 9: Optimized parameters (top-terms top-doc) selected QE runexp-ASRexp-Titleexp-Descexp-AllTerms5333Docs5333study best parameters field expansion, explored parameter variations. chose best performing setting QE run. optimized settingsshown Table 9.QE framework using fields monolingual text retrieval proposedOunis (2007a). suggests improved term-weighting method based field statisticsachieve better retrieval performance. investigation, adopt approachCLIR QE tune single-field QE technique allows us assesseffectiveness field QE. method, QE performed follows:top n terms extracted top n documents retrieved responseexecuting query separate field type (title, description ASR) orderinvestigate individual effectiveness QE.Retrieval carried similarly setting experiment Section 6,combine fields together (see Figure 2), give equal weight 1field. use PL2F model (described Section 6) retrieval experiment.Since query sets (long/short) led similar conclusions previous Sectionsregarding field effectiveness, chose run QE short queries (the CLIRqueries CL-AR-Sh CL-IT-Sh described Section 4.1).conducted several QE runs based using individual field combinations.reason tuned runs assess effectiveness field combinationQE. proposed field-based QE runs follows.exp-ASR: Queries expanded using ASR field only, taking topranking terms top documents retrieved ASR index.exp-Title: Queries expanded using Title field only, taking top rankingterms top documents retrieved Title index.exp-Desc: Queries expanded using Desc field only, taking top rankingterms top documents retrieved Desc index.exp-All : Queries expanded using combination fields, i.e. ASR, TitleDesc.exp-Non : run skips expansion queries single passretrieval using original query. Note use baseline since wantassess effective QE challenging task.268fiCross-Lingual Search User-Generated Internet VideoTable 10: MRR performance QE run.exp ASRexp Titleexp Descexpexp NonCL-AR-Sh0.35020.38200.34700.35710.3726CL-IT-Sh0.37350.40600.40900.4069.4081Table 11: Overall Recall (total found known-items) QE run.exp ASRexp Titleexp Descexpexp NonCL-AR-Sh5352515352CL-IT-Sh5756565656run field-based QE experiments explore MRR performance field.MRR performance across different runs shown Table 10, overall recallresults shown Table 11. seen MRR proposed runs (exp-All,exp-ASR, exp-Title, exp-Desc) improve baseline run (exp-Non). expTitle exp-Desc get less similar MRR performance, exp-ASR achievessignificantly lower MRR. fact multiple sources noise coming eithertranslation fields themselves, together fact known-itemtask, one relevant item may highly ranked initialsearch, justify ineffectiveness QE runs.better understand robustness QE run compare baseline,study difference MRR (MRR) query level proposed runsbaseline run (exp-Non). MRR particular query level QE run (exp-x)indicated MRR = (MRR(exp-x) - MRR(exp-Non)). MRR resultsCL-AR-Sh query across runs shown Figure 4, Figure 5 showsMRR results CL-AR-Sh queries.Since DFR QE model (see Equation 5 6) uses informativeness measureweight extracted top terms, decreases increases MRR valuesshown Figures 4 5 explained follows.MRR = 0, means top ranked terms predicted less informative,reason QE runs minimal effect baseline (exp-Non). BasedDFR definition informativeness (see Equation 5 6), indicatesterms added particular query given low weightcommon top ranked documents, also whole document collection.words, QE run could find helpful terms might potentiallyimprove overall performance. MRR performance run suggestssituation occasionally occurs QE runs across several queries, particularly269fiKhwileh, Ganguly, & Jonesexp-Title exp-Desc. probably arise due recall problemtwo fields have, shown Table 8 (see also single-field retrieval experimentsSection 5). fact, queries exp-Desc exp-Title runs,application QE low zero effect MRR comparedsingle-pass baseline retrieval (exp-Non).MRR > 0, means top ranked terms predicted highly informativerelevant query, reason QE shows positiveincrease baseline. turn suggests exp-ASR runs ableimprove queries (more positive MRR points) terms ranking knownitem runs. also expected due higher recall performancefields.1.01.00.0 0.5 1.0expASR0.0 0.5 1.0expAll01020304050600103040506050601.01.00.0 0.5 1.0expDesc0.0 0.5 1.0expTitle200102030405060010203040Figure 4: MRR across QE runs CL-AR-Sh.Moreover, runs positive MRR queries. Particularly interesting fact combined QE run (exp-All) yield best results.MRR < 0, means top ranked terms predicted highly informativerelevant query, reason QE negativeeffect baseline run. MRR values show incorrect predictionsignificantly impacted runs. particular, exp-ASR run affectedmost, seen negative MRR values across several queriesCL-AR-Sh CL-IT-Sh.conclude experiments setting expansion based TitleDesc fields may help improve MRR improving ranking queries.However, improvement covers limited number queries due coverage270fiCross-Lingual Search User-Generated Internet Video1.01.00.0 0.5 1.0expASR0.0 0.5 1.0expAll01020304050600103040506050601.01.00.0 0.5 1.0expDesc0.0 0.5 1.0expTitle200102030405060010203040Figure 5: MRR across QE runs CL-AR-Sh.recall issues two fields (see Section 5). ASR fields seem better coverageterms improving performance queries. However, coveragenegative sometimes QE model fails pick right terms.Even ASR combined fields using exp-All approach, overallperformance still improve MRR respect baseline (exp-Non).shown experiments, QE run uses single field shows positive MRRvalues queries. However, per-field improvements decrease averagefields combined together (for example, exp-All MRR values shown Table10 lower exp-Title CL-AR-Sh, exp-Desc CL-IT-Sh). combinationapproach sufficiently effective overall retrieval performance stillnegatively impacted noise present fields. next section, proposenovel approach alleviating issue selecting best source expansion.technique adaptively predicts performance QE run, chooses sourcelikely achieve positive impact, elaborated next section.7.2 Selecting Best Source Expansionsection introduce proposed approach improve QE effectivenessCLIR task predicting whether QE needed not, is, select best sourceexpansion. particular, aim design robust QE approach preventminimize negative MRR changes exp-ASR runs have. arguereductions caused two reasons:query performance intiail run perfect known-item task.case happen known relevant document ranked first position.271fiKhwileh, Ganguly, & Jonesadded new expansion terms potentially disturb query performancenegatively impact retrieval effectiveness.mentioned literature (Mitra, Singhal, & Buckley, 1998; Terra & Warren,2005), QE effectiveness challenged query drift issue expansion terms informative, belong topic originalquery. shown Table 1, ASR transcripts 20K length, Desclong 3K length. long descriptive fields may cover many different topicsnoise, may relevant topic query. commondataset used task, since using user-generated videos length,topic ASR quality video may specific consistent theme.address issues propose modified QE technique use pre-retrievalprediction technique decide whether QE likely beneficial sourcereliable expansion. describe prediction technique used approachSection 7.2.1. Section 7.2.2 explains proposed Adaptive QE algorithm. Section 7.2.3reports experiments conduct investigate effectiveness approach.7.2.1 Predicting QE EffectivenessPredicting QE effectiveness proposed within concept selective QE framework Amati et al. (2004), widely used improve QE effectiveness.main concept behind technique disable QE predicted negative impact first-pass retrieval performance. prediction based pre-retrievalmetrics assess application QE make decision whether apply not.predictions based capturing query statistics collection,query difficulty (Gianni, 2003), query clarity score (Cronen-Townsend, Zhou, &Croft, 2002), query length (Amati et al., 2004). Full analysis effectiveness everyprediction techniques effectiveness contained work Ounis (2006).chose use one successful predictors, Average Inverse Collection TermFrequency (AvICTF) (He & Ounis, 2006). use prediction metric proposedAdaptive QE technique predict whether QE needed field combinationreliable expansion.AvICTF predictor previously tested field-based query expansionseveral previous studies (He & Ounis, 2007a; Macdonald, He, Plachouras, & Ounis,2005). reason choose predictor others definition higherpotential work well CLIR addresses term frequency aspect query.principle, term frequency good indicator predicting translation errors.example NEE error like word Aldirimovr would zero term frequencyused give indication overall query performance. AvICTF also lowcost metric uses local collection statistics make prediction, AvICTFvalue higher tuned threshold, query predicted preform wellusing first-pass retrieval therefore query expansion disabled. AvICTFdefined follows.Qcolllog2 Q ( token)FAvICT F =(7)ql272fiCross-Lingual Search User-Generated Internet Videotokencoll number tokens whole collection. ql query length, Fquery term frequency whole collection. next sections, describeadaptive QE algorithm implemented experimental investigation.7.2.2 Adaptive Field-Based QE Techniquesection propose adaptive QE technique, adaptivity concept inspiredwork Ounis (2006). idea behind adaptive method automaticallyset weights fields query running time. redesign conceptauto-select field combination best source expansion. adaptivefield-based QE algorithm implemented explained below:indexing, algorithm processes possible fields combination separateindexes, index one, two n fields, n total number fieldsavailable collection.retrieval query Q:1. algorithm calculates prediction value V across every possible index,selects index axindex highest prediction value axV .2. algorithm makes decision whether apply QE comparing prediction value axV trained threshold. predictedvalue less threshold, algorithm skips QE moves straightretrieval.3. prediction decision proceed QE, algorithm expandsquery Q taking top terms top ranked documents axindexrun retrieval.adaptive QE algorithm predicts effectiveness possible QE field runchooses one likely useful query. words, adaptiveQE make use (exp-ASR, exp-Title, exp-Desc, exp-Non, exp-All) wellpossible QE runs based two fields combination (exp-ASRDesc, expTitleASR, exp-TitleDesc) produce adaptive QE runs follows.exp-Adapt-AR: Expands AR queries (CL-AR-Sh) using proposed adaptiveQE technique.exp-Adapt-IT: Expands queries (CL-IT-Sh) using proposed adaptive QEtechnique.training prediction tuning threshold value, conduct two-fold holdoutevaluation query sets. divide CL-AR-Sh CL-IT-Sh sets trainingtesting query sets described below:AR-train queries set: Contains 30 queries picked randomly CL-AR-Sh set.IT-train queries set: Contains 30 queries picked randomly CL-IT-Sh set.273fiKhwileh, Ganguly, & JonesTest queries set: Contains remaining 30 queries CL-AR-Sh query setremaining 30 queries CL-IT-Sh query set. two sets used evaluateproposed adaptive QE runs (exp-Adapt-AR exp-Adapt-IT).training, similar work Ounis (2007a), perform manualdata sweeping range [3, 15] interval 1 training queries.best threshold chosen exp-adapt-AR AR-train queries 6. best thresholdfound exp-adapt-IT IT-train queries 9. difference thresholdvalues attributed distinct level translation qualities querysets. discussed previously Section 5, queries better CLIR performanceAR ones, thus QE threshold different here.7.2.3 Experimental Results Discussionran proposed adaptive QE using two runs: exp-Adapt-AR exp-Adapt-ITexplained previously. overall performance results two runs test queriesshown Table 12.explained before, adaptive QE runs involve possible QE runs onesingle run, use one predicted better performance, selectionstatistics run indicated Table 13. seen Table 12, overall MRRTable 12: Results adaptive QE runs terms MRR Recall.exp-Adapt-ARexp-Adapt-ITexp-Adapt-ARexp-Adapt-ITMRRAdaptive0.436140.4867RecallExp-Adapt2222(Baseline exp-Non)0.37850.4580Exp-Non (Baseline)2222Table 13: selection statistics adaptive QE runs.exp-ASRDescexp-TitleASRexp-TitleDescexp-Titleexp-Descexp-ASRexp-Nonexp-Allexp-Adapt-AR47412354exp-Adapt-IT67520280performance improves using proposed adaptive QE technique. Also, lookingrecall performance Table 12, appears technique successfully improves274fiCross-Lingual Search User-Generated Internet Video0.50.00.51.01.00.50.00.51.0expAdaptAR1.0expAdaptIT051015202530051015202530Figure 6: MRR (Baseline Vs Adaptive QE runs) languages using test queriesoverall ranking maintaining recall level. attributed factruns able selectively make use best performance individualrun.better understand improvement baseline, similar previous experiment Section 7.1, also calculate MRR values test queryAR-train IT-train query tests Figure 6. MRR values Figure 6 showtechnique reduces chance significant reduction MRR baseline. Overall, appears QE approach robustness improve overall CLIRperformance selecting reliable source expansion individually query.8. Conclusions Researchpaper examined CLVR based text metadata fields Arabic-English, FrenchEnglish Italian-English known-item search task based blip10000 collection.studied retrieval effectiveness challenges three different sources information:ASR transcripts, challenged recognition errors, video titles,short lack content, video descriptions, generic incomplete.first set experiments analysed behaviour sources CLIR examiningCLIR robustness. found ASR transcript field lowest robustness acrossfields performance drop significantly CLIR. explored fieldcombination retrieval explore performance together, investigation showedgiving higher weight titles fields gives improved CLIR performance.general experiments show tuning retrieval settings give higher weighttowards fields lower CLIR robustness degrades retrieval effectiveness.275fiKhwileh, Ganguly, & Joneswork also investigated effectiveness automatic query expansion CLIRsetting task. found information sources varying reliabilityquery expansion, negative impact retrieval effectivenessCLIR framework combined together. proposed adaptive query expansion technique automatically selects reliable source expansion basedwell established query performance prediction technique. results experimental investigation show technique better robustness maintain retrievalperformance CLIR setting.general, found noisy CLIR setting, translation errors,transcription errors, incorrect incomplete UGC metadata varied documentlengths, might single best solution recommended usedanswering queries even expanding/ improving them, rather adaptive approachrelies trained heuristics tuned specifically task collection.concept adaptivity studied next investigations problem.analysis CLIR effectiveness UGC video gives us suggestionsinvestigation many areas. One potential direction work automaticallyassess quality ASR transcripts Description information assign weightsbased quality measures, also explore task dependent tuning machine translation process. Studying CLIR effectiveness UGC sources evidence (suchvisual information social interaction data include tweets, personal profile data)would interesting follow investigation. Another area future study improvedocument representation within framework developing document expansiontechnique tuned improve overall CLIR robustness effectivenesssource evidence. Also, plan expand work studying effectivenessavailable CLIR techniques Hybrid DT CLIR task.ReferencesAlqudsi, A., Omar, N., & Shaker, K. (2012). Arabic machine translation: survey. ArtificialIntelligence Review, 124.Alzghool, M., & Inkpen, D. (2008). Cluster-based model fusion spontaneous speechretrieval. Proceedings ACM SIGIR Workshop Searching SpontaneousConversational Speech, pp. 410. Citeseer.Amati, G., Carpineto, C., & Romano, G. (2004). Query difficulty, robustness, selectiveapplication query expansion. Advances information retrieval, pp. 127137.Springer.Amati, G., & Van Rijsbergen, C. J. (2002). Probabilistic models information retrievalbased measuring divergence randomness. ACM Transactions Information Systems (TOIS), 20 (4), 357389.Amazon (2015). Amazon mechanical turk - welcome. https://www.mturk.com/. Retrieved:2015-03-30.Bagdouri, M., Oard, D. W., & Castelli, V. (2014). Clir informal content arabic forumposts. Proceedings 23rd ACM International Conference ConferenceInformation Knowledge Management, pp. 18111814. ACM.276fiCross-Lingual Search User-Generated Internet VideoBallesteros, L., & Croft, W. B. (1997). Phrasal translation query expansion techniquescross-language information retrieval. ACM SIGIR Forum, Vol. 31, pp. 8491.ACM.Ballesteros, L., & Croft, W. B. (1998). Resolving ambiguity cross-language retrieval.Proceedings 21st annual international ACM SIGIR conference Researchdevelopment information retrieval, pp. 6471. ACM.Bellaachia, A., & Amor-Tijani, G. (2008). Enhanced query expansion english-arabicclir. Database Expert Systems Application, 2008. DEXA08. 19th InternationalWorkshop on, pp. 6166. IEEE.Bendersky, M., Garcia-Pueyo, L., Harmsen, J., Josifovski, V., & Lepikhin, D. (2014).next: retrieval methods large scale related video suggestion. Proceedings20th ACM SIGKDD international conference Knowledge discovery datamining, pp. 17691778. ACM.Bing (2015). Bing translator. http://www.bing.com/translator/. Retrieved: 2015-03-30.BlipTV (2015). Bliptv. https://www.blip.tv. Retrieved: 2015-03-30.Buttcher, S., Clarke, C. L., & Cormack, G. V. (2010). Information retrieval: Implementingevaluating search engines. Mit Press.Carpineto, C., & Romano, G. (2012). survey automatic query expansion informationretrieval. ACM Computing Surveys (CSUR), 44 (1), 1.Chelba, C., Bikel, D., Shugrina, M., Nguyen, P., & Kumar, S. (2012). Large scale languagemodeling automatic speech recognition. arXiv preprint arXiv:1210.8440.Chen, H.-H., Hueng, S.-J., Ding, Y.-W., & Tsai, S.-C. (1998). Proper name translationcross-language information retrieval. Proceedings 17th international conference Computational linguistics-Volume 1, pp. 232236. Association Computational Linguistics.CLEF (2015a). clef initiative (conference labs evaluation forum) - clef2009.http://www.clef-initiative.eu/edition/clef2009. Retrieved: 2015-03-30.CLEF (2015b). clef initiative (conference labs evaluation forum) - homepage.http://www.clef-initiative.eu/. Retrieved: 2015-03-30.Cronen-Townsend, S., Zhou, Y., & Croft, W. B. (2002). Predicting query performance.Proceedings 25th annual international ACM SIGIR conference Researchdevelopment information retrieval, pp. 299306. ACM.Eickhoff, C., Li, W., & de Vries, A. P. (2013). Exploiting user comments audio-visualcontent indexing retrieval. Advances Information Retrieval, pp. 3849.Springer.Eskevich, M. (2014). Towards effective retrieval spontaneous conversational spoken content. Ph.D. thesis, Dublin City University.Eskevich, M., & Jones, G. J. F. (2014). Exploring speech retrieval meetings usingami corpus. Computer Speech & Language.277fiKhwileh, Ganguly, & JonesEskevich, M., Jones, G. J. F., Chen, S., Aly, R., Ordelman, R., & Larson, M. (2012a).Search hyperlinking task mediaeval 2012..Eskevich, M., Jones, G. J., Wartena, C., Larson, M., Aly, R., Verschoor, T., & Ordelman,R. (2012b). Comparing retrieval effectiveness alternative content segmentationmethods internet video search. Content-Based Multimedia Indexing (CBMI),2012 10th International Workshop on, pp. 16. IEEE.Facebook video (2015). Facebook. https://www.facebook.com/facebook/videos. Retrieved: 2015-03-30.Federico, M., Bertoldi, N., Levow, G.-A., & Jones, G. J. F. (2005). Clef 2004 cross-languagespoken document retrieval track. Multilingual Information Access Text, SpeechImages, pp. 816820. Springer.Federico, M., & Jones, G. J. F. (2004). clef 2003 cross-language spoken document retrieval track. Comparative Evaluation Multilingual Information Access Systems,pp. 646652. Springer.Filippova, K., & Hall, K. B. (2011). Improved video categorization text metadatauser comments. Proceedings 34th international ACM SIGIR conferenceResearch development Information Retrieval, pp. 835842. ACM.Gao, J., Nie, J.-Y., Xun, E., Zhang, J., Zhou, M., & Huang, C. (2001). Improving querytranslation cross-language information retrieval using statistical models. Proceedings 24th annual international ACM SIGIR conference Researchdevelopment information retrieval, pp. 96104. ACM.Gianni, A. (2003). Probabilistic Models Information Retrieval based DivergenceRandomness. Ph.D. thesis, Department Computing Science, University Glasgow.Google (2015). Google translate. https://translate.google.com/. Retrieved: 2015-0330.He, B., & Ounis, I. (2006). Query performance prediction. Information Systems, 31 (7),585594.He, B., & Ounis, I. (2007a). Combining fields query expansion adaptive queryexpansion. Information processing & management, 43 (5), 12941307.He, B., & Ounis, I. (2007b). setting hyper-parameters term frequency normalization information retrieval. ACM Transactions Information Systems (TOIS),25 (3), 13.Herbert, B., Szarvas, G., & Gurevych, I. (2011). Combining query translation techniquesimprove cross-language information retrieval. Advances Information Retrieval,pp. 712715. Springer.Inkpen, D., Alzghool, M., Jones, G. J. F., & Oard, D. W. (2006). Investigating crosslanguage speech retrieval spontaneous conversational speech collection. Proceedings Human Language Technology Conference NAACL, CompanionVolume: Short Papers, pp. 6164. Association Computational Linguistics.278fiCross-Lingual Search User-Generated Internet VideoJones, G. J., Zhang, K., Newman, E., & Lam-Adesina, A. M. (2007). Examiningcontributions automatic speech transcriptions metadata sources searchingspontaneous conversational speech..Kishida, K., & Kando, N. (2006). hybrid approach query document translationusing pivot language cross-language information retrieval. Springer.Lam-Adesina, A. M., & Jones, G. J. (2006). Using string comparison context improvedrelevance feedback different text media. String Processing InformationRetrieval, pp. 229241. Springer.Langlois, T., Chambel, T., Oliveira, E., Carvalho, P., Marques, G., & Falcao, A. (2010).Virus: video information retrieval using subtitles. Proceedings 14th International Academic MindTrek Conference: Envisioning Future Media Environments, pp.197200. ACM.Larson, M., Newman, E., & Jones, G. J. F. (2009). Overview videoclef 2008: Automaticgeneration topic-based feeds dual language audio-visual content. EvaluatingSystems Multilingual Multimodal Information Access, pp. 906917. Springer.Larson, M., Newman, E., & Jones, G. J. F. (2010). Overview videoclef 2009: New perspectives speech-based multimedia content enrichment. Multilingual InformationAccess Evaluation II. Multimedia Experiments, pp. 354368. Springer.Lee, C.-J., Chen, C.-H., Kao, S.-H., & Cheng, P.-J. (2010). translate translate?.Proceedings 33rd international ACM SIGIR conference Researchdevelopment information retrieval, pp. 651658. ACM.Lee, C.-J., & Croft, W. B. (2014). Cross-language pseudo-relevance feedback techniquesinformal text. Advances Information Retrieval, pp. 260272. Springer.Leveling, J., Zhou, D., Jones, G. J., & Wade, V. (2010). Document expansion, querytranslation language modeling ad-hoc ir. Multilingual Information AccessEvaluation I. Text Retrieval Experiments, pp. 5861. Springer.Macdonald, C., He, B., Plachouras, V., & Ounis, I. (2005). University glasgow trec2005: Experiments terabyte enterprise tracks terrier.. TREC.Macdonald, C., Plachouras, V., He, B., Lioma, C., & Ounis, I. (2006). University Glasgow WebCLEF 2005: Experiments per-field normalisation language specificstemming. Springer.Magdy, W., & Jones, G. J. (2014). Studying machine translation technologies largedata clir tasks: patent prior-art search case study. Information Retrieval, 17 (5-6),492519.McCarley, J. S. (1999). translate documents queries cross-languageinformation retrieval?. Proceedings 37th annual meeting AssociationComputational Linguistics Computational Linguistics, pp. 208214. AssociationComputational Linguistics.MediaEval (2015). MediaEval Benchmarking Initiative Multimedia Evaluation. http://www.multimediaeval.org/. Retrieved: 2015-03-30.279fiKhwileh, Ganguly, & JonesMitra, M., Singhal, A., & Buckley, C. (1998). Improving automatic query expansion.Proceedings 21st annual international ACM SIGIR conference Researchdevelopment information retrieval, pp. 206214. ACM.Naaman, M. (2012). Social multimedia: highlighting opportunities search miningmultimedia data social media applications. Multimedia Tools Applications,56 (1), 934.Nikoulina, V., Kovachev, B., Lagos, N., & Monz, C. (2012). Adaptation statistical machine translation model cross-lingual information retrieval service context.Proceedings 13th Conference European Chapter AssociationComputational Linguistics, pp. 109119. Association Computational Linguistics.Oard, D. W., & Diekema, A. R. (1998). Cross-language information retrieval. Annual reviewinformation science technology, 33, 223256.Oard, D. W., & Hackett, P. G. (1998). Document translation cross-language text retrieval university maryland. Information Technology: Sixth TextREtrieval Conference (TREC-6), pp. 687696. US Dept. Commerce, TechnologyAdministration, National Institute Standards Technology.Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., Pecina, P., Soergel, D., Huang,X., & Shafran, I. (2007). Overview clef-2006 cross-language speech retrievaltrack. Evaluation multilingual multi-modal information retrieval, pp. 744758. Springer.Over, P., Awad, G. M., Fiscus, J., Antonishek, B., Michel, M., Smeaton, A. F., Kraaij, W.,& Quenot, G. (2011). Trecvid 2010an overview goals, tasks, data, evaluationmechanisms, metrics..Pal, D., Mitra, M., & Datta, K. (2013). Improving query expansion using wordnet. CoRR,abs/1309.4938.Parton, K., McKeown, K. R., Allan, J., & Henestroza, E. (2008). Simultaneous multilingual search translingual information retrieval. Proceedings 17th ACMconference Information knowledge management, pp. 719728. ACM.Pecina, P., Hoffmannova, P., Jones, G. J., Zhang, Y., & Oard, D. W. (2008). Overviewclef-2007 cross-language speech retrieval track. Advances MultilingualMultimodal Information Retrieval, pp. 674686. Springer.Peters, C., Braschler, M., & Clough, P. (2012). Multilingual information retrieval:research practice. Springer Science & Business Media.Pirkola, A., Hedlund, T., Keskustalo, H., & Jarvelin, K. (2001). Dictionary-based crosslanguage information retrieval: Problems, methods, research findings. Informationretrieval, 4 (3-4), 209230.Plachouras, V., He, B., & Ounis, I. (2004). University glasgow trec 2004: Experimentsweb, robust, terabyte tracks terrier.. TREC.Rogati, M., & Yang, Y. (2002). Cross-lingual pseudo-relevance feedback using comparablecorpus. Evaluation Cross-Language Information Retrieval Systems, pp. 151157.Springer.280fiCross-Lingual Search User-Generated Internet VideoSalton, G., & Buckley, C. (1997). Improving retrieval performance relevance feedback.Readings information retrieval, 24 (5), 355363.Schmiedeke, S., Xu, P., Ferrane, I., Eskevich, M., Kofler, C., Larson, M. A., Esteve, Y.,Lamel, L., Jones, G. J. F., & Sikora, T. (2013). Blip10000: social video datasetcontaining spug content tagging retrieval. Proceedings 4th ACMMultimedia Systems Conference, pp. 96101. ACM.Sokolov, A., Hieber, F., & Riezler, S. (2014). Learning translate queries clir. Proceedings 37th international ACM SIGIR conference Research & developmentinformation retrieval, pp. 11791182. ACM.Terra, E., & Warren, R. (2005). Poison pills: harmful relevant documents feedback.Proceedings 14th ACM international conference Information knowledgemanagement, pp. 319320. ACM.Toderici, G., Aradhye, H., Pasca, M., Sbaiz, L., & Yagnik, J. (2010). Finding meaningyoutube: Tag recommendation category discovery. Computer VisionPattern Recognition (CVPR), 2010 IEEE Conference on, pp. 34473454. IEEE.Tong, X., Zhai, C., Milic-Frayling, N., & Evans, D. A. (1996). Ocr correction queryexpansion retrieval ocr dataclarit trec-5 confusion track report.. TREC.TRECVID (2015). Trec video retrieval evaluation home page. http://trecvid.nist.gov/.Retrieved: 2015-03-30.Varshney, S., & Bajpai, J. (2014). Improving performance english-hindi cross language information retrieval using transliteration query terms. arXiv preprintarXiv:1401.3510.White, R. W., Oard, D. W., Jones, G. J. F., Soergel, D., & Huang, X. (2006). OverviewCLEF-2005 cross-language speech retrieval track. Springer.Youtube (2015). Youtube. http://www.youtube.com/. Retrieved: 2015-03-30.YouTube Press (2015). Statistics - YouTube.statistics.html. Retrieved: 2015-03-30.http://www.youtube.com/yt/press/Zhou, D., Truran, M., Brailsford, T., Wade, V., & Ashman, H. (2012). Translation techniques cross-language information retrieval. ACM Computing Surveys (CSUR),45 (1), 1.281fiJournal Artificial Intelligence Research 55 (2016) 95-130Submitted 03/15; published 01/16Translation Alters SentimentSaif M. Mohammadsaif.mohammad@nrc-cnrc.gc.caNational Research Council CanadaMohammad Salamehmsalameh@ualberta.caUniversity AlbertaSvetlana Kiritchenkosvetlana.kiritchenko@nrc-cnrc.gc.caNational Research Council CanadaAbstractSentiment analysis research predominantly English texts. Thus existmany sentiment resources English, less languages. Approachesimprove sentiment analysis resource-poor focus language include: (a) translatefocus language text resource-rich language English, apply powerfulEnglish sentiment analysis system text, (b) translate resources sentimentlabeled corpora sentiment lexicons English focus language, useadditional resources focus-language sentiment analysis system. papersystematically examine options. use Arabic social media posts standin focus language text. show sentiment analysis English translationsArabic texts produces competitive results, w.r.t. Arabic sentiment analysis. showArabic sentiment analysis systems benefit use automatically translatedEnglish sentiment lexicons. also conduct manual annotation studies examinesentiment translation different sentiment source word text.especially relevant building better automatic translation systems. process,create state-of-the-art Arabic sentiment analysis system, new dialectal Arabic sentimentlexicon, first ArabicEnglish parallel corpus independently annotatedsentiment Arabic English speakers.1. Introductionterm sentiment analysis commonly used refer goal determiningvalence polarity piece text, whether positive, negative, neutral. However,generally refer determining ones attitude towards particular targettopic. Automatic sentiment analysis text, especially social media posts, numberapplications commerce, public health, public policy development. past twodecades, vast majority research English texts. Furthermore, many sentiment resources essential automatic sentiment analysis (e.g., sentiment lexicons) existEnglish. Thus growing need effective methods analyzing textlanguages Arabic Chinese, especially posts social media. improvementsstatistical machine translation systems last decade, longer relystrictly monolingual sentiment analysis systemsat least two alternatives mayviable:(a) Run English sentiment analysis system, using English resources, English translations focus language text.c2016National Research Council Canada. rights reserved.fiMohammad, Salameh, & Kiritchenko(b) Use focus-language sentiment analysis system employs focus-language resourcestranslations English resources focus language.paper systematically examine options. use Arabic social media postsspecific instance focus language text. use state-of-the-art Arabic Englishsentiment analysis systems well state-of-the-art Arabic-to-English English-toArabic translation systems. outline advantages disadvantagesmethods listed above, conduct quantitative qualitative experiments determineimpact translation sentiment. benchmarks use manually determined sentimentlabels Arabic posts.results help users determine methods best suited particular needs.Along way, answer several research questions as:1. sentiment prediction accuracy expected Arabic blog posts tweetstranslated English (using current state-of-art techniques), runstate-of-the-art English sentiment analysis system?2. performance compare current state-of-the-art Arabicsentiment system?3. loss sentiment predictability translating Arabic text Englishautomatically vs. manually?4. difficult humans determine sentiment text automatically translatedanother language native language?5. dealing translated text, accurate determining sentiment Arabic text: (1) automatic sentiment analysis translated text, (2)human annotation translated text sentiment?6. Arabic posts sentiment analysis systems benefit additional training dataautomatic translation sentiment-labeled English tweets additionalsentiment lexicons automatic translations existing English lexicons?7. automatic translations words sentiment associations original source words (as listed source language lexicons, say)? not,different reasons lead discrepancies?inferences drawn experiments necessarily apply language pairsArabicEnglish. Languages differ significantly terms characteristicsimpact sentiment. However, similar set experiments used languagepairs well determine impact translation sentiment.experiments two different datasets, show sentiment analysisEnglish translations Arabic texts produces competitive results, w.r.t. Arabic sentimentanalysis. also show translation (both manual automatic) introduces markedchanges sentiment carried text; positive negative texts often translatedtexts neutral. also find certain attributes automatically translatedtext mislead humans regards true sentiment source text, seemaffect automatic sentiment analysis system.show difficult obtain improvement Arabic sentiment analysissystems simply adding training data translation existing labeled English96fiHow Translation Alters Sentimentcorpus, systems benefit use automatically translated English sentimentlexicons. examining subset translated lexicon entries show close 90%entries valid even focus language. word automatic translation mayconvey sentiment poor translation quality wordtranslation used differently two languages.process developing experiments study translation impacts sentiment, created new dialectal Arabic sentiment lexicon state-of-the-art Arabicsentiment analysis system porting NRC-Canadas competition winning system (Mohammad, Kiritchenko, & Zhu, 2013; Kiritchenko, Zhu, & Mohammad, 2014b) Arabic.also created substantial amount sentiment labeled data pertaining Arabic socialmedia texts English translations. first resource textone language translations another language (both manually automaticallyproduced) manually labeled sentiment. sentiment lexiconssentiment-labeled corpora made freely available.1begin survey related work sentiment analysis English, sentiment analysis Arabic, work cross-lingual sentiment analysis (Section 2). Section 3present core method systematically study impact translation sentiment.Section 4 describe developed components needed experiments:translations Arabic texts English, translations English resources Arabic,sentiment-labeled data Arabic English, English sentiment analysis system,Arabic sentiment analysis system. Section 5, present results experimentstranslating focus language text English application English sentiment analysissystem. also conduct qualitative quantitative studies investigatereasons sentiment impacted translation. example, find sentimentexpressions often mistranslated neutral expressions, however automatic sentimentanalysis systems able recover extent errors. Section 6,present results experiments translating English resources Arabic usingfeatures drawn Arabic sentiment analysis. also describe manual annotation study extent automatic Arabic translations sentimentsource English words. Finally, present conclusions future directions Section 7.22. Related Worklast decade, explosion work exploring various aspectssentiment analysis English texts: detecting subjective objective sentences; classifyingsentences positive, negative, neutral; detecting person expressing sentimenttarget sentiment; applying sentiment analysis health, commerce,disaster management. Surveys Pang Lee (2008), Liu Zhang (2012),Mohammad (2016) give details many approaches. However, less workArabic texts. sub-sections below, briefly outline relevant sentiment analysisresearch English texts, Arabic texts, texts one language using resourcesanother (multilingual sentiment analysis).1. http://www.purl.org/net/ArabicSA2. early findings work first presented Salameh, Mohammad, Kiritchenko (2015).97fiMohammad, Salameh, & Kiritchenko2.1 Sentiment Analysis English Social MediaEnglish sentiment analysis systems applied many different kinds texts including customer reviews, newspaper headlines (Bellegarda, 2010), novels (Boucouvalas,2002; John, Boucouvalas, & Xu, 2006; Mohammad & Yang, 2011), emails (Liu, Lieberman,& Selker, 2003; Mohammad & Yang, 2011), blogs (Neviarouskaya, Prendinger, & Ishizuka,2011; Genereux & Evans, 2006; Mihalcea & Liu, 2006), tweets (Mohammad, 2012).Often systems cater specific needs text formality versusinformality, length utterances, etc. Sentiment analysis systems developed specificallytweets include Go, Bhayani, Huang (2009), Pak Paroubek (2010), Agarwal,Xie, Vovsha, Rambow, Passonneau (2011), Thelwall, Buckley, Paltoglou (2011),Brody Diakopoulos (2011), Aisopos, Papadakis, Tserpes, Varvarigou (2012), Bakliwal, Arora, Madhappan, Kapre, Singh, Varma (2012). survey Martnez-Camara,Martn-Valdivia, Urenalopez, Montejoraez (2012) provides overview researchsentiment analysis tweets. last two years, several shared tasks sentimentanalysis organized Conference Semantic Evaluation Exercises (SemEval),allowed comparison different approaches common datasets differentdomains (Wilson, Kozareva, Nakov, Rosenthal, Stoyanov, & Ritter, 2013; Rosenthal, Ritter,Nakov, & Stoyanov, 2014; Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos,& Manandhar, 2014). NRC-Canada system (Kiritchenko et al., 2014b) ranked firstcompetitions, use experiments. Notably, system makes extensiveuse sentiment lexicons handles negation appropriately.3 summarize systemSection 4.3.2.2 Sentiment Analysis Arabic Social MediaSentiment analysis Arabic social media texts several challenges. text oftenregional Arabic dialect rather Modern Standard Arabic (MSA). Unlike MSAstandardized form Arabic, dialectal Arabic spoken form Arabic lacksstrict writing standards. text often includes words languages Arabicmultiple scripts may used express Arabic foreign words. addition, Arabicmorphologically complex language. Negation MSA expressed negationparticles, dialects (Egyptian) expressed using circumfix.studies tackling sentiment analysis Arabic texts (Ahmad,Cheng, & Almas, 2006; Farra, Challita, Assi, & Hajj, 2010; Abdul-Mageed, Diab, & Korayem, 2011; Badaro, Baly, Hajj, Habash, & El-Hajj, 2014). also shared taskdetecting sentiment intensity Arabic phrases (Kiritchenko, Mohammad, & Salameh,2016).4 works closely related studies sentiment analysis Arabic social media (Al-Kabi, Gigieh, Alsmadi, Wahsheh, & Haidar, 2013; Ahmed, Pasquier,& Qadah, 2013; El-Beltagy & Ali, 2013; Mourad & Darwish, 2013; Abdul-Mageed, Diab,& Kubler, 2014). review existing Arabic sentiment analysis systems designed specifically Arabic social media datasets. Abdul-Mageed et al. (2014) trainedSVM classifier manually labeled dataset applied two-stage classification first3. Zhu, Guo, Mohammad, Kiritchenko (2014a) show impact negation cannot properlycaptured simply reversing polarity scope.4. http://alt.qcri.org/semeval2016/task7/98fiHow Translation Alters Sentimentseparates subjective objective sentences classifies subjective positivenegative instances. authors compiled several datasets multiple social mediaresources included chatroom messages, tweets, forum posts, Wikipedia Talk pages.datasets manually labeled two native Arabic speakers. However, resourcesmade publicly available yet. Abdul-Mageed Diab (2014) also used dataseveral resources compile build SANA, large-scale, multi-genre, multidialectlexical resource. SANA covers Egyptian Levantine dialects well MSA. Abbasi,Chen, Salem (2008) deployed Arabic morphological, syntactic stylistic featuressentiment analysis Arabic web forums. efficient feature selection, adoptedEntropy Weighted Genetic Algorithm (EWGA). Mourad Darwish (2013) trained SVMNaive Bayes classifiers Arabic tweets annotated two native Arabic speakers.compare systems performance Section 4.4.2.Refaee Rieser (2014b) manually annotated tweets sentiment two native Arabicspeakers. used SVM classify tweets two-stage approach: polar vs. neutral,positive vs. negative. test system dataset. However, datasetprovided superset data originally used experiments (Refaee &Rieser, 2014a). Thus, performances automatic sentiment analysis systems appliedtwo sets directly comparable.2.3 Multilingual Sentiment AnalysisWork multilingual sentiment analysis mainly addressed mapping sentiment resourcesEnglish morphologically complex languages. Mihalcea, Banea, Wiebe (2007)used English resources automatically generate Romanian subjectivity lexicon usingEnglishRomanian dictionary. generated lexicon used classify Romaniantext. Balahur Turchi (2014) conducted study assess performance statisticalsentiment analysis techniques machine-translated texts. Opinion-bearing English phrasesNew York Times Text (20022005) corpus split training test datasets.English sentiment analysis system trained training dataset predictionaccuracy test set found 68%. Next, training test datasetsautomatically translated German, Spanish, French using publicly availablemachine-translation engines (Google, Bing, Moses). translated test setsmanually corrected errors. German, Spanish, French, sentiment analysissystem trained translated training set language testedtranslated-and-corrected test set. authors observe German, Spanish,French sentiment analysis systems obtain accuracies low sixties (and thusmuch lower 68%). Contrary work, study uses original text focuslanguage, manual automatic translations, well manual automaticsentiment assignments systematically examine effect translation sentiment.Further, use several external sentiment resources well translations withinstate-of-the-art sentiment systems. Also, German, Spanish, French much closerEnglish, Dialectal Arabic English. Finally, deal noisy social media textsopposed polished news media texts. also exists research using sentimentanalysis improve machine translation, work Chen Zhu (2014),beyond scope paper.99fiMohammad, Salameh, & Kiritchenko3. Method Determining Impact Translation Sentimentsystematically study impact translation sentiment analysis, propose twoexperimental setups corresponding (a) (b) described Introduction:Setup A: Translate Arabic text English (manually automatically) annotate English text sentiment (manually automatically). Comparesentiment labels assigned translated English text manual sentiment annotations Arabic text. similar sentiment annotations are, lessimpact translation.Setup B: Translate sentiment annotated corpora lexicons English Arabic(automatically), use additional resources supervised Arabic sentimentclassification. Compare sentiment labels assigned system manualsentiment annotations Arabic text. similar sentiment annotationsare, less impact translation.3.1 Impact Translation Sentiment - Setup A: Translating FocusLanguage Text EnglishSetup explore translation text Arabic English impacts sentiment. Specifically, analyze performance English sentiment analysis system,using English resources, automatic translations Arabic social media texts. setupoutlined below:Identify compile Arabic social media dataset. refer Ar. [Arcomes first two letters Arabic.]Manually translate Ar English. refer English translationsEn(Manl.Trans.). [Manl. manual, Trans. translations.]Automatically translate Ar English. refer English translationsEn(Auto.Trans.). [Auto. automatic.]Manually annotate Ar sentiment. refer sentiment-labeled datasetAr(Manl.Sent.).Manually annotate English datasets [En(Manl.Trans.) En(Auto.Trans.)]sentiment, creating En(Manl.Trans., Manl.Sent.) En(Auto.Trans., Manl.Sent.),respectively.Run state-of-the-art Arabic sentiment analysis system Ar, creating Ar(Auto.Sent.).acts baseline system.Run state-of-the-art English sentiment analysis system English datasets[En(Manl.Trans.) En(Auto.Trans.)], creating En(Manl.Trans., Auto.Sent.)En(Auto.Trans., Auto.Sent.), respectively.100fiHow Translation Alters SentimentFigure 1: Setup A: Translating focus language text English. compare sentimentlabels Ar(Manl.Sent.) (shown shaded box) datasets shownright side figure. Ar(Manl.Sent.) original Arabic text manuallyannotated sentiment.Figure 1 depicts setup. various sentiment-labeled datasets created,compare pairs datasets draw inferences. example, comparing labelsAr(Manl.Sent.) En(Manl.Trans., Manl.Sent.) show different sentimentlabels tend text manually translated Arabic English. comparison also show, example, whether positive tweets tend translatedneutral tweets, extent. Furthermore, results demonstrate feasiblefirst translate Arabic text English use automatic sentiment analysis(Ar(Manl.Sent.) vs. En(Auto.Trans., Auto.Sent.)). Section 5, provide analysisseveral comparisons two different Arabic social media datasets.DATA RESOURCES: list corpora lexicons used Setupshown Table 1. Since manual translation text Arabic English costlyexercise, chose, experiments, existing Arabic social media datasetalready translated BBN Arabic-DialectEnglish Parallel Text (Zbib, Malchiodi,Devlin, Stallard, Matsoukas, Schwartz, Makhoul, Zaidan, & Callison-Burch, 2012).5contains 3.5 million tokens Arabic dialect sentences English translations.use randomly chosen subset 1200 Levantine dialectal sentences, referBBN posts BBN dataset, experiments.also conduct experiments dataset 2000 tweets originating Syria (acountry Levantine dialectal Arabic commonly spoken). tweets collectedMay 2014 polling Twitter API. refer dataset Syrian tweetsSyria dataset.6 Note, however, manual translations Syrian tweets5. https://catalog.ldc.upenn.edu/LDC2012T096. number instances chosen BBN Syria datasets somewhat arbitrary; however,constrained funds available manual sentiment annotations datasetstranslations.101fiMohammad, Salameh, & KiritchenkoResourcepositivea. Focus language (Arabic) corpora(and English translations)BBN posts498Syrian tweets448Number instancesnegativeneutral5751,350126202total1,1992,000b. Resources explored baseline Arabic sentiment systemAutomatic lexicons:Arabic Emoticon LexiconArabic Hashtag LexiconArabic Hashtag Lexicon (dialectal)22,96213,11811,94120,3428,8468,179-43,30421,96420,128c. Resources explored English sentiment systemManual lexicons:Bing Lius LexiconMPQA Subjectivity LexiconNRC Emotion LexiconAutomatic lexicons:NRC Emoticon LexiconNRC Hashtag Lexicon2,0062,7182,3174,7834,9113,3385708,5276,7898,19914,18238,31232,04824,15622,081-62,46854,129Table 1: Resources used Setup A. (Note 1: focus language corpora split testtraining folds part cross-validation experiments. Note 2: NRC EmotionLexicon NRC Emoticon Lexicon similar spelling,two different lexicons.)available. automatic sentiment analysis experiments, focus language corpora(BBN dataset Syria dataset) split test training folds part crossvalidation experiments.use number manually automatically created English sentiment lexiconsEnglish sentiment analysis system (as shown row c. Table 1). compareaccuracies obtained English sentiment analysis system Arabic sentimentanalysis system, create new Arabic wordsentiment association lexiconsdescribed Section 4.4.1. lexicons called Arabic Hashtag Lexicon,Dialectal Arabic Hashtag Lexicon, Arabic Emoticon Lexicon.3.2 Impact Translation Sentiment - Setup B: Translating EnglishSentiment Resources Focus LanguageSetup B explore translation text English Arabic impacts sentiment. Specifically, analyze change performance Arabic sentiment analysissystem allowed also make use automatic translations English sentimentlexicons corpora. setup outlined below:Identify Arabic social media dataset. Manually annotate sentiment splitcorpus test training subsets. refer test corpus Arsentiment-labeled test corpus Ar(Manl.Sent.).102fiHow Translation Alters SentimentIdentify create suitable Arabic sentiment lexicons.Identify suitable English sentiment lexicon(s) corpus English tweets labeledsentiment.Automatically translate English corpus lexicon Arabic. referArabic translation corpus Ar(Auto.Trans.) Arabic translationlexicon ArLex(Auto.Trans.) [Auto. automatic; Lex lexicon.]Train separate Arabic sentiment analysis systems using following setsresources:1. Arabic training corpus only;2. Arabic training corpus Arabic translation English corpus;3. Arabic training corpus Arabic sentiment lexicon;4. Arabic training corpus, Arabic sentiment lexicon, Arabic translation English lexicon.Apply Arabic sentiment analysis systems test set Ar.Figure 2 depicts setup. various sentiment-labeled datasets created,compare automatically labeled sets manual sentiment annotationsAr(Manl.Sent.), calculate accuracies automatic labeling. accuracieshelp answer questions as: useful automatically translated English sentiment resources Arabic sentiment analysis. also perform manual annotation studysubset automatically translated resources determine different kinds errorsresult automatic translations. Section 6, provide analysisexperiments different English resources.DATA RESOURCES: list corpora lexicons used Setup Bshown Table 2. chose Arabic portion BBN Arabic-DialectEnglish ParallelText primary Arabic social media dataset Setup B. Specifically, usesubset 1200 Levantine dialectal sentences, refer BBN posts BBNdataset. English corpus, choose SemEval-2013 Task 2 (Sentiment AnalysisTwitter) training dataset (Wilson et al., 2013) experiments BBNdataset, dataset social media posts. Further, already manually annotatedsentiment.several sentiment lexicons English. chose four manually created lexicons experiments: NRC Emotion Lexicon (Mohammad & Turney, 2010; Mohammad& Yang, 2011), Bing Liu Lexicon (Hu & Liu, 2004), MPQA Subjectivity Lexicon (Wilson,Wiebe, & Hoffmann, 2005), AFINN (Nielsen, 2011). also experimentlexicons automatically generated tweets Kiritchenko et al. (2014b): NRC Emoticon Lexicon (a.k.a. Sentiment140 lexicon) NRC Hashtag Sentiment Lexicon.lexicons helped obtain best results sentiment analysis shared task competitions (Mohammad et al., 2013; Kiritchenko, Zhu, Cherry, & Mohammad, 2014a; Zhu, Kiritchenko, &Mohammad, 2014b).103fiMohammad, Salameh, & KiritchenkoManualArabic'Test'Set'Sen7ment''Assignment'Manual)Automatic:!BaselineArabic'Test'Set'Arabic'Training'Set'Sen7ment''Assignment'Arabic'Test'Set'Arabic'Training'Set'English'Training'Set'Automa'c))Transla'on)Ar(Auto.Trans)'addi7onal'training'data'Sen7ment''Assignment'Arabic'Test'Set'Arabic'Training'Set'Sen7ment''Assignment'Ar(Manl.Sent.)'gold'data'Automa'c)Ar(Auto.Sent.)'Automatic: !Baseline + Translated CorpusAutoma'c)Ar(Auto.Sent.)'Automatic:!Baseline + Arabic lexiconAutoma'c)Ar(Auto.Sent.)'Arabic'Lexicon'Arabic'Test'Set'Arabic'Training'Set'Sen7ment''Assignment'Arabic'Lexicon'English'Lexicon'Automa'c))Transla'on)Automatic:!Baseline + Arabic lexicon +Translated lexiconAutoma'c)Ar(Auto.Sent.)'ArLex(Auto.Trans)'addi7onal'lexicon'Figure 2: Setup B: Translating English sentiment resources focus language.compare sentiment labels Ar(Manl.Sent.) (shown shaded box)datasets shown right side figure. Ar(Manl.Sent.) originalArabic text manually annotated sentiment.104fiHow Translation Alters SentimentResourceNumber instancespositive negative neutrala. Focus language (Arabic) corporaBBN posts498total5751261,1998,179-20,128b. Resources used Arabic sentiment systemAutomatic lexicons:Arabic Hashtag Lexicon (dialectal)11,941c. English resources translated ArabicSentiment-labeled corpus:SemEval-2013 Task 2 corpus3,6201,5494,7439,912Manual lexicons:AFINNBing Lius LexiconMPQA Subjectivity LexiconNRC Emotion Lexicon8782,0062,7182,3171,5984,7834,9113,3385708,5272,4766,7898,19914,18215,21018,34111,53014,241-26,74032,582Automatic lexicons:NRC Emoticon LexiconNRC Hashtag LexiconTable 2: Resources used Setup B. (Note 1: automatic sentiment analysis experiments, focus language corpora split test training folds partcross-validation experiments. Note 2: Automatic translations English resources Arabic done using Google Translate. entries, especiallyautomatic lexicons, left untranslated Google Translateinformation them.)4. Capabilities Needed Performing Experimentsexperimental setups described involve several component tasks: generatingtranslations manually automatically (Section 4.1), manually annotating ArabicEnglish texts sentiment (Section 4.2), automatic sentiment analysis English texts(Section 4.3), automatic sentiment analysis Arabic texts (Section 4.4). describesub-sections below.4.1 Generating TranslationsSetup requires certain Arabic corpora translated English, whereas Setup B requiresEnglish resources (corpus lexicon) translated Arabic. two subsections describe obtained translations.4.1.1 Generating English TranslationsBBN dialectal Arabic dataset comes manual translations English. generate automatic English translations BBN posts Syrian tweets employingin-house multi-stack phrase-based machine translation (MT) system, Portage (Cherry& Foster, 2012). statistical machine translation (SMT) system trained data105fiMohammad, Salameh, & KiritchenkoOpenMT 2012. preprocess training data segmenting Arabic source sidetraining data MADA 3.2 (Habash, Rambow, & Roth, 2009), using Penn ArabicTreebank (PATB) segmentation scheme recommended El Kholy Habash (2012).@ @ @) Ya () used interchangeably, normalize characters bare Alif @ dotless Ya , respectively.Since different forms Arabic characters Alif ( @normalization decreases sparcity Arabic tokens improves translation.English side training data lower-cased tokenized stripping punctuationmarks. set decoders stack size 10000 distortion limit 7. replaceout-of-vocabulary words translated text UNKNOWN token (which shownannotators). decoders log-linear model tuned MIRA (Chiang, Marton, &Resnik, 2008; Cherry & Foster, 2012). KN-smoothed 5-gram language model trainedEnglish Gigaword target side parallel data.4.1.2 Generating Arabic TranslationsSetup B, run SemEval-2013 English tweets dataset (Wilson et al., 2013)Google Translate obtain Arabic translations.7 Even though Google Translate phrasebased statistical MT system primarily designed translate sentences, alsoprovide one-word translations. translations often word representing predominant sense word source language. Thus also use Google Translatetranslate Arabic words English sentiment lexicons listed Table 2.Note Google Translate unable translate words lexicons. Table 2gives number words translated well break sentiment category (positive, negative, neutral). translated lexicons made freely available.8generate manual translations lexicons, Section 6.1, describe studyautomatic translations examined Arabic speaker.4.2 Creating Sentiment Labeled Data Arabic EnglishManual sentiment annotations performed crowdsourcing platform CrowdFlower9three BBN datasets two Syria datasets:1. Original Arabic posts (the BBN Syria datasets), annotated Arabic speakers.2. Manual English translations Arabic posts (available BBN dataset),annotated English speakers.3. Automatic English translations Arabic posts (the BBN Syria datasets), annotated English speakers.Questionnaire 3. shown below. questionnaire 2. similar, exceptstates text created manual translation Arabic posts. questionnaire 1. also similar 3., except Arabic states7. Since in-house system, Portage, designed translate text Arabic English,way round, use publicly available Google Translate experiments Setup B.Google Translate: https://translate.google.com8. http://www.purl.com/net/lexicons9. http://www.crowdflower.com106fiHow Translation Alters Sentimenttarget texts posts social media (no mention translations questionnaire).Questionnaire 3: Judge sentiment postsGeneral Instructions:- Attempt HITs native speaker English.- responses confidential.- possible occasional post may swear word express something offensive.text different something one might find public forum.Task-Specific Instructions:given English sentences translated Arabic using automatic machinetranslation system. translations may ungrammatical hard understand. translation system unable translate word, shows word UNK symbol, representingunknown.Select option best captures sentiment conveyed sentence:- positive- negative- neutral- uncertain positive negativeSelect positive sentence shows positive attitude (possibly toward object event).example:- hope every year good shape- honest dont know say story, nicest sensationSelect negative sentence shows negative attitude (possibly toward object event).example:- new Spiderman movie terrible- government make us bankruptSelect neutral sentence shows neutral attitude (possibly toward object event).example:- Add spices, onion sauce- expresses truly relation IsraelSelect Uncertain positive negative sentence shows uncertain attitudesentence expresses positive negative attitude. example:- strange forward glass car broken yet- like ice cream hate chocolate chipsActual HITsentence translated English Arabic computer algorithm. sentencemay ungrammatical hard follow. Additionally, system unable translateArabic words. words shown UNK symbol.Sentence: Especially companies acknowledge soft target UNK penetrate serwer commercial network .107fiMohammad, Salameh, & KiritchenkoSelect option best captures sentiment conveyed sentence:- positive- negative- neutral- uncertain positive negativepost annotated least ten annotators majority sentiment labelchosen. small number instances annotated label uncertainpositive negative. instances set aside included analysis.Table 3 shows class distribution sentiment labels various datasets. Observerows a. d. neutral tweets constitute 10% original dataBBN Syria datasets. Syrian tweets much higher percentage negative posts,whereas BBN data, percentages positive negative posts comparable.Rows b., c., e. show translated texts tend lose sentiment informationrelatively higher percentage neutral instances translated textoriginal text.post, determine count frequent annotation dividedtotal number annotations. score averaged posts determine interannotator agreement shown last column Table 3. use agreement scorebenchmark compare performance automatic sentiment systems (described below).4.3 English Sentiment Analysisuse English-language sentiment analysis system developed NRC-Canada (Kiritchenko et al., 2014b) experiments. system obtained highest scores tworecent international competitions sentiment analysis tweets SemEval-2013 Task 2(Wilson et al., 2013) SemEval-2014 Task 9 (Rosenthal et al., 2014). briefly describesystem below; details, refer reader work Kiritchenko, Zhu,Mohammad (2014b).linear-kernel Support Vector Machine (Chang & Lin, 2011) classifier trainedavailable training data. classifier leverages variety surface-form, semantic,sentiment lexicon features described below. sentiment lexicon features derivedexisting, general-purpose, manual lexicons, namely NRC Emotion Lexicon (Mohammad &Turney, 2010, 2013), Bing Liu Lexicon (Hu & Liu, 2004), MPQA Subjectivity Lexicon(Wilson et al., 2005).NRC Emotion Lexicon sentiment emotion labels 14,000 words(Mohammad & Turney, 2010; Mohammad & Yang, 2011). labels compiledMechanical Turk annotations.10 Bing Liu Lexicon 6,800 wordssentiment labels (Hu & Liu, 2004). lexicon originally used detectingsentiment customer reviews. MPQA Subjectivity Lexicon, drawsGeneral Inquirer sources, sentiment labels 8,000 words (Wilsonet al., 2005).10. https://www.mturk.com/mturk/welcome108fiHow Translation Alters SentimentBBN dataseta. Ar(Manl.Sent)b. En(Manl.Trans., Manl.Sent)c. En(Auto.Trans., Manl.Sent)Syria datasetd. Ar(Manl.Sent)e. En(Auto.Trans., Manl.Sent)positivenegativeneutralagreement41.5035.0036.1747.9243.2536.5010.5821.7527.3473.8268.0065.7022.4014.2567.5066.1510.1019.6079.0576.10Table 3: Class distribution (in percentage) sentiment annotated datasets.also used automatically generated, tweet-specific lexicons NRC Hashtag Sentiment Lexicon NRC Emoticon Lexicon (Kiritchenko et al., 2014b).11 sub-sectiongives details lexicons generated.4.3.1 Generating English Sentiment Lexiconsablation experiments study Mohammad et al. (2013) showed NRCCanada sentiment analysis system benefited use NRC Hashtag Sentiment Lexicon NRC Emoticon Lexicon. NRC Hashtag Sentiment Lexiconcreated follows. list 77 seed words, synonyms positive negative,compiled Rogets Thesaurus. Then, Twitter API polled collect tweetswords hashtags. tweets positive hashtag emoticonexpress positive sentiment. similarly tweets negative hashtagemoticon express negative sentiment. Hashtags emoticons used tweetscomplex ways, example sarcastic tweets. Nonetheless, majority tweetspositive hashtag emoticon shown positive (and similarly negativehashtags emoticons). Thus algorithm extract positive negative termstweets considers tweet positive positive hashtag negativenegative hashtag. term tweet set, sentiment score computed measuring PMI (pointwise mutual information) term positive negativecategory:SenScore (w) = P I(w, pos) P I(w, neg)(1)w term lexicon. P I(w, pos) PMI score w positiveclass, P I(w, neg) PMI score w negative class. positiveSenScore (w) implies word tends co-occur positive seedsnegative seeds. Thus, likely associated positive sentiment. Similarly,negative score suggests word tends co-occur negative seedspositive seeds. Thus, likely associated negative sentiment. magnitudeSenScore (w) indicates strength association.NRC Emoticon Lexicon (aka Sentiment140 Lexicon) created similar fashionusing emoticons :) :( seeds.11. http://www.purl.com/net/lexicons109fiMohammad, Salameh, & Kiritchenko4.3.2 Pre-processing Feature Generationfollowing pre-processing steps performed. URLs user mentions normalizedhttp://someurl @someuser, respectively. Tweets tokenized part-of-speechtagged CMU Twitter NLP tool (Gimpel, Schneider, OConnor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, & Smith, 2011). Then, tweet representedfeature vector.features:Word character ngrams;POS: number occurrences part-of-speech tag;Negation: number negated contexts. Negation also affects ngram features:word w becomes w NEG negated context;Automatic sentiment lexicons: token w occurring sentence presentlexicon, sentiment score score(w) used compute:- number tokens score(w) 6= 0;P- total score = wtweet score(w);- maximal score = max wtweet score(w);- score last token tweet.features calculated lexicon separately.Manually created sentiment lexicons: three manual sentiment lexicons,following features computed:- sum positive scores tweet tokens;- sum negative scores tweet tokens.Style features: presence/absence all-cap words, hashtags, punctuation marks,emoticons, elongated words.4.4 Arabic Sentiment Analysisbuild Arabic sentiment analysis system reconstructing NRC-Canada Englishsystem deal Arabic text. extracts feature set described Section 4.3.2. also generated word-sentiment association lexicons using processdescribed Section 4.3.1, Arabic words Arabic tweets (more details subsection below). preprocess Arabic text tokenizing CMU Twitter NLP tooldeal specific tokens URLs, usernames, emoticons. use MADAgenerate lemmas. Finally, normalize different forms Alif Ya bare Alifdotless Ya.4.4.1 Generating Arabic Sentiment Lexiconsemoticons hashtag words tweet often act sentiment labels resttweet. use idea, commonly referred distant supervision (Go et al., 2009),generate three different Arabic sentiment lexicons:110fiHow Translation Alters SentimentLexiconArabic Emoticon LexiconArabic Hashtag LexiconArabic Hashtag Lexicon (dialectal)# seedspos neg1211109 121135 348# tweetsposneg520,000 455,282209,78437,209177,55634,705# entries lexiconunigram bigram trigram43,309 229,747 325,36622,007 128,814 233,48120,12893,613 159,986Table 4: Details Arabic Hashtag Lexicon, Arabic Emoticon Lexicon,Dialectal Arabic Hashtag Lexicon.Arabic Emoticon Lexicon: collected close one million Arabic tweetsemoticons (:) :(). purposes generating sentiment lexicon, :)considered positive label (pos) :( considered negative label (neg).word w, occurred least five times tweets, sentiment scorecalculated using formula shown (same described Section 4.3.1,proposed first Mohammad et al., 2013):SentimentScore(w) = P I(w, pos) P I(w, neg)(2)PMI stands Pointwise Mutual Information. refer resulting entriesArabic Emoticon Lexicon.Arabic Hashtag Lexicon: NRC-Canada system used 77 positive negativeseed words generate English NRC Hashtag Sentiment Lexicon (Mohammadet al., 2013; Kiritchenko et al., 2014b). translated English seeds Arabicusing Google Translate. Among translations provided, chose wordsless ambiguous tended strong sentiment Arabic texts. increasecoverage seed list, manually added different inflections translations.polled Twitter API period June August 2014 collected tweetsincluded seed words hashtags. filtering duplicate tweetsretweets, ended 209,784 positive unique tweets 37,209 negative uniquetweets. unigram, bigram, trigram, w, occurred least five timestweets, SenScore (w) calculated described Section 4.3.1.refer lexicon Arabic Hashtag Lexicon.Arabic Hashtag Lexicon (Dialectal): Refaee Rieser (2014a) manually createdsmall sentiment lexicon 483 dialectal Arabic sentiment words tweets. usedwords seeds collect tweets contain them, generated PMI-basedsentiment lexicon described above. refer lexicon DialectalArabic Hashtag Lexicon Arabic Hashtag Lexicon (dialectal).number seeds tweets used create Arabic Hashtag Lexicon, ArabicEmoticon Lexicon, Dialectal Arabic Hashtag Lexicon shown Table 4.table also shows number unigram, bigram, trigram entries lexicons.111fiMohammad, Salameh, & KiritchenkoDatasetSentiment classesNumber instancesfrequent class baselineHuman agreement benchmarksystem, using non-lexicon featuresa. Arabic Emoticon Lexicon featuresb. Arabic Hashtag Lexicon featuresc. Dialectal Arabic Hashtag Lexicon featuresd. lexicon features a., b., c.BBN postspos, neg, neu119947.9573.82Syrian tweetspos, neg, neu200067.5079.0562.4062.9765.3163.4778.3578.9679.3579.00Table 5: Accuracy obtained using features different automatically generated Arabicsentiment lexicons. highest scores shown bold.Arabic Sentiment Labeled Datasetsentiment classesnumber instancesfrequent class baselineHuman agreement benchmarkMourad Darwish Arabic SA systemArabic SA systemMDpos, neg111166.0672.5074.62RRpos,neg268168.9285.23BBN postspos, neg, neu119947.9573.8265.31Syrian tweetspos, neg, neu200067.5079.0579.35Table 6: Accuracy (in percentage) sentiment analysis (SA) systems various Arabicsocial media datasets.4.4.2 EvaluationTable 5 shows ten-fold cross-validation accuracies obtained BBN Syria datasetsusing various Arabic sentiment lexicons discussed above. Observe best results obtained using Dialectal Arabic Hashtag Lexicon. Both, ArabicHashtag Lexicon Arabic Emoticon Lexicon features, added DialectalArabic Hashtag Lexicon features result improvement classification accuracy.Henceforth paper, use Dialectal Arabic Hashtag Lexicon Arabicsentiment lexicon.Existing sentiment-labeled Arabic datasets include one described MouradDarwish (2013), refer MD, one described Refaee Rieser(2014a), refer RR. tested Arabic sentiment system MDRR, well two newly sentiment-annotated Arabic datasetsBBN postsSyrian tweets. Table 6 shows results ten-fold cross-validation experimentsdatasets. MD RR, presented results two-class problem (positivevs. negative) allow comparison prior published results. BBN Syriadatasets, results shown case system identify one threeclasses: positive, negative, neutral. Human agreement scores shown available.Note accuracy system higher previously published resultsMD dataset. previously published results RR dataset small112fiHow Translation Alters SentimentDatasetSentiment classesNumber instancesfrequent class baselineHuman agreement benchmarkSystema. Featuresb. - lexicon featuresc. - ngram featuresc1. - word ngram featuresc2. - char. ngram featuresd. - style featurese. - ngram features - style featuresf. - lexicon features - style featuresBBN postspos, neg, neu119947.9573.82Syrian tweetspos, neg, neu200067.5079.0565.3161.9863.0764.7263.3165.2362.2361.9079.3579.3566.4577.7178.4079.2067.3579.45Table 7: Ablation experiments showing accuracy BBN Syria datasets.larger drop performance removing feature set, usefulfeature set classification.subset (about 1000 instances) Refaee Rieser (2014a) obtained accuracy87%. results Table 6 larger dataset directly comparable.determine impact different feature sets performance conducting ablationexperiments, remove one set features time observe changeperformance. larger drop accuracy, useful removed feature set.Table 7 shows ablation results BBN Syria datasets.Observe BBN dataset largest drop performance occursremove sentiment lexicon features. shows method producing sentiment lexicon effective generating useful wordsentiment association entries. Ngramshelpful sentiment classification, especially Syria dataset. Removingstyle features (features based hashtags, exclamations, etc.) result largedrop performance datasets, likely character ngram featuressubsume much discerning power. Row e. shows performance usesentiment lexicon features (no ngram features style features) row f. shows performance use ngram features (no lexicon features style features).Observe performance using lexicon features alone rather competitiveBBN dataset, suggesting automatically generated sentiment lexicons ablecapture termsentiment association similar extent supervised algorithmlearn ngram features training data. Syria dataset, ngrams alone produceresults reaching human agreement levels. believe may markedlylower type token ratio (lexical diversity) Syrian tweets due skewdataset towards negative class. (Table 15 Section 5.2 shows type token ratiosvarious datasets.)113fiMohammad, Salameh, & KiritchenkoBBN dataseta. Ar(Auto.Sent)b. En(Manl.Trans., Auto.Sent)c. En(Auto.Trans., Auto.Sent)Syria datasetd. Ar(Auto.Sent)e. En(Auto.Trans., Auto.Sent)posnegneu39.7843.1242.8760.0555.6356.050.171.251.0820.6024.7575.3069.754.105.50Table 8: Class distribution (in percentage) resulting automatic sentiment analysis.Data Paira. Ar(Manl.Sent) - Ar(Auto.Sent)b. Ar(Manl.Sent) - En(Manl.Trans., Manl.Sent)c. Ar(Manl.Sent) - En(Manl.Trans., Auto.Sent)d. Ar(Manl.Sent) - En(Auto.Trans., Manl.Sent)e. Ar(Manl.Sent) - En(Auto.Trans., Auto.Sent)f. En(Manl.Trans., Manl.Sent) - En(Auto.Trans., Manl.Sent)g. En(Manl.Trans., Manl.Sent) - En(Manl.Trans., Auto.Sent)h. En(Auto.Trans., Manl.Sent) - En(Auto.Trans., Auto.Sent)Match %65.3171.3167.7357.2162.0860.0863.1169.58Table 9: Setup A: Match percentage pairs sentiment labeled BBN datasets.5. Experiments Sentiment Translation - Setup A: TranslatingFocus Language Text EnglishSetup (as described Section 3.1) analyze performance English sentiment analysis system, using English resources, automatic translations Arabic socialmedia texts. Using methods systems described Sections 4.1, 4.2, 4.3, 4.4,generated translations manually automatically sentiment labeleddatasets mentioned Section 3.1s Experimental Setup (also shown Figure 1). Table 8shows distribution positive, negative, neutral classes various datasetsautomatically labeled sentiment. percentages comparedTable 3 (rows a. d.) show true sentiment distribution BBNSyria datasets. Observe automatic system difficulty assigning neutral classposts. probably small percentage (about 10%) neutral tweetstraining data. Also notice system predominantly guesses negative,also reflection distribution training data. strong bias negativeslessened English translations.Main Result: Tables 9 10 show similar sentiment labels across variouspairs datasets BBN posts Syrian tweets, respectively. example, row a.Table 9 shows comparison Arabic tweets manually annotatedsentiment automatically labeled sentiment Arabic sentimentanalysis system. Column 2 shows percentage instances sentiment labelsmatch across two datasets compared. row a. match percentage 65.31%114fiHow Translation Alters SentimentData Paira. Ar(Manl.Sent) - Ar(Auto.Sent)b. Ar(Manl.Sent) - En(Auto.Trans., Manl.Sent)c. Ar(Manl.Sent) - En(Auto.Trans., Auto.Sent)d. En(Auto.Trans, Manl.Sent) - En(Auto.Trans., Auto.Sent)Match %79.3571.0579.1676.80Table 10: Setup A: Match percentage pairs sentiment labeled Syria datasets.represents accuracy automatic sentiment analysis system Arabic BBNposts.Row b. shows difference labels text manually translated ArabicEnglish, even though sentiment labeling Arabic English done manually.Observe two labels match 71.31% time. However, agreement amonghuman sentiment annotators original Arabic texts 73.82%. So, Englishtranslation affect sentiment, dramatically.Row c. shows results manually translated text run Englishsentiment analysis system labels compared Ar(Manl.Sent.) Observematch pair 67.73%, much lower 71.31% obtainedmanual sentiment labeling. shows English sentiment system performingrather well. (One would expect get match greater 71.31%.) importantly, English sentiment system shows competitive result 62.08% runautomatically translated text (row e.), makes choice viable option sentimentanalysis non-English texts. result inline previous findings cross-lingualinformation retrieval (Nie, Simard, Isabelle, & Durand, 1999) text classification (Amini& Goutte, 2010).Rows d. e. compare Ar(Manl.Sent.) manual automatic sentiment labelingautomatic translations. Since automatic translation Arabic English fairlydifficult, expect match percentages lower rows b. c.,exactly observe. However, unexpected find number row e.higher row d. find pattern corresponding data pairsSyrian tweets well (rows b. c. Table 10). suggests certain attributesautomatically translated text mislead humans regards true sentimentsource text. However, attributes seem affect automatic sentimentanalysis system much. conduct experiments explore reasons behindSection 5.1.Row f. shows manual automatic translation lead 60% matchmanually annotated sentiment labels other. Row g. shows accuracyEnglish automatic sentiment analysis system manually translated text (assumingEnglish sentiment labels gold). Row h. shows accuracy English automatic sentiment analysis system automatically translated text (assuming English sentimentlabels gold). case, systems accuracy 69.58% higher humanagreement automatically translated text (65.7%), shows automatictranslation greatly impacts sentiment perceived humans.115fiMohammad, Salameh, & Kiritchenko5.1 Qualitative Analysis Translations Differ SentimentSource Textseen results experiments previous section, translationstext often preserve original sentiment. Further, exist number instancesmanual sentiment annotations automatic translations differ sentimentsoriginal Arabic text, automatic English sentiment analysis system correctlypredicts sentiment original Arabic text. describe study conducteddetermine main reasons are, frequentlyreasons come play.started creating dataset manual annotations Arabic texts disagreedmanual annotations translations. Specifically, BBN dataset, createdinstances composed of:a. original Arabic tweet,b. manually determined sentiment Arabic tweet (positive, negative,neutral),c. manual English translation Arabic tweet,d. manually determined sentiment translation (positive, negative,neutral).kept instances b. differed d. filtered set keepinginstances automatic English sentiment analysis system correctly predicted b. instances arranged decreasing order inter-annotator agreementsentiment annotation Arabic texts. Since annotation task time intensive,wanted annotate instances high confidence sentimentsoriginal Arabic texts. top 100 instances presented judge spokeEnglish Arabic fluently. refer dataset BBN Manl.Trans.Disagreement Pairs. 100 instances, judge askedopinion b. d. differ. precise directions shown below:Annotation Guidelinesinstance (row), tell us think manually annotated sentimentEnglish translation differs original sentiment Arabic post.Codes:1. Bad translationa.b.c.d.sentiment words disappearsentiment words addedsentiment words replaced opposite sentiment wordssomething sentiment words (also) caused disagreement(may ill-formed text, may grammar, may positionnegators like never, tense, auxiliaries, etc.)2. Translation reasonable (sentiment-wise), sentenceviewed one sentiment Arabic speaking populationdifferent sentiment English-speaking population due culturallife-style differences.116fiHow Translation Alters Sentiment3. know.Note:codes sub-categories. enter 1b, 1c, etc.even enter 1 none sub-categories apply.annotate, discover new categories, addlist codes, use new codes well.one code applies instance, separate comma.example, say 1a, 1d 1b, 1c, 2.repeated annotation procedure, instances involving automatically translated texts:a. original Arabic tweet,b. manually determined sentiment Arabic tweet (positive, negative,neutral),c. automatic English translation Arabic tweet,d. manually determined sentiment translation (positive, negative,neutral).before, kept instances b. differed d., instances automatic English sentiment analysis system correctly predicted b.100 instances highest inter-annotator agreement Arabic sentiment annotationpresented judge. refer dataset BBN Auto.Trans. Disagreement Pairs. judge asked opinion b. d. differ. Since automatictranslations exist BBN Syria datasets, annotation done100 instances Syria dataset well. refer dataset SyriaAuto.Trans. Disagreement Pairs.5.1.1 Annotation Resultstook human judge 12 hours annotate three sets (100 instances each) describedabove. distribution reasons disagreement sentimentoriginal text sentiment translation 100 instances datasetshown Table 11. Since total number instances study 100, numberreason (code) also corresponding percentage. Note since instanceannotated belong one reason, percentages sum 100%. Also,since annotator could choose broad reason category code (for example, 1.), nonesub-categories apply (for example, 1a. 1b.), sum entries subcategoriesneed equal number entries subsuming reason category.judge marked handful instances know categoryadd new reason categories. shows judge largely able determinereason disagreement two manual sentiment annotations involved (oneoriginal Arabic post one English translation), terms reasonspre-specified.117fiMohammad, Salameh, & KiritchenkoReason Disagreement1. Bad Translations1a. sentiment words disappear1b. sentiment words added1c. sentiment words replacedwords opposite sentiment1d. sentiment changed dueill-formed text, grammar, etc2. Cultural differences3. knowPercentage Disagreement PairsBBN datasetSyria datasetManl.Trans. Auto.Trans.Auto.Trans.4195911158800119378267321310312264Table 11: Class distribution reasons disagreement sentimentoriginal text sentiment translation 100 instances threedatasets. Note entries represent number percentageinstances, since subset 100 instances all.large percentage instances manually translated disagreement sets affected judge thought cultural differences. However, bad translationstill significant cause disagreement. cursory examination BBN datasetalso gave us impression manual translations could better.Nonetheless, contrast manually translated disagreement sets, automaticallytranslated disagreement sets markedly high proportion instances (more 90%)bad translation led directly disagreement sentiment. specifically,automatic translations seem often mistranslate sentiment expressions eitherappear translation appear neutral terms translation(58% instances BBN Auto.Trans. 80% Syria Auto.Trans.).instances pertaining 1b. found data. surprising sinceone would expect translator add sentiment none.5.1.2 DiscussionTable 12 shows examples disagreement categories resulting manual translation BBN subset. (We show examples 1b. lack datasub-category.) sub-categories present, table shows original Arabic post, BBN-provided manual translation, comments judge. Table13 shows examples disagreement categories resulting automatictranslation BBN subset.Discussions judge revealed following phenomenon commonly led1a., 1c., 1d., 2.:Ambiguous words: Often word many meanings, one sense certainsentiment (positive, negative, neutral) another sense different sentiment,mistranslated wrong sense leading sentiment disagreement.common automatically translated text, occurs sometimes even manually118fiHow Translation Alters Sentiment1a. Sentiment words disappear@ ,,, QK.Post? KD.knegativewhat,bolded text Arabic expression literally means letloved Gecko. expresses disgust anger. part leftCommentsuntranslated human translator.1c. Sentiment words replaced words opposite sentiment. GA KKBAKPostneutralManl.Trans.Manl.Trans.Commentssecondly, still refreshment room?ambiguous Arabic word mistranslated refreshment insteadrecovery room. surprising human translator made mistake.negativeneutral1d. Sentiment changed due ill-formed text, grammar, tense, etcPostManl.Trans.Q@ PAm .' @ AK@Xk.@ H. Q @Y Q'@negativegood still coming, water leak day projectpost supposed shows sarcasm saying expect good come,meaning worse yet come. expression widely usedCommentsArabic conversations mean something negative. also example 2.2. Cultural DifferencesQPost.. JK B .. k@.neutralManl.Trans.honestly... comment...Although post seem literally negative, many Arabicconversations used express negative opinionsimilar speechless.neutralalthough didnt see crescent homepost associate negative sentiment observing crescent moonIslam associated beginning month holidayneutralCommentsPostManl.Trans.CommentsG@C@ IJ? AJ JK.negativenegativeTable 12: Examples different reasons disagreement sentiment original text sentiment manual translation.translated textespecially translation done crowdsourcing, qualitycontrol checks stringent. Even human translators occasiontempted use Google Translate.Sarcasm: Sarcasm sometimes hard detect, even humans, evendetected, upon translation, differences cultural language norms meantranslation longer appears sarcastic. See example 1d. Table 12.Metaphors: Metaphors, example 1c. Table 13, also hardtranslateagain automatic system, extent even humans. often translated neutral opposite sentiment expressions,contributing 1a. 1c.Word-reordering: Automatic translations often lead poor word-reorderingtarget language, sentiment implications original postnegation terms. Missing misplaced negation term lead different sentiment.Sarcasm also greatly affected word-reordering. See example 1d. Table 13.current statistical machine translation systems evaluated using ngram-basedevaluation metric (BLEU). However, metric often misses (or penalize enough)119fiMohammad, Salameh, & Kiritchenko1a. Sentiment words disappearQ@@AK PAJPostk@.Auto.Trans. match UNK frankly .bolded word dialectal Arabic typo translated system.Commentsmeant sleeeeeping i.e., match boring1c. Sentiment words replaced words opposite sentimentH PAB@Q @ @ J AJKY@PostH. PA.Auto.Trans.negativeneutralnegativeminimum taught relatives clocktwo meanings: scorpions clock arms. Also AJKY@H. PAmeans eitherword lower. post supposed metaphorically state worldCommentstaught relatives hurt like scorpion bites. post mistranslated,leading neutral (instead negative) sentiment.1d. Sentiment changed due ill-formed text, grammar, tense, etcJKQPostK.neutralAuto.Trans.positiveknow , mean , cleanlinesscorrect translation know cleanliness means.Word reordering missing negation led text seemingly positiveCommentssentiment.2. Cultural DifferencesAJK@Post.. ADAg ADJJC@ PAAuto.Trans. dont know putting place .post perceived Arab annotators said conversationCommentsexpress negative attitude toward object@ @ AKA K kP@ @PostAKA QAuto.Trans. God mercy dead God cure patientsSupplications Arabic annotated positive, althoughcontains lots negative phrases. tweet annotatedCommentspositive confidence 1.0, automatic translation negativeconfidence 0.7, thus showing cultural differences perceiving tweetnegativenegativeneutralpositivenegativeTable 13: Examples different reasons disagreement sentiment original text sentiment automatic translation.mistranslations caused many phenomenon listed above. Thus, evident here,means automatically generated translations often carry misleading sentiment.5.2 Quantitative Analysis Features Translations Impact Sentimentprevious section, qualitatively analyzed human annotation sentimenttranslations difficult. section quantitatively explore:causes automatic translations inferior manual translations termspreserving sentiment. (Recall Table 9 b. c. higher scores d.e.)II automatic translation, error prone may be, offers advantagesautomatic sentiment analysis system compared human annotations. (RecallTable 9 row d. lower score row e.)Although hard prove causation, hope experiments shed lightfeatures translated texts impact sentiment.120fiHow Translation Alters SentimentDatasetSystema. featuresb. - lexicon featuresBBN Manl.Trans.BBN Auto.Trans.67.7363.7362.0860.74Table 14: Accuracy sentiment analysis system manual automatic translations, without sentiment lexicons.DatasetBBN posts (Arabic)BBN posts (Manl.Trans.)BBN posts (Auto.Trans.)Syrian tweets (Arabic)Syrian tweets (Auto.Trans.)#types6,0543,5923,108#tokens11,92816,60916,660#types/#tokens0.50750.21630.186611,6676,73135,98357,1530.32420.1178Table 15: Lexical diversity datasets.qualitative analysis previous section suggests one main reasonsmay fact sentiment words original text translatedneutral terms. test quantitatively ablation experiments Englishtranslations (manual automatic), observing effect removing sentiment lexiconfeatures. Table 14 shows results. Observe sentiment lexicon featureshelpful manual translations (improve results 4 percentage points)automatic translations (improves results 1.34 points).hypothesis automatic sentiment analysis system correctly annotates several automatically translated instances manual annotations translation mayfail (II), sentiment system learn appropriate model even mistranslated text especially automatic translation makes consistent errors. example,Q@ @ (Oh God grant victory to) consistently translated God forsake.tweets phrase correctly annotated positive system, markednegative human annotators.true, surmise automatic translations lower lexicaldiversity manual translations. is, automatic translations lower word type(unique term) token ratio manual translations. Table 15 shows number typestokens original Arabic BBN Syria datasets also translations.automatic translations removed UNK tokens determining counts.First, note even human translations lower type token ratiooriginal source text. Additionally, observe hypothesized, type tokenratio markedly higher manual translations compared automatic translationstext. supports hypothesis SMT system translates sourcetokens consistently. Since automatic sentiment analysis system trainedconsistently translated text original sentiment labels source text, stillable determine true sentiment. However, since human sentiment annotators see manyinstances sentiment terms mistranslated neutral terms, unabledetermine true sentiment.121fiMohammad, Salameh, & KiritchenkoSystemAccuracy(in percentage)a. Baseline (uses word ngram style features training fold)61.98b. Baseline + Arabic translation English corpusEnglish corpus: SemEval task 2 training corpus42.78c. Baseline + Arabic translation English lexiconi. English lexicon: AFINN63.41ii. English lexicon: Bing Liu Lexicon62.99iii. English lexicon: MPQA61.91iv. English lexicon: NRC Emotion Lexicon63.48v. English lexicon: NRC Emoticon Lexicon62.40vi. English lexicon: NRC Hashtag Lexicon61.73d. Baseline + Arabic lexiconi. Arabic lexicon: Arabic Emoticon Lexicon62.40ii. Arabic lexicon: Arabic Hashtag Lexicon62.97iii. Arabic lexicon: Arabic Hashtag Lexicon (dialectal)65.31e. Baseline + Arabic lexicon + Arabic translation English lexiconArabic lexicon: Arabic Hashtag Lexicon (dialectal)i. English lexicon: AFINN65.73ii. English lexicon: Bing Liu Lexicon66.15iii. English lexicon: MPQA65.15iv. English lexicon: NRC Emotion Lexicon66.23v. English lexicon: NRC Emoticon Lexicon66.15vi. English lexicon: NRC Hashtag Lexicon64.22f. Baseline + Arabic Hashtag Lexicon (dialectal)+ Arabic translation NRC Emotion Lexicon66.57Table 16: Setup B: Cross-validation experiments BBN dataset. highest scoreoverall highest scores c., d., e. shown bold.6. Experiments Sentiment Translation - Setup B: TranslatingSentiment Resources English Focus Languagedescribe sentiment analysis experiments automatically translate resourcesEnglish focus language (Arabic) improve accuracy sentiment analysissystem operating texts focus language (Setup B). implemented Setup Bdescribed Section 3.2 Figure 2, using capabilities described Section 4.translations obtained using Google Translate. Arabic portion BBN datasetused primary focus language text. baseline system performs ten-fold crossvalidation dataset using word ngrams style features described Section 4.4.systems use additional resourcessome originally created Arabic sourcestranslations English resources.Table 16 shows accuracy automatic sentiment analysis systems. (Generatedsentiment labels compared manual annotation Arabic speakers.) Comparing rowsa. b. infer simply translating sentiment-labeled tweets EnglishArabic using additional training data Arabic sentiment analysis leads122fiHow Translation Alters Sentimentpoor results. saw Section 5, language produced machine translationsystem differs substantially corresponding natural language. Therefore, sentiment analysis system cannot fully benefit additional labeled machine-translatedcorpus asked annotate natural language text test time. Similar observationsreported work Balahur Turchi (2014). possible better resultsmay obtained using one many domain-adaptation techniques proposedliterature, however, leave future work.12also conducted experiments adding Arabic translations English lexiconsbaseline system row a.see results rows c.i. c.vi. table. bestresults obtained using Arabic translations NRC Emotion Lexicon (row c.iv.).Row d. shows results obtained using baseline system additional featuresArabic sentiment lexicon. shown Section 4.4, Dialectal Arabic HashtagLexicon outperformed Arabic lexiconsthose results shown completeness convenience. compare accuracies obtained rows e. f. d.determine using additional features Arabic translations English sentiment lexicons beneficial. Observe translated English lexicons help obtain higheraccuracies row d.iii. (65.31%). best results obtained translationsEnglish sentiment lexicon using NRC Emotion Lexicon. Using NRC Emotion Lexicon along Dialectal Arabic Hashtag Lexicon addition baselinesystem (row f.) gives slight improvement (66.57%).Thus sentiment analysis Arabic social media posts, difficult extractbenefit automatic translations English sentiment labeled sentences. However, improvements obtained using automatic translations English sentiment lexicons.6.1 Manual Examination Automatically Translated EntriesSentiment Lexiconshown above, lexicons created translating existing ones languages beneficial automatic sentiment analysis, even one good lexicons focus language(such Dialectal Arabic Hashtag Lexicon Arabic). However, experimentsexplicitly quantify extent translated entries appropriate,translation alters sentiment source word. conducted manual annotationstudy 300 entries NRC Emotion Lexicon determine percentage entriesappropriate even automatic translation focus language (Arabic).appropriate entry Arabic translation sentiment associationEnglish source word. Additionally, translated entries deemed incorrectfocus language classified coarse error categories. list pre-decided errorcategories presented annotator, annotator also encouraged createnew error categories, required. error categories shown below:1. word completely mistranslated.2. translation perfect, English word translated word relatedcorrect translation. Arabic word provided different sentimentEnglish source word.12. Sampling English corpus obtain similar class distribution Arabic dataset ledsmall improvements.123fiMohammad, Salameh, & KiritchenkopositivenegativeneutralTranslation# English Entries100100100300# positive854594Translation# negative # neutral9692478810898# changed15 (15.0%)8 (08.0%)12 (12.0%)35 (11.7%)Table 17: Annotations NRC Emotion Lexicons sentiment association entries automatic translation Arabic.Error categories1. Mistranslated2. Translated related word3. Translation correct, 3a., 3b., 3c.3a. Different dominant sense3b. Cultural differences3c. reasonsPercentagetotal errors9.738.751.629.022.60.0Table 18: Percentage erroneous entries translated NRC Emotion Lexiconassigned error category.3. translation correct, Arabic word different sentimentEnglish source word.(a) dominant sense Arabic word different dominant senseEnglish source word, different sentiments.(b) Cultural life style differences Arabic English speakers leaddifferent sentiment associations English word translation.(c) reason (give reason can).annotator native speaker Arabic, also fluent English.chose NRC Emotion Lexicon study manually createdled best results experiments (Table 16). Since manual annotationtedious, study, randomly selected 100 positive words, 100 negative words,100 neutral words lexicon.Table 17 shows results human annotation study. 100 positive entriesexamined, 85 marked appropriate Arabic well. Nine translationsmarked negative Arabic, six marked neutral. Similarly, 92%translated negative entries 88% translated neutral entries marked appropriate Arabic. Overall, 11.67% translated entries deemed incorrectArabic.Table 18 gives percentage erroneous entries assigned error category. Observe close 10% errors caused gross mistranslations, close 40%errors caused translations related word, 50% errorscaused, bad translation, differences word used Arabiceitherdifferent sense distributions (29%) cultural differences (22.6%).124fiHow Translation Alters Sentiment7. ConclusionsMuch work sentiment analysis focused English texts. Thus,languages limited sentiment resources. paper conducted several experimentsexploring two broad approaches improving sentiment analysis Arabic social media texthelp English resources state-of-the-art translation systems: (a) translatefocus language text resource-rich language English, apply powerfulEnglish sentiment analysis system translation, (b) translate resourcessentiment labeled corpora sentiment lexicons English focus language,use additional resources focus-language sentiment analysis system. goalsystematically study impact translation (manual automatic) sentiment.experiments show automatic sentiment analysis English translations (evenautomatic translations) Arabic texts lead competitive resultsresultssimilar obtained current state-of-the-art Arabic sentiment analysis systems.Similar findings reported tasks, Information Retrieval (Nieet al., 1999) Text Classification (Amini & Goutte, 2010). Surprisingly, results alsoshow automatic sentiment analysis automatic translations outperforms manualsentiment annotations automatically translated text. suggests SMT errorsimpact human perception sentiment markedly automatic sentiment systems.Furthermore, conduct qualitative quantitative studies investigate observe results. find sentiment expressions often mistranslated neutralexpressions translated. Additionally, automatic translation often makes consistenterrors translating terms, since automatic system learns termsentiment associations training data, learn mistranslated word cue truesentiment, thus recovering error. Sarcasm, metaphoric expressions, incorrectword-reordering common reasons translations fail preserve sentiment.Finally, observe even correctly translated texts sometimes markeddifferent sentiment speakers source language believe be. Thus,sentiment, least extent dependent cultural context annotator.also conducted experiments translating English resources Arabic helpimprove Arabic sentiment analysis systems. Specifically, found using automaticArabic translations many freely available English sentiment lexicons improved accuracy.However, experiments simply added translated, sentiment-labeled, English tweets dataexisting Arabic training data resulted drop accuracy. manual examinationsubset automatic translations English lexicon entries, native speakerArabic marked close 90% appropriate (that is, Arabic wordsentiment association English source word). annotator, fluent Englishwell, categorized remaining 10% entries different error classesreasonsentries valid Arabic. Mistranslation, cultural differences,different sense distributions Arabic English, primary reasons errorsautomatic translation entries sentiment lexicon.Caveats: automatic systems employed experiments, i.e., Arabic sentimentanalysis, English sentiment analysis, machine translation, exhibit state-of-the-art per125fiMohammad, Salameh, & Kiritchenkoformance; nevertheless, improvements possible. Arabic sentiment analysissystem possibly benefit features derived specifically Arabic language.English sentiment analysis system adapted peculiarities machinetranslated texts, notably different regular English. current machinetranslation system trained non-tweet data results high percentageout-of-vocabulary words datasets. Tweets mixture dialects evenmixture languages (e.g., Arabic English). Addressing factors future workgive even insight sentiment altered translation, specific contexts.Data: resources created part project (Arabic sentiment lexicons,Arabic sentiment annotations social media posts, English sentiment annotationstranslations) made freely available.13AcknowledgmentsThanks Kareem Darwish Eshrag Refaee sharing data. Thanks NorahAlkharashi annotating translations reasons sentiment preserved.thank Colin Cherry, Samuel Larkin, Marine Carpuat helpful discussions.ReferencesAbbasi, A., Chen, H., & Salem, A. (2008). Sentiment analysis multiple languages: Featureselection opinion classification web forums. ACM Transactions InformationSystems, 26 (3), 12:112:34.Abdul-Mageed, M., & Diab, M. (2014). SANA: large scale multi-genre, multi-dialectlexicon Arabic subjectivity sentiment analysis. Proceedings 9th International Conference Language Resources Evaluation, LREC 14. EuropeanLanguage Resources Association.Abdul-Mageed, M., Diab, M., & Kubler, S. (2014). SAMAR: Subjectivity sentimentanalysis Arabic social media. Computer Speech & Language, 28 (1), 20 37.Abdul-Mageed, M., Diab, M. T., & Korayem, M. (2011). Subjectivity sentiment analysis modern standard Arabic. Proceedings 49th Annual MeetingAssociation Computational Linguistics: Human Language Technologies, pp. 587591.Agarwal, A., Xie, B., Vovsha, I., Rambow, O., & Passonneau, R. (2011). Sentiment analysisTwitter data. Proceedings Workshop Languages Social Media, LSM11, pp. 3038, Portland, Oregon.Ahmad, K., Cheng, D., & Almas, Y. (2006). Multi-lingual sentiment analysis financialnews streams. Proceedings 1st International Conference Grid Finance.Ahmed, S., Pasquier, M., & Qadah, G. (2013). Key issues conducting sentiment analysisArabic social media text. Proceedings 9th International ConferenceInnovations Information Technology, pp. 7277. IEEE.13. http://www.purl.org/net/ArabicSA126fiHow Translation Alters SentimentAisopos, F., Papadakis, G., Tserpes, K., & Varvarigou, T. (2012). Textual contextualpatterns sentiment analysis microblogs. Proceedings 21st International Conference World Wide Web Companion, WWW 12 Companion, pp.453454, New York, NY, USA.Al-Kabi, M., Gigieh, A., Alsmadi, I., Wahsheh, H., & Haidar, M. (2013). opinion analysis tool colloquial standard Arabic. Proceedings 4th InternationalConference Information Communication Systems, ICICS 13.Amini, M.-R., & Goutte, C. (2010). co-classification approach learning multilingual corpora. Machine learning, 79 (1-2), 105121.Badaro, G., Baly, R., Hajj, H., Habash, N., & El-Hajj, W. (2014). large scale Arabicsentiment lexicon Arabic opinion mining. Proceedings EMNLP WorkshopArabic Natural Language Processing (ANLP), pp. 165173, Doha, Qatar.Bakliwal, A., Arora, P., Madhappan, S., Kapre, N., Singh, M., & Varma, V. (2012). Mining sentiments tweets. Proceedings 3rd Workshop ComputationalApproaches Subjectivity Sentiment Analysis, WASSA 12, pp. 1118, Jeju, Republic Korea.Balahur, A., & Turchi, M. (2014). Comparative experiments using supervised learningmachine translation multilingual sentiment analysis. Computer Speech &Language, 28 (1), 5675.Bellegarda, J. (2010). Emotion analysis using latent affective folding embedding.Proceedings NAACL-HLT Workshop Computational Approaches AnalysisGeneration Emotion Text, pp. 19, Los Angeles, California.Boucouvalas, A. C. (2002). Real time text-to-emotion engine expressive Internet communication. Emerging Communication: Studies New Technologies PracticesCommunication, 5, 305318.Brody, S., & Diakopoulos, N. (2011). Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using wordlengthening detect sentiment microblogs. Proceedings ConferenceEmpirical Methods Natural Language Processing, EMNLP 11, pp. 562570,Stroudsburg, PA, USA.Chang, C.-C., & Lin, C.-J. (2011). LIBSVM: library support vector machines. ACMTransactions Intelligent Systems Technology, 2 (3), 27:127:27.Chen, B., & Zhu, X. (2014). Bilingual sentiment consistency statistical machine translation. Proceedings 14th Conference European Chapter Association Computational Linguistics, pp. 607615, Gothenburg, Sweden. AssociationComputational Linguistics.Cherry, C., & Foster, G. (2012). Batch tuning strategies statistical machine translation.Proceedings Conference North American Chapter AssociationComputational Linguistics: Human Language Technologies, pp. 427436.Chiang, D., Marton, Y., & Resnik, P. (2008). Online large-margin training syntacticstructural translation features. Proceedings Conference EmpiricalMethods Natural Language Processing, EMNLP 08, pp. 224233.127fiMohammad, Salameh, & KiritchenkoEl-Beltagy, S. R., & Ali, A. (2013). Open issues sentiment analysis Arabic socialmedia: case study. Proceedings 9th International Conference Innovations Information Technology, pp. 215220. IEEE.El Kholy, A., & Habash, N. (2012). Orthographic morphological processingEnglishArabic statistical machine translation. Machine Translation, 26 (1-2), 2545.Farra, N., Challita, E., Assi, R. A., & Hajj, H. (2010). Sentence-level documentlevel sentiment mining Arabic texts. Proceedings IEEE InternationalConference Data Mining Workshops, pp. 11141119. IEEE.Genereux, M., & Evans, R. P. (2006). Distinguishing affective states weblogs. Proceedings AAAI Spring Symposium Computational Approaches AnalysingWeblogs, pp. 2729, Stanford, California.Gimpel, K., Schneider, N., OConnor, B., Das, D., Mills, D., Eisenstein, J., Heilman, M.,Yogatama, D., Flanigan, J., & Smith, N. A. (2011). Part-of-speech tagging Twitter:Annotation, features, experiments. Proceedings Annual MeetingAssociation Computational Linguistics, ACL 11, pp. 4247.Go, A., Bhayani, R., & Huang, L. (2009). Twitter sentiment classification using distantsupervision. Tech. rep., Stanford University.Habash, N., Rambow, O., & Roth, R. (2009). MADA+TOKAN: toolkit Arabictokenization, diacritization, morphological disambiguation, POS tagging, stemminglemmatization. Proceedings 2nd International Conference ArabicLanguage Resources Tools, pp. 102109, Cairo, Egypt. MEDAR Consortium.Hu, M., & Liu, B. (2004). Mining summarizing customer reviews. Proceedings10th ACM SIGKDD International Conference Knowledge Discovery DataMining, KDD 04, pp. 168177, New York, NY, USA. ACM.John, D., Boucouvalas, A. C., & Xu, Z. (2006). Representing emotional momentum withinexpressive Internet communication. Proceedings 24th International Conference Internet Multimedia Systems Applications, pp. 183188, Anaheim,CA. ACTA Press.Kiritchenko, S., Mohammad, S., & Salameh, M. (2016). SemEval-2016 Task 7: Determiningsentiment intensity english arabic phrases. Proceedings InternationalWorkshop Semantic Evaluation, SemEval 16.Kiritchenko, S., Zhu, X., Cherry, C., & Mohammad, S. (2014a). NRC-Canada-2014: Detecting aspects sentiment customer reviews. Proceedings 8th InternationalWorkshop Semantic Evaluation (SemEval 2014), pp. 437442, Dublin, Ireland.Kiritchenko, S., Zhu, X., & Mohammad, S. M. (2014b). Sentiment analysis short informaltexts. Journal Artificial Intelligence Research, 50, 723762.Liu, B., & Zhang, L. (2012). survey opinion mining sentiment analysis.Aggarwal, C. C., & Zhai, C. (Eds.), Mining Text Data, pp. 415463. Springer US.Liu, H., Lieberman, H., & Selker, T. (2003). model textual affect sensing using realworld knowledge. Proceedings 8th International Conference IntelligentUser Interfaces, IUI 03, pp. 125132, New York, NY. ACM.128fiHow Translation Alters SentimentMartnez-Camara, E., Martn-Valdivia, M. T., Urenalopez, L. A., & Montejoraez, A. R.(2012). Sentiment analysis Twitter. Natural Language Engineering, 128.Mihalcea, R., Banea, C., & Wiebe, J. (2007). Learning multilingual subjective language viacross-lingual projections. Proceedings 45th Annual Meeting AssociationComputational Linguistics, p. 976.Mihalcea, R., & Liu, H. (2006). corpus-based approach finding happiness. Proceedings AAAI Spring Symposium Computational Approaches AnalysingWeblogs, pp. 139144. AAAI Press.Mohammad, S. M. (2012). #Emotional tweets. Proceedings First Joint ConferenceLexical Computational Semantics, *SEM 12, pp. 246255, Montreal, Canada.Mohammad, S. M. (2016). Sentiment analysis: Detecting valence, emotions,affectual states text. Emotion Measurement.Mohammad, S. M., Kiritchenko, S., & Zhu, X. (2013). NRC-Canada: Building stateof-the-art sentiment analysis tweets. Proceedings 7th InternationalWorkshop Semantic Evaluation Exercises, SemEval 13, Atlanta, Georgia, USA.Mohammad, S. M., & Turney, P. D. (2010). Emotions evoked common wordsphrases: Using Mechanical Turk create emotion lexicon. ProceedingsNAACL-HLT Workshop Computational Approaches Analysis GenerationEmotion Text, pp. 2634, LA, California.Mohammad, S. M., & Turney, P. D. (2013). Crowdsourcing word-emotion associationlexicon. Computational Intelligence, 29 (3), 436465.Mohammad, S. M., & Yang, T. W. (2011). Tracking sentiment mail: genders differemotional axes. Proceedings ACL Workshop Computational ApproachesSubjectivity Sentiment Analysis, WASSA 11, pp. 7079, Portland, OR, USA.Mourad, A., & Darwish, K. (2013). Subjectivity sentiment analysis modern standardArabic Arabic microblogs. Proceedings 4th Workshop ComputationalApproaches Subjectivity, Sentiment Social Media Analysis, WASSA 13, pp.5564.Neviarouskaya, A., Prendinger, H., & Ishizuka, M. (2011). Affect analysis model: novelrule-based approach affect sensing text. Natural Language Engineering, 17,95135.Nie, J.-Y., Simard, M., Isabelle, P., & Durand, R. (1999). Cross-language informationretrieval based parallel texts automatic mining parallel texts Web.Proceedings 22nd Annual International ACM SIGIR Conference ResearchDevelopment Information Retrieval, pp. 7481. ACM.Nielsen, F. A. (2011). new ANEW: Evaluation word list sentiment analysismicroblogs. Proceedings ESWC2011 Workshop Making Sense Microposts: Big things come small packages, pp. 9398.Pak, A., & Paroubek, P. (2010). Twitter corpus sentiment analysis opinionmining. Proceedings 7th Conference International Language ResourcesEvaluation, LREC 10, pp. 13201326, Valletta, Malta.129fiMohammad, Salameh, & KiritchenkoPang, B., & Lee, L. (2008). Opinion mining sentiment analysis. Foundations TrendsInformation Retrieval, 2 (12), 1135.Pontiki, M., Galanis, D., Pavlopoulos, J., Papageorgiou, H., Androutsopoulos, I., & Manandhar, S. (2014). SemEval-2014 Task 4: Aspect based sentiment analysis. Proceedings International Workshop Semantic Evaluation, SemEval 14, pp. 2735,Dublin, Ireland.Refaee, E., & Rieser, V. (2014a). Arabic Twitter corpus subjectivity sentimentanalysis. Proceedings 9th International Conference Language ResourcesEvaluation, LREC 14, Reykjavik, Iceland. European Language Resources Association.Refaee, E., & Rieser, V. (2014b). Subjectivity sentiment analysis Arabic Twitterfeeds limited resources. Proceedings Workshop Free/Open-SourceArabic Corpora Corpora Processing Tools, p. 16.Rosenthal, S., Ritter, A., Nakov, P., & Stoyanov, V. (2014). SemEval-2014 Task 9: Sentimentanalysis Twitter. Proceedings 8th International Workshop SemanticEvaluation, SemEval 14, pp. 7380, Dublin, Ireland.Salameh, M., Mohammad, S., & Kiritchenko, S. (2015). Sentiment translation:case-study arabic social media posts. Proceedings 2015 ConferenceNorth American Chapter Association Computational Linguistics: HumanLanguage Technologies, pp. 767777, Denver, Colorado.Thelwall, M., Buckley, K., & Paltoglou, G. (2011). Sentiment Twitter events. JournalAmerican Society Information Science Technology, 62 (2), 406418.Wilson, T., Kozareva, Z., Nakov, P., Rosenthal, S., Stoyanov, V., & Ritter, A. (2013).SemEval-2013 Task 2: Sentiment analysis Twitter. Proceedings International Workshop Semantic Evaluation, SemEval 13, Atlanta, Georgia, USA.Wilson, T., Wiebe, J., & Hoffmann, P. (2005). Recognizing contextual polarity phraselevel sentiment analysis. Proceedings Conference Human Language Technology Empirical Methods Natural Language Processing, HLT 05, pp. 347354,Stroudsburg, PA, USA.Zbib, R., Malchiodi, E., Devlin, J., Stallard, D., Matsoukas, S., Schwartz, R., Makhoul, J.,Zaidan, O. F., & Callison-Burch, C. (2012). Machine translation Arabic dialects.Proceedings Conference North American Chapter AssociationComputational Linguistics: Human Language Technologies, pp. 4959. AssociationComputational Linguistics.Zhu, X., Guo, H., Mohammad, S., & Kiritchenko, S. (2014a). empirical studyeffect negation words sentiment. Proceedings Annual MeetingAssociation Computational Linguistics, ACL, Vol. 14.Zhu, X., Kiritchenko, S., & Mohammad, S. (2014b). NRC-Canada-2014: Recent improvements sentiment analysis tweets. Proceedings 8th InternationalWorkshop Semantic Evaluation (SemEval 2014), pp. 443447, Dublin, Ireland.130fiJournal Artificial Intelligence Research 55 (2016) 17-61Submitted 03/15; published 01/16Integrating Rules DictionariesShallow-Transfer Machine TranslationPhrase-Based Statistical Machine TranslationVctor M. Sanchez-Cartagenavmsanchez@dlsi.ua.esPrompsit Language EngineeringAv. Universitat s/n. Edifici Quorum IIIE-03202 Elx, SpainJuan Antonio Perez-OrtizFelipe Sanchez-Martnezjaperez@dlsi.ua.esfsanchez@dlsi.ua.esDep. de Llenguatges Sistemes InformaticsUniversitat dAlacantE-03071, Alacant, SpainAbstractdescribe hybridisation strategy whose objective integrate linguistic resourcesshallow-transfer rule-based machine translation (RBMT) phrase-based statistical machine translation (PBSMT). basically consists enriching phrase tablePBSMT system bilingual phrase pairs matching transfer rules dictionary entriesshallow-transfer RBMT system. new strategy takes advantage linguistic resources used RBMT system segment source-language sentencestranslated, overcomes limitations existing hybrid approaches treatRBMT systems black box. Experimental results confirm approach deliverstranslations higher quality existing ones, specially usefulparallel corpus available training SMT system small translating outof-domain texts well covered RBMT dictionaries. combinationapproach recently proposed unsupervised shallow-transfer rule inference algorithmresults significantly greater translation quality baseline PBSMT;case, hand-crafted resource used dictionaries commonly used RBMT.Moreover, translation quality achieved hybrid system built automaticallyinferred rules similar obtained built hand-crafted rules.1. IntroductionStatistical machine translation (SMT) (Koehn, 2010) currently leading paradigmmachine translation (MT) research. SMT systems attractive maybuilt little human effort enough monolingual parallel corpora available.However, parallel corpora always easy harvest, may even exist(under-resourced) language pairs. contrary, rule-based machine translation(RBMT) systems (Hutchins & Somers, 1992) may built without parallel corpus;however, need explicit representation linguistic information, whose codinghuman experts requires considerable amount time.Even large parallel corpus available, SMT systems may still limitations result (i) data sparseness problem makes difficult collect enoughc2016AI Access Foundation. rights reserved.fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnezphrase pairs covering inflected word forms highly inflected languages, (ii)domain problem caused training parallel corpus belongs domain differenttexts translated. One potential solution follow hybrid approach(Thurmair, 2009) combine RBMT system SMT system order mitigatelimitations. approach follow paper, linguistic resourcesshallow-transfer RBMT used enrich phrase table phrase-based SMT(PBSMT) system.Like transfer-based RBMT system, shallow-transfer RBMT systems carrytranslation process three steps: analysis source-language (SL) sentenceproduce SL intermediate representation (IR), transfer SL IR targetlanguage (TL) IR, generation final translation TL IR. Shallow-transferRBMT systems perform complete syntactic analysis input sentenceswork simple IRs consisting sequence lexical forms. lexical form compriseslemma, lexical category morphological inflection information word.shallow-transfer RBMT, Apertium system (Forcada et al., 2011) usedexperiments, analysis step, SL sentence split chunks. chunktranslated shallow-transfer rule translations concatenated orderbuild TL sentence. process similar process carried PBSMTdecoder, builds translation hypotheses segmenting SL sentence phrasestranslating SL phrase according phrase table. systems workflat sub-segments easy integrate chunks RBMT SMT phrase tablescored feature functions commonly used PBSMT. Moreover,use RBMT dictionaries shallow-transfer rules allows PBSMT decoderchoose phrase pairs go beyond word-for-word translation words RBMTdictionaries, well translating inflected word forms contain; thus alleviatingdata sparseness problem. addition, data general-purpose RBMT systemhelp reduce bias SMT system towards domain training corpus.Additionally, even rules RBMT system yet created,automatically inferred small fragment training parallel corpus means(unsupervised) rule inference approach proposed Sanchez-Cartagena, Perez-Ortiz,Sanchez-Martnez (2015). better use therefore made training parallel corpusRBMT dictionaries existing approaches (Schwenk, Abdul-Rauf, Barrault, &Senellart, 2009) simply add dictionaries phrase table. combining ruleinference algorithm hybridisation approach, translation knowledge containedparallel corpus generalised sequences words observedcorpus, share lexical category morphological inflection information wordsobserved.enrichment PBSMT models RBMT linguistic data already exploredauthors (see Section 2.1); however, approach presented paper firstone specifically designed use shallow-transfer RBMT takes advantageway linguistic resources used RBMT system. bestknowledge, general approach Eisele et al. (2008), described Section 2.1,hybrid approach literature applied shallow-transfer RBMT systems.experimental results show hybrid approach outperforms strategy developed Eisele et al. (2008). Moreover, performance hybrid system built using18fiIntegrating Shallow-Transfer Rules Statistical Machine Translationautomatically inferred rules par hybrid system built hand-craftedrules. also worth pointing system (Sanchez-Cartagena, Sanchez-Martnez, &Perez-Ortiz, 2011b) built approach using hand-crafted rules Apertiumproject (Forcada et al., 2011) one winners1 pairwise manual evaluationWMT 2011 shared translation task (Callison-Burch, Koehn, Monz, & Zaidan, 2011)SpanishEnglish language pair. hybridisation approach presented paper,together aforementioned rule inference algorithm, contribute alleviatingdata sparseness problem SMT systems highly inflected languagesinvolved reducing corpus size requirements regards building PBSMT systems.remainder paper organised follows. Section 2 reviews related workhybrid machine translation, including description limitations general hybridisation approach proposed Eisele et al. (2008). Section 3 describes hybridisationstrategy set different alternatives scoring phrase pairs generatedlinguistic resources RBMT system. Two different sets experiments, integrate data Apertium RBMT platform (Forcada et al., 2011), describedorder evaluate hybridisation strategy (Section 4) assess whether automatically inferred rules replace hand-crafted ones hybrid system (Section 5).paper ends human evaluation error analysis (Section 6) concludingremarks (Section 7).2. Related WorkHybrid approaches related presented paper split integrate RBMT elements SMT system (sections 2.1 2.2) integrateSMT elements RBMT architecture (Section 2.3).2 Approaches first groupturn split two groups: use linguistic information existingRBMT system (Section 2.1) use linguistic resources inferred parallelcorpus SMT models estimated (Section 2.2).2.1 Integrating Hand-Crafted Linguistic Resources SMTBilingual dictionaries frequently reused resource RBMT;added SMT systems since early days (Brown et al., 1993). One simpleststrategies, already put practice Apertium bilingual dictionaries (Tyers, 2009), consists adding dictionary entries directly training parallelcorpus. addition obvious increase lexical coverage, Schwenk et al. (2009) statequality alignments obtained also improved words bilingualdictionary appear sentences parallel corpus. However, guaranteedthat, following strategy, multi-word expressions bilingual dictionary ap1. system found statistically significantly better using sign test p 0.10.2. meant strict classification: approaches listed section couldincluded groups. Moreover, approaches outputs different MT systemscombined without making modification inner workings systems involved,system combination (Rosti, Matsoukas, & Schwartz, 2007) listed review because, unlikeapproach, involve creation new MT architectures combine elementsSMT RBMT.19fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnezpear SL sentences translated may split smaller unitsphrase-extraction algorithm. Dictionaries also added SMT systemstogether rule-based enhancements, work Popovic Ney (2006),propose combining dictionaries use hand-crafted rules order reorderSL sentences match structure TL.approaches take advantage full RBMT system. Eisele et al. (2008) presentstrategy based augmentation phrase table include information providedRBMT system. approach treats RBMT system black box, i.e.,algorithm concerned inner workings RBMT system. sentencestranslated hybrid system first translated RBMT system smallphrase table obtained resulting parallel corpus (from on, synthetic corpus).new phrase table directly added original phrase table obtainedtraining parallel corpus. approach following limitations, overcomehybrid approach described paper:Deficient segment alignment. phrase pairs extracted syntheticcorpus usual procedure followed PBSMT (Koehn, 2010, 5.2.3), unaligned words included multiple phrase pairs, since evidencecorrespondence language, phrase pairs made solely unalignedwords extracted. word alignments incorrect, phrase pairsmutual translation may extracted correct phrase pairs present parallelsentences may obtained.3 less reliable word alignments are,severe problem becomes.word alignment synthetic corpus obtained Eisele et al. (2008) mayunreliable owing vocabulary mismatch text translatedalignment models, inferred training corpus.4 limitationbecomes evident text translated share domaintraining corpus, actually data RBMT systemuseful.5Relying word alignments reasonable strategy extracting phrase pairsparallel corpus know built. However,know RBMT system used generate TL side corpus,3. Consider following segment EnglishSpanish parallel sentence: Barcelona City Council Ayuntamiento de Barcelona. word alignment segments link Barcelonalanguages, incorrect phrase pairs Barcelona City Council Barcelona would extracted,whereas correct phrase pair City Council Ayuntamiento would extracted.4. Alignment models contain information words test corpus presenttraining corpus, words therefore aligned likely phrase pairsmutual translation extracted them.5. problem could alleviated building alignment models concatenation syntheticcorpus training corpus, incrementally training (Gao, Lewis, Quirk, & Hwang, 2011)word alignment models. former would computationally expensive, since process wouldcarried time new text translated resulting hybrid system (e.g. buildingword alignment models EnglishSpanish parallel corpus 600 000 sentences describedSection 4.1 took around 6 hours AMD Opteron 2 Ghz processor). latter likely causealignment errors infrequent words synthetic corpus found training corpusinvolved.20fiIntegrating Shallow-Transfer Rules Statistical Machine Translationprecise phrase extraction mechanism takes advantage RBMTsystem uses dictionaries shallow-transfer rules segment SL sentencesused.Inadequate balance different types phrase pairs obtained.probabilities derived Eisele et al. (2008) phrase pairs extractedsynthetic corpus added phrase table consistentindependently estimated two different corpora. one hand, SLphrase translated way training corpus RBMT system,probability corresponding phrase pair increased comparisonphrase pairs SL phrase translated differentway. hand, translations SL phrase differproduced RBMT system, frequency training corpus takenaccount scoring corresponding phrase pairs, noise may consequentlyintroduced case SL phrases low frequency training corpus.instance, phrase pair extracted training corpus whose SL phrase appearsless reliable receive lower score phrase pair whoseSL phrase appears 10 000 times (see Section 3.2). overcome limitationfollowing sophisticated scoring scheme joins synthetic phrase pairsphrase pairs obtained training corpus single list computingphrase translation probabilities (see Section 3.2.3).Another interesting approach Enache, Espana-Bonet, Ranta, Marquez(2012), interlingua RBMT system developed limited domain patenttranslation integrated PBSMT architecture generating synthetic phrase pairschunks extracted SL sentences parsed RBMT system.6philosophy behind hybrid approach synthetic phrase pairs generatedchunks matched shallow-transfer rules. However, significant differences existmethod used score phrase pairs generated RBMT system. Enache et al.use pre-defined single value source-to-target target-to-source phrase translationprobabilities lexical weightings synthetic phrase pairs. consequencesynthetic phrase pairs equiprobable relative weight (compared phrasepairs extracted training parallel corpus) optimised tuning stepSMT training process. proposal, however, relative weight synthetic phrasepairs optimised tuning process thanks use binary feature function,phrases translated way parallel corpus RBMT system receivehigher scores, lexical translation probabilities synthetic phrase pairscomputed based principles SMT: taking account translationsindividual words make phrases.Finally, Rosa, Marecek, Dusek (2012) create set rules appliedoutput SMT system order fix common errors. main differenceproposal lies fact that, although rules similartransfer rules, operate TL side, syntactic analysis performedapplying them.6. parse tree may obtained sentences follow usual structurerestricted domain. occurs case 66.7% sentences test set.21fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez2.2 Adding Morphological Information SMThybridisation approach combined rule inference approach describedSanchez-Cartagena et al. (2015) order integrate set structural transfer rulesinferred SMT training parallel corpus, thereby extending PBSMT modelsnew linguistic information. Since shallow-transfer rules operate lexical forms madelemma, lexical category morphological inflection information, combinationtwo approaches seen novel way extending PBSMT morphologicalfeatures.manner, resulting approach related factored translation models (Koehn& Hoang, 2007), extension PBSMT word replaced setfactors represent lemma, lexical category, morphological inflection information.phrase-based translation model inferred lemmas independent one lexicalcategories morphology. word-based generation model, inferredadditional monolingual data, maps combinations lemmas, lexical category morphological inflection information inflected word forms. main differencesfactored models hybrid approach follows:factored models, translation lemmas morphological information completely independent. types translations combined order generatefinal sequence surface forms (running words), combinatorial explosion likelyproduced (too many combinations lemmas morphological informationneed scored). combinations cannot explored, correct translationhypotheses may pruned (Bojar & Hajic, 2008; Graham & van Genabith, 2010).Moreover, idiomatic translations follow general morphological rulesTL may assigned low probability translation model, eventhough would high probability phrase table built surface forms.strategy differs one followed combination twoapproaches, translation hypotheses built surface-form-based models(like usually used PBSMT) enriched synthetic phrasepairs generated rules inferred training corpus. complexity dealing translations lemmas morphological inflection information moveddecoding training time, rule inference algorithm deals it.7hybrid approach works existing bilingual dictionaries, factored modelsuse bilingual dictionaries all. consequence, translate morphological inflection information different way. factored models probabilityTL morphological inflection factors depends solely morphological inflectionfactors SL sentence. contrast, transfer rules used method obtain morphological inflection attributes TL words either SL wordstranslation according bilingual dictionary. makes formalismexpressive eases treatment certain linguistic phenomena. Consider,instance, case morphological inflection attribute7. worth noting Graham van Genabith (2010) proposed strategy partially mitigatingissues caused fact factored models treat lemmas morphological information totallyindependent elements: extraction training parallel corpus factored templates,phrases decomposed lemma morphological information translation.22fiIntegrating Shallow-Transfer Rules Statistical Machine Translationexists TL (such gender translating English Spanish French).hybrid approach, structural transfer rule gender number agreementnoun adjective would assign gender translationTL according bilingual dictionary SL noun TL noun adjective.type rule inferred small parallel corpus. factored models,however, translation model would presumably assign similar probabilities TLnoun-adjective sequences genders, success agreement woulddepend solely ability TL model differentiate them.relevant approaches morphological attributes integratedtranslation model SMT system found literature. Green DeNero (2012)define new feature function models morpho-syntactic agreements, factoredlanguage models (Kirchhoff & Yang, 2005) assign probabilities TL sentences dependingsequences word forms morphological features, among factors.approaches differ strategy presented paper mainlyperform generalisation enriches translation model translations sequencesSL words unseen training corpus.Riezler Maxwell III (2006) went also added syntactic informationSMT. developed hybrid RBMT-SMT system works follows. SL sentence parsed lexical functional grammar (Riezler et al., 2002) obtain SLintermediate representation (IR). SL IR transferred TL IR applyingset probabilistic rules obtained parallel corpus. rule contains set scoresinspired present phrase table PBSMT system. Finally, TL sentencegenerated TL IR. Since SL sentence parsed many different waysmany different TL IRs generated applying different rules, TL model alsoused addition aforementioned phrase-table-like features. featuresfinally combined means log-linear model, weights optimised meansminimum error rate training (Och, 2003) SMT. results show grammarused able completely parse half sentences test set (partial parse treesobtained instead, resulting translation much worse translationfully parsed sentences), considering sentences could fully parsed,statistically significant improvement PBSMT system trained usingdata. However, human evaluation showed improvement grammaticalitytranslations. main differences proposal following:first, approach Riezler Maxwell III use existing bilingual dictionaries;second, uses syntactic information allows system perform deeper linguistic analysis expense able fully parse input sentences,results drop translation performance. contrast, approach works lexicalcategories morphological inflection information robust ungrammaticalinput.2.3 Integrating Statistical Elements RBMTRegarding enhancement RBMT systems statistical elements, worth notingRBMT systems often use statistical methods part-of-speech tagging (Cutting, Kupiec, Pedersen, & Sibun, 1992) parsing (Federmann & Hunsicker, 2011). Besides23fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnezcomponents, elements SMT integrated RBMT, causing greaterchanges RBMT architecture. instance, multiple hypotheses generatedtransfer step, probable one chosen according TLmodel (Lavie, 2008; Carl, 2007). Another option use phrase pairs instead transferrules transfer step, keep using RBMT analysis generation modules (Crego, 2014). approach Riezler Maxwell III (2006), discussed previously,also uses TL model order choose among translations generated applying rules,integrates elements SMT, feature functions usually encodedSMT phrase table.different alternative consists taking advantage full syntactic analysis performed syntactic-transfer RBMT systems create structure TL sentence,insert phrase translations PBSMT phrase table nodes TLparse tree (Labaka, Espana-Bonet, Marquez, & Sarasola, 2014). SMT, final translation maximum probability according TL model scoresphrase table phrases inserted tree obtained. However,phrase reordering allowed, since structure TL sentences guidedparse tree. set-up also followed systems proposed authors (Federmann et al., 2010; Zbib et al., 2012).3. Enhancement Phrase-Based SMT Shallow-Transfer LinguisticResourcesaccess inner working RBMT system, correspondenceSL segments input sentence translations computed without relyingstatistical word alignments. fact, even necessary translate wholesentence RBMT system. individual translation according bilingualdictionary word, translation segment matches shallow-transferrule constitute minimum set bilingual phrases ensures linguisticinformation RBMT system extracted. Another advantage methodapproach Eisele et al. (2008) lies fact rules match SL segmentwould applied shallow-transfer RBMT system greedyoperating mode also taken account.8 Thus, hybrid strategy first generatessynthetic phrase pairs RBMT linguistic data SL text translated,integrates PBSMT models without decomposition.8. Consider, instance, English sentence visited Bob Alices dog sleeping translatedSpanish shallow-transfer RBMT system. Let us suppose following segmentssentence match shallow-transfer rule: visited matches rule removes personal pronoun (itomitted Spanish), adds corresponding preposition generates visite a; Bob Alicesdog matches rule processes Saxon genitive, adds preposition determiner neededSpanish generates el perro de Bob Alice; Alices dog also matches rule processesSaxon genitive noun phrase acting owner contains single proper noun, generates elperro de Alice. RBMT engine chooses rules applied left-to-right, longest matchmanner, produces visite al perro de Bob Alice estaba durmiendo, means visited Bobs dogAlice sleeping. right translation, visite Bob el perro de Alice estaba durmiendo,obtained rule matches Alices dog applied. Eisele et al.s (2008) method appliedbuild hybrid system, phrase pairs correct translation visited Bob visite BobAlices dog sleeping el perro de Alice estaba durmiendo would extracted.24fiIntegrating Shallow-Transfer Rules Statistical Machine Translation3.1 Generation Synthetic Phrase Pairsway synthetic phrase pairs generated differs depending linguistic resources bilingual dictionaries shallow-transfer rules used. generatebilingual phrase pairs bilingual dictionary, SL surface forms recognisedshallow-transfer RBMT system corresponding SL IRs listed; then,SL IRs translated bilingual dictionary obtain corresponding TL IRs;finally, corresponding TL word forms obtained means RBMT generationmodule.9 instance, generation phrase pairs EnglishSpanish bilingual dictionary Apertium RBMT platform, mappings SL surface formslexical forms houses house N-num:pl however however ADV generated.translated TL bilingual dictionary: resulting phrase pairshouses casas however sin embargo. Since dictionaries may contain multi-wordunits, phrase pairs generated may contain one word (SL TL)sides. Note that, unlike method Eisele et al. (2008), sentences translatedused. Thus, generation phrase pairs bilingual dictionary needsperformed rather time new text translated.Bilingual phrase pairs matching structural transfer rules generated similar way,using SL text translated. Thus, process repeated time new texttranslated hybrid system.10 First, SL sentences analysed orderobtain SL IRs, sequences lexical forms match structuraltransfer rule passed rest RBMT pipeline obtain translations.sequence SL lexical forms matched one structural transfer rule,used generate many bilingual phrase pairs different rules matches.differs way Apertium translates, since cases longest rulewould applied.Let us suppose English sentence little dogs run fast translated Spanish.analysed Apertium following sequence lexical forms: POSP-p:1.num:pl,little ADJ, dog N-num:pl, run VERB-t:inf, fast ADV.11 RBMT system containedtwo rules, one performs swapping number gender agreementadjective noun it, another matches determiner followedadjective noun, swaps adjective noun makes three words agreegender number, segments little ADJ dog N-num:pl POSP-p:1.num:pllittle ADJ dog N-num:pl would used generate following bilingual phrase pairs:little dogs perros pequenos little dogs mis perros pequenos.9. TL IR contains missing values morphological inflection attributes, different TL phrasepossible value attribute generated. instance, mapping SL (English)word form beautiful SL lexical form beautiful ADJ-num:sg two EnglishSpanish phrase pairsgenerated: beautiful bonito beautiful bonita; first phrase adjective beautifultranslated masculine, whereas second case translated feminine.10. step carried without dramatically reducing decoding efficiency thanks factmany steps Apertium translation pipeline implemented partial finite-state transducers (Roche & Schabes, 1997) able process tens thousands words per second averagedesktop computer (Forcada et al., 2011, 4.1).11. meaning abbreviations used represent lexical categories are: POSP = possessive pronoun;ADJ = adjective; N = common noun; VERB = verb; ADV = adverb. Regarding morphologicalinflection information, p:1 = first person, num:pl = plural number t:inf = infinitive mood.25fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-MartnezNote that, unlike generation bilingual phrases bilingual dictionary,generation bilingual phrase pairs shallow-transfer rules guided texttranslated.12 decided order make approach computationallyfeasible avoid meaningless phrases. Consider, instance, rule triggereddeterminer followed adjective noun English. Generating possiblephrase pairs virtually matching rule would involve combining determinersdictionary adjectives nouns, causing generation manymeaningless phrases, wireless boy el nino inalambrico.phrase pairs generated assigned frequency 1, sincegenerated actual parallel corpus. frequencies used score phrasepairs, described next section.3.2 Scoring Synthetic Phrase PairsPBSMT systems usually attach 4 scores (Koehn, 2010, Sec. 5.3) every phrase pairphrase table (translation model): source-to-target target-to-source phrase translationprobabilities source-to-target target-to-source lexical weightings. source-totarget translation probability (t|s) phrase pair (s, t) usually computed meansEq. (1), count() stands frequency phrase pair list phrase pairsextracted training parallel corpus.count(s, t)ti count(s, ti )(t|s) = P(1)purpose lexical weightings act back-off scoring phrase pairslow frequency (Koehn, 2010, Sec. 5.3.3). lexical weighting score phrase pairusually computed product lexical translation probability source wordtarget word aligned. Lexical translation probabilities obtainedlexical translation model estimated maximum likelihood word alignmentsparallel corpus.values four scores synthetic phrase pairs calculated differentways may affect scores phrase pairs extracted original trainingcorpus. respect, desirable scoring method applied syntheticcorpus-extracted phrase pairs increases probability phrase pairs whose SLphrase translated way training corpus RBMT system.addition, scoring method also consider frequency parallel corpusSL phrases translation performed RBMT system agreefound training corpus. Finally, also desirable addition syntheticphrase pairs statistical models involve big computational effort, sinceexecuted every text translated.12. bilingual phrase pairs generated segments training corpus match rule,method would less effective dealing data sparseness, since synthetic phrases generatedrules would available sequences words present training corpus.26fiIntegrating Shallow-Transfer Rules Statistical Machine Translationsection, propose method13 integrating set synthetic phrase pairs obtained RBMT data PBSMT system meets aforementioned requirements. remainder section contains, addition method, descriptionphrase scoring approaches found literature limitations.14strategies presented evaluated described Section 4.3.2.1 Creating Additional Phrase TableOne simple strategy integrating synthetic phrase pairs hybrid SMT systemputting different (synthetic) phrase table, Koehn Schroeder(2007) propose context domain adaptation. decoder builds hypotheses,looks phrase pairs phrase tables phrase pair found both,one instance phrase table used build hypotheses. reasonauthors refer approach alternative decoding paths. scorephrase table receives different weight tuning process, helphybrid system obtain appropriate relative weighting sources phrase pairs.scoring strategy used integrate synthetic phrase pairs PBSMT models, phrase translation probabilities synthetic phrase table computedmeans Eq. (1), done phrase pairs extracted parallel corpus,using counts within set synthetic phrase pairs. lexical weighting scoresphrase pair computed set word alignments lexical translationmodel described Koehn (2010, 5.3.3). lexical translation model usedestimated synthetic corpus generated RBMT bilingual dictionary,described Section 3.1; word alignments used obtained tracing backoperations carried RBMT engine.15Since phrase tables computed totally independent way, phrase translation probabilities phrase pairs appear phrase tables increasedcomparison phrase pairs appear one them. Consider,instance, SL phrase two different translations according RBMT system:b c. source-to-target phrase translation probabilities synthetic phrase table13. method already described Sanchez-Cartagena, Sanchez-Martnez, Perez-Ortiz(2011a); however, first time systematically compared scoring methodsfound literature evaluated automatically inferred rules.14. Methods relevance phrase tables combined must defined advance (i.e.,primary secondary phrase table), fill-up (Bisazza, Ruiz, & Federico, 2011),described section evaluated. leave responsibility adaptingrelative relevance types phrase pairs type texts translated tuning stepSMT training process.15. Apertium engine keeps track step translation pipeline input wordoutput word obtained. path starting input SL surface form followedorder obtain TL surface form aligned it. exception made step pipelineconverts input word multiple output words vice-versa. case, words involvedleft unaligned; done avoid generating many word alignments could incorrect. Let ussuppose Spanish sentence Por otra parte mis amigos americanos han decidido venir translatedEnglish hand American friends decided come Apertium. Spanishphrase Por otra parte analysed Apertium single lexical form. translatedEnglish, produces segment hand generation step. exceptionmade, SL word por would aligned four TL words on, the, hand SLwords otra parte would also aligned set TL words.27fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnezresulting phrase pairs would synth (b|a) = 0.5 synth (c|a) = 0.5. Let us alsosuppose that, extracting phrase pairs parallel corpus, phrase pairs (a, b)(a, d) frequency, phrase pairs source.resulting source-to-target phrase translation probabilities would corpus (b|a) = 0.5corpus (d|a) = 0.5. Although evidence suggests b likelytranslation c d, three translations probability.3.2.2 Phrase Table Linear InterpolationAlternatively, two phrase tables built, linearly interpolatedsingle one (Sennrich, 2012, 2.1). scores attached phrase pairresulting phrase table obtained linear interpolation value corresponding score corpus-extracted phrase table synthetic phrase table.instance, source-to-target phrase translation probability computed shown Eq. (2)below, countsynth () frequency phrase pair list phrase pairsgenerated RBMT system, countcorpus () frequency phrase pair listphrase pairs extracted parallel corpus corpus synth weightsphrase tables; obviously corpus + synth = 1. weights optimised meansperplexity minimisation phrase table built development set (Sennrich, 2012,2.4).countsynth (s, t)countcorpus (s, t)+ synth Pcount(s,)corpustiti countsynth (s, ti )(t|s) = corpus P(2)method, unlike uses two independent phrase tables describedSection 3.2.1, increases phrase translation probability phrase pairs appearphrase tables present one them. phrasepairs (a, b), (a, c) (a, d) mentioned above, resulting probabilities would (b|a) =0.5synth + 0.5corpus = 0.5; (c|a) = 0.5synth ; (d|a) = 0.5corpus . However,method use frequency source phrases training corpusinterpolating phrase tables. source phrase x found trainingcorpus, aligned y, possible translation according RBMTsystem z, source-to-target phrase translation probabilities phrase pairs would(y|x) = corpus (z|x) = synth , respectively. x found 10 000 timestraining corpus, always translated y, probabilities would exactlyweights corpus synth phrase pairs. However,phrase pair (x, y) much reliable found training corpus 10 000 timesfound once. probabilities resulting phrase table reflecteddifference, decoder would presumably able choose better phrase pairsproduce reliable translations.3.2.3 Proposed Strategy: Directly Expanding Phrase TableOne way taking account absolute frequency different phrases trainingcorpus join synthetic phrase pairs corpus-extracted phrase pairs calculatephrase translation probabilities means relative frequency usual. source-totarget phrase translation probabilities resulting phrase table therefore computed28fiIntegrating Shallow-Transfer Rules Statistical Machine Translationfollows:countcorpus (s, t) + countsynth (s, t)ti (countcorpus (s, ti ) + countsynth (s, ti ))(t|s) = P(3)Since countsynth () = 1 synthetic phrase pairs, synthetic phrase pairshare SL side corpus-extracted phrase pair, source-to-target phrase translationprobability synthetic phrase pair may small compared phrase pairextracted training corpus.16 Depending texts translated, maydesirable synthetic phrase pair higher phrase translation probabilitycorpus-extracted phrase pair SL side. order adapt relative weighttexts translated, additional binary feature function flags syntheticphrase pairs added phrase table.17lexical weighting scores phrase table built combination methodobtained using lexical translation model types phrase pairs.model (actually, one model source-to-target another model target-to-sourcelexical weighting) obtained concatenation training parallel corpussynthetic phrase pairs generated RBMT bilingual dictionary. lexical weightingscores computed using word alignments obtained statistical methodscorpus-extracted phrase pairs, usual (Koehn, 2010, 5.2.1), obtainedtracing back operations carried different translation steps Apertiumsynthetic phrase pairs (see Section 3.2.1).3.2.4 Augmenting Training CorpusFinally, simplest approach involves appending RBMT-generated phrase pairstraining corpus running usual PBSMT training algorithm. Unlike previousapproaches, improves alignments original training corpus enricheslexicalised reordering model (Koehn, 2010, 5.4.2), addition phrase table.phrase extraction algorithm (Koehn, 2010, 5.2.3) may, however, split resulting bilingual phrase pairs smaller units may signify multi-word expressionstranslated way appear RBMT bilingual dictionary.16. applies phrase pairs share TL side target-to-source phrase translationprobability.17. order take account absolute frequencies parallel corpora two phrasetables combined obtained, Sennrich (2012, 4.2) proposes weighted counts interpolation method, similar presented paper. two main differencesapproaches. Firstly, order adapt weight types phrases textstranslated, weighted counts approach multiplies frequency phrase pair factorbuilding phrase table; depending origin phrase, different factor used.contrary, method adds binary feature function phrase table. secondly, weightedcounts approach optimises factors determine relative weight type phrase pairmeans perplexity minimisation phrase table built development set (Sennrich, 2012,2.4) isolation, i.e., connection rest elements present log-linear model.contrast, new method optimises weight binary feature function together restelements log-linear model tuning process. Given poor results obtainedphrase table interpolation method weights also optimised means perplexityminimisation experiments reported Section 4.2, weighted counts includedexperimental setup.29fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-MartnezAlthough strategy feasible real-world environment computational cost word aligning whole training corpus document translated,18 worth evaluating strategy enriches datalexicalised reordering model obtained.4. Evaluation Hand-Crafted Resourcesset experiments whose objective evaluating feasibility hybridisationstrategy described Section 3 using hand-crafted linguistic resources Apertium RBMT platform conducted. compare, different language pairs, training corpus sizes text domains, translation quality achieved baseline PBSMTsystem, RBMT system data extracted, Eisele et al.s (2008)approach set hybrid systems using phrase scoring alternatives describedSection 3.2.4.1 Experimental Setuplanguage pairs used evaluation BretonFrench19 EnglishSpanish.20BretonFrench chosen problem resource scarceness:around 60 000 parallel sentences available pair (Tyers, 2009; Tiedemann,2012). EnglishSpanish chosen wide range parallel corpora available allows us perform in-domain out-of-domain evaluations.Moreover, Spanish highly inflected language English not, resultsdirections EnglishSpanish language pair allow us evaluate detail impacthybrid strategy translation highly inflected languages.translation model PBSMT systems EnglishSpanish trainedEuroparl parallel corpus (Koehn, 2005) version 5; 21 TL model trainedcorpus. cases, Q4/2000 portion set aside evaluationpurposes. Different subsets parallel corpus different number sentencesused build systems; however, cases language model trainedwhole TL side Europarl corpus. subsets randomly chosenway larger corpora include sentences smaller ones. different subcorporacontain 10 000, 40 000, 160 000, 600 000 1 272 260 sentences; latter correspondswhole training corpus.Regarding BretonFrench, translation model built using freelyavailable parallel corpus language pair (Tyers, 2009; Tiedemann, 2012),contains short sentences tourism computer localisation domains. Differenttraining corpus sizes used too, namely 10 000, 25 000 54 196 parallel sentences.latter corresponds whole corpus except subsets reserved tuningtesting. EnglishSpanish language pair, sentences randomly chosenway larger corpora include sentences smaller ones. TL model18. Recall different set synthetic phrase pairs generated SL text translated.19. FrenchBreton RBMT system Apertium platform.20. symbol means first language acts SL second one TL.symbol means evaluation performed translation directions.21. http://www.statmt.org/europarl/archives.html#v530fiIntegrating Shallow-Transfer Rules Statistical Machine Translationlearnt monolingual corpus built concatenating target sidewhole parallel training corpus French Europarl corpus provided WMT 2011shared translation task.22Although larger monolingual corpora available target languages included evaluation setup, used experiments focused evaluating impact RBMT data PBSMT translation model.learning TL model monolingual corpus exceed sizebiggest parallel corpus used experiments, risk huge language modelovershadow impact RBMT data SMT translation model reduced. Notethat, real-world environment, size TL model may need limitedhybrid MT system required reduced memory footprint, example,going executed handheld device.23BretonFrench systems tuned using 3 000 parallel sentences randomly chosenavailable parallel corpus evaluated using another randomly chosen subsetsize; obviously subsets used training. in-domain evaluationcould performed language pair. Regarding EnglishSpanish, in-domainout-of-domain evaluations carried out. former performed tuningsystems 2 000 parallel sentences randomly chosen Q4/2000 portion Europarlv5 corpus (Koehn, 2005) evaluating 2 000 random parallel sentencesportion corpus; special care taken avoid overlapping testtuning sets. out-of-domain evaluation performed using newstest2008set tuning newstest2010 test testing; sets belong news domaindistributed part WMT 2010 shared translation task.24 Table 1 summarisesdata concerning corpora used experiments. Sentences contain40 tokens removed parallel corpora, customary, order avoidproblems word alignment tool GIZA++ (Och & Ney, 2003).25experiments carried release 2.1 free/open-source PBSMTsystem Moses (Koehn et al., 2007) together SRILM language modelling toolkit(Stolcke, 2002), used train 5-gram language model using interpolated KneserNey discounting (Goodman & Chen, 1998). Word alignments computed meansGIZA++ (Och & Ney, 2003). weights different feature functions optimisedmeans minimum error rate training (Och, 2003). parallel corpora lowercasedtokenised training, test sets used evaluate systems.hand-crafted shallow-transfer rules dictionaries borrowed Apertium platform (Forcada et al., 2011). particular, engine linguistic resourcesEnglishSpanish, BretonFrench downloaded Apertium Subversion22. http://www.statmt.org/wmt11/translation-task.html23. also EnglishSpanish parallel corpora available, usedexperiments one main objectives hybrid approach presented paper, pointedintroduction, alleviate data sparseness problem SMT.24. http://www.statmt.org/wmt10/translation-task.html25. Preliminary experiments showed that, sentences contained 40 tokens, GIZA++able align them. Sentences 40 tokens also removed tuningtest sets order ensure approach Eisele et al. (2008) able extract phrasepairs needed. Recall method needs align sentences test set RBMTtranslations.31fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-MartnezCorpusLanguage model (English)Language model (Spanish)trainingEuroparl tuningEuroparl testingnewstest2012 tuningnewstest2013 testingSource#words #voc209 562 11 561836 194 20 8833 341 577 36 79812 546 758 61 65426 595 542 82 58542 6425 15742 1145 08034 8786 20948 3677 701#sentences1 650 1521 650 15210 00040 000160 000600 0001 272 2602 0002 0001 7322 215Target#words#voc45 712 294 110 01847 734 244 165 896216 18715 884862 78930 5833 452 06755 58412 971 03594 31527 496 270 125 81343 3486 41142 6616 28936 4107 08550 7459 277(a) EnglishSpanishCorpusLanguage model (French)trainingtuningtesting#sentences2 041 62510 00025 00054 1963 0003 000Source#words #voc146 255 16 711365 856 27 606795 045 41 15744 5868 34043 2768 119Target#words#voc60 356 583 155 028146 55617 588369 39628 333801 78040 27945 0868 90743 4198 832(b) BretonFrenchTable 1: Number sentences, words, size vocabulary training, tuningtest sets used experiments.repository.26 Apertium linguistic data contains 326 228 entries EnglishSpanishbilingual dictionary, 284 EnglishSpanish shallow-transfer rules 138 SpanishEnglishshallow-transfer rules. Regarding BretonFrench, bilingual dictionary contains 21 593entries 254 shallow-transfer rules.27language pair, domain, training corpus size, following systemsbuilt evaluated:baseline: standard PBSMT system.2826. Revisions 24177, 22150 28674, respectively.27. transfer phase split Apertium three steps (Forcada et al., 2011) language pairsused, step works set rules. Specifically, Apertium linguistic data contains216 chunker rules, 60 interchunk rules, 7 postchunk rules EnglishSpanish; 106 chunker rules,31 interchunk rules, 7 postchunk rules SpanishEnglish; 169 chunker rules, 79 interchunkrules 6 postchunk rules BretonFrench.28. features baseline system WMT 2011 shared translation task: http://www.statmt.org/wmt11/baseline.html.32fiIntegrating Shallow-Transfer Rules Statistical Machine TranslationApertium: Apertium shallow-transfer RBMT engine, dictionariestransfer rules borrowed.extended-phrase: hybrid system described Section 3 following strategyscoring phrase pairs generated RBMT data described Section 3.2.3.extended-phrase-dict: above, using dictionaries RBMTsystem (without shallow-transfer rules). comparison systemextended-phrase permits evaluation impact use shallow-transferrules.extended-corpus: hybrid system described Section 3 following strategy usedscore synthetic phrase pairs simply involves adding synthetic phrasepairs training corpus (see Section 3.2.4).two-phrase-tables: hybrid system described Section 3 following strategy usedscore synthetic phrase pairs based two independent phrase tables (Koehn &Schroeder, 2007) (see Section 3.2.1).interpolation: hybrid system described Section 3 following strategy usedscore synthetic phrase pairs based linear interpolation two phrasetables (Sennrich, 2012, 2.1) (see Section 3.2.2). interpolation weights obtained means perplexity minimisation phrase table built tuningset.Eisele: approach Eisele et al. (2008), using alignment model learnttraining corpus obtain word alignments source sentencesRBMT-translated sentences.4.2 Results DiscussionFigures 15 show BLEU (Papineni, Roukos, Ward, & Zhu, 2002) automatic evaluationscore systems evaluated; TER (Snover, Dorr, Schwartz, Micciulla, & Makhoul, 2006)METEOR (Banerjee & Lavie, 2005) behave similar manner. addition, statistical significance difference BLEU, TER METEOR scores obtainedhybridisation approach extended-phrase (see Section 3.2.3) obtainedsystems computed means paired bootstrap resampling (Koehn,2004) (p 0.05; 1 000 iterations).29 results pair-wise comparison reportedtable, included figure, cell represents reference systemapproach extended-phrase compared training corpus size; tablecontains results three evaluation metrics: BLEU (B), TER (T) METEOR(M). arrow pointing upwards () means extended-phrase outperforms referencesystem, arrow pointing downwards () means reference system outperformsextended-phrase, equal sign (=) means difference systemsstatistically significant.29. extended-phrase compared systems expected achieve highesttranslation quality different hybrid approaches, theory overcomes limitationsapproaches (see Section 3.2).33fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez0.320.3BLEU score0.280.260.160.14baselineextended-phraseextended-phrase-dictextended-corpus10000two-phrase-tablesinterpolationEiseleApertium40000160000600000Size training corpus (in sentences)1272260(a) BLEU scores.systemmetricbaselineApertiumextended-phrase-dictextended-corpustwo-phrase-tablesinterpolationEisele10 000BTM=========40 000BTM=============160 000BTM=========600 000BTM===========1 272 260BTM============(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phrasemethods evaluated (a method per row). Columns represent training corpus sizesevaluation metrics: BLEU (B), TER (T) METEOR (M). means extended-phraseoutperforms reference method statistically significant margin, means opposite,= means statistically significant difference them.Figure 1: EnglishSpanish in-domain evaluation, automatic evaluation scores obtained baseline PBSMT system, Apertium, hybrid approaches describedSection 3.2, hybrid approach Eisele et al. (2008). table shows pair-wisecomparison system extended-phrase (see Section 3.2.3).results show hybrid approach described Section 3 (extended-phrase)outperforms RBMT baseline PBSMT system statistically significantmargin different scenarios. Namely, translating out-of-domain texts (texts whosedomain different domain parallel corpus used; occurs trainingcorpus sizes language pairs) translating in-domain texts SMT systemtrained relatively small parallel corpus. Thus, found literature (see Section 2),34fiIntegrating Shallow-Transfer Rules Statistical Machine Translation0.280.26BLEU score0.240.220.20.180.160.1410000baselineextended-phraseextended-phrase-dictextended-corpustwo-phrase-tablesinterpolationEiseleApertium40000160000600000Size training corpus (in sentences)1272260(a) BLEU scores.systemmetricbaselineApertiumextended-phrase-dictextended-corpustwo-phrase-tablesinterpolationEisele10 000BTM=====40 000BTM====160 000BTM===600 000BTM==1 272 260BTM==(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phrasemethods evaluated (a method per row). Columns represent training corpus sizesevaluation metrics: BLEU (B), TER (T) METEOR (M). means extended-phraseoutperforms reference method statistically significant margin, means opposite,= means statistically significant difference them.Figure 2: EnglishSpanish out-of-domain evaluation, automatic evaluation scoresobtained baseline PBSMT system, Apertium, hybrid approaches describedSection 3.2, hybrid approach Eisele et al. (2008). table shows pair-wisecomparison system extended-phrase (see Section 3.2.3).possible confirm shallow-transfer RBMT PBSMT systems combinedhybrid system outperforms them.regard differences observed results in-domain out-ofdomain evaluations, important state that, EnglishSpanish, out-of-domaintuning test sets come general (news) domain RBMT datadeveloped bearing mind translation general texts (mainly news). case,Apertium-generated (synthetic) phrase pairs, contain hand-crafted knowledge35fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez0.320.3BLEU score0.280.260.240.160.140.1210000baselineextended-phraseextended-phrase-dictextended-corpustwo-phrase-tablesinterpolationEiseleApertium40000160000600000Size training corpus (in sentences)1272260(a) BLEU scores.systemmetricbaselineApertiumextended-phrase-dictextended-corpustwo-phrase-tablesinterpolationEisele10 000BTM=========40 000BTM==========160 000BTM============600 000BTM===============1 272 260BTM============(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phrasemethods evaluated (a method per row). Columns represent training corpus sizesevaluation metrics: BLEU (B), TER (T) METEOR (M). means extended-phraseoutperforms reference method statistically significant margin, means opposite,= means statistically significant difference them.Figure 3: SpanishEnglish in-domain evaluation, automatic evaluation scores obtained baseline PBSMT system, Apertium, hybrid approaches describedSection 3.2, hybrid approach Eisele et al. (2008). table shows pair-wisecomparison system extended-phrase (see Section 3.2.3).general domain, cover sequences words input text covered,sparsely found, original training corpus. Contrarily, in-domain tests reveal that,soon PBSMT system able learn reliable information parallelcorpus, synthetic RBMT phrase pairs become useless in-domain test setscome specialised domain parliament speeches. BretonFrench, givensmall size corpus available, hybrid approach outperforms pure RBMTPBSMT approaches experiments performed.36fiIntegrating Shallow-Transfer Rules Statistical Machine Translation0.280.26BLEU score0.240.220.20.180.160.1410000baselineextended-phraseextended-phrase-dictextended-corpustwo-phrase-tablesinterpolationEiseleApertium40000160000600000Size training corpus (in sentences)1272260(a) BLEU scores.systemmetricbaselineApertiumextended-phrase-dictextended-corpustwo-phrase-tablesinterpolationEisele10 000BTM=======40 000BTM=====160 000BTM====600 000BTM======1 272 260BTM=====(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phrasemethods evaluated (a method per row). Columns represent training corpus sizesevaluation metrics: BLEU (B), TER (T) METEOR (M). means extended-phraseoutperforms reference method statistically significant margin, means opposite,= means statistically significant difference them.Figure 4: SpanishEnglish out-of-domain evaluation, automatic evaluation scoresobtained baseline PBSMT system, Apertium, hybrid approaches describedSection 3.2, hybrid approach Eisele et al. (2008). table shows pair-wisecomparison system extended-phrase (see Section 3.2.3).analysis proportion synthetic phrase pairs included decoderfinal translation30 different evaluation scenarios, depicted figures 68, confirms reason differences in-domain out-of-domain results.EnglishSpanish training corpus size hybrid system, proportion syntheticphrases higher out-of-domain evaluation.30. synthetic phrase pair also obtained parallel corpus, considered syntheticfigures 68.37fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez0.260.24BLEU score0.220.20.180.160.14baselineextended-phraseextended-phrase-dictextended-corpus0.120.110000two-phrase-tablesinterpolationEiseleApertium25000Size training corpus (in sentences)54196(a) BLEU scores.systemmetricbaselineApertiumextended-phrase-dictextended-corpustwo-phrase-tablesinterpolationEisele10 000BTM=========25 000BTM=========54 196BTM===========(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phrasemethods evaluated (a method per row). Columns represent training corpus sizesevaluation metrics: BLEU (B), TER (T) METEOR (M). means extended-phraseoutperforms reference method statistically significant margin, means opposite,= means statistically significant difference them.Figure 5: BretonFrench in-domain evaluation, automatic evaluation scores obtained baseline PBSMT system, Apertium, hybrid approaches describedSection 3.2, hybrid approach Eisele et al. (2008). table shows pair-wisecomparison system extended-phrase (see Section 3.2.3).Regarding difference hybrid systems enriched RBMT resources (extended-phrase) include dictionary (extended-phrase-dict),patterns detected. EnglishSpanish, impact shallow-transferrules higher translating out-of-domain texts decreases training corpusgrows. impact therefore higher decoder chooses high proportionApertium phrases (see figures 6 7). Moreover, systems including shallow-transferrules outperform counterparts include dictionary wider margin38fiIntegrating Shallow-Transfer Rules Statistical Machine Translation1two-phrase-tablesinterpolationEiseleextended-phraseextended-phrase-dictextended-corpusProportion synthetic phrases0.80.60.40.201000040000160000600000Size training corpus (in sentences)1272260(a) In-domain evaluation.1two-phrase-tablesinterpolationEiseleextended-phraseextended-phrase-dictextended-corpusProportion synthetic phrases0.80.60.40.201000040000160000600000Size training corpus (in sentences)1272260(b) Out-of-domain evaluation.Figure 6: EnglishSpanish, proportion phrase pairs generated RBMT datachosen decoder translating test set different hybrid approachesdescribed Section 3.2 hybrid approach Eisele et al. (2008).translating out-of-domain texts English Spanish way round.Spanish morphology richer, transfer rules help perform agreement operationstranslating Spanish. contrary, Spanish source language, onemain limitations suffered baseline PBSMT system high number outof-vocabulary (OOV) words, already mitigated integrating dictionaries39fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez1two-phrase-tablesinterpolationEiseleextended-phraseextended-phrase-dictextended-corpusProportion synthetic phrases0.80.60.40.201000040000160000600000Size training corpus (in sentences)1272260(a) In-domain evaluation.1two-phrase-tablesinterpolationEiseleextended-phraseextended-phrase-dictextended-corpusProportion synthetic phrases0.80.60.40.201000040000160000600000Size training corpus (in sentences)1272260(b) Out-of-domain evaluation.Figure 7: SpanishEnglish, proportion phrase pairs generated RBMT datachosen decoder translating test set different hybrid approachesdescribed Section 3.2 hybrid approach Eisele et al. (2008).phrase table extended-phrase-dict approach, shown figures 911.31figures show amount OOV words much higher baseline system31. approach Eisele et al. (2008) number OOV words always 0 phrase tablecontains phrase pairs obtained translating test set RBMT system, RBMT systemcopies verbatim output words appear dictionaries.40fiIntegrating Shallow-Transfer Rules Statistical Machine Translation1two-phrase-tablesinterpolationEiseleextended-phraseextended-phrase-dictextended-corpusProportion synthetic phrases0.80.60.40.201000025000Size training corpus (in sentences)54196Figure 8: BretonFrench, proportion phrase pairs generated RBMT datachosen decoder translating test set different hybrid approachesdescribed Section 3.2 hybrid approach Eisele et al. (2008).SL Spanish SL English reduction amountOOVs adding RBMT dictionaries consequently also higher first case.contrast, positive impact rules limited EnglishSpanishin-domain evaluation, statistically significant improvement hybrid systemenriched solely dictionaries (according three evaluation metrics)observed smallest EnglishSpanish training corpus. fact, trainingcorpus sizes, inclusion shallow-transfer rules hybrid system producesstatistically significant drop translation quality according one three evaluationmetrics (METEOR case EnglishSpanish in-domain evaluation TERcase SpanishEnglish). training parallel corpus belongs domaintest corpus, corpus-extracted phrase pairs likely contain accurate fluenttranslations compared mechanical regular translations providedRBMT shallow-transfer rules. One possible explanation fact degradationcaused rules measured TER METEOR used BLEUtuning (Och, 2003). Consequently, weight feature function flags whetherphrase pairs comes parallel corpus RBMT system setinclusion shallow-transfer rules penalise translation quality measuredBLEU. effect using evaluation metrics tuning yet studied.regard BretonFrench, impact shallow-transfer rules also limited:difference hybrid system enriched shallow-transfer rules systemenriched dictionaries statistically significant training corpussizes evaluated. reason probably sentences test setcomplex grammatical structure: average sentence length 9 words (Tyers, 2009)contains many sentences simply noun phrases. Another possible reason41fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez2500Number OOVs2000two-phrase-tablesinterpolationEiselebaselineextended-phraseextended-phrase-dictextended-corpus150010005000-5001000040000160000600000Size training corpus (in sentences)1272260(a) In-domain evaluation.900080007000two-phrase-tablesinterpolationEiselebaselineextended-phraseextended-phrase-dictextended-corpusNumber OOVs6000500040003000200010000-10001000040000160000600000Size training corpus (in sentences)1272260(b) Out-of-domain evaluation.Figure 9: EnglishSpanish, number out-of-vocabulary words test setdifferent hybrid approaches described Section 3.2 hybrid approach Eisele et al.(2008).may fact quality BretonFrench shallow-transfer rules may lowerquality rules used language pairs, since effort spentdevelopment smaller.regards different phrase scoring approaches defined Section 3.2, differences observed. remarkable differences show inclusionsynthetic phrase pairs great impact, is, EnglishSpanish out-of-domain eval42fiIntegrating Shallow-Transfer Rules Statistical Machine Translation3500two-phrase-tablesinterpolationEiselebaselineextended-phraseextended-phrase-dictextended-corpus3000Number OOVs25002000150010005000-5001000040000160000600000Size training corpus (in sentences)1272260(a) In-domain evaluation.1200010000two-phrase-tablesinterpolationEiselebaselineextended-phraseextended-phrase-dictextended-corpusNumber OOVs80006000400020000-20001000040000160000600000Size training corpus (in sentences)1272260(b) Out-of-domain evaluation.Figure 10: SpanishEnglish, number out-of-vocabulary words test setdifferent hybrid approaches described Section 3.2 hybrid approach Eiseleet al. (2008).uations. Firstly, interpolation strategy frequently outperformed strategies,hybrid systems built usually choose relatively small proportion synthetic phrase pairs. theory, outperform two-phrase-tables strategyassigns higher probabilities synthetic phrase pairs also found training parallel corpus, actually two-phrase-tables approach generally achieves highertranslation quality. One possible reason result may fact that,43fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez6000two-phrase-tablesinterpolationEiselebaselineextended-phraseextended-phrase-dictextended-corpus5000Number OOVs40003000200010000-10001000025000Size training corpus (in sentences)54196Figure 11: BretonFrench, number out-of-vocabulary words test setdifferent hybrid approaches described Section 3.2 hybrid approach Eisele et al.(2008).interpolation method relative weights two types phrase pairs optimisedminimise perplexity set phrase pairs extracted tuning corpus,two-phrase-tables strategy relative weights optimised maximise translationquality minimum error rate tuning algorithm. latter case, interactionphrase pairs rest elements PBSMT system taken accounttuning process. Nevertheless, additional experiments whose objectivecarry in-depth evaluation impact method used optimise relativeweight types phrase pairs need carried out. Concerning extendedcorpus strategy, consistently outperform strategies, probablysynthetic phrase pairs short subphrases clearly improve reordering model. However, already stated, strategy could used real-worldsetting high computational cost aligning synthetic phrase pairstraining corpus together every document translated. Finally, two-phrasetables strategy outperformed extended-phrase strategy experiments carriedEnglishSpanish language pair (except smallest training corpus size,effect increasing probability phrase pairs appear phrasetables, described Section 3.2.1, less relevant). reverse language pair,two-phrase-tables strategy sometimes better, three evaluation metrics never agreedifference strategies small compared EnglishSpanish.results suggest that, least evaluation scenario shallow-transferrules highest impact, phrase scoring strategy defined Section 3.2.3 ableachieve better balance two sources phrase pairs.Finally, hybridisation strategy defined Section 3, together phrase scoringstrategy defined Section 3.2.3, outperforms approach Eisele et al. (2008)44fiIntegrating Shallow-Transfer Rules Statistical Machine Translationlanguage pairs, training corpus sizes domains. biggest differenceapproaches observed small corpora used training. anticipatedSection 2.1, circumstances, reliable alignment models learnttraining corpus therefore reliable phrase pairs obtained input textRBMT translation. approach presented work, contrarily, affectedissue rely word alignments order generate phrase pairsRBMT system. addition, significant difference even trainingcorpus relatively big (more one million parallel sentences). high proportionsynthetic phrase pairs used compared hybrid approaches (see figures 68)suggests approach Eisele et al. able find adequate balancetypes phrase pairs. may synthetic phrase pairs even extractedSL segments match transfer rule straightforwardscoring method used, simply consists concatenating phrase table obtainedtraining parallel corpus obtained RBMT system.5. Evaluation Automatically Inferred Rulesempirically proved previous section, shallow-transfer rules improveperformance PBSMT. However, considerable human effort high levellinguistic knowledge needed create them. order reduce degree humaneffort required achieve improvement, algorithm proposed Sanchez-Cartagenaet al. (2015) used infer set shallow-transfer rules training parallelcorpus PBSMT models built, set rules, togetherbilingual dictionary, used enlarge phrase table previously described.significant boost translation quality could thus achieved sole additionRBMT dictionaries. section, set experiments whose objective assessviability approach presented.method proposed Sanchez-Cartagena et al. (2015) uses parallel corpora infershallow-transfer rules compatible formalism used Apertium (Forcadaet al., 2011). approach inspired method Sanchez-Martnez Forcada(2009), uses generalisation alignment template formalism (Och & Ney, 2004)encode transfer rules, overcomes important limitations method SanchezMartnez Forcada (2009). refer reader paper Sanchez-Cartagena et al.thorough description limitations.approach Sanchez-Cartagena et al. (2015) first literatureproblem automatically inferring transfer rules reduced finding optimal valueminimisation problem. prove translation quality achieved automatically inferred rules generally close obtained hand-crafted rules. Moreover,language pairs, automatically inferred rules even able outperformhand-crafted ones.5.1 Experimental SetupTwo considerations borne mind inferring set shallow-transfer rulesintegrated PBSMT system. Firstly, experiments conducted SanchezCartagena et al. (2015) concluded one features rule inference algorithm,45fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnezgeneralisation alignment templates combinations values morphological inflectionattributes observed training corpus, one causes vast complexityaforementioned minimisation problem brings significant translation qualityboost training corpus small (below 1 000 parallel sentences). Givenfact parallel corpus sizes SMT system starts competitivemuch bigger, generalisation morphological inflection attributes skippedinferring shallow-transfer rules integrated PBSMT. Moreover, preliminaryexperiments showed that, even disabling generalisation non-observed combinations values morphological inflection attributes, global minimisation algorithmstill needs huge amount processing time order infer set rules parallelcorpus contains hundreds thousands parallel sentences.Secondly, rule inference algorithm Sanchez-Cartagena et al. (2015) filters rulesgenerated ensure that, applied shallow-transfer RBMTsystem greedy, left-to-right, longest-match way, groups words needprocessed together translated rule. on, shall referprocess optimising rules chunking. Since, principle, SMT decoder splitsinput sentences possible ways, process might needed. Shallow-transferrules sequences SL lexical categories present corpus would thereforegenerated.ran preliminary experiments results showed consistentdifferences systems whose rules optimised chunkingsystems whose rules not: statistically significant differences foundevaluation metrics. SpanishEnglish, optimising rules chunkingbrings tiny improvement, EnglishSpanish, effect opposite. Sinceimpact rules higher translation out-of-domain texts, effectoptimisation also noticeable scenario.optimisation rules chunking affects resulting hybrid system two ways.one hand, prevents inclusion phrase table multiple noisy phrase pairsgenerated shallow-transfer rules match sequences lexical categoriesneed processed together translating languages involved.Owing fact decoder cannot evaluate translation hypotheses,useless phrase pairs may prevent other, important phrase pairs includedfinal translation. may also occur language model enoughinformation properly score synthetic phrase pairs built noisy rules.point view, optimisation rules chunking positive impacttranslation quality. Furthermore, since SMT system perform greedysegmentation input sentence, rules discarded optimisationchunking RBMT may still useful included PBSMT system. Ruleswould prevent application important rule RBMT engineprevent application rule hybrid system because, principle,possible segmentations taken account. light preliminary results,seems former relevant SpanishEnglish, latter higherpositive impact EnglishSpanish. Since Spanish morphologically complex,rules needed correctly perform agreements, rules discarded46fiIntegrating Shallow-Transfer Rules Statistical Machine Translationoptimisation chunking probably useful. Nevertheless, differences yetstudied greater depth.Bearing considerations mind, experiments carried follows.language pairs, corpora RBMT dictionaries used previous section,new system, extended-phrase-learnt, built; system, rule inference algorithm described Sanchez-Cartagena et al. (2015) applied training corpusoptimisation rules chunking performed. rules inferred,together dictionaries, used enrich PBSMT system followinghybridisation strategy described Section 3. time complexity minimisation problem solved rule inference approach, first 160 000 sentencestraining corpus used rule inference cases corpuslarger 160 000 sentences. words, systems built 160 000, 600 000,whole set parallel sentences use exactly set shallow-transfer rules.32compare new system pure PBSMT baseline built data,hybrid system built Apertium hand-crafted rules dictionaries, hybrid systembuilt strategy Apertium dictionaries, two differentversions RBMT system Apertium: one version using hand-crafted rules anotherversion automatically inferred rules. hybrid systems, scoring methoddescribed Section 3.2.3 used, since scoring method provedperform best experiments described previous section.5.2 Results Discussioncomparison hybrid approach extended-phrase-learnt approachesconsidered section presented figures 1216. results show BLEU(Papineni et al., 2002) automatic evaluation score different systems evaluated; TERMETEOR behavie similar way. addition, statistical significance33difference extended-phrase-learnt systems also presented table,way depicted previous section.comparison PBSMT baseline pure RBMT system showshybrid approach automatically inferred rules behaves way handcrafted rules used: outperforms baselines training corpus smallout-of-domain text translated. comparison performed hybrid systemuses dictionaries, hybrid approach also outperforms dictionary-basedapproach almost cases hybrid approach hand-crafted rules: outof-domain evaluation in-domain evaluation smallest parallel corpus size,although three evaluation metrics agree latter case. words,automatic inference shallow-transfer rules, statistically significant improvementapproach uses dictionaries achieved without using additionallinguistic resources.32. addition, part training corpus used rule inference split two parts:first 4/5 corpus used actual rule inference, last 1/5 employeddevelopment corpus order optimise threshold , experiments described SanchezCartagena et al. (2015). training corpora bigger 10 000 sentences, 2 000 sentencesused optimising , remaining part corpus used rule inference.33. obtained paired bootstrap resampling (Koehn, 2004) (p 0.05; 1 000 iterations).47fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez0.320.3BLEU score0.280.260.160.14baselineextended-phraseextended-phrase-dict10000extended-phrase-learntApertium-learntApertium40000160000600000Size training corpus (in sentences)1272260(a) BLEU scores.systemmetricbaselineApertiumApertium-learntextended-phrase-dictextended-phrase10 000BTM=====40 000BTM=====160 000BTM=====600 000BTM=========1 272 260BTM=======(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phraselearnt methods evaluated (a method per row). Columns represent training corpussizes evaluation metrics: BLEU (B), TER (T) METEOR (M). means extendedphrase-learnt outperforms reference method statistically significant margin, meansopposite, = means statistically significant difference them.Figure 12: EnglishSpanish in-domain evaluation, automatic evaluation scoresobtained baseline PBSMT system, Apertium hand-crafted rules (Apertiumlearnt), Apertium learnt rules (Apertium-learnt), hybrid approach (describedSection 3.2.3) using hand-crafted shallow-transfer rules (extended-phrase), set rulesinferred training corpus (extended-phrase-learnt) rules (extendedphrase-dict). table shows pair-wise comparison system extended-phraselearnt.cases statistically significant difference hybrid systemhand-crafted rules hybrid system automatically inferred rules.occurs, instance, EnglishSpanish out-of-domain evaluation trainingcorpus contains 600 000 sentence pairs. translation quality similar obtainedhand-crafted rules therefore attained without intervention human ex48fiIntegrating Shallow-Transfer Rules Statistical Machine Translation0.260.24BLEU score0.220.20.180.160.1410000baselineextended-phraseextended-phrase-dictextended-phrase-learntApertium-learntApertium40000160000600000Size training corpus (in sentences)1272260(a) BLEU scores.systemmetricbaselineApertiumApertium-learntextended-phrase-dictextended-phrase10 000BTM40 000BTM160 000BTM600 000BTM===1 272 260BTM=(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phraselearnt methods evaluated (a method per row). Columns represent training corpussizes evaluation metrics: BLEU (B), TER (T) METEOR (M). means extendedphrase-learnt outperforms reference method statistically significant margin, meansopposite, = means statistically significant difference them.Figure 13: EnglishSpanish out-of-domain evaluation, automatic evaluation scoresobtained baseline PBSMT system, Apertium hand-crafted rules (Apertiumlearnt), Apertium learnt rules (Apertium-learnt), hybrid approach (describedSection 3.2.3) using hand-crafted shallow-transfer rules (extended-phrase), set rulesinferred training corpus (extended-phrase-learnt) rules (extendedphrase-dict). table shows pair-wise comparison system extended-phraselearnt.perts usually create them.34 rest cases, hybrid system34. Although translation quality systems similar according automatic evaluation metrics,differences amount rules used case. set hand-crafted rulesApertium platform contains hundred rules language pair, number inferredrules ranges 2 000 75 000, depending language pair size training parallelcorpus. figures directly comparable, since rule formalism used hand-craftedrules expressive automatically inferred rules (Sanchez-Cartagena et al., 2015,49fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez0.320.3BLEU score0.280.260.240.180.160.14baselineextended-phraseextended-phrase-dict10000extended-phrase-learntApertium-learntApertium40000160000600000Size training corpus (in sentences)1272260(a) BLEU scores.systemmetricbaselineApertiumApertium-learntextended-phrase-dictextended-phrase10 000BTM====40 000BTM====160 000BTM=====600 000BTM========1 272 260BTM=======(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phraselearnt methods evaluated (a method per row). Columns represent training corpussizes evaluation metrics: BLEU (B), TER (T) METEOR (M). means extendedphrase-learnt outperforms reference method statistically significant margin, meansopposite, = means statistically significant difference them.Figure 14: SpanishEnglish in-domain evaluation, automatic evaluation scoresobtained baseline PBSMT system, Apertium hand-crafted rules (Apertiumlearnt), Apertium learnt rules (Apertium-learnt), hybrid approach (describedSection 3.2.3) using hand-crafted shallow-transfer rules (extended-phrase), set rulesinferred training corpus (extended-phrase-learnt) rules (extendedphrase-dict). table shows pair-wise comparison system extended-phraselearnt.hand-crafted rules outperforms hybrid system dictionaries, translation qualityachieved hybrid system automatically inferred rules (extended-phrase-learnt)lies in-between.3). Nevertheless, error analysis described Section 6.2 shows automatically inferred rulescontain many exceptions applied particular words included hand-crafted ones.50fiIntegrating Shallow-Transfer Rules Statistical Machine Translation0.260.24BLEU score0.220.20.180.160.1410000baselineextended-phraseextended-phrase-dictextended-phrase-learntApertium-learntApertium40000160000600000Size training corpus (in sentences)1272260(a) BLEU scores.systemmetricbaselineApertiumApertium-learntextended-phrase-dictextended-phrase10 000BTM=40 000BTM==160 000BTM=====600 000BTM=====1 272 260BTM===(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phraselearnt methods evaluated (a method per row). Columns represent training corpussizes evaluation metrics: BLEU (B), TER (T) METEOR (M). means extendedphrase-learnt outperforms reference method statistically significant margin, meansopposite, = means statistically significant difference them.Figure 15: SpanishEnglish out-of-domain evaluation, automatic evaluation scoresobtained baseline PBSMT system, Apertium hand-crafted rules (Apertiumlearnt), Apertium learnt rules (Apertium-learnt), hybrid approach (describedSection 3.2.3) using hand-crafted shallow-transfer rules (extended-phrase), set rulesinferred training corpus (extended-phrase-learnt) rules (extendedphrase-dict). table shows pair-wise comparison system extended-phraselearnt.addition, worth noting translation quality approach extendedphrase-learnt drop size training corpus exceeds 160 000 sentencesfull training corpus used rule inference. fact, circumstances (600 000 parallel sentences) significant differences useautomatically inferred rules hand-crafted rules hybrid systems (EnglishSpanish,out-of-domain evaluation). observation probably related fact trans51fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez0.260.240.22BLEU score0.20.180.160.140.120.10.08baselineextended-phraseextended-phrase-dict10000extended-phrase-learntApertium-learntApertium25000Size training corpus (in sentences)54196(a) BLEU scores.systemmetricbaselineApertiumApertium-learntextended-phrase-dictextended-phrase10 000BTM=====25 000BTM====54 196BTM======(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phraselearnt methods evaluated (a method per row). Columns represent training corpussizes evaluation metrics: BLEU (B), TER (T) METEOR (M). means extendedphrase-learnt outperforms reference method statistically significant margin, meansopposite, = means statistically significant difference them.Figure 16: BretonFrench in-domain evaluation, automatic evaluation scoresobtained baseline PBSMT system, Apertium hand-crafted rules (Apertiumlearnt), Apertium learnt rules (Apertium-learnt), hybrid approach (describedSection 3.2.3) using hand-crafted shallow-transfer rules (extended-phrase), set rulesinferred training corpus (extended-phrase-learnt) rules (extendedphrase-dict). table shows pair-wise comparison system extended-phraselearnt.lation performance automatically inferred rules grows slowly sizetraining corpus, rules obtained bigger parallel corpora would probablysimilar obtained fragment 160 000 sentences. Nevertheless, exactimpact proportion training corpus used rule inference different trainingcorpus sizes, language pairs domains merits research.52fiIntegrating Shallow-Transfer Rules Statistical Machine TranslationFinally, also worth noting difference hand-crafted rules (Apertium)automatically inferred rules (Apertium-learnt) used RBMTsystem: cases (BretonFrench EnglishSpanish out-of-domain evaluation)difference translation performance considerably higher differencehybrid systems enriched hand-crafted rules automatically inferred rules(see figures 13 16). occurs RBMT translation completely ledshallow-transfer rules, possible errors encoded automatically inferredrules direct impact output.6. Human Evaluation Error Analysissection reports, one hand, results obtained out-of-domain humanevaluation performed EnglishSpanish largest training parallel corpus used,and, other, analysis translation errors performed different systemsevaluated Section 5.6.1 Human Evaluationorder confirm results obtained automatic evaluation metrics, performed human evaluation EnglishSpanish out-of-domain texts. systemsincluded human evaluation described previous section trainedlargest parallel corpus used.asked 15 users rank (allowing ties) translations produced baselinePBSMT system (baseline), Apertium hand-crafted rules, hybrid approach usingdictionaries (extended-phrase-dict), hybrid approach using automatically inferredrules (extended-phrase-learnt) hybrid approach using hand-crafted rules (extendedphrase). user ranked translations 50 SL sentences test set.users split 5 groups, users group ranked exactly set SLsentences, thus allowing us compute inter-annotator agreement. total, translations250 sentences test set ranked. evaluation method similarfollowed WMT 2012 shared translation task (Callison-Burch et al., 2012).computed ratio wins system (Callison-Burch et al., 2012, Eq. 4)proportion times system ranked better system. scoreallows us sort systems best worst, shown last row Table 2.resulting ordering exactly obtained automatic evaluation metrics(see Figure 13).Table 2 also shows results pairwise comparison systems:cell represents proportion sentences system named row labeloutperforms system named column label. score shown bold type meansdifference statistically significant.35 results entirely confirm resultsobtained automatic evaluation measures: hybrid systems outperform RBMTPBSMT systems, automatically inferred rule allow us build better hybrid sys35. According Sign Test, p 0.10. chose relatively high p-value small amounthuman rankings available.53fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnezextended-phraseextended-phraseextended-phrase-l.extended-phrase-d.baselineApertium>extended-phrase-l.0.550.450.400.400.320.610.460.450.350.56extended-phrase-d.0.600.550.510.350.51baseline0.600.550.490.370.51Apertium0.680.650.650.630.35Table 2: Results human evaluation; extended-phrase-l. abbreviation extendedphrase-learnt extended-phrase-d. abbreviation extended-phrase-dict. lastrow represents proportion times system outperforms system,remaining cells show results pairwise evaluation: represent proportionsentences system named row label outperforms system namedcolumn label. score shown bold type system namedrow label wins often system named column label meansdifference statistically significant.tems using dictionaries external resource, i.e.extended-phrase-learnt outperformsextended-phrase-dict.Finally, inter-annotator agreement computed described Callison-Burch et al.(2012, Sec. 3.2) = 0.503, usually interpreted fair agreement.6.2 Error Analysisaddition assessing translation quality means automatic evaluation metricshuman ranking, also interesting compare different types errors madesystems evaluated section. compared translations performed differentsystems used human evaluation found interesting trends summarisebelow. focused analysis EnglishSpanish language pairrules highest impact (see Section 4.2). Table 3 shows seven examplestranslations refer throughout section.comparison pure RBMT system Apertium, baseline PBSMT systemhybrid system extended-phrase shows two pure systems complementarycombined, number errors reduced. comparing purestatistical system hybrid one, reduction number OOV words observed(e.g. word patterned example #1). also words whose translationspecific domain parliament speeches performed pure PBSMTsystem, translated appropriate way news domainhybrid system. example word feel example #2. differencessystems lexical: hybrid system produces better agreementdeterminers, nouns adjectives (see example #3)36 correctly translates noun phrasesmade adjacent nouns (see example #4),37 among grammatical improvements.36. grammatically correct translation Spanish specialised category categora especializada37. correct translation Spanish adjacent nouns Brno socialists socialistas de Brno;literally means socialists Brno.54fiIntegrating Shallow-Transfer Rules Statistical Machine Translationcompared Apertium RBMT system, hybrid system producesfluent translations TL, probably thanks use TL model. instance,hybrid system deals better sentences regular grammatical structure(see translation example #4).38 Preposition choices also generallybetter hybrid system (for instance, preposition correctly removedhybrid system example #4), translation phrasal verbs (see closingtranslated different systems example #5).results evaluation show translation performance hybrid systembuilt automatically inferred rules (extended-phrase-learnt) close hybridsystem built hand-crafted rules (extended-phrase; see Figure 13). manual inspectiontranslations produced reveals hand-crafted rules automatically inferredrules produce similar translations. one hand, automatically inferred rulesencode many exceptions general translation rules, makes outperformhand-crafted ones case sentences. One common example phenomenonswapping adjectivenoun sequence. adjectives (prepositive adjectives)must swapped translating Spanish automatically inferredrules able learn (for instance, adjective best example #6).hand, hand-crafted rules encode long-range grammatical operations, subjectpredicate agreement example #7 invaded translated invadieron,agrees person number translation 150 drivers, couldautomatically inferred rule inference algorithm considers segments5 tokens.7. Concluding Remarkspaper, hybridisation approach enrich PBSMT modelsdata shallow-transfer RBMT systems presented. confirmeddata shallow-transfer RBMT improve PBSMT systems also resultinghybrid system outperforms pure PBSMT RBMT systems builtdata.hybridisation approach overcomes limitations general-purpose strategyattempts improve PBSMT models data MT systems (Eisele et al.,2008) thanks fact takes advantage way shallow-transferRBMT system uses linguistic resources segment SL sentences. experimentscarried shown hybrid approach outperforms strategy Eisele et al.statistically significant margin wide range situations. fact, system (SanchezCartagena et al., 2011b) built hybridisation approach described workone winners pair-wise manual evaluation WMT 2011 shared translationtask (Callison-Burch et al., 2011) SpanishEnglish.39 effectiveness hybrid38. reference sentence, relatively free translation SL sentence, containword Debera, although appropriate translation context.39. evaluation performed asking users rank translations produced different systems.Users iteratively ranked (from best worst) translations SL sentence produced 5different systems. refer reader description task Callison-Burch et al. (2011)details evaluation. human evaluation described Section 6 carriedsimilar way.55fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez#123systemsourceApertiumbaselineextended-phrasereferencesourceApertiumbaselineextended-phrasereferencesourceApertiumbaselineextended-phrasereferencesourceApertium4baselineextended-phrasereference5sourceApertiumbaselineextended-phrasereferencesourceextended-phrase-l.6extended-phrasereference7sourceextended-phrase-l.extended-phrasereferencesentenceinauguration thick lace, Oslo patterned velvet.Si la inauguracion era sobre encaje grueso, en Oslo sea terciopelo estampado.Si la inauguracion de centro encaje en Oslo, fue patterned.Si la inauguracion era sobre encaje grueso, en Oslo, fue terciopelo estampadoSi para la toma de posesion de trataba de un encaje gordo,en Oslo es un terciopelo con estampado.feels kick.Siente un chut.El considera una patada.El siente una patada.El siente una patadaspecialised category nobody year physics.La categora solo especializada que nadie gano este ano era fsica.La unica categora especializados que nadie gano este ano ha sido fsicaLa unica categora especializada que nadie gano este ano ha sido fsica.La unica categora especializada, sin una mujer premiada, fue fsica.should, ecology aesthetics dont seem interestBrno socialists much.Tenga que, pero la ecologa la estetica parecen para interesarlos socialistas de Brno mucho.Debera, pero la ecologa la estetica parecen interesel Brno socialistas mucho.Debera, pero la ecologa la estetica parecen intereslos socialistas de Brno mucho.Lo que pasa es que la ecologa la estetica parecen interesar demasiadolos socialistas de Brno.opposed principle closing parties.Somos opposed encima principio al encierro abajo de partidos.Nos oponemos por principio la clausura de partidos.Nos oponemos por principio la clausura de los partidos.Por principio nos oponemos la clausura de partidos.Theres best skiers snow borderscounty - real talent, added.hay algunos de los mejores esquiadores fronteras de nieveen el condado aqu - un verdadero talento, anadio.hay algunos de los esquiadores mejores fronteras de nieveen el condado aqu - algunos verdadero talento, anadio.Aqu se encuentran algunos de los mejores esquiadores snowboardersdel condado, talento verdadero, anadio.150 drivers invaded works council meeting [...]Unos 150 conductores invadido una reunion del consejo de empresa [...]Unos 150 conductores invadieron una reunion de consejo de los trabajos [...]Unos 150 conductores invadieron un comite [...]Table 3: Translations Spanish different English sentences extracted out-ofdomain evaluation corpus produced systems evaluated Section 6.remarkable differences highlighted. extended-phrase-l. abbreviation extendedphrase-learnt, hybrid system automatically inferred rules.56fiIntegrating Shallow-Transfer Rules Statistical Machine Translationapproach thereby confirmed automatic human evaluation (results WMT2011 human evaluation compatible human evaluation describedSection 6: experiments, hybrid system built method outperforms purePBSMT system).Moreover, proved rule inference algorithm presented SanchezCartagena et al. (2015) successfully combined hybrid approach, thus allowing hybrid system built using dictionaries hand-crafted linguisticresource. improvement translation quality also achieved wayhand-crafted shallow-transfer rules used. hybrid system automaticallyinferred rules able attain translation quality achieved hybrid systemhand-crafted rules and, even not, often obtains better results hybridsystem uses dictionaries enrich PBSMT models. Additionally, needhuman expert write rules avoided.According results obtained, hybrid approach especially recommendedtraining parallel corpus (for translation model) monolingual corpus (forlanguage model) moderate size domain training corpus differentdomain texts translated.40 use moderate-sized training corporamay necessary order limit size phrase table TL modelhybrid system must executed mobile device limited memory. Moreover,hybrid approach presented work also safely applied scenarios, sincedrops translation quality comparison PBSMT baseline detected.good enough hand-crafted rules available, worth using instead inferringrules parallel training corpus, not, applying rule inferencealgorithm significantly degrade translation quality.41hybridisation method described paper implemented software toolcalled rule2Phrase (Sanchez-Cartagena, Sanchez-Martnez, & Perez-Ortiz, 2012)released GNU GPL v3 free software license. source code freelydownloaded http://www.dlsi.ua.es/~vmsanchez/Rule2Phrase.tar.gz. toolincludes phrase scoring strategies described sections 3.2.3 3.2.4paper.40. EnglishSpanish out-of-domain evaluations described Section 5 repeated using TL modelestimated much bigger monolingual corpora. particular, portion News Crawl monolingual corpus provided WMT 2011 shared translation task (http://www.statmt.org/wmt11/translation-task.html) concatenated Europarl corpus. result, English Spanishmonolingual corpora around 6 200 000 sentences obtained. results evaluationshowed parallel corpus contains around 26 000 000 words used togethermonolingual corpora, difference hybrid system built automatically inferred rulesbaseline SMT system statistically significant evaluation metrics.41. Manually creating transfer rules involves huge human effort. Rule writers must first identifygrammatical divergences languages involved need treated rules sortfrequency texts translated RBMT system. operation called contrastiveanalysis. write rules deal divergencies, starting frequent oneschoosing case possible conflicts rules. Rules written humans maygood enough grammatical divergencies identified contrastive analysis,frequency correctly estimated enough time invested writing rulesdealing important grammatical divergencies.57fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-MartnezAcknowledgmentsResearch funded Spanish Ministry Economy Competitiveness projectsTIN2009-14009-C02-01 TIN2012-32615, Generalitat Valenciana grant ACIF2010/174, European Union Seventh Framework Programme FP7/2007-2013grant agreement PIAP-GA-2012-324414 (Abu-MaTran).ReferencesBanerjee, S., & Lavie, A. (2005). Meteor: automatic metric mt evaluation improved correlation human judgments. Proceedings ACL WorkshopIntrinsic Extrinsic Evaluation Measures Machine Translation and/or Summarization, pp. 6572, Ann Arbor, Michigan, USA.Bisazza, A., Ruiz, N., & Federico, M. (2011). Fill-up versus interpolation methodsphrase-based smt adaptation. Proceedings 8th International WorkshopSpoken Language Translation, San Francisco, California, USA.Bojar, O., & Hajic, J. (2008). Phrase-based deep syntactic English-to-Czech statisticalmachine translation. Proceedings third Workshop Statistical Machinetranslation, pp. 143146, Columbus, Ohio, USA.Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., Goldsmith, M. J., Hajic, J., Mercer,R. L., & Mohanty, S. (1993). dictionaries data too. ProceedingsWorkshop Human Language Technology, pp. 202205, Princeton, New Jersey.Callison-Burch, C., Koehn, P., Monz, C., & Zaidan, O. (2011). Findings 2011 workshop statistical machine translation. Proceedings Sixth WorkshopStatistical Machine Translation, pp. 2264, Edinburgh, Scotland.Callison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut, R., & Specia, L. (2012). Findings2012 workshop statistical machine translation. Proceedings SeventhWorkshop Statistical Machine Translation, pp. 1051, Montreal, Canada.Carl, M. (2007). METIS-II: German English MT system. Proceedings XIMachine Translation Summit, pp. 6573, Copenhagen, Denmark.Crego, J. (2014). SYSTRAN RBMT engine: hybridization experiments. 3rd WorkshopHybrid Approaches Machine Translation (HyTra), Gothenburg, Sweden.Cutting, D., Kupiec, J., Pedersen, J., & Sibun, P. (1992). practical part-of-speech tagger.Proceedings Third Conference Applied Natural Language Processing, pp.133140, Trento, Italy.Eisele, A., Federmann, C., Saint-Amand, H., Jellinghaus, M., Herrmann, T., & Chen, Y.(2008). Using Moses integrate multiple rule-based machine translation engineshybrid system. Proceedings Third Workshop Statistical MachineTranslation, pp. 179182, Columbus, Ohio, USA.Enache, R., Espana-Bonet, C., Ranta, A., & Marquez, L. (2012). hybrid systempatent translation. Proceedings 16th Annual Conference EuropeanAssociation Machine Translation, pp. 269276, Trento, Italy.58fiIntegrating Shallow-Transfer Rules Statistical Machine TranslationFedermann, C., Eisele, A., Uszkoreit, H., Chen, Y., Hunsicker, S., & Xu, J. (2010).experiments shallow hybrid mt systems. Proceedings Joint Fifth Workshop Statistical Machine Translation Metrics, pp. 7781, Uppsala, Sweden.Federmann, C., & Hunsicker, S. (2011). Stochastic parse tree selection existing rbmtsystem. Proceedings Sixth Workshop Statistical Machine Translation, pp.351357, Edinburgh, Scotland.Forcada, M. L., Ginest-Rosell, M., Nordfalk, J., ORegan, J., Ortiz-Rojas, S., Perez-Ortiz,J. A., F. Sanchez-Martnez, G. R.-S., & Tyers, F. M. (2011). Apertium: free/opensource platform rule-based machine translation. Machine Translation, 25 (2), 127144. Special Issue: Free/Open-Source Machine Translation.Gao, Q., Lewis, W., Quirk, C., & Hwang, M.-Y. (2011). Incremental Training Intentional Over-fitting Word Alignment. Proceedings XIII Machine TranslationSummit, pp. 106113, Xiamen, China.Goodman, J., & Chen, S. F. (1998). empirical study smoothing techniques languagemodeling. Tech. rep. TR-10-98, Harvard University.Graham, Y., & van Genabith, J. (2010). Factor templates factored machine translation models. Proceedings 7th International Workshop Spoken LanguageTranslation, pp. 275283, Paris, France.Green, S., & DeNero, J. (2012). class-based agreement model generating accuratelyinflected translations. Proceedings 50th Annual Meeting AssociationComputational Linguistics: Long Papers - Volume 1, pp. 146155, Jeju Island,Korea.Hutchins, W. J., & Somers, H. L. (1992). introduction machine translation, Vol. 362.Academic Press New York.Kirchhoff, K., & Yang, M. (2005). Improved language modeling statistical machinetranslation. Proceedings ACL Workshop Building Using ParallelTexts, pp. 125128, Ann Arbor, Michigan, USA.Koehn, P. (2004). Statistical significance tests machine translation evaluation. Proceedings Conference Empirical Methods Natural Language Processing,Vol. 4, pp. 388395, Barcelona, Spain.Koehn, P. (2005). Europarl: parallel corpus statistical machine translation. Proceedings X Machine Translation Summit, pp. 1216, Phuket, Thailand.Koehn, P. (2010). Statistical Machine Translation. Cambridge University Press.Koehn, P., & Hoang, H. (2007). Factored translation models. Proceedings 2007Joint Conference Empirical Methods Natural Language Processing Computational Natural Language Learning (EMNLP-CoNLL), pp. 868876, Prague.Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B.,Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., & Herbst, E.(2007). Moses: Open source toolkit statistical machine translation. Proceedings45th Annual Meeting ACL Interactive Poster DemonstrationSessions, pp. 177180, Prague, Czech Republic.59fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-MartnezKoehn, P., & Schroeder, J. (2007). Experiments domain adaptation statistical machine translation. Proceedings Second Workshop Statistical MachineTranslation, pp. 224227, Prague, Czech Republic.Labaka, G., Espana-Bonet, C., Marquez, L., & Sarasola, K. (2014). hybrid machinetranslation architecture guided syntax. Machine Translation, 28 (2), 91125.Lavie, A. (2008). Stat-XFER: General Search-Based Syntax-Driven Framework Machine Translation. Gelbukh, A. (Ed.), Computational Linguistics IntelligentText Processing, Vol. 4919 Lecture Notes Computer Science, pp. 362375.Och, F. J., & Ney, H. (2003). systematic comparison various statistical alignmentmodels. Computational Linguistics, 29 (1), 1951.Och, F. J., & Ney, H. (2004). alignment template approach statistical machinetranslation. Computational Linguistics, 30 (4), 417449.Och, F. J. (2003). Minimum error rate training statistical machine translation. Proceedings 41st Annual Meeting Association Computational Linguistics,pp. 160167, Sapporo, Japan.Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). Bleu: method automaticevaluation machine translation. Proceedings 40th Annual MeetingAssociation Computational Linguistics, pp. 311318, Philadelphia, Pennsylvania,USA.Popovic, M., & Ney, H. (2006). Statistical machine translation small amountbilingual training data. 5th LREC SALTMIL Workshop Minority Languages,p. 2529, Genoa, Italy.Riezler, S., King, T. H., Kaplan, R. M., Crouch, R., Maxwell, III, J. T., & Johnson, M.(2002). Parsing wall street journal using lexical-functional grammar discriminative estimation techniques. Proceedings 40th Annual MeetingAssociation Computational Linguistics, ACL 02, pp. 271278, Stroudsburg, PA,USA. Association Computational Linguistics.Riezler, S., & Maxwell III, J. T. (2006). Grammatical machine translation. ProceedingsHuman Language Technology Conference NAACL, Main Conference, pp.248255, New York City, New York, USA.Roche, E., & Schabes, Y. (1997). Introduction. Roche, E., & Schabes, Y. (Eds.), Finitestate language processing, pp. 165. MIT, Cambridge, Massachusetts, USA.Rosa, R., Marecek, D., & Dusek, O. (2012). Depfix: system automatic correctionczech mt outputs. Proceedings Seventh Workshop Statistical MachineTranslation, pp. 362368, Montreal, Canada.Rosti, A.-V., Matsoukas, S., & Schwartz, R. (2007). Improved word-level system combination machine translation. Proceedings 45th Annual MeetingAssociation Computational Linguistics, pp. 312319, Prague, Czech Republic.Sanchez-Cartagena, V. M., Perez-Ortiz, J. A., & Sanchez-Martnez, F. (2015). generalisedalignment template formalism application inference shallow-transfer60fiIntegrating Shallow-Transfer Rules Statistical Machine Translationmachine translation rules scarce bilingual corpora. Computer Speech & Language,32 (1), 4690.Sanchez-Cartagena, V. M., Sanchez-Martnez, F., & Perez-Ortiz, J. A. (2011a). Integratingshallow-transfer rules phrase-based statistical machine translation. ProceedingsXIII Machine Translation Summit, pp. 562569, Xiamen, China.Sanchez-Cartagena, V. M., Sanchez-Martnez, F., & Perez-Ortiz, J. A. (2011b). Universitat dAlacant hybrid machine translation system WMT 2011. ProceedingsSixth Workshop Statistical Machine Translation, pp. 457463, Edinburgh,Scotland.Sanchez-Cartagena, V. M., Sanchez-Martnez, F., & Perez-Ortiz, J. A. (2012). opensource toolkit integrating shallow-transfer rules phrase-based satistical machine translation. Proceedings Third International Workshop Free/OpenSource Rule-Based Machine Translation, pp. 4154, Gothenburg, Sweden.Sanchez-Martnez, F., & Forcada, M. L. (2009). Inferring shallow-transfer machine translation rules small parallel corpora. Journal Artificial Intelligence Research,34 (1), 605635.Schwenk, H., Abdul-Rauf, S., Barrault, L., & Senellart, J. (2009). SMT SPE machinetranslation systems WMT09. Proceedings Fourth Workshop StatisticalMachine Translation, pp. 130134, Athens, Greece.Sennrich, R. (2012). Perplexity minimization translation model domain adaptationstatistical machine translation. Proceedings 13th Conference European Chapter Association Computational Linguistics, pp. 539549, Avignon,France.Snover, M., Dorr, B., Schwartz, R., Micciulla, L., & Makhoul, J. (2006). study translation edit rate targeted human annotation. Proceedings 7th biennialconference Association Machine Translation Americas, pp. 223231,Cambridge, Massachusetts, USA.Stolcke, A. (2002). SRILM extensible language modeling toolkit. Proceedings7th International Conference Spoken Language Processing, pp. 901904, Denver,Colorado, USA.Thurmair, G. (2009). Comparing different architectures hybrid Machine Translationsystems. Proceedings XII Machine Translation Summit, Ottawa, Canada.Tiedemann, J. (2012). Parallel Data, Tools Interfaces OPUS. ProceedingsEight International Conference Language Resources Evaluation, pp. 22142218, Istanbul, Turkey.Tyers, F. M. (2009). Rule-based augmentation training data BretonFrench statisticalmachine translation. Proceedings 13th Annual Conference EuropeanAssociation Machine Translation, pp. 213217, Barcelona, Spain.Zbib, R., Kayser, M., Matsoukas, S., Makhoul, J., Nader, H., Soliman, H., & Safadi, R.(2012). Methods integrating rule-based statistical systems Arabic English machine translation. Machine Translation, 26 (1-2), 6783.61fi