Journal Artificial Intelligence Research 55 (2016) 1059-1090

Submitted 10/15; published 04/16

Learning Concept Graphs Online Educational Data
Hanxiao Liu
Wanli
Yiming Yang
Jaime Carbonell

hanxiaol@cs.cmu.edu
mawanli@cs.cmu.edu
yiming@cs.cmu.edu
jgc@cs.cmu.edu

School Computer Science
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213 USA

Abstract
paper addresses open challenge educational data mining, i.e., problem
automatically mapping online courses different providers (universities, MOOCs, etc.)
onto universal space concepts, predicting latent prerequisite dependencies (directed
links) among concepts courses. propose novel approach inference within
across course-level concept-level directed graphs. training phase, system
projects partially observed course-level prerequisite links onto directed concept-level links;
testing phase, induced concept-level links used infer unknown courselevel prerequisite links. Whereas courses may specific one institution, concepts
shared across different providers. bi-directional mappings enable system perform
interlingua-style transfer learning, e.g. treating concept graph interlingua
transferring prerequisite relations across universities via interlingua. Experiments
newly collected datasets courses MIT, Caltech, Princeton CMU show
promising results.

1. Introduction
large growing amounts online education data present open challenges
significant opportunities machine learning research enrich educational offerings. One
important challenges automatically detect prerequisite dependencies
among massive quantities online courses, support decision making curricula planning students, support course curriculum design teachers based
existing course offerings. One example find coherent sequence courses among
MOOC offerings different providers respect implicit prerequisite relations.
specific example would new student enters university MS
PhD degree. interested machine learning data mining courses, finds
difficult choose among many courses look similar ambiguous course titles
her, Machine Learning, Statistical Machine Learning, Applied Machine Learning, Machine Learning Large Datasets, Scalable Analytics, Advanced Data Analysis,
Statistics: Data Mining, Intermediate Statistics, Statistical Computing, on. Completing courses would imply taking forever graduate, possibly waste big
portion time due overlapping content. Alternately, wants choose
small subset, courses include? order included courses
c
2016
AI Access Foundation. rights reserved.

fiLiu, Ma, Yang, & Carbonell

Courses University 2

Courses University 1
E&M

Mechanics
Calculus

Differential Eq

Matrix

Matrix

Quantum

Algorithms

Topology

Java Prog
Scalable Algs

Num Analysis

Universal Concepts (e.g. Wikipedia Topics)

Figure 1: framework two-level directed graphs: higher-level graphs courses
(nodes) prerequisite relations (links). lower-level graph consists universal concepts (nodes) pairwise preference learning teaching concepts.
links two levels system-assigned weights concepts
course.

without sufficient understanding prerequisite dependencies? Often prerequisites
explicit within academic department implicit across departments. Moreover,
already took several courses machine learning data mining Coursera
undergraduate education, much courses overlap new ones? Without accurate representation content overlap courses overlapped
content reflects prerequisite relations, difficult help find suitable
courses correct order. Universities solve problem old-fashioned way, via
academic advisors, clear address problem context MOOCs
cross-university offerings courses unique IDs described
universally controlled consistent vocabulary.
Ideally, would like universal graph whose nodes canonical discriminant concepts (e.g. convexity eigenvalues) taught broad range courses,
whose links indicate pairwise preferences sequencing teaching concepts.
example, learn concepts PageRank HITS, students already
learned concepts eigenvectors, Markov matrices irreducibility matrices.
means directed links eigenvectors, Markov matrices irreducibility PageRank
HITS concept graph. generalize further, many directed links
concepts one course (say Matrix Algebra) concepts another course (say
Web Mining Link Analysis sub-topic), may infer prerequisite relation two courses. Clearly, directed graph broad coverage universal
1060

fiLearning Concept Graphs Online Educational Data

concepts crucial reasoning course content overlap prerequisite relationships, hence important educational decision making, curriculum planning
students modularization course syllabus design instructors.
obtain knowledge-rich concept graph? Manual specification obviously scalable number concepts reaches tens thousands larger. Using
machine learning automatically induce graph based massive online course materials attractive alternative; however, statistical learning techniques
developed problem, knowledge. Addressing open challenge principled algorithmic solutions novel contribution aim accomplish paper.
call new method Concept Graph Learning (CGL). Specifically, propose multi-level
inference framework illustrated Figure 1, consists two levels graphs
cross-level links. Generally, course would cover multiple concepts, concept may
covered one course. Notice course-level graphs overlap
different universities universal course IDs. However, semantic concepts taught different universities overlap, want learn mappings
non-universal courses universal concept space based online course materials.
paper investigate problem concept graph learning (CGL) new
collections course syllabi (including course names, descriptions, listed lectures, prerequisite relations, etc.) Massachusetts Institute Technology (MIT), California Institute
Technology (Caltech), Carnegie Mellon University (CMU) Princeton. syllabus
data allow us construct initial course-level graph university, may
enriched discovering latent prerequisite links. representing universal
concept space, study four representation schemes (Section 2.1), including 1) using
English words course descriptions, 2) using sparse coding English words, 3) using
distributed word embedding English words, 4) using large subset Wikipedia
categories. representation schemes, provide algorithmic solutions
establish mapping courses concepts, learn concept-level dependencies based observed prerequisite relations course level. second part, i.e.,
explicit learning directed graph universal concepts, unique part
proposed framework. concept graph learned, predict unobserved
prerequisite relations among courses, including training set different universities. words, CGL enables interlingua-style transfer learning
train models course materials universities predict prerequisite
relations courses universities. universal transferability particularly
desirable MOOC environments courses offered different instructors many
universities. mentioned before, course-level sub-graphs different universities
overlap other, prerequisite links local within sub-graph.
Thus enable cross-university transfer, crucial project course-level prerequisite
links different universities onto directed links among universal concepts.
bi-directional inference two directed graphs makes CGL framework
fundamentally different existing approaches graph-based link detection (Kunegis &
Lommatzsch, 2009; Liben-Nowell & Kleinberg, 2007; Lichtenwalter, Lussier, & Chawla,
2010), matrix completion (Candes & Recht, 2009; Fazel, 2002; Johnson, 1990) collaborative filtering (Su & Khoshgoftaar, 2009). is, approach requires explicit learning
1061

fiLiu, Ma, Yang, & Carbonell

concept-level directed graph optimal mapping two levels links
methods (see Section 7 discussion).
main contributions paper1 summarized as:
1. novel framework within- cross-level inference prerequisite relations
course-level concept-level;
2. New algorithmic solutions scalable concept graph learning various (dense,
sparse transductive) settings;
3. New data collections multiple universities syllabus descriptions, prerequisite
links lecture materials;
4. first evaluation prerequisite link prediction within- cross-university
settings.
rest paper organized follows: Section 2 introduces formal definitions
framework optimization objectives; Section 3 provides scalable algorithms
learning large concept graphs; Section 4 extends new method learn sparse, parsimonious concept graph better interpretability; Section 5 explores unlabeled course
pairs leveraged significantly improve prediction performance learned
concept graph; Section 6 describes new datasets collected study future
benchmark evaluations, reports empirical findings; Section 7 discusses related work
concept graphs deployed benefit future educational applications;
Section 8 summarizes main findings study.

2. Framework & Algorithms
Let us formally define methods following notation.
n number courses training set;
p dimension universal concept space (Section 2.1);
X = [x1 , x2 , . . . , xn ]> Rnp collection n courses, xi Rp bag-ofconcepts representation i-th course;
{1, +1}nn collection n2 binary indicators observed prerequisite
relations courses, i.e., yij = 1 means course j prerequisite course
i, yij = 1 otherwise.
Rpp adjacency matrix concept graph, whose elements weights
directed links among concepts. is, matrix model parameters
want optimize given training data X .
1. journal paper substantially extended version previous paper (Yang, Liu, Carbonell, & Ma,
2015).

1062

fiLearning Concept Graphs Online Educational Data

2.1 Representation Schemes
best way represent contents courses learn universal concept
space? explore different answers four alternate choices follows:
1. Word-based Representation (Word): method uses vocabulary course
descriptions plus listed keywords course providers (MIT, Caltech, CMU
Princeton) entire concept (feature) space. applied standard procedures
text preprocessing, including stop-word removal, term-frequency (TF) based term
weighting, removal rare words whose training-set frequency one.
use TF-IDF weighting relative small number documents
(courses) datasets allow reliable estimates IDF part.
2. Sparse Coding Words (SCW): method projects original n-dimensional
vector representations words (the columns course-by-word matrix X) onto
sparse vectors smaller k-dimensional space using Non-negative Matrix Factorization (Lee & Seung, 1999), k much smaller n. One view lower
dimensional components system-discovered latent concepts. Intrigued
successful application sparse coding image processing (Hoyer, 2004), explored
application graph-based inference problem. applying existing sparse
coding algorithm (Kim & Park, 2008) training sets obtained k-dimensional
vector word; taking average word vectors course obtained
bag-of-concepts representation course. resulted n-by-k matrix X,
representing training-set courses k-dimensional space latent concepts.
set k = 100 experiments based cross validation.
3. Distributed Word Embedding (DWE): method also uses dimension-reduced
vectors represent words courses, similar SCW. However, lower dimensional vectors (continuous vector representations) words discovered neural
networks based word usage w.r.t. contextual, syntactic semantic information
(Le & Mikolov, 2014). Intrigued popularity DWE recent research
Natural Language Processing domains (Collobert, Weston, Bottou, Karlen,
Kavukcuoglu, & Kuksa, 2011; Chen, Perozzi, Al-Rfou, & Skiena, 2013), explored
application graph-based inference problem. Specifically, deploy English
word embeddings trained Wikipedia articles (Al-Rfou, Perozzi, & Skiena, 2013),
domain believed semantically close academic courses.
vector representation course obtained aggregating vector representations words contains.
4. Category-based Representation (Cat): method used large subset
Wikipedia categories concept space. selected subset via pooling strategy follows: used words training-set courses form 3509 queries (one
query per course), retrieved top 100 documents per query based cosine
similarity. took union Wikipedia category labels retrieved
documents, removed categories retrieved three queries
less. process resulted total 10,051 categories concept space.
categorization courses based earlier highly scalable very-large category
1063

fiLiu, Ma, Yang, & Carbonell

space work (Gopal & Yang, 2013): classifiers trained labeled Wikipedia
articles applied word-based vector representation course
(weighted) category assignments.
representation schemes may strengths weaknesses.
Word simple natural rather noisy, semantically equivalent lexical variants
unified canonical concepts could systematic vocabulary variation
across universities. Also, scheme work cross-language settings, e.g., course
descriptions English Chinese. Cat would less noisy better cross-language
settings, automated classification step unavoidably introduce errors category
assignments. SCW (sparse coding words) reduces total number model parameters
via dimensionality reduction, may lead robust training (avoiding overfitting)
efficient computation, risk losing useful information projection
original high-dimensional space lower dimensional space. DWE (distributed word
embedding) deploys recent advances representation learning word meanings context.
However, reliable word embedding requires availability large volumes training text
(e.g., Wikipedia articles); potential mismatch training domain (for
large volumes data obtained easily) test domain (for large volumes
data hard costly obtain) could serious issue. Yet another distinction among
representation schemes Word Cat produce human-understandable concepts
links, SCW DWE produce latent factors harder interpret
humans, although methods L1 regularization help interpretability (sec 6.5).
exploring four representation schemes unified framework two-level
graph based inference, examining effectiveness task link prediction
prerequisite relations among courses, aim obtain deeper understanding
strengths weaknesses representational choices.
2.2 Optimization Methods
define problem concept graph learning key part learning-to-predict prerequisite relations among courses, i.e., two-level statistical inference introduced
Section 1 Figure 1. Given training set courses bag-of-concepts representation per course row matrix X, list known prerequisite links per course row
matrix Y, optimize matrix whose elements specify direction (sign)
strength (magnitudes) link concepts. propose two new approaches
problem: classification approach learning-to-rank approach. approaches
deploy extended versions SVM algorithms squared hinge loss,
objective functions optimization different. also propose nearest-neighbor approach comparison, predicts course-level links (prerequisites) without learning
concept-level links.
2.2.1 Classification Approach (CGL.Class)
method, predict score prerequisite link course course j as:
Fij = x>
Axj
1064

(1)

fiLearning Concept Graphs Online Educational Data

Figure 2: weighted connections course course j via matrix encodes
directed links concepts

intuition behind formula shown Figure 2. easily verified
quantity x>
Axj summation weights paths node node
j graph, path weighted using product corresponding xik ,
Akk0 xjk0 . words, assume prerequisite strength two courses
cumulative effect prerequisite strengths concept pairs.
criterion optimizing matrix given training data xi = 1, 2, . . . , n true
labels yij course pairs defined as:
min

ARpp


i2
Xh

1 yij x>
Ax
+ kAk2F
j

2
+

(2)

i,j

(1 v)+ = max(0, 1 v) denotes hinge function, k kF denotes matrix
Frobenius norm. 1st term formula (2) empirical loss; 2nd term regularization term, controlling model complexity based large margin principle.
choose use squared hinge loss (1 v)2+ first term gain first-order continuity
objective function, enabling efficient computation using accelerated gradient descent
(Nesterov, 1983, 1988) (Section 3). efficiency improvement crucial operate pairs courses, thus much larger space normal classification
(e.g. classifying individual courses).
2.2.2 Learning-to-Rank Approach (CGL.Rank)
Inspired learning-to-rank literature (Joachims, Li, Liu, & Zhai, 2007), explored
going beyond binary classifier previous approach one essentially learns
rank prerequisite preferences. Let set course pairs true labels yij = 1
different js, pairs courses true labels yik = 1 different ks,
want system give pairs higher scores pair .
call partial-order preference links conditioned course i.
1065

fiLiu, Ma, Yang, & Carbonell

Let union tuple sets {(i, j, k)|(i, j) , (i, k) } 1, 2, . . . , n.
formulate optimization problem as:
X

min

ARpp

(i,j,k)T

h

i2

>
1 x>
+ kAk2F
Axj xi Axk
2
+

(3)

equivalently, objective rewritten as:
min

ARpp



X



1 XAX

(i,j,k)T

>




ij

+ XAX

>

2
ik +

+


kAk2F
2

(4)

Solving optimization problem requires us extend standard packages SVM algorithms order improve computational efficiency number model parameters
(p2 ) formulation large. example, vocabulary size 15,396 words
MIT dataset, number model parameters 237 million. p
(the number concepts) much larger n (the number courses), one may consider
solving optimization problem dual space instead primal space. However,
even dual space, number dual variables still O(n3 ), corresponding
triplets , kernel matrix order O(n6 ). going address
computational challenges Section 3.
2.2.3 Nearest-Neighbor Approach (kNN)
Different two approaches matrix plays central role, different
baseline, propose predict prerequisite relationship pair courses based
matrices X without A. Let (i0 , j 0 ) new pair courses test set.
score course pair (i, j) training set respect new test pair as:


ff

ff
xi0 , xi xj 0 , xj
i, j = 1, 2, . . . , n
(5)
h, stands inner product two vectors. taking top-scored pairs
training set aggregating corresponding yij s, perform kNN-based
prediction yi0 j 0 new test pair. normalize vectors, dot-products (5)
become cosine similarity. approach requires nearest-neighbor search on-demand;
number course pairs test set large, online computation would
substantial. Via cross validation, found k = 1 (1NN) works best problem
current datasets.
2.2.4 Support Vector Machine (SVM)
another baseline comparision also include SVM following objective:
minp

wR

X h

1 yij (xi xj )> w

i,j


+

+


kwk22
2

(6)

Similar kNN approach unlike CGL, SVM optimization (6)
involve learning matrix (the directed graph universal concepts). feature vector
1066

fiLearning Concept Graphs Online Educational Data

course pair simply pairwise difference two vector representations (the
two bags words) courses. model parameter vector w optimized
labeled training set, used predict prerequisite relation among pair
courses computing w> (xi xj ) whose sign indicates direction relationship,
whose magnitude indicates strength relationship. sorting scores
course pairs fixed test set, ranked list candidate prerequisites
obtained course i.

3. Scalable Algorithms
Though appears gradient-based method directly applicable CGL.Rank
according (4), optimization computationally challenging due following facts:
(a) large number course-level triplets (i, j, k) , n3 worst
case. makes gradient computation w.r.t. loss term (4) expensive.
(b) large size matrix Rpp . example, number entries Word
representation MIT dataset goes 237 million, making matrix manipulations
costly time space.
tackle challenge (a), natural one consider Stochastic Gradient Descent
(SGD). SGD avoids expensive summation O(n3 ) triplets taking noisy
(instead exact) gradient step iteration, noisy gradient step computed
solely based one individual triplet randomly sampled training data. Stochastic
optimization recently successfully applied triplet-based loss functions,
collaborative filtering implicit feedback (Rendle, Freudenthaler, Gantner, & SchmidtThieme, 2009). However, success SGD crucially relies assumption
noisy gradient step sufficiently cheap, true case due challenge (b).
tackle challenge (b), one would consider solving dual problem CGL.Rank
dual space may smaller number coefficients learnin case,
p2 entries folded kernel matrix. However, number dual variables
CGL.Rank equal number triplets, i.e., n3 worst case, scalable optimization
dual space still hard.
following sections, first address (b) reformulating CGL.Rank problem
(4) way optimization objective remains equivalent number
variables substantially reduced (from p2 n2 ). Then, address problem (a)
two specific algorithms substantially reduced number iterations
optimization.
3.1 Reduce Number Variables
Theorem 3.1 (Variable Reduction). Let kernel matrix K = XX > , let h,
matrix inner product. minimizer CGL.Rank optimization (4) B
minimizer following optimization problem
min

BRnn

X
(i,j,k)T

h
i2
ff


1 (KBK)ij + (KBK)ik + KBK, B
2
+
1067

(7)

fiLiu, Ma, Yang, & Carbonell

= X > B X.
Proof. First, let us introduce dummy matrix variable F = XAX > Rnn ,
element F , denoted Fij , corresponds estimated strength prerequisite
dependency course course j.
F rewrite unconstrained optimization (4) constrained optimization
X

min

ARpp ,F Rnn

(1 Fij + Fik )2+ +

(i,j,k)T

subject F = XAX


kAk2F
2

(8)

>

going show optimization (8), degree freedom optimal
actually much smaller p2 .
achieve this, introduce matrix dual variable Rnn corresponding n2
equality constraints F = XAX > . Lagrangian (8) written
L (A, F, ) =

X

(1 Fij + Fik )2+ +

(i,j,k)T


E

kAk2F + F XAX > ,
2

(9)

hard verity (8) convex optimization Slaters condition satisfied,
hence strong duality holds. According stationarity condition, derivative
Lagrangian w.r.t. vanish zero optimal.



E

kAk2F + F XAX > ,
2

> >
=
tr X XA

= X > X


L (A, F, )
=





(10)

=0
= = 1 X > X. worth noticing Rpp contains p2 variables,
completely determined Rnn involves n2 variables (n p).
let us define B 1 Rnn . Combining = 1 X > X = X > B X
constraint F = XAX > , F = KB K K = XX > . Plugging back
expressions F (8) yields optimization (7).
substantially reduced number variables optimization (7) allows us efficiently
compute store gradients concept graph learning.
3.2 Reduce Number Iterations
section introduce two algorithms optimization (7) leads speed
ups reducing total number iterations.
1068

fiLearning Concept Graphs Online Educational Data

3.2.1 Accelerated Gradient Descent
Although gradient descent readily applicable (7), convergence become slow
approach optimal. smoothness objective function (with squared
hinge loss) enables us deploy Nestrerovs accelerated gradient descent (Nesterov, 1983,
1988), ensuring faster convergence rate O(t2 ) rate O(t1 ) gradient
descent, number gradient steps.
Recall Section 3 F = XAX > = KBK. Denote ijk = (1 Fij + Fik )+
ei i-th unit vector Rn .
gradient objective (7) w.r.t. B
ff


F, B
2
(i,j,k)T


X

= 2
ijk (Fij Fik ) + tr BKB > K
2
(i,j,k)T
h




X
>
= 2
K

tr
BKe
e
K
+ KBK
ijk tr BKej e>
k



B =

X

(1 Fij + Fik )2+ +

(11)

(i,j,k)T


= 2K


X





>
K + F
ijk ei e>
j ei ek

(i,j,k)T



P
>
Despite large number course-level triplets , matrix (i,j,k)T ijk ei e>

e
e

j
k
size n n still computed efficiently since majority triplets inactive (i.e. ijk = 0 large number triplet (i, j, k)) optimization. fact,
number operations required evaluating ijk substantially reduced
maintaining specialized data structures order statistics tree (Cormen, Leiserson, Rivest, & Stein, 2001), recently exploited speed gradient
computation rankSVM (Lee & Lin, 2014; Airola, Pahikkala, & Salakoski, 2011).
Detailed implementation CGL.Rank accelerated gradient descent summarized
Algorithm 1.
3.2.2 Inexact Newton Method
often turns bottleneck gradient computation, variable reduction,
dense matrix-matrix multiplication complexity around O(n2.373 ) (Davie & Stothers,
2013). multiplication affordable case since number courses n
dealing thousands. However, scale substantially larger data collections, one may need either consider pruning per-iteration complexity
techniques low-rank kernel approximation (Williams & Seeger, 2001), reducing total number iterations. section, focus latter incorporating
second-order information using Newtons method.
Newtons method going derive inexact (Dembo, Eisenstat, & Steihaug,
1982). is, approximate Newton direction iteration approximately solving linear system via preconditioned Conjugate Gradient method (PCG)
without inverting Hessian. fact, going avoid explicitly writing Hessian
1069

fiLiu, Ma, Yang, & Carbonell

Algorithm 1 CGL.Rank Nestrerovs Accelerated Gradient Descent
1: procedure CGL.Rank.Nestrerov(X, T, , )
2:
K XX > , B 0, Q 0
3:
t1
4:
converge
5:
0
6:
F KBK
7:
(i, j, k)
8:
ijk 1 Fij + Fik
9:
ijk > 0
10:
ij ij + ijk
11:
ik ik ijk
12:
13:
14:
15:
16:
17:

P B (F 2KK)
B P + t1
t+2 (P Q)
QP
tt+1
X > BX
return
2

2

entire optimization process, since Hessian H Rn n (corresponding
n2 model parameters B Rnn ) extremely large.
Denote Tensor (Kronecker) product operator matrices, eij =
2
ei ej (i n + j)-th unit vector Rn . Hessian (7) explicitly derived
H = 2 (K K) (K K) + K K
(12)
P
shorthand (i,j,k)T (eij eik ) (eij eik )> .
Denote vec vectorization operator concatenates columns matrix
single vector. Based
P (11) (12) one verify always true vec (B )
Hvec (B) 2 (K K) (i,j,k)T (eij eik ). Therefore, Newton update
vec (B) vec (B) H 1 vec (B )



X

= vec (B) H 1 Hvec (B) 2 (K K)

(eij eik )

(i,j,k)T

= 2H 1 (K K)

X

(eij eik )

(13)

(i,j,k)T

= 2 [ (K K) + In2 ]1

X

(eij eik )

(i,j,k)T

Though even intractable compute n2 n2 matrix inside inverse operation
(13), updated vec(B) (or matrix B) well approximated solving following
linear system via PCG iterative method
X
2
(eij eik ) = (K K) vec (B) + vec (B)
(14)
(i,j,k)T

1070

fiLearning Concept Graphs Online Educational Data

computation bottleneck PCG lies matrix-vector multiplication (K K) vec (B),
2
2
seems expensive requires huge dense matrix K K Rn n stored
memory. Interestingly, equivalently write expression vec (KBK) playing
vec trick Tensor (Kronecker) product (Van Loan, 2000). reformulation,
aforementioned matrix-vector multiplication becomes affordable since vec (KBK)
computed O(n2.373 ) highly sparse.
suggests complexity iteration PCG cheap
gradient step. also empirically observed inexact Newton method requires
small constant number (typically 3-5) Newton updates reach optimal,
average Newton update 10 PCG iterations suffice yield good results.

4. Learning Sparse Concept Graph
Notice CGL algorithm studied far produces fully dense concept graph A. However, commonly believed dependencies among knowledge concepts
highly sparse. sparse concept graph also desirable visualization purposes thus allowing intuitive user-exploration. reasons, section modify
CGL algorithm produce sparse graphs (sparse-CGL).
straightforward enforce sparsity replacing `2 -norm overP
concept graph
original CGL formulation `1 -norm defined kAk1 := i,j |aij |.
case, sparse CGL optimization objective cast
2
X
>
min
1 x>
Ax
+
x
Ax
+ kAk1
(15)
j
k


ARpp

+

(i,j,k)T

Optimization (15) viewed generalization LASSO (which known produce
sparse solutions), except using pairwise squared hinge loss coefficients matrix forms. Unlike LASSO one optimizes vector coefficient,
parameter space (15) high-dimensional matrix extremely large size.
Due presence `1 -norm objective function, previous parameter
reduction techniques CGL longer applied sparse CGL. following,
going focus directly carrying optimization w.r.t. A. feasible
storing large highly sparse concept graph much cheaper dense case.
4.1 Efficient Optimization Sparse-CGL
Although sub-gradient methods directly applied minimizing non-smooth objective (15), suffer slow convergence rate theoretical empirical
perspectives. Commonly used optimization solvers `1 regularization coordinate descent (CD) (Tseng & Yun, 2009; Chang, Hsieh, & Lin, 2008) longer
closed-form solution case sub-step, needs many p2 steps go
even single cycle model parameters A.
4.1.1 Proximal Gradient Descent
Among family first-order methods, proximal gradient descent (PGD) widely
applied objective functions involving non-smooth components. enjoys several desir1071

fiLiu, Ma, Yang, & Carbonell

able computational properties, including order convergence rate
gradient descent even applied non-smooth objective functions. updating step
PGD efficiently performed long proximal operation efficient.
proximal operator sparse CGL (15) defined
1
kA Zk22 + kZk1
2tk

proxtk (A) := argmin
ZRpp

(16)

tk step size k-th iteration. solution optimization (16)
expressed concisely closed-form:
proxtk (A) = Stk (A)

(17)

: Rpp 7 Rpp known soft-thresholding operator (parameterized )
applied element A. is, i, j


Aij Aij >
[S (A)]ij = 0
(18)
Aij


Aij + Aij <
proximal operator proxtk (A) Stk (A), PGD iteratively applies




A(k) = proxtk A(k1) tk g(A(k1) ) = Stk A(k1) tk g(A(k1) )

(19)

model converges. expression g(A) denotes gradient first
term optimization (15), recast


X
ijk xi (xj xk )>
(20)
g A(k1) = 2
(i,j,k)T


= 2X >


X

ijk ei (ej ek )> X

(21)

(i,j,k)T
>

= 2X X

(22)

sparse CGL, PGD guaranteed reach global optimal since smooth
non-smooth components objective function (15) convex A.
4.1.2 PGD Nesterovs Acceleration
Similar gradient descent CGL discussed previous section 3.2.1, convergence
rate PGD sparse-CGL accelerated applying Nesterovs method.
case, replace (19) PGD



P (k) = Stk A(k1) tk g A(k1)
(23)


k1
A(k) = P (k) +
P (k) P (k1)
(24)
k+2
1072

fiLearning Concept Graphs Online Educational Data

Algorithm 2 Sparse CGL.Rank Accelerated PGD
1: procedure Sparse.CGL.Rank(X, T, , )
2:
0pp , P 0pp , Q 0pp
3:
k1
4:
converge
5:
0nn
6:
F XAX >
7:
(i, j, k)
8:
ijk 1 Fij + Fik
9:
ijk > 0
10:
ij ij + ijk
11:
ik ik ijk
12:
13:
14:
15:
16:
17:

j = 1, 2 . . . p

Pj = Stk Aj + 2tk X > Xj
P + k1
k+2 (P Q)
QP
k k+1
return

use P Rpp capture momentum information historical iterations.
gradient descent PGD, convergence accelerated PGD guaranteed sparse
CGL substantially faster rate.

One might concerned (19) (23), gradient g A(k1) dense
matrix definition (20) therefore expensive memory consumption throughout
optimization. address issue, recall soft-thresholding Stk operator applied
(k)
element-wisely input. Let Pi i-th column P (k) ,

h

(k)
(k1)
(25)
Pi = Stk Ai
tk g A(k1)


(k1)
= Stk Ai
+ 2tk X > N Xi
(26)
suggests sequentially threshold A(k1) tk g A(k1)



via Stk column-by-

(k)
Pi s)

column,
without storing
store resulting sparse column vectors (the
g A(k1) . Details accelerated PGD sparse CGL.Rank summarized Alg. 2.

5. Transductive Concept Graph Learning
real scenarios, observed course-level prerequisite links highly sparse. example,
1,173 2,694,681 (0.043%) possible links observed MIT data
collection. Meanwhile, notice features unlabeled course-level links already
given, massively available training phase. argue transductive
learning particular effective case following reasons:
1. helps better leverage unlabeled data allowing information propagate
labeled unlabeled course pairs.
1073

fiLiu, Ma, Yang, & Carbonell

2. makes weaker assumptions unlabeled (missing) links. contrast
previous CGL formulation unobserved links implicitly treated
negative examples.
derive transductive CGL, start following equivalent form CGL,
derived eliminating original CGL optimization via constraint F = XAX > .
X

min

F Rnn

(1 Fij + Fik )2+ +

(i,j,k)T


vec(F )> (K K)1 vec(F )
2

(27)

viewing second term (27) negative log-likelihood, see original
CGL formulation essentially assumes vec(F ) sampled Gaussian prior distribution
covariance matrix K K K = XX > .
allow transduction labeled unlabeled course-level links, propose
replace inverse pairwise kernel matrix K K (27) associated graph Lapla2
2
cian matrix L Rn n (Chung, 1997), following previous work label propagation
(Zhu, 2005) spectral kernel design (Zhang & Ando, 2006). Formally, define
1

1

L := 2 (D K K) 2

(28)

n2 n2 diagonal matrix diagonal elements equal corresponding row-sum (degree) K K. kernel matrix K symmetrically
normalized, hard show definition (28) reduces
L = K K

(29)

case, vec(F )L vec(F ) becomes (normalized) manifold regularizer (Zhu, Ghahramani, & Lafferty, 2003) enforces predictions F smooth graph
course-level links adjacency matrix K K. particular,
X
2
vec(F )> L vec(F )
kii0 kjj 0 Fi,j Fi0 ,j 0
(30)
(i,j),(i0 ,j 0 )

Given two course-level links (i, j) (i0 , j 0 ), recall kii0 defines similarity
i0 , kjj 0 denotes similarity j j 0 . Hence kii0 kjj 0 denotes similarity
(i, j) (i0 , j 0 ), minimizing (30) essentially enforces similar course-level links
share similar prerequisite strength.
intuitions above, cast optimization transductive CGL
min

F Rnn

X

(1 Fij + Fik )2+ +

(i,j,k)T


vec(F )> L vec(F )
2

(31)

5.1 Efficient Optimization Course-Level Links
worth mentioning though graph Laplacian matrix L course-level links
extremely large, also highly structured. result, able carry operations
involving L fairly efficiently. Moreover, going show explicit storage
full graph Laplacian avoided entire optimization.
1074

fiLearning Concept Graphs Online Educational Data

Algorithm 3 trans-CGL.Rank accelerated GD
1: procedure CGL.Rank.Nestrerov(X, T, , )
2:
K XX > , F rand(n, n), Q 0nn
3:
t1
4:
converge
5:
0nn
6:
(i, j, k)
7:
ijk 1 Fij + Fik
8:
ijk > 0
9:
ij ij + ijk
10:
ik ik ijk
11:
12:
13:
14:
15:
16:

P F (F KF K 2)
F P + t1
t+2 (P Q)
QP
tt+1
X > K 1 F K 1 X
return

following, describe gradient computation carried transCGL. gradient transductive CGL objective (31) w.r.t. F
X
F =
(1 Fij + Fik )2+ + vec1 (L vec(F ))
(32)
(i,j,k)T

X

= 2

ijk (Fij Fik ) + vec1 [(I K K) vec(F )]

(33)

(i,j,k)T


= 2


X

ijk ei (ej ek )> + vec1 (vec(F )) vec1 [(K K) vec(F )] (34)

(i,j,k)T

= 2 + F KF K
(35)
hP

>
:=

e
(e

e
)
. order obtain last equality,
k
(i,j,k)T ijk j
applied vectorization trick (Van Loan, 2000) Tensor (Kronecker) product third
term. Note expression gradient (35) doesnt involve tensor-type operation,
2
2
despite huge Laplacian matrix L Rn n original objective function.
Second-order methods, inexact Newton method applicable trans-CGL.
omit details since derivations similar CGL.
5.2 Projecting Back Concept-Level Links
objective transductive CGL (31) involves course-level prerequisite strength
F instead concept-level graph A. order recover optimal solution
F (31), consider solving following optimization problem
min

ARpp

kAk22

subject F = XAX >
1075

(36)

fiLiu, Ma, Yang, & Carbonell

University
MIT
Caltech
CMU
Princeton

# Courses
2322
1048
83
56

# Prerequisites
1173
761
150
90

# Words
15396
5617
1955
454

Table 1: Datasets Statistics
bilinear system F = XAX > under-determined total number concepts
p assumed greater number courses n. multiple feasible
concept graphs associated F , (36) aims pick concept graph minimum norm
(which usually indicates strong generalization ability).
Solution optimization (36) derived stationarity condition,
written closed-form follows
= X > K 1 F K 1 X

(37)

Details accelerated gradient descent trans-CGL.Rank, including recovery
step concept graph, summarized Alg. 3.

6. Experiments
collected course listings, including course descriptions available prerequisite structure MIT OpenCourseWare, Caltech, CMU Princeton2 . first two complete course catalogs, latter two required spidering scraping, hence
collected Computer Science Statistics CMU, Mathematics Princeton.
implies test within-university prerequisite discovery fourthough
MIT Caltech comprehensiveand cross-university university pairs
training university contains disciplines test university.
Table 1 summarizes datasets statistics.
evaluate performance, use Mean Average Precision (MAP)
preferred metric information retrieval evaluating ranked lists, Area
Curve ROC (ROC/AUC simply AUC) popular link detection evaluations.
6.1 Within-University Prerequisite Prediction
tested methods dataset university. used one third
data testing, remaining two thirds training validation. conducted
5-fold cross validation training two-thirds, i.e., trained model 80%
training/validation dataset, tuned extra parameters remaining 20%. repeated
process 5 times different 80-20% spit run. results 5 runs
averaged reporting results. Figure 3 Table 2 summarize results CGL.Rank,
CGL.Class, 1NN SVM. methods used English words representation
scheme first set experiments.
2. datasets available http://nyc.lti.cs.cmu.edu/teacher/dataset/

1076

fiLearning Concept Graphs Online Educational Data

MIT

Caltech

CMU

Princeton

1.00

AUC
MAP

0.80

0.60

0.40

0.20

Cl

Ra

L.

L.

CG

CG



1N
N
SV


nk

N
SV





1N

Cl
L.

CG

CG

L.

Ra

nk



1N
N
SV


Cl
L.

CG

CG

L.

Ra

nk



1N
N
SV


Cl
L.

CG

CG

L.

Ra

nk

0.00

Figure 3: Different methods within-university prerequisite prediction: methods
used words concepts.

Algorithm
CGL.Rank
CGL.Class
1NN
SVM
CGL.Rank
CGL.Class
1NN
SVM
CGL.Rank
CGL.Class
1NN
SVM
CGL.Rank
CGL.Class
1NN
SVM

Data
MIT
MIT
MIT
MIT
Caltech
Caltech
Caltech
Caltech
CMU
CMU
CMU
CMU
Princeton
Princeton
Princeton
Princeton

AUC
0.96
0.86
0.76
0.78
0.95
0.86
0.60
0.74
0.79
0.70
0.75
0.64
0.92
0.89
0.82
0.71

MAP
0.46
0.34
0.30
0.04
0.33
0.27
0.16
0.03
0.55
0.38
0.43
0.30
0.69
0.61
0.58
0.31

Table 2: Results within-university prerequisite prediction using words concepts.

1077

fiLiu, Ma, Yang, & Carbonell

Notice AUC scores methods much higher MAP scores.
high AUC scores derive large part fact AUC gives equal weight
system-predicted true positives, regardless positions system-produced ranked
lists. hand, MAP weighs heavily true positives higher positions
ranked lists. words, MAP measures performance system harder task:
system needs find true positives (along false positives), also needs
rank higher false positives possible order obtain high MAP score.
Using concrete example, totally useless system makes positive negative
predictions random 50% chances AUC score 50%.
system extremely low score MAP chance true positive
randomly appear top ranked list low true negatives dominate
domain. datasets domain course requires
small number courses prerequisites. Back original point, regardless
popularity AUC link detection evaluations, limitation recognized:
relative performance among methods informative absolute values AUC.
see Figure 3, relative ordering methods AUC MAP
indeed highly correlated across datasets. MAP emphasizes positive instances
top portion ranked list, hence sensible measuring usefulness
system user interacts system-recommended ranked list prerequisites
per query (a course test set).
Comparing results methods, see CGL.Rank clearly dominates
others AUC MAP datasets. CGL.Class second best, outperforming 1NN SVM. Comparing 1NN SVM, two methods comparable
AUC average; however, 1NN better SVM MAP four datasets.
One may wonder SVM relatively poor performance course-level prerequisite prediction particular MAP metric, given works well
text classification learning rank retrieval general. argue SVM
suffering major difficulty prerequisite prediction due extremely sparse
labeled positive training instances (prerequisite pairs). Recall text classification
SVM optimizes one w category; however, prerequisite prediction SVM uses
w generate ranked lists possible queries (i.e., test-set courses). Thus
labeled training instances per query extremely sparse average, resulting poor
performance observed. words, SVM generalized much extremely
small set labeled training instances optimizing global w. kNN (or 1NN)
hand, suffers less SVM inference kNN based local
training instances neighborhood query, instead global generalization
queries.3 Nevertheless, neither kNN SVM competitive comparison
proposed CGL methods, evident set evaluation results.
3. Notice SVM, described (6), essentially going produce identical ranked list prerequisite
courses given course. see this, consider given course i, SVM scores remaining courses
>
>
scorej := (xi xj )> w = x>
w xj w different js. Since given, first term xi w
ignored ranking shared across scores. result, ranked list becomes
irrelevant xi itself. analysis holds course hence ranked list prerequisites
identical courses, leading low MAP score. contrast, CGL scores course j
>
>
scorej = x>
Axj = (Axi ) xj := wi xj ranking coefficient wi personalized i-th
course, hence able produce diverse ranked lists different courses.

1078

fiLearning Concept Graphs Online Educational Data

1.00

1.00

AUC
MAP

0.80
0.60

0.60

0.40

0.40

0.20

0.20

0.00

Word

Cat.

SCW

0.00

DWE

(a) CGL.Rank MIT Data
1.00

0.40

0.40

0.20

0.20
Cat.

SCW

SCW

DWE

AUC
MAP

0.80
0.60

Word

Cat.

1.00

0.60

0.00

Word

(b) CGL.Rank Caltech Data

AUC
MAP

0.80

AUC
MAP

0.80

0.00

DWE

(c) CGL.Rank CMU Data

Word

Cat.

SCW

DWE

(d) CGL.Rank Princeton Data

Figure 4: CGL.Rank different representation schemes within-university prerequisite
prediction

6.2 Effects Representation Schemes
Figure 4 Table 3 summarize results CGL.Rank four representation
schemes described Section 2.1, i.e., Word, Cat, SCW (sparse coding words) DWE
(distributed word embedding), respectively. scores AUC MAP
scale, relative performance suggest Word Cat competitive
(or Word slightly better datasets), followed SCW
DWE. rest empirical results reported paper, focus
performance CGL.Rank Word representation scheme performs
better, space limit allow us present results every possible
permutation method, scheme, dataset metric.
6.3 Cross-University Prerequisite Prediction
set experiments, fixed test sets used within-university
evaluations, alter training sets across universities, yielding transfer learning results models trained data different university
tested. fixing test sets within- cross-university evaluations compare results common basis. competitive performance Cat
comparison Word encouraging, given Wikipedia categories defined
general knowledge, classifiers (SVMs) used category assignment courses
trained Wikipedia articles instead course materials (because
1079

fiLiu, Ma, Yang, & Carbonell

Concept
Word
Cat.
SCW
DWE
Word
Cat.
SCW
DWE
Word
Cat.
SCW
DWE
Word
Cat.
SCW
DWE

Data
MIT
MIT
MIT
MIT
Caltech
Caltech
Caltech
Caltech
CMU
CMU
CMU
CMU
Princeton
Princeton
Princeton
Princeton

AUC
0.96
0.93
0.93
0.83
0.95
0.93
0.91
0.76
0.79
0.77
0.73
0.67
0.92
0.84
0.82
0.77

MAP
0.46
0.36
0.33
0.09
0.33
0.32
0.22
0.12
0.55
0.55
0.43
0.35
0.69
0.68
0.60
0.50

Table 3: CGL.Rank four representations
human assigned Wikipedia category labels courses). means Wikipedia
categories indeed good coverage concepts taught universities (and
probably MOOC courses), pooling strategy selecting relevant subset
Wikipedia categories reasonably successful.
Table 4 Figure 5 show results CGL.Rank (using words concepts). Recall
MIT Caltech data cover complete course catalogs, CMU data
cover Computer Science Statistics, Princeton data
Mathematics. implies measure transfer learning pairs
training university contains disciplines test university. comparing red bars
(when training university test university same) blue bars (when
training university test university), see performance
loss transfer learning, expected. Nevertheless, get transfer,
first report successful transfer learning educational knowledge, especially
prerequisite structures disjoint graphs, across different universities unified
concept graph. results therefore highly encouraging suggest continued efforts
improve. results also suggest interesting points, e.g., MIT might
better coverage topics taught Caltech, compared inverse. And, MIT courses
seem closer Princeton (Math) compared CMU.
6.4 CGL vs Sparse-CGL
subsection evaluate performance CGL (CGL.Rank) sparse-CGL (Section 4) course-level prerequisite prediction. two methods compared budget
constraints, mean number allowed links system-induced concept graph controlled condition comparison. words, control
1080

fiLearning Concept Graphs Online Educational Data

Training
MIT
Caltech
Caltech
MIT
CMU
MIT
Caltech
Princeton
MIT
Caltech

Test
MIT
MIT
Caltech
Caltech
CMU
CMU
CMU
Princeton
Princeton
Princeton

MAP
0.46
0.13
0.33
0.25
0.55
0.34
0.28
0.69
0.46
0.43

AUC
0.96
0.88
0.95
0.86
0.79
0.70
0.62
0.92
0.72
0.58

Table 4: CGL.Rank within-university cross-university settings
1.00

MAP

0.80
0.60
0.40
0.20
U
.
Ca MI CM
Pr
lte T.C U

ch
ce
.C U


n.
U
Ca MI Pri
lte T.P nce
ch rin
.P ce n
rin
ce n

n

CM

Ca MI
lte T.M
ch
.M
Ca

lte
ch
.Ca
lt
.C ec
al h
te
ch

0.00

<Traning Set>.<Test Set>

Figure 5: Results CGL.Rank based words tasks within-university (red)
cross-university (blue) prerequisite prediction.

graph sparsity varying number allowed non-zero elements 4 , compare
performance two methods conditioned fixed degree graph sparsity.
Figure 6 compares performance two methods datasets MIT, Caltech,
CMU Princeton, task within-university prediction course-level prerequisite
relations. horizontal axis graph specifies number non-zero elements
allowed matrix A, ranging 27 212 . vertical axis graph MAP
4. concept graph induced CGL typically dense, sparsified (given desired sparsity)
keeping dominating elements setting remaining elements zero. sparseCGL, hand, sparse graphs directly induced optimization algorithm adjusting
values `1 -regularization strength .

1081

fiLiu, Ma, Yang, & Carbonell

left AUC right data set. CGL curves blue,
sparse-CGL curves red.
Clearly, sparse-CGL consistently outperformed CGL experiments
budget regions datasets, MAP AUC. results highly encouraging effective efficient interaction navigation users system-induced
concept graph, interpretable relatively sparse graph often preferable
densely connected graph. budget constrained graphs would also lead scalable
curriculum planning fast computation on-demand recommendation.
6.5 CGL vs Trans-CGL
subsection compare course-level prerequisite prediction performance CGL
(in particular, CGL.Rank) transductive extension. two methods tested
aforementioned four datasets words representation scheme. experiments conducted 5-fold cross-validation setting described 6.1.
use MAP AUC evaluation metrics summarize results Table 5.
Table 5 see link prediction performance trans-CGL dominates
CGL. justifies previous arguments making good use massive unlabeled
course-level links provided training set help us get better prediction performance.
Meanwhile, notice performance gain obtained trans-CGL MIT
large three institutions. Since MIT dataset substantially larger amount course-level labels, conjecture trans-CGL, many
transductive/semi-supervised learning approaches general, advantageous
available supervision insufficient (compared amount hidden information
unlabeled data). validate thought, repeat experiments two methods down-sampled MIT dataset. is, use random subset available
course-level prerequisite links training, gradually vary size training subset
get multiple set results. results summarized Table 6.
Institution

MIT

Caltech

CMU

Princeton

MAP

CGL
trans-CGL

0.482
0.485

0.477
0.499

0.482
0.539

0.436
0.445

AUC

CGL
trans-CGL

0.956
0.957

0.929
0.941

0.801
0.818

0.634
0.67

Table 5: Comparison course prerequisite prediction performance CGL
trans-CGL MIT, Caltech, CMU Princeton. Results significance
tests (paired t-tests) best method method
dataset denoted significance 1% level.

6.6 Experiment Details
tested efficiency proposed algorithms (based optimization formulation
variable reduction) single machine Intel i7 8-core processor 32GB
1082

fiLearning Concept Graphs Online Educational Data

MAP MIT

AUC MIT

0.45

0.96
0.94
0.92
0.9
0.88
0.86
0.84
0.82
0.8

0.4
0.35
0.3
0.25
0.2

CGL
sparse-CGL

0.15
0.1

CGL
sparse-CGL
96

40

48

20

24

10

2

51

6

25

8

12

96

40

48

20

24

10

2

51

6

25

8

12

MAP Caltech

AUC Caltech

0.22

0.85

0.2

0.8

0.18

0.75

0.16

0.7

0.14

CGL
sparse-CGL

0.12
0.1

CGL
sparse-CGL

0.65
0.6

6

9
40

8

4
20

4

2
10

0.44
0.43
0.42
0.41
0.4
0.39
0.38
0.37
0.36
0.35
0.34
0.33

2
51

6
25

8
12

6

9
40

8

4
20

4

2
10

2
51

6
25

8
12

MAP CMU

AUC CMU
0.82
0.8
0.78
0.76
0.74
0.72

CGL
sparse-CGL

CGL
sparse-CGL

0.7
0.68

6

9
40

8

4
20

4

2
10

2
51

6
25

8
12

6

9
40

8

4
20

4

2
10

2
51

6
25

8
12

MAP Princeton

AUC Princeton

0.44

0.67

0.42

0.66

0.4

0.65

0.38

0.64

0.36
0.34

0.63

CGL
sparse-CGL

0.32
0.3

CGL
sparse-CGL

0.62
0.61

96

40

48

20

24

10

2

51

6

25

8

12

96

40

48

20

24

10

2

51

6

25

8

12

Figure 6: Comparison CGL (blue curves) sparse-CGL (red curves) prediction
course-level prerequisite relations within MIT, Caltech, CMU Princeton,
respectively. x-axis graph specifies budget constraint logscale, number allowed non-zero elements matrix A. y-axis
measures performance MAP left, AUC right.

1083

fiLiu, Ma, Yang, & Carbonell

Training Data
Down-sampling Rate

1
10

1
9

1
8

1
7

1
6

1
5

1
4

1
3

1
2

MAP

CGL
trans-CGL

0.183
0.202

0.149
0.153

0.192
0.213

0.231
0.249

0.245
0.26

0.264
0.285

0.254
0.262

0.258
0.274

0.289
0.307

AUC

CGL
trans-CGL

0.843
0.874

0.832
0.838

0.842
0.845

0.872
0.875

0.873
0.872

0.876
0.892

0.873
0.868

0.897
0.91

0.905
0.914

Table 6: Comparison course prerequisite prediction performance CGL
trans-CGL MIT dataset, training size varies 10% 50%.

RAM. largest MIT dataset 981,009 training triplets 490,505 test triplets,
CGL.Rank gradient descent took 37.3 minutes 1490 iterations reach convergence rate 103 . achieve objective value, accelerated gradient descent
took 3.08 minutes 401MB memory 103 iterations, inexact Newton method
took 43.4 seconds 587MB memory. CGL.Class equally efficient CGL.Rank
terms run time, though latter superior terms result quality. 1NN
baseline, took 2.88 hours since huge number (2 981, 009 490, 505) dot-products
need computed fly.
CPU time consumed trans-CGL.Rank similar CGL.Rank.
sparse-CGL, took accelerated proximal gradient method 2.07 minutes reach
convergence rate 103 MIT 3.9GB peak memory consumption. resulting
concept graph 1519 nonzero entries.

7. Discussion Related Work
Whereas task inferring prerequisite relations among courses concepts,
task inferring concept network course network order transfer learned
relations new, extensions SVM algorithms presented, work
inspired methods related fields, primarily:
collaborative filtering via matrix completion literature focused one
graph, bipartite graph u users preferences (e.g. movies products).
Given known values u-by-m matrix, task estimate remaining values
(e.g. unseen movies products would user like) (Su et al., 2009).
done via methods affine rank minimization (Fazel, 2002) reduce convex
optimization (Boyd & Vandenberghe, 2004).
Another line related research transfer learning (Do & Ng, 2005; Yang, Hanneke,
& Carbonell, 2013; Zhang, Ghahramani, & Yang, 2008). seek transfer prerequisite
relations pairs courses within universities pairs also within universities,
pairs span universities. inspired different transfer
learning literature. Transfer learning traditionally seeks transfer informative features,
priors, latent structures recently regularization penalties (Kshirsagar, Carbonell,
& Klein-Seetharaman, 1990). Instead, transfer shared concepts mappings
course-space concept-space induce prerequisite relations.
Although evaluations primarily focus detecting prerequisite relations among
courses, task one direct application automatically induced univer1084

fiLearning Concept Graphs Online Educational Data

Linear_algebra
Numerical_analysis
Probability_theory
Applied_mathematics_stubs

Functional_analysis

Integral_calculus

Cybernetics

Mathematical_analysis_stubs

Randomness

Mathematical_optimization

Operations_research

Differential_geometry

Fourier_analysis

Computational_science Signal_processing
Dynamical_systems

Figure 7: visualization concept graph produced CGL.Rank based 10,051 academic Wikipedia categories, 2322 courses 1173 prerequisite relations MIT
OpenCourseWare. node denotes concept (i.e. Wikipedia category),
strength link encodes prerequisite strength pair concepts. Concepts small degrees links weak strength removed via
thresholding visualization purposes.

sal concept graph. important applications include automated semi-automated
curriculum planning personal education goals based different backgrounds students, modularization course syllabus design instructors. tasks require
interaction humans (students teachers) system-induced concept graph
well system-recommended options optimal sequences ordered courses. Figure
7 visualizes sub-graph system-induced concept space based course materials
(including partially observed prerequisite structure) MIT OpenCourseWare: nodes
Wikipedia categories (concepts), links system-predicted partial orders
instructors follow teaching concepts, students follow learning
concepts. one see, Linear Algebra, Probability Theory Functional Analysis
hubs (with high degree out-links) graph, indicating concepts
fundamental one acquire pursuing advanced topics
Differential Geometry, Cybernetics Fourier Analysis.
Let us illustrate potential use CGL.Rank development
problem inferring prerequisite relationships observed prerequisites among
courses (almost) available, e.g., context MOOC. Recall MOOC courses
1085

fiLiu, Ma, Yang, & Carbonell

weights X

bigdataanalytics

bioinfomethods2

patterndiscovery

Target courses

weights

weights X

Data_warehousing

Relational_database_management_systems

Data_security

Distributed_computing_architecture

Computer_science_stubs

Project_management_software

Business_intelligence

Information

Information_technology_management

Information_science

Data_modeling

Project_management

Design

Data_management

Software_design_patterns

Urban_studies_and_planning

Data_management

Information_technology

Database_management_systems

Database_management_systems

Target concepts

Prerequisite concepts

statinference
getdata
exdata
compmethods
introstats
dataanalysis
predmachlearn

Prerequisite courses

Figure 8: example prerequisite course recommendation Coursera using concept graph learned MIT OpenCourseWare dataset. map courses
(red, left) student wants learn concept space Wikipedia categories, find prerequisite concepts map back Coursera courses
(red, right). sizes concept nodes middle (green) proportional aggregated weights corresponding links, strengths
course-concept mapping concept-concept prerequisite relations shown
color intensity edges (purple).

offered different institutions across world, cross-provider prerequisite links
among courses explicitly available. instance, among 900+ courses
Coursera currently offers, half mention anything required
background; remaining half, mentioned background requirements often
vague, e.g., Undergraduate-level networking know-how recommended course
Cloud Networking. overly generic vague notions sufficiently informative
students understand true prerequisite relationship among courses, also
useful intelligent systems learn prediction latent prerequisites automatically. 5
example Figure 8 illustrates key idea applying CGL.Rank algorithm
address problems above. first column left consists three Coursera
courses student wants take, i.e., Big Data Analytics Healthcare (bigdataanalytics), Bioinformatic Methods II(bioinfomethods2 ) Pattern Discovery Data
Mining(patterndiscovery), respectively. second column nodes top-10 universal concepts (Wikipedia categories) assigned classifiers (Section 2.1 Cat)
courses, color intensity edges 1st 2nd columns reflects
5. Coursera offer so-called specializations, specialization consists sequence courses
topic. unclear whether specializations may serve prerequisite relations.

1086

fiLearning Concept Graphs Online Educational Data

confidence scores (matrix X) category assignments. size concept node
proportional aggregated confidence scores corresponding edges, indicating
relative importance concept. third column nodes top-10 prerequisite
concepts (also Wikipedia categories) concepts shown 2nd column,
color intensity edges 2st 3nd columns reflects automatically
induced strengths concept-level links (Matrix A) CGL.Rank algorithm,
used MIT OpenCourseWare data training set. nodes 4th column
Coursera courses best cover prerequisite concepts 3rd column; edges
3rd column similar 1st 2nd columns. Together,
chained network allows us make inference connections target
courses (the left-most column) prerequisite courses (the right-most column).
reader see Figure 8, many courses prerequisites highly
relevant focus primary dimension data analytics, still constitute incomplete set second dimension biotechnology represented.
process refining methods thorough testing performedwe offer
example work-in-progress indicating current challenges future directions.

8. Concluding Remarks
conducted new investigation automatically inferring directed graphs
course level concept level, enable prerequisite prediction within-university
cross-university settings.
proposed three approaches: classification approach (CGL.Class), learning rank
approach (CGL.Rank), nearest-neighbor search approach (kNN). CGL.Class
CGL.Rank (deploying adapted versions SVM algorithms) explicitly model concept-level
dependencies directed graph, support interlingua-style transfer learning
across universities, kNN makes simpler prediction without learning concept dependencies. tackle extremely high-dimensional optimization problems (e.g., 2 108
links concept graph MIT courses), novel reformulation CGL.Rank
enables deployment fast numerical solutions. newly collected datasets
MIT, Caltech, CMU Princeton, CGL.Rank proved best MAP ROC/AUC,
computationally much efficient kNN.
extended aforementioned CGL algorithm produce sparse concept
graph based `1 -regularization (sparse-CGL), leverage information massive
unlabeled course pairs based graph-regularization (trans-CGL). developed scalable
optimization strategies support new formulations, conducted experiments
empirically showed sparse-CGL able give interpretable concept
graph, trans-CGL significantly consistently improved performance
ordinary CGL course prerequisite prediction.
also tested four representation schemes course content: using original words,
using Wikipedia categories concepts, using distributed word representation, using
sparse word encoding. first two: original words Wikipedia-derived concepts proved
best. results within- cross-university settings highly encouraging.
1087

fiLiu, Ma, Yang, & Carbonell

envision cross-university transfer learning approaches particularly
important MOOCs courses come different providers across institutions,
seldom explicit prerequisite links. rich suite future work includes:
Testing cross-institution cross-course-provider prerequisite links. tested
cross-university transfer learning, inferred links within target university, rather cross-institutional links. natural extension current work
predict cross-institutional prerequisites. evaluation need labeled ground
truth cross-institutional prerequisites.
Cross-language transfer. Using Wikipedia categories entries different languages, would interesting challenge infer prerequisite relations courses
different languages mapping Wikipedia category/concept interlingua.
Extensions inference single source multiple sources, single media
(text) multiple media (including videos), single granularity level (courses)
multiple levels (including lectures).
Deploying induced concept graph personalized curriculum planning students (as Section 7) syllabus design course modularization teachers.

Acknowledgments
thank reviewers helpful comments. work supported part
National Science Foundation grants IIS-1216282, IIS-1350364 IIS-1546329.

References
MIT OpenCourseWare. http://ocw.mit.edu/index.htm. Accessed: 2016-03-31.
Airola, A., Pahikkala, T., & Salakoski, T. (2011). Training linear ranking SVMs linearithmic time using redblack trees. Pattern Recognition Letters, 32 (9), 13281336.
Al-Rfou, R., Perozzi, B., & Skiena, S. (2013). Polyglot: Distributed word representations
multilingual NLP. CoNLL-2013, 183.
Boyd, S., & Vandenberghe, L. (2004). Convex optimization. Cambridge university press.
Candes, E. J., & Recht, B. (2009). Exact matrix completion via convex optimization.
Foundations Computational mathematics, 9 (6), 717772.
Chang, K.-W., Hsieh, C.-J., & Lin, C.-J. (2008). Coordinate descent method large-scale
l2-loss linear support vector machines. Journal Machine Learning Research, 9,
13691398.
Chen, Y., Perozzi, B., Al-Rfou, R., & Skiena, S. (2013). expressive power word embeddings. ICML 2013 Workshop Deep Learning Audio, Speech, Language
Processing.
Chung, F. R. K. (1997). Spectral graph theory, Vol. 92. American Mathematical Soc.
1088

fiLearning Concept Graphs Online Educational Data

Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).
Natural Language Processing (almost) Scratch. Journal Machine Learning Research, 12, 24932537.
Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001). Introduction algorithms.
MIT Press McGraw-Hill.
Davie, A. M., & Stothers, A. J. (2013). Improved bound complexity matrix multiplication. Proceedings Royal Society Edinburgh: Section Mathematics,
143 (02), 351369.
Dembo, R. S., Eisenstat, S. C., & Steihaug, T. (1982). Inexact Newton methods. SIAM
Journal Numerical analysis, 19 (2), 400408.
Do, C., & Ng, A. Y. (2005). Transfer learning text classification. Proceedings
NIPS-05.
Fazel, M. (2002). Matrix rank minimization applications. Ph.D. thesis, Stanford University.
Gopal, S., & Yang, Y. (2013). Recursive regularization large-scale classification
hierarchical graphical dependencies. Proceedings 19th ACM SIGKDD
international conference Knowledge discovery data mining, pp. 257265. ACM.
Hoyer, P. O. (2004). Non-negative matrix factorization sparseness constraints.
Journal Machine Learning Research, 5, 14571469.
Joachims, T., Li, H., Liu, T.-Y., & Zhai, C. (2007). Learning rank information
retrieval (LR4IR 2007). SIGIR Forum, Vol. 41, pp. 5862.
Johnson, C. R. (1990). Matrix completion problems: survey. Proceedings Symposia
Applied Mathematics, Vol. 40, pp. 171198.
Kim, H., & Park, H. (2008). Nonnegative matrix factorization based alternating nonnegativity constrained least squares active set method. SIAM Journal Matrix
Analysis Applications, 30 (2), 713730.
Kshirsagar, M., Carbonell, J., & Klein-Seetharaman, J. (1990). Transfer learning based
methods towards discovery host-pathogen protein-protein interactions. Proc
ISMB, Vol. 40, pp. 171198.
Kunegis, J., & Lommatzsch, A. (2009). Learning spectral graph transformations link
prediction. Proceedings 26th Annual International Conference Machine
Learning, pp. 561568. ACM.
Le, Q., & Mikolov, T. (2014). Distributed representations sentences documents.
Proceedings 31st International Conference Machine Learning (ICML-14),
pp. 11881196.
Lee, C.-P., & Lin, C.-J. (2014). Large-scale linear rankSVM. Neural computation, 26 (4),
781817.
Lee, D. D., & Seung, H. S. (1999). Learning parts objects non-negative matrix
factorization. Nature, 401 (6755), 788791.
1089

fiLiu, Ma, Yang, & Carbonell

Liben-Nowell, D., & Kleinberg, J. (2007). link-prediction problem social networks.
Journal American society information science technology, 58 (7), 1019
1031.
Lichtenwalter, R. N., Lussier, J. T., & Chawla, N. V. (2010). New perspectives methods
link prediction. Proceedings 16th ACM SIGKDD international conference
Knowledge discovery data mining, pp. 243252. ACM.
Nesterov, Y. (1983). method solving convex programming problem convergence
rate (1/k2). Soviet Mathematics Doklady, Vol. 27, pp. 372376.
Nesterov, Y. (1988). approach construction optimal methods minimization
smooth convex functions. Ekonomika Mateaticheskie Metody, 24, 509517.
Rendle, S., Freudenthaler, C., Gantner, Z., & Schmidt-Thieme, L. (2009). BPR: Bayesian
personalized ranking implicit feedback. Proceedings Twenty-Fifth Conference Uncertainty Artificial Intelligence, pp. 452461. AUAI Press.
Su, X., & Khoshgoftaar, T. M. (2009). survey collaborative filtering techniques. Advances artificial intelligence, 2009, 4.
Tseng, P., & Yun, S. (2009). coordinate gradient descent method nonsmooth separable
minimization. Mathematical Programming, 117 (1-2), 387423.
Van Loan, C. F. (2000). ubiquitous Kronecker product. Journal computational
applied mathematics, 123 (1), 85100.
Williams, C., & Seeger, M. (2001). Using Nystrom method speed kernel machines.
Proceedings 14th Annual Conference Neural Information Processing Systems, No. EPFL-CONF-161322, pp. 682688.
Yang, L., Hanneke, S., & Carbonell, J. (2013). theory transfer learning applications
active learning. Machine learning, 90 (2), 161189.
Yang, Y., Liu, H., Carbonell, J., & Ma, W. (2015). Concept graph learning educational
data. Proceedings Eighth ACM International Conference Web Search
Data Mining, pp. 159168. ACM.
Zhang, J., Ghahramani, Z., & Yang, Y. (2008). Flexible latent variable models multi-task
learning. Machine Learning, 73 (3), 221242.
Zhang, T., & Ando, R. (2006). Analysis spectral kernel design based semi-supervised
learning. Advances neural information processing systems, 18, 1601.
Zhu, X. (2005). Semi-supervised learning literature survey. Tech. rep. TR 1530, University
Wisconsin - Madison.
Zhu, X., Ghahramani, Z., & Lafferty, J. (2003). Semi-supervised learning using Gaussian
fields harmonic functions. Proceedings ICML-03, Vol. 3, pp. 912919.

1090

fiJournal Artificial Intelligence Research 55 (2016) 443-497

Submitted 11/14; published 02/16

Optimally Solving Dec-POMDPs Continuous-State MDPs
Jilles Steeve Dibangoye

jilles-steeve.dibangoye@insa-lyon.fr

Univ de Lyon
INSA-Lyon, CITI-Inria, F-69621, France

Christopher Amato

camato@cs.unh.edu

University New Hampshire
Durham, NH, USA

Olivier Buffet
Francois Charpillet

olivier.buffet@inria.fr
francois.charpillet@inria.fr

Inria Universite de Lorraine CNRS
Villers-les-Nancy, F-54600, France

Abstract
Decentralized partially observable Markov decision processes (Dec-POMDPs) provide general model decision-making uncertainty decentralized settings, difficult solve
optimally (NEXP-Complete). new way solving problems, introduce idea
transforming Dec-POMDP continuous-state deterministic MDP piecewise-linear
convex value function. approach makes use fact planning accomplished
centralized offline manner, execution still decentralized. new Dec-POMDP formulation, call occupancy MDP, allows powerful POMDP continuous-state MDP
methods used first time. provide scalability, refine approach combining heuristic search compact representations exploit structure present multi-agent
domains, without losing ability converge optimal solution. particular, introduce
feature-based heuristic search value iteration (FB-HSVI) algorithm relies feature-based
compact representations, point-based updates efficient action selection. theoretical analysis
demonstrates FB-HSVI terminates finite time optimal solution. include extensive empirical analysis using well-known benchmarks, thereby demonstrating approach
provides significant scalability improvements compared state art.

1. Introduction
Many significant real-world problems involve decision making sequential multiagent environments. Examples include: exploration robots must coordinate perform experiments
unknown planet (Zilberstein, Washington, Bernstein, & Mouaddib, 2002); rescue robots that,
disaster, must safely find victims quickly possible (Paquet, Chaib-draa, Dallaire, & Bergeron, 2010); optimized distributed congestion control noisy computer network (Winstein &
Balakrishnan, 2013); sensor networks multiple sensors work jointly perform large-scale
sensing task strict power constraints (Jain, Taylor, Tambe, & Yokoo, 2009); robot logistics
problems communication limitations sensor uncertainty (Amato, Konidaris, Anders, Cruz,
How, & Kaelbling, 2015). tasks require multiple decision makers, agents, coordinate
actions order achieve common long-term goals. Additionally, uncertainty ubiquitous
domains, effects actions information received agents.

c
2016
AI Access Foundation. rights reserved.

fiDibangoye, Amato, Buffet & Charpillet

Markov decision processes (MDPs) address uncertainty system dynamics, assume centralization. Standard methods solving MDPs, e.g., linear dynamic programming (Bellman,
1957; Puterman, 1994) heuristic search (Barto, Bradtke, & Singh, 1995; Hansen & Zilberstein, 2001), centralized planning execution phases. Partially observable
Markov decision processes (POMDPs) extend MDPs situations uncertainty
state system (Kaelbling, Littman, & Cassandra, 1998; Smith & Simmons, 2006; Shani,
Pineau, & Kaplow, 2013), similarly assume centralized planning execution.
use MDP POMDP models multiple agents present, centralized coordinator agent must global view underlying state (or belief state case POMDP)
entire system, plan behalf teammates. Every time step, agent would transmit appropriate action agent must perform observe resulting state (or
observations agent POMDP). methods, collectively referred centralized
planning centralized control, assume agents communicate step delay cost,
either explicitly messages implicitly observations. Unfortunately, many practical applications, agents permitted share information delay cost; rather,
agent possesses local, unshared observations, acts without full knowledge
others observe plan do. characteristics led development rich body
research decentralized decision-making uncertainty.
decentralized partially observable Markov decision process (Dec-POMDP) standard
formulation cooperative decision-making sequential settings without instantaneous, free
noiseless communication (Bernstein, Givan, Immerman, & Zilberstein, 2002). past
decade, extensive research solution methods Dec-POMDPs, using methods
dynamic programming (Hansen, Bernstein, & Zilberstein, 2004; Boularias & Chaib-draa,
2008; Amato, Dibangoye, & Zilberstein, 2009), optimization (Aras & Dutech, 2010; Amato, Bernstein, & Zilberstein, 2010) heuristic search (Szer, Charpillet, & Zilberstein, 2005; Oliehoek,
Spaan, & Vlassis, 2008; Oliehoek, Spaan, Amato, & Whiteson, 2013). approaches directly
search optimal solution space possible solutions (or policies) become intractable
larger problems. unexpected given worst-case NEXP complexity finite-horizon
Dec-POMDPs (Bernstein et al., 2002).
key assumption many Dec-POMDP algorithms planning centralized long
execution remains decentralized (Hansen et al., 2004; Boularias & Chaib-draa, 2008; Amato
et al., 2009; Szer et al., 2005; Oliehoek et al., 2008, 2013). is, methods represent centralized planning decentralized control centralized planner generates tuple individual
solutions, one individual solution agent. use Dec-POMDP model
require centralized planning (e.g., Velagapudi, Varakantham, Sycara, & Scerri, 2011; Wu, Zilberstein, & Chen, 2011; Banerjee, Lyle, Kraemer, & Yellamraju, 2012), Dec-POMDPs
cooperative framework common assume centralized planning possible. Unfortunately,
current algorithms take full advantage centralized planning assumption.
article extends conference paper published IJCAI13 (Dibangoye, Amato, Buffet, &
Charpillet, 2013), includes introduction centralized solution method recasts
Dec-POMDP continuous-state MDP detailed background, new theorems proofs,
well concise representations policies value functions. novel method also
able produce decentralized solutions, leverages work centralized planning methods significantly increase scalability. Furthermore, show optimal value function aforementioned MDP piecewise-linear convex function. form, theory POMDPs
444

fiOptimally Solving Finite-Horizon Dec-POMDPs

(Kaelbling et al., 1998) applies, allowing POMDP algorithms produce optimal solutions DecPOMDPs. wide range POMDP algorithms, demonstrated significant scalability
(Shani et al., 2013), applied. extend one heuristic search algorithm (Smith &
Simmons, 2004) Dec-POMDP case, number states actions MDP
grows exponentially planning horizon, scalability remains limited.
increase scalability, introduce novel mechanism refines centralized solution
methodology present ways combine classical heuristic search compact representations,
without losing ability converge optimal solution. incorporate compact representations, build feature-based dynamic programming (Tsitsiklis & van Roy, 1996), includes
feature extraction value prediction approximation methods. introduce feature-based
heuristic search value iteration (FB-HSVI) algorithm relies compact representations, pointbased updates efficient action selection. theoretical analysis demonstrates FB-HSVI terminates finite time optimal solution. combination POMDP theory compact
representations greatly reduce problem size solution efficiency retaining optimal
solutions Dec-POMDPs.

Action2

Observation2

Observation2

oa,z ( s)

start

Action2

Observation2

oa,z ( s)

State

ra (s)

oa,z ( s)

State

pa (s, s)

ra (s)

Agent 2s World

State

pa (s, s)

Hidden
oa,z ( s)

oa,z ( s)

Observation1

Time

oa,z ( s)

Observation1

Agent 1s World

Observation1

Action1

Action1



t+1

t+2

Figure 1: graphical model two-agent Dec-POMDP model.

445

fiDibangoye, Amato, Buffet & Charpillet

2. Background Dec-POMDPs
section introduces basic components decentralized partially observable Markov decision
processes (Dec-POMDPs).
2.1 Problem Definition Notations
Consider multiple agents faced task influencing stochastic system evolves
time (see two agent case Figure 1). every time step, agent receives private
observation gives (possibly) incomplete noisy information current state
system. Since states observable, agent cannot choose actions based states.
Instead, consider complete history past actions observations choose action.
Actions produce common immediate reward, system evolves new state next
time step according probability distribution conditioned actions. next time step,
agents face similar problem again, system may different state. goal
agents choose actions based local action-observation sequences cause
system perform optimally respect shared performance criterion (which discuss
below). Dec-POMDP model formalizes interactions agents system.
paper formulates solves general decentralized stochastic control problem process
operates finite planning horizon.
Definition 1. Dec-POMDP represented tuple (I, , A, Z, p, o, r, b0 , ), where:
finite set agents {1, 2, . . . , |I|};
finite set n states;
Ai finite set agent actions; Ai finite set joint actions;
Z finite set agent observations; Z Z finite set joint observations;
p = {pa | A} denotes transition model. pa n n stochastic matrix, pa (s, s)
probability transitioning state agents choose joint action state s;
= {oa,z | A, z Z} observation model. oa,z n 1 vector1 , oa,z ( s)
probability observing z joint action performed resulting state s;
r = {ra | A} reward function; ra 1 n reward vector, ra (s) bounded
reward obtained executing joint action state s;
b0 initial probability distribution states;
number decision steps {0, 1, . . . , 1} (the problem horizon).
def

Remark 1. often use shorthand notation pa,z (s, s) = oa,z ( s)pa (s, s), s, , A,
z Z, combining transition observation models. is, probability transitioning
state observing z joint action performed resulting state s.
1. observation vector stochastic vector.

446

fiOptimally Solving Finite-Horizon Dec-POMDPs

make representation concrete, discuss simple Dec-POMDP, namely
multi-agent tiger problem (Nair, Tambe, Yokoo, Pynadath, & Marsella, 2003). revisit
problem later sections clarify ideas presented paper.
Example 1 (Multi-agent tiger problem description). multi-agent tiger problem, two agents
stand front two closed doors. Behind one doors hungry tiger, behind
door valuable treasure. agents know position either. listening,
rather simply opening one doors, agents gain information position
tiger. listening cost entirely accurate (i.e., reveals correct
information location tiger probability). Moreover, agents cannot communicate observations other. step, agent independently either listen
open one doors. one agents opens door treasure behind (while
agent listens also opens correct door), get reward. either agent opens
door tiger, large penalty incurred. However, open tiger door
time, receive smaller penalty. agents must make decisions listening
opening doors based local observations. door opened agents receive
reward penalty, problem starts tiger randomly repositioned.
refer state multi-agent tiger world tiger left stl (tiger left)
right str (tiger right). actions agent aol (open left), aor
(open right), al (listen). two possible observations agent (even
opening door): hear tiger left zhl (hear left) hear tiger right zhr (hear
right). reward function defined shown Table 1.
actions
agents
listens
opens good door
opens bad door

listens
2
+9
101

opens
good door
+9
+20
100

opens
bad door
101
100
50

Table 1: Reward function definition multi-agent tiger problem
transition observation models described detail follows. joint action
(al , al ) change state world. joint action causes transition state
stl probability 0.5 state str probability 0.5 essentially resetting problem.
world state stl , joint action (al , al ) results observation zhl either agent
probability 0.85 observation zhr probability 0.15; conversely state str . matter
state world in, joint actions result either observation probability 0.5.
example illustrates small Dec-POMDP. Even small Dec-POMDP, coordination
difficult due uncertainty location tiger actions agent.
2.2 Preliminaries
Given -step Dec-POMDP M, would like agents act way maximize
common measure long-term return M. challenge Dec-POMDPs agents
strategy typically must take agents strategies account. end, discuss agent
447

fiDibangoye, Amato, Buffet & Charpillet

team decision rules policies allow agents act based local information,
attempting maximize joint objective.
2.2.1 Private Decision Rules Policies
every time step, agent chooses action executed based actions agent
previously executed observations received. called policy. better
understand concept, introduce notions private histories decision rules.
Definition 2. step-t private history agent length-t sequence actions observadef
tions, ti = (ai0 , zi1 , . . . , zit1 , ait1 , zit ), ait zit denote actions observations agent
time step {0, 1, . . . , 1}.
, ai , zi ) initial private
step-t private history ti follows recursion ti = (t1
t1


history 0 empty. Let set step-t private histories agent I, namely step-t
private history set. private policy specifies private decision rules agent use time steps,
one private decision rule time step.

Definition 3. step-t private decision rule dti : 7 Ai prescribes agent private action
executed private history ti specified time step {0, 1, . . . , 1}.
denote Dit set step-t private decision rules agent time step
{0, 1, . . . , 1}, namely step-t private decision rule set agent I. Decision rules may
randomized deterministic. Dec-POMDPs, MDPs, always exists deterministic
decision rule good randomized decision rule (Oliehoek et al., 2008; Puterman,
1994). reason, focus deterministic (private) decision rules. Hence, private policies
provide agent mapping action selection possible private history.
def

) sequence private decision
Definition 4. (t + 1)-step private policy it:t+t = (dti , . . . , dt+t

rules agent time step time step + , {0, 1, . . . , 1} N.

Example 2 (Multi-agent tiger private decision rule policy descriptions). Figure 2 shows
pair 2-step private policies 10:1 20:1 trees, agent 1 2, respectively.
Agent 1

Agent 2




agent 2s decision rule d02 depends upon 02
aol

al

zhl

zhr

zhl

zhr
agent 2s decision rule d12 depends upon 12

al

aol

aor

aor

Figure 2: pair private policies form trees decomposed decision rules two
steps multi-agent tiger problem.

448

fiOptimally Solving Finite-Horizon Dec-POMDPs

agent 2 example, step-2 private policy 20:1 consists two private decision rules d02
time steps = 0 = 1, respectively. step-0 private decision rule d02 maps empty
private history 02 = private action al . addition, step-1 private decision rule d12 maps
private histories (aol , zhl ) (aol , zhr ) private action aor cases. worth noticing
private decision rules maintain private histories reachable past actions executed
observations received.

d12 ,

far, focused information agent execution phase including: private
histories, decision rules policies. Nevertheless, goal Dec-POMDP planning find
separable joint policy. reason, next discuss joint histories, decision rules
policies.
2.2.2 Separable Joint Decision Rules Policies
section, extend private information available given agent, e.g., private histories,
decision rules policies, joint information consists collection private data. Let
joint histories , separable joint decision rules dt separable joint policies t:t |I|-tuples
private histories (t1 , t2 , . . . , t|I| ), decision rules (dt1 , dt2 , . . . , dt|I| ) policies (1t:t , 2t:t , . . . , |I|
t:t ),

respectively time steps {0, 1, . . . , 1} N. Note concepts
separable sense expressed |I|-tuple using private information, one private
concept agent.
Example 3 (Multi-agent tiger separable joint decision rule joint policy descriptions). Figure
3 depicts 2-step separable joint policy 0:1 (10:1 , 20:1 ) tuple private policies, one
agent {1, 2}. group together private decision rules agents given time step,
separable joint decision rule. example, initial time step = 0, tuple private
decision rules d0 (d01 , d02 ) separable joint decision rule. addition, separable joint decision
rule d0 prescribes agents 1 2 actions al aol , respectively.
Agent 1

Agent 2




separable joint decision rule d0 (d01 , d02 ) depends 0 (01 , 02 )
aol

al

zhl

zhr

zhl

zhr
separable joint decision rule d1 (d11 , d12 ) depends 1 (11 , 12 )

al

aol

aor

aor

Figure 3: 2-step separable joint policy multi-agent tiger problem.
2.3 Acting Optimally
section, discuss criterion used throughout paper compute optimal separable
joint policy starting initial belief-state. proceeding further, first cast DecPOMDPs MDP, namely information-state MDP.
449

fiDibangoye, Amato, Buffet & Charpillet

2.3.1 Information-state Markov Decision Processes
mentioned above, common assumption solving Dec-POMDPs planning takes place
centralized (offline) manner even though agents execute actions decentralized fashion (online).
planning paradigm, centralized algorithm maintains, step, total available
information process controlled. centralized algorithm essentially performs
policy search space separable joint policies. Thus, separable joint decision rule choices
based exhaustive information available centralized algorithm statistics
derived information. illustrated influence diagram Figure 4,
separable joint decision rule time step depends previous separable joint decision rules
initial belief state, hidden states. statistics summarizing exhaustive information
available centralized algorithm called information states (Hauskrecht, 2000). defined
below, complete information states represent trivial case, i.e., exhaustive data.
start

0



t+1

d0

dt

dt+1

r0

rt

rt+1

Figure 4: Influence diagram information-state MDP. Information states (t t+1 ) represented cycles. Joint-decision-rule choices (dt dt+1 ) represented rectangles,
depend current information state, underlying hidden states. Diamonds represent expected immediate rewards r0 , . . . , rt rt+1 . Dashed lines represent
indirect influence time.
def

Definition 5. Dec-POMDPs, step-t complete information state Ct = (b0 , 0:t1 ) length-t
sequence separable joint decision rules starting initial belief state b0 , time steps
{0, 1, . . . , 1}. satisfies recursion: C0 = (b0 ), Ct+1 = (Ct , dt ).
Example 4 (Multi-agent tiger complete information state description). Figure 5 depicts step-2
complete information state sequence separable joint decision rules 2 (b0 , d0 , d1 ) starting
initial belief state b0 . Alternatively, complete information state consists separable joint
policy represented separable joint policy tree together initial distribution b0 .
worth noting that, complete information state, private history agent occurs
one joint history. interdependence joint histories makes Dec-POMDPs
significantly different centralized problems (e.g., MDPs POMDPs) since policies must
remain decentralized. interdependence also explains joint histories, sufficient
optimally planning MDPs POMDPs, longer sufficient optimally planning
Dec-POMDPs. Instead, rely complete information states.

450

fiOptimally Solving Finite-Horizon Dec-POMDPs

b0

separable joint decision rule d0

al , aol

zhl , zhl

zhr , zhr

zhr , zhl

zhl , zhr
separable joint decision rule d1

al , aor

aol , aor

aol , aor

al , aor

Figure 5: step-2 complete information state C2 (b0 , d0 , d1 ) multi-agent tiger problem.
Separable joint decision rules groups private decision rules depicted Figure 3.

need retain complete information states; instead, one rely compact information states. Recall information states quantities summarizing complete
information states. collection random variables {t : {0, 1, . . . , }} taking values
information state space defines information-state Markov decision process (-MDP).
MDP, states information states actions separable joint decision rules illustrated
influence diagram Figure 4. -MDP deterministic since next-step information
state t+1 deterministic function previous information state joint-decision-rule
choice dt i.e., t+1 = P(t , dt ). Furthermore, taking separable joint decision rule dt
information state , expected reward R(t , dt ).
Definition 6. -MDP (S, A, P, R, 0 , ) w.r.t. Dec-POMDP given by:
information-state set, defines set information states , every time
step {0, 1, . . . , 1};
set separable joint decision rules, defines set separable joint
decision rules dt , every time step {0, 1, . . . , 1};
P specifies next-step information state t+1 = P(t , dt ) taking separable joint decision
rule dt information state , every time step {0, 1, . . . , 1};
P
R specifies immediate expected reward R(t , dt ) = s, Pr(s, |t ) rdt () (s) gained
executing separable joint decision rule dt information state , every time step
{0, 1, . . . , 1};
0 initial information state; problems temporal horizon.

451

fiDibangoye, Amato, Buffet & Charpillet

information-state MDP differs original Dec-POMDP state
space implicit. is, information-state space much large generated
stored memory. Instead, information states generated explored state
space search, typically discarded thereafter. Generating separable joint decision rules
corresponding expected rewards also sources complexity current methods discussed later. methods build upon assumption one always convert original
Dec-POMDP information-state MDP using complete information states without losing
optimality (Szer et al., 2005; Oliehoek et al., 2008, 2013). Below, provide formal proof
property sake completeness.
Lemma 1. optimal separable joint policy complete-information-state MDP

optimal separable joint policy original Dec-POMDP M.
Proof. demonstrating proof, need show optimal joint policies
separable joint policies expected value. Throughout proof, use notations
denote -steps separable joint policies Pb0 , () denote joint probability distribution
parameter b0 . proof starts definition optimal separable joint policy
information-state MDP M:
def

= arg max

1
X

R(Ct , dt ).

t=0

Next, replace R(Ct , dt ) immediate expectation rewards received taking joint (separable) decision rule dt complete information state Ct :
= arg max


1
X
t=0

n

E(st ,t )Pb0 ,0:t () rdt (t ) (st ) ,

(Def. R(Ct , dt )).

following holds sum expectations equal expectation sums:
1



X



def

= arg max E(s0 ,a0 ,...,sT 1 ,aT 1 )Pb0 , ()
r
(s
)
= .






t=0

Hence, sufficient search optimal separable joint policy using find optimal
separable joint policy (and vice versa).

lemma allows us interchangeably use either complete-information-state MDPs
original Dec-POMDP counterpart loss optimality.
2.3.2 Optimality Criterion
paper, consider finite-horizon Dec-POMDP (and therefore finite-horizon -MDP
M), optimality criterion find separable joint policy maximizes expected sum
rewards planning horizon starting given belief state. find optimal separable
joint policy, first characterize expected value gained executing arbitrary separable joint policy t:T 1 starting arbitrary step-t information state. characterization
represents Dec-POMDP value function using -MDP notation.
452

fiOptimally Solving Finite-Horizon Dec-POMDPs

Definition 7. Let t:T 1 separable joint policy respect M. value function V M,t:T 1
denotes expected cumulative reward obtained team agents executes t:T 1 time
def PT t1
R(t+k , dt+k ),
step onward. arbitrary information state , V M,t:T 1 (t ) = k=0
t+k+1 = (t , dt , . . . , dt+k ), {0, 1, . . . , 1} k {0, 1, . . . , 1}.
Example 5 (Multi-agent tiger expected values given separable joint policy information
state). Figure 6 depicts mapping step-1 private histories private policies 11:T 1 21:T 1
agents 1 2, respectively. private histories result agents taking one action
receiving observation, i.e., agent 1 took action al received either observation zhl zhr .
mapping ensures decentralized control since private histories map private policies. example,
agent 1s private history (al , zhl ) maps private policy x. However, expected value one
agents private policy depends agents private policies. reason, rely
joint histories induced information state 1 illustrated Figure 7. Figure 7 depicts mapping
joint histories future separable joint policies. joint history pair private histories
Figure 6, one agent. example, joint history {(al , aol ), (zhl , zhl )} maps future
separable joint policy (x, ) separable joint policy tree. contribution separable joint
policy (x, ) expected value depends probability joint history {(al , aol ), (zhl , zhl )}
initial belief.




al

aol
private histories generated
d01 d02

zhl

zhr

zhl

zhr

?

?

?

?

x

x





future separable joint policy
1:T 1 = (11:T 1 , 21:T 1 )

Figure 6: Mappings private histories future private policies agent.
Value function V M,t:T 1 satisfies following recursion:V ,t:T 1 (t ) = R(t , dt )+V M,t+1:T 1 (P(t , dt ))
V M,t+1:T 1 (P(t , dt )) describes future value executing separable joint policy t+1:T 1
time step + 1 onward starting information state t+1 = P(t , dt ).
2.3.3 Bellmans Optimality Equations
standard definitions optimality equations -step -MDP follow. first describe
optimal value given information state highest value separable joint policy
information state. Let t:T 1 set separable joint policies respect M.
( ) information state V ( ) def
{0, 1, . . . , 1}, optimal step-t value function V M,t
=


M,t
453

fiDibangoye, Amato, Buffet & Charpillet

b0


al , aol
joint histories
induced 1 = (b0 , d0 )

zhl , zhl

zhr , zhr

zhr , zhl

zhl , zhr

?, ?

?, ?

?, ?

?, ?

x

x

x

x

future separable joint
policy 1:T 1

Figure 7: Mappings joint histories future separable joint policies.

0
d0



1

|A0 |

d0

1

|A1 |
2



2



2



2

|A2 |






Figure 8: information-state search tree search nodes information states arcs
search tree labeled separable joint decision rules.

454

fiOptimally Solving Finite-Horizon Dec-POMDPs

maxt:T 1 V M, (t ). optimality equations (or Bellmans optimality equations, see Bellman,
1957; Puterman, 1994) are:


def


(P(t , dt )) ,
St , {0, 1, . . . , 1},
(1)
V M,t
(t ) = max R(t , dt ) + V M,t+1
dt

def

() = 0 time step = .
added boundary condition V M,T
Note optimal step-t value function written terms optimal step-(t + 1) value
function. recursion implies efficient procedure computing step-t value functions
discuss below. Moreover, optimal separable joint policy directly extracted
)
optimal value functions. Suppose (V M,t
t{0,1,...,T 1} solutions optimality equations (1)
subject boundary condition, clear optimal separable joint policy 0:T 1 =
(dt )t{0,1,...,T 1} satisfies:



(P(t , dt )) ,
{0, 1, . . . , 1}.
(2)
dt arg maxdt R(t , dt ) + V M,t+1

property implies optimal separable joint policy found first solving optimality equations, time step choosing separable joint decision rule attains
maximum right hand side (2) {0, 1, . . . , 1}.
2.4 Optimally Solving Dec-POMDPs
provide overview dynamic programming heuristic search principles exact methods
build upon. thorough introduction solution methods Dec-POMDPs, reader refer
surveys (e.g., Oliehoek, 2012; Amato, Chowdhary, Geramifard, Ure, & Kochenderfer, 2013).
Notice, however, dynamic programming methods explicitly consider information
states (instead considering value functions underlying system states). construct separable joint policies last step horizon first evaluating possible separable
joint policies step pruning provably lower value full state
space (Hansen et al., 2004; Boularias & Chaib-draa, 2008; Amato et al., 2009). Heuristic search
techniques implicitly use complete information states developing Dec-POMDP solution methods
(Szer et al., 2005; Oliehoek et al., 2008; Oliehoek, Whiteson, & Spaan, 2009; Spaan, Oliehoek, &
Amato, 2011), explicitly use C -MDP representation.
2.4.1 Dynamic Programming Methods
One class Dec-POMDP solution methods based dynamic programming (Howard, 1960).
Here, set -step separable joint policies generated bottom (Hansen et al., 2004).
step, step-t separable joint policies generated build separable joint policies
step t+1. separable joint policy lower value separable joint policy
states possible separable joint policies agents pruned (with linear
programming). generation pruning steps continue desired horizon reached
separable joint policy highest value initial state chosen. Given number
separable joint policies grows doubly exponentially every generation step, importance
pruning away unnecessary separable joint policies crucial. efficient dynamic programming
methods developed, reducing number separable joint policies generated (Amato
et al., 2009) compressing separable joint policy representations (Boularias & Chaib-draa, 2008).
455

fiDibangoye, Amato, Buffet & Charpillet

2.4.2 Heuristic Search Methods
Another class Dec-POMDP solution methods based heuristic search techniques. Unlike dynamic programming methods, heuristic search algorithms take advantage initial complete
information state. Separable joint policies built top using centralized heuristic
search methods search tree shown Figure 8 (Szer et al., 2005).
case, search node complete information state given horizon, t. complete
information states evaluated horizon heuristic value added.
resulting heuristic values over-estimates true value, allowing A*-style search
space possible complete information states, expanding promising search nodes horizon
+ 1 horizon t. principle, A*-style search methods find optimal separable
joint policy, practice doubly exponential growth search tree makes difficult. Recent
work included clustering probabilistically equivalent complete information states (Boularias &
Chaib-draa, 2008) histories (Oliehoek et al., 2009), incrementally expanding nodes
search tree (Spaan et al., 2011; Oliehoek et al., 2013), greatly improving scalability original
algorithms.
2.4.3 Limitations Current Methods
current methods attempt reduce number separable joint policies information
states considered, rely explicit representations consider possible joint histories (even
though many may unreachable). Moreover, existing techniques fail generalize value
functions one information state information states, slows convergence
optimal joint policy. Finally, even though solution methods use offline centralized
planning phase, concise representation information state identified (until now)
allows greater scalability. Simultaneous work one exception developed concise representations based observation histories, show value function resulting
MDP piecewise linear convex (Oliehoek, 2013). able show piecewise linear
convex property develop novel algorithm exploit resulting structure. so,
draw inspiration advances MDP POMDP algorithms discussed below.
Significant progress made solving large MDPs POMDPs. One reason
progress MDPs use approximate dynamic programming function approximation (Tsitsiklis & van Roy, 1996; De Farias & Van Roy, 2003; Powell, 2007) represent
state system value functions concisely. POMDPs, efficient algorithms
developed recasting problems belief MDPs utilize probability distributions states
system, namely belief states (Smallwood & Sondik, 1973). belief MDP continuousstate MDP piecewise linear convex value function, allowing algorithms scale large
problems sometimes retaining performance bounds (Smith & Simmons, 2004; Shani et al.,
2013). take advantage advances recasting Dec-POMDP continuous-state
MDP piecewise linear convex optimal value function. resulting formulation opens
door direct application POMDP methods opens research directions utilizing DecPOMDP structure centralized planning representations. discuss formulation
progress using structure remaining sections.

456

fiOptimally Solving Finite-Horizon Dec-POMDPs

3. Solving Dec-POMDPs Continuous-State MDPs
contribution section threefold. Section 3.1 introduces statistic (i.e., occupancy
state) summarizes information state. Section 3.1.4 demonstrates occupancy states
sufficient optimally solving Dec-POMDPs, i.e., occupancy states sufficient statistics. Occupancy states allow transforming information-state MDPs occupancy-state MDPs. Section 3.1.6 establishes fundamental property resulting MDP, namely piecewise linearity convexity optimal value function occupancy states. Remember
methods assume centralized offline planning actions separable joint decision rules ensure decentralized execution. contributions enable application vast collection
MDP POMDP solution methods Dec-POMDP problems. Finally, Section 3.2 introduces
occupancy-based heuristic search value iteration (OHSVI) algorithm solving occupancy-state
MDPs builds upon HSVI algorithm POMDPs (Smith, 2007).
3.1 Summarizing Complete Information States
providing formal definition occupancy states, start brief motivation. Then,
demonstrate occupancy state induces deterministic process Markov, namely
occupancy-state Markov decision process. Finally, prove occupancy state sufficient
statistic optimal decision-making Dec-POMDPs.
3.1.1 Occupancy State
discussed Section 2.4.2, standard heuristic search methods solving Dec-POMDPs rely
complete information states (Szer et al., 2005; Oliehoek et al., 2008, 2009; Spaan et al., 2011).
complete information states preserve ability find optimal separable joint policy (see
Lemma 1), heuristic search methods using solve small toy problems. One reason
poor behavior complete information states result redundant useless computations.
particular, every time estimate immediate rewards R(C , d) entire multivariate probability distribution Pr(s, |C ) needs computed states joint histories (see Definition
6). operation time-consuming involves exponentially many joint histories, including unreachable ones. Since operation occurs every time step, important reduce
time required. end, introduce statistic called occupancy state maintain
place complete information state.
Definition 8. step-t occupancy state, denoted , defined posterior probability distribudef
tion state st joint history given complete information state Ct , i.e., (st , ) = Pr(st , |Ct ),
{0, 1, . . . , }. denote step-t occupancy simplex, is, set possible step-t
occupancy states.
Example 6 (Multi-agent tiger complete information states occupancy states). Figure 9
depicts complete information state (left-hand side) corresponding occupancy state (righthand side) joint histories states system. illustrate occupancy state tree,
branches joint histories complete information state leaves state-probability
pairs. Note initial belief assumed state types.
occupancy state represents predictive model state system may end
joint history agents may experience given complete information state. such, occupancy
457

fiDibangoye, Amato, Buffet & Charpillet

Complete Information State C1

zhl

Corresponding Occupancy State 1







al

aol

al , aol

zhr

zhl

zhr

zhl , zhl
stl

?

?

?

?

str

.125 .125

zhr , zhr
stl

str

.125 .125

zhr , zhl
stl

str

.125 .125

zhl , zhr
stl

str

.125 .125

Figure 9: step-1 occupancy state 1 corresponds complete information state 1 .
states, need maintain state joint history pairs reachable given complete
information state.
3.1.2 Markov Property
section proves occupancy states induce process Markov. words, future
occupancy states process depend upon present occupancy state next-step
separable joint decision rule.
Theorem 1. Occupancy state t+1 depends current occupancy state separable joint decision rule dt , i.e., arbitrary , A, zt+1 Z ,
X
t+1 ( s, (t , , zt+1 )) = 1{at } (dt (t ))
(s, ) pat ,zt+1 (s, s),
(3)
sS

1{} () indicator function returns 1 actions chosen dt ,
returns 0 otherwise.
Proof. demonstrating theorem also derive procedure updating occupancy states.
Let step-t information state, decompose = (t1 , dt1 ) i.e., information state t1 prior time-step plus known separable joint decision rule dt1 . Definition
8, relate occupancy state information state follows: arbitrary state st
joint history ,
def

(st , ) = Pr(st , |t ).

(4)

substitution = (t1 , dt ) (4) yields
(st , ) = Pr(st , |t1 , dt1 ).

(5)

expansion right-hand side (5) states system end time-step 1
produces
X
(st , ) =
Pr(st1 , st , |t1 , dt1 ).
(6)
st1

458

fiOptimally Solving Finite-Horizon Dec-POMDPs

expansion joint probability (6) product conditional probabilities results
X
(st , ) =
Pr(at1 |t1 , dt1 ) Pr(st , zt |st1 , t1 , t1 , dt1 ) Pr(st1 , t1 |t1 , dt1 ).
(7)
st1

first factor denotes joint action at1 separable joint decision rule dt1 prescribes
t1 . Since assume separable joint decision rules deterministic, Pr(at1 |t1 , dt1 )
{0, 1}. fact, Pr(at1 |t1 , dt1 ) = 1 dt1 (t1 ) = at1 , otherwise Pr(at1 |t1 , dt1 ) = 0. So,
Pr(at1 |t1 , dt1 ) = 1{at1 } (dt1 (t1 )), 1F indicator function.
second factor right-hand side (7) transition probability
X
pat1 ,zt (st1 , st ) Pr(st1 , t1 |t1 , dt1 ).
(8)
(st , ) = 1{at1 } (dt1 (t1 ))
st1

last factor defines prior occupancy state t1 state st1 joint history t1 ,
depend current separable joint decision rule dt1 . Overall (6) becomes
X
(st , ) = 1{at1 } (dt1 (t1 ))
pat1 ,zt (st1 , st ) t1 (st1 , t1 ).
st1

Therefore, calculation occupancy state time-step requires occupancy
state previous time-step 1 current separable joint decision rule.

Equation (3) describes transitions continuous-state MDP states occupancy
states actions separable joint decision rules. process, transitions deterministic
state space continuous. Next, formally define process occupancy states induce.
3.1.3 Occupancy-State Markov Decision Processes
consider MDP described occupancy states; call occupancy-state Markov
decision process.
Definition 9. Let (, A, R, P, b0 , ) occupancy-state Markov decision process
respect Dec-POMDP M, where:
= t{0,1,...,T } occupancy simplex, 0 = b0 initial occupancy state;
= t{0,1,...,T } separable joint decision rule set;
def

R : 7 R reward function: reward (t , dt ) R(t , dt ) =
def

P

s, (s, ) r

dt () (s);

P : 7 transition function: next occupancy state t+1 = P(t , dt ) described
Equation (3) given (t , dt );
b0 initial belief state;
denotes planning horizon.

459

fiDibangoye, Amato, Buffet & Charpillet

Here, states system represent centralized knowledge planner
actions represent separable joint decision rules ensure decentralized execution. (occupancy)
state updated using known transition observation functions Dec-POMDP
given current (occupancy) state chosen separable joint decision rule. rewards
also calculated (as expectation) using known reward model Dec-POMDP. optimal
value functions solutions optimality equations:


def


V M,t
(t ) = max R(t , dt ) + V M,t+1
(P(t , dt )) , {0, 1, , 1},
(9)
dt

def

added boundary condition V () = 0 = .
M,T
)
Notice solution (V M,t
t{0,1,...,T 1} optimality equations Eq. (9) found,
one always retrieve optimal separable joint policy starting initial occupancy state.
achieved iteratively retrieving optimal separable joint decision rules decision steps
{0, 1, . . . , 1}. decision step t, procedure selects current occupancy state
(starting initial occupancy state 0 ). uses max operator retrieve optimal separable
joint decision rule dt current occupancy state :


def

(P(t , dt )) .
(10)
dt = arg maxdt R(t , dt ) + V M,t+1
Thereafter, moves next occupancy state t+1 = P(t , dt ) makes current one.
procedure repeats final decision epoch reached. sequence optimal
separable joint decision rules (d0 , d1 , . . . , dT 1 ) defines optimal separable joint policy
occupancy-state MDP M. Furthermore, Theorem 2 proves optimal separable joint policy
occupancy-state MDP optimal separable joint policy original Dec-POMDP M.
Optimally solving continuous-state MDP, occupancy-state MDP, nontrivial
task. general, exact solution method solving general continuous-state MDPs.
Methods often rely structural assumptions shape optimal value function (Tsitsiklis & van Roy, 1996; De Farias & Van Roy, 2003; Powell, 2007). next demonstrate
useful structure indeed exist occupancy MDPs form optimal value functions
piecewise linear convex functions occupancy states.
3.1.4 Sufficiency Occupancy States
first show occupancy state sufficient statistic optimal decision-making DecPOMDPs. Throughout remainder paper, call statistic sufficient statistic
statistic information state sufficient optimal decision making occupancy-state MDPs.
Theorem 2. Occupancy state = Pr(s, |Ct )sS ,t sufficient statistic complete information state Ct , i.e., sufficient optimally solving occupancy-state MDPs. Furthermore,
optimal joint policy occupancy-state MDP together correct estimation
occupancy states, also optimal information-state MDP (respectively Dec-POMDP M).
Proof. demonstrating sufficiency occupancy state respect corresponding
information state, need demonstrate (a) optimal value function occupancy state
identical corresponding information state (b) future occupancy states depend
upon current occupancy states (and next-step separable joint decision rule). proved (b)
Theorem 1, remains prove statement (a). show induction.
460

fiOptimally Solving Finite-Horizon Dec-POMDPs

sufficiency occupancy state respect corresponding information state trivially holds last time step problem. fact, V (CT ) = V (T ) = 0 arbitrary
M,T

M,T

complete information state CT corresponding occupancy state (since horizon
reached).
assume statement (a) holds time-step + 1, show holds time-step
t. arbitrary step-t information state, Bellmans optimality criterion prescribes following:


V M,t
(Ct ) = max R(t , dt ) + V M,t+1
(Ct+1 ),
dt

(11)



(t+1 ), t+1
(Ct+1 ) = V M,t+1
Ct+1 = (Ct , dt ). induction hypothesis, V M,t+1

corresponds occupancy state associated Ct+1 . Hence, Equation (11) becomes


V M,t
(Ct ) = max R(Ct , dt ) + V M,t+1
(t+1 ).
dt

Moreover, given R(Ct , dt ) =
since = (Pr(s, |Ct )) s, :

P

s,

(12)

rdt () (s) Pr(s, |Ct ) = R(t , dt ), following expression holds



(t+1 ),
(Ct ) = max R(t , dt ) + V M,t+1
V M,t
dt

(13)

( ) = max

ends proof statement (a) time-step t, since V M,t

dt R(t , dt )+V M,t+1 (t+1 ).

consequence, statement (a) holds arbitrary complete information state Ct St
arbitrary time-step {0, 1, . . . , 1}. Combining statements (a) (b), guaranteed
find optimal value function using occupancy states instead information states.
such, optimal joint policy occupancy-state MDP M, together correct estimation occupancy states, also optimal information-state MDP (or original DecPOMDP M). Given optimal value function M, optimal joint policy = (dt )t{0,1,...,T 1}
given by:

dt = arg maxdt R(t , dt ) + V M,t+1
(t+1 ),

(14)

arbitrary information state , {0, 1, . . . , 1}, t+1 correspond Ct

Ct+1 = (Ct , dt ), respectively.
theorem demonstrates optimally solving M, M, guaranteed
find optimal separable joint policy others.
3.1.5 Belief States versus Occupancy States
Note similarity occupancy state Dec-POMDPs belief state
POMDPs. Formally, step-t belief state bt = (P(s|t , b0 )) sS probability distribution
states conditioned step-t history . also sufficient statistic total data available
centralized agent (i.e., action-observation history) algorithm rely find
optimal solution POMDPs. Similarly, occupancy state sufficient statistic total data
available centralized planner (i.e., history separable joint decision rules) algorithm
rely find optimal separable joint policy Dec-POMDPs. However, occupancy
state remains fundamentally different belief state. First, belief state sufficient
461

fiDibangoye, Amato, Buffet & Charpillet

optimal decision making Dec-POMDPs (because geared ensure separability
joint policy). Second, belief state defines time-invariant statistic, i.e., dimension belief
states bounded number states. contrast, dimension occupancy states grows
exponentially horizon. Also, unlike belief state, occupancy state plan
time sufficient statistic used execution time. Instead, agents still condition
actions local action-observation histories Dec-POMDPs. differences make algorithmic
theoretic transfers belief-state MDPs occupancy-state MDPs nontrivial.
3.1.6 Piecewise-Linearity Convexity Property
present one main results paper piecewise-linearity convexity
optimal value function occupancy-state MDP.
discussion, use vector (resp. matrix) representation operators R(, dt ) P(, dt ).
Vector rdt = (rdt () (s))s, denotes immediate reward executing dt starting state
joint history (i.e., R(t , dt ) inner product rdt occupancy state ).
Moreover, operator P(, dt ) transforms step-t occupancy state step-(t + 1) occupancy state,
is, P(, dt ) describes transition matrix pdt P(t , dt ) = pdt every step-t occupancy
state . linear transformations background, following holds.
)
Theorem 3. optimal value functions (V M,t
t{0,1, ,T } (solutions Equations (9)) piecewiselinear convex functions occupancy states. Hence, {0, 1, , 1}, exists
finite set length-n|t | vector values that, arbitrary occupancy state ,
have:

V M,t
(t ) = max ht , i,

(15)



ht , denotes inner product

P P


(s, )

(s, ).

Proof. show (15) holds induction. Since V (T ) = 0 (since horizon
M,T
( ) = max
reached), V M,T

hT , i, () = 0 = {T }.
( ) =
Hence, property holds k = . Assume property holds k + 1, is, V M,k
k
maxk k hk , k k + 1. want prove property k = i.e.,



V M,t
(t ) = max R(t , dt ) + V M,t+1 (P(t , dt )) ,
.
dt

Using linear transformations rdt pdt , following holds:


def

V M,t
(t ) = max R(t , dt ) + V M,t+1 (P(t , dt )) ,
dt
!
dt
= max ht , r + max hP(t , dt ), t+1 ,
t+1 t+1
dt
E


= max max ht , rdt + pdt , t+1 ,
dt t+1 t+1


= max max ht , rdt + t+1 (pdt ) .

(Inductive Hypothesis)
(Rearranging Terms)

dt t+1 t+1

def

Finally, let set length-n|t | vectors = rdt + t+1 (pdt ) separable joint
( ) = max
decision rules dt vectors t+1 t+1 , V M,t

ht , i. consequence,
proof holds every time step {0, 1, . . . , 1}.

462

fiOptimally Solving Finite-Horizon Dec-POMDPs

demonstrated information states value functions represented vector
space, occupancy-state space, without loosing optimality. Next, provide approach
extending MDP POMDP solution methods occupancy-state Markov decision processes.
3.2 Heuristic Search Solution Methods
section presents occupancy-based heuristic search value iteration (OHSVI) algorithm
solving occupancy-state MDPs. algorithm extends heuristic search value iteration (HSVI)
algorithm POMDPs (Smith & Simmons, 2004) well heuristic search algorithms
A* (Hart, Nilsson, & Raphael, 1968) LRTA* (Korf, 1990).
3.2.1 Heuristic Search Value Iteration Occupancy-State MDPs
HSVI state-of-the-art algorithm solving POMDPs (Smith & Simmons, 2004). produces
solutions maintaining two-sided bounds optimal value function updating
number sample trajectories. upper bounds, (U M,t )t{0,1, ,T } , represented (belief)
state-value mappings, lower bounds, (L M,t )t{0,1, ,T } , represented vector sets.
trajectory begins initial belief state continues time horizon reached.
trajectory finished, upper lower bounds updated belief state, reverse order
visit. trajectories also interrupted reached belief state upper
lower bounds equal, since reason expand belief state whose optimal value
provably known. Finally, often useful prune lower upper bounds maintain concise
representations, removing either dominated vectors points, respectively (Pineau, Gordon, &
Thrun, 2006; Smith, 2007).
Algorithm 1: OHSVI algorithm
function OHSVI((L M,t )t{0,1, ,T } , (U M,t )t{0,1, ,T } )
Stop(0 ) Explore (0 )
function Stop(t )
return U M,t (t ) = L M,t (t )
function Explore (t )
Stop(t )
dt arg maxdt R(t , dt ) + U M,t+1 (P(t , dt ))
Explore(P(t , dt ))
update U M,t L M,t

OHSVI (outlined Algorithm 1) operates similar manner described above, remains
fundamentally different HSVI. HSVI generates trajectories belief states OHSVI generates trajectories occupancy states. Also, HSVI generates trajectories (i) picking greedy
action respect upper bound (optimistic exploration), (ii) performing transition
corresponding largest gap error. Due deterministic nature occupancy-state MDPs,
OHSVI need HSVIs gap-based heuristic guide state exploration (Smith, 2007).
Instead, OHSVI always executes greedy separable joint decision rule respect upper
bounds, selects next occupancy state based greedy separable joint decision rule.
463

fiDibangoye, Amato, Buffet & Charpillet

OHSVI also thought extension learning real-time A* (LRTA*) (Korf, 1990)
takes advantage piecewise-linearity convexity optimal value function.
V (t ) 0.2 v1t + 0.8 v2t

0t

2t

1t

(b) upper bound U M,t

(t1 , v1 )

(t2 , v2 )

M,t

M,t

V (t ) ht , kt

(a) lower bound L M,t



= 0.2 t1 + 0.8 t2

Figure 10: (a) Lower bounds represented using sets vectors, vectors dashed lines,
solid lines represent upper surface vectors (lower-bound value function),
circle projection target occupancy onto lower-bound value function.
(b) Upper bounds represented using occupancy-value mappings, dashed lines
denotes convex hull formed points.

OHSVI relies standard approaches represent lower upper bounds piece-wise linear
convex value functions: vector sets occupancy-value mappings, detail next
section.
3.2.2 Vector Sets : Lower Bounds
HSVI algorithms (and depicted Figure 10(a)), lower bound L M,t represented finite collection n|t |-dimensional vectors, every time-step {0, 1, . . . , }
(Smith, 2007; Kaelbling et al., 1998; Hauskrecht, 2000; Smallwood & Sondik, 1973). Lower
bounds iteratively updated using point-based backup steps follows: ,
[
=
{backup(t+1 , )} ,
(16)
backup(t+1 , ) = arg maxgt : dt ht , gdtt i,

(17)

dt



gdtt = rdt + arg maxgt+1 :
dt



t+1 t+1

ht , gdtt+1 i,



gdtt+1 = t+1 (pdt ) ,

(sum vectors Rn|t | )

(18)

(projection Rn|t+1 | 7 Rn|t | )

(19)

vector set prior backup vector set backup. Lower bounds
(L M,t )t{0,1,...,T 1} initializes (t )t{0,1,...,T 1} single vector () = min sS ,aA (T t) ra (s),
{0, 1, . . . , }. Notice vector representation suitable lower bounds.
3.2.3 Occupancy-Value Mappings : Upper Bounds
Upper bounds (U M,t )t{0,1,...,T 1} represented using mappings occupancy states reals,
see e.g., Figure 10(b). upper bound convex hull current point set.
464

fiOptimally Solving Finite-Horizon Dec-POMDPs

possible interpolate value occupancy states whose mapping currently maintained
outdated. achieved using linear approximation methods (e.g., Hauskrecht, 2000;
Smith, 2007). family, sawtooth linear interpolation maps every occupancy state
point set upper-bound value
U M,t ( ) = min {v , v | ( 7 v ) },


v = v + (v v ) D(, ),
X
v =
(s, ) v s, .

(20)
(21)
(22)

s,

refer D(, ) = min s, : (s,)>0 (s, )/(s, ) sawtooth measure. update upper
bound U M,t specific occupancy state using sawtooth, need compute new value ,
add occupancy-value mapping U M,t , follows:
[n

=
( 7 v ) ,
(23)
v = max R(, d) + U M,t+1 (P(, d)),


(24)

point set prior update point set update. upper bound
U M,t initializes = { s, 7 vts, | } using optimal value underlying MDP corner
points, every {0, 1, . . . , }.
Clearly, one eventually find optimal solution occupancy-state MDP using OHSVI
full lower upper bounds. However, quickly becomes intractable maintain
bounds full occupancy-state space since number state joint-history pairs grows
horizon increases. highlights necessity compact representations occupancy
states, decision rules vector values.

4. Solving Dec-POMDPs Lossless Compact Occupancy-State MDPs
previous sections show every Dec-POMDP represented occupancy-state MDP
without losing optimality. difficulty using representation common algorithms
solving MDPs piecewise linear convex value functions quickly run time and/or
memory since state action spaces become intractably large real-world problems.
surprising given NEXP-Complete worst-case complexity general Dec-POMDPs,
realistic Dec-POMDP applications often significant structure.
section, discuss optimally solving occupancy-state MDPs potentially reducing
dimensionality occupancy states, decision rules value functions. Subsection 4.1,
reduce dimensionality occupancy states decision rules constructing clusters
equivalent histories. Next, define compact representations occupancy states decision
rules based upon clusters histories rather single histories. resulting compact
MDP may exponentially fewer states actions original model, optimal value
function compact model may longer piecewise linear convex. Subsection 4.2,
overcome limitation, allowing values one compact occupancy state generalize
another one using parametric value functions. Finally, Subsection 4.3 presents feature-based
heuristic search value iteration algorithm theoretical guarantees.

465

fiDibangoye, Amato, Buffet & Charpillet

4.1 Lossless Compact Occupancy-State MDPs
dimension occupancy states decision rules typically grows exponentially horizon. this, often impractical compute store every component occupancy
state decision rule representations. overcome limitation using compact representations occupancy states decision rules based notions equivalence histories.
notions equivalence fundamental designing analyzing algorithms reducing
dimensionality occupancy-state MDP, thereby improving scalability. Equivalence relations permit us aggregate histories convey information process. target
equivalence relations that, upon replacing group aggregated histories one element
group, allow us produce compact representations occupancy states decision rules
still preserving ability find optimal solution.
4.1.1 Probabilistic Equivalence History Space Aggregation
first present equivalence notions build upon defining compact representations
proving preserve optimality. definitions inspired work concise
information states (Boularias & Chaib-draa, 2008; Oliehoek et al., 2013). Here, connect
research occupancy-state MDPs, later provide natural algorithms constructing effectively solving them.
Definition 10. Private histories , agent locally probabilistically equivalent
respect occupancy state denoted -LPE if, if, state


history
agents I\{i}: Pr(s, | , ) = Pr(s, | , ).
worth noticing -LPE used partition private history set agent I,
time steps {0, 1, . . . , }. partition set nonempty subsets (called clusters)
Bi1 , Bi2 , . . . , Bik private histories, Bi1 Bi2 . . . Bik = . distinguish two
sets private histories agent occupancy state . first set, denoted (),
consists private histories non-zero probability respect . second set, denoted
\it (), consists private histories zero probability respect . difference
particularly important, show later. fact, nonzero private histories play part
demonstrating -LPE preserves optimality. addition, useful note that, agent,
-LPE groups together zero private histories w.r.t. cluster.
Given private histories clustered convey information, representations
compact occupancy states, decision rules value functions depend upon
clusters. Unfortunately, maintaining clusters still requires large amount memory, explains impetus labeled clusters. labeled cluster cluster along label; private
histories cluster match corresponding label. Throughout paper, use following
convention: cluster private histories maps minimum private history (of cluster)
using lexicographical ordering. Specifically, label cluster chosen among private histories
cluster, non-zero probability w.r.t. occupancy state. Therefore, label
cluster also private history cluster, leads clear relationship private
history, cluster corresponding label. Thus, representation compact occupancy states,
decision rules value functions depend upon labels instead clusters.
Nonetheless, compact representations make hard generalize value function
one compact occupancy state another. see later, generalization requires ability
466

fiOptimally Solving Finite-Horizon Dec-POMDPs

quickly check whether given private history belongs specified cluster. group histories
using -LPE, checking whether private history belongs cluster Bi whose label another private
history Bi replaced checking whether private histories -LPE,
arbitrary agent I. Unfortunately, checking whether two private histories agent
-LPE requires enumerating states agents nonzero histories w.r.t. , results
subroutine complexity O(n|i
()|) see Algorithm 3 Appendix B. Given call
procedure exponentially many private histories, importance replacing local probabilistic
equivalence cheaper equivalence relation clear. end, introduce truncation probabilistic equivalence w.r.t. (denoted -TPE). providing formal definition -TPE,
start motivation.
many practical situations, important information process controlled lies
history window (of fixed length) actions observations agents experienced.
Based insight, want truncation probabilistic equivalence group private histories
agent share: (i) model states agent histories (i.e., -LPE)
(ii) common suffix fixed length m. setting, private histories
clustered, checking particular private history belongs cluster replaced
checking whether private history label (another private history) cluster
share suffix specified length significantly faster checking whether
two private histories -LPE: O(m) versus O(n|i
()|). is, private histories
clustered, two private histories suffixes length already known LPE though clustering process choice m. turns defining probabilistic
truncation equivalence w.r.t. , need determine suffix length (i.e., history window)
sufficient called local truncation parameter w.r.t. .
local truncation parameter respect occupancy state be: (i) large
enough ensure nonzero private histories w.r.t. share suffix length also
-LPE; (ii) small enough group maximum number private histories. straightforward
method (Algorithm 4 Appendix B) compute starts parameter = 0 proceeds
follows: (step 1) agent, nonzero private histories w.r.t. share common suffix
length also -LPE one another, set = terminate; (step 2) Otherwise,
increment = + 1 go back step 1. algorithm guaranteed terminate
(the current time step) iterations, i.e., = worst case. later case corresponds
boundary case clustering done i.e., cluster corresponds single joint history.
ready formally define truncation probabilistic equivalence relation.
Definition 11. Private histories , agent truncation probabilistically equivalent
respect occupancy state denote -TPE if, if, hold last
private actions observations given found set histories discussed above.
surprisingly, -TPE also partitions private history sets. However, often produces
clusters -LPE since constrains -LPE non-zero private histories w.r.t. (since also
requires suffixes same). contrast -LPE, spreads zero private histories w.r.t.
different clusters.
Recall advantage -TPE -LPE finding appropriate cluster
private history (i.e., checking whether two private histories equivalent) cheaper using -TPE
using -LPE. former requires comparison suffix fixed length, whereas
latter needs comparison large distributions states joint histories.
467

fiDibangoye, Amato, Buffet & Charpillet

worth noticing that, best knowledge, TPE LPE weakest assumptions
date reduce dimensionality full occupancy states. Nevertheless, DecPOMDPs benefit data reduction approaches. precisely, unlikely
assumptions provide concise occupancy states totally unstructured domains. Fortunately,
real-world domains often structured, demonstrated Section 5. Next, address
problem using equivalence relations private histories find compact representations
occupancy states, decision rules real vectors.
4.1.2 Compact States, Actions, Vectors MDPs
define compact representations occupancy states, decision rules real vectors based
upon aforementioned equivalence relations. Notice following definitions depend
either local truncation probabilistic equivalence relations labeled clusters
generate. Intuitively, compact occupancy states distributions states joint labels, compact decision rules mappings labels private actions, compact real vectors mappings pairs states joint labels reals. Notationally, let Li label set agent
occupancy state generates (i.e., set labels generated histories make
), Bi labeled cluster agent label Li let L = iI Li .
Definition 12. compact occupancy state , denoted , distribution states joint
labels: (i )iI L ,
X
X X
def
(s, 1 , 2 , |I| ).
(25)
...
(s, 1 , 2 , . . . , |I| ) =
1 B1 2 B2

|I| B|I|

Example 7 (Multi-agent tiger full occupancy states compact occupancy states). Figure
11 depicts full occupancy state (left-hand side) corresponding compact occupancy state
(right-hand side) joint histories hidden states. obtain compact occupancy state,
show (al , zhl ) (al , zhr ) 1 -LPE agent green, (al , zhl ) (aol , zhr ) 1 -LPE
agent red. consequence, one group histories Bgreen {(al , zhl ), (al , zhr )} agent
green, Bred {(aol , zhl ), (aol , zhr )} agent red, replace cluster single label.
compact decision rule w.r.t. agent I, denoted di : Li 7 Ai , mapping label
set private action set. addition, compact private policy w.r.t. (0 , 1 , . . . , 1 ) agent I,
denoted = (di )t{0,1,...,T 1} , sequence compact decision rules agent I. similar
definition follows compact separable joint policies = (i )iI .
compact real vector w.r.t. , denoted Rn|L | , mapping pairs state
joint label L reals.
Intuitively, distribution reassigns probability mass joint cluster (Bi )iI corresponding joint label (i )iI . Algorithms 3 4 (see Appendix B) present straightforward way
compute compact occupancy state using local truncation probabilistic equivalence relations,
respectively. worth noticing that, given occupancy state , -LPE always produces
number joint labels |L | less equal number labels -TPE produces.
-TPE stricter form local probabilistic equivalence, discussed above. Hence,
-LPE produces compact occupancy states, decision rules real vectors concise
-TPE.
468

fiOptimally Solving Finite-Horizon Dec-POMDPs

Full Occupancy State 1

Compact Occupancy State 1





al , aol

al , aol

zhl , zhl
stl

str

.125 .125

zhr , zhr
stl

str

.125 .125

zhr , zhl
stl

str

.125 .125

zhl , zhr
stl

zhl , zhl

str

stl

.125 .125

str

0.5 0.5

Figure 11: step-1 compact occupancy state 1 corresponds full occupancy state 1 .
demonstrating compact occupancy states sufficient optimal decision making
occupancy-state MDPs, rely notion compact occupancy-state MDPs.
Definition 13. compact occupancy-state MDP w.r.t. occupancy-state MDP given tuple
A, R, P, 0 , ):
(,
= t{0,1,...,T } space compact occupancy states, 0 = 0 initial compact
occupancy state;
= t{0,1,...,T } space compact joint (decentralized) decision rules;
P
R : 7 R compact reward function R(, ) = s, (s, ) rd () (s);

def
P : 7 compact transition function P(, ) = , = P(, );

denotes planning horizon.
Analogous occupancy-state MDPs, compact occupancy states updated using known
transition observation functions Dec-POMDP given current compact occupancy state
chosen compact separable joint decision rule. rewards also calculated (as expectations) using known reward model Dec-POMDP. Finally, optimal value function
solution optimality equations:


def


V M,t
(t ) = max R(t , dt ) + V M,t+1
( P(t , dt )) ,
{0, 1, . . . , 1},
(26)
dt

() = 0.
added boundary condition V M,T

4.1.3 Sufficiency Compact Occupancy States
Below, prove that, planning compact occupancy states instead full occupancy
states, optimal separable joint policy compact model immediately induces corresponding optimal separable joint policy original model. proceeding further, first
demonstrate optimality compact policies.
469

fiDibangoye, Amato, Buffet & Charpillet

Note proof mirrors similar proof showing histories agent clustered
without change optimal policy loss value using probabilistic equivalence
traditional Dec-POMDP representation (Oliehoek et al., 2013). extend ideas
occupancy-state MDP case incorporate truncation probabilistic equivalence show
compact policies preserve optimality.
Theorem 4 (Optimality compact policies). occupancy-state MDPs, exists optimal policies agent depend upon labels agent produced based either local
truncation probabilistic equivalence respect occupancy states optimal policies induce,
private histories.
Proof. construct proof induction. first show last step problem,
agent policy depends labels, private histories.

Given occupancy state 1 1 policies
1 1 agents I\{i}, policy
1 1 agent best response
1 . is, chooses every private history


1 1 private action maximize value based model (Pr(s, Ti1 |Ti 1 , 1 )) s,i
1
occupancy state 1 private history Ti 1 induce possible histories agents
resulting states system:
X


(27)
Pr(s, Ti1 |Ti 1 , 1 ) ra ,T 1 (T 1 ) (s).
1 (Ti 1 ) arg maxai Ai
s,Ti1

assigning private actions private histories given 1 , nonzero private histories w.r.t. 1
affect outcome. is, zero probability private histories w.r.t. 1 prescribed private action without losing optimality (e.g., private action identical associated labels
generated based either local truncation probabilistic equivalence relations). Hence, policy
1 depends zero probability private histories w.r.t. 1 corresponding labels.
Next, restrict attention nonzero probability private histories w.r.t. 1 . Recall private
histories family -TPE one another also -LPE one another. Therefore,
demonstrate property private histories family -LPE one another,
proof follows immediately -TPE.
Assume Ti 1 nonzero probability private history w.r.t. 1 . Ti 1 cluster
1 -LPE private histories label 1 (another nonzero probability private history w.r.t. 1 ),
equality (Pr(s, Ti1 |Ti 1 , 1 ))s,i = (Pr(s, Ti1 |iT 1 , 1 ))s,i holds. consequence,
1
1
policy 1 depends nonzero probability private history w.r.t. 1 corresponding
labels:
X


(28)
Pr(s, Ti1 |iT 1 , 1 ) ra ,T 1(T 1 ) (s).
1 (Ti 1 ) arg maxai Ai
s,Ti1

Therefore, property holds last step problem, arbitrary agent I.
allows us define policies last step mappings labels private actions, i.e., compact
policies.
Next, rely concept private policy tree, tree represents actions
nodes observations edges depth number stages go. root node
determines first private action taken. depending private observation received,
agent executes another private action; leaf node reached. policy tree
470

fiOptimally Solving Finite-Horizon Dec-POMDPs

it:T 1 portion private policy it:T 1 , prescribes private actions taken agent
remaining stages starting given private history ti . abuse notation, let
it:T 1 = it:T 1 (ti ), agents time steps {0, 1, . . . , }.
induction step, show that, agent policy-tree step + 1 onward depends
private histories corresponding labels either equivalence relations,
agents policy tree step onward also depends private histories
corresponding labels either equivalence relations. Given occupancy state policy




t:T 1 t:T 1 agents I\{i}, policy t:T 1 t:T 1 agent follows:
X



it:T 1 (ti ) arg maxi t:T 1
Pr(s, ti |ti , ) t:T 1 ,t:T 1(t ) (s),
(29)
t:T 1

s,ti







t:T 1 ,t:T 1 (t )

associated vector
(it:T 1 ,
t:T 1 (t )) separable joint policy-tree
value. Again, assigning private actions private histories given , nonzero probability
private histories w.r.t. matter. fact, property trivially holds zero probability private
histories w.r.t. . nonzero probability private histories w.r.t. , property holds private
histories belong cluster -TPE private histories since histories would belong
cluster -LPE private histories, previously discussed.
reason, restrict attention nonzero probability private histories ti w.r.t.
belong cluster -TPE private histories label . Then, equality (Pr(s, ti |ti , ))s,ti =
(Pr(s, ti |it , )) s,ti holds. Hence,

it:T 1 (ti ) arg maxi

t:T 1 t:T 1

X







Pr(s, ti |it , ) t:T 1 ,t:T 1(t ) (s).

(30)

s,ti

Consequently, agent policy depends private history step problem
corresponding labels either equivalence relations. demonstrates compact policy
lose information.

Theorem 5. Compact occupancy state based either local truncation probabilistic equivalence relations sufficient statistic occupancy state . Furthermore, optimal separable
joint policy compact occupancy-state MDP M, together correct estimation compact occupancy states, immediately induces optimal separable joint policy occupancy-state
MDP M.
Proof. proof proceeds similarly Theorem 2. is, proving sufficiency
compact occupancy state respect corresponding full occupancy state, need
demonstrate: (a) optimal value function compact occupancy state identical
corresponding occupancy state (b) next-step compact occupancy states depend upon
current compact occupancy states (and next-step compact separable joint decision rules).
stated (b) Definition 13, statement (a) remains proved. show induction.
sufficiency compact occupancy state respect corresponding occupancy
( ) = V ( ) = 0
state trivially holds last step problem. fact, V M,T


M,T

arbitrary occupancy state corresponding compact occupancy state (since horizon
reached). assume statement (a) holds time-step + 1, show

471

fiDibangoye, Amato, Buffet & Charpillet

holds time-step t. arbitrary step-t occupancy state, Bellmans optimality criterion
produces following equality:
def



V M,t
(t ) = max R(t , dt ) + V M,t+1
(t+1 ),

(31)

dt



(t+1 ) = V M,t+1
(t+1 ).
t+1 = P(t , dt ). inductive hypothesis, V M,t+1
results equality:


V M,t
(t ) = max R(t , dt ) + V M,t+1
( P(t , dt )).

(32)

dt

addition, Theorem 4 demonstrated restricting attention compact (joint) decision
rules A, preserve optimality. Thus, obtain:
X
X
R(t , dt ) =
rdt () ( s)
s, (s, ) (s, ),
(Theorem 4)
s,

=

X

s,

r

dt ()

( s) ( s, ),

(Definition )

s,

= R(t , dt ),

(dt () = dt ()).

Hence,


( P(t , dt )),
V M,t
(t ) = max R(t , dt ) + V M,t+1

(33)

dt


(t ).
= V M,t

(34)

Therefore, statement (a) holds arbitrary occupancy state arbitrary time
step {0, 1, . . . , 1}. Combining statements (a) (b), guaranteed find optimal
value function using compact occupancy states instead occupancy states. addition,
def
)
given optimal value function (V M,t
t{0,1,...,T } , optimal compact policy = (dt )t{0,1,...,T 1}
obtained successive one-step lookaheads: {0, 1, . . . , 1},

( P(t , dt )),
dt max R(t , dt ) + V M,t+1

(35)

dt

def

0 = 0 t+1 = P(t , dt ). immediately induces separable joint policy =
(dt )t{0,1,...,T 1} original occupancy-state MDP that, every agent every private history ti , set dti (ti ) = dti (it ), ti cluster label . Since
yield expected value starting initial occupancy state, separable joint policy
optimal original occupancy-state MDP M.

solving compact problem instead original one, circumvent exhaustive enumeration occupancy states decision rules preserve ability find optimal solution
original problem. worth noticing optimal value function compact occupancy space PWLC. compact occupancy states expressed using different
label sets. However, exploiting PWLC property optimal value function full
occupancy-state space, develop methods generalize value one compact occupancy
472

fiOptimally Solving Finite-Horizon Dec-POMDPs

state another. Next, propose method incrementally improving lower upper bounds
narrow range optimal value function. But, contrast OHSVI algorithm,
using compact occupancy states compact real vectors. precisely, compact upper
lower bounds defined full occupancy states, compact representations.
4.2 Feature-Based Compact Bounds
algorithm, upper lower bounds crucial importance. narrow range
optimal value function, determine suboptimal regions search space, speed
convergence towards exact solution. approach approximates full lower- upper-bound
heuristic functions heuristic functions defined full occupancy space (Tsitsiklis &
van Roy, 1996; Hauskrecht, 2000; Roy, Gordon, & Thrun, 2005). new heuristic functions
typically compact (with respect traditional high-dimensional vector point sets discussed
Section 3.2.2 Section 3.2.3), easier compute full bounds. heuristic
functions formulated feature-based compact functions combine dimensionality
reduction (using clustering methods discussed above) function approximation (Tsitsiklis &
van Roy, 1996). Thus, demonstrate new heuristic functions valid bounds,
analyze dimension reduction approximation methods.
4.2.1 Feature-Based Compact States Vectors
One think feature-based compact representations dimension reduction model representing high-dimensional bounds using bounds lower dimensionality. However, approach
requires set basis functions (or feature set), lower-dimensional bounds expressed effectively. basis function (or feature) function maps high-dimensional data
one salient feature problem hand. setting example, feature indicator
function whether private history matches one specified label. Features core
feature-based compact representations full occupancy states real vectors, ultimately
serve represent upper lower bounds using either feature-based compact vector point sets
similar way standard vector point set representations.
Definition 14. feature indicator function s,(i )iI : 7 {0, 1} one specified state
one specified joint label (i )iI L occupancy state induces i.e.,
(i )iI ,
(
1, = and, I, belongs cluster label ,

s,(i )iI ( s, ( )iI ) =
0, otherwise.
def

feature set w.r.t. , denoted , given = { s, |s , iI Li }.
feature set w.r.t. represents partition state joint history space equivalence
relation -LPE -TPE induces. partition set nonempty subsets (B s, ) sS ,L (called
labeled joint clusters) sS ,L B s, = . labeled joint cluster B s, consists
cross-product singleton {s} labeled clusters B1 , B2 , . . . , B|I| equivalence
relation -LPE -TPE generates. Therefore, feature interpreted way check whether
state joint history pair belongs specified labeled joint cluster. Thus, features provide
473

fiDibangoye, Amato, Buffet & Charpillet

alternative (possibly lossy) representation compact occupancy states. One express
compact representation full occupancy state using feature set , follows


X X

=
s, (s, ) (s, )
.
sS ()

s,

Specifically, state labels Li agent I,
def

( s, 1 , 2 , . . . , |I| ) =

X

X

...

1 B1 2 B2

=

X

X

( s, 1 , 2 , |I| ),

|I| B|I|
def

( = (1 , 2 , . . . , |I| ))

( s, ),

( s,)B s,

=

X X

s, (s, ) (s, ),

sS ()

analogous property holds feature-based compact real vectors w.r.t. . is, feature-based
compact real vector | |-dimensional real vector expressed using .
worth noticing standard feature-based approaches assume unique feature set data
expressed based unique feature set, eases bound generalization (Tsitsiklis &
van Roy, 1996). setting, however, different occupancy states yield different (possibly disjoint)
feature sets. Hence, feature-based compact occupancy states (or real vectors) expressed based
different feature sets, making hard transfer value one feature-based compact occupancy
state another one. Bounds generalize naturally among feature-based compact occupancy states
share feature set. remedy this, introduce basis change operations
feature-based compact occupancy states real vectors.
4.2.2 Change Feature Set
enabling bound generalization, necessary work one feature set. Hence,
important able easily transform feature vectors calculated respect one feature set
corresponding (possibly lossy) representations respect another feature set. ease
change feature set, necessary match features original feature set
destination feature set. introduce two heuristic methods match features different
feature sets, thereby allowing change feature sets.
Definition 15. Let feature sets w.r.t. occupancy states , respectively.
projection feature-based compact occupancy state onto feature set , denoted F (),
given probability distribution:


X

def
F () =
,
(36)
s, (s, ) (s, )
s,

s,

F () reassigns probability mass (s, ) pair (s, ), s, , pair
( s, ), s, , match, i.e., s, (s, ) = 1.
474

fiOptimally Solving Finite-Horizon Dec-POMDPs

change feature set describes function original feature set destination feature
set . particular, surjective function (i.e., every feature s, destination set
least one corresponding feature s, original set s, (s, ) = 1).
transformation assigns feature destination set probability mass corresponding
features original set. heuristic method provides guarantee F () share
optimal value. replacing range features original set single feature
destination set, produce compact possibly lossy representations, ultimately precludes
ability preserve value original compact occupancy state. Fortunately, since use
representations provide bounds, even lossy representations generate useful bounds.
Definition 16. Let feature sets w.r.t. occupancy states , respectively.
projection feature-based compact real vector using feature set , denoted G ( ), given
feature vector:



X
def
(37)
s, ( s, ) (s, )
G ( ) =
s,

s,

change feature set applies real vectors, describes function destination
feature set original feature set . Specifically, every pair ( s, ) along feature s,
destination set corresponding pair (s, ) along feature s, original set
i.e., one s, ( s, ) = 1. transformation assigns pair ( s, )
destination set value (s, ) corresponding pair (s, ) original set. Since
pairs original set values represented G ( ), resulting feature vector
lossy representation original vector . loss resulting feature vector depends
original destination feature sets, choice equivalence relation histories.
previously mentioned, make use either -LPE -TPE relations.
Using -LPE, distinguish state joint-history pairs involve non-zero
probability private history sets w.r.t. (i.e., non-zero probability pairs), state joint-history
pairs involve zero probability private history sets w.r.t. (i.e., zero probability pairs).
sake conciseness, compact real vectors maintain values associated non-zero probability
pairs. zero probability pairs default value, example (T t) min sS ,aA ra (s).
Hence, change feature set pairs ( s, ) destination set, corresponding non-zero probability pair (s, ) original set, inherits value (s, ). However,
pairs ( s, ) destination set, corresponding zero probability pair (s, ) original set,
inherits default (and loose) value. number zero probability pairs occupancy
state far larger number non-zero probability pairs, feature vectors result
change basis via -LPE multiple components loose value.
Using -TPE, distinguish state joint-history pairs joint-history
suffixes length see Definition 10. many pairs destination set, would
associated corresponding zero probability pair original set using -LPE. Using -TPE, however, pairs associated non-zero probability pair original set.
Specifically, pair ( s, ) destination set, whose probability zero respect , corresponding zero probability pair original set using -LPE. Yet, pair ( s, ) destination
set non-zero probability pair (s, ) original set share common length history suffix,
( s, ) associated (s, ) using -TPE. Thus, feature vectors result change
475

fiDibangoye, Amato, Buffet & Charpillet

feature set using -TPE involve fewer components default, loose value using
-LPE.
Although heuristic methods change feature set guaranteed produce representations retain information compact counterparts, many cases resulting
(possibly lossy) representation sufficient generalize bounds entire compact occupancy
space. bound property (i.e., ability overestimate underestimate optimal value)
resulting representation determined examining methods change feature set.
following theorem (proved Appendix) establishes F G mappings use
preserve bound property.
Theorem 6. arbitrary feature set , change feature set based mappings G
F preserve bound property i.e., ,
(), V (F ());
a. R, V M,t
M,t


() h, G ()i, V () h, G (G ())i.
b. R| | , V M,t



M,t

Theorem 6 shows bound properties given occupancy state preserved occupancy state obtained upon change feature set using heuristic methods F G. Next,
discuss approximate full upper lower bounds entire occupancy space.
4.2.3 Compact Point-Value Mappings: Upper Bounds
full upper-bound value function approximated finite set points sawtooth
interpolation rule estimates value arbitrary point compact occupancy space
relying points already experienced associated values. key aspect heuristic
approximation sawtooth interpolation rule compact occupancy states.
full upper-bound value function, sawtooth interpolation approximate points
expressed within basis set. Here, demonstrate apply even points
expressed different feature sets means change feature set. Let = {(1 7 v1 ), (2 7
v2 ), . . . , (k 7 vk )} set point-value pairs represents approximate function U defined
compact occupancy space, point satisfies Theorem 6a. approximate
value arbitrary compact occupancy state based point-value pair ( 7 v ) obtained
using sawtooth interpolation way similar calculation full upper-bound
value function:


v = v + (v v ) D(F (), ),
v =

P

s,

(38)

(s, ) v s, . Theorem 6a, know feature vector F () shares

upper bound v . addition, F () expressed using feature set
. Hence sawtooth interpolation apply produce upper-bound value v compact
occupancy state based point-value pair ( 7 v ). optimization respect compact
occupancy state acquired choosing best overall upper-bound value pointvalue pairs :


U ( ) = min {v , v | ( 7 v ) }.
476

(39)

fiOptimally Solving Finite-Horizon Dec-POMDPs

heuristic approximation differs full upper-bound value function requires change feature set F compact occupancy states instead full occupancy states.
Hence, sawtooth interpolation computationally efficient full upperbound value function, compact occupancy states lower dimensionality. accuracy resulting upper-bound values, clear sawtooth interpolation compares
full upper-bound value function (i.e., whether sawtooth interpolation
weakens upper bounds). Yet, collection point-value pairs obtained selection compact occupancy states combined define approximate function upper-bound value
function discussed next.
feature-based compact value function (U M,t )t{0,1,...,T } compact occupancy space
represented using sets (t )t{0,1,...,T } point-value pairs estimate values arbitrary
compact occupancy state. Initially, set contains |S | point-value pairs { s, 7 vts, | }
represent step-t optimal value function underlying MDP. sawtooth interpolation
estimates U M,t compact occupancy state follows:


U M,t ( ) = min {v , v | ( 7 v ) },

{0, 1, . . . , }.

Set updated every compact occupancy state , using point-based backup step
follows:
[n

=
( 7 v ) ,
{0, 1, . . . , }


v = maxd R( , ) + U M,t+1 ( P( , )). Approximate function (U M,t )t{0,1,...,T }
upper-bound value function similarly full upper-bound value function stated
proven Appendix A.
Theorem 7. Feature-based compact value function (U M,t )t{0,1,...,T } , iteratively updated, upper
bounds optimal value function entire compact occupancy space.
4.2.4 Compact Vector Sets: Lower Bounds
full lower bound full occupancy space approximated finite set compact
real vectors along associated feature sets linear function updates. take inspiration
initialization, evaluation update routines full lower-bound value function. One
]
important aspect approximation lies definition update operation, denoted backup.
Let set compact real vectors represents approximate value function,
compact real vector satisfies Theorem 6b. Then, new compact real vector compact
occupancy state compact decision rule computed efficiently full lowerbound value function (Section 3.2.2):



gd = rd + arg maxg : , gd ,
(40)






gd = G P(,d ) () (pd ) expression projection G P(,d ) () onto feature set



, G P(,d ) () expression feature set P(,d ) . optimization respect


compact occupancy state acquired choosing compact vector best overall
477

fiDibangoye, Amato, Buffet & Charpillet

value vectors gd :


] , ) = arg max
backup(
g



:




, gd .

(41)



principal difference respect full lower-bound value function lies use transformation G compact real vectors instead high-dimensional real vectors. Hence, backup
operation efficient full lower-bound value function, since operations
use lower dimensional vectors, likely save significant time.
change feature set may produce weaker bounds. Nonetheless, collection compact vectors
obtained selection compact occupancy states combined define approximate
function lower-bound value function discussed next.
feature-based compact value function (L M,t )t{0,1,...,T 1} compact occupancy space
represented using sets (t )t{0,1,...,T 1} compact real vectors along associated feature
sets estimate values arbitrary compact occupancy state. Initially, set contains
single compact real vector given
() = (T t) min ra (s),

{0, 1, . . . , }.

sS ,aA

(42)

max-vector rule estimates L M,t compact occupancy follows:
L M,t (t ) =

max
(7t )t

ht , Gt (t )i,

{0, 1, . . . , }.

(43)

Set updated every compact occupancy state , using point-based backup steps
follows:
[n

] t+1 , )) .
=
(t 7 backup(
(44)
Approximate function (L M,t )t{0,1,...,T 1} lower-bound value function since main difference
respect full lower-bound value function lies use G , preserves bound
property. complete proof, reader refer Appendix A.
Theorem 8. Feature-based compact value function (L M,t )t{0,1,...,T } , lower bounds optimal value
function entire compact occupancy space.
4.3 Feature-Based Heuristic Search Value Iteration Algorithm
section presents feature-based heuristic search value iteration algorithm (FB-HSVI)
iteratively updates feature-based compact lower- upper-bound representations. also discuss
FB-HSVIs theoretical guarantees.
4.3.1 Algorithm Description
Similar OHSVI (Algorithm 1), FB-HSVI (Algorithm 2) solves occupancy-state MDPs generating trajectories occupancy states iteratively updating lower upper bounds,
case FB-HSVI, compact occupancy states feature-based compact lower
(L M,t )t{0,1,...,T } upper bounds (U M,t )t{0,1,...,T } . FB-HSVI improves scalability OHSVI
478

fiOptimally Solving Finite-Horizon Dec-POMDPs

several ways. First, FB-HSVI replaces full exact representations compact representations all:
occupancy states, decision rules, lower upper bounds. addition, combines stopping criteria
HSVI (Smith, 2007) optimal classical heuristic search methods (e.g., Hart et al., 1968;
Korf, 1990), may result efficient pruning unnecessary subspaces.
Algorithm 2: FB-HSVI Algorithm.
function FB-HSVI((L M,t )t{0,1, ,T } , (U M,t )t{0,1, ,T } )
initialize L M,t U M,t {0, , 1}.
Stop (0 , 0) Explore(0, 0)
function Explore(t, gt )
Stop (t , gt )
dt arg maxdt R(t , dt ) + U M,t+1 ( P(t , dt ))
Update U M,t
Explore( P(t , dt ), R(t , dt ) + gt )
Update L M,t
return gt
function Stop(t , gt )
return U M,t (t ) L M,t (t ) L M,0 (0 ) gt + U M,t (t )
FB-HSVI differs OHSVI four main ways:
1. compact representation occupancy states, significantly reduces search space;
2. compact representation decision rules, speeds decision-rule selection;
3. compact representation lower upper bounds, speeds convergence;
4. enhanced value function generalization combination stopping criteria, results
efficient pruning unnecessary subspaces.
4.3.2 Stopping Criteria
stopping criteria FB-HSVI build upon optimal classical heuristic search methods
(e.g., Hart et al., 1968; Korf, 1990; Smith, 2007). determine stop current trajectory
compact occupancy states algorithm. Ideally, optimal criterion would measure distance current trajectory optimal trajectory, known. Instead, use two
criteria based upper lower bound values trajectories compact occupancy states.
upper bound current trajectory, denote f (t ), sum two functions: (1)
past trajectory-reward function g(0 , ), sum rewards starting compact occupancy state 0 current occupancy state ; (2) future trajectory-reward
compact occupancy state , admissible heuristic estimate, e.g., upper-bound U M,t (t )
compact occupancy state .
first criterion relies fact reason expand occupancy state
f (t ) less equal L M,0 (0 ), since cannot lead solution better current best
solution; criterion previously used optimal classical heuristic search methods (e.g., Hart et al.,
1968; Korf, 1990).

479

fiDibangoye, Amato, Buffet & Charpillet

Criterion 1. trajectory occupancy states (0 , . . . , ) interrupted whenever heuristic value
f (0 ) less equal L M,0 (0 ) i.e., L M,0 (0 ) f (0 ). best solution found far
optimal expanded occupancy state frontier search space2
heuristic-value f (t ) higher L M,0 (0 ).
second criterion builds upon fact reason expand occupancy state
upper bound less equal lower bound (Smith, 2007).
Criterion 2. trajectory occupancy states (0 , . . . , ) interrupted whenever upper bound
U M,t (t ) less equal lower bound L M,t (t ) i.e., L M,t (t ) U M,t (t ). best solution
found far optimal upper lower bounds initial occupancy state equal.
interrupting trajectory satisfies either criterion 1 2, FB-HSVI preserves
ability find optimal separable joint policy, shown below.
4.3.3 Convergence Guarantees
Theorems 7 8 show feature-based compact functions (L M,t )t{0,1, ,T } (U M,t )t{0,1, ,T }
iteratively updated FB-HSVI (Algorithm 2) upper lower-bounds optimal value function.
Next, prove upon update trajectory compact occupancy state bounds
depreciated, least one compact occupancy state improves bounds. Since finite
number compact occupancy states, bounds ultimately converge optimal value
initial occupancy state. Here, use argument show FB-HSVI converges optimal
separable joint policy finite number iterations. end, compact occupancy state
said finished either criterion 1 2 satisfied; otherwise finished. Moreover,
compact occupancy states last time step finished since criterion 2 satisfied
last time step .
Theorem 9. FB-HSVI algorithm always terminates finite number trials solution found termination separable joint policy lower bound induces optimal.
Proof. First, show contradiction algorithm cannot terminate optimal solution found. Suppose algorithm terminates finding optimal solution value
f (0 ). Then, sequence f (0 ) values generated planning f 0 (0 ), f 1 (0 ), . . . , f k (0 ),
f 0 (0 ) initial lower bound solution found, f 1 (0 ) value first
solution found, f k (0 ) last solution found. addition, know hypothesis
f 0 (0 ) < f 1 (0 ) < . . . < f k (0 ) f (0 ), last inequality holds assumption
algorithm may terminate suboptimal solution.
consider optimal path 0 , 1 , . . . , leading initial occupancy state terminal
occupancy state. assumption optimal path found, must
occupancy state along path generated expanded. possible
f (t ) f k (0 ). admissibility f , know f (t ) f (0 ) therefore f (t )
f (0 ) > f (0 ) {0, 1, . . . , k}. contradiction, follows algorithm
cannot terminate optimal solution found.
Next, show trial turns least one finished occupancy state finished one.
Suppose algorithm yet terminated trial executed. Let last two occupancy
2. Typically, search algorithms involves expanding nodes adding unexpanded neighboring nodes priority
queue, called frontier search space.

480

fiOptimally Solving Finite-Horizon Dec-POMDPs

states encountered forward expansion t+1 . Given trial terminated
t+1 , know trial, finished t+1 finished. t+1 results
greedy separable joint decision rule selection , know finished
updated. two scenarios possible, corresponds stopping
condition:
either t+1 yields optimal value, also yield optimal value updated,
making finished occupancy state update;
t+1 f (t+1 ) lower equal L M,0 (0 ), also f (t ) lower
equal L M,0 (0 ) updated.
Thus, executing trial causes occupancy state , finished, become finished.
Finally, show algorithm terminates finite number trials. end,
note search graph algorithm tree similar Figure 8, bounded branching
factor | | depth {0, 1, . . . , }. hypothesis, occupancy states appear depth <
finished. Thus, total number occupancy states depths time step upper
bounded total number information states depths time step :
1
| Tt=0
St | = |A |

|I||Z |T |A |T 1
,
|Z ||A | 1

(45)

= maxiI Ai Z = maxiI Z . Given least one occupancy state becomes finished
1 | trials, causing
trial, initial occupancy state must become finished |Tt=0

algorithm terminate.

Another important property FB-HSVI refines upper lower bounds throughout planning. Since compact occupancy state expansions interleaved updates, FB-HSVI
offers anytime solution. Furthermore, cutting FB-HSVI trials time, know
difference current best solution optimal one bounded.
Theorem 10. iteration FB-HSVI, current solution separable joint policy
induced current lower bound within = U M,0 (0 ) L M,0 (0 ) optimal solution.
Proof. Formally, difference value executing separable joint policy lb induced
current lower bound instead optimal separable joint policy written follows:
V M, (0 ) V M,lb (0 ) = V M, (0 ) L M,0 (0 ),

(V M,lb (0 ) = L M,0 (0 ))

(46)

U M,0 (0 ) L M,0 (0 ).

(V M, (0 ) U M,0 (0 ))

(47)

Consequently, whenever FB-HSVI interrupted, current solution within given
optimal solution.


5. Experiments
section empirically demonstrates validates importance feature-based heuristic
search value iteration (FB-HSVI) algorithm. show FB-HSVI outperforms existing exact
algorithms tested domains literature FB-HSVI solve problems
unprecedented time horizons.
481

fiDibangoye, Amato, Buffet & Charpillet

5.1 Experimental Setup
discussed throughout paper, many key components affect performance
FB-HSVI. key components include (upper lower) bound representations, bound
update methods, history compression, value generalization, initial upper bound.
present three variants FB-HSVI, denoted OHSVI, FB-HSVI-LPE FB-HSVI-TPE.
two latter differ notion history equivalence use feature-based compact
representations (see Table 2). equivalence relation given, FB-HSVI refers default
(and better performing) implementation, FB-HSVI-TPE.
Algorithm
OHSVI
FB-HSVI-LPE
FB-HSVI-TPE

Bound Representations
full
feature-based compact
feature-based compact

Compression
none
lpe
tpe

Table 2: review selected algorithmic components use.
selected benchmarks goal spanning range properties may affect
performance Dec-POMDP solver. Table 3, review selected domains properties. domains downloaded http://masplan.org.

Dec-Tiger
Mabc
Grid-Small
Recycling-Robots
Box Pushing
Mars Rovers

domain parameters
N |S | |Ai | |Z |
2
2
3
2
2
4
2
2
2 16
5
2
2
4
3
2
2 100 4
5
2 256 6
8

=2
6561
256
390625
6561
3.34 107
1.69 1014

|0:t | different
=5
= 10
3.43 1030
1.39 10977
1.84 1019
3.23 10616
5.42 1044
3.09 101431
30
3.43 10
1.39 10977
5.23 10940
1.25 102939746
7285
1.88 10
2.57 10238723869

Table 3: Domain parameters maximum number separable joint policies per horizons.

5.2 Empirical Analysis Algorithms
section, compare FB-HSVI exact solvers. exact Dec-POMDP solvers considered state-of-the-art methods including: GMAA*-ICE (Oliehoek et al., 2013), IPG (Amato
et al., 2009), MILP (Aras & Dutech, 2010), LPC (Boularias & Chaib-draa, 2008). IPG
LPC perform dynamic programming, GMAA*-ICE performs heuristic search MILP mixed
integer linear programming method. Results GMAA*-ICE (provided Matthijs Spaan), IPG,
MILP, LPC conducted different machines. this, timing results directly comparable, likely differ small constant factor. three FB-HSVI
variants (Table 2) implemented framework, using identical basic operations,
occupancy state value function updates, separable joint decision rule selection. terminate FB-HSVI whenever distance lower upper bounds within = 0.01.
time limit set 1000ms.

482

fiOptimally Solving Finite-Horizon Dec-POMDPs

5.2.1 Comparing Exact Planners


MILP

LPC

IPG ICE OHSVI
multi-agent tiger
0.32 0.01
0.161
55.4 0.01 28.567
2286 108
347

2
3
4
5
6
7
8
9
10


4.9
72

0.17
1.79
534

2
3
4
5
10
30
70
100











recycling robot
0.30
36
0.04
1.07
36
0.555
42.0
72
696.8
1812
72

2
3
4
5
6
7
8
9
10
20
30











meeting 3x3 grid


93.029







FB-HSVI

L0, (0 )

0.03
0.40
1.36
9.65
24.42
33.11
41.21
58.51
65.57

4.00
5.1908
4.8027
7.0264
10.381
9.9935
12.217
15.572
15.184

0.01
0.10
0.30
0.34
0.52
1.13
2.13
2.93

7.000
10.660
13.380
16.486
31.863
93.402
216.47
308.78

0.03
0.04
0.79
1.30
1.88
2.55
18.06
24.39
34.42
291.1
456.6

0.0
0.133
0.432
0.894
1.491
2.19
2.96
3.80
4.68
14.35
24.33

Table 4: Experiments comparing computation times (in seconds) exact solvers (part 1).
Time limit violations indicated , indicate unknown values. Bold entries
correspond best known results benchmarks, terms computational
time expected value.

Tables 4 5 show performance results exact algorithms. algorithm, reported
computation time, includes time compute heuristic values appropriate (since
algorithms use heuristics). also reported best expected cumulative reward
L M,0 (0 ) initial occupancy state. Tables 4 5 clearly show FB-HSVI allows significant improvement state-of-the-art solvers: tested benchmarks provide results
longer horizons solved previously (the bold entries). many cases, (epsilon)
optimal solution found horizons order magnitude larger previously
solvable. two main reasons FB-HSVIs performance. First, searches space
483

fiDibangoye, Amato, Buffet & Charpillet

policies mapping lower-dimensional features actions, whereas exact solvers search
space policies mapping full histories actions. addition, uses value function mapping occupancy states reals allowing generalize value function unvisited occupancy
states whereas solvers use value functions mapping partial policies reals. FB-HSVI performs best domain possesses structure results compact value function,
recycling robot mabc domains.


MILP

LPC

2
3
4
5
10
30
50
100











IPG
ICE
OHSVI
broadcast channel


0.036


3.446





2
3
4
5
6
7
8
9
10

0
0.65
1624






0
0.18
4.09
77.4

2
3
4
5
6
7
8
9
10









cooperative box-pushing
1.07
36
0.294
6.43
540
1138
2596

2
3
4
5
6
7
8
9
10







83
389

grid small
36
0.911
36
1512
242605

Mars rovers
1.0
0.027
1.0
1.881
103

FB-HSVI

L0, (0 )

0.02
0.22
0.32
0.33
0.78
14.0
41.7
473.3

2
2.99
3.89
4.79
9.29
27.42
45.50
90.76

0
0.1
0.73
1.39
3.51
8.30
42.2
69.03
581.2

0.37
0.91
1.55
2.24
2.97
3.71
4.47
5.23
6.03

0.1
0.457
0.622
5.854
10.7
24.96
28.97
184.3
293.7

17.600
66.081
98.593
107.72
120.67
156.42
191.22
210.27
223.74

0.10
0.23
0.47
0.82
3.97
5.81
22.8
26.5
62.7

5.80
9.38
10.18
13.26
18.62
20.90
22.47
24.31
26.31

Table 5: Experiments comparing computation times (in seconds) exact solvers (part 2).

484

fiOptimally Solving Finite-Horizon Dec-POMDPs

5.2.2 Choosing Method Keep Information Concise
compare local truncation probabilistic equivalence notions introduced maintain concise representations occupancy states, decision rules, value functions.
Box-Pushing

Mars Rover

400

TPE
LPE

103

|H|

|H|

300
200
100

102

101

0
2

3

4
Horizon

5

2

6

Recycling Robot

3

4

5 6 7
Horizon

8

9

10

Grid-Small

40
102
|H|

|H|

30
20
10

101

0
2

3

4

5 6 7
Horizon

8

9

10

2

3

4
5
Horizon

Dec-Tiger

7

6

mabc
20

100
|H|

|H|

15
10

50
5
0

0
2

3

4
5
Horizon

6

2

7

3

4

5 6 7
Horizon

8

9

10

Figure 12: Comparison compression methods maintain concise data FB-HSVI-LPE
FB-HSVI-TPE. graphs shows memory requirements convergence
time exceeds y-axis given various number planning horizons x-axis.

Clearly, algorithms use feature-based compact representations provide significant savings
number maintained histories (e.g., OHSVI). Using OHSVI,
number generated histories grows (in worst case) exponentially planning horizon.
exponential growth explains OHSVI, use history aggregation,

485

fiDibangoye, Amato, Buffet & Charpillet

cannot scale beyond planning horizon = 4 tested domains (see Tables 4 5).
Recycling Robot horizon = 5, experimental results together Table 3 show algorithms
use compression methods maintain 30 orders magnitude less separable joint policies
algorithms not. number histories retained important occupancy states,
decision rules, value functions mappings reachable histories (or corresponding labels).
end, compare LPE TPE selection benchmarks various planning
horizons.
previously discussed, though LPE yields compact occupancy states concise
result TPE, latter eases generalization bounds, speeds
convergence optimal solution. Figure 12 reports total number joint labels denoted
|L| explicitly maintained FB-HSVI-LPE FB-HSVI-TPE various planning
horizons. observe TPE yields concise bound representations LPE
benchmarks planning horizons (i.e., using TPE number |L| lower using LPE).
particular, notice that, tested domains, bounded number labels sufficient
representing optimal near-optimal value functions. TPE often succeeds identifying
memory-bounded parametric space, resulting concise value functions, whereas LPE often
fails.
Recycling Robot problem example, TPE yields 6 joint labels (i.e.,
histories) horizons whereas LPE maintains 38 different joint labels, number
keeps growing planning horizon increases. fact, (Dibangoye, Amato, & Doniec, 2012;
Becker, Zilberstein, Lesser, & Goldman, 2004) demonstrated Recycling-Robot problem,
recent private observation sufficient summarize past private histories agent
(i.e., four joint observations necessary). Here, TPE yields 6 joint labels
relies joint action-observation histories rather joint observation histories. BroadcastChannel domain, TPE yields 4 joint labels horizons whereas LPE produces
20 different joint labels. Again, results due underlying structure BroadcastChannel domain. scenario, future states world conditionally independent
joint histories. Hence, TPE forget joint histories, reason states. Another
domain interest Dec-Tiger problem. problem, = 6, TPE produces
30 joint labels horizons whereas LPE maintains 126 different joint labels.
assumption always exists optimal separable joint policy periodic (with period
3) Dec-Tiger domain. words, exists optimal separable joint policy
depends histories upon recent three action-observation pairs. Also, many
scenarios equivalence relations would fail identify memory-bounded space
histories, even space exists. example, important information history may
spread time steps, necessarily last ones.

6. Discussion
demonstrated method solve Dec-POMDPs larger
previously solved, many practical applications much larger domains considered
paper. result, additional methods may necessary solve large problems
permit construction concise feature space preserving optimality. concern
since numbers states histories impact occupancy states, separable joint decision rules,
value functions. Maintaining objects large feature spaces prohibitive. highlights

486

fiOptimally Solving Finite-Horizon Dec-POMDPs

necessity addressing scalability issue FB-HSVI concise (and possibly
lossy) feature spaces. direction, already extended general methodology presented
paper along two lines: error-bounded approximations tractable subclasses.
6.1 Error-Bounded Approximations
FB-HSVI find optimal solution maintains concise representations preserve
optimality. advantage liability. one hand, problems reasonable
size, algorithm find optimal solution. hand, many realistic applications,
run time memory. scalability limitations FB-HSVI maintains
accurate estimates (compact) occupancy states, value functions decision rules. improve
scalability Dec-POMDP solvers, many researchers investigated approximate solutions.
notable example family includes memory-bounded dynamic programming (MBDP)
algorithm finite-horizon Dec-POMDPs (Seuken & Zilberstein, 2007; Carlin & Zilberstein, 2008;
Dibangoye, Mouaddib, & Chaib-draa, 2009; Kumar & Zilberstein, 2010; Wu, Zilberstein, & Chen,
2010). dynamic programming methods require bounded computational resources
produce heuristic solutions empirically perform well standard Dec-POMDP benchmarks.
However, methods possess theoretical guarantees concerning quality
solutions.
Recently, introduced framework monitoring error FB-HSVI replacing exact
estimate (compact) occupancy states, decision rules value functions, approximate
counterparts (Dibangoye, Mouaddib, & Chaib-draa, 2011; Dibangoye, Buffet, & Charpillet, 2014).
resulting algorithm solve Dec-POMDP instances larger planning horizon still
providing strong theoretical guarantees.
also worth noting that, FB-HSVI trial-based, used anytime algorithm. is, alternates generation occupancy-state trajectory update
current best value function. algorithm proceeds, current (best) value function
improved expense increased computational time. algorithm terminated either
satisfactory value function attained, allocated planning time exceeded. either
case, algorithm always provide online performance bounds returned value function
illustrating far optimal value function current one is.
future, also would like explore using occupancy states observation histories
(rather action-observation histories), shown sufficient (along actionobservation histories) simultaneous work (Oliehoek, 2013). inclusion observation
histories could lead scalability gains reducing dimensionality feature space.
6.2 Tractable Subclasses
Many attempts address scalability issues Dec-POMDPs rely use tractable subclasses. subclasses additional assumptions allow concise representations
occupancy states, decision rules value functions; therefore speed convergence
towards optimal solution.
instance, already shown occupancy states states (and agent histories) used transition- observation-independent Dec-MDPs (Becker et al., 2004) (where
state fully determined joint observation) greatly increase scalability preserving
optimality (Dibangoye et al., 2012; Dibangoye, Amato, Doniec, & Charpillet, 2013). restrict487

fiDibangoye, Amato, Buffet & Charpillet

ing attention decentralized Markov policies (i.e., mappings private states private actions)
reduce complexity significantly (NP versus NEXP), make possible optimally solve
larger problems. plan investigate forms tractable structures including temporal dependencies constraints induce structured domains single multi-agent settings
(Dibangoye, Chaib-draa, & Mouaddib, 2008; Dibangoye, Shani, Chaib-Draa, & Mouaddib, 2009;
Pajarinen, Hottinen, & Peltonen, 2013). line research, introduced novel approach
called structural analysis means discovering underlying structural properties embedded
certain decentralized decision-making problems (Dibangoye, Buffet, & Simonin, 2015).
also applied general methodology presented paper scale number
agents involved process. end, consider domains exhibit locality interactions (Dibangoye, Amato, Buffet, & Charpillet, 2015, 2014). Examples include networked
distributed partially observable Markov decision processes (ND-POMDPs) (Nair, Varakantham,
Tambe, & Yokoo, 2005). plan explore applying methodology FB-HSVI DecPOMDPs agents joint dynamics rewards, well domains delayed communication (Ooi & Wornell, 1996; Grizzle, Marcus, & Hsu, 1981; Oliehoek & Spaan, 2012), means
reducing memory burden.
secondary (but less important) issue concerning scalability Dec-POMDPs pertains
efficient methods update occupancy states value functions planning stage. locality
interaction among agents may exploited statically (e.g., Nair et al., 2005; Kumar & Zilberstein,
2009; Amato, Konidaris, & Kaelbling, 2014) dynamically (e.g., Canu & Mouaddib, 2011)
considering factorization graphical models representation hence improve scalability.
critical issue number occupancy states necessary obtain good solution may
exponential planning horizon. So, techniques efficiently update occupancy
states value functions great importance.

7. Conclusion
paper describes novel way representing Dec-POMDPs, continuous-state MDPs
piecewise-linear convex value functions, scalable algorithm generating -optimal solutions. summarize key contributions below.
exploiting assumption centralized planning decentralized execution, method
recasts Dec-POMDP problem equivalent deterministic centralized fully observable
MDP (using information available agents). Next, identify concise statistic
occupancy state represents state resulting fully observable MDP, call
occupancy-state MDP. demonstrate optimal value functions occupancy MDPs
piecewise linear convex functions occupancy states. also prove optimal
solution occupancy-state MDP optimal solution corresponding Dec-POMDP.
also present feature-based heuristic search value iteration (FB-HSVI) algorithm find
optimal solution occupancy-state MDP. algorithm builds theory solving
POMDPs MDPs, occupancy-state MDP allows methods directly applied
Dec-POMDPs first time. believe FB-HSVI major step forward scalable exact
solutions Dec-POMDPs. scalability achieved defining feature-based compact occupancy states decision rules use equivalence relations private histories.
concise representations permit us circumvent exhaustive enumeration otherwise
intractable number occupancy states decision rules.

488

fiOptimally Solving Finite-Horizon Dec-POMDPs

Another aspect improved scalability stems generalization value function.
achieved piecewise linear convex functions occupancy-state MDP.
show that, although feature-based compact lower upper bounds longer piecewiselinear convex, still generalize value functions entire feature-based compact
occupancy-state space.
Experimentally, show FB-HSVI able outperform current state-of-the-art exact
Dec-POMDP solvers common benchmark domains. results show -optimal solutions
found larger horizons problems horizons sometimes order
magnitude larger previously solved.

8. Acknowledgements
would like thank Matthijs Spaan providing results GMAA*-ICE well Frans
Oliehoek, Akshat Kumar anonymous reviewers helpful comments. Research supported part AFOSR MURI project #FA9550-09-1-0538.

Appendix A. Correctness Feature-Based Compact Bounds
A.1 Proof Theorem 6
(). Hence, obtain successively:
(a) hypothesis, , v V M,t

()
v V M,t

(48)


= V M,t
()
X
def
= max
(s, ) (s, )


max


= max


( = F () Theorem 5),

(49)


(by definition V M,t
()),

(50)

( projected onto ),

(51)

s,



X X

(s, ) (s, ) ( s, )
s,



s,

X

s,

( s, ) ( s, ),

(52)

s,




X X



s, ( s, ) ( s, ) (s, )
max




( projected onto ),

(53)

s, s,

def


= V M,t
(F ()),

(54)

ends proof Theorem 6.a.

() h, G ()i arbitrary R| | .
(b) hypothesis, V M,t


() h, G (G ())i arbitrary feature set , successively show:
prove V M,t


X
X
X
def
s, (s, )
s, ( s, ) ( s, ),
(s, )
h, G (G ())i =
s,

s,

s,

(55)





X

X
X


( s, )
(s, )
=
s, ( s, ) s, (s, ) , (Re-ordering). (56)



s,

s,

s,

489

fiDibangoye, Amato, Buffet & Charpillet

proceeding further, need prove quantity s, (s, ) greater equal quantity
s, s, ( s, ) s, (s, ) (in bracket last expression above). end, start
interpretations expression. first expression asks whether state-history pair (s, )
belongs cluster along feature s, , affirmative answer results value s, (s, ) = 1
otherwise 0. Let , feature whose cluster includes state-history pair (s, ). Then,
second expression asks whether state-history pairs ( , ) (s, ) belong cluster along
P
feature s, , affirmative answer result value s, s, ( s, ) s, (s, ) = 1
otherwise 0. Clearly, second expression stricter form first expression, hence s, (s, )
P
P
greater equal s, s, ( s, ) s, (s, ). Thus, replacing s, s, ( s, ) s, (s, )
s, (s, ), obtain:


X

X
X


(s, )
( s, )
h, G (G ())i =
s, ( s, ) s, (s, )
(57)



s,
s,
s,
X
X
(s, )
s, (s, ) ( s, ),
(58)

P

s,

s,

def

= h, G ()i

(59)


(),
V M,t

(60)

ends proof Theorem 6.b.





A.2 Proof Theorem 7
proof proceeds induction. Heuristic function (U M,t )t{0,1,...,T } , initially upper bounds
optimal value function, since initialized using underlying MDP value function.
induction step, assume heuristic function (U M,t )t{0,1,...,T } represented using point sets
(t )t{0,1,...,T } upper bounds optimal value function.
,
Next, show that, arbitrary time step {0, 1, . . . , }, heuristic function U M,t

update U M,t resulting upper bound v occupancy state , also upper bound V M,t
entire compact occupancy space . is,

:



( ).
( ) V M,t
U M,t ( ) U M,t

(61)

( ). Using sawtooth interpolation approach3 ,
first show : U M,t ( ) U M,t
obtain successively:



def


( ) = min v , v | ( 7 v ) { 7 v } ,
U M,t
(see sawtooth definition)
(62)


= min v , U M,t ( ) ,
(63)

U M,t ( ),

(64)

proves first part expression (Eq. 61).
( ) V ( ). end, distinguish
Now, show : U M,t
M,t
update U M,t .
3. Here, adapted sawtooth interpolation replace full occupancy states compact occupancy states.

490

fiOptimally Solving Finite-Horizon Dec-POMDPs

update,

following holds:


( ),
( ) V M,t
U M,t

:

(65)

inductive hypothesis.
update, obtain two important results. one hand, resulting
value v upper bound :
def

v = max R(, ) + U M,t+1 ( P(, )),

(66)


( P(, )),
max R(, ) + V M,t+1

(67)





def


().
= V M,t

(68)


hand, show v one extrapolate upper bound value v
compact occupancy state . mainly thanks Theorem 6.a, demonstrate:
() v holds, arbitrary
, expression V (F ()) v holds
expression V M,t
M,t
well. sawtooth interpolation method concludes argument follows:




v = v + (v v ) D(F (), ),
def


( ).
V M,t

(69)
(70)

fact, sawtooth interpolation always generate upper-bound one compact occupancy
state upper bound compact occupancy state long expressed
feature set.

A.3 Proof Theorem 8
proof proceeds induction well. Heuristic function (L M,t )t{0,1,...,T } initially lower bounds
optimal value function since initialize using value function associated worst
separable joint policy. one prescribes agents joint action yields minimum
def
reward, time steps, i.e., L M,t () = (T t) min s,a ra (s), {0, 1, . . . , }.
induction step, assume heuristic function (L M,t )t{0,1,...,T } represented using compact
vector sets (t )t{0,1,...,T } lower bounds optimal value function. Next, show
arbitrary time step {0, 1, . . . , }, heuristic function LM,t , results update lower

bound L M,t produces compact vector along feature set , also lower bound V M,t
entire compact occupancy space . is,

( ).
L M,t ( ) LM,t ( ) V M,t

:

(71)

first show : L M,t ( ) LM,t ( ). fact,
def

L M,t ( ) = max h , G ()i,

(72)



max
{ }
def

= LM,t ( ),
491

h , G ()i,

(73)
(74)

fiDibangoye, Amato, Buffet & Charpillet

proves first part.
( ). end, distinguish
Now, show : LM,t ( ) V M,t
update L M,t .
update,

induction hypothesis, have:
:


LM,t ( ) V M,t
( ).

(75)

update, obtain: ,
def



( P( , )),
V M,t
( ) = max R( , ) + V M,t+1

(76)





= max h , rd + max h pd , G
t+1



=

h , rd + G

max

h , rd + G

A,t+1





max



A,t+1



max
A,t+1


p


p

()i,

(77)

() (pd ) i,

(re-arranging terms )

(78)

() (pd ) i,

(replace t+1 t+1 )

(79)

(Lemma 6)

(80)

(retain one element).

(81)



h , G (rd + G


p


p

() (pd ) ))i,

] , t+1 ))i,
h , G (backup(

( ), prove
Merging together arguments update, i.e., LM,t ( ) V M,t
second part proof. ends proof.


Appendix B. Subroutines
section gives subroutines required compute feature-based compact occupancy states
using either local truncation probabilistic equivalence relations (Algorithm 3).
Algorithm 3: Compact feature-based occupancy state LPE.
function Compact-LPE(t)
{(s, ) : (s, ) > 0}
foreach (s, )
S\{(s, )} (s, ) (s, )
foreach ( s, )
AreStateJointHistoryPairsLPE((s, ), ( s, ), )
S\{( s, )} (s, ) (s, ) + ( s, )
return
function Compact-TPE(t)
mt getTruncationParam(t) ()
foreach (s, )
S\{(s, )} (s, ) (s, )
foreach ( s, )
AreStateJointHistoryPairsTPE((s, ), ( s, ), mt )
S\{( s, )} (s, ) (s, ) + ( s, )
return

492

fiOptimally Solving Finite-Horizon Dec-POMDPs

Algorithm 4: Subroutines compact feature-based occupancy state LPE TPE.
function ArePrivateHistoriesLPE(ti, ti , )
foreach ti
(t )
Pr(s, ti |0 , ) , Pr(s, ti |0 , ) return False
return True
function AreStateJointHistoryPairsLPE((s, htiiiI ), ( s, hti iiI ), )
, return False
foreach
ArePrivateHistoryLPE(ti, ti , ) return False
return True
function getTruncationParam(t)
m0
foreach
foreach ti , ti (t )
Suffix(ti, m) = Suffix(ti, m)
ArePrivateHistoriesLPE(ti , ti , ) + 1;
return
function ArePrivateHistoriesTPE(ti, ti , mt )
return Suffix(ti, mt ) = Suffix(ti, mt )
function AreStateJointHistoryPairsTPE((s, htiiiI ), ( s, hti iiI ), mt )
, return False
foreach
ArePrivateHistoryLPE(ti, ti , mt ) return False
return True

References
Amato, C., Bernstein, D. S., & Zilberstein, S. (2010). Optimizing fixed-size stochastic controllers
POMDPs decentralized POMDPs. Journal Autonomous Agents Multi-Agent
Systems, 21(3), 293320.
Amato, C., Chowdhary, G., Geramifard, A., Ure, N. K., & Kochenderfer, M. J. (2013). Decentralized control partially observable Markov decision processes. 54th IEEE Conference
Decision Control.
Amato, C., Dibangoye, J. S., & Zilberstein, S. (2009). Incremental policy generation finitehorizon DEC-POMDPs. Proceedings Nineteenth International Conference Automated Planning Scheduling.
Amato, C., Konidaris, G. D., Anders, A., Cruz, G., How, J. P., & Kaelbling, L. P. (2015). Policy
search multi-robot coordination uncertainty. Proceedings Robotics: Science
Systems Conference.
Amato, C., Konidaris, G. D., & Kaelbling, L. P. (2014). Planning macro-actions decentralized POMDPs. Proceedings Thirteenth International Conference Autonomous
Agents Multiagent Systems.

493

fiDibangoye, Amato, Buffet & Charpillet

Aras, R., & Dutech, A. (2010). investigation mathematical programming finite horizon
decentralized POMDPs. Journal Artificial Intelligence Research, 37, 329396.
Banerjee, B., Lyle, J., Kraemer, L., & Yellamraju, R. (2012). Sample bounded distributed reinforcement learning decentralized POMDPs. Proceedings Twenty-Sixth AAAI
Conference Artificial Intelligence, pp. 12561262, Toronto, Canada.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamic programming. Artificial Intelligence, 72(1-2), 81138.
Becker, R., Zilberstein, S., Lesser, V. R., & Goldman, C. V. (2004). Solving transition independent
decentralized Markov decision processes. Journal Artificial Intelligence Research, 22,
423455.
Bellman, R. E. (1957). Dynamic Programming. Dover Publications, Incorporated.
Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). complexity decentralized control Markov decision processes. Mathematics Operations Research, 27(4).
Boularias, A., & Chaib-draa, B. (2008). Exact dynamic programming decentralized POMDPs
lossless policy compression. Proceedings Eighteenth International Conference
Automated Planning Scheduling, pp. 2027.
Canu, A., & Mouaddib, A.-I. (2011). Collective decision partial observability - dynamic
local interaction model. IJCCI (ECTA-FCTA), pp. 146155.
Carlin, A., & Zilberstein, S. (2008). Value-based observation compression DEC-POMDPs.
Proceedings Seventh International Conference Autonomous Agents Multiagent
Systems.
De Farias, D. P., & Van Roy, B. (2003). linear programming approach approximate dynamic
programming. Operations Research, 51(6), 850865.
Dibangoye, J. S., Amato, C., Buffet, O., & Charpillet, F. (2014). Exploiting separability multiagent planning continuous-state MDPs. Proceedings Thirteenth International
Conference Autonomous Agents Multiagent Systems, pp. 12811288.
Dibangoye, J. S., Amato, C., Buffet, O., & Charpillet, F. (2015). Exploiting separability multiagent planning continuous-state MDPs (extended abstract). Proceedings TwentyFifth International Joint Conference Artificial Intelligence, pp. 42544260.
Dibangoye, J. S., Amato, C., & Doniec, A. (2012). Scaling decentralized MDPs heuristic
search. Proceedings Twenty-Eighth Conference Uncertainty Artificial Intelligence, pp. 217226.
Dibangoye, J. S., Amato, C., Doniec, A., & Charpillet, F. (2013). Producing efficient error-bounded
solutions transition independent decentralized MDPs. Proceedings Twelfth International Conference Autonomous Agents Multiagent Systems, pp. 539546.
Dibangoye, J. S., Buffet, O., & Simonin, O. (2015). Structural results cooperative decentralized control models. Proceedings Twenty-Fifth International Joint Conference
Artificial Intelligence, pp. 4652.
Dibangoye, J. S., Mouaddib, A.-I., & Chaib-draa, B. (2009). Point-based incremental pruning
heuristic solving finite-horizon DEC-POMDPs. Proceedings Eighth International Conference Autonomous Agents Multiagent Systems, pp. 569576.
494

fiOptimally Solving Finite-Horizon Dec-POMDPs

Dibangoye, J. S., Mouaddib, A.-I., & Chaib-draa, B. (2011). Toward error-bounded algorithms
infinite-horizon Dec-POMDPs. Proceedings Tenth International Conference
Autonomous Agents Multiagent Systems, pp. 947954.
Dibangoye, J. S., Shani, G., Chaib-Draa, B., & Mouaddib, A.-I. (2009). Topological order planner POMDPs. Proceedings Twenty-Second International Joint Conference
Artificial Intelligence, pp. 16841689.
Dibangoye, J. S., Amato, C., Buffet, O., & Charpillet, F. (2013). Optimally solving Dec-POMDPs
continuous-state MDPs. Proceedings Twenty-Fourth International Joint Conference
Artificial Intelligence.
Dibangoye, J. S., Buffet, O., & Charpillet, F. (2014). Error-bounded approximations infinitehorizon discounted decentralized POMDPs. Proceedings Twenty-Fourth European
Conference Machine Learning, pp. 338353.
Dibangoye, J. S., Chaib-draa, B., & Mouaddib, A.-I. (2008). novel prioritization technique
solving Markov decision processes. Proceedings 21th International Conference
Florida Artificial Intelligence Research Society, pp. 537542.
Grizzle, J. W., Marcus, S. I., & Hsu, K. (1981). Decentralized control multiaccess broadcast
network. 20th IEEE Conference Decision Control including Symposium
Adaptive Processes, Vol. 20, pp. 390391.
Hansen, E. A., Bernstein, D. S., & Zilberstein, S. (2004). Dynamic programming partially observable stochastic games. Proceedings Nineteenth National Conference Artificial
Intelligence, pp. 709715.
Hansen, E. A., & Zilberstein, S. (2001). LAO*: heuristic search algorithm finds solutions
loops. Artificial Intelligence, 129(1-2), 3562.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). formal basis heuristic determination
minimum cost paths. IEEE Trans. Systems Science Cybernetics, 4(2), 100107.
Hauskrecht, M. (2000). Value-function approximations partially observable Markov decision
processes. Journal Artificial Intelligence Research, 13, 3394.
Howard, R. A. (1960). Dynamic Programming Markov Processes. M.I.T. Press.
Jain, M., Taylor, M. E., Tambe, M., & Yokoo, M. (2009). DCOPs meet real world: Exploring
unknown reward matrices applications mobile sensor networks. Proceedings
Twenty-Second International Joint Conference Artificial Intelligence, pp. 181186.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning acting partially
observable stochastic domains. Artificial Intelligence, 101(1-2), 99134.
Korf, R. E. (1990). Real-time heuristic search. Artificial Intelligence, 42(2-3), 189211.
Kumar, A., & Zilberstein, S. (2009). Constraint-based dynamic programming decentralized
POMDPs structured interactions. Proceedings Eighth International Conference
Autonomous Agents Multiagent Systems, pp. 561568.
Kumar, A., & Zilberstein, S. (2010). Point-based backup decentralized POMDPs: complexity
new algorithms. Proceedings Ninth International Conference Autonomous
Agents Multiagent Systems, pp. 13151322.
495

fiDibangoye, Amato, Buffet & Charpillet

Nair, R., Tambe, M., Yokoo, M., Pynadath, D. V., & Marsella, S. (2003). Taming decentralized
POMDPs: Towards efficient policy computation multiagent settings. Proceedings
Eighteenth International Joint Conference Artificial Intelligence, pp. 705711.
Nair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2005). Networked distributed POMDPs:
synthesis distributed constraint optimization POMDPs. Proceedings Twentieth
National Conference Artificial Intelligence, pp. 133139.
Oliehoek, F. A. (2012). Decentralized POMDPs. Wiering, M., & van Otterlo, M. (Eds.), Reinforcement Learning: State Art, Vol. 12, pp. 471503. Springer Berlin Heidelberg,
Berlin, Germany.
Oliehoek, F. A. (2013). Sufficient plan-time statistics decentralized POMDPs. Proceedings
Twenty-Fourth International Joint Conference Artificial Intelligence.
Oliehoek, F. A., Spaan, M. T. J., Amato, C., & Whiteson, S. (2013). Incremental clustering
expansion faster optimal planning Dec-POMDPs. Journal Artificial Intelligence
Research, 46, 449509.
Oliehoek, F. A., Spaan, M. T. J., & Vlassis, N. A. (2008). Optimal approximate Q-value functions decentralized POMDPs. Journal Artificial Intelligence Research, 32, 289353.
Oliehoek, F. A., & Spaan, M. T. (2012). Tree-based solution methods multiagent POMDPs
delayed communication. Proceedings Twenty-Sixth AAAI Conference Artificial
Intelligence.
Oliehoek, F. A., Whiteson, S., & Spaan, M. T. J. (2009). Lossless clustering histories decentralized POMDPs. Proceedings Eighth International Conference Autonomous
Agents Multiagent Systems, pp. 577584.
Ooi, J. M., & Wornell, G. W. (1996). Decentralized control multiple access broadcast channel:
Performance bounds. Proc. 35th IEEE Conference Decision Control, Vol. 1,
pp. 293298. IEEE.
Pajarinen, J., Hottinen, A., & Peltonen, J. (2013). Optimizing spatial temporal reuse wireless
networks decentralized partially observable Markov decision processes. IEEE Transactions Mobile Computing, 13(4). Preprint.
Paquet, S., Chaib-draa, B., Dallaire, P., & Bergeron, D. (2010). Task allocation learning multiagent environment: Application robocuprescue simulation. Multiagent Grid Systems, 6(4), 293314.
Pineau, J., Gordon, G. J., & Thrun, S. (2006). Anytime point-based approximations large
POMDPs. Journal Artificial Intelligence Research, 27, 335380.
Powell, W. B. (2007). Approximate Dynamic Programming: Solving Curses Dimensionality
(Wiley Series Probability Statistics). Wiley-Interscience.
Puterman, M. L. (1994). Markov Decision Processes, Discrete Stochastic Dynamic Programming.
Wiley-Interscience, Hoboken, New Jersey.
Roy, N., Gordon, G. J., & Thrun, S. (2005). Finding approximate POMDP solutions belief
compression. Journal Artificial Intelligence Research, 23, 140.

496

fiOptimally Solving Finite-Horizon Dec-POMDPs

Seuken, S., & Zilberstein, S. (2007). Improved memory-bounded dynamic programming DECPOMDPs. Proceedings Twenty-Third Conference Uncertainty Artificial Intelligence.
Shani, G., Pineau, J., & Kaplow, R. (2013). survey point-based POMDP solvers. Journal
Autonomous Agents Multi-Agent Systems, 27(1), 151.
Smallwood, R. D., & Sondik, E. J. (1973). optimal control partially observable Markov
decision processes finite horizon. Operations Research, 21(5), 10711088.
Smith, T. (2007). Probabilistic Planning Robotic Exploration. Ph.D. thesis, Robotics
Institute, Carnegie Mellon University, Pittsburgh, PA.
Smith, T., & Simmons, R. (2004). Heuristic search value iteration POMDPs. Proceedings
Twentieth Conference Uncertainty Artificial Intelligence, pp. 520527, Arlington,
Virginia, United States.
Smith, T., & Simmons, R. G. (2006). Focused real-time dynamic programming MDPs: Squeezing heuristic. Proceedings Twenty-First AAAI Conference Artificial
Intelligence, pp. 12271232.
Spaan, M. T. J., Oliehoek, F. A., & Amato, C. (2011). Scaling optimal heuristic search DecPOMDPs via incremental expansion. Proceedings Twenty-Third International Joint
Conference Artificial Intelligence, pp. 20272032.
Szer, D., Charpillet, F., & Zilberstein, S. (2005). MAA*: heuristic search algorithm solving
decentralized POMDPs. Proceedings Twenty-First Conference Uncertainty
Artificial Intelligence, pp. 568576.
Tsitsiklis, J. N., & van Roy, B. (1996). Feature-based methods large scale dynamic programming. Machine Learning, 22(1-3), 5994.
Velagapudi, P., Varakantham, P., Sycara, K., & Scerri, P. (2011). Distributed model shaping
scaling decentralized POMDPs hundreds agents. Proceedings Tenth International Conference Autonomous Agents Multiagent Systems, pp. 955962.
Winstein, K., & Balakrishnan, H. (2013). TCP ex Machina: Computer-generated congestion control.
SIGCOMM, Hong Kong.
Wu, F., Zilberstein, S., & Chen, X. (2010). Point-based policy generation decentralized
POMDPs. Proceedings Ninth International Conference Autonomous Agents
Multiagent Systems, pp. 13071314.
Wu, F., Zilberstein, S., & Chen, X. (2011). Online planning multi-agent systems bounded
communication. Artificial Intelligence, 175(2), 487511.
Zilberstein, S., Washington, R., Bernstein, D. S., & Mouaddib, A.-I. (2002). Decision-theoretic
control planetary rovers. Revised Papers International Seminar Advances
Plan-Based Control Robotic Agents, pp. 270289, London, UK. Springer-Verlag.

497

fiJournal Artificial Intelligence Research 55 (2016) 603-652

Submitted 07/15; published 03/16

Large-Scale Election Campaigns:
Combinatorial Shift Bribery
Robert Bredereck

robert.bredereck@tu-berlin.de

TU Berlin,
Berlin, Germany

Piotr Faliszewski

faliszew@agh.edu.pl

AGH University Science Technology,
Krakow, Poland

Rolf Niedermeier
Nimrod Talmon

rolf.niedermeier@tu-berlin.de
nimrodtalmon77@gmail.com

TU Berlin,
Berlin, Germany

Abstract
study complexity combinatorial variant Shift Bribery problem
elections. standard Shift Bribery problem, given election
voter preference order set candidates outside agent,
briber, pay voter rank bribers favorite candidate given number
positions higher. goal ensure victory bribers preferred candidate.
combinatorial variant problem, introduced paper, models settings
possible affect position preferred candidate multiple votes, either positively
negatively, single bribery action. variant problem particularly
interesting context large-scale campaign management problems (which,
technical side, modeled bribery problems). show that, general, combinatorial variant problem highly intractable; specifically, NP-hard, hard
parameterized sense, hard approximate. Nevertheless, provide parameterized
algorithms approximation algorithms natural restricted cases.

1. Introduction
study computational complexity election campaign management case
campaign actions (such airing TV advertisement, launching web-based campaign, organizing meetings voters) may large-scale effects affect multiple
voters. Further, interested settings actions positive effects (for example, voters may choose rank promoted candidate higher
find arguments presented given advertisement appealing) well negative ones
(for example, voters find advertisement aggressive). Thus,
setting, two major issues faced campaign manager (a) choosing actions
c
2016
AI Access Foundation. rights reserved.

fiBredereck, Faliszewski, Niedermeier, & Talmon

positively affect many voters possible (b) balancing negative effects
campaigning actions (for example, concentrating negative effects voters
disregard promoted candidate anyway).
research falls within field computational social choice, subarea multiagent
systems. use standard election model, given set C candidates
collection V voters, represented preference order (that is, ranking
candidates preferred one least preferred one). assume
know preferences voters. perfect knowledge impossible
practice, assumption convenient simplification models fact may
(approximate) information preelection polls sources.
consider two voting rules, Plurality rule (where pick candidate
ranked first voters) Borda rule (where candidate c gets
voter v many points candidates v prefers c to, pick
candidate points). rules chosen Plurality rule
widespread rule practice Borda rule well-studied
context campaign management.
Within computational social choice, term campaign management (introduced
Elkind, Faliszewski, & Slinko, 2009; Elkind & Faliszewski, 2010) alternative name
bribery family problems (introduced Faliszewski, Hemaspaandra, & Hemaspaandra, 2009a) cases one focuses modeling actions available election
campaigns: result money spent campaign manager, voters change
votes. paper study campaign management Shift Bribery
problem (Elkind et al., 2009; Elkind & Faliszewski, 2010; Bredereck, Chen, Faliszewski,
Nichterlein, & Niedermeier, 2014a; Bredereck, Faliszewski, Niedermeier, & Talmon, 2016).
Shift Bribery candidate p want win, voter v
price v (i) voter willing shift p forward positions preference
order1 , ask lowest cost ensuring p winner (see Section 1.1
references campaign management problems).
Shift Bribery problem one major drawback model campaign management. incapable capturing large-scale effects campaign actions. particular,
one puts forward TV spot promoting given candidate, voters react
positively rank candidate higher, oblivious it, react
negatively, ranking candidate lower. Shift Bribery cannot model correlated
effects. paper introduce study Combinatorial Shift Bribery problem, allowing campaign actions effects, positive negative, whole groups
voters.
interested understanding realistic model campaign management
affects complexity problem. Indeed, Shift Bribery is, computationally,
well-behaved problem. example, Plurality rule solvable polynomial time
Borda rule NP-complete (Elkind et al., 2009), polynomial-time
2-approximation algorithm (Elkind et al., 2009; Elkind & Faliszewski, 2010)
fixed-parameter (FPT) algorithms, either exact capable finding solutions arbitrarily
close optimal ones (Bredereck et al., 2014a). work, ask extent
1. course, price necessarily reflect direct money transfer voter, rather cost
convincing voter change mind.

604

fiCombinatorial Shift Bribery

retain good computational properties allow large-scale effects.
results surprising positively negatively:
1. Combinatorial Shift Bribery becomes NP-complete W[1]-hard even
Plurality rule, even restrictive choice parameters, even correlated
effects particular campaign actions limited two voters. Moreover,
hardness results imply good, general approximation algorithms exist
allow negative effects campaign actions.
2. spite above, still possible derive relatively good (approximation)
algorithms, Plurality rule Borda rule, provided
restrict effects campaign actions positive either
involve voters each, involve groups consecutive voters (with respect
ordering voters might correspond, example, time).
results summarized Table 1 Section 4. generality problem
combinatorial nature natural obtain many hardness results. Yet,
extent strength surprising, fact also find nontrivial landscape
tractable cases.
1.1 Related Work
work builds top two main research ideas. First, studying campaign management/bribery problems, and, second, studying combinatorial variants election
problems.
study computational complexity bribery elections initiated
Faliszewski et al. (2009a), continued number researchers (Faliszewski, Hemaspaandra, Hemaspaandra, & Rothe, 2009b; Hazon, Lin, & Kraus, 2013; Mattei, Goldsmith,
& Klapper, 2012a; Mattei, Pini, Rossi, & Venable, 2012b). Elkind et al. (2009) Elkind
Faliszewski (2010) realized formalism election bribery problems useful
point view planning election campaigns. particular, defined
Swap Bribery problem restricted variant, Shift Bribery. former possible, given price, swap two adjacent candidates given vote. latter,
allowed shift preferred candidate forward. Various problems, modeling
different flavors campaign management, studied, including, example,
possibility alter number approved/ranked candidates (Baumeister, Faliszewski,
Lang, & Rothe, 2012; Faliszewski, Reisch, Rothe, & Schend, 2014; Schlotter, Faliszewski,
& Elkind, 2011). Different (positive) applications bribery problems include, example,
Margin Victory problem, goal briber prevent candidate winning. possible low cost, suggests election
could tampered (Cary, 2011; Magrino, Rivest, Shen, & Wagner, 2011; Xia,
2012; Reisch, Rothe, & Schend, 2014).
point view, related works Elkind et al. (2009), Elkind
Faliszewski (2010), Bredereck et al. (2014a, 2016), Dorn Schlotter (2012).
former ones study Shift Bribery, generalize (parameterized complexity
Shift Bribery studied Bredereck et al., 2014a, Shift Bribery multiwinner
605

fiBredereck, Faliszewski, Niedermeier, & Talmon

elections studied Bredereck et al., 2016), whereas work Dorn Schlotter
(2012) pioneers use parameterized complexity analysis (swap) bribery problems.
work largely inspired Bulteau, Chen, Faliszewski, Niedermeier,
Talmon (2015) Chen, Faliszewski, Niedermeier, Talmon (2015), introduced
studied combinatorial variants election control. Election control well-studied
topic computational social choice, initiated Bartholdi, Tovey, Trick (1992)
studied numerous researchers (we point readers Faliszewski, Hemaspaandra,
& Hemaspaandra, 2010; Faliszewski & Rothe, 2015, detailed account). Briefly put,
control problems model attempts changing election results changing structure. standard types control include adding, deleting, partitioning candidates
voters. Control problems, especially related adding deleting voters, quite
relevant issues campaign management, and, indeed, Section 5 show connection Combinatorial Shift Bribery (combinatorial) control adding
voters (Bulteau et al., 2015).
idea combinatorial shift bribery somewhat related problem lobbying
multiple referenda, introduced Christian, Fellows, Rosamond, Slinko (2007)
(parameterized study provided Bredereck, Chen, Hartung, Kratsch, Niedermeier,
Suchy, & Woeginger, 2014b; probabilistic variant studied, also parameterized
sense, Binkele-Raible, Erdelyi, Fernau, Goldsmith, Mattei, & Rothe, 2014). There,
number yes/no elections goal ensure election
majority voters vote yes. single lobbying action convince one voter
vote yes elections. combinatorial shift bribery single election
single action affect multiple voters, whereas lobbying problem multiple
elections action affects one voter.
stress use term combinatorial variants election problems
different one used well-established line work regarding combinatorial
candidate spaces (see Lang & Xia, 2015, works, example, Boutilier, Brafman,
Hoos, & Poole, 2004; Conitzer, Lang, & Xia, 2009; Mattei et al., 2012b). work use
term combinatorial refer combinations voters affected bribery
action.
1.2 Organization Paper
providing preliminaries Section 2, give formal definition Combinatorial Shift Bribery problem Section 3. Section 4 give overview
results. shed light connections Combinatorial Shift Bribery
problem Combinatorial Control Section 5. Then, Section 6, present
series strong hardness results covering classes shift actions restrictive sets parameters (for example, many results already apply case two
candidates). Section 7, develop several exact algorithms special cases Combinatorial Shift Bribery, Section 8 describe approximation algorithms
Combinatorial Shift Bribery. proofs available appendices
(either given proof relies ideas already presented proofs, oras
case Theorem 9when proof particularly involved). end conclusions
Section 9.
606

fiCombinatorial Shift Bribery

2. Preliminaries
section, briefly describe model elections, define two voting rules
study, review basic concepts parameterized complexity.
2.1 Elections
election E = (C, V ) consists set C = {c1 , . . . , cm } candidates collection
V = (v1 , . . . , vn ) voters. voter represented preference order, is,
linear ranking candidates preferred one least preferred one;
use voters preference orders interchangeably. example, C = {c1 , c2 , c3 },
voter v1 may preference order v1 : c1 c2 c3 indicate likes c1 best,
c2 , c3 (for clarity, treat voters females candidates males).
assume arbitrary (but fixed) canonical order set candidates (for example, one could order candidates lexicographically names).


subset C candidates, writing within preference order means listing

didates canonical order, writing means listing reverse
order.
2.2 Voting Rules
voting rule R function that, given election E = (C, V ), outputs set R(E) C
(tied) election winners. candidate c R(E) said R-winner
election E. consider two election rules, Plurality rule Borda rule.
assign points candidates output highest score. Plurality
rule, candidate receives one point voter ranks first. Borda
rule, candidate receives points voter prefers candidate exactly
ones.
use nonunique-winner model. is, candidates selected given
voting rule viewed equally successful winners (in practice, course, one use
sort tie-breaking rule resolve situation, disregarding ties simplifies
analysis; however, interested reader consult papers effects tie-breaking
complexity election problems, e.g. Obraztsova & Elkind, 2011; Obraztsova, Elkind,
& Hazon, 2011).
2.3 Parameterized Complexity
assume familiarity standard notions regarding algorithms complexity theory,
briefly review notions regarding parameterized complexity theory (Downey & Fellows,
2013; Flum & Grohe, 2006; Niedermeier, 2006).
parameterized complexity theory measure complexity given problem
respect input size particular parameter problem. Typical
parameters election problems include number candidates, number voters,
solution size (for example, number campaign actions one perform; see
Betzler, Bredereck, Chen, & Niedermeier, 2012, survey parameterized complexity
voting). say parameterized problem fixed-parameter tractable (is FPT)
algorithm given input instance parameter k solves problem
607

fiBredereck, Faliszewski, Niedermeier, & Talmon

g(k)|I|O(1) time, g computable function |I| length encoding
I. also hierarchy hardness classes parameterized problems,
two important levels formed classes W[1] W[2]. convenient
way defining classes appropriate reduction notion complete
problems. Specifically, say parameterized problem reduces parameterized
problem B two computable functions, h h0 , following properties:
given instance parameter k, h(I) outputs FPT time (i.e., time g(k)|I|O(1)
computable function g) instance 0 B parameter k 0 h0 (k),
yes-instance 0 yes-instance B. words, h
many-one reduction B allowed run FPT time, required
output instance whose parameter upper-bounded function input instances
parameter.
class W[1] defined class problems parameterically reduce
Clique problem, W[2] class problems parameterically reduce Set
Cover problem, problems parameterized solution size (that is,
value h definitions).
Clique
Input: undirected graph G = (V (G), E(G)) integer h.
Question: set H h vertices edge
pair vertices H?
Set Cover
Input: universe set X, family subsets X, integer h.
Question: subset 0 h subsets whose union gives X?
sometimes consider special variants problems describe detail
within relevant proofs.
parameterized problem contained class XP algorithm that, given
instance parameter k, solves time |I|g(k) , g computable
function. holds FPT W[1] W[2] XP. point readers interested
details regarding parameterized complexity theory (and design parameterized
algorithms) textbooks Downey Fellows (2013), Flum Grohe (2006),
Niedermeier (2006).

3. Combinatorial Shift Bribery Problem
section first define Combinatorial Shift Bribery problem full
generality and, then, describe simplify remainder
study.
3.1 Definition
Let R voting rule. definition R-Combinatorial Shift Bribery somewhat involved, therefore first define necessary components. given election
E = (C, V ) preferred candidate p C. goal ensure p R-winner
election. end, number possible actions choose from.
608

fiCombinatorial Shift Bribery

Let := |C| number candidates E let n := |V | number
voters. shift action f n-dimensional vector (possibly negative) integers,
f = (f (1) , . . . , f (n) ). R-Combinatorial Shift Bribery given family F =
(f1 , . . . , f ) shift actions. particular shift action models possible campaigning
action, airing TV spot organizing meeting voters. components
given shift action measure effects action particular voters. given
subset F 0 F available shift actions, define effect F 0 voter vi (1 n)
P
(i)
E (i) (F 0 ) = fj F 0 fj . Further, shift action fj (1 j ) comes nonnegative
integer cost w(fj ) application.
voter vi (1 n) individual threshold function : Z Z describing
shift actions affect voter. require (0) = 0 nondecreasing.
Let F 0 collection shift actions. applying shift actions F 0 , voter vi
(1 n) shifts preferred candidate p > 0 positions forward (a) E (i) (F 0 ) > 0,
(b) (t) E (i) (F 0 ) < (t + 1). shift > 0 positions back (a) E (i) (F 0 ) < 0,
(b) (t) E (i) (F 0 ) > (t 1).
Finally, given nonnegative integer B, budget.PWe ask existence
collection F 0 F available shift actions total cost fj F 0 w(fj ) B
applying p R-winner given election. case,
say F 0 successful. Consider following example.
Example 1. Consider election below, set candidates C = {a, b, c, p},
collection voters V = (v1 , v2 , v3 ), p preferred candidate. three
available shift actions, unit cost (i.e., w(f1 ) = w(f2 ) = w(f3 ) = 1).
election
v1 : c b p
v2 : b c p
v3 : p b c

shift actions

2
6
0

4 0 2

0
3
0
f1

f2

f3

threshold functions that:
1. 1 (1) = 4, 1 (0) = 0, 1 (1) = 6, 1 (2) = 100.
2. 2 (0) = 0, 2 (1) = 2, 2 (2) = 2 (3) = 100.
3. 3 (3) = 3 (2) = 100, 3 (1) = 3, 3 (0) = 0.
use Borda rule. Candidates a, b, c, p have, respectively, 4, 6, 4, 4 points.
easy see applying single shift action ensure ps victory. However,
applying shift actions F 0 = {f2 , f3 } results p winner. total effect
two shift actions (6, 2, 3). According threshold functions, means p
shifted forward one position v1 v2 , shifted back one position v3 .
shifts, modified election looks follows:
609

fiBredereck, Faliszewski, Niedermeier, & Talmon

v10
v20
v30

election
:cpba
:bapc
:apbc

is, apply shift actions F 0 = {f2 , f3 }, candidate c
3 points, candidates 5 points each. Thus, a, b, p tied winners
F 0 indeed successful set shift actions.
4
Formally, given voting rule R, define R-Combinatorial Shift Bribery
problem follows:
R-Combinatorial Shift Bribery
Input: election E = (C, V ), C = {c1 , . . . , cm } set candidates
V = (v1 , . . . , vn ) collection voters, set F = {f1 , . . . , f } shift
actions costs w(f1 ), . . . , w(f ), threshold functions 1 , . . . , n , nonnegative integer budget B. One candidates designated preferred
candidate p.
Question: subset F 0 F shift actions total cost B
apply shift actions F 0 candidate p R-winner
resulting election?
definition quite complicated, captures important features campaigning. example, use threshold functions allows us model voters
unwilling change position preferred candidate beyond certain range, irrespective strength campaign. fact different shift actions different
costs models fact particular actions (for example, airing TV spots organizing
meetings) may come different costs.
3.2 Relation Standard Shift Bribery
necessary comment relation Combinatorial Shift Bribery
problem non-combinatorial variant, Shift Bribery (Elkind et al., 2009; Elkind &
Faliszewski, 2010).
non-combinatorial variant Shift Bribery problem defined similarly
combinatorial one, voters threshold functions instead
collection shift actions costs, voter vi shift-bribery price
function . cost shifting preferred candidate forward positions vi
preference order (t) (only forward shifts allowed). require (0) = 0
functions nondecreasing. Formally, following definition (R
voting rule).
R-Shift Bribery
Input: election E = (C, V ), C = {c1 , . . . , cm } set candidates V = (v1 , . . . , vn ) collection voters, collection (1 , . . . , n )
shift-bribery price functions, nonnegative integer budget B. One
610

fiCombinatorial Shift Bribery

candidates designated preferred candidate p.
Question:
vector (s1 , . . . , sn ) natural numbers (a)
Pn
i=1 (si ) B (b) voter vi shift p forward si positions,
p R-winner resulting election?
intuitively seems R-Shift Bribery simpler combinatorial
cousin, making observation formal requires care.
Proposition 1. Let R voting rule. holds R-Shift Bribery many-one reduces
R-Combinatorial Shift Bribery polynomial time.
Proof. Consider instance R-Shift Bribery election E = (C, V ),
C = {c1 , . . . , cm } V = (v1 , . . . , vn ), collection shift-bribery price functions
(v1 , . . . , vn ), budget B. Without loss generality, take c1 preferred
candidate denote p. form instance R-Combinatorial Shift
Bribery election, budget, preferred candidate,
shift actions, costs, voters threshold functions constructed

Pt
mj
,
follows: voter vi , set threshold function (t) = j=1 2
number positions possible shift preferred candidate
forward vi preference order, create shift action fi,t zero effect
voters vi , effect 2mt ; cost
P fi,t w(fi,t ) = (t) (t 1).
sequence (s1 , . . . , sn ) ni=1 (si ) B p R-winner
election voter vi shift p forward si positions, also
solution constructed instance Combinatorial Shift Bribery: vi ,
use shift actions fi,1 , . . . , fi,si , total bribery cost Shift
Bribery instance and, implementing shifts, vi preferred candidate
shifted exactly si positions.
assume constructed Combinatorial Shift Bribery instance yesinstance. Consider subset F 0 shift actions whose total cost B
ensure p R-winner election (and recall shift action used
once). voter vi V , define si largest integer shift
actions fi,1 , . . . , fi,si belong F 0 . Let us fix voter vi . claim applying
shift actions F 0 (in Combinatorial Shift Bribery instance), preferred
candidate shifted forward exactly si positions. definition si , immediate
shifted forward least si positions. shifted forward
positions
following reason: shift actions fi,1 , . . . , fi,si total effect
P mj
vi equal sj=1
2
, equal (si ). definition, shift action fi,si +1
F 0 . sum remaining shift actions effect vi smaller than:

X
j=si +2

mj

2

=

ms
2
X

2j = 2msi 1 1.

j=0

However, (si + 1) (si ) = 2msi 1 . means even used shift actions
aside fi,si +1 , vi preference order still would shift p exactly si positions.
conclusion, means implementing shift actions F 0 ensures
voter vi shift p forward exactly si positions. Further, vi
611

fiBredereck, Faliszewski, Niedermeier, & Talmon

w(fi,1 ) + + w(fi,si ) = (si ). Therefore, sequence (s1 , . . . , sn ) witnesses
input instance Shift Bribery yes-instance total cost shifts
B (as combinatorial instance) ensure p winner (as
combinatorial instance).
Since reduction clearly runs polynomial time, proof complete.
construction proof somewhat involved, especially one takes
account simply shows Combinatorial Shift Bribery problem indeed
generalizes much simpler, non-combinatorial, one. Nonetheless, somewhat contrived
use threshold functions seems necessary. Indeed, Combinatorial Shift
Bribery problem restricted shift actions positive entries exactly one
voter each, used simple linear threshold functions, would obtain Shift
Bribery case convex price functions (Bredereck et al., 2014a).
general variant Shift Bribery problem which, example, NP-hardness
results Elkind et al. (2009) hold (as shown Bredereck et al., 2014a), nonetheless
general one.
3.3 General Hardness Result
turns Combinatorial Shift Bribery problem, defined Section 3.1
above, general allows following, sweeping, hardness result.2
Theorem 2. Plurality rule Borda rule, Combinatorial Shift
Bribery NP-hard even five voters two candidates budget constraints.
Borda rule, Combinatorial Shift Bribery NP-hard also three voters
four candidates.
Proof. reduce following (weakly NP-hard) variant Subset Sum problem
(it simple exercise show NP-hardness reduction classic Subset
Sum problem):
Subset Sum (Zero Variant)
Input: set := {a1 , . . . , } integers.
P
Question: nonempty set A0 ai A0 ai = 0?
Given instance = {a1 , . . . , } Subset Sum (Zero Variant), construct
instance Plurality-Combinatorial Shift Bribery two candidates. Since
Plurality rule Borda rule coincide elections two candidates, hardness
result transfers Borda-Combinatorial Shift Bribery (and, fact, almost
natural voting rules).
construct following election:
2. Note, however, prove weak NP-hardness. is, result may hold assume
occurring numbers encoded unary. contrary, hardness proofs paper give
strong hardness results independent number encoding issues.

612

fiCombinatorial Shift Bribery

election
v1 : p
v2 : p
v3 : p
v4 : p
v5 : p

shift actions



a1





a1









1 ... 1




0
0




0
0


f1

...

fn

is, element ai A, set F shift actions contains one shift action fi
effect ai v1 , effect ai v2 , effect 1 v3 , effect two voters.
voter threshold functions follows. Candidate p shifted last position
v1 v2 effect voters negative (that is, 1 (1) = 2 (1) = 1).
Candidate p shifted top position third voter effect positive (that
is, 3 (1) = 1). set cost shift action one set budget
n. Thus budget allows us pick combination shift actions.
direction, let A0 non-empty subset whose element-wise sum equals
zero. applying F 0 := {fi | ai A0 }, p winner: Since A0 sums zero,
effect first two voters. effect third voter positive, A0
non-empty. Thus p preferred three five voters wins election.
direction, let F 0 F subset shift actions makes p winner.
Then, F 0 must non-empty p win initial election. claim
element-wise
sum A0 := {ai | fi F 0 } zero. sake contradiction, assume
P
ai A0 ai 6= 0. sum negative, would negative effect
first voter, would preferred three voters five, would win election.
sum positive, would effect second voter taking
role first one.
Using similar idea, show reduce Subset Sum (Zero Variant)
Borda-Combinatorial Shift Bribery three voters four candidates. Given
input before, construct following instance:
election
v1 : p d1 d2 d3
v2 : p d1 d2 d3
v3 : d1 d2 d3 p

shift actions



3a1
3an




3a1 . . . 3an




3
3


f1

...

fn

is, element ai A, F contains one shift action fi effect 3ai v1 ,
effect 3ai v2 , effect 3 v3 . voter vi threshold function
(t) = t. effect, p shifted last position first second voter
effect voters negative, shifted top position third vote
effect positive. shift action unit cost, set budget
n (i.e., pick combination shift actions).
613

fiBredereck, Faliszewski, Niedermeier, & Talmon

Observe d1 original winner election obtains seven points whereas
p obtains six points.
direction, let A0 non-empty subset whose element-wise sum equals
zero. apply shift actions F 0 := {fi | ai A0 } p becomes winner: Since A0 sums
zero, effect first two voters. effect third voter positive
A0 non-empty. Thus, p preferred candidate voters
wins election.
direction, let F 0 F subset shift actions makes p winner.
Then, F 0 must non-empty p win initial election. show
element-wise
sum A0 := {ai | fi F 0 } zero. sake contradiction assume
P
ai A0 ai 6= 0. sum negative, would negative effect
first voter p would obtain six points, whereas d1 would obtain seven. sum
positive, would effect roles first second voter
switched.
Effectively, Theorem 2 shows studying large-scale effects campaign actions
full-fledged R-Combinatorial Shift Bribery problem leads hopelessly
intractable problem: hardness even elections fixed number
candidates fixed number voters.
3.4 Restricted Variants Combinatorial Shift Bribery
Given hardness results Theorem 2, throughout remainder paper
focus restricted variants Combinatorial Shift Bribery problem. assume
individual threshold functions identity functions (that is, voter
integer t, holds (t) = t), assume shift action unit
cost, consider restricted types shift actions. assumptions require
additional discussion.
restrictions threshold functions costs shift actions seem
basic and, fact, even satisfied instances built proof Theorem 2. reason assuming that, one hand, seems beyond point
study instances involved Theorem 2, and, hand,
interact restrictions, leading tractable cases. But, important
consequences.
First, using identity threshold functions means model societies prone
propaganda. identity threshold functions cannot differentiate voters
less responsive actions. Second, assuming every shift action
unit cost models settings costs particular campaign actions similar
enough small differences irrelevant; actual number actions
choose perform sufficiently good approximation real cost. true,
example, case organizing meetings voters, often comparable
prices. also likely case shift actions model actions airing TV
spots: spot similar cost produce/broadcast. greatest disadvantage
assuming unit costs longer model mixed campaigns use actions
several different types (meetings voters, TV spots, web campaigns, etc.).
614

fiCombinatorial Shift Bribery

restrictions types allowed shift actions even greater impact
nature campaigns study. study following classes shift actions:
Unrestricted Shift Actions. put restrictions allowed shift actions;
models general (and, naturally, least tractable) setting.
Bounded-Effect Shift Actions. consider parameter require
shift action f = (f (1) , . . . , f (n) ) holds j (1 j n),
|f (j) | . still general setting, assume campaigning
action limited impact voter.
Unit-Effect Shift Actions. class bounded-effect shift actions = 1.
given voter, applying given shift action either leave preferred candidate
p unaffected shift p one position down.
Interval Shift Actions. subclass unit-effect shift actions never affect
voters negatively, shift action interval voters
affected positively (the interval respect order voters
input collection V ). class shift actions models campaigns associated
time window certain voters reached, campaigns local given
neighborhoods3 (for example, include putting multiple posters, organizing
meetings, etc.). speak 1z -interval shift actions mean interval shift actions
shift action affects z voters.
Unit-Effect Two Voters Shift Actions. subclass unit-effect shift actions
affect two voters most. focus shift actions affect voters
positively, denoted (+1, +1)-shift actions, affect one voter positively
one voter negatively, denoted (+1, 1)-shift actions. reason studying
families model particularly natural types election campaigns,
rather establish limits tractability problem. example,
consider (+1, 1)-shift actions understand intractable shift actions
negative effects; (+1, 1)-shift actions simplest shift actions type
may useful campaign (one would never deliberately use shift action
affects preferred candidate negatively).

Figure 1 presents difference bounded-effect shift actions, unit-effect shift
actions, unit-effect two voters shift actions, interval shift actions graphically.
discuss next section, type allowed shift actions huge impact
computational complexity problem.
3. neighborhood scenario, take simplified view society voters lives line.
course, would natural take two-dimensional neighborhoods account. view
interesting direction future research, time consider simple settings
possible. time window scenario, natural ordering voters point time
cast votes affected campaign.

615

fiBredereck, Faliszewski, Niedermeier, & Talmon

2

2
1

1

1
2

1
1

1

1

1

1

1

= 2; = 5
1

1

1

2

Unit-Effect
1

1

1
z
1
1

(+1, 1)

1
1

(+1, +1)

1

1z

Figure 1: Restrictions shift actions. visualize (from left right, top bottom):
shift action maximum effect = 2 single shift action maximum number
= 5 voters affected single shift action; unit-effect shift action; shift action
effect +1 one voter effect 1 another voter (+1, 1); shift action
effect +1 two voters (+1, +1); shift action effect +1 interval
size z 1z . intended interpretation voters listed vertically, top
bottom.

4. Overview Results
provide high-level overview results. turns even rather strong
restrictions place (that is, restrictions defined Section 3.4), Combinatorial Shift
Bribery computationally hard settings. present quest
understanding border tractability intractability Combinatorial Shift
Bribery. end, employ following techniques ideas.
1. seek regular complexity results (NP-hardness results) parameterized
complexity results (FPT algorithms, W[1]-hardness W[2]-hardness results,
XP algorithms).
2. consider structural restrictions sets available shift actions.
3. seek approximation algorithms inapproximability results (that is, approximation hardness results).
616

fiCombinatorial Shift Bribery

parameterized complexity results, consider following parameters: (a)
number n voters, (b) number candidates, (c) budget B, (d)
maximum effect single shift action, (e) maximum number voters affected
single shift action.
discussions (in)approximability Combinatorial Shift Bribery regard
task minimizing cost ensuring preferred candidates victory. means
that, example, 2-approximation algorithm decide possible ensure
preferred candidates victory all, and, so, output successful set shift
actions total cost twice high optimal one.
summarize results Table 1. results show Combinatorial Shift
Bribery highly intractable. Theorems 5, 6, 7, show problem computationally hard (in terms NP-hardness, W[2]-hardness, inapproximability even FPT
algorithms) Plurality rule Borda rule, even various restricted
forms unit-effect shift actions, even two candidates. means that, essence,
problem hard natural voting rules, since two candidates natural voting rules
boil Plurality rule.
Further, Theorem 8 Theorem 11 show problems W[1]-hard even
take number candidates budget joint parameter, even extremely
restricted shift actions. problem remains hard (for case Borda rule)
parameterized number voters (Theorem 9). contrary, case
Plurality parameterization number voters obtain tractability.
obtain several approximability results. essence, results possible
cases shift actions negative results. intuitive reason fact
shift actions negative effects, computationally hard check
whether preferred candidate win even without restrictions budget.
approximation algorithms based results non-combinatorial
variant problem, due Elkind et al. (2009) Elkind Faliszewski (2010).
Either use non-combinatorial algorithms directly, subroutines algorithms,
derive results plugging Combinatorial Shift Bribery-specific blocks
framework developed Elkind et al. (2009) Elkind Faliszewski (2010).

5. Connection Combinatorial Control
study combinatorial variants problems modeling ways affecting election results
initiated Bulteau et al. (2015), considered combinatorial control adding voters
(Combinatorial-CCAV) Plurality rule Condorcet rule. turns
Plurality rule reduce problem (Combinatorial) CCAV
(Combinatorial) Shift Bribery. non-combinatorial variants problems
give much since problems easily seen polynomial-time solvable.
However, strong hardness results Plurality-Combinatorial-CCAV
transfer case Plurality-Combinatorial Shift Bribery. Formally, PluralityCombinatorial-CCAV defined follows (Bulteau et al., 2015).
Plurality-Combinatorial-CCAV
Input: set C candidates preferred candidate p C, collection V
617

fiBredereck, Faliszewski, Niedermeier, & Talmon

Table 1: Overview results. show exact algorithms approximation algorithms Plurality-Combinatorial Shift Bribery Borda-Combinatorial
Shift Bribery, different restrictions shift actions (see Figure 1). Results marked
follow work Elkind et al. (2009), follow work Bredereck
et al. (2014a), follow work Elkind Faliszewski (2010), follow
work Bredereck et al. (2016). Note variants XP
parameterized budget B (Observation 1).
shift actions

regular
Shift Bribery
(convex prices)

rule

exact complexity

approximability

Plurality

poly.-time solvable (O)



Borda

NP-complete(O),
FPT B (),
W[1]-hard n ()

unit effect

2-approximable
poly. time (,O),
FPT-approximation
scheme n ()

W[2]-h B even

inapproximable even

= 2 (Thm. 5),

FPT-time B even

XP n (Prop. 12)

= 2 (Thm. 6)

Plurality

FPT n (Thm. 13)



Borda

W[1]-hard n (Thm. 9)



inapproximable even
FPT-time n (Cor. 10)

(+1, 1)
NP-h even = 2 (Thm. 7),


W[1]-h B
combined (Thm. 8)

Plurality
(+1, +1)

1z -intervals



inapproximable
even = 2 (Thm. 7)

FPT n (Thm 13)



W[1]-h B

2-approximable

combined (Thm. 8)

poly. time (Thm. 15)

Plurality

FPT n (Thm. 13)

Borda





W[1]-h B (Thm. 11)

618

z-approximable
poly. time (Thm. 14)
2z-approximable
poly. time (Thm. 14)
2-approximable
mz time (Thm. 16)

fiCombinatorial Shift Bribery

registered voters (having preference orders C), collection W unregistered voters (having preference orders C), bundling function : W 2W
(for w W holds w (w)), budget k.
Question: collection W 0 W
k voters p

0
winner modified election (C, V w0 W 0 (w ))?
Intuitively, unregistered voter w W , bundle, (w) (given explicitly
input), add w election (for example, somehow convincing
vote), voters bundle also join election (for example, people choose
vote influence friend).
Theorem 3. Plurality-Combinatorial-CCAV polynomial-time many-one reducible
Plurality-Combinatorial Shift Bribery. instance Plurality-CombinatorialCCAV candidates, reduction outputs instance Plurality-Combinatorial
Shift Bribery + 1 candidates.
Proof. Consider input instance Plurality-Combinatorial-CCAV candidate set
C, collection registered voters V , collection unregistered voters W , bundling function
, preferred candidate p C, limit k number voters add.
form instance Plurality-Combinatorial Shift Bribery, follows.
form candidate set C 0 = C {d}, new candidate. form
set voters V 0 following way.
1. voter v V , include v V 0 , preference orders extended rank
last.
2. voter w W ranks p first, include V 0 two voters, xw ,
preference order form p , x0w , preference order form
p .
3. voter w W ranks candidate c C \ {p} first, include
V 0 voter xw preference order p c , voter x0w preference order
p .
4. include 4|W ||C| voters V 0 preference orders achieve
following effects: (a) c C score s(c) election (C, V ), c ranked first
4|W | + s(c) voters V 0 , (b) ranked first exactly 2|W | voters V 0 .
achieve effects, c C \ {p} include 4|W | voters rank c first,
include 3|W | voters rank p first, include |W | voters rank first.
voter w W , introduce shift action fw following effects:
(w), w0 ranks p first fw effect 1 xw0 (but x0w0 ) w0 ranks
candidate C \ {p} first, fw effect 1 xw0 effect +1 x0w0 (all
entries zeros). finishes construction. provide proof correctness
following example reduction.

w0

Example 2. Consider following input Plurality-Combinatorial-CCAV,
preferred candidate p budget k 1.
619

fiBredereck, Faliszewski, Niedermeier, & Talmon

registered voters
v1 : p

unregistered voters
w1 : p

bundling function
(w1 ) = {w1 , w3 }

v2 : p

w2 : p

(w2 ) = {w2 }

v3 : p

w3 : p

(w3 ) = {w2 , w3 }

construct following input Plurality-Combinatorial Shift Bribery; notice
number entries shift action 33.
election
v1 : p
v2 : p
v3 : p
xw1 : p
x0w1 : p
xw2 : p
x0w2 : p
xw3 : p
x0w3 : p
12 dummies :
9 dummies : p
3 dummies :

shift actions

0
0
0

0 0 0

0 0 0


1 0 0


0 0 0

0 1 1


0 1 1


1 0 1

0 0 0


0 0 0


0 0 0
0
0
0
fw1

fw2

fw3

Note adding voter w1 input election Plurality-Combinatorial-CCAV
results p winner election. Correspondingly, applying shift action fw1 results
p winner input election Plurality-Combinatorial Shift Bribery. 4
see correctness construction, note applying shift action corresponding bundle voter w W effect differences scores
candidates C adding bundle (w) original control instance.
specifically, disregarding score now, following.
w0 (w) ranks p first, increase score p one,
w0 (w) ranks candidate c C \ {p} first, increase score
c one. Further, score candidate never grow beyond 4|W | PluralityCombinatorial Shift Bribery instance score p never fall 4|W |.
Therefore, never prevent p winner.
Thus, reduction correct. Furthermore, reduction computed polynomial time outputs Plurality-Combinatorial Shift Bribery instance one
candidate input Plurality-Combinatorial-CCAV instance. also observe output instance uses unit-effect shift actions affect twice
many voters largest bundle input instance.
620

fiCombinatorial Shift Bribery

Based proof Theorem 3 results Bulteau et al. (2015), obtain
following result.
Corollary 4. Plurality-Combinatorial Shift Bribery W [2]-hard respect
budget B even = 3, W [1]-hard respect B even shift actions unit
effect 6 voters, NP-hard even shift actions unit effects
4 voters.
Proof. result follows applying reduction proof Theorem 3
Plurality-Combinatorial-CCAV instances produced reductions Theorems 2, 1,
4 Bulteau et al. (2015), respectively.

6. Hardness Results
results previous section show bound hit hard instances
Combinatorial Shift Bribery even restricted setting. section explore
restrictive hard settings are. results organized type shift actions
allowed.
6.1 Results General Unit-Effect Shift Actions
start considering unit-effect shift actions. allowed effects positive only,
obtain NP-hardness W[2]-hardness parameterizing budget B.
allow also negative unit-effects, problem gets even harder go beyond
hope approximation algorithm, even approximation algorithm allowed
run FPT time parameterizing budget B. Quite strikingly, results hold
even two candidates.
Theorem 5. Plurality rule Borda rule, Combinatorial Shift
Bribery NP-hard W[2]-hard parameter budget B, even two candidates
even shift action effects either +1 0 voter.
Proof. provide parameterized reduction Set Cover (recall Section 2.3). Let
(S, X, h) instance Set Cover, = {S1 , . . . , Sm } family subsets
universe X = {x1 , . . . , xn }, h number sets use cover X.
construct instance Plurality-Combinatorial Shift Bribery two candidates. Note that, since Borda rule Plurality rule coincide elections two
candidates, hardness result transfers Borda-Combinatorial Shift Bribery.
construction follows. p candidates.
element xi X create element voter vi preference order p. Create another set
n dummy voters preference order p. set F shift actions contains
set Sj function fj effect +1 element voters corresponding
elements set (that is, fj [i] = 1 xi Sj fj [i] = 0 otherwise). Finally,
set B := h. finishes construction. Clearly, reduction computed
polynomial time. Consider following example applying reduction.
Example 3. Let input Set Cover X = {x1 , x2 , x3 , x4 , x5 } =
{S1 , S2 , S3 }, S1 = {1, 2, 5}, S2 = {2, 3}, S3 = {3, 4}, h = 2. construct
following input Plurality-Combinatorial Shift Bribery.
621

fiBredereck, Faliszewski, Niedermeier, & Talmon

election
v1 : p
v2 : p
v3 : p
v4 : p
v5 : p
5 dummies : p

shift actions

0
0
1

1 1 0


0 1 1


0 0 1

1 0 0

0

0

0

f1

f2

f3

Note {S1 , S3 } set cover, and, analogously, choosing f1 f3 results p
winner election.
4
remains show set cover size h successful
set shift actions size h.
part, assume set cover 0 size h. Then, applying
0
F = {fj | Sj 0 } makes p win election: Since 0 set cover, p preferred
candidate n element voters and, hence, winner election.
part, assume set shift actions F 0 F size h
whose application makes p win election. Then, p must preferred candidate
element voters bribed election shift action effect dummy
voter. Since n element voters n dummy voters, 0 := {Sj | fj F 0 } set
cover. Finally, since B = h, 0 size h.
Allowing also negative (but unit) effects voters, adapt reduction
Theorem 5 show strong inapproximability result. inapproximability result follows
since corresponding reduction, yes-instances, correct solutions use
exact given budget.
Theorem 6. Unless W[2] = FPT, Combinatorial Shift Bribery inapproximable (in
FPT time parameter B) Plurality rule Borda rule, even two
candidates unit-effect shift actions.
Proof. modify reduction Theorem 5 show inapproximability result.
Let (S, X, h) Set Cover instance = {S1 , . . . , Sm } X = {x1 , . . . , xn }.
Without loss generality, assume |S| > h. construct instance PluralityCombinatorial Shift Bribery two candidates follows. (Since two
candidates only, proof applies case Borda-Combinatorial Shift Bribery
well.)
|S|
element xi X, create |S| element voters vi1 , . . . , vi , preference
order p, set Sj create set voter vj0 preference order p d.
Create |S| |X| + |S| 2h dummy voters, preference order p. set F
shift actions contains, set Sj , shift action fj effect 1
element voter corresponding element set effect 1 set voter
corresponding set. Finally, set B := h. completes construction,
clearly computable polynomial time.
622

fiCombinatorial Shift Bribery

Next, show successful set shift actions size h
set cover size h.
part, assume set cover 0 size h. Then, F 0 =
{fj | Sj 0 } successful set shift actions: since 0 set cover, p
preferred candidate |S| |X| element voters also preferred candidate
least |S| h set voters (corresponding sets set cover). Moreover,
preferred candidate |S| |X| + |S| 2h dummy voters also preferred
candidate h set voters (corresponding sets set cover). Hence,
either p wins p tie winners.
part, assume successful set shift actions F 0 F
size h. Then, p must preferred candidate element voters
bribed election: element voter p, would least
|S| 1 element voters p (the element voters corresponding
element). Thus would total |S|(|X| 1) element voters |S| set
voters prefer p, least |S| |X| + |S| 2h dummy voters |S| element voters
prefer d. Since assumed |S| > h, would mean p winner. Thus,
must 0 := {Sj | fj F 0 } set cover, and, due budget constraint,
follows |S 0 | h.
Finally, show Plurality-Combinatorial Shift Bribery inapproximable
even FPT time parameterized budget. Assume, sake contradiction, successful set shift actions F 0 F |F 0 | > B exists. Then, bribed
election, least |S| |X| + |S| 2h dummy voters also |F 0 | h + 1 set voters prefer d,
|S| |X| element voters |S| (h + 1) set voters prefer p. Thus,
unique winner. Hence, successful bribery action must optimal respect
budget FPT-algorithm Plurality-Combinatorial Shift Bribery (parameterized budget) would solve W[2]-hard problem Set Cover (parameterized
solution size) FPT time; contradiction assumption FPT 6= W[2].
6.2 Results Shift Actions Unit Effect Two Voters
previous section limit number voters affected shift action.
focus case unit-effect shift action affect two voters. First show Combinatorial Shift Bribery remains NP-hard hard
approximate (+1, 1)-shift actions. provide parameterized hardness results
(+1, 1) (+1, +1)-shift actions. proof relatively similar one
Theorem 6 defer Appendix A.
Theorem 7. Unless P = NP, Combinatorial Shift Bribery inapproximable (in
polynomial time) Plurality rule Borda rule, even two candidates
(+1, 1)-shift actions.
opposed Theorem 6, result yield W[2]-hardness parameter budget B. proof uses reduction Set Cover
value budget size universe set X. insist parameterized
hardness unit effects two voters, accept larger sets candidates.
However, increase large: show W[1]-hardness Combinatorial
Shift Bribery jointly parameterized budget number candidates.
623

fiBredereck, Faliszewski, Niedermeier, & Talmon

Theorem 8. Plurality rule Borda rule, Combinatorial Shift
Bribery W[1]-hard combined parameter (m, B), even either
(+1, 1)-shift actions (+1, +1)-shift actions.
Proof. four cases consider. begin Plurality rule (+1, +1)-shift
actions.
Plurality Rule (+1, +1)-Shift Actions. describe parameterized reduction W[1]-hard Clique problem, parameterized solution size, PluralityCombinatorial Shift Bribery (+1, +1)-shift actions, parameterized (m, B).
Let (G, h) instance Clique V (G) = {u1 , . . . , un0 } E(G) = {e1 , . . . , em0 }.
create following instance Plurality-Combinatorial Shift Bribery. set
candidates {p} D, = {d1 , . . . , dh1 }. vertex ui V (G), cre

ate vertex voter vi preference order p. Moreover, create n0 2h dummy


voters preference order p each. edge {ui , uj } E(G), create
shift action f{ui ,uj } effect 1 vertex voters vi vj , effect 0

voters. Finally, set budget B := h2 . completes construction,
computable polynomial time. Consider following example.
Example 4. following graph, looking clique size h = 3.
u2

u3

u5

u4

u1

u7

u6

construct following input Plurality-Combinatorial Shift Bribery.
election
v1 : d1 d2 p
v2 : d1 d2 p
v3 : d1 d2 p
v4 : d1 d2 p
v5 : d1 d2 p
v6 : d1 d2 p
v7 : d1 d2 p
1 dummy : p d1 d2

shift actions

1
1
1

1 0 0


0 0 0

0 1 0


0 0 1


0 0 0

0 0 0

0
0
0


1

0


0

0


0


0

1

0


0

0


0

0


1


1

0

0


0

0


0

0


1


0

1

0

fu1 ,u2

fu1 ,u7

fu5 ,u6

fu5 ,u7

fu1 ,u4

fu1 ,u5

Note (v1 , v5 , v7 ) form clique size 3 input graph Clique, and, accordingly, applying set shift actions {fu1 ,u5 , fu1 ,u7 , fu5 ,u7 } results p winner
election Plurality-Combinatorial Shift Bribery.
4
624

fiCombinatorial Shift Bribery

Without loss generality, assume d1 ranked first (arbitrary fixed)


order D. Observe n0 vertex voters h dummy voters rank d1 first.
also n0 h dummy voters rank p first. Hence, make p win election,
one needs h additional voters rank p first (and, effect, rank d1 first).
remains show constructed instance contains successful set shift actions F 0 size h (G, h) contains clique size h.
part, let H V (G) set h vertices forming clique let E 0 E(G)
set edges vertices H. Then, observe F 0 = {f{ui ,uj } |
{ui , uj } E 0 } successful set shift actions: vertex voter vi corresponding
clique vertex ui H, candidate p shifted h 1 positions forward. means that,
total, h vertex voters rank p first p ties winner election.
part, let F 0 successful set shift actions. Since dummy voters
affected shift action, follows order make p winner election,
p must shifted top position least h vertex voters. is, total, p must
shifted h (h 1) positions forward. Since F 0 size B = h2 = h (h 1)/2

shift action affects two vertex voters, F 0 must size exactly h2 affecting

exactly h vertex voters. construction, implies h2 edges G incident
exactly h different vertices possible h vertices form clique.
finishes proof Plurality rule (+1, +1)-shift actions.
remaining cases proof quite similar (although, technically, involved)
present Appendix B.
quite natural consider Combinatorial Shift Bribery also different
perspective. Instead asking happens small number candidates, might
ask complexity Combinatorial Shift Bribery small number voters
(see, example, Brandt, Harrenstein, Kardel, & Seedig, 2013; Chen et al., 2015,
motivation looking elections voters interesting). case
obtain hardness Borda rule. Indeed, later show PluralityCombinatorial Shift Bribery FPT parameter number voters. proof
next theorem quite involved available Appendix C.
Theorem 9. Borda-Combinatorial Shift Bribery W[1]-hard respect number n voters, even (+1, 1)-shift actions budget constraints.
proof Theorem 9 reduce Strongly Regular Multicolored
Clique problem, and, importantly, impose budget constraints. Thus, follows approximation algorithm Borda-Combinatorial Shift Bribery (running FPT time parameterized number voters) would yield FPT algorithm Strongly Regular Multicolored Clique parameterized
solution size. effect, following corollary.
Corollary 10. Unless W[1] = FPT, Borda-Combinatorial Shift Bribery inapproximable even FPT-time parameter n, even (+1, 1)-shift actions.
results Theorem 9 Corollary 10 compare interestingly
non-combinatorial variant Borda-Shift Bribery. recently, complexity
Borda-Shift Bribery parameterized number voters unknown. Eventually
625

fiBredereck, Faliszewski, Niedermeier, & Talmon

(in different paper, submitting one journal publication) shown
problem W[1]-hard (Bredereck et al., 2016), far simpler proof
one used here. Nonetheless, Theorem 9 Corollary 10 still carry significant value.
Earlier, Bredereck et al. (2014a) shown FPT approximation scheme
Borda-Shift Bribery parameterized number voters, Corollary 10 shows
result generalize combinatorial setting.
6.3 Results Interval Shift Actions
conclude discussion hardness results considering Combinatorial Shift
Bribery interval shift actions. previous section allowed shift actions
non-zero effects two voters each, two voters could chosen arbitrarily. show hardness result case positively affect multiple
voters, voters form consecutive interval input election.
Theorem 11. Plurality rule Borda rule, Combinatorial Shift
Bribery NP-hard even interval shift actions.
Proof. consider Plurality rule first give many-one reduction following
variant strongly NP-hard Numerical Matching Target Sums problem.
Numerical Matching Target Sums
Input: Three sets integers = {a1 , . . . , }, B = {b1 , . . . , bt }, X =
{x1 , . . . , xt }, (1) numbers encoded unary, (2) 3t numbers
distinct, (3) two numbers B sum
number X.
Question: elements B paired [t]
sum ith pair exactly xi ?
standard variant problem, presented classic text Garey
Johnson (1979), restrictions integers sets A, B, X.
assume numbers encoded unary problem strongly NPhard. Further, Hulett, Will, Woeginger (2008) shown problem remains
NP-hard case 3t integers distinct. Finally, see third
restriction change complexity problem suffices consider following
transformation: Given instance (A, B, X) Numerical Matching Target
Sums, add 2 max(A B X) + 1 integer B X. produces
equivalent instance two numbers, B, sum
number X.
Plurality Rule. Let (A, B, X) instance Numerical Matching
Target Sums let denote largest integer B X. create instance
Plurality-Combinatorial Shift Bribery follows. set candidates is:
C := {p, d, ca1 , . . . , cat , cb1 , . . . , cbt , cx1 , . . . , cxt }.
create following voters.
626

fiCombinatorial Shift Bribery

1. pair integers ai x` X, introduce:
(a) One voter preference order

cai p C \ {p, cai },
(b) ai voters preference order

cx` p C \ {p, cx` },
(c) 2y (ai + 1) voters preference order

p C \ {p, d}.
voters called (ai , x` )-voters exactly 2y them.
pair (ai , x` ), construct shift action faxi` effect 1 exactly set (ai , x` )
voters.
2. pair integers bj B x` X, introduce:
(a) One voter preference order

cbj p C \ {p, cbj },
(b) bj voters preference order

cx` p C \ {p, cx` },
(c) 2y (bj + 1) voters preference order

p C \ {p, d}.
voters called (bj , x` )-voters exactly 2y them.
pair (bj , x` ), construct shift action fbxj` effect 1 exactly set (bj , x` )
voters.
3. Let q := 4ty. create sufficiently many dummy voters ensure that, altogether,
candidates following scores:
(a) p q points,
(b) i, cai cbi q + 4ty + 1 points each,
(c) ` [t], cx` q + 4ty + x` points.
shift action affects dummy voters.
627

fiBredereck, Faliszewski, Niedermeier, & Talmon

Finally, set budget B := 2t. completes reduction. easy see
computable polynomial time (because numbers encoded unary)
order voters shift action effects consecutive interval z := 2y
voters.
remains show constructed instance Plurality-Combinatorial Shift
Bribery contains successful set F 0 shift actions size 2t (A, B, X)
yes-instance Numerical Matching Target Sums.
part, let := {(ai1 , bj1 ), . . . , (ait , bjt )} solution Numerical Matching Target Sums, is, set integer pairs integer B
occurs exactly ai` + bj` = x` holds ` [t]. Observe
F 0 := {faxi` , fbxj` | (ai` , bj` ) S} successful set shift actions. Since integer
`

`

B occurs exactly (some pair of) S, candidate cai candidate cbj
loses one point. Since ai` + bj` = x` ` [t], candidate cx` loses x` points.
construction, p gains 4ty points set shift actions size 2t. Thus, p wins
election.
part, let F 0 successful set shift actions size 2t (if
successful action smaller size could extend size 2t shift actions
negative effects). applying shift actions F 0 , p gains 4ty points.
make p winner election, candidate cai candidate cbj needs lose one
point, candidate cx` needs lose x` points. Thus, ai exactly
x`
x`
one fai F 0 bj B exactly one fbj F 0 . Since integers
B X distinct two integers B sum
integer X, x` X least one shift action faxi` effect ai`
`
voters prefer cxl , one shift action fbxj` effect bj` voters prefer cx` . Since
`
candidates cx` |F 0 | = 2t, follows exactly two shift actions
effect voters preferring cx` . Since cx` lose least x` points, holds
ai` + bj` x` . fact, pigeonhole principle, holds ai` + bj` x` . Hence,
successful set 2t shift actions, solution Numerical
Matching Target Sums instance.
Borda Rule. Borda rule, almost reduction works. Specifically,
still exists integer q set requirements required
proof Plurality rule hold Borda rule (with respect different
q). Importantly, since p second position preference profiles
voters, holds score differences, applying shift actions, similar
Plurality rule Borda rule. Thus, proof correctness Plurality rule
transfers Borda rule.

Throughout section shown number hardness results
restrictive assumptions regarding available shift actions. following sections
seek positive algorithmic results.
628

fiCombinatorial Shift Bribery

7. Exact Algorithms
spite pessimism looming previous section, section show two
exact FPT XP algorithms R-Combinatorial Shift Bribery. Then, Section 8,
present several efficient approximation algorithms.
begin observing R-Combinatorial Shift Bribery solved polynomial time, provided assume budget B constant. reason
need choose B shift actions available ones, number shift
actions available upper-bounded input size.
Observation 1. Plurality-Combinatorial Shift Bribery Borda-Combinatorial Shift Bribery XP parameterized budget B.
restrict instances contain bounded-effect shift actions,
show R-Combinatorial Shift Bribery solved polynomial time, provided
number n voters treated constant.
Proposition 12. maximum effect every shift action upper-bounded universal constant, Plurality-Combinatorial Shift Bribery Borda-Combinatorial Shift Bribery XP parameterized number n voters.
Proof. Let value bounding, component-wise, effect shift action. First,
observe (2 + 1)n types different shift actions. Second, observe
one knows budget spent type shift actions, one easily check
whether corresponding set shift actions makes p winner election. Thus use
following algorithm: try possibilities distributing budget B among
(2 + 1)n types shift actions check whether one makes p winner.
so, accept. Otherwise reject.
Proposition 12 holds even shift action comes individual cost
voter individual threshold function, can, given budget, always
select cheapest set shift actions given type. Further, expressing problem
integer linear program (ILP) using famous result Lenstra (1983),
Plurality rule strengthen XP-membership FPT-membership.
Theorem 13. bounded-effect shift actions (where treat bound universal
constant), Plurality-Combinatorial Shift Bribery FPT parameterized
number n voters.
Proof. Given instance Plurality-Combinatorial Shift Bribery n voters,
algorithm proceeds follows. First, guess subset voters
guarantee p ranked first (there 2n guesses try). guessed set voters,
test whether p would winner election p shifted top position
guessed voters ranked first remaining voters. guessed
subset V 0 voters test positive, check whether possible ensure
(by applying shift actions whose cost exceed budget) voters V 0
rank p first. follows.
Let universal constant bounding, component-wise, effect shift action.
Observe (2+1)n types different shift actions. shift action
629

fiBredereck, Faliszewski, Niedermeier, & Talmon

type z, introduce variable xz denoting number times shift action type z
present solution. voter vi , denote svi (p) position p original
preference order vi . voter vi V 0 , add following constraint:
P

P

x
svi (p).
z
[,]
{z:fz effect vi }
ensures p indeed shifted top position vi preference list. add
budget constraint:
X
xz B,
ensuring solution respects budget. Finally, shift action type z add
constraint ensuring use many shift actions type z available
input. finishes description ILP. result Lenstra (1983),
solve ILP FPT time, (2 + 1)n integer variables.
Roughly speaking, Theorem 13 reason Theorem 9 apply
Plurality rule. setting, Plurality-Combinatorial Shift Bribery tractable.
Note Theorem 13 applies case shift action unit cost,
i.e., case focus paper. Nonetheless, believe possible
lift Theorem 13 case shift action individual cost, applying
ideas Bredereck, Faliszewski, Niedermeier, Skowron, Talmon (2015a).

8. Approximation Algorithms
explore possibility finding approximate solutions Combinatorial Shift
Bribery. focus approximating cost shift actions necessary ensure ps
victory (for example, 2-approximate algorithm finds solution ensures ps victory
whenever possible, uses twice many shift actions necessary).
Theorems 6 7, know cannot hope find approximate algorithms
cases Combinatorial Shift Bribery shift actions negative effects.
Thus, section, focus unit-effect shift actions positive effects.
also simplifies situation always check possible ensure ps victory:
suffices apply available shift actions check p winner (indeed,
able perform check heart inapproximability results Section 6).
approximation algorithms proceed either directly invoking algorithms
non-combinatorial variant Shift Bribery Elkind et al. (2009) Elkind
Faliszewski (2010), plugging algorithms framework. start
former approach describe latter.
Theorem 14. shift action effects either 0 1 voter, PluralityCombinatorial Shift Bribery -approximated polynomial-time BordaCombinatorial Shift Bribery 2-approximated polynomial time, denotes maximum number voters affected shift action.
Proof. general idea approximation algorithms split shift action
affects 0 voters 0 shift actions, affecting single voter only. effect
630

fiCombinatorial Shift Bribery

construct non-combinatorial instance Shift Bribery solve exactly,
case Plurality rule, 2-approximately, case Borda rule.
Specifically, construction goes follows. Let (i) denote number shift actions
affecting voter i. Given instance Combinatorial Shift Bribery, form
instance Shift Bribery identical, except instead shift actions,
price functions voters: set price function voter
j (i), shifting p j positions costs j, j > (i), shifting p j positions
costs (2B + 1)j (where B total number shift actions available; note
exponential function (2B + 1)j ensures price functions convex
easily identify situations one shifts p (i) positions).4
describe use construction case Plurality rule
case Borda rule.
Plurality Rule. first translate input instance non-combinatorial
Plurality-Shift Bribery instance described above. Then, apply known, exact,
polynomial-time algorithm Plurality-Shift Bribery (Elkind et al., 2009)
instance. Let cost solution found non-combinatorial instance.
> B, impossible ensure ps victory combinatorial instance (because
number available shift actions insufficient).
B, obtain solution F Plurality-Combinatorial Shift Bribery
instance follows. voter v (non-combinatorial) bribed election
ranks p first, select shift actions combinatorial instance v ranks p first.
Note |F | F indeed (combinatorial) solution.
sake contradiction, assume successful set shift actions F 0
size smaller |F |/. However, easy see set shift actions would
correspond bribery cost smaller non-combinatorial instance. Since
cost optimal solution non-combinatorial instance, contradiction.
Borda Rule. case Borda-Combinatorial Shift Bribery follows analogously, instead using polynomial-time exact algorithm non-combinatorial
instance, use 2-approximation algorithm Borda-Shift Bribery (Elkind et al.,
2009; Elkind & Faliszewski, 2010). Let cost solution found. > 2B,
impossible ensure ps victory.
Otherwise, obtain solution F combinatorial instance, vote v
non-combinatorial solution shifts p positions, include shift actions
affect voter. |F | s, F correct solution combinatorial
instance.
existed solution F 0 combinatorial instance used less |F |/(2)
shift functions, would solution non-combinatorial instance
cost smaller |F |/2 s/2. Since used 2-approximate algorithm noncombinatorial instance, impossible.
mention might possible improve approximation ratio given Theorem 14, least Borda rule. idea might cast problem variant
4. Strictly speaking, need ensure price functions convex, variant
Shift Bribery generalize paper, stick consistency.

631

fiBredereck, Faliszewski, Niedermeier, & Talmon

Set Multicover problem, generalization Set Cover problem
element covering requirement. Then, one could use approximation algorithm Set Multicover problem (for example, one suggested Rajagopalan
& Vazirani, 1998) plug 2-approximation algorithm Elkind Faliszewski
(2010).
achieve better approximation guarantees Borda rule,
restrict allowed shift actions. obtain results use framework Elkind
Faliszewski (2010). essence, shown following: given variant
Shift Bribery, either Plurality rule Borda rule, one provide
function computes obtain highest number points preferred
candidate given budget B, 2-approximation algorithm variant
Shift Bribery.5 Note get-most-points-for-p algorithm solve Shift
Bribery. maximizes score p, ensure candidate receives
higher score. Indeed, optimal solution might increase score p smaller extent,
expense dangerous opponents.
Theorem 15. Borda-Combinatorial Shift Bribery 2-approximable polynomial
time (+1, +1)-shift actions.
Proof. discussion preceding theorem statement, suffices provide function
given instance Combinatorial Shift Bribery budget B finds set
shift actions obtain highest possible number points preferred candidate
p without exceeding budget.
general idea achieving compute maximum b-matching auxiliary
multigraph (multigraphs allow multiple edges vertices). b-matching
multigraph G function b : V (G) N (called covering function) edge-induced
subgraph G vertex u degree b(u). known b-matching
computed polynomial time (Gabow, 1983).
construct auxiliary multigraph G follows. voter vi create vertex
ui . shift action effect 1 voter ui effect 1 voter uj , create edge
{ui , uj }. Then, define covering function b b(ui ) number positions
p shifted forward preference order voter vi (that is, position p
preference order voter vi ).
G b-matching size least B, corresponds set shift actions
increase score p 2B, highest gain possible. G b-matching
size k < B, take shift actions corresponding edges b-matching
(these shift actions maximize number points p gain shift actions
move p within two votes) greedily select shift actions pushes p forward
one vote, use budget (at point, every shift action affect p single
vote only). Thus function computes highest point gain possible p, given
budget.
Next, consider interval shift actions. is, fix order voters
restrict shift action effect voters comprise intervals. (In fact,
5. fact, result applies scoring rules, paper focus Plurality rule
Borda rule only.

632

fiCombinatorial Shift Bribery

could also allow holes inside intervals.) Unfortunately, algorithm requires XP
time parameterization length longest interval.
Theorem 16. Plurality rule Borda rule, Combinatorial Shift
Bribery 2-approximated XP-time interval shift actions, provided take
, upper bound number voters affected shift action, parameter.
Proof. per discussion preceding Theorem 15, suffices describe find set
shift actions maximize number points preferred candidate p gains
given budget.
end, use dynamic programming algorithm. Consider input
Combinatorial Shift Bribery election E = (C, V ), preferred candidate p,
budget B spend increasing ps score. Let := |C| n := |V |.
V = (v1 , . . . , vn ). algorithm uses following table partial results. numbers x, y, s0 , . . . , s1 table entry:
[x, y, s0 , s1 , . . . , s1 ]
denotes maximum number additional points candidate p gain voters v1 , . . . , vx condition (1) exactly shift actions used,
affects voters set {v1 , . . . , vx }, (2) {0, ..., 1},
candidate p shifted position si preference order voter vxi . is, iterate
voters store effect applied shift actions last voters.
size table n B m+1 .
algorithm almost Plurality rule Borda rule.
difference computing scores candidates. Let z, 0 z 1, denote
position p preference order voter (position 0 means p ranked
first). Then, score(z) mean score p gains voter. Plurality
rule score(z) = 1 z = 0 score(z) = 0 otherwise. Borda rule
score(zi ) = zi 1. set voters vector z1 , . . . , zt (for [n] zi
{0, . . . , 1}) denotes positions p preference orders voters,
write score(z1 , . . . , zt ) mean score p gains voters. is:
score(z1 , . . . , zt ) =

X

score(zi ).

i[t]

Given preparation, ready describe algorithm (jointly Plurality
rule Borda rule).
Initialization. initialize entries [, y, s0 , s1 , . . . , s1 ] table follows.
check whether set shift actions effects voters
(v1 , . . . , v ) applying set shift actions moves candidate p positions s0 , . . . , s1 preference orders voters v1 , . . . , v , respectively.
exists, set [, y, s0 , s1 , . . . , s1 ] score(s0 , s1 , . . . , s1 ). Otherwise, set
[, y, s0 , s1 , . . . , s1 ] . (We explain check set shift actions exists
end proof.)
633

fiBredereck, Faliszewski, Niedermeier, & Talmon

Recursion Step. compute table entries [x, y, s0 , s1 , . . . , s1 ] x > , one
compute subsets shift actions (for [y]) whose last affected voter vx , ensure
together yi shift actions whose last affected voter set {v1 , . . . , vx1 }that
j, 0 j 1, p shifted position sj preference order vxj .
specifically, update phase compute x, < x n, y, 0
B, vector (s0 , . . . , s1 ) {0, . . . , 1} table entry [x, y, s0 , s1 , . . . , s1 ]
follows. say vector (s0 , s1 , . . . , s1 ) {0, . . . , m} (x, i)-realizable
(0 y), set shift actions whose last affected voter vx
j, 0 j 1, shifts candidate p sj positions preference
order voter vxj . write R(x, i) denote set vectors {0, . . . , 1}
(x, i)-realizable (we describe compute R(x, i) later). Then, compute
[x, y, s0 , s1 , . . . , s1 ] follows:
[x, y, s0 , s1 , . . . , s1 ] = max{T [x 1, i, , s0 s1 , . . . , s1 s1 , ]
+ score(s0 , s1 , . . . , s1 ) score(s1 s1 , . . . , s1 s1 ) |
0 y, 0 1, (s0 , s1 , . . . , s1 ) R(x, i)}
Informally, realizable total effect shift actions whose last affected voter
vx , number points candidate p gains number additional points
candidate p gains shift actions last affected voter (v1 , . . . , vx1 )
plus number additional points candidate p gains shift actions
last affected voter vx (to avoid double counting, expressed difference
middle line formula).
next show compute R(x, i). try every vector (s0 , . . . , s1 ) {0, . . . ,

1} check (x, i)-realizable. Perhaps easiest way
formulate problem integer linear program (ILP) constant number
variables.
Let (s0 , . . . , s1 ) vector want check (x, i)-realizable.
subset Q {0, . . . , 1}, say shift action type Q affects exactly
voters vxi Q. subset Q, introduce integer variable xQ ,
denoting number shift actions type Q used (x, i)-realization vector.
solve following ILP:
X

xQ =

(1)

xQ{0} =

(2)

Q{0,...,1}

X
Q{1,...,1}

X

j : 0 j 1

xQ = sj

(3)

jQ

(Note middle constraint ensures last affected voter vx .) Since
number variables ILP 2 , follows famous result Lenstra (1983)
ILP solved XP time respect parameter (indeed, even
FPT time). Using ILP without middle constraint, check
vectors (s0 , . . . , s1 ) use initialization step.
634

fiCombinatorial Shift Bribery

Coming back dynamic program, clear finding obtain maximum score p respecting budget found taking maximum
table entries [n, B 0 , s0 , s1 , . . . , s1 ], possible values B 0 , 0 B 0 B,
(s0 , s1 , . . . , s1 ) {0, . . . , 1} .
section showed indeed possible achieve approximation
algorithms special cases Combinatorial Shift Bribery problem,
settings algorithms efficient quite restrictive. means
practice one might want seek good heuristics use algorithms guidance
initial search.

9. Conclusion
defined combinatorial variant Shift Bribery problem (Elkind et al., 2009;
Elkind & Faliszewski, 2010; Bredereck et al., 2014a) studied computational
complexity. motivation research desire understand computational
difficulty imposed correlated, large-scale effects campaign actions. respect,
work motivated combinatorial study election control, studied Bulteau et al.
(2015) Chen et al. (2015). found even various restricted special
cases numerous parameterizations, Combinatorial Shift Bribery problem
highly intractable worst case. Nonetheless, found initial positive results,
mainly form approximation algorithms. Interestingly, approximation results
quite strongly rely results non-combinatorial Shift Bribery.
number research directions motivated work. example, Plurality-Combinatorial Shift Bribery Borda-Combinatorial Shift
Bribery solved polynomial-time (+1, +1) shift actions interval actions
assumption number candidates constant?
generally, results suggest studying restrictions problem.
example, since parameterizing number available shift actions immediately gives
fixed-parameter tractability results, natural question whether natural parameterizations exist could also lead positive results.
Naturally, one might consider voting rules well. interesting Condorcetconsistent rules, Copeland rule, since rules tend behave rather differently
scoring rules. mention results hold voting rules:
specifically, Theorem 2, Theorem 5, Theorem 6, Theorem 7 hold voting rules
theorems hold elections two candidates, voting rules
behave elections two candidates; Observation 1 Theorem 12
basically brute-force algorithms results hold voting rules well;
statements regarding Borda rule Theorem 14, Theorem 15, Theorem 16 hold
scoring rules, since underlying 2-approximation algorithm Elkind Faliszewski
(2010) works scoring rules.
Further, might also interesting consider domain restrictions regarding voters
preferences (for example, single-crossing seems particularly natural context interval
shift actions, since means shift action affects voters somewhat similar
preferences), well-demonstrated restricting domain voters lead
635

fiBredereck, Faliszewski, Niedermeier, & Talmon

tractability (see Theorem 10 Bulteau et al., 2015, example combinatorial
control setting). However, pursuing direction would require careful discussion
shift actions applied. example, allow single-crossing election cease
single-crossing bribery?
Acknowledgments
Robert Bredereck supported DFG project PAWS (NI 369/10). Nimrod Talmon supported DFG Research Training Group Methods Discrete Structures (GRK 1408) currently Weizmann Institute Science. Piotr Faliszewski
supported DFG project PAWS (NI 369/10) AGH University grant
11.11.230.124 (statutory research).
preliminary short version work presented 2015 International
Conference Autonomous Agents Multiagent Systems (AAMAS 15) (Bredereck,
Faliszewski, Niedermeier, & Talmon, 2015b).

Appendix A. Proof Theorem 7
Theorem 7. Unless P = NP, Combinatorial Shift Bribery inapproximable (in
polynomial time) Plurality rule Borda rule, even two candidates
(+1, 1)-shift actions.
Proof. give many-one reduction Set Cover. Let (S, X, h) Set Cover
instance, = {S1 , . . . , Sm } X = {x1 , . . . , xn } (we assume every element
belongs least one set). construct instance Plurality-Combinatorial Shift
Bribery. set budget B := |X|. candidate set {p, d}, p
preferred candidate. element voter vi element xi , preference
order p. set voter vjS set Sj , preference order p d. also
|X| + |S| 2h 1 dummy voters, preference order p. element
xi set Sj , xi Sj construct shift action fji effect +1 vi
effect 1 vjS . completes construction. easy see computable
polynomial time.
Next, show successful set shift actions (note size
set important, is, allow infinite budget) set cover
size h.
part, assume set cover 0 size h. show
build successful set shift actions. start F 0 = element xi ,
choose arbitrary set Sj 0 contains xi add corresponding function
fji F 0 . applying F 0 , observe p becomes winner: |X| element voters
|S| h set voters prefer p |X| + |S| 2h 1 dummy voters h set voters prefer d.
part, assume successful set shift actions F 0 F .
Let h0 number applying shift actions F 0 , p preferred
exactly |S| h0 set voters (that is, shift actions F 0 correspond h0 sets S). p
winner, majority voters (i.e., least |X| + |S| h voters) must prefer p. Thus,
applying F 0 , least X (h h0 ) element voters prefer p. means
collection h0 sets jointly cover least |X| (h h0 ) elements. Since every
636

fiCombinatorial Shift Bribery

element belongs set, extend collection set cover adding
h h0 sets (in worst case, one set uncovered element). proves
set cover (S, X, h) completes part.
Note argumentation made assumptions regarding size F 0 .
Hence, finding solution Plurality-Combinatorial Shift Bribery instance,
including approximate solutions approximation factor, implies finding set cover
size h. means unless P = NP, Plurality-Combinatorial Shift Bribery
inapproximable polynomial time.

Appendix B. Remaining Cases Proof Theorem 8
Theorem 7. Unless P = NP, Combinatorial Shift Bribery inapproximable (in
polynomial time) Plurality rule Borda rule, even two candidates
(+1, 1)-shift actions.
Borda Rule (+1, +1)-Shift Actions. slightly modify reduction used Plurality rule (+1, +1)-shift actions. Specifically, describe parameterized reduction W[1]-hard Clique problem, parameterized solution
size, Borda-Combinatorial Shift Bribery (+1, +1)-shift actions, parameterized
(m, B).
Let (G, h) instance Clique V (G) = {u1 , . . . , un0 } E(G) = {e1 , . . . , em0 }.
create instance Borda-Combinatorial Shift Bribery follows. set
candidates {p} D, = {d1 , . . . , dh1 }. create following voters.
1. vertex ui V (G), create corresponding vertex voter vi preference
order:
d1 dh1 p.
2. create n0 2h dummy voters, preference order:
p d2 dh1 d1 .
3. create h dummy voters, preference order:
dh1 p d2 dh2 d1 .
4. create n0 h dummy voters, preference order:
p d1 dh1 .
5. create n0 h dummy voters, preference order:
d1 p d2 dh1 .
edge {ui , uj } E(G), create shift action f{ui ,uj } effect 1 vertex

voters vi vj effect 0 voters. Finally, set budget B := h2 .
completes construction, computable polynomial time.
637

fiBredereck, Faliszewski, Niedermeier, & Talmon

proof correctness follows lines proof Plurality rule
(+1, +1)-shift actions, instead counting number approvals, need compute
Borda scores candidates. Indeed, reason additional dummy
voters.
particular, construction ensures d1 original winner election
difference Borda score p Borda score d1 exactly h2 .
Furthermore, shift action increase score p two. Hence, make
p co-winner one must increase score p h(h 1) decrease score d1 h.
possible shift actions correspond edges clique size h.
Plurality Rule (+1, 1)-Shift Actions. still reduce W[1]hard Clique problem, parameterized solution size, reduction bit
involved.
Let (G, h) Clique instance graph G n0 := |V (G)| vertices
0
:= |E(G)| edges. construct Plurality-Combinatorial Shift Bribery instance
follows. Let set candidates {p, d} D, := {d1 , . . . , dh1 }, create
following voters:
1. vertex vi , create
preference order:

h
3



(h)
vertex voters vi1 , . . . , vi 3 corresponding vi ,
d1 dh1 p.

2. edge ej = {vi1 , vi2 }, create corresponding edge voter wj preference
order:
p d1 dh1 .


3. Create 2 h2 + (n0 2h) h3 m0 dummy voters, preference order:
p d1 dh1 .
edge ej = {vi1 , vi2 }, construct 2

h
3



shift actions, denoted

( h)
(h)
fe1j ,vi , . . . , fej3,vi1 fe1j ,vi , . . . , fej3,vi2 ,
2

1

h
3

], (a) fezj ,vi effect +1 viz1 effect 1 wj , (b)
1

fezj ,vi effect +1 viz2 effect 1 wj . Finally, set budget B := 2 h2 h3 .
2
completes construction. easy see computable polynomial time
parameterized reduction.
Observe that, initially, edge voters dummy
voters prefer
p, vertex


h
h
0
voters prefer d1 . Therefore, initial score p 2 2 + (n 2h) 3 , initial score

d1 n0 h3 . assume, without loss generality, means d1
winner election (instances satisfying assumption solved constant
time).
remains show constructed instance contains successful set shift actions F 0 size h (G, h) contains clique size B. general
z [



638

fiCombinatorial Shift Bribery

idea choose shift actions corresponding edges connecting nodes
h-size clique, ensure p becomes preferred candidate h h3

additional vertex voters, making d1 preferred candidate h2 additional
edge voters.
Formally, part, let H V (G) set h vertices forming clique let
E 0 E(G) set edges connecting vertices H. choose following set
shift actions:

h
0
0
z
z
]}.
F = {fej ,vi , fej ,vi | ej = {vi1 , vi2 } E , z [
1
2
3
show F 0 successful set
shift actions. end, observe vertex
h
z
0
voter vi vi V z [ 3 ], candidate p shifted h 1 positions forward, therefore

p becomes preferred candidate voters. means h h3 additional vertex
voters prefer p (and, thus, prefer d1 anymore). Furthermore, p shifted backwards
voters {wj | ej E 0 }, is, d1 becomes preferred candidate


h
h
0
2 edge voters p remains preferred candidate 2 edge voters. Thus,
p tie winners.
part, let F 0 successful set shift actions.
p winner
make

election, p must shifted top position least h h3 h2 vertex voters (no
type voters affected positively). pigeonhole
principle, vertex

h
voters correspond least h different vertices (there 3 voters corresponding

vertex). effect, least h2 edge voters must effected negatively d1 becomes
preferred candidate.
Thus, make p win election p must shifted

top



position least h h3 vertex voters. implies |F 0 | (h1)h h3 = 2 h2 h3 = B
and, hence, |F 0 | = B. Itfollows p shifted backwards making d1 preferred
candidate exactly h2 edge voters p must shifted top position

exactly h h3 vertex voters corresponding exactly h different vertices. construction,
implies h vertices form clique, done.
Borda Rule (+1, 1)-Shift Actions. Borda rule, reduction is,
again, bit involved, main idea Plurality rule.
Let (G, h) instance Clique graph G n0 := |V (G)| vertices
0
:= |E(G)| edges. construct Borda-Combinatorial Shift Bribery instance
follows. set candidates {p, d} D, := {d1 , . . . , dh1 }, create
following voters:

(h)
1. vertex vi , create h3 vertex voters vi1 , . . . , vi 3 corresponding vi ,
preference order:


p.
2. edge ej = {vi1 , vi2 }, create corresponding edge voter wj preference
order:
d1 dh2 p dh1 .



h
h 2
0 h
0
0
2 + (n 3 + )(h 1) ( 3 h )
3. Let L :=
. Without loss generality,
h1
assume L integer (this requires simple modifications input clique
639

fiBredereck, Faliszewski, Niedermeier, & Talmon

instance only). create L dummy voters, preference order:
p dh1 d1 .
edge ej = {vi1 , vi2 }, construct 2

h
3



shift actions, denoted

(h)
( h)
fe1j ,vi , . . . , fej3,vi1 fe1j ,vi , . . . , fej3,vi2 ,
1

2

h
3

], (a) fezj ,vi effect +1 viz1 effect 1 wj , (b)
1

fezj ,vi effect +1 viz2 effect 1 wj . Finally, set budget B := 2 h2 h3 .
2
completes construction. easy see computable polynomial time.
proof correctness follows lines proof correctness Plurality
rule and, thus, omitted.
z [



Appendix C. Proof Theorem 9
Theorem 9. Borda-Combinatorial Shift Bribery W[1]-hard respect number n voters, even (+1, 1)-shift actions budget constraints.
Proof. reduce following W[1]-hard problem (Mathieson & Szeider, 2012, Lemma
3.2).
Strongly Regular Multicolored Clique
Input: Two integers, h, undirected graph G = (V, E),
vertex one h colors [h], vertex adjacent exactly
vertices color different own.
Question: exist clique size h containing one vertex
color class?
Given instance Strongly Regular Multicolored Clique, construct
instance Combinatorial Shift Bribery, Borda rule. general idea
reduction follows. set important candidates consists preferred
candidate p candidates correspond edges. technical reasons,
edge e = {v, v 0 }, introduce two candidates, e1 e2 ; one associated
touching vertex v associated touching vertex v 0 . (In fact,
introduce edge candidates vertex candidates, use
ensure correct structure election appropriate bribery behavior.) build two
groups voters, vertex-selecting voters edge-electing voters. first group
implements picking vertices clique (one vertex color), second
group implements picking edges (one edge pair colors). ensure given
set shift actions chance successful, must hold h vertices
h
2 edges picked. Importantly, holds even unbribed election.
make sure p wins election picked voters edges correspond clique (with vertices color). end, define voters
two numbers, , that:
640

fiCombinatorial Shift Bribery

1. h vertices picked vertex-selecting voters, different color.
vertex-selecting voters give points edge candidate associated
touching one selected vertices, + 1 points edge candidates.
means picking vertex decrease score edge candidates
edges touch vertex.

2. h2 edges picked edge-selecting voters, one edge pair colors.
edge-selecting voters give + 1 points edge candidate corresponds
picked edge, points remaining edge candidates. means that,
picking edge, increase score candidates corresponding it.
3. Candidate p gets + + 1 points, irrespective shift actions apply.
Note unbribed election every candidate gets + + 2 points p
always gets + + 1 points. Thus challenge ensure every candidate gets
+ + 1 points. description, possible pick vertices
edges correspond size h clique (of vertices different colors). Indeed,
selected edge e touch two selected vertices, e1 e2 would receive
+ 1 points edge-selecting candidates least one would receive + 1
points vertex-selecting voters. effect, p would winner.
Without loss generality, assume edges vertices selected unbribed
election form clique (otherwise would trivial solution input
problem could output fixed yes-instances Borda-Combinatorial Shift
Bribery).
Construction. formally describe reduction, give example
applying simple instance, finally show correctness reduction.
illustrate aspects correctness proof using example.
Candidates. set candidates somewhat involved. important candidates
preferred candidate p sets edge candidates, E1 E2 , defined below.
Let E(G) = {e1 , . . . , e } set edges graph G. create two edge-candidate
sets: E 1 = {e11 , . . . , e1 } E 2 = {e21 , . . . , e2 }. [h], let ni number
vertices G color let V = {v1i , . . . , vni } set vertices.
color vertex vji V , define neighborhood vji follows:
0

N (vji ) := {e1` | e` = {vji , vji 0 } E < i0 }
0

{e2` | e` = {vji , vji 0 } E > i0 }.
(This, perhaps bit strange way using color numbers pick edge candidates either
E 1 E 2 , implementing fact edge e E(G) two candidates, e1
e2 , associated touching different endpoints e.)
technical reasons need candidates follows. adjust scores
candidates, introduce single dummy candidate d. create two
candidate sets E 0 = {e01 , . . . , e0 } E 3 = {e31 , . . . , e3 } act guards
edge-selecting voters. V create two candidate sets U := {uij | vji V }
641

fiBredereck, Faliszewski, Niedermeier, & Talmon



U 0i = {u0ij | vji V } U := 1ih U U 0 := 1ih U 0i act guards
vertex-selecting voters.
final set candidates C := U U 0 E 0 E 1 E 2 E 3 {p, d}.
Vertex-Selecting Voters. describe vertex-selecting voters. color
vertex vji , define following parts preference orders (for j = 1, assume
uij1 u0ij1 uini u0ini respectively):

A(vji ) : uij N (vji ) u0ij ,

B(vji ) : uij1 N (vji ) u0ij1 .
color create three pairs voters. voters first pair, wi wi0 ,
following preference orders:


wi : p A(v1i ) A(v2i ) A(v3i ) A(vni ) Ri ,


wi0 : Ri B(v1i ) B(vni ) B(vni 1 ) B(v2i ) p,
Ri set remaining candidates, is, Ri := C \ ({p} U U 0i N (v1i )
N (vni )). voters second pair, qi qi0 , preference orders
reverse wi reverse wi0 , respectively. Finally, voters last pair, qi
qi0 , preference orders:


qi : C \ ({d} N (v1i )) N (v1i ),

qi0 : N (v1i ) C \ ({d} N (v1i )) d.
effect, first two pairs voters jointly give 2(|C| 1) points candidates.
last pair gives |C| 1 points candidates N (v1i ) |C| points
candidates (except d, receives less |C| 1 points).
Let := h(2(|C|1)+|C|)1. Altogether, vertex-selecting voters give following
scores candidates: candidates N (v11 )N (v12 ) N (v1h ) receive points
candidates, except d, receive + 1 points (d receives less points). Thus,
unbribed election v11 , . . . , v1h selected vertices.
color i, introduce (ni 1) ((h 1) + 2) shift actions effect 1
voter wi effect +1 voter wi0 . understand number shift
actions comes from, note that: (1) vertex vji , |N (vji )| = (h 1) (each
vertex connected vertices color different own), (2) A(vji )
B(vji ) candidates N (vji ) surrounded two vertex candidates, (3)
integer, 1 ni 1, applying t((h 1) + 2) shift actions effect
candidates N (v1i ) gain one point (i.e., v1i ceases selected), candidates
) lose one point (i.e., v
N (vt+1
t+1 becomes selected), candidate changes
score (later argue applying numbers shift actions multiples
((h 1) + 2) cannot ensure ps victory).
642

fiCombinatorial Shift Bribery

Edge-Selecting Voters. edge-selecting voters, need following additional
notation. Let Ex,y denote set candidates representing edges vertices
color x color y, is,
q{0,1,2,3}

Ex,y := {e`

| e` = {vjx , vjy0 } E}.

write nx,y denote number edges vertices color x color y.
idx,y
z refer index z-th edge vertices color x y. example,
e3 , e7 e57 three edges vertices colors 1 2, n1,2 = 3,
1,2
1,2
id1,2
1 = 3, id2 = 7, id3 = 57.
pair {x, y} distinct colors edge eidx,y
, introduce following
j
x,y
parts preference orders (for j = nx,y , assume idj+1 = idx,y
1 ):
R(eidx,y
) : e0idx,y e1idx,y e2idx,y e3idx,y ,
j
):
S(eidx,y
j

j

j

j

e0idx,y
j+1

e1idx,y
j

e2idx,y
j





j



e3idx,y .
j+1

pair {x, y} distinct colors introduce three pairs voters. voters
0 , following preference orders:
first pair, wx,y wx,y

wx,y : R(eidx,y
) p R(eidx,y
) R(eidx,y
) R(eidx,y
) Rx,y ,
nx,y
1
2
3

0
) S(eidx,y
) S(eidx,y
) S(eidx,y
) p,
wx,y
: Rx,y S(eidx,y
nx,y
n
1
2
1
x,y

Rx,y set remaining candidates, is, Rx,y := C \ ({p} Ex,y ).
0 , preference orders reverse w
voters second pair, qx,y qx,y
x,y
0
0 ,
reverse wx,y , respectively. Finally, voters last pair, qx,y qx,y
following preference orders:

qx,y : e1idx,y e2idx,y C \ ({d, e1idx,y , e2idx,y }),
1
1
1
1
12
0
2
1
qx,y : C \ ({d, eidx,y , eidx,y }) eidx,y eidx,y d.
1

1

1

1

first two pairs voters jointly give 2(|C| 1) points candidates.
last pair gives |C| points e1idx,y e2idx,y , |C| 1 points candidates
1
1
(except d, receives
less

|C|

1
points).

Let := 3 h2 (|C| 1). Altogether, pair distinct colors {x, y}, edgeselecting voters give + 1 points candidates e1idx,y e2idx,y . candidates receive
1
1
points (except d, receives less points). Thus unbribed election
selected edges exactly first edges pair colors (that is, edges
form eidx,y
, pair distinct colors {x, y}).
1
pair {x, y} distinct colors, create 4(nx,y 1) shift actions effect 1
0 . intuition behind shift actions similar
voter wx,y effect +1 voter wx,y
case vertex-selecting voters. make following observations: (1)
edge eidx,y four candidates listed R(eidx,y ) four candidates listed S(eidx,y ),
`
`
`
(2) integer, 1 nx,y 1, apply 4t shift actions, candidates
643

fiBredereck, Faliszewski, Niedermeier, & Talmon

v11

v12

e3

e1

v21

e4

e2

v22

e5

e6
v13

v23

V 1 = {v11 , v21 }, V 2 = {v12 , v22 }, V 3 = {v13 , v23 }, h = 3, = 1
Figure 2: 3-colored graph six vertices vertex adjacent one vertex
color classes V 1 , V 2 V 3 , own.
ceases selected), candidates e1idx,y e2idx,y
e1idx,y e2idx,y lose one point (edge eidx,y
1
1

t+1

1

t+1

gain one point (edge eidx,y
becomes selected), scores candidates remain
t+1
unchanged (we later argue apply number shift actions
multiple 4, p certainly winner resulting election).
conclude construction, set budget B := (that is, use many
shift actions like). easy verify reduction computable polynomial
time introduce number voters function h (thus,
parameterized reduction). proving correctness construction, consider
following example (we refer correctness proof well).
Example 5. Consider Strongly Regular Multicolored Clique instance (d, h, G)
= 1, h = 3, graph G Figure 2. construction produces following set
candidates:
C := U U 0 E 0 E 1 E 2 E 3 {p, d},

01 02 02 03 03
U = {u11 , u12 , u21 , u22 , u31 , u32 }, U 0 = {u01
1 , u2 , u1 , u2 , u1 , u2 }


E = {ei1 , ei2 , . . . , ei6 }, 0 3.
Furthermore, have:
N (v11 ) := {e11 , e12 },

N (v21 ) := {e13 , e16 },

N (v12 ) := {e23 , e14 },

N (v22 ) := {e21 , e15 },

N (v13 ) := {e22 , e25 },

N (v23 ) := {e24 , e26 }.
644

fiCombinatorial Shift Bribery

vertex-selecting group voters, create following voters. color
i, create two voters wi wi0 :

1
1
1
1
01
w1 : p u11 e11 e12 u01
1 u2 e3 e6 u2 R ,


1
1
1
01
w10 : R1 u12 e11 e12 u01
2 u1 e3 e6 u1 p,

2
2
2
1
02
w2 : p u21 e23 e14 u02
1 u2 e1 e5 u2 R ,


2
2
1
02
w20 : R2 u22 e23 e14 u02
2 u1 e1 e5 u1 p,

3
3
2
2
03
w3 : p u31 e22 e25 u03
1 u2 e4 e6 u2 R ,


3
2
2
03
w30 : R3 u32 e22 e25 u03
2 u1 e4 e6 u1 p,
Ri := C \ ({p} U U 0i N (v1i ) N (vni )), 1 3. voters
add voter reversed preferences. (This means that, far, candidates obtain
total score.) finish group voters creating color two voters,
qi qi0 , preference orders:


qi : C \ ({d} N (v1i )) N (v1i ),

qi0 : N (v1i ) C \ ({d} N (v1i )) d.
ensures color i, candidates N (v1i ) get points,
candidates get + 1 points (except gets points). create 4 shift actions
effect 1 voter wi effect +1 voter wi0 .
edge-selecting second group voters, recall Ex,y denotes set candidates representing edges vertices color x color y. Specifically, have:
E1,2 :={e01 , e11 , e21 , e31 e03 , e13 , e23 , e33 },
E1,3 :={e02 , e12 , e22 , e32 e06 , e16 , e26 , e36 },
E2,3 :={e04 , e14 , e24 , e34 e05 , e15 , e25 , e35 }.
0 , follows:
pair {x, y} distinct colors create two voters, wx,y wx,y


w1,2 : e01 e11 e21 e31 p e03 e13 e23 e33 R1,2 ,

0
w1,2
: R1,2 e01 e13 e23 e31 e03 e11 e21 e33 p,

w1,3 : e02 e12 e22 e32 p e06 e16 e26 e36 R1,3 ,

0
w1,3
: R1,3 e02 e16 e26 e32 e06 e12 e22 e36 p,

w2,3 : e04 e14 e24 e34 p e05 e15 e25 e35 R2,3 ,

0
w2,3
: R2,3 e04 e15 e25 e34 e05 e14 e24 e35 p,
Rx,y := C \ ({p} E[x, y]). voters add voter reversed
0
preferences. Further, pair {x, y} distinct colors, add two voters qx,y qx,y
645

fiBredereck, Faliszewski, Niedermeier, & Talmon

follows:

qx,y : e1idx,y e2idx,y C \ ({d, e1idx,y , e2idx,y }),
1
1
1
1
12
1
2
0
qx,y : C \ ({d, eidx,y , eidx,y }) eidx,y eidx,y d.
1

1

1

1

Altogether, pair {x, y} distinct colors, candidates e1idx,y e2idx,y get +1 points
1
1
candidates get points (except d, gets less points). pair {x, y}
distinct colors, create 4 shift actions effect 1 voter wx,y effect +1
0 .
voter wx,y
4
Properties Construction. discuss several properties construction.
properties play significant rule showing correctness reduction.
illustrate arguments, come back example time time. begin
looking scores candidates.
Lemma 1. following claims hold:
1. unbribed election, every candidate receives + + 2 points every
candidate {p} U U 0 E 0 E 3 receives exactly + + 1.
2. every bribed election, score p exactly + + 1.
3. applying successful set shift actions, score p + + 1
scores candidates + + 1.
Proof. easy see first claim holds based discussion give
throughout construction. second claim holds (a) applying every shift
action decreases one score p one vote increases one another vote
(there sufficiently shift actions whole instance applying shift
action always moves p within two votes shift action acts). last claim
follows directly second one.
(Lemma)
Let us consider process selecting vertices. description vertexselecting voters said that, initially, color vertex v1i selected,
integer t, 1 ni 1, apply t((h 1) + 2) shift actions affect voters wi wi0 ,

v1i ceases selected vt+1
becomes selected. argue apply
number shift actions divisible ((h 1) + 2), p winner
resulting election.
see case, recall preference orders voter wi wi0
exactly (h 1) candidates E 1 E 2 pair candidates {uij , u0ij }.
Furthermore, p passes candidate uij preference order voter wi (increasing
uij score one), must also pass candidate uij preference order voter wi0
(decreasing uij score one). Otherwise, uij would end score + + 2 and,
1, p would winner (there possibilities influence score uij
shifting p preference lists wi wi0 ). Hence, p also passes candidate u0ij
candidates uij u0ij preference lists wi wi0 . This, however,
means p winner election, number applied shift actions
646

fiCombinatorial Shift Bribery

Unbribed voters w2 w20 :


2
2
2
1
02
w2 : p u21 e23 e14 u02
1 u2 e1 e5 u2 R


2
2
1
02
w20 : R2 u22 e23 e14 u02
2 u1 e1 e5 u1 p
Applying two shift actions effect -1 w2 +1 w20 :
+1 +1

2
2
2
1
02
w2 : u21 e23 p e14 u02
1 u2 e1 e5 u2 R
2
-1
-1


2 e2 p e1 u02
w20 : R2 u22 e23 e14 u02

u
2
1
1
5
1
+2
Applying (h 1) + 2 = 4 shift actions effect -1 w2 +1 w20 :
+1 +1 +1 +1

2
2
2
1
02
w2 : u21 e23 e14 u02
1 p u2 e1 e5 u2 R
4
-1
-1
-1
-1

2
0
2
2 e2 e1 u02
w2 : R u2 e23 e14 u02

p

u
2
1
1
5
1
+4
Figure 3: Illustration bribery actions affecting first voter group running example
(Example 5). Note that, unbribed election, every candidate U U 0 obtains
+ + 1 points total. color one type shift actions
affects voter wi wi0 : shift actions effect 1 voter wi effect +1
voter wi0 . shift action affect voter first group. Applying
multiple ((h 1) + 2) shift actions effect 1 voter wi effect +1 voter wi0
ensures candidates U U 0i receive + + 1 points total, whereas
applying number shift actions implies candidate U
receives + + 2 points and, hence, p cannot win. illustrate color 2
running example.
effects voters wi wi0 multiple ((h 1) + 2) (p passes candidate uij ,
candidate u0ij , h candidates between). Figure 3 provides illustration
reasoning.
next discuss selecting edges. case vertex-selecting voters, description construction argued (a) initially pair {x, y} distinct
colors, edge eidx,y
selected (b) applying 4t, 1 nx,y 1, shift actions
1
0 , e x,y ceases selected e x,y becomes selected.
affect voters wx,y wx,y
id1
idt+1
argue used number shift actions multiple four,
p certainly would winner election.
0
see case, note designed preference orders wx,y wx,y
candidates e0idx,y e3idx,y , j {2, . . . , nx,y }, follow p vote wx,y
j

j

0 . effect, apply shift action affects
order precede p wx,y

647

fiBredereck, Faliszewski, Niedermeier, & Talmon

0 :
Unbribed voters w2,3 w2,3


w2,3 : e04 e14 e24 e34 p e05 e15 e25 e35 R2,3

0 :R2,3 e0 e1 e2 e3 e0 e1 e2 e3 p
w2,3
4
5
5
4
5
4
4
5
0 :
Applying two shift actions effect -1 w2,3 +1 w2,3
+1 +1

w2,3 : e04 e14 e24 e34 e05 e15 p e25 e35 R2,3
2
-1
-1



0
2,3
0
1
2
3
0
1
2
w2,3 :R e4 e5 e5 e4 e5 e4 p e4 e35
+2
0 :
Applying four shift actions effect -1 w2,3 +1 w2,3
+1 +1 +1 +1

w2,3 : e04 e14 e24 e34 e05 e15 e25 e35 p R2,3
4
-1
-1
-1
-1

0 :R2,3 e0 e1 e2 e3 p e0 e1 e2 e3
w2,3
4
5
5
4
5
4
4
5
+4

Figure 4: Illustration bribery actions affecting second voter group running
example. Note unbribed election, every candidate E 0 E 4 obtains +
+ 1 points total. pair colors x one type shift
0 : shift actions effect 1 voter w
actions affects voter wx,y wx,y
x,y
0 . shift action affect voter second
effect +1 voter wx,y
group. Applying multiple 4 shift actions effect 1 voter wx,y effect +1
0
ensures candidates E 0 E 4 receive + + 1 points
voter wx,y
voters, whereas applying number shift actions implies
candidate E 0 receives + + 2 points and, hence, p cannot win. illustrate
color pair 2 3 running example.
0
voters wx,y wx,y
number times multiple four, one
candidates obtains + + 2 points. Since way affect score
candidates, Lemma 1, case p cannot winner. illustrate effect
Figure 4.

Solution Example 5. complete correctness proof, let us illustrate
solution example.
unbribed election selects vertex v11 , v12 , v13 edges e1 , e2 e4 . Hence,
example, candidate e24 receives + +2 points p (who receives + +1 points)
winner.
applying four shift actions effect 1 w2 effect +1 w20 , select v22
instead v12 vertex color 2 clique (as depicted bottom Figure 3).
648

fiCombinatorial Shift Bribery

0 , select e
applying four shift actions effect 1 w2,3 effect +1 w2,3
5
instead e4 edge color 2 color 3 clique (as depicted
bottom Figure 4). Now, candidate {e11 , e12 , e21 , e22 , e15 , e25 } receives + 1 points
edge-selecting voters, points vertex-selecting voters. Every
candidate receives + 1 points vertex-selecting voters
points edge-selecting voters. Hence, p (with + + 1 points) winner.
solution corresponds left 3-colored triangle Figure 2.

Correctness. remains show successful set shift actions
constructed Borda-Combinatorial Shift Bribery instance
h-colored clique graph G.
part, assume h-colored clique H V (G). Without loss
generality, let H = {vz11 , . . . , vzhh } let EH := {{v, v 0 } | v, v 0 H}. Furthermore, let zx,y
denote index edge Ex,y representing edge EH vertex
EH .
color x vertex color y. is, zx,y = j eidx,y
j
easy verify following set shift actions successful:
1. color [h], include (zi 1)((h 1) + 2) shift actions effects
voters wi wi0 .
2. pair {x, y} distinct colors, include 4(zx,y 1) shift actions effects
0 .
voters wx,y wx,y
words, select vertices edges corresponding clique. effect,
scores candidates + + 1 (except d, receives lower score). p
among tied winners.
part, assume successful set shift actions consider
election applying shift actions. construction, know edge-selecting
voters pick exactly one edge pair distinct colors. Hence graph induced
edges contains vertices h different colors. graph contains h
vertices, graph must h-colored clique (this graph cannot contain fewer
h vertices). sake contradiction, let us assume graph contains
h vertices. Thus two selected edges, ej ej 0 , incident two different vertices,
vi ej vi0 ej 0 , color. construction (and way vertex-selecting
voters work), least one sets N (vi ) N (vi0 ) candidates set receive
+ 1 points vertex-selecting voters. However, since ej ej 0 selected
edge-selecting voters, voters give + 1 points candidates e1j , e2j ,
e1j 0 , e2j 0 . Hence, least one candidates receives + + 2 points total and,
Lemma 1, p winner. contradiction, graph induced
selected edges must h-colored clique.

References
Bartholdi, III, J. J., Tovey, C. A., & Trick, M. A. (1992). hard control
election. Mathematical Computer Modelling, 16 (89), 2740.
649

fiBredereck, Faliszewski, Niedermeier, & Talmon

Baumeister, D., Faliszewski, P., Lang, J., & Rothe, J. (2012). Campaigns lazy voters:
Truncated ballots. Proceedings 11th International Conference Autonomous
Agents Multiagent Systems (AAMAS 12), pp. 577584. IFAAMAS.
Betzler, N., Bredereck, R., Chen, J., & Niedermeier, R. (2012). Studies computational
aspects votinga parameterized complexity perspective. Multivariate Algorithmic Revolution Beyond, Vol. 7370 LNCS, pp. 318363. Springer.
Binkele-Raible, D., Erdelyi, G., Fernau, H., Goldsmith, J., Mattei, N., & Rothe, J. (2014).
complexity probabilistic lobbying. Discrete Optimization, 11, 121.
Boutilier, C., Brafman, R. I., Hoos, C. D. H. H., & Poole, D. (2004). CP-nets: tool
representing reasoning conditional ceteris paribus preference statements.
Journal Artificial Intelligence Research, 21, 135191.
Brandt, F., Harrenstein, P., Kardel, K., & Seedig, H. G. (2013). takes few:
hardness voting constant number agents. Proceedings 12th International Conference Autonomous Agents Multiagent Systems (AAMAS 13),
pp. 375382. IFAAMAS.
Bredereck, R., Chen, J., Faliszewski, P., Nichterlein, A., & Niedermeier, R. (2014a). Prices
matter parameterized complexity shift bribery. Proceedings 28th
AAAI Conference Artificial Intelligence (AAAI 14), pp. 13981404. AAAI Press.
Bredereck, R., Chen, J., Hartung, S., Kratsch, S., Niedermeier, R., Suchy, O., & Woeginger,
G. (2014b). multivariate complexity analysis lobbying multiple referenda.
Journal Artificial Intelligence Research, 50, 409446.
Bredereck, R., Faliszewski, P., Niedermeier, R., Skowron, P., & Talmon, N. (2015a). Elections candidates: Prices, weights, covering problems. Fourth
International Conference Algorithmic Decision Theory (ADT 2015), Vol. 9346
LNCS, pp. 414431. Springer.
Bredereck, R., Faliszewski, P., Niedermeier, R., & Talmon, N. (2015b). Large-scale election campaigns: Combinatorial shift bribery. Proceedings 14th International
Conference Autonomous Agents Multiagent Systems (AAMAS15), pp. 6775.
Bredereck, R., Faliszewski, P., Niedermeier, R., & Talmon, N. (2016). Complexity shift
bribery committee elections. Proceedings Twenty-Ninth AAAI Conference
Artificial Intelligence (AAAI 16).
Bulteau, L., Chen, J., Faliszewski, P., Niedermeier, R., & Talmon, N. (2015). Combinatorial
voter control elections. Theoretical Computer Science, 589, 99120.
Cary, D. (2011). Estimating margin victory instant-runoff voting. Presented
2011 Electronic Voting Technology Workshop/Workshop Trustworthy Elections.
Chen, J., Faliszewski, P., Niedermeier, R., & Talmon, N. (2015). Elections voters: Candidate control easy. Proceedings 29th AAAI Conference
Artificial Intelligence (AAAI 15), pp. 20452051.
Christian, R., Fellows, M. R., Rosamond, F. A., & Slinko, A. (2007). complexity
lobbying multiple referenda. Review Economic Design, 11 (3), 217224.
650

fiCombinatorial Shift Bribery

Conitzer, V., Lang, J., & Xia, L. (2009). hard control sequential elections via
agenda?. Proceedings 21st International Joint Conference Artificial
Intelligence (IJCAI 10), pp. 103108. AAAI Press.
Dorn, B., & Schlotter, I. (2012). Multivariate complexity analysis swap bribery. Algorithmica, 64 (1), 126151.
Downey, R. G., & Fellows, M. R. (2013). Fundamentals Parameterized Complexity.
Springer.
Elkind, E., & Faliszewski, P. (2010). Approximation algorithms campaign management.
Proceedings 6th International Workshop Internet Network Economics
(WINE 10), Vol. 6484 LNCS, pp. 473482. Springer.
Elkind, E., Faliszewski, P., & Slinko, A. (2009). Swap bribery. Proceedings 2nd
International Symposium Algorithmic Game Theory (SAGT 09), Vol. 5814
LNCS, pp. 299310. Springer.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2010). Using complexity protect
elections. Communications ACM, 53 (11), 7482.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. A. (2009a). hard bribery
elections?. Journal Artificial Intelligence Research, 35, 485532.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L. A., & Rothe, J. (2009b). Llull
Copeland voting computationally resist bribery constructive control. Journal
Artificial Intelligence Research, 35, 275341.
Faliszewski, P., Reisch, Y., Rothe, J., & Schend, L. (2014). Complexity manipulation,
bribery, campaign management Bucklin Fallback voting. Proceedings
13th International Conference Autonomous Agents Multiagent Systems
(AAMAS 14), pp. 13571358. IFAAMAS.
Faliszewski, P., & Rothe, J. (2015). Control bribery voting. Brandt, F., Conitzer,
V., Endriss, U., Lang, J., & Procaccia, A. D. (Eds.), Handbook Computational
Social Choice, chap. 7. Cambridge University Press.
Flum, J., & Grohe, M. (2006). Parameterized Complexity Theory. Springer.
Gabow, H. N. (1983). efficient reduction technique degree-constrained subgraph
bidirected network flow problems. Proceedings 15th Annual ACM Symposium
Theory Computing (STOC 83), pp. 448456. ACM.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide Theory
NP-Completeness. Freeman.
Hazon, N., Lin, R., & Kraus, S. (2013). change groups collective decision?.
Proceedings 23rd International Joint Conference Artificial Intelligence
(IJCAI 13), pp. 198205. AAAI Press.
Hulett, H., Will, T. G., & Woeginger, G. J. (2008). Multigraph realizations degree
sequences: Maximization easy, minimization hard. Operations Research Letters,
36 (5), 594596.
651

fiBredereck, Faliszewski, Niedermeier, & Talmon

Lang, J., & Xia, L. (2015). Voting combinatorial domains. Brandt, F., Conitzer, V.,
Endriss, U., Lang, J., & Procaccia, A. D. (Eds.), Handbook Computational Social
Choice, chap. 9. Cambridge University Press.
Lenstra, H. W. (1983). Integer programming fixed number variables. Mathematics
Operations Research, 8 (4), 538548.
Magrino, T., Rivest, R., Shen, E., & Wagner, D. (2011). Computing margin victory IRV elections. Presented 2011 Electronic Voting Technology Workshop/Workshop Trustworthy Elections.
Mathieson, L., & Szeider, S. (2012). Editing graphs satisfy degree constraints: parameterized approach. Journal Computer System Sciences, 78 (1), 179191.
Mattei, N., Goldsmith, J., & Klapper, A. (2012a). complexity bribery manipulation tournaments uncertain information. Proceedings 25th International Florida Artificial Intelligence Research Society Conference (FLAIRS 12),
pp. 549554. AAAI Press.
Mattei, N., Pini, M., Rossi, F., & Venable, K. (2012b). Bribery voting combinatorial
domains easy. Proceedings 11th International Conference Autonomous
Agents Multiagent Systems (AAMAS 12), pp. 14071408. IFAAMAS.
Niedermeier, R. (2006). Invitation Fixed-Parameter Algorithms. Oxford University Press.
Obraztsova, S., & Elkind, E. (2011). complexity voting manipulation
randomized tie-breaking. Proceedings 22nd International Joint Conference
Artificial Intelligence (IJCAI 11), pp. 319324. AAAI Press.
Obraztsova, S., Elkind, E., & Hazon, N. (2011). Ties matter: Complexity voting manipulation revisited. Proceedings 10th International Conference Autonomous
Agents Multiagent Systems (AAMAS 11), pp. 7178.
Rajagopalan, S., & Vazirani, V. V. (1998). Primal-dual RNC approximation algorithms
set cover covering integer programs. SIAM Journal Computing, 28 (2),
525540.
Reisch, Y., Rothe, J., & Schend, L. (2014). margin victory Schulze, Cup,
Copeland elections: Complexity regular exact variants. Proceedings
Seventh European Starting AI Researcher Symposium (STAIRS-2014), pp. 250259.
IOS Press.
Schlotter, I., Faliszewski, P., & Elkind, E. (2011). Campaign management approvaldriven voting rules. Proceedings 25th AAAI Conference Artificial Intelligence (AAAI 11), pp. 726731. AAAI Press.
Xia, L. (2012). Computing margin victory various voting rules. Proceedings
13th ACM Conference Electronic Commerce (EC 12), pp. 982999. ACM
Press.

652

fiJournal Artificial Intelligence Research 55 (2016) 10251058

Submitted 07/15; published 04/16

Semi-supervised Learning Induced Word Senses
State Art Word Sense Disambiguation
Osman Baskaya

obaskaya@ku.edu.tr

Department Computer Sciences Engineering
Koc University
Istanbul, Turkey

David Jurgens

jurgens@stanford.edu

Department Computer Science
Stanford University
Stanford, CA, USA

Abstract
Word Sense Disambiguation (WSD) aims determine meaning word context, successful approaches known benefit many applications Natural Language Processing. Although supervised learning shown provide superior WSD
performance, current sense-annotated corpora contain sufficient number instances per word type train supervised systems words. unsupervised
techniques proposed overcome data sparsity problem, techniques
outperformed supervised methods. paper, propose new approach
building semi-supervised WSD systems combines small amount sense-annotated
data information Word Sense Induction, fully-unsupervised technique
automatically learns different senses word based used. three experiments, show sense induction models may effectively combined ultimately
produce high-performance semi-supervised WSD systems exceed performance
state-of-the-art supervised WSD techniques trained sense-annotated data.
anticipate results released software also benefit evaluation practices
sense induction systems working low-resource languages demonstrating
quickly produce accurate WSD systems minimal annotation effort.

1. Introduction
Word Sense Disambiguation (WSD) identifies particular meaning word context,
whether bass refers fish instrument. Correctly performing WSD grounds
ambiguous natural language concrete semantic representation, numerous
benefits downstream applications, knowledge extraction (Navigli & Ponzetto,
2010; Hartmann, Gurevych, & Lap, 2013) machine translation (Carpuat & Wu, 2007;
Chan, Ng, & Chiang, 2007). Traditionally, supervised approaches WSD offered
superior performance (Kilgarriff & Rosenzweig, 2000; Mihalcea, Chklovski, & Kilgarriff,
2004; Agirre & Soroa, 2007; Navigli, 2009). However, major limitation building highperformance supervised WSD systems limited amount sense-annotated
corpora training models word types, largest corpora containing
hundreds thousands annotated tokens (Petrolito & Bond, 2014). result,
unsupervised WSD techniques proposed fill need high-coverage systems
c
2016
AI Access Foundation. rights reserved.

fiBaskaya & Jurgens

(Yarowsky, 1995; Agirre et al., 2014; Moro et al., 2014). techniques capable
disambiguating many word types, surpassed accuracy supervised
systems take advantage sense-annotated data available.
alternative approach supervised WSD build semi-supervised approaches using
Word Sense Induction (WSI), often referred Word Sense Induction Disambiguation (WSID) models (Agirre et al., 2006). Word Sense Induction fully unsupervised
technique examines contexts word used order learn (a) words
different meanings, referred induced senses, (b) disambiguate new
usage word instance one induced senses. induced senses learned
WSI method provide key link building WSID system: labeling usages
induced senses reference sense inventory WordNet (Fellbaum,
1998) OntoNotes (Hovy et al., 2006), mapping function learned effectively
transform annotation using induced senses annotation using senses reference inventory. WSI model constructed large corpus examples,
induced senses become associated many contextually-disambiguating features.
result, features associated induced sense used disambiguation,
are, proxy sense mapping, used features disambiguating
reference senses despite contextual features potentially never seen
data annotated references senses. Thus, close correspondence exists
induced reference senses, robust WSID system created ultimately able
disambiguate usages contexts unlike seen data annotated reference senses
virtue features associated induced senses. contrast, discriminatory
capabilities supervised WSD system limited features observed training data. Hence, ability WSID system leverage induced sense annotations
potentially remove knowledge acquisition bottleneck requiring significant amounts
sense-annotated data (Gale, Church, & Yarowsky, 1992).
Despite potential WSID, little analysis done construct
models maximize performance. Instead, WSID systems primarily
evaluated within SemEval tasks focusing word sense induction (Agirre & Soroa,
2007; Manandhar, Klapaftis, Dligach, & Pradhan, 2010; Jurgens & Klapaftis, 2013).
WSID performance evaluations promising, three important open questions remain.
First, current evaluations, WSID systems used technique Agirre et al.
(2006) converting induced sense annotations reference inventory. However,
performance impact process measured, alternative methods
tested. Second, current WSID evaluations controlled distribution
frequency senses training test data, significantly affect performance
expected generalizability results (Agirre & Martinez, 2000); settings
raise question much current systems performances attributable ease
disambiguating due test datas sense distribution. Third, despite potential
advantages WSID low-resource languages, study directly compared WSID
supervised WSD systems equal conditions test whether one setup
preferred based amount sense-annotated data available.
Addressing questions previously hindered lack large sense-annotated
data set. However, overcome limitation using recent resource Pilehvar
Navigli (2013), approximates polysemous nouns WordNet using pseudowords
1026

fiSemi-supervised Learning Induced Word Senses State Art WSD

accurately model difficulty disambiguating nouns. Here, pseudoword
made two monosemous lemmas, referred pseudosenses,
models particular sense word. example, disemous noun pic two WordNet
senses: (1) motion picture, (2) photograph. two senses represented
monosemous nouns movie photo, respectively. simulate sense-annotated data,
occurrences pseudowords pseudosenses replaced unique token (e.g., replacing
usages movie photo token denoting pseudoword); then, analogous
disambiguation task, WSD system shown occurrence pseudoword asked
decide pseudosense originally present. Crucially, (1) pseudowords
approximate real-world disambiguation difficulty (2) pseudosense-annotated data
easily created sampling occurrences pseudosenses corpus, resource enables performing comprehensive evaluation WSID arbitrarily-large amounts
annotated data direct generalizability real-world WSD performance (Pilehvar &
Navigli, 2014).
paper offers following four key contributions. First, provide comprehensive evaluation setting WSID tests systems millions instances two orders
magnitude previous evaluations thereby providing statistically-robust results
evaluated terms. Furthermore, evaluation setting uses high-quality pseudowords
effectively simulate properties WordNet senses, allows us precisely
control sense distribution test training data order measure effect
performance. Second, show method transforming induced senses
WordNet senses significant impact WSID performance, using appropriate method, WSID performance significantly outperforms formerly-competitive baselines
multiple tests sets. Third, demonstrate combining WSI models ensemble
WSID provides statistically-significant performance improvements pseudoword
real-world data. Fourth, direct comparisons state-of-the-art supervised WSD system, demonstrate ensemble WSID system outperforms supervised WSD
fewer several hundred sense-annotated instances available, indicating WSID
indeed overcome knowledge acquisition bottleneck.
results offer two important practical implications. researchers working
low-resource languages, comparisons WSID supervised WSD demonstrate
relatively small amount sense-annotated data needed state-of-theart performance WSD. Second, demonstrate current approach creating
WSID systems artificially masks true capabilities underlying WSI models,
thus future evaluations WSID systems conducted SemEval
consider using evaluation construction procedure described herein.

2. Word Sense Induction Disambiguation Systems
WSID system consists two key components: WSI model function converts
models sense annotations another sense inventory, formalized Agirre
et al. (2006). First, WSI model induces senses base corpus. Second, training
corpus labeled using induced senses WSI model senses
reference inventory. co-labeled corpus serves training data building classifier
predicts reference sense label given induced sense annotation.
1027

fiBaskaya & Jurgens

constructing WSID systems, two key questions examined: (1)
impact sense mapping function WSID performance, (2) whether multiple WSI
models may effectively combined. Answering questions essential identifying
degree performance WSID system due capabilities
underlying WSI model versus mapping process used create it. follows,
first describe WSI models used paper illustrate learn senses.
Then, formalize sense mapping function define range possibilities
may computed propose function used effectively combine WSI
models WSID ensemble.
2.1 WSI Models
Multiple techniques proposed effectively learn different meanings
word (Navigli, 2012), many approaches using either (a) graph-based representations
words semantic relationships (b) distributional approaches identifying regularities words contexts. Therefore, increase robustness results, four recent
WSI methods selected experiments. Models balanced using
lexical distributions using graphs: AI-KU (Baskaya, Sert, Cirik, & Yuret, 2013)
HDP (Lau, Cook, McCarthy, Newman, & Baldwin, 2012), use token statistics
induce senses, Chinese Whispers (Biemann, 2006) SquaT (Di Marco & Navigli,
2012), construct graphs induce senses. Following prior evaluations (Manandhar
et al., 2010; Jurgens, 2012; Jurgens & Klapaftis, 2013), disambiguation, allow
models report multiple senses per context; setting, induced sense denotes
prototypical meaning annotation represents much current usage resembles meanings. Following, summarize models induction disambiguation
procedures.
2.1.1 AI-KU
Baskaya et al. (2013) represent context target word using high probability
lexical substitutes according statistical language model. language model built
identify relative probabilities 4-gram sequences FastSubs (Yuret, 2012) applied identify words appear position target word context.
example, one instance bass may substitutes fish, another instance
may guitar. instance represented 100 substitutes, sampled
probability distribution most-probable 100 substitutes instance; substitutes transformed vector representation, reflecting sampled frequencies
each. instance-substitute vectors projected lower dimensionality using S-CODE (Maron, Lamar, & Bienenstock, 2010). final S-CODE based vectors
clustered using k-means. Much like Schutze (1992), AI-KU requires specifying number
clusters ahead time, often setting k larger necessary number. However,
determine number senses, AI-KU performs post-processing step remove clusters
contain instances, likely artifacts forcing k clusters
non-empty. remaining clusters treated senses word.
1028

fiSemi-supervised Learning Induced Word Senses State Art WSD

2.1.2 HDP
Lau et al. (2012) propose system based Hierarchical Dirichlet Process (HDP)
(Teh, Jordan, Beal, & Blei, 2006), nonparametric extention Latent Dirichlet allocation
(Blei, Ng, & Jordan, 2003; Griffiths & Steyvers, 2004). HDP automatically infers
number topics topics probability distribution generating tokens
corpus. sense induction, HDP model inferred contexts target word,
produces distribution topics context. topic treated distinct
sense word. Given new context word, HDP model used infer
topic distribution, thereby identifying senses present. output, report
full distribution senses context, weighted probabilities.
2.1.3 Chinese Whispers (CW)
Biemann (2006) proposes inducing senses using Chinese Whispers (CW), nonparametric graph clustering algorithm. CW form unsupervised label propagation
vertex initially assigned unique label label propagation run either
convergence fixed number iterations completed. graph constructed
lexically-associated terms, vertices assigned cluster typically form topicallyrelated groups. sense induction, graph constructed words associated
specific term (e.g., computing statistical associations corpus) CW
completes, vertex cluster considered features distinct sense term.
CW, graphs constructed three steps. First, 2 association computed
words base corpus. Then, words associated words pseudosenses
ranked 2 1000 words largest 2 retained.
retained neighbors, additional edges added 1000 neighbors highest 2 , excluding edges pseudosenses. sense features used
disambiguate new contexts computing overlap content words context.
disambiguation, report senses containing least one word context, weighted
number matching features.
2.1.4 SquaT
Navigli Crisafulli (2010) construct co-occurrence graph, pruned
induce sense clusters. Term co-occurrences base corpus scored using Dice
2c(w1 ,w2 )
coefficient: two terms w1 , w2 , Dice(w1 , w2 ) = c(w
c(w) frequency
1 )+c(w2 )
occurrence. Edges added graph Dice coefficient greater
threshold . graph construction begins co-occurrences target term
proceeds add edges newly-included neighbors. framework allows
multiple pruning methods induction; adopt Squares pruning method,
shown perform best. Simply, edges removed ratio observed potential
squares (closed paths length 4) edge participates threshold .
pruned, resulting disconnected components graph denote separate senses.
efficiency, use noun, verb, adjective lemmas graphs. resulting
graph produces sets lemmas associated sense, sense disambiguation performed
way Chinese Whispers.
1029

fiBaskaya & Jurgens

2.2 Sense Mapping Functions
mapping function supervised classifier that, given annotation one
induced senses, produces new sense annotation instance using sense
different inventory, induced senses essentially acting features. Agirre et al.
(2006) proposed first mapping function based matrix multiplication,
used 39 systems participating WSID shared task evaluations (Agirre & Soroa,
2007; Manandhar et al., 2010; Jurgens & Klapaftis, 2013) many subsequent papers
WSID (e.g., see Brody & Lapata, 2009; Klapaftis & Manandhar, 2010; Van de Cruys &
Apidianaki, 2011; Lau et al., 2012; Wang, Bansal, Gimpel, Ziebart, & Yu, 2015). sense
co-occurrence matrix computed training corpus, columns denoting
n induced senses rows denoting reference senses; cell (i, j) records
two senses co-occurrence frequencies. sense mapping, induced sense annotation
represented n-dimensional vector u non-zero values dimensions denoting
annotated senses. product uM produces m-dimensional vector v containing
distribution reference senses; sense largest corresponding value v
resulting annotation.
mapping function widely used WSID systems, comes two limitations: (1) induced senses considered equally informative producing reference
sense annotation, (2) weights assigned sense annotation effectively incorporated instances induced labeling multiple senses (Jurgens, 2012),
due part methods relative simplicity machine learning technique. Therefore,
constructing WSID systems, evaluate six alternate supervised learning algorithms
performing mapping function: Support Vector Machines (SVMs) linear
radial basis function (RBF) kernels, Decision Trees based either entropy Gini impurity, naive Bayes classifiers using either Multinomial Bernoulli distributions. six
classifiers trained feature vectors induced sense distinct feature
produce single sense label reference inventory. Feature vectors weighted
values provided WSI models annotation, except SVM classifier,
whose instance weights scaled [0,1], Bernoulli naive Bayes positive values set 1 due requirement binary data. Classifiers implemented
using SciKit (Pedregosa et al., 2011).
2.3 Ensemble WSID Model
Many WSI models including used exploit different sources lexical information
inducing senses thus identify different features distinguishing senses.
prior work WSD combined complementary WSD systems improve performance
ensemble model (Pedersen, 2000; Florian & Yarowsky, 2002; Brody, Navigli, &
Lapata, 2006; Sgaard & Johannsen, 2010), work pursued analogous ensemble
approach WSID.1 propose new heterogeneous ensemble WSID system built
output four WSI models. instance, output WSI systems
combined instance labeled induced senses systems, shown
Figure 1; combined annotations used features mapping function
1. note Stevens (2012) suggested using consensus clustering sense induction way creating
ensemble; however, quantitative analysis performed.

1030

fiSemi-supervised Learning Induced Word Senses State Art WSD

Single Model WSID
WSI
Sys.

Input context

bank fishing...

Induced
sense A#2

Mapping
Function

WordNet
sense
bank#n#1

Mapping
Function

WordNet
sense
bank#n#1

Ensemble WSID
WSI
Sys.

Induced
sense A#2

WSI
Sys. B

Induced
sense B#7

WSI
Sys. C

Induced
sense C#1

Figure 1: comparison single-model ensemble WSID systems, showing
ensemble combines output multiple WSI models using single mapping function.
predict sense. WSI models capture different aspects context,
ensemble-based system potentially identify induced senses combinations thereof
produce accurate mapping senses reference sense inventory.

3. Experimental Design
evaluate WSID WSD systems, first two experiments use common pseudoword
disambiguation task. Following, describe task data.
3.1 Pseudoword Disambiguation
Pseudowords provide analogous form polysemous data evaluating WSD systems.
pseudoword made two monosemous lemmas, referred pseudosenses.
occurrences pseudosenses corpus replaced unique token.
corresponding disambiguation task, WSD system must decide pseudosenses
originally present given occurrence token, effectively simulating traditional
sense disambiguation task. Independently proposed Gale et al. (1992) Schutze
(1992), pseudoword disambiguation fills important evaluation gap large amounts
sense-annotated data unavailable.
However, disambiguation performance pseudowords guaranteed model
difficulty disambiguating real words. example, Schutze (1998) uses pseudoword
pseudosenses banana door, semantically dissimilar; deciding
words pseudosenses akin disambiguating sense homograph,
known easier disambiguation task (Ide & Veronis, 1998; Navigli et al., 2007).
contrast, polysemous homographs instead senses semantic
related way therefore may appear similar contexts (Apresjan, 1974; Rodd,
Gaskell, & Marslen-Wilson, 2002; Palmer, Dang, & Fellbaum, 2007; Martnez Alonso et al.,
2013). Thus, constructing pseudowords arbitrarily-selected monosemous terms underestimates difficulty sense disambiguation results based pseudowords
would necessarily generalize.
1031

fiBaskaya & Jurgens

noun
doubles
pic
ca
drawer
tapestry
headshot

pseudosenses
badminton, tennis
movie, photo
calcium, california
desk, treasurer, cartoonist
complexity, cloth, rug
photo, soccer, gunfire

Table 1: Examples pseudowords polysemous nouns WordNet monosemous
lemmas comprise pseudosenses
Pilehvar Navigli (2013) propose solution problem appropriately choosing
pseudosenses disambiguation difficulty mirrors real-world data. pseudoword dataset created pseudoword models sense properties one
15,935 polysemous nouns WordNet 3.0. Specifically, pseudosenses selected closely
mimic inter-sense similarities corresponding polysemous word mining WordNets ontology find monosemous words (pseudosenses) whose structural arrangement
close correspondence polysemous words senses ontology.
noun hierarchy WordNet contains sufficient structure, dataset generated
nouns.2 inclusion parts speech ultimately desirable, nouns
alone represent significant challenge WSD systems multiple evaluations focused entirely disambiguating nouns (e.g., see Navigli, Jurgens, & Vannella, 2013). Table
1 shows example nouns corresponding pseudosenses used experiments.
practical utility pseudowords demonstrated Pilehvar Navigli
(2014), showed pseudowords disambiguation difficulty accurately mirrored
corresponding polysemous words. Specifically, WSD systems trained
noun portion Senseval-3 dataset (Mihalcea et al., 2004) dataset made
nouns corresponding pseudowords. resulting disambiguation performance
pseudosense-annotated dataset highly correlated performance Senseval3 data. results indicate using pseudowords, pseudosense-annotated
datasets used closely approximate real-world WordNet WSD performance, thereby
avoiding performance over-estimates caused early methods constructing pseudowords (Gaustad, 2001).
3.2 Data
experiments performed subset data Pilehvar Navigli (2013).
original dataset includes pseudosenses likely introduce noise results due
errors part speech tagging word takes part named entity
present WordNet. Therefore, control possible sources noise, exclude
pseudosenses (1) lemma also plural form another lemma, e.g., spirits, (2)
2. However, note principle, pseudowords could constructed comparatively
shallower verb hierarchy WordNet potentially adjectives using data Tsvetkov et al.
(2014).

1032

fiSemi-supervised Learning Induced Word Senses State Art WSD

F(Pseudowords correspondence X)

Pilehvar Navigli (2013) pseudowords
Pseudowords used study
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1

10

100

1000

Degree pseudoword correspondence real data
(lower better)

Figure 2: cumulative distribution functions degree correspondence
word derived pseudoword, specified dataset Pilehvar Navigli (2013).
Lower values indicate closer correspondence.

lemma may another part speech, e.g., freezing, (3) lemma occurs fewer
1,000 contexts Gigaword part named entity. test third
condition, used TreeTagger (Schmid, 1994) identify named entity mentions part
speech tagging corpus. third criteria necessary ensure sufficient number
instances available training testing, discussed later section 4.1.
dataset provides rating pseudoword indicating closely pseudosenses model senses corresponding word WordNet. example, pseudoword doubles shown Table 1 two pseudosenses, monosemous words tennis
badminton, closely model two senses (1) badminton played two players,
(2) tennis played two players. Replacing one pseudosenses
monosemous word desk would lower resulting pseudowords degree correspondence
since desk similar either words senses thus, disambiguation task
would potentially easier due dissimilarity contexts pseudosenses
appear.
subset data used experiments selected follows. First, pseudowords filtered according three aforementioned criteria. Second, remaining
pseudowords ranked according degree correspondence. Third, 920 senses
selected ranking match distribution polysemy values WordNet
(e.g., percentage disemous lemmas). third step performed
order ensure degrees polysemy dataset representative
distribution full dataset Pilehvar Navigli (2013). Figure 2 shows degree
correspondence real words (a) pseudowords used study (b)
full dataset Pilehvar Navigli (2013), highlighting subset used
experiments significantly higher correspondence real-world data full
dataset therefore maximally representative expected real-world performance.
1033

fiBaskaya & Jurgens

WordNet

Pseudowords

12000

700

Number words

500
8000
400
6000
300
4000
200
2000

Number pseudowords

600

10000

100

0

0
0

5

10
15
20
25
30
Number senses (degree polysemy)

35

Figure 3: Distributions number senses polysemous nouns WordNet
number pseudosenses selected pseudowords, chosen closely match
polysemy distribution WordNet
Pseudowords final dataset two twelve pseudosenses. Figure 3
shows polysemy distribution number senses selected pseudowords, compared polysemy distribution nouns WordNet. mapping functions described previously Section 2.2 parametric, five additional high-correspondence
disemous pseudowords also selected use parameter tuning, number senses
common dataset therefore representative.
3.3 Sense Distributions
frequency distribution words senses often peaked, one two senses occurring frequently rest (Passonneau, Salleb-Aouissi, & Ide, 2009). particular
sense distribution word greatly affect WSID performance, artificially-inflated
performance settings one sense occurs frequently induced senses
mapped sense; cases, WSID performance generalizable datasets
sense distribution may vary. Controlling effect sense distribution
real-world test data requires significant number annotated instances select, currently possible existing annotated corpora. However, using
pseudowords, sense distribution may precisely controlled gathering required
number usages pseudosense match desired distribution.
Precisely controlling sense distribution allows us measure WSID performance
within two extremes. first distribution, leverage correspondence
pseudowords senses WordNet senses simulate real-world sense distributions based
SemCor (Miller et al., 1993).
Specifically, noun corresponding pseudoword, measure frequencies
nouns senses SemCor, determines relative frequency
pseudowords pseudosenses. However, words still infrequent SemCor
1034

fiSemi-supervised Learning Induced Word Senses State Art WSD

accurately measure expected frequency senses. Therefore, words fewer
ten occurrences use average sense distribution computed words
polysemy least ten occurrences SemCor. refer resulting
dataset SemCor sense distribution.
SemCor distribution measures difficulty WSID expected setting
senses likely occur others. However, presence majority
class potentially mask important underlying performance differences systems;
one sense likely, models performance necessarily representative
ability distinguish senses word. Therefore, second distribution, words senses appear uniform probability. training
test data uniform sense distribution, WSD systems cannot use often-effective
strategy always choosing most-frequent sense seen training data. Furthermore,
Uniform-distributed data allows measuring ability sense mapping function
find correspondence induced reference senses induced senses
equivalent amounts data. uniform sense distribution representative
real-world data, comparison models performances SemCor- Uniformdistributed datasets provides critical insight disambiguation capabilities
expected generalizability new data arbitrary perturbations underlying sense
distribution. example, model performs well SemCor-distributed data
Uniform-distributed data, result suggests model effective identifying most-frequent sense; contrast, models perform well SemCor-
Uniform-distributed data would expected maintain accuracy data
sense distributions based ability discriminate senses one sense
frequent.

4. Experiment 1: Evaluating WSID Mapping
first experiment measures impact sense mapping function two ways. First,
given wide-spread use Agirre et al. (2006) mapping function, assess whether
six alternatives described Section 2.2 consistently improve WSID performance.
Second, assess whether proposed sense mapping functions effectively fuse
induced sense annotations multiple WSI models produce accurate ensemble model.
4.1 Experimental Setup
follows, detail parameters training WSI systems
training test data constructed.
4.1.1 WSID Systems
WSI models trained base corpus, ukWaC (Baroni, Bernardini, Ferraresi,
& Zanchetta, 2009), though emphasize models induce senses corpus
different ways. WSI parameter values used pseudowords. AI-KU
uses settings language model, S-CODE Fastsubs (Yuret, 2012) algorithms
reported Baskaya et al. (2013). Using setup SemEval 2013 WSI task,
AI-KU calculates lexical substitutes using SRILM (Stolcke, 2002) ukWaC
1035

fiBaskaya & Jurgens

Training partitions

Training instances

Data partitions
Uniform(
Test partition

Test instances
SemCor(

(a)

(b)

)

)
(c)

Figure 4: schematic cross validation. Data partitions initially contain equal
number instances per pseudosense (a), shown different colored boxes. fold
validation, four partitions used training one test (b). instances
partition sampled according distribution (shown italics), produces
training test datasets (c).

(Ferraresi, Zanchetta, Baroni, & Bernardini, 2008) corpus construct 4-gram
language model. k-means algorithm used AI-KU, k arbitrarily set 10,
parameter tuning. HDP uses two parameters specify variability
senses corpus, 0 , set values reported Lau
et al. (2012) Lau, Cook, Baldwin (2013). SquaT parameters set
0.00125 0.25, respectively, limited grid search showed values produced
sufficiently large graphs pseudowords. Chinese Whispers model nonparametric,
parameter choices needed.
total, twenty eight WSID configurations built combination
seven sense mapping functions (Sec. 2.2) four WSI models (Sec. 2.1). Additionally,
seven ensemble WSID systems built training mapping functions
induced sense labelings four WSI systems using default configuration.
4.1.2 Cross-Validation Evaluation
Systems evaluated using five-fold cross validation, modifications ensure
folds comparable sizes distribution types training data leaked
test data changing sense distribution test data. Initially, corpus
instances pseudoword divided five partitions, partition contains
number instances pseudosense. instances partition
filtered match desired sense distribution; filtering process deterministic
partition always instances particular distribution across folds. Figure
4 visualizes process. evaluation, four filtered partitions form training data
one partition used test data. Importantly, setup ensures instances remain
consistent partition used different folds validation. note
case ensemble WSID system, underlying WSI models trained
training data identical folds, ensuring separation test training data.
1036

fiSemi-supervised Learning Induced Word Senses State Art WSD

reported experiments use sense distributions training testing (either SemCor Uniform). However, evaluation setup sufficiently general
support using arbitrary distributions, including different distributions training
testing data, shown example Figure 4; results using additional combinations
distributions reported Supplementary Material.
4.1.3 Evaluation Data
data partitions drawn Gigaword corpus (Graff, Kong, Chen, &
Maeda, 2003). Instances pseudosenses filtered ensure correct part speech
remove occurrences pseudosense part named entity. Ultimately
partition test data contained 200 instances senses, filtered
according desired distribution. SemCor-distributed data, frequent sense
200 instances, senses proportional numbers based relative
sense frequencies. note setup chosen instead using fixed number
instances per partition bias results polysemous words
whose rarer sense would comparatively fewer instances fixed-size setting
case, WSID accuracy would significantly affected models ability
identify senses. number instances varies SemCor-distributed
data, corresponding Uniform-distributed data pseudoword balanced
total size, evenly distributed senses.
Two baselines used test data: Random Frequent Sense (MFS).
Random baseline simply picks randomly among senses; MFS baseline
selects frequent sense word, often performs competitively skewed
distributions SemCor outperformed many WSID models previous studies
(McCarthy et al., 2004; Kilgarriff, 2004; Navigli, 2009). Note Uniform sense
distribution, MFS Random baselines equivalent.
4.1.4 Scoring
Systems evaluated using standard WSD precision, recall F1 metrics (Navigli,
2009). Precision measures percentage sense assignments provided WSID system
identical gold standard. Recall measures percentage instances
correctly labeled system. system labels instances, precision recall
equivalent. number instances per term scales number senses,
precision recall considered microaverages WSD performance across words.
4.1.5 Parameter Tuning
Five disemous words used tune parametric mapping function. Using grid
search parameter values, WSID configuration scored using identical fivefold cross-validation process. parameter values produced highest average F1
across folds selected use WSID models mapping function.
1037

fiBaskaya & Jurgens

SquaT
CW

AI-KU
HDP

Ensemble
MFS

Random

1

F1 Score

0.9
0.8
0.7
0.6
0.5
0.4

e

e



.T

ec




i)



)
py

ro
nt

(G

(E

)

BF

(R
)

r
ea

B

6)

0
20

lN

B

.(

N

al


(L

.T

ec




SV


SV

lli

et

ia
om
tin

ul



ou
rn


e

irr
Ag

Figure 5: Average performance WSID systems training testing data follow
SemCor sense distribution

4.2 Results Discussion
results WSID systems using single WSI model demonstrate high sense disambiguation performance possible using suitable sense mapping function
multiple WSI models may effectively combined ensemble.
4.2.1 Single-Model Results
WSID system evaluation showed clear impact choice mapping function.
Results untuned WSID systems using SemCor distribution shown Figure
5.3 nearly systems, Agirre et al. (2006) method mapped induced senses
most-frequent sense seen SemCor-distributed training data, ignoring sense
distinctions recognized WSI model. Agirre et al. method produce
WSID systems outperform using Multinomial Nave Bayes, performance
says little discriminative capabilities WSID systems effectively prevents
meaningful comparison, hindering testing development new WSID systems.
contrast performance using Agirre et al. mapping function, SVM
Bayesian functions produced two WSID systems outperformed MFS
baseline. best performance using single WSI model comes SVM
linear kernel, provides slightly higher performance RBF kernel. Indeed,
WSID system using AI-KU model linear kernel SVM mapping function
3. WSID systems, tuning parameters mapping function provided little performance
improvement either sense distribution. therefore omit tuned results brevity, report
scores Supplementary Materials.

1038

fiSemi-supervised Learning Induced Word Senses State Art WSD

SquaT
CW

AI-KU
HDP

Ensemble
MFS

Random

1

F1 Score

0.9
0.8
0.7
0.6
0.5
0.4

e

e



.T

ec




i)



)
py

ro
nt

(G

(E

)

BF

(R
)

B

6)

0
20

lN

r
ea


(L

.T

ec




SV


SV

B

.(

N

al

ia
om
tin

ul



lli

et

ou
rn


e

irr
Ag

Figure 6: Average performance WSID systems training testing data follow
Uniform sense distribution
3.8% increase F1 score MFS baseline, statistically significant p < 106
using McNemars test significance.
impact choice sense mapping function WSID performance even
evident Uniform-distribution dataset. results, shown Figure 6, reveal
significant differences discriminatory capabilities WSID systems. WSID systems
using Agirre et al. mapping function perform well average, indicating function
capable learning effective correspondence senses single sense
dominates frequency. Nevertheless, WSI models enjoy consistently-higher performance
using SVM mapping function, statistically significant improvement
p < 106 . Furthermore, even worst-performing model, SquaT, still able
double performance MFS baseline using SVM Decision Tree mapping
functions.
single-model results Uniform distribution also provide insight models would expected perform new datasets sense distributions differ
SemCor-distributed data. Systems performances relatively close
tested SemCor dataset differed 0.04 F1 linear-kernel SVM;
contrast, systems differed 0.278 F1 Uniform-distributed data,
indicating significant differences WSI models abilities find meaningful sense distinctions. Furthermore, clear difference seen distributional graph-based
WSI approaches, suggesting distributional techniques may robust potential
changes corpuss sense distribution.
overall ranking individual-model WSID systems consistent across
different mapping functions. However, rank oscillation appear HDP
AI-KU models Uniform distribution setting CW SquaT
1039

fiBaskaya & Jurgens

models SemCor distribution setting. cases, SVM-based mapping functions
provide highest average performance across systems produce identical rankings.
such, view ranking differences mapping functions artifact
mapping function itself, rather due actual performance differences
systems.
4.2.2 Ensemble-Model Results
nearly WSID configurations, ensemble WSID system obtains substantial performance gains MFS baseline best WSID system built single
WSI model. SemCor-distributed data (Fig. 5), ensemble linear kernel SVM
produces highest performance WSID configurations, achieving 9.4% increase
F1 MFS 4.2% increase next-closest system (AI-KU). Furthermore,
except using Agirre et al. (2006) Multinomial Nave Bayes mapping functions,
ensemble WSID systems outperforms individual WSID systems. testing
training Uniform sense distribution, ensemble WSID system achieves even
substantial gains WSID systems, shown Figure 6, 13.6% increase
F1 next-closest system.
results distributions indicate using linear kernel SVM sense
mapping provides consistently superior WSID performance robust variations
choice WSI model. Furthermore, annotations induced senses contain
complementary sources information, case ensemble sense labeling,
SVM mapping function able produce better quality sense annotations.
success ensemble using mapping functions method
Agirre et al. (2006) highlights potential obstacle research community: prior
attempts building ensemble WSID methods may considered, performance
benefit would observed due current community-wide practice using
method Agirre et al. Further, work raises possibility new mapping functions
could developed more-effectively combined induced senses.
4.2.3 Quantifying Impact Polysemy Disambiguation Performance
Given high performance using linear-kernel SVM mapping function, performed follow-up analysis measure performance effect relative number
senses per word. analysis separates improvement relative difficulty
disambiguation provides more-complete picture WSID performance. Performances
pseudowords six senses combined due words relative infrequency. Figures 7 8 show performances per term SemCor Uniform sense
distributions, respectively, using box whisker plot. Whiskers denote maximum
minimum F1 pseudoword, boxes denote first third quartiles,
middle line denotes median performance. baselines performances change
polysemy, plotted horizontal line.
seen Figures 7 8, ensemble WSID system offers superior performance
across levels polysemy. example, although one sense disemous words occurs
vast majority instances SemCor data, ensemble WSID performance
still able surpass MFS baseline nearly words (as shown left-most
1040

fiSemi-supervised Learning Induced Word Senses State Art WSD

1

F1 Score

0.8

0.6

0.4

0.2

0

SquaT
CW
AI-KU
HDP
2

Ensemble
MFS
Random
3

4
5
Pseudoword Polysemy

6

Figure 7: Performance WSID systems using linear-kernel SVM different polysemy
SemCor-distributed training testing data
1

F1 Score

0.8

0.6

0.4

0.2

0

SquaT
CW
AI-KU
HDP
2

Ensemble
MFS
Random
3

4

5

6

Pseudoword Polysemy

Figure 8: Performance WSID systems using linear-kernel SVM different polysemy
Uniform-distributed training testing data
cluster boxes Figure 7). results settings indicate ensemble WSID
model would offer superior performance new data arbitrary sense distributions.
Indeed, current datasets, words either two three senses (87%),
ensemble WSID model sees smallest improvement MFS. evaluated
1041

fiBaskaya & Jurgens

corpus containing words senses, overall performance improvement
WSID ensemble MFS baseline would even higher reported main
results Experiment 1 (Figure 5).

5. Experiment 2: Comparing WSID Supervised WSD
sufficient sense-annotated data available, supervised machine learning typically
shown produce best-performing WSD systems. However, results Experiment 1 indicate high WSD performance also possible using semi-supervised WSID
model. approaches require amount sense-annotated data, raises
question circumstances one approach expected outperform
other. Therefore, Experiment 2, perform direct comparison semi-supervised
WSID systems current state art supervised WSD, using identical training
data. results experiment direct implications sense annotation efforts
deciding much data necessary high performance.
5.1 Experimental Setup
follows, describe configuration supervised WSD system used
comparison training data created.
5.1.1 Supervised WSD
comparison, use It-Makes-Sense (IMS) (Zhong & Ng, 2010), state-of-the-art supervised WSD algorithm. disambiguate usage sentence-length context, IMS extracts
features consisting neighboring lemmas POS along neighboring collocation pairs. IMS uses linear-kernel SVM feature vectors predicting sense.
experiments, IMS trained using default algorithmic parameter values specified
publicly-available implementation.
experiments intentionally measuring disambiguation ability fullytrained IMS system provided authors;4 rather, experiments intended directly compare results using current state-of-the-art supervised WSD algorithm.
would possible retrain WSID systems training data used authors fully-trained model, annotated corpora used original experiments
readily available sense distributions corpora controlled for, making
conclusions experiment difficult generalize.
5.1.2 Training Test Data
Experiment 2, multiple datasets created increasing amounts training data
order measures ability WSID supervised WSD condition.
datasets generated similarly instances allocated sense distributions
Experiment 1. pseudoword, SemCor-distributed training data constructed
selecting k instances frequent sense, senses assigned proportional
number instances based relative frequency. different number
4. http://www.comp.nus.edu.sg/~nlp/software.html

1042

fiSemi-supervised Learning Induced Word Senses State Art WSD

AIKU
HDP

CW
SquaT

Ensemble
IMS

MFS

F1 Score

1

0.9

0.8

0
75

0
50

0
25
0
20

0
15

0
10

75

50

25

10

k

Figure 9: Performance IMS WSID systems SemCor-distributed data

instances per word SemCor-distributed training data, Uniform-distributed
data created way account difference: Given specific k word
n total instances senses, corresponding Uniform-distributed training
n
data constructed including
instances pseudowords senses.
notational clarity, use k denote equivalently-sized Uniform-distributed dataset
whose corresponding SemCor-distributed training data k instances.
Training test data generated Gigaword data used previous
experiments, using five folds cross validation. Training data generated k = {10,
25, 50, 75, 100, 150, 200, 250, 500, 750}. WSID IMS systems trained
k k datasets created four partitions tested fifth, full partition.
test set identical Experiment 1 instances used
testing, resulting performances k k directly comparable results
Experiment 1.
5.1.3 WSID Systems
WSID systems constructed using procedure used previous experiments
(cf. Sec. 4.1). simplicity, report WSID systems using linear kernel SVM,
provided highest performance. Full results configuration available
Supplementary Material.
5.2 Results Discussion
results reveal WSID systems offer superior performance IMS
annotated instances available. Figures 9 10 show resulting F1 scores
1043

fiBaskaya & Jurgens

AIKU
HDP

CW
SquaT

Ensemble
IMS

MFS
Random

0.9

F1 Score

0.8

0.7

0.6

0.5

0
75

0
50

0
25
0
20

0
15

0
10

75

50

25

10

k^

Figure 10: Performance IMS WSID systems Uniform-distributed data

SemCor- Uniform-distributed data, x-axis drawn log scale. random baseline
(F1=0.432) omitted Figure 9 better visual contrast.
SemCor-distributed data, IMS outperforms single-model WSID systems nearly
values k, though AI-KU HDP models closely competitive differing less
1% F1 k 75. contrast single-model WSID systems, ensemble WSID
system outperforms IMS starting k=25 k=500. ensemble performance
differences 25 k 250 statistically significant p < 106 IMS WSID
performances k=500 statistically equivalent. Given publicly-distributed IMS
system trained average 35 instances per word type, results suggest
training ensemble WSID model data would provide superior performance.
training instances available, cases words, IMS algorithm
able correctly learn back-off strategy always selecting frequent sense
training data, thereby ensuring performs least well MFS baseline.
contrast, mapping function WSID models slightly noisier learn
accurate mapping induced senses reference senses, resulting performance
MFS baseline.
similar trend seen testing Uniform-distributed data (Figure 10), though
ensemble WSID system outperforms IMS k = 10 k=200, point
statistically equivalent, HDP model initially outperforms IMS well
k = 25. results Uniform-distributed setting indicate WSID models
provide accurate discriminatory techniques.
Together results suggest ensemble WSID models offer significant advantages supervised WSD except little large amounts sense-annotated data
available. Indeed, 97 11,685 polysemous lemmas SemCor fewer
200 instances, suggests ensemble WSID system may offer better performance existing supervised systems trained corpus. results also
1044

fiSemi-supervised Learning Induced Word Senses State Art WSD

indicate using unsupervised features WSI models, ensemble WSID
system able break knowledge acquisition bottleneck acquire information
disambiguation available annotated data alone.
Last, note increasing amount training data consistently improves
performance IMS, providing decreasing benefit WSID models. contrast
highlights difference systems learn. Training WSID model additional
sense-annotated data cannot directly improve disambiguation performance contextual
features used disambiguation fixed underlying WSI model, independent training data. contrast, providing data supervised WSD system
may enable learn new features disambiguation. Nevertheless, performance
WSID depend sufficient sense-annotated data train correct mapping
function, shown large performance improvements k=10 k=25 shown
Figures 9 10 increase training data provides substantial improvement
sense mapping.

6. Experiment 3: Evaluation SemEval Systems
Experiments 1 2 demonstrated WSID models capable accurate WSD
individual WSI models combined ensemble, resulting system
capable outperforming fully-supervised WSD. However, experiments performed
controlled conditions pseudoword data. Therefore, Experiment 3, test whether
observed performance improvements carry real-world data. three tests using
thirty WSI models two sense inventories, evaluate impact new
mapping function ensemble construction extensions prior WSID evaluations.
6.1 Experimental Setup
evaluate ensemble WSID setup sense-annotated data, use three SemEval
tasks included WSID evaluation: 2007 Task 2 (Agirre & Soroa, 2007), 2010 Task 10
(Manandhar et al., 2010), 2013 Task 13 (Jurgens & Klapaftis, 2013). repeat
tasks exact evaluation setup, exception sense mapping function originally
used task replaced SVM using linear kernel.
Two significant differences exist tasks setup compared earlier experiments. 2007 2010 tasks use OntoNotes senses (Hovy et al., 2006), known
coarse-grained WordNet senses pseudowords models. Second,
2013 task also focuses instances multiple meanings may evident (e.g., due
ambiguity syllepsis) therefore includes gold standard data instances
multiple sense labels. experimental setup focused instances
one sense interpretation, adopt single-sense evaluation described Jurgens
Klapaftis (2013) using subset 4122 instances task data single
sense annotation.
Ensemble systems created using induced sense answers systems
participated task linear kernel SVM perform sense mapping.
intentionally use original WSI models rather four models used earlier
experiments order test benefits proposed WSID configuration different settings quantify generalizability. However, note two highest-performing
1045

fiBaskaya & Jurgens

Ensembles
SemEval

MFS

Best System

All-Systems

Best-Configuration

2007
2010
2013

0.787
0.587
0.477

0.816
0.624
0.640

0.828
0.680
0.640

0.670
0.657

Table 2: comparison best-performing system SemEval WSID task
proposed ensemble method.
WSI systems prior experiments, AI-KU HDP, also participated 2013 task,
ensemble results task expected similar. task, consider
two ensembles: (1) outputs WSI systems, (2) outputs best configuration system, measured according WSID performance original
task. note 2007 task allowed one configuration per system, one
ensemble produced.
6.2 Results
results ensemble single WSI-model systems, described next, demonstrate
benefits using new WSID construction procedure.
6.2.1 Ensemble Results
three tasks, ensemble WSID configuration shows performance improvements
best-performing system MFS. Table 2 shows results three tasks,
including scores best-performing system task originally tasks MFS
score. Improvements MFS significant p < 106 using McNemars test
significance. Similarly, improvements best systems task significant
p < 106 ensemble configurations, exception ensemble SemEval2007, significant p < 104 . Furthermore, note improvement
ensemble task larger difference tasks best secondbest systems, indicating substantial increase performance. results demonstrate
consistent performance improvements SVM-based ensemble WSID model even
using different sense inventories entirely different sets WSI systems.
6.2.2 Single-Model Results
Single-system WSID models varied whether use SVM mapping function improved performance, shown Figure 11.5 SemEval-2007, systems obtained lower
F1 using SVM, average decrease 0.032 change overall
system ranking. contrast, systems performed better 2010 2013 tasks
using SVM, average F1 increases 0.043 0.004. Although mixed trends
initially seem contradictory prior experiments results showing consistent benefit
SVM, performance differences partly due differences task setup
5. Full score details available Supplementary Material.

1046

fiSemi-supervised Learning Induced Word Senses State Art WSD

0.8

0.7
Linear-kernel SVM

0.7
Linear-kernel SVM

Linear-kernel SVM

0.9

0.6
0.5
0.4
0.3

0.7
0.7

0.8
0.9
Agirre et al. (2006)

(a) SemEval-2007

0.6

0.5
0.3

0.4 0.5 0.6 0.7
Agirre et al. (2006)

(b) SemEval-2010

0.5

0.6
0.7
Agirre et al. (2006)

(c) SemEval-2013

Figure 11: Comparisons F1 scores system SemEval tasks using
linear-kernel SVM mapping function (y-axis) versus Agirre et al. (2006) (x-axis).
Points diagonal indicate improvement using SVM function.

WSI systems designed label usage single induced sense.
contrast, systems prior experiments reported multiple induced senses per instance, weighted applicable sense instance. multiple senses
provide richer feature set training enable recognizing cases lower-weighted
induced senses provide information correct sense annotation. WSI system
reports single sense, WSID system performance upper-bound based
reference sense highest conditional probability, given induced sense.
Even single induced sense reported, using SVM mapping function still
significantly impact resulting performance, shown 2010 task. Here, multiple systems lowest ranked achieved significant improvements F1 seeing
0.30 absolute increases. performances differences also highlight unique feature
2010 task; Pedersen (2010) submitted four systems generated random sense assignments, ranked high 18th 26 systems. SVM-based ranking
correctly assigns four random-answer submissions lowest four ranks. Indeed,
overall system ranking task changes dramatically originally-reported
ranking (Spearmans =0.14). results (Fig. 11b) suggest systems performing random chance, systems actually differ little abilities task
previously low-ranked systems actually offer superior performance. Thus,
overall performance task high, SVM-based moel reveals
true discriminatory capabilities WSI systems, partially masked
many induced senses mapped frequent reference sense, artificially increasing
performance.

7. Related Work
present study touches upon three bodies prior work word sense induction
relationship WSD, semi-supervised WSD, work pseudowords.
7.1 Word Sense Induction Disambiguation
Purandare Pedersen (2004) Niu, Li, Srihari, Li (2005a) produce sense induction models assign induced senses directly reference senses, rather
1047

fiBaskaya & Jurgens

creating mapping function converts induced sense annotations. report finding
induced senses closely correspond existing definitions reference sense inventories
neither analyze performance disambiguating new instances reference senses,
role WSID systems study. noted Section 2, Agirre et al. (2006)
first formalized WSID process. experiments, WSID system built
HyperLex WSI model (Veronis, 2004) mapping function; resulting system
obtained 0.06 improvement F1 score MFS baseline default parameters
0.11 improvement MFS tuned, suggesting high WSID performance possible. Last, Jurgens (2012) notes potential WSI models annotate
items multiple senses proposes modification mapping function Agirre
et al. improve performance multiple induced senses annotation weighted.
experiments, WSID systems new mapping function able outperform
MFS baseline.
7.2 Pseudowords
Since first proposed word sense disambiguation (Gale et al., 1992; Schutze, 1992),
pseudowords incorporated evaluations multiple tasks, specific recommendations improve construction tasks modeling selectional
preferences (Chambers & Jurafsky, 2010), modeling word co-occurrence (Dagan, Lee, &
Pereira, 1999), machine translation (Duan, Zhang, & Li, 2010), tasks Information Retrieval (Stokoe, 2005) even improving word embeddings (Liu, Liu, Chua, & Sun, 2015).
Indeed WSD, new techniques proposed adapting pseudowords
languages Chinese (Lu, Ting, & Sheng, 2004; Lu, Wang, Yao, Liu, & Li, 2006)
creating pseudowords accurately model difficulty WSD (Nakov &
Hearst, 2003; Otrusina & Smrz, 2010; Pilehvar & Navigli, 2013), though approach
Pilehvar Navigli (2013) used shown closely correlate
real-world performance.
related study work analyzing word senses using pseudowords.
Cook Hirst (2011) simulate sense properties lemmas Senseval-3 lexical
sample task (Mihalcea et al., 2004) order model process lemmas acquire
new senses; however, pseudowords analyzed contextual features rather using
WSD done study. test discriminatory ability WSI models, Jurgens
Stevens (2011) create set disemous pseudowords pseudosenses varying
degrees similarity. represent full range pseudosense similarities, similarity
words pseudosenses measured using corpus-based distributional similarity
pseudosenses paired pseudoword based similar corpus frequencies
positions along similarity spectrum. Sense induction models tested according
ability discriminate pseudosenses different similarity levels. However, pseudosenses used study monosemous limits ability replicate
effect new corpora, may potentially different sense distributions polysemous pseudosenses. Last, similar study Pilehvar Navigli (2014),
used pseudoword dataset analyze supervised unsupervised
WSD. findings corroborate study, indicating pseudowords
1048

fiSemi-supervised Learning Induced Word Senses State Art WSD

Pilehvar Navigli (2013) used design WSD-related evaluations mirror
real-world performance.

7.3 Semi-supervised WSD
Beyond WSID, approaches applied semi-supervised learning WSD. Mihalcea (2004) applies co-training self-training supervised classifier Lee Ng
(2002), showing techniques reduce disambiguation error many words
using high-confidence automatically-labeled examples; however, techniques required
parameter tuning, parameter set providing high performance words. Pham,
Ng, Lee (2005) investigate four semi-supervised techniques, showing spectral graph transduction co-training performed best, performance high
purely-supervised WSD methods. Rather use automatically-labeled data, Yuret
(2007) generates new contexts existing training data using lexical substitution,
ultimately improve performance new substitutes
added. contrast, works seen improvement fully-supervised
systems using semi-supervised techniques. Niu, Ji, Tan (2005b) construct graph
word uses, edges weighted usages contextual similarity. annotated instances labeled graph senses label propagation run
graph infer remaining instances labels, resulting performance superior
SVM-based WSD comparison system. Similarly, Kubler Zhekova (2009) able
filter automatically-annotated data based expected quality supplement
training data. combination manually- automatically-annotated data
provided slight improvement original data, though note approach
able automatically annotate small number contexts per word, illustrating main
challenge semi-supervised learning significantly increasing number training instances. Martinez, De Lacalle, Agirre (2008) identify monosemous synonyms target
nouns using WordNet query examples synonyms Web create
corpus automatically sense-annotated examples training. WSD performance
closely related distribution word senses, additional heuristics used estimate sense distribution testing data (McCarthy et al., 2004) training
supervised WSD system automatically-produced data. resulting system attained
significantly higher performance unsupervised systems still outperformed
fully-supervised systems.
common thread works need extensive filtering unlabeled
instances order obtain performance improvements; including many lower-quality
examples typically resulted performance supervised technique trained
data. contrast, several WSID systems tested outperformed state art
without need filtering instances used WSI algorithms. difference
need filtering suggests WSI may robust noise unlabeled instances
WSI could even potential preprocessing step finding instances
paradigmatic induced senses later use input semi-supervised techniques.
1049

fiBaskaya & Jurgens

8. Conclusion
paper presents comprehensive analysis construction evaluation WSID
systems. Systems tested using novel evaluation design incorporating 920 pseudowords
data set Pilehvar Navigli (2013), whose pseudosenses closely approximate
properties disambiguation difficulty noun senses WordNet 3.0. tests
million instances, provide three empirical contributions. First, demonstrate
choice mapping function used convert induced senses significantly
affect WSID performance linear kernel SVM significantly improves upon
current state art practices (Agirre et al., 2006), performance increases 3.8%
F1 settings. Second, demonstrate using linear kernel SVM, joining multiple WSI models ensemble WSID system yields large improvements,
seen using prior state art (an 8.5% F1 increase). benefit
ensemble setup demonstrated tests real sense-annotated data using multiple ensemble configurations different sense inventories, highlighting
robustness. Third, direct comparison state art supervised WSD
system (Zhong & Ng, 2010), demonstrate ensemble WSID system offers superior performance supervised system using training data except
hundreds annotated instances available, suggesting WSID viable mechanism overcoming knowledge acquisition bottleneck. line
research, released implementations WSI models implementation pseudoword testing framework freely-available open source software
(https://github.com/osmanbaskaya/mapping-impact). Furthermore, practical result
effort, intended release large-scale all-words WSID system based ensemble
model.
results study motivate three interesting avenues future work plan
explore. First, results indicate annotated instances necessary
relatively high WSD performance. Recent work shown controlling
difficulty humans annotating contexts (as measured using Passonneau
& Carpenter, 2014), quality training data and, subsequently, performance
WSD system may improved (Lopez de Lacalle & Agirre, 2015). Together, findings
suggest high performance WSID systems could quickly created appropriately
curating instances annotated training data. future work, intend
measure effect annotation selection WSID examine whether WSI process
might also informative instances select human annotation.
Second, experiments conducted English-language pseudowords. future
work, plan develop analogous pseudoword data WordNet ontologies languages (Bond & Foster, 2013) replicate experiments multilingual data
measure potential language-specific effects sense-annotated data sparse. also
plan investigate using translation cross-lingual sense mappings transfer information English languages way gathering annotations WSD
systems, analogous done part speech tagging (Duong et al., 2014).
Third, examined WSI models trained tested multi-domain ukWaC
corpus. Typically, WSD performed much worse tested novel domains,
typically contain dissimilar contexts training data cases,
1050

fiSemi-supervised Learning Induced Word Senses State Art WSD

words may different dominant senses (Magnini et al., 2002; Preiss & Stevenson,
2013). However, prior works shown small amount sense-annotation
novel domain significantly improve WSD performance new domain (Khapra et al.,
2010). future work, evaluate whether WSI system used effectively
annotate new instances novel domain instead requiring manual annotation, thus
providing unsupervised method domain adaptation.

Acknowledgments
thank Mohammad Taher Pilehvar many thoughtful discussions assistance
pseudoword dataset. also thank reviewers comments suggestions.

References
Agirre, E., de Lacalle, O. L., & Soroa, A. (2014). Random walks knowledge-based word
sense disambiguation. Computational Linguistics, 40 (1), 5784.
Agirre, E., & Martinez, D. (2000). Exploring automatic word sense disambiguation
decision lists web. Proceedings COLING-2000 Workshop Semantic Annotation Intelligent Content, pp. 1119. Association Computational
Linguistics.
Agirre, E., Martnez, D., de Lacalle, O. L., & Soroa, A. (2006). Evaluating optimizing parameters unsupervised graph-based WSD algorithm. Proceedings
TextGraphs: First Workshop Graph Based Methods Natural Language
Processing, pp. 8996. Association Computational Linguistics.
Agirre, E., & Soroa, A. (2007). Semeval-2007 task 02: Evaluating word sense induction
discrimination systems. Proceedings Fourth International Workshop
Semantic Evaluations, pp. 712. ACL.
Apresjan, J. D. (1974). Regular polysemy. Linguistics, 12 (142), 532.
Baroni, M., Bernardini, S., Ferraresi, A., & Zanchetta, E. (2009). WaCky wide web:
collection large linguistically processed web-crawled corpora. Language
Resources Evaluation, 43 (3), 209226.
Baskaya, O., Sert, E., Cirik, V., & Yuret, D. (2013). Ai-ku: Using substitute vectors
co-occurrence modeling word sense induction disambiguation. Proceedings
Seventh International Workshop Semantic Evaluation (SemEval), pp. 300306.
Biemann, C. (2006). Chinese whispers: efficient graph clustering algorithm application natural language processing problems. Proceedings First Workshop
Graph Based Methods Natural Language Processing, pp. 7380. Association
Computational Linguistics.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal
Machine Learning Research, 3, 9931022.
1051

fiBaskaya & Jurgens

Bond, F., & Foster, R. (2013). Linking extending open multilingual wordnet. Proceedings 51st Annual Meeting Association Computational Linguistics
(ACL), pp. 13521362.
Brody, S., & Lapata, M. (2009). Bayesian word sense induction. Proceedings 12th
Conference European Chapter Association Computational Linguistics
(EACL), pp. 103111. Association Computational Linguistics.
Brody, S., Navigli, R., & Lapata, M. (2006). Ensemble methods unsupervised wsd.
Proceedings 21st International Conference Computational Linguistics
44th annual meeting Association Computational Linguistics (COLINGACL), pp. 97104. Association Computational Linguistics.
Carpuat, M., & Wu, D. (2007). Improving statistical machine translation using word sense
disambiguation. Proceedings Joint Conference Empirical Methods Natural Language Processing Computational Natural Language Learning (EMNLPCoNLL), pp. 6172. Association Computational Linguistics.
Chambers, N., & Jurafsky, D. (2010). Improving Use Pseudo-Words Evaluating
Selectional Preferences. Proceedings 48th Annual Meeting Association
Computational Linguistics (ACL). Association Computational Linguistics.
Chan, Y., Ng, H., & Chiang, D. (2007). Word sense disambiguation improves statistical
machine translation. Proceedings Association Computational Linguistics.
Association Computational Linguistics.
Cook, P., & Hirst, G. (2011). Automatic identification words novel infrequent
senses. Proceedings 25th Pacific Asia Conference Language Information
Computation (PACLIC), pp. 265274.
Dagan, I., Lee, L., & Pereira, F. C. (1999). Similarity-based models word cooccurrence
probabilities. Machine Learning, 34 (1-3), 4369.
Di Marco, A., & Navigli, R. (2012). Clustering diversifying web search results
graph-based word sense induction. Computational Linguistics, 39 (4).
Duan, X., Zhang, M., & Li, H. (2010). Pseudo-word phrase-based machine translation. Proceedings 48th Annual Meeting Association Computational
Linguistics (ACL), pp. 148156. Association Computational Linguistics.
Duong, L., Cohn, T., Verspoor, K., Bird, S., & Cook, P. (2014). get
1000 tokens? case study multilingual pos tagging resource-poor languages.
Proceedings Conference Empirical Methods Natural Language Processing
(EMNLP), pp. 886897. Association Computational Linguistics.
Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press.
Ferraresi, A., Zanchetta, E., Baroni, M., & Bernardini, S. (2008). Introducing evaluating
ukWaC, large web-derived corpus English. Proceedings 4th Web
Corpus Workshop (WAC).
Florian, R., & Yarowsky, D. (2002). Modeling consensus: Classifier combination word
sense disambiguation. Proceedings Conference Empirical Methods Natural
Language Processing (EMNLP), pp. 2532. Association Computational Linguistics.
1052

fiSemi-supervised Learning Induced Word Senses State Art WSD

Gale, W. A., Church, K. W., & Yarowsky, D. (1992). Work statistical methods
word sense disambiguation. AAAI Fall Symposium Probabilistic Approaches
Natural Language, pp. 5460.
Gaustad, T. (2001). Statistical corpus-based word sense disambiguation: Pseudowords vs.
real ambiguous words. Proceedings ACL Student Research Workshop, pp.
6166. Association Computational Linguistics.
Graff, D., Kong, J., Chen, K., & Maeda, K. (2003). English Gigaword, LDC2003T05..
Linguistic Data Consortium.
Griffiths, T. L., & Steyvers, M. (2004). Finding scientific topics. Proceedings National
Academy Sciences, 101 (suppl 1), 52285235.
Hartmann, S., Gurevych, I., & Lap, U. K. P. (2013). Framenet way babel: Creating
bilingual framenet using wiktionary interlingual connection. Proceedings
51th Annual Meeting Association Computational Linguistics (ACL 2013).
Association Computational Linguistics.
Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., & Weischedel, R. (2006). OntoNotes:
90% solution. Proceedings 2006 Conference North American Chapter
Association Computational Linguistics Human Language Technology
(NAACL-HLT), pp. 5760. Association Computational Linguistics.
Ide, N., & Veronis, J. (1998). Introduction special issue word sense disambiguation:
state art. Computational linguistics, 24 (1), 240.
Jurgens, D. (2012). Evaluation Graded Sense Disambiguation using Word Sense
Induction. Proceedings First Joint Conference Lexical Computational
Semantics (*SEM). Association Computational Linguistics.
Jurgens, D., & Klapaftis, I. (2013). SemEval-2013 Task 13: Word Sense Induction Graded
Non-Graded Senses. Proceedings Seventh International Workshop
Semantic Evaluation (SemEval). Association Computational Linguistics.
Jurgens, D., & Stevens, K. (2011). Measuring impact sense similarity word sense
induction. Proceedings First Workshop Unsupervised Learning NLP,
pp. 113123. Association Computational Linguistics.
Khapra, M., Kulkarni, A., Sohoney, S., & Bhattacharyya, P. (2010). words domain
adapted wsd: Finding middle ground supervision unsupervision.
Proceedings 48th Annual Meeting Association Computational Linguistics (ACL), pp. 15321541. Association Computational Linguistics.
Kilgarriff, A., & Rosenzweig, J. (2000). Framework results english senseval. Computers Humanities, 34 (1), 1548.
Kilgarriff, A. (2004). dominant commonest sense word?. Text, Speech
Dialogue, pp. 103111. Springer.
Klapaftis, I. P., & Manandhar, S. (2010). Word sense induction & disambiguation using
hierarchical random graphs. Proceedings Conference Empirical Methods
Natural Language Processing (EMNLP), pp. 745755. Association Computational
Linguistics.
1053

fiBaskaya & Jurgens

Kubler, S., & Zhekova, D. (2009). Semi-supervised learning word sense disambiguation:
Quality vs. quantity.. Proceedings Conference Recent Advances Natural
Language Processing (RANLP).
Lau, J. H., Cook, P., & Baldwin, T. (2013). unimelb: Topic Modelling-based Word Sense
Induction. Proceedings Seventh International Workshop Semantic Evaluation (SemEval), pp. 307311. Association Computational Linguistics.
Lau, J. H., Cook, P., McCarthy, D., Newman, D., & Baldwin, T. (2012). Word sense
induction novel sense detection. Proceedings 13th Conference
European Chapter Association Computational Linguistics (EACL 2012).
Association Computational Linguistics.
Lee, Y. K., & Ng, H. T. (2002). empirical evaluation knowledge sources learning
algorithms word sense disambiguation. Proceedings Conference Empirical Methods Natural Language Processing (EMNLP), pp. 4148. Association
Computational Linguistics.
Liu, Y., Liu, Z., Chua, T.-S., & Sun, M. (2015). Topical word embeddings. Proceedings
29th AAAI Conference Artificial Intelligence (AAAI).
Lopez de Lacalle, O., & Agirre, E. (2015). Crowdsourced word sense annotations
difficult words examples. Proceedings 11th International Conference
Computational Semantics (IWCS).
Lu, Z., Ting, L., & Sheng, L. (2004). Combining neural networks statistics chinese word sense disambiguation. Proceedings Third SIGHAN Workshop
Chinese Language Processing.
Lu, Z., Wang, H., Yao, J., Liu, T., & Li, S. (2006). equivalent pseudoword solution
chinese word sense disambiguation. Proceedings 21st International Conference Computational Linguistics 44th annual meeting Association
Computational Linguistics (COLING-ACL), pp. 457464. Association Computational Linguistics.
Magnini, B., Strapparava, C., Pezzulo, G., & Gliozzo, A. (2002). role domain information word sense disambiguation. Natural Language Engineering, 8 (4), 359373.
Manandhar, S., Klapaftis, I. P., Dligach, D., & Pradhan, S. S. (2010). SemEval-2010 Task
14: Word Sense Induction & Disambiguation. Proceedings Fifth International
Workshop Semantic Evaluation (SemEval), pp. 6368. Association Computational Linguistics.
Maron, Y., Lamar, M., & Bienenstock, E. (2010). Sphere Embedding: Application
Part-of-Speech Induction. Lafferty, J., Williams, C. K. I., Shawe-Taylor, J., Zemel,
R. S., & Culotta, A. (Eds.), Advances Neural Information Processing Systems 23
(NIPS), pp. 15671575.
Martinez, D., De Lacalle, O. L., & Agirre, E. (2008). use automatically acquired
examples all-nouns word sense disambiguation.. Journal Artificial Intelligence
Resesarch (JAIR), 33, 79107.
Martnez Alonso, H., et al. (2013). Annotation regular polysemy: empirical assessment
underspecified sense. Ph.D. thesis, Universitat Pompeu Fabra.
1054

fiSemi-supervised Learning Induced Word Senses State Art WSD

McCarthy, D., Koeling, R., Weeds, J., & Carroll, J. (2004). Finding Predominant Word
Senses Untagged Text. Proceedings 42nd Annual Meeting Association
Computational Linguistics (ACL), p. 279, Morristown, NJ, USA. Association
Computational Linguistics.
Mihalcea, R. (2004). Co-training self-training word sense disambiguation. Proceedings Conference Natural Language Learning (CoNLL). Association
Computational Linguistics.
Mihalcea, R., Chklovski, T., & Kilgarriff, A. (2004). Senseval-3 English Lexical Sample Task. Proceedings Third International Workshop Evaluation
Systems Semantic Analysis Text (Senseval), pp. 2528. Association
Computational Linguistics.
Miller, G. A., Leacock, C., Tengi, R., & Bunker, R. T. (1993). semantic concordance.
Proceedings workshop Human Language Technology, pp. 303308. Association
Computational Linguistics.
Moro, A., Raganato, A., & Navigli, R. (2014). Entity linking meets word sense disambiguation: unified approach. Transactions Association Computational
Linguistics, 2, 231244.
Nakov, P. I., & Hearst, M. A. (2003). Category-based pseudowords. Proceedings
2003 Conference North American Chapter Association Computational
Linguistics Human Language Technology (NAACL-HLT), pp. 6769. Association
Computational Linguistics.
Navigli, R. (2009). Word sense disambiguation: survey.
(CSUR), 41 (2), 10.

ACM Computing Surveys

Navigli, R. (2012). quick tour word sense disambiguation, induction related approaches. SOFSEM 2012: Theory practice computer science, pp. 115129.
Springer.
Navigli, R., & Crisafulli, G. (2010). Inducing word senses improve web search result clustering. Proceedings Conference Empirical Methods Natural Language
Processing (EMNLP), pp. 116126. Association Computational Linguistics.
Navigli, R., Jurgens, D., & Vannella, D. (2013). Semeval-2013 task 12: Multilingual word
sense disambiguation. Proceedings 7th International Workshop Semantic
Evaluation (SemEval).
Navigli, R., Litkowski, K. C., & Hargraves, O. (2007). Semeval-2007 Task 07: Coarse-grained
English All-words Task. Proceedings 4th International Workshop Semantic
Evaluations (SemEval), pp. 3035. Association Computational Linguistics.
Navigli, R., & Ponzetto, S. P. (2010). Babelnet: Building large multilingual semantic
network. Proceedings 48th Annual Meeting Association Computational Linguistics (ACL), pp. 216225. Association Computational Linguistics.
Niu, C., Li, W., Srihari, R. K., & Li, H. (2005a). Word independent context pair classification model word sense disambiguation. Proceedings Ninth Conference
Computational Natural Language Learning (CoNLL), pp. 3339. Association
Computational Linguistics.
1055

fiBaskaya & Jurgens

Niu, Z., Ji, D., & Tan, C. (2005b). Word sense disambiguation using label propagation based
semi-supervised learning. Proceedings 43rd Annual Meeting Association
Computational Linguistics (ACL), pp. 395402. Association Computational
Linguistics.
Otrusina, L., & Smrz, P. (2010). new approach pseudoword generation.. Proceedings Seventh International Conference Language Resources Evaluation
(LREC).
Palmer, M., Dang, H. T., & Fellbaum, C. (2007). Making fine-grained coarse-grained
sense distinctions, manually automatically. Natural Language Engineering,
13 (02), 137163.
Passonneau, R. J., & Carpenter, B. (2014). benefits model annotation. Transactions Association Computational Linguistics, 2, 311326.
Passonneau, R., Salleb-Aouissi, A., & Ide, N. (2009). Making sense word sense variation. Proceedings NAACL HLT Workshop Semantic Evaluations: Recent
Achievements Future Directions.
Pedersen, T. (2000). Simple Approach Building Ensembles Naive Bayesian Classifiers
Word Sense Disambiguation. Proceedings 1st North American chapter
Association Computational Linguistics conference (NAACL), pp. 6369.
Association Computational Linguistics.
Pedersen, T. (2010). Duluth-WSI: SenseClusters Applied Sense Induction Task
SemEval-2. Proceedings 5th International Workshop Semantic Evaluations, pp. 363366.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,
M., Prettenhofer, P., Weiss, R., Dubourg, V., et al. (2011). Scikit-learn: Machine
learning python. Journal Machine Learning Research, 12, 28252830.
Petrolito, T., & Bond, F. (2014). survey WordNet annotated corpora. Proceedings
Seventh Global Wordnet Conference, pp. 236245.
Pham, T. P., Ng, H. T., & Lee, W. S. (2005). Word sense disambiguation semisupervised learning. Proceedings 19th AAAI Conference Artificial Intelligence (AAAI).
Pilehvar, M. T., & Navigli, R. (2013). Paving way large-scale pseudosense-annotated
dataset. Proceedings 2013 Conference North American Chapter
Association Computational Linguistics: Human Language Technologies (NAACLHLT), pp. 11001109. Association Computational Linguistics.
Pilehvar, M. T., & Navigli, R. (2014). large-scale pseudoword-based evaluation framework
state-of-the-art word sense disambiguation. Computational Linguistics, 40 (4), 837
881.
Preiss, J., & Stevenson, M. (2013). Unsupervised domain tuning improve word sense
disambiguation.. Proceedings 2013 Conference North American Chapter Association Computational Linguistics: Human Language Technologies
(NAACL-HLT), pp. 680684. Association Computational Linguistics.
1056

fiSemi-supervised Learning Induced Word Senses State Art WSD

Purandare, A., & Pedersen, T. (2004). Word Sense Discrimination Clustering Contexts
Vector Similarity Spaces. Proceedings Conference Computational
Natural Language Learning (CoNLL), pp. 4148. Boston.
Rodd, J., Gaskell, G., & Marslen-Wilson, W. (2002). Making sense semantic ambiguity:
Semantic competition lexical access. Journal Memory Language, 46 (2),
245266.
Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. Proceedings
International Conference New Methods Language Processing.
Schutze, H. (1992). Dimensions meaning. Proceedings Supercomputing 92, pp.
787796.
Schutze, H. (1998). Automatic word sense discrimination. Computational Linguistics, 24 (1),
97123.
Sgaard, A., & Johannsen, A. (2010). Robust semi-supervised ensemble-based methods
word sense disambiguation. Advances Natural Language Processing, pp. 401
405. Springer.
Stevens, K. (2012). Evaluating unsupervised ensembles applied word sense induction. Proceedings ACL 2012 Student Research Workshop, pp. 2530. Association
Computational Linguistics.
Stokoe, C. (2005). Differentiating homonymy polysemy information retrieval. Proceedings conference Human Language Technology Empirical Methods
Natural Language Processing (HLT-EMNLP), pp. 403410. Association Computational Linguistics.
Stolcke, A. (2002). SRILM Extensible Language Modeling Toolkit. Proceedings
International Conference Spoken Language Processing vol. 2, pp. 901904.
Teh, Y. W., Jordan, M. I., Beal, M. J., & Blei, D. M. (2006). Hierarchical dirichlet processes.
Journal American Statistical Association, 101 (476), 15661581.
Tsvetkov, Y., Schneider, N., Hovy, D., Bhatia, A., Faruqui, M., & Dyer, C. (2014). Augmenting english adjective senses supersenses. Proceedings Language
Resources Evaluation Conference (LREC).
Van de Cruys, T., & Apidianaki, M. (2011). Latent Semantic Word Sense Induction
Disambiguation. Proceedings 49th Annual Meeting Association
Computational Linguistics (ACL), pp. 14761485. Association Computational Linguistics.
Veronis, J. (2004). HyperLex: lexical cartography information retrieval. Computer
Speech & Language, 18 (3), 223252.
Wang, J., Bansal, M., Gimpel, K., Ziebart, B. D., & Yu, C. T. (2015). sense-topic model
word sense induction unsupervised data enrichment. Transactions
Association Computational Linguistics, 3, 5971.
Yarowsky, D. (1995). Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. Proceedings 33rd annual meeting Association Computational
Linguistics (ACL), pp. 189196. Association Computational Linguistics.
1057

fiBaskaya & Jurgens

Yuret, D. (2007). Ku: Word sense disambiguation substitution. Proceedings
4th International Workshop Semantic Evaluations (SemEval), pp. 207213. Association Computational Linguistics.
Yuret, D. (2012). FASTSUBS: Efficient Exact Procedure Finding Likely
Lexical Substitutes Based N-Gram Language Model. IEEE Signal Processing
Letters, 19 (11), 725728.
Zhong, Z., & Ng, H. (2010). makes sense: wide-coverage word sense disambiguation system free text. Proceedings ACL 2010 System Demonstrations. Association
Computational Linguistics.

1058

fiJournal Artificial Intelligence Research 55 (2016) 131-163

Submitted 3/2015; published 1/2016

Distributional Correspondence Indexing
Cross-Lingual Cross-Domain Sentiment Classification.
Alejandro Moreo Fernandez
Andrea Esuli

alejandro.moreo@isti.cnr.it
andrea.esuli@isti.cnr.it

Istituto di Scienza e Tecnologie dellInformazione
Consiglio Nazionale delle Ricerche
56124 Pisa,

Fabrizio Sebastiani

fsebastiani@qf.org.qa

Qatar Computing Research Institute
Hamad bin Khalifa University
PO Box 5825, Doha, QA

Abstract
Domain Adaptation (DA) techniques aim enabling machine learning methods learn effective classifiers target domain available training data belongs
different source domain. paper present Distributional Correspondence
Indexing (DCI) method domain adaptation sentiment classification. DCI derives
term representations vector space common domains dimension
reflects distributional correspondence pivot, i.e., highly predictive term
behaves similarly across domains. Term correspondence quantified means distributional correspondence function (DCF). propose number efficient DCFs
motivated distributional hypothesis, i.e., hypothesis according terms
similar meaning tend similar distributions text. Experiments show
DCI obtains better performance current state-of-the-art techniques cross-lingual
cross-domain sentiment classification. DCI also brings significantly reduced
computational cost, requires smaller amount human intervention. final contribution, discuss challenging formulation domain adaptation problem,
cross-domain cross-lingual dimensions tackled simultaneously.

1. Introduction
Automated text classification methods usually rely training set labelled examples
order learn classifier predict classes unlabelled documents. One
important bottleneck supervised machine learning methods deal
dependence high-quality annotated examples order model
trained. Deploying model domain examples available thus
entails substantial human labelling effort.
Transfer learning (TL see e.g., Pan & Yang, 2010; Pan, Zhong, & Yang, 2012) focuses alleviating problem leveraging training examples different, although
related, source domain (a.k.a. out-domain, auxiliary domain) amount
available labelled examples higher. TL allows making use examples order
train effective classifier target domain (a.k.a. in-domain), thus allowing
diminish completely away cost involved manual generation training
c
2016
AI Access Foundation. rights reserved.

fiMoreo, Esuli, & Sebastiani

documents target domain. consequence, TL applied one fundamental assumptions traditional machine learning, i.e., training test
data randomly drawn distribution (the so-called iid assumption),
longer holds.
One applicative scenario particular interest TL sentiment classification, task
classifying opinion-laden documents conveying positive negative sentiment towards given entity (e.g., product, policy, political candidate). Determining users
stance towards entity utmost importance market research, customer relationship management, social sciences, political science among others, several
automated methods proposed purpose (see e.g., Liu, 2012; Pang & Lee,
2008). However, sentiment classification needs deal completely new entity,
likely amount available, pre-labelled opinion-laden documents scarce even
null. cases, promptly generating sentiment classifier might become difficult, due
considerable cost time involved producing representative corpus training
documents.
sentiment classification, TL finds natural application domain adaptation (DA),
i.e., task adapting sentiment classifier operate new domain. example, might want use training set book reviews written English classify
movie reviews written English, classify book reviews written German.
former case typically known cross-domain adaptation, second one instead known cross-lingual adaptation (Pan et al., 2012). article use
notation Ls Cs Lt Ct indicate domain adaptation setup, Ls Lt
source target languages, Cs Ct source target domains, respectively.
Therefore, previously discussed examples written EnglishBooksEnglishDVDs
(or simply BooksDVDs short), example cross-domain adaptation,
EnglishBooksGermanBooks, example cross-lingual adaptation.
common practice text classification represent dataset term-document
matrix according so-called bag-of-words model (BoW), value Mij
function frequency term fi document dj dataset whole.
Accordingly, rows (resp., columns) regarded vectorial representations terms
(resp., documents) vector space generated documents (resp., terms). Here,
expectation distances vectors vector space model (VSM) reflect
semantic distance terms documents. Since term dimension
vector space documents lie, terms represented orthogonal dimensions
even similar meanings. example, term beautiful viewed
dissimilar nice awful; base vector space therefore unaware
semantic nuance, lies hidden joint term distributions. Statistical methods like
Latent Semantic Indexing (LSA Deerwester, Dumais, Landauer, Furnas, & Harshman,
1990; Landauer & Dumais, 1997) Latent Dirichlet Allocation (LDA Blei, Ng, &
Jordan, 2003) attempt discover hidden correlations among terms. Discovering
semantic correspondences becomes crucial dealing different domains;
example, adapting target domain book reviews source domain film
reviews, identifying cross-domain semantic correspondences among important terms (e.g.,
book film, writer director, length duration) might helpful task, likely
decision boundaries model hinge upon terms.
132

fiDistributional Correspondence Indexing

respect aspect, general assumption domain adaptation large
sets unlabelled documents source target domain available.
leveraging unlabelled collections, various techniques applied order better
explore term distributions two domains, attempt map similarities
terms two domains. idea rests belief meaning
term somehow determined distribution text terms co-occurs with,
idea generally referred distributional hypothesis (Harris, 1954).
discovery hidden correlations highly predictive terms, goal
improving document representations, studied Structural Learning paradigm
(Ando & Zhang, 2005). correlations discovered learning predictive structures
input data auxiliary binary problems consist predicting term presence
using surrounding terms unlabelled data applying LSA predictors. framework extended Structural Correspondence Learning domain
adaptation (SCL Blitzer, McDonald, & Pereira, 2006). SCL unifies latent space
correspondences among terms different domains derive auxiliary prediction problems pivot terms highly predictive terms expected behave similar way
domains (e.g., intriguing, annoying, captivating film book reviews). SCL
first applied cross-domain adaptation sentiment classification (Blitzer, Dredze, &
Pereira, 2007). later applied cross-lingual adaptation (Prettenhofer & Stein, 2010)
enhancing pivot term one equivalent translations target language
(e.g., intriguing intrigante, annoying noioso, captivating travolgente EnglishBooks
ItalianBooks cross-lingual adaptation). Even though SCL successfully validated
cross-domain cross-lingual scenarios, suffers considerable computational
cost, deriving intermediate optimizations auxiliary predictive problems
use LSA.
method present paper, dub Distributional Correspondence Indexing (DCI), inspired SCL follows different, simpler approach,
direct application distributional hypothesis. propose mine distributional
correspondences term small set pivot terms. hypothesize
correspondences approximately invariant across domains terms
equivalent roles two domains. example, expect distributional correspondence source term fs = book pivots p1s = intriguing, p2s = annoying,
p3s = captivating, approximately similar distributional correspondence
target term ft = film pivots p1t , p2t , p3t (Books DVDs cross-domain
adaptation). Analogously, expect distributional correspondence source
(English) term fs = book pivots p1s = intriguing, p2s = annoying, p3s = captivating,
approximately similar distributional correspondence target (Italian)
term ft = libro pivot translations p1t = intrigante, p2t = irritante p3t = accattivante (EnglishBooks ItalianBooks cross-lingual adaptation). Contrarily SCL, define
distributional correspondences distributional correspondence function
directly mines term vectors unlabelled collections, computed efficiently.
present work extension work Esuli Moreo Fernandez (2015),
preliminary intuitions underlying method applied cross-lingual case.
present improved version DCI compares favourably respect state
art extensive experimental comparisons carried two popular senti133

fiMoreo, Esuli, & Sebastiani

ment classification datasets cover cross-domain cross-lingual adaptation.
experiments also show DCI substantially smaller computational cost
respect competition. cross-lingual scenario, show DCI require smaller
amount human intervention order create cross-lingual pivots. final contribution, explore general complex formulation domain adaptation
problem combines cross-domain setting cross-lingual setting; present
experimental results compare method state-of-the-art methods.
rest article structured follows. Section 2 overviews related work
domain adaptation. Section 3 formally states problem presents notation
going use. Section 4 formally defines distributional correspondence functions,
method whole described Section 5. Section 6 presents experiments
analysis results, Section 7 concludes.

2. Related Work
section offers brief overview main related methods literature domain
adaptation sentiment classification. Traditionally, two different types approaches
domain adaptation identified. first group instance transfer methods aim
re-weighting relative importance training document, order compensate
disagreements source target marginal probability distributions (Dai,
Xue, Yang, & Yu, 2007; Gao, Fan, Jiang, & Han, 2008). methods applied
cross-domain (and cross-lingual) adaptation problems, since cannot solve
problem posed fact that, cross-lingual setting, term sets source
target domains disjoint. second group feature-representation transfer methods project documents domains common vector space standard
classification algorithms could applied. DCI method propose belongs
feature-representation transfer class, thus applied cross-domain
cross-lingual settings. following review relevant work crossdomain setting (Section 2.1) cross-lingual setting (Section 2.2). interested
reader check (Pan & Yang, 2010; Pan et al., 2012) surveys transfer learning
methods.
2.1 Cross-Domain Adaptation
Structural Correspondence Learning method (Blitzer et al., 2006), already discussed
introduction, extends Structural Learning paradigm Ando Zhang (2005)
introducing concept pivot features. SCL applied cross-domain adaptation (Blitzer et al., 2007) leveraging notion predictive power pivot. similar criterion adopted discern domain-specific domain-independent
terms Spectral Feature Alignment (SFA Pan, Ni, Sun, Yang, & Chen, 2010), method
clustering domain-specific terms source target domains latent space
mining relationships domain-independent terms.
Aside sets unlabelled documents domain, methods take advantage
external general-purpose knowledge resources order bridge gap domains.
example, Wang, Domeniconi, Hu (2008) extended co-clustering approach
propagate labels two domains using Wikipedia represent documents
134

fiDistributional Correspondence Indexing

means concepts. recently, Bridging Information Gap method (Xiang, Cao, Hu,
& Yang, 2010) followed similar motivation, exploiting Wikipedia Open Directory
Project general-purpose knowledge base. sentiment classification, related
methods use sentiment lexicon external resource. Joint Sentiment-Topic Model
(JSTM), proposed He, Lin, Alani (2011) extension Latent Dirichlet Allocation, consists augmenting terms polarity-bearing topics using sentiment lexicon
repository prior word sentiment. JSTM found perform better SCL
comparably SFA. Denecke (2009) studied viability SentiWordNet, well-known
sentiment lexicon, lexicon cross-domain sentiment adaptation; main drawback
methods dependence availability suitable public resources
/ lexicons language application targets. Li, Pan, Jin, Yang, Zhu
(2012a) alleviated constraint automatically co-extracting topic lexicon sentiment lexicon target domain, exploiting information source domain.
Similarly, Bollegala, Weir, Carroll (2011) obtained sentiment-sensitive thesaurus
augment term vectors. main peculiarity approach lexicon created
mining multiple source domains. Similarly, following completely different approach
based deep learning architectures, Stacked Denoising Autoencoder (SDAsh ) method
(Glorot, Bordes, & Bengio, 2011) exploits unlabelled information contained multiple
domains order improve vector representations terms unsupervised fashion.
Glorot et al. found method (which use one baselines experiments)
scale well large multi-domain collections, outperforming SCL SFA using 22
different domains unlabelled documents.
branches research related cross-domain methods binary classification
exist tested sentiment classification topic classification, using
popular datasets Reuters-21578, 20-Newsgroups, SRAA. relevant examples include Spectral Domain Transfer Learning (Ling, Dai, Xue, Yang, & Yu, 2008), Matrix
Trifactorization (Zhuang, Luo, Xiong, He, Xiong, & Shi, 2011), Topic Correlation Analysis
(Li, Jin, & Long, 2012b), Topic-Bridged Probabilistic LSA (Xue, Dai, Yang, & Yu,
2008). Aside fact methods explicitly designed classify according sentiment, approaches also faced different problem setting, i.e., test
set target domain available though labels omitted modelling
classification hypothesis, collection unlabelled documents available
source target domain. approaches thus fall domain transductive
learning (see e.g., Joachims, 1999), thus directly related approach,
completely inductive.
2.2 Cross-Lingual Adaptation
review prior work cross-lingual adaptation, covering three different types approaches: (i) methods relying automatic machine translation, (ii) methods exploiting
parallel corpora, (iii) methods exploiting unlabelled topic-specific collections.
Rigutini, Maggini, Liu (2005) proposed method cross-lingual adaptation
first translates source documents target language means automatic machine translation service. Then, Expectation Maximization method refines translated
representations mining statistical properties large sets unlabelled documents
135

fiMoreo, Esuli, & Sebastiani

target language. Along lines, Wan, Pan, Li (2011) proposed bi-weighting
method re-weight terms training instances order correct word drift
machine translation could introduced translation process. Motivated
lack labelled Chinese sentiment corpora, Wan (2009) proposed instead English-Chinese
co-training approach based automatic machine translation. method translates
labelled English (source) documents Chinese (target), Chinese unlabelled documents English. classifier created languages, later
used classify respective set unlabelled documents improve model. Finally,
Chinese test document attached translation equivalent English given
input classifier.
Even though machine translation represents promising solution cross-lingual problems (a solution presumably become viable field machine
translation evolves), current machine translation services always free-to-use,
available language pairs either, computationally expensive. things
equal, cross-lingual methods rely thus preferable.
Latent Semantic Analysis well-known technique originated within monolingual
text analysis (Deerwester et al., 1990) later extended deal cross-lingual retrieval (Dumais, Letsche, Littman, & Landauer, 1997) multilingual classification (Xiao
& Guo, 2013). LSA consists mapping original term-document matrix lowerdimensional latent semantic space captures (linear) relations among original
terms documents. cross-lingual context, mapping performed via singular value decomposition original term-document matrix extracted multilingual
documents. main problem use LSA cross-lingual applications
dependence parallel corpus. order relax constraint, Xiao Guo (2014)
proposed method induces cross-lingual terms via matrix completion. method
requires small set parallel documents used construct dual-language
co-occurrence matrix; LSA applied completed dual-language matrix order
produce low-dimensional interlingual representation. Cross-lingual Kernel Canonical
Correlation Analysis (KCCA Vinokourov, Shawe-Taylor, & Cristianini, 2002) produces
instead semantic cross-lingual representation investigating correlations aligned
bilingual fragments. KCCA takes advantage kernel functions order map aligned texts
high-dimensional space manner correlations mappings
mutually maximized. Finally, Oriented Principal Component Analysis (OPCA Platt,
Toutanova, & Yih, 2010) finds discriminative projection maximizes document
variance across languages, time minimizing distance comparable
documents, thus avoiding use artificially concatenated documents.
techniques based correlation analysis discussed rather
expensive computational point view, use requires availability
parallel comparable corpus. reason Moen Marsi (2013) proposed use
Random Indexing (Sahlgren, 2005), computationally lighter indexing approach
approximates LSA (Kanerva, Kristofersson, & Holst, 2000), alternative use crosslingual information retrieval. Cross-lingual Random Indexing requires monolingual
corpus language, plus bilingual dictionary.
Even though approaches discussed succeed discovering hidden
correlations terms belonging different languages, still based
136

fiDistributional Correspondence Indexing

availability suitable parallel corpus bilingual dictionary. response, Gliozzo
Strapparava (2005, 2006) provided means automatically obtaining Multilingual
Domain Model (MDM) defining soft relations words domain topics. Making
lack bilingual dictionary parallel corpus, MDM automatically
obtained comparable corpora performing LSA. similar vein, Rapp (1999)
proposed method acquiring bilingual dictionary based assumption
correlation word co-occurrence patterns different languages (Rapp, 1995).
dimensions co-occurrence matrices rearranged make translation
equivalents word correspond identical positions vector, using
small bilingual dictionary. Word translation performed vector similarity
two co-occurrence matrices, ignoring dimensions aligned. dictionary
iteratively expanded inclusion newly translated terms. Koehn Knight
(2002) proposed method automatically constructing word-level translation lexicon
taking monolingual corpus language input, neither requiring corpora
parallel even comparable, requiring availability initial dictionary.
Roughly speaking, done first taking words identical spelling (cognates)
similar spelling initial entries dictionary, expanding dictionary
assuming context, frequency, word correlations approximately preserved
across languages. recently, Peirsman Pado (2010) proposed method induce
selectional preferences resource-poor languages also takes advantage cognates.
bilingual vector space initially derived taking cognates dimensions
vector space. space, subsequently bootstrapped large (unparsed) corpora
languages, allows direct word translations performed based vector distances.
realistic cross-lingual setting expect sort labelled corpus
available target domain, machine translation tools, available, still
expensive. Therefore, Prettenhofer Stein (2010, 2011) assume word translation
oracle available limited budget calls (450, experiments).
resulting word translations allow Structural Correspondence Learning (SCL see Section
2.1) applied cross-lingual domain adaptation pairing source pivot
equivalent translation target language. Even though cross-lingual SCL succeeds
relaxing constraints discussed thanks fact need
linguistic resource, still suffers considerable computational cost deriving
need perform intermediate optimizations structural problems, need
use LSA. Following similar strategy relying bilingual pivots, DCI method requires
significantly fewer word translations, avoids use expensive statistical analysis
technique.
method bears resemblance Explicit Semantic Analysis (ESA), method
indexes given text respect set explicitly given external categories
(Gabrilovich & Markovitch, 2007). work Sorg Cimiano (2008, 2012) different
language-specific views Wikipedia articles considered external categories
semantic term vectors defined. dimension thus models strength
association given term given article cross-lingual information retrieval
setting (CL-ESA). Arguably, main difference method CL-ESA relies
high-dimensional spaces (about 10,000 dimensions) interlingual universal concepts,
DCI instead projects term low-dimensional space (about 100 dimensions)
137

fiMoreo, Esuli, & Sebastiani

highly predictive concepts, i.e., bilingual pivots. Additionally, method require
sort external resource apart word translation oracle, strength
association rather defined terms distributional correspondence, computed efficiently
unlabelled sets (Section 4).

3. Problem Statement
section formally state problem set notation going use
throughout paper.
Sentiment classification may viewed task approximating unknown target
function : X Y, indicates documents ought classified, means
function : X Y, called classifier, X denotes space documents
= {+1, 1} denotes space labels, indicating positive (+1) negative (-1)
sentiment. domain adaptation (see e.g., Pan & Yang, 2010) customary define
domain pair = hF, P (X)i, P (X) marginal probability distribution
governs likelihood documents (represented term space F ) drawn.
Given source domain Ds = hFs , Ps (X)i target domain Dt = hFt , Pt (X)i, domain
adaptation subtask transfer learning consists improving accuracy
classifier Dt using knowledge Ds , domain Ds 6= Dt .
precondition Ds 6= Dt leads two different interpretations domain adaptation problem. one side, cross-domain adaptation (e.g., DVDs Books) typically
characterized Fs = Ft Ps 6= Pt ; is, term space common trivially
made common joining two term spaces marginal probability distributions
differ. side, cross-lingual adaptation (e.g., EnglishBooks GermanBooks)
typically characterized Fs 6= Ft Ps = Pt ; is, documents drawn similar
distributions described different term spaces.
paper also define investigate third case domain adaptation,
term spaces probability distributions differ. crossdomain/cross-lingual adaptation problem, characterized Fs 6= Ft Ps 6= Pt .
argue case particular interest, since enables cross-lingual adaptation
performed even absence auxiliary dataset acts bridge two-steps
adaptation (e.g., EnglishBooks GermanBooks GermanDVDs). example, sentiment classifier resource-poor language needs deployed analyses sentiment
new topic, common scenario one want leverage data
resource-rich language (e.g., English) different, already known topic. scenario
realistic generalization domain adaptation problem; best knowledge,
nobody tackled published work.
final note regarding notation, use subscripts indicate whether
data drawn source target domain, respectively. Accordingly,
Us denotes unlabelled source dataset, Ut refers unlabelled target dataset.
Similarly, rs et denote training set test set, respectively.
138

fiDistributional Correspondence Indexing

4. Distributional Correspondence Functions
goal section introduce Distributional Correspondence Functions (DCFs).
first characterize family DCFs propose specific ones.
4.1 Definition
DCFs family real-valued functions quantify degree correspondence
two terms f f j comparing context distribution vectors f f j
unlabelled collection U . context distribution vector unit-length n-dimensional
vector models term relates set contexts. context text unit
term could appear (e.g., sentence, fixed-size window, entire document);
fci denotes value vector term f context c, fci = 0 f appear
context c. cases fci > 0 determined weighting function use,
might lead different interpretations DCF, e.g., probability function
event space (Section 4.2), kernel vector space (Section 4.3). define pi
prevalence f , i.e., portion contexts fci > 0, i.e.,
pi =

|{c|fci > 0}|
n

(1)

n dimensionality vector space, i.e., number different contexts.
work take documents contexts, fci = 0 means term f appear
document c, pi portion documents unlabelled collection f
appears.
DCF function : Rn Rn R, sign (f , f j ) indicates polarity
correspondence, i.e., positive values indicate positive correlation negative values
indicate negative correlation; (f , f j ) = 0 indicates null correspondence. force
DCFs (f , f j ) = 0 expected correspondence measured
randomly chosen pairs vectors prevalence pi pj terms f f j
set. rationale choice high-prevalence terms higher probability
non-zero values number common positions, case measures
(see Section 4.3) record level correspondence due non-perfect orthogonality
vectors, happens much rarely low-prevalence terms. want
factor bias, centering DFC expected correspondence value.
4.2 Probability-Based DCF
section discuss probability-based DCFs derived information theory.
distribution P (f ) term f across contexts modelled using binomial event
space, thus considering presence absence term context; P (f )

thus denotes probability f occurs random context, P (f ) denotes
probability f occur it.
first part Table 1 shows probability-based DCFs investigate. consider
Pointwise Mutual Information (P I, ratio joint distribution
product marginal distributions), simple probabilistic function (here called
j
Linear ) contrasts probabilities f conditioned f j f , respectively.
139

fiMoreo, Esuli, & Sebastiani

also consider Mutual Information (M I, reduction entropy distribution due
observation another distribution) asymmetric version. rationale
asymmetry per se symmetric respect positive negative correlation;
is, two cases (a) f occurs contexts f j also
occurs, (b) f occurs contexts f j occur, obtain
score. scenario two kinds correlation must kept distinct,
high positive correlation indicates semantic similarity, high negative correlation
indicates lack semantic similarity. reason invert sign DCF
negative correlation case, defining function detects negative correlation
using true positive rate (tpr = P (f , f j )/P (f j )) true negative rate (tnr =
j
j
P (f , f )/P (f )), i.e.,

+1 (tpr + tnr > 1)
j
(f , f ) =
(2)
1
otherwise
multiplying (f , f j ) I, yield Asymmetric Mutual Information (AMI, see
Table 1).
4.3 Kernel-Based DCFs
section consider different kernel functions DCFs. Kernel functions similarity
functions, typically used e.g., within support vector machines operate high- (and
potentially infinite-) dimensional spaces. case values context vector
numeric, thus indicating relative importance term given context,
usually computed function frequency term context corpus,
e.g., tf idf . consider normalized context vectors, i.e., weighting document-byterm matrix normalize term vectors unit length.
popular vector similarity measure probably cosine similarity, measures
cosine angle them, defined
cos(f , f j ) =

hf , f j
kf kkf j k

(3)

also consider DCFs popular kernels: polynomial kernel Radial
Basis Function (RBF a.k.a. Gaussian) kernel, i.e.,
polynomiala,b (f , f j ) = (a + hf , f j i)b


j



(4)
j 2

RBF (f , f ) = exp{kf f k }

(5)

qP
j 2
n

j

kf f j k =
c=1 (fc fc ) Euclidean distance f f .
Since non-zero values frequency vector always positive, turns
expected value dot product Euclidean distance two random
distributional vectors greater zero. order satisfy necessary condition
imposed DCFs kernels centred zero factoring bias. Let
ri rj two unit-length vectors prevalences pi pj , whose non-zero values

independently distributed random; expected value non-zero positions pi n1

pj n1 , respectively. expected value dot product Euclidean
140

fiDistributional Correspondence Indexing

distance ri rj are, respectively,
1
1

= pi pj

pi n pj n

2
1
1
1
1

j
+ n(pi pi pj )
E[kr r k] = npi pj

+ n(pj pi pj )
pi n
pj n
pi n
pj n

= 2(1 pi pj )
E[hri , rj i] = pi pj n

(6)
(7)

resulting DCFs, obtained factoring expected values corresponding
kernels, reported second part Table 1.
Table 1: Mathematical forms DCFs discussed work.
Probability-based DCFs
Linear
Pointwise Mutual Information
Asymmetric Mutual Information

Mathematical form
P (f |f j ) P (f |f j )
P (f , f j )
P (f )P (f j )
X
(f , f j )
log2

X

x{f ,f } y{f j ,f j }

Kernel-based DCFs
Cosine
Polynomial
RBF

P (x, y) log2

P (x, y)
P (x)P (y)

Mathematical form
hf , f j

pi pj

j
kf kkf k
(a + hf , f j i)b (a +



pi pj )b
n
2

exp{kf f j k2 } exp 4 1 pi pj

5. Distributional Correspondence Indexing
section explain Distributional Correspondence Indexing (DCI) method
detail, paying special attention step workflow.
DCI method works first identifying small set pivot terms (or pivots,
short Section 5.1), projects term new vector space
dimension measures correspondence term pivot (Section 5.2). term
thus assigned new vectorial representation referred term
profile, simply profile. term space post-processed first normalizing
dimension (Section 5.3) unifying source target term profiles certain
terms expect behave similarly domains, pivots (Section 5.4).
Documents projected cumulating (i.e., summing) profiles terms
occur (Section 5.5), classifier trained usual. order
141

fiMoreo, Esuli, & Sebastiani

clarify explanation use running example throughout different steps.
aim tracking case Books Electronics, i.e., domain adaptation
Books domain Electronics one (more details datasets given
Section 6.1).
5.1 Pivot Selection
Pivots terms shared across source target domains, meant link
them, thus enabling knowledge transfer process. Blitzer et al. (2006) defined pivots
terms occur frequently source target domains behave similarly
domains. Subsequent work (Blitzer et al., 2007; Prettenhofer & Stein, 2010; Pan et al., 2010)
extended definition pivots take account also co-occurrence relation
terms class labels source domain, shown informative terms
prediction task better pivots. idea later adapted cross-lingual setting
Prettenhofer Stein (2010), fixed frequency threshold , called support,
used filter infrequent pivot candidates, ranking remaining candidates
mutual information respect labels source domain. Prettenhofer Stein
also introduced notion word translation oracle (WTO), i.e., translator that, given
word source language, provides translation target language. method
Prettenhofer Stein assumes possibility issuing limited number calls
oracle.
pointed Pan et al. (2010), pivot selection using mutual information help
identify predictive terms source domain, guarantee terms
act similarly domains. respect, think even though support
threshold might serve filter problematic candidates, strategy suboptimal.
example, candidate occurring 29 times 31 times 50,000 source
target domain, respectively, discarded support set = 30,
pivot occurring 5000 times 31 times could chosen, even clear
second case role two domains different.
good pivot highly task-dependent, also present similar degree
domain-dependence two domains. formalized intuition via function
(f ) = (f )st (f )

(8)

(f ) terms strength pivot, (f ) quantifies informativeness (to
estimated training set rs ) term f classification task, st (f )
measures cross-consistency term f estimated Us Ut , quantifies
similarly term behaves two domains.
Following previous research, instantiate (f ) via mutual information. Ideally,
st : F [0, 1] defined way st (f ) 1 distribution
f consistent across domains, st (f ) 0 importance f varies lot
one domain another. Given lack labelling information target domain,
adopt simple heuristic relates prevalence domains, might expect
comparable prevalence use text terms posses similar degree domain142

fiDistributional Correspondence Indexing

dependence1 . thus define
st (f ) =

min{psi , pti }
max{psi , pti }

(9)

psf (resp., pti ) prevalence f measured set Us (resp., Ut ). top-ranked
terms according value frequency greater Us Ut ,
selected pivots; user defined parameter indicating number pivots select.
Table 2 exemplifies pivot selection process Books Electronics adaptation.
instance, cross-consistency weight succeeds penalizing adjective boring, might
good candidate predicting polarity book reviews rather uninformative
electronic devices, thus pushed top-100 list (only 10 elements
shown due space limitations).
Table 2: Top-10 terms ranked according mutual information (Is (f )) (left), mutual
information combined cross-consistency ((f ) = (f )st (f )) (right).
#
1
2
3
4
5
6
7
8
9
10

term
waste
boring
disappointing
excellent

waste
bad
dont

money

score
0.029
0.029
0.029
0.026
0.021
0.021
0.021
0.019
0.018
0.018

term
waste
excellent
bad

dont
waste
highly recommend
disappointing
great
money

score
0.028
0.022
0.020
0.018
0.017
0.017
0.014
0.013
0.012
0.012

5.2 Term Profiles
implement rather direct application distributional hypothesis, following
intuition semantics term captured distributional correspondence
pivots. thus build term profile f~ source target term f (including
pivot terms) m-dimensional vector
f~ = ((f , p1 ), (f , p2 ), . . . , (f , pm ))

(10)

f pi context distribution vector unlabelled collection term
f profiled ith pivot, respectively, selected DCF.
Table 3 displays term profiles associated four relevant terms running
example, i.e., boring, excellent, waste, reliable. Note excellent waste pivots
opposite polarity, two domain-dependent terms, i.e., boring
1. Note st () function meant capture cross-domain drift, thereby ignore
cross-lingual setting simply defining st (f ) = 1 case domain knowledge
collections.

143

fiMoreo, Esuli, & Sebastiani

informative Books reviews reliable informative Electronics reviews.
DCF used example cosine kernel. Note boring representation
target side, term appear Electronics dataset. Note also
pivot terms associated vectorial representations somehow close
domains. happen reliable, domain-dependent term
plays different roles two domains. also note correspondence tends
positive terms similar polarity (e.g., waste bad), negative
otherwise (e.g., excellent bad). Finally, notice cos(waste, waste) 6= 1
cos(excellent, excellent) 6= 1, due correction factor introduced cosine formula
(see Table 1).

Table 3: Term profiles generated terms boring, excellent, waste, reliable (rows)
different dimensions (columns) source (left) target (right).
first 5 dimensions, corresponding pivots waste, excellent, bad, no, dont,
shown due space restrictions.

boring
excellent
waste
reliable

waste
0.058
-0.030
0.957
-0.012

excellent
-0.042
0.938
-0.030
-0.002

Books
bad
0.029
-0.051
0.007
-0.007


-0.029
-0.082
-0.013
0.004

dont
0.004
-0.065
0.161
0.016

...
...
...
...
...

waste
-0.042
0.959
0.001

excellent
0.927
-0.042
0.014

Electronics
bad

-0.023 -0.021
0.028 0.025
0.005 -0.004

dont
-0.018
0.138
-0.008

...
...
...
...
...

5.3 Normalization Term Profiles
dimension space reflects distributional correspondence given pivot.
Pivots high prevalence likely generate high DCF values, could lead
dominant dimensions profile vectors; could detrimental learning
phase. avoid effect, center profile dimension expected value
rescale standard deviation (Equation 11), values profile dimensions
approximately normally distributed N (0, 1), i.e.,
f~0 =

f~1 1 f~2 2
f~m
,
,...,
1
2


!
(11)

f~i ith dimension term profile f~ (see Equation 10),
mean standard deviation ith dimension, respectively. normalization, term profile vectors rescaled unit length.
Table 4 demonstrates effect term normalization example terms discussed
Table 3. Note that, normalization, source target profiles pivot terms
seem get closer vectorial space. Furthermore, target representation reliable
turns consistent intuitions, reflects negative correspondence
waste, stronger positive correspondence excellent.
144

fiDistributional Correspondence Indexing

Table 4: Effect term normalization terms boring, excellent, waste, reliable.

boring
excellent
waste
reliable

waste
0.223
-0.021
0.555
-0.091

excellent
-0.163
0.861
-0.017
0.020

Books
bad
0.105
-0.038
0.005
-0.056


-0.125
-0.080
-0.008
0.076

dont
0.020
-0.050
0.094
0.169

...
...
...
...
...

waste
-0.035
0.572
-0.011

excellent
0.909
-0.032
0.119

Electronics
bad

-0.024 -0.038
0.016 0.018
0.005 -0.146

dont
-0.023
0.087
-0.138

...
...
...
...
...

5.4 Unification Term Profiles
assume pivot terms behave similarly two languages, unify term
profiles simply averaging source profile target profile normalizing
result unit length. Unification also applied profiles terms appear
source target domains frequency greater support .
rationale behind unification correct possible misalignment source
target term profiles terms receive vectorial representation
domains, pivot terms proper nouns. done order equalize across
domains contribution term document representation (see below).
Table 5 shows term profiles running example unification. Note
boring experience change target counterpart even exist. Term
reliable also affected normalization, frequency Books domain
exceed support , set 30 example. Finally, term profiles pivots
excellent waste unified, i.e., computed average respective source
target profile representations, normalized unit norm.
Table 5: Term profile unification terms boring, excellent, waste, reliable.

boring
excellent
waste
reliable

waste
0.223
-0.028
0.570
-0.091

excellent
-0.163
0.893
-0.025
0.020

Books
bad
0.105
-0.031
0.010
-0.056


-0.125
-0.059
0.005
0.076

dont
0.020
-0.037
0.091
0.169

...
...
...
...
...

waste
-0.028
0.570
-0.011

excellent
0.893
-0.025
0.119

Electronics
bad

-0.031 -0.059
0.010 0.005
0.005 -0.146

dont
-0.037
0.091
-0.138

...
...
...
...
...

5.5 Document Indexing
Finally, train test documents indexed profile space via weighted sum
profile vectors associated terms. is, document dj represented
m-dimensional vector
X
d~j =
wij f~0
(12)


fi dj

wij weight term fi document dj according weighting function (in
experiments used standard cosine-normalized tf idf ), f~i0 normalized
unified term profile vector fi .
145

fiMoreo, Esuli, & Sebastiani

6. Experiments
section experimentally compare DCI method, implemented using different
DCFs, state-of-the-art methods proposed literature.
6.1 Datasets
test method two popular, publicly available sentiment datasets: Multi-Domain
Sentiment Dataset (version 2.0) Webis-CLS-10. former dataset frequently used
evaluating cross-domain adaptation, latter often used evaluating
cross-lingual methods. also use Webis-CLS-10 explore cross-domain/crosslingual setting.
6.1.1 Multi-domain Sentiment Dataset (version 2.0)
Multi-Domain Sentiment (MDS) dataset, first proposed Blitzer et al. (2007), contains English product reviews taken Amazon.com four domains Books (B),
DVDs (D), Electronics (E), Kitchen (K) appliances. order facilitate reproducibility
allow fair comparison results reported previous literature, used
pre-processed version dataset used previous evaluations, made publicly
available Blitzer et al. (see MSD dataset, 2007) . pre-processed version, terms
extracted taking unigrams bigrams; reviews originally rated higher 3
stars labelled positive, rated lower 3 stars negative; reviews
intermediate ratings removed. dataset comprises 1000 positive reviews
1000 negative reviews four domains, set unlabelled documents ranging 3,586 5,945 documents domain. Table 6 shows number labelled
unlabelled documents, number distinct terms total number terms
dataset. According evaluation procedure followed proposers
methods compare against, randomly split labelled dataset training set
1600 instances test set 400 instances.
Table 6: Main characteristics Multi-Domain Sentiment dataset (version 2.0).
Domain
Books
DVDs
Electronics
Kitchen

Labelled
2,000
2,000
2,000
2,000

Unlabelled
4,465
3,586
5,681
5,945

Terms
195,887
188,778
111,407
93,474

Occurrences
445,793
370,844
392,699
351,162

6.1.2 Webis-CLS-10
Webis-CLS-10, first proposed Prettenhofer Stein (2010), cross-lingual sentiment
collection consisting Amazon product reviews written four languages (English (E),
German (G), French (F), Japanese (J)), covering three product domains (Books (B),
DVDs (D), Music (M)). language-domain pair 2,000 training documents, 2,000 test documents, 9,000 50,000 unlabelled documents depending
146

fiDistributional Correspondence Indexing

language-domain combination (see Table 7 details). used preprocessed version dataset made publicly available authors (see Webis-CLS
dataset, 2010), terms correspond uni-grams. Following work Prettenhofer
Stein, consider English source language, since far realistic
scenario. Documents either labelled positive negative, following procedure Blitzer et al. (2007). Positive negative examples balanced sets (see
Table 7 details). labelled dataset split perfectly balanced training set
2,000 instances test set 2,000 instances. split proposed Prettenhofer
Stein; baseline methods compare use exactly corpus
training test.
Table 7: Main characteristics Webis-CLS-10 dataset.
Domain
EB
ED
EM
GB
GD
GM
FB
FD
FM
JB
JD
JM

Labelled
4,000
4,000
4,000
4,000
4,000
4,000
4,000
4,000
4,000
4,000
4,000
4,000

Unlabelled
50,000
30,000
25,220
50,000
50,000
50,000
32,870
9,358
15,940
50,000
50,000
50,000

Terms
62,499
50,124
38,632
105,360
100,265
95,952
52,664
26,117
39,001
51,179
53,318
53,078

Occurrences
6,289,014
4,001,678
2,664,955
6,618,037
6,303,371
5,688,874
2,427,178
714,105
1,371,800
7,637,325
7,263,796
6,284,653

6.2 Evaluation Metrics
Following practice common related literature, adopt standard accuracy
evaluation measure. Accuracy measures proportion correctly classified documents
total number outcomes (Equation 13), i.e.,
Acc =

TP + TN
TP + FP + FN + TN

(13)

P , N , F P , F N stand numbers true positives, true negatives, false
positives, false negatives, respectively. Note measure perfectly adequate
choice since datasets balanced respect positive negative classes.
6.3 Baseline Methods
experimentally compare DCI, using different DCFs, different baseline methods proposed literature cross-domain cross-lingual domain adaptation. limit
comparison algorithms evaluated corpora, report results taken
original papers. explicitly mentioned, results baseline algorithms
147

fiMoreo, Esuli, & Sebastiani

obtained using datasets, though obviously using different random splits.
MDS dataset, following common practice, run experiments multiple random
splits average them. Webis-CLS-10 dataset instead used split proposed
Prettenhofer Stein (2010) (more details below).
upper bound, implemented method (hereafter called Upper) trains
SVM classifier training set target domain. lower bound instead
implemented method trains SVM classifier source domain applies
trained classifier directly target domain, i.e., without carrying sort
knowledge transfer (NoTrans). considering various languages, also report
machine translation baseline (MT), first translates target documents
source language (i.e., English experiments) giving input SVM
classifier trained source domain; used pre-translated documents provided
Prettenhofer Stein (2011).
cross-domain adaptation baselines consider Structural Correspondence
Learning using Mutual Information select pivots (SCL-MI Blitzer et al., 2007), Spectral
Feature Alignment (SFA Pan et al., 2010), multiple sources Sentiment Sensitive Thesaurus
(SST Bollegala et al., 2011), Stacked Denoising Autoencoder (Glorot et al., 2011)
trained domain pairs (SDA) trained 22 domains available former version
MDS dataset (a method authors abbreviate SDAsh ).
cross-lingual adaptation baselines consider cross-lingual Latent Semantic
Indexing (LSI Dumais et al., 1997), cross-lingual Kernel Canonical Correlation Analysis
(KCCA Vinokourov et al., 2002), Oriented Principal Component Analysis (OPCA
Platt et al., 2010), Two-Step Learning method (TSL Xiao & Guo, 2013), Semi-Supervised
Matrix Completion (SSMC Xiao & Guo, 2014), cross-lingual version Structural
Correspondence Learning (SCL Prettenhofer & Stein, 2011).
Since published results compare cross-lingual/cross-domain
adaptation, baseline consider SCL-MI (Prettenhofer & Stein, 2011) reusing
publicly available source code (Natural Language Understanding Toolkit, 2011) running
experiments.
6.4 Implementation Details Parameter Setting
implemented method (see DCI-source, 2015) part JaTeCS (2015) framework. used popular SVMlight (2008) implementation Support Vector Machines
learning device, default parameters, DCI baselines NoTrans, Upper,
MT.
experiments set support (see Section 5.1) = 30, following indications Prettenhofer Stein (2010) Webis-CLS-10 dataset. Since amount
unlabelled documents MDS Dataset one order magnitude smaller, case
set = 1.
emulate word oracle sake fair comparison reused bilingual dictionary2 created evaluating cross-lingual SCL Prettenhofer Stein (2010).
dictionary emulates context-unaware word-translation oracle, i.e., source word
2. Note used different pivot selection criterion, detailed Section 5.1, therefore oracle
could queried translate words never considered cited work, thus might

148

fiDistributional Correspondence Indexing

mapped likely translation; potential problems arising ambiguity
single words simply disregarded.
One important factor take consideration number calls issued
oracle; oracle simulates human translator, number thus indicator
human effort required perform domain adaptation. limited number translations
top 2m terms highest mutual information, number pivots
words. order perform comparisons methods, fixed number pivots
= 100, corresponds minimal setup tested Prettenhofer Stein
(2010). Section 6.6 explore impact accuracy due variations value
m.
Parameters polynomial RBF kernel DCFs optimized via grid search
Books DVDs cross-domain adaptation MDS dataset, EnglishBooks
GermanBooks cross-lingual adaptation Webis-CLS-10 dataset (as done previous
research, Prettenhofer & Stein, 2010). actual values ended using b = 0.5
= 0.82 MDS, b = 0.8 = 0.88 Webis-CLS-10. set = 0
(homogeneous) polynomial DCF cases, perceive consistent
improvement justifies complex grid search exploration two parameters.
used normalized tf idf weighting criterion represent co-occurrence matrices
experiments.
6.5 Experimental Results
section present results experiments using different DCFs. Experiments presented different domain adaptation setups, including cross-domain adaptation (Section 6.5.1), cross-lingual adaptation (Section 6.5.2), cross-domain/cross-lingual
adaptation (Section 6.5.3). Additional related experiments conducted Section
6.6.
sake brevity consistently notation Ls Cs Lt Ct introduced
earlier, use single upper-case character (as defined Section 6.1) denote
languages domains involved experimental setup. example, EB GD
denotes experiment EnglishBooks used source GermanDVDs used
target.
6.5.1 Cross-Domain Results
Table 8 reports results obtained MDS dataset, including (a) performance averages
product category, i.e., computed averaging results obtained product
category considered target domain, (b) global averages. results reported
correspond accuracy different methods3 computed via 5-fold cross-validation,
i.e., using 1,600 training documents 400 test documents run. direct comparison many methods (He et al., 2011; Xia & Zong, 2011; Denecke, 2009;
Ponomareva & Thelwall, 2012) would also feasible principle. omitted direct comnot present dictionary. cases happened rarely however, preferred simply skip
candidates rather completing bilingual dictionary, order guarantee fair comparison.
3. Missing results ones reported original papers.

149

fiMoreo, Esuli, & Sebastiani

parisons methods since previous research shown comparable,
superior, SFA.
Table 8: Cross-domain adaptation MDS dataset.
Task
ED EB
EE EB
EK EB
EB ED
EE ED
EK ED
EB EE
ED EE
EK EE
EB EK
ED EK
EE EK
Books
DVDs
Electronics
Kitchen
Average

NoTrans
0.728
0.707
0.709
0.772
0.706
0.727
0.708
0.730
0.827
0.745
0.740
0.840
0.715
0.735
0.755
0.775
0.745

Upper
0.844
0.844
0.844
0.847
0.847
0.847
0.869
0.869
0.869
0.902
0.902
0.902
0.844
0.847
0.869
0.902
0.866

SCL-MI
0.797
0.754
0.686
0.758
0.762
0.769
0.759
0.741
0.868
0.789
0.814
0.859
0.746
0.763
0.789
0.821
0.780

SFA
0.775
0.757
0.748
0.814
0.772
0.766
0.725
0.767
0.851
0.788
0.808
0.868
0.760
0.784
0.781
0.821
0.786

SST
0.763
0.788
0.836
0.852
0.810

SDA
0.724
0.768
0.807
0.804
0.902
0.835
0.806
0.872
0.802
0.844
0.803
0.777
0.766
0.847
0.827
0.808
0.812

SDAsh
0.768
0.780
0.837
0.855
0.905
0.854
0.824
0.875
0.820
0.846
0.821
0.811
0.795
0.871
0.840
0.826
0.833

Linear
0.825
0.766
0.783
0.808
0.768
0.788
0.810
0.822
0.855
0.834
0.858
0.864
0.791
0.788
0.829
0.852
0.815

PMI
0.827
0.763
0.783
0.811
0.779
0.789
0.822
0.832
0.851
0.839
0.856
0.864
0.791
0.793
0.835
0.853
0.818

AMI
0.811
0.753
0.769
0.806
0.765
0.781
0.793
0.812
0.843
0.822
0.846
0.851
0.778
0.784
0.816
0.840
0.804

Cos
0.824
0.764
0.790
0.817
0.774
0.799
0.822
0.824
0.858
0.835
0.864
0.868
0.793
0.797
0.835
0.856
0.820

Poly
0.830
0.776
0.791
0.829
0.799
0.807
0.826
0.833
0.863
0.844
0.861
0.874
0.799
0.811
0.841
0.860
0.828

RBF
0.825
0.765
0.784
0.815
0.771
0.798
0.821
0.826
0.857
0.835
0.863
0.867
0.791
0.795
0.835
0.855
0.819

configurations DCI outperform compared methods, exception
SDAsh ; MI-based DCF performed slightly worse SST average.
noted, however, SST SDAsh use different problem setting, since train
multiple domains. concretely, SST trained three four domains
MDS dataset create sentiment thesaurus, SDAsh exploited 22 domains included
previous version MDS dataset train auto-encoder. Furthermore, SDA
SDAsh rely deep learning approach, paradigm requires significant
computational power many parameters tuned. Notwithstanding this, DCI
polynomial kernel DCF (with two parameters) obtained three best averaged
results (i.e., Books, Electronics, Kitchen) five, leveraging additional
domain, requiring low computational cost (as later discussed Section 6.7).
Table 9 reports experiments cross-domain adaptation using Webis-CLS-10 dataset.
results consistent previous observations, i.e., polynomial cosinebased DCFs best performers, followed PMI RBF functions. case
best results obtained DCI close Upper, surprisingly surpass
ED EB EM EB. conjecture improvement may due larger
size unlabelled sets, one order magnitude greater respect MDS
dataset, thus allowing robust evaluations cross-domain consistency function
st () DCF.
6.5.2 Cross-Lingual Results
Table 10 reports results Webis-CLS-10 dataset cross-lingual adaptation.
discussed earlier, source language always English, target languages include
German, French, Japanese.
DCI outperformed MT baseline average cases PMI DCF
German case. kernel-based DCFs outperformed compared methods terms
150

fiDistributional Correspondence Indexing

Table 9: Cross-domain performance Webis-CLS-10 dataset.
Task
ED EB
EM EB
EB ED
EM ED
EB EM
ED EM
Books
DVDs
Music
Average

NoTrans
0.803
0.783
0.798
0.778
0.786
0.804
0.793
0.788
0.795
0.792

Upper
0.829
0.829
0.831
0.831
0.845
0.845
0.829
0.831
0.845
0.835

SCL-MI
0.839
0.823
0.810
0.797
0.804
0.823
0.831
0.804
0.814
0.816

Linear
0.840
0.828
0.798
0.802
0.825
0.831
0.834
0.800
0.828
0.821

PMI
0.843
0.838
0.812
0.821
0.835
0.833
0.841
0.817
0.834
0.830

AMI
0.831
0.826
0.788
0.798
0.816
0.815
0.829
0.793
0.816
0.812

Cos
0.851
0.840
0.818
0.821
0.838
0.829
0.846
0.819
0.834
0.833

Poly
0.855
0.841
0.818
0.822
0.836
0.832
0.848
0.820
0.834
0.834

RBF
0.848
0.838
0.806
0.816
0.831
0.827
0.843
0.811
0.829
0.828

Table 10: Cross-lingual performance Webis-CLS-10 dataset.
Task
EB GB
ED GD
EM GM
EB FB
ED FD
EM FM
EB JB
ED JD
EM JM
German
French
Japanese
Average

Upper
0.868
0.835
0.859
0.862
0.872
0.890
0.812
0.834
0.842
0.854
0.875
0.829
0.852

MT
0.808
0.800
0.791
0.821
0.795
0.765
0.692
0.722
0.714
0.800
0.794
0.709
0.767

SCL-MI
0.833
0.809
0.829
0.813
0.804
0.781
0.770
0.764
0.773
0.824
0.799
0.769
0.797

LSI
0.776
0.796
0.727
0.792
0.778
0.726
0.738
0.754
0.734
0.766
0.765
0.742
0.758

KCCA
0.791
0.776
0.695
0.767
0.782
0.748
0.792
0.782
0.735
0.754
0.766
0.770
0.763

OPCA
0.747
0.766
0.714
0.746
0.705
0.718
0.745
0.737
0.750
0.742
0.723
0.744
0.736

TSL
0.792
0.819
0.726
0.813
0.820
0.766
0.794
0.793
0.762
0.779
0.800
0.783
0.787

SSMC
0.819
0.823
0.813
0.831
0.827
0.805
0.738
0.776
0.775
0.818
0.821
0.763
0.801

Linear
0.798
0.826
0.844
0.746
0.823
0.816
0.779
0.822
0.826
0.823
0.795
0.809
0.809

PMI
0.714
0.819
0.850
0.761
0.823
0.827
0.731
0.768
0.816
0.794
0.804
0.772
0.790

AMI
0.797
0.800
0.837
0.768
0.801
0.818
0.711
0.797
0.807
0.811
0.796
0.772
0.793

Cos
0.827
0.822
0.856
0.842
0.827
0.844
0.758
0.801
0.839
0.835
0.838
0.799
0.824

Poly
0.837
0.833
0.844
0.819
0.806
0.840
0.754
0.795
0.832
0.838
0.822
0.794
0.818

RBF
0.829
0.788
0.801
0.844
0.846
0.803
0.782
0.761
0.826
0.806
0.831
0.790
0.809

average accuracy, best result obtained one kernel-based DCFs 11
12 cases. best performing DCFs cosine polynomial DCFs.
6.5.3 Cross-Domain/Cross-Lingual Results
Table 11 reports experiments cross-domain/cross-lingual setting.
setting arguably difficult one, since term space
marginal probabilities domains differ, reflected noticeable degradation
MT results. Notwithstanding this, consistent observations could derived results. cosine polynomial DCFs confirm superiority respect
compared methods. best result obtained DCI 17 18 cases; half
cases cosine DCF obtained best result.
6.5.4 Statistical Significance Tests
Statistical significance tests (paired t-test accuracy values Table 8) indicate
DCI configurations, exception MI, significantly better p < 0.01
SCL, SCL-MI, SFA MDS dataset. polynomial DCF, obtained
10 best results 12, higher-confidence levels obtained, i.e., p < 0.001. t-test
runs Webis-CLS-10 reveals kernel-based DCFs Linear DCF
151

fiMoreo, Esuli, & Sebastiani

Table 11: Cross-domain/cross-lingual accuracy Webis-CLS-10 dataset.
Task
ED GB
EM GB
EB GD
EM GD
EB GM
ED GM
ED FB
EM FB
EB FD
EM FD
EB FM
ED FM
ED JB
EM JB
EB JD
EM JD
EB JM
ED JM
German
French
Japanese
Books
DVDs
Music
Average

Upper
0.868
0.868
0.835
0.835
0.859
0.859
0.862
0.862
0.872
0.872
0.889
0.889
0.812
0.812
0.834
0.834
0.842
0.842
0.854
0.874
0.829
0.847
0.847
0.863
0.852

MT
0.789
0.751
0.774
0.773
0.768
0.768
0.788
0.765
0.783
0.780
0.771
0.745
0.700
0.642
0.708
0.693
0.673
0.710
0.771
0.772
0.688
0.739
0.752
0.739
0.743

SCL-MI
0.823
0.825
0.784
0.792
0.811
0.824
0.790
0.784
0.780
0.745
0.762
0.757
0.725
0.708
0.742
0.756
0.742
0.776
0.810
0.770
0.742
0.776
0.767
0.779
0.774

Linear
0.823
0.791
0.790
0.778
0.786
0.844
0.744
0.810
0.810
0.798
0.822
0.836
0.738
0.711
0.813
0.792
0.826
0.817
0.802
0.803
0.783
0.770
0.797
0.822
0.796

PMI
0.764
0.821
0.796
0.829
0.812
0.844
0.798
0.833
0.816
0.822
0.753
0.826
0.675
0.621
0.663
0.828
0.699
0.804
0.811
0.808
0.715
0.752
0.792
0.790
0.778

AMI
0.811
0.705
0.788
0.772
0.793
0.828
0.747
0.785
0.788
0.761
0.794
0.827
0.715
0.636
0.710
0.721
0.811
0.762
0.783
0.784
0.726
0.733
0.757
0.803
0.768

Cos
0.824
0.812
0.827
0.834
0.843
0.816
0.848
0.845
0.823
0.841
0.833
0.847
0.761
0.721
0.805
0.790
0.831
0.816
0.826
0.840
0.787
0.802
0.820
0.831
0.818

Poly
0.818
0.791
0.825
0.814
0.833
0.835
0.846
0.843
0.793
0.829
0.824
0.849
0.741
0.689
0.789
0.763
0.826
0.817
0.819
0.831
0.771
0.788
0.802
0.831
0.807

RBF
0.824
0.800
0.783
0.808
0.807
0.832
0.852
0.789
0.841
0.775
0.829
0.855
0.741
0.722
0.782
0.711
0.827
0.804
0.809
0.824
0.765
0.788
0.783
0.826
0.799

better SCL-MI statistical significance confidence level p < 0.01; Cosine
DCF obtained p = 0.854 107 Polynomial DCF p = 0.522 105 .
6.6 Experiments
section presents experiments aimed testing influence different parameters modules DCI, performance standard text classification setting.
Regarding effect parameters, show trend plots representative cases,
considering Linear function representative example probabilistic-based DCF
Cosine function example kernel-based DCF. plot involves three
settings, one scenario: cross-domain adaptation, cross-lingual adaptation,
cross-domain/cross-lingual adaptation. sake brevity selected illustrative
examples, omitting experiments showing similar behaviour. First, investigated sensitivity value parameter m, indicates number pivots select. Figure
1 shows performance varies variation m, 5 500 pivots.
overall tendency displayed plots performance tends stabilise
increases. Adaptations involving cross-lingual setting seem strongly affected
152

fiDistributional Correspondence Indexing

Cosine DCF

Accuracy

Linear DCF
0.9

0.9

0.85

0.85

0.8

0.8

0.75

0.75

0.7

0.7

ED->GD

0.65

0.65

ED->GM

0.6

0.6

0.55

0.55

ED->EB

0.5

0.5
0

100

200

300

400

500

0

100

200

300

400

500





Figure 1: Variation accuracy variation number pivots different setups.

number pivots. attribute effect limited capability pivot
reflect term correspondence imprecision introduced context-unaware
single-word translations oracle. negative effect seems however reduce
number pivots increases. method scales well number pivots terms
efficiency (see below), might indication simply increasing pivot set
size could feasible alternative rather moving complicated definitions
cross-lingual pivots order cover translation nuances. Larger fluctuations could
observed < 75, also surprising peaks performance extremely small values
m. example, DCI obtained 81.1% accuracy Linear DCF ED GM
adaptation 30 pivots, baselines obtained 76.8% (MT) 82.4% (SCLMI, uses 450 pivots). similar experiment reported Prettenhofer Stein
(2010) CL-SCL, varying parameter range [100, 800]. direct comparison
shows method achieves better accuracy smaller values m. Given
number calls oracle directly related (i.e., calls cross-lingual case,
2m cross-domain/cross-lingual case due cross-consistency reweighting, see
Section 5.1), follows DCI requires less human effort creating bilingual pivots.
unlabelled collection plays key role domain adaptation, responsible
term-distribution representation; thus expect better estimations distributions larger collections. investigated unlabelled set size affects
performance method. plot accuracy score obtained different reduction
ratios preserving balance Figure 2.
expected, observed trend shows accuracy high large unlabelled collections, performance tends stabilize addition unlabelled examples. Better
performance observed cross-domain experiments, even smaller distributional
representations.
also validated empirically different elements constitute DCI
method, including cross-distortion factor pivot selection, dimensionality standardization, unification process. reasons conciseness report
global improvement linear DCF averaged dataset, since consistent variations
observable DCFs. found consistent improvement 0.47% 1.02
153

fiMoreo, Esuli, & Sebastiani

Accuracy

Linear DCF

Cosine DCF

0.9

0.9

0.85

0.85

0.8

0.8

0.75

0.75

0.7

0.7

EB->EM

0.65

0.65

EB->JB

0.6

0.6

0.55

0.55

EB->JD

0.5

0.5
0

0.2

0.4
0.6
unlabeled reduction ratio

0.8

1

0

0.2

0.4
0.6
unlabeled reduction ratio

0.8

1

Figure 2: Variation accuracy variation unlabelled corpus size different
setups.

accuracy due cross-consistency pivot selection, 1.785% 3.63 due dimensionality
standardization, 1.261% 2.18 due unification.
experiments reveal classification performance seems benefit adaptation involves semantically close domains, case BooksDVDs MDS
dataset, EnglishGerman Webis-CLS-10. Analogously, performance seems degrade source target domains dissimilar, example KitchenBooks
MDS EnglishJapanese Webis-CLS-10. noticed literature
reducing distance representations source target domains crucial
order allow better knowledge transfer. Given probability distributions
unknown, distance sometimes computed approximation (the proxy Adistance Ben-David, Blitzer, Crammer, & Pereira, 2006) considers source target
documents two samples drawn distribution. proxy A-distance computed
dA = 2(1 2), error produced SVM trained discriminate
source target domains.
Figure 3 graphically compares, MDS dataset, proxy A-distances
domains (i) raw representations, (ii) DCI representations. dA
clearly reduced cross-domain space generated DCI, contributes explain
improvement performance respect baseline NoTrans raw representation. reduction even noticeable semantically close domains
ElectronicsKitchen BooksDVDs. Hence, DCI projects domains common
vector space source target distributions get effectively closer other,
thus facilitating transfer knowledge them.
Finally, Table 12 reports performance accuracy text classification setting, is,
assuming test data follows marginal distribution represented
term space training data. case, consider baselines (a) well-known
BoW representation tf idf weighting, (b) SCL-MI.
Even though amount experiments text classification case small
allow substantial claim, surprising that, runs, DCI 100 dimensions
yielded better results traditional BoW representation considering terms.
topic investigate future research.
154

fiDistributional Correspondence Indexing

Proxy A-distance raw data

2



DK

DE

BK
BD

1.75

EK
1.5
1.25
1
0.75
0.5
0.5

0.75

1

1.25

1.5

1.75

2

Proxy A-distance DCI
Figure 3: Proxy A-distances domains MDS dataset. vertical axis displays dA raw data (NoTrans), horizontal axis displays dA
vector space produced DCI using cosine DCF. abscissa coordinate
point (e.g., BK) averaged dA produced domain adaptation
directions (e.g., EB EK EK EB).

Table 12: Text Classification performance Webis-CLS-10 dataset.
Task
EB EB
ED ED
EM EM
Average

BoW
0.829
0.831
0.845
0.835

SCL-MI
0.828
0.815
0.832
0.825

Linear
0.848
0.819
0.838
0.835

PMI
0.855
0.826
0.846
0.842

AMI
0.836
0.798
0.825
0.820

Cos
0.854
0.818
0.841
0.838

Poly
0.856
0.821
0.844
0.840

RBF
0.853
0.809
0.835
0.832

6.7 Efficiency
computational cost DCI asymptotically bound cost projecting f terms
two domains m-dimensional space, could roughly estimated
O(f mc), c component due cost comparing two term distribution
models, depends average prevalence c terms unlabelled corpus
155

fiMoreo, Esuli, & Sebastiani

typically much smaller effective number unlabelled documents result
sparsity.
Note probabilistic functions discussed Section 4 implemented
efficiently using sparse data structures. example, calculating joint probability
P (v, w) achieved O(c) steps intersecting two hash sets c expected elements.
kernel-based DCFs discussed depend dot product Euclidean distance,
also computed O(c) iterating non-zero values. Thus, DCI
computational cost O(f mc); note fixed parameter, overall cost
also considered O(f c). relevant alternatives typically involve singular
value decomposition matrix multiplication, thus resulting O(df c) algorithms,
number documents contexts collection.
performed efficiency tests comparing DCI SCL Webis-CLS-10 dataset.
test run combinations source target classes target languages,
amounts 36 runs. dedicated computer4 run experiments
number 10 threads. Table 13 shows averaged time scores obtained.
Table 13: Running time (in seconds) DCI two different DCFs (linear cosine)
SCL.
Min (s)
Max (s)
Average (s)
Standard Deviation

Linear
6.406
22.119
11.553
3.976

Cosine
7.501
27.032
17.774
5.516

SCL-MI
449.988
859.719
678.834
98.324

SCL suffers much higher computational costs DCI. average, DCI reduced
98.3% 97.4% computational cost respect SCL linear cosine
DCFs, respectively. SCL required = 450 binary optimization problems translations,
required performing LSA predictive parameters. DCI-based method obtained better results substantially less time. efficiency tests suggest DCI could
scale well larger datasets.
6.8 Embeddings
final note, intuitions behind DCI strong relationships behind
word embeddings, deep learning, research area gained interest
last renewed years. Neural language models trained obtain meaningful term representations seem capture interesting language regularities (Bengio, 2009). Although
deep learning applied cross-domain adaptation Glorot et al. (2011) (a work
used baseline Section 6.5.1), cross-lingual adaptation requires additional effort.
is, consistently obtain bilingual word embeddings, large unlabelled datasets
aligned corpora (Zou, Socher, Cer, & Manning, 2013) bilingual dictionaries (Mikolov, Le,
& Sutskever, 2013) typically required. Assuming small set words translated (no
4. Computer specifications: 64-bit Intel Core (TM) Genuine-Intel I7 12 processors 3.47GH, 24GB
RAM, running Ubuntu 14.04.2 LTS.

156

fiDistributional Correspondence Indexing

200 words experiments), method obtains term profiles perform
consistently languages classification task. Table 14 illustrates semantic
properties captured term profiles; lists similar (via cosine similarity)
target terms given source term.

Table 14: Five similar terms three target languages (German, French,
Japanese) given three terms (beautifully, classical, delightful) English
Music domain.
beautifully
schone ( beautiful)
liebevoll ( loving)
sehnsucht ( longing)
ungewohnlich ( unusual)
phantastisch ( fantastic)

0.635
0.596
0.533
0.510
0.507

classical
adagio
Martenot
Charles-Marie
violoncelle ( cello)
soliste ( soloist)

0.767
0.746
0.736
0.727
0.720

delightful
( attractive)
( portrayed)
( scenes)
( delicate)
( taste)

0.610
0.546
0.545
0.542
0.538

word embeddings, even assuming external resources available, additional optimization problem, posed geometrical transformation involving scaling
rotating data matrices, subsequently required order align two embedding
spaces. done Mikolov et al. (2013) forcing embedding representations
words bilingual dictionary get closer matrix
transformation. Apart additional computational cost may involve, believe
method might directly applicable scenario cross-domain
cross-lingual adaptations tackled simultaneously. main reason
final transformation aims aligning meaning words taken bilingual dictionary domains, could play different roles across domains, i.e.,
pivots. embeddings generated DCI require computationally expensive
post-processing, correspondences roles different terms domains turn
directly captured DCF scores pivots.
illustrate this, used LSI plot bidimensional space important term
profiles EB GM adaptation obtained DCI cosine DCF. Figure 4
shows two zoomed-in areas bilingual space. Noticeably, left-most part plot
seems represent positive sentiment, right-most one seems capture
negative sentiment. relevant semantic correspondences could directly observed.
Semantically related English words, expecting, expected, hoping, projected
close together space. interestingly, related semantics seem preserved
across languages, e.g., English words boring, irritating, bored German words erschreckend
(terrifying), acherlich (ridiculous), schrecklich (terribly) projected regions
space close other. interesting cases could regarded examples crosssemantic correspondence. example classic love (book genres) English reviews
projected close folk rock (music genres) German reviews, incidental
semantic correspondence emerged due juxtaposition cross-domain crosslingual adaptation.
157

fiMoreo, Esuli, & Sebastiani

Figure 4: Vector profiles word embeddings obtained EB GM adaptation. Zoom
positive (left) negative (right) sentiment area. plot obtained
applying LSI terms deemed highly informative mutual information.

preliminary experiments suggest DCI embeddings could potentially useful tasks natural language processing. however require dedicated
investigation defer future work.

7. Conclusions Future Work
proposed Distributional Correspondence Indexing, efficient method domain
adaptation represents terms vectorial space based distributional correspondence respect small, fixed set terms. representation motivated
Harris distributional hypothesis notion pivot term Blitzer et al. (2006);
method indexes documents different domains common vector space based
semantic correspondence.
Empirical evaluation two popular sentiment analysis benchmarks shows
method outperforms several state-of-the-art approaches different domain-adaptation settings, including cross-domain cross-lingual sentiment adaptation. also proposed extended formulation domain adaptation problem, tackles crossdomain adaptation cross-language adaptation time; present
experiments system compares favourably related approaches.
point view efficiency, show method require modest computational resources,
indication DCI scale well huge collections; particular,
cross-lingual case required smaller amount human intervention competing approaches order create pivot set. presented high-performance DCFs
parameter-free, valuable characteristic domain adaptation setting,
given expected count labelled data drawn target distribution
parameters could optimized.
bilingual pivots created context-unaware word-translator oracle represent arguably oversimplified naive approach translation problem. Notwithstanding this,
DCI seems compensate aggregative contribution partial semantics scat158

fiDistributional Correspondence Indexing

tered several pivots. regard, interested enhancing concept pivots
cross-lingual adaptation general direction better captures context-aware
multi-word translation, attempt polylingual case. possible directions
might include enriching term representation incorporate part-of-speech tags
syntactic information, also keeping track contexts given term
appeared. Moreover, motivated empirical evidences cross-domain experiments suggested comparable performance could achieved even extremely
reduced sets pivots, investigate sophisticated pivot selection techniques
better characterizing concept pivot geometrical properties vector space
generate. also plan put test DCI domains settings, including
multi-class multi- single-label datasets, highly imbalanced classes, transductive
problems.

Acknowledgements
grateful Xavier Glorot sending us additional details experiments reported previous work (Glorot et al., 2011).
paper extension short paper Esuli Moreo Fernandez (2015).
Fabrizio Sebastiani leave Consiglio Nazionale delle Ricerche, Italy.

References
Ando, R. K., & Zhang, T. (2005). framework learning predictive structures
multiple tasks unlabeled data. Journal Machine Learning Research, 6,
18171853.
Ben-David, S., Blitzer, J., Crammer, K., & Pereira, F. (2006). Analysis representations
domain adaptation. Proceedings 20th Annual Conference Neural
Information Processing Systems (NIPS 2006), pp. 137144, Vancouver, CA.
Bengio, Y. (2009). Learning deep architectures AI. Foundations Trends Machine
Learning, 2 (1), 1127.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal
Machine Learning Research, 3, 9931022.
Blitzer, J., Dredze, M., & Pereira, F. (2007). Biographies, Bollywood, boom-boxes
blenders: Domain adaptation sentiment classification. Proceedings 45th
Annual Meeting Association Computational Linguistics (ACL 2007), pp.
440447, Prague, CZ.
Blitzer, J., McDonald, R., & Pereira, F. (2006). Domain adaptation structural correspondence learning. Proceedings 4th Conference Empirical Methods
Natural Language Processing (EMNLP 2006), pp. 120128, Sydney, AU.
Bollegala, D., Weir, D., & Carroll, J. (2011). Using multiple sources construct sentimentsensitive thesaurus cross-domain sentiment classification. Proceedings
49th Annual Meeting Association Computational Linguistics (ACL 2011),
pp. 132141, Portland, US.
159

fiMoreo, Esuli, & Sebastiani

Dai, W., Xue, G.-R., Yang, Q., & Yu, Y. (2007). Transferring nave Bayes classifiers text
classification. Proceedings 22nd AAAI Conference Artificial Intelligence
(AAAI 2007), pp. 540545, Vancouver, CA.
DCI-source (2015) http://hlt.isti.cnr.it/dciext/.
Deerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A.
(1990). Indexing latent semantic analysis. Journal American Society
Information Science, 41 (6), 391407.
Denecke, K. (2009). SentiWordNet scores suited multi-domain sentiment classification?. Proceedings 4th International Conference Digital Information
Management (ICDIM 2009), pp. 3338, Ann Arbor, US.
Dumais, S. T., Letsche, T. A., Littman, M. L., & Landauer, T. K. (1997). Automatic crosslanguage retrieval using latent semantic indexing. Working Notes AAAI
Spring Symposium Cross-language Text Speech Retrieval, pp. 1824, Stanford,
US.
Esuli, A., & Moreo Fernandez, A. (2015). Distributional correspondence indexing crosslanguage text categorization. Proceedings 37th European Conference
Information Retrieval (ECIR 2015), pp. 104109, Wien, AT.
Gabrilovich, E., & Markovitch, S. (2007). Computing semantic relatedness using Wikipediabased explicit semantic analysis. Proceedings 20th International Joint Conference Artifical Intelligence (IJCAI 2007), pp. 16061611, San Francisco, US.
Gao, J., Fan, W., Jiang, J., & Han, J. (2008). Knowledge transfer via multiple model
local structure mapping. Proceedings 14th ACM International Conference
Knowledge Discovery Data Mining (KDD 2008), pp. 283291, Las Vegas, US.
Gliozzo, A., & Strapparava, C. (2005). Cross-language text categorization acquiring
multilingual domain models comparable corpora. Proceedings ACL
Workshop Building Using Parallel Texts, pp. 916, Ann Arbor, US.
Gliozzo, A., & Strapparava, C. (2006). Exploiting comparable corpora bilingual dictionaries cross-language text categorization. Proceedings 44th Annual
Meeting Association Computational Linguistics (ACL 2006), pp. 553560,
Sydney, AU.
Glorot, X., Bordes, A., & Bengio, Y. (2011). Domain adaptation large-scale sentiment
classification: deep learning approach. Proceedings 28th International
Conference Machine Learning (ICML 2011), pp. 513520, Bellevue, US.
Harris, Z. S. (1954). Distributional structure. Word, 10 (23), 146162.
He, Y., Lin, C., & Alani, H. (2011). Automatically extracting polarity-bearing topics
cross-domain sentiment classification. Proceedings 49th Annual Meeting
Association Computational Linguistics (ACL 2011), pp. 123131, Portland,
US.
JaTeCS (2015) http://hlt.isti.cnr.it/jatecs/.
160

fiDistributional Correspondence Indexing

Joachims, T. (1999). Transductive inference text classification using support vector
machines. Proceedings 16th International Conference Machine Learning
(ICML 1999), pp. 200209, Bled, SL.
Kanerva, P., Kristofersson, J., & Holst, A. (2000). Random indexing text samples latent semantic analysis. Proceedings 22nd Annual Conference Cognitive
Science Society, p. 1036, Austin, US.
Koehn, P., & Knight, K. (2002). Learning translation lexicon monolingual corpora.
Proceedings ACL 2002 Workshop Unsupervised Lexical Acquisition, pp.
916, Philadelphia, US.
Landauer, T. K., & Dumais, S. T. (1997). solution Platos problem: latent
semantic analysis theory acquisition, induction, representation knowledge.
Psychological Review, 104 (2), 211240.
Li, F., Pan, S. J., Jin, O., Yang, Q., & Zhu, X. (2012a). Cross-domain co-extraction sentiment topic lexicons. Proceedings 50th Annual Meeting Association
Computational Linguistics (ACL 2012), pp. 410419, Jeju Island, KR.
Li, L., Jin, X., & Long, M. (2012b). Topic correlation analysis cross-domain text classification. Proceedings 26th AAAI Conference Artificial Intelligence (AAAI
2012), pp. 9981004, Toronto, CA.
Ling, X., Dai, W., Xue, G.-R., Yang, Q., & Yu, Y. (2008). Spectral-domain transfer learning.
Proceedings 14th ACM International Conference Knowledge Discovery
Data Mining (KDD 2008), pp. 488496, Las Vegas, US.
Liu, B. (2012). Sentiment Analysis Opinion Mining. Morgan Claypool Publishers,
San Rafael, US.
Mikolov, T., Le, Q. V., & Sutskever, I. (2013). Exploiting Similarities among Languages
Machine Translation. ArXiv e-prints, arXiv:1309.4168 [cs.CL].
Moen, H., & Marsi, E. (2013). Cross-lingual random indexing information retrieval.
Proceedings 1st International Conference Statistical Language Speech
Processing (SLSP 2013), pp. 164175, Tarragona, ES.
MSD dataset (2007) http://www.cs.jhu.edu/~mdredze/datasets/sentiment/.
Natural Language Understanding Toolkit (2011) https://github.com/pprett/nut.
Pan, S. J., Ni, X., Sun, J.-T., Yang, Q., & Chen, Z. (2010). Cross-domain sentiment classification via spectral feature alignment. Proceedings 19th International
Conference World Wide Web (WWW 2010), pp. 751760, Raleigh, US.
Pan, S. J., & Yang, Q. (2010). survey transfer learning. IEEE Transactions
Knowledge Data Engineering, 22 (10), 13451359.
Pan, W., Zhong, E., & Yang, Q. (2012). Transfer learning text mining. Aggarwal,
C. C., & Zhai, C. (Eds.), Mining Text Data, pp. 223258. Springer, Heidelberg, DE.
Pang, B., & Lee, L. (2008). Opinion mining sentiment analysis. Foundations Trends
Information Retrieval, 2 (1/2), 1135.
161

fiMoreo, Esuli, & Sebastiani

Peirsman, Y., & Pado, S. (2010). Cross-lingual induction selectional preferences
bilingual vector spaces. Proceedings 8th Annual Conference North
American Chapter Association Computational Linguistics (NAACL 2010),
pp. 921929, Los Angeles, US.
Platt, J. C., Toutanova, K., & Yih, W.-t. (2010). Translingual document representations
discriminative projections. Proceedings 8th Conference Empirical
Methods Natural Language Processing (EMNLP 2010), pp. 251261, Cambridge,
US.
Ponomareva, N., & Thelwall, M. (2012). neighbours help? exploration graph-based
algorithms cross-domain sentiment classification. Proceedings 2012 Joint
Conference Empirical Methods Natural Language Processing Computational
Natural Language Learning (EMNLP/CoNLL 2012), pp. 655665, Jeju Island, KR.
Prettenhofer, P., & Stein, B. (2010). Cross-language text classification using structural
correspondence learning. Proceedings 48th Annual Meeting Association
Computational Linguistics (ACL 2010), pp. 11181127, Uppsala, SE.
Prettenhofer, P., & Stein, B. (2011). Cross-lingual adaptation using structural correspondence learning. ACM Transactions Intelligent Systems Technology, 3 (1), Article 13.
Rapp, R. (1995). Identifying word translations non-parallel texts. Proceedings
33rd Annual Meeting Association Computational Linguistics (ACL 1995), pp.
320322, Cambridge, US.
Rapp, R. (1999). Automatic identification word translations unrelated English
German corpora. Proceedings 37th Annual Meeting Association
Computational Linguistics (ACL 1999), pp. 519526, College Park, US.
Rigutini, L., Maggini, M., & Liu, B. (2005). EM-based training algorithm crosslanguage text categorization. Proceedings 3rd IEEE/WIC/ACM International Conference Web Intelligence (WI 2005), pp. 529535, Compiegne, FR.
Sahlgren, M. (2005). introduction random indexing. Proceedings Workshop
Methods Applications Semantic Indexing, Copenhagen, DK.
Sorg, P., & Cimiano, P. (2008). Cross-language information retrieval explicit semantic
analysis. Working Notes 2008 Cross-Language Evaluation Forum (CLEF
2008), Aarhus, DE.
Sorg, P., & Cimiano, P. (2012). Exploiting Wikipedia cross-lingual multilingual
information retrieval. Data Knowledge Engineering, 74, 2645.
SVMlight (2008) http://svmlight.joachims.org/.
Vinokourov, A., Shawe-Taylor, J., & Cristianini, N. (2002). Inferring semantic representation text via cross-language correlation analysis. Proceedings 16th Annual
Conference Neural Information Processing Systems (NIPS 2002), pp. 14731480,
Vancouver, CA.
Wan, C., Pan, R., & Li, J. (2011). Bi-weighting domain adaptation cross-language text
classification. Proceedings 22nd International Joint Conference Artificial
Intelligence (IJCAI 2011), pp. 15351540, Barcelona, ES.
162

fiDistributional Correspondence Indexing

Wan, X. (2009). Co-training cross-lingual sentiment classification. Proceedings
47th Annual Meeting Association Computational Linguistics
4th International Joint Conference Natural Language Processing (ACL/IJCNLP
2009), pp. 235243, Singapore, SN.
Wang, P., Domeniconi, C., & Hu, J. (2008). Using Wikipedia co-clustering-based crossdomain text classification. Proceedings 8th IEEE International Conference
Data Mining (ICDM 2008), pp. 10851090, Pisa, IT.
Webis-CLS dataset (2010)
http://www.uni-weimar.de/en/media/chairs/webis/
research/corpora/corpus-webis-cls-10/.
Xia, R., & Zong, C. (2011). POS-based ensemble model cross-domain sentiment
classification. Proceedings 5th International Joint Conference Natural
Language Processing (IJCNLP 2011), pp. 614622, Chiang Mai, TH.
Xiang, E. W., Cao, B., Hu, D. H., & Yang, Q. (2010). Bridging domains using worldwide knowledge transfer learning. IEEE Transactions Knowledge Data
Engineering, 22 (6), 770783.
Xiao, M., & Guo, Y. (2013). novel two-step method cross-language representation
learning. Proceedings 27th Annual Conference Neural Information Processing Systems (NIPS 2013), pp. 12591267, Lake Tahoe, US.
Xiao, M., & Guo, Y. (2014). Semi-supervised matrix completion cross-lingual text
classification. Proceedings 28th AAAI Conference Artificial Intelligence
(AAAI 2014), pp. 16071614, Quebec City, CA.
Xue, G.-R., Dai, W., Yang, Q., & Yu, Y. (2008). Topic-bridged PLSA cross-domain text
classification. Proceedings 31st ACM International Conference Research
Development Information Retrieval (SIGIR 2008), pp. 627634, Singapore,
SN.
Zhuang, F., Luo, P., Xiong, H., He, Q., Xiong, Y., & Shi, Z. (2011). Exploiting associations
word clusters document classes cross-domain text categorization.
Statistical Analysis Data Mining, 4 (1), 100114.
Zou, W. Y., Socher, R., Cer, D. M., & Manning, C. D. (2013). Bilingual word embeddings
phrase-based machine translation. Proceedings 11th Conference Empirical
Methods Natural Language Processing (EMNLP 2013), pp. 13931398, Seattle, US.

163

fiJournal Artificial Intelligence Research 55 (2016) 953-994

Submitted 09/15; published 04/16

Bilingual Distributed Word Representations
Document-Aligned Comparable Data
Ivan Vulic

iv250@cam.ac.uk

University Cambridge
Department Theoretical Applied Linguistics
9 West Road, CB3 9DP, Cambridge, UK

Marie-Francine Moens

marie-francine.moens@cs.kuleuven.be

KU Leuven
Department Computer Science
Celestijnenlaan 200A, 3001 Heverlee, Belgium

Abstract
propose new model learning bilingual word representations non-parallel
document-aligned data. Following recent advances word representation learning,
model learns dense real-valued word vectors, is, bilingual word embeddings (BWEs).
Unlike prior work inducing BWEs heavily relied parallel sentence-aligned corpora and/or readily available translation resources dictionaries, article reveals
BWEs may learned solely basis document-aligned comparable data without additional lexical resources syntactic information. present comparison
approach previous state-of-the-art models learning bilingual word representations comparable data rely framework multilingual probabilistic
topic modeling (MuPTM), well distributional local context-counting models.
demonstrate utility induced BWEs two semantic tasks: (1) bilingual lexicon
extraction, (2) suggesting word translations context polysemous words. simple
yet effective BWE-based models significantly outperform MuPTM-based contextcounting representation models comparable data well prior BWE-based models,
acquire best reported results tasks three tested language pairs.

1. Introduction
huge body work distributional semantics word representation learning almost
exclusively revolves around distributional hypothesis (Harris, 1954) - idea
states similar words occur similar contexts. current corpus-based approaches
semantics rely contextual evidence one way another. Roughly speaking, word
representations typically learned using two families distributional context-based
models: (1) global matrix factorization models latent semantic analysis (LSA)
(Landauer & Dumais, 1997) generative probabilistic models latent Dirichlet
allocation (LDA) (Blei, Ng, & Jordan, 2003), model word co-occurrence
document paragraph level; (2) local context window models represent words
sparse high-dimensional context vectors, model word co-occurrence level
selected neighboring words (Turney & Pantel, 2010), generative probabilistic models
learn probability distribution vocabulary word context window latent
variable (Deschacht & Moens, 2009; Deschacht, De Belder, & Moens, 2012).
c
2016
AI Access Foundation. rights reserved.

fiVulic & Moens

hand, dense real-valued vectors known distributed representations
words word embeddings (WEs) (e.g., Bengio, Ducharme, Vincent, & Janvin, 2003; Collobert & Weston, 2008; Mikolov, Chen, Corrado, & Dean, 2013a; Pennington, Socher, &
Manning, 2014) introduced recently, first part neural network based architectures statistical language modeling. WEs serve richer coherent word
representations ones obtained aforementioned traditional distributional
semantic models, illustrative comparative studies available recently published
relevant work (e.g., Mikolov, Yih, & Zweig, 2013d; Baroni, Dinu, & Kruszewski, 2014; Levy,
Goldberg, & Dagan, 2015).
natural extension interest monolingual multilingual word embeddings
occurred recently (e.g., Klementiev, Titov, & Bhattarai, 2012; Hermann & Blunsom, 2014b).
operating multilingual settings, highly desirable learn embeddings words
denoting similar concepts close shared bilingual embedding space (e.g.,
representations English word school Spanish word escuela
similar). BWEs may used myriad multilingual natural language
processing tasks beyond, fundamental tasks leaning bilingual meaning
representations, e.g., computing cross-lingual multilingual semantic word similarity
extracting bilingual word lexicons using induced bilingual embedding space (see Figure 1). However, models critically require (at least) sentence-aligned parallel data
readily-available translation dictionaries induce bilingual word embeddings (BWEs)
consistent closely aligned different languages.
1.1 Contributions
best knowledge, article presents first work showcase bilingual word embeddings may induced directly basis comparable data without
additional bilingual resources sentence-aligned parallel data translation dictionaries. focus document-aligned comparable corpora (e.g., Wikipedia articles
aligned inter-wiki links, news texts discussing theme).
new bilingual embedding learning model makes use pseudo-bilingual documents
constructed merging content two coupled documents document pair,
propose evaluate two different strategies construct pseudo-bilingual
documents: (1) merge randomly shuffle strategy randomly permutes words
languages pseudo-bilingual document, (2) length-ratio shuffle strategy,
deterministic method retains monolingual word order intermingling words
cross-lingually. additional pre-training shuffling strategies ensure source
language words target language words occur contexts source target
language word. monolingual model skip-gram negative sampling (SGNS)
word2vec package (Mikolov, Sutskever, Chen, Corrado, & Dean, 2013c)
trained shuffled pseudo-bilingual documents. procedure, steer semantically similar words different languages towards similar representations
shared bilingual embedding space, effectively use available bilingual contexts instead
monolingual ones. model treats documents bags-of-words (i.e., include
syntactic information) even rely sentence boundary information.
954

fiBilingual Distributed Word Representations Document-Aligned Data

summary, main contributions article are:
(1) present BWE Skip-Gram (BWESG), first model induces bilingual word
embeddings directly document-aligned non-parallel data. test evaluate two
main variants model based pre-training shuffling step. main strength
presented model lies favourable trade-off simplicity effectiveness.
(2) provide qualitative quantitative analysis model. draw analogies
comparisons prior work inducing word representations data type:
document-aligned comparable corpora (e.g., models relying multilingual probabilistic
topic modeling framework (MuPTM)).
(3) demonstrate utility induced BWEs word type level task bilingual lexicon extraction (BLE) Wikipedia data three language pairs. BLE model
based BWEs significantly outperforms MuPTM-based context-counting BLE
models, acquires best reported scores benchmarking BLE datasets.
(4) demonstrate utility induced BWEs word token level task
suggesting word translations context (SWTC) (Vulic & Moens, 2014) three
language pairs. SWTC model based BWEs significantly outscores best
scoring MuPTM-based SWTC models setting without use parallel data
translation dictionaries, acquires best reported results benchmarking SWTC datasets.
(5) also present comparison state-of-the-art BWE induction models (Mikolov,
Le, & Sutskever, 2013b; Hermann & Blunsom, 2014b; Gouws, Bengio, & Corrado, 2015)
BLE SWTC. Results reveal simple yet effective approach on-par
outperforms BWE induction models rely parallel data readily available
dictionaries learn shared bilingual embedding spaces. addition, preliminary experiments BWESG parallel Europarl data demonstrate model also useful
trained sentence-aligned data, reaching performance benchmarking BWE
induction models parallel data (e.g., Hermann & Blunsom, 2014b).

2. Related Work
section motivate opt building model inducing bilingual
word embeddings comparable document-aligned data. clearer overview,
split related work three broad clusters: (1) monolingual word embeddings, (2) bilingual
word embeddings, (3) bilingual word representations document-aligned data.
2.1 Monolingual Word Embeddings
idea representing words continuous real-valued vectors dates way back mid80s (Rumelhart, Hinton, & Williams, 1986; Elman, 1990). idea met resurgence
decade ago (Bengio et al., 2003), neural language model learns word embeddings
part neural network architecture statistical language modeling. work inspired
approaches learn word embeddings within neural-network language modeling
framework (Collobert & Weston, 2008; Collobert, Weston, Bottou, Karlen, Kavukcuoglu, &
Kuksa, 2011). Word embeddings tailored capture semantics encode continuous
955

fiVulic & Moens

Monolingual

vs

Bilingual

Figure 1: toy 3D shared bilingual embedding space Gouws et al. (2015):
monolingual spaces words similar meanings similar representations, bilingual spaces words two different languages similar meanings
similar representations (both mono- cross-lingually).
notion semantic similarity (as opposed semantically poorer discrete representations),
necessary share information words text units.
Recently, skip-gram continuous bag-of-words (CBOW) model Mikolov et
al. (2013a, 2013c) revealed full neural-network structure needed
learn high-quality word embeddings (with extremely decreased training times compared
full-fledged neural network models, see Mikolov et al., 2013a full analysis
complexity models). models fact simple single-layered architectures,
objective predict words context given word (skip-gram)
predict word given context (CBOW). Similar models called vector log-bilinear models
recently proposed (Mnih & Kavukcuoglu, 2013). models inspired skip-gram
CBOW GloVe (Global Vectors Word Representation) (Pennington et al., 2014),
combines local global contexts word unified model, model
relies dependency-based contexts instead simpler word-based contexts (Levy &
Goldberg, 2014a), new models steadily emerging (e.g., Lebret & Collobert, 2014;
Lu, Wang, Bansal, Gimpel, & Livescu, 2015; Stratos, Collins, & Hsu, 2015; Trask, Gilmore,
& Russell, 2015; Liu, Jiang, Wei, Ling, & Hu, 2015).
interesting finding discussed recently (Levy & Goldberg, 2014b): popular skip-gram model negative sampling (SGNS) (Goldberg & Levy, 2014) simply
model implicitly factorizes word-context matrix, cells containing pointwise
mutual information (PMI) scores respective word context pairs, shifted
global constant. words, SGNS performs exactly thing traditional
distributional models (i.e., context counting plus context weighting and/or dimensionality
reduction), slight improvement performance SGNS (Baroni et al., 2014; Levy
et al., 2015).
low-dimensional vectors, besides improving computational efficiency, lead
better generalizations, even allowing generalize vocabularies observed labelled
data, hence partially alleviating ubiquitous problem data sparsity. utility
validated proven various semantic tasks semantic word similarity,
synonymy detection word analogy solving (Mikolov et al., 2013d; Baroni et al., 2014;
Pennington et al., 2014). Moreover, word embeddings proven serve useful
956

fiBilingual Distributed Word Representations Document-Aligned Data

unsupervised features plenty downstream NLP tasks named entity recognition,
chunking, semantic role labeling, part-of-speech tagging, parsing, selectional preferences
(Turian, Ratinov, & Bengio, 2010; Collobert et al., 2011; Chen & Manning, 2014).
Due simplicity, well efficacy consequent popularity various tasks
(Mikolov et al., 2013c; Levy & Goldberg, 2014b), clear advantage similarity tasks
compared traditional models distributional semantics (Levy et al., 2015)
article focus adaptation SGNS (Mikolov et al., 2013c). Section 3,
provide brief overview model, follow new bilingual
model based SGNS.
2.2 Bilingual Word Embeddings
Bilingual word representations could serve useful source knowledge problems
cross-lingual information retrieval (Levow, Oard, & Resnik, 2005; Vulic, De Smet, & Moens,
2013), statistical machine translation (Wu, Wang, & Zong, 2008), document classification
(Ni, Sun, Hu, & Chen, 2011; Klementiev et al., 2012; Hermann & Blunsom, 2014b; Chandar, Lauly, Larochelle, Khapra, Ravindran, Raykar, & Saha, 2014; Vulic, De Smet, Tang,
& Moens, 2015), bilingual lexicon extraction (Tamura, Watanabe, & Sumita, 2012; Vulic
& Moens, 2013a), knowledge transfer annotation projection resource-rich
resource-poor languages myriad NLP tasks dependency parsing, POS tagging, semantic role labeling selectional preferences (Yarowsky & Ngai, 2001; Pado &
Lapata, 2009; Peirsman & Pado, 2010; Das & Petrov, 2011; Tackstrom, Das, Petrov, McDonald, & Nivre, 2013; Ganchev & Das, 2013; Tiedemann, Agic, & Nivre, 2014; Xiao
& Guo, 2014). interesting application domains machine translation (e.g., Zou,
Socher, Cer, & Manning, 2013; Wu, Dong, Hu, Yu, He, Wu, Wang, & Liu, 2014; Zhang, Liu,
Li, Zhou, & Zong, 2014) cross-lingual information retrieval (e.g., Vulic & Moens, 2015).
Moreover, making transition monolingual bilingual settings building
shared bilingual embedding space (see Figure 1 illustrative example), one able
extend rather generalize semantic tasks semantic similarity computation, synonymy detection word analogy computation across languages. Following success
monolingual settings, body recent work word representation learning therefore
focused learning bilingual word embeddings (BWEs).
current research inducing BWEs critically relies sentence-aligned parallel
data readily available bilingual lexicons achieve coherence representations across
languages (e.g., build similar representations similar concepts different languages
January-januari, dog-hund sky-hemel). may cluster current work three
different groups: (1) models rely hard word alignments obtained parallel
data constrain learning BWEs (Klementiev et al., 2012; Zou et al., 2013; Wu et al.,
2014); (2) models use alignment parallel data sentence level (Kocisky,
Hermann, & Blunsom, 2014; Hermann & Blunsom, 2014a, 2014b; Chandar et al., 2014; Shi,
Liu, Liu, & Sun, 2015; Gouws et al., 2015); (3) models critically require readily
available bilingual lexicons (Mikolov et al., 2013b; Faruqui & Dyer, 2014; Xiao & Guo,
2014). main disadvantage models limited availability parallel data
bilingual lexicons, resources scarce and/or domain-restricted plenty
language pairs. work, significantly alleviate requirements: unlike prior work,
957

fiVulic & Moens

show BWEs may induced solely basis document-aligned comparable
data without additional need parallel data bilingual lexicons. Note (in
theory) work Hermann Blunsom (2014b), Chandar et al. (2014) may also
extended setting document-aligned data, two models originally
rely sentence embeddings computed aggregations single word embeddings
plus sentence alignments. work, testing comparing BiCVM model
Hermann Blunsom, show models work well practice
replacing strong bilingual signal coded parallel sentences noisy bilingual
signal given document alignments non-parallel data.
2.3 Bilingual Word Representations Document-Aligned Data
Prior work inducing bilingual word representations early days followed tradition window-based context-counting distributional models (Rapp, 1999; Gaussier, Renders, Matveeva, Goutte, & Dejean, 2004; Laroche & Langlais, 2010) required
bilingual lexicon critical resource. order tackle issue, recent work relies
supervision-lighter framework multilingual probabilistic topic modeling (MuPTM)
(Mimno, Wallach, Naradowsky, Smith, & McCallum, 2009; Boyd-Graber & Blei, 2009;
De Smet & Moens, 2009; Ni, Sun, Hu, & Chen, 2009; Zhang, Mei, & Zhai, 2010; Fukumasu,
Eguchi, & Xing, 2012) similar models latent structure induction (Haghighi,
Liang, Berg-Kirkpatrick, & Klein, 2008; Daume III & Jagarlamudi, 2011).
Words setting represented real-valued vectors conditional topic probability scores P (zk |wi ), regardless actual language. Topics zk fact latent
inter-lingual concepts discovered directly multilingual comparable data using multilingual topic model bilingual LDA. discuss MuPTM-based representations
detail Section 4.1.
MuPTM-based bilingual word representations induced comparable data demonstrated utility tasks cross-lingual semantic similarity computation bilingual
lexicon extraction (Vulic, De Smet, & Moens, 2011; Liu, Duh, & Matsumoto, 2013)
suggesting word translations context (Vulic & Moens, 2014). work, compare
state-of-the-art MuPTM-based word representations induced type
comparable corpora BWEs learned new model two semantic tasks.
Another recent model (Sgaard, Agic, Martnez Alonso, Plank, Bohnet, & Johannsen,
2015) also able learn document-aligned data. count-based model
builds binary word vectors denoting occurrence word document pair.
Dimensionality reduction applied post-hoc induced sparse vectors. Since
links documents known, model able learn cross-lingual correspondences
words and, consequently, bilingual word representations. Exactly idea
already introduced baseline model Vulic et al. (2011), TF-IDF weights
used instead binary indices, dimensionality reduction applied post-hoc.
model Vulic et al. surpassed baseline models document-aligned data
briefly discussed Section 4.1, model Sgaard et al. obtains results
similar BWE baselines compared work (described Section 4.2).
958

fiBilingual Distributed Word Representations Document-Aligned Data

3. BWESG: Model Architecture
new bilingual model extension SGNS bilingual settings documentaligned comparable training data. section describes underlying SGNS two
variants SGNS-based BWE induction model.
3.1 Skip-Gram Negative Sampling (SGNS)
departure point log-linear SGNS Mikolov et al. (2013c) implemented
word2vec package.1 SGNS model learns word embeddings (WEs) similar way
neural language models (Bengio et al., 2003; Collobert & Weston, 2008), without
non-linear hidden layer.
monolingual setting, assume one language L vocabulary V , corpus
words w V , along contexts c V c , V c context vocabulary.
Contexts word wn typically neighboring words context window size cs
(i.e., wncs , . . . , wn1 , wn+1 , . . . , wn+cs ), effectively holds V c V .2
word type w V associated vector w
~ Rd (its pivot word representation
pivot word embedding, see Figure 2), vector w~c Rd (its context embedding).
dimensionality vectors, which, model input parameter,
set advance training procedure commences. entries vectors
latent, treated parameters learned model. short, idea
skip-gram model scan corpus (which typically unannotated, Mikolov
et al., 2013a) word word turn (i.e., pivot words), learn pairs
(word, context word). learning goal maximize ability predicting context
words pivot word corpus. Let ob = 1 denote pair words (w, v)
observed corpus thus belongs training set D. probability (w, v)
defined softmax function:
P (ob = 1|w, v, ) =

1
1 + exp(w
~ v~c )

(1)

word token w corpus treated turn pivot pairs word tokens
(w, w 1),...,(w, w t(cs)) appended D, t(cs) integer sampled
uniform distribution {1, . . . , cs}.3 global training objective J maximize
probabilities pairs indeed observed corpus:
J = arg max


X

log

(w,v)D

1
1 + exp(w
~ v~c )

(2)

parameters model, is, pivot context word embeddings
learned. One may see objective function trivial solution setting
1. https://code.google.com/p/word2vec/
2. Testing options context selection dependency-based contexts (Levy & Goldberg, 2014a)
beyond scope work, shown contexts may lead gains
final WEs (Kiela & Bottou, 2014).
3. original skip-gram model utilizes dynamic window sizes, cs denotes maximum window
size. Moreover, model takes account sentence boundaries context selection, is, selects
context words words occurring sentence pivot word.

959

fiVulic & Moens

w
~ = v~c , w
~ v~c = V al, V al large enough number (Goldberg & Levy, 2014).
order prevent trivial training scenario, negative sampling procedure comes
picture (Collobert & Weston, 2008; Mikolov et al., 2013c).
short, idea behind negative sampling present model set D0
artificially created sampled negative pivot-context word pairs (w, v 0 ),
assumption serve negative examples, is, occur observed/positive
(word, context) pairs training corpus. model adjust parameters
way also maximize probability negative pairs occur
corpus. interested reader may find details negative sampling
procedure, new exact objective function along derivation elsewhere (Levy &
Goldberg, 2014b), illustrative purposes simplicity, present approximative
objective function negative sampling Goldberg Levy:
X
X
1
1
J = arg max
log
(3)
+
log

1 + exp(w
~ v~c )
1 + exp(w
~ v~c0 )
(w,v)D
(w,v 0 )D0
free parameters updated using stochastic gradient descent backpropagation,
learning rate typically controlled Adagrad (Duchi, Hazan, & Singer, 2011)
global linearly decreasing learning rate. optimizing objective eq. (3),
model incrementally pushes observed pivot WEs towards context WEs collocates
corpus. words distributional hypothesis - training, words occur
similar contexts end similar word embeddings. words, link
terminology distributional hypothesis modeling assumptions SGNS - words
predict similar contexts end similar word embeddings.
3.2 Final Model - BWESG: BWE Skip-Gram
next step, propose novel method extends SGNS work bilingual
document-aligned comparable data. Let us assume possess document-aligned
comparable corpus, defined C = {d1 , d2 , . . . , dN } = {(dS1 , dT1 ), (dS2 , dT2 ), . . . , (dSN , dTN )}.
dj = (dSj , dTj ) denotes pair aligned documents source language LS target
language LT respectively, N number pairs corpus. V V
vocabularies associated languages LS LT . goal learn shared bilingual
embedding space given data (Figure 1) document alignments bilingual
signal training. present two strategies that, coupled SGNS, lead
shared bilingual spaces. overview architecture learning BWEs documentaligned comparable data two strategies given Figures 2(a) 2(b).
3.2.1 Merge Shuffle
first step, merge two documents dSj dTj aligned document pair dj
single pseudo-bilingual document d0j . Following that, randomly shuffle
newly constructed pseudo-bilingual document. shuffle (random) permutation
word tokens given two different languages forming pseudo-bilingual document.
pre-training shuffling step (see Figure 2(a)) assures word w, regardless
actual language, obtains word collocates vocabularies. idea obtaining
bilingual contexts pivot word pseudo-bilingual document steer
960

fiBilingual Distributed Word Representations Document-Aligned Data

(a) Merge Shuffle

(b) Length-Ratio Shuffle

Figure 2: architecture BWE Skip-Gram (BWESG) model learning bilingual
word embeddings document-aligned comparable data two different pretraining strategies: (1) non-deterministic merge shuffle, (2) deterministic
length-ratio shuffle. Source language words documents drawn gray
boxes, target language words documents drawn blue boxes.
right side figures (separated vertical dashed lines) illustrates
pseudo-bilingual document constructed pair two aligned documents.

final model towards constructing shared bilingual space. Since model depends
alignment document level, order ensure bilingual contexts instead
monolingual contexts, intuitive assume larger window sizes lead better
bilingual embeddings. test hypothesis effect window size Section 7.3.
another interpretation, since model relies (pseudo-bilingual) document level
co-occurrence, window size parameter controls amount random data
dropout, is, number positive document-level training examples. locality
feature SGNS preserved due shuffling procedure.
961

fiVulic & Moens

3.2.2 Length-Ratio Shuffle
non-deterministic uncontrollable nature merge shuffle procedure opens
possibility accidentally obtaining bad shuffles result sub-optimal word
representations. Therefore, also propose deterministic strategy building pseudobilingual documents suitable bilingual training. Source target language words
inserted (initially empty) pseudo-bilingual document turn based ratio
document lengths, word order preserved. Document lengths measured terms
word tokens, let us denote mS mT aligned document pair (dSj , dTj ).
Let us assume, without loss generality, mS mT . procedure proceeds
follows (if mT > mS procedure proceeds analogous manner roles dSj
dTj reversed):
1. Pseudo-bilingual document d0j empty: d0j = {}.
mS
c.
2. Compute ratio: R = b

3. Scan aligned documents dS dT simultaneously (3.1) append R word
tokens dSj d0j ; (3.2) append 1 word token dTj . Repeat steps 3.1
3.2 word tokens dTj inserted d0j .
4. Insert remaining mS mod mT word tokens dSj d0j .
Using simple example, assume English (EN) document {F rodo, Sam, orcs,
goblins, ordor, ring} Spanish (ES) document {anillo, orcos, mago}: pseudobilingual document would formed inserting 1 Spanish word 2 English words (as
length ratio 6:3 = 2:1). final pseudo-bilingual document is:
{F rodoEN , SamEN , anilloES , orcsEN , goblinsEN , orcosES , ordorEN , ringEN , magoES }.

another interpretation, length-ratio shuffle strategy constructs single permutation/shuffle pseudo-bilingual document controlled word order two aligned
documents well length ratio. before, model relies pseudo-bilingual
document level co-occurrence, window size parameter controls amount (now
non-random) data dropout. difference lies fact procedure keeps word
order intact monolingually constructing pseudo-bilingual document.
final BWE Skip-gram (BWESG) model relies monolingual variant
SGNS (or monolingual induction model) trained shuffled/permuted
pseudo-bilingual documents (using proposed strategies).4 model learns
word embeddings source target language words aligned shared embedding
dimensions. BWESG-based representation word w, regardless actual language,
d-dimensional vector: w
~ = [f1 , . . . , fk , . . . , fd ]. fk R denotes value kth shared inter-lingual feature within d-dimensional shared bilingual embedding space.
Since words share embedding space, semantic similarity words may
computed monolingually across languages. extensively use property
evaluation tasks.
4. also experimenting GloVe CBOW, falling short SGNS average.

962

fiBilingual Distributed Word Representations Document-Aligned Data

4. Baseline Representation Models
quickly navigate approaches bilingual word representation learning
document-aligned comparable data. set models comparison may roughly
clustered two main groups: (Group I) pre-BWE baseline representation models
document-aligned data, (Group II) benchmarking BWE induction models
originally developed learning document-aligned comparable data. essential compare BWESG model frameworks learning representations
document-aligned data (Group I), also crucial detect main strengths
BWESG model compared approaches BWE learning framework
also adjusted learn document-aligned data (Group II).
4.1 Group I: Baseline Representation Models Document-Aligned Data
briefly describe three benchmarking Group models.
4.1.1 Basic-MuPTM
early approaches (e.g., Dumais, Landauer, & Littman, 1996; Carbonell, Yang, Frederking, Brown, Geng, Lee, Frederking, E, Geng, & Yang, 1997) tried mine topical structure
document-aligned comparable texts using monolingual topic model (e.g., LSA
LDA) trained pseudo-bilingual documents target document simply appended
source language counterpart, used discovered latent topical structure
shared semantic space words documents two languages may
represented uniform way.
recent work multilingual probabilistic topic modeling (MuPTM) (Mimno et al.,
2009; De Smet & Moens, 2009; Vulic et al., 2011) showed word representations higher
quality may built multilingual topic model bilingual LDA (BiLDA) trained
jointly document-aligned comparable corpora retaining structure corpus
intact (i.e., need construct pseudo-bilingual documents).
MuPTM discovers latent structure observed data form K latent
cross-lingual topics z1 , . . . , zK optimally describe generation observed data.
Extracting latent cross-lingual topics actually implies learning per-document topic distributions document corpus (probability scores P (zk |dj )), discovering
language-specific representations topics given per-topic word distributions
language (probability scores P (wiS |zk ) P (wiT |zk )). Latent cross-lingual topics
fact distributions vocabulary words, language-specific representation
language. Per-document topic distributions per-topic word distributions
obtained training topic model multilingual data. representation
word w V (or analogous manner w V ) K-dimensional vector:
w
~ = [P (z1 |w), . . . , P (zk |w), . . . , P (zK |w)].
call representation model (RM) Basic-MuPTM (BMu). Since number
topics, is, number vector dimensions K typically high (Dinu & Lapata, 2010;
Vulic et al., 2011), additional feature pruning (Reisinger & Mooney, 2010) may employed
order retain descriptive dimensions MuPTM-based representation,
963

fiVulic & Moens

shown improve performance several semantic tasks (e.g., BLE
SWTC) (Vulic & Moens, 2013a; Vulic et al., 2015).
multilingual topic model typically trained Gibbs sampling (Geman & Geman,
1984; Steyvers & Griffiths, 2007; Vulic et al., 2015). Similar SGNS/BWESG training
procedure, Gibbs sampling MuPTM/BiLDA also scans training corpus word
word, cyclically updates topic assignments word token. However, unlike
BWESG uses subset document-level training examples, Gibbs sampling
MuPTM uses words source language document well words
coupled target language document influence topic assignment pivot word.
BWESG design relying data dropout leads decreased training times computation
costs obtain final representations compared Basic-MuPTM.
4.1.2 Association-MuPTM
Another representation also based MuPTM framework: contains association
scores P (wa |w) w, wa V V (Vulic & Moens, 2013a) dimensions
realP
P
(w
valued word vectors. association scores computed P (wa |w) = K
|zk )
k=1

P (zk |w) (Griffiths, Steyvers, & Tenenbaum, 2007), word vector (|V | + |V |)S |w), P (w |w), . . . , P (w
dimensional vector: w
~ = [P (w1S |w), . . . , P (w|V
|w)].
S|
1
|V |
Basic-MuPTM, original word representation may also pruned post-hoc. call
representation model Association-MuPTM (AMu). Since approach relies
MuPTM training plus additional |V | |V | computations estimate association scores,
cost obtaining Association-MuPTM representations even higher BasicMuPTM, leads robust word representations BLE task (Vulic & Moens,
2013a). Basic-MuPTM Association-MuPTM produce high-dimensional
real-valued vectors plenty near-zero dimensions (the number dimensions typically measured thousands) pruned afterwards pruning parameter often set ad-hoc, BWESG produces lower-dimensional dense real-valued vectors,
additional post-hoc feature pruning required BWESG.
4.1.3 Traditional-PPMI
traditional approach building bilingual word representations (cross-lingual) distributional semantics compute weighted co-occurrence scores (e.g., using PMI, TF-IDF)
pivot words context words window predefined size, plus external bilingual lexicon align context words/dimensions across languages (Gaussier et al.,
2004; Laroche & Langlais, 2010). weighting function (WeF), standard choice
distributional semantics yields optimal near-optimal results group semantic
tasks (Bullinaria & Levy, 2007), smoothed positive pointwise mutual information
statistic (Pantel & Lin, 2002; Turney & Pantel, 2010). Furthermore, order induce context words without need readily available lexicon, employ bootstrapping
procedure Peirsman Pado (2011), Vulic Moens (2013b). representation
model called Traditional-PPMI (TPPMI). word representation R-dimensional
vector: w
~ = [sc1 (w, c1 ), . . . , sck (w, ck ), . . . , scK (w, cK )]. dimensions vector space
K one-to-one word translation pairs ck = (cSk , cTk ), sck (w, ck ) weighted co964

fiBilingual Distributed Word Representations Document-Aligned Data

occurrence score pivot word w k-th context feature, one computes
co-occurrence score using cSk w V , cTk w V .
Vector dimensions ck = (cSk , cTk ) Traditional-PPMI representation similar
models WeFs typically frequent reliable translation pairs
corpus. opposed BWESG, obtained word vectors high-dimensional
(typically thousands dimensions) sparse real-valued vectors. addition, traditionalPPMI purely local distributional model deriving distributional context knowledge
narrow context windows (typically 3-10 surrounding words, e.g., Laroche & Langlais, 2010).
bootstrapping approach (Vulic & Moens, 2013b) use induce TraditionalPPMI representation starts automatically learned seed lexicon one-to-one translation pairs obtained using model (e.g., Basic-MuPTM Association-MuPTM),
gradually detects new dimensions shared bilingual semantic space. refer
interested reader relevant literature (Vulic & Moens, 2013b) details.
4.2 Group II: BWE Induction Models Adjusted Document-Aligned Data
provide quick overview three representative benchmarking BWE models
learn different types bilingual monolingual data.
4.2.1 BiCVM
Hermann Blunsom (2014b) introduced model called BiCVM (Bilingual Compositional
Vector Model) learns bilingual word embeddings sentence-aligned parallel corpus
C = {s1 , s2 , . . . , sN } = {(sS1 , sT1 ), (sS2 , sT2 ), . . . , (sSN , sTN )}.5 sj = (sSj , sTj ) denotes pair
aligned sentences. model assumes aligned sentences meaning,
implies sentence representations similar. Assume two functions f
g map sentences given source language respectively semantic
representations Rd , representation dimensionality. energy
model given two sentences (sSj , sTj ) C defined as: E(sSj , sTj ) = ||f (sSj ) g(sTj )||.
goal minimize E semantically equivalent sentences (i.e., aligned sentences)
corpus. order prevent model degenerating, use noise-contrastive
large-margin update ensures representations non-aligned sentences observe
certain margin other. every pair parallel sentences (sSj , sTj ), sample
number additional negative sentence pairs (sSj , nTneg ) corpus (i.e., sampled
pairs observed positive pairs C). noise samples used formulating
hinge loss follows: E(sSj , sTj ) = max(mrg + E(sSj , sTj , nTneg ), 0), mrg
margin, E(sSj , sTj , nTneg ) = E(sSj , sTj ) E(sSj , nTneg ). loss minimized every
pair parallel sentences corpus L2-regularization model parameters.
number noise samples per positive pair hyper-parameter model.
semantic signal propagated aligned sentences back individual words obtain
bilingual word embeddings. BiCVM model originally built sentencealigned parallel data, exactly idea may applied document-aligned non-parallel
data. paper, test ability learn noisier comparable data. BWESG
5. similar (but expensive) model also learns parallel sentence-aligned data
also introduced Chandar et al. (2014).

965

fiVulic & Moens

model compared BiCVM inducing BWEs data types: comparable
parallel.
4.2.2 Mikolovs Mapping
Another collection BWE induction models (Mikolov et al., 2013b; Faruqui & Dyer, 2014;
Dinu, Lazaridou, & Baroni, 2015; Lazaridou, Dinu, & Baroni, 2015) assumes following
setup: first, two monolingual embedding spaces, RdimS RdimT , induced separately
two languages using standard monolingual model SGNS (Mikolov
et al., 2013a, 2013c). dimS dimT denote dimensionality monolingual embedding
spaces source target language respectively. bilingual signal provided
form word translation pairs (xi , yi ), xi V , yi V , x~i RdimS , y~i RdimT .
Training cast multivariate regression problem: implies learning function
maps source language vectors training data corresponding target
language vectors. standard approach (Mikolov et al., 2013b; Dinu et al., 2015)
assume linear map W RdimS dimT , L2 -regularized least-squares error objective
(i.e., ridge regression) used learn map W: learned solving following
optimization problem (typically stochastic gradient descent):
minWRdimS dimT ||XW Y||2F + ||W||2F .
X matrices obtained respective concatenation source language
target language vectors training pairs. linear map W estimated,
previously unseen source language word vector x~u may straightforwardly mapped
target language embedding space RdimT Wx~u . mapping vectors ~x, x V ,
target embedding space RdimT fact serves bilingual embedding space (Figure 1).
Although main strength model ability learn embeddings larger
monolingual training sets, model may also adjusted setting
training data document-aligned comparable data follows: (1) Automatically learn
seed lexicon reliable one-to-one translation pairs document-aligned data using
bootstrapping approach Vulic Moens (2013b), (2) Train two separate monolingual
embedding spaces two separated halves document-aligned data set (i.e., using
source language documents target language documents), (3) Learn mapping
two spaces using pairs Step 1.
4.2.3 BilBOWA
Another collection BWE induction models jointly optimizes two monolingual objectives,
cross-lingual objective acting cross-lingual regularizer training (Klementiev et al., 2012; Gouws et al., 2015; Soyer, Stenetorp, & Aizawa, 2015). idea
behind joint training may summarized simplified formulation (Luong, Pham, &
Manning, 2015): (MonoS + MonoT ) + Bi.
monolingual objectives onoS onoT ensure similar words language assigned similar embeddings aim capture semantic structure
language, whereas cross-lingual objective Bi ensures similar words across languages assigned similar embeddings, ties two monolingual spaces together
bilingual space. Parameters govern influence monolingual bilingual
966

fiBilingual Distributed Word Representations Document-Aligned Data

components.6 bilingual signal models, acting cross-lingual regularizer joint training, provided sentence-aligned parallel data. Although
use data sources, models differ choice monolingual cross-lingual
objectives. work, opt BilBOWA model Gouws et al. (2015) representative model included comparisons, due previous solid performance
robustness BLE task, reduced complexity reflected fast computations
massive datasets, well public availability. short, BilBOWA model combines
SGNS monolingual objectives together cross-lingual objective minimizes L2 -loss bag-of-word vectors parallel sentences. details
exact training procedure, refer interested reader Gouws et al.s
work.
Again, although main strength model ability learn embeddings
larger monolingual training sets, model may also adjusted setting
document- sentence-aligned data by: (1) using two halves aligned corpus separate monolingual training, (2) using alignment signal bilingual training.

5. Word Representations Semantic Word Similarity
Assume induced bilingual word representations, regardless chosen
RM. Given two words wi wj , irrespective actual language, may compute
degree semantic similarity applying similarity function (SF) vector

: sim(w , w ) = SF (
,

representations
w
w
w

j

j
wj ). Different choices (or rather families
of) SFs cosine, Kullback-Leibler Jensen-Shannon divergence, Hellinger
distance, Jaccard index, etc. (Lee, 1999; Cha, 2007), different RMs typically require
different SFs produce optimal near-optimal results various semantic tasks.
working word embeddings, standard choice SF cosine similarity (cos) (Mikolov
et al., 2013c), also typical choice traditional distributional models (Bullinaria
& Levy, 2007). similarity computed follows:



w
wj
sim(wi , wj ) = cos(wi , wj ) =
| |
|
|w
w

j

(4)

hand, good choice SF working probabilistic RMs
Basic-MuPTM Association-MuPTM RS Hellinger distance (Pollard, 2001; Cha,
2007; Kazama, Saeger, Kuroda, Murata, & Torisawa, 2010), displays excellent results
BLE task (Vulic & Moens, 2013a). similarity words wi wj using
Hellinger distance computed follows:
v
u K q
q
2
X
1 u
P (fk0 |wi ) P (fk0 |wj )
(5)
sim(wi , wj ) =
2 i=1
Note Hellinger distance applicable word representations probability
distributions, case Basic-MuPTM Association-MuPTM. P (fk0 |wi ) de6. Setting = 0 reduces model setting similar BiCVM (Hermann & Blunsom, 2014b). = 1
results models Klementiev et al. (2012), Gouws et al. (2015), Soyer et al. (2015).

967

fiVulic & Moens

notes probability score k-th dimension (fk0 ) vector representation
Basic-MuPTM Association-MuPTM.7
word wi , build ranked list RL(wi ) consists words wj
ranked according respective semantic similarity scores sim(wi , wj ). Additionally,
label ranked list RL(wi ) pruned position RLM (wi ). Since may
retain language labels words training multilingual settings (e.g., language labels
marked different colors Figure 2), may compute: (1) monolingual similarity,
e.g., given wi V , retain wj V ranked list (analogous wi V ),
(2) cross-lingual similarity (CLSS), e.g., given wi V , retain wj V , (3)
multilingual similarity, retain words wj V V . computing CLSS
wi , similar word cross-lingually called cross-lingual nearest neighbor.
employ models context-insensitive CLSS word type level extract bilingual lexicons document-aligned sentence-aligned data, compare
representation models BLE task Section 7.
5.1 Context Sensitive Models (Cross-Lingual) Semantic Similarity
context-insensitive models semantic similarity provide ranked lists semantically
similar words invariably isolation, operate level word types.
explicitly encode different word senses. practice, means that, given sentence
coach team satisfied game yesterday., context-insensitive
CLSS models able detect Spanish word entrenador similar
polysemous English word coach context sentence Spanish word
autocar, although autocar listed semantically similar word coach globally/invariably without observed context. another example, Spanish words
partido, encuentro, cerilla correspondencia highly similar another ambiguous
English word match observed isolation, given Spanish sentence unable find match pocket light cigarette., clear strength
cross-lingual semantic similarity change context cerilla exhibits strong
cross-lingual semantic similarity match within particular sentential context.
goal build BWE-based models cross-lingual semantic similarity
context, similar context-aware CLSS models proposed Vulic Moens (2014). Two
key questions are: (i) provide BWE-based representations beyond word level
represent context word token?; (ii) use contextual knowledge
context-sensitive model semantic similarity?
Following Vulic Moens (2014), given word token w context (e.g., window
words, sentence, paragraph, document), build context set rather context
bag Con(w) = {cw1 , . . . , cwr } harvesting r neighboring words chosen context scope
(e.g., context bag may comprise content-bearing words sentence
pivot word token, so-called sentential context). order present context Con(w)
d-dimensional embedding space, need apply model semantic composition

learn d-dimensional vector representation Con(w).
7. Prior work shown results Basic-MuPTM Association-MuPTM slightly higher
cosine replaced Hellinger distance. Therefore, particular case opted
Hellinger distance report competitive baseline.

968

fiBilingual Distributed Word Representations Document-Aligned Data

Formally, given word w, may specify vector representation context bag
Con(w) d-dimensional vector/embedding:


Con(w) = cw1 ? cw2 ? . . . ?
cw
r

(6)

, . . . ,
d-dimensional WEs learned data, ? compositional

cw
cw
1
r
vector operator addition, point-wise multiplication, tensor product, etc.
plethora models semantic composition proposed relevant literature, differing choice vector operators, input structures required knowledge
(Mitchell & Lapata, 2008; Baroni & Zamparelli, 2010; Rudolph & Giesbrecht, 2010; Socher,
Huval, Manning, & Ng, 2012; Blacoe & Lapata, 2012; Clarke, 2012; Hermann & Blunsom,
2014b; Milajevs, Kartsaklis, Sadrzadeh, & Purver, 2014), name few. work,
driven observed linear linguistic regularities embedding spaces (Mikolov et al.,
2013d), opt simple addition (denoted +) Mitchell Lapata (2008)
compositional operator, due simplicity, ease applicability bag-of-words
contexts, relatively solid performance various compositional tasks (Mitchell &

Lapata, 2008; Milajevs et al., 2014). d-dimensional embedding Con(w) then:


Con(w) = cw1 + cw2 + . . . +
cw
r

(7)

use BWE-based RM, may compute context-sensitive semantic similarity score sim(wi , tj , Con(wi )) tj wi given context Con(wi ) shared
bilingual embedding space follows:



sim(wi , tj , Con(wi )) = SF (wi0 , tj )

(8)





tj V target language word, tj word representation, wi0 new
contextualized vector representation wi modulated context Con(wi ), is,
context-aware representation. Vulic Moens (2014) introduced linear interpolation
two d-dimensional vectors plausible solution modulation/contextualization.
modulation representation wi computed follows:



+
wi0 = (1 )
w
Con(wi )


(9)


word embedding w computed word type level,

w
Con(wi )


embedding context bag computed using eq. (7), interpolation parameter.
Another set similar models yield context-sensitive similarity computations
proposed recently, displayed competitive results regardless
simplicity (Melamud, Levy, & Dagan, 2015). Here, present two best scoring contextsensitive models adapt bilingual setting:
P
SF (wi , tj ) + cwi Con(wi ) SF (cwi , tj )
Add-Melamud: sim(wi , tj , Con(wi )) =
|Con(wi )| + 1


Mult-Melamud: sim(wi , tj , Con(wi )) = |Con(wi )|+1 SF (wi , tj )
SF (cwi , tj )
cwi Con(wi )

969

fiVulic & Moens

Note Mult model one avoid negative values, simple shift allpositives interval required, e.g., shifted cosine score becomes cos0 (x, y) = cos(x,y)+1
.
2
Unlike models Vulic Moens, two models aggregate single word
representations one vector represents context, compute similarity scores
separately word context. details regarding models,
refer interested reader original Melamud et al.s work .
employ models context-sensitive CLSS word token level compare
representation models task suggesting word translations context Section 8.

6. Training Setup
section, provide insight training data experimental setup
BWESG model baseline models.
6.1 Training Data
induce bilingual word embeddings well directly comparable baseline representations prior work, use dataset comprising subset comparable Wikipedia data available three language pairs (Vulic & Moens, 2013b, 2014)8 : (i)
collection 13, 696 Spanish-English Wikipedia article pairs (ES-EN), (ii) collection
18, 898 Italian-English Wikipedia article pairs (IT-EN), (iii) collection 7, 612
Dutch-English Wikipedia article pairs (NL-EN). corpora theme-aligned comparable corpora, is, aligned document pairs discuss similar themes, general
direct translations other. directly comparable prior work two
evaluation tasks (Vulic & Moens, 2013b, 2014), retain nouns occur least
5 times corpus. Lemmatized word forms recorded available, original
forms otherwise. TreeTagger (Schmid, 1994) used POS tagging lemmatization.
preprocessing steps vocabularies comprise 7,000 13,000 noun types
language language pair, training corpora quite small: ranging
approximately 1.5M tokens NL-EN 4M ES-EN. Exactly training
data vocabularies used train representation models comparison (both
Group Group II, see Section 4).
also demonstrate simple straightforward train BWESG parallel sentence-aligned data using modeling principles. purpose, use
Europarl.v7 (Koehn, 2005) three language pairs obtained OPUS website
(Tiedemann, 2012).9 preprocessing step, retain words occurring
least 5 times corpus. corpus contains approximately 2M parallel sentences,
vocabularies order magnitude larger smaller Wikipedia data
(i.e., varying 45K EN word types 75K NL word types), corpora sizes
approximately 120M tokens. Data statistics two data sources, Wikipedia vs Europarl,
provided Table 1. statistics reveal different nature two corpora,
significantly variance noise reported Wikipedia data.
8. Available online: people.cs.kuleuven.be/~ivan.vulic/software/
9. http://opus.lingfil.uu.se/

970

fiBilingual Distributed Word Representations Document-Aligned Data

Corpus:

Wikipedia

Europarl

Pair:

ES-EN

IT-EN

NL-EN

ES-EN

IT-EN

NL-EN

Average length (OTHER)
Average length (EN)
Average length difference

111
174
127

84
154
125

51
129
102

29
28
3

29
29
4

27
27
4

Table 1: Training data statistics: Non-parallel document-aligned Wikipedia vs parallel
sentence-aligned Europarl three language pairs. = ES, NL.
Lengths measured word tokens. Averages rounded closest integer.
6.2 Trained BWESG Models
test effect random shuffling merge shuffle BWESG strategy,
trained BWESG model 10 random corpora shuffles three training corpora.
also train BWESG length-ratio shuffle strategy. parameters set
default suggested parameters SGNS word2vec package: stochastic gradient
descent (SGD) linearly decreasing global learning rate 0.025, 25 negative samples,
subsampling rate 1e 4, 15 epochs.
varied number dimensions = 100, 200, 300. also trained
BWESG = 40 directly comparable readily available sets BWEs
prior work (Chandar et al., 2014). Moreover, test effect window size final
results, i.e., number positives used training, varied maximum window
size cs 4 60 steps 4.10
make pre-training training code BWESG publicly available, along
BWESG-based bilingual word embeddings three language pairs at:
http://liir.cs.kuleuven.be/software.php.
6.3 Baseline Representations: Group
parameters baseline representation models (i.e., topic models settings,
number dimensions K, values feature pruning, window size, weighting
similarity functions) optimized prior work. Therefore, settings adopted
directly previous work (Griffiths et al., 2007; Bullinaria & Levy, 2007; Dinu & Lapata,
2010; Vulic & Moens, 2013a, 2013b; Kiela & Clark, 2014), encourage interested
reader check details exact parameter setup relevant literature. provide
short overview here.
Basic-MuPTM Association-MuPTM, work Vulic Moens (2013a),
bilingual latent Dirichlet allocation (BiLDA) model trained K = 2000 topics
standard values hyper-parameters: = 50/K, = 0.01 (Steyvers & Griffiths,
2007). Post-hoc semantic space pruning employed pruning parameter set
200 Basic-MuPTM 2000 Association-MuPTM. refer reader
relevant paper details.
Traditional-PPMI, work Vulic Moens (2013b), seed lexicon
automatically obtained bootstrapping initial seed lexicon reliable pairs stem10. remind reader slightly abuse terminology here, BWESG windows include
locality component more.

971

fiVulic & Moens

ming Association-MuPTM model (with parameters AssociationMuPTM listed above). window size fixed 6 directions.
refer reader paper details.
6.4 Baseline Representations: Group II
baseline BWE models trained number dimensions BWESG:
= 100, 200, 300. model-specific parameters taken suggested prior work.
BiCVM, use tool released authors.11 train additive model,
hinge loss margin mrg = original paper, batch size 50, noise parameter
10. models trained 200 iterations.
Mikolov, train two monolingual SGNS models using original word2vec
package, SGD global learning rate 0.025, 25 negative samples, subsampling rate
1e 4, 15 epochs. seed lexicon required learn mapping two monolingual spaces exactly Traditional-PPMI.
BilBOWA, use SGD global learning rate 0.15 training12 , 25 negative
samples, subsampling rate 1e 4, 15 epochs. BilBOWA Mikolov, vary
window size way BWESG.
6.5 Similarity Functions
Unless stated otherwise, similarity function used similarity computations
RMs cosine (cos). exceptions Basic-MuPTM Association-MuPTM
Hellinger distance (HD) used since consistently outperformed cosine
two RM types prior work (see Footnote 7).
6.6 Roadmap Experiments
first experiment, quickly visually inspect obtained lists semantically similar words using BWESG bilingual representation model. Following that, compare
BWESG-based models bilingual lexicon extraction (BLE) suggesting word translations context (SWTC) groups baseline models discussed Section 4.
experiments results BLE task presented Section 7, experiments
results SWTC presented Section 8.

7. Evaluation Task I: Bilingual Lexicon Extraction
One may employ context-insensitive CLSS models Section 5 extract bilingual
lexicons automatically data.

11. https://github.com/karlmoritz/bicvm
12. Suggestions parameter values received personal correspondence authors. software available online: https://github.com/gouwsmeister/bilbowa

972

fiBilingual Distributed Word Representations Document-Aligned Data

Spanish-English (ES-EN)
(1)
reina

(2)
reina

(3)
reina

(Spanish)

(English)

rey
trono
monarca
heredero
matrimonio
hijo
reino
reinado
regencia
duque

queen(+)
heir
throne
king
royal
reign
succession
princess
marriage
prince

Italian-English (IT-EN)
(1)
madre

Dutch-English (NL-EN)

(2)
madre

(3)
madre

(Combined) (Italian)

(English)

(Combined) (Dutch)

queen(+)
rey
trono
heir
throne
monarca
heredero
king
matrimonio
royal

mother(+)
father
sister
wife
daughter
son
friend
childhood
family
cousin

mother(+)
padre
moglie
father
sorella
figlia
figlio
sister
fratello
wife

padre
moglie
sorella
figlia
figlio
fratello
casa
amico
marito
donna

(1)
schilder

(2)
schilder

(3)
schilder

(English)

(Combined)

kunstschilderpainter(+)
schilderij
painting
kunstenaar portrait
olieverf
artist
portret
canvas
schilderen brush
frans
cubism
nederlands art
componist poet
beeldhouwer drawing

painter(+)
kunstschilder
painting
schilderij
kunstenaar
portrait
olieverf
portret
schilderen
artist

Table 2: Example lists top 10 semantically similar words 3 language pairs obtained using BWESG (length-ratio shuffle); = 200, cs = 48; (col 1.) source
language words (ES/IT/NL) listed target language words skipped
(monolingual similarity); (2) target language words (EN) listed (crosslingual similarity); (3) words languages listed (multilingual similarity). correct one-to-one translation marked (+).

7.1 Task Description
harvesting cross-lingual nearest neighbors, one able build bilingual lexicon
one-to-one translation pairs (wiS , wjT ). test validity BWEs baseline
representations BLE task.
7.2 Experimental Setup
Test Data language pair, evaluate standard 1,000 ground truth one-to-one
translation pairs built three language pairs (ES/IT/NL-EN) Vulic Moens
(2013a, 2013b). Translation direction ES/IT/NL EN. data available online.13
Evaluation Metrics Since build one-to-one bilingual lexicon harvesting
one-to-one translation pairs, lexicon quality best reflected Acc1 score, is,
number source language (ES/IT/NL) words wiS ground truth translation pairs
top ranked word cross-lingually correct translation language
(EN) according ground truth total number ground truth translation pairs
(=1000) (Gaussier et al., 2004; Tamura et al., 2012; Vulic & Moens, 2013b). Similar trends
observed within lenient setting Acc5 Acc10 scores, omit
results clarity fact actual BLE performance best reflected Acc1 .

13. http://people.cs.kuleuven.be/ ivan.vulic/software/

973

fiVulic & Moens

Spanish-English (ES-EN)

Italian-English (IT-EN)

BWESG

BMu

AMu

TPPMI

BWESG

BMu

AMu

TPPMI

cebolla

cebolla

cebolla

cebolla

golfo

golfo

golfo

golfo

onion(+)
dish
marinade
cuisine
soup
sauce
cheese
coriander
vegetable
tortilla

dessert
salad
nut
walnut
rice
toast
porridge
paddy
tuber
potato

dessert
walnut
salad
nut
hazelnut
porridge
rice
marinade
toast
paddy

sauce
cheese
garlic
salad
chili
onion(+)
cuisine
flavor
bread
dish

gulf(+)
coast
coastline
bay
island
peninsula
settlement
shore
tourism
ferry

whale
dolphin
coast
suborder
cadmium
ferry
monsoon
fjord
isthmus
mainland

coast
isthmus
coastline
fjord
ferry
monsoon
mainland
seaside
isle
suborder

coast
sea
island
bay
lagoon
harbour
beach
shore
river
lake

Table 3: Example lists top 10 semantically similar words ES-EN IT-EN, obtained
using BWESG (length-ratio, = 200, cs = 48), three representation
models Group I. correct translation marked (+).
7.3 Results Discussion
Table 2 displays top 10 semantically similar words monolingually, across-languages
combined/multilingually one ES, NL word, Table 4 shows first set
BLE results.
7.3.1 Experiment 0: Qualitative Analysis Comparison
BWESG able find semantically coherent lists words three directions similarity (i.e., monolingual, cross-lingual, multilingual). combined (multilingual) ranked
lists, words languages represented top similar words. initial qualitative analysis already demonstrates ability BWESG induce shared bilingual
embedding space using document alignments bilingual signals.14
another brief analysis, qualitatively compare cross-lingual ranked lists acquired
BWESG three baseline CLSS/BLE models Group I. lists one
ES word one word presented Table 3. two example words, BWESG
model able rank actual correct translations nearest cross-lingual
neighbors. already symptomatic word gulf, correct translation
golfo, occur ranked list RL10 (golf o) case three baseline
models. soon quantitatively confirm initial suspicion, demonstrate
BWESG superior three baseline models BLE task.
aside, Table 3 also clearly reveals difficulty judging quality models
computing semantic similarity/relatedness solely based observed output
models. lists RL10 (cebolla) RL10 (golf o) appear significantly different across
14. also conducted small experiment solving word analogies using monolingual English embedding spaces, repeated experiment vocabulary bilingual EnglishSpanish/Italian/Dutch embedding spaces. results follow findings Faruqui Dyer (2014),
slight (and often insignificant) fluctuations SGNS vectors reported (e.g., fluctuations < 1% average experiments) moving monolingual bilingual embedding
spaces. may conclude linguistic regularities established monolingual embedding spaces
(Mikolov et al., 2013d) induced SGNS also hold bilingual embedding spaces induced BWESG.

974

fiBilingual Distributed Word Representations Document-Aligned Data

Pair:
BWESG
Merge Shuffle
cs:16,MIN
cs:16,AVG
cs:16,MAX
cs:48,MIN
cs:48,AVG
cs:48,MAX

ES-EN

IT-EN

NL-EN

d=100

d=200

d=300

d=100

d=200

d=300

d=100

d=200

d=300

0.607
0.617
0.625
0.658
0.665
0.675

0.600
0.613
0.630
0.676
0.685
0.694

0.577
0.596
0.613
0.672
0.688
0.705

0.585
0.599
0.607
0.662
0.669
0.677

0.597
0.601
0.606
0.677
0.683
0.692

0.571
0.583
0.596
0.672
0.683
0.689

0.293
0.300
0.307
0.378
0.389
0.394

0.244
0.254
0.267
0.366
0.381
0.395

0.219
0.224
0.233
0.354
0.363
0.377

d=100

d=200

d=300

d=100

d=200

d=300

d=100

d=200

d=300

0.627
0.678

0.610
0.701

0.602
0.703

0.613
0.679

0.614
0.689

0.595
0.692

0.303
0.397

0.275
0.396

0.237
0.382

BWESG
Shuffling
cs:16
cs:48

d=100

d=200

d=300

d=100

d=200

d=300

d=100

d=200

d=300

0.218
0.511

0.176
0.497

0.139
0.480

0.209
0.523

0.198
0.540

0.162
0.526

0.070
0.214

0.068
0.198

0.049
0.197

BMu
AMu
TPPMI

0.441
0.518
0.577

0.441
0.518
0.577

0.441
0.518
0.577

0.575
0.618
0.647

0.575
0.618
0.647

0.575
0.618
0.647

0.237
0.236
0.206

0.237
0.236
0.206

0.237
0.236
0.206

BWESG
Length-Ratio
cs:16
cs:48

Table 4: BLE performance terms Acc1 scores tested BLE models SpanishEnglish, Italian-English Dutch-English bilingual word representations
learned document-aligned Wikipedia data. BWESG merge
shuffle report maximum (MAX), minimum (MIN) average (AVG) scores
10 random corpora shuffles. Highest scores per column bold.
four models, yet lists contain words appear semantically related source
word. Therefore, require systematic quantitative task-oriented comparison
induced word representations.
7.3.2 Experiment I: BWESG vs Group
Table 4 shows first set results BLE task: report scores two different
BWESG strategies well BWESG model shuffle pseudo-bilingual
documents. previous best reported Acc1 scores baseline representations
training+test combination also reported table. zooming table
multiple times, summarize important findings.
BWESG vs Baseline Representations results clearly reveal superior performance BWESG model BLE relies new framework inducing
bilingual word embeddings document-aligned comparable data BLE models
relying previously used bilingual word representations type training
data. increase Acc1 scores best scoring baseline models 22.2% ES-EN,
7% IT-EN 67.5% NL-EN.
BWESG Shuffling Strategy Although BWESG strategies display results
established baselines, clear advantage length-ratio shuffle strategy,
displays solid robust performance across variety parameters three
language pairs. Another advantage strategy fact deterministic
outcome suffer sub-optimal random shuffles. summary, suggest
975

fiAcc1 scores

Vulic & Moens

0.7

0.7

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.5

0.4

0.3

0.2

0.1
0.1

= 100
= 200
= 300

0.0
4

8 12 16 20 24 28 32 36 40 44 48 52 56 60
Window size

(a) Spanish-English

0.1

= 100
= 200
= 300

0.0
4

8 12 16 20 24 28 32 36 40 44 48 52 56 60
Window size

(b) Italian-English

= 100
= 200
= 300

0.0
4

8 12 16 20 24 28 32 36 40 44 48 52 56 60
Window size

(c) Dutch-English

Figure 3: Acc1 scores BLE task BWESG length-ratio shuffle 3 language
pairs, varying values parameters cs d. Solid (red) horizontal lines
denote highest baseline Acc1 scores language pair. Thicker dotted
lines refer BWESG without shuffling.
using length-ratio shuffle strategy future work, along line opt
strategy experiments.
results also reveal shuffling universally useful, BWESG without shuffling
relies largely monolingual contexts cannot reach performance BWESG
shuffling. partial remedy problem train BWESG documentlevel training pairs (i.e., increasing window size), leads prohibitively
expensive models, nonetheless BWESG without shuffling larger cs-s still falls
short BWESG shuffling strategies (see also Figures 3(a)-3(c)).
Window Size: Number Training Pairs results confirm intuition larger
window sizes, i.e., training examples lead better results BLE task.
embedding dimensions d-s, BWESG exhibits superior performance cs = 48
cs = 16, performance cs = 48 cs = 60 seems relatively stable: intuitively,
training pairs leads slightly better BLE performance, curve slowly flattens
(Figures 3(a)-3(c)). finding reveals even coarse tuning parameters
might lead optimal near-optimal scores BLE BWESG.
Differences across Language Pairs lower increase Acc1 scores IT-EN attributed fact test set IT-EN comprises words occurrence frequencies 200 training data (Vulic & Moens, 2013a), two test sets
comprise randomly sampled words covering frequency spectra. expected, models
comparison able effectively utilize distributional signals higher-frequency words,
BWESG still displays best performance, improvements Acc1 scores
statistically significant (using McNemars statistical significance test, p < 0.05).15
Further, lowest overall scores models comparison observed NL-EN.
attribute using less training data NL-EN compared ES-EN IT-EN
(i.e., training corpora ES-EN IT-EN almost triple size training corpora
NL-EN). However, observe increase obtained BWESG even
prominent setting limited training data. lower results TPPMI compared
15. McNemars significance test common NLP literature, especially Acc1 scores
reported. utilizes standard 22 contingency table, may observed paired version
common chi-square test. reader referred original work McNemar (1947).

976

fiBilingual Distributed Word Representations Document-Aligned Data

Pair:
BWESG
Length-Ratio
cs:48

ES-EN

IT-EN

NL-EN

d=100

d=200

d=300

d=100

d=200

d=300

d=100

d=200

d=300

0.678

0.701

0.703

0.679

0.689

0.692

0.397

0.396

0.382

Mikolov
cs:4
cs:8
cs:16
cs:48
cs:60

0.187
0.305
0.344
0.311
0.324

0.151
0.306
0.396
0.375
0.389

0.282
0.420
0.486
0.477
0.479

0.368
0.462
0.472
0.458
0.460

0.382
0.518
0.539
0.536
0.538

0.533
0.582
0.602
0.591
0.597

0.042
0.076
0.117
0.132
0.151

0.068
0.095
0.161
0.178
0.180

0.120
0.145
0.184
0.202
0.209

BiCVM
iterations:200

0.342

0.384

0.403

0.309

0.366

0.377

0.068

0.084

0.083

Table 5: BLE results: Comparison BWESG (1) BWE induction model
Mikolov et al. (2013b) relying SGNS, (2) BiCVM: BWE induction model
Hermann Blunsom (2014b) initially developed parallel sentence-aligned
data. models trained document-aligned training Wikipedia
data exactly vocabularies.
two baseline models also attributed overall lower quality size
NL-EN training data, reflected lower quality seed lexicons necessary
start bootstrapping procedure Vulic Moens (2013b).
Computational Complexity BWESG trained larger values cs yields richer
semantic representations, also naturally leads increased training times. However, due
lightweight design supporting SGNS, times order magnitude
lower training times Basic-MuPTM Association-MuPTM. Typically, several
hours needed train BWESG = 300 cs 48 60, whereas takes two
three days train bilingual topic model K = 2000 training set using
multi-threaded architectures 10 Intel(R) Xeon(R) CPU E5-2667 2.90GHz processors.
BWESG model scales expected (i.e., training time increases linearly window
size parameters equal), enjoys advantages (training time-wise
memory-wise) original word2vec package. logical explanation behaviour
follows interpretation SGNS provided Levy Goldberg (2014a), e.g., using
window size 48 instead window size 16 basically means using 3 times positive
examples training (e.g., approximately 15 minutes needed train 300-dimensional
ES-EN BWESG embeddings cs = 16 using Wikipedia data opposed 46
minutes cs = 48, measured 10 Intel(R) Xeon(R) processors).
7.3.3 Experiment II: BWESG vs BWE Induction Models (Group II)
experiments conducted using BWESG length-ratio shuffle strategy.
Note models comparison use exactly data sources vocabularies
BWESG Group models previous section. results BiCVM
Mikolov model summarized Table 5: comparison reveals clear prominent
advantage BWESG model given data training setup.
report absolute scores BilBOWA model setup much lower
two baseline models. BiCVM model, although theory fit learn
977

fiVulic & Moens

0.6

0.62

0.6
0.55

Acc1 scores

0.58

0.5

0.56

0.54
0.45

0.4
2

4

8
Window size

= 100
= 200
= 300
= 100
= 200
= 300
16

(BWESG)
(BWESG)
(BWESG)
(BilBOWA)
(BilBOWA)
(BilBOWA)

0.52

= 100 (BiCVM)
= 200 (BiCVM)
= 300 (BiCVM)

0.5
48

2

4

(a) Spanish-English

8
Window size

16

48

(b) Italian-English

0.7

0.66

Acc1 scores

0.62

0.58

0.54

0.5

0.46
2

4

8
Window size

16

48

(c) Dutch-English

Figure 4: Comparison BWESG (solid curves) two models rely parallel training data: (1) BilBOWA (dotted curves), (2) BiCVM: BWE induction modelinitially developed parallel sentence-aligned data (dashed horizontal
lines). models trained sentence-aligned training Europarl
data exactly vocabularies. BLE performed search
space models. x axes log scale.
document-aligned data, unable compete BWESG learning BWEs
noisier setting non-parallel data.
also present preliminary study compare BWSESG Group II models
setup parallel sentence-aligned data. Results summarized Figures 4(a)-4(c).16
preliminary results clearly demonstrate BWESG able learn BWEs
parallel data without slightest change modeling principles. BilBOWA
model displays better results lower values cs parameter, surprise,
16. Note absolute scores directly comparable BLE scores model trained
Wikipedia data (Tables 4 5) due different training data, different preprocessing steps
vocabularies. Different vocabularies also result different BLE search spaces coverages test
sets (e.g., common Spanish nouns test set nadador (swimmer) colmillo
(tusk) observed Europarl due domain shift).

978

fiBilingual Distributed Word Representations Document-Aligned Data

BWESG model comparable even better baseline models larger window
sizes. BiCVM model, implicitly utilizes entire sentence span training
also outperforms BWESG smaller windows, BWESG performs significantly
better larger windows. BWESG performance flattens quicker
Wikipedia data (compare results cs = 16 cs = 48), easily explained
decreased length aligned items provided Table 1 (i.e., sentences vs documents).
English-Spanish, also compare BWESG pre-trained 40-dimensional embeddings Chandar et al. (2014), embeddings also induced
Europarl data. models Acc1 score 0.432 = 40, BWESG obtains Acc1
scores 0.502 (d = 40, cs = 8), 0.535 (d = 40, cs = 16) 0.529 (d = 40, cs = 48).

8. Evaluation Task II: Suggesting Word Translations Context
another task, test ability BWEs produce context-sensitive semantic similarity
modeling (see Section 5.1), turn may used solve task suggesting word
translations context (SWTC) proposed recently (Vulic & Moens, 2014). goal
build BWESG-based models SWTC given sentential context, similar
prior work. show new BWESG-based SWTC models outperform best
SWTC models Vulic Moens, well SWTC models rely baseline
word representations discussed Section 4.
8.1 Task Description
Given occurrence polysemous word wi V context occurrence,
SWTC task choose correct translation target language LT particular
occurrence wi given set C(wi ) = {t1 , . . . , ttq }, C(wi ) V , tq possible
translations/meanings. may refer C(wi ) inventory translation candidates
wi . task suggesting word translations context (SWTC) may interpreted
ranking tq translation candidates respect observed local context Con(wi )
occurrence word wi . best scoring translation candidate according
scores sim(wi , tj , Con(wi )) (see Section 5.1) ranked list correct translation
particular occurrence wi observing local context Con(wi ).
8.2 Experimental Setup
Test Data use SWTC test set introduced recently (Vulic & Moens, 2014). test
set comprises 15 polysemous nouns three languages (ES, NL) along sets
translation candidates (i.e., sets C). polysemous noun, test sets provide
24 sentences extracted Wikipedia illustrate different senses translations
pivot polysemous noun, accompanied annotated correct translation sentence. yields 360 test sentences language pair (and 1080 test sentences total).
additional set 100 sentences (5 polysemous nouns plus 20 sentences
noun) used development set tune parameter (see Section 5.1)
language pairs models comparison. summary, final aim may formulated
follows: polysemous word wi ES/IT/NL, goal suggest correct
translation English given sentential context.
979

fiVulic & Moens

Evaluation Metrics Since task present list possible translations SWTC
model, let model decide single likely translation given word
sentential context, measure performance Top 1 accuracy (Acc1 ).
8.3 Results Discussion
compare Group Group II models. Note Group models
held previously best reported SWTC scores training Wikipedia data
also use work.
8.3.1 Experiment I: BWESG vs Group
Models Comparison (1) BWESG+add. RM: BWESG. SF: cos. Composition: addition. = 1.0. value suggests context used disambiguate
meaning polysemous word guess likely translation context.17
(2) BMu+HD+S. RM: BasicMuPTM. SF: Hellinger distance. Composition: SmoothedFusion18 Vulic Moens (2014). = 0.9.
(3) BMu+Cue+S. RM: BasicMuPTM. SF: Cue Association measure (Steyvers & Griffiths, 2007; Vulic & Moens, 2013a). Composition: Smoothed-Fusion. = 0.9.
Cue similarity
models computed association score
PKis tailored probabilistic
0
0
P (ti |wi ) = k=1 P (ti |zk )P (zk |wi ), zk denotes k-th latent feature, P (zk |wi0 ) denotes modulated probability score obtained smoothing probabilistic representations wi context Con(wi ).
(4) TPPMI+add. RM: Traditional-PPMI. SF: cos. Composition: addition. = 0.9.
Again, parameters baseline representation models adopted directly
prior work optimized development sets comprising additional 100 sentences (Vulic & Moens, 2014). addition, BMu+HD+S BMu+Cue+S also rely
procedure context sorting pruning (Vulic & Moens, 2014), idea retain
context words semantically similar given pivot polysemous word,
use computations. procedure, however, produces significant gains
probabilistic models (BMu+HD+S BMu+Cue+S), therefore, employ
models. BMu+HD+S BMu+Cue+S context sorting pruning
best scoring models introductory SWTC paper Vulic Moens
currently produce state-of-the-art SWTC results test sets.19
Table 6 summarizes results comparison Group models SWTC task.
NO-CONTEXT refers context-insensitive majority baseline (i.e., always choosing
semantically similar translation candidate obtained BWESG word type level,
without taking account context information).
17. also experimented context-sensitive CLSS models proposed Melamud et al. (2015),
report actual scores model, although displaying similar relative ranking
different representation models, consistently outperformed models Vulic Moens (2014)
evaluation runs: 0.75-0.80 vs 0.60-0.65 models Melamud et al. (2015).
18. short, Smoothed-Fusion probabilistic variant context-sensitive modeling idea presented
equations (7)-(9). details, check work Vulic Moens (2014).
19. omit results Association-MuPTM RM since SWTC models based Association-MuPTM
consistently outperformed SWTC models based Basic-MuPTM across different settings.

980

fiBilingual Distributed Word Representations Document-Aligned Data

Pair:
BWESG+add
Length-Ratio
cs:16
cs:48

ES-EN

IT-EN

NL-EN

d=100

d=200

d=300

d=100

d=200

d=300

d=100

d=200

d=300

0.794*
0.752*

0.767*
0.758*

0.752*
0.764*

0.817*
0.814*

0.789
0.831*

0.794
0.814*

0.778*
0.797*

0.769*
0.789*

0.767*
0.775*

BWESG+add
Shuffling
cs:16
cs:48

d=100

d=200

d=300

d=100

d=200

d=300

d=100

d=200

d=300

0.717
0.731

0.717
0.692

0.694
0.686

0.747
0.775

0.728
0.778

0.728
0.758

0.722
0.739

0.686
0.733

0.678
0.719

NO-CONTEXT

0.406

0.406

0.406

0.408

0.408

0.408

0.433

0.433

0.433

BMu+HD+S
BMu+Cue+S
TPPMI+add

0.664
0.703
0.619

0.664
0.703
0.619

0.664
0.703
0.619

0.731
0.761
0.706

0.731
0.761
0.706

0.731
0.761
0.706

0.669
0.712
0.614

0.669
0.712
0.614

0.669
0.712
0.614

Table 6: comparison SWTC models Spanish-English, Italian-English DutchEnglish bilingual word representations learned document-aligned
Wikipedia data. asterisk (*) denotes statistically significant improvements
BWESG+add strongest baseline according McNemars statistical
significance test (p < 0.05). Highest scores per column bold.
BWESG vs Baseline Representations results reveal BWESG outperforms
baseline bilingual word representations Group also SWTC task. improvements prominent reported values parameters cs, often
statistically significant even compared strongest baseline (which finetuned BMu+Cue+S model context sorting pruning three language pairs
Vulic & Moens, 2014). increase Acc1 scores strongest baseline 12.9%
ES-EN, 11.9% IT-EN, 12.4% NL-EN. obtained results surpass previous
state-of-the-art scores currently best reported results SWTC datasets
using non-parallel data learn semantic representations.
BWESG Shuffling Strategy Although BWESG without shuffling (due reduced complexity SWTC task compared BLE) already displays encouraging results,
clear advantage length-ratio shuffle strategy, displays excellent
performance three language pairs. simple words, shuffling useful.
Dimensionality Number Training Pairs Unlike BLE task, highest Acc1 scores average obtained using lower-dimensional word embeddings (i.e.,
= 100). phenomenon may attributed effect semantic composition
reduced complexity SWTC task compared BLE task. First, although enlarging dimensionality embeddings leads increased semantic expressiveness within

Senses:

2 senses

3 senses

4 senses

Model

Acc1

Acc1

Acc1

BMu+Cue+S
BWESG+add

0.827
0.834

0.619
0.804

0.417
0.583

Table 7: comparison best scoring baseline model BMu+Cue+S best scoring
BWESG+add model different clusters words (2-sense, 3-sense 4-sense
words) Spanish-English.
981

fiVulic & Moens

shared bilingual embedding space, may harmful working composition
models, since simple additive model semantic composition may produce erroneous dimensions constructing higher-dimensional context embeddings single
word embeddings. Second, due design, SWTC task requires coarser-grained representations BLE. BLE task goal detect translation word
vocabulary typically spans (tens of) thousands words, SWTC task
goal detect likely translation word given sentential context,
small closed vocabulary 2-4 possible translations translation inventory.
Therefore, highly likely even low-dimensional embeddings sufficient produce plausible rankings SWTC task, time, sufficient
expressive enough find correct translations BLE. training pairs (i.e., larger
windows) still yield better results average SWTC task. summary, choice
representation granularity dependent actual task, consequently leads
conclusion optimal values cs largely task-specific (compare also results
Table 4 Table 6).
Testing Polysemy order test whether gain performance BWESG+add
derived mostly effective handling easiest set words, is, bisemous
words (polysemous words 2 translation candidates), performed additional experiment, measured Acc1 scores separately words 2, 3,
4 different senses. Results indicate performance gain comes mostly gains
trisemous tetrasemous words, scores bisemous words comparable.
Table 7 shows Acc1 different clusters words ES-EN, similar scoring patterns
observed IT-EN NL-EN.
Differences across Language Pairs Due reduced complexity SWTC, may
also observe relatively higher results NL-EN compared ES-EN IT-EN,
opposed relative performance BLE task, scores NL-EN
much lower scores ES-EN IT-EN. Since SWTC less difficult task
requires coarse-grained representations, even limited amounts training data may sufficient learn word embeddings useful specific task. finding
line recent work Gouws Sgaard (2015).
8.3.2 Experiment II: BWESG vs. BWE Induction Models (Group II)
test BWE induction models SWTC task, using training setup
sets embeddings introduced Section 7.3.3 BLE task. representations
plugged context-sensitive CLSS modeling framework Section 5.1,
optimization parameters SWTC conducted manner
BWESG. results Mikolov model BiCVM summarized Table 8.
results BilBOWA similar BiCVM, report brevity.
BWESG outperforms BWE induction models SWTC task
confirms utility cross-lingual semantic modeling. model Mikolov et al. (2013b)
constitutes stronger baseline: Good results SWTC task model
interesting finding per se. model competitive BWESG
baseline representations models document-aligned data difficult BLE task
using noisy one-to-one translation pairs, performance less complex SWTC
982

fiBilingual Distributed Word Representations Document-Aligned Data

Pair:
BWESG+add
Length-Ratio
cs:16
cs:48

ES-EN

IT-EN

NL-EN

d=100

d=200

d=300

d=100

d=200

d=300

d=100

d=200

d=300

0.794
0.752

0.767
0.758

0.752
0.764

0.817
0.814

0.789
0.831

0.794
0.814

0.778
0.797

0.769
0.789

0.767
0.775

cs:4
cs:8
cs:16
cs:48
cs:60

0.742
0.767
0.769
0.678
0.636

0.739
0.750
0.744
0.642
0.658

0.725
0.747
0.747
0.669
0.656

0.733
0.767
0.758
0.714
0.725

0.706
0.747
0.755
0.714
0.725

0.692
0.744
0.758
0.747
0.742

0.692
0.694
0.725
0.725
0.722

0.700
0.697
0.700
0.711
0.728

0.700
0.672
0.689
0.708
0.722

BiCVM
iterations:200

0.547

0.567

0.539

0.636

0.664

0.642

0.586

0.567

0.581

Mikolov

Table 8: SWTC results: Comparison BWESG (1) BWE induction model
Mikolov et al. (2013b) relying SGNS, (2) BiCVM: BWE induction model
Hermann Blunsom (2014b) initially developed parallel sentence-aligned
data. models trained document-aligned training Wikipedia
data exactly vocabularies.
task reduced search space solid even model relies imperfect set
translation pairs learn mapping two monolingual embedding spaces.
8.3.3 Discussion
analyzing influence pre-training shuffling results two different evaluation
tasks, may safely establish utility inducing bilingual word embeddings using
BWESG model. already presented two shuffling strategies work, one
line future work investigate different possibilities blending words two
different vocabularies pseudo-bilingual documents structured systematic
manner. instance, one approach generating pseudo-training sentences learning
textual perceptual modalities recently introduced (Hill & Korhonen,
2014). However, straightforward extend approach generation
pseudo-bilingual training documents.
Another idea vein build artificial training data higher-quality starting
noisy comparable data by: (1) computing semantically similar words monolingually
across-languages noisy data, (2) retaining highly reliable pairs similar
words using automatic selection procedure (Vulic & Moens, 2012), (3) building pseudobilingual documents using reliable context word pairs. words, questions is:
possible choose positive training pairs systematically reduce noise stemming non-parallel data? construction artificial training data training
data would proceed bootstrapping fashion, model able
steadily reduce noise inherently present comparable data. idea improving
corpus comparability touched upon previous work (Li & Gaussier, 2010; Li,
Gaussier, & Aizawa, 2011).
entire framework proposed article theory completely language
pair agnostic make language pair dependent modeling assumptions,
acknowledge fact three language pairs comprise languages coming
983

fiVulic & Moens

phylum, is, Indo-European language family. Future extensions also include porting
framework distant language pairs share roots
alphabet (e.g., English-Chinese/Hindi/Arabic), benchmarking test sets
still scarce variety semantic tasks (e.g., SWTC) (Camacho-Collados, Pilehvar,
& Navigli, 2015). believe larger window sizes may solve difficulties different
word orderings (e.g., Chinese-English).

9. Conclusions Future Work
proposed described Bilingual Word Embeddings Skip-Gram (BWESG), simple yet effective bilingual word representation learning model able induce bilingual word embeddings solely basis document-aligned comparable data. BWESG
based omnipresent skip-gram negative sampling (SGNS). presented
two ways build pseudo-bilingual documents monolingual SGNS (or
monolingual induction model) may trained produce shared bilingual embedding
spaces. BWESG model make language-pair dependent assumptions
requires language-pair specific external resources bilingual lexicons, predefined category/ontology knowledge parallel data. showed model may trained
non-parallel parallel data without changes modeling principles, which, complemented simplicity lightweight design makes potentially useful
tool researchers machine translation information retrieval.
employed induced BWEs two semantic tasks: (1) bilingual lexicon extraction
(BLE), (2) suggesting word translations context (SWTC). new BWESG-based
BLE SWTC models outperform previous state-of-the-art models BLE SWTC
document-aligned comparable data related BWE induction models (Mikolov et al.,
2013b; Chandar et al., 2014; Gouws et al., 2015). findings article follow
recently published surveys Baroni et al. (2014), Levy et al. (2015) regarding
solid robust performance neural word representations/word embeddings semantic
tasks: new BWESG-based models BLE SWTC significantly outscore previous
state-of-the-art distributional approaches tasks across different parameter settings.
Even encouraging fact new state-of-the-art results attained using
default parameter settings BWESG model suggested word2vec package
without development set. (finer) tuning model parameters future work
may lead higher-quality bilingual embedding spaces.
Several straightforward lines future research already tackled Section 7
Section 8. instance, current length-ratio shuffling strategy may replaced
advanced shuffling method future work. Moreover, BWEs induced BWESG
may used semantic tasks besides ones discussed work, would
interesting experiment types context aggregation selection beyond
bag-of-words assumption, dependency-based contexts (Levy & Goldberg, 2014a),
objective functions training vein proposed Levy Goldberg
(2014b). Similar evolution multilingual probabilistic topic modeling, another path
future work may lead investigating bilingual models learning BWEs
able jointly learn separate documents aligned document pairs, without need
construct pseudo-bilingual documents.
984

fiBilingual Distributed Word Representations Document-Aligned Data

natural step text representation learning research extend focus
single word representations composite phrase, sentence document representations
(Hermann & Blunsom, 2013; Kalchbrenner, Grefenstette, & Blunsom, 2014; Le & Mikolov,
2014; Soyer et al., 2015; Kiros, Zhu, Salakhutdinov, Zemel, Torralba, Urtasun, & Fidler,
2015; Hill, Cho, Korhonen, & Bengio, 2016). article, relied simple composition model based vector addition, shown model performs excellent
SWTC task. However, long run model means sufficient
effectively capture complex compositional phenomena data. Several models
aim learn sentence document embeddings proposed recently,
critically rely sentence-aligned parallel data. yet seen build structured
multilingual phrase, sentence document embeddings solely basis comparable data. low-cost multilingual embeddings beyond word level extracted
comparable data may find application variety tasks statistical machine
translation (Mikolov et al., 2013b; Zou et al., 2013; Zhang et al., 2014; Wu et al., 2014),
semantic tasks multilingual semantic textual similarity (Agirre, Banea, Cardie, Cer,
Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, & Wiebe, 2014), cross-lingual information
retrieval (Vulic et al., 2013; Vulic & Moens, 2015) cross-lingual document classification
(Klementiev et al., 2012; Hermann & Blunsom, 2014b; Chandar et al., 2014).
another future research path, may use knowledge BWEs obtained
BWESG document-aligned data learn bilingual correspondences (e.g., word translation pairs lists semantically similar words across languages) may turn
used learning large unaligned multilingual datasets (Mikolov et al., 2013b; Al-Rfou,
Perozzi, & Skiena, 2013). long run, idea may lead large-scale learning models huge amounts multilingual data without requirement parallel data
manually built bilingual lexicons.

Acknowledgments
work done Ivan Vulic postdoctoral researcher Department Computer Science, KU Leuven supported PDM Kort fellowship (PDMK/14/117).
work also supported SCATE project (IWT-SBO 130041) ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (648909). would also
like thank anonymous reviews helpful suggestions helped us greatly
improve presentation work.

References
Agirre, E., Banea, C., Cardie, C., Cer, D., Diab, M., Gonzalez-Agirre, A., Guo, W., Mihalcea, R., Rigau, G., & Wiebe, J. (2014). SemEval-2014 task 10: Multilingual semantic
textual similarity. Proceedings 8th International Workshop Semantic
Evaluation (SEMEVAL), pp. 8191. Association Computational Linguistics.
Al-Rfou, R., Perozzi, B., & Skiena, S. (2013). Polyglot: Distributed word representations
multilingual NLP. Proceedings Seventeenth Conference Computational
Natural Language Learning (CoNLL), pp. 183192.
985

fiVulic & Moens

Baroni, M., Dinu, G., & Kruszewski, G. (2014). Dont count, predict! systematic comparison context-counting vs. context-predicting semantic vectors. Proceedings
52nd Annual Meeting Association Computational Linguistics (ACL),
pp. 238247.
Baroni, M., & Zamparelli, R. (2010). Nouns vectors, adjectives matrices: Representing adjective-noun constructions semantic space. Proceedings 2010
Conference Empirical Methods Natural Language Processing (EMNLP), pp.
11831193.
Bengio, Y., Ducharme, R., Vincent, P., & Janvin, C. (2003). neural probabilistic language
model. Journal Machine Learning Research, 3, 11371155.
Blacoe, W., & Lapata, M. (2012). comparison vector-based representations semantic composition. Proceedings 2012 Joint Conference Empirical Methods Natural Language Processing Computational Natural Language Learning
(EMNLP-CoNLL), pp. 546556.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. Journal
Machine Learning Research, 3, 9931022.
Boyd-Graber, J., & Blei, D. M. (2009). Multilingual topic models unaligned text.
Proceedings 25th Conference Uncertainty Artificial Intelligence (UAI),
pp. 7582.
Bullinaria, J. A., & Levy, J. P. (2007). Extracting semantic representations word
co-occurrence statistics: computational study. Behavior Research Methods, 39 (3),
510526.
Camacho-Collados, J., Pilehvar, M. T., & Navigli, R. (2015). framework construction monolingual cross-lingual word similarity datasets. Proceedings
53rd Annual Meeting Association Computational Linguistics 7th
International Joint Conference Natural Language Processing (ACL-IJCNLP), pp.
17.
Carbonell, J. G., Yang, J. G., Frederking, R. E., Brown, R. D., Geng, Y., Lee, D., Frederking,
Y., E, R., Geng, R. D., & Yang, Y. (1997). Translingual information retrieval:
comparative evaluation. Proceedings 15th International Joint Conference
Artificial Intelligence (IJCAI), pp. 708714.
Cha, S.-H. (2007). Comprehensive survey distance/similarity measures probability density functions. International Journal Mathematical Models Methods
Applied Sciences, 1 (4), 300307.
Chandar, S., Lauly, S., Larochelle, H., Khapra, M. M., Ravindran, B., Raykar, V. C., &
Saha, A. (2014). autoencoder approach learning bilingual word representations.
Proceedings 27th Annual Conference Advances Neural Information
Processing Systems (NIPS), pp. 18531861.
Chen, D., & Manning, C. (2014). fast accurate dependency parser using neural
networks. Proceedings 2014 Conference Empirical Methods Natural
Language Processing (EMNLP), pp. 740750.
986

fiBilingual Distributed Word Representations Document-Aligned Data

Clarke, D. (2012). context-theoretic framework compositionality distributional
semantics. Computational Linguistics, 38 (1), 4171.
Collobert, R., & Weston, J. (2008). unified architecture natural language processing:
Deep neural networks multitask learning. Proceedings 25th International
Conference Machine Learning (ICML), pp. 160167.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. P. (2011).
Natural language processing (almost) scratch. Journal Machine Learning
Research, 12, 24932537.
Das, D., & Petrov, S. (2011). Unsupervised part-of-speech tagging bilingual graphbased projections. Proceedings 49th Annual Meeting Association
Computational Linguistics: Human Language Technologies (ACL-HLT), pp. 600609.
Daume III, H., & Jagarlamudi, J. (2011). Domain adaptation machine translation
mining unseen words. Proceedings 49th Annual Meeting Association
Computational Linguistics: Human Language Technologies (ACL-HLT), pp. 407412.
De Smet, W., & Moens, M.-F. (2009). Cross-language linking news stories Web
using interlingual topic modeling. Proceedings CIKM 2009 Workshop
Social Web Search Mining (SWSM@CIKM), pp. 5764.
Deschacht, K., De Belder, J., & Moens, M.-F. (2012). latent words language model.
Computer Speech & Language, 26 (5), 384409.
Deschacht, K., & Moens, M.-F. (2009). Semi-supervised semantic role labeling using
latent words language model. Proceedings 2009 Conference Empirical
Methods Natural Language Processing (EMNLP), pp. 2129.
Dinu, G., & Lapata, M. (2010). Measuring distributional similarity context. Proceedings 2010 Conference Empirical Methods Natural Language Processing
(EMNLP), pp. 11621172.
Dinu, G., Lazaridou, A., & Baroni, M. (2015). Improving zero-shot learning mitigating
hubness problem. ICLR Workshop Papers.
Duchi, J. C., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods online
learning stochastic optimization. Journal Machine Learning Research, 12,
21212159.
Dumais, S. T., Landauer, T. K., & Littman, M. (1996). Automatic cross-linguistic information retrieval using Latent Semantic Indexing. Proceedings SIGIR Workshop
Cross-Linguistic Information Retrieval, pp. 1623.
Elman, J. L. (1990). Finding structure time. Cognitive Science, 14, 179211.
Faruqui, M., & Dyer, C. (2014). Improving vector space word representations using multilingual correlation. Proceedings 14th Conference European Chapter
Association Computational Linguistics (EACL), pp. 462471.
Fukumasu, K., Eguchi, K., & Xing, E. P. (2012). Symmetric correspondence topic models
multilingual text analysis. Proceedings 25th Annual Conference Advances
Neural Information Processing Systems (NIPS), pp. 12951303.
987

fiVulic & Moens

Ganchev, K., & Das, D. (2013). Cross-lingual discriminative learning sequence models
posterior regularization. Proceedings 2013 Conference Empirical
Methods Natural Language Processing (EMNLP), pp. 19962006.
Gaussier, E., Renders, J.-M., Matveeva, I., Goutte, C., & Dejean, H. (2004). geometric
view bilingual lexicon extraction comparable corpora. Proceedings
42nd Annual Meeting Association Computational Linguistics (ACL), pp.
526533.
Geman, S., & Geman, D. (1984). Stochastic relaxation, Gibbs distributions,
Bayesian restoration images. IEEE Transactions Pattern Analysis Machine
Intelligence, 6 (6), 721741.
Goldberg, Y., & Levy, O. (2014). Word2vec explained: Deriving Mikolov et al.s negativesampling word-embedding method. CoRR, abs/1402.3722.
Gouws, S., Bengio, Y., & Corrado, G. (2015). BilBOWA: Fast bilingual distributed representations without word alignments. Proceedings 32nd International Conference
Machine Learning (ICML), pp. 748756.
Gouws, S., & Sgaard, A. (2015). Simple task-specific bilingual word embeddings.
Proceedings 2015 Conference North American Chapter Association
Computational Linguistics: Human Language Technologies (NAACL-HLT), pp.
13861390.
Griffiths, T. L., Steyvers, M., & Tenenbaum, J. B. (2007). Topics semantic representation.
Psychological Review, 114 (2), 211244.
Haghighi, A., Liang, P., Berg-Kirkpatrick, T., & Klein, D. (2008). Learning bilingual lexicons
monolingual corpora. Proceedings 46th Annual Meeting Association Computational Linguistics: Human Language Technologies (ACL-HLT), pp.
771779.
Harris, Z. S. (1954). Distributional structure. Word, 10 (23), 146162.
Hermann, K. M., & Blunsom, P. (2013). role syntax vector space models
compositional semantics. Proceedings 51st Annual Meeting Association
Computational Linguistics (ACL), pp. 894904.
Hermann, K. M., & Blunsom, P. (2014a). Multilingual distributed representations without
word alignment. Proceedings 2014 International Conference Learning
Representations (ICLR).
Hermann, K. M., & Blunsom, P. (2014b). Multilingual models compositional distributed
semantics. Proceedings 52nd Annual Meeting Association Computational Linguistics (ACL), pp. 5868.
Hill, F., Cho, K., Korhonen, A., & Bengio, Y. (2016). Learning understand phrases
embedding dictionary. Transactions ACL, 4, 1730.
Hill, F., & Korhonen, A. (2014). Learning abstract concept embeddings multi-modal
data: Since probably cant see mean. Proceedings 2014 Conference
Empirical Methods Natural Language Processing (EMNLP), pp. 255265.
988

fiBilingual Distributed Word Representations Document-Aligned Data

Kalchbrenner, N., Grefenstette, E., & Blunsom, P. (2014). convolutional neural network
modelling sentences. Proceedings 52nd Annual Meeting Association
Computational Linguistics (ACL), pp. 655665.
Kazama, J., Saeger, S. D., Kuroda, K., Murata, M., & Torisawa, K. (2010). Bayesian
method robust estimation distributional similarities. Proceedings 48th
Annual Meeting Association Computational Linguistics (ACL), pp. 247256.
Kiela, D., & Bottou, L. (2014). Learning image embeddings using convolutional neural
networks improved multi-modal semantics. Proceedings 2014 Conference
Empirical Methods Natural Language Processing (EMNLP), pp. 3645.
Kiela, D., & Clark, S. (2014). systematic study semantic vector space model parameters.
Proceedings 2nd Workshop Continuous Vector Space Models
Compositionality (CVSC), pp. 2130.
Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Torralba, A., Urtasun, R., & Fidler,
S. (2015). Skip-thought vectors. Proceedings 28th Annual Conference
Advances Neural Information Processing Systems (NIPS).
Klementiev, A., Titov, I., & Bhattarai, B. (2012). Inducing crosslingual distributed representations words. Proceedings 24th International Conference Computational Linguistics (COLING), pp. 14591474.
Koehn, P. (2005). Europarl: parallel corpus statistical machine translation. Proceedings 10th Machine Translation Summit (MT SUMMIT), pp. 7986.
Kocisky, T., Hermann, K. M., & Blunsom, P. (2014). Learning bilingual word representations marginalizing alignments. Proceedings 52nd Annual Meeting
Association Computational Linguistics (ACL), pp. 224229.
Landauer, T. K., & Dumais, S. T. (1997). Solutions Platos problem: Latent Semantic Analysis theory acquisition, induction, representation knowledge.
Psychological Review, 104 (2), 211240.
Laroche, A., & Langlais, P. (2010). Revisiting context-based projection methods termtranslation spotting comparable corpora. Proceedings 23rd International
Conference Computational Linguistics (COLING), pp. 617625.
Lazaridou, A., Dinu, G., & Baroni, M. (2015). Hubness pollution: Delving crossspace mapping zero-shot learning. ACL, pp. 270280.
Le, Q. V., & Mikolov, T. (2014). Distributed representations sentences documents.
Proceedings 31th International Conference Machine Learning (ICML),
pp. 11881196.
Lebret, R., & Collobert, R. (2014). Word embeddings Hellinger PCA. Proceedings
14th Conference European Chapter Association Computational
Linguistics (EACL), pp. 482490.
Lee, L. (1999). Measures distributional similarity. Proceedings 37th Annual
Meeting Association Computational Linguistics (ACL), pp. 2532.
989

fiVulic & Moens

Levow, G.-A., Oard, D. W., & Resnik, P. (2005). Dictionary-based techniques crosslanguage information retrieval. Information Processing Management, 41 (3), 523
547.
Levy, O., & Goldberg, Y. (2014a). Dependency-based word embeddings. Proceedings
52nd Annual Meeting Association Computational Linguistics (ACL),
pp. 302308.
Levy, O., & Goldberg, Y. (2014b). Neural word embedding implicit matrix factorization.
Proceedings 27th Annual Conference Advances Neural Information
Processing Systems (NIPS), pp. 21772185.
Levy, O., Goldberg, Y., & Dagan, I. (2015). Improving distributional similarity lessons
learned word embeddings. Transactions ACL, 3, 211225.
Li, B., & Gaussier, E. (2010). Improving corpus comparability bilingual lexicon extraction comparable corpora. Proceedings 23rd International Conference
Computational Linguistics (COLING), pp. 644652.
Li, B., Gaussier, E., & Aizawa, A. (2011). Clustering comparable corpora bilingual
lexicon extraction. Proceedings 49th Annual Meeting Association
Computational Linguistics: Human Language Technologies (ACL-HLT), pp. 473478.
Liu, Q., Jiang, H., Wei, S., Ling, Z.-H., & Hu, Y. (2015). Learning semantic word embeddings based ordinal knowledge constraints. Proceedings 53rd Annual
Meeting Association Computational Linguistics 7th International
Joint Conference Natural Language Processing (ACL-IJCNLP), pp. 15011511.
Liu, X., Duh, K., & Matsumoto, Y. (2013). Topic models + word alignment = flexible
framework extracting bilingual dictionary comparable corpus. Proceedings
17th Conference Computational Natural Language Learning (CoNLL), pp.
212221.
Lu, A., Wang, W., Bansal, M., Gimpel, K., & Livescu, K. (2015). Deep multilingual correlation improved word embeddings. Proceedings 2015 Conference
North American Chapter Association Computational Linguistics: Human
Language Technologies (NAACL-HLT), pp. 250256.
Luong, T., Pham, H., & Manning, C. D. (2015). Bilingual word representations monolingual quality mind. Proceedings 1st Workshop Vector Space Modeling
Natural Language Processing, pp. 151159.
McNemar, Q. (1947). Note sampling error difference correlated
proportions percentages. Psychometrika, 12 (2), 153157.
Melamud, O., Levy, O., & Dagan, I. (2015). simple word embedding model lexical
substitution. Proceedings 1st Workshop Vector Space Modeling Natural
Language Processing, pp. 17.
Mikolov, T., Chen, K., Corrado, G. S., & Dean, J. (2013a). Efficient estimation word
representations vector space. Proceedings 2013 International Conference
Learning Representations (ICLR): Workshop Papers.
990

fiBilingual Distributed Word Representations Document-Aligned Data

Mikolov, T., Le, Q. V., & Sutskever, I. (2013b). Exploiting similarities among languages
machine translation. CoRR, abs/1309.4168.
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013c). Distributed
representations words phrases compositionality. Proceedings
27th Annual Conference Advances Neural Information Processing Systems
(NIPS), pp. 31113119.
Mikolov, T., Yih, W., & Zweig, G. (2013d). Linguistic regularities continuous space word
representations. Proceedings 14th Meeting North American Chapter Association Computational Linguistics: Human Language Technologies
(NAACL-HLT), pp. 746751.
Milajevs, D., Kartsaklis, D., Sadrzadeh, M., & Purver, M. (2014). Evaluating neural word
representations tensor-based compositional settings. Proceedings 2014
Conference Empirical Methods Natural Language Processing (EMNLP), pp.
708719.
Mimno, D., Wallach, H., Naradowsky, J., Smith, D. A., & McCallum, A. (2009). Polylingual
topic models. Proceedings 2009 Conference Empirical Methods Natural
Language Processing (EMNLP), pp. 880889.
Mitchell, J., & Lapata, M. (2008). Vector-based models semantic composition. Proceedings 46th Annual Meeting Association Computational Linguistics
(ACL), pp. 236244.
Mnih, A., & Kavukcuoglu, K. (2013). Learning word embeddings efficiently noisecontrastive estimation. Proceedings 27th Annual Conference Advances
Neural Information Processing Systems (NIPS), pp. 22652273.
Ni, X., Sun, J.-T., Hu, J., & Chen, Z. (2009). Mining multilingual topics Wikipedia.
Proceedings 18th International World Wide Web Conference (WWW), pp.
11551156.
Ni, X., Sun, J.-T., Hu, J., & Chen, Z. (2011). Cross lingual text classification mining
multilingual topics Wikipedia. Proceedings 4th International Conference
Web Search Web Data Mining (WSDM), pp. 375384.
Pado, S., & Lapata, M. (2009). Cross-lingual annotation projection semantic roles.
Journal Artificial Intelligence Research, 36, 307340.
Pantel, P., & Lin, D. (2002). Discovering word senses text. Proceedings 8th
ACM SIGKDD International Conference Knowledge Discovery Data Mining
(KDD), pp. 613619.
Peirsman, Y., & Pado, S. (2010). Cross-lingual induction selectional preferences
bilingual vector spaces. Proceedings 11th Meeting North American
Chapter Association Computational Linguistics: Human Language Technologies (NAACL-HLT), pp. 921929.
Peirsman, Y., & Pado, S. (2011). Semantic relations bilingual lexicons. ACM Transactions
Speech Language Processing, 8 (2), article 3.
991

fiVulic & Moens

Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global vectors word representation. Proceedings 2014 Conference Empirical Methods Natural
Language Processing (EMNLP), pp. 15321543.
Pollard, D. (2001). Users Guide Measure Theoretic Probability. Cambridge University
Press.
Rapp, R. (1999). Automatic identification word translations unrelated English
German corpora. Proceedings 37th Annual Meeting Association
Computational Linguistics (ACL), pp. 519526.
Reisinger, J., & Mooney, R. J. (2010). mixture model sharing lexical semantics.
Proceedings 2010 Conference Empirical Methods Natural Language
Processing (EMNLP), pp. 11731182.
Rudolph, S., & Giesbrecht, E. (2010). Compositional matrix-space models language.
Proceedings 48th Annual Meeting Association Computational Linguistics (ACL), pp. 907916.
Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations
back-propagating errors. Nature, 323, 533536.
Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. Proceedings
International Conference New Methods Language Processing.
Shi, T., Liu, Z., Liu, Y., & Sun, M. (2015). Learning cross-lingual word embeddings via
matrix co-factorization. Proceedings 53rd Annual Meeting Association
Computational Linguistics 7th International Joint Conference Natural
Language Processing (ACL-IJCNLP), pp. 567572.
Socher, R., Huval, B., Manning, C. D., & Ng, A. Y. (2012). Semantic compositionality
recursive matrix-vector spaces. Proceedings 2012 Joint Conference
Empirical Methods Natural Language Processing Computational Natural
Language Learning (EMNLP-CoNLL), pp. 12011211.
Sgaard, A., Agic, v., Martnez Alonso, H., Plank, B., Bohnet, B., & Johannsen, A. (2015).
Inverted indexing cross-lingual nlp. Proceedings 53rd Annual Meeting
Association Computational Linguistics 7th International Joint Conference Natural Language Processing (ACL-IJCNLP), pp. 17131722.
Soyer, H., Stenetorp, P., & Aizawa, A. (2015). Leveraging monolingual data crosslingual compositional word representations. Proceedings 2015 International
Conference Learning Representations (ICLR).
Steyvers, M., & Griffiths, T. (2007). Probabilistic topic models. Handbook Latent Semantic Analysis, 427 (7), 424440.
Stratos, K., Collins, M., & Hsu, D. (2015). Model-based word embeddings decompositions count matrices. Proceedings 53rd Annual Meeting Association
Computational Linguistics 7th International Joint Conference Natural
Language Processing (ACL-IJCNLP), pp. 12821291.
Tackstrom, O., Das, D., Petrov, S., McDonald, R., & Nivre, J. (2013). Token type
constraints cross-lingual part-of-speech tagging. Transactions ACL, 1, 112.
992

fiBilingual Distributed Word Representations Document-Aligned Data

Tamura, A., Watanabe, T., & Sumita, E. (2012). Bilingual lexicon extraction comparable corpora using label propagation. Proceedings 2012 Joint Conference
Empirical Methods Natural Language Processing Computational Natural
Language Learning (EMNLP-CoNLL), pp. 2436.
Tiedemann, J. (2012). Parallel data, tools interfaces OPUS. Proceedings
8th International Conference Language Resources Evaluation (LREC), pp.
22142218.
Tiedemann, J., Agic, Z., & Nivre, J. (2014). Treebank translation cross-lingual parser
induction. Proceedings 18th Conference Computational Natural Language
Learning (CoNLL), pp. 130140.
Trask, A., Gilmore, D., & Russell, M. (2015). Modeling order neural word embeddings
scale. Proceedings 32nd International Conference Machine Learning
(ICML), pp. 22662275.
Turian, J. P., Ratinov, L., & Bengio, Y. (2010). Word representations: simple general
method semi-supervised learning. Proceedings 48th Annual Meeting
Association Computational Linguistics (ACL), pp. 384394.
Turney, P. D., & Pantel, P. (2010). frequency meaning: Vector space models
semantics. Journal Artifical Intelligence Research, 37 (1), 141188.
Vulic, I., De Smet, W., & Moens, M.-F. (2011). Identifying word translations comparable corpora using latent topic models. Proceedings 49th Annual Meeting
Association Computational Linguistics: Human Language Technologies (ACLHLT), pp. 479484.
Vulic, I., De Smet, W., & Moens, M.-F. (2013). Cross-language information retrieval models
based latent topic models trained document-aligned comparable corpora.
Information Retrieval, 16 (3), 331368.
Vulic, I., De Smet, W., Tang, J., & Moens, M. (2015). Probabilistic topic modeling
multilingual settings: overview methodology applications. Information
Processing Management, 51 (1), 111147.
Vulic, I., & Moens, M.-F. (2012). Detecting highly confident word translations comparable corpora without prior knowledge. Proceedings 13th Conference
European Chapter Association Computational Linguistics (EACL),
pp. 449459.
Vulic, I., & Moens, M.-F. (2013a). Cross-lingual semantic similarity words similarity semantic word responses. Proceedings 14th Meeting
North American Chapter Association Computational Linguistics: Human
Language Technologies (NAACL-HLT), pp. 106116.
Vulic, I., & Moens, M.-F. (2013b). study bootstrapping bilingual vector spaces
non-parallel data (and nothing else). Proceedings 2013 Conference Empirical Methods Natural Language Processing (EMNLP), pp. 16131624.
Vulic, I., & Moens, M.-F. (2014). Probabilistic models cross-lingual semantic similarity context based latent cross-lingual concepts induced comparable data.
993

fiVulic & Moens

Proceedings 2014 Conference Empirical Methods Natural Language
Processing (EMNLP), pp. 349362.
Vulic, I., & Moens, M.-F. (2015). Monolingual cross-lingual information retrieval models
based (bilingual) word embeddings. Proceedings 38th Annual International ACM SIGIR Conference Research Development Information Retrieval
(SIGIR), pp. 363372.
Wu, H., Dong, D., Hu, X., Yu, D., He, W., Wu, H., Wang, H., & Liu, T. (2014). Improve
statistical machine translation context-sensitive bilingual semantic embedding
model. Proceedings 2014 Conference Empirical Methods Natural Language Processing (EMNLP), pp. 142146.
Wu, H., Wang, H., & Zong, C. (2008). Domain adaptation statistical machine translation domain dictionary monolingual corpora. Proceedings 22nd
International Conference Computational Linguistics (COLING), pp. 9931000.
Xiao, M., & Guo, Y. (2014). Distributed word representation learning cross-lingual
dependency parsing. Proceedings 18th Conference Computational Natural
Language Learning (CoNLL), pp. 119129.
Yarowsky, D., & Ngai, G. (2001). Inducing multilingual POS taggers NP bracketers
via robust projection across aligned corpora. Proceedings 2nd Meeting
North American Chapter Association Computational Linguistics (NAACL),
pp. 200207.
Zhang, D., Mei, Q., & Zhai, C. (2010). Cross-lingual latent topic extraction. Proceedings
48th Annual Meeting Association Computational Linguistics (ACL),
pp. 11281137.
Zhang, J., Liu, S., Li, M., Zhou, M., & Zong, C. (2014). Bilingually-constrained phrase
embeddings machine translation. Proceedings 52nd Annual Meeting
Association Computational Linguistics (ACL), pp. 111121.
Zou, W. Y., Socher, R., Cer, D., & Manning, C. D. (2013). Bilingual word embeddings
phrase-based machine translation. Proceedings 2013 Conference Empirical
Methods Natural Language Processing (EMNLP), pp. 13931398.

994

fiJournal Artificial Intelligence Research 55 (2016) 361-387

Submitted 04/15; published 02/16

Bayesian Optimization Billion Dimensions
via Random Embeddings
Ziyu Wang

ziyu.wang@cs.ox.ac.uk

Department Computer Science, University Oxford

Frank Hutter

fh@cs.uni-freiburg.de

Department Computer Science, University Freiburg

Masrour Zoghi

m.zoghi@uva.nl

Department Computer Science, University Amsterdam

David Matheson

davidm@cs.ubc.ca

Department Computer Science, University British Columbia

Nando de Freitas

nando@cs.ox.ac.uk

Department Computer Science, University Oxford
Canadian Institute Advanced Research

Abstract
Bayesian optimization techniques successfully applied robotics, planning,
sensor placement, recommendation, advertising, intelligent user interfaces automatic
algorithm configuration. Despite successes, approach restricted problems
moderate dimension, several workshops Bayesian optimization identified
scaling high-dimensions one holy grails field. paper, introduce
novel random embedding idea attack problem. resulting Random EMbedding
Bayesian Optimization (REMBO) algorithm simple, important invariance properties, applies domains categorical continuous variables. present
thorough theoretical analysis REMBO. Empirical results confirm REMBO
effectively solve problems billions dimensions, provided intrinsic dimensionality
low. also show REMBO achieves state-of-the-art performance optimizing
47 discrete parameters popular mixed integer linear programming solver.

1. Introduction
Let f : X R function compact subset X RD . address following global
optimization problem
x? = arg max f (x).
xX

particularly interested objective functions f may satisfy one
following criteria: closed-form expression, expensive evaluate,
easily available derivatives, non-convex. treat f blackbox function
allows us query function value arbitrary x X . address objectives
challenging nature, adopt Bayesian optimization framework.
nutshell, order optimize blackbox function f , Bayesian optimization uses
prior distribution captures beliefs behavior f , updates prior
sequentially acquired data. Specifically, iterates following phases: (1) use
c
2016
AI Access Foundation. rights reserved.

fiWang, Hutter, Zoghi, Matheson, & de Freitas

t=2
objective fn (f( ))

observation (x)

acquisition max
acquisition function (u( ))

t=3

new observation (xt )

t=4

posterior mean (( ))

posterior uncertainty
(( ) ( ))

Figure 1: Three consecutive iterations Bayesian optimization toy one-dimensional
problem. unknown objective function approximated Gaussian process (GP) iteration. figure shows mean confidence intervals
process. also shows acquisition function lower green shaded
plots. acquisition high GP predicts high objective (exploitation) prediction uncertainty high (exploration). Note
area far left remains under-sampled, (despite high uncertainty)
correctly predicted unlikely improve highest observation.

prior decide input x X query f next; (2) evaluate f (x); (3) update
prior based new data hx, f (x)i. Step 1 uses so-called acquisition function
quantifies expected value learning value f (x) x X . procedure
illustrated Figure 1.
362

fiBayesian Optimization Billion Dimensions

role acquisition function trade exploration exploitation; popular
choices include Thompson sampling (Thompson, 1933; Hoffman, Shahriari, & de Freitas,
2014), probability improvement (Jones, 2001), expected improvement (Mockus, 1994),
upper-confidence-bounds (Srinivas, Krause, Kakade, & Seeger, 2010), online portfolios
(Hoffman, Brochu, & de Freitas, 2011). typically optimized choosing
points predictive mean high (exploitation) variance large
(exploration). Since typically analytical expression easy evaluate,
much easier optimize original objective function, using off-the-shelf
numerical optimization algorithms.1
term Bayesian optimization coined several decades ago Jonas Mockus (1982).
popular version method known efficient global optimization experimental
design literature since 1990s (Jones, Schonlau, & Welch, 1998). Often, approximation objective function obtained using Gaussian process (GP) priors.
reason, technique also referred GP bandits (Srinivas et al., 2010). However,
many approximations objective proposed, including Parzen estimators (Bergstra, Bardenet, Bengio, & Kegl, 2011), Bayesian parametric models (Wang
& de Freitas, 2011), treed GPs (Gramacy, Lee, & Macready, 2004) random forests
(Brochu, Cora, & de Freitas, 2009; Hutter, 2009; Hutter, Hoos, & Leyton-Brown, 2011).
may suitable GPs number iterations grows without bound,
objective function believed discontinuities. also note often
assumptions smoothness objective function encoded without use
Bayesian paradigm, leading similar algorithms theoretical guarantees (see,
example, Bubeck, Munos, Stoltz, & Szepesvari, 2011, references therein).
rich literature Bayesian optimization, details refer readers
tutorial treatments (Brochu et al., 2009; Jones et al., 1998; Jones, 2001; Lizotte, Greiner,
& Schuurmans, 2011; Mockus, 1994; Osborne, Garnett, & Roberts, 2009) recent theoretical results (Srinivas et al., 2010; Bull, 2011; de Freitas, Smola, & Zoghi, 2012).
Bayesian optimization demonstrated outperform state-of-the-art blackbox optimization techniques function evaluations expensive number
allowed function evaluations therefore low (Hutter, Hoos, & Leyton-Brown, 2013).
recent years, found increasing use machine learning community (Rasmussen,
2003; Brochu, de Freitas, & Ghosh, 2007; Martinez-Cantin, de Freitas, Doucet, & Castellanos, 2007; Lizotte, Wang, Bowling, & Schuurmans, 2007; Frazier, Powell, & Dayanik,
2009; Azimi, Fern, & Fern, 2010; Hamze, Wang, & de Freitas, 2013; Azimi, Fern, & Fern,
2011; Hutter et al., 2011; Bergstra et al., 2011; Gramacy & Polson, 2011; Denil, Bazzani,
Larochelle, & de Freitas, 2012; Mahendran, Wang, Hamze, & de Freitas, 2012; Azimi, Jalali,
& Fern, 2012; Hennig & Schuler, 2012; Marchant & Ramos, 2012; Snoek, Larochelle, &
Adams, 2012; Swersky, Snoek, & Adams, 2013; Thornton, Hutter, Hoos, & Leyton-Brown,
1. optimization step fact circumvented using treed multi-scale optimistic optimization
recently demonstrated Wang de Freitas (2014). also exist several involved Bayesian
non-linear experimental design approaches constructing acquisition function, utility
optimized involves entropy aspect posterior. includes work Hennig
Schuler (2012) finding maxima functions, works Kueck, de Freitas, Doucet (2006)
Kueck, Hoffman, Doucet, de Freitas (2009) learning functions, work Hoffman, Kueck,
de Freitas, Doucet (2009) estimating Markov decision processes. works rely expensive
approximate inference methods computing intractable integrals.

363

fiWang, Hutter, Zoghi, Matheson, & de Freitas

2013). Despite many success stories, approach restricted problems moderate
dimension, typically 10. course, great many problems
needed. However, advance state art, need scale methodology
high-dimensional parameter spaces. goal paper.
difficult scale Bayesian optimization high dimensions. ensure global
optimum found, require good coverage X , dimensionality increases,
number evaluations needed cover X increases exponentially. result,
little progress challenging problem, exceptions. Bergstra et al. (2011) introduced non-standard Bayesian optimization method based tree one-dimensional
density estimators applied successfully optimize 238 parameters complex vision architecture (Bergstra, Yamins, & Cox, 2013). Hutter et al. (2011) used random forests
models Bayesian optimization achieve state-of-the-art performance optimizing
76 mixed discrete/continuous parameters algorithms solving hard combinatorial
problems, successfully carry combined model selection hyperparameter optimization 768 parameters Auto-WEKA framework (Thornton et al., 2013).
Eggensperger, Feurer, Hutter, Bergstra, Snoek, Hoos, Leyton-Brown (2013) showed
two methods indeed yielded best performance high-dimensional hyperparameter optimization (e.g., deep belief networks). However, based weak
uncertainty estimates fail even optimization simple functions
lack theoretical guarantees.
linear bandits case, Carpentier Munos (2012) recently proposed compressed
sensing strategy attack problems high degree sparsity. Also recently, Chen,
Castro, Krause (2012) made significant progress introducing two stage strategy
optimization variable selection high-dimensional GPs. first stage, sequential
likelihood ratio tests, couple tuning parameters, used select relevant
dimensions. This, however, requires relevant dimensions axis-aligned
ARD kernel. Chen colleagues provide empirical results synthetic examples (of
400 dimensions), provide key theoretical guarantees.
Many researchers noted certain classes problems dimensions
change objective function significantly; examples include hyper-parameter optimization
neural networks deep belief networks (Bergstra & Bengio, 2012), well
machine learning algorithms various state-of-the-art algorithms solving N P-hard
problems (Hutter, Hoos, & Leyton-Brown, 2014). say problems low
effective dimensionality. take advantage property, Bergstra Bengio (2012)
proposed simply use random search optimization rationale points
sampled uniformly random dimension densely cover low-dimensional
subspace. such, random search exploit low effective dimensionality without knowing
dimensions important. paper, exploit property new
Bayesian optimization variant based random embeddings.
Figure 2 illustrates idea behind random embeddings nutshell. Assume know
given = 2 dimensional black-box function f (x1 , x2 ) = 1 important
dimensions, know two dimensions important one.
perform optimization embedded 1-dimensional subspace defined x1 = x2
since guaranteed include optimum.
364

fiBayesian Optimization Billion Dimensions

x1

x1

x*

g

dd

Em

x2

Important

x*

Unimportant

x2

Figure 2: function D=2 dimesions d=1 effective dimension: vertical
axis indicated word important right hand side figure. Hence,
1-dimensional embedding includes 2-dimensional functions optimizer.
efficient search optimum along 1-dimensional random
embedding original 2-dimensional space.

first demonstrated recent IJCAI conference paper (Wang, Zoghi, Hutter,
Matheson, & de Freitas, 2013), random embeddings enable us scale Bayesian optimization
arbitrary provided objective function low intrinsic dimensionality. Importantly,
algorithm associated idea, called REMBO, restricted cases
axis-aligned intrinsic dimensions applies d-dimensional linear subspace.
Djolonga, Krause, Cevher (2013) recently proposed adaptive, expensive,
variant REMBO theoretical guarantees.
journal version work, expand presentation provide details throughout. particular, expand description strategy selecting
boundaries low-dimensional space setting kernel length scale parameter;
show means additional application (automatic configuration random forest
body-part classifiers) performance technique collapse
problem obvious low effective dimensionality. experiments (Section
4) also show REMBO solve problems previously untenable high extrinsic dimensions, REMBO achieve state-of-the-art performance optimizing 47
discrete parameters popular mixed integer linear programming solver.

2. Bayesian Optimization
mentioned introduction, Bayesian optimization two ingredients need
specified: prior acquisition function. work, adopt GP priors.
review GPs briefly refer interested reader book Rasmussen
Williams (2006). GP distribution functions specified mean function m()
365

fiWang, Hutter, Zoghi, Matheson, & de Freitas

covariance k(, ). specifically, given set points x1:t , xi RD ,
f (x1:t ) N (m(x1:t ), K(x1:t , x1:t )),
K(x1:t , x1:t )i,j = k(xi , xj ) serves covariance matrix. common choice k
squared exponential function (see Definition 7 page 371), many choices
possible depending degree belief smoothness objective function.
advantage using GPs lies analytical tractability. particular, given
observations x1:t corresponding values f1:t , fi = f (xi ), new point x ,
joint distribution given by:




f1:t
m(x1:t )
K(x1:t , x1:t ) k(x1:t , x )
N
,
.
f

k(x , x1:t )
k(x , x )
simplicity, assume m(x1:t ) = 0 = 0. Using Sherman-MorrisonWoodbury formula, one easily arrive posterior predictive distribution:
f |Dt , x N ((x |Dt ), (x |Dt )),
data Dt = {x1:t , f1:t }, mean variance
(x |Dt ) = k(x , x1:t )K(x1:t , x1:t )1 f1:t
(x |Dt ) = k(x , x ) k(x , x1:t )K(x1:t , x1:t )1 k(x1:t , x ).
is, compute posterior predictive mean () variance () exactly
point x .
iteration Bayesian optimization, one re-compute predictive mean
variance. two quantities used construct second ingredient Bayesian
optimization: acquisition function. work, report results expected
improvement acquisition function (Mockus, 1982; Vazquez & Bect, 2010; Bull, 2011):
u(x|Dt ) = E(max{0, ft+1 (x) f (x+ )}|Dt ).
definition, x+ = arg maxx{x1:t } f (x) element best objective value
first steps optimization process. next query is:
xt+1 = arg max u(x|Dt ).
xX

Note utility favors selection points high variance (points regions
well explored) points high mean value (points worth exploiting). also
experimented UCB acquisition function (Srinivas et al., 2010; de Freitas et al.,
2012) found yield similar results. optimization closed-form acquisition
function carried off-the-shelf numerical optimization procedures, DIRECT (Jones, Perttunen, & Stuckman, 1993) CMA-ES (Hansen & Ostermeier, 2001);
based GP model blackbox function f require additional
evaluations f .
Bayesian optimization procedure shown Algorithm 1.
366

fiBayesian Optimization Billion Dimensions

Algorithm 1 Bayesian Optimization
1: Initialize D0 .
2: = 1, 2, . . .
3:
Find xt+1 RD optimizing acquisition function u: xt+1 = arg maxxX u(x|Dt ).
4:
Augment data Dt+1 = Dt {(xt+1 , f (xt+1 ))}.
5:
Update kernel hyper-parameters.
6: end

3. Random Embedding Bayesian Optimization
introducing new algorithm theoretical properties, need define
mean effective dimensionality formally.
Definition 1. function f : RD R said effective dimensionality de ,
de D,
exists linear subspace dimension de x> RD
x RD , f (x> + x ) = f (x> ), denotes orthogonal
complement ;
de smallest integer property.
call effective subspace f constant subspace.
definition simply states function change along coordinates
x , refer constant subspace. Given definition,
following theorem shows problems low effective dimensionality solved via
random embedding.
Theorem 2. Assume given function f : RD R effective dimensionality
de random matrix RDd independent entries sampled according N (0, 1)
de . Then, probability 1, x RD , exists Rd
f (x) = f (Ay).
Proof. Please refer appendix.
Theorem 2 says given x RD random matrix RDd , probability
1, point Rd f (x) = f (Ay). implies optimizer x?
RD , point y? Rd f (x? ) = f (Ay? ). Therefore, instead optimizing
high dimensional space, optimize function g(y) = f (Ay) lower dimensional
space. observation gives rise new Random EMbedding Bayesian Optimization
(REMBO) algorithm (see Algorithm 2). REMBO first draws random embedding (given
A) performs Bayesian optimization embedded space.
many practical optimization tasks, goal optimize f compact subset
X RD (typically box), f often evaluated outside X . Therefore,
REMBO selects point Ay outside box X , projects Ay onto X
evaluating f . is, g(y) = f (pX (Ay)), pX : RD RD standard projection
operator box-constraint: pX (y) = arg minzX kz yk2 ; see Figure 3. still need
describe REMBO chooses bounded region Rd , inside performs
367

fiWang, Hutter, Zoghi, Matheson, & de Freitas



d=1

Algorithm 2 REMBO: Bayesian Optimization Random Embedding. Blue text denotes parts changed compared standard Bayesian Optimization.
1: Generate random matrix RDd
2: Choose bounded region set Rd
3: Initialize D0 .
4: = 1, 2, . . .
5:
Find yt+1 Rd optimizing acquisition function u: yt+1 = arg maxyY u(y|Dt ).
6:
Augment data Dt+1 = Dt {(yt+1 , f (Ayt+1 ))}.
7:
Update kernel hyper-parameters.
8: end

D=2

Convex projection




din

g

Em





Figure 3: Embedding = 1 = 2. box illustrates 2D constrained space
X , thicker red line illustrates 1D constrained space Y. Note
Ay outside X , projected onto X . set must chosen large enough
projection image, AY, onto effective subspace (vertical axis
diagram) covers vertical side box.

Bayesian optimization. important REMBOs effectiveness depends
size Y. Locating optimum within easier small, set small
may actually contain global optimizer. following theorem, show
choose way depends effective dimensionality de
optimizer original problem contained low dimensional space constant
probability.
Theorem 3. Suppose want optimize function f : RD R effective dimension
de subject box constraint X RD , X centered around 0. Suppose
effective subspace f span de basis vectors,
let x?> X optimizer f inside . random matrix
independent standard Gaussian
entries, exists optimizer y? Rd

de
?
?
?
?
f (Ay ) = f (x> ) ky k2 kx> k2 probability least 1 .
Proof. Please refer appendix.
368

fiBayesian Optimization Billion Dimensions

Theorem 3 says set X original space box constraint,
exists anoptimizer x?> X de -sparse probability least 1 ,
ky? k2 de kx?> k2 f (Ay? ) = f (x?> ). box constraint X = [1, 1]D (which
always achievable rescaling), probability least 1


de ?
de p
?
de .
ky k2
kx> k2


Hence, choose Y, must ensure ball radius de /, centred origin, lies
inside Y.
practice, found
unlikely optimizer falls corner
?
box constraint, implying kx> k < de . Thus setting big may unnecessarily
wasteful. improve understanding effect, developed simulation study,
drew random Gaussian matrices, used map various potential optimizers
? Y, studied norms y? .
x?> corresponding points y>
>
Assume simplicity presentation axis-aligned de -dimensional (the
argument applies > de ). section random matrix maps points
random Gaussian matrix dimension de de . Let us call section
matrix B. Since random Gaussian matrices rotationally invariant distribution,

orthonormal matrix random Gaussian matrix B, OB = B.

1
is, OB B equal distribution. Similarly, B1 , OB1 = BOT
= B1 .


Therefore, B1 also rotationally invariant. Hence, kB1 x> k = kB1 x0> k long
kx> k2 = kx0> k2 . Following equivalence supremum norm projected vectors,
suffices choose point largest norm [1, 1]de simulations. chose
x> = [1, 1, , 1].
conducted simulations several embedding dimensions, de {1, 2, , 50},
drawing 10000 random Gaussian matrices computing kB1 xk . found
empirical probability 1 (for decreasing values ), case
1
max{log(de ), 1}.



simulations indicate could set = 1 max{log(de ), 1}, 1 max{log(de ), 1} e .

Wedid experiments particular chose = log(d)/ d,
[ d, d]d . Note Theorem 3 useful choice, suggests
room improve aspect theory.
careful readers may wonder effect extrinsic dimensionality D.
following theorem, show given intrinsic dimensions, extrinsic
dimensionality effect all; words, REMBO invariant
addition unimportant dimensions.
kB1 xk <

Theorem 4 (Invariance addition unimportant dimensions). Let f : Rde R
N, de , define fD : RD R fD adds de truly unimportant
(D2 D1 )d random
dimensions f : fD (z) = f (z1:de ). Let A1 RD1

A0 R
A1
Gaussian matrices D2 D1 let A2 =
. Then, REMBO run using
A0
369

fiWang, Hutter, Zoghi, Matheson, & de Freitas

dimension de bounded region yields exactly function values
run A1 fD1 run A2 fD2 .
Proof. need show Rd , fD1 (A1 y) = fD2 (A2 y) since
step REMBO (line 6 Algorithm 2) one differs two
algorithm runs. function evaluation step yields results every Rd ,
two REMBO runs behave identically since algorithm
otherwise identical


A1
A1
deterministic selection Step 1. Since A2 =
, A2 =
.
A0
A0
Since D2 D1 de , first de entries D2 1 vector A2 first de entries
A1 y. thus fD1 (A1 y) = f ([A1 y]1:de ) = f ([A2 y]1:de ) = fD2 (A2 y).
Finally, show REMBO also invariant rotations sense given different rotation matrices, running REMBO would result distributions observed
function values. argument made concise following results.
Lemma 5. Consider function f : RD R. Let fR : RD R fR (x) = f (Rx)
orthonormal matrix R RDD . Then, REMBO run bounded region yields
exactly sequence function values run f run R1
fR matrix RDd .
Proof. REMBO uses f (resp. fR R1 A) one spot (in line 6). Thus,
proof trivial showing f (Ayt+1 ) = fR (R1 Ayt+1 ) simple algebra:
fR (R1 Ayt+1 ) = f (RR1 Ayt+1 ) = f (Ayt+1 ).

Theorem 6 (Invariance rotations). Consider function f : RD R. Let fR : RD R
fR (x) = f (Rx) orthonormal matrix R RDD . Then, given random
Gaussian matrices A1 RDd A2 RDd , REMBO run bounded region yields
distribution sequence function values run A1 f run
A2 fR .


Proof. Since R orthonormal, R1 A1 = A2 . Therefore, REMBO run bounded
region yields distribution sequence function values run R1 A1
fR run A2 fR . also Lemma 5 REMBO run bounded
region yields exactly sequence function values run A1 f
run R1 A1 fR . conclusion follows combining previous arguments.
3.1 Increasing Success Rate REMBO
Theorem 3 guarantees contains optimum probability least 1;
probability optimizer lies outside Y. several ways guard
problem. One simply run REMBO multiple times different independently
drawn random embeddings. Since probability failure embedding ,
probability optimizer included considered space k independently
drawn embeddings k . Thus, failure probability vanishes exponentially quickly
370

fiBayesian Optimization Billion Dimensions

number REMBO runs, k. Note also independent runs trivially
parallelized harness power modern multi-core machines large compute clusters.
Another way increasing REMBOs success rate increase
dimensionality

uses internally. > de , probability 1 dde different embeddings
dimensionality de . is, need select de columns RDd represent
de relevant dimensions x. algorithm achieve setting remaining
de sub-components d-dimensional vector zero. Informally, since
embeddings, likely one include optimizer.
experiments, assess merits shortcomings two strategies.
3.2 Choice Kernel
Since REMBO uses GP-based Bayesian optimization search region Rd ,
need define kernel two points y(1) , y(2) Y. begin standard
definition squared exponential kernel:
Definition 7. Let KSE (y) = exp(kyk2 /2). Given length scale ` > 0, define
corresponding squared exponential kernel
!
y(1) y(2)
(2)
(1)
k` (y , ) = KSE
`
possible work two variants kernel. First, use k`d (y1 , y2 )
Definition 7. refer kernel low-dimensional kernel. also adopt
implicitly defined high-dimensional kernel X :
!
(1) ) p (Ay(2) )
p
(Ay
X
X
k`D (y(1) , y(2) ) = KSE
,
`
pX : RD RD projection operator box-constraint (see
Figure 3).
Note using high-dimensional kernel, fitting GP dimensions. However, search space longer box X , instead given much
smaller subspace {pX (Ay) : Y}. Importantly, practice easier maximize
acquisition function subspace.
kernel choices strengths weaknesses. low-dimensional kernel
benefit requiring construction GP space intrinsic dimensionality
d, whereas high-dimensional kernel requires GP constructed space
extrinsic dimensionality D. However, low-dimensional kernel may waste time exploring
region embedding outside X (see Figure 2) two points far apart
region may projected via pX nearby points boundary X . highdimensional kernel affected problem search conducted directly
{pX (Ay) : Y} distances calculated X Y.
choice kernel also depends whether variables continuous, integer
categorical. categorical case important often encounter optimization
371

fiWang, Hutter, Zoghi, Matheson, & de Freitas

Algorithm 3 Bayesian Optimization Hyper-parameter Optimization.
input Threshold .
input Upper lower bounds U > L > 0 hyper-parameter.
input Initial length scale hyper-parameter ` [L, U ].
1: Initialize C = 0
2: = 1, 2, . . .
3:
Find
p xt+1 optimizing acquisition function u: xt+1 = arg maxxX u(x|Dt ).
4:
2 (xt+1 ) <
5:
C =C +1
6:
else
7:
C=0
8:
end
9:
Augment data Dt+1 = {Dt , (xt+1 , f (xt+1 ))}
10:
mod 20 = 0 C = 5
11:
C = 5
12:
U = max{0.9`, L}
13:
C=0
14:
end
15:
Learn hyper-parameter optimizing log marginal likelihood using
DIRECT CMA-ES: ` = arg maxl[L,U ] log p(f1:t+1 |x1:t+1 , l)
16:
end
17: end
problems contain discrete choices. define kernel categorical variables as:



(1)
(2)
(1)
(2) 2
k (y , ) = exp h(s(Ay ), s(Ay )) ,
2
y(1) , y(2) Rd , function maps continuous d-dimensional vectors discrete
D-dimensional vectors, h defines distance two discrete vectors.
detail, s(x) first uses pX project x x [1, 1]D . dimension xi x,
maps xi discrete value scaling rounding. experiments, following Hutter
(1)
(2)
(2009), defined h(x(1) , x(2) ) = |{i : xi 6= xi }| impose artificial ordering
values categorical parameters. essence, measure distance
two points low-dimensional space Hamming distance mappings
high-dimensional space.
3.3 Hyper-parameter Optimization
Bayesian optimization (and therefore REMBO), difficult manually estimate
true length scale hyper-parameter problem hand. avoid manual steps
achieve robust performance across diverse sets objective functions, paper
adopted adaptive hyper-parameter optimization scheme. length scale GPs often
set maximizing marginal likelihood (Rasmussen & Williams, 2006; Jones et al., 1998).
However, demonstrated Bull (2011), approach, implemented naively, may
guarantee convergence. true approaches maximize marginal
372

fiBayesian Optimization Billion Dimensions

likelihood, also approaches rely Monte Carlo sampling posterior
distribution (Brochu, Brochu, & de Freitas, 2010; Snoek et al., 2012) number
data small, unless prior informative.
Here, propose optimize length scale parameter ` maximizing marginal
likelihood subject upper bound U decreased algorithm starts
exploiting much. Full details given Algorithm 3. say algorithm

p exploiting standard deviation maximizer acquisition function
(xt+1 ) less threshold 5 consecutive iterations. Intuitively, means
algorithm emphasize exploration (searching new parts space,
predictive uncertainty high) 5 consecutive iterations. criterion
met, algorithm decreases upper bound U multiplicatively re-optimizes
hyper-parameter subject new bound. Even criterion met hyperparameter re-optimized every 20 iterations. optimization acquisition
function, algorithm runs DIRECT (Jones et al., 1993) CMA-ES (Hansen &
Ostermeier, 2001) uses result best two options. astute reader may
wonder difficulty optimizing acquisition functions. REMBO, however,
found optimization acquisition function problem since
need optimize low-dimensional space acquisition function evaluations
cheap, allowing us tens thousands evaluations seconds (empirically) suffice
cover low-dimensional space well.
motivation algorithm rather err side small length
scale: given squared exponential kernel k` , smaller length scale another kernel
k, one show function f RKHS characterized k also element
RKHS characterized k` . Thus, running expected improvement, one safely use
k` instead k kernel GP still preserve convergence (Bull, 2011). argue
(with small enough lower bound L) algorithm would eventually reduce upper
bound enough allow convergence. Also, algorithm would explore indefinitely
L required positive. experiments, set initial constraint [L, U ]
[0.01, 50] set = 0.002.
want stress fact argument known hold class
kernels continuous domains (e.g. squared exponential Matern class kernels).
Although believe similar argument could made integer categorical
kernels, rigorous arguments concerning convergence kernels remain challenge
Bayesian optimization.

4. Experiments
study REMBO empirically. first use synthetic functions small intrinsic dimensionality de = 2 extrinsic dimension 1 billion demonstrate REMBOs
independence D. Then, apply REMBO automatically optimize 47 parameters widely-used mixed integer linear programming solver demonstrate
achieves state-of-the-art performance. However, also warn blind application
REMBO. illustrate this, study REMBOs performance tuning 14 parameters random forest body part classifier used Kinect. application,
= 14 parameters appear important, REMBO (based = 3) finds
373

fiWang, Hutter, Zoghi, Matheson, & de Freitas

reasonable solutions (better random search comparable domain experts
achieve), standard Bayesian optimization outperform REMBO (and domain experts) moderate-dimensional spaces. optimistically, random forest tuning
application shows REMBO fail catastrophically clear
optimization problem low effective dimensionality.
4.1 Experimental Setup
experiments, used single robust version REMBO automatically sets
GPs length scale parameter described Section 3.3. code REMBO, well
data used experiments publicly available https://github.com/ziyuw/rembo.
experiments required substantial computational resources, computational expense experiment depending mostly cost evaluating
respective black-box function. synthetic experiments Section 4.2 required
minutes run method, optimizing mixed integer programming solver
Section 4.4 required 4-5 hours per run, optimizing random forest classifier
Section 4.5 required 4-5 days per run. total, used half year CPU time
experiments paper. first two experiments, study effect
two methods increasing REMBOs success rate (see Section 3.1) running different
numbers independent REMBO runs different settings internal dimensionality
d.
4.2 Bayesian Optimization Billion Dimensions
experiments section employ standard de = 2-dimensional benchmark function
Bayesian optimization, embedded D-dimensional space. is, add 2
additional dimensions affect function all. precisely, function
whose optimum seek f (x1:D ) = g(xi , xj ), g Branin function
g(x1 , x2 ) = (x2

5.1 2 5
1
x1 + x1 6)2 + 10(1
) cos(x1 ) + 10
2
4

8

j selected using random permutation. measure performance optimization method, used optimality gap: difference best
function value found optimal function value.
evaluate REMBO using fixed budget 500 function evaluations spread
across multiple interleaved runs example, using k = 4 interleaved REMBO runs,
allowed 125 function evaluations. study choices k
considering several combinations values. results Table 1 demonstrate
interleaved runs helped improve REMBOs performance. note 13/50 REMBO
runs, global optimum indeed contained box REMBO searched
= 2; reason poor mean performance REMBO = 2 k = 1.
However, remaining 37 runs performed well, REMBO thus performed well
using multiple interleaved runs: failure rate 13/50=0.26 per independent
run, failure rate using k = 4 interleaved runs 0.264 0.005. One could easily
achieve arbitrarily small failure rate using many independent parallel runs. Using
larger also effective increasing probability optimizer falling REMBOs
374

fiBayesian Optimization Billion Dimensions

Figure 4: Comparison random search (RANDOM), Bayesian optimization (BO), method
Chen et al. (2012) (HD BO), REMBO. Left: = 25 extrinsic dimensions;
Right: = 25, rotated objective function; Bottom: = 109 extrinsic
dimensions. plot means 1/4 standard deviation confidence intervals
optimality gap across 50 trials.

box time slows REMBOs convergence (such interleaving
several short runs loses effectiveness).
Next, compared REMBO standard Bayesian optimization (BO) random
search, extrinsic dimensionality = 25. Standard BO well known perform
well low dimensions, degrade tipping point 15-20 dimensions.
results = 25 (see Figure 4, left) confirm BO performed rather poorly
critical dimensionality (merely tying random search). REMBO,
hand, still performed well 25 dimensions.
One important advantage REMBO contrast approach Chen
et al. (2012) require effective dimension coordinate aligned.
demonstrate fact empirically, rotated embedded Branin function orthogonal rotation matrix R RDD . is, replaced f (x) f (Rx). Figure 4 (middle)
shows REMBOs performance affected rotation.
Finally, since REMBO independent extrinsic dimensionality long
intrinsic dimensionality de small, performed well = 1 000 000 000 dimensions
375

fiWang, Hutter, Zoghi, Matheson, & de Freitas

k
10
5
4
2
1

d=2
0.0022 0.0035
0.0004 0.0011
0.0001 0.0003
0.1514 0.9154
0.7406 1.8996

d=4
0.1553 0.1601
0.0908 0.1252
0.0654 0.0877
0.0309 0.0687
0.0143 0.0406

d=6
0.4865 0.4769
0.2586 0.3702
0.3379 0.3170
0.1643 0.1877
0.1137 0.1202

Table 1: Optimality gap de = 2-dimensional Branin function embedded = 25
dimensions, REMBO variants using total 500 function evaluations.
variants differed internal dimensionality number interleaved
runs k (each run allowed 500/k function evaluations). show
mean standard deviations optimality gap achieved 500 function
evaluations.

(see Figure 4, right). best knowledge, existing method
run high dimensionality random search.
reference, also evaluated method Chen et al. (2012) functions,
confirming handle rotation gracefully: performed best nonrotated case = 25, performed worst rotated case. could used
efficiently = 1, 000. Based Mann-Whitney U test Bonferroni
multiple-test correction, performance differences statistically significant, except
Random vs. standard BO. Finally, comparing REMBO method Chen et al. (2012),
also note REMBO much simpler implement results reliable
(with interleaved runs).
4.3 Synthetic Discrete Experiment
section, test high-dimensional kernel synthetic experiment. Specifically,
optimize Branin function, restrict domain 225 discrete points
regular grid. above, added 23 additional irrelevant dimensions make problem
25-dimensional total.
used small fixed budget 100 function evaluations algorithms involved
problem would require 225 evaluations solved completely.
used k = 4 interleaved runs REMBO. compare REMBO random search
standard BO. REMBO, use high-dimensional kernel handle discrete
nature problem. result comparison summarized Figure 5. Standard
BO suffered high extrinsic dimensionality performed slightly worse
random search. REMBO, hand, performed well setting.
4.4 Automatic Configuration Mixed Integer Linear Programming Solver
State-of-the-art algorithms solving hard computational problems tend parameterize
several design choices order allow customization algorithm new problem domains. Automated methods algorithm configuration recently demonstrated
substantial performance gains state-of-the-art algorithms achieved fully
376

fiBayesian Optimization Billion Dimensions

Figure 5: Comparison random search (RANDOM), Bayesian optimization (BO),
REMBO. = 25 extrinsic dimensions. plot means 1/4 standard deviation confidence intervals optimality gap across 50 trials.

automated fashion (Mockus, Mockus, & Mockus, 1999; Hutter, Hoos, Leyton-Brown, &
Stutzle, 2009; Hutter, Hoos, & Leyton-Brown, 2010; Vallati, Fawcett, Gerevini, Hoos, &
Saetti, 2011; Bergstra et al., 2011; Wang & de Freitas, 2011). successes led
paradigm shift algorithm development towards active design highly parameterized frameworks automatically customized particular problem domains
using optimization (Hoos, 2012; Bergstra et al., 2013; Thornton et al., 2013). resulting algorithm configuration problems shown low dimensionality (Hutter
et al., 2014), here, demonstrate REMBO exploit low dimensionality
even discrete spaces typically encountered algorithm configuration. use configuration problem obtained Hutter et al. (2010), aiming configure 40 binary
7 categorical parameters lpsolve (Berkelaar, Eikland, & Notebaert, 2016) , popular
mixed integer programming (MIP) solver downloaded 40 000 times
last year. objective minimize optimality gap lpsolve obtain time
limit five seconds MIP encoding wildlife corridor problem computational
sustainability (Gomes, van Hoeve, & Sabharwal, 2008). Algorithm configuration usually
aims improve performance representative set problem instances, effective
methods need solve two orthogonal problems: searching parameter space effectively
deciding many instances use evaluation (to trade computational overhead over-fitting). contribution first problems; focus
effectively different methods search parameter space, consider configuration
single problem instance.
Due discrete nature optimization problem, could apply REMBO
using high-dimensional kernel categorical variables kD (y(1) , y(2) ) described Section 3.2. proven theoretical guarantees discrete optimization
377

fiWang, Hutter, Zoghi, Matheson, & de Freitas

Figure 6: Performance various methods configuration lpsolve; show optimality gap lpsolve achieved configurations found various methods (lower better). Left: single run method; Right: performance
k = 4 interleaved runs.

problems, REMBO appears effectively exploit low effective dimensionality least
particular optimization problem.
Figure 6 (left) compares BO, REMBO, baseline random search ParamILS
(Hutter et al., 2009) SMAC (Hutter et al., 2011). ParamILS SMAC specifically
designed configuration algorithms many discrete parameters define
current state art problem. Nevertheless, SMAC vanilla REMBO
method performed best. Based Mann-Whitney U test Bonferroni multiple-test
correction, yielded statistically significantly better results Random
standard BO; performance differences significant. figure shows
REMBO = 5 avoid clutter, optimize parameter;
value tried (d = 3) resulted indistinguishable .
synthetic experiment, REMBOs performance could improved
using multiple interleaved runs. However, shown Hutter, Hoos, Leyton-Brown
(2012), multiple independent runs also improve performance SMAC especially
ParamILS. Thus, fair, re-evaluated approaches using interleaved runs. Figure
6 (right) shows ParamILS REMBO benefitted interleaving k = 4 runs.
However, statistical test results change, still showing SMAC REMBO
outperformed Random BO, significant performance differences.
4.5 Automatic Configuration Random Forest Kinect Body Part Classifier
evaluate REMBOs performance optimizing 14 parameters random
forest body part classifier. classifier closely follows proprietary system used
Microsoft Kinect (Shotton, Fitzgibbon, Cook, Sharp, Finocchio, Moore, Kipman, & Blake,
2011) available https://github.com/david-matheson/rftk.
378

fiBayesian Optimization Billion Dimensions

Figure 7: Left: ground truth depth, ground truth body parts predicted body parts;
Right: features specified offsets u v.

begin describing details dataset classifier order build
intuition objective function parameters optimized. data
used consists pairs depth images ground truth body part labels. Specifically,
used 1 500 pairs 320x240 resolution depth body part images,
synthesized random pose CMU mocap dataset. Depth, ground truth body
parts predicted body parts (as predicted classifier described below) visualized
one pose Figure 7 (left). 19 body parts plus one background class.
20 possible labels, training data contained 25 000 pixels, randomly selected
500 training images. validation test data contained pixels 500 validation
test images, respectively.
random forest classifier applied one pixel P time. node
decision trees, computes depth difference two pixels described offsets
P compares threshold. training time, many possible pairs offsets
generated random, pair yielding highest information gain training
data points selected. Figure 7 (right) visualizes potential feature pixel
green box: computes depth difference pixels red box white
box, specified respective offsets u v. training time, u v drawn two
independent 2-dimensional Gaussian distributions, parameterized
two mean parameters 1 2 three covariance terms 11 , 12 , 22 (21 = 12
symmetry). constitute 10 parameters need optimized,
range [-50,50] mean components [1, 200] covariance terms. Low
covariance terms yield local features, high terms yield global features. Next
ten parameters, random forest classifier four standard parameters, outlined
Table 2. well known computer vision many parameters described
important. Much research devoted identifying best values, results
dataset specific, without definitive general answers.
objective optimizing RF classifier parameters find parameter setting
learns best classifier given time budget five minutes. enable competitive
performance short amount time, node tree random subset
379

fiWang, Hutter, Zoghi, Matheson, & de Freitas

Table 2: Parameter ranges random forest classifier. purpose optimization,
maximum tree depth number potential offsets transformed
log space.
Parameter

Range

Max. tree depth
Min. No. samples non leaf nodes
No. potential offsets evaluate
Bootstrap per tree sampling

[1 60]
[1 100]
[1 5000]
[T F]

data points considered. Also note parameters include number
trees random forest; since performance improves monotonically , created
many trees possible time budget. Trees constructed depth first returned
current state time budget exceeded. Using fixed budget results
subtle optimization problem complex interactions various
parameters (maximum depth, number potential offsets, number trees accuracy).
unclear priori whether low-dimensional subspace 14 interacting parameters exists captures classification accuracy resulting random forests.
performed large-scale computational experiments REMBO, random search,
standard Bayesian optimization (BO) study question. experiment, used
high-dimensional kernel REMBO avoid potential over-exploration problems
low-dimensional kernel described Section 3.2. believed = 14 dimensions
would small enough avoid inefficiencies fitting GP dimensions. belief
confirmed observation standard BO (which operates = 14 dimensions)
performed well problem.
Figure 8 (left) shows results obtained single run random search,
BO, REMBO. Remarkably, REMBO clearly outperformed random search, even based
= 3 dimensions.2 However, since extrinsic dimensionality
moderate = 14, standard Bayesian optimization performed well, since
limited low-dimensional subspace outperformed REMBO. Nevertheless, several
REMBO runs actually performed well, comparably best runs BO. Consequently, running k = 4 interleaved runs method, REMBO performed almost
well BO, matching performance 450 function evaluations (see Figure
8, right).
conclude parameter space RF classifier appear
clear low effective dimensionality; since extrinsic dimensionality moderate,
leads REMBO perform somewhat worse standard Bayesian optimization,
still possible achieve reasonable performance based little = 3 dimensions.
2. Due large computational expense experiment (in total half year CPU time),
performed conclusive experiments = 3; preliminary runs REMBO = 4 performed
somewhat worse = 3 budget 200 function evaluations, still improving
point.

380

fiBayesian Optimization Billion Dimensions

Figure 8: Performance various methods optimizing RF parameters body part
classification. methods, show RF accuracy (mean 1/4 standard
deviation across 10 runs) 2.2 million non background pixels 500pose validation set, using RF parameters identified method.
results test set within 1% results validation set. Left:
performance single run method; Right: performance k = 4
interleaved runs.

5. Conclusion
demonstrated possible use random embeddings Bayesian optimization optimize functions extremely high extrinsic dimensionality provided
low intrinsic dimensionality de . Moreover, resulting REMBO algorithm
coordinate independent requires simple modification original Bayesian
optimization algorithm; namely multiplication random matrix. proved REMBOs
independence theoretically empirically validated optimizing low-dimensional
functions embedded previously untenable extrinsic dimensionalities 1 billion.
also theoretically empirically showed REMBOs rotational invariance. Finally,
demonstrated REMBO achieves state-of-the-art performance optimizing 47 discrete parameters popular mixed integer programming solver, thereby providing
evidence observation (already put forward Bergstra, Hutter colleagues) that,
many problems great practical interest, number important dimensions indeed
appears much lower extrinsic dimensionality.
note central idea work using otherwise unmodified optimization procedure randomly embedded space principle could applied arbitrary
optimization procedures. Evaluating effciency technique procedures
interesting topic future work.
381

fiWang, Hutter, Zoghi, Matheson, & de Freitas

Acknowledgements
thank Christof Schotz proofreading draft article. Frank Hutter gratefully
acknowledges funding German Research Foundation (DFG) Emmy Noether
grant HU 1900/2-1.

Appendix A. Proof Theorem 2
Proof. Since f effective dimensionality de , exists effective subspace RD ,
rank(T ) = de . Furthermore, x RD decomposes x = x> + x ,
x> x . Hence, f (x) = f (x> + x ) = f (x> ). Therefore, without loss
generality, suffice show x> , exists Rd
f (x> ) = f (Ay).
Let RDde matrix, whose columns form orthonormal basis . Hence,
x> , exists c Rde x> = c. Let us assume
rank de . rank de , exists (T A)y = c.
orthogonal projection Ay onto given
Ay = c = x> .
Thus Ay = x> + x0 x0 since x> projection Ay onto . Consequently,
f (Ay) = f (x> + x0 ) = f (x> ).
remains show that, probability one, matrix rank de . Let
Ae RDde submatrix consisting de columns A, i.i.d. samples distributed according N (0, I). Then, ai i.i.d. samples N (0, ) =
2
N (0de , Ide de ), Ae , considered element Rde , sample
2
N (0d2e , Id2e d2e ). hand, set singular matrices Rde Lebesgue
measure zero, since zero set polynomial (i.e. determinant function)
polynomial functions Lebesgue measurable. Moreover, Normal distribution absolutely continuous respect Lebesgue measure, matrix Ae almost
surely non-singular, means rank de true A, whose
columns contain columns Ae .

Appendix B. Proof Theorem 3
Proof. Since X box constraint, projecting x? get x?> X . Also, since
x? = x?> + x x , f (x? ) = f (x?> ). Hence, x?> optimizer.
using argument appeared Proposition 1, easy see probability
1 x Rd Ay = x + x x . Let matrix whose
columns form standard basis . Without loss generality, assume


= de
0
Then, shown Proposition 2, exists y? Rd Ay? = x?> . Note
column A,


Ide 0

ai N 0,
.
0 0
382

fiBayesian Optimization Billion Dimensions

Therefore Ay? = x?> equivalent By? = x?> B Rde de random matrix
independent standard Gaussian entries x?> vector contains first de
entries x?> (the rest 0s). Theorem 3.4 (Sankar, Spielman, & Teng, 2003),



de
1
P kB k2
.

Thus, probability least 1, ky? k kB1 k2 kx?> k2 = kB1 k2 kx?> k2



de
?
kx> k2 .

References
Azimi, J., Fern, A., & Fern, X. (2010). Batch Bayesian optimization via simulation matching.
Advances Neural Information Processing Systems, pp. 109117.
Azimi, J., Fern, A., & Fern, X. (2011). Budgeted optimization concurrent stochasticduration experiments. Advances Neural Information Processing Systems, pp.
10981106.
Azimi, J., Jalali, A., & Fern, X. (2012). Hybrid batch Bayesian optimization. International Conference Machine Learning.
Bergstra, J., Bardenet, R., Bengio, Y., & Kegl, B. (2011). Algorithms hyper-parameter
optimization. Advances Neural Information Processing Systems, pp. 25462554.
Bergstra, J., & Bengio, Y. (2012). Random search hyper-parameter optimization. Journal Machine Learning Research, 13, 281305.
Bergstra, J., Yamins, D., & Cox, D. D. (2013). Making science model search: Hyperparameter optimization hundreds dimensions vision architectures.
International Conference Machine Learning, pp. 115123.
Berkelaar, M., Eikland, K., & Notebaert, P. (2016). lpsolve : Open source (Mixed-Integer)
Linear Programming system. http://lpsolve.sourceforge.net/.
Brochu, E., Brochu, T., & de Freitas, N. (2010). Bayesian interactive optimization
approach procedural animation design. Proceedings 2010 ACM SIGGRAPH/Eurographics Symposium Computer Animation, pp. 103112.
Brochu, E., Cora, V. M., & de Freitas, N. (2009). tutorial Bayesian optimization
expensive cost functions, application active user modeling hierarchical
reinforcement learning. Tech. rep. UBC TR-2009-23 arXiv:1012.2599v1, Dept.
Computer Science, University British Columbia.
Brochu, E., de Freitas, N., & Ghosh, A. (2007). Active preference learning discrete
choice data. Advances Neural Information Processing Systems, pp. 409416.
Bubeck, S., Munos, R., Stoltz, G., & Szepesvari, C. (2011). X-armed bandits. Journal
Machine Learning Research, 12, 16551695.
Bull, A. D. (2011). Convergence rates efficient global optimization algorithms. Journal
Machine Learning Research, 12, 28792904.
383

fiWang, Hutter, Zoghi, Matheson, & de Freitas

Carpentier, A., & Munos, R. (2012). Bandit theory meets compressed sensing high
dimensional stochastic linear bandit. Artificial Intelligence Statistics, pp. 190
198.
Chen, B., Castro, R., & Krause, A. (2012). Joint optimization variable selection highdimensional Gaussian processes. International Conference Machine Learning.
de Freitas, N., Smola, A., & Zoghi, M. (2012). Exponential regret bounds Gaussian process bandits deterministic observations. International Conference Machine
Learning.
Denil, M., Bazzani, L., Larochelle, H., & de Freitas, N. (2012). Learning attend
deep architectures image tracking. Neural Computation, 24 (8), 21512184.
Djolonga, J., Krause, A., & Cevher, V. (2013). High dimensional Gaussian process bandits.
Advances Neural Information Processing Systems, pp. 10251033.
Eggensperger, K., Feurer, M., Hutter, F., Bergstra, J., Snoek, J., Hoos, H., & Leyton-Brown,
K. (2013). Towards empirical foundation assessing Bayesian optimization
hyperparameters. NIPS Workshop Bayesian Optimization Theory Practice.
Frazier, P., Powell, W., & Dayanik, S. (2009). knowledge-gradient policy correlated
normal beliefs. INFORMS journal Computing, 21 (4), 599613.
Gomes, C. P., van Hoeve, W., & Sabharwal, A. (2008). Connections networks: hybrid
approach. International Conference Integration Artificial Intelligence
Operations Research, Vol. 5015, pp. 303307.
Gramacy, R. B., Lee, H. K. H., & Macready, W. G. (2004). Parameter space exploration
Gaussian process trees. International Conference Machine Learning, pp.
4552.
Gramacy, R., & Polson, N. (2011). Particle learning gaussian process models sequential
design optimization. Journal Computational Graphical Statistics, 20 (1),
102118.
Hamze, F., Wang, Z., & de Freitas, N. (2013). Self-avoiding random dynamics integer
complex systems. ACM Transactions Modelling Computer Simulation, 23 (1),
9:19:25.
Hansen, N., & Ostermeier, A. (2001). Completely derandomized self-adaptation evolution
strategies. Evolutionary Computation, 9 (2), 159195.
Hennig, P., & Schuler, C. (2012). Entropy search information-efficient global optimization. Journal Machine Learning Research, 98888, 18091837.
Hoffman, M., Brochu, E., & de Freitas, N. (2011). Portfolio allocation Bayesian optimization. Uncertainty Artificial Intelligence, pp. 327336.
Hoffman, M., Kueck, H., de Freitas, N., & Doucet, A. (2009). New inference strategies
solving Markov decision processes using reversible jump MCMC. Uncertainty
Artificial Intelligence, pp. 223231.
384

fiBayesian Optimization Billion Dimensions

Hoffman, M., Shahriari, B., & de Freitas, N. (2014). correlation budget constraints
model-based bandit optimization application automatic machine learning.
Artificial Intelligence Statistics.
Hoos, H. H. (2012). Programming optimization. Communications ACM, 55 (2),
7080.
Hutter, F. (2009). Automated Configuration Algorithms Solving Hard Computational
Problems. Ph.D. thesis, University British Columbia, Vancouver, Canada.
Hutter, F., Hoos, H., & Leyton-Brown, K. (2014). efficient approach assessing
hyperparameter importance. International Conference Machine Learning.
Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2010). Automated configuration mixed
integer programming solvers. Conference Integration Artificial Intelligence
Operations Research, pp. 186202.
Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2011). Sequential model-based optimization
general algorithm configuration. Learning Intelligent Optimization, pp.
507523.
Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2012). Parallel algorithm configuration.
Learning Intelligent Optimization, pp. 5570.
Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2013). evaluation sequential modelbased optimization expensive blackbox functions. Proceedings GECCO-13
Workshop Blackbox Optimization Benchmarking (BBOB13).
Hutter, F., Hoos, H. H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: automatic
algorithm configuration framework. Journal Artificial Intelligence Research, 36,
267306.
Jones, D. R., Perttunen, C. D., & Stuckman, B. E. (1993). Lipschitzian optimization without
Lipschitz constant. J. Optimization Theory Applications, 79 (1), 157181.
Jones, D. (2001). taxonomy global optimization methods based response surfaces.
Journal Global Optimization, 21 (4), 345383.
Jones, D., Schonlau, M., & Welch, W. (1998). Efficient global optimization expensive
black-box functions. Journal Global optimization, 13 (4), 455492.
Kueck, H., de Freitas, N., & Doucet, A. (2006). SMC samplers Bayesian optimal nonlinear design. IEEE Nonlinear Statistical Signal Processing Workshop, pp. 99102.
Kueck, H., Hoffman, M., Doucet, A., & de Freitas, N. (2009). Inference learning
active sensing, experimental design control. Pattern Recognition Image
Analysis, Vol. 5524, pp. 110.
Lizotte, D., Greiner, R., & Schuurmans, D. (2011). experimental methodology
response surface optimization methods. Journal Global Optimization, 53 (4), 138.
Lizotte, D., Wang, T., Bowling, M., & Schuurmans, D. (2007). Automatic gait optimization
Gaussian process regression. International Joint Conference Artificial
Intelligence, pp. 944949.
385

fiWang, Hutter, Zoghi, Matheson, & de Freitas

Mahendran, N., Wang, Z., Hamze, F., & de Freitas, N. (2012). Adaptive MCMC
Bayesian optimization. Journal Machine Learning Research - Proceedings Track,
22, 751760.
Marchant, R., & Ramos, F. (2012). Bayesian optimisation intelligent environmental
monitoring. NIPS workshop Bayesian Optimization Decision Making.
Martinez-Cantin, R., de Freitas, N., Doucet, A., & Castellanos, J. A. (2007). Active policy
learning robot planning exploration uncertainty. Robotics, Science
Systems.
Mockus, J. (1982). Bayesian approach global optimization. Systems Modeling
Optimization, Vol. 38, pp. 473481. Springer.
Mockus, J. (1994). Application Bayesian approach numerical methods global
stochastic optimization. J. Global Optimization, 4 (4), 347365.
Mockus, J., Mockus, A., & Mockus, L. (1999). Bayesian approach randomization
heuristic algorithms discrete programming. American Math. Society.
Osborne, M. A., Garnett, R., & Roberts, S. J. (2009). Gaussian processes global optimisation. Learning Intelligent Optimization, pp. 115.
Rasmussen, C. E. (2003). Gaussian processes speed hybrid Monte Carlo expensive
Bayesian integrals. Bayesian Statistics 7.
Rasmussen, C. E., & Williams, C. K. I. (2006). Gaussian Processes Machine Learning.
MIT Press.
Sankar, A., Spielman, D., & Teng, S. (2003). Smoothed analysis condition numbers
growth factors matrices. Tech. rep. Arxiv preprint cs/0310022, MIT.
Shotton, J., Fitzgibbon, A., Cook, M., Sharp, T., Finocchio, M., Moore, R., Kipman, A.,
& Blake, A. (2011). Real-time human pose recognition parts single depth
images. IEEE Computer Vision Pattern Recognition, pp. 12971304.
Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical Bayesian optimization machine learning algorithms. Advances Neural Information Processing Systems,
pp. 29602968.
Srinivas, N., Krause, A., Kakade, S. M., & Seeger, M. (2010). Gaussian process optimization
bandit setting: regret experimental design. International Conference
Machine Learning, pp. 10151022.
Swersky, K., Snoek, J., & Adams, R. P. (2013). Multi-task Bayesian optimization.
Advances Neural Information Processing Systems, pp. 20042012.
Thompson, W. R. (1933). likelihood one unknown probability exceeds another
view evidence two samples. Biometrika, 25 (3/4), 285294.
Thornton, C., Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2013). Auto-WEKA: Combined selection hyperparameter optimization classification algorithms. ACM
SIGKDD Conference Knowledge Discovery Data Mining, pp. 847855.
Vallati, M., Fawcett, C., Gerevini, A. E., Hoos, H. H., & Saetti, A. (2011). Generating
fast domain-optimized planners automatically configuring generic parameterised
planner. ICAPS Planning Learning Workshop.
386

fiBayesian Optimization Billion Dimensions

Vazquez, E., & Bect, J. (2010). Convergence properties expected improvement algorithm fixed mean covariance functions. Journal Statistical Planning
Inference, 140, 30883095.
Wang, Z., & de Freitas, N. (2011). Predictive adaptation hybrid Monte Carlo
Bayesian parametric bandits. NIPS Deep Learning Unsupervised Feature Learning Workshop.
Wang, Z., & de Freitas, N. (2014). Bayesian multiscale optimistic optimization. Artificial
Intelligence Statistics.
Wang, Z., Zoghi, M., Hutter, F., Matheson, D., & de Freitas, N. (2013). Bayesian optimization high dimensions via random embeddings. International Joint Conference
Artificial Intelligence, pp. 17781784.

387

fiJournal Artificial Intelligence Research 55 (2016) 499-564

Submitted 07/15; published 02/16

Module Extraction Expressive Ontology Languages
via Datalog Reasoning
Ana Armas Romero
Mark Kaminski
Bernardo Cuenca Grau
Ian Horrocks

ana.armas@cs.ox.ac.uk
mark.kaminski@cs.ox.ac.uk
bernardo.cuenca.grau@cs.ox.ac.uk
ian.horrocks@cs.ox.ac.uk

Department Computer Science,
University Oxford,
Wolfson Building, Parks Road,
Oxford, OX1 3QD, UK

Abstract
Module extraction task computing (preferably small) fragment
ontology preserves class entailments signature interest . Extracting
modules minimal size well-known computationally hard, often algorithmically
infeasible, especially highly expressive ontology languages. Thus, practical techniques
typically rely approximations, provably captures relevant entailments,
guaranteed minimal. Existing approximations ensure preserves
second-order entailments w.r.t. , stronger condition required
many applications, may lead unnecessarily large modules practice.
paper propose novel approach module extraction reduced reasoning
problem datalog. approach generalises existing approximations elegant way.
importantly, allows extraction modules tailored preserve specific
kinds entailments, thus often significantly smaller. evaluation wide
range ontologies confirms feasibility benefits approach practice.

1. Introduction
Module extraction task computing, given ontology signature interest
, (preferably small) subset (a module) preserves class -entailments
relevant application hand. module therefore indistinguishable
w.r.t. relevant -entailments, application safely rely instead
tasks concern symbols .
Module extraction received great deal attention recent years (Seidenberg
& Rector, 2006; Stuckenschmidt, Parent, & Spaccapietra, 2009; Cuenca Grau, Horrocks,
Kazakov, & Sattler, 2008; Kontchakov, Wolter, & Zakharyaschev, 2010; Del Vescovo, Parsia, Sattler, & Schneider, 2011; Nortje, Britz, & Meyer, 2013; Gatens, Konev, & Wolter,
2014). Modules found numerous applications ontology reuse (Cuenca Grau et al.,
2008; Jimenez-Ruiz, Cuenca Grau, Sattler, Schneider, & Berlanga Llavori, 2008), matching
(Jimenez-Ruiz & Cuenca Grau, 2011), debugging (Suntisrivaraporn, Qi, Ji, & Haase, 2008;
Ludwig, 2014) classification (Armas Romero, Cuenca Grau, & Horrocks, 2012; Tsarkov
& Palmisano, 2012; Cuenca Grau, Halaschek-Wiener, Kazakov, & Suntisrivaraporn, 2010).
c 2016 AI Access Foundation. rights reserved.

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

preservation relevant entailments formalised via inseparability relations (Konev,
Lutz, Walther, & Wolter, 2009). strongest notion model inseparability, requires must possible turn model model (re-)interpreting
symbols outside ; case, preserves second-order -entailments
(Konev, Lutz, Walther, & Wolter, 2013). weaker flexible notion
deductive inseparability, requires entail -formulas
particular query language. Unfortunately, decision problems associated module
extraction generally high complexity even undecidable, especially expressive
ontology languages. model inseparability, checking whether module w.r.t.
undecidable even restricted lightweight description logic (DL) EL (Konev
et al., 2013), standard reasoning tractable (Baader, Brandt, & Lutz, 2005).
deductive inseparability, problem typically decidable lightweight DLs
reasonable query languages, albeit still high worst-case complexity; instance,
ExpTime-complete EL consider concept inclusions query language (Lutz
& Wolter, 2010). Practical algorithms ensure minimality extracted modules
known ELI ontologies satisfying particular acyclicity condition (Konev et al.,
2013) well dialects DL-Lite (Kontchakov et al., 2010). best knowledge, complexity module extraction ontology languages based
DLs, variants datalog (Cal, Gottlob, Lukasiewicz, Marnette, & Pieris, 2010),
remains largely unexplored.
Practical module extraction techniques typically based sound approximations,
ensure computed fragment module (i.e., inseparable w.r.t.
), provide minimality guarantee. popular techniques based
family polynomially checkable conditions based notion syntactic locality
(Cuenca Grau, Horrocks, Kazakov, & Sattler, 2007a; Cuenca Grau et al., 2008; Sattler,
Schneider, & Zakharyaschev, 2009). locality-based module enjoys number
desirable properties w.r.t. signature interest:
(P1) model inseparable O, thus preserving second-order -entailments O.
(P2) depleting, sense \ inseparable empty ontology;
implies relevant information left behind extracting O.
(P3) self-contained, preserves relevant entailments w.r.t. ,
also w.r.t. symbols signature.
(P4) justification-preserving, sense subset-minimal fragment
preserving -entailment (each justification) contained M.
(P5) computed efficiently, even ontologies expressive description logics.
Model inseparability ensures modules used regardless query language
relevant application hand. Depletingness self-containment identified important properties ontology reuse modular ontology development tasks
(Sattler et al., 2009; Jimenez-Ruiz et al., 2008). Finally, preservation justifications
enables use modules optimising debugging explanation services (Schlobach &
Cornet, 2003; Kalyanpur, Parsia, Horridge, & Sirin, 2007), well incremental reasoning
(Suntisrivaraporn, 2008; Cuenca Grau et al., 2010).
500

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

Locality-based module extraction techniques easy implement, surprisingly
eective practice. main drawback extracted modules rather
large, limits usefulness applications (Del Vescovo, Klinov, Parsia, Sattler, Schneider, & Tsarkov, 2013). One way address issue develop techniques
approximate minimal modules closely, still fulfilling properties (P1)(P4).
Eorts direction confirmed locality-based modules far optimal
practice (Gatens et al., 2014); however, techniques apply rather restricted
ontology languages utilise algorithms high worst-case complexity.
Another approach computing smaller modules weaken properties (P1)(P4),
stronger many applications require. particular, model inseparability
strong condition, deductive inseparability w.r.t. query language suitable
application hand would usually suffice.
paper, propose novel approach reduces module extraction reasoning
problem basic rule-based language datalog (Abiteboul, Hull, & Vianu, 1995; Dantsin,
Eiter, Gottlob, & Voronkov, 2001). connection module extraction datalog
first observed Suntisrivaraporn (2008), showed locality ?-module extraction
EL ontologies could reduced propositional datalog reasoning. approach takes
connection much farther, generalises locality-based modules elegant way.
key distinguishing features approach follows:
applicable ontology languages based description logics, also
expressive rule-based knowledge representation formalisms extend datalog
existential quantification disjunction head rules (Cal et al., 2010;
Bourhis, Morak, & Pieris, 2013; Alviano, Faber, Leone, & Manna, 2012).
sensitive dierent inseparability relations proposed literature;
particular, extract deductively inseparable modules query language
tailored specific requirements application hand. allows us
relax property (P1) extract significantly smaller modules.
cases, modules depleting capture justifications relevant entailments; moreover, approach adapted either ensure dispense
self-containment, depending application needs.
ensures tractability module extraction DL-based ontology languages,
also enables use highly scalable o-the-shelf datalog reasoners.
implemented approach using RDFox datalog reasoner (Motik, Nenov,
Piro, Horrocks, & Olteanu, 2014). evaluation complex, real-world, ontologies shows
module size consistently decreases consider weaker inseparability relations,
could significantly improve usefulness modules applications.

2. Preliminaries
Section 2.1 introduce language first-order rules, powerful enough
fully capture expressive rule-based ontology languages datalog (Cal et al.,
2010), datalog,_ (Bourhis et al., 2013; Alviano et al., 2012), well mainstream
501

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

description logics (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003).
results paper hold arbitrary knowledge bases consisting first-order
rules, hence applicable wide range knowledge representation formalisms.
Section 2.2 introduce syntax first-order semantics description logic
SROIQ (Horrocks, Kutz, & Sattler, 2006), underpins W3C standard ontology
language OWL 2 (Motik, Patel-Schneider, & Parsia, 2012; Cuenca Grau, Horrocks, Motik,
Parsia, Patel-Schneider, & Sattler, 2008). introduce normal form SROIQ
establish correspondence first-order rules. Finally, Section 2.3 briefly recall
well-known hyperresolution calculus first-order logic (Bachmair & Ganzinger, 2001),
exploit many technical results show module preserves
required consequences given ontology.
Throughout paper, assume basic familiarity first-order logic use
standard first-order logic notions, predicates, constants, variables, terms, atoms,
formulas, sentences, interpretations entailment (written |=). define signature
set predicates; furthermore, given first-order sentence , use Sig( ) denote
signature . say -sentence Sig( ) . Analogously, denote
Ct( ) set constants . definitions extend naturally sets sentences;
indeed, later paper speak -rules, -datasets, -ontologies
obvious meaning. restriction signatures contain predicates separate
treatment constants convenient working inseparability relations later on.
set function-free sentences F 0 (model) conservative extension set F
model F model J F 0 domain
AI = AJ 2 Sig(F) aI = aJ 2 Ct(F).

deviate slightly standard definition first-order logic definition
include nullary symbols > ?, interpreted true false respectively every first-order interpretation. Similarly, consider first-order logic without
equality hence assume interpreted identity relation
domain every interpretation. Instead, treat ?, > ordinary predicates,
meaning axiomatise explicitly every knowledge base. assume ?
nullary, > unary binary. Given set F function-free sentences,
define following sets sentences F ? , F > , F .
F ? empty F contains occurrences ?, singleton set {?} otherwise.
F > empty F contains occurrence >; otherwise, set
{ 8x1 , . . . , xn [A(x1 , . . . , xn ) ! >(xi )] | 2 Sig(F) n-ary, 1 n }
F empty F contains occurrences ; otherwise, consists sentences
(EQ1)(EQ5) given next. Sentence (EQ1) instantiated constant 2 Ct(F);
furthermore, sentences (EQ2) (EQ5) instantiated n-ary predicate
502

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

Sig(F) xi x = x1 , . . . , xn :
!aa

(EQ1)

8x [A(x) ! xi xi ]

(EQ2)

8x, [x ! x]

8x, y, z [x ^ z ! x z]

8x, [A(x) ^ xi ! A(x1 , . . . , xi

(EQ3)
(EQ4)
1 , y, xi+1 , . . . , xn )]

(EQ5)

consider substitutions functional mappings two sets terms. Given
substitution term domain , abuse notation expression denotes t. Substitutions applied formulas: given atom A(t1 , ..., tn ),
A(t1 , ..., tn ) = A(t1 , ..., tn ), given non-atomic formula ,
result applying atoms . application extended sets formulas natural
way. Given two substitutions , composition substitution
t( ) = (t ) domain . say compatible
coincide intersection domains. compatible, union
substitution [ t( [ ) = domain t( [ ) =
domain . Finally, use dom( ) (resp. range( )) denote domain
(resp. range) .
2.1 Rule-Based First-Order Languages
Rule-based languages prominent knowledge representation formalisms closely related
ontology languages (Dantsin et al., 2001; Bry, Eisinger, Eiter, Furche, Gottlob, Ley, Linse,
Pichler, & Wei, 2007; Cal et al., 2010). paper, focus monotonic formalisms
hence rule languages seen fragments first-order logic. next
define general notion first-order rule underpins datalog datalog,_
families languages (Cal et al., 2010; Alviano et al., 2012).
fact function-free ground atom. finite set facts called dataset. rule r
function-free first-order sentence form
8x['(x) ! 9y (x, y)]

(1)

x disjoint vectors variables, ' (possibly empty) conjunction distinct
atoms constants variables x; built atoms constants
variables x [ using conjunction (^) disjunction (_). Note fact also
rule. Formula ' rule body 9y (x, y) rule head. head rule
empty, case represent . Universal quantifiers rules omitted
brevity. Rules required safe, is, universally quantified variables
head must occur body. rule datalog head either empty consists
single atom variables universally quantified. Note that, set F
function-free sentences, set F ? [ F > [ F contains datalog rules.
(first-order) ontology finite set rules satisfying O? [ O> [ O.
assume w.l.o.g. dierent rules share existentially quantified variables
rule empty head ? ! .
503

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

roles
(R, x, y)
(R , x, y)

=
=

R(x, y)
R(y, x)

(?c , x)
(>c , x)
(o, x)
(C, x)
(C1 u C2 , x)
(C1 C2 , x)
(9R.C, x)
(8R.C, x)
(9R.Self, x)
( nR.C, x)
( nR.C, x)
axioms
(C1 v C2 )
(R1 Rm v S)
(Disj(R1 , R2 ))
(Ref(R))

=
=
=
=
=
=
=
=
=
=
=

?
>(x)
xo
(C, x)
(C1 , x) ^ (C2 , x)
(C1 , x) _ (C2 , x)
9y[(R, x, y) ^ (C, y)]
8y[(R, x, y) ! (C, y)]
(R, x, x) V
V
9x1 , . . . , xn [ ((R, x, xi ) ^ (C, xi )) ^ i6=j (xi xj )]
V
W
8x1 , . . . , xn+1 [ ((R, x, xi ) ^ (C, xi )) ! i6=j xi xj ]

concepts

=
=
=
=

8x[(C1 , x) ! V
(C2 , x)]

8x1 , . . . , xm+1 [ i=1 (Ri , xi , xi+1 ) ! (S, x1 , xm+1 )]
8x, y[(R1 , x, y) ^ (R2 , x, y) ! ?]
8x[>(x) ! (R, x, x)]

Figure 1: Semantics SROIQ via translation first-order logic.
datalog program ontology containing datalog rules. Given datalog program
P dataset D, materialisation, denoted P(D), set facts entailed
P [ D. materialisation computed time polynomial size using
forward chaining (Abiteboul et al., 1995; Dantsin et al., 2001).
conclude section, define languages typically used querying first-order
ontologies. define Boolean positive existential query (Boolean PEQ) non-empty
sentence q built function-free atoms using 9, ^ _; query holds w.r.t.
ontology |= q. Boolean PEQ conjunctive query (CQ) disjunctionfree. following proposition, proof straightforward, establishes useful
connection Boolean PEQ evaluation entailment first-order rules.
V
Proposition 1. Let ontology, r = ni=1 (x) ! 9y (x, y) rule, let
substitution mapping universally quantified variables r fresh distinct constants.
Then, |= r [ { }ni=1 |= 9y .
2.2 Description Logics
Description logics (DLs) (Baader et al., 2003) family knowledge representation
formalisms correspond decidable fragments first-order logic. DLs logical formalisms underpinning standard ontology languages: OWL DL based
description logic SHOIN (Horrocks, Patel-Schneider, & van Harmelen, 2003), whereas
revision OWL 2 based expressive logic SROIQ (Horrocks et al., 2006;
Cuenca Grau et al., 2008; W3C OWL Working Group, 2012).
504

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

basic building blocks SROIQ pairwise disjoint countable sets atomic
concepts, correspond unary predicates, atomic roles, correspond binary
predicates, individuals, correspond constants. role R either atomic
role inverse atomic role S. Complex concepts constructed according
following grammar, ?c >c special bottom top concepts,
atomic concept, R role, individual n 1:
C ::= ?c | >c | | {o} | C | C1 u C2 | C1 C2 |
9R.C | 8R.C | 9R.Self |

nR.C | nR.C

assume concept expressions form 1R.C replaced equivalent
9R.C. general concept inclusion axiom (GCI) expression form C1 v C2 ,
C1 C2 concepts. role inclusion axiom (RIA) expression form
R1 Rm v R Ri role R atomic role. role disjointness axiom
expression form Disj(R1 , R2 ) R1 R2 roles. Finally, reflexivity axiom
expression form Ref(R) R role.
SROIQ ontology finite set GCIs, RIAs, role disjointness reflexivity axioms. order ensure decidability basic reasoning tasks, SROIQ ontology
must satisfy certain additional conditions (e.g., set RIAs must satisfy regularity
condition); conditions are, however, immaterial results paper
refer reader work Horrocks et al. (2006) details.
semantics SROIQ given direct translation first-order logic
(Baader et al., 2003; Motik, 2006) using mapping function Figure 1. Given
SROIQ ontology O, let FO = { () | 2 }; define
?
>

(O) = FO [ FO
[ FO
[ FO

first-order interpretation model model (O).
Note (O) always set first-order rules defined Section 2.1. However,
always polynomially normalised entailment preserving SROIQ ontology
O0 (O0 ) set rules. next define normalised SROIQ ontologies
assume onwards (unless otherwise stated) SROIQ ontologies normalised.
Definition 2. SROIQ ontology normalised consists axioms form
v ?c v {o} >c v {o} v v B1 B2 A1 u A2 v B
v 9R.B v 9R.Self 9R.A v B 9R.Self v v nR.B
R1 R2 v R v Disj(R, S) Ref(R)
A(i) , B(i) atomic concepts, individual, R(i) , atomic roles, n

1.



Table 1 shows application normalised axioms. Clearly, (O) set rules
whenever normalised. Moreover, establishes bijection (O)
case. Since (O) semantically equivalent, thus natural identify them,
shall remainder paper.
505

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks


v ?c
v {o}
>c v
{o} v
v B1 B2
A1 u A2 v B
v 9R.B
v 9R.Self
9R.A v B
9R.Self v
v nR.B
R1 R2 v
R vS
Disj(R, S)
Ref(R)

()
A(x) ! ?
A(x) ! x
>(x) ! A(x)
( ! A(o))
A(x) ! B1 (x) _ B2 (x)
A1 (x) ^ A2 (x) ! B(x)
A(x) ! 9y[R(x, y) ^ B(y)]
A(x) ! R(x, x)
R(x, y) ^ A(y) ! B(x)
R(x, x) ! A(x)
V
W
A(x) ^ n+1
i=1 [R(x, yi ) ^ B(yi )] !
i6=j yi yj
R1 (x, y) ^ R2 (y, z) ! S(x, z)
R(x, y) ! S(y, x)
R(x, y) ^ S(x, y) ! ?
>(x) ! R(x, x)

Table 1: Correspondence normalised SROIQ axioms rules.
Proposition 3. Let SROIQ ontology let O0 result exhaustively
applying rewriting rules Figure 2. Then, O0 satisfies following properties:
(i) normalised; (ii) size polynomial size (assuming unary encoding
numbers); (iii) conservative extension O.
Proof. easy see rewrite rule always applicable every axiom
normalised; furthermore, rule applicable normalised axioms. Thus, O0 normalised.
Furthermore, note rules Figure 2 syntactic variant structural transformation first-order logic (Nonnengart & Weidenbach, 2001). implies O0
computed time polynomial size (assuming unary encoding numbers),
also conservative extension O.
2.3 Hyperresolution Proofs
Reasoning w.r.t. ontologies realised means hyperresolution (Robinson, 1965;
Bachmair & Ganzinger, 2001), generalises forward chaining datalog.
HyperresolutionVis applicable
sets first-order clausesuniversally quantified senW
tences form ! j j j atoms (possibly containing function symbols). Thus, applicable ontologies containing existentially quantified rules
Skolemisation subsequent transformation Conjunctive Normal Form (CNF).
rule r form (1) existentially quantified variable r, let fyr
function symbol globally unique r arity |x|, let sk substitution
sk (y) = fyr (x) r y. Skolemisation r sentence
sk(r) = '(x) ! (x, y)sk
506

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

R1

?c v C
v {o}
9S .D v C
9S.D v C
mS .D v C
mS.D v C
u C1 v C2
C1 v C2
C1 C2 v C3
8S .C1 v C2
8S.C1 v C2
9S .Self v C
(m 1)S .C1 v C2
(m 1)S.C1 v C2
C v >c
{o} v
C v 9S .D
C v 9S.D
C v 8S .D
C v 8S.D
C v 9S .Self
C v nS .D
C v nS.D
C1 v C2
C1 v C2
C1 v C2 u C3
C1 v mS .C2
C1 v mS.C2
D1 v D2
R2 R 3 Rk v
Q
RvS
R Q vS
Disj(R, Q )
Disj(Q , R)
Ref(Q )

)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)
)

X v {o}, v X
9P.X v D, v P
9S.X v C, v X
mP.D v C, v P
>c v (m 1)S.D C
X u C1 v C2 , v X
> c v C1 C2
C1 v C3 , C2 v C3
8P.C1 v C2 , P v
>c v 9S.X C2 , X u C v ?c
9P.Self v C, v P
(m 1)P.C1 v C2 , P v
>c v mS.C1 C2
{o} v X, X v
C v 9P.D, P v
C v 9S.X, X v
C v 8P.D, v P
C v 8S.X, X v
C v 9P.Self, P v
C v nP.D, v P
C v nS.X, X v
C1 v X C2 , X v
C 1 u C 2 v ?c
C1 v C2 , C1 v C3
C1 v mP.C2 , P v
C1 v 9S.Xi , Xi v C2 , Xi u Xj v ?c (1i<jm)
D1 v X, X v D2
R1 R2 v P, P R3 Rk v
P R v S, Q v P
R P v S, Q v P
Disj(R, P ), Q v P
Disj(P, R), Q v P
Ref(Q)

Figure 2: Normalisation SROIQ axioms, C(i) concepts, D(i) non-atomic
concepts dierent ?c >c , X fresh atomic concept, Q
atomic roles, R(i) roles, P fresh atomic role, 2, n 1, k 3.

CNF sk(r) set first-order clauses conservative extension sk(r).
CNF obtained polynomial time using standard structural transformation
(Nonnengart & Weidenbach, 2001). paper consider arbitrary fixed function
507

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

mapping rule r CNF sk(r). function extends ontologies
obvious way, refer (O) clausification O. well-known properties
Skolemisation structural transformation |= (O) |=
ontology first-order sentence Sig(O).
V
W
Let r = ni=1 !
j=1 j clause let 'i = _ 1 n ground
disjunctions atoms single atom; furthermore,
letW general unifier
W
(MGU) , . ground disjunction atoms ni=1 _
j=1 j hyperresolvent
r '1 , . . . , 'n . disjunction empty, case denote . Let
C set clauses, dataset ' disjunction ground atoms. hyperresolution
proof (or simply proof ) ' C [ pair = (T, ) directed, rooted
tree, mapping nodes disjunctions ground atoms
node v following properties satisfied:
1. v root (v) = ',
2. v leaf either ( ! (v)) 2 C (v) 2 D,
3. v children w1 , . . . , wn
(w1 ), . . . , (wn ).

(v) hyperresolvent clause C

support , denoted supp(), set clauses C take part
described properties 2 3 above. write C [ ` ' indicate exists
proof ' C [ D. Hyperresolution sound (if C [ ` ' C [ |= '), complete
following sense: C [ |= ' exists ' C [ ` (Robinson,
1965). particular, C unsatisfiable C [ ` .
Given proofs = (T, ) 0 = (T 0 , 0 ), say embeddable 0
exists mapping : ! 0 satisfying following properties v 2 : (i) v
leaf , (v) leaf 0 ; (ii) w ancestor v (w) ancestor
(v) 0 , (iii) (v) 0 ((v)) [ {?}. Furthermore, given substitution , say
embeddable 0 modulo embeddable proof (T 0 , 0 ),
0 (v) = 0 (v) v 2 0 .


3. Module Extraction
section, recapitulate key notions inseparability relation module
proposed description logic literature (Cuenca Grau et al., 2008; Kontchakov
et al., 2010; Konev et al., 2013; Sattler et al., 2009; Konev et al., 2009). Furthermore,
required, adapt notions setting first-order rules prove basic
results exploited throughout paper.
3.1 Inseparability Relations Modules
Intuitively, given ontology signature , module w.r.t. subset
indistinguishable w.r.t. reasoning tasks predicates
considered interest. indistinguishability criteria depend specific task hand,
usually formalised means inseparability relations.

508

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

Definition 4. inseparability relation family = { | set predicates }
equivalence relations ontologies satisfying following properties:
O0 conservative extension = Sig(O), O0 ;
O1 implies O2 O1 O2 .



first property ensures inseparability stable model-preserving transformations, whereas second one ensures consistent monotonicity
first-order logic. following definition captures common inseparability relations
studied literature.
Definition 5. signature , say ontologies O0
0
0
-model inseparable (O
), every model (resp. ) exists
0

J
model J (resp. O) domain = 2 .

-query inseparable (O q O0 ) Boolean PEQ q dataset
[ |= q O0 [ |= q.
-fact inseparable (O f O0 ) fact
[ |= O0 [ |= .

dataset

-implication inseparable (O O0 ) -rule r form A(x) ! B(x)
|= r O0 |= r.

observations notions introduced Definition 5 order. First,
note definition query fact inseparability quantification queries
datasets relative signature ; standard convention adopted
literature (Lutz & Wolter, 2010; Baader, Bienvenu, Lutz, & Wolter, 2010). Second,
restriction Boolean queries definition query inseparability strictly technical:
obvious extension non-Boolean queries leads equivalent definition. Finally,
observe notion fact inseparability natural generalisation inseparability
w.r.t. atomic instance queries description logics (Lutz & Wolter, 2010).
Example 6. Let us consider ontology Oex Figure 3, serve running
example. Let us also consider signatures fragments Mi Oex given next:
1 = {B, C, D, H}

M1 = {r5 , r6 , r7 , r8 }

3 = {A, C, D, R}

M3 = {r1 , r2 }

2 = {A, B}

M2 = ;

non-tautological 1 -implication entailed Oex D(x) ! H(x), also
follows M1 ; thus, M1 1 -implication inseparable Oex . Furthermore, subset
Oex containing M1 entail D(x) ! H(x) hence 1 -implication
inseparable Oex . see later on, requirement fact inseparability
stronger implication inseparability; indeed, M1 1 -fact inseparable
Oex since, D1 = {B(a), C(a)}, Oex [ D1 |= D(a) M1 [ D1 6|= D(a).
509

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

r1
r2
r3
r4
r5
r6
r7
r8

:
:
:
:
:
:
:
:

A(x) ! 9y1 [R(x, y1 ) ^ B(y1 )]
v 9R.B
A(x) ! R(x, o)
v 9R.{o}
B(x) ^ C(x) ! D(x)
BuC vD
R(x, y) ^ C(y) ! E(x)
9R.C v E
D(x) ! F (x) _ G(x)
DvF tG
F (x) ! 9y2 S(x, y2 )
F v 9S.>c
S(x, y) ! H(x)
9S.>c v H
G(x) ! H(x)
GvH

Figure 3: Example ontology Oex rule DL notation.
checked M2 2 -fact inseparable Oex . is, however, 2 -query
inseparable: D2 = {A(a)}, Oex [ D2 |= 9yB(y) M2 [ D2 6|= 9yB(y).
Finally, consider M3 3 . see later on, M3 3 -query inseparable
Oex ; however, 3 -model inseparable. Indeed, interpretation
= {a, o}, AI = {a}, B = C = {o}, = ; RI = {(a, o)} model .
3
interpretation, however, cannot extended model r3 (or, consequently,
model O) without reinterpreting A, C, R. also see ensure 3 -model
inseparability suffices extend M3 rule r3 .

Model inseparability characterised terms preservation second-order consequences (Konev et al., 2013): ontologies O0 -model inseparable
second-order -sentence ' |= ' O0 |= '. Additionally,
show next, query fact inseparability characterised terms preservation
first-order rules datalog rules, respectively.
Proposition 7. following statements hold signature pair
ontologies O1 O2 :
1. O1 q O2 O1 |= r , O2 |= r holds -rule r non-empty head.

2. O1 f O2 O1 |= r , O2 |= r holds datalog -rule r non-empty head.

q
Proof. prove first statement;
Vnthe second one analogous. Suppose O1 O2
consider arbitrary rule r = i=1 (x) ! 9y (x, y) 6= ,
substitution mapping universally quantified variables r fresh distinct constants.
Furthermore, consider dataset = { }ni=1 , Boolean PEQ q = 9y (x, y)
(note q indeed Boolean PEQ since hypothesis non-empty). Proposition 1,
Oi |= r Oi [ |= q. Together O1 q O2 , implies O1 |= r , O2 |= r.
Assume O1 |= r , O2 |= r holds -rule r non-empty
head. Let
V
q Boolean PEQ -dataset consider rule r =
2D ! q.
Proposition 1 Oi [D |= q Oi |= r, since, assumption, O1 |= r , O2 |= r,
follows O1 [ |= q , O2 [ |= q hence O1 O2 -query inseparable.

immediately follows inseparability relations Definition 5 naturally
ordered strongest weakest non-trivial given next:
q
f


( ( (

510

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

Furthermore, identify classes entailments relevant inseparability relation.
Definition 8. inseparability relation 2 {m, q, f, i}, let relS function
mapping ontology signature set relevant entailments follows:
8
{ | |= second-order -sentence }
=
>
>
<
{ r | |= r r -rule non-empty head }
= q
relS (O, ) =
{
r
|

|=
r

r


datalog
-rule

non-empty
head
}
= f
>
>
:
{ r | |= r r form A(x) ! B(x) A, B 2 } =



following theorem establishes inseparability relations Definition 5
fully characterised preservation relevant -entailments Definition 8.
Theorem 9. Let O0 ontologies, signature, let 2 {m, q, f, i}. Then,
O0 relS (O, ) = relS (O0 , ).
Proof. direct consequence Definitions 5 8, Proposition 7 characterisation
model inseparability terms second-order entailments (Konev et al., 2013).
Inseparability relations allow us formalise modules well desirable properties.
Definition 10. Let ontology, signature, inseparability relation,
let O. say -module M. Furthermore,
minimal M0 ( -module O;
self-contained S[Sig(M) M;
depleting \ ;; strongly depleting \ S[Sig(M) ;.
Finally, define justification sentence |= subset-minimal
O0 O0 |= . say justification-preserving

relS (O, ) justification O0 O0 M.

Example 11. Consider ontologies signatures Example 6. see
Mi module Oex ; particular, M1 i1 -module, M2 f2 -module , M3
q3 -module, M3 [ {r3 }

3 -module.
inseparability requirement ensures modules used instead reasoning purposes, provided entailments relevant application hand
captured given inseparability relation contain symbols .
Minimality ensures module contains little irrelevant information possible
still satisfying inseparability requirement. Although minimality clearly desirable
applications modules (e.g., reasoning small ontology subsets typically
preferable reasoning whole ontology), extracting modules minimal size
invariably hard (and often algorithmically infeasible) (Lutz & Wolter, 2010; Konev
511

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

et al., 2013); thus, practical techniques aim computing modules typically much
smaller O, albeit necessarily minimal.
Self-contained modules inseparable w.r.t. relevant signature
, also w.r.t. signature. Depletingness ensures relevant information
left behind extracting module O, i.e., \ inseparable
empty ontology. basic form depletingness formulated terms ,
whereas stronger variant requires inseparability w.r.t. symbols well. Selfcontained depleting modules especially well-suited ontology reuse modular
ontology development applications. instance, self-contained depleting,
developer remodel sub-domain characterised replacing
new set axioms, guarantee changes performed
unintended interactions rest O.
Justification-preservation enables use modules ontology debugging repair
(Schlobach & Cornet, 2003; Kalyanpur et al., 2007; Kalyanpur, Parsia, Sirin, & Hendler,
2005; Horridge, Parsia, & Sattler, 2008; Kalyanpur, Parsia, Sirin, & Cuenca Grau, 2006).
justification entailment useful form explanation; furthermore, ontology
repair services typically rely computation justifications unintended entailment first step towards obtaining repair plan. Computing justifications, however,
computationally intensive task practical module extraction techniques
eectively exploited optimise process (Suntisrivaraporn et al., 2008).
conclude section briefly discussing impact normalisation module
extraction. pointed Section 2.2, technical results applicable ontologies
consisting rules; referring DL ontologies, implicitly assume given
rule form therefore normalised. argue normalisation techniques stemming
structural transformation preserve inseparability hence possible obtain
module DL ontology module normalisation computed.
Definition 12. normalisation function norm maps SROIQ ontologies normalised
SROIQ ontologies s.t. following holds ontologies domain:
norm(O) conservative extension O;
O1 O2 implies norm(O1 ) norm(O2 ).



Definition 12 captures standard normalisation techniques stemming structural transformation, one discussed Section 2.2. Furthermore, typically
straightforward practice keep track correspondence axioms
original ontology norm(O). shown following proposition,
correspondence allows us efficiently obtain module module norm(O)
computed.
Proposition 13. Let signature, inseparability relation, norm normalisation function. Then, norm(M) norm(O).
Proof. definition, satisfies O1 O2 O1 implies O2 O.
Therefore, norm(M) contains -module norm(O) norm(M) -module
norm(O) itself. hand, since norm(O) (resp. norm(M)) conservative
512

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning


v ?c
v {o}
>c v
{o} v
v B1 B2
A1 u A2 v B
v 9R.B
v 9R.Self
9R.A v B
9R.Self v
v mR.B
R 1 R2 v
RvS
Disj(R, S)
Ref(R)

?-local w.r.t.
2
/
2
/
never
never
2
/
A1 2
/ A2 2
/
2
/
2
/
R 2
/ 2
/
R 2
/
2
/ R 2
/ B 2
/
R1 2
/ R2 2
/
R 2
/
R 2
/ 2
/
never

>-local w.r.t.
never
never
2
/
2
/
B1 2
/ B2 2
/
B 2
/
{R, B} \ = ;
R 2
/
B 2
/
B 2
/
never
2
/
2
/
never
R 2
/

Table 2: Syntactic locality normalised SROIQ axioms
extension (resp. M), asume w.l.o.g. contains symbols
Sig(norm(O)) \ Sig(O), norm(O) (resp. norm(M) M). Since
equivalence relation, follows norm(M) norm(O).
3.2 Syntactic Locality
many inseparability relations introduced Section 3.1, checking whether
module w.r.t. typically high complexity, often undecidable, even
rather lightweight ontology languages (Lutz & Wolter, 2010; Konev et al., 2013).
Consequently, practical module extraction techniques typically based approximations, ensure computed module, yet necessarily minimal
one. One approximation often exploited practice based notion
syntactic locality (Cuenca Grau et al., 2007a; Cuenca Grau, Horrocks, Kazakov, & Sattler,
2007b; Sattler et al., 2009; Cuenca Grau et al., 2008).
Intuitively, normalised SROIQ axiom ?-local (resp. >-local) treating atomic
concepts roles outside ? (resp. >) concept role, respectively, leads
axiom obvious tautology.
Definition 14. normalised SROIQ axiom ?-local (resp. >-local) w.r.t. signature
satisfies conditions given second (resp. third) column Table 2.
normalised SROIQ ontology ?-local (resp. >-local) w.r.t. axioms
?-local (resp. >-local) w.r.t. . Finally, say local w.r.t. either ?-local
>-local w.r.t. .

key properties ?- >-locality, established existing literature (Cuenca
Grau et al., 2008; Sattler et al., 2009), summarised following proposition.
513

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Proposition 15. Let SROIQ ontology, signature x 2 {?, >}.
1. x-local w.r.t.
;.
2. \M x-local w.r.t. [Sig(M), self-contained, strongly
depleting, justification-preserving
-module O.
Property 2 Proposition 15 immediately suggests notion locality-based module.
Definition 16. Let normalised SROIQ ontology, signature, x 2 {?, >}.
x-module w.r.t. , denoted Mx[O,] , smallest subset
\ x-local w.r.t. [ Sig(M).
?> -module w.r.t. least fixpoint sequence {Mi }i 1
M1 = M?
2:
[O,] Mi defined follows
Mi =

(

M>
[Mi
M?
[Mi

1 ,]
1 ,]

odd
even


Example 17. Consider ontology Oex Figure 3 signature = {B, C, D, R}.
>
?>
M?

[O ex ,] = {r3 r8 }, M[O ex ,] = {r1 r3 }, M[O ex ,] = {r3 }.
Locality-based modules Definition 16 computed polynomial time. Furthermore, Proposition 15, self-contained, strongly depleting justificationpreserving. are, however, generally minimal, even amongst strongly depleting
self-contained modules.

4. Overview
provide high-level overview approach module extraction, based
novel reduction reasoning problem datalog. approach builds recent
techniques exploit datalog engines ontology reasoning (Kontchakov, Lutz, Toman,
Wolter, & Zakharyaschev, 2011; Stefanoni, Motik, & Horrocks, 2013; Zhou, Nenov, Cuenca
Grau, & Horrocks, 2014; Zhou, Cuenca Grau, Nenov, Kaminski, & Horrocks, 2015).
connection module extraction datalog first observed (Suntisrivaraporn,
2008), shown ?-module extraction lightweight DL EL+
reduced propositional datalog reasoning.
approach takes connection much farther providing unified framework
supports module extraction arbitrary ontologies consisting first-order rules, well
wide range inseparability relations. Modules obtained using approach
tailored requirements application hand. addition significantly
smaller practice, modules preserve features syntactic locality modules:
widely applicable, efficiently computed practice, satisfy wide
range additional properties.
follows, fix w.l.o.g. arbitrary ontology signature Sig(O).
Unless otherwise stated, definitions theorems parameterised .
stated Section 2, assume rules share variables.
514

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

overall strategy extract module roughly summarised
following steps:1
1. Choose substitution mapping existentially quantified variables fresh
Skolem constants, obtain datalog program P
(a) Skolemising rules using obtain function-free rules ' !
may contain ^ _ head;

,

(b) replacing resulting rules ' ! set { ' ! | atom }
datalog rules; way, disjunctions head rules turned conjunctions split dierent datalog rules.
Clearly, program P logically entails thus preserves consequences.
2. Choose -dataset D0 initial facts compute materialisation P [ D0 .
3. Choose set Dr relevant facts materialisation (possibly containing symbols
outside ), compute supporting rules P 0 P fact.
4. Output subset rules correspond rule P 0 .
subset described fully determined substitution datasets
D0 Dr . main intuition behind module extraction approach pick
, D0 Dr (and hence also M) proof -consequence '
preserved inseparability relation interest embedded collection
proofs P [ D0 relevant fact Dr . way, ensure contains
necessary rules entail '.
Example 18. illustrate strategy might work practice, consider running
example ontology Oex Figure 3 signature = {B, C, D, H}.
Assume goal compute module -implication inseparable
Oex . Recall Example 6 sentence ' = D(x) ! H(x) non-trivial
-implication entailed Oex , therefore requirement |= '.
Furthermore, note proving Oex |= ' amounts proving Oex [ {D(a)} |= H(a)
fresh constant (cf. Proposition 1 Section 2.1).
Figure 4(a) depicts hyperresolution proof showing H(a) derived
D(a) set clauses corresponding r5 r8 , rule r6 transformed
clause r6 = F (x) ! S(x, fyr26 (x)). follows = {r5 r8 } -implication inseparable
Oex since covers support . Moreover, minimal since H(a) cannot
derived subset {r5 r8 }.
approach, take D0 Dr contain, respectively, initial fact D(a)
fact H(a) proved. also make map variables y1 y2 fresh constants cy1
cy2 , respectively. resulting datalog program P shown Figure 5.
Figure 4(b) depicts proofs 0 00 H(a) P [ {D(a)}. support proof 00
datalog program consists rules r500 r8 , stem rules r5 r8 Oex ;
1. simplicity, section overlook certain technical details presence constants
O. thoroughly addressed later on.

515

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

H(a)
r7

H(a)
r7

H(a)
r8

S(a, fyr26 (a)) _ H(a)

S(a, cy2 )

G(a)

r60

r500

F (a) _ H(a)
r8

F (a)

D(a)

F (a) _ G(a)
r5

D(a)

D(a)

0

r6

r50

00



(a)

(b)

Figure 4: Proofs H(a) D(a) (a) Oex (b) corresponding datalog program
r10
r2
r3
r4
r50
r60
r7
r8

:
:
:
:
:
:
:
:

A(x) ! R(x, cy1 )
r100 : A(x) ! B(cy1 )
A(x) ! R(x, o)
B(x) ^ C(x) ! D(x)
R(x, y) ^ C(y) ! E(x)
D(x) ! F (x)
r500 : D(x) ! G(x)
F (x) ! S(x, cy2 )
S(x, y) ! H(x)
G(x) ! H(x)

Figure 5: Datalog program obtained Oex using = {y1 7! cy1 , y2 7! cy2 }
see, however, {r5 , r8 } ( hence entail '. situation
arises consider 0 only, case would recover rules r5 r7 .
datalog program strengthening Oex one particular proof
datalog program may translate back proof original ontology. Indeed,
order compute M, need consider supports 0 00 , case
would successfully recover M.

example, approach would allow us compute minimal module. is,
however, case general: since P strengthening given ontology may
proofs P [ D0 facts Dr correspond proofs -consequence
ontology, may lead inclusion unnecessary rules module.
following sections describe approach formally.
516

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

Section 5, define general notion module setting, captures
degrees freedom framework uniquely specifies datalog program P
module corresponding specific choices , D0 , Dr . Furthermore,
establish key correspondence proofs original ontology sets
proofs P [ D0 , exploit many subsequent technical results.
Section 6, describe concrete module settings inseparability relations introduced Section 3.1, namely implication (Section 6.1), fact (Section 6.2),
query (Section 6.3), model inseparability (Section 6.4) also show
locality ?-modules precisely captured instantiation framework.
Section 7, consider variants inseparability relations Section 3.1 studied
literature, describe specific module settings them. results show
framework easily adapted capture new inseparability relations
hence illustrate generality versatility approach.
Section 8, show modules consistent intuition stronger
inseparability relations lead larger modules. this, introduce notion
homomorphism module settings, allow us establish containment relations modules specified Sections 6 7.
Section 9, study additional properties modules. show
depleting justification-preserving inseparability relations previous sections. modules, however, may strongly depleting self-contained;
although may beneficial, allows us extract smaller modules, properties still important ontology reuse scenarios. Hence, propose technique
ensures extracted modules also strongly depleting self-contained.
Section 10, briefly discuss complexity module extraction within
framework show tractability DL-based ontology languages.
Finally, Section 11, discuss optimality module settings introduced
Sections 6 7. particular, although modules minimal general,
aim determining whether modules obtained settings Sections 6 7
smallest possible within framework.

5. Notion Module Setting
section present framework module extraction. key notion
module setting, captures declarative way main elements approach
discussed Section 4.
Definition 19. module setting tuple

= h, D0 , Dr

substitution mapping constant existentially quantified variable
(possibly fresh) constant;
D0 dataset mentioning predicates ;
517

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Dr dataset mentioning predicates Sig(O) [ {?}.
rule r = '(x) ! 9y (x, y) O, let
(r) = { (' ! ) |
program P

}

defined
P =

support

atom

[

(r)

r2O

set

supp( ) = { r | r 2 supp() proof P [ D0 fact Dr }.
Finally, F = { r 2 | supp( ) \ (r) 6= ; }, module
following subset O:
= F [ F ? [ F > [ F .

defined


mapping datasets D0 Dr constitute degrees freedom
framework, Definition 19 ensures specific choices parameters module
setting fully determine module .
datalog program P obtained applying rule r
time splitting head atoms r dierent rules. application turns
existentially quantified variables (possibly fresh) constants hence transforms
set rules variables universally quantified; additionally, maps
constants occurring (possibly dierent) constants. Since required
injective, possible map existentially quantified variable constant
constant. see next, P strengthening sense
preserves consequences coupled arbitrary dataset. Analogous datalog
strengthenings exploited overestimate reasoning outcomes description logic
ontologies (Krotzsch, Rudolph, & Hitzler, 2008b; Stefanoni et al., 2013; Krotzsch, Rudolph,
& Hitzler, 2008a; Zhou et al., 2014; Zhou, Nenov, Cuenca Grau, & Horrocks, 2013; Zhou,
Cuenca Grau, Horrocks, Wu, & Banerjee, 2013).
support supp( ) collects datalog rules participating proof P [ D0
relevant fact Dr . Intuitively, support captures image module
P . Finally, module consists rules corresponding datalog
rule support supp( ).
Example 20. Let us reconsider Example 18 Section 4, chose map variables
y1 y2 fresh constants cy1 cy2 , whereas D0 Dr contain, respectively, initial
fact D(a) fact H(a) proved. Definition 19 ensures P consists precisely
datalog rules Figure 5. support supp( ) consists rules support
0 00 shown Figure 4. Finally, consists rules r5 r8 , required.

following lemma establishes key correspondence hyperresolution proofs
(the clausification of) sets proofs datalog program P . correspondence already manifest Figure 4 running example. Given arbitrary dataset
518

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

substitution constants constants compatible (i.e.,
coincide intersection
domains), lemma shows proof
Wn
disjunction facts ' = i=1 (O) [ corresponding set proofs
disjunct P [ . implies, particular, P indeed strengthening
O. Furthermore, set support structure preserving: every clause
(r) participating , proof datalog rule (r) support;
finally, proof embeddable hence structure compatible
(cf. Section 2.3).2
Lemma 21. Let module setting let corresponding substitution
. Let dataset arbitrary substitution constants constants
compatible . Finally, let ' (possibly empty) disjunction facts
= (T, ) proof ' (O) [ D. exists non-empty set proofs
P [ satisfying following properties:
1. 0 2 proof
proof .

2 ' [ {?}. Furthermore,

2'

2. r 2 (r) \ supp() 6= ;, either r = ? ! exists 0 2
(r) \ supp(0 ) 6= ;.
3. 0 2 embeddable modulo [ .
Proof. prove results suffices show
(a) 2 ' exists proof 0 P [ embeddable
modulo [ ,
(b) r 2 s.t. (r) \ supp() 6= ;, either r = ? ! exists 2 ' [ {?}
proof 0 P [ embeddable modulo [ ,
(r) \ supp(0 ) 6= ;.
order able reason induction depth , prove
properties hold even ' disjunction (not necessarily function-free) ground atoms.
d=0
supp() = ; ' fact '( [ ) = ' 2 exists trivial
proof 0 P [ '( [ ), clearly embeddable via [ .

supp() 6= ; (O) must contain clause form ( ! '). Since,
assumption, rule empty head ? ! , must case
' 6= 2 ' exists proof 0 P [ = ( [ )
depth 0 supported ( ! ) 2 (r) embeddable modulo [ .
either case, properties satisfied.

2. Lemma 21, well subsequent technical results, prove statements (O)
rather O. cases, consider extension functional terms fyr (t)
occurring (O) mapped whenever domain ; slight abuse notation
sake simplicity, also refer extended substitution .

519

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

d>0
Let = (T, ) v root w1 , . . . , wn children v. Consider clause
2 (O)
V ' hyperresolvent (w1 ), . . . , (wn ). Then, must
form ni=1 i0 ! '0
(wi ) = _ 1 n,
W
' = ni=1 _ '0 MGU ,


0


1 n.

(a) Let 2 '. need find proof 0 = (T 0 , 0 ) ( [ ) P [
embeddable modulo [ .
2 induction hypothesis
find proof.
2 '0 holds
= 0
0 2 '0 . induction hypothesis, proof = (T , ) P [


( [ ) embeddable modulo [ proper subproof .
MGU i0 , = i0 , ( [ )
MGU ( [ ) i0 . Indeed, = i0 implies ( [ ) = ( i0 )( [ );
hand, since eect functional term f (t)
depend t, ( i0 )( [ ) = ( i0 ( [ ))( ( [ )); moreover,
[ extends domain constants O,
0
( i0 ( [ ))( ( [ )) = ( i0 )( ( [ )),
Vnand0 thus 0 ( [ ) = ( )( ( [ )).
hence combine ( i=1 ! ) 2 P , obtain proof
( 0 )( ( [)) = ( 0 )( [) = ( [) P [D clearly also embeddable
modulo [ .

(b) Consider r 2 exists r0 2 (r) \ supp(). need find
2 ' [ {?} proof 0 = (T 0 , 0 ) ( [ ) P [ embeddable
modulo [ rule (r) support.
Assume first r0 = r 6= ? ! . must '0 6= since,
assumption, ? ! rule empty head. pick
V0 2 '0 0 proof ( 0 )( [ ) P [ supported
( ni=1 i0 ! 0 ) 2 (r), saw considering property (a).
Assume r0 6= r 6= ? ! . must r0
supports proof _ subproof . Since depth < d,
i.h. must i00 2 _ [ {?} proof 00i = (T 00 , 00 ) i00 ( [ )
P [ supported rule (r) embeddable .
00
0
00
00
2 [ {?} ' [ {?} = proof looking for. =
(and 6= ?) combine 00 suitable proofs j ( [ )
remaining j (which know exist i.h.), before, construct proof 0
P [ ( [ ) 2 ', embeddable modulo [ .
Lemma 21 Proposition 1 establish datasets D0 Dr chosen
ensure preserves required -consequences.
Suppose required preserve -consequence r = '(x) ! 9y (x, y)
O. Proposition 1, given substitution mapping variables x distinct constants
c, must case [ '(c) |= 9y (c, y). Since P strengthening also
P [ '(c) |= 9y (c, y). Assume choose D0 Dr satisfy
following requirements:
520

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

1. instantiation '(c) body r must embeddable D0 ;
2. set facts materialisation P [ '(c) satisfying instantiation
9y (c, y) head r must embeddable Dr .
completeness hyperresolution, given P datalog program, must
exist fact s.t. |= 9y (c, y) P [ '(c) ` . Lemma 21 Definition 19,
aforementioned requirements D0 Dr suffice guarantee preserve
consequence r.
Example 22. Consider running example Section 4 associated module
setting = h, D0 , Dr given Example 20. see choices D0 = {D(a)}
Dr = {H(a)} satisfy sufficient requirements entail ' = D(x) ! H(x).
First, instantiation body D(x) ' isomorphic (and hence embeddable into)
D0 . Second, materialisation P [ {D(a)} consists facts F (a), S(a, cy2 ) H(a),
latter isomorphic instantiation head '; since chose Dr
consist precisely H(a), second requirement also satisfied.

following theorem makes precise aforementioned sufficient requirements
preserve entailment -rule r. Furthermore, shows whenever entails
r, also contains justifications r original ontology O.
Theorem 23. Let r = '(x) ! 9y (x, y) rule
= h, D0 , Dr module setting
substitution mapping variables x pairwise distinct
constants, exists another substitution compatible
(' ) D0 ,
(( ) ) 0 [ {?} Dr substitution
P [ (' ) |= (( ) ) 0 .

0

mapping variables constants

|= r |= r. Furthermore, O0 justification r O, O0 .
Proof. Since O, follows monotonicity first-order logic |= r whenever
|= r. prove opposite direction implication, suffices show
O0 justification r O, O0 .
= then, minimality O0 , given substitution mapping variables x fresh
distinct constants, must proof (O) [ ' supp() \ (r) 6= ;
r 2 O0 . assumption, exists substitution compatible
(' ) D0 . Lemma 21, r 2 O0 either r = ? ! exists
proof ? P [ (' ) ? supp(? ) \ (r) 6= ;. r = ? ! then, since
assumption rule empty head ? ! , particular must also
case |= '(x) ! ?. suffices show case also |= '(x) ! ?
(as next considering 6= ): then, follows ? 2 Sig(M ) therefore
r = ? ! 2 ontology. r 6= ? ! then, since (' ) D0
? 2 Dr , follows r 2 .
W
6= assume w.l.o.g. = ni=1 > 0 conjunction
atoms. fresh predicate Q, consider ontology
OQ = {

(x, y)

! Q(x) | 1 n }
521

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

immediate that, subset O00 O, O00 |= r O00 [ OQ |= '(x) ! Q(x).
0
0
0
Therefore, minimality O0 , must OQ
Q [ OQ
0 , given substitution
justification '(x) ! Q(x) [ OQ . minimality O0 [ OQ
mapping variables x fresh distinct constants, must proof Q(x)
0 .
(O [ OQ ) [ ' (r) \ supp() 6= ; r 2 O0 [ OQ
assumption, substitution compatible
(' ) D0 . Since OQ contain existentially quantified variables,
constants occur already O, exists module setting Q = hQ , D0Q , DrQ
[ OQ Q = , therefore P Q = P [ OQ . Lemma 21,
0 either = ? ! exists proof 0 P Q [ (' )
2 O0 O0 [ OQ
either (Q(x) ) ? s0 2 Q (s) \ supp(0 ) 6= ;.
= ? ! must proof (O [ OQ ) [ ' disjunction
form ? _ (with possibly empty). Lemma 21 exists proof ? P Q [ (' )
?. Furthermore, since ? mention Q, neither body rule
P Q = P [OQ , proof ? must actually proof P [(' ) . (' )s D0
? 2 Dr , follows supp(? ) supp( ). Hence ? 2 Sig(M ), consequently
s=?!2M .

Otherwise, 0 proof = ? then, before, 0 must proof P [ (' ) .
Then, since (' )s D0 ? 2 Dr , s0 2 supp( ) thus 2 . 0
proof = (Q(x) ) , let 0 = (T, ) v root w1 , . . . , wm children.
0
rule applied top 0 must OQ therefore dierent
Vm .
particular, rule must form (x, y) ! Q(x), (( ) ) = j=1 (wj )
extension y. Clearly, substitution 0 domain
(( ) ) = (( ) ) 0 . rule s0 must thus support proof 0j
P Q [ (' )s (wj ). Since r mention Q neither (wj ),
0 must fact proof P [ (' ) . implies P [ (' )s |= (wj )
hence assumption (wj ) 2 Dr . Finally, since (' ) D0 , s0 2 supp( )
consequently 2 .

6. Modules Inseparability Relation
Theorem 23 tells us choose module setting corresponding module
preserves particular consequence O. However, order -module
given inseparability relation S, must preserve one, (possibly
infinitely many) relevant consequences relS (O, ) (recall Theorem 9 Section 3).
section consider inseparability relation 2 {m, q, f, i}, formulate
specific module setting provably yields -module O. Later Section 11
consider optimality module settingsthat is, whether may exist
dierent setting yields smaller module relevant inseparability relation.
6.1 Implication Inseparability
running example immediately suggests natural module setting
guarantees implication inseparability.
522



= hi , D0i , Dri

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

example, pick substitution general possible Skolemising existentially quantified variable distinct fresh constant mapping constants
occurring themselves. pick D0i Dri rely application sufficient
conditions established Theorem 23 (quadratically many) -implications
A(x) ! B(x) x = (x1 , . . . , xn ). precisely, capture instantiations
body A(x) define D0 contain fact A(c1A , . . . , cnA ) involving fresh constants ciA uniquely
associated predicate A; furthermore, capture head implication, define
Dr contain fact B(c1A , . . . , cnA ). way, dataset D0i contains linearly many
Dri quadratically many facts size signature .
Definition 24. existentially quantified variable O, let cy fresh constant.
Furthermore, 2 arity n, let cA = (c1A , . . . , cnA ) array fresh constants.
module setting = hi , D0i , Dri defined follows:
= { 7! cy | existentially quantified } [ { c 7! c | c 2 Ct(O) },
D0i = { A(cA ) | 2 };
Dri = { B(cA ) | 6= B predicates arity } [ {?}.



module setting reminiscent datalog encodings typically used check
whether concept subsumed another concept B w.r.t. lightweight ontology
(Krotzsch et al., 2008b; Stefanoni et al., 2013). There, existentially quantified variables
rules also skolemised fresh constants produce datalog program P,
checked whether P [ {A(a)} |= B(a).
module setting captures implication inseparability straightforward consequence Theorem 23.
Theorem 25. O.
Proof. Consider arbitrary rule form A(x) ! B(x) x = (x1 , . . . , xn ) vector
distinct variables A, B distinct n-ary predicates . Let substitution
mapping x1 , . . . , xn distinct constants c1 , . . . , cn , another substitution
ci = ciA . definition (A(x) ) 2 D0i (B(x) ) 2 Dri , thus,
Theorem 23, follows |= A(x) ! B(x) |= A(x) ! B(x).
6.2 Fact Inseparability
Theorem 9, fact inseparability requires preservation datalog -rules entailed
O. Thus, contrast implication inseparability, may require preservation
large (and possibly even infinite) set entailments. Unsurprisingly, module setting
cannot used capture fact inseparability illustrated following example.
Example 26. Consider Oex 1 = {B, C, D, H}, = {r5 r8 }. seen
Example 18, = {B(a), C(a)} case Oex [ |= D(a), also
[ 6|= D(a); hence, 1 -fact inseparable Oex .
Equivalently, Oex entails datalog rule r3 = B(x) ^ C(x) ! D(x), whereas
hence Theorem 23 longer applicable since D, instantiates body
r3 , cannot embedded D0i = {B(cB ), C(cC ), D(cD ), H(cH )}.

523

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Thus, next define suitable module setting f = hf , D0f , Drf capture fact
inseparability. previous case, exploit sufficient conditions given
Theorem 23. end, first need make sure D0f (resp. Drf ) captures possible
body (resp. head) instantiations possible datalog rules may entailed
O. achieve choosing D0f Drf constrained -dataset
possible, typically referred literature critical dataset (Marnette,
2009; Cuenca Grau, Horrocks, Krotzsch, Kupke, Magka, Motik, & Wang, 2013).
Definition 27. Let signature let fresh constant. critical -dataset
defined follows:
n

z }| {


= { A(, . . . , ) | n-ary predicate }



Indeed, straightforward see every -dataset (and hence datalog rule
mapping every constant .
instantiation) embedded
case , choose substitution f general possible mapping
existentially quantified variables distinct fresh constants. However, contrast ,
require constants occurring mapped rather themselves.
choice justified following example.
Example 28. Consider Oex = {A, C, E}. Clearly, = {A(a), C(o)}
Oex [ |= E(a) due rules r2 r4 Oex . pick f , maps
constant Oex itself, would obtain f = ; even choose D0f Drf
critical -dataset. Indeed, relevant fact E() would provable P f [ D0f .
ready define

f

formally.

Definition 29. Let constants cy Definition 24, let fresh constant.
module setting f = hf , D0f , Drf defined follows:
f = { 7! cy | existentially quantified } [ { c 7! | c 2 Ct(O) },
,
D0f =
[ {?}.
Drf =



Example 30. datalog program generated f Oex coincides Figure 5
rules except r2 , becomes
r20 : A(x) ! R(x, )
consider 1 = {B, C, D, H} = {B(a), C(a)} Example 26, clearly
P f [D0f ` D() 2 Drf since {B(), C()} D0f . unique proof D() P f [D0f
supported r3 ; guarantees r3 2 f thus corresponds directly
proof D(a) f [ D. Hence, f [ |= D(a), required.
consider signature = {A, C, E} Example 28, observe
Figure 6 choice mapping constant ensures module contains
necessary rules r2 r4 .

524

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

E()
r4

E(a)
r4
R(a, c)
r2

R(, )

C(o)

C()

r20

A(a)

A()
(a)

(b)

Figure 6: Proofs (a) E(a) Oex [ {A(a), C(o)} (b) E() P
exploit Theorem 23 show
Theorem 31.

f

f

f

[ D0f

captures fact inseparability.

f O.

Proof. Let r = ' !
datalog rule . Let
substitution mapping
variables r pairwise distinct constants, substitution mapping constant
range . definition f (' ) D0f ( ) Drf thus,
Theorem 23 follows |= r f |= r. Finally, Proposition 7, implies
f f O.
6.3 Query Inseparability
Positive existential queries constitute much richer query language facts allow
existentially quantified variables. Thus, query inseparability requirement inevitably
leads larger modules.
Example 32. Consider Oex = {A, B}. Given -dataset = {A(a)}
-query q = 9yB(y), Oex [ |= q (due rule r1 ). case, however, f
empty thus f [ 6|= q. Indeed, additional facts materialisation
P f [ {A(), B()} R(, cy1 ) B(cy1 ), hence neither r10 r100 (cf. Figure 5)
constrained enough
supp( f ). suggests that, although critical -dataset
embed every -dataset, may need consider additional relevant facts capture
proofs -queries. particular, rule r1 implies B non-empty extension
whenever does: dependency checked q. captured
considering fact B(cy1 ) relevant, case r1 would included module.
Theorem 9, query inseparability requires preservation -rules entailed
(and datalog). particular, first-order rules may involve
existentially quantified variables, correspond framework Skolem constants.
naturally suggests module setting q diers f -facts involving
Skolem constants (and mentioning ) also considered relevant.
Definition 33. Let constants cy Definition 29. define module setting
q
q
q
q = h , D0 , Dr follows:
q = f ,
525

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

B(fyr11 (a))

B(cy1 )
r100

r1,2
A(a)

A()

(a)

(b)

Figure 7: Proofs (a) B(fyr11 (a)) Oex [ {A(a)} (b) B(cy1 ) P

q

[ D0q

,
D0q =

Drq = { A(a1 , . . . , ) | 2 , aj either cy } [ {?}.



Example 34. Coming back Example 32, observe Figure 7 proof
B(fyr11 (a)) (Oex )[{A(a)} supported r1,2 : A(x) ! B(fyr11 (x)) recovered
proof B(cy1 ) P q [ D0q supported r100 . definition q ensures
B(cy1 ) 2 Drq , hence r1 2 q .

Theorem 35.

q

q O.

Proof. Let r = ' ! 9y rule non-empty head. Let substitution
mapping variables r pairwise distinct constants substitution maps
0 Dq
constant range . definition q (' ) D0q , also
r
substitution 0 mapping variables r constants Ct(D0q [ Drq ) [ range(q ).
Thus, Theorem 23, follows |= r q |= r. Proposition 7, implies
q q O.
6.4 Model Inseparability
Model inseparability diers substantially previous inseparability relations:
rather preservation rule-shaped consequences, requires preservation
models (and hence second-order consequences). Theorem 23, repeatedly
exploited show modules preserve required entailments, relies properties hyperresolution (a first-order logic calculus); hence, applicable show
preservation second-order logic consequences. particular, following example
illustrates, modules generated q may -model inseparable O.
Example 36. Consider Oex = {A, C, D, R}, case q = {r1 , r2 }.
saw Example 6, interpretation = {a, o}, AI = {a}, B = C = {o},
DI = ; RI = {(a, o)} model q ; however, readily checked
cannot extended model without reinterpreting A, C, R.

Intuitively, constructing model O, fixing interpretation certain predicates restricts ways remaining predicates interpreted. restrictions obviously determined dependencies introduced rules O.
capture model inseparability, need ensure preserves relevant dependencies predicates . this, pick D0 way model
526

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

R(, )

R(, )

A()

A()

r1

D()
r3

r20

B()

C()

r1
A()
Figure 8: proofs P



[ D0m facts Drm Example 39.

embedded materialisation P [ D0 ; turn, choose Dr
way captures -reducts models. case, proofs
P [ D0 facts Dr capture dependencies predicates .
Example 37. Note Example 36 cannot embedded materialisation
P q [ D0q since facts {B, C} contains C() B(cy1 ),
constant cannot mapped cy1 .

; now, contrast
capture models O, pick D0 =
q,
choose substitution maps existentially quantified variables constants
. Furthermore, ensure Dr captures -reducts models,
well.
pick Dr

Definition 38. module setting



= hm , D0m , Drm follows:

= { 7! | existentially quantified } [ { c 7! | c 2 Ct(O) },
,
D0m =
[ {?}.
Drm =



Example 39. Consider Oex , Example 36. substitution maps
existentially quantified variables r1 r6 . Thus, rules r1 r6 Oex correspond
following rules P :
r1
r6

r1 : A(x) ! R(x, ), r1 : A(x) ! B()
r6 : F (x) ! S(x, )

materialisation P [ D0m contains facts A(), C(), D(), R(, ), B(). Consequently, possible embed interpretation aforementioned materialisation mapping .
Figure 8 shows (non-trivial) proofs P [ D0m facts Drm . observe
module consists rules r1 r3 . Clearly, model extended
model Oex since predicates occur head rule Oex \M .
Theorem 40 shows module -model inseparable O. Indeed, every
model extended model following way: (i) predicates
occurring materialisation P [ D0m interpreted empty, (ii) predicates
support (and hence occurring ) interpreted I, (iii)
predicates arity n interpreted ( )n .
527

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Theorem 40.




O.

Proof. Let model . W.l.o.g., assume defined Sig(O).
Let J interpretation domain I,
8
2 [ Sig(supp( ))
<
J
arity(A)

=
2 Sig(P (D0m )) \ ( [ Sig(supp( )))
:
;
otherwise

Note [ Sig(supp( )) Sig(P (D0m )).
Consider r : ' ! 2 O. show J |= r.
Assume first = . r = ? ! need check ? interpreted
false. ? 2
/ Sig(P (D0m )) case definition J . ? 2 Sig(P (D0m ))
then, since ? 2 Drm , must ? 2 Sig(supp( )) therefore also ? 2 Sig(M ).
implies ? ! 2 thus, since model , must case
? interpreted false. Since ? 2 Sig(supp( )), definition J
?J ?I false.
Assume 6= . Sig( ) 6 Sig(P (D0m )) then, P [ D0m mentions
one constant (namely, ), follows also Sig(') 6 Sig(P (D0m )) therefore 'J = ;
J |= r. Hence, following assume Sig( ) Sig(P (D0m )).
Sig( ) \ ( [ Sig(supp( ))) = ;, AJ = arity(A) 2 Sig( )
immediate J |= r. Otherwise suppose exists substitution
variables r J |= ' (if substitution exists J |= r holds trivially).
'J 6= ; must Sig(') Sig(P (D0m )). Let substitution maps
variables ; constant P [D0m , folows ' P (D0m )
(D ).
thus also
P
0
assumption, exists = A(, . . . , ) 2
2 [Sig(supp( )).
' P (D0m ), proof ,r P [ D0m supported rule
(r). 2 then, definition , 2 Dr consequently r 2 .
If, hand, 2
/ , must 0 2 Drm proof 0 0 P [ D0m
proof A(, . . . , ) subproof 0 . Replacing subproof A,r
results another proof 0 P [ D0m supported rule (r). Thus
r 2 case well. Rules (r) body r, r 2 implies
Sig(') Sig(supp( )), thus J agree Sig('). assumption, J |= ' ,
(D ),
|= ' well, and, since r 2 , also |=
. Finally, since
P
0


J

Sig( ) Sig(P (D0 )) therefore

, J |=
. Since
arbitrary, conclude J |= r.
modules generated similar spirit locality-based modules
certain symbols outside signature module interpreted either empty set
universal relation (of relevant arity) interpretation domain.
show later on, M?
[O,] whenever normalised SROIQ ontology. However,
illustrated following example, modules incomparable >- ?> -modules.
Example 41. = Oex = {D, F } following:




= {r5 }

M>
[O,] = {r1 -r3 }
528



M?>
[O,] = ;.

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning



?>
Consequently, neither contained M>
[O,] , M[O,] . see converse
also true, consider = {r1 , r9 }, r9 = C(x) ! B(x), = {A, C, R}. Then,
following:







?>
M>
[O,] = M[O,] =

= {r1 }



already mentioned modules generated included locality
?-modules. conclude section, show modified precisely capture
?-modules. this, suffices modify making Dr critical dataset
entire signature (instead ) given following definition.
Definition 42. module setting

b

= hb , D0b , Drb follows:

b = ,
,
D0b =

Drb = DSig(O)
[ {?}.



following proposition shows
SROIQ ontology relative .

b

coincides ?-locality module

Proposition 43. normalised SROIQ ontology,

b

= M?
[O,] .

Proof. definition, M?
[O,] smallest subset every axiom \
b
?-local w.r.t. [ Sig(M). show M?
[O,] , suffices show that,
r 2 \ b , r ?-local w.r.t. [ Sig(M b ). Consider r 2 \ b . Since Drb contains
facts occur P b (D0b ), supp( b ) consists rules support
proof P b [D0b . Furthermore, Sig(M b )[ = Sig(P b (D0b )). Therefore,
proof P b [ D0b rule b (r) support.
constant mentioned P b [ D0b , means predicate body
r occur Sig(P b (D0b )) = Sig(supp( b )) [ . observed
Tables 1 2, implies r ?-local w.r.t. Sig(supp( b )) [ .
0
b
b
see b M?
[O,] , consider r 2 . must exist r 2 (r)
0
b
?
r 2 supp() proof P b [ D0 . show r 2 M[O,] , let us reason
induction depth .
= 0 body r empty thus r ?-local w.r.t. signature.
follows r 2 M?
[O,] .

> 0 suffices consider case r0 rule applied top , since
already know induction hypothesis that, 2 rule
b (s) support (proper) subproof , 2 M?
[O,] . Since
b
Sig(D0 ) = , implies that, = (T, ) v root w1 , . . . , wn
children, Sig( (wi )) Sig(M?
[O,] ) [ . Consequently, body r
must signature fully contained Sig(M?
[O,] ) [ . follows r
?
?-local w.r.t. Sig(M[O,] ) [ hence r 2 M?
[O,] .
529

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

7. Additional Inseparability Relations
bulk research module extraction focused inseparability relations
considered Section 6. section show framework seamlessly
adapted interesting inseparability relations.
7.1 Classification Inseparability
Classificationthe problem identifying subsumption relationships pairs
atomic concepts pairs atomic roles DL ontologyis fundamental reasoning
task ontology engineering. Classifying first-order ontology amounts computing,
predicate O, entailed implications A(x) ! B(x) B predicate
arity A, referred subsumer O.
Locality ?-modules successfully exploited optimising classification DL
ontologies (Tsarkov & Palmisano, 2012; Cuenca Grau et al., 2010; Armas Romero et al.,
2012). addition model-inseparable given ontology satisfying
properties Proposition 15 Section 3.2, ?-modules enjoy additional property
makes well-suited optimising classification (Cuenca Grau et al., 2007a, 2010):
Proposition 44. Let ontology, signature, r rule form A(x) !
2 either = ? = B(x) B 2 Sig(O). Then, |= r M?
[O,] |= r.
follows Proposition 44 ?-module = {A} captures subsumers O, hence indistinguishable w.r.t. implications
body. capture additional property ?-modules means
following inseparability relation.
Definition 45. Ontologies O0 -classification inseparable (O c O0 )
rule r form A(x) ! , 2 either
=
= B(x)
B 2 Sig(O [ O0 ), |= r O0 |= r. Furthermore, relc function mapping
ontology signature following set rules:
relc (O, ) = {r=A(x) !

| |= r, 2

=?

=B(x) B 2 Sig(O)}



follows straightforwardly definition c O0 holds relc (O, ) coincides relc (O0 , ); hence, Theorem 9 Section 3 trivially extends classification
inseparability. Furthermore, readily checked c ( non-trivial
signature , hence classification inseparability (as expected) stronger requirement
implication inseparability. Finally, although ?-modules ensure classification inseparability O, also model-inseparable (a much stricter requirement) and, shown
Section 13, much larger necessary ontology classification.
Example 46. Consider example ontology Oex . observe classification inseparability stronger requirement implication inseparability considering = {G}.
Clearly, = ; -implication inseparable Oex , whereas -classification inseparability requires rule r8 contained M. see ?-modules dier minimial classification-inseparable modules consider = {A}; case,
ex therefore empty ontology
M?
[O ex ,] = {r1 , r2 }, subsumers
ex
already -classification inseparable .

530

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

following module setting extends Definition 24 capture classification
inseparability. one would naturally expect, required modification extend
Dri facts involving predicates outside .
Definition 47. 2 arity n, let cA = (c1A , . . . , cnA ) array fresh
constants. module setting c = (c , D0c , Drc ) defined follows:
c = ,
D0c = D0i ,
Drc = { B(cA ) | 2 B 2 Sig(O) distinct predicates arity } [ {?}.

Example 48. Consider = Oex = {A}. Since fact Drc provable
P c [ D0c module c empty thus also minimal case.

show c captures implication inseparability using argument analogous
proof Theorem 25.
Theorem 49.

c

c O.

7.2 Weak Query Inseparability
One possible applications modules based query inseparability optimise
query answering. particular, -query inseparable O, [ |= q
M[D |= q -query q -dataset D; thus, replace answer
arbitrary query w.r.t. arbitrary data provided symbols deemed relevant.
aforementioned notion query inseparability useful situations
data unknown frequently changing, many situations data ontology
considered fixed hence one could potentially extract smaller modules requiring
robust extensions arbitrary data.
Botoeva, Kontchakov, Ryzhikov, Wolter, Zakharyaschev (2014) investigated restricted notion query inseparability well-suited cases data
ontology considered fixed. paper, refer restricted notion
weak query inseparability.
0
Definition 50. Ontologies O0 -weak query inseparable (O wq
)
Boolean PEQ q |= q O0 |= q. Furthermore, relwq function
mapping ontology signature set

relwq (O, ) = { q | |= q q Boolean PEQ Sig(q) }



0
Again, Theorem 9 extends naturally weak query inseparability; indeed, wq

relwq (O, ) coincides relwq (O0 , ). Moreover, requirements Definition 50 weaker query inseparability hence q ( wq
.
result, weak query inseparability may yield smaller modules, show Section 13.
next propose module setting wq captures weak query inseparability.
main dierence wq q Section 6.3 initial dataset D0wq chosen

531

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

. natural choice given longer requirement
empty rather
arbitrary -datasets must embeddable D0wq . Furthermore, wq diers q
maps constants Ct(O) (same ).

Definition 51. Let constants cy , existentially quantified variable O,
Definition 24. module setting wq = hwq , D0wq , Drwq defined follows:
wq = ,
D0wq = ;,
Drwq = { A(a1 , . . . , ) | 2 , aj either Ct(O) equals cy } [ {?}.



Example 52. Consider extension Oex fact ! D(i) (seen ground rule)
signature = {B, C, D, H}. readily checked q = {r3 , r5 r8 },
whereas wq = {r5 r8 }.

Theorem 53.

wq

wq
O.

Proof. Boolean PEQ q seen rule ( ! q). Consequently, wq wq

follows Theorem 23 similar argument proof Theorem 35.

8. Module Containment
Intuitively, expressive language preservation consequences
required, larger modules need be. instance, since f ( ,
expected module obtained implication inseparability contained
module f fact inseparability. next show modules Sections 6 7
consistent intuition.
first step introduce notion homomorphism module settings,
allow us establish containment corresponding modules.
Definition 54. module setting
= h, D0 , Dr i, let Ct( ) denote set constants occurring D0 , Dr , range . substitution : Ct( ) ! Ct( 0 )
homomorphism 0 following conditions hold:
= 0 ;
D0 D00 ;
Dr Dr0 .
write

,!

0

denote homomorphism



0

exists.



fact ,! 0 (witnessed homomorphism ) implies 0
general , sense proof P [ D0 fact Dr embedded
0
(via ) support-preserving way proof P [ D00 fact Dr0 . follows
0
supp( ) contained supp( 0 ) (modulo ) hence also .
Theorem 55. ,

0

s.t.

,!

0,

0

.
532

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

Proof. Let = h, D0 , Dr 0 = h0 , D00 , Dr0 i, let : Ct( ) ! Ct( 0 ) homomorphism 0 . Since Dr Dr0 , suffices show rule r 2
proof P [ D0 fact 2 Dr supp() \ (r) 6= ;, exists proof 0
0
0
P [ D00 supp(0 ) \ (r) 6= ;. prove following general claim.
Let r rule proof fact (not necessarily Dr ) P [ D0
supp() \ (r) 6= ;. show induction depth proof
0
0
0 P [ D00 supp(0 ) \ (r) 6= ;.
d=0
Since, assumption, supp() \ (r) 6= ;, r = ( ! ) ( ! ) 2 (r)
0
= 2 , also ( ! 0 ) 2 (r). Since defined
constants O, holds ( ) = () = 0 , hence proof
0
0
P [ D00 supported rule (r).
d>0
Let = (T, ) v root w1 , . . . , wn children v. Let
rule used derive (v) (w1 ), . . . , (wn ). Finally, let
subproof (wi ). supp(i ) \ (r) 6= ;, claim follows
0
induction hypothesis since 2 P . Otherwise, 2 (r). Moreover,
0
induction hypothesis, every (wi ) proof P [ D00 , claim follows
0
since 2 (r).
straightforward construct homomorphisms module settings Sections 6 7 accordance containment relationship corresponding inseparability relations. following result, establishes intuitive relationships
modules, follows immediately Theorem 55.
Corollary 56.


f






wq

q
c











b

b

q

already illustrated examples throughout Sections 6 7, containment
relations strict many . Furthermore, easily checked
containment relations complete, sense module settings unrelated
Corollary 56 (e.g., f wq ) incomparable.

9. Depletingness, Self-Containment, Justification-Preservation
minimal requirement module preserve relevant consequences w.r.t.
given inseparability relation. argued Section 3, however, applications
desirable modules satisfy additional properties. section, establish whether
modules Sections 6 7 satisfy (strong) depletingness, self-containment,
justification-preservation properties enjoyed locality-based modules.
establish results, convenient abstract away notion module
setting fixed consider instead families module settings; is,
functions assign module setting pair .
533

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Definition 57. module setting family function
maps pair ontology
signature module setting . Given inseparability relation
S, say
S-admissible if, pair , (O,) -module
O. Furthermore, say
depleting (resp. strongly depleting, self-contained,
(O,)
justification-preserving)
.
Finally, 2 {m, q, f, i, c, wq}, denote (S-admissible) family
induced module setting defined Sections 6 7.

9.1 Depletingness Justification-Preservation
discussed Section 3, depletingness ensures \ inseparable
empty ontology hence relevant information left behind extracting M.
illustrated following example, modules depleting.
Example 58. Consider consisting following rules let = {A, B}:
s1 = A(x) ! B(x) ^ C(x)

s2 = A(x) ! D(x) ^ E(x)

s3 = D(x) ! B(x)

Clearly, M1 = {s1 } M2 = {s2 , s3 } implication-inseparable hence
-modules. However, neither depleting.

next show modules defined Sections 6 7 depleting.
Proposition 59.



depleting 2 {m, q, f, i, c, wq}.

Proof. Let arbitrary let = (O, ). Theorems 25, 31, 35, 40,
49 53, suffices show (O\M, ) = ;. definition module (cf. Definition 19), holds (O\M, ) O\M. Furthermore, since O\M O, follows
(O\M, ) M. Consequently, (O\M, ) = ;.
next show modules also justification-preserving hence seamlessly exploited ontology debugging applications.
Proposition 60.



justification-preserving 2 {m, q, f, i, c, wq}.

Proof. Let 2 relS (O, ) let O0 justification O. need check

O0 (O,) . Since 2 relS (O, ) O0 |= , follows definition relS

0
2 relS (O0 , ). Theorems 25, 31, 35, 40, 49 53, (O ,) O0 ,

0
therefore (O ,) |= . definition module (cf. Definition 19) immediate

0

O0 implies (O ,) (O,) . Finally, minimality O0 ,


0
(O ,) = O0 therefore O0 (O,) .
conclude addressing eect normalisation properties. Similarly
treatment normalisation Proposition 13 Section 3, show
recover depleting justification-preserving module SROIQ ontology one
module normalisation norm(O).
Proposition 61. Let inseparability relation, let
module setting family
S-admissible depleting. Let norm normalisation function. Let
SROIQ ontology let following holds:
534

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

1.

(norm(O),)

norm(M)

2. norm(O\M) norm(O)\norm(M).

Then, depleting justification-preserving -module O.

Proof. Let O0 = norm(O). Proposition 13 implies -module O0 .
0
next show depleting. Since (O ,) norm(M), O0 \norm(M)
0
0
O0 \M (O ,) . Proposition 59 implies O0 \M (O ,) ;, hence monotonicity
first-order logic also O0 \norm(M) = norm(O\M) ;. Since norm(O\M) conservative
extension O\M, Definition 4 norm(O\M) O\M, thus O\M ;.
show justification-preserving, let ' 2 relS (O, ) consider justification ' O. Suppose 2 O\M. 2 O\M
0
norm() norm(O\M) = O0 \norm(M). Since (O ,) norm(M), implies
0
norm() \ (O ,) = ;. hand, since norm(O) conservative extension
O, ' 2 relS (norm(O), ). Also, norm(O) conservative extension
O, norm(O) |= ' must justification O0 ' norm(O) O0 .
0
0
Proposition 60, (O ,) justification-preserving, consequently O0 (O ,) .
Furthermore, minimality proper subset whose normalisation in0
cludes O0 . follows norm() \ O0 6= ; hence norm() \ (O ,) 6= ;.
contradiction stems assuming 2 \ M. Therefore M, i.e.,
justification-preserving.
9.2 Self-Containment Strong Depletingness
contrast locality-based modules, modules obtained using approach neither
strongly depleting, self-contained. see this, consider following example.
Example 62. Let = {A, D} = {r10 r15 }
r10
r11
r12
r13
r14
r15

=
=
=
=
=
=

( ! A(o))
A(x) ! 9y.[R(x, y) ^ B(y)]
A(x) ! 9y.[R(x, y) ^ C(y)]
R(x, y) ! D(x)
B(x) ! C(x)
( ! C(i))

Let M1 = {r10 r13 } M2 = {r11 r13 }. check


f

=M

=M

q
c

=M



= M2

=M

wq

= M1

Clearly, |= C(i) wq 6|= C(i) C signature M1 ; hence, wq
self-contained. Furthermore, wq strongly depleting since \M wq |= C(i).
remaining inseparability relations, observe |= B(x) ! C(x) Mi 6|= B(x) ! C(x)
1 2. Since B C signatures M1 M2 follows
none modules self-contained (note implications relevant consequences
relations wq). Furthermore, since also \ Mi |= B(x) ! C(x),
conclude modules also strongly depleting.

535

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Self-containment strong depletingness always needed applications. Thus,
fact modules satisfy default beneficial may allow
us compute smaller modules.
However, mentioned Section 3, properties useful certain ontology
reuse scenarios. next show framework adapted satisfy
properties whenever required. achieved via fixpoint construction
modules computed w.r.t. iterative extensions initial signature. fixpoint
constructions reminiscent standard algorithms computing locality modules
(Cuenca Grau et al., 2007a, 2008).
Definition 63. Let module setting family inseparability relation S.
define family Sself function mapping least fixpoint
sequence {Mi }i 0 defined next:
0 =
=

1

[ Sig(Mi

1)

Mi =

> 0

(O,

i)



0


aforementioned fixpoint well-defined: since [ Sig(O) 0
[ Sig(O) finite, must i0 0 i0 = j Mi0 = Mj
j > i0 . show that, adaptation, modules satisfy required properties.
Proposition 64.


self

self-contained strongly depleting 2 {m, q, f, i, c, wq}.

Proof. Let = Sself (O, ). immediate self-contained -module O.
Strong depletingness follows Proposition 59 2 {m, q, f, i, c, wq}.
construction Definition 63 straightforwardly adapted case nonnormalised SROIQ ontologies following approach Proposition 61.

10. Complexity Module Extraction
section, argue modules efficiently computed many practically
relevant cases. this, analyse complexity following decision problem.
Definition 65. Let L class ontologies let 2 {m, q, f, i, c, b, wq} inseparability relation. decision problem isInModule[L,S] follows:
Input: ontology 2 L, signature rule r 2 O.
Output: True r 2

(O,)

.



Furthermore, consider following classes ontologies, strongly connected DL-based ontology languages.
Definition 66. Let k fixed non-negative integer. class Lkarity consists
ontologies predicates arity k.
graph conjunction atoms ' undirected graph G' = (V, E) V
set variables occurring ', E contains edge pair variables
536

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

occur together atom '. tree decomposition G' tree = (W, F ),
exists labelling mapping vertex w 2 W subset (w) V ,
following conditions satisfied:
v 2 V , exists v 2 W v 2 (w),
{v, v 0 } 2 E, exists w 2 W {v, v 0 } (w),
v 2 V , set { w 2 W | v 2 (w) } induces (connected) subtree .
width tree decomposition maxw2W (| (w)| 1). treewidth '
minimum width tree decompositions G' . treewidth rule defined
treewidth body. Finally, class Lktw consists ontologies rule
treewidth k.

rules correspong SROIQ ontologies fixed predicate arity,
also bodies tree-shaped (see Section 2.2). latter implies rules stemming
SROIQ ontologies treewidth one.
already discussed, appealing feature approach module extraction
delegated o-the-shelf datalog reasoner, regardless language
ontologies expressed. following proposition establishes datalog program initial dataset exploited approach polynomial size; furthermore,
datalog transformation definition module setting (see Definition 19
Section 5) alter shape rules original ontology significant way.
Proposition 67. Let ontology Sig(O) signature. Furthermore, let
2 {m, q, f, i, c, b, wq} (O, ) = = h, D0 , Dr i. Then, P D0 size linear
|O|. Furthermore, Lkarity (resp. Lktw ) fixed k, P .
Proof. clear Definition 5 (r) contains datalog rule atom
head r. Thus, P clearly size linear w.r.t. |O|. Furthermore, D0 contains one fact
predicate , since Sig(O), follows D0 also size linear w.r.t.
|O|. Finally, transformation increase arity predicates body
rule (r) coincides r hence preserves treewidth.
computational properties datalog programs bounded arity and/or treewidth
well-understood: fact entailment NP-complete combined complexity programs
bounded arity, complexity drops PTime additionally restrict
programs bounded treewidth. Bounded arity predicates implies corresponding
materialisation polynomially bounded size, thus computed polynomial
number steps (i.e., applications immediate consequence operator); furthermore,
bounded treewidth rule bodies implies step performed polynomial
time (Grohe, Schwentick, & Segoufin, 2001; Chekuri & Rajaraman, 2000).
complexity module extraction, however, determined datalog
reasoning, also complexity computing support proofs involved
relevant entailment.
following theorem, proved Zhou et al. (2014), establishes computing
support also reduced standard datalog reasoning. Given program P, dataset
537

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

D, set facts F , main idea extend P additional rules
facts responsible computing support proofs facts F P [ D.
support recorded means fresh predicates: auxiliary predicates Q used
record relevant facts form Q(c); furthermore, rule r 2 P represented
fresh constant dr , fresh unary predicate Rel used capture relevant rules
P support.
Theorem 68 (Zhou et al., 2014). Let P datalog program, let dataset,
let F set facts materialisation P [ D. Let Rel fresh unary predicate
and, predicate Q occurring P [ D, let Q fresh predicate arity.
Furthermore, let dr fresh constant r 2 P.
Let (D, F ) dataset (D, F ) = [ { P (c) | P (c) 2 F } let (P)
smallest
V datalog program including P containing following rules
r=
j=1 B1 (xj ) ! H(x) P:
H(x) ^ B1 (x1 ) ^ . . . ^ Bm (xm ) !
H(x) ^ B1 (x1 ) ^ . . . ^ Bm (xm ) !

Rel(dr )
Bj (x1 )
..
.

H(x) ^ B1 (x1 ) ^ . . . ^ Bm (xm ) ! Bj (xm )
Then, rule 2 P support proof P [ fact F Rel(ds )
materialisation (P) [ (D, F ).
datalog program (P) (resp. dataset (D, F )) Theorem 68 size polynomial |P| (resp. |D| |F |) use predicates arity greater
predicates used P. However, (P) may larger treewidth P. next
argue case SROIQ ontologies increase treewidth bounded.
Proposition 69. Let normalised SROIQ ontology Sig(O) signature.
Furthermore, let module setting . (P ) treewidth 2.
Proof. rule r 2 whose head formed (one more) atoms
unary, mention one variable, straightforward ( (r)) still
consists rules tree-shaped bodies. r 2 mention
two variables, also straightforward ( (r)) also consists rules
tree-shaped bodies. Finally, r mentions two variables, head contains
atoms mention one variable, r must one following forms:
V
W
A(x) ^ m+1
i=1 [R(x, yi ) ^ B(yi )] !
i6=j yi yj
Then, bodies rules ( (r)) form
A(x) ^

m+1
^
i=1

[R(x, yi ) ^ B(yi )] ^ yi1 yi2

1 1 < i2

Hence, treewidth 2 due cycle length 3 formed R(x, yi1 ), R(x, yi2 )
yi1 yi2 .
538

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

R1 (x, y) ^ R2 (y, z) ! S(x, z)
body single rule ( (r)) form R1 (x, y)^R2 (y, z)^S(x, z),
treewidth 2 due cycle length 3 formed three atoms.
cases involving equality body rules, namely x ^ z ! x z,
A(x1 , x2 ) ^ x1 ! A(y, x2 ), A(x1 , x2 ) ^ x2 ! A(x1 , y), analogous
previous case.
following theorem establishes two practically relevant cases module extraction framework performed polynomial time. first show modules
ensuring model-inseparability computable polynomial time arbitrary ontologies signatures. Then, establish modules remaining inseparability
relations considered paper also computable polynomial time classes
ontologies whose extended datalog program (P ) Definition 68 bounded
predicate arity treewidth.
Theorem 70. Let L class ontologies, 2 {m, q, f, i, c, wq}, LS,
class datalog programs:
LS, = { (P

(O,)

following

) | 2 L, Sig(O) }

problem isInModule[L,S] decidable polynomial time either following conditions satisfied:
= m,
0

LS, Lkarity \ Lktw fixed non-negative integers k k 0 .
Proof. Consider arbitrary ontology signature . Let (O, ) = h, D0 , Dr

P = P (O,) , F = Dr \P(D0 ). Proposition 67, P D0 computed time
polynomial size O, (P). Therefore, suffices show (D0 , F )
obtained polynomial time materialisation (P) [ (D0 , F ).
= Dr (D0 , F ) computed linear time. Furthermore, easy
see computing materialisation (P) [ (D0 , F ) also feasible linear time
since boils propositional datalog reasoning.
0
0
Consider 2 {q, f, i, c, wq}. Note (P) 2 Lkarity \Lktw implies P 2 Lkarity \Lktw .
Hence, P(D0 ) computed time polynomial size O. Since F always
set facts P(D0 ) predicates , computed polynomial time.
Moreover, dataset (D0 , F ) computed polynomial time well thus,
0
since (P) 2 Lkarity \ Lktw , materialisation (P) [ (D0 , F ).
Tractability module extraction w.r.t. SROIQ ontologies immediate consequence Theorem 70 Proposition 69.
Corollary 71. Let 2 {m, q, f, i, c, wq} let normalised SROIQ ontology.
module computable polynomial time.
539

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

11. Optimality
already discussed, general, modules minimal corresponding
inseparability relation. is, however, interest determine module setting families
yield smallest possible modules given inseparability relation within limits
framework. end, next present study suitable notion optimality
applicable module setting families.
notion module setting family Definition 57 rather general
establish relationship dierent module settings family. order study optimality, makes sense restrict families satisfying certain
uniformity conditions. Roughly speaking, consider family uniform (i) existentially quantified variables constants ontologies treated homogeneously within
setting (i.e., dierent existential variables receive treatment, dierent
constants) well consistently across dierent settings; (ii) signatures treated
monotonically across settings (i.e., 0 members family ontology
signatures 0 0 , treat predicates Sig(O)\0
exactly way).
Definition 72. module setting family uniform if, pair ontologies O, O0
signatures , 0 , module settings (O, ) = h, D0 , Dr (O0 , 0 ) = h0 , D00 , Dr0
satisfy following properties, Ex(F) denotes set existentially quantified
variables F:
1. = 0 , |Ct(O)| |Ct(O0 )| |Ex(O)| |Ex(O0 )|, exists injective
substitution : dom() ! dom(0 ) mapping variables variables constants
constants
= 0 ,

D0 = { A(c) | A(c) 2 D00 , c constants Ct( (O, )) },
Dr = { A(c) | A(c) 2 Dr0 , c constants Ct( (O, )) }.
2. = O0 0 ,
= 0 ,

D0 = { A(c) 2 D00 | 2 },

{ A(c) 2 Dr | 2 } = { A(c) 2 Dr0 | 2 },

{ A(c) 2 Dr | 2 Sig(O)\0 } = { A(c) 2 Dr0 | 2 Sig(O)\0 }.
inseparability relation, let class uniform module setting families
0
S-admissible. say S-optimal 2 (O,) (O,)
every 0 2 pair .

easy see S-admissible families , 2 {m, q, f, i, c, wq},
uniform. Furthermore, following theorem establishes , , c , wq
also optimal respective inseparability relations. proof theorem rather
technical deferred appendix.
540

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

Theorem 73. family



S-optimal 2 {m, i, c, wq}.

contrast, families f q fact query inseparability optimal.
see this, consider ontology consisting following rules:
A(x) ! B(x)

B(x) ! A(x)

C(x) ! D(x)

Furthermore, let = {A, C, D}. module setting f = f (O, ) yields f = O.
Indeed, = {A(a)} non-trivial proof A(a) [ involves rules
A(x) ! B(x) B(x) ! A(x), included module. However,
clear = {C(x) ! D(x)} already -fact inseparable O.
define dierent setting = h, D0 , Dr i, whose corresponding module precisely M. this, idea define D0 Dr way aforementioned
proofs tautological statements avoided. Consider D0 Dr follows:
D0 = {X(c0Y ) | X, 2 } [ {X(c1Y ) | X, 2 X 6= }
Dr = {Y (c1Y ) | 2 }

Datasets D0 Dr disjoint, guarantees proofs -tautologies taken
account therefore indeed M. construction D0 Dr given
example ontology signature generalised define uniform module setting
family provides counter-example optimality f . is, however, price
pay smaller modules, namely increase size module settings. Indeed,
(O, ) size exponential , whereas size f (O, ) remains polynomial.
exponential blowup clearly undesirable practice.
following theorem establishes f q optimal. do, however,
work well practice, shown evaluation presented Section 13. proof
theorem works proposing better module setting families which, cases, incur
aforementioned exponential blowup. conjecture blowup unavoidable
optimal module setting family fact query inseparability (if family exists).
case Theorem 73, proof technical deferred appendix.
Theorem 74. family



S-optimal 2 {f, q}.

12. Related Work
Module extraction received great deal attention literature. Section 12.1
discuss complexity inseparability checking dierent ontology languages
inseparability relations. Section 12.2 recapitulate existing module extraction techniques based inseparability, provide brief overview practical applications.
Finally, Section 12.3 discuss number related problems, forgetting, uniform
interpolation, partition-based reasoning.
12.1 Inseparability Relations
Inseparability relations originate notions model deductive conservative extensions description modal logics (Antoniou & Kehagias, 2000; Ghilardi, Lutz, &
541

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Wolter, 2006a; Ghilardi, Lutz, Wolter, & Zakharyaschev, 2006b), constitute foundation module extraction techniques (Konev et al., 2009).
Model inseparability undecidable DLs extend EL (Lutz & Wolter, 2010).
is, however, tractable ELI ontologies acyclic (Konev et al., 2013); furthermore,
coNexpTimeNP -complete description logic ALCI signatures restricted
N
consist atomic concepts (Konev et al., 2013). DL-LiteN
bool DL-Litehorn
coNexpTime-hard (Kontchakov et al., 2010), matching upper bound known
best knowledge.
complexity query inseparability studied mainly lightweight description logics. ExpTime-complete EL (Lutz & Wolter, 2010), p2 -complete
N
coNP-complete DL-LiteN
bool DL-Litehorn , respectively (Kontchakov et al., 2010),
H
ExpTime-complete DL-LiteH
core DL-Litehorn (Konev, Kontchakov, Ludwig,
Schneider, Wolter, & Zakharyaschev, 2011; Botoeva et al., 2014). Baader et al. (2010)
considered variant query inseparability signature datasets restricted
signature queries not, identified decidable sufficient conditions
inseparability ELI, checked polynomial time EL. complexity
weak query inseparability (see Section 7.2) studied Botoeva et al. (2014);
known P-complete DL-Litecore , DL-Litehorn ELH, ExpTime-complete
H
DL-LiteH
core DL-Litehorn , 2ExpTime-complete Horn-ALCHI Horn-ALCI.
complexity implication classification inseparability coincides
standard reasoning tasks subsumption checking (Konev et al., 2009). description logic literature implication inseparability studied general form:
given L-ontologies O0 signature , problem determine whether
O0 entail concept inclusion axioms C v C (possibly complex) L-concepts . setting, inseparability found p2 -complete
N
DL-LiteN
bool , coNP-complete DL-Litehorn (Kontchakov et al., 2010), ExpTime-complete
EL (Lutz & Wolter, 2010), 2ExpTime-complete ALC (Ghilardi et al., 2006a), ALCI,
ALCQ ALCQI (Konev et al., 2009), undecidable ALCQIO (Konev et al., 2009).
Note variant implication inseparability highly dependent ontology language, whereas results largely logic-independent. leave investigation
inseparability relations within framework interesting problem future work.
12.2 Module Extraction
Practical module extraction techniques typically based approximations, ensure
computed module (model) inseparable given ontology, yet necessarily minimal. One approximation, discussed detail Section 3, based
syntactic locality (Cuenca Grau et al., 2007a, 2008; Sattler et al., 2009). implementation ?-, >- ?> -module extraction integrated OWL API (Horridge &
Bechhofer, 2011), alternative implementation downloaded separate Java
library.3 semantic counterpart syntactic locality, semantic locality, proposed
work Cuenca Grau et al. (2007b). Deciding semantic locality is, given DL,
hard checking satisfiability w.r.t. empty TBox, hence tractable logics restricted expressivity. reason modules based syntactic locality,
3. https://www.cs.ox.ac.uk/isg/tools/ModuleExtractor/

542

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

extracted polynomial time, preferred choice practice. Furthermore, exhaustive comparison syntactic semantic locality modules revealed
dierence significant practical cases (Del Vescovo et al.,
2013).
Reachability-based modules (Suntisrivaraporn, 2008; Nortje, Britz, & Meyer, 2012;
Nortje et al., 2013) refinement syntactic locality modules. Available
three flavours (?-, >- ?> -reachability), also computed polynomial time.
?-reachability modules coincide ?-modules, >- ?> -reachability modules
generally subsets syntactic locality counterparts. refinement comes
cost losing depletingness, although reachability modules still self-contained
preserve justifications consequences reference signature .
Konev et al. (2013) Gatens et al. (2014) developed module extraction techniques
acyclic ELI acyclic ALCQI, respectively. techniques ensure modules
self-contained depleting, case ELI also minimal. polynomial
algorithm ELI implemented system MEX, general, non-tractable
algorithm ALCQI implemented system AMEX. contrast locality
reachability modules, applicability techniques limited relatively restricted
class ontologies, tractability guaranteed even restricted class.
Kontchakov et al. (2010) exploited decidability query inseparability DL-LiteN
bool
DL-LiteN
horn module extraction. techniques yield minimal minimal depleting modules complexity corresponding inseparability relation
(p2 -complete coNP-complete, respectively).
Baader et al. (2010) proposed exponential-time algorithms extracting modules
ELI ontologies preserve variant query inseparability. Furthermore, showed
computing modules feasible polynomial time EL ontologies.
Recently, Rousset Ulliana (2015) studied modularity context deductive
triple stores, is, RDF triple stores equipped set datalog rules. preservation
properties modules, however, dierent ones considered work.
Del Vescovo et al. (2011) considered problem finding polynomial representation
modules ontology, particular notion module. proposed representation called atomic decomposition applicable notion module satisfies
certain properties include self-containment depletingness. atomic decomposition ontology suitable notion module computed polynomial time
using module extraction algorithm oracle.
Module extraction identified key task support knowledge reuse (Cuenca
Grau et al., 2008; Jimenez-Ruiz et al., 2008). Modules also exploited optimise
ontology matching (Jimenez-Ruiz & Cuenca Grau, 2011) computation justifications (Suntisrivaraporn et al., 2008; Ludwig, 2014) ontology debugging explanation.
Finally, module extraction techniques applied optimising ontology classification
(Armas Romero et al., 2012; Tsarkov & Palmisano, 2012; Suntisrivaraporn, 2008; Cuenca
Grau et al., 2010) integrated reasoners Chainsaw.
543

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

12.3 Related Problems
Module extraction strongly related notions forgetting (uniform) interpolation
(Eiter, Ianni, Schindlauer, Tompits, & Wang, 2006; Ludwig & Konev, 2014; Koopmann &
Schmidt, 2014; Konev, Walther, & Wolter, 2009; Nikitina & Rudolph, 2014; Wang, Wang,
Topor, & Pan, 2010). uniform interpolant L-ontology signature
L-ontology O0 mentions symbols inseparable w.r.t.
given inseparability relation. contrast modules, uniform interpolants
required subsets cannot contain symbol outside (all remaining
symbols thus forgotten). latter requirement implies uniform interpolants
given may always exist (Konev et al., 2009; Lutz & Wolter, 2011; Wang,
Wang, Topor, Pan, & Antoniou, 2014).
Amir McIlraith (2005) investigated partition-based reasoning techniques propositional first-order logic, goal improve efficiency reasoning
knowledge base first dividing axioms related partitions. topology
partitions described means graph, nodes represent partitions edge
two partitions labelled symbols common. graph
structure exploited distributed message-passing algorithm, correctness
ensured Craigs interpolation theorem first-order logic. Similar problems
reasoning techniques studied context description logics Konev, Lutz,
Ponomaryov, Wolter (2010) well Schlicht Stuckenschmidt (2009). key
concern partition-based reasoning find partitioning exhibits suitable balance number partitions, size, number common symbols
partitions order enable efficient distributed reasoning. setting, partitions
necessarily capture meaning given signature input knowledge base
therefore fundamentally dierent modules.
Konev, Ludwig, Walther, Wolter (2012) studied problem computing logical
dierence ontologies O0 is, set queries receive dierent answers
w.r.t. O0 . Computing logical dierence (or concise representation thereof)
identified valuable resource ontology versioning tasks (Jimenez-Ruiz, Cuenca
Grau, Horrocks, & Berlanga Llavori, 2011) closely related inseparability checking;
indeed, inseparable ontologies empty dierence.
Finally, Zhou et al. (2014) proposed hybrid approach ontology-based query answering bulk computation delegated datalog reasoner. Given ontology
O, dataset , query q(x), candidate answer tuple c, core technique approach
compute fragments O0 D0 [ |= q(c) O0 [ D0 |= q(c).
Similarly modules, fragments computed first strengthening datalog program P exploiting datalog reasoner identify axioms facts
responsible validity q(c). contrast query inseparable modules, however,
fragment O0 [ D0 guaranteed preserve fixed query q(c) w.r.t. fixed dataset
D, rather queries w.r.t. datasets reference signature.

13. Implementation Evaluation
section, present prototype module extraction system discuss results
evaluation suite real-world ontologies.
544

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

13.1 PrisM System
implemented prototype system module extraction Java, called PrisM,
bundles RDFox black-box datalog reasoner (Motik et al., 2014). PrisM
available online academic license.4
PrisM accepts input OWL 2 ontology O, signature parameter
indicates relevant inseparability relation. system currently supports whole
OWL 2, exception datatypes; furthermore, supports inseparability
relations 2 {m, q, f, i, c, wq} defined Sections 6 7.
PrisM computes S-module Mout w.r.t. according following steps.
1. Compute normalisation norm(O) O, norm normalisation function
defined rules Figure 2 Section 2 (see also Proposition 3). Then, apply
mapping Figure 1 obtain equivalent set rules O0 = (norm(O)).
2. Consider S-module setting = (S , D0S , DrS ) O0 defined Section 6
(for 2 {m, q, f, i}) 7 (for 2 {c, wq}). Then, compute corresponding module
follows:
(a) Construct datalog program P
D0S DrS .

(b) Compute support supp(



specified Definition 19, datasets

S)

exploiting result Theorem 68. this,

S)

specified Definition 19.

compute datalog program (P ) dataset (D0S , DrS ) using
transformation
defined Theorem 68;
construct materialisation (P ) [ (D0S , DrS ) using RDFox;

construct supp( ) set rules r 2 P Rel(dr ) 2

(c) Construct



supp(

3. Return Mout consisting axioms 2 (norm()) \



6= ;.

first step normalises input OWL 2 ontology set rules O0 . PrisM provides
optimisation O0 constructed locality-based module O, rather
itself. Specifically, use ?-module M?
[O,] classification inseparability,


?>
?> -module M[O,] inseparability relations. optimisation
compromise correctness overall procedure since model inseparability stronger
inseparability relations.
second step computes relevant S-module set rules O0 signature . essentially done following Definition 19; computationally
demanding part construction support supp( ), achieved via materialisation using RDFox black-box manner.
Finally, third step constructs module Mout input ontology
module corresponding set rules O0 . this, PrisM keeps track correspondence axioms normalisation norm(O). Correctness
step ensured Proposition 13.
4. http://www.cs.ox.ac.uk/isg/tools/PrisM/

545

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

ID
00001
00004
00024
00026
00029
00032
00347
00350
00351
00354
00463
00471
00477
00512
00545
00774
00775
00778
00786

name
ACGT-v1.0
BAMS-simplified
DOLCE
GALEN-no-FIT
GALEN-doctored
GALEN-undoctored
LUBM-one-uni
OBI
AERO
NIF-gross-anatomy
Fly-anatomy-XP
FMA-lite
Gazetteer
Lipid
Molecule-role
RNA-v0.2
Roberts-family
SNOMED
NCI-v12.04e

predicates
2,019
1,199
603
29,073
3,740
3,762
68
2,965
355
4,166
8,047
78,986
150,981
1,289
9,222
338
183
54,982
93,628

rules
5,512
18,976
2,148
66,191
7,447
7,818
84,771
10,952
669
7,134
42,107
168,828
382,158
5,222
153,020
938
2,020
191,891
193,453

disjunctive
105
0
53
0
0
0
0
77
11
51
0
0
0
541
0
34
1
18,323
65

existential
259
16,782
184
26,973
2,367
2,715
8
1,168
100
1,506
9,433
42,734
156,743
893
6,276
90
73
60,377
76,957

expressivity
SROIQ(D)
ALEHIF +
SHOIN (D)
ALEH
ALEHIF +
ALEHIF +
ALEHI + (D)
SHOIN (D)
SROIQ(D)
SROIF(D)
ALERI +
ALEH+
ALE +
ALCHIN
ALE +
SRIQ(D)
SROIQ(D)
SH
SH(D)

Table 3: Test ontologies
13.2 Evaluation
evaluated system set test ontologies identified work Glimm,
Horrocks, Motik, Stoilos, Wang (2014) non-trivial reasoning. ontologies
normalised prior module extraction make DL axioms equivalent rules.
details ontologies given Table 3.5 first second columns
table indicate ontology ID name Oxford Ontology Repository. third
fourth columns provide number predicates rules resulting ontology
normalisation. fifth sixth columns specify many rules contain
disjunction existential quantification head. Finally, last column indicates
DL expressivity6 normalised ontology given OWL API.
experiments performed server 2 Intel Xeon E5-2670 2.60GHz processors, 8 physical cores serve 2 virtual cores each, making total 32
virtual cores. experiments allocated 90GB RAM, RDFox always run
16 threads. compared module sizes extraction times using system
locality-based ?- ?> -modules, computed using OWL API.
followed experimental methodology work Del Vescovo et al. (2013), two
kinds signatures considered:
genuine signatures, correspond signature individual axioms,
random signatures, include signatures several axioms.
5. ontologies used experiments available download https://krr-nas.cs.ox.ac.uk/
2015/jair/PrisM/testOntologies.zip.
6. refer reader work Baader et al. (2003) detailed account DL naming conventions.

546

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

Unlike work Del Vescovo et al. (2013), defined random signatures simply
random subsets ontology signature, extracted signatures using randomised
graph sampling algorithm. first represented syntactic dependencies symbols
(normalised) ontology graph, traversed graph randomised way
visited set number n nodes. symbols corresponding visited nodes
taken random signature.7 advantage approach yields
signatures semantically connected, believe likely case
practical applications. number n chosen default 0.1% total graph
increased two orders magnitude cases resulting signatures
typically contained less 15 predicates thus small provide additional
information w.r.t. genuine signatures.
kind signature ontology, considered sample 400
runs averaged module sizes module extraction times. one hand,
compared modules produced c (Section 7) ?-modules.
kind modules literature guarantee notion classification inseparability.
hand, compared modules produced , q , wq , f ,

(Sections 6 7) ?> -modules. discussed Section 12.2, system
(to best knowledge) capable computing modules specific deductive
inseparability relations considered paper. Furthermore, module extraction
systems ensure model inseparability, MEX AMEX, applicable
rather restricted ontology languages. Consequently, ?> -modules seemed best available
option comparison approach.
Tables 4 5 provide average number rules kind module genuine
random signatures, respectively. tables, total number rules normalised
ontology provided top comparison purposes, whereas average size
signatures considered specified towards bottom. Table 5 additionally includes
percentage n dependency graph covered random walks random
signatures obtained.
observe module size consistently decreases consider weaker inseparability relations. modules produced c several orders magnitude smaller
?-modules, cases 00463, 00471, 00477 00545. Although rather
extreme cases, observed cases least 25% decrease size (see 00026, 00029,
00032, 00347, 00350, 00351, 00786). modules model inseparability improve reasonably ?> -modules cases, although greatest dierence size course
?> -modules -modules, reaching one order magnitude ontologies
(see 00471 00477, also 0004, 00463 00786 genuine signatures). realistic
ontologies, small proportion predicate pairs related atomic implication,
often still case considering datalog overestimation ontology;
thus, dierence size ?> -modules rather unsurprising.
naturally also big dierence size wq -modules modules whenever
ontologies mention constants, since former case obviously empty.
worth observing that, even though several cases modules q
modules similar size (e.g. 00471), also cases dier signifi7. functionality required perform random walks currently integrated RDFox.

547

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

total
?
c

>?

q
wq
f


||

00001
5,512
678
558
674
584
563
514
563
558
2

00004
18,976
18,306
16,942
18,297
18,297
17,151
0
17,108
655
3

00024
2,148
1,000
883
990
910
884
875
884
882
2

00463
00471
00477
total 42,107 168,828 382,158
? 22,348 47,192 214,820
112
12
<1
c
>?
221
20
9
217
12
8

107
12
8
q
0
0
0
wq
80
1
<1
f
12
1
<1

||
3
2
3

00026
66,191
14,253
9,799
13,749
13,686
9,448
0
5,962
3,279
3

00029
7,447
187
94
114
112
96
0
96
18
3

00032
7,818
690
479
596
592
533
0
533
130
3

00347
84,771
84,726
23,651
58,186
44,244
44,368
43,761
31,322
11,234
2

00350
10,952
803
558
768
624
596
538
596
558
2

00351
669
133
74
130
97
77
64
77
67
2

00512
00545
5,222 153,020
261 143,399
86
6
34
2
32
1
29
1
0
0
29
<1
27
<1
2
3

00774
938
80
76
80
80
78
0
78
76
2

00775
00778
00786
2,020 191,891 193,453
1,916
433
1,140
1,491
426
390
1,913
427
1,138
1,498
426
1,138
1,492
426
385
1,490
0
0
1,492
426
371
1,491
397
120
2
3
3

00354
7,134
826
618
786
675
626
111
626
617
2

Table 4: Average module sizes genuine signatures.
cantly (e.g. 00786). Similarly, q modules f modules similar size cases
(e.g. 00350) others (e.g. 00471), happens q wq (exemplified ontologies 00775 00354), f (see ontologies 00512 00004).
observations suggest modules faithfully reflect dierences
inseparability relations considered, could oer significant advantages
practical applications
Tables 6 7 provide average module extraction time (in milliseconds) genuine
random signatures, respectively. extraction modules consistently slower
locality-based modules; however, average extraction time rarely exceeds
1 minute, often 10 seconds (especially genuine signatures).
suggests modules feasible practical applications. Furthermore, since
extraction time invariably spent datalog reasoner, used black
box, future advancements area datalog reasoning lead performance
gains systems implementing technique.

14. Conclusion Future Work
paper, proposed novel approach module extraction based reduction
datalog reasoning. contrast existing techniques, approach applicable
description logics, also highly expressive first-order rule formalisms. Furthermore,
techniques easily customised capture wide range inseparability
relations studied literature. cases modules satisfy many desirable properties,
548

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

total
?
c

>?

q
wq
f


||
%

00001
5,512
857
691
854
759
736
517
735
688
43
1

00004
18,976
18,904
17,607
18,894
18,894
18,579
0
18,536
2,511
82
1

00024
2,148
1,053
933
1,044
964
942
875
942
931
20
1

00463
00471
00477
total 42,107 168,828 382,158
? 23,139 49,345 215,886
595
402
38
c
>?
982
1,658
1,050
973
1,450
1,049

757
1,450
1,049
q
0
0
0
wq
664
74
16
f
333
74
16

||
28
154
312
%
0.1
0.1
0.1

00026
66,191
27,771
17,879
27,184
27,175
18,315
0
18,255
17,646
107
0.1

00029
7,447
1,890
1,223
1,726
1,719
1,380
0
1,364
1,060
104
1

00032
7,818
3,279
2,483
3,108
3,101
2,633
0
2,620
2,314
107
1

00347
84,771
84,732
58,203
80,153
63,031
63,031
62,455
58,980
50,362
11
10

00350
10,952
1,795
1,084
1,758
1,611
1,389
539
1,389
1,080
92
1

00351
669
315
208
311
274
265
81
241
191
56
10

00512
00545
5,222 153,020
1,555 143,448
1,199
28
837
16
819
14
774
14
0
0
766
5
467
5
66
19
1
0.1

00774
938
371
338
371
369
368
0
368
338
58
10

00775
00778
00786
2,020 191,891 193,453
1,979 11,766 16,820
1,527 11,342
7,974
1,977 11,762 16,817
1,561 11,651 16,817
1,557 11,644
8,969
1,506
0
0
1,557 11,342
8,415
1,526 11,342
6,228
42
202
326
10
0.1
0.1

00354
7,134
1,537
1,240
1,501
1,388
1,279
113
1,278
1,238
79
1

Table 5: Average module sizes random signatures.
makes well-suited applications ontology reuse, debugging, modular
ontology development, reasoning optimisation. Last, least, modules
efficiently computed reusing o-the-shelf datalog reasoners experimental
evaluation confirms suitability practice.
envisage many directions future work, outline next.
State-of-the-art modular DL reasoners, Chainsaw, currently
rely ?-modules split workload fully-fledged OWL reasoner
efficient reasoner lightweight DL. would natural exploit modules
classification inseparability (cf. Section 7.1) instead ?-modules since
reasoners focus mainly classification tasks. believe using modules
modular reasoning significantly improve separation workload lead
better use lightweight reasoner.
far, use modules optimising data reasoning tasks, fact entailment
query answering, rather limited. Indeed, well-known ?-modules
well-suited tasks (Cuenca Grau et al., 2008). PAGOdA
reasoning system know exploits techniques akin module extraction
data reasoning (Zhou et al., 2014, 2015). would interesting investigate
techniques could exploited improve PAGOdAs performance. Furthermore,
549

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

?

c

>?

q
wq
f


?

c

>?

q
wq
f


00001
27
846
43
831
857
857
846
847

00004
64
36,784
95
18,244
62,680
32,972
62,797
35,158

00024
11
1,066
19
1,013
1,074
1,069
1,074
1,072

00463
122
4,176
199
775
485
455
463
456

00471 00477
506 1,154
9,729 52,529
805 1,851
1,543 4,995
824 1,792
790 1,753
788 1,772
792 1,759

00026
273
30,256
418
25,884
29,051
27,272
28,254
28,499

00029
24
295
39
231
241
229
246
234

00032
27
989
42
826
903
861
910
890

00347
274
14,823
463
15,268
14,688
10,677
14,495
10,285

00350
95
747
124
724
798
787
784
789

00351
3
130
5
126
136
140
131
132

00512 00545
18
363
378 34,705
30
616
166 2,783
225
595
210
552
229
580
229
579

00774
5
161
11
147
161
154
164
164

00775
10
963
18
857
955
966
967
965

00778
722
1,657
1,056
1,590
1,708
1,612
1,700
1,691

00786
569
3,449
899
3,340
3,475
3,419
3,526
3,479

00354
31
6,588
43
722
3,530
767
3,840
3,826

Table 6: Average extraction times milliseconds genuine signatures.

?

c

>?

q
wq
f


?

c

>?

q
wq
f


00001
26
1,076
101
1,011
1,064
1,056
1,058
1,078

00004
58
36,376
84
18,114
171,497
32,474
179,759
34,866

00024
10
1,162
18
1,077
1,132
1,153
1,122
1,144

00026
284
56,125
457
49,792
55,343
51,342
54,854
54,068

00029
35
2,989
54
2,675
2,814
2,670
2,785
2,765

00032
39
5,737
60
4,900
5,533
5,170
5,477
5,368

00347
276
15,708
485
20,885
20,654
15,390
20,822
15,063

00350
107
2,065
144
1,936
2,077
2,010
2,067
2,108

00351
4
423
9
384
423
390
417
427

00463
138
5,441
192
1,615
1,431
1,332
1,391
1,382

00471
549
13,291
793
3,202
2,669
2,638
2,640
2,664

00477
1,172
51,707
1,768
6,073
3,191
3,157
3,157
3,223

00512
23
20,712
37
2,188
2,939
2,756
2,964
2,891

00545
342
31,616
576
2,504
591
567
569
562

00774
5
768
10
640
750
715
748
767

00775
10
1,064
20
967
1,046
1,028
1,047
1,048

00778
841
34,980
1,210
20,409
34,330
20,293
33,565
34,774

00786
698
44,463
1,070
41,283
43,785
43,007
44,604
44,168

00354
29
13,664
45
1,467
8,591
1,591
8,358
8,376

Table 7: Average extraction times milliseconds random signatures.
also envision potential applications incremental stream reasoning,
data frequently changing queries ontologies seen fixed.
conjecture optimal module setting families fact query inseparability
incur exponential blowup w.r.t. ones chosen remains open.
550

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

use ?-modules exploit debugging explanation systems DL ontologies
proved rather successful (Suntisrivaraporn et al., 2008). Given modules
also justification-preserving, would interesting evaluate eectiveness
modules implication inseparability setting.
Finally, use module extraction techniques far largely restricted
description logics. techniques are, however, widely applicable could
exploited number reasoning tasks ontology languages datalog
datalog,_ , currently gaining momentum.

Acknowledgments
paper extended version conference publication (Armas Romero, Kaminski,
Cuenca Grau, & Horrocks, 2015). work supported Royal Society
University Research Fellowship, EPSRC projects Score!, MaSI3 , DBOnto,
EU FP7 project Optique. would like thank anonymous referees
valuable comments suggestions.

Appendix A. Proofs Section 11
Theorem 73. family



S-optimal 2 {m, i, c, wq}.

prove result 2 {m, i, c, wq} separately.
Theorem 75.



m-optimal.

Proof. Suppose m-optimal. must m-admissible, uniform

family (O,) 6 (O,) . Let (O, ) = hm , D0m , Drm
(O, ) = h, D0 , Dr i. Theorem 55, (O, ) 6,! (O, ), hence
mapping : Ct( (O, )) ! Ct( (O, )) must either 6= D0m 6 D0
Drm 6 Dr .
Suppose two existentially quantified variables y1 y2
y1 6= y2 . Consider ontology O0 consisting following rules:
p1
p2
p3
p4

:
:
:
:

A(x) ! S(x, b)
A(x) ! 9y1 [R(x, y1 ) ^ B(y1 )]
A(x) ! 9y2 [R(x, y2 ) ^ C(y2 )]
S(x, b) ^ R(x, z) ^ B(z) ^ C(z) ! D(x)

signature 0 = {A, D, R} let (O0 , 0 ) = h0 , D00 , Dr0 i. second property
uniformity, 0 substitution (O0 , ;) (O0 , ), therefore,
first property uniformity, y1 0 6= y2 0 . follows p4 never applied
0
0
0
0
P (O , ) (D00 ) hence support (O0 , 0 ) (O , ) {p1 , p2 , p3 }.
interpretation = {i}, AI = B = C = {i}, DI = ;, aI =
0
0
RI = = {(i, i)} model (O , ) , cannot extended model
0
0
O0 without changing interpretation A, R. follows (O , )
551

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

0

m-admissible. origin
0 -module , contradicting hypothesis
contradiction assumption map existentially quantified
variables constant, therefore must constant c = c
variable universally quantified O. Suppose exists
constant b0 b0 6= c. first property uniformity 0 must also
map b constant y1 y2 . p4 never applied
0
0
0
0
P (O , ) (D00 ) (O , ) {p1 , p2 , p3 }, which, already shown,
contradiction. Consequently, must map constants c well.
means exists substitution : Ct( (O, )) ! Ct( (O, ))
= . particular must = c. hypothesis, must case
either D0m 6 D0 Drm 6 Dr .
Suppose D0m 6 D0 . must predicate X 2 X(c, . . . , c) 2
/ D0 .
00
Consider ontology consisting following rules:

p5 : X(x, . . . , x) ! 9yR(x, y)
p6 : R(x, x) ! A(x)
signature 00 = {X, A}, let (O00 , 00 ) = h00 , D000 , Dr00 i. uniformity , 00
maps c, fact X(c, . . . , c) initial dataset (O, {X}),
(O, 00 ), D000 . Consider dataset
= {A(a), A(c)} [ { X(t) | 2 {a, c}arity(X) , 6= (c, . . . , c) }
fresh constant. substitution maps c constants
(O00 , 00 ) homomorphism (O00 , 00 ) = h00 , D, (Dr00 )i. Theorem 55,
00
00
implies (O , ) . Clearly, R-fact P (D) R(a, c), p5
00
00
support therefore (O , ) {p5 }. interpretation J
0
00
J = {i}, X J = (i, . . . , i), AJ = ; RJ = {(i, i)} model (O , ) ,
cannot extended model O00 without changing interpretation X A.
00
00
00
follows (O , )
00 -module , contradicts hypothesis
m-admissible. contradiction stems assumption D0m 6 D0 , therefore
must D0m D0 .
hypothesis, implies Drm 6 Dr , must predicate 2
(c, . . . , c) 2
/ Dr . Consider ontology O000 consisting following rules:
p7 : A(x) ! 9yR(x, y)
p8 : R(x, x) ! (x, . . . , x)
signature 000 = {A, }, let (O000 , 000 ) = h000 , D0000 , Dr000 i. uniformity
, 000 maps c. Consider dataset = {A(c), (c, . . . , c), A(b), (b, . . . , b)},
fresh constant. mapping maps c constants
(O000 , 000 ) b homomorphism (O000 , 000 ) = h000 , D, Dr000 i. Theorem 55,
000
00
implies (O , ) . easy see R-facts materialisation
P [ R(a, c) R(c, c), proof P [ supported p8 proof
(c, . . . , c). However, uniformity , (c, . . . , c) relevant facts
(O, {Y }), (O, {A, }), Dr000 . follows p8 support
000
000
(O , ) {p7 }. interpretation K K = {i}, AK = {i},
552

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

0

00

RK = {(i, i)} K = ; model (O , ) , cannot extended model
000
000
O000 without changing interpretation . follows (O , )
000

m-admissible.
000 -module , contradicts hypothesis
origin contradiction assumption Drm 6 Dr , therefore must
Drm Dr . mapping thus witness (O, ) ,! (O, ), ultimately
contradicting assumption m-optimal.
Theorem 76.



i-optimal.

Proof. Consider module setting family i0 pair ontology
signature , i0 (O, ) = hi0 , D0i0 , Dri0 follows:
i0 = { 7! cy | existentially quantified } [ { c 7! c | c 2 Sig(O) constant }
D0i0 = { A(cA,B ) | 6= B predicates arity }
Dri0 = { B(cA,B ) | 6= B predicates arity } [ Dri0?
cy fresh constant, cA,B = c1A,B , . . . , cnA,B , array fresh constants
pair A, B 2 distinct n-ary predicates, Dri0? = {?} contains two distinct
predicates arity Dri0? = ; otherwise.

First, show i-optimal i0 i-optimal proving (O,) =

0 (O,) . Let us fix arbitrary , let (O, ) = hi , D0i , Dri
i0 (O, ) = hi0 , D0i0 , Dri0 i. easy see i0 (O, ) ,! (O, ), therefore




Theorem 55 0 (O,) (O,) . (O,) 0 (O,) , first
(O,)
(O,)
note P
= P 0
. assume w.l.o.g. contains least two
predicates A, B arityotherwise non-trivial -implications. Given

proof = (T, ) P (O,) [ D0i B(cA ) (resp. ?) proof 0 = (T, 0 )
0 (v) = (v) node v , substitution cA = cA,B , proof

B(cA,B ) (resp. ?) P 0 (O,) [ D0i0 satisfying supp(0 ) = supp(). Consequently,


supp( (O, )) supp( i0 (O, )) hence (O,) 0 (O,) .
Now, suppose i0 i-optimal. Then, must uniform, i-admissible family

0 (O,) 6 (O,) . Let (O, ) = h, D0 , Dr i. Since
i0 injective, exists mapping : Ct( i0 (O, )) ! Ct( (O, )) i0 = .
condition determines eect dom(i0 ), since dom(i0 ) disjoint
set { cA,B | 6= B predicates arity }, asume either
D0i0 D0 , two distinct predicates A, B 2 arity
D0 contains A-facts. Suppose latter case, consider ontology
O0 = {A(x) ! B(x)}. uniformity , initial dataset (O0 , ) also contains
0
A-facts therefore support (O0 , ) empty (O ,) = ; 6|= A(x) ! B(x).
contradicts i-admissibility , hence must D0i0 D0 . Finally, suppose
Dri0 6 Dr ; particular must contain two distinct n-ary predicates, since otherwise
Dri0 = ; thus trivially Dri0 6 Dr . case two possible situations:
?2
/ Dr .
Let A, B 2 distinct predicates arity consider ontology
553

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

O00 = {A(x) ! C(x) _ D(x), C(x) ! B(x), D(x) ! ?} C fresh predicates. Clearly, O00 |= A(x) ! B(x) uniformity , ? set
00
relevant facts (O00 , ), therefore D(x) ! ? 2
/ (O ,) . Consequently
00
(O ,) 6|= A(x) ! B(x), contradicting assumption i-admissible.
B(cA,B ) 2
/ Dr pair distinct A, B 2 arity.
Consider O000 = {A(x) ! B(x)}. uniformity , B-facts set
00
relevant facts (O000 , {A, B}), therefore (O ,{A,B}) = ; |6 = A(x) ! B(x),
contradicting assumption i-admissible.
follows Dri0 Dr , homomorphism (O, ) i0 (O, ), thus

0 (O,) (O,) , ultimately contradicts assumption i0 (resp. )
i-optimal.
Theorem 77.

c

c-optimal.

Proof. Analogous Theorem 76
Theorem 78.

wq

wq-optimal.

Proof. Suppose wq wq-optimal. must uniform wq-admissible
wq
family
s.t. (O,) 6 (O,) . Let (O, ) = h, D0 , Dr
wq (O, ) = h wq , wq, wq i. Theorem 55, given mapping : Ct( wq ) ! Ct( )
r
0
must either wq 6= D0wq 6 D0 Drwq 6 Dr .
Since wq injective Dwq = ;, assume wq = ,
wq
D0 D0 Drwq 6 Dr . must predicate X 2 array c
size arity(X) constants Ct(O)[{ cy | exist. quant. } X(c) 2
/ Dr .
Consider ontology O0 = {( ! X(c))}; clearly, O0 |= X(c). uniformity, X(c) also
0
relevant facts (O0 , ), implies (O ,) = ; 6|= X(c). contradicts
wq
wq-admissibility , hence must Dr Dr , makes homomorphism.
follows wq wq-optimal.
Theorem 74. family



S-optimal 2 {f, q}.

Again, prove result 2 {f, q} separately.
f

Proposition 79. family

f-optimal.

Proof. Consider arbitrary fixed pair . Let Ct(O) = {c1 , . . . , cn }. Furthermore, predicate B 2 array v 2 {1, . . . , arity(B) + n}arity(B) consider
set constants { iB,v | 0 arity(B) + n }
1. 0B,v , . . . , arity
B,v (B) fresh constants
arity(B)+n

2. arity
B,v (B), . . . , B,v

arity(B)+i

B,v

= ci 2 Ct(O) 1 n.

w1
wn
Let w
B,v denote array (B,v , . . . , B,v ) whenever w = (w1 , . . . , wn ), consider

uniform module setting family

f
0



f (O, )
0

= hf0 , D0f0 , Drf0 follows:

f0 = { 7! cy | existentially quantified } [ { c 7! c | c 2 Ct(O) }
554

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

arity(B) ,
D0f0 = {A(w
B,v ) | A, B 2 , v 2 {1, . . . , arity(B) + n}

v
w 2 {0, . . . , arity(B) + n}arity(A) , A(w
B,v ) 6= B(B,v )}

Drf0 = { B(vB,v ) | B 2 , v 2 {1, . . . , arity(B)}arity(B) } [ {?}
show family f0 f-admissible. Consider datalog rule r = ' ! 2
relf(O,) . W.l.o.g. assume 2
/ 'otherwise r would tautological hence
entailed ontology. Let substitution mapping variables r distinct
fresh constants. Since
2
/ '
injective, also
2
/ ' . fact
must form B(c) B 2 (if B = ? would c = ;) consider
injective substitution mapping constants c [ Ct(O) {1, . . . , arity(B) + n}
satisfying c = arity(B) + c = ci 2 Ct(O). Let another substitution defined
constants Sig(' [ )
c
B,c c 2 c [ Ct(O)
c =
0B,c otherwise
Note compatible f0 .
f0
arity(B) .
easy see ( ) = B(c
B,c ) 2 Dr since c 2 {1, . . . , arity(B) + n}
w
hand, fact (' ) must form A(B,(c) ) 2
w 2 {0, . . . , arity(B) + n}arity(A) .
2
/ ' injective, easy see
c
w
A(B,c ) 6= B(B,c ). Consequently, (' ) Drf0 . Theorem 23, follows |= r
f

0 (O,) |= r, hence f0 f-admissible.
Finally, consider = {A(x) ! B(x), B(x) ! A(x)} = {A}. easy see
f
f
(O,) = 6 ; = 0 (O,) , thus f f-optimal.
Proposition 80. family

q

q-optimal.

Proof. Consider arbitrary fixed pair . Let Ct(O) = {c1 , . . . , cn } let
{y1 , . . . , ym } existentially quantified variables mentioned O, {cy1 , . . . , cym }
corresponding set fresh constants. Furthermore, predicate B 2
array v 2 {1, . . . , arity(B) + n + m}arity(B) consider set { iB,v | 0 arity(B) + n + }
constants
1. 0B,v , . . . , arity
B,v (B) fresh constants,
arity(B)+n

2. arity
B,v (B), . . . , B,v
arity(B)+n+1

3. B,v

arity(B)+i

B,v

arity(B)+n+m

, . . . , B,v

= ci 2 Ct(O) 1 n,

arity(B)+n+i

B,v

= cyi 1

w1
wn
Let w
B,v denote array (B,v , . . . , B,v ) whenever w = (w1 , . . . , wn ), consider
uniform module setting family q0 q0 (O, ) = hq0 , D0q0 , Drq0 follows:

q0 = { 7! cy | existentially quantified } [ { c 7! c | c 2 Ct(O) }
arity(B) ,
D0q0 = {A(w
B,v ) | A, B 2 , v 2 {1, . . . , arity(B) + n + m}

v
w 2 {0, 1, . . . , arity(B) + n}arity(A) , A(w
B,v ) 6= B(B,v )}

555

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Drq0 = { B(vB,v ) | B 2 , v 2 {1, . . . , arity(B) + n + m}arity(B) } [ {?}
show family q0 q-admissible.
Consider rule r = '(x) ! 9y (x, y) 2 relq (O, ), justification O0 r
q
rule 2 O0 . show q0 (O, ) q-admissible suffices show 0 (O,) .
Wn
assume w.l.o.g. ? 2
/ ' (otherwise r would tautological) = i=1
> 0 conjunction atoms. Similarly proof Theorem 23,
consider fresh predicate Q ontology
OQ = {

(x, y)

! Q(x) | 1 n }

well module setting Q = hq0 , D0Q , DrQ [ OQ , satisfying particular
q
P Q = P 0 (O,) [OQ . argued proof Theorem 23, given 2 O0 substitution
mapping variables x fresh distinct constants, must proof = (T, )
Q(x) (O [ OQ ) [ ' (s) \ supp() 6= ;. Furthermore, thanks O0
justification, assume laconic, particular leaf node v 2
ancestor w v must (v) 6 (w).
q
Lemma 21, either = ? ! exists proof 0 = (T 0 , 0 ) P 0 (O,) [
(' )q0 either Q(x) ? embeddable modulo q0 supp(0 ) \
q
0 (O,) (s) 6= ;.
= ? ! must rule s0 O0 ? 2 Sig(O0 ). show
next, considering case 6= ? ! , must s0 2 . Therefore ? 2 Sig(M ),
consequently = ? ! 2 .
Otherwise, 0 = (T, ) proof Q(x) , discussed proof Theorem 23,
q
rule 0 (O,) (r) must
used subproof 00 = (T 00 , 00 ) 0 proof

( 0 )q0 2 extension 0 y. Let v root 00
w1 , . . . , wn leaves, definition must 00 (v) = ( 0 )q0 .
q
v also leaf 00 must rule form ( ! ( 0 )q0 ) 0 (O,) (s),
terms ( 0 )q0 must constants domain q0 . implies
q
( 0 )q0 2 Drq0 , consequently ( ! ( 0 )q0 ) 2 supp( q0 (O, )) 2 0 (O,) .
Suppose v leaf 00 . First all, note (' )q0 = ' due 0q
modifying constants ' using functional terms, therefore 00 (wi ) 2 '
0 (
0 ) q0 2
i. functional terms
/ ' , since constants cy
constants range fresh hypothesis, therefore 00 (wi ) 6= 00 (v)
i. Suppose functional terms 0 . 0 q0 = 0 . know
0 , hence 00 , embeddable modulo q0 . follows must exist
node v 0 2 leaf (v) (v 0 )q0 , also collection
0 ' 0
leaves w10 , . . . , wn0 00 (wi ) = (wi0 )q0 2 ' q0 . Since neither
q
use functional terms, 0 modify constants, must fact (v) (v 0 )
00 (w ) = (w 0 ) i. Furthermore, (w ) 6 (v 0 ) i, implies



0 mentioning
that, also case 00 (wi ) 6= 00 (v) i. Therefore, regardless
00
00
0
functional terms,
(wi ) 6= (v) i. Now, fact
must form
B(c) B 2 since 2 hypothesis r 2 relq (O, ). Let injective
substitution mapping constants c [ range() {1, . . . , arity(B) + n + m} satisfying
(i) c = arity(B) + c = ci 2 Ct(O) (ii) c = arity(B) + n + c = cyi . Let
556

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

another substitution defined constants mentioned 00
c
B,c c 2 c [ range()
c =
0B,c otherwise
Note compatible . Since c 2 {1, . . . , arity(B) + n + m}arity(B) , easy
f0
see 00 (v) = B(c
B,c ) 2 Dr . hand, previously observed,
00
(wi ) mentions constants set {cy1 , . . . , cym }, hence 00 (wi ) must
arity(A) . Furthermore,
form A(w
B,(c) ) 2 w 2 {0, . . . , arity(B) + n}

00 (v) i, follows also 00 (w ) 6= 00 (v) thus 00 (w ) 2 f0
6
=


0
i. proof 00 = (T 00 , 00 ) 00 (v) = ( 00 (v)) clearly proof
q
q
P 0 (O,) [ D0q0 00 (v) support 00 , therefore 2 0 (O,) .
q
Finally, 0 proof ? must proof P 0 (O,) . Since assumption
? 2
/ ', labels leaves 0 must also dierent ?. check also
q
case 2 0 (O,) , suffices follow similar argument one
mapping 0 defined constants mentioned 0

c
c 2 Ct(O) [ {cy1 , . . . , cym }
c =
0
B,c otherwise
00 (w

i)

proved family q0 q-admissible. use show
q q-optimal. proof Theorem 79, consider ontology = {A(x) ! B(x), B(x) ! A(x)} signature = {A}, observe
q
q
(O,) = 6 ; = 0 (O,) , thus q q-optimal.

References
Abiteboul, S., Hull, R., & Vianu, V. (1995). Foundations Databases. Addison-Wesley.
Alviano, M., Faber, W., Leone, N., & Manna, M. (2012). Disjunctive datalog existential
quantifiers: Semantics, decidability, complexity issues. Theory Practice
Logic Programming, 12 (4-5), 701718.
Amir, E., & McIlraith, S. A. (2005). Partition-based logical reasoning first-order
propositional theories. Artificial Intelligence, 162 (1-2), 4988.
Antoniou, G., & Kehagias, A. (2000). note refinement ontologies. International
Journal Intelligent Systems, 15 (7), 623632.
Armas Romero, A., Cuenca Grau, B., & Horrocks, I. (2012). MORe: Modular combination
OWL reasoners ontology classification. Cudre-Mauroux, P., Heflin, J., Sirin, E.,
Tudorache, T., Euzenat, J., Hauswirth, M., Parreira, J. X., Hendler, J., Schreiber, G.,
Bernstein, A., & Blomqvist, E. (Eds.), Proceedings 11th International Semantic
Web Conference, Part I, Vol. 7649 Lecture Notes Computer Science, pp. 116.
Springer.
Armas Romero, A., Kaminski, M., Cuenca Grau, B., & Horrocks, I. (2015). Ontology module
extraction via datalog reasoning. Bonet, B., & Koenig, S. (Eds.), Proceedings
29th AAAI Conference Artificial Intelligence, pp. 14101416. AAAI Press.
557

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Baader, F., Bienvenu, M., Lutz, C., & Wolter, F. (2010). Query predicate emptiness
description logics. Lin, F., Sattler, U., & Truszczynski, M. (Eds.), Proceedings
12th International Conference Principles Knowledge Representation
Reasoning. AAAI Press.
Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. Kaelbling, L. P.,
& Saffiotti, A. (Eds.), Proceedings 19th International Joint Conference Artificial Intelligence, pp. 364369. Professional Book Center.
Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (Eds.).
(2003). Description Logic Handbook: Theory, Implementation, Applications.
Cambridge University Press.
Bachmair, L., & Ganzinger, H. (2001). Resolution theorem proving. Robinson, J. A.,
& Voronkov, A. (Eds.), Handbook Automated Reasoning, pp. 1999. Elsevier
MIT Press.
Botoeva, E., Kontchakov, R., Ryzhikov, V., Wolter, F., & Zakharyaschev, M. (2014). Query
inseparability description logic knowledge bases. Baral, C., Giacomo, G. D.,
& Eiter, T. (Eds.), Proceedings 14th International Conference Principles
Knowledge Representation Reasoning. AAAI Press.
Bourhis, P., Morak, M., & Pieris, A. (2013). impact disjunction query answering
guarded-based existential rules. Rossi, F. (Ed.), Proceedings 23rd
International Joint Conference Artificial Intelligence. IJCAI/AAAI.
Bry, F., Eisinger, N., Eiter, T., Furche, T., Gottlob, G., Ley, C., Linse, B., Pichler, R., & Wei,
F. (2007). Foundations rule-based query answering. Antoniou, G., Amann, U.,
Baroglio, C., Decker, S., Henze, N., Patranjan, P., & Tolksdorf, R. (Eds.), Proceedings
3rd International Reasoning Web Summer School, Tutorial Lectures, Vol. 4636
Lecture Notes Computer Science, pp. 1153. Springer.
Cal, A., Gottlob, G., Lukasiewicz, T., Marnette, B., & Pieris, A. (2010). Datalog+/-:
family logical knowledge representation query languages new applications.
Proceedings 25th Annual ACM/IEEE Symposium Logic Computer
Science, pp. 228242. IEEE Computer Society.
Chekuri, C., & Rajaraman, A. (2000). Conjunctive query containment revisited. Theoretical
Computer Science, 239 (2), 211229.
Cuenca Grau, B., Halaschek-Wiener, C., Kazakov, Y., & Suntisrivaraporn, B. (2010). Incremental classification description logics ontologies. Journal Automated Reasoning,
44 (4), 337369.
Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2007a). right amount:
Extracting modules ontologies. Williamson, C. L., Zurko, M. E., PatelSchneider, P. F., & Shenoy, P. J. (Eds.), Proceedings 16th International World
Wide Web Conference, pp. 717726. ACM.
Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2007b). logical framework
modularity ontologies. Veloso, M. M. (Ed.), Proceedings 20th International
Joint Conference Artificial Intelligence, pp. 298303.
558

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2008). Modular reuse ontologies: Theory practice. Journal Artificial Intelligence Research, 31, 273318.
Cuenca Grau, B., Horrocks, I., Krotzsch, M., Kupke, C., Magka, D., Motik, B., & Wang, Z.
(2013). Acyclicity notions existential rules application query answering
ontologies. Journal Artificial Intelligence Research, 47, 741808.
Cuenca Grau, B., Horrocks, I., Motik, B., Parsia, B., Patel-Schneider, P. F., & Sattler, U.
(2008). OWL 2: next step OWL. Journal Web Semantics, 6 (4), 309322.
Dantsin, E., Eiter, T., Gottlob, G., & Voronkov, A. (2001). Complexity expressive
power logic programming. ACM Computing Surveys, 33 (3), 374425.
Del Vescovo, C., Klinov, P., Parsia, B., Sattler, U., Schneider, T., & Tsarkov, D. (2013).
Empirical study logic-based modules: Cheap cheerful. Alani, H., Kagal, L.,
Fokoue, A., Groth, P. T., Biemann, C., Parreira, J. X., Aroyo, L., Noy, N. F., Welty,
C., & Janowicz, K. (Eds.), Proceedings 12th International Semantic Web Conference, Part I, Vol. 8218 Lecture Notes Computer Science, pp. 84100. Springer.
Del Vescovo, C., Parsia, B., Sattler, U., & Schneider, T. (2011). modular structure
ontology: Atomic decomposition. Walsh, T. (Ed.), Proceedings 22nd International Joint Conference Artificial Intelligence, pp. 22322237. IJCAI/AAAI.
Eiter, T., Ianni, G., Schindlauer, R., Tompits, H., & Wang, K. (2006). Forgetting managing rules ontologies. Proceedings 2006 IEEE / WIC / ACM International
Conference Web Intelligence, pp. 411419. IEEE Computer Society.
Gatens, W., Konev, B., & Wolter, F. (2014). Lower upper approximations depleting
modules description logic ontologies. Schaub, T., Friedrich, G., & OSullivan,
B. (Eds.), Proceedings 21st European Conference Artificial Intelligence, Vol.
263 Frontiers Artificial Intelligence Applications, pp. 345350. IOS Press.
Ghilardi, S., Lutz, C., & Wolter, F. (2006a). damage ontology? case conservative extensions description logics. Doherty, P., Mylopoulos, J., & Welty, C. A.
(Eds.), Proceedings 10th International Conference Principles Knowledge
Representation Reasoning, pp. 187197. AAAI Press.
Ghilardi, S., Lutz, C., Wolter, F., & Zakharyaschev, M. (2006b). Conservative extensions
modal logic. Governatori, G., Hodkinson, I. M., & Venema, Y. (Eds.), Proceedings
6th Advances Modal Logic Conference, pp. 187207. College Publications.
Glimm, B., Horrocks, I., Motik, B., Stoilos, G., & Wang, Z. (2014). HermiT: OWL 2
reasoner. Journal Automated Reasoning, 53 (3), 245269.
Grohe, M., Schwentick, T., & Segoufin, L. (2001). evaluation conjunctive
queries tractable?. Vitter, J. S., Spirakis, P. G., & Yannakakis, M. (Eds.), Proceedings 33rd Annual ACM Symposium Theory Computing, pp. 657666.
ACM.
Horridge, M., & Bechhofer, S. (2011). OWL API: java API OWL ontologies.
Semantic Web, 2 (1), 1121.
Horridge, M., Parsia, B., & Sattler, U. (2008). Laconic precise justifications OWL.
Sheth, A. P., Staab, S., Dean, M., Paolucci, M., Maynard, D., Finin, T. W., &
559

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Thirunarayan, K. (Eds.), Proceedings 7th International Semantic Web Conference, Vol. 5318 Lecture Notes Computer Science, pp. 323338. Springer.
Horrocks, I., Kutz, O., & Sattler, U. (2006). even irresistible SROIQ. Doherty,
P., Mylopoulos, J., & Welty, C. A. (Eds.), Proceedings 10th International Conference Principles Knowledge Representation Reasoning, pp. 5767. AAAI
Press.
Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). SHIQ RDF
OWL: making web ontology language. Journal Web Semantics, 1 (1), 726.
Jimenez-Ruiz, E., & Cuenca Grau, B. (2011). LogMap: Logic-based scalable ontology
matching. Aroyo, L., Welty, C., Alani, H., Taylor, J., Bernstein, A., Kagal, L., Noy,
N. F., & Blomqvist, E. (Eds.), Proceedings 10th International Semantic Web
Conference, Part I, Vol. 7031 Lecture Notes Computer Science, pp. 273288.
Springer.
Jimenez-Ruiz, E., Cuenca Grau, B., Horrocks, I., & Berlanga Llavori, R. (2011). Supporting
concurrent ontology development: Framework, algorithms tool. Data & Knowledge
Engineering, 70 (1), 146164.
Jimenez-Ruiz, E., Cuenca Grau, B., Sattler, U., Schneider, T., & Berlanga Llavori, R.
(2008). Safe economic re-use ontologies: logic-based methodology tool
support. Bechhofer, S., Hauswirth, M., Homann, J., & Koubarakis, M. (Eds.),
Proceedings 5th European Semantic Web Conference, Vol. 5021 Lecture Notes
Computer Science, pp. 185199. Springer.
Kalyanpur, A., Parsia, B., Horridge, M., & Sirin, E. (2007). Finding justifications OWL
DL entailments. Aberer, K., Choi, K., Noy, N. F., Allemang, D., Lee, K., Nixon,
L. J. B., Golbeck, J., Mika, P., Maynard, D., Mizoguchi, R., Schreiber, G., & CudreMauroux, P. (Eds.), Proceedings 6th International Semantic Web Conference
2nd Asian Semantic Web Conference, Vol. 4825 Lecture Notes Computer
Science, pp. 267280. Springer.
Kalyanpur, A., Parsia, B., Sirin, E., & Cuenca Grau, B. (2006). Repairing unsatisfiable
concepts OWL ontologies. Sure, Y., & Domingue, J. (Eds.), Proceedings
3rd European Semantic Web Conference, Vol. 4011 Lecture Notes Computer
Science, pp. 170184. Springer.
Kalyanpur, A., Parsia, B., Sirin, E., & Hendler, J. A. (2005). Debugging unsatisfiable classes
OWL ontologies. Journal Web Semantics, 3 (4), 268293.
Konev, B., Kontchakov, R., Ludwig, M., Schneider, T., Wolter, F., & Zakharyaschev, M.
(2011). Conjunctive query inseparability OWL 2 QL tboxes. Burgard, W., &
Roth, D. (Eds.), Proceedings 25th AAAI Conference Artificial Intelligence.
AAAI Press.
Konev, B., Ludwig, M., Walther, D., & Wolter, F. (2012). logical dierence
lightweight description logic EL. Journal Artificial Intelligence Research, 44, 633
708.
Konev, B., Lutz, C., Ponomaryov, D. K., & Wolter, F. (2010). Decomposing description
logic ontologies. Lin, F., Sattler, U., & Truszczynski, M. (Eds.), Proceedings
560

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

12th International Conference Principles Knowledge Representation
Reasoning. AAAI Press.
Konev, B., Lutz, C., Walther, D., & Wolter, F. (2009). Formal properties modularisation.
Stuckenschmidt, H., Parent, C., & Spaccapietra, S. (Eds.), Modular Ontologies:
Concepts, Theories Techniques Knowledge Modularization, Vol. 5445 Lecture
Notes Computer Science, pp. 2566. Springer.
Konev, B., Lutz, C., Walther, D., & Wolter, F. (2013). Model-theoretic inseparability
modularity description logic ontologies. Artificial Intelligence, 203, 66103.
Konev, B., Walther, D., & Wolter, F. (2009). Forgetting uniform interpolation largescale description logic terminologies. Boutilier, C. (Ed.), Proceedings 21st
International Joint Conference Artificial Intelligence, pp. 830835.
Kontchakov, R., Lutz, C., Toman, D., Wolter, F., & Zakharyaschev, M. (2011). combined approach ontology-based data access. Walsh, T. (Ed.), Proceedings
22nd International Joint Conference Artificial Intelligence, pp. 26562661. IJCAI/AAAI.
Kontchakov, R., Wolter, F., & Zakharyaschev, M. (2010). Logic-based ontology comparison
module extraction, application DL-Lite. Artificial Intelligence, 174 (15),
10931141.
Koopmann, P., & Schmidt, R. A. (2014). Count forget: Uniform interpolation SHQontologies. Demri, S., Kapur, D., & Weidenbach, C. (Eds.), Proceedings 7th
International Joint Conference Automated Reasoning, Vol. 8562 Lecture Notes
Computer Science, pp. 434448. Springer.
Krotzsch, M., Rudolph, S., & Hitzler, P. (2008a). Description logic rules. Ghallab,
M., Spyropoulos, C. D., Fakotakis, N., & Avouris, N. M. (Eds.), Proceedings
18th European Conference Artificial Intelligence, Vol. 178 Frontiers Artificial
Intelligence Applications, pp. 8084. IOS Press.
Krotzsch, M., Rudolph, S., & Hitzler, P. (2008b). ELP: Tractable rules OWL 2. Sheth,
A. P., Staab, S., Dean, M., Paolucci, M., Maynard, D., Finin, T. W., & Thirunarayan,
K. (Eds.), Proceedings 7th International Semantic Web Conference, Vol. 5318
Lecture Notes Computer Science, pp. 649664. Springer.
Ludwig, M. (2014). Just: tool computing justifications w.r.t. ELH ontologies.
Bail, S., Glimm, B., Jimenez-Ruiz, E., Matentzoglu, N., Parsia, B., & Steigmiller, A.
(Eds.), Proceedings 3rd International Workshop OWL Reasoner Evaluation,
Vol. 1207 CEUR Workshop Proceedings, pp. 17. CEUR-WS.org.
Ludwig, M., & Konev, B. (2014). Practical uniform interpolation forgetting ALC
tboxes applications logical dierence. Baral, C., Giacomo, G. D., & Eiter, T.
(Eds.), Proceedings 14th International Conference Principles Knowledge
Representation Reasoning. AAAI Press.
Lutz, C., & Wolter, F. (2010). Deciding inseparability conservative extensions
description logic EL. Journal Symbolic Computation, 45 (2), 194228.
561

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

Lutz, C., & Wolter, F. (2011). Foundations uniform interpolation forgetting
expressive description logics. Walsh, T. (Ed.), Proceedings 22nd International
Joint Conference Artificial Intelligence, pp. 989995. IJCAI/AAAI.
Marnette, B. (2009). Generalized schema-mappings: termination tractability.
Paredaens, J., & Su, J. (Eds.), Proceedings 28th ACM SIGMOD Symposium
Principles Database Systems, pp. 1322. ACM.
Motik, B. (2006). Reasoning Description Logics Using Resolution Deductive
Databases. Ph.D. thesis, Univesitat Karlsruhe (TH), Karlsruhe, Germany.
Motik, B., Nenov, Y., Piro, R., Horrocks, I., & Olteanu, D. (2014). Parallel materialisation
datalog programs centralised, main-memory RDF systems. Brodley, C. E., &
Stone, P. (Eds.), Proceedings 28th AAAI Conference Artificial Intelligence,
pp. 129137. AAAI Press.
Motik, B., Patel-Schneider, P. F., & Parsia, B. (2012). OWL 2 web ontology language
structural specification functional-style syntax..
Nikitina, N., & Rudolph, S. (2014). (Non-)succinctness uniform interpolants general
terminologies description logic EL. Artificial Intelligence, 215, 120140.

Nonnengart, A., & Weidenbach, C. (2001). Computing small clause normal forms.
Robinson, J. A., & Voronkov, A. (Eds.), Handbook Automated Reasoning, pp. 335
367. Elsevier MIT Press.
Nortje, R., Britz, K., & Meyer, T. (2012). normal form hypergraph-based module
extraction SROIQ. Gerber, A., Taylor, K., Meyer, T., & Orgun, M. (Eds.),
Proceedings 8th Australasian Ontology Workshop, Vol. 969 CEUR Workshop
Proceedings, pp. 4051. CEUR-WS.org.
Nortje, R., Britz, K., & Meyer, T. (2013). Reachability modules description logic
SRIQ. McMillan, K. L., Middeldorp, A., & Voronkov, A. (Eds.), Proceedings
19th International Conference Logic Programming, Artificial Intelligence
Reasoning, Vol. 8312 Lecture Notes Computer Science, pp. 636652. Springer.
Robinson, J. A. (1965). Automatic deduction hyper-resolution. International Journal
Computer Mathematics, 1 (3), 227234.
Rousset, M.-C., & Ulliana, F. (2015). Extracting bounded-level modules deductive
RDF triplestores. Bonet, B., & Koenig, S. (Eds.), Proceedings 29th AAAI
Conference Artificial Intelligence, pp. 268274. AAAI Press.
Sattler, U., Schneider, T., & Zakharyaschev, M. (2009). kind module
extract?. Grau, B. C., Horrocks, I., Motik, B., & Sattler, U. (Eds.), Proceedings
22nd International Workshop Description Logics, Vol. 477 CEUR Workshop
Proceedings. CEUR-WS.org.
Schlicht, A., & Stuckenschmidt, H. (2009). Distributed resolution expressive ontology
networks. Polleres, A., & Swift, T. (Eds.), Proceedings 3rd International
Conference Web Reasoning Rule Systems, Vol. 5837, pp. 87101. Springer.
Schlobach, S., & Cornet, R. (2003). Non-standard reasoning services debugging
description logic terminologies. Gottlob, G., & Walsh, T. (Eds.), Proceedings
562

fiModule Extraction Expressive Ontology Languages via Datalog Reasoning

18th International Joint Conference Artificial Intelligence, pp. 355362. Morgan
Kaufmann.
Seidenberg, J., & Rector, A. L. (2006). Web ontology segmentation: Analysis, classification
use. Carr, L., Roure, D. D., Iyengar, A., Goble, C. A., & Dahlin, M. (Eds.),
Proceedings 15th International World Wide Web Conference, pp. 1322. ACM.
Stefanoni, G., Motik, B., & Horrocks, I. (2013). Introducing nominals combined
query answering approaches EL. desJardins, M., & Littman, M. L. (Eds.),
Proceedings 27th AAAI Conference Artificial Intelligence. AAAI Press.
Stuckenschmidt, H., Parent, C., & Spaccapietra, S. (Eds.). (2009). Modular Ontologies:
Concepts, Theories Techniques Knowledge Modularization, Vol. 5445 Lecture
Notes Computer Science. Springer.
Suntisrivaraporn, B. (2008). Module extraction incremental classification: pragmatic
approach EL+ ontologies. Bechhofer, S., Hauswirth, M., Homann, J., &
Koubarakis, M. (Eds.), Proceedings 5th European Semantic Web Conference,
Vol. 5021 Lecture Notes Computer Science, pp. 230244. Springer.
Suntisrivaraporn, B., Qi, G., Ji, Q., & Haase, P. (2008). modularization-based approach
finding justifications OWL DL entailments. Domingue, J., & Anutariya,
C. (Eds.), Proceedings 3rd Asian Semantic Web Conference, Vol. 5367 Lecture
Notes Computer Science, pp. 115. Springer.
Tsarkov, D., & Palmisano, I. (2012). Chainsaw: metareasoner large ontologies.
Horrocks, I., Yatskevich, M., & Jimenez-Ruiz, E. (Eds.), Proceedings 1st International Workshop OWL Reasoner Evaluation, Vol. 858 CEUR Workshop
Proceedings. CEUR-WS.org.
W3C OWL Working Group (2012). OWL 2 web ontology language document overview
(second edition). W3C recommendation, World Wide Web Consortium.
Wang, K., Wang, Z., Topor, R. W., Pan, J. Z., & Antoniou, G. (2014). Eliminating concepts
roles ontologies expressive descriptive logics. Computational Intelligence,
30 (2), 205232.
Wang, Z., Wang, K., Topor, R. W., & Pan, J. Z. (2010). Forgetting knowledge bases
DL-Lite. Annals Mathematics Artificial Intelligence, 58 (1-2), 117151.
Zhou, Y., Cuenca Grau, B., Horrocks, I., Wu, Z., & Banerjee, J. (2013). Making
triple store: Query answering OWL 2 using RL reasoner. Schwabe, D.,
Almeida, V. A. F., Glaser, H., Baeza-Yates, R. A., & Moon, S. B. (Eds.), Proceedings
22nd International World Wide Web Conference, pp. 15691580. International
World Wide Web Conferences Steering Committee / ACM.
Zhou, Y., Cuenca Grau, B., Nenov, Y., Kaminski, M., & Horrocks, I. (2015). PAGOdA:
Pay-as-you-go ontology query answering using datalog reasoner. Journal Artificial
Intelligence Research, 54, 309367.
Zhou, Y., Nenov, Y., Cuenca Grau, B., & Horrocks, I. (2013). Complete query answering
horn ontologies using triple store. Alani, H., Kagal, L., Fokoue, A., Groth,
P. T., Biemann, C., Parreira, J. X., Aroyo, L., Noy, N. F., Welty, C., & Janowicz, K.
563

fiArmas Romero, Kaminski, Cuenca Grau, & Horrocks

(Eds.), Proceedings 12th International Semantic Web Conference, Part I, Vol.
8218 Lecture Notes Computer Science, pp. 720736. Springer.
Zhou, Y., Nenov, Y., Cuenca Grau, B., & Horrocks, I. (2014). Pay-as-you-go OWL query
answering using triple store. Brodley, C. E., & Stone, P. (Eds.), Proceedings
28th AAAI Conference Artificial Intelligence, pp. 11421148. AAAI Press.

564

fiJournal Artificial Intelligence Research 55 (2016) 317-359

Submitted 08/15; published 02/16

Adaptive Contract Design Crowdsourcing Markets:
Bandit Algorithms Repeated Principal-Agent Problems
Chien-Ju Ho

ch624@cornell.edu

Cornell University, Ithaca, NY, USA

Aleksandrs Slivkins

slivkins@microsoft.com

Microsoft Research, New York, NY, USA

Jennifer Wortman Vaughan

jenn@microsoft.com

Microsoft Research, New York, NY, USA

Abstract
Crowdsourcing markets emerged popular platform matching available
workers tasks complete. payment particular task typically set
tasks requester, may adjusted based quality completed work,
example, use bonus payments. paper, study requesters
problem dynamically adjusting quality-contingent payments tasks. consider
multi-round version well-known principal-agent model, whereby round
worker makes strategic choice effort level directly observable
requester. particular, formulation significantly generalizes budget-free online task
pricing problems studied prior work. treat problem multi-armed bandit
problem, arm representing potential contract. cope large (and
fact, infinite) number arms, propose new algorithm, AgnosticZooming,
discretizes contract space finite number regions, effectively treating region
single arm. discretization adaptively refined, promising regions
contract space eventually discretized finely. analyze algorithm,
showing achieves regret sublinear time horizon substantially improves
non-adaptive discretization (which competing approach literature).
results advance state art several different topics: theory crowdsourcing
markets, principal-agent problems, multi-armed bandits, dynamic pricing.

1. Introduction
Crowdsourcing harnesses human intelligence common sense complete tasks
difficult accomplish using computers alone. Crowdsourcing markets, Amazon Mechanical Turk CrowdFlower, platforms designed match available human workers
tasks complete. Using platforms, requesters may post tasks would
like completed, along amount money willing pay. Workers
choose whether accept available tasks complete work.
course human workers equal, human-produced work. tasks,
proofreading English text, easier workers others, requiring less
effort produce high quality results. Additionally, workers dedicated
others, willing spend extra time make sure task completed properly. encourage
high quality results, requesters may set quality-contingent bonus payments top
base payment task, rewarding workers producing valuable output.
c
2016
AI Access Foundation. rights reserved.

fiHo, Slivkins, & Vaughan

viewed offering workers contract specifies much paid based
quality output.1
examine requesters problem dynamically setting quality-contingent payments
tasks. consider setting time evolves rounds. round, requester
posts new contract, performance-contingent payment rule specifies different levels
payment different levels output quality. random, unidentifiable worker
arrives market strategically decides whether accept requesters task
much effort exert; choice effort level directly observable requester.
worker completes task (or chooses complete it), requester observes
workers output, pays worker according offered contract, adjusts
contract next round. properties random worker (formally: distribution
workers types) known requester, may learned time.
goal requester maximize expected utility, value receives completed
work minus payments made. call dynamic contract design problem.
concreteness, consider special case worker strategically choose
perform task low effort high effort, task may completed either
low quality high quality. low effort incurs cost results low quality,
turn brings value requester. high effort leads high quality
positive probability (which may vary one worker another, unknown
requester). requester observes quality completed tasks, therefore cannot
always infer effort level. example captures two main tenets model:
properties random worker unknown requester workers strategic
decisions unobservable.
treat dynamic contract design problem multi-armed bandit (MAB) problem,
arm representing potential contract. Since action space large (potentially
infinite) well-defined real-valued structure, natural consider algorithm
uses discretization. algorithm, AgnosticZooming, divides action space
regions, chooses among regions, effectively treating region single metaarm. discretization defined adaptively, promising areas
action space eventually discretized finely less promising areas.
general idea adaptive discretization appeared prior work MAB (Kleinberg,
Slivkins, & Upfal, 2008; Bubeck, Munos, Stoltz, & Szepesvari, 2011a; Slivkins, 2014, 2011),
approach adaptive discretization new problem-specific. main difficulty,
compared prior work, algorithm given information links
observable numerical structure contracts expected utilities thereof.
analyze performance, propose concept called width dimension measures
nice particular problem instance is. show AgnosticZooming achieves
regret sublinear time horizon problem instances small width dimension.
particular, width dimension d, achieves regret O(log (d+1)/(d+2) )
rounds. problem instances large width dimension, AgnosticZooming matches
performance naive algorithm uniformly discretizes space runs
1. tasks, labeling websites relevant particular search query not, verifying
quality work may difficult completing task. tasks assigned batches,
batch containing one instances correct answer already known (often called
gold data). Quality-contingent payments based known instances.

318

fiAdaptive Contract Design Crowdsourcing Markets

standard bandit algorithm. illustrate general results via corollaries special
cases, including high-low example described above. support theoretical results
simulations.
Further, consider special case setting worker chooses whether
accept reject given task. special case corresponds dynamic task pricing
problem previously studied literature. results significantly improve prior
work problem.
contributions summarized follows. define broad, practically important setting crowdsourcing markets; identify novel problem-specific structure,
algorithm regret bounds; distill ideas prior work work
structures; argue approach productive deriving corollaries comparing
prior work; identify analyze specific examples theory applies.
main conceptual contributions model adaptive discretization approach
mentioned above. Finally, paper prompts research dynamic contract design
along several directions outline conclusion.
1.1 Related Work
work builds three areas research. First, model viewed multi-round
version classical principal-agent model contract theory (Laffont & Martimort,
2002). single round model corresponds basic principal-agent setting,
adverse selection (unknown workers type) moral hazard (unobservable workers decisions). Unlike much existing work contract theory, prior worker types
known principal, may learned time. Accordingly, techniques
different employed contract theory.
Second, methods build developed rich literature MAB
continuous outcome spaces. closest line work Lipschitz MAB (Kleinberg
et al., 2008), algorithm given distance function arms,
expected rewards arms assumed satisfy Lipschitz-continuity (or relaxation
thereof) respect distance function (Agrawal, 1995; Kleinberg, 2004; Auer,
Ortner, & Szepesvari, 2007; Kleinberg et al., 2008; Bubeck et al., 2011a; Slivkins, 2014).
related techniques idea adaptive discretization (Kleinberg et al., 2008;
Bubeck et al., 2011a; Slivkins, 2014), particular, zooming algorithm (Kleinberg
et al., 2008; Slivkins, 2014). However, zooming algorithm cannot applied directly
setting required numerical similarity information immediately
available. problem also arises web search advertising, natural
assume algorithm observe tree-shaped taxonomy arms (Kocsis &
Szepesvari, 2006; Munos & Coquelin, 2007; Pandey, Agarwal, Chakrabarti, & Josifovski,
2007) used explicitly reconstruct relevant parts underlying metric
space (Slivkins, 2011; Bull, 2013). take different approach, using notion virtual
width estimate similarity information. Explicit comparisons results
prior MAB work made throughout paper.
Finally, work follows several theoretical papers pricing crowdsourcing
markets (Kleinberg & Leighton, 2003; Badanidiyuru, Kleinberg, & Singer, 2012; Singer
& Mittal, 2013; Singla & Krause, 2013; Badanidiyuru, Kleinberg, & Slivkins, 2013).
319

fiHo, Slivkins, & Vaughan

particular, Badanidiyuru et al. (2012) Singla Krause (2013) study version
setting simple, single-price contracts (independent output), focus
dealing global budget constraint.
thorough literature review (including discussion related empirical
work) found Section 9.

2. Setting: Dynamic Contract Design Problem
section, formally define problem set solve discuss
implications several aspects model.
2.1 Model
start describing static model, captures happens single round
interaction requester worker. described above, version
standard principal-agent model (Laffont & Martimort, 2002). define dynamic
model, extension static model multiple rounds, new worker arriving
round. detail objective pricing algorithm simplifying
assumptions make throughout paper. Finally, compare setting
classic multi-armed bandit problem.
2.1.1 Static Model
begin description occurs interaction requester
single worker. requester first posts task may completed worker,
contract specifying worker paid completes task. task
completed, requester pays worker specified contract, requester
derives value completed task; normalization, assume value derived
[0, 1]. requesters utility given task value minus payment
worker.
worker observes contract decides whether complete task,
also chooses level effort exert, turn determines cost (in terms time,
energy, missed opportunities) distribution quality work. model
quality, assume (small) finite set possible outcomes result
worker completing task (or choosing complete it), realized outcome
determines value requester derives task. realized outcome
observed requester, contract requester offers mapping
outcomes payments worker.
emphasize two crucial (and related) features principal-agent model:
mapping effort level outcomes randomized, effort level
directly observed requester. line standard observation
crowdsourcing even honest, high-effort workers occasionally make errors.
workers utility given task payment requester minus
cost corresponding chosen effort level. Given contract offered, worker
chooses effort level strategically maximize expected utility. Crucially,
chosen effort level directly observable requester.
320

fiAdaptive Contract Design Crowdsourcing Markets

workers choice perform task modeled separate effort level zero
cost (called null effort level) separate outcome zero value zero payment
(called null outcome) null effort level deterministically leads null
outcome, effort level lead outcome.
mapping outcomes requesters value called requesters value
function. mapping effort levels costs called cost function,
mapping effort levels distributions outcomes called production function.
purposes paper, worker completely specified two functions;
say cost function production function comprise workers type. Unlike
traditional versions principal-agent problem, setting workers type
observable requester, prior given.
2.1.2 Dynamic Model
dynamic model consider paper natural extension static model
multiple rounds multiple workers. still concerned single requester.
round, new worker arrives. assume stochastic environment
workers type round i.i.d. sample fixed unknown distribution
types, called supply distribution. requester posts new task contract
task. tasks type, sense set possible effort
levels set possible outcomes tasks. worker strategically
chooses effort level maximize expected utility task. Based
chosen effort level workers production function, outcome realized.
requester observes outcome (but workers effort level) pays worker
amount specified contract. type arriving worker never revealed
requester. requester adjust contract one round another, total
utility sum utility rounds. simplicity, assume number
rounds known advance, though assumption relaxed using standard
doubling trick (Cesa-Bianchi & Lugosi, 2006) full executions algorithm
repeated phases exponentially increasing time horizons.
2.1.3 Dynamic Contract Design Problem
Throughout paper, take point view requester interacting workers
dynamic model. algorithms examine dynamically choose contracts offer
round goal maximizing requesters expected utility. problem
instance consists several quantities, known algorithm,
not. known quantities number outcomes, requesters value
function, time horizon (i.e., number rounds). latent quantities
number effort levels, set worker types, supply distribution. algorithm
adjusts contract round round observes realized outcomes receives
feedback.
focus contracts bounded (offer payments [0, 1]), monotone (assign
equal higher payments outcomes higher value requester). Let X
set bounded, monotone contracts. compare given algorithm given
subset candidate contracts Xcand X. Letting OPT(Xcand ) optimal utility
321

fiHo, Slivkins, & Vaughan

contracts Xcand , goal minimize algorithms regret R(T |Xcand ), defined
OPT(Xcand ) minus algorithms expected utility.
subset Xcand may finite infinite, possibly Xcand = X. natural
example finite Xcand set bounded, monotone contracts payments
integer multiples > 0; call uniform mesh granularity ,
denote Xcand ().
2.1.4 Notation
Let v() value function requester, v() denoting value outcome .
Let set outcomes let number non-null outcomes.
index outcomes = {0, 1, 2 , . . . , m} order increasing value (ties broken
arbitrarily), convention 0 null outcome.
Let ci () fi () cost function production function type i.
cost choosing effort level e ci (e),
Pthe probability obtaining outcome
chosen effort e fi (|e). Let Fi (|e) = 0 fi ( 0 |e) probability obtaining
outcome least good chosen effort e.
Recall contract x function outcomes (non-negative) payments.
contract x offered worker sampled i.i.d. supply distribution, V (x)
expected value requester, P (x) 0 expected payment, U (x) = V (x)P (x)
expected utility requester. Let OPT(Xcand ) = supxXcand U (x).
2.1.5 Assumption: First-Order Stochastic Dominance (FOSD)
Given two effort levels e e0 , say e FOSD e0 type Fi (|e) Fi (|e0 )
outcomes , strict inequality least one outcome.2 say type
satisfies FOSD assumption two distinct effort levels, one effort level FOSD
type i. assume types satisfy assumption.
2.1.6 Assumption: Consistent Tie-Breaking
multiple effort levels maximize expected utility given worker contract x,
assume tie broken consistently sense worker chooses effort
level contract leads particular tie. assumption minor;
avoided (with minor technical complications) adding random perturbations
contracts. assumption implicit throughout paper.
2.2 Discussion
jumping results, discuss implications several aspects model
detail.
2.2.1 Number Outcomes
results assume small number outcomes. regime important practice
several reasons. First, tasks naturally small number outcomes.
2. mimics standard notion FOSD two distributions linearly ordered set.

322

fiAdaptive Contract Design Crowdsourcing Markets

example, binary labeling task four possible outcomes completed:
{yes/no} {correct/incorrect}. Second, often makes sense group together multiple
outcomes similar value requester (such false positives false negatives)
value known precisely. added benefit contracts become
simpler workers perspective. Third, even task completed many
different ways, quality may difficult evaluate fine granularity; good example
translation sentence. Fourth, even fine-grained quality evaluation exists,
error count speech transcription tasks, may difficult make consistent
across different tasks.
Even = 2 non-null outcomes, setting studied before. special
case = 1 equivalent dynamic pricing problem Kleinberg Leighton
(2003); obtain improved results it, too.
2.2.2 Benchmark
benchmark OPT() considers contracts bounded monotone. practice,
restricting contracts may appealing human parties involved. However,
restriction without loss generality: problem instances monotone
contracts optimal; see Appendix example. Further, clear whether
bounded monotone contracts optimal among monotone contracts.
benchmark OPT(Xcand ) relative given set Xcand , typically finite
discretization contract space. two reasons this. First, crowdsourcing platforms may require payments multiples minimum unit (e.g., one
cent), case natural restrict attention contracts satisfying
constraint. Second, achieving guarantees relative OPT(X) full generality
problem appears beyond reach techniques. many machine learning
scenarios, useful consider restricted benchmark set set alternatives compare to.3 settings, considered important handle arbitrary benchmark sets,
do.
One known approach obtain guarantees relative OPT(X) start
finite Xcand X, design algorithm guarantees relative OPT(Xcand ), then,
separate result, bound discretization error OPT(X) OPT(Xcand ). choice
Xcand drives tradeoff discretization error regret R(T |Xcand ),
one choose Xcand optimize tradeoff. However, one upper-bound
discretization error (very) simple special cases (see Section 5), unclear whether
extended full generality dynamic contract design.
2.2.3 Alternative Worker Models
One crucial tenets model workers maximize expected utility.
rationality assumption standard economics, often used make
problem amenable rigorous analysis. However, considerable literature
suggesting practice workers may deviate rational behavior. Thus,
worth pointing results rely heavily rationality assumption.
3. particularly relevant analogy contextual bandits policy sets (Dudik, Hsu, Kale, Karampatziakis,
Langford, Reyzin, & Zhang, 2011).

323

fiHo, Slivkins, & Vaughan

FOSD assumption (which also fairly standard) circumvented, too. fact,
assumptions regarding worker behavior serve enable us prove Lemma 3.1,
specifically guarantee collective worker behavior satisfies natural
increment payment property used proof Lemma 3.1: requester increases
increment payment particular outcome (as described next section),
probability obtaining outcome least good also increases. particular,
property consistent worker behavior takes account long-term effects
changes reputation scores. also consistent workers acting upon subjective
(and possibly incorrect) beliefs offered contract, beliefs
guaranteed base payment may actually depend quality submitted work.4
2.2.4 Minimum Wage
ethical legal reasons one may want enforce form minimum wage.
expressed within model minimal payment completed task, i.e.,
non-null outcome. algorithm easily modified accommodate
constraint. Essentially, suffices restrict action space contracts pay least
completed task. Formally, increment space defined Section 3
[, 1] [0, 1]m1 rather [0, 1]m , quadrants cell defined
splitting cell half dimension. results easily carry version
(restricting Xcand contracts pay least completed task). omit
discussion issue sake simplicity.
2.2.5 Comparison Multi-Armed Bandits (MAB)
Dynamic contract design modeled special case MAB problem
additional, problem-specific structure. basic MAB problem defined follows.
algorithm repeatedly chooses actions fixed action space collects rewards
chosen actions; available actions traditionally called arms. specifically, time
partitioned rounds, round algorithm selects arm receives
reward chosen arm. information, reward algorithm
would received choosing alternative arm, revealed. MAB problem
stochastic rewards, reward arm given round i.i.d. sample
distribution depends arm round. standard measure
algorithms performance regret respect best fixed arm, defined
difference expected total reward benchmark (usually best fixed arm)
algorithm.
Thus, dynamic contract design naturally modeled MAB problem
stochastic rewards, arms correspond monotone contracts. prior work
MAB large infinite action spaces often assumes known upper bounds similarity arms. precisely, prior work would assume algorithm given
metric contracts expected rewards Lipschitz-continuous respect
4. worker model incorporates subjective beliefs suggested Ho, Slivkins, Suri,
Vaughan (2015) based experimental evidence, model satisfies increment payment property
mentioned above.

324

fiAdaptive Contract Design Crowdsourcing Markets

D, i.e., upper bounds |U (x)U (y)| D(x, y) two contracts x, y.5 However,
setting upper bounds absent. hand, problem
auxiliary structure compared standard MAB setting. particular, algorithms
reward decomposes value payment, determined outcome,
turn probabilistically determined workers strategic choice effort
level. Effectively, auxiliary structure provides soft information similarity
contracts, sense numerically similar contracts usually (but always)
induce similar response workers.
2.2.6 Applicability Model
Despite considerable generality, model somewhat idealized. Let us discuss several
potential concerns regarding applicable realistic model is.
implicit intuition behind using performance-based payments incentivize better quality. growing empirical literature incentives crowdsourcing markets
finds happens types tasks others. particular, experiments work Ho et al. (2015) suggest happens task
effort-responsive, sense one obtain higher quality work increasing effort,
effort levels costly worker. observation consistent
worker model: indeed, effort-responsiveness task joint property production
function cost function implies significant response sufficiently increased
quality-based payments. Ho et al. propose pilot experiments determine whether given
type tasks effort-responsive, turn would shed light whether use qualitybased payments tasks practice. comprehensive discussion related
empirical work found Section 9. also observe model results
single non-null outcome applicable novel even task effort-responsive.
Following bulk prior work dynamic pricing MAB, assume
collective worker response given contract, given distribution outcomes,
depend contracts offered past, algorithm used
choose future contracts. Thus, model possibility price experimentation
may alter future worker responses, workers may try game system.
effects easy model extremely difficult analyze, even relatively
simple scenario single non-null outcome emphasis adaptive discretization.
Additionally, available empirical work provide sufficient guidance
choose realistic model effects among theoretically plausible alternatives.
leave future work.
MAB point view, model incorporate possibility
worker response may intrinsically change time, fact requesters may
hard budget constraints total amount money spend. reflects
limitations state-of-the-art work MAB: adaptive discretization, budgets, adversarial change time fairly well-understood separately, two (let
alone three) studied jointly. said, conjecture techniques
would useful generalizing dynamic pricing dynamic contract design richer
settings.
5. upper bound informative D(x, y) < 1.

325

fiHo, Slivkins, & Vaughan

Likewise, model scenario requesters stream tasks overwhelms
crowdsourcing market causes drastic change available worker population
(and therefore worker response). particular, assume worker pool
sufficiently large accommodate requester. relatively benign assumption
large crowdsourcing system.
deploy dynamic selection prices contracts practice (regardless particular
algorithm used) crowdsourcing platform needs enable requesters change
prices/contracts relatively fast response observed worker responses. feature
currently instrumented commercial platforms Amazon Mechanical Turk,
appears easily implementable engineering point view. believe main
hurdle would incorporate dynamic price/contract selection overall economic
design market. Given multitude existing crowdsourcing markets relative
ease deploying new market designs, believe direction well worth studying.

3. Algorithm: AgnosticZooming
section, specify algorithm. call AgnosticZooming zooms
promising areas action space, without knowing precise
measure similarity contracts. zooming viewed dynamic
form discretization. stating algorithm itself, discuss discretization
action space detail, laying groundwork approach.
3.1 Discretization Action Space
round, AgnosticZooming algorithm partitions action space several regions chooses among regions, effectively treating region meta-arm.
section, discuss subsets action space used regions, introduce
useful notions properties subsets.
3.1.1 Increment Space Cells
describe approach discretization, useful think contracts terms
increment payments. Specifically, represent monotone contract x : [0, )
vector x [0, )m , number non-null outcomes x = x()x( 1)
0 non-null outcome . (Recall convention 0 null outcome
x(0) = 0.) call vector increment representation contract x, denote
incr(x). Note x bounded, incr(x) [0, 1]m . Conversely, call contract
weakly bounded monotone increment representation lies [0, 1]m .
contract necessarily bounded.
discretize space weakly bounded contracts, viewed multi-dimensional
unit cube. precisely, define increment space [0, 1]m convention
every vector represents corresponding weakly bounded contract. region discretization closed, axis-aligned m-dimensional cube increment space; henceforth,
cubes called cells. size cell length one side. cell called
relevant contains least one candidate contract. relevant cell called atomic
contains exactly one candidate contract, composite otherwise.
326

fiAdaptive Contract Design Crowdsourcing Markets

composite cell C, algorithm use two contracts: maximal corner,
denoted x+ (C), increment payments maximal, minimal corner,
denoted x (C), increment payments minimal. two contracts
called anchors C. atomic cell C, algorithm use one contract:
unique candidate contract, also called anchor C. Note anchors
necessarily candidate contracts.
3.1.2 Virtual Width
take advantage problem structure, essential estimate similar
contracts within given composite cell C are. Ideally, would like know maximal
difference expected utility:
width(C) = supx,yC |U (x) U (y)| .
estimate width using proxy, called virtual width, expressed terms
anchors:


VirtWidth(C) = V (x+ (C)) P (x (C)) V (x (C)) P (x+ (C)) .
(1)
definition one crucial place problem structure used. (Note
difference utility anchors.) useful due following lemma (proved
Section 3.3).
Lemma 3.1. types satisfy FOSD assumption consistent tie-breaking holds,
width(C) VirtWidth(C) composite cell C.
Recall proof lemma place paper use
assumptions worker behavior. developments hold model worker
behavior satisfies Lemma 3.1.
3.2 Description Algorithm
ideas place, ready describe algorithm. high-level
outline AgnosticZooming simple. algorithm maintains set active cells
cover increment space times. Initially, single active cell
comprising entire increment space. round t, algorithm chooses one active
cell Ct using upper confidence index posts contract xt sampled uniformly random
among anchors cell. observing feedback, algorithm may choose
zoom Ct , removing Ct set active cells activating relevant quadrants
thereof, quadrants cell C defined 2m sub-cells half size
one corners center C. remainder section, specify
cell Ct chosen (the selection rule), algorithm decides whether zoom
Ct (the zooming rule).
Let us first introduce notation. Consider cell C active round t. Let
U (C) expected utility single round C chosen algorithm,
i.e., average expected utility anchor(s) C. Let nt (C) number times
cell chosen round t. Consider rounds C chosen
327

fiHo, Slivkins, & Vaughan

algorithm round t. Let Ut (C) average utility rounds.
composite cell C, let Vt+ (C) Pt+ (C) average value average payment
rounds anchor x+ (C) chosen. Similarly, let Vt (C) Pt (C) average
value average payment rounds anchor x (C) chosen. Accordingly,
estimate virtual width composite cell C time


Wt (C) = Vt+ (C) Pt (C) Vt (C) Pt+ (C) .
(2)
bound deviations, define confidence radius
p
radt (C) = crad log(T )/nt (C),

(3)

absolute constant crad ; analysis, crad 16 suffices. show
high probability sample averages defined stay within radt (C) respective
expectations. high probability event holds, width estimate Wt (C) always
within 4 radt (C) VirtWidth(C).
algorithm pseudocode summarized Algorithm 1. selection rule
zooming rule explained detail below.
ALGORITHM 1: AgnosticZooming
Inputs: subset Xcand X candidate contracts.
Data structure: Collection cells. Initially, = { [0, 1]m }.
round = 1
Let Ct = argmaxCA (C), () defined Equation (4).
Sample contract xt u.a.r. among anchors Ct . \\ Anchors defined Section 3.1.
Post contract xt observe feedback.
|Ct Xcand | > 1 5 radt+1 (Ct ) < Wt+1 (Ct )
{all relevant quadrants Ct } \ {Ct }. \\ C relevant |C Xcand | 1.

3.2.1 Selection Rule
selection rule follows. round t, algorithm chooses active cell C
maximal index (). (C) upper confidence bound expected utility
candidate contract C, defined
(
Ut (C) + radt (C)
C atomic cell,
(C) =
(4)
Ut (C) + Wt (C) + 5 radt (C) otherwise.
nt (C) = 0, Ut (C) Wt (C) initialized finite values. Since radt (C)
infinite nt (C) = 0, AgnosticZooming first select cell never selected
time t.
3.2.2 Zooming Rule
zoom composite cell Ct
Wt+1 (Ct ) > 5 radt+1 (Ct ),
328

fiAdaptive Contract Design Crowdsourcing Markets

i.e., uncertainty due random sampling, expressed confidence radius, becomes
sufficiently small compared uncertainty due discretization, expressed virtual
width. never zoom atomic cells.
3.2.3 Notes Integer Payments
practice may necessary allow contracts payments integer
multiples amount , e.g., whole cents. (In case assume candidate
contracts property, too.) redefine two anchors composite
cell: maximal (resp., minimal) anchor nearest allowed contract maximal
(resp., minimal) corner. Width redefined supremum allowed contracts
given cell. modifications, analysis goes without significant
changes. omit discussion issue.
3.3 Proof Lemma 3.1 (virtual width)
two vectors x, x0 <m , write x0 x x0 pointwise dominates x, i.e., x0j xj
j. two monotone contracts x, x0 , write x0 x incr(x0 ) incr(x).
Claim 3.2. Consider worker whose type satisfies FOSD assumption two weakly
bounded contracts x, x0 x0 x. Let e (resp., e0 ) effort levels exerted
worker offered contract x (resp., x0 ). e FOSD e0 .
Proof. sake contradiction, assume e FOSD e0 . Note e 6= e0 .
Let workers type. Recall Fi (|e) denotes probability generating
outcome 0 given effort level e. Define F = ( Fi (1|e) , . . . , Fi (m|e) ), define F0
similarly e0 .
Let x x0 increment representations x x0 . Given contract x,
workers expected utility effort level e Ui (x|e) = x F ci (e). Since e optimal
effort level given contract, Ui (x|e) Ui (x|e0 ), therefore
x F x F0 ci (e) ci (e0 ).
Similarly, since e0 optimal effort level given contract x0 ,
x0 F0 x0 F ci (e0 ) ci (e).
Combining two inequalities, obtain
(x x0 ) (F F0 ) 0.

(5)

Note Equation (5) holds equality Ui (x|e) = Ui (x|e0 ) Ui (x0 |e) =
Ui (x0 |e0 ), worker breaks tie e e0 different way two different
contracts. contradicts consistent tie-breaking assumption. However, Equation (5)
cannot hold strict equality, either, x0 x (since e FOSD e0 )
F F0 Fi (|e) > Fi (|e0 ) outcome > 0. Therefore obtain
contradiction, completing proof.
proof Claim 3.2 place paper directly use consistent
tie-breaking assumption. (But rest paper relies claim.)
329

fiHo, Slivkins, & Vaughan

Claim 3.3. Assume types satisfy FOSD assumption. Consider weakly bounded
contracts x, x0 x0 x. V (x0 ) V (x) P (x0 ) P (x).
Proof. Consider worker, let type. Let e e0 chosen effort levels
contracts x x0 , respectively. FOSD assumption, either e = e0 , e0 FOSD
e, e FOSD e0 . Claim 3.2 rules latter possibility.
Define vectors F F0 proof Claim 3.2. Note F0 F.
P = x F P 0 = x0 F0 expected payment contracts x x0 ,
respectively. Further, letting v denote increment representation requesters value
outcome, V = v F V 0 = v F0 expected requesters value contracts
x x0 , respectively. Since x0 x F0 F, follows P 0 P V 0 V . Since
holds worker, also holds expectation workers.
finish proof Lemma 3.1, consider composite cell C anchors x+ =
x = x (C), fix contract x C. Since x+ x x , Claim 3.3
follows V (x+ ) V (x) V (x ) P (x+ ) P (x) P (x ). Therefore |U (x)
U (y)| VirtWidth(C). Taking supremum x C over, obtain width(C)
VirtWidth(C), claimed.
x+ (C)

4. Regret Bounds Discussion
present main regret bound AgnosticZooming. Formulating result requires
new, problem-specific structure. Stated terms structure, result somewhat difficult access. explain significance, state several corollaries, compare
results prior work.
4.1 Main Result
start main regret bound. Like algorithm itself, regret bound parameterized set Xcand candidate contracts; goal bound algorithms
regret respect candidate contracts.
Recall OPT(Xcand ) = supxXcand U (x) optimal expected utility candidate
contracts. algorithms regret respect candidate contracts R(T |Xcand ) =
OPT(Xcand ) U , time horizon U expected cumulative utility
algorithm.
Define badness (x) contract x X difference expected utility
optimal candidate contract x: (x) = OPT(Xcand ) U (x). Let X = {x Xcand :
(x) }.
interested cells potentially used AgnosticZooming.
Formally, recursively define collection feasible cells follows: (i) cell [0, 1]m
feasible, (ii) feasible cell C, relevant quadrants C feasible. Note
definition feasible cell implicitly depends set Xcand candidate contracts:
definition, feasible cell one contains candidate contract.
Let F denote collection feasible, composite cells C VirtWidth(C)
. Xcand , let F (Y ) collection cells C F overlap ,
let N (Y ) = |F (Y )|; sometimes write N (Y |Xcand ) place N (Y ) emphasize
dependence Xcand .
330

fiAdaptive Contract Design Crowdsourcing Markets

Using structure defined above, main theorem stated follows. prove
theorem Section 6.
Theorem 4.1. Consider dynamic contract design problem types satisfying
FOSD assumption constant number outcomes. Consider AgnosticZooming, parameterized set Xcand candidate contracts. Assume max(2m + 1, 18).
absolute constant 0 > 0 > 0,
X

R(T |Xcand ) + O(log )

=2j : jN

N 0 (X |Xcand )
.


(6)

Remark 1. discussed Section 2.2, target practically important case small
number outcomes. impact larger exponential dependence
O() notation, and, importantly, increased number candidate policies (typically
exponential given granularity).
Remark 2. regret bounds depend number worker types, line
prior work dynamic pricing. Essentially, bandit approaches tend
depend expected reward given arm (and perhaps also variance),
finer properties distribution.
Equation (6) shape similar several regret bounds literature,
discussed below. make apparent, observe regret bounds bandits
metric spaces often stated terms covering numbers. (For fixed collection
F subsets given ground set X, covering number subset X relative
F smallest number subsets F sufficient cover .) numbers
N (Y |Xcand ) are, essentially, covering feasible cells virtual width close
. make point precise follows. Let -minimal cell cell F
contain cell F . Let Nmin (Y ) covering number relative
collection -minimal cells, i.e., smallest number -minimal cells sufficient
cover .
N (Y ) dlog 1 e Nmin (Y ) Xcand 0,

(7)

smallest size feasible cell.6 Thus, Equation (6) easily restated
using covering numbers Nmin () instead N ().
4.2 Corollary: Polynomial Regret
Literature regret-minimization often states polynomial regret bounds form
R(T ) = O(T ), < 1. covering-number regret bounds precise versatile, exponent polynomial regret bound expresses algorithms performance
particularly succinct lucid way.
bandits metric spaces exponent typically determined appropriately defined notion dimension, covering dimension,7 succinctly
6. prove Equation (7), observe cell C F (Y ) exists -minimal cell C 0 C,
-minimal cell C 0 exist dlog 1 e cells C F (Y ) C 0 C.
7. Given covering numbers N (), covering dimension smallest 0 N (Y ) =
O(d ) > 0.

331

fiHo, Slivkins, & Vaughan

captures difficulty problem instance. Interestingly, dependence
dimension typically shape; = (d + 1)/(d + 2), several different notions
dimension. line tradition, define width dimension:
n

WidthDim = inf 0 : N 0 (X |Xcand ) > 0 , > 0.
(8)
Note width dimension depends Xcand problem instance, parameterized constant > 0. optimizing choice Equation (6), obtain
following corollary.
Corollary 4.2. Consider setting Theorem 4.1. > 0, let = WidthDim .

R(T |Xcand ) O( log ) (1+d)/(2+d) .

(9)

width dimension similar zooming dimension work Kleinberg
et al. (2008) near-optimality dimension work bandits metric spaces
(Bubeck et al., 2011a).
4.3 Comparison Prior Work
compare results previous work non-adaptive discretization bandits
metric spaces.
4.3.1 Non-Adaptive Discretization
One approach prior work directly applicable dynamic contract design
problem non-adaptive discretization. algorithm, call NonAdaptive,
runs off-the-shelf MAB algorithm, treating set candidate contracts Xcand arms.8
concreteness, following prior work (Kleinberg & Leighton, 2003; Kleinberg,
2004; Kleinberg et al., 2008), use well-known algorithm UCB1 (Auer, Cesa-Bianchi, &
Fischer, 2002) off-the-shelf MAB algorithm.
compare AgnosticZooming NonAdaptive, useful derive several worstcase corollaries Theorem 4.1, replacing N (X ) various (loose) upper bounds.9
Corollary 4.3. setting Theorem 4.1, regret AgnosticZooming upperbounded follows:
P
(a) R(T |Xcand ) + =2j : jN O(|X | /), (0, 1).
p
(b) R(T |Xcand ) O( |Xcand |).
O() notation hides logarithmic dependence .
best known regret bounds NonAdaptive coincide Corollary 4.3
poly-logarithmic factors. However, regret bounds Theorem 4.1 may significantly
better ones Corollary 4.3. discuss next section,
context specific example.
8. simplify proofs lower bounds, assume candidate contracts randomly
permuted given MAB algorithm.
9. use facts X Xcand , N (Y ) N0 (Y ), N0min (Y ) |Y | subsets X.

332

fiAdaptive Contract Design Crowdsourcing Markets

4.3.2 Bandits Metric Spaces
Consider variant dynamic contract design algorithm given priori
information similarity contracts: function : Xcand Xcand [0, 1]
|U (x) U (y)| D(x, y) two candidate contracts x, y. algorithm given
function (call algorithm D-aware), machinery bandits metric spaces
(Kleinberg et al., 2008; Bubeck et al., 2011a) used perform adaptive discretization
obtain significant advantage NonAdaptive. argue obtain similar
results AgnosticZooming without knowing D.
practice, similarity information would coarse, probably aggregated according
predefined hierarchy. formalize idea, hierarchy represented
collection F subsets Xcand , D(x, y) function smallest subset
F containing x y. hierarchy F natural given structure
contract space. One natural hierarchy collection feasible cells,
corresponds splitting cells half dimension. Formally, D(x, y) = f (Cx,y )
f f (Cx,y ) width(Cx,y ), Cx,y smallest feasible cell containing
x y.
Given shape D, let us state regret bounds D-aware algorithms work
Kleinberg et al. (2008) Bubeck et al. (2011a). simplify notation, assume
action space restricted Xcand . regret bounds similar shape
Theorem 4.1:
R(T |Xcand ) + O(log )

(X )
N()


X
=2j :

jN



,

(10)

numbers N () similar high-level meaning N (), nearly coincide
Nmin () D(x, y) = VirtWidth(Cx,y ). One use Equation (10) derive
polynomial regret bound like Equation (9).
precise comparison, focus results work Kleinberg et al.
(2008). (The regret bounds Bubeck et al., 2011a similar spirit, stated
terms slightly different structure.) covering-type regret bound work
Kleinberg et al. (2008) focuses balls radius according distance D,
N (Y ) smallest number balls sufficient cover . special case
D(x, y) = VirtWidth(Cx,y ) balls radius precisely feasible cells virtual width
. similar (albeit technically same) -minimal cells
definition Nmin ().
Further, covering numbers N (Y ) determine zooming dimension:
n


ZoomDim = inf 0 : N/8
(X ) > 0 , > 0.
(11)
definition coincides covering dimension worst case, much
smaller nice problem instances X significantly small subset Xcand .
definition, one obtains polynomial regret bound version Equation (9) = ZoomDim .
conclude AgnosticZooming essentially matches regret bounds D-aware
algorithms, despite fact D-aware algorithms access much information.
333

fiHo, Slivkins, & Vaughan

5. Special Case: High-Low Example
apply machinery Section 4 special case, show AgnosticZooming
significantly outperforms NonAdaptive.
basic special case one non-null outcome. Essentially,
worker makes strategic choice whether accept reject given task (where reject
corresponds null effort level), choice fully observable. setting
studied (Kleinberg & Leighton, 2003; Badanidiyuru et al., 2012; Singla & Krause,
2013; Badanidiyuru et al., 2013); call dynamic task pricing. contract
completely specified price p non-null outcome. supply distribution
summarized function S(p) = Pr[accept|p], corresponding expected utility
U (p) = S(p)(v p), v value non-null outcome. special case
already quite rich, S() arbitrary non-decreasing function. using
adaptive discretization, achieve significant improvement prior work; see Section 8
discussion.
consider somewhat richer setting workers strategic decisions
observable; salient feature setting, called moral hazard contract
theory literature. two non-null outcomes (low high), two non-null effort
levels (low high). Low outcome brings zero value requester, high outcome
brings value v > 0. Low effort level inflicts zero cost worker leads low outcome
probability 1. assume workers break ties effort levels consistent
way: high better low better null. (Hence, low effort incurs zero cost,
possible outcomes low high.) call high-low example;
perhaps simplest example features moral hazard.
example, workers type consists pair (ch , h ), ch 0 cost
high effort h [0, 1] probability high outcome given high effort. Note
dynamic task pricing equivalent special case h = 1.
following claim states crucial property high-low example.
Claim 5.1. Consider high-low example fixed supply distribution. probability obtaining high outcome given contract x Pr[high outcome | contract x] depends
p = x(high) x(low); denote probability S(p). Moreover, S(p) non-decreasing
p. Therefore:
expected utility U (x) = S(p)(v p) x(low).
discretization error OPT(X) OPT(Xcand ()) 3, > 0.
bound discretization error, essential S(p) non-decreasing p.
Recall Xcand (), uniform mesh granularity > 0, consists bounded,
monotone contracts payments N.
purposes, supply distribution summarized via function S(). Denote
U (p) = S(p)(v p). Note U (x) maximized setting x(low) = 0, case
U (x) = U (p). Thus, algorithm knows given high-low example, set
x(low) = 0, thereby reducing dimensionality search space. problem
essentially reduces dynamic task pricing S().
However, general algorithm know whether presented highlow example (because effort levels observable). follows
consider algorithms restrict x(low) = 0.
334

fiAdaptive Contract Design Crowdsourcing Markets

5.1 Nice Supply Distribution
focus supply distribution nice, sense S() satisfies
following two properties:
S(p) Lipschitz-continuous: |S(p) S(p0 )| L|p p0 | constant L.
U (p) strongly concave, sense U 00 () exists satisfies U 00 () C < 0.
L C absolute constants. call strongly Lipschitz-concave.
properties fairly natural. example, satisfied h
worker types marginal distribution ch piecewise uniform
density 1 , absolute constant 1.
show choice Xcand X, AgnosticZooming small width dimension
setting, therefore small regret.
Lemma 5.2. Consider high-low example strongly Lipschitz-concave supply distribution. width dimension 21 , given Xcand X. Therefore,
AgnosticZooming Xcand regret R(T |Xcand ) = O(log ) 3/5 .
contrast performance NonAdaptive, parameterized natural
choice Xcand = Xcand (). focus R(T |X): regret w.r.t. best contract X.
show AgnosticZooming achieves R(T |X) = O(T 3/5 ) wide range Xcand , whereas
NonAdaptive cannot better R(T |X) = O(T 3/4 ) Xcand = Xcand (), > 0.
Lemma 5.3. Consider setting Lemma 5.2. Then:
(a) AgnosticZooming Xcand Xcand (T 2/5 ) regret R(T |X) = O(T 3/5 log ).
(b) NonAdaptive Xcand = Xcand () cannot achieve regret R(T |X) < o(T 3/4 )
problem instances, > 0. 10

5.2 Proofs
Proof Claim 5.1. Consider contract x x(low) = b x(high) = b + p,
worker type (ch , h ). worker exerts high effort, pays cost ch receives
expected payment h (p + b) + (1 h )b, total expected payoff ph + b ch .
expected payoff exerting low effort b. Therefore choose exert high effort
ph + b ch b, i.e., ch /h p, choose exert low effort otherwise.
Therefore


Pr[high outcome | contract x] = E h 1{ch /h p} .
(ch ,h )

function p, call S(p). Moreover, non-decreasing function simply
expression inside expectation non-decreasing p.
trivially follows U (x) = S(p)(v p) x(low).
upper-bound discretization error using standard approach work
dynamic pricing (Kleinberg & Leighton, 2003). Fix discretization granularity > 0.
> 0, exists contract x X OPT(X) U (x ) < . Round x (high)
10. lower bound holds even UCB1 NonAdaptive replaced MAB algorithm.

335

fiHo, Slivkins, & Vaughan

x (low) down, respectively, nearest integer multiple ; let x Xcand ()
resulting contract. Denoting p = x(high) x(low) p = x (high) x (low),
see p p p + 2. follows
U (x) U (x ) 3 OPT(X) 3.
Since holds > 0, conclude OPT(X) OPT(Xcand ()) 3.
Proof Lemma 5.2. calculate width dimension, need count number
feasible cells increment space (i) virtual width larger equal
O() (ii) overlap X , set contracts badness smaller .
first characterize X . use xp,b denote contract x(high) = p + b
x(low) = b. benefit representation that, p b would two axes
increment space. Let xp ,0 optimal contract. Since U (xp,b ) strongly concave
p, know b, exist constants c1 c2 p [0, 1],
c1 (p p)2 U (xp ,b ) U (xp,b ) c2 (p p)2 . Also know U (xp ,b ) = U (xp ,0 ) b.
Therefore.
X = {xp,b : (p p )2 + b O()}
also write


X = {xp,b : p h ( ) p p + h ( ) b O()}

Intuitively, X contains contracts {xp,b } p O( ) away p b O()
away b = 0.
Next characterize virtual width cell. use Cp,b,d denote cell
size anchors {xp,b , x(p+d),(b+d) }. derive expected payment value
two anchors as:
P + (Cp,b,d ) = (p + d)S(p + d) + b +
V + (Cp,b,d ) = vS(p + d)
P (Cp,b,d ) = pS(p) + b
V (Cp,b,d ) = vS(p)
definition, get (we use dF represent S(p + d) S(p) simplification)
VirtWidth(Cp,b,d ) = (v + p)dF + dS(p) + dF + d.
count number feasible cells virtual width larger h ()
overlaps X . Note since total number feasible cells Cp,b,d large
small, treat number cells large constant. Also, relevant
cell Cp,b,d , p p . Therefore, care feasible cells Cp,b,d small
p close p .
Since S(p) Lipschitz, dF = O(d). Therefore, relevant cell Cp,d ,
VirtWidth(Cp,b,d ) = O(d)
Given two arguments, know number cells virtual width

larger also overlaps X O(/) O( /) = O(1/2 ). Therefore
width dimension 1/2.
336

fiAdaptive Contract Design Crowdsourcing Markets

Proof Sketch Lemma 5.3(b). Consider version NonAdaptive runs off-theshelf MAB algorithm ALG candidate contracts Xcand = Xcand (). ALG, arms
candidate contracts; recall arms randomly permuted
given ALG.
Fix > 0. easy construct problem instance discretization error Error ,
OPT(X) OPT(Xcand ()) (). Note Xcand contains N = ( 2 ) suboptimal contracts suboptimal w.r.t. OPT(Xcand ). (For example, contracts x x(low) > 0
suboptimal.)
Fix problem instance MAB N suboptimal arms. Using standard lowerbound arguments MAB, one show one runs ALG problem instance
obtainedby randomly permuting arms I, expected regret rounds
least ( N ).

Therefore, R(T |Xcand ) ( N ). follows


R(T |X) ( N ) + Error ( / + ) (T 3/4 ).

6. Proof Main Regret Bound (Theorem 4.1)
prove main result Section 4. high-level approach define
clean execution algorithm execution high-probability events
satisfied, derive bounds regret conditional clean execution. analysis
clean execution involve probabilistic arguments. approach tends
simplify regret analysis.
start listing simple invariants enforced AgnosticZooming:
Invariant 6.1. round execution AgnosticZooming:
(a) active cells relevant,
(b) candidate contract contained active cell,
(c) Wt (C) 5 radt (C) active composite cell C.
Note zooming rule essential ensure Invariant 6.1(c).
Throughout, say algorithm activates cell cell added
collection active cells. cell stays active activated.
6.1 Analysis Randomness
Definition 6.2 (Clean Execution). execution AgnosticZooming called clean
round active cell C holds
|U (C) Ut (C)| radt (C),
|VirtWidth(C) Wt (C)| 4 radt (C)

(12)
(if C composite).

(13)

Lemma 6.3. Assume crad 16 max(1 + 2m , 18). Then:
(a) Pr [ Equation (12) holds rounds t, active cells C ] 1 2 2 .
(b) Pr [ Equation (13) holds rounds t, active composite cells C ] 1 16 2 .
Consequently, execution AgnosticZooming clean probability least 1 1/T .
337

fiHo, Slivkins, & Vaughan

Lemma 6.3 follows standard concentration inequality known Chernoff
Bounds. However, one needs careful conditioning details.
Proof Lemma 6.3(a). Consider execution AgnosticZooming. Let N total
number activated cells. Since 2m cells activated one round,
N 1 + 2m 2 . Let Cj min(j, N )-th cell activated algorithm. (If multiple
quadrants activated round, order according fixed ordering
quadrants.)
Fix feasible cell C j 2 . claim
Pr [ |U (C) Ut (C)| radt (C) rounds | Cj = C ] 1 2 4 .

(14)

Let n(C) = n1+T (C) total number times cell C chosen algorithm.
N: 1 n(C) let Us requesters utility round C
chosen s-th time. Further, let DC distribution U1 , conditional
event n(S) 1. (That is, per-round reward choosing cell C.) Let U10 , . . . , UT0
family mutually independent random variables, distribution DC .
n , conditional event {Cj = C} {n(C) = n}, tuple (U1 , . . . , Un )
joint distribution tuple (U10 , . . . , Un0 ). Consequently, applying Chernoff
Bounds latter tuple, follows
fi
hfi

fi q
P
fi
Pr fiU (C) n1 ns=1 Us fi n1 crad log(T ) fi {Cj = C} {n(C) = n}
1 2 2crad 1 2 5 .
Taking Union Bound n , plugging radt (Cj ), nt (Cj ), Ut (Cj ),
obtain Equation (14).
Now, let us keep j fixed Equation (14), integrate C. precisely, let us
multiply sides Equation (14) Pr[Cj = C] sum feasible cells C.
obtain, j 2 :
Pr [ |U (Cj ) Ut (Cj )| radt (Cj ) rounds ] 1 2 4 .

(15)

(Note obtain Equation (15), need take Union Bound
feasible cells C.) conclude, take Union Bound j 1 + 2 .
Proof Sketch Lemma 6.3(b). show
fi
fi

Pr fiV + (C) Vt+ (C)fi radt (C) rounds t, active composite cells C 1

4
,
T2

(16)

similarly V (), P + () P (). four statements proved similarly,
using technique Lemma 6.3(a). follows, sketch proof one
four cases, namely Equation (16).
given composite cell C, interested rounds anchor x+ (C)
selected algorithm. Letting n+
(C) number times anchor chosen
time t, let us define corresponding notion confidence radius:

1 crad log
+
radt (C) =
.
2
n+
(C)
338

fiAdaptive Contract Design Crowdsourcing Markets

technique proof Lemma 6.3(a), establish following
high-probability event:
fi
fi +
fiV (C) V + (C)fi rad+ (C).
(17)


precisely, prove
Pr [ Equation (17) holds rounds t, active composite cells C ] 1 2 2 .
Further, need prove w.h.p. anchor x+ (C) played sufficiently often.
1
11
Noting E[n+
(C)] = 2 nt (C), establish auxiliary high-probability event:
n+
(C)

1
2

nt (C) 14 radt (C).

(18)

precisely, use Chernoff Bounds show that, crad 16,
Pr [ Equation (18) holds rounds t, active composite cells C ] 1 2 2 .

(19)

Now, letting n0 = (crad log )1/3 , observe
nt (C) n0
nt (C) < n0




1
n+
(C) 4 nt (C)
radt (C) 1




+
(C),
fi
firad+t (C) rad
fiV (C) V + (C)fi radt (C).


fi
fi
Therefore, Equations (17) (18) hold, fiV + (C) Vt+ (C)fi radt (C).
completes proof Equation (16).
6.2 Analysis Clean Execution
rest analysis focuses clean execution. Recall Ct cell chosen
algorithm round t.
Claim 6.4. clean execution, I(Ct ) OPT(Xcand ) round t.
Proof. Fix round t, let x candidate contract. Invariant 6.1(b), exists
active cell, call Ct , contains x .
claim (Ct ) U (x ). consider two cases, depending whether Ct
atomic. Ct atomic anchor unique, U (Ct ) = U (x ), (Ct ) U (x )
clean execution. Ct composite
(Ct ) U (Ct ) + VirtWidth(Ct )


U (Ct )


U (x )

+

clean execution

width(Ct )

Lemma 3.1
definition width, since x Ct .

proved (Ct ) U (x ). Now, selection rule (Ct ) (Ct )
U (x ). Since holds candidate contract x , claim follows.
11. constant
proof.

1
4

Equation (18) enable consistent choice n0 remainder

339

fiHo, Slivkins, & Vaughan

Claim 6.5. clean execution, round t, index (Ct ) upper-bounded
follows:
(a) Ct atomic I(Ct ) U (Ct ) + 2 radt (Ct ).
(b) Ct composite I(Ct ) U (x) + O(radt (Ct )) contract x Ct .
Proof. Fix round t. Part (a) follows (Ct ) = Ut (Ct ) + radt (Ct ) definition
index, Ut (Ct ) U (Ct ) + radt (Ct ) clean execution.
part (b), fix contract x Ct . Then:
Ut (Ct ) U (Ct ) + radt (Ct )

clean execution

U (x) + width(Ct ) + radt (Ct )

definition width

U (x) + VirtWidth(Ct ) + radt (Ct )

Lemma 3.1

U (x) + Wt (Ct ) + 5 radt (Ct )

clean execution.

(Ct ) = Ut (Ct ) + Wt (Ct ) + 5 radt (Ct )

(20)

definition index

U (x) + 2 Wt (Ct ) + 10 radt (Ct )

Equation (20)

U (x) + 20 radt (Ct )

Invariant 6.1(c).

relevant cell C, define badness (C) follows. C composite, (C) =
supxC (x) maximal badness among contracts C. C atomic x C
unique candidate contract C, (C) = (x).
Claim 6.6. clean execution, (C) O(radt (C)) round active
cell C.
Proof. Claims 6.4 6.5, (Ct ) O(radt (Ct )) round t. Fix round
let C active cell round. C never selected round t, claim
trivially true. Else, let recent round C selected
algorithm. (C) O(rads (C)). claim follows since rads (C) = radt (C).
Claim 6.7. clean execution, cell C selected O(log /((C))2 ) times.
Proof. Claim 6.6, (C) O(radT (C)). claim follows definition radT
Equation (3).
Let n(x) n(C) number times contract x cell C, respectively, chosen
algorithm. regret algorithm
R(T |Xcand ) =

P

xX

n(x) (x)

P

cells C

n(C) (C).

(21)

next result (Lemma 6.8) upper-bounds right-hand side Equation (21) clean
execution. Lemma 6.3, suffices complete proof Theorem 4.1
Lemma 6.8. Consider clean execution AgnosticZooming. (0, 1),
P

cells C

n(C) (C) + O(log )
340

P

=2j : jN

|F (X2 )|
.


fiAdaptive Contract Design Crowdsourcing Markets

proof Lemma 6.8 relies simple properties (), stated below.
Claim 6.9. Consider two relevant cells C Cp . Then:
(a) (C) (Cp ).
(b) (C) > 0, C overlaps X .
Proof. prove part (a), one needs consider two cases, depending whether cell Cp
composite. is, claim follows trivially. Cp atomic, C atomic, too,
(C) = (Cp ) = (x), x unique candidate contract Cp .
part (b), exists candidate contract x C. easy see (x) (C)
(again, consider two cases, depending whether C composite.) So, x X .
Proof Lemma 6.8. Let denote sum question. Let collection
cells ever activated algorithm. Among cells, consider badness
order :
G := { C : (C) [, 2) } .
Claim 6.7, algorithm chooses cell C G O(log /2 ) times,
n(C) (C) O(log /).
Fix (0, 1) observe cells C (C) contribute
. Therefore suffices focus G , /2. follows
P
+ O(log ) =2i /2 |G | .
(22)
bound |G | follows. Consider cell C G . cell called leaf never
zoomed (i.e., removed active set) algorithm. C activated
round cell Cp zoomed on, Cp called parent C. consider two cases,
depending whether C leaf.
(i) Assume cell C leaf. Since (C) < 2, C overlaps X2 Claim 6.9(b).
Note C zoomed round, say round 1.
5 radt (C) Wt (C)

zooming rule

VirtWidth(C) + 4 radt (C)

clean execution,

radt (C) VirtWidth(C). Therefore, using Claim 6.6,
(C) O(radt (C)) O(VirtWidth(C)).
follows C F() (X2 ).
(ii) Assume cell C leaf. Let Cp parent C. Since C Cp , (C)
(Cp ) Claim 6.9(a). Therefore, invoking case (i),
(C) (Cp ) O(VirtWidth(Cp )).
Since (C) < 2, C overlaps X2 Claim 6.9(b), therefore Cp .
follows Cp F() (X2 ).
fi
fi
Combing two cases, follows |G | (2m + 1) fiF() (X2 )fi. Plugging
(22) making appropriate substitution () simplify resulting expression,
obtain regret bound Theorem 4.1
341

fiHo, Slivkins, & Vaughan

7. Simulations
evaluate performance AgnosticZooming simulations. AgnosticZooming
compared two versions NonAdaptive use, respectively, two standard bandit
algorithms: UCB1 (Auer et al., 2002) Thompson Sampling (Thompson, 1933)
Gaussian priors. algorithms, round numerical score (called index )
computed arm, arm maximal index chosen. UCB1, index
arm high-confidence upper bound expected reward arm. Thompson
Sampling, index sampled independently Bayesian posterior distribution
arms expected reward.
7.1 Setup
consider generalized version high-low example Section 5
requesters value low outcome could nonzero. results reported below,
set requesters values V (high) = 1 V (low) = .3, probability obtaining
high outcome given high effort h = .8. explicitly report results,
additionally tried wide range alternative values V (high), V (low), h found
similar qualitatively. Intuitively, varying requesters values h
changes contracts algorithms converge (that is, optimal arms),
impact problem structure; width dimension settings.
generalized high-low example, workers type characterized cost ch
high effort. consider three supply distributions:
Uniform: ch uniformly distributed [0, 1].
Homogeneous: ch every worker.
Two-type: ch uniformly distributed two values, c0h c00h .
first two distributions represent extreme cases workers either
extremely homogeneous extremely diverse. third distribution one way get
middle ground. distribution, run algorithm 100 times.12
Homogeneous Supply Distribution, ch drawn uniformly random [0, 1] run.
Two-Type Supply Distribution, c0h c00h drawn independently uniformly
[0, 1] run.
UCB1 AgnosticZooming, replace logarithmic confidence terms
small constants. find beneficial practice algorithms, consistent
prior work (Radlinski, Kleinberg, & Joachims, 2008; Slivkins, Radlinski, & Gollapudi,
2013). algorithms, tried several different constants found performance
sensitive particular constant used long order 1.
results reported below, set confidence terms equal 1. UCB1, means

given arm played na times,
p index average reward plus 1/ na .
AgnosticZooming, means radt () = 1/nt ().
three algorithms run Xcand = Xcand (), > 0 parameter
specifying granularity discretization.
12. standard errors plots order 0.001 less. (Note point
average 100 runs also average previous rounds.)

342

fiAdaptive Contract Design Crowdsourcing Markets

7.2 Overview Results.
Across simulations, AgnosticZooming either outperforms nearly matches NonAdaptive.
performance appear suffer large hidden constants appear
analysis. find AgnosticZooming converges faster NonAdaptive
near-optimal smaller. consistent intuition AgnosticZooming
focuses exploring promising regions contract space. large,
AgnosticZooming converges slowly NonAdaptive, eventually achieves similar
performance. Further, find AgnosticZooming small performs well compared
NonAdaptive larger . particular, much worse initially, much better
eventually.
simulations suggest time horizon known advance one tune
, NonAdaptive achieve near-optimal performance. However, real applications approximately optimal may difficult compute, may known
advance. AgnosticZooming performs consistently well wide range therefore
require prior knowledge careful tuning .
7.3 Detailed Results
algorithm, compute time-averaged cumulative utility rounds given
b (T, ), various values .
granularity , denoted U
b (T, ) changes
First, fix time horizon 5,000 rounds, study U
. results shown Figure 1. observe AgnosticZooming either closely
matches outperforms versions NonAdaptive across supply distributions
values . AgnosticZooming performs consistently well different
performance versions NonAdaptive decreases rapidly small.
Second, study three algorithms perform time. Specifically, plot
b (T, ), three values , namely 0.02, 0.08, 0.2. Since setting =
vs. U
0.08 close optimal examples, values represent, respectively, values small, adequate, large. results shown Figure 2.
small values , AgnosticZooming quickly zooms promising regions contract space, leading faster converge alternatives. However, large,
AgnosticZooming converges slowly, eventually achieves similar performance.
regime, AgnosticZooming reap benefits adaptive discretization
mesh candidate contracts sparse, still suffers overhead. suggests
time horizon known advance one optimize given ,
NonAdaptive achieve near-optimal performance. AgnosticZooming performs consistently different choices therefore require either prior knowledge
careful tuning .
demonstrate benefit know tune , compare
performance AgnosticZooming small NonAdaptive different
b (T, ). See Figure 3.
values . algorithm choice , plot vs. U
show results Uniform Supply Distribution since results distributions similar. Additionally, omit results Thompson Sampling since
UCB1 performed better experiments.13 find small , AgnosticZooming
13. conjecture replaced logarithmic confidence term UCB1 1.

343

fi0.4

0.4

0.3

0.3

0.2

0.2

0.1
0.0

AgnosticZooming
UCB1
ThompsonSampling

0.1
0.2
0.00

0.05

0.10


0.15

Average Utility

Average Utility

Ho, Slivkins, & Vaughan

0.1
0.0

AgnosticZooming
UCB1
ThompsonSampling

0.1
0.2

0.20

0.00

(a) Uniform Supply Distribution

0.05

0.10


0.15

0.20

(b) Homogeneous Supply Distribution

0.4

Average Utility

0.3
0.2
0.1
0.0

AgnosticZooming
UCB1
ThompsonSampling

0.1
0.2
0.00

0.05

0.10


0.15

0.20

(c) Two-Type Supply Distribution

Figure 1: requesters average per-round utility 5,000 rounds vs. choice
initial discretization .

small converges nearly fast NonAdaptive larger . large,
AgnosticZooming small matches NonAdaptive optimal .
Finally, Figure 4, confirm intuition OPT(Xcand ()) decreases
granularity . end, run AgnosticZooming 50,000 rounds (so algorithm
time nearly converge optimal contract), examine average utility
last 5,000 rounds. expected, see average requester utility achievable
small significantly higher utility achievable larger.
simulation results suggest AgnosticZooming performs well across different
supply distributions different settings , requiring careful tuning algorithm
parameters. Given smaller value , better payoff optimal
contract OPT(Xcand ()), AgnosticZooming small good algorithm variety
settings.
344

fiAdaptive Contract Design Crowdsourcing Markets

AgnosticZooming
0.4
0.3
0.2
0.1
0.0
0.1
0.2

ThompsonSampling

Uniform: =0.02

Uniform: =0.08

Uniform: =0.20

Two-Type: =0.02

Two-Type: =0.08

Two-Type: =0.20

Homogeneous: =0.02

Homogeneous: =0.08

Homogeneous: =0.20

Average Utility

0.4
0.3
0.2
0.1
0.0
0.1
0.2

UCB1

0.4
0.3
0.2
0.1
0.0
0.1
0.2
0

1000 2000 3000 4000 5000

0

1000 2000 3000 4000 5000

0

1000 2000 3000 4000 5000

Time

Figure 2: requesters average per-round utility time different supply distributions discretization sizes.

8. Application Dynamic Task Pricing
discuss dynamic task pricing, seen special case dynamic contract
design exactly one non-null outcome. identify important family
problem instances AgnosticZooming out-performs NonAdaptive.
8.1 Background
dynamic task pricing problem, basic version, defined follows.
one principal (buyer) sequentially interacts multiple agents (sellers).
round t, agent arrives, one item sale. principal offers price pt item,
agent agrees sell pt ct , ct [0, 1] agents private
cost item. principal derives value v item bought; utility
value bought items minus payment. time horizon (the number rounds)
known. private cost ct independent sample fixed distribution,
called supply distribution. interested prior-independent version,
supply distribution known principal. algorithms goal choose
offered prices pt maximize expected utility principal.
345

fiHo, Slivkins, & Vaughan

Uniform

0.5

AgnosticZooming: = .02
UCB1: = .02
UCB1: = .08
UCB1: = .20

0.4

Average Utility

0.3
0.2
0.1
0.0
0.1
0.2
0

1000

2000

Time

3000

4000

5000

Figure 3: requesters average per-round utility time using AgnosticZooming
small compared NonAdaptive three different values .

Uniform

Average Utility Last 5k rounds

0.4
0.3
0.2
0.1
0.0
0.1

AgnosticZooming

0.2
0.00

0.05

0.10

0.15


0.20

0.25

0.30

Figure 4: Average requester utility last 5,000 rounds 50,000-round run
AgnosticZooming different values .

Dynamic task pricing seen special case dynamic contract design
exactly one non-null outcome (which corresponds sale). Indeed,
special case exactly one non-null effort level e without loss generality (because
non-null effort levels deterministically lead non-null outcome).
346

fiAdaptive Contract Design Crowdsourcing Markets

One crucial simplification compared full generality dynamic contract design
discretization error easily bounded above: 14
OPT(X) OPT(Xcand ())

> 0.

Worst-case regret bounds implicit prior work dynamic inventory-pricing (Kleinberg & Leighton, 2003).15 Let NonAdaptive() denote algorithm NonAdaptive Xcand =
Xcand (). Then, analysis work Kleinberg Leighton (2003), NonAdaptive()
achieves regret R(T ) = O(T + 2 ). optimized R(T ) = O(T 2/3 )
= O(T 1/3 ). Moreover, matching lower bound: R(T ) = (T 2/3 )
algorithm.
Further, folklore result NonAdaptive() achieves regret R(T ) = O(T 2/3 )
= (T 1/3 ). (We sketch lower-bounding example proof Lemma 8.4,
make paper self-contained.)
8.2 Preliminaries
contract summarized single number: offered price p non-null
outcome. Let F (p) probability worker accepting task price p, let
U (p) = F (p) (v p) corresponding expected utility algorithm.
Note contracts trivially monotone optimal contract bounded
without loss generality. follows OPT(X) = supp0 U (p), optimal expected
utility possible prices.
cell C price interval C = [p, p0 ] [0, 1], virtual width


VirtWidth(C) = v F (p0 ) p F (p) v F (p) p0 F (p0 ) .
8.3 Results: General Case
using AgnosticZooming Xcand = X.
First, let us prove reasonable choice worst case: namely,
achieve optimal O(T 2/3 ) regret.
Lemma 8.1. Consider dynamic task pricing problem. AgnosticZooming Xcand =
X achieves regret O(T 2/3 log ).
Proof Sketch. Fix > 0. key observation VirtWidth(C) either
p0 p 4 , F (p0 ) F (p) 4 . Call C red cell former happens, blue cell
otherwise. Therefore collection mutually disjoint cells virtual width
O( 1 ) red cells O( 1 ) blue cells, hence O( 1 ) cells total.
follows O( 1 ) active cells virtual width .
So, notation Theorem 4.1 N () O( 1 ). follows width
dimension 1, turn implies desired regret bound.
14. Recall Xcand () denotes set prices [0, 1] integer multiples given > 0; call
set additive -mesh.
15. algorithmic result dynamic task pricing easy modification analysis work
Kleinberg Leighton (2003) dynamic inventory-pricing. lower bound work Kleinberg
Leighton also translated dynamic inventory-pricing dynamic task pricing without
introducing new ideas. omit details version.

347

fiHo, Slivkins, & Vaughan

8.4 Results: Nice Problem Instances
focus problem instances piecewise-uniform costs bounded density. Formally,
say instance dynamic task pricing k-piecewise-uniform costs interval
[0,1] partitioned k N sub-intervals supply distribution uniform
sub-interval. problem instance -bounded density, 1 supply
distribution probability density function almost everywhere, density
1
. Using full power Theorem 4.1, obtain following regret bound.
Theorem 8.2. Consider dynamic task pricing problem k-piecewise-uniform costs
-bounded density, absolute constants k N > 1. AgnosticZooming
Xcand = X achieves regret R(T ) = O(T 3/5 ).
Proof Sketch. Since supply distribution density , follows F ()
Lipschitz-continuous function Lipschitz constant . follows cell virtual
width least diameter least (/), > 0. (Note cell
simply sub-interval [p, q] [0, 1], diameter simply q p.)

Second, claim X contained union k intervals diameter O( ).
see this, consider partition [0, 1] k subintervals supply distribution
uniform density subinterval. Let [pj , qj ] j-th subinterval. Let pj
local optimum U () subinterval, let Xj, = {x [pj , qj ] : U (pj ) U (x) }.

X j Xj, . show Xj, [pj , pj + ] = O( ).
Recall N0 (X ) number feasible cells virtual width least 0
overlap X . follows N0 (X ) k times maximal number

feasible cells diameter least (/) overlap interval diameter O( ).
Therefore: N0 (X ) = O(k3/2 1/2 log 1 ). Moreover, less sophisticated upper
bound N0 (X ): number feasible cell diameter least (/).
N0 (X ) = O(/)(log 1 ). theorem follows plugging upper bounds
N0 (X ) Equation (6).
8.5 Comparison NonAdaptive
Consider NonAdaptive(0 ), 0 = (T 1/3 ) granularity required optimal
worst-case performance. Call problem instance nice 2-piecewise-uniform costs
-bounded density, sufficiently large absolute constant ; say = 4
concreteness. claim AgnosticZooming outperforms NonAdaptive(0 ) nice
problem instances.
Lemma 8.3. NonAdaptive(0 ) achieves regret R(T ) = (T 2/3 ) worst case
nice problem instances.
Proof Sketch. Recall k = 2 supply distribution density 1 interval
[0, p0 ], density 2 interval [p0 , 1], numbers 1 , 2 , p0 . pick p0
sufficiently far point Xcand (0 ). Note function U () parabola
two intervals. adjust densities U () achieves maximum
p0 , maximum either two parabolas sufficiently far p0 .
discretization error Xcand (0 ) least (0 ), implies regret (0 ).
348

fiAdaptive Contract Design Crowdsourcing Markets

8.6 Lower Bound NonAdaptive
provide specific lower-bounding example worst-case performance NonAdaptive(),
arbitrary > 0. Let F family problem instances k-piecewiseuniform costs -bounded density, k N = 4.
Lemma 8.4. Let R (T ) maximal
problem inp regret NonAdaptive()
2/3
stances F. R (T ) = (T + /) (T ).
Proof Sketch. piecewise-uniform costs, F (0) = 0 F (p) = 1. Assume
principal derives value v = 1 item. expected utility price p
U (p) = F (p)(1 p).
Fix > 0. Use following problem instance. Let P = [ 25 , 35 ] {4j + : j N}.
Set U (p) = 41 p P0 . Further, pick p P/2 set U (p ) = 41 + ().
defines F (p) p P {0, 1, p }. rest prices, define F () via linear
interpolation. completes description problem instance.
show X consists N = ( 1 ) candidate contracts. Therefore, using stanp

dard lower-bounding arguments MAB, obtain R(T |Xcand ) ( N ) = ( /).
Further, show discretization error least (), implying R(T )
R(T |Xcand ) + (T ).

9. Related Work
paper related three different areas: contract theory, market design crowdsourcing, online decision problems. outline connections
areas.
9.1 Contract Theory
model viewed extension classic principal-agent model contract
theory (Laffont & Martimort, 2002). basic version classic model,
single principal interacts single agent whose type (specified cost function
production function, described Section 2) generally assumed known.
principal specifies contract mapping outcomes payments principal commits
make agent. agent chooses action (i.e., effort level) stochastically
results outcome order maximize expected utility given contract.
principal observes outcome, cannot directly observe agents effort level, creating
moral hazard problem. goal principal design contract maximize
expected utility, difference utility receives
outcome payment makes. maximization written constrained
optimization problem, shown linear contracts optimal.
adverse selection variation principal-agent problem relaxes assumption
agents type known. existing literature principal-agent problem
adverse selection focuses applying revelation principle (Laffont & Martimort, 2002).
setting, principal offers menu contracts, contract chosen agent
reveals agents type. problem selecting menu contracts maximizes
principals expected utility formulated constrained optimization.
349

fiHo, Slivkins, & Vaughan

work differs classic setting consider principal interacting
multiple agents, principal may adjust contract time online manner.
Several authors considered extensions classic model multiple agents.
Levy Vukina (2002) show multiple agents optimal set individual
linear contracts agent rather single uniform contract agents, offer
variety descriptive explanations common see uniform contracts
practice. Babaioff, Feldman, Nisan (2006) consider setting one principal
interacts multiple agents, observes single outcome function
agents effort levels. Misra, Nair, Daljord (2012) consider variant
algorithm must decide set uniform contract many agents
select subset agents hire.
Alternative online versions problem considered literature well.
dynamic principal agent problem (Sannikov, 2008; Williams, 2009; Sannikov, 2012),
single principal interacts single agent repeatedly period time. agent
choose exert different effort different time, outcome time function
efforts exerted agent t. principal cannot observe agents
efforts observe outcome. goal principal design optimal
contract time maximize payoff. work different line work since
consider setting multiple agents different, unknown types. algorithm
needs learn distribution agent types design optimal contract accordingly.
Conitzer Garera (2006) study online principal agent problem similar
setting ours. However, focus empirically comparing different online algorithms,
including bandit approaches uniform discretization, gradient ascent, Bayesian
update approaches problem. goal provide algorithm nice theoretical
guarantees.
Bohren Kravitz (2013) study setting outcome unverifiable.
address issue, propose assign bundle tasks worker. verify
outcome, task bundle chosen verifiable task non-trivial
probability. verifiable task either gold standard task known answer
task assigned multiple workers verification. payment task bundle
conditional outcome verified tasks. setting, assume task
outcome verifiable. relax assumption adopting similar approaches.
9.2 Incentives Crowdsourcing Systems
Researchers recently begun examine design incentive mechanisms encourage
high-quality work crowdsourcing systems. Jain, Chen, Parkes (2012) explore ways
award virtual points users online question-and-answer forums improve
quality answers. Ghosh Hummel (2011, 2013) Ghosh McAfee (2011) study
distribute user generated content (e.g., Youtube videos) users encourage
production high-quality internet content people motivated attention. Ho,
Zhang, Vaughan, van der Schaar (2012) Zhang van der Schaar (2012) consider
design two-sided reputation systems encourage good behavior workers
requesters crowdsourcing markets. also consider crowdsourcing markets,
350

fiAdaptive Contract Design Crowdsourcing Markets

work differs focuses design monetary contracts, perhaps
natural incentive scheme, incentivize workers exert effort.
problem closest studied context crowdsourcing
systems online task pricing problem requester unlimited supply
tasks completed budget B spend (Badanidiyuru et al., 2012; Singer
& Mittal, 2013). Workers private costs arrive online, requester sets single
price arriving worker. goal learn optimal single fixed price time.
work viewed generalization task pricing problem, special
case setting number non-null outcomes fixed 1.
also empirical work examining workers behavior varies based
financial incentives offered crowdsourcing markets. Mason Watts (2009) study
workers react changes performance-independent financial incentives. study,
increasing financial incentives increases number tasks workers complete,
quality output. Yin, Chen, Sun (2013) provide potential explanation
phenomenon using concept anchoring effect: workers cost completing task
influenced first price worker sees task. Horton Chilton (2010) run
experiments estimate workers reservation wage completing tasks. show
many workers respond rationally offered contracts, whereas workers appeared
target payment mind.
recent research studies effects performance-based payments (PBPs). Harris
(2011) runs MTurk experiments resume screening, workers get bonus
perform well. concludes quality work better PBPs
uniform payments. Yin et al. (2013) show varying magnitude bonus
much effect certain settings. Ho et al. (2015) perform comprehensive set
experiments aimed determining whether, when, PBPs increase quality
submitted work. results suggest PBPs increase quality tasks
increased time effort leads higher quality work. results also suggest
workers may interpret contract performance-based even stated (since
requesters always option reject work). Based evidence, propose
new model worker behavior extends principal-agent model explicitly reflect
workers subjective beliefs likelihood paid.
Overall, previous empirical work demonstrates workers crowdsourcing markets
respond change financial incentives, behavior always
follow traditional rational-worker model similar people real-world market.
work, start analysis rational-worker assumption ubiquitous economic theory, demonstrate results still hold without assumptions
long collective worker behavior satisfies natural properties (namely, long
Lemma 3.1 holds). note results hold generalized worker model proposed Ho et al. (2015), consistent experimental evidence discussed
above.
351

fiHo, Slivkins, & Vaughan

9.3 Sequential Decision Problems
sequential decision problems, algorithm makes sequential decisions time. Two
directions relevant paper multi-armed bandits (MAB) dynamic
pricing.
MAB studied since 1933 (Thompson, 1933) operations research, economics,
several branches computer science including machine learning, theoretical computer
science, AI, algorithmic economics. survey prior work MAB beyond scope
paper; reader encouraged refer work Cesa-Bianchi Lugosi (2006)
Bubeck Cesa-Bianchi (2012) background prior-independent MAB,
work Gittins, Glazebrook, Weber (2011) background Bayesian MAB.
briefly discuss lines work MAB directly relevant paper.
setting modeled prior-independent MAB stochastic rewards: reward given arm i.i.d. sample time-invariant distribution, neither
distribution Bayesian prior known algorithm. basic formulation
(with small number arms) well understood (Lai & Robbins, 1985; Auer et al., 2002;
Bubeck & Cesa-Bianchi, 2012). handle problems large infinite number arms,
one typically needs side information similarity arms. typical way model
side information, called Lipschitz MAB (Kleinberg et al., 2008), algorithm
given distance function arms, expected rewards assumed satisfy
Lipschitz-continuity (or relaxation thereof) respect distance function (Agrawal,
1995; Kleinberg, 2004; Auer et al., 2007; Kleinberg et al., 2008; Bubeck et al., 2011a;
Slivkins, 2014). related paper idea adaptive discretization
often used setting (Kleinberg et al., 2008; Bubeck et al., 2011a; Slivkins, 2014),
particularly zooming algorithm (Kleinberg et al., 2008; Slivkins, 2014). particular,
general template algorithm similar one zooming algorithm (but
selection rule zooming rule different, reflecting lack priori
known similarity information).
settings (including ours), numerical similarity information required Lipschitz MAB immediately available. example, applications web search
advertising natural assume algorithm observe tree-shaped taxonomy arms (Kocsis & Szepesvari, 2006; Munos & Coquelin, 2007; Pandey et al., 2007;
Slivkins, 2011; Bull, 2013). particular, Slivkins (2011) Bull (2013) explicitly reconstruct (the relevant parts of) metric space defined taxonomy. different
direction, Bubeck, Stoltz, Yu (2011b) study version Lipschitz MAB
Lipschitz constant known, essentially recover performance NonAdaptive
setting.
MAB partial monitoring (Audibert & Bubeck, 2010; Bartok, Foster, Pal,
Rakhlin, & Szepesvari, 2014; Antos, Bartok, Pal, & Szepesvari, 2013), round
algorithm receives auxiliary feedback rewards round (along reward
chosen arm), goal take advantage auxiliary feedback. Dynamic
task pricing cast framework: given price p accepted, higher
price would too, rejected, lower price would be. However,
aware way link dynamic task pricing existing results partial monitoring
352

fiAdaptive Contract Design Crowdsourcing Markets

via connection. general version dynamic contract design appear fit
partial monitoring framework, essentially due moral hazard.
Dynamic pricing (a.k.a. online posted-price auctions) refers settings
principal interacts agents arrive time offers agent price
transaction, selling buying item. version principal sells items
extensively studied operations research, typically Bayesian setting; see
work den Boer (2015) literature review. study prior-independent,
non-parameterized formulations initiated work Blum, Kumar, Rudra,
Wu (2003) Kleinberg Leighton (2003) continued several others (Besbes
& Zeevi, 2009; Babaioff, Dughmi, Kleinberg, & Slivkins, 2015; Besbes & Zeevi, 2012; Wang,
Deng, & Ye, 2014; Badanidiyuru et al., 2013; Badanidiyuru, Langford, & Slivkins, 2014).
Further, Badanidiyuru et al. (2012) Singla Krause (2013) studied version
principal buys items, equivalently commissions tasks; call version
dynamic task pricing. Modulo budget constraints, essentially special case
setting round worker offered chance perform task specified
price, either accept reject offer. particular, workers strategic choice
directly observable. general settings studied (Badanidiyuru et al., 2013,
2014; Agrawal & Devanur, 2014; Agrawal, Devanur, & Li, 2015).16 However, work
(after initial papers, see Blum et al., 2003 Kleinberg & Leighton, 2003) focused
models constraints principals supply budgets, imply
improved results specialized unconstrained settings.

10. Conclusions
Motivated applications crowdsourcing markets, define dynamic contract design
problem, multi-round version principal-agent model unobservable strategic
decisions. treat problem multi-armed bandit problem, design algorithm
problem, derive regret bounds compare favorably prior work. main
conceptual contribution, aside identifying model, adaptive discretization
approach rely Lipschitz-continuity assumptions. provably improve
uniform discretization approach prior work, general case
illustrative special cases. theoretical results supported simulations.
generality shortcomings model discussed Section 2.2.
believe dynamic contract design problem deserves study, several
directions outline below.
1. clear whether provable results improved, perhaps using substantially
different algorithms relative different problem-specific structures. particular, one
needs establish lower bounds order argue optimality; lower bounds
dynamic contract design currently known.
2. adaptive discretization approach may fine-tuned improve performance
practice. particular, definition index (C) given feasible cell C
16. papers Badanidiyuru et al. (2014) Agrawal Devanur (2014) concurrent independent work respect conference publication paper, work Agrawal et al.
(2015) subsequent work.

353

fiHo, Slivkins, & Vaughan

may re-defined several different ways. First, use information C
sophisticated way, similar sophisticated indices basic K-armed
bandit problem; example, see work Garivier Cappe (2011). Second, index
incorporate information cells. Third, defined smoother,
probabilistic way, e.g., Thompson Sampling (Thompson, 1933).
3. Deeper insights structure (static) principal-agent problem needed,
primarily order optimize choice Xcand , set candidate contracts.
natural target uniform mesh Xcand (). optimize granularity , one
needs upper-bound discretization error OPT(Xcand ) OPT(Xcand ()) terms
function f () f () 0 0. first-order open question resolve
whether done general case, provide specific example cannot.
related open question concerns effect increasing granularity: upper-bound
difference OPT(Xcand ()) OPT(Xcand (0 )), > 0 > 0, terms function 0 .
Further, known whether optimal mesh contracts fact uniform mesh.
Also interest effect restricting attention monotone contracts.
prove monotone contracts may optimal (Appendix A), significance
phenomenon unclear. One would like characterize scenarios restricting
monotone contracts alright (in sense best monotone contract good,
much worse, best contract), scenarios restriction results
significant loss. latter scenarios, different algorithms may needed.
4. much extensive analysis special cases order. general results
difficult access (which appears inherent property general problem),
immediate direction special cases deriving lucid corollaries current regret
bounds. particular, desirable optimize choice candidate contracts. Apart
massaging current results, one also design improved algorithms derive
specialized lower bounds. Particularly appealing special cases concern supply distributions
mixtures small number types, supply distributions belong
(simple) parameterized family unknown parameter.
Going beyond current model, natural direction incorporate budget constraint, extending corresponding results dynamic task pricing. main difficulty
settings distribution two contracts may perform much better
fixed contract; see work Badanidiyuru et al. (2013) discussion. Effectively,
algorithm needs optimize distributions. first step, one use nonadaptive discretization conjunction general algorithms bandits budget
constraints, sometimes called bandits knapsacks (Badanidiyuru et al., 2013; Agrawal
& Devanur, 2014). However, clear choose optimal mesh contracts
(as discussed throughout paper), mesh likely uniform (because
uniform special case dynamic task pricing budget; see Badanidiyuru et al., 2013 discussion). eventual target research direction marry
adaptive discretization techniques prior work bandits knapsacks.

354

fiAdaptive Contract Design Crowdsourcing Markets

Acknowledgments
thank anonymous reviewers useful comments. Much research
completed Ho intern Microsoft Research. research partially supported NSF grant IIS-1054911. opinions, findings, conclusions, recommendations authors alone.

Appendix A. Monotone Contracts May Optimal
section provide example problem instance monotone contracts
suboptimal (at least restricting attention contracts non-negative
payoffs). example, three non-null outcomes (i.e., = 3), two non-null
effort levels, low effort high effort, denote e` eh respectively.
single worker type. Since one type, drop subscript
describing cost function c. let c(e` ) = 0, let c(eh ) positive value less
0.5(v(2) v(1)). worker chooses low effort, outcome equally likely 1
3. worker chooses high effort, equally likely 2 3. easy verify
type satisfies FOSD assumption. Finally, simplicity, assume workers
break ties high effort effort level favor high effort,
workers break ties low effort null effort level favor low effort.
Lets consider optimal contract. Since single worker type
workers type break ties way, consider separately best contract
would make workers choose null effort level, best contract would make
workers choose low effort, best contract would make workers choose high
effort, compare requesters expected value each.
Since c(e` ) = 0 workers break ties low effort null effort favor low
effort, contract would cause workers choose null effort; workers always
prefer low effort null effort.
easy see best contract (in terms requester expected value) would
make workers choose low effort would set x(1) = x(3) = 0 x(2) sufficiently low
workers would enticed choose high effort; setting x(2) = 0 sufficient.
case, expected value requester would 0.5(v(1) + v(3)).
lets consider contracts cause workers choose high effort. worker chooses
high effort, expected value requester
0.5(v(2) x(2) + v(3) x(3)).

(23)

Workers choose high effort
0.5(x(1) + x(3)) 0.5(x(2) + x(3)) c(eh )

0.5x(1) 0.5x(2) c(eh ).

(24)

find contract maximizes requesters expected value workers choose
high effort, want maximize Equation 23 subject constraint Equation 24.
Since x(3) doesnt appear Equation 24, set 0 maximize Equation 23.
355

fiHo, Slivkins, & Vaughan

Since x(1) appear Equation 23, set x(1) = 0 make Equation 24
easy possible satisfy. see optimal occurs x(2) = 2c(eh ).
Plugging contact x Equation 23, expected utility case 0.5(v(2) +
v(3)) c(eh ). Since assumed c(eh ) < 0.5(v(2) v(1))), strictly preferable
constant 0 contract, fact unique optimal contract. Since x(2) > x(3),
unique optimal contract monotonic.

References
Agrawal, R. (1995). continuum-armed bandit problem. SIAM J. Control Optimization, 33 (6), 19261951.
Agrawal, S., & Devanur, N. R. (2014). Bandits concave rewards convex knapsacks.
15th ACM Conf. Economics Computation (EC).
Agrawal, S., Devanur, N. R., & Li, L. (2015). Contextual bandits global constraints
objective.. Technical report, arXiv:1506.03374.
Antos, A., Bartok, G., Pal, D., & Szepesvari, C. (2013). Toward classification finite
partial-monitoring games. Theor. Comput. Sci., 473, 7799.
Audibert, J., & Bubeck, S. (2010). Regret Bounds Minimax Policies Partial
Monitoring. J. Machine Learning Research (JMLR), 11, 27852836.
Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis multiarmed
bandit problem.. Machine Learning, 47 (2-3), 235256.
Auer, P., Ortner, R., & Szepesvari, C. (2007). Improved Rates Stochastic ContinuumArmed Bandit Problem. 20th Conf. Learning Theory (COLT), pp. 454468.
Babaioff, M., Dughmi, S., Kleinberg, R. D., & Slivkins, A. (2015). Dynamic pricing
limited supply. ACM Trans. Economics Computation, 3 (1), 4.
Babaioff, M., Feldman, M., & Nisan, N. (2006). Combinatorial agency. 7th ACM Conf.
Electronic Commerce (EC).
Badanidiyuru, A., Kleinberg, R., & Singer, Y. (2012). Learning budget: posted price
mechanisms online procurement. 13th ACM Conf. Electronic Commerce
(EC), pp. 128145.
Badanidiyuru, A., Kleinberg, R., & Slivkins, A. (2013). Bandits knapsacks. 54th
IEEE Symp. Foundations Computer Science (FOCS).
Badanidiyuru, A., Langford, J., & Slivkins, A. (2014). Resourceful contextual bandits.
27th Conf. Learning Theory (COLT).
Bartok, G., Foster, D. P., Pal, D., Rakhlin, A., & Szepesvari, C. (2014). Partial monitoring
- classification, regret bounds, algorithms. Math. Oper. Res., 39 (4), 967997.
Besbes, O., & Zeevi, A. (2009). Dynamic pricing without knowing demand function:
Risk bounds near-optimal algorithms. Operations Research, 57, 14071420.
Besbes, O., & Zeevi, A. J. (2012). Blind network revenue management. Operations Research,
60 (6), 15371550.
356

fiAdaptive Contract Design Crowdsourcing Markets

Blum, A., Kumar, V., Rudra, A., & Wu, F. (2003). Online learning online auctions.
14th ACM-SIAM Symp. Discrete Algorithms (SODA), pp. 202204.
Bohren, J. A., & Kravitz, T. (2013). Incentives spot market labor output
unverifiable. Working paper.
Bubeck, S., & Cesa-Bianchi, N. (2012). Regret Analysis Stochastic Nonstochastic
Multi-armed Bandit Problems. Foundations Trends Machine Learning, 5 (1),
1122.
Bubeck, S., Munos, R., Stoltz, G., & Szepesvari, C. (2011a). Online Optimization XArmed Bandits. J. Machine Learning Research (JMLR), 12, 15871627.
Bubeck, S., Stoltz, G., & Yu, J. Y. (2011b). Lipschitz bandits without lipschitz constant.
22nd Intl. Conf. Algorithmic Learning Theory (ALT), pp. 144158.
Bull, A. D. (2013). Adaptive-treed bandits. Tech. rep. 1302.2489, arxiv.org.
Cesa-Bianchi, N., & Lugosi, G. (2006). Prediction, learning, games. Cambridge Univ.
Press.
Conitzer, V., & Garera, N. (2006). Online learning algorithms online principal-agent
problems (and selling goods online). International Conference Machine Learning
(ICML).
den Boer, A. V. (2015). Dynamic pricing learning: Historical origins, current research,
new directions. Surveys Operations Research Management Science. Forthcoming.
Dudik, M., Hsu, D., Kale, S., Karampatziakis, N., Langford, J., Reyzin, L., & Zhang, T.
(2011). Efficient optimal leanring contextual bandits. 27th Conf. Uncertainty
Artificial Intelligence (UAI).
Garivier, A., & Cappe, O. (2011). KL-UCB Algorithm Bounded Stochastic Bandits
Beyond. 24th Conf. Learning Theory (COLT).
Ghosh, A., & Hummel, P. (2011). game-theoretic analysis rank-order mechanisms
user-generated content. 12th ACM Conf. Electronic Commerce (EC).
Ghosh, A., & Hummel, P. (2013). Learning incentives user-generated content: Multiarmed bandits endogenous arms. Proc. 4th Conference Innovations
Theoretical Computer Science (ITCS).
Ghosh, A., & McAfee, P. (2011). Incentivizing high-quality user-generated content. 20th
Intl. World Wide Web Conf. (WWW).
Gittins, J., Glazebrook, K., & Weber, R. (2011). Multi-Armed Bandit Allocation Indices.
John Wiley & Sons.
Harris, C. G. (2011). Youre hired! examination crowdsourcing incentive models
human resource tasks. CSDM.
Ho, C., Slivkins, A., Suri, S., & Vaughan, J. W. (2015). Incentivizing high quality crowdwork. 24th Intl. World Wide Web Conf. (WWW).
Ho, C.-J., Zhang, Y., Vaughan, J. W., & van der Schaar, M. (2012). Towards social norm
design crowdsourcing markets. HCOMP.
357

fiHo, Slivkins, & Vaughan

Horton, J. J., & Chilton, L. B. (2010). labor economics paid crowdsourcing. 11th
ACM Conf. Electronic Commerce (EC).
Jain, S., Chen, Y., & Parkes, D. (2012). Designing incentives online question-and-answer
forums. Games Economic Behavior.
Kleinberg, R. (2004). Nearly tight bounds continuum-armed bandit problem.
18th Advances Neural Information Processing Systems (NIPS).
Kleinberg, R., & Leighton, T. (2003). value knowing demand curve: Bounds
regret online posted-price auctions.. 44th IEEE Symp. Foundations
Computer Science (FOCS), pp. 594605.
Kleinberg, R., Slivkins, A., & Upfal, E. (2008). Multi-armed bandits metric spaces.
40th ACM Symp. Theory Computing (STOC), pp. 681690.
Kleinberg, R. D., & Leighton, F. T. (2003). value knowing demand curve: Bounds
regret online posted-price auctions. IEEE Symp. Foundations Computer
Science (FOCS).
Kocsis, L., & Szepesvari, C. (2006). Bandit Based Monte-Carlo Planning. 17th European
Conf. Machine Learning (ECML), pp. 282293.
Laffont, J.-J., & Martimort, D. (2002). Theory Incentives: Principal-Agent
Model. Princeton University Press.
Lai, T. L., & Robbins, H. (1985). Asymptotically efficient Adaptive Allocation Rules.
Advances Applied Mathematics, 6, 422.
Levy, A., & Vukina, T. (2002). Optimal linear contracts heterogeneous agents.
European Review Agricultural Economics.
Mason, W., & Watts, D. (2009). Financial incentives performance crowds.
HCOMP.
Misra, S., Nair, H. S., & Daljord, O. (2012). Homogenous contracts heterogeneous
agents: Aligning salesforce composition compensation. Working Paper.
Munos, R., & Coquelin, P.-A. (2007). Bandit algorithms tree search. 23rd Conf.
Uncertainty Artificial Intelligence (UAI).
Pandey, S., Agarwal, D., Chakrabarti, D., & Josifovski, V. (2007). Bandits Taxonomies:
Model-based Approach. SIAM Intl. Conf. Data Mining (SDM).
Radlinski, F., Kleinberg, R., & Joachims, T. (2008). Learning diverse rankings multiarmed bandits. 25th Intl. Conf. Machine Learning (ICML), pp. 784791.
Sannikov, Y. (2008). continuous-time version principal-agent problem.
Review Economics Studies.
Sannikov, Y. (2012). Contracts: theory dynamic principal-agent relationships
continuous-time approach. 10th World Congress Econometric Society.
Singer, Y., & Mittal, M. (2013). Pricing mechanisms crowdsourcing markets. 22nd
Intl. World Wide Web Conf. (WWW).
Singla, A., & Krause, A. (2013). Truthful incentives crowdsourcing tasks using regret
minimization mechanisms. 22nd Intl. World Wide Web Conf. (WWW).
358

fiAdaptive Contract Design Crowdsourcing Markets

Slivkins, A. (2011). Multi-armed bandits implicit metric spaces. 25th Advances
Neural Information Processing Systems (NIPS).
Slivkins, A. (2014). Contextual bandits similarity information. J. Machine Learning
Research (JMLR), 15 (1), 25332568. Preliminary version COLT 2011.
Slivkins, A., Radlinski, F., & Gollapudi, S. (2013). Ranked bandits metric spaces: Learning optimally diverse rankings large document collections. J. Machine Learning
Research (JMLR), 14 (Feb), 399436. Preliminary version 27th ICML, 2010.
Thompson, W. R. (1933). likelihood one unknown probability exceeds another
view evidence two samples.. Biometrika, 25 (3-4), 285294.
Wang, Z., Deng, S., & Ye, Y. (2014). Close gaps: learning-while-doing algorithm
single-product revenue management problems. Operations Research, 62 (2), 318331.
Williams, N. (2009). dynamic principal-agent problems continuous time. Working
Paper.
Yin, M., Chen, Y., & Sun, Y.-A. (2013). effects performance-contingent financial
incentives online labor markets. AAAI.
Zhang, Y., & van der Schaar, M. (2012). Reputation-based incentive protocols crowdsourcing applications. Infocom.

359

fiJournal Artificial Intelligence Research 55 (2016) 209-248

Submitted 03/15; published 01/16

Synthetic Treebanking Cross-Lingual
Dependency Parsing
Jorg Tiedemann

jorg.tiedemann@helsinki.fi

Department Modern Languages, University Helsinki
P.O. Box 24, FI-00014 University Helsinki, Finland

Zeljko Agic

zeljko.agic@hum.ku.dk

Center Language Technology, University Copenhagen
Njalsgade 140, 2300 Copenhagen S, Denmark

Abstract
parse languages treebanks available? contribution
addresses cross-lingual viewpoint statistical dependency parsing, attempt
make use resource-rich source language treebanks build adapt models
under-resourced target languages. outline benefits, indicate drawbacks
current major approaches. emphasize synthetic treebanking: automatic creation
target language treebanks means annotation projection machine translation.
present competitive results cross-lingual dependency parsing using combination
various techniques contribute overall success method.
include detailed discussion impact part-of-speech label accuracy parsing
results provide guidance practical applications cross-lingual methods truly
under-resourced languages.

1. Introduction
Languages dialects army navy famous saying popularized
sociolinguist Max Weinreich. modern times, quote could rephrasedand languages
definedas dialects part-of-speech tagger, treebank, machine translation
system. Even though proposition would disqualify languages world,
true existence many languages threatened due insufficient resources
technical support. Natural language processing (NLP) becomes increasingly important
peoples everyday life look, example, success word prediction, spelling
correction, instant on-line translation. Building linguistic resources tools, however,
expensive time-consuming, one great challenges computational linguistics
port existing models new languages domains.
Modern NLP requires data, often annotated explicit linguistic information,
tools learn them. However, sufficient quantities electronic data sources
available handful languages whereas languages
privilege draw resources (Bender, 2011; Uszkoreit & Rehm, 2012; Bender,
2013). Speakers low-density languages countries live able invest
large data collection time-consuming annotation efforts, goal cross-lingual
c
2016
AI Access Foundation. rights reserved.

fiTiedemann & Agic

NLP share rich linguistic information poorly supported languages, making
possible build tools resources without starting scratch.
paper, consider task statistical dependency parsing (Kubler, McDonald,
& Nivre, 2009). Top-performing dependency parsers typically trained dependency
treebanks include several thousands manually annotated sentences. statistical
parsing models known robust efficient, yielding high accuracy unseen
texts. However, even moderately-sized treebanks take lot time resources produce
(Abeille, 2003), point, unavailable scarce even major languages.
Thus, similar areas NLP research, face challenge posed abstract:
parse languages dependency treebanks available? Without
annotated training data basically four options data-driven NLP:
1. build parsing models learn raw data using unsupervised machine
learning techniques.
2. manually annotated data scarcely available, resort various approaches
semi-supervised learning, leveraging various sources fortuitous data (Sgaard,
2013).
3. transfer existing models tools new languages.
4. transfer data resource-rich languages resource-poor languages
build tools data sets.
four viewpoints studied intensively connection dependency parsing
NLP general. parsing, first option especially difficult unsupervised
approaches still fall far behind rest field (Sgaard, 2012). Unsupervised models
also difficult evaluate applications build labeled information problems
making use structures produced models. Semi-supervised learning either
augments well-resourced environments improved cross-domain robustness, largely
coincides cross-lingual approaches loosely defined (Sgaard, 2013).
Therefore, surprising final two options attracted quite popularity
gained lot merit enabling parsing low-resource languages. paper,
exclusively look techniques.
basic idea behind transfer approaches tools resources exist
resource-rich source languages used build corresponding tools resources underresourced target languages means adaptation. statistical dependency parsing
cross-lingual approach essentially means either take parsing model apply
another language use treebanks train parsers new language target
language adaptation taking place workflow stages. can, thus, divide
main approaches cross-lingual dependency parsing two categories: model transfer
data transfer.
Model transfer methods appealing property focus language
universals structures identified various languages without side-stepping
(semi-)automatic creation annotated data target language. strong
line research looking identification cross-lingual features used
port models tools new languages. One biggest drawbacks extreme
210

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

abstraction generic features cannot cover language-specific properties natural
languages. Therefore, methods often restricted closely related languages
performance usually far fully supervised target-specific parsing models.
Data transfer methods, hand, emphasize creation artificial training
data used standard machine learning techniques build models
target language. work focused annotation projection use
parallel data, is, documents translated languages. Statistical alignment
techniques make possible map linguistic annotation one language another.
Another recent approach proposes translation treebanks (Tiedemann, Agic, & Nivre,
2014) enables projection annotation without parsing unrelated parallel corpora.
methods create synthetic data sets without manual intervention and, therefore,
group techniques general term synthetic treebanking, main focus
paper.
structure paper follows. brief outlook contributions
work, first provide overview cross-lingual dependency parsing approaches.
that, discuss depth experiments synthetic treebanks, inspect
annotation projection parallel data sets translated treebanks. also include
thorough study impact part-of-speech (PoS) tagging cross-lingual parsing.
concluding final remarks prospects future work, discuss impact
contribution comparison selected recent approaches, terms empirical
assessment underlying requirements imposed truly under-resourced languages.
1.1 Contributions
paper addresses annotation projection treebank translation detailed
systematic investigation various techniques strategies. build previous
work cross-lingual parsing (Tiedemann et al., 2014; Tiedemann, 2014, 2015) extend
study detailed discussions advantages drawbacks method. also
include new idea back-projection integrates machine translation parsing
workflow. main contributions following:
1. provide overview various approaches cross-lingual dependency parsing
detailed discussions properties utilized techniques.
2. present new competitive cross-lingual parsing results using synthetic treebanks.
ground results discussion related work implications truly
under-resourced languages.
3. provide thorough study impact PoS tagging cross-lingual dependency
parsing.
delving details let us first review selected current approaches
cross-lingual dependency parsing connect work presented paper related
research.
211

fiTiedemann & Agic

2. Current Approaches Cross-Lingual Dependency Parsing
section provides overview cross-lingual dependency parsing. discuss
previously outlined annotation projection model transfer approaches depth
including recent developments field. Cross-lingual parsing combines many efforts
dependency treebanking, creating standards PoS syntactic annotations.
start outlining current practices empirical evaluation cross-lingual parsers,
linguistic resources used benchmarking.
2.1 Treebanks Evaluation
supervised setting, cross-lingual dependency parsing amounts training parser
treebank, applying target text. However, empirical quality assessment
parser target data introduces certain additional constraints. evaluate
supervised cross-lingual parsers, require least following three components:
1. parser generators: trainable, language-independent dependency parsing systems,
2. dependency treebanks source languages,
3. held-out evaluation sets target languages.
years following venerable CoNLL 2006 2007 shared task campaigns
dependency parsing (Buchholz & Marsi, 2006; Nivre, Hall, Kubler, McDonald, Nilsson,
Riedel, & Yuret, 2007), many mature parsers made publicly available across different
parsing paradigms. resolves first point list, choosing applyand
comparing betweendifferent approaches parsing cross-lingual setup nowadays
made trivial abundant parser availability. easily benchmark respectable
number parsers accuracy, processing speed, memory requirements.
Experimental setup cross-lingual parsing thus amounts choosing training
testing data, defining evaluation metrics.
2.1.1 Intrinsic Extrinsic Evaluation
perform intrinsic extrinsic evaluation dependency parsing. intrinsic evaluation,
typically apply evaluation metrics gauge various aspects parsing accuracy
held-out data, extrinsic evaluation, parsers scored gains yielded
subsequentor downstreamtasks make use dependency parses additional
input.
Dependency parsers intrinsically evaluated labeled (LAS) unlabeled (UAS)
attachment scores: portions correctly paired heads dependents dependency
trees, without keeping track edge labels, respectively. Sometimes also
evaluate labeled (LEM) unlabeled (UEM) exact match scores, determine often
parsers correctly parse entire sentences. detailed exposition dependency
parser evaluation, see work Nivre (2006) Kubler et al. (2009), also note
Plank et al. (2015) provide detailed insight correlations various
dependency parsing metrics human judgements quality parses.
212

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

monolingual intrinsic evaluation scenario, either predefined held-out test
data disposal, cross-validate slicing treebank training test
sets. cases, treebank test sets belong resource,
created using annotation scheme, turn typically stems
underlying syntactic theory. However, given heterogenous development syntactic
theories, subsequently treebanks different languages (Abeille, 2003),
necessarily hold cross-lingual setup. Moreover, excluding recent treebanking
developmentswhich discuss bit sectionprior 2013, odds
randomly sampling pool publicly available treebanks drawing source-target
pair annotated (or even similar) scheme virtually non-existent.
syntactic annotation schemes generally differ in: (a) rules attaching dependents
heads, (b) dependency relation labels, is, syntactic tagsets. Given two
treebanks incompatible syntactic annotations, without performing conversions,
likely expect similarities head attachment rules, syntactic tagsets.
fact present initial cross-lingual parsing experiments (Zeman & Resnik,
2008; McDonald, Petrov, & Hall, 2011; Sgaard, 2011). initial efforts charting
cross-lingual dependency parsing mainly used CoNLL shared task datasets,
evaluated UAS. rare exceptions are, example, generally under-resourced
Slavic languages (Agic, Merkler, & Berovic, 2012) subscribing (slightly modified versions
of) Prague Dependency Treebank scheme (Bohmova, Hajic, Hajicova, & Hladka, 2003).
recently, substantial effort undertaken bridging annotation scheme gap
dependency treebanking facilitate uniform syntactic processing worlds languages.
effort resulted two editions Google Universal Treebanks (UDT) (McDonald et al.,
2013), turn recently superseded Universal Dependencies project (UD)
(Nivre et al., 2015). projects, Stanford typed dependencies (SD) (De Marneffe,
MacCartney, & Manning, 2006) used adaptable basis designing underlying
annotation scheme, applying using human expert annotators several languages.
datasets made possible first reliable cross-lingual dependency parsing experiments,
namely ones McDonald et al. (2013), also enabled use LAS default
evaluation metric, like monolingual parsing. reasons, UDT UD
de facto standard datasets benchmarking cross-lingual parsers today, CoNLL
datasets still used mainly backward compatibility previous research. another
effort, HamleDT dataset (Zeman et al., 2014), 30 treebanks automatically converted
Prague scheme, SD, also frequently used evaluation campaigns.
currently note preference UDT UD, since produced
manual annotation.
Given short exposition dependency treebanking relation cross-lingual
parsing, paper, opt using UDT experiments. choice sources
targets, Cartesian product dataset: treat available languages
sources targets. common approach cross-lingual parsing,
even research uses English source-only language, treats
languages targets.
extrinsic evaluation cross-lingual parsing much less developed, although
arguments favor convincing. Namely, underlying goal cross-lingual
parsing enabling processing actual under-resourced languages. languages,
213

fiTiedemann & Agic

even parsing test sets may readily available. conducting empirical evaluations
extreme cases, might resort downstream applications (Elming et al., 2013).
choice downstream tasks might pose separate challenge case, devising
feasible (and representative) tasks extrinsic evaluation cross-lingual dependency parsing
remains largely unaddressed. paper, deal intrinsic evaluation.
2.1.2 Part-of-Speech Tagging
noted brief introduction model transfer, dependency parsers make heavy use
PoS features. syntactic annotations, sources targets may may
shared PoS annotation layers, moreover, PoS taggers may may available
target languages.
issue PoS compatibility arguably less difficult resolve structural
labeling differences dependency trees, PoS tags less straightforwardly
mapped one another. point, also note recent approaches learning
PoS tag conversions (Zhang, Reichart, Barzilay, & Globerson, 2012), systematically
facilitate conversions. Furthermore, efforts UDT/UD also build shared PoS
representation, so-called Universal PoS (UPoS) (Petrov et al., 2012). UD extends
UPoS specification introducing additional PoS tags17 instead initial 12and
providing support standardized morphological features noun gender
case, verb tense. said, added features yet readily available,
shared representation UDT/UD amounts 12- 17-tag-strong PoS tagset.
treatment source languages respect PoS tagging, work
cross-lingual parsing presumes existence taggers, even tests gold standard PoS
input. Recently, Petrov (2014) argued strongly use predicted PoS cross-lingual
parsing, make realistic testing environment, especially increased
availability weakly supervised PoS taggers (Li et al., 2012; Garrette et al., 2013).
paper, experiment gold standard predicted PoS features order stress
impact tagging accuracy parsing performance. also discuss implications
choices enabling processing truly under-resourced languages.
2.2 Model Transfer
proceed sketch main approaches cross-lingual dependency parsing: model
transfer, annotation projection, treebank translation. also reflect usage
cross-lingual word representations cross-lingual parsing, particularly emphasize
annotation projection treebank translation approaches.
Simplistic model transfer amounts applying source models targets
adaptation, still rather successful closely related languages (Agic et al.,
2014). However, flavor model transfer recently attracted fair amount
interest owes availability cross-lingually harmonized annotation (Petrov et al., 2012)
makes possible use shared PoS features across languages. straightforward
technique train delexicalized parsers heavily rely UPoS tags. Figure 1 illustrates
basic idea behind models. simple technique shown success
closely related languages (McDonald et al., 2013). Several improvements achieved
using multiple source languages (McDonald et al., 2011; Naseem, Barzilay, & Globerson,
214

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

label 2
label 1

label 3

(1) delexicalize
pos1

src1
lexicalized

parser

pos2

pos3

src2 src3

pos4

src4

(4) re-train
pos2

trg1

pos1

trg2

(2) train

delexicalized

parser

(3) parse
pos3

trg3

label 1

pos4

trg4

label 3

label 2

Figure 1: illustration delexicalized model transfer, implication
lexicalization option self-training.

2012), additional cross-lingual features used transfer models new
language, cross-lingual word clusters (Tackstrom, McDonald, & Uszkoreit, 2012)
word-typology information (Tackstrom, McDonald, & Nivre, 2013b). ways
re-lexicalize models well. Figure 1 suggests self-learning procedure adds lexical
information data sets automatically annotated using delexicalized
models. Various data selection techniques used focus reliable cases improve
value induced lexical features.
advantage transferred models require parallel data, least
generic form. However, reasonable models require kind target
language adaptation parallel comparable data sets usually necessary perform
adaptations. largest drawback model transfer strong abstraction
language-specific features universal properties. many fine-grained linguistic
differences, kind coarse-grained universal knowledge often informative enough
(Agic et al., 2014). Consequently, large majority recent approaches aim bridging
representational deficiency.
2.3 Cross-Lingual Word Representations
Model transfer requires abstract features capture universal properties languages.
use cross-lingual word clusters already mentioned previous section,
benefits monolingual clustering dependency parsing well-known (Koo,
Carreras, & Collins, 2008). Recently, distributed word representations entered NLP
various models (Collobert et al., 2011). so-called word embeddings capture
distributional properties words continuous vector representations used
measure syntactic semantic relations even across languages (Mikolov, Le, & Sutskever,
2013). monolingual variety found many applications NLP. Distributed word
representations cross-lingual dependency parsing first applied recently Xiao
Guo (2014). explore word embeddings another useful abstraction enables
robust model transfer across languages. However, apply techniques
215

fiTiedemann & Agic

old CoNLL data sets cannot provide labeled attachment scores comparable results
settings.
Several recent publications show bilingual word embeddings learned aligned
bitexts improve semantic representations. Faruqui Dyer (2014) use canonical correlation
analysis find cross-lingual projections monolingual vector space models. Zou, Socher,
Cer, Manning (2013) learn bilingual word embeddings fixed word alignments.
Klementiev, Titov, Bhattarai (2012) treat cross-lingual representation learning
multitask learning problem cross-lingual interactions based word alignments
word embeddings shared across various tasks. techniques
significant value improved model transfer may act necessary target language
adaptation move beyond language universals feature transfer models.
cross-lingual parsing, envision word representations valuable addition
model transfer direction regularization. said, usage maintains
previously listed advantages drawbacks model transfer, adds another prerequisite:
availability parallel texts inducing embeddings.
recent developments creating cross-lingual embeddings without parallel text (Gouws &
Sgaard, 2015) applicability dependency parsing yet verified. Here,
note recent contribution Sgaard et al. (2015), use inverted indexing
cross-lingually overlapping Wikipedia articles produce truly inter-lingual word embeddings.
show competitive scores cross-lingual dependency parsing, address
contribution related work discussion.
2.4 Annotation Projection
use parallel corpora automatic word alignment transferring linguistic annotation source language new target language quite long tradition NLP.
pioneering work Yarowsky, Ngai, Wicentowski (2001) followed number
researchers, various tasks, transfer dependency annotation among others
(Hwa et al., 2005). basic idea use existing tools models annotate source
side parallel corpus use alignment guide mapping annotation
target side corpus. Assuming source language annotation sufficiently
correct aligned target language reflects syntactic patterns,
train parsers projected data bootstrap tools languages without explicit linguistic
resources syntactically annotated treebanks. Figure 2 illustrates general idea
annotation projection case syntactic dependencies parser model induction.
Note PoS labels typically projected well along dependency relations.
first attempts directly map dependency information coming diverse treebanks
resulted rather poor performance. work, Hwa et al. (2005) rely additional
post-processing rules transform results reasonable structures. argued
previous subsection, one main problems early work incompatibility
treebanks individually developed various languages following different
guidelines using different label sets. latter also reason labeled
attachment scores could reported work, makes difficult place
cross-lingual approaches relation standard models trained target language.
216

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

label 2
label 1

pos1

src1

(1) parse

label 3

pos2

pos3

src2 src3

pos4

(2) project

trg1

trg2

trg3

trg4

pos2

pos1

pos3

pos4

label 1

parser

parser

src4

word-aligned bitext

lexicalized

lexicalized

label 3

(3) train
label 2

Figure 2: illustration syntactic annotation projection system cross-lingual
dependency parsing.

Less frequent, also possible, scenario source side parallel
corpus contains manual annotation (Agic et al., 2012). addresses problem created
projecting noisy annotations, presupposes parallel corpora manual annotation,
rarely available. Additionally, problem incompatible annotation still remains.
introduction cross-lingually harmonized treebanks changed situation significantly (McDonald et al., 2013). data sets use identical labels adhere similar
annotation guidelines make possible directly compare structures projected
languages. work Tiedemann (2014), explore projection strategies
discuss success annotation projection comparison cross-lingual approaches. work builds direct correspondence assumption (DCA) proposed
Hwa et al. (2005). define several projection heuristics make possible project
dependency structure given word alignments target language sentence.
basic procedures cover different types word alignments. One-to-one alignments
straightforward case dependency relations simply copied. Unaligned
source language tokens covered additional DUMMY nodes capture relations
connected token source language (see left-most graph Figure 3).
Many-to-one links resolved keeping link head aligned source
language tokens deleting links (see graph middle). One-to-many
alignments handled introducing additional DUMMY nodes act immediate
parent target language, capture dependency relation source
side annotation (see right-most graph Figure 3). Many-to-many alignments
treated two steps. First apply rule one-to-many alignments
many-to-one rule. Finally, unaligned target language tokens simply dropped
removed target sentence.
issues explicitly covered original publication algorithm.
example, entirely clear sequence rules applied
labels projected. rules, example, change alignment structure
may cause additional unaligned source tokens need handled rules.
implementation, first apply one-to-many rule cases sentence
217

fiTiedemann & Agic

label 2
label 1

label 2

label 3

label 1

pos1

src1
pos2

trg1

pos2

pos3

src2 src3

pos1

trg2

pos3

pos4

src4

pos1

pos1

pos2

pos3

pos2

pos1

pos4

src1

src2 src3

src4
pos1

trg1

trg2

trg3

src2

dummy dummy

DUMMY trg1

trg2

dummy

label 3

pos3

src3
pos2

trg3

pos3

trg4

label 2

dummy

label 1
label 2

label 2

pos2

src1

pos4

pos4

trg3 DUMMY

label 1

label 1

label 3

label 1

label 2

Figure 3: Annotation projection heuristics special alignment types: Unaligned source
words (left graph), many-to-one alignments (center), one-to-many alignments
(right graph).

applying many-to-one rule and, thereafter, resolving unaligned source tokens.
final step includes mapping dependency relations remaining one-to-one
alignments. one-to-many alignments, transfer PoS dependency labels
newly created DUMMY node (following rule one-to-one alignments resolving
one-to-many link) previously aligned target language tokens obtain DUMMY
PoS labels dependency relation governing DUMMY node also labeled
DUMMY (see Figure 3).
Projecting syntactic dependency annotation creates several problems well. First
all, crossing word alignments cause large amount non-projectivity projected
data. percentage non-projective structures goes 50% UDT
data (Tiedemann et al., 2014). Furthermore, projection heuristics lead conflicting
annotation shown authentic example illustrated Figure 4. issues put
additional burden learning algorithms many cross-lingual errors caused
complex ambiguous cases.
Nevertheless, Tiedemann (2014) demonstrates annotation projection competitive
cross-lingual methods merits explored Tiedemann (2015).
cc
adpobj

Tous ses produits sont de qualit

et

dune fraicheur exemplaires .

... high- quality DUMMY ...
adpobj
dummy
dummy

cc

added nonprojectivity
inconsistencies

Figure 4: Issues annotation projection illustrated real-life example.
218

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

label 2

Treebank translation

label 1

pos1

src1

label 3

pos2

pos3

src2 src3

pos4

src4

(1) translate

(2) project
trg1

trg2

trg3

trg4

pos2

pos1

pos3

pos4

label 1

lexicalized

parser

label 3

(3) train
label 2

Figure 5: illustration synthetic treebanking approach translation.
2.5 Translating Treebanks
notion translation cross-lingual parsing first introduced Zhao, Song, Kit,
Zhou (2009), use bilingual lexicon lookup-based target adaptation. similar
method also adopted Durrett et al. (2012). simplistic lookup approach used
Agic et al. (2012), exploit availability parallel corpus two closely related
languages, one side corpus dependency treebank. former evaluates
UAS 9 languages CoNLL datasets, latter research deals
Croatian Slovene smaller scale.
Tiedemann et al. (2014) first use full-scale statistical machine translation (SMT)
synthesize treebanks SMT-facilitated target language adaptations cross-lingual
parsing. use UDT LAS evaluation, also performing subset experiments
CoNLL 2007 data backward compatibility. paper, often refer to,
build work. Figure 5 illustrates general idea technique,
proceed discuss implications.
sketched introduction, core synthetic treebanking idea
concept automatic source-to-target treebank translation. workflow consists
following steps:
1. Take source-target parallel corpus large monolingual target language corpus
train (ideally top-performing) SMT system, orif availableapply existing
source-target machine translation system.
2. Given source language treebank, translate target language. Word-align
original sentence translation, preserve phrase alignments provided
SMT system.
3. Use alignments project dependency annotations source treebank
target translation, turn creating artificial (or synthetic) treebank
target language.
4. Train target language parser synthesized treebank, apply (or evaluate)
target language data.
219

fiTiedemann & Agic

sketch treebank translation opens large parameter tuning search space,
also outlines various properties approach. discuss briefly, defer
reader detailed expositions many intricacies papers (Tiedemann et al.,
2014; Tiedemann, 2014, 2015).
2.5.1 Components
prerequisites building SMT-supported cross-lingual parsing system are: (a)
availability parallel corpora, (b) platform building state-of-the-art SMT systems, (c)
algorithms robust annotation projection, (d) previously listed resources needed
cross-lingual parsing general: treebanks parsers.
Parallel corpora available large number language pairs, even outside
benchmarking frameworks CoNLL UDT. size domains parallel
data influences quality SMT, subsequently cross-lingual parsers.
SMT community typically experiments Europarl dataset (Koehn, 2005),
many datasets also freely available cover many languages,
OPUS collection (Tiedemann, 2012). Ideally, parallel corpora used SMT
large, source-target pairs, may necessarily case. Moreover,
corpora might spread across domains interest, leading decreased performance.
Domain dependence thus inherent choice parallel corpora training SMT
systems. Here, note recent contribution Agic et al. (2015), learn hundred PoS
taggers truly under-resourced languages using label propagation multi-parallel
Bible corpus, indicating possibility bootstrapping NLP tools even hostile
environments, subsequent applicability tools across domains.
paper, opt using Moses (Koehn et al., 2007) de facto standard
platform conducting SMT research. summary, since approach SMT goes
beyond dictionary lookup Durrett et al. (2012), mainly experiment phrasebased models, gaining target language adaptations form lexical
features reordering. projection algorithms synthetic treebanking
whole transferred annotation projection approaches. do, however, consider
various parametrizations, Tiedemann et al. (2014) previously proposed novel
algorithm, Tiedemann (2014) thoroughly compared various approaches annotation
projection.
2.5.2 Advantages Drawbacks
Automatic translation advantage use manually verified annotation
source language treebank given word alignment, integral part
translation model. Recent advances statistical machine translation (SMT) combined
ever-growing availability parallel corpora making realistic alternative.
relation annotation projection obvious involve parallel data one side
annotated. However, use direct translation brings two important advantages.
First all, using SMT, accumulate errors two sources: tooltagger
parserused annotate source language bilingual corpus, noise coming
alignment projection. Instead, use gold standard annotation source
language safely assumed much higher quality automatic
220

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

Input: source tree S, target sentence ,
word alignment A, phrase segmentation P
Output: syntactic heads head[],
word attributes attr[]
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27

Input: node s, source tree root ROOT,
target sentence , word alignment
Output: node t*

1 == ROOT
treeSize = max distance root(S) ;
2
return ROOT ;
attr = [] ;
3 end
head = [] ;
use phrase4 unaligned src(s,A)
2
5
= head of(s,S) ;
segmentation
unaligned trg(t,A)
6
== ROOT
2 trg phrase(t,P)
7
return ROOT
;
function:
find_aligned:
[sx ,..,sy ] = aligned to(t) ;
8
end
= find
highest([s
,..,sy ],S)
;
9 end
Input:
source
tree S,xtarget
sentence
T,
Input: node s, source tree root ROOT,
word
alignment
A, phrase segmentation
P
10 p = 0 ; target sentence , word alignment
= find
aligned(s,S,T,A)
;
Output:
heads
Output:
node t*
11 t* = undef
;
attr[t] =syntactic
DUMMY
; head[],
walk tree
word
attributes
12 2
dothen
head[t]
= ; attr[]
1 aligned(s,A)
== ROOT
unaligned
13
2position(t,T)
> p
1 treeSize = max distance root(S) ;
return ROOT
;
end
14
t* = ;
3 end
else 2 attr = [] ;
attach 15
p = position(t,T)
;
4
unaligned; src(s,A)
[s3 xhead
,..,sy=] =[] aligned
to(t) ;
highest node
4


2


5
= head of(s,S) ;
16
end
= find highest([sx ,..,sy ],S) ;
5
unaligned
trg(t,A)
6
== ROOT
17 end
attr[t]
=ifattr(s)
;
t; 2 trg phrase(t,P)
7 ;
return ROOT ;
18 return t*
s6= head of(s,S)
7
[sx ,..,sy ] = aligned to(t) ;
8
end
8= find aligned(s,S,T,A)
;
= find highest([s
x ,..,sy ],S) ;
9 end
==
Figure 3: Procedure find aligned().
10 p = 0 ;
9
= find aligned(s,S,T,A) ;
heuristics
[s
,..,s
]
=

src
phrase(s,P)
;
x

11 t* = undef ;
10
attr[t] = DUMMY ;
s* = find highest([sx ,..,sy ],S) ;
multiple targets:
12 2 aligned(s,A)
11
head[t] = ;
= head of(s*,S) ;
13
position(t,T) > p
take
right-most
12
end
features

options

optimized
using
MaltOpt =else
find aligned(s,S,T,A) ;
14
t* = ;
13
p = position(t,T)
head[t] [s
=xt,..,s
; ] = aligned to(t) ;
timizer.15The accuracy
given ;in Table 3 set
14
16
end
end
15
= find highest([sx ,..,sy ],S) ;
labeled
attachment
scores (LAS). include
17 end
attr[t] = attr(s) ;
end 16
punctuation


evaluation.
Ignoring punctua18 return t* ;
17
= head of(s,S) ;
end

tion generally leads slightly higher scores
Figure
3: Procedure
finddoaligned().
noted
experiments

report
DUMMY
nodes
numbers
here.proposed
Note also
Tiedemann
columns et al.
represent target languages (used testing),
features options optimized using MaltOpwhile
rows denote source languages (used
timizer. accuracy given Table 3 set
training),
McDonald
al. (2013).
labeled
attachment et
scores
(LAS). include
Frompunctuation
table,
weour

see

baseline
evaluation.
Ignoring
punctuascores
compatible


ones


tion generally leads slightly higheroriginal
scores
chine translation
annotation
projection.
Here,
presented
(McDonald
etof
al.,cross-domain
2013),
annotation
obtained
using
tool trained

data,
especially
light

noted
ourbyexperiments



report
Figure 2: Annotation
projection
algorithm.experiments

also
look

delexicalized
models
trained

included

Table
3

reference.

differences

numbers
here.
Note
also


columns
accuracy drops. Moreover, using SMT may help bypassing domain shift problems,

translated treebanks show effect machine

due

parser
selection,


use

transitionrepresent

target
languages
(used

testing),
common applying tools trained (and evaluated) one resource text

translationdelexicalized
without additional
features.
transferlexical
parsing
following based
ap- parser

rows
denote
source
languages (used

beam
search
perceptron
another domain.
proach McDonald et al. (2013). Second,learning

inalong
training),
inof
McDonald
et Nivre
al. (2013).
lines
Zhang
(2011)
5.1 Baseline
Results
present
results obtained parsers trained


table,


see


baseline
whereas

rely

greedy
transition-based
parsSecondly, assume SMT produce output much closer

target language treebanks produced using ing
ma-withscores

compatible

onesIninthe
thefoloriginal
linear
support
vector
machines.
First

present

baseline
parsing
scores.

input
manual
translations

parallel
texts
usually
are.
Even


may
seem
like
chine translation annotation projection. Here,
presented
(McDonald
et al.,as
2013),
lowing,experiments
compare
results
baseline
baselines explore
are: (i)
monolingual
baseshortcoming

general,


case

annotation
projection


rather


advantage,
also look delexicalized models trainedwe
onhaveincluded
Tablesetup
3 forinreference.
differences
comparable
experiments.
line, i.e., training testing using lantranslated
show effect machine

makestreebanks
moreto straightforward

less
error-prone

transfer
annotation

due

parser
selection,


use
transitionHowever, improvements shown
also
guage datatranslation
without
Universal
Dependency Treeadditional lexical
features.
based
parser

beam
search

perceptron
source

target.
Furthermore,

alignment

words

phrases

inherently
apply comparison (McDonald et al., 2013).
bank (ii) delexicalized baseline, i.e., applying
learning
along
lines Zhang
Nivre (2011)
provided


output


common
SMT
models.
Hence,

additional
procedures

delexicalized
parsers
across
languages.
5.1 Baseline Results
whereas weTreebanks
rely greedy transition-based pars5.2
Translated
beFor
performed
top
theMaltParser
translated
corpus. Recent
research
(Zhao
et
al.,
2009;
Durrett
monolingual
baseline,
moding linear support vector machines. folFirst present baseline parsing scores.
et
2012)

attempted

address
synthetic
datatowe
creation
results
syntactic
els al.,
trained


original
treebanks

uniNow

turn
thewill
experiments
translated
tree- asvia
lowing,
compare
parsing
baseline
baselines explore are: (i) monolingual baseversal
POS
labels

lexical
features

leavbanks.

consider
two
setups.
First,

look
atmodels
bilingual line,
lexica.
Tiedemann
et al.using
(2014)
extend
proposing
three
different
weidea

comparable
setup

experiments.
i.e., training
testing

lan-
ing


language-specific
features


exthe
effect

translation

training
delexicalHowever,
mostand
improvements
shown
also
automatic
translation
based Dependency
induced
bilingual
lexica
phrase-based
translation
guage data
Universal
Treeist bank
original
Theauthors
delexicalized
ized
Incomparison
way, algorithm


perform
aetdirect
apply


(McDonald
al., 2013).the
treebanks.
(ii)work,
delexicalized
baseline,
i.e.,
applying
models.


propose
parsers.
new
projection

avoids
parsers trained universal
POSlanguages.
labels
comparison baseline performance presented
parsers
across
creation delexicalized
DUMMY
nodes
target language

discussed section 2.4.
5.2
Translated
above.
second
setupTreebanks
considers fully lexfor language



applied



monolingual baseline, MaltParser modThe
procedure

summarized


pseudo-code
shown

Figure
6.
languages els
without
modification.

models,with uniicalizedNow
models
trained
translatedontreebanks.
trained
original
treebanks
turn
theon
experiments
translated treet = find aligned(s,S,T,A) ;
==
Figure202: Annotation
[sxprojection
,..,sy ] = srcalgorithm.
phrase(s,P) ;
Figure 6:21 Annotation
projection
without
s* = find
highest([sx ,..,s
],S) ;
22 (2014).
= head of(s*,S) ;
23
= find aligned(s,S,T,A) ;
delexicalized
transfer parsing
following ap24
head[t] = ;
proach 25McDonald
endet al. (2013). Second,
end obtained parsers trained
present the26 results
27 end
target language
treebanks produced using ma18
19

versal POS labels lexical features leaving language-specific features they221
exist original treebanks. delexicalized
parsers trained universal POS labels
language applied
languages without modification. models,

banks. consider two setups. First, look
effect translation training delexicalized parsers. way, perform direct
comparison baseline performance presented
above. second setup considers fully lexicalized models trained translated treebanks.

fiTiedemann & Agic

root
p
adpmod

nsubj

adpmod adpobj

adpobj
det

amod

PRON VERB ADP NOUN ADJ ADP DET NOUN .
Ils tiraient

balles reelles sur
la
foule .



firing live rounds crowd .
PRON PRON VERB ADP NOUN ADP DET NOUN .
adpmod adpobj

nsubj
nsubj

det
adpobj

adpmod
p

root

Figure 7: example sentence translated French English projections using
algorithm shown Figure 6. boxes indicate segmentation used
phrase-based translation model.

key feature algorithm makes use segmentation sentences
phrases together counterparts language applied
underlying translation model. use information handle unaligned tokens
without creating additional DUMMY nodes described Figure 6. However, contrary
expectations, algorithm work well practice Tiedemann
et al. (2014) show empirically simple word-to-word translation model outperforms
phrase-based systems projection algorithm cases. Part problem
ambiguous projection PoS labels handling one-to-many many-to-one alignments.
example shown Figure 7. assigned pronouns due
links French Ils certainly confuses model trained projected data.
treebank translation approach using phrase-based SMT explored
Tiedemann (2014). Tiedemann (2015) introduces use syntax-based SMT crosslingual dependency parsing. work, authors propose several improvements
DCA-based projection heuristics originally developed Hwa et al. (2005). Simple
techniques reduce number DUMMY elements projected data help
significantly improve results cross-lingual parsing. also realized placement
DUMMY nodes crucial. Strategies choose positions minimize risk
additional non-projectivity useful improve parser model induction. mainly
use techniques developed work experiments described section 3.
drawbacks synthetic treebanking approach related hybrid nature:
a) inherits syntax projection risks annotation projection approach
success bound projection quality, b) critically depends quality
SMT, turn depends size quality underlying parallel corpora.
222

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

latter point, experiments Tiedemann et al. (2014) reveal compelling
robustness cross-lingual parsing SMT noise framework, paper
also argue projection synthetic texts simpler projection actual
parallel text. Another important drawback need large parallel data sets train
reasonable translation models languages consideration. Alternatively,
handcrafted rule-based system could applied well. However, systems data
sets rarely available low-resource languages. hand, techniques
improve machine translation via bridge languages. Tiedemann Nakov (2013)
demonstrate small amounts parallel data successfully used building
translation models truly under-resourced languages. approach creating synthetic
training data statistical machine translation low resource languages fits well
spirit synthetic treebanking.
2.6 Truly Under-Resourced Languages?
point, outlined underlying concepts major approaches
cross-lingual dependency parsing today. also discussed intricacies enabling
cross-lingual parser evaluation. Here, proceed discuss two outlooksnamely,
way implement cross-lingual parsers, way evaluate parsing
accuracyreflect dependency parsing truly under-resourced languages.
makes language under-resourced? Following Uszkoreit Rehm (2012),
acknowledge many facets involved attempting address question. Generally,
however, under-resourced language distinguished lacking basic NLP-enabling
linguistic resources, PoS-tagged corpora treebanks. paper, take dependency parsing-oriented viewpoint, allows casting issue under-resourcedness
specific terms dependency parsing enablement given language. Thus, language
under-resourced cannot build dependency parser or, otherwise said,
dependency treebank exists language. Since statistical dependency parsing critically
depends availability PoS tagging, make additional requirement,
turn implies following three levels resource availability. Note list
parsing-oriented specialization general discussion low-resource languages
introduction.
1. PoS-tagged corpus treebank available given language,
virtue those, hands PoS tagger dependency parser
language. call languages well-resourced resource-rich languages
dependency parsing viewpoint, use dedicated native language resources
parse texts written language.
2. given language, PoS tagging parsing resources available.
includes annotated corpora NLP tools. address languages
under-resourced low-resource languages, cannot natively parse
syntactic dependencies, neither annotate PoS tags.
3. PoS-tagged corpus PoS tagger available given language,
treebanks parsers exist it. Even NLP support languages
223

fiTiedemann & Agic

PoS annotation, still approach under-resourced viewpoint
dependency parsing.
want parse languages group 2 syntactic dependencies, must address
issuesthe unavailability supporting resources PoS tagging dependency
parsingand often even basic processing facilities sentence splitters tokenizers.
NLP, often call languages truly under-resourced. Group 3 somewhat easier,
presumably address dependency-syntactic processing layer.
recent years, field dealt extensivelyand large, separatelywith
providing low-resource languages PoS taggers dependency parsers. Taking two
examples account, Das Petrov (2011) show bootstrap accurate taggers
using parallel corpora, Agic et al. (2015) take under-resourcedness extreme
presuming severe data sparsity still manage yield reasonable PoS taggers
large number low-resource languages. thus safe conclude even
severely under-resourced languages, reasonable PoS taggers made available using one
techniques, already available off-the-shelf.
reasoning underlies current approaches cross-lingual dependency parsing,
presume availability PoS annotations, natively publicly available
related research. Since also required least intrinsically evaluate resulting
parsers, conduct empirical assessments exclusive group languages least
syntactically annotated test data available. effect, evaluating proxy,
truly under-resourced languages enjoy even basic test set availability. top
that, various top-performing approaches cross-lingual parsingsuch previously
discussed annotation projection, treebank translation, word representation-supported
model transferintroduce additional constraints requirements. often, presume
availability large source-target parallel corpora. One might argue accordingly
make poor case low-resource languages amassing prerequisites methods
work, thus departing definition low-resource language. turn,
favor current approaches, argue following.
current research enabling PoS tagging under-resourced languages justifies
separate handling cross-lingual dependency parsing presuming availability
PoS tagging. refer reader work Tackstrom et al. (2013a)
detailed exposition state-of-the-art results, together previously mentioned
work bootstrapping taggers.
McDonald et al. (2013) validate evaluation proxy showing uniform
syntactic representation partially enables inferential reasoning performance
ported parsers truly under-resourced languages. Namely, show typological
similarity plays important role predicting quality transferred parsers.
built by, example, Rosa Zabokrtsky (2015), use data-driven
language similarity metric actually predict best sources given targets
cross-lingual parsing.
remaining prerequisites top-level cross-lingual parsing, treebank
translation approach argue paper, amount source-target parallel
224

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

corpora possibly also monolingual target corpora. may first seem
substantial added requirement, note text corpora readily
available expert-annotated linguistic resources, collections OPUS
(Tiedemann, 2012) provide large quantities cross-domain data many languages.
claim, Agic et al. (2015) illustrate annotation projection could
applied learn PoS taggers hundreds, possibly even thousands languages using
nothing translations (parts of) Bible simple setup.
concluding, duly note perceived disconnect evaluating cross-lingual
parsers actually enabling dependency parsing languages lack respective
resources. argue former constitutes empirical research, latter
primarily engineering feat, thus obliged follow field adhering
former contribution. However, note devising multiple systematic
downstream evaluation scenarios truly under-resourced languages sorely needed
point fields development, would resolve important disconnect cross-lingual
NLP research.
proceed discuss core paper: empirical validation synthetic
treebanking approach cross-lingual parsing. reflect prerequisites
truly under-resourced languages related work discussion follows exposition
synthetic treebanking.

3. Synthetic Treebanking Experiments
section, discuss series experiments systematically explore various
cross-lingual parsing models based annotation projection treebank translation. Here,
assess properties specific approach, compare intrinsically
baseline. provide comparison selected recent work section 4.
setup, always use test sets provided Universal Dependency Treebank
version 1 (UDT) (McDonald et al., 2013) cross-lingually harmonized annotation
makes possible perform fair evaluations across languages including labeled attachment scores (LAS), use primary evaluation metric. Similar previous
literature, include punctuation calculation LAS ensure comparability
related literature (Tiedemann, 2014). experiments, apply mate-tools (Bohnet,
2010) train graph-based dependency parsers, gives us competitive performance
settings. leave Korean experiments due fact
bitexts domain languages, need annotation
projection SMT training. Thus, experiment using five languages: English (en),
French (fr), German (de), Spanish (es), Swedish (sv).
3.1 Baseline
initial baseline delexicalized model straightforward train provided
training data UDT. Table 1 lists attachment scores achieved applying
models across languages. scores confirm results McDonald et al. (2013); minor
differences due different choices training algorithms. Note always
use columns represent target languages test rows refer source languages
225

fiTiedemann & Agic

used training, projection translation. also always report scores sourcetarget pairs, reporting averages highest per-target scores might arguably make
biased insight methods.
target language
LAS
de
en
es
fr
sv

de
70.84
48.60
47.16
46.77
52.53

en
45.28
82.44
47.31
47.94
48.24

es
48.90
56.25
71.45
62.66
52.95

fr
49.09
58.47
62.39
73.71
55.02

sv
52.24
59.42
54.63
54.89
74.55

mate-tools (coarse)
mate-tools (full)

78.38
80.34

91.46
92.11

82.30
83.65

82.30
82.17

84.52
85.97

Table 1: Results delexicalized models. comparison also LASs
lexicalized models bottom table. coarse uses coarse-grained PoS labels
full adds even fine-grained PoS information.

see, results around 10 LAS points fully lexicalized models
significant drops observed training languages even though
quite closely related. unexpected considering naive approach using
coarse-grained PoS label sequences without modification type information
training models. note, however, decrease accuracy drastic
typologically closest language pair (French-Spanish). following section,
discuss various ways adapting cross-lingual models target language,
start annotation projection aligned parallel corpora.
3.2 Improved Annotation Projection
Annotation projection used connection word-aligned bilingual parallel corpora
(bitexts). experiments, use Europarl (Koehn, 2005) language pair
following basic setup Tiedemann (2014). baseline model applies DCA
projection heuristics presented Hwa et al. (2005) first 40,000 sentences
bitext corpus (repetitions sentences included). Word alignments produced
using IBM model 4 implemented GIZA++ (Och & Ney, 2003) trained typical
pipeline common statistical machine translation using Moses toolbox (Koehn
et al., 2007). use entire Europarl corpus version 7 train alignment models
obtain proper statistics reliable parameter estimates. asymmetric alignments
symmetrized intersection grow-diag-final-and heuristics. results
baseline projection model given Table 2.
value word-aligned bitext clearly seen performance crosslingual parser models. outperform naive delexicalized models large margin.
However, still pretty far away supervised monolingual models even
related language pairs. Tiedemann (2015) discusses various improvements
projection algorithm significant effects performance trained models. One
226

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

de
en
es
fr
sv

de

62.28
60.46
61.27
62.96

en
53.27

49.34
53.46
51.07

es
57.69
62.29

66.51
61.82

fr
60.49
65.54
68.10

64.99

sv
65.25
66.97
64.67
62.75


Table 2: Baseline performance LAS DCA-based annotation projection 40,000
parallel sentences tested target language test sets.

problem DCA algorithm creation DUMMY nodes labels disturb
training procedures. Many nodes easily removed without loosing much
information. Figure 8 illustrates approach deletes DUMMY leaf nodes collapses
dependency relations run via internal DUMMY nodes single out-going edges.
Adding modification DCA projection heuristics achieve significant
improvements various language pairs. Table 3 summarizes LASs models
new treatment DUMMY nodes.
Tiedemann (2015) also introduces new procedure treating one-to-many word
alignments. original algorithm, cause additional DUMMY nodes act
parents aligned target language tokens. new approach takes advantage
different alignment symmetrization algorithms uses high-precision links coming
intersection asymmetric word alignments find head multi-word unit,
whereas links high-recall symmetrization used attach words head
word. Figure 9 illustrates procedure means sentence pair Europarl.
Finally, Tiedemann (2015) also proposes discard trees remaining DUMMY
nodes. may remove 90% training examples assuming availability
large bitexts makes possible project additional sentences fill training data.
Discarding projected trees DUMMY nodes effectively removes sentence pairs
non-literal translations complex alignment structures case less suited
label 2

label 2
label 1

label 1

label 3

src1

src2 src3

src4

pos1

pos2

pos4

pos3

label 2

label 3

label 1

src1

src2 src3

src4

pos1

pos2

pos3

pos4

dummy

pos2

pos4

pos3

DUMMY trg1

trg2

trg3

DUMMY


trg1

trg2

trg3

label 3

src1

src2 src3

src4

pos1

pos2

pos3

pos4

pos1

pos2

pos4

trg1

trg2

trg3


pos1

dummy

label 2

dummy

label 3

label 1

label 2

label 1

Figure 8: Removing DUMMY nodes projected parse trees: (i) Delete DUMMY leaf
nodes. (ii) Collapse unary productions DUMMY nodes.

227

fiTiedemann & Agic

de
de
en
es
fr
sv

**

62.97+0.69
59.880.58
61.59+0.32
62.160.80

en
53.54+0.27
48.850.49
53.120.34
51.31+0.24

es
60.17+2.48
**
63.80+1.51

fr
62.35+1.86
**
66.47+0.93
68.55+0.45

**

*

**

67.00+0.49
62.58+0.76

sv
66.99+1.74
67.19+0.22
**
65.33+0.66
**
64.52+1.77
**

65.38+0.39

Table 3: Results collapsing dependency relations unary dummy nodes removing
dummy leaves (difference annotation projection baseline superscript).
Improvements marked ** statistically significant according McNemars
test p < 0.01 improvements marked * statistically significant
p < 0.05.

root

p

dobj
det
amod

nsubj

adpmod

adpobj

PRON VERB DET ADJ
NOUN
Wir wollen eine echte Wettbewerbskultur


want
true
PRON VERB DET ADJ
nsubj

amod
det
dobj

culture
NOUN

ADP


NOUN
Europa

.
.


competition Europe .
DUMMY DUMMY ADP NOUN .
adpobj
DUMMY
DUMMY
adpmod

root
p

Figure 9: Projecting German English using alternative treatment one-tomany word alignments. Dotted lines links grow-diag-final-and
symmetrization heuristics solid lines refer links intersection word
alignments.

annotation projection. Table 4 summarizes results method tested setup.
observe significant improvements language pairs compared baseline
approach two cases also better results previous setting shown
Table 3.
228

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

de
de
en
es
fr
sv

en
53.80+0.53

****

50.10+0.76
53.88+0.42
****
52.36+1.29

****

**
***

63.52+1.24
60.65+0.19
****
62.49+1.22
****
63.83+0.87

es
61.34+3.65
***
63.18+0.89

****
***

***

68.15+1.64
63.29+1.47

fr
62.32+1.83
**
67.04+1.50
*
68.81+0.71
**

**

sv
68.20+2.95
****
67.74+0.77
***
65.79+1.12
**
64.83+2.08

****

66.12+1.13

Table 4: Discarding trees include DUMMY nodes; results 40,000 accepted trees.
Results marked ** * significantly better projection baseline
(with p < 0.01 p < 0.05, respectively) results marked **** ***
also significantly better ones Table 3 (with p < 0.01 p < 0.05,
respectively).

3.3 Phrase-Based Treebank Translation
Treebank translation interesting alternative annotation projection. main
advantage skip noisy source-side annotation out-of-domain bitext
able project information source target language. Furthermore, word alignment
tightly coupled statistical translation models makes straightforward
use links projection. Finally, advantage projection machine
translation prefers literal translations similar syntactic structures. Unrestricted human
translations much varied proper alignment translation equivalents
necessarily straightforward. machine translation, mapping tokens
token n-grams essential favors successful annotation projection. largest
drawback is, course, translation quality. Machine translation difficult task
use annotation projection requires least level quality even though
necessarily interested semantically adequate translations.
first approach applies model proposed Tiedemann et al. (2014), using
standard phrase-based SMT model translate source language treebanks target
language. projection based DCA heuristics similar ones applied
annotation projection described previous section. also apply modification
DUMMY node handling introduced before. However, cannot apply alternative
treatment one-to-many alignments different types word alignment
translation model. also filter trees remaining DUMMY nodes
would cause serious reduction already small-sized treebanks. contrast
projection bitexts cannot add data fill training data.
experiments, MT setup generic uses Moses toolbox
training, tuning decoding (Koehn et al., 2007). translation models trained
entire Europarl corpus version 7 without language-pair-specific optimization. Word
alignments essentially used experiments annotation
projection section 3.2. tuning use MERT (Och, 2003) newstest2011 data
provided annual workshop statistical machine translation (WMT).1 Swedish
1. http://www.statmt.org/wmt14.

229

fiTiedemann & Agic

use sample OpenSubtitles2012 corpus (Tiedemann, 2012). language
model standard 5-gram model based combination Europarl News data
provided source. apply modified Kneser-Ney smoothing without pruning,
applying KenLM tools (Heafield, Pouzyrevsky, Clark, & Koehn, 2013) estimating
LM parameters.
de
de
en
es
fr
sv

en
56.24+2.70

**
50.65+1.80
**
55.69+2.57
**
53.01+1.70

**
**

59.413.56
53.945.94
**
57.054.54
**
58.573.59

**

**

**

es
57.652.52
63.760.04

68.66+1.66
62.69+0.11

fr
59.063.29
**
67.99+1.52
**
69.70+1.15
**

sv
64.622.37
67.52+0.33
**
62.732.60
**
62.771.75
**

64.760.62

Table 5: Results phrase-based treebank translation (difference corresponding annotation projection model DUMMY node removal Table 3 superscript).
Results marked ** significantly different projection results (with
p < 0.01).

results experiments phrase-based SMT summarized Table 5.
large extent, confirm findings Tiedemann (2014) translation approach
advantages projection automatically annotated parallel corpora.
language pairs, labeled attachment scores significantly projection
results even though parsers trained much smaller data sets (the treebanks
typically much smaller 40,000 sentences language pairs). striking also
outcome German target language, seems hardest language
translate data set. surprising German general considered
difficult target language setup languages are, example, supported
WMT. also applies use German source language surprising exception
translating English. Overall, good results English may influenced
strong impact language model draw large monolingual resources.
3.4 Syntax-Based Treebank Translation
Tiedemann (2015) introduces use syntax-based SMT another alternative treebank
translation. standard syntax-based MT models supported Moses based
synchronous phrase-structure grammars induced word-aligned parallel data.
Several modes available. case, mostly interested tree-to-string
models use synchronous tree substitution grammars (STSGs). assumption
structural relations induced parallel corpus fixed given
source-side analysis improve projection syntactic relations used combination
syntax-based translation.
order make possible use dependency information framework synchronous STSGs convert projective dependency trees bracketing structure
used train tree-to-string models Moses. use yield word
230

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

PRON VERB PRON . PRON ADP DET
NOUN
PRT VERB
Ich
bitte
Sie , sich
zu einer Schweigeminute zu erheben
nsubj

dobj

aux

det
adpobj

p

adpmod
dobj

root

xcomp

ROOT
nsubj

VERB

dobj

p

PRON

bitte

PRON

.

Sie

,

Ich

xcomp

adpobj

PRON ADP
sich

aux

adpmod

dobj

zu

dt

NOUN

VERB

PRT erheben
zu

DET Schweigeminute
einer

Figure 10: dependency tree taken automatically annotated parallel data
lossy conversion constituency representation.

define span sentence forms constituent label taken
relation word head.
Dependency trees certainly optimal kind constituency-based SMT
model usually flat provide deep hierarchical structures
common phrase-structure trees. However, previous research shown
valuable syntactic information pushed model way
beneficial projecting dependency relations. Note use PoS tags additional
pre-terminal nodes enrich information given system.
training models used data sets word alignments
used phrase-based SMT. However, require number additional steps listed below:

tag source side parallel corpus PoS tagger trained UDT
training data using HunPos (Halacsy, Kornai, & Oravecz, 2007).
231

fiTiedemann & Agic

parse tagged corpus using MaltParser model trained UDT
feature model optimized MaltOptimizer (Ballesteros & Nivre, 2012).2
projectivize trees using MaltParser convert nested tree annotations
explained (Tiedemann, 2015).
extract synchronous rule tables word aligned bitext source side
syntax score rules using Good Turing discounting. use size limit
replacing sub-phrases non-terminals source side restrict number
non-terminals right-hand side extracted rules three. Furthermore,
allow consecutive non-terminals source side increase coverage,
allowed default settings hierarchical rule extractor Moses.
tune model using MERT data sets before.
Finally, convert training data UDT source language translate
target language using tree-to-string model created above.
results approach listed Table 6. see syntax-based models
superior phrase-based models almost cases. majority language
pairs also see improvement annotation projection approach even though
training data much smaller. confirms findings Tiedemann (2015)
outperforms results large margin due parsing model used experiments.
de
de
en
es
fr
sv

**

62.670.30
57.132.75
**
61.410.18
**
61.730.43

en
58.60+5.06

**

**

es
61.00+0.83
**
64.58+0.78
**

**

52.65+3.80
56.83+3.71
**
52.13+0.82

**



68.97+1.97
62.340.24

fr
63.45+1.10

68.45+1.98

69.37+0.82

**



sv
67.88+0.89
**
68.16+0.97
**
63.551.78

62.561.96

**

64.500.88

Table 6: Results syntax-based treebank translation (difference corresponding
annotation projection model Table 5 superscript). Numbers bold face
better corresponding phrase-based SMT model. Results marked
** significantly different phrase-based translation results (p < 0.01);
significantly different projection model (p < 0.01 p < 0.05,
respectively).

3.5 Translation Back-Projection
Another possibility cross-lingual parsing integration translation actual
parsing pipeline. basic idea use tools languages, dependency
parsers, without modification adjusting input match expectations tool,
2. use MaltParser efficiency reasons. parsing performance slightly baseline
models trained mate-tools parsing fast require parsing bitexts.

232

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

label 2
label 1

pos1

src1

label 3

pos2

pos3

src2 src3

(2) parse

lexicalized

parser

pos4

src4
(3) project

(1) translate
trg1

trg2

trg3

trg4

pos2

pos1

pos3

pos4

label 1

label 3

label 2

Figure 11: Translation back-projection: Input data translated source language
existing parsers (step 1), parsed source language (step 2) and, finally,
parse tree projected back original target language.

example, translating language parser accepts. much
spirit text normalization approaches frequently used NLP historical
documents user-generated content input modified way
existing tools standard language applied. Figure 11 illustrates approach
applied dependency parsing.
advantage approach rely optimized parsers trained
manually corrected treebanks. However, several significant drawbacks. First
all, loose efficiency due additional translation step required parsing
time. crucial disadvantage rules approach many applications
require parsed information large scale data sets real-time responses. Another
important drawback noise coming translation leading kind input,
parser usually trained and, therefore, hard time handle correctly.
Finally, also problem back-projection. Unfortunately, straightforward
reverse projection heuristics discussed earlier. cannot introduce DUMMY nodes
fill gaps required projecting entire structure DUMMY labels
useful either. projection heuristics discussed section 3.2 help avoid DUMMY nodes
and, therefore, apply extensions experiments. Another problem related
unaligned target words. DCA algorithm (including modified versions discussed
far), tokens simply deleted attached dependency tree all.
method, however, possible back-projection tokens need
attached. reason, implement new rule attaches unaligned token
either preceding consecutive word attached tree themselves.
case simply attach ROOT. Another problem label
added dependency due lack knowledge set
label DUMMY. way, get credit LAS may least improve
UASs. test approach using syntax-based SMT translation model.
results listed table 7.
233

fiTiedemann & Agic

de
en
es
fr
sv

de

44.8617.42
36.6923.77
37.4423.83
36.8426.12

en
35.9217.35

41.917.43
42.0011.46
35.2315.84

es
32.9024.79
48.0814.21

55.5410.97
31.9629.86

fr
36.6823.81
48.1917.35
54.7813.32

33.7431.25

sv
45.5619.69
51.7415.23
43.2321.44
42.3920.36


Table 7: Back-projection results comparison annotation projection baseline
section 3.2 (Table 3).

scores low, even fall behind baseline delexicalized
models. extreme drop performance actually bit surprising considering
strong disadvantages discussed may expected well. Another reason
extreme differences performance also fact need rely predicted PoS
labels translated data piping source language parser.
certainly strong disadvantage procedure comparison evaluations based
gold standard PoS annotation entirely fair. See also section 3.8 discussions
impact PoS label accuracy parsing performance.
3.6 Annotation Projection Translation Quality
interesting question whether correlation translation quality
performance cross-lingual parsers based translated treebanks. approximation
treebank translation quality computed BLEU scores well-established MT test
sets WMT shared task, case newstest 2012.3
Figure 12 illustrates correlation BLEU scores obtained newstest data
LASs corresponding cross-lingual parsers. First all, see
MT performance phrase-based syntax-based models quite comparable
noticeable exceptions syntax-based SMT significantly better (French-English
French-Spanish, rather surprising). However, looking language pairs
see increased parsing performance seem due improvements
translation rather due better fit models syntactic annotation projection
(see German, example). Nevertheless, observe weak correlation BLEU
scores LAS within class models one notable outlier, Spanish-English.
correlation reflects importance syntactic relation languages success
machine translation annotation projection. Closely related languages like French
Spanish top level tasks whereas French Spanish map well
German. Translations English exception evaluation. Translation models
often work well direction whereas annotation projection English underperforms
experiments.
3. Note leave Swedish test test set available language.

234

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

labeled attachment score (LAS)

70
68

RPB-SMT = 0.463

66

Rsyntax-SMT = 0.340

fr-es
en-fr
en-fr

64

es-fr
es-fr

fr-es

en-es
en-es

62

en-de

60

fr-de

de-fr
de-es

58

fr-de

en-de
es-de

56

de-fr

de-en de-es

fr-en

de-en fr-en

54

es-de
es-en
es-en

52
10

15

20

25

30

35

BLEU

Figure 12: Correlation BLEU scores cross-lingual parsing accuracy (using
Pearsons correlation coefficient).

3.7 System Combination Multi-Source Models
far, interested transferring syntactic information one source language
target language using one specific model cross-lingual parsing. However,
approaches easily combined focus creation synthetic
training data. least two possibilities explored.
1. combine data several source languages increase amount training
data obtain evidence various languages projected target language.
2. Several models combined benefit various strengths model
may work complementary information.
paper, opt simple approach test ideas. concatenate
data sets augment training data train standard parsing models usual. First,
look multi-source models within paradigm. Table 8 lists labeled attachment
scores obtain combining data sets source languages train target
language parsers projected annotations.
table, see able achieve significant improvements
languages models except Spanish. Furthermore, English French
obtain overall best result presented paper combined syntax-based SMT
projections. final system combination, merge data sets languages
models. results parsers trained combined data sets shown
Table 9.
4. results multi-source multi-model system combinations provided Tiedemann (2015).

235

fiTiedemann & Agic

LAS
best published result4
best individual model

de
60.94
63.83

en
56.58
58.60

es
68.45
68.97

fr
69.15
69.70

sv
68.95
68.20

annotation projection
phrase-based SMT
syntax-based SMT

66.76
61.85
65.89

55.30
60.94
61.56

67.37
68.08
68.60

69.48
71.54
72.78

71.95
71.69
72.14

Table 8: Results combining projected data source languages train target language
parsing models. Numbers italics worse one models trained
data individual language pairs.

LAS
UAS
ACC

de
67.60
75.27
81.99

en
57.05
64.54
72.75

es
69.36
76.85
82.22

fr
72.03
79.21
83.06

sv
73.40
81.28
83.04

Table 9: Results combining projected data source languages train target language
parsing models. Additionally LAS also includes unlabeled attachment scores
(UAS) label accuracy (ACC) make easier compare results
related work.

German, French Swedish yields yet another significant improvement
labeled attachment scores close 70% even above. results represent highest
scores reported task far outperform previously published scores
large margin. expect sophisticated system combinations would push
results even further.
3.8 Gold vs. Predicted PoS Labels
common evaluate results gold PoS labels given test set
target language treebank. disregard impact PoS qualityoften present
related workmakes unrealistic evaluation scenario. previous section,
discussed results use gold standard annotation order make possible compare
results baselines related work. section, look details
replacing PoS labels predicted values. Here, report results
treebank translation approach using syntax-based SMT test case. approaches
show similar trends.
first experiment looks case annotated data available target
language training PoS taggers. use HunPos (Halacsy et al., 2007) train models
training data language use replace gold standard tags test
sets PoS labels models predict. results experiments applied
translated treebanks section 3.4 shown Table 10.
236

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

de
en
es
fr
sv
PoS tagger

de

58.703.97
53.373.76
57.184.23
57.634.10

en
56.492.11

50.891.76
54.941.89
50.171.96

es
57.523.48
61.253.33

64.324.65
59.362.98

fr
59.993.46
64.324.13
65.244.13

60.893.61

sv
62.685.20
63.974.19
58.784.77
58.224.34


95.24

97.56

95.37

95.08

95.86

Table 10: Results cross-lingual parsing predicted PoS labels coming taggers
trained target language treebanks. numbers superscript give
difference result gold standard labels (Table 6). last row shows
overall accuracy PoS tagger.

see PoS labels strong impact parsing performance. language
pairs, observe significant drop LAS even quite accurate taggers,
proves one need careful applying models real-life scenarios. next
experiment stresses point even more. Here, replace PoS labels tags
predicted taggers trained noisy translated treebanks projected
annotation. Note need remove training examples DUMMY labels reduce
errors tagger.

de
en
es
fr
sv

de

85.33
82.39
83.76
84.79

en
es
fr
sv
81.32 81.23 82.41 84.29

84.41 85.56 86.32
81.05

89.37 83.26
80.64 89.95

84.11
81.66 86.05 84.81


Table 11: PoS tagging accuracy models trained translated treebanks.

Table 11 lists accuracy taggers trained noisy projected data.
observe significant drop tagger performance completely plausible considering
substantial noise added translation projection also considering
limited size data use training. Treebanks considerably smaller
annotated corpora usually taken training PoS classifiers. applying
taggers test sets observe dramatic drop parsing performance expected.
Table 12 lists results experiments.
findings conclude cross-lingual techniques still require lot
improvement become practically useful low-resource scenarios real world.
done experiment annotation projection approach observed
behavior even though rely larger data sets training taggers.
performance drop using predicted PoS labels trained noisy data sets amounts
237

fiTiedemann & Agic

de
en
es
fr
sv

de

51.896.81
44.598.78
49.727.46
47.949.69

en
46.0410.45

47.813.08
49.045.90
44.235.94

es
48.618.91
59.371.88

61.303.02
55.024.34

fr
50.369.63
62.371.95
59.815.43

52.798.10

sv
52.739.95
60.433.54
52.126.66
51.107.12


Table 12: Results cross-lingual parsing predicted PoS labels coming taggers
trained projected treebanks. difference results predicted labels
Table 10 shown superscript.

10 LAS points cases similar see treebank translation approach.
omit results add new information discussion.
Finally, also need check whether system combinations multi-source models
help improve quality cross-lingual parsers predicted PoS labels. this,
use strategy section 3.7 concatenate various data files train
parser models combine models language pairs. words, use
models trained section 3.7 evaluate test sets automatically tagged
PoS labels. Again, use two settings: 1) apply PoS taggers trained manually
verified data setsthe monolingual target language treebanks, 2) use PoS taggers
trained projected translated treebanks. latter data sets
disposal and, therefore, expect better PoS model well. Table 13 lists final
results comparison ones obtained gold standard annotation.

PoS
PoS
PoS
PoS

de
78.38
70.84
52.53
67.60

en
91.46
82.44
48.24
61.56

es
82.30
71.45
62.66
69.36

fr
82.30
73.71
62.39
72.78

sv
84.52
74.55
59.42
73.40

monolingual PoS tagger accuracy
combined projected PoS tagger accuracy

95.24
88.47

97.56
88.24

95.37
88.06

95.08
89.83

95.86
88.07

monolingual baseline predicted PoS
delexicalized monolingual predicted PoS
best delexicalized cross-lingual predicted PoS
combined cross-lingual predicted PoS
combined cross-lingual projected PoS model

73.03
64.25
48.36
63.14
57.84

88.38
72.81
43.87
55.16
51.66

76.59
60.49
52.94
64.99
61.40

76.79
64.06
52.47
67.91
63.86

77.83
65.77
49.84
67.93
61.58

monolingual baseline
delexicalized monolingual
best delexicalized cross-lingual
best cross-lingual model






gold
gold
gold
gold

Table 13: comparison models evaluated gold standard PoS annotation (four
top-level systems) models tested automatically tagged data.

First all, see best cross-lingual models outperform delexicalized
cross-lingual models large margin. come close delexicalized models trained
238

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

target language data exception English works much better
original data set. lower part table, observe scores drop significantly
gold standard PoS labels replaced predicted tags. Note four systems
using predicted PoS labels apply tagger trained monolingual verified target language
data gives quite high accuracy. final system table one
applies PoS model trained projected translated data. tagger models
much less accurate, shown middle Table 13, influence degradation
visible attachment scores obtained systems. However, models reflect
real-world scenario annotated available target language, even
training PoS taggers. advantage projection translation approaches
model possible all, whereas delexicalized transfer models always require
existing tools produce shared features used prediction system. Note also
cross-lingual models outperform delexicalized models trained
verified target language datawith English striking exceptionwhich remarkable
given noisy data trained on.

3.9 Impact Dataset Sizes
large, data-driven dependency parsers benefit introducing additional
training data. subsection, control amount training data provided
method, observe impact LAS cross-lingually. experiment improved
annotation projection (see Section 3.2), introduce 60 thousand sentences
projected dependency trees. five target UDT languages experiment,
provide four learning curves representing four source languages. plot results
Figure 13.
observe virtually transferred parsers benefit introduction additional
training data, albeit improvements slight models level
around 20 thousand sentences. source languages follow LAS learning
curve patterns targets, observe trend violations specific
source-target pairs. that, observe clear source-target preferences,
source orderings LAS mostly remain training set sizes.
lower-ranked sources benefit even degrade introducing training data,
example, Spanish parser induced German data, English parser created
projecting Swedish trees. said, worth noting best source-target
pairs, targets always benefit introducing source data: German English
Swedish, English German French, Spanish French vice versa,
Swedish German English. clear indicator future improvements,
method apparently benefits adding data. time, learning
curves show benefits truly under-resourced languages, largest relative gains
already reached relatively modest quantities 20 thousand sentence pairs. Moreover,
typological groupings former list top-performing source-target pairs quite
apparent, case throughout experiments.
239

fien
es
fr
sv
0

LAS (projected Spanish)

LAS (projected English)

63
62
61
60
59
58
57
56
55
54
53

10k 20k 30k 40k 50k
nr projected sentences

68
66
64
62
60
de
en
fr
sv

58
56
54

10k 20k 30k 40k 50k
nr projected sentences
LAS (projected Swedish)

0

54
53
52
51
50
49
48
47
46
45
44

60k

de
es
fr
sv
0

LAS (projected French)

LAS (projected German)

Tiedemann & Agic

60k

60k

70
68
66
64
62
de
en
es
sv

60
58
56
0

68
67
66
65
64
63
62
61
60
59

10k 20k 30k 40k 50k
nr projected sentences

10k 20k 30k 40k 50k
nr projected sentences

60k

de
en
es
fr
0

10k 20k 30k 40k 50k
nr projected sentences

60k

Figure 13: impact training data: Different sizes projected data training crosslingual parsing models.

4. Comparison Related Work
sectionhaving thoroughly analyzed synthetic treebankingwe revert top-level
discussion cross-lingual parsing. it, contrast approach several selected
alternatives related work, sketch properties viewpoint enabling
dependency parsing truly under-resourced languages. proceed outlining
comparison.
already compared various synthetic treebanking approaches one another
delexicalized transfer baseline McDonald et al. (2013) section 3. Here,
aim introducing number top-performing representatives methods discussed
240

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

overview section: competitive model transfer approach, approach dealing
distributed word representations, annotation projection-motivated approach.
replicating approaches would time-consuming, constrain search
approaches also report scores UDT version 1 respective publication,
compare referencing. select following approaches discussion.
Delex: delexicalized model transfer baseline McDonald et al. (2013).
report scores Sgaard et al. (2015) used arc-factored adaptation
mate-tools parser, replication original, conveniently
report multiple metrics. discuss metrics below, note used
gold PoS.
Multi: reimplementation McDonald et al. (2011) multi-source projected system
(multi-proj. original paper) Xia (2014). provide
competitive baseline system. original work predates UDT evaluates
heterogenous CoNLL treebanks, Xia (2014) evaluate UDT
treebanks report scores. Note parsing model preprocessing
inherent setup, differing original setup McDonald et al.
(2011). setup details described text, Xia.
Proj: improved annotation projection approach described section 3.2.
final approach subsection, dependency relations
unary dummy nodes collapsed, dummy leaves removed, Europarl trees
remaining dummy nodes discarded (see Table 4). scores given gold
PoS tags.
Trans G & P: report best syntax-based cross-lingual treebank translation
scores gold predicted PoS, respectively. PoS predictions come
HMM tagger (Halacsy et al., 2007). taggers trained target language
treebanks, score 95% average (see Table 10).
Comb G & P: multi-source syntax-based cross-lingual parsers.
build Trans G & P approaches: instead single sources, multiple
treebanks translated target languages, providing combined synthetic
treebanks train parsers on. before, also report scores gold HunPospredicted PoS.
Rosa: multi-source delexicalized transfer approach Rosa Zabokrtsky
(2015), weighted variant. method, target parsed multiple
sources, parse assigned weight based empirically established
language similarity metric. target sentence, multiple parses constitute
digraph, top (Sagae & Lavie, 2006)-style maximum spanning tree voting
scheme implemented. use gold PoS tags.
Sgaard: model, delexicalized model transfer augmented inter-lingual
word representations based inverted indexing via Wikipedia concept links (Sgaard
et al., 2015). choose recent illustrative example leveraging word
241

fiTiedemann & Agic

Target
language
de
en
es
fr
sv

Baselines
Delex Multi
56.80

63.21
66.00
67.49

69.21

72.57
74.60
75.87

Proj
72.65
62.79
74.92
76.13
76.96

Synthetic treebanking
TransG TransP CombG
70.62
65.10
75.71
76.33
76.98

67.59
63.62
72.16
72.95
73.61

75.27
64.54
76.85
79.21
81.28

CombP
71.79
63.15
73.20
76.06
76.83

Recent approaches
Rosa Sgaard Xia
56.80
42.60
72.70

50.80

56.56

64.03
66.22
67.32

74.01

75.60
76.93
79.27

Table 14: Comparison cross-lingual parsing methods. contrast rest paper,
report UAS scores attain maximum coverage results reported
related work.

embeddings improving cross-lingual dependency parsing. use embeddingsenabled version Bohnets parser (Bohnet, 2010) gold PoS tags. report
multi-source results.
Xia: approach Xia (2014) novel method leverages Europarl
train probabilistic parsing models resource-poor languages maximizing
combination likelihood parallel data confidence unlabeled data.
report best approach (marked +U paper), makes use
parallel unlabeled data. use top-performing PoS taggers trained
target languages, reaching least 95% accuracy.
discussing results, make number remarks comparison. First,
target language, report best obtained score method, rather possibly
misleading averages complex source-target matrices. related work, English
used target language. Second, contrast remainder paperand
contrary guidelines evaluating cross-lingual parsers following McDonald et al.
(2013)we report UAS only. targeted exclusively facilitating comparison
related work, contributions part still report UAS scores, even
working UDT. see unfortunate, also note LAS-enabled
replication study exceeds scope match focus contribution. Third,
also related able control experiment parameters, note
issue reporting scores gold predicted PoS, different ways obtaining
predicted annotations. record differences list above. Finally, note
referenced contributions explicitly state whether scoring included
punctuation not, whereas include experiments.
results given Table 14 proceed discuss detail,
reflecting methods intricacies requirements process.
table, visually group methods baselines (Delex, Multi),
proposed approaches (Proj, Trans, Comb), selected recent contributions crosslingual dependency parsing (Rosa, Sgaard, Xia). design, highlight
best scores, results directly comparable, especially respect lack
control sources features facilitating parsing, PoS tags. also note
Rosa evaluated HamleDT treebanks (Rosa, Masek, Marecek, Popel, Zeman,
242

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

& Zabokrtsky, 2014) UDT, still provide reference, implements
interesting addition Delex sort intermediate step towards Multi.
first observe Rosa Sgaard rarely surpass Delex baseline.
come surprise, baseline uses advanced graph-based dependency
parser (Bohnet, 2010): contrast, Rosa uses arc-factored parser (McDonald, Pereira,
Ribarov, & Hajic, 2005), Sgaard implements first-order version parser
Bohnet (2010) leverages cross-lingual word representations. said, discrepancy
first- second-order graph-based parsers appears factor
explaining slight (if any) gains provided two approaches. Namely, Rosa
approach multi-source delexicalized parsing based maximum spanning tree-style
voting, uses empirically obtained dataset similarity metrics weighting arcs
voting schemes. such, even yields slight improvements respective fair
baselinesas provided paper describing approach (Rosa & Zabokrtsky, 2015)it
still bound impoverished feature representation informing parser, inherited
Delex builds on, preventing method reaching higher accuracies. Sgaard
attempts alleviate introducing cross-lingual word representations feature
space. report approach, Sgaard et al. (2015) observe slight improvements
baselines, apparent word representations utilize work much
better NLP tasks dont involve syntactic representations, indicating might
appropriate facilitating cross-lingual parsing substantially.
considered Rosa Sgaardcomparing two approaches Delex
baseline, establishing inferiority remaining approaches, including synthetic
trebankingwe turn interesting part discussion, contributions
compared one another, Xia. also include competitive Multi baseline
McDonald et al. (2011) discussion.
improved annotation projection Proj appears competitive method,
none approaches surpass large margin. also consistently beats
Multi, albeit PoS annotations comparable. Syntax-based treebank translation
(Trans) surpasses narrow margin four five targets, German
exception, multi-source variant (Comb) adds approximately 3-5 LAS points
difference, English exception. approaches using predicted PoS tags
contrasted Xia, noting datasets, tagging approach (HunPos)
performs slightly (Stanford) average. observe Xia exhibits slight
advantage top approach (CombP) across targets, also noteon top
differences taggersthat approach also utilizes unlabeled data semi-supervised
parser augmentation. said, Xia (2014) document minor decreases
removing unlabeled sources, implement arc-factored dependency parser
pipeline. Thus, note i) synthetic treebanking approaches Xia currently
represent competitive approaches cross-lingual dependency parsing,
slight empirical edge latter, ii) research neededin form
extensive replicative survey cross-lingual parsingto empirically gauge various
intricacies two approaches, influential contributions field,
work McDonald et al. (2011) Xiao Guo (2014). also note recent
contribution Rasooli Collins (2015), also deals parallel corpora
projections, showing promising results.
243

fiTiedemann & Agic

point, viewpoint enabling processing truly under-resourced
languages, interesting mark following observation. Table 14, apparent
disconnect scores methods exploit parallel data sources (Multi, Proj,
Trans, Comb, Xia), methods dont (Delex, Rosa, Sgaard): methods
make use parallel resources perform significantly better. clear
indicator reaching top-level cross-lingual parsing performance, least
current line-up standard dependency parsers, need lexical features provided
parallel corpora. observation appears us clear guideline future work
cross-lingual parsing, enablement NLP under-resourced languages.

5. Conclusions Future Work
paper discussed various approaches cross-lingual dependency parsing,
reviewing comparing number commonly used methods. Furthermore, included
extensive study annotation projection treebank translation, presented
competitive results cross-lingual dependency parsing task parsing data
cross-lingually harmonized annotation included Universal Dependency Treebank.
future work includes incorporation cross-lingual word embeddings model
transfer another component system combinations discuss paper.
also look wider range languages using growing set harmonized data sets
Universal Dependencies project. Especially interesting use techniques truly
under-resourced languages. explore cross-lingual parsing means bootstrapping
tools languages. also aim implementing large-scale replicative survey
cross-lingual dependency parsing, show contribution empirical
assessment would timely beneficial fast-developing field.

Acknowledgements
thank four anonymous reviewers detailed comments, significantly
contributed improving quality publication. also acknowledge Joakim Nivre
discussions synthetic treebanking, Hector Martnez Alonso suggestions
improving readability paper.

References
Abeille, A. (2003). Treebanks: Building Using Parsed Corpora. Springer.
Agic, Z., Merkler, D., & Berovic, D. (2012). Slovene-Croatian Treebank Transfer Using
Bilingual Lexicon Improves Croatian Dependency Parsing. Proceedings IS-LTC,
pp. 59.
Agic, Z., Hovy, D., & Sgaard, A. (2015). Bit Bible: Learning
POS Taggers Truly Low-resource Languages. Proceedings ACL, pp. 268272.
Agic, Z., Tiedemann, J., Merkler, D., Krek, S., Dobrovoljc, K., & Moze, S. (2014). Crosslingual Dependency Parsing Related Languages Rich Morphosyntactic Tagsets.
Proceedings LT4CloseLang, pp. 1324.
244

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

Ballesteros, M., & Nivre, J. (2012). MaltOptimizer: Optimization Tool MaltParser.
Proceedings EACL, pp. 5862.
Bender, E. M. (2011). Achieving Evaluating Language-independence NLP.
Linguistic Issues Language Technology, 6 (3), 126.
Bender, E. M. (2013). Linguistic Fundamentals Natural Language Processing: 100
Essentials Morphology Syntax. Morgan & Claypool Publishers.
Bohmova, A., Hajic, J., Hajicova, E., & Hladka, B. (2003). Prague Dependency
Treebank. Treebanks, pp. 103127. Springer.
Bohnet, B. (2010). Top Accuracy Fast Dependency Parsing Contradiction.
Proceedings COLING, pp. 8997.
Buchholz, S., & Marsi, E. (2006). CoNLL-X Shared Task Multilingual Dependency
Parsing. Proceedings CoNLL, pp. 149164.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).
Natural language processing (almost) scratch. Journal Machine Learning
Research, 12, 24932537.
Das, D., & Petrov, S. (2011). Unsupervised Part-of-Speech Tagging Bilingual GraphBased Projections. Proceedings ACL, pp. 600609.
De Marneffe, M.-C., MacCartney, B., & Manning, C. D. (2006). Generating Typed Dependency Parses Phrase Structure Parses. Proceedings LREC, pp. 449454.
Durrett, G., Pauls, A., & Klein, D. (2012). Syntactic Transfer Using Bilingual Lexicon.
Proceedings EMNLP-CoNLL, pp. 111.
Elming, J., Johannsen, A., Klerke, S., Lapponi, E., Martinez Alonso, H., & Sgaard, A.
(2013). Down-stream Effects Tree-to-dependency Conversions. Proceedings
NAACL, pp. 617626.
Faruqui, M., & Dyer, C. (2014). Improving Vector Space Word Representations Using
Multilingual Correlation. Proceedings EACL, pp. 462471.
Garrette, D., Mielens, J., & Baldridge, J. (2013). Real-World Semi-Supervised Learning
POS-Taggers Low-Resource Languages.. Proceedings ACL, pp. 583592.
Gouws, S., & Sgaard, A. (2015). Simple Task-specific Bilingual Word Embeddings.
Proceedings NAACL.
Halacsy, P., Kornai, A., & Oravecz, C. (2007). HunPos Open-source Trigram Tagger.
Proceedings ACL, pp. 209212.
Heafield, K., Pouzyrevsky, I., Clark, J. H., & Koehn, P. (2013). Scalable Modified Kneser-Ney
Language Model Estimation. Proceedings ACL, pp. 690696.
Hwa, R., Resnik, P., Weinberg, A., Cabezas, C., & Kolak, O. (2005). Bootstrapping Parsers
via Syntactic Projection across Parallel Texts. Natural Language Engineering, 11 (3),
311325.
Klementiev, A., Titov, I., & Bhattarai, B. (2012). Inducing Crosslingual Distributed
Representations Words. Proceedings COLING, pp. 14591474.
245

fiTiedemann & Agic

Koehn, P. (2005). Europarl: Parallel Corpus Statistical Machine Translation.
Proceedings MT Summit, pp. 7986.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B.,
Shen, W., Moran, C., Zens, R., Dyer, C. J., Bojar, O., Constantin, A., & Herbst, E.
(2007). Moses: Open Source Toolkit Statistical Machine Translation. Proceedings
ACL, pp. 177180.
Koo, T., Carreras, X., & Collins, M. (2008). Simple Semi-supervised Dependency Parsing.
Proceedings ACL, pp. 595603.
Kubler, S., McDonald, R., & Nivre, J. (2009). Dependency Parsing. Morgan & Claypool
Publishers.
Li, S., Graca, J. V., & Taskar, B. (2012). Wiki-ly Supervised Part-of-speech Tagging.
Proceedings EMNLP-CoNLL, pp. 13891398.
Ma, X., & Xia, F. (2014). Unsupervised Dependency Parsing Transferring Distribution
via Parallel Guidance Entropy Regularization. Proceedings ACL), pp. 1337
1348.
McDonald, R., Nivre, J., Quirmbach-Brundage, Y., Goldberg, Y., Das, D., Ganchev, K.,
Hall, K., Petrov, S., Zhang, H., Tackstrom, O., Bedini, C., Bertomeu Castello, N.,
& Lee, J. (2013). Universal Dependency Annotation Multilingual Parsing.
Proceedings ACL, pp. 9297.
McDonald, R., Pereira, F., Ribarov, K., & Hajic, J. (2005). Non-Projective Dependency
Parsing using Spanning Tree Algorithms. Proceedings EMNLP, pp. 523530.
McDonald, R., Petrov, S., & Hall, K. (2011). Multi-Source Transfer Delexicalized
Dependency Parsers. Proceedings EMNLP, pp. 6272.
Mikolov, T., Le, Q. V., & Sutskever, I. (2013). Exploiting Similarities among Languages
Machine Translation. http://arxiv.org/pdf/1309.4168.pdf.
Naseem, T., Barzilay, R., & Globerson, A. (2012). Selective Sharing Multilingual
Dependency Parsing. Proceedings ACL, pp. 629637.
Nivre, J. (2006). Inductive dependency parsing. Springer.
Nivre, J., Bosco, C., Choi, J., de Marneffe, M.-C., Dozat, T., Farkas, R., Foster, J., & Ginter,
F. e. a. (2015). Universal dependencies 1.0..
Nivre, J., Hall, J., Kubler, S., McDonald, R., Nilsson, J., Riedel, S., & Yuret, D. (2007).
CoNLL 2007 Shared Task Dependency Parsing. Proceedings CoNLL
Shared Task Session EMNLP-CoNLL 2007, pp. 915932.
Och, F. J. (2003). Minimum Error Rate Training Statistical Machine Translation.
Proceedings ACL, pp. 160167.
Och, F. J., & Ney, H. (2003). Systematic Comparison Various Statistical Alignment
Models. Computational Linguistics, 29 (1), 1952.
Petrov, S. (2014). Towards Universal Syntactic Processing Natural Language. Proceedings LT4CloseLang, p. 66.
246

fiSynthetic Treebanking Cross-Lingual Dependency Parsing

Petrov, S., Das, D., & McDonald, R. (2012). Universal Part-of-Speech Tagset.
Proceedings LREC, pp. 20892096.
Plank, B., Martnez Alonso, H., Agic, v., Merkler, D., & Sgaard, A. (2015). Dependency
Parsing Metrics Correlate Human Judgments?. Proceedings CONLL, pp.
315320.
Rasooli, M. S., & Collins, M. (2015). Density-Driven Cross-Lingual Transfer Dependency
Parsers. Proceedings EMNLP.
Rosa, R., Masek, J., Marecek, D., Popel, M., Zeman, D., & Zabokrtsky, Z. (2014). HamleDT
2.0: Thirty Dependency Treebanks Stanfordized. Proceedings LREC, pp. 2334
2341.
Rosa, R., & Zabokrtsky, Z. (2015). KLcpos3 - Language Similarity Measure Delexicalized
Parser Transfer. Proceedings ACL, pp. 243249.
Sagae, K., & Lavie, A. (2006). Parser Combination Reparsing. Proceedings NAACL,
pp. 129132.
Sgaard, A. (2011). Data Point Selection Cross-language Adaptation Dependency
Parsers. Proceedings ACL, pp. 682686.
Sgaard, A. (2012). Unsupervised Dependency Parsing Without Training. Natural Language
Engineering, 18 (02), 187203.
Sgaard, A. (2013). Semi-Supervised Learning Domain Adaptation Natural Language
Processing. Morgan & Claypool Publishers.
Sgaard, A., Agic, Z., Martnez Alonso, H., Plank, B., Bohnet, B., & Johannsen, A. (2015).
Inverted Indexing Cross-lingual NLP. Proceedings ACL, pp. 17131722.
Tackstrom, O., Das, D., Petrov, S., McDonald, R., & Nivre, J. (2013a). Token Type
Constraints Cross-lingual Part-of-speech Tagging. Transactions Association
Computational Linguistics, 1, 112.
Tackstrom, O., McDonald, R., & Nivre, J. (2013b). Target Language Adaptation
Discriminative Transfer Parsers. Proceedings NAACL, pp. 10611071.
Tackstrom, O., McDonald, R., & Uszkoreit, J. (2012). Cross-lingual Word Clusters
Direct Transfer Linguistic Structure. Proceedings NAACL, pp. 477487.
Tiedemann, J. (2014). Rediscovering Annotation Projection Cross-Lingual Parser
Induction. Proceedings COLING, pp. 18541864.
Tiedemann, J., Agic, Z., & Nivre, J. (2014). Treebank Translation Cross-Lingual Parser
Induction. Proceedings CoNLL, pp. 130140.
Tiedemann, J., & Nakov, P. (2013). Analyzing use character-level translation
sparse noisy datasets. Proceedings RANLP, pp. 676684.
Tiedemann, J. (2012). Parallel Data, Tools Interfaces OPUS. Proceedings
LREC, pp. 22142218.
Tiedemann, J. (2015). Improving Cross-Lingual Projection Syntactic Dependencies.
Proceedings NoDaLiDa.
247

fiTiedemann & Agic

Uszkoreit, H., & Rehm, G. (2012). Language White Paper Series. Springer.
Xiao, M., & Guo, Y. (2014). Distributed Word Representation Learning Cross-Lingual
Dependency Parsing. Proceedings CoNLL, pp. 119129.
Yarowsky, D., Ngai, G., & Wicentowski, R. (2001). Inducing Multilingual Text Analysis
Tools via Robust Projection Across Aligned Corpora. Proceedings HLT, pp. 18.
Zeman, D., Dusek, O., Marecek, D., Popel, M., Ramasamy, L., Stepanek, J., Zabokrtsky,
Z., & Hajic, J. (2014). HamleDT: Harmonized Multi-language Dependency Treebank.
Language Resources Evaluation, 48 (4), 601637.
Zeman, D., & Resnik, P. (2008). Cross-Language Parser Adaptation Related
Languages. Proceedings IJCNLP, pp. 3542.
Zhang, Y., Reichart, R., Barzilay, R., & Globerson, A. (2012). Learning Map
Universal POS Tagset. Proceedings EMNLP-CoNLL, pp. 13681378.
Zhao, H., Song, Y., Kit, C., & Zhou, G. (2009). Cross Language Dependency Parsing Using
Bilingual Lexicon. Proceedings ACL-IJCNLP, pp. 5563.
Zou, W. Y., Socher, R., Cer, D., & Manning, C. D. (2013). Bilingual Word Embeddings
Phrase-Based Machine Translation. Proceedings EMNLP, pp. 13931398.

248

fiJournal Artificial Intelligence Research 55 (2016) 409-442

Submitted 07/15; published 02/16

Automatic Description Generation Images: Survey
Models, Datasets, Evaluation Measures
Raffaella Bernardi

bernardi@disi.unitn.it

University Trento, Italy

Ruket Cakici

ruken@ceng.metu.edu.tr

Middle East Technical University, Turkey

Desmond Elliott

d.elliott@uva.nl

University Amsterdam, Netherlands

Aykut Erdem
Erkut Erdem
Nazli Ikizler-Cinbis

aykut@cs.hacettepe.edu.tr
erkut@cs.hacettepe.edu.tr
nazli@cs.hacettepe.edu.tr

Hacettepe University, Turkey

Frank Keller

keller@inf.ed.ac.uk

University Edinburgh, UK

Adrian Muscat

adrian.muscat@um.edu.mt

University Malta, Malta

Barbara Plank

bplank@cst.dk

University Copenhagen, Denmark

Abstract
Automatic description generation natural images challenging problem
recently received large amount interest computer vision natural language processing communities. survey, classify existing approaches based
conceptualize problem, viz., models cast description either generation problem retrieval problem visual multimodal representational
space. provide detailed review existing models, highlighting advantages
disadvantages. Moreover, give overview benchmark image datasets
evaluation measures developed assess quality machine-generated
image descriptions. Finally extrapolate future directions area automatic image
description generation.

1. Introduction
past two decades, fields natural language processing (NLP) computer
vision (CV) seen great advances respective goals analyzing generating
text, understanding images videos. fields share similar set methods rooted artificial intelligence machine learning, historically developed
separately, scientific communities typically interacted little.
Recent years, however, seen upsurge interest problems require
combination linguistic visual information. lot everyday tasks nature,
e.g., interpreting photo context newspaper article, following instructions
conjunction diagram map, understanding slides listening lecture.
c
2016
AI Access Foundation. rights reserved.

fiBernardi et al.

addition this, web provides vast amount data combines linguistic visual
information: tagged photographs, illustrations newspaper articles, videos subtitles,
multimodal feeds social media. tackle combined language vision tasks
exploit large amounts multimodal data, CV NLP communities moved
closer together, example organizing workshops language vision
held regularly CV NLP conferences past years.
new language-vision community, automatic image description emerged
key task. task involves taking image, analyzing visual content, generating
textual description (typically sentence) verbalizes salient aspects
image. challenging CV point view, description could principle
talk visual aspect image: mention objects attributes,
talk features scene (e.g., indoor/outdoor), verbalize people
objects scene interact. challenging still, description could even refer
objects depicted (e.g., talk people waiting train, even
train visible arrived yet) provide background knowledge
cannot derived directly image (e.g., person depicted Mona Lisa).
short, good image description requires full image understanding, therefore
description task excellent test bed computer vision systems, one much
comprehensive standard CV evaluations typically test, instance, accuracy
object detectors scene classifiers limited set classes.
Image understanding necessary, sufficient producing good description.
Imagine apply array state-of-the-art detectors image localize objects
(e.g., Felzenszwalb, Girshick, McAllester, & Ramanan, 2010; Girshick, Donahue, Darrell,
& Malik, 2014), determine attributes (e.g., Lampert, Nickisch, & Harmeling, 2009; Berg,
Berg, & Shih, 2010; Parikh & Grauman, 2011), compute scene properties (e.g., Oliva &
Torralba, 2001; Lazebnik, Schmid, & Ponce, 2006), recognize human-object interactions (e.g., Prest, Schmid, & Ferrari, 2012; Yao & Fei-Fei, 2010). result would
long, unstructured list labels (detector outputs), would unusable image
description. good image description, contrast, comprehensive concise
(talk important things image), formally correct,
i.e., consists grammatically well-formed sentences.
NLP point view, generating description natural language generation (NLG) problem. task NLG turn non-linguistic representation
human-readable text. Classically, non-linguistic representation logical form,
database query, set numbers. image description, input image representation (e.g., detector outputs listed previous paragraph), NLG
model turn sentences. Generating text involves series steps, traditionally
referred NLP pipeline (Reiter & Dale, 2006): need decide aspects
input talk (content selection), need organize content (text
planning) verbalize (surface realization). Surface realization turn requires choosing right words (lexicalization), using pronouns appropriate (referential expression
generation), grouping related information together (aggregation).
words, automatic image description requires full image understanding,
also sophisticated natural language generation. makes interesting
410

fiAutomatic Description Generation Images: Survey

task embraced CV NLP communities.1 Note
description task become even challenging take account
good descriptions often user-specific. instance, art critic require different
description librarian journalist, even photograph. briefly
touch upon issue talk difference descriptions captions
Section 3 discuss future directions Section 4.
Given automatic image description interesting task, driven
existence mature CV NLP methods availability relevant datasets, large
image description literature appeared last five years. aim survey
article give comprehensive overview literature, covering models, datasets,
evaluation metrics.
sort existing literature three categories based image description
models used. first group models follows classical pipeline outlined above:
first detect predict image content terms objects, attributes, scene types,
actions, based set visual features. Then, models use content information
drive natural language generation system outputs image description.
term approaches direct generation models.
second group models cast problem retrieval problem. is, create
description novel image, models search images database similar
novel image. build description novel image based descriptions
set similar images retrieved. novel image described simply
reusing description similar retrieved image (transfer), synthesizing
novel description based description set similar images. Retrieval-based models
subdivided based type approach use represent images
compute similarity. first subgroup models uses visual space retrieve images,
second subgroup uses multimodal space represents images text jointly.
overview models reviewed survey, category
fall into, see Table 1.
Generating natural language descriptions videos presents unique challenges
image-based description, additionally requires analyzing objects
attributes actions temporal dimension. Models aim solve description generation videos proposed literature (e.g., Khan, Zhang, &
Gotoh, 2011; Guadarrama, Krishnamoorthy, Malkarnenkar, Venugopalan, Mooney, Darrell,
& Saenko, 2013; Krishnamoorthy, Malkarnenkar, Mooney, Saenko, & Guadarrama, 2013;
Rohrbach, Qiu, Titov, Thater, Pinkal, & Schiele, 2013; Thomason, Venugopalan, Guadarrama, Saenko, & Mooney, 2014; Rohrbach, Rohrback, Tandon, & Schiele, 2015; Yao, Torabi,
Cho, Ballas, Pal, Larochelle, & Courville, 2015; Zhu, Kiros, Zemel, Salakhutdinov, Urtasun,
Torralba, & Fidler, 2015). However, existing work description generation used
static images, focus survey.2
survey article, first group automatic image description models three
categories outlined provide comprehensive overview models
1. Though image description approaches circumvent NLG aspect transferring human-authored
descriptions, see Sections 2.2 2.3.
2. interesting intermediate approach involves annotation image streams sequences sentences, see work Park Kim (2015).

411

fiBernardi et al.

category Section 2. examine available multimodal image datasets used
training testing description generation models Section 3. Furthermore, review
evaluation measures used gauge quality generated descriptions
Section 3. Finally, Section 4, discuss future research directions, including possible
new tasks related image description, visual question answering.

2. Image Description Models
Generating automatic descriptions images requires understanding humans
describe images. image description analyzed several different dimensions (Shatford, 1986; Jaimes & Chang, 2000). follow Hodosh, Young, Hockenmaier (2013)
assume descriptions interest survey article ones
verbalize visual conceptual information depicted image, i.e., descriptions
refer depicted entities, attributes relations, actions
involved in. Outside scope automatic image description non-visual descriptions,
give background information refer objects depicted image (e.g.,
location image taken took picture). Also, relevant
standard approaches image description perceptual descriptions, capture
global low-level visual characteristics images (e.g., dominant color image
type media photograph, drawing, animation, etc.).
following subsections, give comprehensive overview state-of-the-art approaches description generation. Table 1 offers high-level summary field, using
three categories models outlined introduction: direct generation models, retrieval models visual space, retrieval model multimodal space.
2.1 Description Generation Visual Input
general approach studies group first predict likely meaning
given image analyzing visual content, generate sentence reflecting
meaning. models category achieve using following general pipeline
architecture:
1. Computer vision techniques applied classify scene type, detect objects present image, predict attributes relationships hold
them, recognize actions taking place.
2. followed generation phase turns detector outputs words
phrases. combined produce natural language description
image, using techniques natural language generation (e.g., templates, n-grams,
grammar rules).
approaches reviewed section perform explicit mapping images
descriptions, differentiates studies described Section 2.2 2.3,
incorporate implicit vision language models. illustration sample model
shown Figure 1. explicit pipeline architecture, tailored problem hand,
constrains generated descriptions, relies predefined sets semantic classes
scenes, objects, attributes, actions. Moreover, architecture crucially assumes
412

fiAutomatic Description Generation Images: Survey

Reference

Generation

Farhadi et al. (2010)
Kulkarni et al. (2011)
Li et al. (2011)
Ordonez et al. (2011)
Yang et al. (2011)
Gupta et al. (2012)
Kuznetsova et al. (2012)
Mitchell et al. (2012)
Elliott Keller (2013)
Hodosh et al. (2013)
Gong et al. (2014)
Karpathy et al. (2014)
Kuznetsova et al. (2014)
Mason Charniak (2014)
Patterson et al. (2014)
Socher et al. (2014)
Verma Jawahar (2014)
Yatskar et al. (2014)
Chen Zitnick (2015)
Donahue et al. (2015)
Devlin et al. (2015)
Elliott de Vries (2015)
Fang et al. (2015)
Jia et al. (2015)
Karpathy Fei-Fei (2015)
Kiros et al. (2015)
Lebret et al. (2015)
Lin et al. (2015)
Mao et al. (2015a)
Ortiz et al. (2015)
Pinheiro et al. (2015)
Ushiku et al. (2015)
Vinyals et al. (2015)
Xu et al. (2015)
Yagcioglu et al. (2015)

Retrieval
Visual Space Multimodal Space
X

X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X

X
X
X

X
X
X
X
X
X
X
X
X

X
X
X
X
X
X
X
X
X

X
X
X

Table 1: overview existing approaches automatic image description.
categorized literature approaches directly generate description image
(Section 2.1), approaches retrieve images via visual similarity transfer description new image (Section 2.2), approaches frame task retrieving
descriptions images multimodal space (Section 2.3).

413

fiBernardi et al.

Figure 1: automatic image description generation system proposed Kulkarni et al.
(2011).
accuracy detectors semantic class, assumption always met
practice.
Approaches description generation differ along two main dimensions: (a) image
representations derive descriptions from, (b) address sentence generation problem. terms representations used, existing models conceptualized
images number different ways, relying spatial relationships (Farhadi et al., 2010),
corpus-based relationships (Yang et al., 2011), spatial visual attributes (Kulkarni
et al., 2011). Another group papers utilizes abstract image representation
form meaning tuples capture different aspects image: objects detected,
attributes detections, spatial relations them, scene type
(Farhadi et al., 2010; Yang et al., 2011; Kulkarni et al., 2011; Li et al., 2011; Mitchell et al.,
2012). recently, Yatskar et al. (2014) proposed generate descriptions denselylabeled images, incorporate object, attribute, action, scene annotations. Similar
spirit work Fang et al. (2015), rely prior labeling objects,
attributes, etc. Rather, authors train word detectors directly images
associated descriptions using multi-instance learning (a weakly supervised approach
training object detectors). words returned detectors fed
language model sentence generation, followed re-ranking step.
first framework explicitly represent structure image relates
structure description Visual Dependency Representations (VDR) method
proposed Elliott Keller (2013). VDR captures spatial relations
objects image form dependency graph. graph related
syntactic dependency tree description image.3 initial work using VDRs
relied corpus manually annotated VDRs training, recent approaches
induce VDRs automatically based output object detector (Elliott & de Vries,
2015) labels present abstract scenes (Ortiz et al., 2015).4 idea explicitly
representing image structure using description generation picked
3. VDRs proven useful description generation, also image retrieval (Elliott,
Lavrenko, & Keller, 2014).
4. Abstract scenes schematic images, typically constructed using clip-art. employed avoid
need object detector, labels positions objects know. example Zitnick
Parikhs (2013) dataset, see Section 3 details.

414

fiAutomatic Description Generation Images: Survey

Lin et al. (2015), parse images scene graphs, similar VDRs
represent relations objects scene. generate scene
graphs using semantic grammar.5
Existing approaches also vary along second dimension, viz., approach
sentence generation problem. one end scale, approaches use
n-gram-based language models. Examples include works Kulkarni et al. (2011)
Li et al. (2011), generate descriptions using n-gram language models trained
subset Wikipedia. approaches first determine attributes relationships
regions image regionprepositionregion triples. n-gram language
model used compose image description fluent, given language model.
approach Fang et al. (2015) similar, uses maximum entropy language model
instead n-gram model generate descriptions. gives authors flexibility
handling output word detectors core model.
Recent image description work using recurrent neural networks (RNNs) also
regarded relying language modeling. classical RNN language model: captures
probability generating given word string, given words generated far.
image description setup, RNN trained generate next word given
string far, also set image features. setting, RNN therefore
purely language model (as case n-gram model, instance), hybrid
model relies representation incorporates visual linguistic features.
return detail Section 2.3.
second set approaches use sentence templates generate descriptions.
(typically manually) pre-defined sentence frames open slots need filled
labels objects, relations, attributes. instance, Yang et al. (2011) fill
sentence template selecting likely objects, verbs, prepositions, scene types
based Hidden Markov Model. Verbs generated finding likely pairing
object labels Gigaword external corpus. generation model Elliott
Keller (2013) parses image VDR, traverses VDRs fill slots
sentence templates. approach also performs limited content selection
learning associations VDRs syntactic dependency trees training time;
associations allow select appropriate verb description test time.
approaches used linguistically sophisticated approaches generation.
Mitchell et al. (2012) over-generate syntactically well-formed sentence fragments
recombine using tree-substitution grammar. related approach pursued
Kuznetsova et al. (2014), tree-fragments learnt training set existing
descriptions fragments combined test time form new descriptions.
Another linguistically expressive model recently proposed Ortiz et al. (2015).
authors model image description machine translation VDRsentence pairs
perform explicit content selection surface realization using integer linear program
linguistic constraints.
systems presented far aimed directly generating novel descriptions. However,
argued Hodosh et al. (2013), framing image description natural language generation (NLG) task makes difficult objectively evaluate quality novel descriptions
5. Note graphs also used image retrieval Johnson, Krishna, Stark, Li, Shamma, Bernstein,
Fei-Fei (2015) Schuster, Krishna, Chang, Fei-Fei, Manning (2015).

415

fiBernardi et al.

Figure 2: description model based retrieval visual space proposed Ordonez
et al. (2011).
introduces number linguistic difficulties detract attention underlying image understanding problem (Hodosh et al., 2013). time, evaluation
generation systems known difficult (Reiter & Belz, 2009). Hodosh et al. therefore propose approach makes possible evaluate mapping images
sentences independently generation aspect. Models follow approach
conceptualize image description retrieval problem: associate image
description retrieving ranking set similar images candidate descriptions.
candidate descriptions either used directly (description transfer)
novel description synthesized candidates (description generation).
retrieval images ranking descriptions carried two ways:
either visual space multimodal space combines textual visual
information space. following subsections, survey work follows two
approaches.
2.2 Description Retrieval Visual Space
studies group pose problem automatically generating description
image retrieving images similar query image (i.e., new image
described); illustrated Figure 2. words, systems exploit similarity
visual space transfer descriptions query images. Compared models
generate descriptions directly (Section 2.1), retrieval models typically require large amount
training data order provide relevant descriptions.
terms algorithmic components, visual retrieval approaches typically follow
pipeline three main steps:
1. Represent given query image specific visual features.
2. Retrieve candidate set images training set based similarity measure
feature space used.
3. Re-rank descriptions candidate images making use visual
and/or textual information contained retrieval set, alternatively combine
fragments candidate descriptions according certain rules schemes.
One first model follow approach Im2Text model Ordonez et al.
(2011). GIST (Oliva & Torralba, 2001) Tiny Image (Torralba, Fergus, & Freeman, 2008)
416

fiAutomatic Description Generation Images: Survey

descriptors employed represent query image determine visually similar
images first retrieval step. retrieval-based models consider result
step baseline. re-ranking step, range detectors (e.g., object, stuff,
pedestrian, action detectors) scene classifiers specific entities mentioned
candidate descriptions first applied images better capture visual content,
images represented means detector classifier responses. Finally,
re-ranking carried via classifier trained semantic features.
model proposed Kuznetsova et al. (2012) first runs detectors classifiers used re-ranking step Im2Text model query image extract
represent semantic content. Then, instead performing single retrieval combining
responses detectors classifiers Im2Text model does, carries
separate image retrieval step visual entity present query image collect related phrases retrieved descriptions. instance, dog detected given
image, retrieval process returns phrases referring visually similar dogs
training set. specifically, step used collect three different kinds phrases.
Noun verb phrases extracted descriptions training set based
visual similarity object regions detected training images query
image. Similarly, prepositional phrases collected stuff detection query
image measuring visual similarity detections query training
images based appearance geometric arrangements. Prepositional phrases
additionally collected scene context detection measuring global scene similarity computed query training images. Finally, description generated
collected phrases detected object via integer linear programming (ILP)
considers factors word ordering, redundancy, etc.
method Gupta et al. (2012) another phrase-based approach. retrieve
visually similar images, authors employ simple RGB HSV color histograms,
Gabor Haar descriptors, GIST SIFT (Lowe, 2004) descriptors image features. Then, instead using visual object detectors scene classifiers, rely
textual information descriptions visually similar images extract
visual content input image. Specifically, candidate descriptions segmented phrases certain type (subject, verb), (subject, prep, object),
(verb, prep, object), (attribute, object), etc. best describe input image determined according joint probability model based image similarity Google search counts, image represented triplets form
{((attribute1, object1), verb), (verb, prep, (attribute2, object2)), (object1, prep, object2)}.
end, description generated using three top-scoring triplets based fixed
template. increase quality descriptions, authors also apply syntactic
aggregation subject predicate grouping rules generation step.
Patterson et al. (2014) first present large-scale scene attribute dataset
computer vision community. dataset includes 14,340 images 707 scene
categories, annotated certain attributes list 102 discriminative
attributes related materials, surface properties, lighting, affordances, spatial layout.
allows train attribute classifiers dataset. paper, authors
also demonstrate responses attribute classifiers used global
image descriptor captures semantic content better standard global image
417

fiBernardi et al.

descriptors GIST. application, extended baseline model Im2Text
replacing global features automatically extracted scene attributes, giving better
image retrieval description results.
Mason Charniaks (2014) description generation approach differs models
discussed formulates description generation extractive summarization
problem, selects output description considering textual information
final re-ranking step. particular, authors represented images using scene
attributes descriptor Patterson et al. (2014). visually similar images identified training set, next step, conditional probabilities observing word
description query image estimated via non-parametric density estimation
using descriptions retrieved images. final output description determined using two different extractive summarization techniques, one depending
SumBasic model (Nenkova & Vanderwende, 2005) based Kullback-Leibler
divergence word distributions query candidate descriptions.
Yagcioglu et al. (2015) proposed average query expansion approach based
compositional distributed semantics. represent images, use features extracted
recently proposed Visual Geometry Group convolutional neural network (VGG-CNN;
Chatfield, Simonyan, Vedaldi, & Zisserman, 2014). features activations
last layer deep neural network trained ImageNet, proven
effective many computer vision problems. Then, original query expanded
average distributed representations retrieved descriptions, weighted
similarity input image.
approach Devlin et al. (2015) also utilizes CNN activations global image
descriptor performs k-nearest neighbor retrieval determine images
training set visually similar query image. selects description
candidate descriptions associated retrieved images best describes
images similar query image, like approaches Mason
Charniak (2014) Yagcioglu et al. (2015). approach differs terms
represent similarity description select best candidate
whole set. Specifically, propose compute description similarity based
n-gram overlap F-score descriptions. suggest choose output
description finding description corresponds description highest
mean n-gram overlap candidate descriptions (k-nearest neighbor centroid
description) estimated via n-gram similarity measure.
2.3 Description Retrieval Multimodal Space
third group studies casts image description generation retrieval problem,
multimodal space (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014).
intuition behind models illustrated Figure 3, overall approach
characterized follows:

1. Learn common multimodal space visual textual data using training
set imagedescription pairs.
418

fiAutomatic Description Generation Images: Survey

2. Given query, use joint representation space perform cross-modal (image
sentence) retrieval.

Figure 3: Image descriptions retrieval task proposed works Hodosh et al.
(2013), Socher et al. (2014), Karpathy et al. (2014)6 .
contrast retrieval models work visual space (Section 2.2),
unimodal image retrieval followed ranking retrieved descriptions, image
sentence features projected common multimodal space. Then, multimodal
space used retrieve descriptions given image. advantage approach
allows bi-directional models, i.e., common space also used
direction, retrieving appropriate image query sentence.
section, first discuss seminal paper Hodosh et al. (2013) description
retrieval, present recent approaches combine retrieval approach
form natural language generation. Hodosh et al. map images sentences
common space. joint space used image search (find
plausible image given sentence) image annotation (find sentence describes
image well), see Figure 3. earlier study authors proposed learn common meaning space (Farhadi et al., 2010) consisting triple representation form
hobject, action, scenei. representation thus limited set pre-defined discrete
slot fillers, given training information. Instead, Hodosh et al. use KCCA,
kernelized version CCA, Canonical Correlation Analysis (Hotelling, 1936), learn
joint space. CCA takes training dataset image-sentence pairs, i.e., Dtrain = {hi, si},
thus input two different feature spaces, finds linear projections newly induced common space. KCCA, kernel functions map original items higher-order
space order capture patterns needed associate image text. KCCA
shown previously successful associating images (Hardoon, Szedmak, & ShaweTaylor, 2004) image regions (Socher & Fei-Fei, 2010) individual words set
tags.
Hodosh et al. (2013) compare KCCA approach nearest-neighbor (NN) baseline
uses unimodal text image spaces, without constructing joint space. drawback
KCCA applicable smaller datasets, requires two kernel matrices
6. Source http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/

419

fiBernardi et al.

kept memory training. becomes prohibitive large datasets.
attempts made circumvent computational burden KCCA, e.g.,
resorting linear models (Hodosh & Hockenmaier, 2013). Alternatively, Sun, Gan,
Nevatia (2015) used automatically discovered concepts images form semantic
space, performed sentence retrieval accordingly. However, recent work description
retrieval instead utilized neural networks construct joint space image description
generation.
Socher et al. (2014) use neural networks building sentence image vector representations mapped common embedding space. novelty
work use compositional sentence vector representations. First, image word
representations learned single modalities, finally mapped common
multimodal space. particular, use DT-RNN (Dependency Tree Recursive Neural
Network) composing language vectors abstract word order syntactic difference semantically irrelevant. results 50-dimensional word embeddings.
image space, authors use nine layer neural network trained ImageNet data,
using unsupervised pre-training. Image embeddings derived taking output
last layer (4,096 dimensions). two spaces projected multi-modal space
max-margin objective function intuitively trains pairs correct image
sentence vectors high inner product. authors show model outperforms previously used KCCA approaches work Hodosh Hockenmaier
(2013).
Karpathy et al. (2014) extend previous multi-modal embeddings model. Rather
directly mapping entire images sentences common embedding space,
model embeds fine-grained units, i.e., fragments images (objects) sentences
(dependency tree fragments), common space. final model integrates global
(sentence image-level) well finer-grained information outperforms previous
approaches, DT-RNN (Socher et al., 2014). similar approach pursued
Pinheiro et al. (2015), propose bilinear phrase-based model learns mapping
image representations sentences. constrained language model used
generate representation. conceptually related approach pursued Ushiku
et al. (2015): authors use common subspace model maps feature vectors
associated phrase nearby regions space. generation, beamsearch based decoder templates used.
Description generation systems difficult evaluate, therefore studies reviewed
treat problem retrieval ranking task (Hodosh et al., 2013; Socher et al.,
2014). approach valuable enables comparative evaluation,
retrieval ranking limited availability existing datasets descriptions.
alleviate problem, recent models developed extensions multimodal
spaces; able rank sentences, also generate (Chen & Zitnick,
2015; Donahue et al., 2015; Karpathy & Fei-Fei, 2015; Kiros et al., 2015; Lebret et al., 2015;
Mao et al., 2015a; Vinyals et al., 2015; Xu et al., 2015).
Kiros et al. (2015) introduced general encoder-decoder framework image description
ranking generation, illustrated Figure 4. Intuitively method works follows.
encoder first constructs joint multimodal space. space used rank
images descriptions. second stage (decoder) uses shared multimodal
420

fiAutomatic Description Generation Images: Survey

Figure 4: encoder-decoder model proposed Kiros et al. (2015).

representation generate novel descriptions. model, directly inspired recent
work machine translation, encodes sentences using LongShort Term Memory (LSTM)
recurrent neural network, image features using deep convolutional network (CNN).
LSTM extension recurrent neural network (RNN) incorporates builtin memory store information exploit long range context. Kiros et al.s (2015)
encoder-decoder model, vision space projected embedding space LSTM
hidden states; pairwise ranking loss minimized learn ranking images
descriptions. decoder, neural-network-based language model, able generate novel
descriptions multimodal space.
Another work carried time similar latter
described paper Donahue et al. (2015). authors propose model also
based LSTM neural architecture. However, rather projecting vision space
embedding space hidden states, model takes copy static image
previous word directly input, fed stack four LSTMs. Another
LSTM-based model proposed Jia et al. (2015), added semantic image information
additional input LSTM. model Kiros et al. (2015) outperforms prior
DT-RNN model (Socher et al., 2014); turn, Donahue et al. report outperform
work Kiros et al. (2015) task image description retrieval. Subsequent work
includes RNN-based architectures Mao et al. (2015a) Vinyals et al. (2015),
similar one proposed Kiros et al. (2015) achieve comparable
results standard datasets. Mao, Wei, Yang, Wang, Huang, Yuille (2015b) propose
interesting extension Mao et al.s (2015a) model learning novel visual concepts.
Karpathy Fei-Fei (2015) improve previous models proposing deep visualsemantic alignment model simpler architecture objective function. key
insight assume parts sentence refer particular unknown regions
image. model tries infer alignments segments sentences regions
images based convolutional neural networks image regions, bidirectional
RNN sentences structured objective aligns two modalities. Words
image regions mapped common multimodal embedding. multimodal
recurrent neural network architecture uses inferred alignments learn generate
421

fiBernardi et al.

novel descriptions. Here, image used condition first state recurrent
neural network, generates image descriptions.
Another model generate novel sentences proposed (Chen & Zitnick, 2015).
contrast previous work, model dynamically builds visual representation
scene description generated. is, word read generated
visual representation updated reflect new information. accomplish
simple RNN. model achieves comparable better results prior studies,
except recently proposed deep visual-semantic alignment model (Karpathy & Fei-Fei,
2015). model Xu et al. (2015) closely related also uses RNN-based
architecture visual representations dynamically updated. Xu et al.s (2015)
model incorporates attentional component, gives way determining
regions image salient, focus description regions.
resulting improvement description accuracy, also makes possible analyze
model behavior visualizing regions attended word
generated model.
general RNN-based ranking generation approach also followed Lebret
et al. (2015). Here, main innovation linguistic side: employ bilinear
model learn common space image features syntactic phrases (noun phrases, verb
phrases, prepositional phrases). Markov model utilized generate sentences
phrase embedding. visual side, standard CNN-based features used.
results elegant modeling framework, whose performance broadly comparable
state art.
Finally, two important directions less explored are: portability weakly supervised learning. Verma Jawahar (2014) evaluate portability bi-directional
model based topic models, showing performance significantly degrades. highlight importance cross-dataset image description retrieval evaluation. Another interesting observation models require training set fully-annotated
image-sentence pairs. However, obtaining data large quantities prohibitively expensive. Gong et al. (2014) propose approach based weak supervision transfers
knowledge millions weakly annotated images improve accuracy description
retrieval.
2.4 Comparison Existing Approaches
discussion previous subsections makes clear approach image
description particular strengths weaknesses. example, methods
cast task generation problem (Section 2.1) advantage types
approaches produce novel sentences describe given image. However,
success relies heavily accurately estimate visual content
well able verbalize content. particular, explicitly employ computer
vision techniques predict likely meaning given image; methods
limited accuracy practice, hence fail identify important objects
attributes, valid description generated. Another difficulty lies
final description generation step; sophisticated natural language generation crucial
422

fiAutomatic Description Generation Images: Survey

guarantee fluency grammatical correctness generated sentences. come
price considerable algorithmic complexity.
contrast, image description methods cast problem retrieval
visual space problem transfer retrieved descriptions novel image (Section 2.2)
always produce grammatically correct descriptions. guaranteed design,
systems fetch human-generated sentences visually similar images. main issue
approach requires large amounts images human-written descriptions.
is, accuracy (but grammaticality) descriptions reduces size
training set decreases. training set also needs diverse (in addition
large), order visual retrieval-based approaches produce image descriptions
adequate novel test images (Devlin et al., 2015). Though problem mitigated
re-synthesizing novel description retrieved ones (see Section 2.2).
Approaches cast image description retrieval multimodal space problem
(Section 2.3) also advantage generating human-like descriptions
able retrieve appropriate ones pre-defined large pool descriptions.
However, ranking descriptions requires cross-modal similarity metric compares
images sentences. metrics difficult define, compared unimodal
image-to-image similarity metrics used retrieval models work visual space.
Additionally, training common space images sentences requires large training
set images annotated human-generated descriptions. plus side,
multimodal embedding space also used reverse problem, i.e., retrieving
appropriate image query sentence. something generation-based
visual retrieval-based approaches capable of.

3. Datasets Evaluation
wide range datasets automatic image description research. images
datasets associated textual descriptions differ certain
aspects size, format descriptions descriptions collected. review common approaches collecting datasets, datasets themselves,
evaluation measures comparing generated descriptions ground-truth texts.
datasets summarized Table 2, examples images descriptions given
Figure 5. readers also refer dataset survey Ferraro, Mostafazadeh, Huang,
Vanderwende, Devlin, Galley, Mitchell (2015) analysis similar ours. provides
basic comparison existing language vision datasets. limited
automatic image description, reports simple statistics quality metrics
perplexity, syntactic complexity, abstract concrete word ratios.
3.1 Image-Description Datasets
Pascal1K sentence dataset (Rashtchian et al., 2010) dataset commonly
used benchmark evaluating quality description generation systems.
medium-scale dataset, consists 1,000 images selected Pascal 2008
object recognition dataset (Everingham, Van Gool, Williams, Winn, & Zisserman, 2010)
includes objects different visual classes, humans, animals, vehicles.
423

fiBernardi et al.

Images

Texts

Judgments

Objects

Pascal1K (Rashtchian et al., 2010)
VLT2K (Elliott & Keller, 2013)
Flickr8K (Hodosh & Hockenmaier, 2013)
Flickr30K (Young et al., 2014)
Abstract Scenes (Zitnick & Parikh, 2013)
IAPR-TC12 (Grubinger et al., 2006)
MS COCO (Lin et al., 2014)

1,000
2,424
8,108
31,783
10,000
20,000
164,062

5
3
5
5
6
15
5


Partial
Yes



Collected

Partial
Partial


Complete
Segmented
Partial

BBC News (Feng & Lapata, 2008)
SBU1M Captions (Ordonez et al., 2011)
Deja-Image Captions (Chen et al., 2015)

3,361
1,000,000
4,000,000

1
1
Varies


Collected7






Table 2: Image datasets automatic description generation models. split
overview image description datasets (top) caption datasets (bottom) see
main text explanation distinction.
image associated five descriptions generated humans Amazon Mechanical
Turk (AMT) service.
Visual Linguistic Treebank (VLT2K; Elliott & Keller, 2013) makes use images
Pascal 2010 action recognition dataset. augments images three, twosentence descriptions per image. descriptions collected AMT specific
instructions verbalize main action depicted image actors involved (first
sentence), also mentioning important background objects (second sentence).
subset 341 images Visual Linguistic Treebank, object annotation
available (in form polygons around objects mentioned descriptions).
subset, manually created Visual Dependency Representations (see Section 2.1) also
included (three VDRs per images, i.e., total 1023).
Flickr8K dataset (Hodosh et al., 2013) extended version Flickr30K
dataset (Young et al., 2014) contain images Flickr, comprising approximately 8,000
30,000 images, respectively. images two datasets selected
user queries specific objects actions. datasets contain five descriptions per image collected AMT workers using strategy similar Pascal1K
dataset.
Abstract Scenes dataset (Zitnick & Parikh, 2013; Zitnick, Parikh, & Vanderwende,
2013) consists 10,000 clip-art images descriptions. images created
AMT, workers asked place fixed vocabulary 80 clip-art objects
scene choosing. descriptions sourced worker-created
scenes. authors provided descriptions two different forms. first
group contains single sentence description image, second group includes two
alternative descriptions per image. two descriptions consist three simple
sentences sentence describing different aspect scene. main advantage
dataset affords opportunity explore image description generation without
7. Kuznetsova et al. (2014) ran human judgments study 1,000 images dataset.

424

fiAutomatic Description Generation Images: Survey

1. One jet lands airport another takes
next it.
2. Two airplanes parked airport.
3. Two jets taxi past other.
4. Two parked jet airplanes facing opposite directions.
5. two passenger planes grassy plain

1. several people chairs small child
watching one play trumpet
2. man playing trumpet front little boy.
3. People sitting sofa man playing
instrument entertainment.

(a) Pascal1K8

(b) VLT2K9

1. man snowboarding structure snowy
hill.
2. snowboarder jumps air snowy
hill.
3. snowboarder wearing green pants trick
high bench
4. Someone yellow pants ramp
snow.
5. man performing trick snowboard high
air.

1. yellow building white columns background
2. two palm trees front house
3. cars parking front house
4. woman child walking square

(c) Flickr8K10

(d) IAPR-TC1211

1. cat anxiously sits park stares
unattended hot dog someone left
yellow bench

1. blue smart car parked parking lot.
2. vehicles wet wide city street.
3. Several cars motorcycle snow covered street.
4. Many vehicles drive icy street.
5. small smart car driving city.

(e) Abstract Scenes12

(f) MS COCO13

Figure 5: Example images descriptions benchmark image datasets.

425

fiBernardi et al.

need automatic object recognition, thus avoiding associated noise.
recent version dataset created part visual question-answering
(VQA) dataset (Antol, Agrawal, Lu, Mitchell, Batra, Zitnick, & Parikh, 2015). contains
50,000 different scene images realistic human models five single-sentence
descriptions.
IAPR-TC12 dataset introduced Grubinger et al. (2006) one earliest
multi-modal datasets contains 20,000 images descriptions. images originally retrieved via search engines Google, Bing Yahoo, descriptions
produced multiple languages (predominantly English German). image
associated one five descriptions, description refers different aspect
image, applicable. dataset also contains complete pixel-level segmentation
objects.
MS COCO dataset (Lin et al., 2014) currently consists 123,287 images five
different descriptions per image. Images dataset annotated 80 object categories, means bounding boxes around instances one categories
available images. MS COCO dataset widely used image description, something facilitated standard evaluation server recently
become available14 . Extensions MS COCO currently development, including
addition questions answers (Antol et al., 2015).
One paper (Lin et al., 2015) uses NYU dataset (Silberman, Kohli, Hoiem, &
Fergus, 2012), contains 1,449 indoor scenes 3D object segmentation.
dataset augmented five descriptions per image Lin et al.
3.2 Image-Caption Datasets
Image descriptions verbalize seen image, i.e., refer objects,
actions, attributes depicted, mention scene type, etc. Captions, hand,
typically texts associated images verbalize information cannot seen
image. caption provides personal, cultural, historical context image
(Panofsky, 1939). Images shared social networking photo-sharing websites
accompanied descriptions captions, mixtures types text. images
newspaper museum typically contain cultural historical texts, i.e., captions
descriptions.
BBC News dataset (Feng & Lapata, 2008) one earliest collections
images co-occurring texts. Feng Lapata (2008) harvested 3,361 news articles
British Broadcasting Corporation News website, constraint article
includes image caption.
8.
9.
10.
11.
12.

Source http://nlp.cs.illinois.edu/HockenmaierGroup/pascal-sentences/index.html
Source http://github.com/elliottd/vlt
Source https://illinois.edu/fb/sec/1713398
Source http://imageclef.org/photodata
Source http://research.microsoft.com/en-us/um/people/larryz/clipart/SemanticClassesRender
/Classes_v1.html
13. Source http://mscoco.org/explore
14. Source http://mscoco.org/dataset/#captions-eval

426

fiAutomatic Description Generation Images: Survey

SBU1M Captions dataset introduced Ordonez et al. (2011) differs
previous datasets web-scale dataset containing approximately one million
captioned images. compiled data available Flickr user-provided image
descriptions. images downloaded filtered Flickr constraint
image contained least one noun one verb predefined control lists. resulting
dataset provided CSV file URLs.
Deja-Image Captions dataset (Chen et al., 2015) contains 4,000,000 images
180,000 near-identical captions harvested Flickr. 760 million images downloaded
Flickr calendar year 2013 using set 693 nouns queries. image
captions normalized lemmatization stop word removal create corpus
near-identical texts. instance, sentences bird flies blue sky bird
flying blue sky normalized bird fly blue sky (Chen et al., 2015). Image
caption pairs retained captions repeated one user normalized
form.

3.3 Collecting Datasets
Collecting new imagetext datasets typically performed crowd-sourcing harvesting data web. images datasets either sourced
existing task computer vision community Pascal challenge (Everingham
et al., 2010) used Pascal1K VLT2K datasets directly Flickr,
case Flickr8K/30K, MS COCO, SBU1M Captions, Deja-Image Captions datasets,
crowdsourced, case Abstract Scenes dataset. texts imagedescription
datasets usually crowd-sourced Amazon Mechanical Turk Crowdflower; whereas
texts imagecaption datasets harvested photo-sharing sites,
Flickr, news providers. Captions usually collected without financial incentive
written people sharing images, journalists.
Crowd-sourcing descriptions images involves defining simple task
performed untrained workers. Examples task guidelines used Hodosh et al.
(2013) Elliott Keller (2013) given Figure 6. instances, care taken
clearly inform potential workers expectations task. particular,
explicit instructions given descriptions written, examples
good texts provided. addition, Hodosh et al. provided extensive examples
explain would constitute unsatisfactory texts. options available control
quality collected texts: minimum performance rate workers common
choice; pre-task selection quiz may used determine whether workers
sufficient grasp English language (Hodosh et al., 2013).
issue remuneration crowd-sourced workers controversial, higher payments always lead better quality crowd-sourced environment (Mason & Watts,
2009). Rashtchian et al. (2010) paid $0.01/description, Elliott Keller (2013) paid $0.04
average 67 seconds work produce two-sentence description. best
knowledge, information available datasets.
427

fiBernardi et al.

(a) Mechanical Turk Interface used collect Flickr8K dataset15 .

(b) Mechanical Turk Interface used collect VLT2K dataset.

Figure 6: Examples Mechanical Turk interfaces collecting descriptions.

3.4 Evaluation Measures
Evaluating output natural language generation (NLG) system fundamentally
difficult task (Dale & White, 2007; Reiter & Belz, 2009). common way assess
quality automatically generated texts subjective evaluation human experts.
15. Source Appendix work Hodosh et al. (2013)

428

fiAutomatic Description Generation Images: Survey

NLG-produced text typically judged terms grammar content, indicating
syntactically correct relevant text is, respectively. Fluency generated
text sometimes tested well, especially surface realization technique involved
generation process. Automatically generated descriptions images
evaluated using NLG techniques. Typically, judges provided image
well description evaluation tasks. Subjective human evaluations
machine generated image descriptions often performed Mechanical Turk help
questions. far, following Likert-scale questions used test datasets
user groups various sizes.
description accurately describes image (Kulkarni et al., 2011; Li et al., 2011;
Mitchell et al., 2012; Kuznetsova et al., 2012; Elliott & Keller, 2013; Hodosh et al.,
2013).
description grammatically correct (Yang et al., 2011; Mitchell et al., 2012;
Kuznetsova et al., 2012; Elliott & Keller, 2013, inter alia).
description incorrect information (Mitchell et al., 2012).
description relevant image (Li et al., 2011; Yang et al., 2011).
description creatively constructed (Li et al., 2011).
description human-like (Mitchell et al., 2012).
Another approach evaluating descriptions use automatic measures,
BLEU (Papineni, Roukos, Ward, & Zhu, 2002), ROUGE (Lin & Hovy, 2008), Translation
Error Rate (Feng & Lapata, 2013), Meteor (Denkowski & Lavie, 2014), CIDEr (Vedantam, Lawrence Zitnick, & Parikh, 2015). measures originally developed evaluate output machine translation engines text summarization systems,
exception CIDEr, developed specifically image description evaluation.
measures compute score indicates similarity system output
one human-written reference texts (e.g., ground truth translations summaries).
approach evaluation subject much discussion critique (Kulkarni
et al., 2011; Hodosh et al., 2013; Elliott & Keller, 2014). Kulkarni et al. found weakly
negative correlation human judgments unigram BLEU Pascal 1K
Dataset (Pearsons = -0.17 0.05). Hodosh et al. studied Cohens correlation
expert human judgments binarized unigram BLEU unigram ROUGE retrieved
descriptions Flickr8K dataset. found best agreement humans
BLEU ( = 0.72) ROUGE ( = 0.54) system retrieved sentences originally associated images. Agreement dropped one reference sentence
available, reference sentences disjoint proposal sentences.
concluded neither measure appropriate image description evaluation
subsequently proposed imagesentence ranking experiments, discussed detail below. Elliott Keller analyzed correlation human judgments automatic
evaluation measures retrieved system-generated image descriptions Flickr8K
VLT2K datasets. showed sentence-level unigram BLEU, point
429

fiBernardi et al.

time de facto standard measure image description evaluation, weakly
correlated human judgments. Meteor (Banerjee & Lavie, 2005), less frequently used
translation evaluation measure, exhibited highest correlation human judgments.
However, Kuznetsova et al. (2014) found unigram BLEU strongly correlated
human judgments Meteor image caption generation.
first large-scale image description evaluation took place MS COCO
Captions Challenge 2015,16 featuring 15 teams dataset 123,716 training images
41,000 images withheld test dataset. number reference texts testing
image either five 40, based insight measures may benefit
larger reference sets (Vedantam et al., 2015). automatic evaluation measures
used, image description systems outperformed humanhuman upper bound,17
whether five 40 reference descriptions provided. However, none systems
outperformed humanhuman evaluation judgment elicitation task used. Meteor
found robust measure, systems beating human text one
two submissions (depending number references); systems outperformed
humans seven five times measured CIDEr; according ROUGE BLEU,
system nearly always outperformed humans, confirming unsuitability
evaluation measures.
models approach description generation problem cross-modal retrieval perspective (Hodosh & Hockenmaier, 2013; Hodosh et al., 2013; Socher et al., 2014;
Gong et al., 2014; Karpathy et al., 2014; Verma & Jawahar, 2014) also able use measures information retrieval, median rank (mRank), precision k (S@k),
recall k (R@k) evaluate descriptions return, addition text-similarity
measures reported above. evaluation paradigm first proposed Hodosh et al.,
reported high correlation human judgments imagesentence based ranking
evaluations.
Table 3, summarize image description approaches discussed survey,
list datasets evaluation measures employed approaches.
seen recent systems (starting 2014) converged use
large description datasets (Flickr8K/30K, MS COCO) employ evaluation measures
perform well terms correlation human judgments (Meteor, CIDEr). However,
use BLEU, despite limitations, still widespread; also use human evaluation
means universal literature.

4. Future Directions
survey demonstrates, CV NLP communities witnessed upsurge
interest automatic image description systems. help recent advances deep
learning models images text, substantial improvements quality automatically generated descriptions registered. Nevertheless, series challenges
image description research remain. following, discuss future directions
line research likely benefit from.
16. Source http://mscoco.org/dataset/cap2015
17. Calculated collecting additional human-written description, compared
reference descriptions.

430

fiAutomatic Description Generation Images: Survey

Reference

Approach

Farhadi et al. (2010)
Kulkarni et al. (2011)
Li et al. (2011)
Ordonez et al. (2011)
Yang et al. (2011)

MultRetrieval
Generation
Generation
VisRetrieval
Generation

Datasets

Gupta et al. (2012)
Kuznetsova et al. (2012)
Mitchell et al. (2012)
Elliott Keller (2013)
Hodosh et al. (2013)

Pascal1K
Pascal1K
Pascal1K
SBU1M
IAPR,
Flickr8K/30K,
COCO
VisRetrieval
Pascal1K, IAPR
VisRetrieval
SBU1M
Generation
Pascal1K
Generation
VLT2K
MultRetrieval Pascal1K, Flickr8K

Gong et al. (2014)
Karpathy et al. (2014)
Kuznetsova et al. (2014)
Mason Charniak (2014)
Patterson et al. (2014)
Socher et al. (2014)
Verma Jawahar (2014)
Yatskar et al. (2014)
Chen Zitnick (2015)

MultRetrieval
MultRetrieval
Generation
VisRetrieval
VisRetrieval
MultRetrieval
MultRetrieval
Generation
MultRetrieval

Donahue et al. (2015)

MultRetrieval

Devlin et al. (2015)
Elliott de Vries (2015)
Fang et al. (2015)

VisRetrieval
Generation
Generation

Jia et al. (2015)
Generation
Karpathy Fei-Fei (2015) MultRetrieval
Kiros et al. (2015)
Lebret et al. (2015)
Lin et al. (2015)
Mao et al. (2015a)
Ortiz et al. (2015)
Pinheiro et al. (2015)
Ushiku et al. (2015)

MultRetrieval
MultRetrieval
Generation
MultRetrieval
Generation
MultRetrieval
Generation

Vinyals et al. (2015)

MultRetrieval

Xu et al. (2015)
Yagcioglu et al. (2015)

MultRetrieval
VisRetrieval

Measures
BLEU
Human, BLEU
Human, BLEU

BLEU, ROUGE, Meteor,
CIDEr, R@k
Human, BLEU, ROUGE
Human, BLEU
Human
Human, BLEU
Human, BLEU, ROUGE,
mRank, R@k
SBU1M, Flickr30K
R@k
Flickr8K/30K, COCO
BLEU, Meteor, CIDEr
SBU1M
Human, BLEU, Meteor
SBU1M
Human, BLEU
SBU1M
BLEU
Pascal1K
mRank, R@k
IAPR, SBU1M, Pascal1K BLEU, ROUGE, P@k
data
Human, BLEU
Flickr8K/30K, COCO
BLEU, Meteor, CIDEr,
mRank, R@k
Flickr30K, COCO
Human, BLEU, mRank,
R@k
COCO
BLEU, Meteor
VLT2K, Pascal1K
BLEU, Meteor
COCO
Human, BLEU, ROUGE,
Meteor, CIDEr
Flickr8K/30K, COCO
BLEU, Meteor
Flickr8K/30K, COCO
BLEU, Meteor, CIDEr,
mRank, R@k
Flickr8K/30K
R@k
Flickr30K, COCO
BLEU, R@k
NYU
ROUGE
IAPR, Flickr30K, COCO BLEU, mRank, R@k
Abstract Scenes
Human, BLEU, Meteor
COCO
BLEU
Pascal1K, IAPR, SBU1M, BLEU
COCO
Pascal1K,
SBU1M, BLEU, Meteor, CIDEr,
Flickr8K/30K
mRank, R@k
Flickr8K/30K, COCO
BLEU, Meteor
Flickr8K/30K, COCO
Human, BLEU, Meteor,
CIDEr

Table 3: overview approaches, datasets, evaluation measures reviewed
survey organised chronological order.

431

fiBernardi et al.

4.1 Datasets
earliest work image description used relatively small datasets (Farhadi et al., 2010;
Kulkarni et al., 2011; Elliott & Keller, 2013). Recently, introduction Flickr30K,
MS COCO large datasets enabled training complex models
neural networks. Still, area likely benefit larger diversified datasets
share common, unified, comprehensive vocabulary. Vinyals et al. (2015) argue
collection process quality descriptions datasets affect performance
significantly, make transfer learning datasets effective expected.
show learning model MS COCO applying datasets collected
different settings SBU1M Captions Pascal1K, leads degradation BLEU
performance. surprising, since MS COCO offers much larger amount training
data Pascal1K. Vinyals et al. put it, largely due differences
vocabulary quality descriptions. learning approaches likely suffer
situations. Collecting larger comprehensive datasets developing
generic approaches capable generating naturalistic descriptions across domains
therefore open challenge.
supervised algorithms likely take advantage carefully collected large
datasets, lowering amount supervision exchange access larger unsupervised
data also interesting avenue future research. Leveraging unsupervised data
building richer representations description models another open research challenge
context.
4.2 Measures
Designing automatic measures mimic human judgments evaluating suitability image descriptions perhaps urgent need area image description
(Elliott & Keller, 2014). need dramatically observed latest evaluation results MS COCO Challenge. According existing measures, including latest CIDEr
measure (Vedantam et al., 2015), several automatic methods outperform human upper bound (this upper bound indicates similar human descriptions other).
counterintuitive nature result confirmed fact human judgments used evaluation, output even best system judged worse
human generated description time (Fang et al., 2015). However, since
conducting human judgment experiments costly, major need improved automatic measures highly correlated human judgments. Figure 7 plots
Epanechnikov probability density estimate (a non-parametric optimal estimator) BLEU,
Meteor, ROUGE, CIDEr scores per subjective judgment Flickr8K dataset. human judgments obtained human experts (Hodosh et al., 2013). BLEU
confirmed unable sufficiently discriminate lowest three human
judgments, Meteor CIDEr show signs moving towards useful separation.
4.3 Diversity Originality
Current algorithms often rely direct representations descriptions see training time, making descriptions generated test time similar. results many
432

fiAutomatic Description Generation Images: Survey

Human Judgement

Human Judgement
0.10

Perfect
Minor mistakes
aspects
relation

0.00

0.00

0.02

0.05

0.04

0.10

0.06

0.08

0.15

Perfect
Minor mistakes
aspects
relation

0

20

40

60

80

100

0

20

BLEU

40

60

80

100

Meteor
Human Judgement
Perfect
Minor mistakes
aspects
relation

0

0

1

100

2

3

200

4

5

300

6

Perfect
Minor mistakes
aspects
relation

400

7

Human Judgement

0.0

0.2

0.4

0.6

0.8

0.00

1.0

0.05

0.10

0.15

0.20

CIDEr

ROUGE

Figure 7: Probability density estimates BLEU, Meteor, ROUGE, CIDEr scores
human judgments Flickr8K dataset. y-axis shows probability density,
x-axis score computed measure.

433

fiBernardi et al.

repetitions limits diversity generated descriptions, making difficult reach
human levels performance. situation demonstrated Devlin et al. (2015),
show best model able generate 47.0% unique descriptions. Systems generate diverse original descriptions repeat already
seen, also infer underlying semantics therefore remain open challenge. Chen
Zitnick (2015) related approaches take step towards addressing limitations
coupling description visual representation generation.
Jas Parikh (2015) introduces notion image specificity, arguing domain image descriptions uniform, certain images specific others.
Descriptions non-specific images tend vary lot people tend describe nonspecific scene different aspects. notion effects description systems
measures investigated detail.

4.4 Tasks
Another open challenge visual question-answering (VQA). natural language
question-answering based text significant goal NLP research long
time (e.g., Liang, Jordan, & Klein, 2012; Fader, Zettlemoyer, & Etzioni, 2013; Richardson, Burges, & Renshaw, 2013; Fader, Zettlemoyer, & Etzioni, 2014), answering questions
images task recently emerged. Towards achieving goal, Malinowski
Fritz (2014a) propose Bayesian framework connects natural language questionanswering visual information extracted image parts. recently, image
question answering methods based neural networks developed (Gao, Mao,
Zhou, Huang, & Yuille, 2015; Ren, Kiros, & Zemel, 2015; Malinowski, Rohrbach, & Fritz,
2015; Ma, Lu, & Li, 2016). Following effort, several datasets task
released: DAQUAR (Malinowski & Fritz, 2014a) compiled scene depth images
mainly focuses questions type, quantity color objects; COCOQA (Ren et al., 2015) constructed converting image descriptions VQA format
subset images MS COCO dataset; Freestyle Multilingual Image Question Answering (FM-IQA) Dataset (Gao et al., 2015), Visual Madlibs dataset (Yu, Park,
Berg, & Berg, 2015) VQA dataset (Antol et al., 2015), built images
MS COCO, time question-answer pairs collected via human annotators
freestyle paradigm. Research emerging field likely flourish near future. ultimate goal VQA build systems pass (recently developed)
Visual Turing Test able answer arbitrary questions images
precision human observer (Malinowski & Fritz, 2014b; Geman, Geman, Hallonquist, &
Younes, 2015).
multilingual repositories image description interesting direction
explore. Currently, among available benchmark datasets, IAPR-TC12
dataset (Grubinger et al., 2006) multilingual descriptions (in English German).
Future work investigate whether transferring multimodal features monolingual description models results improved descriptions compared monolingual baselines.
434

fiAutomatic Description Generation Images: Survey

would interesting study different models new tasks multilingual multimodal
setting using larger syntactically diverse multilingual description corpora.18
Overall, image understanding ultimate goal computer vision natural language generation one ultimate goals NLP. Image description
goals interconnected topic therefore likely benefit individual advances
two fields.

5. Conclusions
survey, discuss recent advances automatic image description closely related
problems. review analyze large body existing work highlighting common
characteristics differences existing research. particular, categorize
related work three groups: (i) direct description generation images, (i) retrieval
images visual space, (iii) retrieval images multimodal (joint visual
linguistic) space. addition, provided brief review existing corpora
automatic evaluation measures, discussed future directions vision language
research.
Compared traditional keyword-based image annotation (using object recognition,
attribute detection, scene labeling, etc.), automatic image description systems produce
human-like explanations visual content, providing complete picture scene.
Advancements field could lead intelligent artificial vision systems,
make inferences scenes generated grounded image descriptions
therefore interact environments natural manner. could also
direct impact technological applications visually impaired people
benefit accessible interfaces.
Despite remarkable increase number image description systems recent
years, experimental results suggest system performance still falls short human performance. similar challenge lies automatic evaluation systems using reference
descriptions. measures tools currently use sufficiently highly correlated human judgments, indicating need measures deal
complexity image description problem adequately.

Acknowledgments
thank anonymous reviewers useful comments. work partially supported European Commission ICT COST Action iV&L Net: European Network Integrating Vision Language (IC1037). RC, AE, EE, NIC
funded Scientific Technological Research Council Turkey (TUBITAK) research grant 113E116. FK would like acknowledge ERC funding starting grant
203427 Synchronous Linguistic Visual Processing. DE supported ERCIM
ABCDE Fellowship 2014-23.
18. Multimodal Translation Shared Task 2016 Workshop Machine Translation use
English German translated version Flickr30K corpora. See http://www.statmt.org/wmt16/
multimodal-task.html details.

435

fiBernardi et al.

References
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C. L., & Parikh, D. (2015).
Vqa: Visual question answering. International Conference Computer Vision.
Banerjee, S., & Lavie, A. (2005). METEOR: Automatic Metric MT Evaluation
Improved Correlation Human Judgments. Annual Meeting Association Computational Linguistics Workshop Intrinsic Extrinsic Evaluation
Measures MT and/or Summarization.
Berg, T. L., Berg, A. C., & Shih, J. (2010). Automatic attribute discovery characterization noisy web data. European Conference Computer Vision.
Chatfield, K., Simonyan, K., Vedaldi, A., & Zisserman, A. (2014). Return devil
details: Delving deep convolutional nets. British Machine Vision Conference.
Chen, J., Kuznetsova, P., Warren, D., & Choi, Y. (2015). Deja image-captions: corpus
expressive descriptions repetition. North American Chapter Association
Computational Linguistics.
Chen, X., & Zitnick, C. L. (2015). Minds eye: recurrent visual representation image
caption generation. IEEE Conference Computer Vision Pattern Recognition.
Dale, R., & White, M. E. (Eds.). (2007). Workshop Shared Tasks Comparative
Evaluation Natural Language Generation: Position Papers.
Denkowski, M., & Lavie, A. (2014). Meteor Universal: Language Specific Translation Evaluation Target Language. Conference European Chapter Association Computational Linguistics Workshop Statistical Machine Translation.
Devlin, J., Cheng, H., Fang, H., Gupta, S., Deng, L., He, X., Zweig, G., & Mitchell, M.
(2015). Language Models Image Captioning: Quirks Works.
Annual Meeting Association Computational Linguistics.
Donahue, J., Hendricks, L. A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko,
K., & Darrell, T. (2015). Long-term recurrent convolutional networks visual recognition description. IEEE Conference Computer Vision Pattern Recognition.
Elliott, D., & de Vries, A. P. (2015). Describing images using inferred visual dependency
representations. Annual Meeting Association Computational Linguistics.
Elliott, D., & Keller, F. (2013). Image Description using Visual Dependency Representations. Conference Empirical Methods Natural Language Processing.
Elliott, D., & Keller, F. (2014). Comparing Automatic Evaluation Measures Image
Description. Annual Meeting Association Computational Linguistics.
Elliott, D., Lavrenko, V., & Keller, F. (2014). Query-by-Example Image Retrieval using
Visual Dependency Representations. International Conference Computational
Linguistics.
Everingham, M., Van Gool, L., Williams, C. K. I., Winn, J., & Zisserman, A. (2010).
PASCAL Visual Object Classes (VOC) Challenge. International Journal Computer
Vision, 88 (2), 303338.
436

fiAutomatic Description Generation Images: Survey

Fader, A., Zettlemoyer, L., & Etzioni, O. (2013). Paraphrase-driven learning open question answering. Annual Meeting Association Computational Linguistics.
Fader, A., Zettlemoyer, L., & Etzioni, O. (2014). Open question answering curated
extracted knowledge bases. ACM SIGKDD Conference Knowledge Discovery
Data Mining.
Fang, H., Gupta, S., Iandola, F., Srivastava, R., Deng, L., Dollar, P., Gao, J., He, X.,
Mitchell, M., Platt, J., Zitnick, C. L., & Zweig, G. (2015). captions visual
concepts back. IEEE Conference Computer Vision Pattern Recognition.
Farhadi, A., Hejrati, M., Sadeghi, M. A., Young, P., Rashtchian, C., Hockenmaier, J., &
Forsyth, D. (2010). Every picture tells story: Generating sentences images.
European Conference Computer Vision.
Felzenszwalb, P. F., Girshick, R. B., McAllester, D., & Ramanan, D. (2010). Object detection discriminatively trained part-based models. IEEE Transactions Pattern
Analysis Machine Intelligence, 32 (9), 16271645.
Feng, Y., & Lapata, M. (2008). Automatic Image Annotation Using Auxiliary Text Information. Annual Meeting Association Computational Linguistics.
Feng, Y., & Lapata, M. (2013). Automatic caption generation news images. IEEE
Transactions Pattern Analysis Machine Intelligence, 35 (4), 797812.
Ferraro, F., Mostafazadeh, N., Huang, T., Vanderwende, L., Devlin, J., Galley, M., &
Mitchell, M. (2015). survey current datasets vision language research.
Conference Empirical Methods Natural Language Processing.
Gao, H., Mao, J., Zhou, J., Huang, Z., & Yuille, A. (2015). talking machine?
dataset methods multilingual image question answering. International
Conference Learning Representations.
Geman, D., Geman, S., Hallonquist, N., & Younes, L. (2015). Visual turing test computer
vision systems. Proceedings National Academy Sciences, 112 (12), 36183623.
Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies accurate object detection semantic segmentation. IEEE Conference Computer
Vision Pattern Recognition.
Gong, Y., Wang, L., Hodosh, M., Hockenmaier, J., & Lazebnik, S. (2014). Improving ImageSentence Embeddings Using Large Weakly Annotated Photo Collections. European
Conference Computer Vision.
Grubinger, M., Clough, P., Muller, H., & Deselaers, T. (2006). IAPR TC-12 benchmark:
new evaluation resource visual information systems. International Conference
Language Resources Evaluation.
Guadarrama, S., Krishnamoorthy, N., Malkarnenkar, G., Venugopalan, S., Mooney, R., Darrell, T., & Saenko, K. (2013). Youtube2text: Recognizing describing arbitrary
activities using semantic hierarchies zero-shot recognition. International Conference Computer Vision.
Gupta, A., Verma, Y., & Jawahar, C. V. (2012). Choosing linguistics vision describe
images. AAAI Conference Artificial Intelligence.
437

fiBernardi et al.

Hardoon, D. R., Szedmak, S., & Shawe-Taylor, J. (2004). Canonical correlation analysis:
overview application learning methods. Neural Computation, 16 (12),
26392664.
Hodosh, M., & Hockenmaier, J. (2013). Sentence-based image description scalable,
explicit models. IEEE Conference Computer Vision Pattern Recognition
Workshops.
Hodosh, M., Young, P., & Hockenmaier, J. (2013). Framing Image Description Ranking Task: Data, Models Evaluation Metrics. Journal Artificial Intelligence
Research, 47, 853899.
Hotelling, H. (1936). Relations two sets variates. Biometrika, 0, 321377.
Jaimes, A., & Chang, S.-F. (2000). conceptual framework indexing visual information
multiple levels. IST SPIE Internet Imaging.
Jas, M., & Parikh, D. (2015). Image specificity. IEEE Conference Computer Vision
Pattern Recognition.
Jia, X., Gavves, E., Fernando, B., & Tuytelaars, T. (2015). Guiding long-short term
memory model image caption generation. International Conference Computer Vision.
Johnson, J., Krishna, R., Stark, M., Li, L.-J., Shamma, D. A., Bernstein, M., & Fei-Fei, L.
(2015). Image retrieval using scene graphs. IEEE Conference Computer Vision
Pattern Recognition.
Karpathy, A., & Fei-Fei, L. (2015). Deep visual-semantic alignments generating image
descriptions. IEEE Conference Computer Vision Pattern Recognition.
Karpathy, A., Joulin, A., & Fei-Fei, L. (2014). Deep Fragment Embeddings Bidirectional
Image Sentence Mapping. Advances Neural Information Processing Systems.
Khan, M. U. G., Zhang, L., & Gotoh, Y. (2011). Towards coherent natural language description video streams. International Conference Computer Vision Workshops.
Kiros, R., Salakhutdinov, R., & Zemel, R. S. (2015). Unifying visual-semantic embeddings
multimodal neural language models. Advances Neural Information Processing Systems Deep Learning Workshop.
Krishnamoorthy, N., Malkarnenkar, G., Mooney, R., Saenko, K., & Guadarrama, S. (2013).
Generating Natural-Language Video Descriptions Using Text-Mined Knowledge.
Annual Conference North American Chapter Association Computational Linguistics: Human Language Technologies.
Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg, A. C., & Berg, T. L. (2011). Baby
talk: Understanding generating simple image descriptions. IEEE Conference
Computer Vision Pattern Recognition.
Kuznetsova, P., Ordonez, V., Berg, A. C., Berg, T. L., & Choi, Y. (2012). Collective
Generation Natural Image Descriptions. Annual Meeting Association
Computational Linguistics.
438

fiAutomatic Description Generation Images: Survey

Kuznetsova, P., Ordonezz, V., Berg, T. L., & Choi, Y. (2014). TREETALK: Composition
compression trees image descriptions. Conference Empirical Methods
Natural Language Processing.
Lampert, C. H., Nickisch, H., & Harmeling, S. (2009). Learning detect unseen object
classes between-class attribute transfer. IEEE Conference Computer Vision
Pattern Recognition.
Lazebnik, S., Schmid, C., & Ponce, J. (2006). Beyond bags features: Spatial pyramid
matching recognizing natural scene categories. IEEE Conference Computer
Vision Pattern Recognition.
Lebret, R., Pinheiro, P. O., & Collobert, R. (2015). Phrase-based image captioning.
International Conference Machine Learning.
Li, S., Kulkarni, G., Berg, T. L., Berg, A. C., & Choi, Y. (2011). Composing simple image
descriptions using web-scale n-grams. SIGNLL Conference Computational
Natural Language Learning.
Liang, P., Jordan, M. I., & Klein, D. (2012). Learning dependency-based compositional
semantics. Computational Linguistics, 39 (2), 389446.
Lin, C.-Y., & Hovy, E. (2008). Automatic evaluation summaries using n-gram cooccurrence statistics. Annual Conference North American Chapter
Association Computational Linguistics: Human Language Technologies.
Lin, D., Fidler, S., Kong, C., & Urtasun, R. (2015). Generating multi-sentence natural
language descriptions indoor scenes. British Machine Vision Conference.
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., & Zitnick,
C. L. (2014). Microsoft COCO: Common objects context. European Conference
Computer Vision.
Lowe, D. (2004). Distinctive image features scale-invariant keypoints. International
Journal Computer Vision, 60 (4), 91110.
Ma, L., Lu, Z., & Li, H. (2016). Learning answer questions image using convolutional
neural network. AAAI Conference Artificial Intelligence.
Malinowski, M., & Fritz, M. (2014a). multi-world approach question answering
real-world scenes based uncertain input. Advances Neural Information Processing Systems.
Malinowski, M., & Fritz, M. (2014b). Towards visual turing challenge. Advances
Neural Information Processing Systems Workshop Learning Semantics.
Malinowski, M., Rohrbach, M., & Fritz, M. (2015). Ask neurons: neural-based
approach answering questions images. International Conference Computer Vision.
Mao, J., Xu, W., Yang, Y., Wang, J., & Yuille, A. L. (2015a). Deep captioning multimodal recurrent neural networks (m-RNN). International Conference Learning
Representations.
439

fiBernardi et al.

Mao, J., Wei, X., Yang, Y., Wang, J., Huang, Z., & Yuille, A. L. (2015b). Learning like
child: Fast novel visual concept learning sentence descriptions images.
International Conference Computer Vision.
Mason, R., & Charniak, E. (2014). Nonparametric Method Data-driven Image Captioning. Annual Meeting Association Computational Linguistics.
Mason, W. A., & Watts, D. J. (2009). Financial incentives performance crowds.
ACM SIGKDD Workshop Human Computation.
Mitchell, M., Han, X., Dodge, J., Mensch, A., Goyal, A., Berg, A. C., Yamaguchi, K.,
Berg, T. L., Stratos, K., Daume, III, H., & III (2012). Midge: generating image
descriptions computer vision detections. Conference European Chapter
Association Computational Linguistics.
Nenkova, A., & Vanderwende, L. (2005). impact frequency summarization. Tech.
rep., Microsoft Research.
Oliva, A., & Torralba, A. (2001). Modeling shape scene: holistic representation
spatial envelope. International Journal Computer Vision, 42 (3), 145175.
Ordonez, V., Kulkarni, G., & Berg, T. L. (2011). Im2text: Describing images using 1 million
captioned photographs. Advances Neural Information Processing Systems.
Ortiz, L. M. G., Wolff, C., & Lapata, M. (2015). Learning Interpret Describe
Abstract Scenes. Conference North American Chapter Association
Computational Linguistics.
Panofsky, E. (1939). Studies Iconology. Oxford University Press.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: method automatic evaluation machine translation. Annual Meeting Association
Computational Linguistics.
Parikh, D., & Grauman, K. (2011). Relative attributes. International Conference
Computer Vision.
Park, C., & Kim, G. (2015). Expressing image stream sequence natural
sentences. Advances Neural Information Processing Systems.
Patterson, G., Xu, C., Su, H., & Hays, J. (2014). SUN Attribute Database: Beyond Categories Deeper Scene Understanding. International Journal Computer Vision,
108 (1-2), 5981.
Pinheiro, P., Lebret, R., & Collobert, R. (2015). Simple image description generator via
linear phrase-based model. International Conference Learning Representations
Workshop.
Prest, A., Schmid, C., & Ferrari, V. (2012). Weakly supervised learning interactions
humans objects. IEEE Transactions Pattern Analysis Machine
Intelligence, 34 (3), 601614.
Rashtchian, C., Young, P., Hodosh, M., & Hockenmaier, J. (2010). Collecting image annotations using amazons mechanical turk. North American Chapter Association
Computational Linguistics: Human Language Technologies Workshop Creating
Speech Language Data Amazons Mechanical Turk.
440

fiAutomatic Description Generation Images: Survey

Reiter, E., & Belz, A. (2009). investigation validity metrics automatically evaluating natural language generation systems. Computational Linguistics,
35 (4), 529588.
Reiter, E., & Dale, R. (2006). Building Natural Language Generation Systems. Cambridge
University Press.
Ren, M., Kiros, R., & Zemel, R. (2015). Image question answering: visual semantic embedding model new dataset. International Conference Machine Learningt
Deep Learning Workshop.
Richardson, M., Burges, C. J., & Renshaw, E. (2013). MCTest: challenge dataset
open-domain machine comprehension text. Conference Empirical Methods
Natural Language Processing.
Rohrbach, A., Rohrback, M., Tandon, N., & Schiele, B. (2015). dataset movie description. International Conference Computer Vision.
Rohrbach, M., Qiu, W., Titov, I., Thater, S., Pinkal, M., & Schiele, B. (2013). Translating
Video Content Natural Language Descriptions. International Conference
Computer Vision.
Schuster, S., Krishna, R., Chang, A., Fei-Fei, L., & Manning, C. D. (2015). Generating
semantically precise scene graphs textual descriptions improved image retrieval. Conference Empirical Methods Natural Language Processing Vision
Language Workshop.
Shatford, S. (1986). Analyzing subject picture: theoretical approach. Cataloging
& Classification Quarterly, 6, 3962.
Silberman, N., Kohli, P., Hoiem, D., & Fergus, R. (2012). Indoor segmentation support
inference RGBD images. European Conference Computer Vision.
Socher, R., & Fei-Fei, L. (2010). Connecting modalities: Semi-supervised segmentation
annotation im- ages using unaligned text corpora. IEEE Conference
Computer Vision Pattern Recognition.
Socher, R., Karpathy, A., Le, Q. V., Manning, C. D., & Ng, A. (2014). Grounded Compositional Semantics Finding Describing Images Sentences. Transactions
Association Computational Linguistics, 2, 207218.
Sun, C., Gan, C., & Nevatia, R. (2015). Automatic concept discovery parallel text
visual corpora. International Conference Computer Vision.
Thomason, J., Venugopalan, S., Guadarrama, S., Saenko, K., & Mooney, R. (2014). Integrating Language Vision Generate Natural Language Descriptions Videos
Wild. International Conference Computational Linguistics.
Torralba, A., Fergus, R., & Freeman, W. T. (2008). 80 million tiny images: large data
set nonparametric object scene recognition. IEEE Transactions Pattern
Analysis Machine Intelligence, 30 (11), 19581970.
Ushiku, Y., Yamaguchi, M., Mukuta, Y., & Harada, T. (2015). Common subspace model
similarity: Phrase learning caption generation images. International
Conference Computer Vision.
441

fiBernardi et al.

Vedantam, R., Lawrence Zitnick, C., & Parikh, D. (2015). Cider: Consensus-based image description evaluation. IEEE Conference Computer Vision Pattern
Recognition.
Verma, Y., & Jawahar, C. V. (2014). Im2Text Text2Im: Associating Images Texts
Cross-Modal Retrieval. British Machine Vision Conference.
Vinyals, O., Toshev, A., Bengio, S., & Erhan, D. (2015). Show tell: neural image
caption generator. IEEE Conference Computer Vision Pattern Recognition.
Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., Zemel, R., & Bengio, Y.
(2015). Show, attend tell: Neural image caption generation visual attention.
International Conference Machine Learning.
Yagcioglu, S., Erdem, E., Erdem, A., & Cakici, R. (2015). Distributed Representation
Based Query Expansion Approach Image Captioning. Annual Meeting
Association Computational Linguistics.
Yang, Y., Teo, C. L., Daume, III, H., & Aloimonos, Y. (2011). Corpus-guided sentence generation natural images. Conference Empirical Methods Natural Language
Processing.
Yao, B., & Fei-Fei, L. (2010). Grouplet: structured image representation recognizing
human object interactions. IEEE Conference Computer Vision Pattern
Recognition.
Yao, L., Torabi, A., Cho, K., Ballas, N., Pal, C., Larochelle, H., & Courville, A. (2015).
Describing videos exploiting temporal structure. International Conference
Computer Vision.
Yatskar, M., Galley, M., Vanderwende, L., & Zettlemoyer, L. (2014). See Evil, Say
Evil: Description Generation Densely Labeled Images. Joint Conference
Lexical Computation Semantics.
Young, P., Lai, A., Hodosh, M., & Hockenmaier, J. (2014). image descriptions visual
denotations: New similarity metrics semantic inference event descriptions.
Transactions Association Computational Linguistics, 2, 6778.
Yu, L., Park, E., Berg, A. C., & Berg, T. L. (2015). Visual madlibs: Fill blank
description generation question answering. International Conference Computer Vision.
Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., & Fidler, S.
(2015). Aligning books movies: Towards story-like visual explanations watching
movies reading books. International Conference Computer Vision.
Zitnick, C. L., Parikh, D., & Vanderwende, L. (2013). Learning visual interpretation
sentences. International Conference Computer Vision.
Zitnick, C. L., & Parikh, D. (2013). Bringing semantics focus using visual abstraction.
IEEE Conference Computer Vision Pattern Recognition.

442

fiJournal Artificial Intelligence Research 55 (2016) 1-15

Submitted 11/15; published 01/16

Introduction Special Issue Cross-Language
Algorithms Applications
Marta R. Costa-jussa

marta.ruiz@upc.edu

TALP Research Center
Universitat Politecnica de Catalunya
Jordi Girona 13, 08034, Barcelona

Srinivas Bangalore

sbangalore@interactions.net

Interactions Labs
41 Spring Street,
Murray Hill, NJ 07974, USA

Patrik Lambert

patrik.lambert@upf.edu

Computational Linguistics Group
Universitat Pompeu Fabra
Roc Boronat 138, 08018 Barcelona, Spain

Llus Marquez

lmarquez@qf.org.qa

Qatar Computing Research Institute
Hamad Bin Khalifa University
Tornado Tower (10th floor), PO.Box 5825,
West Bay, Doha, Qatar

Elena Montiel-Ponsoda

elena.montiel@upm.es

Ontology Engineering Group
Universidad Politecnica de Madrid
Campus de Montegancedo s/n, Boadilla del Monte,
28660 Madrid

Abstract

increasingly global nature everyday interactions, need multilingual technologies support efficient effective information access communication
cannot overemphasized. Computational modeling language focus
Natural Language Processing, subdiscipline Artificial Intelligence. One current
challenges discipline design methodologies algorithms crosslanguage order create multilingual technologies rapidly. goal JAIR special
issue Cross-Language Algorithms Applications (CLAA) present leading research area, emphasis developing unifying themes could lead
development science multi- cross-lingualism. introduction, provide
reader motivation special issue summarize contributions
papers included. selected papers cover broad range cross-lingual
technologies including machine translation, domain language adaptation sentiment
analysis, cross-language lexical resources, dependency parsing, information retrieval
knowledge representation. anticipate special issue serve invaluable
resource researchers interested topics cross-lingual natural language processing.
c
2016
AI Access Foundation. rights reserved.

fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsoda

1. Introduction
Due increasingly global nature society, commonplace us
encounter information multitude languages communicate across languages
everyday lives. rapid growth multilinguality information-driven society
reflected number languages used Internet. decade,
Internet transformed predominantly English information source
linguistically variegated information source today. Multilingual access
processing pose novel challenges core Artificial Intelligence discipline speech
natural language processing which, solved, provide transformational technologies
information access broader population.
complexity processing multiple languages computational model emerges
due different syntactic structures concept, also different
underlying conceptual structures. challenges necessitate development crosslanguage natural language processing tools able translate link structures
concepts across different languages.
Cross-language extensions popular applications tasks information retrieval, question answering, sentiment analysis lexical disambiguation, among others,
developed respond present needs global society. Similarly, multilingual resources novel ways represent multilingual knowledge emerged
spread recent years. Researchers developers made considerable advances
applications leveraging relevant data available diverse languages. multilingual processing becoming key research issue, given interdisciplinary nature,
range approaches linguistic statistical perspectives explored
order create viable cross-lingual technology.
Interestingly, challenges cross-lingual natural language processing appeals
academic industrial research. provides new business opportunities breaking
language barriers fragment potential market. opportunities include, instance, possibility companies institutions learn users different
locations think products (cross-language sentiment analysis), perform document
search multiple languages (cross-language information retrieval question answering),
link knowledge bases clients using different languages (cross-language knowledge representation), expand market several linguistically diverse markets (machine
translation localization).
importance cross-lingual natural language processing clearly seen
attention received recent workshops, invited talks, publications
communitys premier conferencesAssociation Computational Linguistics (ACL), European Association Computational Linguistics (EACL), North American Association
Computational Linguistics (NAACL), Empirical Methods Natural Language Processing
(EMNLP), International Conference Computational Linguistics (COLING), Extended
Semantic Web Conference (ESWC), International Semantic Web Conference (ISWC), International Conference Language Resources Evaluation (LREC), International
Conference Knowledge Capture (KCAP). past eighteen months, one
every five papers conferences related cross-language algorithms applications (figures vary 17% 25%, average 20.4%). recently published book
2

fiSpecial Issue Cross-Language Algorithms Applications

focused multilingual natural language processing techniques (Bikel & Zitouni, 2012)
highlights importance research area. book comprehensively
presents, 600 pages, basic theory relevant techniques 16
different aspects multilingual natural language processing.
1.1 Cross-Language Algorithms Applications Special Issue
special issue intended provide broad view recent advances
current research directions pursued area multilingual natural language
processing linguistic, computational language resource creation perspectives.
Active research multiple recent workshops area (e.g., HyTra, see Costa-juss,
Banchs, Rapp, Lambert, Eberle & Babych, 2013; WMT, see Bojar, Chatterjee, Federmann,
Haddow, Huck, Hokamp, Koehn, Loncheva, Monz, Negri, Post, Scarton, Specia & Turchi,
2015; CLEF, see Forner, Moller, Paredes, Rosso & Stein, 2013; Promise, see Bener, Minku
& Turhan, 2014; MSW, see Gracia, MacGrae & Vulcu, 2015; AKBC, see Suchanek,
Riedel, Singh & Talukdar, 2012) spanning cross-language natural language applications,
tools crowdsourcing resource creation, deeper relationships bridging
language barriers modalities perception summarized special issue.
response solicitation papers late 2014, received 34 research papers
variety cross-lingual technologies applied language processing tasks.
papers carefully reviewed least three reviewers drawn pool
100 reviewers. Based reviews extensive follow discussions, selected 8
high quality papers offer exciting research directions span range topics
cross-lingual language processing including machine translation, domain language
adaptation sentiment cross-language lexical resources, dependency parsing, information retrieval knowledge representation. introduction covers exactly eight
papers accepted special issue official timeline. papers
summarized Section 2, organized topic. attempt accommodate broader
sample interesting papers cross-language algorithms applications, papers
appropriate topic meet special issue deadlines also added
JAIR web page1 accepted journal.
Finally, intended special issue disconnected melange success stories,
provide underlying theme unifies research directions area. hope
collection provides reader opportunity observe similarities differences
across topics, algorithms applications. anticipate scientific community
view cross-lingual speech language processing fertile productive field
research, potential developing science technologies
lasting impact everyday lives.

2. Special Issue Overview
section, survey topics cover papers special issue
papers themselves. topics machine translation, domain language adaptation
1. http://www.jair.org/specialtrack-claa.html

3

fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsoda

sentiment analysis cross-language lexical resources, dependency parsing, information
retrieval knowledge representation.
2.1 Machine Translation
key technology multilingual information society Machine Translation (MT)
source language speech text automatically converted target language
speech text.
Web content generated multiple languages Internet users becoming linguistically diverse, machine translation provides cheapest quickest way
understand multilingual information. Besides core application necessary
multilingual world, machine translation shown highly relevant component
technology many cross-language natural language processing tasks, sentiment
analysis, information retrieval knowledge representation.
Historically, machine translation approaches categorized rule-driven
data-driven approaches. Rule-based machine translation (Hutchins & Sommers, 1992) requires deep linguistic knowledge language pairs involved translation significant amount human labor. Since rules based linguistic intuitions, easier
identify issues extend them. recently, data-driven approaches
learn translation models minimal human supervision large bilingual
parallel corporatexts source text paired target text (Sanchez-Martnez
& Forcada, 2009). Data-driven translation systems find probable target text given
source text. systems extended phrase-based, syntax-based, hierarchical phrase-based neural-based systems order capture longer contexts
sentence. Phrase-based systems (Koehn, Och, & Marcu, 2003) use sequences words
bilingual units, called phrases, whose probability computed log-linear combination feature functions including translation language models. Syntax-based
systems use syntactic units extracted parse trees (Quirk, Menezes, & Cherry, 2005).
Hierarchical phrase-based systems combine phrase-based syntax-based approaches
using synchronous context-free grammars (Chiang, 2007). Neural-based end-to-end translation systems typically use encoder-decoder approach learn embedded representations
input sentence (encoder), used context generate words
translation (decoder) (Bahdanau, Cho, & Bengio, 2015).
boundaries rule-based statistical machine translation narrowed
proposals hybrid machine translation systems (Costa-jussa, 2015).
special issue, two research papers machine translation combine rules,
statistics machine learning.
Integrating Rules Dictionaries Shallow-Transfer Machine Translation Phrase-Based Statistical Machine Translation Sanchez-Cartagena,
Perez-Ortiz Sanchez-Martnez presents hybrid approach machine translation
integrating rules dictionaries shallow-transfer system (in particular, Apertium,
see Armentano-Oller & Forcada, 2006) phrase-based system (in particular, Moses,
see Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens,
Dyer, Bojar, Constantin & Herbst, 2007). Deep linguistic knowledge rule-based systems transferred statistical system. integration especially useful
4

fiSpecial Issue Cross-Language Algorithms Applications

parallel corpus available training phrase-based system small translating out-of-domain texts. authors discuss different methods enriching translation
tables (including previous work authors references). Finally,
worth mentioning approach used Apertiums rules one best-ranked
systems WMT 2011 international evaluation campaign Spanish-English (SanchezCartagena, Sanchez-Martnez, & Perez-Ortiz, 2011).
Two relevant features paper are: (i) integration rules statistics presented, takes advantage deep knowledge underlying rule-based system,
especially way linguistic resources used rule-based system segmenting source-language sentences; (ii) complete analysis hybrid system, including
automatic manual evaluation, different corpora sizes, domains language pairs,
comparison another popular hybrid MT system (Eisele, Federmann, Uszkoreit, SaintAmand, Kay, Jellinghaus, Hunsicker, Herrmann, & Chen, 2008).
Cross-Lingual Bridges Models Lexical Borrowing Tsvetkov Dyer
introduces hybrid model lexical borrowing, demonstrated machine translation alleviate problem lexical coverage low-resourced languages. authors
start hypothesis languages borrow terms languages point
existence. propose computational model linguistic borrowing, intended
identify donor words resource-rich language given loan word resource-poor
language. model develops set morpho-phonological transformations combining linguistic constraints using optimality theory machine learning score loanword
candidates. model consists three parts: (i) conversion orthographic word forms
pronunciations,(ii) generation loan word pronunciation candidates, (iii) ranking generated candidates using optimality-theoretic constraints. first two steps
rule-based third learned data.
lexical borrowing model applied Swahili-English, Maltese-English, Romanian-English MT. authors leverage model indirect way. instance, improving resource-poor Swahili-English MT system, identify translation candidates
out-of-vocabulary (OOV) Swahili words borrowed Arabic, using Arabic-to-Swahili
borrowing model resource-rich Arabic-English MT system. experimental results
show approach effectively reduces impact OOV source words improves
translation quality significantly.
one considers lexical borrowing different task transliteration cognate
identification could first computational model lexical borrowing used
downstream natural language processing application.
2.2 Domain Language Adaptation Sentiment
rapidly growing repository user-generated, subjective texts Internet
form blogs, social networks, information channels consumer sites expressing opinions various issues, sentiment towards products, services personal perspectives
events.
Sentiment analysis task analyzing opinions, sentiments emotions expressed
towards entities products, services, organizations, issues, various attributes
entities (Liu, 2012). two main sentiment analysis approaches presented
5

fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsoda

literature machine learning approach (mostly supervised learning) opinionlexicon-based approach, based rules.
necessary resourcesthe training data, subjective text analyzed
analysis outcomeare available required language, cross-language sentiment
analysis methods needed bootstrap system. main cross-language sentiment
analysis approaches described literature via lexicon transfer, via corpus transfer, via test translation via joint classification. lexicon transfer approach,
source sentiment lexicon transferred target language lexicon-based classifier built target language (Mihalcea, Banea, & Wiebe, 2007). corpus transfer
approach consists transferring source training corpus target language
building corpus-based classifier target language (Banea, Mihalcea, & Wiebe, 2008).
test translation approach, test sentences target language translated
source language classified using source language classifier (Bautin, Vijayarenu, & Skiena, 2008). Work joint classification includes co-training (Wan, 2009), joint
learning (Lu, Tan, Cardie, & K. Tsou, 2011) structural correspondence learning (Wei &
Pal, 2010; Prettenhofer & Stein, 2010).
authors already studied impact automatic sentiment analysis transferring lexicons corpora via machine translation (Mihalcea et al., 2007; Banea et al.,
2008).
Translation Alters Sentiment Mohammad, Salameh Kiritchenko goes
step analysis two respects. First, authors conduct systematic
evaluation impact automatic manual translation automatic manual
sentiment analysis. Second, authors perform qualitative quantitative analysis
understand reasons obtained results. summary, paper provides deeper
understanding sentiments altered common cross-language settings.
experiments performed using Arabic sentiment analysis system
EnglishArabic machine translation system, showing state-of-the-art performance.
authors first show automatic sentiment analysis English translations (even coming MT) achieve competitive results. Interestingly, also show automatic
sentiment analysis automatic translations outperforms manual sentiment annotation
automatically translated text. qualitative quantitative analysis results
also reveals interesting facts. example, sentiment expressions often mistranslated
neutral expressions. automatic sentiment analysis system recover consistent translation errors learning true sentiments mistranslated words. common
causes translation failing preserve sentiments sarcasm, metaphoric expressions,
incorrect word-reordering.
Distributional Correspondence Indexing Cross-Lingual Cross-Domain
Sentiment Classification Esuli, Moreo Sebastiani proposes novel domain adaptation method, also evaluated language adaptation. paper explores general
complex formulation domain adaptation problem combines cross-domain
cross-language settings. proposed adaptation method, called Distributional Correspondence Indexing, inspired Structural Correspondence Learning follows
different, simpler approach, direct application distributional hypothesis.
approach assumes terms across domains and/or languages show similar distributional properties relative small set pivot terms, behave similarly across
6

fiSpecial Issue Cross-Language Algorithms Applications

domains/languages. authors show approach outperforms existing methods
cross-domain/language sentiment classification, lower computational cost.
Since digital documents increasing variety topics languages produced,
often need processed immediately, better solutions tackle bottleneck
scarcity training data increasing importance. idea leveraging resources
language learn classifier another language similar transfer learning
domain adaptation. contribution paper goes direction unifying domain
language adaptation framework.
2.3 Lexical Resources
natural language applications question-answering, sentiment analysis, document classification, mention few, demand lexical knowledge different
natural languages, termed cross-language lexical resources, also well-known
multilingual lexical resources. range non-structured resources parallel corpora (EU JRC-Acquis Corpus, see Steinberger, Pouliquen, Widiger, Ignat, Erjavec,
Tufis, & Varga, 2006), glossaries (e.g., IFLA Multilingual Glossary Art Librarians, see
Libraries, 1996) machine-readable dictionaries (Oxford online dictionaries2 ),
structured resources terminological databases (IATE3 ), thesauri (AGROVOC 4 ),
lexicons (EuroWordNet, see Vossen, 1998; MultiWordNet, see Pianta, Bentivogli, & Girardi, 2002), ontologies (e.g., EUROVOC SKOS, see Smedt & Vatanat, 2009; FAO
geopolitical ontology, see Kim, Iglesias-Sucasas, & Viollier, 2013).
creation multilingual resources involves manual costly processes,
many approaches pursue automation several steps development process.
Without aim exhaustive, briefly describe typical approaches methods
result multilingual lexical resources. (i) well-known family multilingual
wordnets developed around Princeton English WordNet (Fellbaum, 1998). Basically, two
approaches followed: (a) merging approach wordnets created separately
mapped afterwards English WordNet, difficulties mapping task
involves; (b) expansion approach English WordNet translated
corresponding target languages, facilitating subsequent mapping task. (ii) Onlinecollaborative resources, example, Wiktionary5 . take advantage wisdom
crowd also Internet bots automatically generate entries, well
algorithms import lexical information machine readable dictionaries. Similarly,
Wikipedia whole family Wikimedia projects created collaborative
manner constitute de facto multilingual resources thanks hyperlinks among
entries different languages. resources built similar way take advantage
structured information contained include YAGO (Mahdisoltani, Biega, &
Suchanek, 2015) BabeLNet (Navigli & Ponzetto, 2012). (iii) Mono- multilingual
content linguistic datasets exposed linked according Linked Data paradigm6 ,
set best practices publishing structured data linking datasets.
2.
3.
4.
5.
6.

http://www.oxforddictionaries.com
http://iate.europa.eu
http://aims.fao.org/vest-registry/vocabularies/agrovoc-multilingual-agricultural-thesaurus
https://www.wiktionary.org
http://linkeddata.org

7

fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsoda

resources published linked data multilingual others monolingual,
linked datasets close domains, become multilingual graph
navigable data. mapping linking step crucial approach. (iv) Translation
resources also considered subtype lexical resources also used
purposes obtaining multilingual data, source translations, means
mapping linking monolingual resources (for details machine translation see
section 2.1).
sense, sufficiently demonstrated quality coverage
lexical translation resources vital applications built top them,
performance applications depends directly them. paper included
special issue describes analyzes resources detail, also
evaluates coverage correctness context ontology mapping.
Effectiveness Automatic Translations Cross-lingual Ontology Mapping
Abu Helou Palmonari presents large-scale study effectiveness several
lexical translation resources purpose obtaining candidate matches
ontology concepts lexicalized different languages. Many works cross-language ontology
mapping rely multilingual translation resources obtain translation candidates
concept lexicalization source ontology, subsequently support selection
potential matches target ontology. paper evaluates machine translation service, GoogleTranslate7 , multilingual encyclopedic dictionary semantic network,
BabelNet8 , correctness coverage suggested translations mapping selection capabilities (word-disambiguation). evaluation based wordnets different
natural languages (Arabic, Italian, Slovene Spanish) manual alignments provided
wordnet English WordNet. perform three experiments taking
account various types lexical units (monosemous words, polysemous words, one-word
units multiple-word units), define specific measures (translation correctness, word
sense coverage, synset coverage synonym coverage) serve better evaluate
quality translations. results experiments provide insights coverage
correctness resources provide, effect combining results
resources, impact taking account translation directionality, differences
word categories mapped lexicalizations, among others.
results paper directly applied improve cross-language ontology
mapping task, also contribute speed development multilingual lexical
resources, general, help building truly multilingual Linked Open Data
Cloud, particular.
2.4 Cross-Language Dependency Parsing
natural language application relies language processing tools part-ofspeech taggers parsers confronted significant challenge scaling new
languages, languages may set tools. Building tools
desired language requires manual annotation sufficient amounts texts machine learning programs trained on. Annotation efforts undertaken
7. https://translate.google.com
8. http://babelnet.org

8

fiSpecial Issue Cross-Language Algorithms Applications

invaluable resources varying amounts texts created past decades
certain languages, example, Penn Treebank (Marcus, Marcinkiewicz, & Santorini,
1993), French Treebank (Abeille, 2003), NEGRA Treebank (Skut, Brants, & Uszkoreit,
1998), Prague Dependency Treebank (Hajic, Bohmova, Hajicova, & Vidova-Hladka, 2000).
However, task annotation effort time-consuming, expensive requires highly
skilled personnel.
late nineties, availability texts pair languages realized
parallel text corpora, researchers (Bangalore, 1998; Yarowsky, Ngai, & Wicentowski, 2001)
explored idea transferring annotations one language pair second
language pair order rapidly create annotated resource train language
processing tools second language. line research followed number
researchers projecting variety annotations language corpora using
annotated corpus bootstrap language processing tools target language.
particular interest projection annotations part-of-speech tags, phrase structure
annotations, dependency structure annotations.
Synthetic Treebanking Cross-lingual Dependency Parsing Tiedemann
Agic presents detailed discussion options transplant dependency trees
one language another order train dependency parsers target language.
paper introduces two options bootstrapping dependency parser language: model
transfer approach dependency parsing model transferred new language,
annotation transfer approach; paper advocates annotation transfer approach
creation synthetic treebanks. paper provides comprehensive analysis
various options creating synthetic treebank using parallel corpus including:
(i) projecting parsers output source text onto target text (ii) translating
existing high quality treebank source language target language. central idea
creating synthetic treebanks involves use statistical machine translation models,
phrase-based syntax-based, order translate texts source language
treebanks target language, project source language structures
translated target language sentence mediated word alignment information produced
translation process. challenges reconciling dependency structures
lexical translations richer one-to-one discussed length. paper reports
extensive parsing accuracy results parsers trained projected treebanks large
set language pairs, studies correlation quality translation model
quality target language parser.
2.5 Cross-Language Information Retrieval
increase number webpages multiple languages, search query
Web needs retrieve webpages authored languages language
query. Cross-language information retrieval technology intersection machine
translation information retrieval addresses challenge.
Cross-language information retrieval employed different strategies matching
query set multilingual documents: cognate-based matching (Montalvo, Martnez,
Casillas, & Fresno, 2007), matching query document translation, matching
mapping interlingua (Banchs & Costa-jussa, 2013). popular ap9

fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsoda

proaches query translation addressed either employing bilingual dictionaries (Hedlund, Airio, Keskustalo, Lehtokangas, Pirkola, & Jarvelin, 2004) using machine
translation (Kishida, 2008). Approaches combine techniques found
work work Zhang, Jones, Zhang (2008), example. quality query
translation indirectly observed final retrieval results. Kishida (2008) shows
regressive model ease search given query translation quality
explain 60% variation performance. Kettunen (2009) shows
long topics, correlations achieved retrieval results machine translation
metrics high (almost 90%) short topics correlation lower still clear
(almost 60%). Cross-language video retrieval, another related cross-language information
retrieval task, involves automatic speech recognition.
Utilisation Metadata Fields Query Expansion Cross-Lingual Search
User-Generated Internet Video Khwileh, Jones Ganguly presents one
first use-cases cross-language video retrieval social media content. paper focuses
challenges associated user generated informal content (i.e., noise, sparseness
metadata, content different lengths, informal language register, etc.) rather
professionally produced content. Noise errors propagate step
processing: speech recognition automatic translation query expansion.
authors use query translation approach bridge vocabulary gap
users query relevant content video application. Automatic translation
done using Google Translate retrieval expansion done Divergence
Randomness IR model. explore effectiveness three different sources
information: transcripts automatic speech recognition, video titles, descriptions.
Among three sources, leveraging video titles improves retrieval performance
experiments. addition, authors propose adaptive query expansion technique
automatically selects reliable source expansion based well established
query performance prediction technique. Results show approach robust
particular setting.
2.6 Cross-Language Knowledge Representation
Knowledge representation systems aim formalize representations world,
certain domain knowledge, way interpretable computers.
achieved identifying domain concepts relations exist among them,
representing information formal framework, e.g., using Description Logic
formalism. Knowledge representation systems intended language-independent
meaning representations. However, different representations domain knowledge co-exist, since design certain knowledge representation may
particular vision world, specific interests, certain application mind,
among factors.
One difficulties cross-language knowledge representation regards conceptual
differences observed across languages cultures. Indeed, certain representations prone reflect cultural particularities shared understood
way cultural systems. involves existence certain concepts
exist knowledge systems, relevant them, different
10

fiSpecial Issue Cross-Language Algorithms Applications

granularity levels representation concepts (Espinoza, Montiel-Ponsoda, & GomezPerez, 2009). reasons, cross-language knowledge representation challenge
current multilingual Web. Two main approaches followed obtain knowledge representation systems support several languages: (i) inclusion lexicalizations
several natural languages describe concepts relations formalized certain
knowledge representation system; (ii) existence several knowledge representation systems
whose concepts relations expressed different natural language,
linked mapped establish correspondences equivalences them. former
approach commonly applied internationalized standardized domains, whereas
latter typical culturally-influenced domains, termed Cimiano, Montiel-Ponsoda,
Buitelaar, Espinoza, Gmez-Prez (2010).
globalized interconnected world, cross-language information access increasing importance. several approaches cross-language document similarity
reported, including machine translation, probabilistic topic models, classification
matrix factorization, little previous work task linking documents across
languages refer events (Pouliquen, Steinberger, Ignat, Ksper, & Temnikova, 2004; Pouliquen, Steinberger, & Deguernel, 2008; Leban, Fortuna, Brank, & Grobelnik, 2014). actually difficult computationally expensive task, especially
many language pairs involved, small number real-life working
systems performing task exist. One example existing service providing crosslanguage cluster linking European Media Monitor (EMM) (Pouliquen et al., 2008).
special issue includes contribution tackling task, topic modeling
used represent knowledge expressed documents, linking task based
similarity-based entity-related features.
News Across Languages - Cross-Lingual Document Similarity Event
Tracking Rupnik, Muhic, Leban, Skraba, Fortuna Grobelnik addresses problem event tracking large multilingual stream, and, specifically, link
collections articles different languages refer event. authors
consider major languages also less-resourced languages. approach based representations documents analogous multilingual topics, valid multiple
languages. representations learned using Wikipedia training corpus.
used compute cross-language similarities documents regardless language. posterior cross-language cluster linking performed two steps. First,
speed-up process, similarity function used identify small set potential
linking candidates cluster. Then, final decision taken based supervised
classification model whose features include similarity-based entity-related features.
comprehensive experimental study, authors show canonical correlation analysis
best-performing method compute multilingual similarities. Moreover, show
similarity-based features greatly benefit additional semantic extraction-based
features.

Acknowledgements
authors want thank Dan Roth, Mark Sammons anonymous reviewer
useful comments suggestions previous versions document. work
11

fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsoda

supported 7th Framework Program European Commission
International Outgoing Fellowship Marie Curie Action (IMTraP-2011-29951), IntraEuropean Fellowship CrossLingMind-2011-300828 project LIDER (610782);
European Regional Development Fund (ERDF/FEDER); Spanish Ministerio de
Economa Competitividad SpeechTech4All project (TEC2012-38939-C03-02)
project 4V: volumen, velocidad, variedad validez en la gestion innovadora de
datos (TIN2013-46238-C4-2-R).

References
Abeille, A. (2003). Treebanks: Building Using Parsed Corpora. Springer.
Armentano-Oller, C., & Forcada, M. L. (2006). Open-source machine translation
small languages: Catalan aranese occitan. Workshop Strategies developing machine translation minority languages, pp. 5154.
Bahdanau, D., Cho, K., & Bengio, Y. (2015). Neural Machine Translation Jointly
Learning Align Translate. CoRR, abs/1409.0473.
Banchs, R., & Costa-jussa, M. R. (2013). Cross-Language Document Retrieval using
Non-linear Semantic Mapping. Applied Artificial Intelligence Journal, 27 (9), 781
802.
Banea, C., Mihalcea, R., & Wiebe, J. (2008). Bootstrapping Method Building Subjectivity Lexicons Languages Scarce Resources. Proceedings International Conference Linguistic Resources Evaluation (LREC), pp. 27642767,
Marrakech, Morocco.
Bangalore, S. (1998). Transplanting Supertags English Spanish. Proceedings
TAG+4 Workshop.
Bautin, M., Vijayarenu, L., & Skiena, S. (2008). International Sentiment Analysis News
Blogs. Proc. International Conference Weblogs Social Media,
pp. 1926, Seattle, U.S.A.
Bener, A., Minku, L., & Turhan, B. (Eds.). (2014). PROMISE 14: Proceedings 10th
International Conference Predictive Models Software Engineering, New York,
NY, USA. ACM.
Bikel, D., & Zitouni, I. (2012). Multilingual Natural Language Processing Applications:
Theory Practice. IBM Press.
Bojar, O., Chatterjee, R., Federmann, C., Haddow, B., Huck, M., Hokamp, C., Koehn, P.,
Logacheva, V., Monz, C., Negri, M., Post, M., Scarton, C., Specia, L., & Turchi, M.
(2015). Findings 2015 Workshop Statistical Machine Translation. Proceedings Tenth Workshop Statistical Machine Translation, pp. 146, Lisbon,
Portugal.
Chiang, D. (2007). Hierarchical Phrase-Based Translation. Computational Linguistics,
33 (2), 201228.
Cimiano, P., Montiel-Ponsoda, E., Buitelaar, P., Espinoza, M., & Gomez-Perez, A. (2010).
Note Ontology Localization. Journal Applied Ontology, 5(2), 127137.
12

fiSpecial Issue Cross-Language Algorithms Applications

Costa-jussa, M. R. (2015). Much Hybridization Machine Translation Need?.
Journal American Society Information Technology (JASIST), 6 (10), 2160
2165.
Costa-jussa, M. R., Banchs, R., Rapp, R., Lambert, P., Eberle, K., & Babych, B. (2013).
Workshop hybrid approaches translation: Overview developments. Proceedings Second Workshop Hybrid Approaches Translation, pp. 16, Sofia,
Bulgaria.
Eisele, A., Federmann, C., Uszkoreit, H., Saint-Amand, H., Kay, M., Jellinghaus, M., Hunsicker, S., Herrmann, T., & Chen, Y. (2008). Hybrid Architectures Multi-Engine
Machine Translation. Proceedings Translating Computer 30. ASLIB/IMI,
ASLIB.
Espinoza, M., Montiel-Ponsoda, E., & Gomez-Perez, A. (2009). Ontology Localization.
Proceedings 5th International Conference Knowledge Capture (KCAP09),
pp. 3340.
Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press.
Forner, P., Moller, H., Paredes, R., Rosso, P., & Stein, B. (2013). Information Access
Evaluation. Multilinguality, Multimodality, Visualization. Springer.
Gracia, J., MacCrae, J., & Vulcu, G. (Eds.). (2015). Proceedings Fourth Workshop
Multilingual Semantic Web. CEUR.
Hajic, J., Bohmova, A., Hajicova, E., & Vidova-Hladka, B. (2000). Prague Dependency Treebank: Three-Level Annotation Scenario. Abeille, A. (Ed.), Treebanks:
Building Using Parsed Corpora, pp. 103127. Amsterdam:Kluwer.
Hedlund, T., Airio, E., Keskustalo, H., Lehtokangas, R., Pirkola, A., & Jarvelin, K. (2004).
Dictionary-based Cross-Language Information Retrieval: Learning Experiences
CLEF 2000-2002. Information Retrieval, 7 (1), 99119.
Hutchins, W. J., & Sommers, H. L. (1992). Introduction Machine Translation, Vol.
362. Academic Press, New York.
Kettunen, K. (2009). Choosing Best MT Programs CLIR PurposesCan MT
Metrics Helpful?. Proceedings 31th European Conference IR Research
Advances Information Retrieval, pp. 706712.
Kim, S., Iglesias-Sucasas, M., & Viollier, V. (2013). FAO Geopolitical Ontology: Reference country-Based Information. Journal Agricultural Food Information,
14 (1).
Kishida, K. (2008). Prediction Performance Cross-language Information Retrieval Using Automatic Evaluation Translation. Library Information Science Research,
30 (2), 138144.
Koehn, P., Och, F., & Marcu, D. (2003). Statistical Phrase-Based Translation. Proceedings Conference North American Chapter Association
Computational Linguistics Human Language Technology (NAACL-HLT), pp. 48
54.
13

fiCosta-jussa, Bangalore, Lambert, Marquez & Montiel-Ponsoda

Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan,
B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., & Herbst, E.
(2007). Moses: Open Source Toolkit Statistical Machine Translation. Proceedings
45th Annual Meeting ACL Interactive Poster Demonstration
Sessions, ACL 07, pp. 177180.
Leban, G., Fortuna, B., Brank, J., & Grobelnik, M. (2014). Event registry: Learning
world events news. Proceedings Companion Publication 23rd
International Conference World Wide Web Companion (WWW Companion14),
pp. 107110, Seoul, Korea.
Libraries, I. S. o. A. (Ed.). (1996). Multilingual Glossary Art Librarians: English
Indexes Dutch, French, German, Italian, Spanish Swedish. De Gruyter.
Liu, B. (2012). Sentiment Analysis Opinion Mining. Synthesis Lectures Human
Language Technologies. Morgan & Claypool Publishers.
Lu, B., Tan, C., Cardie, C., & K. Tsou, B. (2011). Joint Bilingual Sentiment Classification
Unlabeled Parallel Corpora. Proceedings Annual Meeting Association Computational Linguistics: Human Language Technologies (ACL-HLT), pp.
320330, Portland, Oregon, USA.
Mahdisoltani, F., Biega, J., & Suchanek, F. M. (2015). YAGO3: Knowledge Base
Multilingual Wikipedias. Proceedings Conference Innovative Data Systems
Research (CIDR 2015).
Marcus, M. P., Marcinkiewicz, M. A., & Santorini, B. (1993). Building Large Annotated
Corpus English: Penn Treebank. Computational Linguistics, 19 (2), 313330.
Mihalcea, R., Banea, C., & Wiebe, J. (2007). Learning Multilingual Subjective Language via
Cross-Lingual Projections. Proceedings Annual Meeting Association
Computational Linguistics (ACL), pp. 976983, Prague, Czech Republic.
Montalvo, S., Martnez, R., Casillas, A., & Fresno, V. (2007). Multilingual News Clustering:
Feature Translation vs. Identification Cognate Named Entities. Pattern Recognition
Letters, 28 (16), 23052311.
Navigli, R., & Ponzetto, S. P. (2012). BabelNet: Automatic Construction, Evaluation Application Wide-Coverage Multilingual Semantic Network. Artificial
Intelligence, 193, 217250.
Pianta, E., Bentivogli, L., & Girardi, C. (2002). MultiWordNet: developing aligned
multilingual database. Proceedings Frist International Conference Global
WordNet.
Pouliquen, B., Steinberger, R., & Deguernel, O. (2008). Story Tracking: Linking Similar
News Time Across Languages. Proceedings COLING 2008 Workshop Multi-source Multilingual Information Extraction Summarization, pp.
4956, Manchester, UK.
Pouliquen, B., Steinberger, R., Ignat, C., Ksper, E., & Temnikova, I. (2004). Multilingual
cross-lingual news topic tracking. Proceedings International Conference
Computational Linguistics (COLING), pp. 959965, Geneva, Switzerland. COLING.
14

fiSpecial Issue Cross-Language Algorithms Applications

Prettenhofer, P., & Stein, B. (2010). Cross-Language Text Classification Using Structural
Correspondence Learning. Proceedings Annual Meeting Association
Computational Linguistics (ACL), pp. 11181127, Uppsala, Sweden.
Quirk, C., Menezes, A., & Cherry, C. (2005). Dependency Treelet Translation: Syntactically
Informed Phrasal SMT. Proceedings Annual Meeting Association
Computational Linguistics (ACL), pp. 271279.
Sanchez-Cartagena, V. M., Sanchez-Martnez, F., & Perez-Ortiz, J. A. (2011). Universitat dAlacant hybrid machine translation system WMT 2011. Proceedings
Sixth Workshop Statistical Machine Translation, pp. 457463, Edinburgh,
Scotland.
Sanchez-Martnez, F., & Forcada, M. L. (2009). Inferring Shallow-Transfer Machine Translation Rules Small Parallel Corpora. Journal Artificial Intelligence Research,
34, 605635.
Skut, W., Brants, T., & Uszkoreit, H. (1998). Linguistically Interpreted Corpus German
Newspaper Text. Proceedings ESSLLI Workshop Recent Advances
Corpus Annotation, Saarbrucken, Germany.
Smedt, J. D., & Vatanat, B. (2009). https://lists.w3.org/archives/public/public-eswthes/2010feb/att-0023/ontology.html..
Steinberger, R., Pouliquen, B., Widiger, A., Ignat, C., Erjavec, T., Tufis, D., & Varga, D.
(2006). JRC-Acquis: multilingual aligned parallel corpus 20+ languages.
Proceedings 5th International Conference Language Resources Evaluation (LREC2006), Genoa, Italy.
Suchanek, F., Riedel, S., Singh, S., & Pratim Talukdar, P. (Eds.)., (2012). AKBC-WEKEX
12: Proceedings Joint Workshop Automatic Knowledge Base Construction
Web-scale Knowledge Extraction, Stroudsburg, PA, USA. Association Computational Linguistics.
Vossen, P. (Ed.). (1998). EuroWordNet: Multilingual Database Lexical Semantic
Networks. Kluwer Academic Publishers, Norwell, MA, USA.
Wan, X. (2009). Co-Training Cross-Lingual Sentiment Classification. Proceedings
Joint Conference 47th Annual Meeting ACL 4th International Joint Conference Natural Language Processing AFNLP, pp. 235243,
Singapore.
Wei, B., & Pal, C. (2010). Cross Lingual Adaptation: Experiment Sentiment Classifications. Proceedings Annual Meeting Association Computational
Linguistics (ACL), pp. 258262, Uppsala, Sweden.
Yarowsky, D., Ngai, G., & Wicentowski, R. (2001). Inducing Multilingual Text Analysis
Tools via Robust Projection across Aligned Corpora. Proceedings First International Conference Human Language Technology Research, pp. 18, San Diego,
CA, USA.
Zhang, Y., Jones, G. J., & Zhang, K. (2008). Dublin City University CLEF 2007:
Cross-Language Speech Retrieval Experiments. Lecture Notes Computer Science,
703711.
15

fiJournal Artificial Intelligence Research 55 (2016) 685714

Submitted 11/15; published 03/16

Quadratization Roof Duality Markov Logic Networks
Roderick de Nijs

rsdenijs@tum.de

Institute Computer Science
Albrechtstr. 28, 49076 Osnabruck, Germany

Christian Landsiedel
Dirk Wollher
Martin Buss

christian.landsiedel@tum.de
dw@tum.de
mb@tum.de

Lehrstuhl fur Steuerungs- und Regelungstechnik
Theresienstr. 90, 80333 Munchen, Germany

Abstract
article discusses quadratization Markov Logic Networks, enables
efficient approximate MAP computation means maximum flows. procedure
relies pseudo-Boolean representation model, allows handling models
order. employed pseudo-Boolean representation used identify problems
guaranteed solvable low polynomial-time. Results common benchmark
problems show proposed approach finds optimal assignments variables
excellent computational time approximate solutions match quality ILPbased solvers.

1. Introduction
First-order probabilistic models promising paradigm overcoming limitations
classical first-order logic ability capture uncertainty often present
real-world problems. allow describing relational knowledge compactly,
size representation independent number objects domain.
knowledge models defined use parfactors (Poole, 2003),
templates representing large numbers factors graphical model describes
probability distribution possible world configurations. underlying graphical
model finite, possible ground first-order model perform inference
propositional level. reason, interest identify tractable cases propositional
problems first-order level well finding efficient approximate algorithms case
exact inference possible. Although computing typical inference queries
propositional model NP-Hard general, problems lend traditional
optimization approaches.
article deals models whose MAP problem represented optimization finite number binary variables. case Markov Logic Networks
(MLNs), parfactors weighted logic rules propositionalized
define Markov Random Field Boolean random variables. contributions
work stem great part representing parfactors using Boolean polynomials known
pseudo-Boolean functions. First, shown models certain parfactors,
2016 AI Access Foundation. rights reserved.

fide Nijs, Landsiedel, Wollherr, & Buss

MAP computation equivalent maximizing polar unimodular pseudo-Boolean
function, done low-polynomial time. allows identify MLNs
tractable MAP problem. Secondly, shown quadratization techniques pseudoBoolean functions generalized parfactors. One benefit transformations
enable using Quadratic Pseudo-Boolean Optimization (QPBO) ground
model, popular algorithm computer vision yet evaluated MLNs.
literature quadratizations reviewed, show previous work
discussing quadratization MLNs (Fierens, Kersting, Davis, Chen, & Mladenov, 2013)
equivalent particular choice pseudo-Boolean quadratization within quadratization framework parfactors. Based generalized roof duality (Kahl & Strandmark,
2012), new quadratization also introduced.
Experimental evaluation quadratization techniques combination QPBO
algorithm show large benefits performance attained real-world problems,
sophisticated quadratization techniques deliver better results
one employed Fierens et al. (2013).
combination quadratization QPBO shown competitive strategy
approaching MAP problem MLNs.
1.1 Outline
remainder article organized follows: rest section reviews
optimization methods available MLNs related work. Section 2, mathematical background, Markov Random Fields pseudo-Boolean functions presented,
along notation used article. Section 3 describes transformation parfactors pseudo-Boolean form, discusses cases identified tractable.
Section 4 presents general framework quadratization parfactors well comparison existing approach MLNs. Different quadratization strategies discussed.
Section 5, thorough computational evaluation approach performed datasets
literature well new problems. article concludes discussion
methods results Section 6.
1.2 Related Work
use first-order representations provides compact flexible way encode knowledge model design phase. trade-off convenience ground
models generally large treewidths, making MAP estimation common
queries NP-hard general. ground model may thousands millions
variables interactions, need fast memory-efficient optimization algorithms. section tries give overview recent prominent methods
inference models.
Various algorithms based heuristic random search used approximate
MAP solution. Alchemy system (Richardson & Domingos, 2006) implements
probabilistic hill-climbing algorithm named MaxWalkSat. lazy variant MaxWalkSat (Singla & Domingos, 2006b) also developed, which, splitting network
active inactive part, considerably reduces memory footprint algorithm.
Tuffy system (Niu, Re, Doan, & Shavlik, 2011) reformulated algorithm within
686

fiQuadratization Roof Duality Markov Logic Networks

relational database faster grounding additional scalability. also capable
detecting weakly connected components ground network, used parallelizing inference. extension parallelization inference MLNs based
partitioning network grounding found using minimum cuts, used
importance sampling inference framework (Beedkar, Del Corro, & Gemulla, 2013).
alternative approach based conversion ground factors linear constraints,
MAP problem formulated integer linear program (ILP). One
advantage formulation solution linear relaxation gives optimistic
estimate optimal cost. cost optimal solution lies
particular assignment optimistic estimate. make approach practical
larger problems, necessary reduce number linear constraints considered
solver. purpose, cutting plane algorithm MLNs presented (Riedel,
2009). iterative approach ignores constraints far satisfied previous intermediate solutions, includes become unsatisfied next
iteration. RockIt system (Noessner, Niepert, & Stuckenschmidt, 2013) shows
structurally similar constraints created first-order model aggregated
single one, presents parallelization scheme splits problem multiple
ILPs. combination techniques achieves excellent execution times. third type
algorithms perform queries propositionalized network, operate instead
potentially much smaller lifted network. Algorithms lifted MAP seen
significant advances recent years (Apsel & Brafman, 2012; Sarkhel, Venugopal, Singla, &
Gogate, 2014; Mittal, Goyal, Gogate, & Singla, 2014). However, approaches
applied efficiently problems specific types relations evidence,
discussed article.

article, used polynomial representation potential functions FOPMs. use polynomials representation Bayesian networks
suggested name network polynomials (Darwiche, 2003). polynomials
compiled arithmetic circuits used inference (Huang, Chavira, & Darwiche,
2006). idea carried first-order models create tractable subset Markov
Logic Networks (Domingos & Webb, 2012), whose network polynomial used make
queries efficient.

also used fact maximization pseudo-Boolean function
transformed maximization quadratic pseudo-Boolean function. pseudoBoolean functions interpreted factor graphs, transformation seen
reduction MAP problem general binary factor graph MAP problem
pairwise binary factor graph. thorough study reductions inference problems
general factor graphs restricted factor graph models presented Eaton
Ghahramani (2013). Close work also idea pairwise MLNs (Fierens
et al., 2013), relies transformation logical formulas compute quadratic
MLN equivalent original MLN higher order. detailed comparison
approach given Section 4.4.
687

fide Nijs, Landsiedel, Wollherr, & Buss

2. Preliminaries
section review concepts required understanding rest
article define useful notational conventions.
2.1 First-Order Logic Concepts
First-order logic makes statements objects world. object belongs
certain domain, domains seen semantic type object. References
objects made use terms. Terms either constants, refer specific
object, logical variables, represent range objects, functions, map
terms terms. Like objects, logical variables constants typed, meaning
represent objects certain domain. predicate represents relation
arguments. predicate applied specific terms atom. Atoms also called positive
literals logical negation negative literals.
first-order logic formula expression involving atoms, connected connectives (, , , , , =) quantifiers (,). Atoms formulas said ground
expression contained terms constants.
2.2 Notation Conventions
atom P (t1 , . . . , tn ) created applying predicate P arity n tuple terms
(t1 , . . . , tn ). arguments predicate typed, associated domain.
Ground atoms represented x literals (first-order ground) represented
u; negation written u. Logical variables (logvars) denoted capital letters
X, Y, Z. Vectors atoms, ground atoms, terms denoted a, x, t, respectively;
instance, first-order expression F involves multiple atoms written F (a) =
F (a1 , a2 , . . . , ). easily go Boolean real values, logical True False
reinterpreted represent 1 0 necessary. Replacing literal negated
equivalent, e.g., u 1 u, complementation operation. Also, superscript (),
B, used specifically refer positive negated literals, e.g., u(1) = u
u(0) = u. Observe u() = u(1) .
substitution terms set different terms 0 according mapping
0 denoted . substitution operation applied first-order expression f
written f .
ground substitution mapping C non-constant terms constant terms
C. Given set logical variables L, set possible ground substitutions satisfy
constraints C written gr(L : C). case C = , number ground
substitutions size Cartesian product domains L.
2.3 Markov Random Fields
Markov Random Field undirected graphical model defined
P (x) =

N
1
(xi ),
Z


688

fiQuadratization Roof Duality Markov Logic Networks

factors nonnegative functions, xi tuples binary random variables,

normalization constant. Normally exponential functions, quantity
PZ
N
log (xi ) referred energy function, whose minimum defines MAP
state P (x).
2.4 First-Order Probabilistic Models
First-order probabilistic models way expressing probability distributions,
MRFs, large degree structure. models conveniently specified using
parfactors (Poole, 2003). Parfactors composed (parametrized) potential function
constraints valid ground substitutions parameters. parfactor g
represented tuple (C, (a)), C set constraints (a) potential
parfactor, real-valued function first-order atoms a. potential function
given table associates value 2n truth states atoms (Poole,
2003; de Salvo Braz, 2007) function. ground substitution defines factor
clique potential Markov Random Field Bayes Network. logical variables
appear parfactor denoted L, LV (a) used specifically refer logical
variables appear atoms. set valid ground substitutions parfactor
logical variables L constraints C (discussed Section 2.4.2) denoted gr(L : C).
purposes, set parfactors G defines Markov Random Field log-linear form
sum groundings


X
X
1
P (x) = exp
g (ag ) ,
(1)
Z
gG gr(Lg :Cg )

x contains propositional variables arise grounding first-order atoms.
Markov Logic Networks first-order probabilistic models use set weighted
first-order logic rules specify Markov Random Field. rules generally specified
manually capture available knowledge intuitions domain. weights
capture relative importance rules set manually learnt data.
Markov Logic Networks easily described using parfactors. this, weighted
first-order logic rule Markov Logic Network associated parfactor empty
constraint set potential function takes value weight satisfying
assignments rule, 0 otherwise. models, (1) assigns high probabilities
world states satisfy many ground parfactors positive weight
negative weight.
common types queries (1) marginal probabilities
propositional variables (P (x1 ), P (x2 ), . . .) probable configuration unknown variables x = arg maxx P (x), also referred Maximum-a-Posteriori (MAP)
assignment.
order able ground models, take similar assumptions original
MLN formulation, namely domains assumed finite, unique names domain closure. practice also allow assume logical atoms function-free.
However, although potential functions MLNs {0, w}-valued, w R, formulation
allows potentials take different values R every assignment.
689

fide Nijs, Landsiedel, Wollherr, & Buss

2.4.1 Evidence
model may conditioned truth value certain ground atoms. symbols
PT PF represent sets ground atoms predicate P known True
False, respectively. instance, P (o) PT denotes ground atom P (o)
known True. case evidence available, PT PF empty. set
ground atoms predicate P unknown truth value represented PU . PU
empty, predicate fully observed.
2.4.2 Constraints
ground substitutions logical variables parfactor subject constraints.
representation, substitution constraint parfactor conjunction
set individual constraints associated parfactor. Disjunctions constraints
expressed multiple parfactors representation. Constraints used
following cases:
1. Expressing (in)equality relation logical variables. two logical variables
X, belonging domain, X = X 6= respectively restrict
ground substitutions map X either different
constants.
2. Expressing ground substitutions must map logical variables elements
set objects P denoted C (t, P ), P generally one evidence
groups PT , PF , PU predicate P .
Constraints principle also expressed potential function using fully observed
auxiliary predicates, normally done MLN formalism. instance, X 6=
enforced taking conjunction potential parfactor new fully
observed atom AreDifferent(X, ). Similarly, constraint C (X, PF ) represented
taking conjunction potential new atom BelongsToFalseEvidenceP (X).
However, expressing relationships form constraints simplifies discussion
Section 3.
Example 1. friends similar smoking behavior described parfactor
g potential function F riends(X, ) Smokes(X) Smokes(Y ) constraint X 6=
. people domain {A, B}, ground substitutions associated parfactor
gr((X, ) : {X 6= }) = {(X, ) (A, B), (X, ) (B, A)}.
2.5 Pseudo-Boolean Functions
function f : Bn R called pseudo-Boolean function. Let x = [x1 , x2 , . . . , xn ]
(1)
(1)
vector n binary variables. Consider also set literals, L := {x1 , . . . , xn ,
(0)
(0)
x1 , . . . , xn }. pseudo-Boolean function terms expressed literals L
coefficients ei R, = 0, . . . , n written
(x) = e0 + e1 m1 (x1 ) + e2 m2 (x2 ) + . . . + en mn (xn )
690

(2)

fiQuadratization Roof Duality Markov Logic Networks

mi (xi ) monomials literals L,
mi (xi ) :=



(

)

xi,ji,j .

j

ei 0 0 < n (2), pseudo-Boolean function said posiform.
many possible posiform representations pseudo-Boolean function.
standard polynomials, order (or degree) pseudo-Boolean function
term highest degree. pseudo-Boolean function expressed set
positive literals, i.e. = 1 i, called multi-linear polynomial representation
X
X
X
eij xi xj +
eijk xi xj xk + . . . ,
ei xi +
(x) = e0 +


1i<jn

1i<j<kn

ei , eij , eijk , . . . R. representation unique always obtained
another representation eliminating negated literals using complementation x(0) =
1 x(1) . Conversely, posiform always obtained pseudo-Boolean function.
Complementing literal term ei mi (xi ) ei < 0 produces two new terms, one
order e0i > 0 one one order lower e00i < 0. applying
procedure starting highest-order terms, negative terms eliminated.

3. Parfactors Pseudo-Boolean Potentials
chapter describes potential functions parfactors described manipulated terms pseudo-Boolean functions, equivalent model representations
translated pseudo-Boolean formulation. representation allows easy recognition cases inference ground probabilistic model tractable.
detailed Section 3.3.
3.1 First-Order Pseudo-Boolean Functions
Potential functions parfactors defined first-order atoms. Therefore, call
pseudo-Boolean function first-order atoms first-order pseudo-Boolean function.
Substitutions applied individual term, pseudo-Boolean function
(a) form (2) substitution
(a) = e0 + e1 m1 (a1 ) + e2 m2 (a2 ) + + en mn (an ).
Remark 1. notions term order different pseudo-Boolean functions
first-order logic. However, expect context generally clear enough avoid
confusions.
3.2 Conversion Potential Representations
general, potential function n variables given form table converted
pseudo-Boolean form simple technique (Boros & Hammer, 2002), creates one
term 2n configurations nonzero coefficient table.
number terms reduced converting pseudo-Boolean function
691

fide Nijs, Landsiedel, Wollherr, & Buss

multi-linear polynomial. However, potentials Markov Logic Networks given
first-order logic sentences expressed conjunctive normal form. allows
translated directly pseudo-Boolean form. Namely, clause U
replaced
_
^
u=1
u,
(3)
uU

uU

conjunctions replaced products. typical case single-clause
formula u1 u2 . . . un takes value w satisfied, equivalent compact pseudoBoolean representation w wu1 u2 . . . un . course, inverse procedure used
transform potential form back logic representation.
Markov Random Field results grounding model like (1) represented
P (x) =

1
exp (T (x)) ,
Z

energy function obtained summing groundings parfactors
G, normalization constant Z unknown.
3.3 Tractable Classes
tractability MAP estimate MLN analyzed considering classes
tractable pseudo-Boolean functions. First, discuss classes polynomial-time optimizable
pseudo-Boolean functions general case.
3.3.1 Classes Tractable Pseudo-Boolean Functions
section gives overview relevant tractable classes pseudo-Boolean functions.
Figure 1 illustrates relations function classes.
Supermodular functions satisfy f (x1 ) + f (x2 ) f (x1 x2 ) + f (x1 x2 ), elementwise AND/OR operations binary x1 , x2 . functions maximizable
strong polynomial time (Orlin, 2009). However, recognition supermodularity
polynomials order 4 co-NP-Complete (Gallo & Simeone, 1989).
Supermodular functions expressible quadratic functions written maximizations auxiliary variables quadratic supermodular functions. instance,
x1 x2 x3 = maxw x1 w + x2 w + x3 w 2w, w B. functions certain
structure, expressibility recognized efficiently (Zivny & Jeavons, 2008). However, unknown whether recognition expressible functions easier
general supermodularity recognition problem (Zivny, Cohen, & Jeavons, 2009). One
may try obtain equivalent quadratic supermodular function solving linear
program (Ramalingam, Russell, Ladicky, & Torr, 2011).
Polar functions (Billionnet & Minoux, 1985) supermodular functions
term positive coefficient composed positive negative literals,
e.g., x1 x2 x3 + 2x1 x2 + x2 x4 x5 . form strict subset set expressible
functions orders 4.
692

fiQuadratization Roof Duality Markov Logic Networks

Unimodular functions functions converted polar functions switching subset variables. instance, f (x1 , x2 , x3 ) = x1 x2 x3 + 2x1 x2
2 using switched
associated polar function g(x1 , x2 , x3 ) = x1 x2 x3 + 2x1 x
variable x2 = 1 x2 . Undoing switching operation maximizer polar
function gives solution original problem. Unimodular functions recognizable polynomial time (Crama, 1989). switching operations used
make function supermodular (but polar) permutable supermodular function (Schlesinger, 2007), mainly interesting optimization functions
nonbinary discrete variables.
Remark 2. f supermodular function, f submodular, thus results
maximization supermodular function minimization submodular function
interchangeable. overview given section, keep context maximization
supermodular functions, since reflects many original publications.
Permutable Supermodular
Supermodular

Permutable Supermodular

Expressible

Unimodular
Expressible

Polar

Polar
Supermodular

Unimodular

Figure 1: Relations classes functions. Left: pseudo-Boolean functions
order. Right: third-order (cubic) case. Shaded regions optimizable
maximum flows.

main interest expressible supermodular functions quadratic supermodular
functions optimized computing maximum flow O(n3 ), considerably
faster O(n6 ) complexity general supermodular maximization. However,
process transforming expressible supermodular function equivalent quadratic
representation introduces additional variables. Consequently, k auxiliary variables
introduced, true complexity O((n + k)3 ).
Note although recognition general supermodularity expressibility
hard problems, classes closed conical combinations, sums
supermodular functions also supermodular.
3.3.2 Tractable Parfactor Models
Although inference NP-hard general, possible guarantee certain inference tasks tractable restricting expressiveness formalism potential functions (Domingos & Webb, 2012). Translating results classes
pseudo-Boolean functions above, possible easily identify models tractable MAP
inference. this, following result used
Proposition 1. parfactors expressible potentials, maximization ground
model expressed maximization supermodular quadratic function.
693

fide Nijs, Landsiedel, Wollherr, & Buss

Proof. follows facts a) definition, expressible functions written
maximization supermodular quadratic pseudo-Boolean function b) sums
supermodular functions also supermodular.
Consequently, potentials converted supermodular expressible pseudoBoolean function, model optimized computing maximum flow.
result applies potential, {0, w}-valued potentials used MLNs.
hence possible consider potentials defined table taking 2n
distinct values. However, flexibility makes harder enforce expressibility design
level. classes logic rules used MLNs guaranteed expressible,
used recognize design MLNs tractable MAP problem.
3.3.3 Polar MLNs
Let = {a1 , a2 , . . . , } set first order atoms, J {1, 2, . . . , n} B.
logic potentials form
^ () ^ (1)
ai
aj
.
(4)
iI

jJ

transformed equivalent polar posiform following cases
J = . case (4) single conjunction pseudo-Boolean form
()
ai .
iI

J 6= . conjunctions conflict. case disjunction
replaced sum without changing underlying truth table expression.
equivalent polar posiform
() (1)
ai +
aj
.
iI

jJ

J = |J|= 1. easy see
^ ()
^
(1)
ai aj
=
iI

()

ai

(1)

aj

,

iI{j}

employed transform expression previous case.
MLNs parfactors polar tractable MAP problem
computed maximum flow.
Example 2. MLN potential positive weight potential (P (X) Q(Y ) (Z))
(R(X) (Z)) form (4) = 1, = {P (X), Q(Y ), (Z)}, J = {R(X), (Z)}.
J = {T (Z)} 6= , equivalent polar potential P (X)Q(Y )T (Z) +
R(X) (Z).
694

fiQuadratization Roof Duality Markov Logic Networks

3.3.4 Unimodular MLNs
Unimodular functions also solved efficiently converted polar functions. Finding variables need switched obtain polar function performed
polynomial-time (Crama, 1989). recognition procedure may efficient performed first-order level, would require describe switches ground atoms
grounding. show switches ground variables also
represented compactly first-order level, makes possible decide whether
model unimodular without analyzing ground model. Define switching operation
replacing literal l = P (t)() expression P (t)(1) , P new predicate.
new literal switched predicate interpreted referring switched ground atoms.
However, representation may become inconsistent ground atoms represented
switched non-switched atom intersect, ground atom switch
treated independent variables.
Definition 1 (Shattering). (de Salvo Braz, 2007) set parfactors G shattered
groundings every pair atoms appearing parfactors G either identical
disjoint.
model shattered partitioning parfactors (de Salvo Braz, 2007).
Definition 2 (Consistent switch). shattered model switched atoms consistent
every pair atoms identical groundings, either neither atoms
switched.
shattered model, inconsistencies avoided ensuring switching
operation preserves consistency. Namely, starting shattered model
switched atoms, switching atoms groundings ensures consistency
requirement fulfilled times. switching sets parfactors polar,
ground model unimodular polar representation directly available.
Example 3. Consider model G parfactors (, P (X)Q(Y ) (Z)) (,
Q(X)T (Y )). model shattered obtain switched model G0 parfactors (, P (X)Q(Y ) (Z)) (, Q(X) (Y )). switching terms
parfactors either positive negative literals assume nonnegative
values, ground model polar pseudo-Boolean function.
Unfortunately, shattering condition strong enough partitioning parfactors guarantee unimodular model recognized. example, model
single parfactor ({X 6= }, F (X, )F (Y, X)) shattered shown
switching set makes polar, made polar switching
atoms. finer partitioning parfactors one given shattering may allow
represent desired switching set. However, find partitioning general
case unknown.
3.4 Non-tractable Case
following discuss optimization pseudo-Boolean functions fall
within described tractable classes. observed multiple linear programming relaxations quadratic pseudo-Boolean problems optimum (Boros &
695

fide Nijs, Landsiedel, Wollherr, & Buss

Hammer, 2002), known roof dual bound. generally optimistic bound
true optimum problem, linear programming relaxes integrality constraint
variables. roof dual bound also obtained efficiently solving
maximum flow problem specially constructed network. minimization setting,
network represents tightest submodular relaxation original quadratic function (Kahl & Strandmark, 2012). addition lower bound, solution relaxed
problem gives persistencies. Persistencies assignments subset variables
form part least one optimal solution. persistencies found full set
variables, form minimizer problem. Otherwise, problem simplified
fixing persistencies value produce smaller problem preserves
minimum.
preprocessing step reduces size problem solves completely,
combined optimization method. approach known Quadratic Pseudo-Boolean Optimization (QPBO) (Kolmogorov & Rother, 2007) computer
vision literature.
Furthermore, network used compute initial roof dual bound
persistencies used search additional persistencies approximate
configuration remaining variables means probing (Boros, Hammer, &
Tavares, 2006) improving (Rother, Kolmogorov, Lempitsky, & Szummer, 2007) techniques. Probing heuristically chooses variable x residual problem recomputes
maximum flow assumptions x = 0 x = 1. Analyzing value
remaining variables assumptions, may possible find additional
persistencies improve lower bound. improve method fixes subset variables given assignment efficiently searches configurations remaining
variables improve cost respect original assignment. procedure
executed multiple times guaranteed decrease quality approximate
solution.
described techniques applicable quadratic problems. next section
concerned models described purely pairwise interactions
variables. models, use quadratization techniques employed,
enables use QPBO algorithm extensions problems. Also,
connection approach alternative based generalized roof duality
shown Section 4.2.

4. Quadratization
section quadratization pseudo-Boolean functions reviewed
shown methods applied parfactor models. quadratization procedure
also known order reduction.
4.1 Quadratization Pseudo-Boolean Function
quadratization pseudo-Boolean function (x) new function (x, w)
(x) = min (x, w),
w

696

(5)

fiQuadratization Roof Duality Markov Logic Networks

(x, w) quadratic pseudo-Boolean function defined auxiliary slack variables
w.
(x), always exists (x, w) satisfying (5) (Rosenberg, 1975; Ishikawa,
2011). Furthermore, (x) belongs expressible set submodular functions described
Zivny et al. (2009), submodular (x, w) also found. quadratization,
problem finding minx (x) replaced quadratic problem minx,w (x, w).
Example 4. function f (x1 , x2 , x3 ) = x1 x2 x3 quadratization
min (x1 , x2 , x3 , w) = min x1 w x2 w x3 w + 2w.
w

w

verified minimizing (x1 , x2 , x3 , w) respect w assignment
(x1 , x2 , x3 ) B3 verifying evaluates 1 (1, 1, 1) 0 everywhere else.
minx1 ,x2 ,x3 f (x1 , x2 , x3 ) conveniently computed quadratic optimization minx1 ,x2 ,x3 ,w (x1 , x2 , x3 , w). quadratization obtained ISH technique
discussed following.
general, quadratization necessarily determined whole function once;
instead, single multiple terms specific algebraic form replaced step
equivalent quadratic representation. substitutions introduces
minimization independent slack variables, minimizations distributed
whole expression, final quadratization joint optimization
slack variables.
given (x) many possible ways obtain quadratization. practice,
quadratizations a) slack variables, b) positive quadratic
thus non-submodular terms c) easily computable. Condition a) important
quadratization needs applied many times, e.g., many higher order terms;
b) number magnitude experimentally known correlate complexity
resulting optimization (Kolmogorov & Rother, 2007; Gallagher, Batra, & Parikh, 2011);
c) satisfy conditions, finding quadratization may become complex.
instance, finding quadratization smallest number slack variables NPcomplete problem approaches (Boros & Hammer, 2002).
4.2 Quadratization Techniques
Quadratization techniques pseudo-Boolean functions rely identifying subexpressions
match template quadratic form known. Potentials known
expressible quadratized using submodularity-preserving functions presented
Zivny Jeavons (2008). following review existing techniques valid
pseudo-Boolean function introduce new quadratization.
4.2.1 General Quadratization Individual Terms (ISH)
technique used quadratize individual higher-order term. general
formulas given Ishikawa (2011), restricted cases given Kolmogorov
Zabin (2004) Freedman Drineas (2005).
697

fide Nijs, Landsiedel, Wollherr, & Buss

binary variables x1 , . . . , xd negative coefficient (a < 0),
(1 )

ax1

(d )

xd

()

= min aw{S1
wB

(d 1)}.

Example 4 illustrates case cubic function. positive coefficient (a > 0)
quadratization
( )
( )
ax1 1 xd

=a

nd
X

min

w1 ,...,wnd B

()

wi (ci,d (S1

()

+ 2i) 1) + aS2

i=1

definitions
()
S1

=


X

( )
xi ,

()
S2

i=1

=

d1 X
d1
X

( ) ( )
xi xj j

i=1 j=i+1

(

1,
d1
, ci,d =
nd =
2
2,


()

()

(S1 1)
= 1
2
odd = nd
otherwise

reduction negative term always performed single slack variable,
results quadratic submodular function. positive terms, number slack
variables nd proportional 21 d. general, resulting function submodular
quadratization creates positive quadratic terms. However, terms
involve slack variables, may canceled existing quadratic terms opposite
sign. fact, seen quadratization always submodularity-preserving
cubic functions (Zivny & Jeavons, 2008). hand, original function
submodular, quadratization may produce non-submodular terms worse
experimental results methods.
4.2.2 Asymmetric Quadratization (ASM)
(Gallagher et al., 2011) asymmetric quadratization term ax1 x2 x3 > 0
given
ax1 x2 x3 = min a(w x2 w x3 w + x1 w + x2 x3 ).
w

obtained transforming ax1 x2 x3 ax2 x3 ax1 x2 x3 using ISH
method. Observe left-hand side equality symmetric, right-hand
side not. Reordering variables thus creates three different quadratizations. Also,
right-hand side two non-submodular terms (compared three ISH
method), one involve slack variable w. existing term
negative coefficient variables non-submodular term without w,
combined may cancel out, leaving single non-submodular term. Asymmetric
quadratizations created terms order using similar procedure.
possibility eliminating non-submodular terms motivates search combination quadratizations optimizes certain cost representing quality
quadratization. shown total magnitude non-submodular terms
good cost function (Gallagher et al., 2011). call ASM procedure minimizing
698

fiQuadratization Roof Duality Markov Logic Networks

quantity finding quadratization expression, term select
one asymmetric ISH quadratic forms. Note that, propositional
level, number terms may large, resulting large optimization problem
choice quadratization per term.
4.2.3 Preprocessing Positive Terms (FIX)
(Fix, Gruber, Boros, & Zabih, 2011) approach seen preprocessing method
avoid quadratization ISH positive terms. transforms multiple positive higher-order
terms common subset variables negative higher-order terms. summarize
procedure case common subset variables consists single variable.
Consider set terms contain common variable x1 , H
positive coefficient H > 0. Then, holds assignment binary variables
x1 , . . . , x n
!
X
HS

H


jH

xj = min

X

w{0,1}

H

X

x1 w +

HS

HS

H



xj

jH\{1}

X
HS

H w



xj .

jH\{1}

Observe last sum terms order left-hand side
negative sign, positive terms lower order. transformation
repeatedly applied positive higher-order terms eliminated.
point ISH quadratization negative terms applied, introducing one additional slack
variable per term. method decide common variable perform
transformation, x1 thus selected arbitrarily.
4.2.4 Generalized Roof Duality (GRD)
new quadratization method based generalized roof duality theory (Kolmogorov, 2012) practical implementation (Kahl & Strandmark, 2012). nonsubmodular, higher-order function (x), approach finds submodular relaxation (x, y)

(x) = (x, x)
(x, y) = (y, x)

(symmetry)

submodular expressible.

(6)

constraints, solution relaxed problem provides lower bound
persistent assignments variables, similar roof dual relaxation quadratic
function. restricting search relaxations expressible functions, result
optimized solving maximum flow problem. following proposition presents
link submodular relaxations computed GRD quadratizations.
proposition used compute quadratization function roof dual
bound equivalent GRD bound function.
Proposition 2. Let (x, y) relaxation (x) satisfies (6). exists
quadratic (x, w) s.t. a) roofdual((x, w)) minx,y (x, y), b) minw (x, w) = (x).
699

fide Nijs, Landsiedel, Wollherr, & Buss

Proof. Let (x, y, w) submodular quadratization (x, y). Define
1
0 (x, y, w, v) = ((x, y, w) + (y, x, v)).
2
easy check minw,v 0 (x, y, w, v) = (x, y). Also, (x, y, w) quadratic submodular, easy see (y, x, v) must also submodular (negative
quadratic terms multi-linear form). Consequently, 0 (x, y, w, v) submodular.
a) Let (x, w) = 0 (x, x, w, w) verify
0 (x, y, w, v) = 0 (y, x, v, w)

Thus, 0 (x, y, w, v) relaxation (x, w) conditions (6). However, roof dual
bound known larger submodular relaxations (Kahl & Strandmark,
2012),
roofdual((x, w)) min 0 (x, y, w, v) = min (x, y).
x,y,w,v

x,y

b) follows
1
min (x, w) = min 0 (x, x, w, w) = min ((x, x, w) + (x, x, w)) = (x, x) = (x).
w 2
w
w

Condition b) Proposition 2 ensures (x) quadratization (x), condition
a) ensures roof dual bound quadratization least relaxation
(x, y).
Example 5. cubic term x1 x2 x3 compute third-order submodular relaxation using (Kahl & Strandmark, 2012). ISH quadratization submodularitypreserving cubic functions, method used make relaxation quadratic,
procedure Proposition 2 results
1
(w1 x1 + w1 x2 w1 x3 w1 w2 x1 w2 x2 + w2 x3
2
+ w2 + 2x1 x2 x1 x2 + x3 + 1).

x1 x2 x3 = min

w1 ,w2

relaxation obtained using generalized roof duality theory expressed
quadratization function. practice, method allows us use GRD approach
design quadratizations specially suited potentials hand.
4.3 Quadratization MLNs
quadratization MLN expresses probability distribution using parfactors
quadratic pseudo-Boolean potentials optimization slack atoms.
Definition 3 (First-order quadratization). Let (a) pseudo-Boolean potential applied
first-order atoms a, construct l = (l1 , l2 , . . . , l|LV (a)| ), li LV (a) arbitrary
ordering. quadratization , (x) = minw (x, w1 , w2 , . . . , wk )
700

fiQuadratization Roof Duality Markov Logic Networks

define first-order quadratization (a) = minb (a, b1 , b2 , . . . , bk ), bi , =
1, 2, . . . , k slack atoms. Slack atoms created using new predicate Bi arity
|LV (a)| every slack variable wi bi = Bi (l1 , l2 , . . . , l|LV (a)| ), = 1, 2, . . . , k.
definition allows compactly represent individual quadratizations many
ground potentials similar structure. Also, particular form original
quadratic representation assumed, quadratization technique used. instance, applying ISH quadratization first-order potential P (X)Q(X, Z)Q(Y, Z),
Example 4, results quadratization P (X)B1 (Y, X, Z) Q(X, Z)B1 (Y, X, Z)
Q(Y, Z)B1 (Y, X, Z) + 2B1 (Y, X, Z).
obtain convenient quadratization whole model following remarks
useful.
Remark 3. quadratization performed new first-order slack predicates,
different parfactors never grounded expressions ground slack atom.
Remark 4. Slack predicates applied logical variables appearing expression
quadratized. Consequently, two ground substitutions , 0 create ground
slack atoms also define minimizations function: b = b0 (a, b) =
(a, b)0 .
definition first-order quadratizations, express quadratization
pseudo-Boolean probabilistic first-order logic model parfactors G


X
X
1
P (x) = exp
(ag )
Z
gG gr(Lg :Cg )


X
X
1
= exp
(ag )
Z
gG gr(Lg :Cg )


X
X
1
= exp
min g (ag , bg )
bg
Z
gG gr(Lg :Cg )


X
X
1
= exp
min g (ag , bg )
(7)
bg
Z
gG gr(Lg :Cg )


X
X
1
= exp min
g (ag , bg ) ,
(8)
w
Z
gG gr(Lg :Cg )

optionally expressed maximization


P (x) =

1
exp max
w
Z


X

X

gG gr(Lg :Cg )

701

g (ag , bg ) ,

fide Nijs, Landsiedel, Wollherr, & Buss

w contains ground slack variables {bg | gr(Lg : Cg ), g G}. step
(7) (8) follows sets variables produced different grounding substitutions either identical disjoint. different parfactors, minimizations
always independent, implied Remark 3. single parfactor g two groundings minbg (ag , bg ) minbg 0 (ag 0 , bg 0 ), either bg 6= bg 0 bg = bg 0 .
first case, combine two independent minimizations minbg ,bg 0 (ag , bg ) +
(ag 0 , bg 0 ). second case, Remark 4 implies minbg 2(ag , bg ).
4.4 Relation Pairwise MLNs
concept pairwise MLNs (Fierens et al., 2013) one presented
produce new model quadratic parfactors, cost introducing additional
optimization slack variables. pairwise MLN approach shows MLN
transformed max-equivalent MLN quadratic clauses, concept
max-equivalence similar (5). review approach, first assume
clauses three literals. case, clause three literals
rewritten new clauses positive normal form, conjunctions three
positive literals. form, transformation suggested splits conjunction
three positive literals new rules involve two atoms use new
auxiliary slack atom. third-order case, procedure described special
case approach. Namely, show equivalent using ISH quadratization
Section 4.2.1 MLN pseudo-Boolean form.
Consider MLN rule weight > 0 parfactor (, (, P (t1 )Q(t2 )R(t3 ))).
pairwise MLN approach would transform parfactors quadratic potential functions
{(, P (t1 ) B(t4 )), (, Q(t2 ) B(t4 )), (, R(t3 ) B(t4 )), (2, B(t4 ))}.

(9)

parfactor also represented pseudo-Boolean form (, (P (t1 )Q(t2 )R(t3 ))).
allows use ISH quadratization presented Example 4
P (t1 )Q(t2 )R(t3 ) = (P (t1 )Q(t2 )R(t3 ))
= min (P (t1 )B(t4 ) R(t3 )B(t4 ) Q(t2 )B(t4 ) + 2B(t4 ))
B(t4 )

= max (P (t1 )B(t4 ) + R(t3 )B(t4 ) + Q(t2 )B(t4 ) 2B(t4 )).
B(t4 )

observed terms potential function correspond one-toone ones created pairwise MLN approach (9). fact, decomposing
parfactor new ones quadratic terms would give form
pairwise MLN. analogous procedure case < 0 shows
special case three literals, pairwise MLNs replicated pseudo-Boolean form
expressing formulas multi-linear form using ISH quadratization
third-order case.
Although ISH quadratization often recommended computer vision literature
case > 0 (in maximization setting), known lead
tightest roof dual relaxations < 0 (Fix et al., 2011; Gallagher et al., 2011).
702

fiQuadratization Roof Duality Markov Logic Networks

experimental section extend observations problems domain probabilistic
logic verify quadratization methods lead improvements inference quality.
model contains clauses n > 3 literals, pairwise MLN approach suggests
use auxiliary atoms recursively transform clauses new ones n 1 literals,
clause three literals. instance, clause n = 4 literals
transformed two clauses three literals, one infinite weight.
point, procedure applied third-order clauses, creating
total 3 slack atoms.
contrast, approach works pseudo-Boolean representation potential.
case, additional preprocessing first transform clause third-order
required, quadratizations pseudo-Boolean functions order. Recall
clauses compactly represented pseudo-Boolean form using (3). Consequently,

ISH method used quadratize clauses n literals either one n2 slack
atoms, depending sign whether optimization formulated maximization
minimization. particular case clause four literals single
slack atom required. slack atom many propositionalizations,
reducing number slack atoms creates large difference number propositional
slack variables. Finally, approach create hard rules, lead bad
conditioning optimization algorithms.
4.5 Normalization Parfactors Quadratization
Consider parfactor (, P (X)P (Y )Q(Z) + P (Z)Q(Z)) true evidence predicate
Q, QT 6= . Although potential cubic general, particular ground
substitutions quadratic. case groundings
Q(Z) QT , cases potential simplifies P (X)P (Y ) + P (Z). potential also quadratic ground substitutions X = ,
becomes P (X)Q(Z) + P (Z)P (Z). Finally, groundings = Z cubic
potential P (X)P (Z)Q(Z) + P (Z)Q(Z), factorizes (P (X) + 1)P (Z)Q(Z).
groundings, quadratization exploiting structure potential could computed.
examples highlight structure potentials change particular
groundings, might even require slack variables become quadratic.
motivates putting parfactors form potentials structure
groundings. effect normalization reduction number slack
variables possibility compute quadratizations tailored different form
potentials. achieved combining two types splittings parfactors,
creating equivalent first-order representation makes different forms potentials
explicit first-order level.
4.5.1 Splitting Atoms
first preprocessing method simplifies parfactor incorporating evidence atom
P (t). proceeds replacing parfactor three new ones, additional
constraint C (t, PT ), C (t, PF ), C (t, PU ). together constraints cover possible ground substitutions atom, new parfactors produce groundings
original one, ground model changed. Importantly, potentials
703

fide Nijs, Landsiedel, Wollherr, & Buss

parfactors constraints C (t, PT ), C (t, PF ) simplified replacing atom
P (t) True False, respectively.
Definition 4 (Fixed atoms). atom predicate P fixed C (t, PT ), C (t, PF )
C (t, PU ) constraint set C.
parfactor n atoms, fixing atoms create 3n new parfactors.
practice, number smaller a) potential may become 0 constant
simplification (Shavlik & Natarajan, 2009) b) sets PT , PF , PU may
empty groundings associated it.
4.5.2 Splitting Logical Variables
Atoms constructed predicate may ground ground atom
specific grounding substitutions. example, parfactor (, P (X)P (Ann)) represents
quadratic potential ground substitutions X except = {X Ann},
becomes linear potential. simplifications identified grounding
observing form atoms predicate may unify, splitting
parfactor accordingly. example creates parfactors (X 6= Ann, P (X)P (Ann))
(, P (Ann)). Splitting logical variables repeated unification
atoms longer possible. ensures valid ground substitution parfactor
maps atoms parfactor distinct ground atoms.
Example 6. Consider parfactor (, P (X, )P (X, Z)Q(X) S(Z)) assume
predicates arguments domain D. knowledge available P , i.e., (PT = ,
PF = , PU = D), evidence groundings Q, i.e., (QT D,
QF = ,QU = \ QT ), full knowledge groundings S, i.e., (ST D, SF D,
SU = ).
start splitting parfactor atom S(Z), fully observed. case
S(Z) constrained True cancels potential left out. SU = ,
case constraint C (X, SU ) also ignored. S(X) constrained
False, potential simplifies P (X, )P (X, Z)Q(X). Proceeding similarly
atoms, remains one parfactor simplified potential additional constraints
C (Z, SF ), C (X, QU ), C ((X, ), PU ), C ((X, Z), PU ).
fixing atoms, observe two atoms constructed predicate P
produce ground substitutions = Z. Splitting condition produces two
new parfactors, potential simplifies P (X, )Q(X) case = Z.
steps, original parfactor transformed two new parfactors:
(a) =P (X, )P (X, Z)Q(X)
C ={C (Z, SF ), C (X, QU ), C ((X, ), PU ), C ((X, Z), PU ), 6= Z}

(a) =P (X, )Q(X)
C ={C (Y, SF ), C (X, QU ), C ((X, ), PU )}

704

fiQuadratization Roof Duality Markov Logic Networks

Note preprocessing step, potential function gone quartic
cubic function. Computing quadratization potential original form
substituting possible groundings produces |D|3 ground slack atoms. contrast,
splitting variables results fewer ground substitutions parfactor, since
curtailed constraints (and number ground slack variables).
4.6 Benefits Limitations First-Order Quadratization
Except ISH method, introduced methods compute quadratization considering
interactions terms expression. considering interactions,
methods create quadratization whose roof dual relaxation tighter, producing better
bounds persistencies.
However, methods also need additional processing problem.
processing performed propositional model, may become large. instance,
Gallagher et al. (2011) suggested implement ASM method constructing second
Markov Random Field variables represent possible quadratizations
term. Also, approach based generalized roof duality needs solve auxiliary
linear program find best submodular relaxation. FIX approach solve
optimization, needs perform multiple transformation steps model,
clear order perform them.
Instead, MLNs, general procedure presented Section 4 used obtain
quadratic MLN applying quadratization techniques individual parfactors. advantage procedure computational cost obtaining quadratization
independent size ground problem. Furthermore, parfactors provide
compact description structure ground model, exploited
sophisticated quadratization methods. auxiliary problems required
quadratization methods become extremely small solved individual parfactors,
still produce good quadratic potential groundings parfactor.
hand, parfactor representation model unique, thus
resulting quadratic MLN may depend manipulations performed parfactors.
fact, shattering model may give representative template structure
ground problem. work, merge parfactors constraints
preprocessing step, take steps partition combine
parfactors. Section 5 evaluate quadratization methods separately.
However, practice methods may combined restrictions, possible
combination different methods may give best results.
4.7 Quadratize-Solve-Simplify-Repeat (QSSR)
QPBO algorithm applied general quadratic minimization problem,
variables persistency found. discussed Section 3.4,
cases probe improve procedures used increase number persistencies
compute approximate solution. However, computational cost make
inefficient problems large number unsolved variables. quadratized models,
mitigated observing many slack variables longer necessary
computing persistencies given roof duality. fixing variable
705

fide Nijs, Landsiedel, Wollherr, & Buss

x persistent value higher-order problem simplifies cancels terms containing x. Thus, constructing new problem incorporates information
solved variables much smaller number slack variables, used obtain
additional persistencies. procedure quadratizing model first-order level,
solving problem partially QPBO algorithm simplifying higher-order
problem obtained persistencies repeated additional persistencies
found. denote iterative computational procedure Quadratize-Solve-SimplifyRepeat (QSSR). using algorithm experiments, stop problem
solved first simplification, additional iterations generally produce
additional persistencies comparison probing. Kahl Strandmark (2012) used
similar procedure iteratively solving fixing variables context computing
generalized roof dual bound.

5. Computational Experiments
presented quadratization methods evaluated impact performance
QPBO algorithm extensions. performance QPBO-based inference
also evaluated problems quadratization required. Finally compare
overall pipeline existing inference engines.
5.1 Datasets
evaluate approach various standard MLNs datasets well additional
problems. characteristics problems summarized Table 1. first set
datasets similar ones employed evaluation state-of-the-art engines
Tuffy (Niu et al., 2011) RockIt (Noessner et al., 2013). link prediction problem
UWCSE dataset (LP) tries find relations faculty members students.
relational classification (RC) Cora dataset determines category research
papers. information extraction (IE) problem models obtain dataset records
parsed sources. webKB dataset used predict university department
website belongs, given hyperlink relations contained words (KB). entity resolution (ER) problem Cora dataset obtained Alchemy website.
goal problem identify citations referring paper. trained
model available problem, trained Alchemy using first five
available splits evaluation (Singla & Domingos, 2006a). Friends smokers (F&S)
common test model social network friendship relations, smoking habits
cancer occurrences. Evidence generated described Singla Domingos (2008)
domain size 200 persons. F&S problem relatively simple, additional
problem weights formulas negated also considered (-F&S).
order gain broader insight performance inference algorithms
higher-order problems, created two additional third-order problems. first one
based KB problem webKB dataset mentioned (KB3).
original KB inference problem uses words contained page contents well link
structure infer page categories, third-order problem webKB dataset created
querying class page, also jointly inferring links
page, solely word tokens appearing page. Learning performed
706

fiQuadratization Roof Duality Markov Logic Networks

Alchemy, size problem reduced inference performed
atoms True ground truth number randomly sampled
atoms.
second new third-order problem image denoising (ID) model, tries
restore noisy binary image. rules indicating observed value
pixel correspond denoised value two rules indicating groups three
horizontally vertically neighbouring pixels take value. easy see
associated MAP problem rules fall within described cases MLNs
whose rules converted polar pseudo-Boolean functions described Section 3.3.2,
thus solved exactly. unary rules given weight 1.0. ensure
terms smoothing rules cancel out, rule pixels given weight
0.35 rule pixels 0.3. 90 90 pixels random binary image used
evidence, pixel 50% chance off.
5.2 Engines
compare approach MAP-inference solvers Alchemy, Tuffy RockIt.
Alchemy original solver MLNs and, contrast engines,
use relational database ground model, lead long grounding times.
Alchemy Tuffy optimize ground model using MaxWalkSAT, stochastic search
technique made scale well large problems. RockIt uses ILP solver
exploits symmetries model reduce number constraints.
number constraints may large, takes iterative approach
constraints violated current solution added solver.
make sure problem solved implementations,
preprocessing required. First, formulas existential clauses ignored formulas converted conjunctive normal form. Then, Tuffy internally transforms
formulas negative weight approximate formula positive weight, apply transformation. Unfortunately, transformation applied
higher-order problems, reduces order formula. Lastly, ER KB3
problems use method compactly specify ground atoms query, assumes
query atoms False. query variables, also known canopies (Singla
& Domingos, 2006a), used eliminate large number uninteresting variables,
created using cheap distance metric (McCallum, Nigam, & Ungar, 2000). neither Tuffy RockIt support input format, given extensive
list False evidence atoms instead.
5.3 Results Quadratic Problems
quadratic problems literature, analyze performance QPBO additional persistencies computed probing extension described Section 3.4. Table 2
shows QPBO algorithm gives persistent solution variables, even
provides exact solution KB problem. probe procedure also solves IE
problem exactly, still leaves unsolved variables RC LP problems.
general, inference times problems extremely short.
707

fide Nijs, Landsiedel, Wollherr, & Buss

5.4 Comparison Quadratization Methods

higher-order problems performance described quadratization
methods evaluated. includes pairwise MLN approach, equivalent
ISH quadratization problems cubic potentials. potentials parfactors
expressed multi-linear polynomial quadratizing model.

First evaluate number persistencies obtained different
problems Table 3. expected, submodularity, ID problem completely
solved methods. Friends Smokers creates non-submodular terms,
also solved exactly methods. remaining problems solved
methods, cases small number variables fixed. general,
observed methods aware terms potential
produce better results ISH, applies fixed transformation. final probing
step computationally expensive, may significantly increase number
solved variables, even solve problems exactly. noted step
important even approximate solution variables subsequently obtained
using improve method. Otherwise, number persistencies small, improve
method needs operate model potentially many variables, also needs
optimize slack variables stemming remaining higher-order terms.

Table 1: Summary characteristics described datasets associated ground
networks grounded higher-order form multi-linear representation. Trivially satisfied dissatisfied factors ignored.
IE

KB

RC

LP

ID

F&S -F&S KB3

ER

Formulas
1024
106
15
24
4
6
6
66
1331
Domains
4
3
3
8
1
1
1
3
5
Query Predicates
2
1
1
1
1
3
3
2
4
Observed Predicates
16
2
3
21
2
0
0
1
6
Ground atoms
336670 9079 9650
4624 8100 40180 40180 8190 10948
Factors
351001 31283 58485 161806 55800 127982 127982 22627 910670
Higher order factors
0
0
0
0 15840 32220 32220 6736 424580

708

fiQuadratization Roof Duality Markov Logic Networks

Table 3: Percentage variables solved. Step 1) Initial QPBO result 2) QPBO result
QSSR simplification 3) Probe. Inference time seconds step parentheses. () complete

Step

ISH

FIX

ASM

GRD

ID

1

100.0 (0.01)

100.0 (0.01)

100.0 (0.01)

100.0 (0.01)

F&S

1
2

100.0 (0.02)

100.0 (0.82)

100.0 (0.86)

99.6 (1.0)
100.0 (0.0)

-F&S

1
2
3

19.4 (1.79)
19.4 (1.61)


19.4
19.4


(0.9)
(0.78)

99.6 (0.42)
99.6 (0.02)
99.6 (2.66)

99.6 (1.11)
99.6 (0.02)
99.6 (2.54)

KB3

1
2
3

55.0 (0.06)
56.5 (0.04)
65.8 (19.42)

82.4 (0.03)
86.8 (0.01)
100.0 (0.1)

82.3 (0.02)
86.6 (0.01)
100.0 (0.05)

60.6 (0.08)
62.7 (0.04)
96.7 (5.59)

ER

1
2
3

92.1 (0.46)
92.3 (0.03)
92.8 (7.6)

91.9 (1.04)
92.3 (0.04)
93.0 (162.36)

95.0 (0.47)
95.0 (0.02)
95.3 (5.57)

95.4 (1.49)
96.1 (0.03)
96.6 (7.08)

5.5 Approximate Inference
also compared quality approximate solutions engines
total running times. problems formulated minimizations, solutions
engines evaluated ground model. Table 4 observed
quadratic problems, engines achieve optimal costs, known
optimality guarantee given QPBO Table 2 small MIP gap
used RockIt. exception LP problem, solver used RockIt
problems obtaining tight bound, Tuffy QPBO+I provide better solutions.
higher-order problems, ASM quadratization achieves best cost
lowest computation time cases. Using GRD reduction performs slightly worse,
possibly quadratization improve step needs executed

Table 2: Percentage persistencies given QPBO algorithm using
probing technique different quadratic problems.

Persistencies
Persistencies (probe)
Qpbo time (s)
Probe time (s)

IE

KB

RC

LP

99.87
100
0.030
0.150

100

90.30
90.30
0.006
0.021

85.58
86.22
0.069
4.800

709

0.002

fide Nijs, Landsiedel, Wollherr, & Buss

variables. Tuffy perform well higher order problems, possibly
internal transformation uses approximation original formula.

noted computation times affected multiple factors. Whereas
Alchemy, Tuffy approach make clear distinction grounding inference, RockIt uses cutting plane algorithm incrementally grounds factors
satisfied current solution, leads large speedups many factors
easily satisfied solution largely homogeneous. hand, ID
problem example approach produces considerably longer running times.
Another factor ability specify evidence form canopies, allows
relational database execute queries grounding efficiently.

Table 4: Resulting cost different engines various quadratic higher-order problems. Alchemy Tuffy run increasing number flips
significant advances made. RockIt run relative gaps 1 10n ,
n = 9, 8, . . . convergence achieved within hour. compared
method using ASM GRD quadratization higher-order
problems, using improve residual problem advances made
20 iterations. Total running times seconds parenthesis. (*) Guaranteed
optimal cost persistencies () ground within 1 hour.

Alchemy

Tuffy

RockIt

QPBO+I

IE
KB
RC
LP


-111113.5(162)

-480.8
(119)

-4511.6
(17)
-111274.1 (115)
-4031.7
(17)
-686.3
(424)

-4511.6
-111312.4*
-4031.8
-507.7

ID
F&S
-F&S
KB3
ER

1772.7 (442)
-3.8
(159)
-182338.7 (47)
21.1
(543)
-10739.5 (551)

1784.2
(25)
-4.2*
(3)
-191856.9(3230)
-1045.3
(308)
-14128.9 (433)

-1003.8* (244)
-4.2*
(5)
-185267.3
(8)
-1492.8
(256)
-15271.3 (1902)

(19)
(27)
(11)
(13)

-4511.6
(22)
-111312.4*
(6)
-4031.8
(9)
-732.6
(9)
ASM+QPBO+I
-1003.8*
(5)
-4.2*
(6)
-193715.3
(12)
-1484.4
(57)
-15430.7
(101)

GRD+QPBO+I
-1003.8*
(6)
-4.2*
(9)
-193715.3
(14)
-1476.9
(101)
-15430.5
(113)

Figure 2, evolution cost ER problem running time
improve shown different quadratizations. problem, methods converge
solution similar costs, convergence much faster cases ISH
GRD quadratizations used.
710

fiQuadratization Roof Duality Markov Logic Networks

15100

ASM
FIX
GRD
ISH

15150
15200

Cost

15250
15300
15350
15400
15450
1

2

3

4

Improve (s)

Figure 2: Cost higher-order ER model function time spent improve method, using different quadratization techniques. Improve starts
solving one iteration original problem removing redundant slacks,
described Section 4.7.

6. Conclusions Future Work
article discussed use pseudo-Boolean functions, quadratization techniques, MAP inference methods based roof duality MLNs. shown
quadratization method pseudo-Boolean functions employed MLNs firstorder level, generalizing previously existing approach. enables use quadratization methods exploit structure problem without need solve possibly
large auxiliary optimization problem. Various quadratization approaches adapted
work first-order models, including novel approach leverages connection
generalized roof duality theory.
Additionally, best knowledge, first work discusses
recognition (super-)submodularity expressibility MLNs. particular, allows
guarantee expressibility restricted set MLNs. Although class potentials
guaranteed expressible using submodular quadratic functions limited,
potentials seen wide applications computer vision.
presented techniques extensively evaluated problems literature
well various additional problems. higher-order problems, choice quadratization
approach shown important factor quality results. methods
exploit first-order representation problem often enabled QPBO solve
larger part problem perform better approximate inference. cases, large
parts problems could solved exactly, approximate solutions matched
improved quality solvers. optimization times problems
observed small, often much shorter rest pipeline,
optimize. total, timings show competitive results relation
state-of-the-art inference engines.
711

fide Nijs, Landsiedel, Wollherr, & Buss

various paths future work. work explored performance
improved partitioning model presented shattering techniques,
using manipulations parfactors. Also, possibility obtain better
approximate solutions using move making algorithms (Lempitsky, Rother, Roth, & Blake,
2010) tested, algorithms may leverage first-order representation
finding good moves. additional interesting aspect consider integrate approach cutting plane techniques lifting techniques, expected give
significant performance benefit problems large number factors.

Acknowledgments
authors also wish thank anonymous reviewers valuable comments
helpful suggestions. research leading results partly received funding European Research Council European Unions Seventh Framework
Programme (FP/2007-2013) / ERC Grant Agreement no. 26787, Project SHRINE,
Technische Universitat Munchen - Institute Advanced Study (www.tum-ias.de), funded
German Excellence Initiative.

References
Apsel, U., & Brafman, R. I. (2012). Exploiting uniform assignments first-order MPE.
Proc. Conf. Uncertainty Artificial Intelligence, pp. 7483.
Beedkar, K., Del Corro, L., & Gemulla, R. (2013). Fully parallel inference markov logic
networks. 15th GI-Symposium Database Systems Business, Technology
Web, Magdeburg, Germany. Bonner Kollen.
Billionnet, A., & Minoux, M. (1985). Maximizing supermodular pseudoboolean function:
polynomial algorithm supermodular cubic functions. Discrete Applied Mathematics, 12 (1), 111.
Boros, E., Hammer, P. L., & Tavares, G. (2006). Preprocessing unconstrained quadratic
binary optimization. Tech. rep., Rutgers Center Operations Research.
Boros, E., & Hammer, P. (2002). Pseudo-boolean optimization. Discrete Applied Mathematics, 123 (1), 155225.
Crama, Y. (1989). Recognition problems special classes polynomials 01 variables.
Mathematical Programming, 44 (1-3), 139155.
Darwiche, A. (2003). differential approach inference bayesian networks. J.
ACM, 50 (3), 280305.
de Salvo Braz, R. (2007). Lifted First-Order Probabilistic Inference. Ph.D. thesis, University
Illinois Urbana-Champaign.
Domingos, P., & Webb, W. A. (2012). tractable first-order probabilistic logic.. Proc.
Conf. Artificial Intelligence. AAAI.
Eaton, F., & Ghahramani, Z. (2013). Model reductions inference: Generality pairwise,
binary, planar factor graphs. Neural computation, 25 (5), 12131260.
712

fiQuadratization Roof Duality Markov Logic Networks

Fierens, D., Kersting, K., Davis, J., Chen, J., & Mladenov, M. (2013). Pairwise markov
logic. Inductive Logic Programming, pp. 5873. Springer.
Fix, A., Gruber, A., Boros, E., & Zabih, R. (2011). graph cut algorithm higher-order
Markov random fields. Int. Conf. Computer Vision, pp. 10201027. IEEE.
Freedman, D., & Drineas, P. (2005). Energy minimization via graph cuts: Settling
possible. Conf. Computer Vision Pattern Recognition, pp. 939946. IEEE
Computer Society.
Gallagher, A. C., Batra, D., & Parikh, D. (2011). Inference order reduction markov
random fields. Proc. Int. Conf. Computer Vision Pattern Recognition, pp.
18571864. IEEE.
Gallo, G., & Simeone, B. (1989). supermodular knapsack problem. Mathematical
Programming, 45 (1-3), 295309.
Huang, J., Chavira, M., & Darwiche, A. (2006). Solving MAP exactly searching
compiled arithmetic circuits. Proc. Conf. Artificial Intelligence, Vol. 6, pp. 37.
AAAI.
Ishikawa, H. (2011). Transformation general binary MRF minimization first-order
case. Trans. Pattern Analysis Machine Intelligence, 33 (6), 12341249.
Kahl, F., & Strandmark, P. (2012). Generalized roof duality. Discrete Applied Mathematics,
160 (16-17), 24192434.
Kolmogorov, V., & Rother, C. (2007). Minimizing nonsubmodular functions graph
cuts-a review. Trans. Pattern Analysis Machine Intelligence, 29 (7), 12741279.
Kolmogorov, V. (2012). Generalized roof duality bisubmodular functions. Discrete
Applied Mathematics, 160 (4-5), 416426.
Kolmogorov, V., & Zabin, R. (2004). energy functions minimized via graph
cuts?. Trans. Pattern Analysis Machine Intelligence, 26 (2), 147159.
Lempitsky, V., Rother, C., Roth, S., & Blake, A. (2010). Fusion moves markov random
field optimization. Trans. Pattern Analysis Machine Intelligence, 32 (8), 1392
1405.
McCallum, A., Nigam, K., & Ungar, L. H. (2000). Efficient clustering high-dimensional
data sets application reference matching. Proc. Int. Conf. Knowledge
Discovery Data Mining, pp. 169178. ACM.
Mittal, H., Goyal, P., Gogate, V. G., & Singla, P. (2014). New rules domain independent
lifted MAP inference. Proc. Advances Neural Information Processing Systems,
pp. 649657.
Niu, F., Re, C., Doan, A., & Shavlik, J. (2011). Tuffy: Scaling statistical inference
markov logic networks using RDBMS. Proc. VLDB Endowment, 4 (6), 373384.
Noessner, J., Niepert, M., & Stuckenschmidt, H. (2013). Rockit: Exploiting parallelism
symmetry MAP inference statistical relational models.. AAAI Workshop:
Statistical Relational Artificial Intelligence.
713

fide Nijs, Landsiedel, Wollherr, & Buss

Orlin, J. B. (2009). faster strongly polynomial time algorithm submodular function
minimization. Mathematical Programming, 118 (2), 237251.
Poole, D. (2003). First-order probabilistic inference. Gottlob, G., & Walsh, T. (Eds.),
Int. Joint Conf. Artificial Intelligence, pp. 985991. Morgan Kaufmann.
Ramalingam, S., Russell, C., Ladicky, L., & Torr, P. H. S. (2011). Efficient minimization
higher order submodular functions using monotonic boolean functions. CoRR,
abs/1109.2304.
Richardson, M., & Domingos, P. (2006). Markov logic networks. Machine learning, 62 (1),
107136.
Riedel, S. (2009). Cutting plane map inference markov logic. Int. Workshop Statistical
Relational Learning.
Rosenberg, I. (1975). Reduction bivalent maximization quadratic case. Cahiers
du Centre detudes de Recherche Operationnelle, 17, 7174.
Rother, C., Kolmogorov, V., Lempitsky, V., & Szummer, M. (2007). Optimizing binary
MRFs via extended roof duality. Proc. Conf. Computer Vision Pattern Recognition, pp. 18. IEEE.
Sarkhel, S., Venugopal, D., Singla, P., & Gogate, V. (2014). Lifted MAP inference
markov logic networks. Proc. Int. Conf. Artificial Intelligence Statistics, pp.
859867.
Schlesinger, D. (2007). Exact solution permuted submodular minsum problems.
Energy Minimization Methods Computer Vision Pattern Recognition, pp. 28
38. Springer.
Shavlik, J. W., & Natarajan, S. (2009). Speeding inference markov logic networks
preprocessing reduce size resulting grounded network. Proc. Int.
Joint Conf. Artificial Intelligence, pp. 19511956.
Singla, P., & Domingos, P. (2008). Lifted first-order belief propagation. Proc. National
Conf. Artificial Intelligence, Vol. 2, pp. 10941099.
Singla, P., & Domingos, P. (2006a). Entity resolution markov logic. Proc. Int. Conf.
Data Mining, pp. 572582. IEEE.
Singla, P., & Domingos, P. (2006b). Memory-efficient inference relational domains.
Proc. National Conf. Artificial Intelligence, Vol. 21, pp. 488493.
Zivny, S., Cohen, D. A., & Jeavons, P. G. (2009). expressive power binary submodular
functions. Discrete Applied Mathematics, 157 (15), 33473358.
Zivny, S., & Jeavons, P. G. (2008). submodular functions expressible using
binary submodular functions?. Tech. rep. CS-RR-08-08, University Oxford.

714

fiJournal Artificial Intelligence Research 55 (2016) 565-602

Submitted 08/15; published 03/16

Finding Strategyproof Social Choice Functions
via SAT Solving
Felix Brandt

brandtf@in.tum.de

Technical University Munich (TUM)
Munich, Germany

Christian Geist

geist@in.tum.de

Technical University Munich (TUM)
Munich, Germany

Abstract
promising direction computational social choice address research problems
using computer-aided proving techniques. particular SAT solvers, approach
shown viable proving classic impossibility theorems
Arrows Theorem also finding new impossibilities context preference extensions. paper, demonstrate computer-aided techniques also
applied improve understanding strategyproof irresolute social choice functions.
functions, however, require evolved encoding otherwise search space
rapidly becomes much large. contribution two-fold: present efficient
encoding translating problems SAT leverage encoding prove new
results strategyproofness respect Kellys Fishburns preference extensions. example, show Pareto-optimal majoritarian social choice function
satisfies Fishburn-strategyproofness. Furthermore, explain human-readable proofs
results extracted minimal unsatisfiable cores corresponding SAT
formulas.

1. Introduction
Ever since famous Four Color Problem solved using computer-assisted approach,
clear computers contribute significantly verifying existing
also finding proving new results. Due rigorous axiomatic foundation, social
choice theory appears field computer-aided theorem proving particularly
promising line research. Perhaps best known result context stems Tang
Lin (2009), reduce well-known impossibility results Arrows theorem
finite instances, checked satisfiability (SAT) solver (see, e.g., Biere,
Heule, van Maaren, & Walsh, 2009). Geist Endriss (2011) able extend
method fully-automatic search algorithm impossibility theorems context
preference relations sets alternatives. paper, apply techniques
improve understanding strategyproofness context set-valued, so-called
irresolute, social choice functions. types problems, however, complex
require evolved encoding otherwise search space rapidly becomes large.
Table 1 illustrates quickly number involved objects grows that, result,
exhaustive search doomed fail.
c
2016
AI Access Foundation. rights reserved.

fiBrandt & Geist

Alternatives
Choice sets
Tournaments
Canonical tournaments
Majoritarian SCFs

4

5

6

7

15
64
4
50, 625

31
1,024
12
1018

63
32,768
56
10101

127
2 106
456
10959

Table 1: Number objects involved problems irresolute majoritarian SCFs

contribution two-fold. one hand, provide extended framework
SAT-based computer-aided theorem proving techniques statements social choice
theory related research areas. Despite complexity, framework allows
extraction human-readable proofs, eliminates need extensive (and difficult)
verification underlying techniques. hand, rather reproducing
existing results, solve open problems, independent interest,
context irresolute strategyproof social choice functions. results unlikely
found without help computers, strengthens importance
approach.
results obtained computer-aided theorem proving already found attention
social choice community (Chatterjee & Sen, 2014) similar techniques proven
quite effective problems economics, too. Examples ongoing work
Frechette, Newman, Leyton-Brown (2016) SAT solvers used
development execution FCCs upcoming reverse spectrum auction, recent results
Drummond, Perrault, Bacchus (2015) solve stable matching problems via SAT
solving, well work Tang Lin (2011) apply SAT solving discover classes
two-player games unique pure Nash equilibrium payoffs. another recent paper,
Caminati, Kerber, Lange, Rowat (2015) verified combinatorial Vickrey auctions via
higher-order theorem provers. respect, approach bears similarities automated
mechanism design (see, e.g., Conitzer & Sandholm, 2002), desirable properties
encoded mechanisms computed fit specific problem instances. also
body work logical formalizations important theorems social choice theory,
prominently, Arrows Theorem (see, e.g., Nipkow, 2009; Grandi & Endriss, 2013; Cina &
Endriss, 2015), directed towards formalizing verifying existing
results.
Given universality SAT-based method ease adaptation (e.g., testing similar conjectures minimal effort simply replacing altering axioms),
expect similar techniques applicable open problems social
choice theory related research areas future. Results different variants
no-show paradox (Brandl, Brandt, Geist, & Hofbauer, 2015; Brandt, Geist, & Peters,
2016c) support hypothesis. noted, however, thatat least currentlyan
expert user programmer required operate systems. interesting question
remains whether possible develop automatic proof assistant allows researchers quickly test hypotheses small domains without giving much generality
efficiency.
566

fiFinding Strategyproof Social Choice Functions via SAT Solving

Let us turn towards social choice theoretic results. Formally, social choice
function (SCF) defined function maps individual preferences set alternatives set socially most-preferred alternatives. SCF strategyproof agent
obtain preferred outcome misrepresenting preferences. well-known
Gibbard-Satterthwaite theorem that, restricting attention SCFs always return single alternative, trivial SCFs strategyproof. assumption
single-valuedness, however, criticized unreasonably restrictive (see,
e.g., Gardenfors, 1976; Kelly, 1977; Taylor, 2005; Barbera, 2010). proper definition
strategyproofness general setting irresolute SCFs requires specification
preferences sets alternatives. Rather asking agents specify
preferences sets (which requires exponential space would bound various
rationality constraints), typically assumed preferences single alternatives
extended preferences sets. course, various ways extend
preferences sets (see, e.g., Gardenfors, 1979; Duggan & Schwartz, 2000; Taylor, 2005),
leads different class strategyproof SCFs. function yields
preference relation subsets alternatives given preference relation single
alternatives called set extension preference extension. paper, focus two
set extensions attributed Kelly (1977) Fishburn (1972),1 shown
arise uniquely natural assumptions (Gardenfors, 1979; Erdamar & Sanver, 2009;
see also Section 2.2 paper).
strategyproofness Kellys extension (henceforth Kelly-strategyproofness)
known rather restrictive condition (Kelly, 1977; Barbera, 1977; Nehring, 2000),
SCFs Pareto rule, omninomination rule, top cycle, uncovered
set, minimal covering set, bipartisan set shown Kelly-strategyproof
(Brandt, 2015). Interestingly, prominent SCFs majoritarian, i.e.,
based pairwise majority relation ordered respect set inclusion. results suggest bipartisan set may finest Kelly-strategyproof
majoritarian SCF. paper, show case automatically generating Kelly-strategyproof SCF strictly contained bipartisan set. Brandt
(2015) furthermore showed that, mild condition, Kelly-strategyproofness carries
coarsenings SCF. Thus, finding inclusion-minimal Kelly-strategyproof SCFs
particular interest. address problem automating search functions
small domains report findings.
Existing results suggest demanding notion Fishburn-strategyproofness
may satisfied rather indiscriminating SCFs top cycle (Feldman, 1979;
Brandt & Brill, 2011; Sanver & Zwicker, 2012).2 Using computer-aided proving technique, able confirm suspicion proving that, within domain majoritarian SCFs, Fishburn-strategyproofness incompatible Pareto-optimality. order
achieve impossibility, manually prove novel characterization Pareto-optimal ma1. Gardenfors (1979) attributed extension Fishburn weakest extension satisfies
certain set axioms proposed Fishburn (1972). authors, however, refer Gardenfors
extension, term reserve extension due Gardenfors (1976) himself.
2. negative result Ching Zhou (2002) uses Fishburns extension much stronger notion
strategyproofness.

567

fiBrandt & Geist

joritarian SCFs induction step, allows us generalize computer-generated
impossibility larger numbers alternatives.
paper structured follows. Section 2, present general mathematical framework use throughout paper introduce new condition
tournament-strategyproofness, show equivalent standard strategyproofness majoritarian SCFs. Section 3, describe computer-aided proving method
explain encode main questions paper SAT problems. also
describe optimization techniques features approach. Section 4, report main findingsan impossibility possibility resultand discuss possible
extensions limits. Section 5, novel approach proof extraction
computer-generated results presented. provide human-readable proof main
result verified without help computers. Finally, Section 6 wrap
work give outlook research directions.

2. Mathematical Framework Strategyproofness
section, provide terminology notation required results introduce notions strategyproofness majoritarian SCFs allow us abstract away
reference preference profiles.
2.1 Social Choice Functions
Let N = {1, . . . , n} set least three voters preferences finite set
alternatives. convenience, assume n odd, entails pairwise
majority relation antisymmetric. preferences voter N represented
complete, antisymmetric, transitive preference relation Ri A. interpretation
(x, y) Ri , usually denoted x Ri y, voter values alternative x least much
alternative y. set preference relations denoted R(A).
set preference profiles, i.e., finite vectors preference relations, given R (A).
typical element R (A) R = (R1 , . . . , Rn ). accordance conventional
notation, write Pi strict part Ri , i.e., x Pi x Ri Ri x. Note
difference Ri Pi Ri reflexive Pi not. order
improve readability, write Ri : x, y, z shorthand x Pi Pi z. preference
profile, weight ordered pair alternatives wR (x, y) defined majority
margin |{i N | x Ri y}| |{i N | Ri x}|.
central objects study social choice functions, i.e., functions map
individual preferences voters nonempty set socially preferred alternatives.
Definition 1. social choice function (SCF) function f : R (A) 2A \ .
SCF resolute |f (R)| = 1 R R (A), otherwise irresolute.
restrict attention majoritarian SCFs, tournament solutions,
defined using majority relation. majority relation RM preference profile R
relation defined
(x, y) RM wR (x, y) 0,
568

fiFinding Strategyproof Social Choice Functions via SAT Solving

alternatives x, A. SCF f said majoritarian neutral3
0 .
outcome depends majority relation, i.e., f (R) = f (R0 ) whenever RM = RM
before, write PM strict part RM , i.e., PM b RM b b RM a.
alternative x called Condorcet winner R x PM A\{x}.
words, Condorcet winner best alternative respect majority relation
seems natural majoritarian SCFs select Condorcet winner. Unfortunately,
clear-cut winners exist general variety so-called Condorcet extensions,
i.e., SCFs uniquely return Condorcet winner whenever one exists differ
treatment remaining cases, proposed literature. paper,
consider following majoritarian Condorcet extensions (see, e.g., Laslier, 1997; Brandt,
Brill, & Harrenstein, 2016a, information).
Top Cycle Define dominant set non-empty set alternatives
alternative x \ x PM y. top cycle TC (also
known weak closure maximality, GETCHA, Smith set) defined (unique)
inclusion-minimal dominant subset A.4
Uncovered Set Let C denote covering relation A, i.e., x C (x covers y)
x PM and, z A, PM z implies PM z. uncovered set UC
contains alternatives covered according C, i.e., UC (R) = {x | C
x A}.
Bipartisan Set Consider symmetric two-player zero-sum game set
actions players given payoffs defined follows. Suppose first
player chooses second player chooses b. payoff first player 1
PM b, 1 b PM a, 0 otherwise. bipartisan set BP contains alternatives
played positive probability unique Nash equilibrium game.
SCF f called refinement another SCF g f (R) g(R) preference
profiles R R (A). short, write f g case. shown
BP UC TC (see, e.g., Laslier, 1997).
main result, define well-known notion Pareto-optimality: SCF f
Pareto-optimal never selects Pareto-dominated alternative x A, i.e., x
/ f (R)
whenever exists Pi x N .
2.2 Strategyproofness
Although investigation strategyproof SCFs universal sense
applied set extension, paper concentrate two well-known set
extensions attributed Kelly (1977) Fishburn (1972).5 two set extensions
3. Neutrality postulates permutation alternatives SCF produces
outcome (modulo permutation). See also Section 3.1.1.
4. easily seen set dominant sets ordered respect set inclusion therefore
admits unique minimal element. Assume contradiction two dominant sets X,
contained other. Then, exists x X \ \ X. definition dominant sets
requires x PM PM x, contradiction.
5. Another natural well-known set extension Gardenfors leads even stronger notion strategyproofness, cannot satisfied interesting majoritarian SCF (Brandt & Brill, 2011). Note

569

fiBrandt & Geist

defined follows: Let Ri preference relation X, two nonempty
subsets A.
X RiK x Ri x X .

(Kelly, 1977)

One interpretation extension voters completely unaware mechanism
(e.g., lottery) used pick winning alternative (Gardenfors, 1979; Erdamar
& Sanver, 2009). words, contains exactly pairwise comparisons voters
make without knowledge mechanism (e.g., {a, b} RiK {c} Pi b Pi c).
X RiF following three conditions satisfied:
x Ri x X \ X ,
Ri z X z \ X,

(Fishburn, 1972)

x Ri z x X \ z \ X.
extension one may assume winning alternative picked lottery according underlying priori distribution voters aware (Ching & Zhou,
2002). Alternatively, existence chairman breaks ties according linear,
unknown, preference relation also rationalizes preference extension (Erdamar & Sanver,
2009). interpretations, extension describes exactly conclusions
voter aware tie-breaking method draw (e.g., {a, b} RiF {b, c} Pi b Pi c,
hold Kellys extension RiK ).
easy see X RiK implies X RiF pair sets X, A.
plan prove results entire classes set extensions, call set
extension E independent irrelevant alternatives (IIA) comparison two sets X
depends restriction individual preferences X . Formally, E satisfies
IIA pairs preference relations Ri , Ri0 nonempty sets X,
Ri |XY = Ri0 |XY holds
X RiE X Ri0 E .
mild natural condition, satisfied previously mentioned
set extensions major set extension literature aware of.
Based set extension E, state corresponding notion P E strategyproofness irresolute SCFs. Note contrast related papers (e.g.,
Ching & Zhou, 2002; Sato, 2008), interpret preference extensions fully specified (incomplete) preference relations rather minimal conditions set preferences.
Again, write PiE asymmetric part RiE , set extension E.
Definition 2. Let E set extension. SCF f P E -manipulable voter
exist preference profiles R R0 Rj = Rj0 j 6= f (R0 ) E-preferred
f (R) voter i, i.e.,
f (R0 ) PiE f (R).
SCF called P E -strategyproof P E -manipulable.
negative result Fishburn-strategyproofness trivially carries demanding set
extensions.

570

fiFinding Strategyproof Social Choice Functions via SAT Solving

1

2, 3

4

5

6

7

e
c


b



e
b
c


e
b
c


c
e
b



b
c

e


b

c
e


(a) preference profile R

b

b



c



c

e



e



(b) corresponding (strict)
majority relation PM

(c) manipulated (strict) ma0
jority relation PM
first
agent submits b, a, c, d, e
preferences. edges
impacted change
depicted bold.

Figure 1: Let choice sets indicated shaded nodes; example taken
proof Theorem 3 (cf. Section 5.1.3). first agent R P F -manipulate submitting b, a, c, d, e preferences (since f (R0 ) = {a, c, d, e} P1F {a, b, c, d} = f (R)),
constitute P K -manipulation (since {a, b, c, d} {a, c, d, e} incomparable
according Kelly-extension).
follows observation set extensions P F -strategyproofness implies
example illustrating notions strategyproofness shown
Figure 1.
SCFs, TC P F -strategyproof, BP P K - P F -strategyproof,
whereas UC known satisfy P K -strategyproofness (Brandt & Brill, 2011; Brandt,
2015).
P K -strategyproofness.

2.3 Tournament-Strategyproofness
order allow efficient encoding, would like omit references preference
profiles replace succinct representation expressive
power. majoritarian SCFs, natural choice use (strict) majority relation,
which, odd number voters, represented tournament:
tournament asymmetric complete binary relation set alternatives A.6 thus view majoritarian SCFs functions defined tournaments rather
preference profiles, and, slight abuse notation,7 write f (T ) instead f (R)
= PM strict part majority relation R. We, furthermore, denote
\ 0 := {e : e
/ 0 } edge difference two tournaments 0 .
encoding efficient, important formalize notion strategyproofness using references tournaments rather preference profiles.
6. Note tournaments defined edge set only. Since exactly one edge
pair vertices, vertex set derived edge set.
7. may noted that, majoritarian SCFs map profiles (with arbitrary, fixed number
voters) sets alternatives, interpretation via tournaments abstracts away reference
individual voters. implications Theorems 1 3, depend upon presence
sufficient number voters. discuss required number voters Section 5.2.

571

fiBrandt & Geist

following definition serves purpose shown equivalent standard
notion strategyproofness majoritarian SCFs.
Definition 3. majoritarian SCF f said P E -tournament-manipulable exist
tournaments T, 0 preference relation R \ 0
f (T 0 ) PE f (T ).
majoritarian SCF called P E -tournament-strategyproof P E -tournamentmanipulable.
Theorem 1. majoritarian SCF P E -strategyproof P E -tournamentstrategyproof.
Proof. show majoritarian SCF P E -manipulable P E tournament-manipulable.
direction left right, let f P E -manipulable majoritarian SCF.
exist preference profiles R, R0 integer j Ri = Ri0 6= j
0 strict majority relations
f (R0 ) PjE f (R). Define tournaments := PM 0 := PM
R R0 , respectively. Since R R0 differ voter j, follows \ 0 Rj ,
i.e., edges reversed 0 must Rj . Thus, R := Rj ,
get f tournament-manipulable.
converse, let f P E -tournament-manipulable majoritarian SCF. SCF f
admits manipulation instance, i.e., two tournaments T, 0 preference
relation R \ 0 f (T 0 ) PE f (T ).
proof McGarveys Theorem (McGarvey, 1953), construct preference

majority relation:
profile R = (R1 , . . . , Rn1 ) 0 strict part PM
start empty profile and, strict edge (a, b) 0 , add two voters ia,b
ja,b preferences
Ria,b : a, b, x1 , . . . , xm2 Rja,b : xm2 , . . . , x1 , a, b, respectively.
x1 , . . . , xm2 denotes arbitrary enumeration 2 alternatives \ {a, b}.
holds weights wR (a, b) edges (a, b)
(
2 (a, b) 0
wR (a, b) =
0 (a, b) \ 0 .
Note number voters n 1 R even (and m2 2).
adding R n-th voter, get profile R := (R , R ) odd number voters
required. wR (a, b) 1 edges (a, b) and, thus, R (strict)
majority relation. second profile R0 defined contain first n 1
voters R reversed preference R n-th voter (i.e., R0 := (R , R )).8
profile R0 0 (strict) majority relation (since wR0 (a, b) = 1
8. Immunity manipulation reversing preferences considered Sanver Zwicker (2012)
name half-way monotonicity. proof entails (weak) half-way monotonicity equivalent strategyproofness majoritarian SCFs.

572

fiFinding Strategyproof Social Choice Functions via SAT Solving

Setting axioms
LP
solver

nauty

Results

Tournament
solver

CNF encoder

Model decoder

SAT solver
Figure 2: High-level system architecture
edges (a, b) \ 0 weights edges 0 least 1 again),
completes manipulation instance. I.e., found preference profiles R, R0
differ voter n (who truthful preferences R ) holds
f (R0 ) = f (T 0 ) PE f (T ) = f (R).

3. Methodology
method applied paper similar yet powerful ones presented
Tang Lin (2009) Geist Endriss (2011). Rather translating whole
problem navely SAT, evolved approach, resolves large degree freedom
already encoding problem, employed. approach comparable
way SMT (satisfiability modulo theories) solving works: core SAT
solver; certain aspects problem, however, dealt separate theory solving
unit accepts richer language makes use specific domain knowledge (Biere
et al., 2009, ch. 26). general idea, however, remains encode problem
language suitable SAT solving apply SAT solver efficient universal
problem solving machine.
desirable, using existing tools higher-order formalizations directly rather
specific approach, unfortunately, option. instance, formalization strategyproof majoritarian SCFs higher-order logic (HOL) accepted Nitpick (Blanchette
& Nipkow, 2010) straightforward, highly flexible, well-readable, successful
proofs counterexamples involving three alternatives search space
exceeded.9 optimized formalization, derived together author
Nitpick (at cost reduced readability flexibility), extends performance
four alternatives, turns low results.
9. hand, strict formalization required Nitpick helped identify formally inaccurate definition Fishburn-strategyproofness Gardenfors (1979) (which later repeated
authors).

573

fiBrandt & Geist

Concretely, approach following (see also high-level architecture Figure 2): given domain size n want check whether exists majoritarian
SCF f satisfies set axioms (e.g., P F -strategyproofness Pareto-optimality).
encode setting well given axioms propositional formula (SAT
instance) let SAT solver decide whether formula satisfying assignment.
satisfying assignment, decode concrete instance majoritarian
SCF f satisfies required properties. formula unsatisfiable, know
function f exists.
see, depending problem, preparatory tasks solved
actual encoding: (i ) sets, tournaments, preference relations enumerated;
(ii ) isomorphisms tournaments determined using tool nauty (McKay &
Piperno, 2013); (iii ) choice sets specific SCFs computed (e.g., via matrix multiplication UC linear programming BP ).
following, describe detail general setting majoritarian
SCFs well desirable properties, strategyproofness, encoded SAT
problem CNF (conjunctive normal form).10 First, describe initial encoding,
expressive enough encode required properties, allows small domain sizes
(depending axioms) four five alternatives only. Second, explain
encoding optimized increase overall performance orders magnitude
larger instances seven alternatives solvable.
3.1 Initial Encoding
design, SAT solvers operate propositional logic. direct nave propositional
encoding problem would, however, require huge number propositional variables
since many higher-order concepts involved (e.g., sets alternatives, preference relations
sets well alternatives, functions tuples relations sets).
approach, use one type variable encode SCFs. variables
form cT,X tournament X set alternatives.11 semantics
variables cT,X f (T ) = X, i.e., majoritarian SCF f selects
set alternatives X choice set preference profile (strict) majority
m(m1)

m(m+1)

relation . total, gives us high manageable number 2 2 2m = 2 2
variables initial encoding.
encoding variables cT,x alternatives x rather sets would require less
variable symbols. encoding, however, leads much complexity generated
clauses, offsets savings. imbalance best exhibited
encoding strategyproofness statements always made forVpairs outcomes
(i.e.,
V
sets alternatives). occurrence cT,X could replaced xX cT,x yX
c
T,y .
/
since formula contains conjunction within disjunction, possible
10. Converting arbitrary propositional formula navely CNF lead exponential blow-up
length formula. are, however, well-known efficient techniques (e.g., Tseitins encoding, see
Tseitin, 1983) avoid cost introducing linearly many auxiliary variables. apply
techniques manually needed.
11. algorithms, subroutine c(T, X) take care compact enumeration variables. Since
know advance many tournaments non-empty subsets are, simply use standard
enumeration method pairs objects.

574

fiFinding Strategyproof Social Choice Functions via SAT Solving

CNF, either expansionV(and therefore exponential blow-up) replacement (e.g.,
helper variable cT,X xX cT,x ) would required.
following two subsections demonstrate initial encoding contextual
well explicit axioms CNF.
3.1.1 Context Axioms
Apart explicit axioms, going describe next subsection,
axioms need considered order fully model context
majoritarian SCFs. purpose, arbitrary function maps tournaments
non-empty sets vertices called tournament choice function. Using
initial encoding three axioms introduced, ensure functionality
tournament choice function neutrality respected (making tournament solution):
(1) functionality, (2) canonical isomorphism equality, (3) orbit condition.
first axiom ensures relational encoding f variables cT,X indeed models
function rather arbitrary relation, i.e., tournament exactly one
set X variable cT,X set true. formal terms written
(T ) ((X) cT,X (Y, Z) 6= Z (cT,Y cT,Z ))


!
^
_
^


cT,X
(cT,Y cT,Z ) .


X

(1)

6=Z

illustrative example, corresponding simple pseudo-code generating CNF
file found Appendix B.
second third axiom together constitute neutrality tournament choice
function f , which, formally, written
(f (T )) = f ((T )) tournaments permutations : A.
direct encoding neutrality axiom, however, would tedious due quantification permutations. addition, reformulation canonical isomorphism
equality orbit condition enables substantial optimization encoding
see Section 3.2. require observations order precisely state two
axioms.
use well-known fact graph isomorphisms define equivalence relation
set tournaments.12 equivalence class, pick representative
canonical tournament class. tournament , unique canonical
representation (denoted Tc ). also pick one potentially many isomorphisms
Tc canonical isomorphism denote .13 allows us
formulate axiom canonical isomorphism equality.
Definition 4. tournament choice function f satisfies canonical isomorphism equality
f (T ) = (f (Tc )) tournaments .

(2)

12. Two tournaments 0 isomorphic permutation : (T ) = 0 .
13. practice, tool nauty automatically compute canonical representations tournaments
isomorphisms.

575

fiBrandt & Geist



b
c

e



Figure 3: orbits tournament

OT = {{a, b, c}, {d}, {e}}. corresponding

b c e
automorphism would =
. C := {a, b, c} represents component
b c e
sense elements x C holds x PM e PM x.
last three context axioms, definition orbit clarified.
orbits tournament equivalence classes alternatives according following
equivalence relation: two alternatives a, b considered equivalent
automorphism : maps b, i.e., (a) = b. set orbits
tournament denoted OT . example found Figure 3.
Definition 5. tournament choice function f satisfies orbit condition
f (Tc ) f (Tc ) =

(3)

canonical tournaments Tc orbits OTc .
shown tournament choice function, neutrality equivalent
conjunction orbit condition canonical isomorphism equality, equivalently,
class tournament choice functions satisfying orbit condition canonical
isomorphism equality equal class tournament solutions. formalize
statement Lemma 1. proof Lemma 1 based standard arguments
category theory presented Appendix A.
Lemma 1. tournament choice function, neutrality equivalent conjunction
orbit condition canonical isomorphism equality.
3.1.2 Explicit Axioms
Many axioms efficiently encoded proposed encoding language. section
present main conditions required achieve results Section 4. Clearly,
important one strategyproofness. formal terms, P E -tournament-strategyproofness
written


(T, 0 , R \ 0 ) f (T 0 ) PE f (T )
^^ ^
^
(4)

(cT,X cT,Y )
0 R \T 0 PE X

T, 0 tournaments, R preference relation, X, non-empty subsets
A. algorithmic encoding strategyproofness omitted since present
optimized version Section 3.2.
576

fiFinding Strategyproof Social Choice Functions via SAT Solving

Another property SCFs play important role results one
refinement another (known) SCF g. Fortunately, easily encoded using
framework:
(T )(X g(T )) f (T ) = X
^ _
cT,X .


(5)

Xg(T )

desire resulting SCF f different g (for instance, obtain strict
refinement conjunction Axiom (5)), encode additional clause:
(T ) f (T ) 6= g(T )
_
cT,g(T ) .

(6)



Finally, even properties regarding cardinalities choice sets encoded.
following axiomstating |f (T )| < |g(T )| least one tournament will,
instance, useful Section 4.1.1 searching SCFs return small choice sets:
(T )(X) |X| < |g(T )| f (T ) = X
_ _

cT,X .


(7)

X
|X|<|g(T )|

3.2 Optimized Encoding Improved Performance
order efficiently solve instances four alternatives, need streamline
initial encoding without weakening logical expressiv power. section,
present three optimization techniques found effective.
3.2.1 Obvious Redundancy Elimination
straightforward first step reduce obvious redundancy within axioms.
example, consider axiom strategyproofness, wherein order determine whether
outcome = f (T 0 ) preferred outcome X = f (T )we consider preference
relations R \ 0 . suffices, however, stop finding first preference
relation PE X already know = f (T 0 ) X = f (T )
true.
Similarly, many axioms, exclude considering symmetric pairs objects (e.g.,
functionality tournament choice function, need consider pairs
sets (X, ) (Y, X)).
3.2.2 Canonical Tournaments
main efficiency gain achieved making use canonical isomorphism
equality (see Section 3.1.1) encoding. Recall condition states
tournament choice set f (T ) determined choice set f (Tc ) corresponding canonical tournament Tc applying respective canonical isomorphism .
577

fiBrandt & Geist

foreach Canonical tournament Tc
foreach Tournament 0
RTc \T 0 {R | R preference relation R Tc \ 0 };
foreach Set X
foreach Set
boolean found false;
foreach R RTc \T 0
found setExt(R , E).prefers(Y, X)
variable not(c(Tc , X));
variable not(c(Tc0 , T10 (Y )));
newClause();
found true;
Algorithm 1: P E -tournament-strategyproofness (optimized)
Therefore, suffices formulate axioms single representative equivalence
class tournaments, case, canonical tournament. magnitudes Table 1
illustrate formulation dramatically reduces required number variables,
size CNF formula, time required encoding it.
particular, axioms replace outer quantifier quantifier Tc
ranges canonical tournaments only.14 case strategyproofness, however,
second tournament 0 restriction canonical tournaments potentially
strong enough capture full power axiom. therefore keep 0
arbitrary tournament make sure need variable symbols cTc0 ,Y canonical
tournaments CNF encoding. achieved canonical isomorphism
0 since Condition (2), f (T 0 ) = f (Tc0 ) = T10 (Y ). optimized
encoding shown Algorithm 1.
Furthermore, since longer make statements within CNF formula
non-canonical tournaments, canonical isomorphism equality condition becomes
empty condition and, thus, dropped encoding.
3.2.3 Approximation Logically Related Properties
Approximation standard tool SAT/SMT speed solving process.
instance, over-approximation help find unsatisfiable instances faster solving
parts full problem description CNF. partial CNF formula found
unsatisfiable, superset also trivially unsatisfiable. Since common manipulation
instances literature require one edge tournament reversed, one can,
instance, use over-approximation form single-edge-strategyproofness, slightly
weaker variant (tournament-)strategyproofness |T \ 0 | = 1.15
14. tool nauty capable enumerating non-isomorphic (i.e., canonical) tournaments.
15. obvious whether condition actually strictly weaker tournamentstrategyproofness, identified Pareto-optimal SCFs Kelly-single-edge-strategyproof
Kelly-tournament-strategyproof (cf. Section 4.1.1).

578

fiFinding Strategyproof Social Choice Functions via SAT Solving

solver returns single-edge-strategyproof SCF satisfies
set properties , know immediately also strategyproof SCF
satisfies . used form approximation prove results Remark 2.16
similar fashion, one also apply logically simpler conditions, ones
Brandt Brill (2011), slightly stronger weaker P E -strategyproofness
specific set extensions E order logically under- over-approximate problems,
respectively. logically simpler conditions help improve encoding
solving times, none required obtain results presented paper.
Another way over-approximate problems restrict domain SCF
(e.g., random sampling), explore somewhat detail extracting
small proofs Section 5.1.1.
3.3 Finding Refinements Incremental Solving
order obtain results refined (i.e., inclusion-minimal) otherwise minimal
SCFs, important also produce property SAT solver satisfactory
way. Generally, since task SAT solver generate one satisfying assignment, necessarily output finest SCF satisfy given set properties.
iterated incremental solving, however, force SAT solver generate
progressively finer simply different SCFs satisfy set desired properties.17
refinements, achieved adding clauses encode desired SCF
must (strictly) finer previously found solution (see, e.g., formulation Section 3.1.2). finest SCF desired properties found, adding
clauses leads unsatisfiable formula, SAT solver detects therefore verifies
minimality solution.
final solving step, main tools hand required results,
significant ones describe next section.

4. Results Discussion
present two main findings:
exists strict refinement BP P K -strategyproof (Theorem 2).
majoritarian SCFs 5, P F -strategyproofness Pareto-optimality
incompatible (Theorem 3). < 5, UC satisfies P F -strategyproofness Paretooptimality.
minor results mentioned discussions proceeding proofs Section 4.2.1.
16. = 7 approximation required reach result, also enabled speed-up smaller
instances: running time = 6, example, reduced almost five hours three minutes.
17. Note finding refinement SCF equivalent finding smaller/minimal model
SAT sense; encoding assignments number satisfied variables.

579

fiBrandt & Geist

4.1 Minimal Kelly-Strategyproof SCFs
Brandt (2015) showed every coarsening f P K -strategyproof SCF f 0 P K strategyproof f (R) = f 0 (R) whenever |f 0 (R)| = 1. Thus, interesting question
identify finest (or inclusion-minimal ) P K -strategyproof SCFs.
previous results suggested BP could aor even thefinest majoritarian
SCF satisfies P K -strategyproofness, first provide counterexample assertions using = 5 alternatives, second show also larger domain sizes
exist majoritarian refinements BP still P K -strategyproof return significantly
smaller choice sets BP .
Theorem 2. exists majoritarian Condorcet extension refines BP still
P K -strategyproof. consequence, BP even finest majoritarian Condorcet extension satisfying P K -strategyproofness.
Proof. Within seconds implementation finds satisfying assignment = 5
encoding explicit axioms refinement BP (implies Condorcet extension) P K strategyproofness. corresponding majoritarian SCF decoded assignment
defined like BP exception depicted Figure 4.



b
c

e



Figure 4: Tournament P K -strategyproof refinement BP possible. C :=
{a, b, c} represents component sense elements x C holds
x PM e PM x. BP chooses whole set tournament, refined
solution selects {a, b, c, d} only.

Using technique described Section 3.3, furthermore confirmed obtained SCF refinement BP five alternatives still P K -strategyproof.
Note, however, satisfy (natural, strong) property compositionconsistency (see, e.g., Laslier, 1997). Thus, remains open whether BP might characterized anor even theinclusion-minimal, P K -strategyproof, composition-consistent
majoritarian SCF.18
able resolve open problem completely, proved following statements extending approach also cover composition-consistency. BP
inclusion-minimal, P K -strategyproof, composition-consistent majoritarian SCF
18. Although already domain five alternatives inclusion-minimal, P K strategyproof, composition-consistent Condorcet extensions, could find using computeraided method, counterexamples might extend larger domains.

580

fiFinding Strategyproof Social Choice Functions via SAT Solving

5.19 7, BP inclusion-minimal majoritarian SCF satisfying setmonotonicity 20 composition-consistency. result might extend larger instances, holds 5 alternatives properties uniquely characterize
BP .
we, however, drop composition-consistency again, find multiple inclusionminimal majoritarian SCFs refinements BP still P K -strategyproof. Interestingly, SCFs turn discriminating others sense
average yield significantly smaller choice sets. following section
going search discriminating SCFs analyze average size respective
choice sets.
4.1.1 Finding Discriminating Kelly-Strategyproof SCFs
Many P K -strategyproof tournament solutions criticized discriminating enough. known, instance, large random tournaments, TC
UC select alternatives probability approaching 1 (Scott & Fey, 2012), BP
selects exactly half alternatives average fixed number alternatives
(Fisher & Reeves, 1995). discriminating tournament solutions, hand,
Copeland, Markov, Slater rules violate P K -strategyproofness. Using
computer-aided approach, search discriminating majoritarian SCFs
satisfy P K -strategyproofness. Though spirit automated mechanism design (see, e.g., Conitzer & Sandholm, 2002), apply techniques mostly improve
understanding P K strategyproofness related axioms rather propose
generated tournament solutions actual use.
measure discriminating power majoritarian SCFs, use average
relative size avg(f ) choice sets returned SCF f . Formally define
avg(f ) :=

X
1
|f (T )|,
|A| |T|


set labeled tournaments |A| = alternatives. call SCF f
discriminating another SCF g avg(f ) < avg(g). Given set axioms ,
try find discriminating SCF f (i.e., minimal value avg(f ))
f satisfies axioms .
theory would possible encode relevant axioms enumerate SCFs required properties incrementally applying Axiom (6),
number SCFs usually much large. instead refine initial solution
applying Axioms (5) (6) indicated Section 3.3,
find inclusion-minimal SCF, necessarily discriminating SCF f . thus
proceed via Algorithm 2, guaranteed find discriminating SCF f without
enumerating candidates SCFs. algorithm starts constructing initial candidate SCF satisfies required axioms, iteratively refines much possible
(via conjunction Axioms (5) (6)), encodes additional axiom stating
19. = 6 already find refinement properties.
20. Set-monotonicity postulates choice set invariant weakening unchosen alternatives;
implies P K -strategyproofness (Brandt, 2015).

581

fiBrandt & Geist

future solutions must yield choice set strictly smaller cardinality least
one tournament (Axiom (7)). algorithm repeats refinement encoding
process solution found. Since Axiom (7) necessary condition
avg(f ) < avg(g), sure finest SCF f returned.
SCF smallestSolution null;
CNF minimalRequirements encodeAxioms();
minimalRequirements preprocess(minimalRequirements); // optional
isSatisfiable(minimalRequirements)
CNF currentRequirements minimalRequirements;
SCF currentSolution solve(currentRequirements);
canBeRefined(currentSolution)
Append Axioms (5) (6) currentRequirements g = currentSolution;
currentSolution solve(currentRequirements);
// inclusion-minimal solution found
avgSize(currentSolution) < avgSize(smallestSolution)
smallestSolution currentSolution;
Append Axiom (7) minimalRequirements g = currentSolution;
return smallestSolution;
Algorithm 2: search algorithm find cardinality-minimal SCF f (i.e., minimal
value avg(f )) satisfies given set axioms. reminder, Axioms (5) (6)
encode strict refinement g; Axiom (7) encodes |f (T )| < |g(T )| tournament
T.
Preprocessing generally optional Algorithm 2; = 6 we, however, use
unit propagation order reduce size resulting SAT instance.21 Note
optimization techniques described Section 3.2 (in particular, canonical tournaments)
also applied here.
results analysis exhibited Figure 5. four alternatives
axioms consideration lead minimal size avg(f ), larger domains,
P K -strategyproofness allows smaller choice sets BP (e.g., 45% instead 50%
alternatives = 6). Interestingly, gap BP discriminating SCFs satisfy P K -strategyproofness extraordinarily large; particular,
moving P K -strategyproofness P K -single-edge-strategyproofness allows
sizable reduction avg(f ). related property Kelly-participation, Brandl et al.
(2015) remarked average size choice sets reduced almost 50% compared BP , supports intuition participation weaker property
strategyproofness (even though logically two independent).
BP set-monotonicity yield exact values avg(f ) 6,
somewhat surprising found SCFs coarsenings BP yet setmonotonic domain size. SCFs, however, set-monotonic refinements
discriminating BP . Interestingly, generalize larger
21. case Kelly-strategyproofness, unit propagation deletion duplicate clauses reduced
CNF formula 600 million three million clauses.

582

fiFinding Strategyproof Social Choice Functions via SAT Solving

60 %

56%
50%

50 %

45%

minf (avg(f ))

40 %

38%

Uncovered set UC
Bipartisan set BP
Set-monotonicity
P K -strategyproofness
P K -single-edgestrategyproofness

30 %

20 %

18%

restriction

10 %

2

3

4
5
6
Number alternatives |A| =

Figure 5: comparison minimal values (rounded) avg(f ) majoritarian, Paretooptimal SCFs f satisfy given axioms (e.g., P K -strategyproofness). Interestingly,
values set-monotonicity identical ones BP . Non-solid dots represent
upper bounds, i.e., cases could compute SCF f value avg(f )
guarantee indeed minimal.

583

fiBrandt & Geist

domains since found discriminating majoritarian SCF f = 7 satisfies
set-monotonicity Pareto optimality selecting 49.73% alternatives
average.
demanding axioms usually lead larger choice sets (for instance, SCF
always returns alternatives trivially satisfies many axioms), one might view minimal
value avg(f ) attempt quantify strength axiom. leave
detailed study quantification future work.
4.2 Incompatibility Fishburn-Strategyproofness Pareto-Optimality
order prove main result incompatibility Pareto-optimality P F strategyproofness first show following lemma, establishes that, majoritarian
SCFs, notion Pareto-optimality equivalent refinement uncovered
set (UC ).22
Lemma 2. majoritarian SCF f Pareto-optimal refinement
UC .
Proof. well-known, already observed Fishburn (1977), UC Paretooptimal, implies refinements also Pareto-optimal.
direction left right, let f Pareto-optimal majoritarian SCF
arbitrary tournament. suffices show f (T ) never contain covered alternative
(since f (T ) UC (T ) contains uncovered alternatives only). let b alternative
covered another alternative a. going construct preference profile
R (strict) majority relation b Pareto-dominated a.
Together Pareto-optimality f implies b
/ f (T ). use variant
well-known construction McGarvey (1953), triples rather pairs
alternatives. Note voter need ensure strictly prefers b
order obtain desired Pareto-dominance b. Starting empty profile,
alternative x
/ {a, b} add two voters Rx1 , Rx2 profile. two voters
defined depending x ranked relative b order establish edges
a, x b, x. Note since x implies x b (because C b), edge (a, b)
cannot contained three-cycle x and, thus, forms transitive triple x.
Case 1: x (implies x b)
Rx1 : x, a, b, v1 , . . . , vm3 ; Rx2 :

vm3 , . . . , v1 , x, a, b

Case 2a: x x b
Rx1 : a, x, b, v1 , . . . , vm3 ;

Rx2 :

vm3 , . . . , v1 , a, x, b

Case 2b: x b x
Rx1 : a, b, x, v1 , . . . , vm3 ;

Rx2 :

vm3 , . . . , v1 , a, b, x

v1 , . . . , vm3 denotes arbitrary enumeration m3 alternatives A\{a, b, x}.
cases, two voters cancel pairwise comparisons
(a, b), (x, a) (x, b). remaining edges (y, z) (with {y, z} {a, b} = )
22. stronger version lemma shown Brandt, Geist, Harrenstein (2016b).

584

fiFinding Strategyproof Social Choice Functions via SAT Solving

add two voters (now even closer construction McGarvey.)
R(y,z)1 : y, z, a, b, v1 , . . . , vm4
R(y,z)2 : vm4 , . . . , v1 , a, b, y, z,
together establish edge (y, z), reinforce (a, b) cancel otherwise. Note order
achieve odd number voters, arbitrary voter added without changing
majority relation (as edges weight least two far). completes
construction preference profile R (strict) majority relation
b Pareto-dominated a.
establish full result (which admit proof counterexample
Theorem 2) wesimilarly previous approachesmake use inductive argument.
Lemma 3. set extension E satisfies IIA, exists majoritarian SCF f
+ 1 alternatives P E -strategyproof Pareto-optimal, also exists
majoritarian SCF f 0 alternatives satisfies two properties.
Proof. Let f UC majoritarian SCF + 1 2 alternatives P E strategyproof. define fe restriction f alternatives based
tournaments alternative e Condorcet loser, i.e., alternative x
(y, x) \ {x}. formal terms, define
fe (T ) := f (T +e ),
+e tournament obtained adding alternative e Condorcet
loser. restriction f well-defined SCF since alternative e cannot contained
f (T +e ) UC (T +e ) = UC (T ), last equation follows simple observation
covering relation unaffected deleting Condorcet losers.
need show alternative e restriction fe majoritarian
SCF P E -strategyproof Pareto-optimal. Since holds e A,
pick e arbitrarily.
Majoritarian: fact fe majoritarian SCF carries trivially f .
P E -strategyproofness: Assume contradiction fe P E -strategyproof.
Then, Theorem 1 exist tournaments 0 alternatives
fe (T 0 ) PE fe (T ) R \ 0 . since fe (T 0 ) = f (T 0+e ) fe (T ) = f (T +e )
(and fact E satisfies IIA), get
f (T 0+e ) PE f (T +e ),
contradicts P E -tournament-strategyproofness f (as two tournaments 0+e
+e form manipulation instance), thus P E -strategyproofness.
Pareto-optimality: Lemma 2, equivalent refinement UC .
Thus, let arbitrary tournament alternatives consider following
chain set inclusions, proves fe UC :
fe (T ) = f (T +e ) UC (T +e ) = UC (T ).
585

fiBrandt & Geist

virtue Lemma 3 suffices check claim restricted domain
= 5, following lemma.
Lemma 4. exactly five alternatives (i.e., = 5) majoritarian SCF f
satisfies P F -strategyproofness Pareto-optimality.
Proof. base case = 5 alternatives verified using computer-aided approach,
i.e., checked that, |A| = 5 alternatives, satisfying assignment
encoding P F -tournament-strategyproofness (cf. Theorem 1) refinement
UC (cf. Lemma 2), SAT solver confirmed within seconds. human-readable
proof claim extracted computer-aided approach presented
Section 5.1.2.
Finally, papers main result regarding P F -strategyproofness follows directly
Lemmas 3 4.
Theorem 3. number alternatives 5 majoritarian SCF f
satisfies P F -strategyproofness Pareto-optimality.
Proof. prove statement inductively. base case = 5 covered Lemma 4.
induction step, apply contrapositive Lemma 3 E := F, directly
yields desired results.
number voters required impossibility kept implicit far,
upper bound m2 1 = 19 voters derived construction
proof Theorem 1. Section 5 see, however, human-readable proof
Theorem 3 extracted, requires seven voters.
consequence Theorem 3, virtually common tournament solutionsexcept
top cycle (see Remark 2)fail P F -strategyproof.
4.2.1 Remarks
turn towards technique proof extraction, let us discuss insights
regarding Theorem 3, been, large extent, enabled universality
presented method.
Remark 1 (Strengthenings). shown computer-aided method
Theorem 3 holds even without assumption neutrality. Since then, however,
optimizations based canonical tournaments longer used, extracted proofs
(cf. Section 5) much complex therefore decided present result
neutrality here.23
theorem strengthened additionally requiring P F -single-edgestrategyproofness (cf. Section 3.2) even weaker variant P F -strategyproofness
manipulator allowed swap two adjacent alternatives (see, e.g., Sato, 2013).
23. addition, running times much longer, which, however, major concern given many
conjectures tested result.

586

fiFinding Strategyproof Social Choice Functions via SAT Solving

Remark 2 (The Top Cycle TC ). Note Theorem 3 conflict fact
TC P F -strategyproof, as, 4 alternatives, TC strictly coarser UC
therefore Pareto-optimal. Possibly, TC even finest majoritarian Condorcet
extension satisfies P F -strategyproofness 5. able verify
5 7 using computer program. case four alternatives, UC strict
refinement TC (as method shows) still P F -strategyproof. = 8 time
space requirements appear prohibitive; already = 7 (despite optimizations
approximations) encoding solving problem takes almost 24 hours,
= 6 runs three minutes. obvious whether inductive argument
extend verified instances larger numbers alternatives (as, instance,
induction step would require least five alternatives).
Remark 3 (Other Preference Extensions). advantage computer-aided approach universality. can, instance, easily adapt implementation
check set extensions ones Kelly Fishburn.
Interestingly, main result relies small fraction power Fishburn
extension: suffices compare disjoint sets sets contained one another.
formal terms, following set extension suffices impossibility:

K

X Ri X = ,

X RiF X RiF X X,



otherwise.
Actually, would even suffice compare sets X |X | 3.
also checked strengthening Fishburn extension: voter prefers set X
set X better optimistic pessimistic expectations.
Formally, X RiOP
x Ri x X ,
Ri x x X.
extension weakening optimistic pessimistic notions strategyproofness Duggan-Schwartz Theorem (Duggan & Schwartz, 2000). majoritarian setting, P OP -strategyproofness leads analogous impossibility Theorem 3
4 already.
Remark 4 (Generality Lemma 3). Note proofs individual properties
within inductive proof Lemma 3 rely definition fe stand
independently other. Furthermore, may noted Lemma 3 even
shown refinements arbitrary majoritarian SCFs g whose choice set g(T )
shrink Condorcet losers removed (rather Pareto-optimal majoritarian
SCFs).

5. Proof Extraction
major concern regarding computer-aided proofs difficulty checking correctness. implementation correctly confirmed number existing results
587

fiBrandt & Geist

considered testing, doubts correctness new results naturally
remain. SAT solvers offer kind proof trace, checked thirdparty-software. This, however, guarantee correctness encoding
confirms unsatisfiability corresponding CNF formula.
section, show human-readable proofs desired statements
extracted approach, verified manual mathematical
proof. general idea proof extraction technique lies finding analyzing
minimal unsatisfiable core (also referred minimal unsatisfiable set (MUS))
SAT instance. unsatisfiable core CNF formula subset clauses already
unsatisfiable itself. subset clauses unsatisfiable core satisfiable,
core called minimal. case, minimal unsatisfiable core contains information
concrete instances axioms employed obtain impossibility
(e.g., manipulation instances, applications Pareto optimality, etc). information
extracted straightforward way reveals structure arguments proof.
exemplify technique Section 5.1, extract human-readable proof
main result (Theorem 3). Section 5.2 additionally enrich proof set
minimal corresponding preference profiles, shows result Theorem 3
holds setting least seven voters.
general, extracting human-readable proofs serves two separate purposes.
one hand, human-readable proof significantly raise confidence correctness
results, basically making verification approach obsolete since results
directly verifiable. hand, extracted proofs sometimes
provide additional insight problems via arguments structure. case,
number voters required impossibility would (easily) accessible
directly.
5.1 Human-Readable Proof Theorem 3
order extract human-readable proof Theorem 3, actually main ingredient
Lemma 4, follow series three steps:
1. Obtain suitable MUS CNF formula encodes P F -tournamentstrategyproof refinement UC five alternatives
2. Decode MUS human-readable format
3. Interpret human-readable MUS obtain human-readable proof
first two steps computer-aided largely automated, step three
requires manual effort.
5.1.1 Obtaining Suitable MUS CNF Formula
Extracting minimal unsatisfiable core feature offered range SAT solvers.
paper, use PicoMUS (part PicoSAT, Biere, 2008) job.24
24. Compiled trace support order use core extraction addition clause selector variables.
significantly improves size resulting MUS.

588

fiFinding Strategyproof Social Choice Functions via SAT Solving

noted, however, MUS inclusion-minimal, necessarily represent
smallest unsatisfiable set (i.e., minimal number clauses variables).25
number clauses turned good proxy proof complexity length,
tried find MUS small number clauses. run complete,
optimized SAT encoding described Section 3.2, PicoMUS returns MUS
55 clauses. already massive reduction compared three million clauses
original problem instance, found even smaller MUS 16 clauses
randomly sampling sets tournaments used instead full domain
tournaments generating problem files. Another heuristic approach considering
neighborhoods single tournaments (for instance, tournaments reached
changing two edges transitive tournament) yielded less significant
improvement total 25 clauses.
seems natural larger domains generally better lead required impossibility often smaller domains, larger domains actually tend towards
larger proofs even miss small proofs. instance, domain size = 200
(consisting labeled tournaments) proof smaller 18 clauses found,
number runs = 50 produced four proofs 16 clauses each.26
Therefore, setting, medium-sized domain (s = 50 = 100 experiments)
appears best suited. complete results running time proof size analysis given
different domain sizes obtained Figures 8 9 Appendix C.
5.1.2 Decoding MUS Human-Readable Format
next step make obtained MUS accessible humans. end, first
(automatically) add comments original CNF manipulation clause
creation, select comments belong clauses MUS. comments
contain witnesses manipulation instances found, i.e., information original
tournament , manipulated tournament 0 , respective choice sets f (T ) f (T 0 ),
original preferences manipulator R (compare Definition 3). Furthermore,
variable symbol easily decoded tournament choice set represents,
helpful particular non-manipulation clauses (orbit condition Paretooptimality).
result step presented Figure 6, tournament represented
lower triangular representation adjacency matrix (see proof Lemma 4
Section 5.1.3 graphical representations).
5.1.3 Interpreting MUS Obtaining Human-Readable Proof
witnessed MUS small step textual, human-readable proof.
bit practice, one quickly understand structure proof: starts
orbit condition first line refinement condition last line,
25. tool CAMUS Liffiton Sakallah (2008) theoretically capable finding smallest
MUS (with minimal number clauses), terminate reasonable amount time
large CNF instances.
26. addition, medium-sized domains efficient regarding running time per generated proof,
admittedly plays minor, still important role given total running time large
domains 20 hours.

589

fiBrandt & Geist

p c n f 341 16
218 231 232 233
202 330 0
c : 1111111111
233 202 0
c : 1101100111
234 202 0
c : 1101100111
218 218 0
c : 1101100111
232 232 0
c : 1101100111
248 338 0
c : 1101100111
231 202 0
c : 1101100111
247 202 0
c : 1101100111
314 314 0
c : 1100101110
c : 1100101110
318 318 0
c : 1100101110
c : 1100101110
322 322 0
c : 1100101110
326 326 0
c : 1100101110
334 202 0
c : 1100101110
202 0
314 318 322 326

234 247 248 0
> [ e ] ; : 1011100111 > [ , e ] ; P : b , , c , e ,
> [ e ] ; : 0010100111 > [ ] ; P : b , c , , , e
> [ , e ] ; : 0010100111 > [ ] ; P : b , c , , , e
> [ ] ; : 1001000100 > [ e ] ; P : e , c , , , b
> [ , b , c , ] ; : 1001000100 > [ , c , , e ] ; P : e , c , , , b
> [ , b , c , , e ] ; : 1100100101 > [ b , c , e ] ; P : b , e , c , ,
> [ b , c , ] ; : 1111111111 > [ e ] ; P : , e , b , c ,
> [ b , c , , e ] ; : 1111111111 > [ e ] ; P : , e , b , c ,
> [ c ] ; : 1100100101 > [ e ] ; P : b , , e , , c
> [ c ] ; : 1100110110 > [ b ] ; P : b , c , , e ,
> [ ] ; : 1100100101 > [ b ] ; P : b , , e , , c
> [ ] ; : 1100110110 > [ ] ; P : b , c , e , ,
> [ c , ] ; : 1100110110 > [ , b ] ; P : b , e , , c ,
> [ e ] ; : 1100110110 > [ ] ; P : b , c , , e ,
> [ , e ] ; : 1001111010 > [ ] ; P : c , , , e , b
330 334 338 0

Figure 6: version extracted MUS, manipulation instances (here: binary
clauses) decoded human-readable format: two mappings tournaments
(original manipulated 0 ) choice sets truthful preferences manipulator P . information covers variables thus suffices also decode remaining
clauses.

590

fiFinding Strategyproof Social Choice Functions via SAT Solving

Truthful choice


{e}





{a} {e}



{b, c, d}
f (T1 ) =

{b, c, d} {e}





{a}



{a} {b, c, d}


{c}





{d}
f (T2 ) = {c, d}



{e}




{d, e}
f (Te0 ) UC (Te0 ) = {e}

Manipulated choice

Manipulators
preferences

f (Ta ) UC (Ta ) = {a}

b, c, d, a, e

f (Te ) UC (Te ) = {e}

a, e, b, c,

(
{e}
T10 isof (T10 ) =
{a, c, d, e} morphic27 T1


{b}



{a}
T20 isof (T20 ) =
{a, b} morphic27 T2



{d}
f (Td ) UC (Td ) = {d}
f (T2 ) = {c, e}

28

e, c, a, d, b

b, e, a, c,

b, c, d, e,
c, a, d, e, b
a, c, b, e,

Table 2: Set manipulation instances (one per line) conclude f (T1 ) = =
{a, b, c, d, e} f (T2 ) = {c, d, e}. truthful choices considered leads
P F -tournament-manipulation instance (a contradiction assumption P F tournament-strategyproofness). tournaments defined Figure 7.
leave (limited) possibilities respective choice sets, excludes possible
choices one another suitable manipulation instances. full proof runs follows.
Proof Lemma 4. contradiction, let f majoritarian SCF = {a, b, c, d, e}
satisfies P F -strategyproofness Pareto-optimality. Recall that, Theorem 1, f
P F -tournament-strategyproof, too, Lemma 2 refinement UC
(i.e., f UC ). Let furthermore T1 T2 tournaments depicted Figure 7.
proceed three steps: first, show f (T1 ) = UC (T1 ) = A. Second, argue
f (T2 ) = UC (T2 ) = {c, d, e}. last, prove two insights actually forms
basis manipulation instance, leads desired contradiction.
Let us start f (T1 ) = UC (T1 ) = A. First, note since alternatives {b, c, d}
form orbit know either {b, c, d} f (T1 ) {b, c, d} f (T1 ) = (cf. Definition 5).
going exclude remaining choice sets P F -tournament-manipulation instances. first example, suppose f (T1 ) = {e}. voter individual preferences
P : b, c, d, a, e could reverse edges (b, a) (b, c) T1 transitive tournament Ta Condorcet winner results (which needs uniquely selected f since
f UC ). Since, however, {a} PF {e}, contradicts P F -tournament-strategyproofness.
example also works exclude f (T1 ) = {a, e}. Note arguments correspond lines 5 8 extracted MUS Figure 6. (analogous) manipulation
instances possible choice sets = {a, b, c, d, e} given Table 2
Figure 7.
591

fiBrandt & Geist

b

b

b



c



c



c

e



e



e



(a) T1

(b) T2

(c) Ta

b

b

b



c



e



e

(d) Te

(e)

b

c





e

T10

c


(f)

b

T20

b



c



c



c

e



e



e



(g) Td

(h) T200

(i) Te0

Figure 7: Tournaments required proof Lemma 4. uncovered sets
marked grey; edges (for Te0 : be) reversed manipulating voter
(cf. Table 2) depicted thick edges. Note proof would also succeed less edge
reversals Ta , Te , Td , Te0 (such tournaments Condorcet winners
rather transitive). transitive tournaments isomorphic, however,
thus succinctly represented single clause 202 extracted MUS.
f (T2 ) = UC (T2 ) = {c, d, e}, first observe f (T2 ) UC (T2 ) = {c, d, e} hence
need exclude strict subset {c, d, e}. proceed giving possible
manipulation instance subsets. complete list found Table 2
Figure 7. Observe last line Table 2 excludes f (T2 ) = {c, e} considering
manipulated choice (known) truthful choice f (Te0 ) UC (Te0 ) = {e}.
last step, provide manipulation instance based f (T1 ) = f (T2 ) =
{c, d, e}. this, first observe renaming alternatives get f (T200 ) = {b, c, e}
manipulation instance results voter preferences P0 : b, e, c, d, a.
!
!
b c e
b c e
27. isomorphisms 1 =
2 =
, respectively.
b e c
c e b
28. SAT solver actually returned isomorphic copy instance, restructured improve
readability.

592

fiFinding Strategyproof Social Choice Functions via SAT Solving

voter reverse edges (d, a) (e, c) T1 create T200 obtain P F -preferred
outcome {b, c, e}, contradiction P F -strategyproofness f .
Note actually manipulation instance f (T1 ) = {a} {b, c, d}
f (T10 ) = {a, c, d, e} requires Fishburn-extension; instances Kellyextension suffices.
5.2 Number Voters Required
previous parts paper taken advantage fact condition
tournament-strategyproofness abstracted away reference voters. interesting
ask, however, many voters least required obtained impossibility
Theorem 3 hold. construction proof Theorem 1 gives implicit upper
bound m2 1 = 19 voters, improved seven voters.
slightly modifying techniques described Brandt, Geist, Seedig (2014),
able (automatically) construct minimal preference profiles steps Proof 5.1.3.
Brandt et al. (2014) provided SAT-formulation whether given majority relation
induced given number voters, extended framework include axioms
manipulation instances. detail, re-used axioms linear preferences
majority implications, added axioms truthful preferences manipulator
majority implications manipulated profile.
profiles generated steps proof Lemma 4 Section 5.1.3
given Appendix D. largest profiles contains seven voters,
profiles easily extended seven voters adding pairs voters opposite
preferences. observation shows seven smallest number voters
achieved extracted proof, remains open whether, another proof,
number voters reduced seven.

6. Conclusion
extended applied computer-aided theorem proving based SAT solving
extensively analyze Kelly- Fishburn-strategyproof majoritarian SCFs. led
range results, positive negative. important novel contribution work
ability extract human-readable proof negative SAT instances. eliminates
need verify computer-aided method since impossibility results directly
checked based human-readable proofs. Based ease adaptation
proposed method, anticipate insights spring overall approach
future. Apart simply applying system investigate strategyproofness,
potential applications related line work include:
Unrestricted SCFs order reduce complexity, studied majoritarian SCFs
only. framework, however, applicable way general SCFs,
operate full preference profiles (rather majority relations). challenge
find suitable representation preference profiles potentially corresponding
inductive arguments number voters.
593

fiBrandt & Geist

axioms preliminary experiments suggest technique easily
applied range properties strategyproofness, deserve
investigation. many cases suffices formalize implement additional
axioms. particular interest could properties link behavior SCFs
different domain sizes. initial steps direction, able extend approach
cover property participation (Brandl et al., 2015; Brandt et al., 2016c) well
weak version composition-consistency (cf. Section 4.1).
Smallest number voters required mentioned Section 5.2, Theorem 3 holds
number voters n 7, known whether number minimal. One
could adapt proof extraction presented Section 5 search smallest proof
number voters, rather number clauses, settle question.
Generalization inductive argument
appears reasonable investigate
whether inductive argument Lemma 3 generalized whole class
properties/axioms, ideally based logical form. work Geist Endriss
(2011), would enable automated search theorems SCFs.
Apart concrete ideas, applications general approach envisioned
many areas theoretical economics.

Acknowledgments
material based upon work supported Deutsche Forschungsgemeinschaft
grants BR 2312/7-2 BR 2312/9-1. paper benefitted discussions COST
Action Meeting IC1205 Computational Social Choice (Maastricht, 2014), 13th International Conference Autonomous Agents Multiagent Systems (Paris, 2014),
5th International Workshop Computational Social Choice (Pittsburgh, 2014),
Dagstuhl Seminar Computational Social Choice: Theory Applications (Dagstuhl,
2015). authors particular thank Jasmin Christian Blanchette, Markus Brill, Hans
Georg Seedig, Bill Zwicker helpful discussions support, three anonymous reviewers valuable comments suggestions improve paper.

Appendix A. Proof Lemma 1
first show orbit condition equivalent statement automorphisms:
Lemma 5. Let f tournament choice function. following statement equivalent orbit condition:
(f (Tc )) = f (Tc ) canonical tournaments Tc automorphisms .

(8)

Proof. Let f tournament choice function Tc canonical tournament. direction left right, let furthermore OTc orbit Tc . pick two alternatives
a, b O. show either alternatives chosen f neither one is. Since
b orbit, must automorphism Tc (a) = b.
Observe f (Tc ) b (f (Tc )) b f (Tc ), last
step application Condition (8).
594

fiFinding Strategyproof Social Choice Functions via SAT Solving

converse, let automorphism Tc , pick arbitrary alternative
consider inverse image 1 (a) =: b. Since b orbit, holds
orbit condition f (Tc ) b f (Tc ). Furthermore, (b) =
get f (Tc ) (f (Tc )). Thus, f (Tc ) = (f (Tc )),
wanted prove.
Next prove general statement split isomorphism canonical
isomorphism automorphism.
Lemma 6. isomorphism : Tc decomposed canonical isomorphism
automorphism : Tc Tc . I.e., isomorphism : Tc
automorphism : Tc Tc = .
Proof. Define : Tc Tc setting := T1 . Since inverses compositions
isomorphisms isomorphisms,
follows

directly automorphism.
Furthermore, = T1 = T1 = .
Lemmas 5 6 together used prove Lemma 1:
Lemma 1. tournament choice function, neutrality equivalent conjunction
orbit condition canonical isomorphism equality.
Proof. Let f tournament choice function first note Lemma 5 might use
Condition (8) rather orbit condition. Therefore, direction left right
trivially true.
direction right left, first show canonical isomorphism
equality (2) together Condition (8) implies neutrality canonical tournaments:
let Tc canonical tournament, permutation define 0 := (Tc ). Lemma 6,
decompose isomorphism : Tc 0 = T0 automorphism
Tc . following chain equalities holds, proves claim canonical
tournaments:
(2)

(8)

f ((Tc )) = f (T 0 ) = 0 (f (Tc0 )) = 0 (f (Tc )) = 0 ((f (Tc ))) = (f (Tc )). (9)
arbitrary tournaments permutations , write (Tc ) obtain
f ((T )) = f ((T (Tc ))) = f (( )(Tc )),
which, since Tc canonical, equal
(2)

( )(f (Tc ))) = (T (f (Tc ))) = (f (T ))
Condition (9). finishes proof.

595

fiBrandt & Geist

Appendix B. Pseudo-Code Encoding
present (as illustrative example) simple pseudo-code Algorithm 3 generate CNF form Axiom 1 (functionality tournament choice function; cf. Section 3.1.1).
foreach Tournament
foreach Set X
variable(c(T, X));
newClause();
foreach Set
foreach Set Z 6=
variable not(c(T, ));
variable not(c(T, Z));
newClause();
Algorithm 3: Functionality tournament choice function

Appendix C. MUS Search Analysis (Running Time Size MUS)
appendix, present complete results running time (Figure 8) MUS
size (measured number clauses; Figure 9) analyses given different sizes randomly
sampled domains. setting, sizes = 50 = 100 appear offer good results
terms running time actually finding small proofs.

sample size

10

11

20

64.4

78

50

18.2
359

100

8.2
699

200

8.6
951

19.5

1,000

500
0

200 400 600 800 1,000

70.5
0
20
40
60
80
Average running time per proof
(in seconds)

Number proofs found
(out 1000 runs)

Figure 8: Number unsatisfiable instances (i.e., proofs found) running time results
heuristics different numbers sampled tournaments (labeled, 1000 runs).

596

fiFinding Strategyproof Social Choice Functions via SAT Solving

s=10

2

(16,1)

1
0

s=20

5

(16,1)

s=50

0
60
40
(16,4)

20
0

s=100

100
50

s=200

Number proofs founds (in 1000 runs)

10

0
200
150
100
50
0

(16,1)

(18,29)

s=500

300
200
(19,3)

100
0
0

20

40
60
80
100
120
Size proofs (number clauses)

140

160

Figure 9: sizes MUSes (proofs) heuristics different numbers sampled
tournaments (labeled). size MUS obtained running full domain
indicated red line. improved readability, size multiplicity smallest
MUS explicitly listed.

597

fiBrandt & Geist

Appendix D. Profiles Extracted Proof Theorem 3
display MUS Figure 6 enriched minimal preference profiles
step proof Theorem 3. profiles generated checked minimality
computer (and using SAT solver) less second each.
p c n f 341 16
218 231 232 233 234 247 248 0
Agent 0 : b , c , , , e
Agent 1 : , e , c , , b
Agent 2 : e , , b , c ,
202 330 0
c : 1111111111 > [ e ] ; : 1011100111 > [ , e ] ; P : b , , c , e ,
Agent 0 : b , , c , e ,
Agent 1 : c , , , e , b
Agent 2 : e , c , , b ,
Agent 3 : , e , , c , b
Agent 4 : e , , b , , c
Manipulated p r e f e r e n c e f g e n 0 : b , , c , , e
233 202 0
c : 1101100111 > [ e ] ; : 0010100111 > [ ] ; P : b , c , , , e
234 202 0
c : 1101100111 > [ , e ] ; : 0010100111 > [ ] ; P : b , c , , , e
Agent 0 : b , c , , , e
Agent 1 : , e , , b , c
Agent 2 : e , c , , b ,
Manipulated p r e f e r e n c e f g e n 0 : , c , b , , e
218 218 0
c : 1101100111 > [ ] ; : 1001000100 > [ e ] ; P : e , c , , , b
232 232 0
c : 1101100111 > [ , b , c , ] ; : 1001000100 > [ , c , , e ] ; P : e , c , , , b
Agent 0 : e , c , , , b
Agent 1 : , , e , b , c
Agent 2 : , , e , b , c
Agent 3 : , e , b , c ,
Agent 4 : c , e , b , ,
Agent 5 : b , c , , e ,
Agent 6 : b , , c , e ,
Manipulated p r e f e r e n c e f g e n 0 : b , , c , , e
248 338 0
c : 1101100111 > [ , b , c , , e ] ; : 1100100101 > [ b , c , e ] ; P : 1 > 4 >
2 > 3 > 0
Agent 0 : 1 > 4 > 2 > 3 > 0
Agent 1 : 2 > 3 > 0 > 4 > 1
Agent 2 : 2 > 4 > 3 > 1 > 0
Agent 3 : 0 > 4 > 3 > 1 > 2
Agent 4 : 1 > 0 > 4 > 2 > 3
Manipulated p r e f e r e n c e f g e n 0 :
1 > 2 > 0 > 4 > 3
231 202 0
c : 1101100111 > [ b , c , ] ; : 1111111111 > [ e ] ; P : 0 > 4 > 1 > 2 > 3

598

fiFinding Strategyproof Social Choice Functions via SAT Solving

247 202 0
c : 1101100111 > [ b , c , , e ] ; : 1111111111 > [ e ] ; P : 0 > 4 > 1 > 2 > 3
Agent 0 : 0 > 4 > 1 > 2 > 3
Agent 1 : 3 > 1 > 2 > 0 > 4
Agent 2 : 4 > 2 > 3 > 1 > 0
Manipulated p r e f e r e n c e f g e n 0 :
4 > 0 > 3 > 2 > 1
314 314 0
c : 1100101110 > [ c ] ; :
Agent 0 : 1 > 3 > 4 > 0 > 2
Agent 1 : 4 > 3 > 1 > 2 > 0
Agent 2 : 4 > 1 > 2 > 0 > 3
Agent 3 : 2 > 0 > 3 > 4 > 1
Agent 4 : 2 > 0 > 3 > 4 > 1
Manipulated p r e f e r e n c e f
1 > 2 > 0 > 4 > 3
c : 1100101110 > [ c ] ; :
Agent 0 : 1 > 2 > 3 > 4 > 0
Agent 1 : 0 > 3 > 4 > 2 > 1
Agent 2 : 0 > 4 > 2 > 3 > 1
Agent 3 : 4 > 1 > 2 > 0 > 3
Agent 4 : 3 > 4 > 1 > 2 > 0
Manipulated p r e f e r e n c e f
3 > 1 > 2 > 0 > 4

1100100101 > [ e ] ; P : 1 > 3 > 4 > 0 > 2

agent 0 :
1100110110 > [ b ] ; P : 1 > 2 > 3 > 4 > 0

agent 0 :

318 318 0
c : 1100101110 > [ ] ; : 1100100101 > [ b ] ; P : 1 > 3 > 4 > 0 > 2
c : 1100101110 > [ ] ; : 1100110110 > [ ] ; P : 1 > 2 > 4 > 0 > 3
Agent 0 : 1 > 2 > 4 > 0 > 3
Agent 1 : 3 > 4 > 1 > 2 > 0
Agent 2 : 4 > 0 > 2 > 3 > 1
Agent 3 : 2 > 0 > 3 > 4 > 1
Agent 4 : 1 > 0 > 3 > 4 > 2
Manipulated p r e f e r e n c e f g e n 0 :
3 > 1 > 2 > 0 > 4
322 322 0
c : 1100101110 > [ c , ] ; : 1100110110 > [ , b ] ; P : 1 > 4 > 0 > 2 > 3
Agent 0 : 1 > 4 > 0 > 2 > 3
Agent 1 : 2 > 0 > 3 > 4 > 1
Agent 2 : 3 > 4 > 1 > 2 > 0
Manipulated p r e f e r e n c e f g e n 0 :
1 > 0 > 3 > 4 > 2
326 326 0
c : 1100101110 > [ e ] ; : 1100110110 > [ ] ; P : 1 > 2 > 3 > 4 > 0
Agent 0 : 1 > 2 > 3 > 4 > 0
Agent 1 : 0 > 3 > 4 > 2 > 1
Agent 2 : 0 > 4 > 2 > 3 > 1
Agent 3 : 4 > 1 > 2 > 0 > 3
Agent 4 : 3 > 4 > 1 > 2 > 0
Manipulated p r e f e r e n c e f g e n 0 :
3 > 1 > 2 > 0 > 4

599

fiBrandt & Geist

334 202 0
c : 1100101110 > [ , e ] ; : 1001111010 > [ ] ; P : 2 > 0 > 3 > 4 > 1
Agent 0 : 2 > 0 > 3 > 4 > 1
Agent 1 : 1 > 4 > 0 > 2 > 3
Agent 2 : 3 > 4 > 1 > 2 > 0
Manipulated p r e f e r e n c e f g e n 0 :
3 > 1 > 0 > 4 > 2
202 0
Agent 0 : 4 > 3 > 2 > 1 > 0
314 318 322 326 330 334 338 0
Agent 0 : 2 > 0 > 3 > 4 > 1
Agent 1 : 3 > 4 > 1 > 2 > 0
Agent 2 : 4 > 1 > 2 > 0 > 3

References
Barbera, S. (1977). Manipulation social decision functions. Journal Economic Theory,
15 (2), 266278.
Barbera, S. (2010). Strategy-proof social choice. Arrow, K. J., Sen, A. K., & Suzumura,
K. (Eds.), Handbook Social Choice Welfare, Vol. 2, chap. 25, pp. 731832.
Elsevier.
Biere, A. (2008). PicoSAT essentials. Journal Satisfiability, Boolean Modeling
Computation (JSAT), 4, 7579.
Biere, A., Heule, M., van Maaren, H., & Walsh, T. (Eds.). (2009). Handbook Satisfiability,
Vol. 185 Frontiers Artificial Intelligence Applications. IOS Press.
Blanchette, J. C., & Nipkow, T. (2010). Nitpick: counterexample generator higherorder logic based relational model finder. Proceedings First International
Conference Interactive Theorem Proving, pp. 131146. Springer.
Brandl, F., Brandt, F., Geist, C., & Hofbauer, J. (2015). Strategic abstention based
preference extensions: Positive results computer-generated impossibilities. Proceedings 24th International Joint Conference Artificial Intelligence (IJCAI),
pp. 1824. AAAI Press.
Brandt, F. (2015). Set-monotonicity implies Kelly-strategyproofness. Social Choice
Welfare, 45 (4), 793804.
Brandt, F., & Brill, M. (2011). Necessary sufficient conditions strategyproofness irresolute social choice functions. Proceedings 13th Conference
Theoretical Aspects Rationality Knowledge (TARK), pp. 136142. ACM Press.
Brandt, F., Brill, M., & Harrenstein, P. (2016a). Tournament solutions. Brandt, F.,
Conitzer, V., Endriss, U., Lang, J., & Procaccia, A. D. (Eds.), Handbook Computational Social Choice, chap. 3. Cambridge University Press.
Brandt, F., Geist, C., & Harrenstein, P. (2016b). note McKelvey uncovered set
Pareto optimality. Social Choice Welfare, 46 (1), 8191.
600

fiFinding Strategyproof Social Choice Functions via SAT Solving

Brandt, F., Geist, C., & Peters, D. (2016c). Optimal bounds no-show paradox via
SAT solving. Proceedings 15th International Conference Autonomous
Agents Multi-Agent Systems (AAMAS), pp. 314322. IFAAMAS.
Brandt, F., Geist, C., & Seedig, H. G. (2014). Identifying k-majority digraphs via SAT
solving. Proceedings 1st AAMAS Workshop Exploring Beyond Worst
Case Computational Social Choice (EXPLORE).
Caminati, M. B., Kerber, M., Lange, C., & Rowat, C. (2015). Sound auction specification
implementation. Proceedings 16th ACM Conference Economics
Computation (ACM-EC), pp. 547564. ACM Press.
Chatterjee, S., & Sen, A. (2014). Automated reasoning social choice theory
remarks. Mathematics Computer Science, 8 (1), 510.
Ching, S., & Zhou, L. (2002). Multi-valued strategy-proof social choice rules. Social Choice
Welfare, 19 (3), 569580.
Cina, G., & Endriss, U. (2015). syntactic proof Arrows theorem modal logic
social choice functions. Proceedings 14th International Conference
Autonomous Agents Multi-Agent Systems (AAMAS), pp. 10091017. IFAAMAS.
Conitzer, V., & Sandholm, T. (2002). Complexity mechanism design. Proceedings
18th Annual Conference Uncertainty Artificial Intelligence (UAI), pp.
103110.
Drummond, J., Perrault, A., & Bacchus, F. (2015). SAT effective complete
method solving stable matching problems couples. Proceedings 24th
International Joint Conference Artificial Intelligence (IJCAI), pp. 518525. AAAI
Press.
Duggan, J., & Schwartz, T. (2000). Strategic manipulability without resoluteness shared
beliefs: Gibbard-Satterthwaite generalized. Social Choice Welfare, 17 (1), 8593.
Erdamar, B., & Sanver, M. R. (2009). Choosers extension axioms. Theory Decision,
67 (4), 375384.
Feldman, A. (1979). Manipulation Pareto rule. Journal Economic Theory, 21,
473482.
Fishburn, P. C. (1972). Even-chance lotteries social choice theory. Theory Decision,
3 (1), 1840.
Fishburn, P. C. (1977). Condorcet social choice functions. SIAM Journal Applied Mathematics, 33 (3), 469489.
Fisher, D. C., & Reeves, R. B. (1995). Optimal strategies random tournament games.
Linear Algebra Applications, 217, 8385.
Frechette, A., Newman, N., & Leyton-Brown, K. (2016). Solving station repacking problem. Proceedings 30th AAAI Conference Artificial Intelligence (AAAI).
AAAI Press.
Gardenfors, P. (1976). Manipulation social choice functions. Journal Economic Theory,
13 (2), 217228.
601

fiBrandt & Geist

Gardenfors, P. (1979). definitions manipulation social choice functions. Laffont,
J. J. (Ed.), Aggregation Revelation Preferences. North-Holland.
Geist, C., & Endriss, U. (2011). Automated search impossibility theorems social
choice theory: Ranking sets objects. Journal Artificial Intelligence Research, 40,
143174.
Grandi, U., & Endriss, U. (2013). First-order logic formalisation impossibility theorems
preference aggregation. Journal Philosophical Logic, 42 (4), 595618.
Kelly, J. S. (1977). Strategy-proofness social choice functions without single-valuedness.
Econometrica, 45 (2), 439446.
Laslier, J.-F. (1997). Tournament Solutions Majority Voting. Springer-Verlag.
Liffiton, M. H., & Sakallah, K. A. (2008). Algorithms computing minimal unsatisfiable
subsets constraints. Journal Automated Reasoning, 40 (1), 133.
McGarvey, D. C. (1953). theorem construction voting paradoxes. Econometrica,
21 (4), 608610.
McKay, B. D., & Piperno, A. (2013). Practical graph isomorphism, II. Journal Symbolic
Computation.
Nehring, K. (2000). Monotonicity implies generalized strategy-proofness correspondences. Social Choice Welfare, 17 (2), 367375.
Nipkow, T. (2009). Social choice theory HOL: Arrow Gibbard-Satterthwaite. Journal
Automatated Reasoning, 43, 289304.
Sanver, M. R., & Zwicker, W. S. (2012). Monotonicity properties adaption
irresolute social choice rules. Social Choice Welfare, 39 (23), 371398.
Sato, S. (2008). strategy-proof social choice correspondences. Social Choice Welfare,
31, 331343.
Sato, S. (2013). sufficient condition equivalence strategy-proofness nonmanipulability preferences adjacent sincere one. Journal Economic Theory, 148, 259278.
Scott, A., & Fey, M. (2012). minimal covering set large tournaments. Social Choice
Welfare, 38 (1), 19.
Tang, P., & Lin, F. (2009). Computer-aided proofs Arrows impossibility
theorems. Artificial Intelligence, 173 (11), 10411053.
Tang, P., & Lin, F. (2011). Discovering theorems game theory: Two-person games
unique pure nash equilibrium payoffs. Artificial Intelligence, 175 (1415), 20102020.
Taylor, A. D. (2005). Social Choice Mathematics Manipulation. Cambridge
University Press.
Tseitin, G. S. (1983). complexity derivation propositional calculus. Automation Reasoning, pp. 466483. Springer.

602

fiJournal Artificial Intelligence Research 55 (2016) 995-1023

Submitted 08/15; published 04/16

Distributed Representation-Based Framework
Cross-Lingual Transfer Parsing
Jiang Guo
Wanxiang Che

JGUO @ IR . HIT. EDU . CN
CAR @ IR . HIT. EDU . CN

Research Center Social Computing Information Retrieval
Harbin Institute Technology
Harbin, Heilongjiang, China

David Yarowsky

YAROWSKY @ JHU . EDU

Center Language Speech Processing
Johns Hopkins University
Baltimore, MD, USA

Haifeng Wang

WANGHAIFENG @ BAIDU . COM

Baidu Inc., Beijing, China

Ting Liu

TLIU @ IR . HIT. EDU . CN

Research Center Social Computing Information Retrieval
Harbin Institute Technology
Harbin, Heilongjiang, China

Abstract
paper investigates problem cross-lingual transfer parsing, aiming inducing dependency parsers low-resource languages using training data resource-rich
language (e.g., English). Existing model transfer approaches typically dont include lexical features, transferable across languages. paper, bridge lexical feature gap
using distributed feature representations composition. provide two algorithms
inducing cross-lingual distributed representations words, map vocabularies two different languages common vector space. Consequently, lexical features non-lexical
features used model cross-lingual transfer. Furthermore, framework flexible
enough incorporate additional useful features cross-lingual word clusters. combined
contributions achieve average relative error reduction 10.9% labeled attachment score
compared delexicalized parser, trained English universal treebank transferred
three languages. also significantly outperforms state-of-the-art delexicalized models augmented projected cluster features identical data. Finally, demonstrate models
boosted minimal supervision (e.g., 100 annotated sentences) target languages, great significance practical usage.

1. Introduction
Dependency Parsing one long-standing central problems natural language processing (NLP). goal dependency parsing induce implicit tree structures natural language
sentence following dependency grammar, highly beneficial various downstream
tasks, question answering, machine translation knowledge mining/representation.
majority work dependency parsing dedicated resource-rich languages, English Chinese. languages, exists large-scale annotated treebanks used
2016 AI Access Foundation. rights reserved.

fiG UO , C , YAROWSKY, WANG & L IU

supervised training dependency parsers, Penn Treebank (Marcus, Marcinkiewicz,
& Santorini, 1993; Xue, Xia, Chiou, & Palmer, 2005). However, languages
world, even labeled training data parsing, labor intensive
time consuming manually annotate treebanks languages. fact given rise
range research unsupervised methods (Klein & Manning, 2004), transfer methods (Hwa, Resnik, Weinberg, Cabezas, & Kolak, 2005; McDonald, Petrov, & Hall, 2011) linguistic
structure prediction.
Considering unsupervised methods fall far behind transfer methods terms
accuracy, well difficulty evaluation, focus transfer methods study.
attempt build parsers low-resource languages exploiting treebanks resource-rich
languages. two approaches linguistic transfer general, namely data transfer
model transfer. Data transfer methods emphasizes creation artificial training data
used supervised training target language side. appealing property
learn language-specific linguistic structures effectively. major drawbacks
requirement parallel data noise automatically created training data introduced
word alignment-based projection. hand, model transfer methods build models
source language side, used directly parsing target languages without need
creating annotated data target languages.
paper falls latter category. major obstacle transferring parsing system
one language another lexical features (e.g., words) directly transferable
across languages. address challenge, McDonald et al. (2011) built delexicalized parser parser non-lexical features. delexicalized parser makes sense POS
tag features significantly predictive unlabeled dependency parsing. However, labeled
dependency parsing, especially semantic-oriented dependencies like Stanford typed dependencies (De Marneffe et al., 2006; De Marneffe & Manning, 2008), non-lexical features
predictive enough. Tackstrom, McDonald, Uszkoreit (2012) proposed learn cross-lingual
word clusters multilingual paralleled unlabeled data word alignments, apply
clusters features semi-supervised delexicalized parsing. Word clusters thought
kind coarse-grained representations words. Thus, approach partially fills gap lexical
features cross-lingual learning dependency parsing.
paper proposes novel approach cross-lingual dependency parsing based
pure distributed feature representations. contrast discrete feature representations used
traditional dependency parsers, distributed representations map symbolic features continuous
representation space, shared across languages. Therefore, model ability
utilize lexical non-lexical features naturally. Specifically, framework contains two
primary components:
neural network-based dependency parser. expect non-linear model dependency
parsing study, distributed feature representations shown effective non-linear architectures linear architectures (Wang & Manning, 2013). Chen
Manning (2014) proposed transition-based dependency parser using neural network
architecture, simple works well benchmark datasets. Briefly, model simply replaces predictor transition-based dependency parser well-designed neural
network classifier. provide explanations merits model Section 3,
well adapt cross-lingual task.
996

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

Cross-lingual word representation learning. key filling lexical feature gap
project representations features different languages common vector
space, preserving translational equivalence. study compare two approaches
learning cross-lingual word representations Section 4. first approach named
robust projection, second approach based canonical correlation analysis.
approaches simple implement scalable large data.
Another drawback model transfer methods focus universal structures across various languages, thus lack ability recovering target language-specific
structures. Therefore, necessary conduct target language adaptation top transferred models. introduce practical straightforward solution incorporating minimal
supervision target languages (Section 6).
evaluate models universal multilingual treebanks v2.0 (McDonald et al., 2013).
Case studies include transferring English (EN) German (DE), Spanish (ES) French
(FR). Experiments show incorporating lexical features, performance cross-lingual
dependency parsing improved significantly. embedding cross-lingual cluster features (Tackstrom et al., 2012), achieve average relative error reduction 10.9% labeled
attachment score (LAS), compared delexicalized parsers. also significantly outperforms delexicalized models McDonald et al. augmented cluster features identical
data. addition, show using small amount labeled training data (e.g., 100 sentences) target language side parameter adaptation (minimal supervision), performance
cross-lingual transfer system boosted, recalls language-specific dependency
structures improved dramatically.1
original major contributions paper include:
propose novel flexible cross-lingual learning framework dependency parsing
based distributed representations, effectively incorporate lexical nonlexical features.
present two novel effective approaches inducing cross-lingual word representation
bridge lexical feature gap cross-lingual dependency parsing transfer.
show cross-lingual word cluster features effectively embedded model,
leading significant additive improvements.
show cross-lingual transfer systems easily effectively adapted
target languages minimal supervision, demonstrating great potential practical usage.

2. Background
section describes necessary background crucial understanding transfer
parsing framework.
1. article thoroughly revised extended version work Guo, Che, Yarowsky, Wang, Liu (2015).
provide detailed linguistic methodological background cross-lingual parsing. Additional extensions
primarily include experiments analysis target language adaptation minimal supervision. system
made publicly available at: https://github.com/jiangfeng1124/acl15-clnndep.

997

fiG UO , C , YAROWSKY, WANG & L IU

punct
root

dobj
nsubj

ROOT


PRON

amod


VERB

good
ADJ

control
NOUN

.
.

Figure 1: example labeled dependency tree.

2.1 Dependency Parsing
Given input sentence x = w0 w1 ...wn wi ith word x, goal dependency
parsing build dependency tree, denoted = {(h, m, l) 0 h n; 0 <
n, l L}, (h, m, l) indicates directed arc head word wh modifier wm
dependency label l, L label set (Figure 1).
mainstream models proposed dependency parsing described
either graph-based models transition-based models (McDonald & Nivre, 2007). Graph-based
models (Eisner, 1996; McDonald, Crammer, & Pereira, 2005) view parsing problem finding
highest scoring tree directed graph. score dependency tree typically factored
scores small independent structures. way factorization defines order
model also complexity inference process (McDonald & Pereira, 2006; Carreras,
2007; Koo & Collins, 2010). instance, first-order models factored dependency arcs,
thus also known arc-factored models. Higher-order models would consider expressive
substructures sibling grandchild structures. Transition-based models instead aim
predict transition sequence initial parser state terminal states, conditioned
parsing history (Yamada & Matsumoto, 2003; Nivre, 2003; Nivre, Hall, & Nilsson, 2004).
approach lot interest since fast (linear time projective parsing) incorporate
rich non-local features (Zhang & Nivre, 2011).
considered past simple transition-based parsing using greedy decoding
local training accurate graph-based parsers globally trained use exact
inference algorithms. However, Chen Manning (2014) showed greedy transition-based
parsers significantly improved well-designed neural network architecture. approach considered new paradigm parsing, based pure distributed
feature representations. recently, architecture improved different ways.
example, Weiss, Alberti, Collins, Petrov (2015) combined neural network structured
perceptron, use beam-search decoding, achieving new state-of-the-art performance. Dyer, Ballesteros, Ling, Matthews, Smith (2015) instead explored novel techniques learning
better representations parser states utilizing long short-term memory networks (LSTM).
work also includes Zhou, Zhang, Huang, Chen (2015) applied structured learning
beam-search decoding neural network model. study, choose original
Chen & Mannings architecture, without losing generality, build basic dependency parsing
models cross-lingual transfer.
998

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

2.2 Distributed Representations NLP
Recent years seen numerous attempts learning distributed representations different natural language objects, morphemes, words phrases, sentences documents. Using
distributed representations, symbolic units embedded dense, continuous lowdimensional vector space, thus often referred embeddings.2
Distributed representation attractive NLP several reasons. First, provides straightforward way measuring similarities natural language objects. distributed
representations, easily tell two words/phrases/documents similar semantic
even aspects simply measuring cosine distance vectors.
Second, learned large-scale unannotated data general, thus highly beneficial various downstream applications source alleviate data sparsity.
straightforward way applying distributed representations NLP tasks fed distributed
feature representations existing supervised NLP systems augmented features, semisupervised fashion (Turian, Ratinov, & Bengio, 2010). Despite simplicity effectiveness,
shown potential distributed representations cannot fully exploited generalized linear models adopted traditional NLP systems (Wang & Manning,
2013). One remedy discretize distributed feature representations, convert continuous, dense low-dimensional vectors traditional discrete, sparse high-dimensional
space, studied Guo, Che, Wang, Liu (2014). However, believe non-linear system
(e.g., neural network) powerful promising solution. decent progress already
made paradigm NLP various tasks, neural sequence labeling (Collobert
et al., 2011), dependency parsing (Chen & Manning, 2014), sentence classification (Kim, 2014)
machine translation (Sutskever, Vinyals, & Le, 2014).
Third, provides kind representation shared across languages, tasks
even diverse modalities data resources. property motivated lines research multilingual representation learning (Klementiev et al., 2012; Chandar P et al., 2014; Hermann &
Blunsom, 2014), multi-task learning (Collobert & Weston, 2008) multi-modal learning (Srivastava & Salakhutdinov, 2012). also primary motivation work facilitates
cross-lingual transfer parsing via multilingual distributed representation learning words.

3. Cross-Lingual Dependency Parsing
section, first describe primary transition-based dependency parsing model utilizing
neural networks, details cross-lingual transfer.
3.1 Neural Network Architecture Transition-Based Dependency Parsing
section, first briefly describe transition-based dependency parsing arc-standard
parsing algorithm. revisit neural network architecture transition-based dependency
parsing proposed Chen Manning (2014).
discussed Section 2.1, transition-based parsing generates dependency tree predicting transition sequence initial parser state terminal state. Several transition-based
parsing algorithms presented literature, arc-standard arc-eager algorithms projective parsing (Nivre, 2003, 2004), list-based algorithm (Nivre, 2008)
2. paper, two terminologies used interchangeably.

999

fiG UO , C , YAROWSKY, WANG & L IU

swap-based algorithm (Nivre, 2009) non-projective parsing. Different algorithms different
transition actions. Take arc-standard algorithm example, parsing state (typically known
configuration) represented tuple consisting stack S, buffer B, partially derived forest (i.e., set dependency arcs) A. Given input word sequence x = w1 w2 , ..., wn ,
initial configuration represented as: [w0 ]S , [w1 w2 , ..., wn ]B , , terminal configuration [w0 ]S , []B , A, w0 pseudo word indicating root whole dependency
tree. Denoting Si (i = 0, 1, ...) ith element stack, Bi (i = 0, 1, ...) ith element buffer,3 arc-standard system defines three types transition actions: L EFT-A RC(r),
R IGHT-A RC(r), HIFT, r dependency relation.
r

L EFT-A RC(r): extend new arc (S1
S0 ) (S0 head S1 modifier)
remove S1 stack.
r

R IGHT-A RC(r): extend new arc (S1
S0 ) (S1 head S0 modifier)
pop S0 stack.
HIFT: move B0 buffer stack. Precondition B empty.
typical approach greedy arc-standard parsing build multi-class classifier (e.g.,
support vector machines, maximum entropy models) predicting transition action given feature vector extracted specific configuration. conventional feature engineering suffers
problem sparsity, incompleteness expensive feature computation (Chen & Manning,
2014), neural network model provides effective solution.
architecture neural network based dependency parsing model illustrated Figure 2. Unlike high-dimensional, sparse discrete features used traditional parsing models,
neural network model, apply distributed feature representations. Primarily, three types
information extracted configuration Chen & Mannings model: word features, POS
features relation features respectively. study, add non-local features including distance features indicating distance two items, valency features indicating
number children given item (Zhang & Nivre, 2011). distance valency features
discretized buckets. features projected embedding layer via corresponding lookup tables (i.e., embedding matrices), estimated training
process. complete feature templates used system shown Table 1.
Then, feature compositions performed hidden layer via cube activation function:
h = g(x) = (W1 [xw , xt , xr , xd , xv ] + b1 )3
W1 weight matrix input layer hidden layer, b1 bias vector.
Feature compositions important dependency parsing NLP general.
Researchers used cost-intensive manual feature engineering design large set feature
templates. However, approach cannot cover potentially useful features. Lei, Xin, Zhang,
Barzilay, Jaakkola (2014) showed full feature representation derived
Kronecker product multiple views features, results tensor model. representing
tensor low-rank form using C ANDECOMP /PARAFAC (CP) tensor decomposition (Kolda &
Bader, 2009), number parameters effectively reduced, thus suitable tasks
limited training data (Cao & Khudanpur, 2014).
3. S0 /B0 top/head element stack/buffer.

1000

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

Softmax Layer:
= ( )



Hidden Layer:
= = ( + )3



Transition Actions


Hidden Representation


Input Layer:

= [ , , , , , ]

Feature Extraction





Words

Clusters

Lexical features
ROOT

Parsing Configurations

Stack
has_VERB

Lookup Tables
1


POS tags



,

Relations

Distance,
Valency

Non-lexical features
good_ADJ

Buffer
Control_NOUN

._.

nsubj
He_PRON

Figure 2: Neural network model dependency parsing. Cluster features introduced
Section 5.2 5.3.
Type

Feature Templates
w
, = 0, 1, 2
ESwi , EB


Word

w
w
w
w
Elc1(S
, Erc1(S
, Elc2(S
, Erc2(S
, = 0, 1
i)
i)
i)
i)
w
w
Elc1(lc1(S
, Erc1(rc1(S
, = 0, 1
))
))

, = 0, 1, 2
ESt , EB


POS





Elc1(S
, Erc1(S
, Elc2(S
, Erc2(S
, = 0, 1
i)
i)
i)
i)


Elc1(lc1(S
, Erc1(rc1(S
, = 0, 1
))
))

Relation

r
r
r
r
Elc1(S
, Erc1(S
, Elc2(S
, Erc2(S
, = 0, 1
i)
i)
i)
i)
r
r
Elc1(lc1(S
, Erc1(rc1(S
, = 0, 1
))
))

Distance



ES
, ES
0 ,S1
0 ,B0

Valency

ESlv0 , ESlv1 , ESrv1

Table 1: Feature templates neural network model transition-based dependency parsing.
{w,c,t,r,d,lv,rv}
Ep
indicates various feature embeddings element position p. lc1
(rc1) first child left (right) lc2 (rc2) second child left (right).

indicates lexical features, indicates non-lexical features.
suggest cube activation function g(x) = x3 viewed special case
low-rank tensor. verification, g(x) expanded as:
g(w1 x1 + ... + wm xm + b) =
(wi wj wk )xi xj xk + b(wi wj )xi xj + ...

i,j,k

i,j

1001

fiG UO , C , YAROWSKY, WANG & L IU

treat bias term b x0 x0 = 1, weight corresponding feature
combination xi xj xk wrote wi wj wk , exactly rank-1 component tensor low-rank form using CP tensor decomposition. Consequently, cube activation function
implicitly derives full feature combinations. fact, add many features possible
input layer improve parsing accuracy. show Section 5.2 Brown-cluster
features readily incorporated model.
composed features propagated output layer, generating probabilistic distribution output labels (i.e., transition actions) via softmax activation function: =
sof tmax(W2 h). use following objective function train model:
J () =

1 N
2
CrossEnt(di , yi ) +
N i=0
2

CrossEnt(p, q) cross-entropy two distributions p q:
CrossEnt(p, q) = pk ln qk
k

parameters trained using back-propagation. model, typically consists
embedding matrices weights network. However, cases, may exclude
word embedding matrix E w , indicates word embeddings constrained fixed
(i.e., without updating) training.
3.2 Cross-Lingual Transfer
idea cross-lingual transfer using parser examined straightforward. contrast
traditional approaches discard rich lexical features (delexicalizing) transferring
models one language another, model transferred using full model trained
source language side (i.e., English).
Since non-lexical feature (POS, relation, distance, valency) embeddings directly transferable languages, key component framework cross-lingual learning
lexical feature embeddings (i.e., word embeddings). cross-lingual word embeddings
induced, first learn dependency parser source language side. that, parser
directly used parsing target language data.
3.2.1 U NIVERSAL EPENDENCIES
discussed previously, cross-lingual model transfer assumes universal grammatical structures
identified multiple languages. Therefore, evaluated test set target language
either unlabeled attachment score (UAS) labeled attachment score (LAS), performance
transfer parsing rely heavily multilingual consistency annotation schemes. Generally
syntactic annotation schemes differ head-finding rules (e.g., choice lexical versus functional head) dependency relation labels (i.e., syntactic tagset). challenging task
construct multilingual treebanks consistent annotations. initial cross-lingual parsing studies, CoNLL shared task datasets (Buchholz & Marsi, 2006) broadly used. However,
inconsistencies occur head-finding rules syntactic tagset across languages,
made difficult evaluate cross-lingual parsers.
1002

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

order overcome difficulties, new collection multilingual treebanks homogeneous syntactic dependency annotation presented recently, namely Universal Dependency Treebanks (UDT) (McDonald et al., 2013). universal annotation scheme created
harmonizing available treebanks slightly different variants Stanford typed dependencies (De Marneffe et al., 2006), along universal Part-of-Speech tags (Petrov, Das, & McDonald, 2012). dataset greatly facilitates research multilingual syntactic analysis, also
makes possible use LAS evaluation. fact, UDT already used standard
dataset benchmarking research cross-lingual transfer parsing (Ma & Xia, 2014; Tiedemann,
2014; Zhang & Barzilay, 2015; Duong, Cohn, Bird, & Cook, 2015a, 2015b; Rasooli & Collins,
2015). efforts towards universal dependencies include recent Universal Dependencies project (UD) 4 HamleDT (Zeman et al., 2014). paper, conduct experiments
UDT (v2.0) 5 dataset without losing generality.
3.2.2 P ROJECTIVE VS . N - PROJECTIVE PARSING
Non-projectivity common phenomenon multilingual dependency parsing. term nonprojectivity indicates dependency tree crossing-arcs, often appear morphologically rich languages. Various algorithms proposed graph-based transitionbased parsing algorithms produce non-projective trees. example, arc-standard algorithm
(Section 3.1) readily extended adding swap action handle non-projectivity,
gives expected linear worst-case O(n2 ) complexity (Nivre, 2009). strategies include
list-based algorithm (Nivre, 2008) adapted Covington algorithm (Covington, 2001), combination list-based swap-based algorithm (Choi &
McCallum, 2013). Unfortunately, systematically comparison different
algorithms literature far.
study, however, focus projective parsing non-projective
trees source language (English) training data. Consequently, non-projectivities target languages handled moment.6

4. Cross-Lingual Word Representation Learning
Prior introducing approaches cross-lingual word representation learning, briefly review
basic model learning monolingual word embeddings, constitutes subprocedure
cross-lingual approaches.
4.1 Continuous Bag-of-Words Model
recent years, various approaches studied learning word embeddings largescale plain texts. approaches generally derived so-called distributional hypothesis (Firth, 1957): shall know word company keeps. study, consider
Continuous Bag-of-Words (CBOW) model (Mikolov, Chen, Corrado, & Dean, 2013) imple4. https://universaldependencies.github.io/docs/
5. https://github.com/ryanmcd/uni-dep-tb
6. Note target languages address paper, non-projectivity pervasive. Specifically, proportion projective trees presented training corpus respectively 91% DE, 94% ES, 88%
FR.

1003

fiG UO , C , YAROWSKY, WANG & L IU

mented open-source toolkit word2vec.7 basic principle CBOW model predict
individual word sequence given bag context words within fixed window size
input, using log-linear classifier. model avoids non-linear transformation hidden
layers, hence trained high efficiency.
large window size, grouped words using resulting word embeddings topically similar; whereas small window size, grouped words syntactically similar (Bansal, Gimpel, & Livescu, 2014). set window size 1 parsing task.
Next, introduce approach inducing bilingual word embeddings. general, expect
bilingual word embeddings preserve translational equivalences. example, cooking (English) close translation: kochen (German) embedding space.
4.2 Robust Alignment-Based Projection
first method inducing cross-lingual word embeddings two stages. First, learn word
embeddings source language (S) corpora monolingual case, project
monolingual word embeddings target language (T), based word alignments.
Given sentence-aligned parallel corpus D, first conduct unsupervised bidirectional word
alignment, collect alignment dictionary. Specifically, word-aligned sentence pair
D, keep alignments conditional alignment probability exceeding threshold = 0.95
discard others. Specifically, let = {(wiT , wjS , ci,j ), = 1, 2, ..., NT ; j = 1, 2, ..., NS }
alignment dictionary, ci,j number times ith target word wiT aligned
j th source word wjS . NS NT vocabulary sizes. use shorthand (i, j)
denote word pair . projection formalized weighted average
embeddings translation words:
ci,j
v(wiT ) =
v(wjS )
(1)

c
i,
(i,j)AT
ci, = j ci,j , v(w) embedding w.
Obviously, simple projection method one drawback: assigns word embeddings
target language words occur word aligned data, typically smaller
monolingual datasets. Therefore, order improve robustness projection, utilize
morphology-inspired mechanism, propagate embeddings in-vocabulary words out-ofT
vocabulary (OOV) words. Specifically, OOV word woov
, extract list candidate
words similar terms edit distance (Levenshtein distance), set averaged

vector embedding woov
. formally,

v(woov
) = Avg (v(w ))
w C


C = {ww EditDist(woov
, w) }

(2)

reduce noise, choose small edit distance threshold = 1.
process robust projection viewed two-stage graph-propagation algorithm,
illustrated Figure 3 (left panel). Embeddings first propagated source language words
target language words appear bilingual lexicons. Next, monolingual propagation
performed obtain OOV word embeddings target language, using edit distance metric.
7. http://code.google.com/p/word2vec/

1004

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

1

Source Language



1
Bilingual Lexicon
(weighted)

Target Language

Parallel data
Wiktionary
PanLex




2



1





2













2

2



CCA
1



In-Vocabulary words
Out-of-Vocabulary words





Figure 3: Illustration robust projection (left) CCA (right) inducing cross-lingual word
embeddings.

4.3 Canonical Correlation Analysis
second approach consider similar Faruqui Dyer (2014), uses CCA
improve monolingual word embeddings multilingual correlation. CCA way measuring
linear relationship multidimensional variables. two multidimensional variables,
CCA aims find two projection matrices map original variables new basis (lowerdimensional), correlation two variables maximized.
refer readers work Hardoon, Szedmak, Shawe-Taylor (2004) theoretical
foundations algorithm specifics CCA. lets treat CCA black box, see CCA
applied inducing bilingual word embeddings. Suppose already two pre-trained
monolingual word embeddings (e.g., English German): Rn1 d1 Rn2 d2 .
first step, extract one-to-one alignment dictionary alignment dictionary
AST .8 Here, , indicating every word translated one word , vice
versa.
process illustrated Figure 3 (right panel). Denoting dimension resulting word
embeddings min(d1 , d2 ). First, derive two projection matrices V Rd1 , W Rd2
respectively using CCA:
V, W = CCA( , )

(3)

Then, V W used project entire vocabulary :
= V,

= W

(4)

Rn1 Rn2 resulting word embeddings cross-lingual task.
8. also worth trying, observed slight performance degradation experimental setting.

1005

fiG UO , C , YAROWSKY, WANG & L IU

4.4 Pros Cons
Contrary robust projection approach, CCA assigns embeddings every word monolingual vocabulary. However, one potential limitation CCA assumes linear transformation
word embeddings, difficult satisfy. mean time, training source language
parser using CCA cross-lingual word embeddings, constrained E w fixed,
mentioned Section 3.1, otherwise, translational equivalence broken. robust projection approach, however, doesnt limitation. discussion experiments
presented Section 5.3.2.
Note approaches generalized lower-resource languages parallel bitexts
available. way, dictionary readily obtained either using bilingual lexicon
induction approaches (Mann & Yarowsky, 2001; Koehn & Knight, 2002; Haghighi, Liang, BergKirkpatrick, & Klein, 2008), online-resources like Wiktionary9 Panlex.10

5. Experiments
section describes experiments. first describe data settings used experiments, results.
5.1 Data Settings
pre-training word embeddings, use WMT-2011 monolingual news corpora
English, German Spanish.11 French, combined WMT-2011 WMT-2012 monolingual news corpora.12 got word alignment counts using fast-align toolkit cdec (Dyer
et al., 2010) parallel news commentary corpora (WMT 2006-10) combined Europarl corpus English{German, Spanish, French}.13
training neural network dependency parser, set number hidden units
400. dimension embeddings different features shown Table 2.

Dim.

Word
50

POS
50

Label
50

Distance
5

Valency
5

Cluster
8

Table 2: Dimensions various types feature embeddings.
Mini-batch adaptive stochastic gradient descent (AdaGrad) (Duchi, Hazan, & Singer, 2011)
used optimization. CCA approach, use implementation Faruqui Dyer
(2014).
employ universal dependency treebanks (UDT v2.0) reliable evaluation
approach cross-lingual dependency parsing. universal multilingual treebanks annotated
using universal POS tagset (Petrov et al., 2012) contains 12 POS tags, well
universal dependencies defines 40 dependency relations. follow standard split
treebanks languages.
9.
10.
11.
12.
13.

https://www.wiktionary.org/
http://panlex.org/
http://www.statmt.org/wmt11/
http://www.statmt.org/wmt12/
http://www.statmt.org/europarl/

1006

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

5.2 Baseline Systems
compare approach following systems.
first baseline, evaluate delexicalized transfer neural network-based parser
[D ELEX], use non-lexical features (Figure 2). investigate effect
non-local features (distance, valency). delexicalized systems include
non-local features referred [D ELEX (basic)].
also compare approach delexicalized parser presented McDonald et al.
(2013) [M C D13], used perceptron-based transition-based parser beam size 8,
along richer non-local features (Zhang & Nivre, 2011). re-implementation approach
framework Zpar (Zhang & Clark, 2011) referred [M C D13 ].
Furthermore, consider strong baseline system proposed Tackstrom et al. (2012),
utilized cross-lingual word cluster features enhance perceptron-based delexicalized
parser [M C D13 +Cluster]. use alignment dictionary described Section 4.2
induce cross-lingual word clusters. re-implement P ROJECTED clustering approach described work Tackstrom et al., assigns target word cluster
often aligned:
c(wiT ) = arg max ci,j 1[c(wjS ) = k]
k

(i,j)AT

Obviously, method also drawback words occur alignment dictionary (OOV) cannot assigned cluster. Therefore, use strategy described
Section 4.2 find likely clusters OOV words. Instead computing average
embeddings, solve argmax problem:

) = arg max
c(woov
k

1[c(w ) = k]

w C

(5)


, w) }
C = {wEditDist(woov

set 1 constantly. Instead clustering model Uszkoreit Brants (2008), use
Brown clustering (1992) induce hierarchical word clusters, word represented
bit-string. use word cluster feature templates Tackstrom et al. (2012), set
number Brown clusters 256.
5.3 Experimental Results
parsing models trained using development data English early-stopping.
Table 3 lists results cross-lingual transfer experiments dependency parsing. Table 4
summarizes experimental gains detailed Table 3.
first examine benefit brought non-local distance valency features. observed
comparison ELEX (basic) ELEX, marginal improvements obtained DE
FR, significant improvements ES. Therefore, adopted features
following experiments.
delexicalized system obtains slightly lower performance reported McDonald
et al. (2013) (M C D13), used greedy decoding local training. re-implementation
McDonald et al.s work attains comparable performance C D13. languages consider study, using cross-lingual word embeddings either alignment-based projection
CCA, obtain statistically significant improvements delexicalized system,
1007

fiG UO , C , YAROWSKY, WANG & L IU

ELEX (basic)
ELEX
P ROJ
P ROJ+Cluster
CCA
CCA+Cluster

Unlabeled Attachment Score (UAS)
EN
DE
ES
FR
AVG
83.63 56.85 67.28 68.70 64.28
83.67 57.01 68.05 68.85 64.64
91.96 60.07 71.42 71.36 67.62
92.33 60.35 71.90 72.93 68.39
90.62 59.42 68.87 69.58 65.96
92.03 60.66 71.33 70.87 67.62

Labeled Attachment Score (LAS)
EN
DE
ES
FR
AVG
79.37 47.06 56.43 57.73 53.74
79.42 47.12 56.99 57.78 53.96
90.48 49.94 61.76 61.55 57.75
90.91 51.54 62.28 63.12 58.98
88.88 49.32 59.65 59.50 56.16
90.49 51.29 61.69 61.50 58.16

C D13

83.33

58.50

68.07

70.14

65.57

78.54

48.11

56.86

58.20

54.39

C D13
C D13 +Cluster

84.44
90.21

57.30
60.55

68.15
70.43

69.91
72.01

65.12
67.66

80.30
88.28

47.34
50.20

57.12
60.96

58.80
61.96

54.42
57.71

Table 3: Cross-lingual transfer dependency parsing English test dataset 4 universal multilingual treebanks. Results measured unlabeled attachment score (UAS)
labeled attachment score (LAS). ELEX (basic) delexicalized model without nonlocal features (distance, valency). denotes re-implementation C D13. Since
model varies different target languages CCA-based approach, indicates
averaged UAS/LAS.

Experimental Contribution
P ROJ
vs. ELEX
CCA
vs. ELEX
P ROJ
vs. C D13
CCA
vs. C D13
P ROJ+Cluster
vs. P ROJ
CCA+Cluster
vs. CCA
C D13 +Cluster vs. C D13
P ROJ+Cluster
vs. ELEX
CCA+Cluster
vs. ELEX
P ROJ+Cluster
vs. C D13
CCA+Cluster
vs. C D13
P ROJ+Cluster
vs. C D13 +Cluster
CCA+Cluster
vs. C D13 +Cluster

DE/ES/FR Avg. (Relative)
+3.79 (8.2%)
+2.19 (4.8%)
+3.33 (7.3%)
+1.74 (3.8%)
+1.23 (2.9%)
+2.00 (4.6%)
+3.29 (7.2%)
+5.02 (10.9%)
+4.20 (9.1%)
+4.46 (9.8%)
+3.74 (8.2%)
+1.27 (3.0%)
+0.45 (1.1%)

Table 4: Summary experimental gains detailed Table 3, absolute LAS gain
relative error reduction. gains statistically significant using MaltEval (Nilsson
& Nivre, 2008) p < 0.01.

UAS LAS. Interestingly, notice P ROJ consistently outperforms CCA significant
margin, comparable C D13 +Cluster. analysis observation conducted Section 5.3.1 5.3.2.
1008

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

Type
Cluster

Feature Templates
c
ESc , EB
, = 0, 1, 2

c
c
c
c
Elc1(Si ) , Erc1(S
, Elc2(S
, Erc2(S
, = 0, 1
i)
i)
i)

c
c
Elc1(lc1(S
, Erc1(rc1(S
, = 0, 1
))
))

Table 5: Word cluster feature templates.

framework flexible incorporating richer features simply embedding
continuous vectors. Thus embed cross-lingual word cluster features model,
together proposed cross-lingual word embeddings. cluster feature templates shown
Table 5, similar POS tag feature templates. shown Table 3, significant
additive improvements obtained P ROJ CCA embedding cluster features.
Compared delexicalized system, relative error reduced 13.1% UAS,
12.6% LAS. combined system outperforms C D13 +Cluster significantly .
5.3.1 E FFECT ROBUST P ROJECTION
Since P ROJ induction cross-lingual word clusters, use edit distance measure
OOV words, would like see affects performance parsing.
Intuitively, higher coverage projected words test dataset promote parsing
performance more. verify this, conduct experiments settings using
P ROJ+Cluster model. robust projection, examine effect edit distances ranging
1 3. Results shown Table 6. Improvements observed languages using
robust projection edit distance measure, especially FR, highest coverage gain
obtained robust projection. also observe slightly improvements DE ES using
edit distance 2. performance starts degrade gets larger. reasonable, since
larger edit distance increases word coverage, also introduces noise.

Simple

DE

ES

FR

coverage
UAS
LAS
coverage
UAS
LAS
coverage
UAS
LAS

91.37
59.74
50.84
94.51
70.97
61.34
90.83
71.17
61.72

=1
94.70
60.35
51.54
96.67
71.90
62.28
97.60
72.93
63.12

Robust
=2
96.50
60.53
51.70
97.75
72.00
62.34
98.33
72.79
63.02

Table 6: Effect robust projection.

1009

=3
97.47
60.53
51.69
98.47
71.93
62.27
98.58
72.70
62.94

fiG UO , C , YAROWSKY, WANG & L IU

5.3.2 E FFECT F INE -T UNING W ORD E MBEDDINGS
Another reason effectiveness P ROJ CCA lies fine-tuning word embeddings
training parser.
CCA viewed joint method inducing cross-lingual word embeddings.
training source language dependency parser cross-lingual word embeddings derived
CCA, EN word embeddings fixed. Otherwise, translational equivalence
broken. However, P ROJ, limitation. Word embeddings updated
non-lexical feature embeddings, order obtain accurate dependency parser. refer
procedure fine-tuning process word embeddings. verify benefits fine-tuning,
conduct experiments see relative loss word embeddings fixed training. Results
shown Table 7, indicates fine-tuning indeed offers considerable help.

DE
ES
FR

UAS
LAS
UAS
LAS
UAS
LAS

Fixed
59.74
49.44
70.10
61.31
70.65
60.69

Fine-tuning
60.07
49.94
71.42
61.76
71.36
61.50


+0.33
+0.50
+1.32
+0.45
+0.71
+0.81

Table 7: Effect fine-tuning word embeddings.

5.4 Compare Existing Bilingual Word Embeddings
section, compare bilingual embeddings several previous approaches context dependency parsing. best knowledge, first work evaluation
bilingual word embeddings syntactic tasks.
approaches consider include multi-task learning approach (Klementiev et al., 2012)
[MTL], bilingual auto-encoder approach (Chandar P et al., 2014) [B IAE], bilingual compositional vector model (Hermann & Blunsom, 2014) [B ICVM], bilingual bag-of-words
approach (Gouws et al., 2015) [B ILBOWA].
MTL B IAE, adopt released word embeddings directly due inefficiency
training.14 B ICVM B ILBOWA, re-run systems dataset previous
experiments.15 Results summarized Table 8.
CCA P ROJ consistently outperforms approaches languages, P ROJ performs best. inferior performance MTL B IAE partly due low word coverage.
example, cover 31% words universal DE test treebank, whereas CCA
P ROJ covers 70%. Moreover, B IAE, B ICVM B ILBOWA introduce sentence-level translational equivalence objectives regularizers learning bilingual word embeddings.
approaches advantageous dont assume/require word alignment. However, word-toword translational equivalence cannot well preserved way.
14. MTL embeddings normalized training.
15. B ICVM uses bilingual parallel dataset.

1010

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

MTL (Klementiev et al., 2012)
B IAE (Chandar P et al., 2014)
B ICVM (Hermann & Blunsom, 2014)
B ILBOWA (Gouws et al., 2015)
CCA
P ROJ

DE
UAS
LAS
56.93 46.22
53.74 43.68
56.30 46.99
54.51 44.95
59.42 49.32
60.07 49.94

ES
UAS
67.71
58.81
67.78
67.23
68.87
71.42

LAS
58.43
46.66
58.08
56.16
59.65
61.76

FR
UAS
LAS
67.51 57.27
60.10 49.47
69.13 58.13
64.82 52.73
69.58 59.50
71.36 61.55

Table 8: Comparison existing bilingual word embeddings. MTL B IAE, use
released bilingual word embeddings.

Target Word (ES)

china
(china)

problemas
(problems)

septiembre
(september)

P ROJ
india
russia
taiwan
chinese
problem
difficulties
troubles
issues
october
august
january
december

CCA
russia
indonesia
beijing
chinese
problems
woes
troubles
dilemmas
december
july
october
june

Neighboring Words (EN)
MTL
B IAE
china
korea
independent india
sumitomo
chinese
malaysian
brazil
events
problem
sanctions
greatly
conditions
highlighted
laws
scale
december
month
february
april
july
scheduled
march
november

B ICVM
chinese
chinois
sino
33.55
problematic
problematical
difficulties
troubles
11th
11.00
11
eleventh

B ILBOWA
helsinki
bulgarians
constituting
market
deficiencies
situations
omissions
attentively
a.m
p.m
twelve
1998-1999

Table 9: Target words Spanish 4 similar words English, induced various
approaches.

verify assumption, taking EN/ES case study. manually inspect 4
similar words (by cosine similarity) English given set words Spanish (Table 9).
observe semantic syntactic shifting k-nearest neighbors prediction B IAE,
B ICVM B ILBOWA, whereas P ROJ CCA give translational equivalent predictions.
example, B ICVM yields adjective like problematical target noun problemas; B ILBOWA yields
semantic-related word market china. general, P ROJ robust approach, behaving
consistently well sampled words.
worth noting dont assume/require bilingual parallel data CCA P ROJ.
need practice bilingual lexicon paired languages. especially important
generalizing approaches lower-resource languages, parallel texts available.
1011

fiG UO , C , YAROWSKY, WANG & L IU

6. Target-Language Adaptation Minimal Supervision
important us distinguish linguistic structures learned via cross-lingual transfer
versus learned basis monolingual information language
parsed. Intuitively, cross-lingual approaches learn common dependency structures
shared source target language. However, many languages,
specialized (language-specific) syntactic characteristics learned data
target language.
Take adjective-noun order example, Spanish French, adjectives often appears
nouns, thus forming right-directed arc labeled amod, whereas English, amod
(adjectival modifier) arcs mostly left-directed, illustrated Figure 4. Another example
subject-verb-object order. German, verbs often appear end sentence V2 position,
causes much left-directed dobj (direct object) arcs English (Figure 5).
differences clearly observed universal treebanks. Table 10 shows significant
distribution divergence left-directed right-directed arcs dobj amod relations
treebanks different languages.
Relation: dobj; Language: EN vs. DE
dobj
dobj
ratio
EN
38,395
764
50.3 : 1
DE
4,277
3,457
1.2 : 1
Relation: amod; Language: EN vs. ES, FR
amod amod
ratio
EN
1,667
57,864
1 : 34.7
ES
14,876
5,205
2.9 : 1
FR
12,919
4,910
2.6 : 1

Table 10: Distribution divergences left-directed right-directed arcs dobj relation EN
DE (top), amod relation EN ES/FR (bottom).

amod

amod

NOUN

ADJ

NOUN

ADJ

Spanish:

Consejo

Superior

conflictos

sociales

ADJ

NOUN

ADJ

NOUN

English:

Superior

Council

social

conflicts

amod

amod

Figure 4: Reverse direction amod relation Spanish English. French also adjectives following nouns.

1012

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

root
advmod

det

dobj

ADV

DET

NOUN

VERB

DE:

endlich

den

richtigen

gefunden

EN:

finally

found



right man

ADV

VERB

DET

NOUN

advmod

det
dobj
root

Figure 5: Reverse direction dobj relation German English.
Therefore, section, investigate much cross-lingual transfer model improved annotating small amount labeled training data target language side. Even though
building large-scale treebanks low-resource languages supervised learning costly, annotating dependency structures small amount sentences (e.g., 100) difficult.
still conduct experiments universal dependency treebanks, provide labeled
training data multiple languages. language studied (DE, ES, FR), incrementally
augment amount labeled sentences 100 1,000 step 100, adapt parameters cross-lingual transfer model specific target language. Theoretically, since target
language treebanks contain non-projective trees, would make sense apply non-projective
algorithms (e.g., swap-based) target language adaptation. way, however, W2
re-trained scratch, doesnt show good performance experiments since minimally supervised data small. Consequently, still rely arc-standard algorithm
adaption. process almost training source language parser described
Section 3, except word embedding matrix E w fixed, rest parameters
(E {t,l,d,v,c} , W1 , W2 , b1 ) optimized using augmented labeled data target language,
taking Equation 3.1 objective function. development data used process, thus
simply perform parameter updating 2,000 iterations.
addition, built another strong baseline system employs augmented labeled
training data supervised learning. system, utilize word embeddings Brown
clusters features, derived separately language.
shown Figure 6, results really promising. P ROJ+Cluster CCA+Cluster
systems consistently outperform delexicalized system supervised system significant margin. P ROJ+Cluster CCA+Cluster general achieve comparable performances,
CCA+Cluster slightly better.
worthy noting performances P ROJ+Cluster CCA+Cluster boosted
augmenting 100 sentences. Take DE example, UAS increased 60.35% 68.91%,
LAS 51.54% 61.54%, nearly equal effect using 1,000 sentences
supervised learning. observation demonstrates great potential cross-lingual transfer
system practical usage.
1013

fi

85

85

75

80

G UO , C , YAROWSKY, WANG & L IU










80






















UAS







75

UAS

65

UAS






60







75

70




80







70

PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

0

200

400

600

800

1000

0

200






400

600

800

1000

200

600

800

1000












75

75























60







70

70



LAS

50



65

LAS



65

55

LAS

400

80





PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

Labeled training data (FR)



65

0

Labeled training data (ES)

80

70

Labeled training data (DE)





65



65

PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

45

50



70

55






0

200

400

600

800

Labeled training data (DE)

1000



60

PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

55

60



55

PROJ+Cluster
CCA+Cluster
Delexicalized
Supervised

40

45



0

200

400

600

800

Labeled training data (ES)

1000

0

200

400

600

800

1000

Labeled training data (FR)

Figure 6: Target-language adaptation incrementally augmenting labeled training data (sentences) fine-tune cross-lingual transfer model. Performances evaluated using
UAS (top) LAS (bottom). Note points whose x coordinates 0 represent
cross-lingual transfer performance, labeled training data used.

Analysis. primary hypothesis incorporating data target language, model
able learn special syntactic patterns consistent source language.
verify this, study influence target-language adaptation two special relations:
dobj (DE) amod (ES, FR), measuring precision recall changes use
100 target language sentences. Results shown respectively Table 11 Table 12.
observe great improvements recall relations, indicates model indeed gains
ability learning target-language-specific dependency structures supervision
100 sentences.

7. Related Studies
cross-lingual annotation projection method pioneered Yarowsky, Ngai, Wicentowski (2001) shallow NLP tasks (POS tagging, NER, etc.), later applied dependency
parsing (Hwa et al., 2005; Smith & Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann,
2014). work along line dedicated improving robustness syntactic pro1014

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

Relation: dobj; Language: DE
Precision Recall
PROJ+Cluster
41.45
31.09
+100
41.90
51.40

0.45 20.31
CCA+Cluster
39.47
31.74
+100
43.59
57.57

4.12 25.83

Table 11: Effect minimal supervision (100 sentences) dobj.
Relation: amod; Language: ES, FR
ES
FR
Precision Recall Precision Recall
PROJ+Cluster
94.97
80.05
92.94
81.70
+100
91.60
92.52
93.61
95.75

3.37 12.47
0.67 14.05
CCA+Cluster
93.37
77.31
92.08
72.22
+100
91.85
92.77
92.77
96.41

1.52 15.46
0.69 24.19

Table 12: Effect minimal supervision (100 sentences) amod.

jection alleviating noise errors introduced word alignment-based projection. Typical
approaches include soft projection (Li, Zhang, & Chen, 2014), treebank translation (Tiedemann, Agic, & Nivre, 2014), distribution transfer (Ma & Xia, 2014), recently proposed
density-driven projection (Rasooli & Collins, 2015). worth mentioning remarkable results
achieved annotation projection methods (Tiedemann, 2015; Rasooli & Collins,
2015), due large part parsers trained target language side.
cross-lingual model transfer, learning cross-lingual feature representations promising direction. Typical approaches include cross-lingual word clustering (Tackstrom et al., 2012)
employed paper baseline system, projection features (Durrett, Pauls, & Klein, 2012). Kozhevnikov Titov (2014) derived linear projection maps target instances
source-side feature representations, extent similar CCA approach. Xiao
Guo (2014) learned cross-lingual word embeddings applied MSTParser linguistic
transfer, inspired work. Sgaard et al. (2015) obtained multi-source unified word embeddings via inverted indexing Wikipedia, applied various NLP tasks. However,
results didnt show significant improvements parsing. Nevertheless, idea utilizing multisource information learning cross-lingual word embeddings makes great sense. recently,
Duong et al. (2015a, 2015b) also utilized neural network architecture parameter sharing
parsers different languages. However, approach requires annotated treebanks
target language side, makes distinct transfer parsing framework. addition
representation learning, attempts also made integrate monolingual linguistic features parsing models, manually constructed universal dependency parsing rules (Naseem,
1015

fiG UO , C , YAROWSKY, WANG & L IU

Chen, Barzilay, & Johnson, 2010) manually specified typological features (Naseem, Barzilay,
& Globerson, 2012; Zhang & Barzilay, 2015).
Using neural networks dependency parsing new approach. best knowledge, Mayberry Miikkulainen (1999) presented first work explored neural networks
shift-reduce constituent-based parsing. used one-hot feature representations. Henderson
(2004) used simple synchrony network predict parse decisions constituency parser,
first use neural networks broad-coverage Penn Treebank parser. Titov Henderson (2007) applied Incremental Sigmoid Belief Networks constituent-based parsing. Garg
Henderson (2011) later extended work transition-based dependency parsing using Temporal Restricted Boltzman Machine. parsers, however, much less scalable practice.
Earlier progress made using deep learning parsing includes work Collobert (2011)
Socher et al. (2013) constituent-based parsing, Stenetorp (2013) built recursive neural
networks transition-based dependency parsing.

8. Conclusion
paper proposes novel framework based distributed representations cross-lingual dependency parsing. Two algorithms proposed induction cross-lingual word representations,
namely robust projection CCA, bridge lexical feature gap.
Experiments show using cross-lingual word embeddings derived either approach,
transferred parsing performance improved significantly delexicalized system.
notable observation projection method performs significantly better CCA. Additionally, framework flexibly able incorporate cross-lingual word cluster features,
significant gains use. combined system significantly outperforms delexicalized systems languages, average 10.9% error reduction LAS,
significantly outperforms models McDonald et al. (2013) augmented projected word
cluster features.
Furthermore, show performance cross-lingual transfer system specific target language boosted minimal supervision language, great
significance practical usage.

Acknowledgments
grateful Manaal Faruqui providing bilingual resources. thank Ryan McDonald
pointing evaluation issue experiment. also thank Sharon Busching
proofreading anonymous reviewers insightful comments suggestions. work
supported National Key Basic Research Program China via grant 2014CB340503
National Natural Science Foundation China (NSFC) via grant 61133012 61370164.
Corresponding author: Wanxiang Che, E-mail: car@ir.hit.edu.cn.

References
Bansal, M., Gimpel, K., & Livescu, K. (2014). Tailoring continuous word representations dependency parsing. Proceedings 52nd Annual Meeting Association Computa1016

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

tional Linguistics (Volume 2: Short Papers), pp. 809815, Baltimore, Maryland. Association
Computational Linguistics.
Brown, P. F., Desouza, P. V., Mercer, R. L., Pietra, V. J. D., & Lai, J. C. (1992). Class-based n-gram
models natural language. Computational linguistics, 18(4), 467479.
Buchholz, S., & Marsi, E. (2006). Conll-x shared task multilingual dependency parsing.
Proceedings Tenth Conference Computational Natural Language Learning (CoNLLX), pp. 149164, New York City. Association Computational Linguistics.
Cao, Y., & Khudanpur, S. (2014). Online learning tensor space. Proceedings 52nd
Annual Meeting Association Computational Linguistics (Volume 1: Long Papers),
pp. 666675, Baltimore, Maryland. Association Computational Linguistics.
Carreras, X. (2007). Experiments higher-order projective dependency parser. Proceedings
CoNLL Shared Task Session EMNLP-CoNLL 2007, pp. 957961, Prague, Czech
Republic. Association Computational Linguistics.
Chandar P, S., Lauly, S., Larochelle, H., Khapra, M., Ravindran, B., Raykar, V. C., & Saha, A.
(2014). autoencoder approach learning bilingual word representations. Advances
Neural Information Processing Systems 27, pp. 18531861. Curran Associates, Inc.
Chen, D., & Manning, C. (2014). fast accurate dependency parser using neural networks.
Proceedings 2014 Conference Empirical Methods Natural Language Processing
(EMNLP), pp. 740750, Doha, Qatar. Association Computational Linguistics.
Choi, J. D., & McCallum, A. (2013). Transition-based dependency parsing selectional branching. Proceedings 51st Annual Meeting Association Computational Linguistics (Volume 1: Long Papers), pp. 10521062, Sofia, Bulgaria. Association Computational
Linguistics.
Collobert, R. (2011). Deep learning efficient discriminative parsing. Proceedings 14th
International Conference Artificial Intelligence Statistics (AISTATS), pp. 224232,
Fort Lauderdale, FL, USA. JMLR.org.
Collobert, R., & Weston, J. (2008). unified architecture natural language processing: Deep
neural networks multitask learning. Proceedings 25th International Conference
Machine Learning, ICML 08, pp. 160167, Helsinki, Finland. ACM.
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011). Natural
language processing (almost) scratch. Journal Machine Learning Research, 12, 2493
2537.
Covington, M. A. (2001). fundamental algorithm dependency parsing. Proceedings
39th annual ACM southeast conference, pp. 95102.
De Marneffe, M.-C., MacCartney, B., Manning, C. D., et al. (2006). Generating typed dependency
parses phrase structure parses. Proceedings Fifth International Conference
Language Resources Evaluation (LREC06), pp. 449454, Genoa, Italy. European
Language Resources Association (ELRA).
De Marneffe, M.-C., & Manning, C. D. (2008). stanford typed dependencies representation.
COLING 2008: Proceedings workshop Cross-Framework Cross-Domain Parser
Evaluation, pp. 18, Manchester, UK. Association Computational Linguistics.
1017

fiG UO , C , YAROWSKY, WANG & L IU

Duchi, J., Hazan, E., & Singer, Y. (2011). Adaptive subgradient methods online learning
stochastic optimization. Journal Machine Learning Research, 12, 21212159.
Duong, L., Cohn, T., Bird, S., & Cook, P. (2015a). Low resource dependency parsing: Cross-lingual
parameter sharing neural network parser. Proceedings 53rd Annual Meeting
Association Computational Linguistics 7th International Joint Conference
Natural Language Processing (Volume 2: Short Papers), pp. 845850, Beijing, China.
Association Computational Linguistics.
Duong, L., Cohn, T., Bird, S., & Cook, P. (2015b). neural network model low-resource universal dependency parsing. Proceedings 2015 Conference Empirical Methods
Natural Language Processing, pp. 339348, Lisbon, Portugal. Association Computational
Linguistics.
Durrett, G., Pauls, A., & Klein, D. (2012). Syntactic transfer using bilingual lexicon. Proceedings 2012 Joint Conference Empirical Methods Natural Language Processing
Computational Natural Language Learning, pp. 111, Jeju Island, Korea. Association
Computational Linguistics.
Dyer, C., Ballesteros, M., Ling, W., Matthews, A., & Smith, N. A. (2015). Transition-based dependency parsing stack long short-term memory. Proceedings 53rd Annual Meeting
Association Computational Linguistics 7th International Joint Conference
Natural Language Processing (Volume 1: Long Papers), pp. 334343, Beijing, China. Association Computational Linguistics.
Dyer, C., Lopez, A., Ganitkevitch, J., Weese, J., Ture, F., Blunsom, P., Setiawan, H., Eidelman, V.,
& Resnik, P. (2010). cdec: decoder, alignment, learning framework finite-state
context-free translation models. Proceedings ACL 2010 System Demonstrations, pp.
712, Uppsala, Sweden. Association Computational Linguistics.
Eisner, J. M. (1996). Three new probabilistic models dependency parsing: exploration.
Proceedings 16th conference Computational linguistics-Volume 1, pp. 340345,
Copenhagen, Denmark. Association Computational Linguistics.
Faruqui, M., & Dyer, C. (2014). Improving vector space word representations using multilingual
correlation. Proceedings 14th Conference European Chapter Association Computational Linguistics, pp. 462471, Gothenburg, Sweden. Association
Computational Linguistics.
Firth, J. R. (1957). synopsis linguistic theory 19301955. Studies linguistic analysis, pp.
132. Blackwell.
Garg, N., & Henderson, J. (2011). Temporal restricted boltzmann machines dependency parsing.
Proceedings 49th Annual Meeting Association Computational Linguistics:
Human Language Technologies, pp. 1117, Portland, Oregon, USA. Association Computational Linguistics.
Gouws, S., Bengio, Y., & Corrado, G. (2015). Bilbowa: Fast bilingual distributed representations
without word alignments. Proceedings 32nd International Conference Machine
Learning (ICML), pp. 748756, Lille, France.
Guo, J., Che, W., Wang, H., & Liu, T. (2014). Revisiting embedding features simple semisupervised learning. Proceedings 2014 Conference Empirical Methods Natural
1018

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

Language Processing (EMNLP), pp. 110120, Doha, Qatar. Association Computational
Linguistics.
Guo, J., Che, W., Yarowsky, D., Wang, H., & Liu, T. (2015). Cross-lingual dependency parsing
based distributed representations. Proceedings 53rd Annual Meeting Association Computational Linguistics 7th International Joint Conference Natural
Language Processing (Volume 1: Long Papers), pp. 12341244, Beijing, China. Association
Computational Linguistics.
Haghighi, A., Liang, P., Berg-Kirkpatrick, T., & Klein, D. (2008). Learning bilingual lexicons
monolingual corpora. Proceedings ACL-08: HLT, pp. 771779, Columbus, Ohio.
Association Computational Linguistics.
Hardoon, D. R., Szedmak, S., & Shawe-Taylor, J. (2004). Canonical correlation analysis:
overview application learning methods. Neural computation, 16(12), 26392664.
Henderson, J. (2004). Discriminative training neural network statistical parser. Proceedings 42nd Meeting Association Computational Linguistics (ACL04), Main
Volume, pp. 95102, Barcelona, Spain.
Hermann, K. M., & Blunsom, P. (2014). Multilingual models compositional distributed semantics. Proceedings 52nd Annual Meeting Association Computational
Linguistics (Volume 1: Long Papers), pp. 5868, Baltimore, Maryland. Association Computational Linguistics.
Hwa, R., Resnik, P., Weinberg, A., Cabezas, C., & Kolak, O. (2005). Bootstrapping parsers via
syntactic projection across parallel texts. Natural language engineering, 11(03), 311325.
Jiang, W., Liu, Q., & Lv, Y. (2011). Relaxed cross-lingual projection constituent syntax.
Proceedings 2011 Conference Empirical Methods Natural Language Processing,
pp. 11921201, Edinburgh, Scotland, UK. Association Computational Linguistics.
Kim, Y. (2014). Convolutional neural networks sentence classification. Proceedings
2014 Conference Empirical Methods Natural Language Processing (EMNLP), pp. 17461751, Doha, Qatar. Association Computational Linguistics.
Klein, D., & Manning, C. (2004). Corpus-based induction syntactic structure: Models dependency constituency. Proceedings 42nd Meeting Association Computational Linguistics (ACL04), Main Volume, pp. 478485, Barcelona, Spain.
Klementiev, A., Titov, I., & Bhattarai, B. (2012). Inducing crosslingual distributed representations
words. Proceedings COLING 2012, pp. 14591474, Mumbai, India. COLING
2012 Organizing Committee.
Koehn, P., & Knight, K. (2002). Learning translation lexicon monolingual corpora. Proceedings ACL-02 Workshop Unsupervised Lexical Acquisition, pp. 916, Philadelphia, Pennsylvania, USA. Association Computational Linguistics.
Kolda, T. G., & Bader, B. W. (2009). Tensor decompositions applications. SIAM review, 51(3),
455500.
Koo, T., & Collins, M. (2010). Efficient third-order dependency parsers. Proceedings
48th Annual Meeting Association Computational Linguistics, pp. 111, Uppsala,
Sweden. Association Computational Linguistics.
1019

fiG UO , C , YAROWSKY, WANG & L IU

Kozhevnikov, M., & Titov, I. (2014). Cross-lingual model transfer using feature representation
projection. Proceedings 52nd Annual Meeting Association Computational
Linguistics (Volume 2: Short Papers), pp. 579585, Baltimore, Maryland. Association
Computational Linguistics.
Lei, T., Xin, Y., Zhang, Y., Barzilay, R., & Jaakkola, T. (2014). Low-rank tensors scoring
dependency structures. Proceedings 52nd Annual Meeting Association
Computational Linguistics (Volume 1: Long Papers), pp. 13811391, Baltimore, Maryland.
Association Computational Linguistics.
Li, Z., Zhang, M., & Chen, W. (2014). Soft cross-lingual syntax projection dependency parsing. Proceedings COLING 2014, 25th International Conference Computational
Linguistics: Technical Papers, pp. 783793, Dublin, Ireland. Dublin City University Association Computational Linguistics.
Ma, X., & Xia, F. (2014). Unsupervised dependency parsing transferring distribution via
parallel guidance entropy regularization. Proceedings 52nd Annual Meeting
Association Computational Linguistics (Volume 1: Long Papers), pp. 13371348,
Baltimore, Maryland. Association Computational Linguistics.
Mann, G. S., & Yarowsky, D. (2001). Multipath translation lexicon induction via bridge languages.
Proceedings Second Meeting North American Chapter Association
Computational Linguistics Language Technologies, NAACL 01, pp. 18, Pittsburgh,
Pennsylvania. Association Computational Linguistics.
Marcus, M. P., Marcinkiewicz, M. A., & Santorini, B. (1993). Building large annotated corpus
english: penn treebank. Computational linguistics, 19(2), 313330.
Mayberry, M. R., & Miikkulainen, R. (1999). Sardsrn: neural network shift-reduce parser.
Proceedings Sixteenth International Joint Conference Artificial Intelligence, pp.
820827. Morgan Kaufmann Publishers Inc.
McDonald, R., Crammer, K., & Pereira, F. (2005). Online large-margin training dependency
parsers. Proceedings 43rd Annual Meeting Association Computational
Linguistics (ACL05), pp. 9198, Ann Arbor, Michigan. Association Computational Linguistics.
McDonald, R., & Nivre, J. (2007). Characterizing errors data-driven dependency parsing
models. Proceedings 2007 Joint Conference Empirical Methods Natural
Language Processing Computational Natural Language Learning (EMNLP-CoNLL), pp.
122131, Prague, Czech Republic. Association Computational Linguistics.
McDonald, R., Nivre, J., Quirmbach-Brundage, Y., Goldberg, Y., Das, D., Ganchev, K., Hall, K.,
Petrov, S., Zhang, H., Tackstrom, O., Bedini, C., Bertomeu Castello, N., & Lee, J. (2013).
Universal dependency annotation multilingual parsing. Proceedings 51st Annual
Meeting Association Computational Linguistics (Volume 2: Short Papers), pp. 92
97, Sofia, Bulgaria. Association Computational Linguistics.
McDonald, R., Petrov, S., & Hall, K. (2011). Multi-source transfer delexicalized dependency
parsers. Proceedings 2011 Conference Empirical Methods Natural Language
Processing, pp. 6272, Edinburgh, Scotland, UK. Association Computational Linguistics.
1020

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

McDonald, R. T., & Pereira, F. C. (2006). Online learning approximate dependency parsing
algorithms. Proceedings 11st Conference European Chapter Association Computational Linguistics, pp. 8188, Trento, Italy. Association Computer
Linguistics.
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation word representations
vector space. International Conference Learning Representations (ICLR) Workshop.
Naseem, T., Barzilay, R., & Globerson, A. (2012). Selective sharing multilingual dependency
parsing. Proceedings 50th Annual Meeting Association Computational
Linguistics (Volume 1: Long Papers), pp. 629637, Jeju Island, Korea. Association Computational Linguistics.
Naseem, T., Chen, H., Barzilay, R., & Johnson, M. (2010). Using universal linguistic knowledge
guide grammar induction. Proceedings 2010 Conference Empirical Methods
Natural Language Processing, pp. 12341244, Cambridge, MA. Association Computational Linguistics.
Nilsson, J., & Nivre, J. (2008). Malteval: evaluation visualization tool dependency
parsing.. Proceedings Sixth International Language Resources Evaluation
(LREC08), pp. 161166, Marrakech, Morocco. European Language Resources Association
(ELRA).
Nivre, J. (2003). efficient algorithm projective dependency parsing. Proceedings
8th International Workshop Parsing Technologies (IWPT), pp. 149160, Nancy, France.
Association Computational Linguistics.
Nivre, J. (2004). Incrementality deterministic dependency parsing. Proceedings Workshop Incremental Parsing: Bringing Engineering Cognition Together, pp. 5057,
Barcelona, Spain. Association Computational Linguistics.
Nivre, J. (2008). Algorithms deterministic incremental dependency parsing. Computational
Linguistics, 34(4), 513553.
Nivre, J. (2009). Non-projective dependency parsing expected linear time. Proceedings
Joint Conference 47th Annual Meeting ACL 4th International Joint
Conference Natural Language Processing AFNLP, pp. 351359, Suntec, Singapore.
Association Computational Linguistics.
Nivre, J., Hall, J., & Nilsson, J. (2004). Memory-based dependency parsing. HLT-NAACL 2004
Workshop: Eighth Conference Computational Natural Language Learning (CoNLL-2004),
pp. 4956, Boston, Massachusetts, USA. Association Computational Linguistics.
Petrov, S., Das, D., & McDonald, R. (2012). universal part-of-speech tagset. Proceedings
Eighth International Conference Language Resources Evaluation (LREC-2012),
pp. 20892096, Istanbul, Turkey. European Language Resources Association (ELRA).
Rasooli, M. S., & Collins, M. (2015). Density-driven cross-lingual transfer dependency parsers.
Proceedings 2015 Conference Empirical Methods Natural Language Processing, pp. 328338, Lisbon, Portugal. Association Computational Linguistics.
Smith, D. A., & Eisner, J. (2009). Parser adaptation projection quasi-synchronous grammar
features. Proceedings 2009 Conference Empirical Methods Natural Language
Processing, pp. 822831, Singapore. Association Computational Linguistics.
1021

fiG UO , C , YAROWSKY, WANG & L IU

Socher, R., Bauer, J., Manning, C. D., & Andrew Y., N. (2013). Parsing compositional vector
grammars. Proceedings 51st Annual Meeting Association Computational
Linguistics (Volume 1: Long Papers), pp. 455465, Sofia, Bulgaria. Association Computational Linguistics.
Sgaard, A., Agic, v., Martnez Alonso, H., Plank, B., Bohnet, B., & Johannsen, A. (2015). Inverted
indexing cross-lingual nlp. Proceedings 53rd Annual Meeting Association
Computational Linguistics 7th International Joint Conference Natural Language Processing (Volume 1: Long Papers), pp. 17131722, Beijing, China. Association
Computational Linguistics.
Srivastava, N., & Salakhutdinov, R. R. (2012). Multimodal learning deep boltzmann machines. Advances Neural Information Processing Systems 25, pp. 22222230. Curran
Associates, Inc.
Stenetorp, P. (2013). Transition-based dependency parsing using recursive neural networks. Deep
Learning Workshop NIPS, Lake Tahoe, Nevada, USA.
Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence sequence learning neural networks. Advances Neural Information Processing Systems 27, pp. 31043112. Curran
Associates, Inc.
Tackstrom, O., McDonald, R., & Uszkoreit, J. (2012). Cross-lingual word clusters direct transfer
linguistic structure. Proceedings 2012 Conference North American Chapter
Association Computational Linguistics: Human Language Technologies, pp. 477
487, Montreal, Canada. Association Computational Linguistics.
Tiedemann, J. (2014). Rediscovering annotation projection cross-lingual parser induction.
Proceedings COLING 2014, 25th International Conference Computational Linguistics: Technical Papers, pp. 18541864, Dublin, Ireland. Dublin City University Association Computational Linguistics.
Tiedemann, J. (2015). Cross-lingual dependency parsing universal dependencies predicted
PoS labels., 340349.
Tiedemann, J., Agic, v., & Nivre, J. (2014). Treebank translation cross-lingual parser induction.,
130140.
Titov, I., & Henderson, J. (2007). Fast robust multilingual dependency parsing generative
latent variable model. Proceedings CoNLL Shared Task Session EMNLP-CoNLL
2007, pp. 947951, Prague, Czech Republic. Association Computational Linguistics.
Turian, J., Ratinov, L.-A., & Bengio, Y. (2010). Word representations: simple general method
semi-supervised learning. Proceedings 48th Annual Meeting Association
Computational Linguistics, pp. 384394, Uppsala, Sweden. Association Computational Linguistics.
Uszkoreit, J., & Brants, T. (2008). Distributed word clustering large scale class-based language
modeling machine translation. Proceedings ACL-08: HLT, pp. 755762, Columbus,
Ohio. Association Computational Linguistics.
Wang, M., & Manning, C. D. (2013). Effect non-linear deep architecture sequence labeling.
Proceedings Sixth International Joint Conference Natural Language Processing,
pp. 12851291, Nagoya, Japan. Asian Federation Natural Language Processing.
1022

fiR EPRESENTATION L EARNING C ROSS -L INGUAL RANSFER PARSING

Weiss, D., Alberti, C., Collins, M., & Petrov, S. (2015). Structured training neural network
transition-based parsing. Proceedings 53rd Annual Meeting Association
Computational Linguistics 7th International Joint Conference Natural Language
Processing (Volume 1: Long Papers), pp. 323333, Beijing, China. Association Computational Linguistics.
Xiao, M., & Guo, Y. (2014). Distributed word representation learning cross-lingual dependency
parsing. Proceedings Eighteenth Conference Computational Natural Language
Learning, pp. 119129, Ann Arbor, Michigan. Association Computational Linguistics.
Xue, N., Xia, F., Chiou, F.-D., & Palmer, M. (2005). penn chinese treebank: Phrase structure
annotation large corpus. Natural language engineering, 11(02), 207238.
Yamada, H., & Matsumoto, Y. (2003). Statistical dependency analysis support vector machines.
Proceedings 8th International Workshop Parsing Technologies (IWPT), pp. 195
206, Nancy, France. Association Computational Linguistics.
Yarowsky, D., Ngai, G., & Wicentowski, R. (2001). Inducing multilingual text analysis tools via
robust projection across aligned corpora. Proceedings first international conference
Human language technology research, pp. 18, San Diego, CA, USA. Association
Computational Linguistics.
Zeman, D., Dusek, O., Marecek, D., Popel, M., Ramasamy, L., Stepanek, J., Zabokrtsky, Z., &
Hajic, J. (2014). Hamledt: Harmonized multi-language dependency treebank. Language
Resources Evaluation, 48(4), 601637.
Zhang, Y., & Barzilay, R. (2015). Hierarchical low-rank tensors multilingual transfer parsing.
Proceedings 2015 Conference Empirical Methods Natural Language Processing,
pp. 18571867, Lisbon, Portugal. Association Computational Linguistics.
Zhang, Y., & Clark, S. (2011). Syntactic processing using generalized perceptron beam
search. Computational Linguistics, 37(1), 105151.
Zhang, Y., & Nivre, J. (2011). Transition-based dependency parsing rich non-local features.
Proceedings 49th Annual Meeting Association Computational Linguistics: Human Language Technologies, pp. 188193, Portland, Oregon, USA. Association
Computational Linguistics.
Zhao, H., Song, Y., Kit, C., & Zhou, G. (2009). Cross language dependency parsing using bilingual
lexicon. Proceedings Joint Conference 47th Annual Meeting ACL
4th International Joint Conference Natural Language Processing AFNLP, pp.
5563, Suntec, Singapore. Association Computational Linguistics.
Zhou, H., Zhang, Y., Huang, S., & Chen, J. (2015). neural probabilistic structured-prediction
model transition-based dependency parsing. Proceedings 53rd Annual Meeting
Association Computational Linguistics 7th International Joint Conference
Natural Language Processing (Volume 1: Long Papers), pp. 12131222, Beijing, China.
Association Computational Linguistics.

1023

fiJournal Articial Intelligence Research 55 (2016) 1091-1133

Submitted 09/2015; published 04/2016

Semantic Visualization
Neighborhood Graph Regularization
Tuan M. V. Le
Hady W. Lauw

vmtle.2012@phdis.smu.edu.sg
hadywlauw@smu.edu.sg

School Information Systems
Singapore Management University
80 Stamford Road, Singapore 178902

Abstract
Visualization high-dimensional data, text documents, useful map
similarities among various data points. high-dimensional space, documents
commonly represented bags words, dimensionality equal vocabulary
size. Classical approaches document visualization directly reduce visualizable
two three dimensions. Recent approaches consider intermediate representation
topic space, word space visualization space, preserves semantics
topic modeling. aiming good model parameters
observed data, previous approaches considered local consistency among data
instances. consider problem semantic visualization jointly modeling topics
visualization intrinsic document manifold, modeled using neighborhood graph.
document topic distribution visualization coordinate. Specically,
propose unsupervised probabilistic model, called Semafore, aims preserve
manifold lower-dimensional spaces neighborhood regularization framework
designed semantic visualization task. validate ecacy Semafore,
comprehensive experiments number real-life text datasets news articles
Web pages show proposed methods outperform state-of-the-art baselines
objective evaluation metrics.

1. Introduction
Text documents come various avors, Web pages, news articles, blog posts, emails,
messages social media Twitter. much English, increasing
amounts content various languages well. backdrop growth volume, diversity, complexity various corpora, need useful tools analyze
wealth text content. One form analysis look paper visualization. dierent types visualizations, temporal longitudinal,
networked, natures. interested form visualization
represent collection documents coordinates low-dimensional space,
learn similarities dierences among documents based distances
visualization space.
Visualization high-dimensional data important exploratory data analysis task,
actively studied various academic communities. HCI community
interested presentation information, well interface aspects (Chi, 2000),
machine learning community interested quality dimensionality reduction
c
2016
AI Access Foundation. rights reserved.

fiLe & Lauw

(Van der Maaten & Hinton, 2008), i.e., transform high-dimensional representation lower-dimensional representation shown scatterplot.
visualization form simple, widely applicable across various domains.
Consider therefore problem visualizing documents scatterplot. Commonly,
document represented bag words, i.e., vector word counts. highdimensional representation would reduced coordinates visualizable 2D (or 3D)
space. One pioneering technique Multidimensional Scaling (MDS) (Kruskal, 1964).
goal preserve distances high-dimensional space low-dimensional embedding. applied documents, visualization technique generic high-dimensional
data, e.g., MDS, may necessarily preserve topical semantics. Words often ambiguous, issues polysemy, word carries multiple senses,
synonymy, dierent words carry sense. dimensions original representation (which words) may accurately capture ambiguity, aects
quality reduced representation (which visualization space) well.
model semantics documents way resolve ambiguity,
current popular approach topic modeling, PLSA (Hofmann, 1999) LDA
(Blei, Ng, & Jordan, 2003). document associated probability distribution
set topics. topic probability distribution words vocabulary.
way, polysemous words separated dierent topics, synonymous words
grouped topic.
Topic modeling another form dimensionality reduction: word space
topic space. word space refers documents original representation, usually
bag words. topic space refers simplex topic distributions. documents
probability distribution topics eectively representation document
topic space. However, topic model designed visualization.
one possible visualization plot documents topic distributions simplex, 2D
visualization space could express three topics, limiting.
Given success modeling semantics documents, therefore ask question
whether best forms dimensionality reductions (visualization
topic modeling) documents. end goal arrive visualization documents
consistent semantic representation (topics), well original
representation (words). coupling distinct task topic modeling visualization
respectively, enables novel capabilities. one thing, topic modeling helps create
richer visualization, associate coordinate visualization space
topic word distributions, providing semantics visualization space.
another, tight integration potentially allows visualization serve way
explore tune topic models, allowing users introduce feedback (Hu, Boyd-Graber,
Satino, & Smith, 2014) model visual interface (Choo, Lee, Reddy, & Park,
2013). capabilities support several use case scenarios. One potential use case
document organizer system. visualization could potentially help assigning categories
documents, showing closely related documents labeled. Another
augmented retrieval system. Given query, results may include relevant
documents, also similar documents (neighbors visualization).
1092

fiSemantic Visualization Neighborhood Graph Regularization

1.1 Problem Statement
refer task jointly modeling topics visualization semantic visualization.
input set documents D. specied number topics Z visualization
dimensionality (assumed 2D, without losing generality), goal derive,
every document D, latent coordinate visualization space, probability
distribution Z topics. focus documents description,
approach would apply visualization data types latent factor modeling,
i.e., topic model, makes sense.
straightforward way undergo two-step reductions. rst reduction,
original representation documents reduced topic distributions using topic modeling. second reduction, documents topic distributions reduced
visualization coordinates. approach may value compared direct reduction word space visualization space. However, ideal, disjoint
reductions could mean errors may propagate rst second reduction,
resulting visualization may faithfully capture original representation.
better way solve problem join two reductions single, joint
process produces topic distributions visualization coordinates. approach
rst pioneered PLSV (Iwata, Yamada, & Ueda, 2008), also showed
joint approach outperformed disjoint approach. PLSV derives latent parameters
maximizing likelihood observing documents. goal concerned
error model observation.
literature, found algorithms ensure smoothness tend perform
better learning tasks (Zhou, Bousquet, Lal, Weston, & Scholkopf, 2004). Smoothness
concerns preserving observed proximity documents. objective arises naturally assumption intrinsic geometry data low-rank, non-linear
subspace within high-dimensional space. Therefore, preserving neighborhood structure
important learning tasks. assumption well-accepted machine learning
community (Laerty & Wasserman, 2007), nds application supervised
unsupervised learning (Belkin & Niyogi, 2003; Zhou et al., 2004; Zhu, Ghahramani, Lafferty, et al., 2003). Recently, preponderance evidence assumption also
applies text data particular (Cai, Mei, Han, & Zhai, 2008; Cai, Wang, & He, 2009;
Huh & Fienberg, 2012). therefore propose incorporate assumption new
unsupervised, semantic visualization model.
1.2 Overview
propose unsupervised probabilistic model jointly derives topic distributions
visualization coordinates intrinsic geometry data. proposed model called
Semafore, stands SEmantic visualization MAniFOld REgularization.
build neighborhood regularization framework semantic visualization model.
framework involves new issues resolve, including regularization function,
space regularization take place.
model evaluated series real-life, publicly available datasets,
also benchmark datasets used document classication task. advantage statistical
method, ours, dependent specic language. Two datasets
1093

fiLe & Lauw

English, one Brazilian Portuguese. model unsupervised (class
label neither required used learning), objectively quantify visualization quality, leverage class label information. common assumption documents
class expected neighbors original space (Belkin, Niyogi, &
Sindhwani, 2006; Zhou et al., 2004; Zhu et al., 2003), suggests also
close visualization space. investigate eectiveness Semafore placing
documents class nearby visualization space, systematically compare
existing baselines without one properties, namely: joint modeling
topic visualization, neighborhood regularization.
1.3 Contributions
visualization topic modeling are, separately, well-studied problems, interface
two, semantic visualization, relatively new problem, previous
work. work, make following contributions.
propose incorporating neighborhood structure semantic visualization.
respect, propose probabilistic model Semafore, two integrated components. One kernelized semantic visualization model, enabling substitution
kernel functions relate visualization coordinates topic distributions (see
Section 3.3). neighborhood graph regularization framework semantic
visualization described Section 4.1.
Realizing neighborhood graph regularization involves exploration
incorporate appropriate forms neighborhood structure. respect,
investigate eects neighborhood graph construction techniques knearest neighbors (k-NN), -ball, disjoint minimum spanning trees (DMST),
well dierent edge weight estimations heat-kernel (see Section 4.2)
context semantic visualization.
Section 5, describe requisite learning algorithms based maximum
posteriori (MAP) estimation using expectation-maximization (EM), order
parameters various regularization functions kernels propose.
nal contribution evaluation Semafores eectiveness series reallife, public datasets described Section 6, shows Semafore outperforms
existing baselines well-established objective visualization metric.
prior work (Le & Lauw, 2014b), proposed problem described preliminary model. extended article, signicant technical changes provide
signicantly comprehensive discussion model. instance, discuss
Student-t kernel, addition previously introduced Gaussian kernel. Furthermore, investigate ecacies dierent neighborhood graph constructions, including
-ball DMST graphs, addition previously introduced kNN graph.
graph weights also enhanced investigation heat kernel, addition
simple-minded binary scheme previously. discussed Section 6.3, enhancements
collectively result statistically signicant improvements previous model. Beyond
1094

fiSemantic Visualization Neighborhood Graph Regularization

technical enhancements, also provide comprehensive model analysis empirical validation, including richer quantitative qualitative discussions visualizations
resulting topic models, well metric measure topic interpretability based
pairwise mutual information.

2. Related Work
section, discuss dierent aspects work, identify related papers
literature, point key conceptual dierences.
2.1 Visualization Dimensionality Reduction
One way perform visualization using generic dimensionality reduction technique.
techniques come several avors, depending objective. Principal component
analysis (PCA) (Jollie, 2005) identies components explain variance
data. Related PCA singular value decomposition (SVD) (Golub & Van Loan,
2012). Comparatively, independent component analysis (ICA) (Comon, 1994) identies
components independent one another, whereas linear discriminant analysis
(Fishers LDA) (Fisher, 1936) identies components discriminate
known class labels. generic, techniques frequently applied feature
extraction, optimized visualization. focus properties
components (e.g., orthogonality, independence) rather intrinsic relationship
among data instances. Furthermore, based linear projections, may
capture non-linearities data well.
Another category techniques, directly related visualization,
embedding approach. aims preserve high-dimensional similarities dierences
low-dimensional embedding. One pioneering work multidimensional scaling
(MDS) (Kruskal, 1964). Given set pairwise distances ij data points j,
MDS determines coordinates xi xj respectively, embedded visualization
distance ||xi xj || approximates ij much possible. MDS, distance
preserved ij frequently linear distance, measuring distance along straight line
two points input space. Instead linear distance, Isomap (Tenenbaum,
De Silva, & Langford, 2000) seeks preserve geodesic distance, nding shortest paths
graph edges connecting neighboring data points. LLE (Roweis & Saul, 2000) seeks
preserve linear distances, among neighboring points avoiding need
estimate pairwise distances widely separated data points. Recently also
works applying similar concept embedding using probabilistic modeling,
PE (Iwata, Saito, Ueda, Stromsten, Griths, & Tenenbaum, 2007), SNE (Hinton & Roweis,
2002), t-SNE (Van der Maaten & Hinton, 2008), GTM (Bishop, Svensen, & Williams,
1998). Yet others based semi-denite programming (Shaw & Jebara, 2007, 2009).
Alternatively, several embedding techniques aim preserve relationship among
data instances, rather properties local minima (Kim & Torre, 2010).
Importantly, techniques optimized semantic visualization,
model topics all. coordinates reect semantic meaning,
reecting optimization objective.
1095

fiLe & Lauw

related works far seek address semantic visualization
task directly. closest previous work topic modeling visualization
single generative process Probabilistic Latent Semantic Visualization (PLSV) (Iwata
et al., 2008), also shows joint approach outperforms separate approach.
PLSV builds upon foundation topic modeling technique Probabilistic Latent
Semantic Analysis (PLSA) (Hofmann, 1999) incorporating visualization coordinates,
build upon foundation PLSV incorporating RBF kernels (Section 3.3)
neighborhood structure (Section 4).
also related works share similar objective, share
paradigm visualization topic modeling. instance, LDA-SOM (Millar, Peterson, &
Mendenhall, 2009) rst conducts topic modeling using Latent Dirichlet Allocation (LDA)
(Blei et al., 2003), separately embeds documents topic distributions
Self-Organizing Map (SOM) (Kohonen, 1990). However, joint model,
SOM uses dierent visualization space Euclidean space interested
in. another instance, SSE (Le & Lauw, 2014a) builds Spherical Admixture
Model (SAM) (Reisinger, Waters, Silverthorn, & Mooney, 2010) belonging class
spherical topic models targeted spherical (unit vector) reprepresentations topics
documents, directly comparable equivalent simplex representation
multinomial modeling (probability distribution words) adopted work well
PLSV.
semantic visualization, refer task joining visualization topic modeling. related, dierent, task topic visualization, objective visualize
topics, terms keywords dominant topic (Chaney & Blei, 2012;
Chuang, Manning, & Heer, 2012), topics dominant corpus (Wei, Liu, Song,
Pan, Zhou, Qian, Shi, Tan, & Zhang, 2010), topics related one another
(Gretarsson, Odonovan, Bostandjiev, Hollerer, Asuncion, Newman, & Smyth, 2012).
2.2 Topic Modeling
Topic model involves statistical modeling text (documents words) order discover
abstract concepts topics occur corpus. Beginning latent semantic
indexing (Dumais, Furnas, Landauer, Deerwester, Deerwester, et al., 1995), topic model
evolves modern probabilistic treatments, Probabilistic Latent Semantic
Analysis (PLSA) (Hofmann, 1999) Latent Dirichlet Allocation (LDA) (Blei et al.,
2003). Intuitively, topic captures collection words tend co-occur
describe concept. appeal producing highly interpretable
statistical models let users make semantic sense corpus. text-only
document corpora, topic models also applied cases links observed
addition text (McCallum, Wang, & Corrada-Emmanuel, 2007).
Meanwhile, assumption intrinsic geometry data non-linear low
dimensional subspace within high-dimensional space nds application supervised
unsupervised (Belkin & Niyogi, 2003) learning algorithms. especially prevalent
semi-supervised learning (Zhou et al., 2004; Zhu et al., 2003) way bridge labeled
unlabeled data. Regularization technique realize assumption long
history (Belkin et al., 2006). specic form regularization function varies among
1096

fiSemantic Visualization Neighborhood Graph Regularization

applications. study assumption unsupervised topic models begins
LapPLSI (Cai et al., 2008), introduces regularization PLSA (Hofmann, 1999),
minimizing Euclidean distance neighboring documents topic distributions.
Follow-up work introduce distance functions (Cai et al., 2009; Wu, Bu, Chen, Zhu,
Zhang, Liu, Wang, & Cai, 2012). previous work focus maintaining proximity
similar documents, DTM (Huh & Fienberg, 2012) adds new criterion also maintain
distance among dierent documents. work dierent also need
contend visualization aspects, topic modeling.
2.3 Semantic Similarity
topic models, alternative mechanisms learn semantic relationship
documents. One way measuring semantic similarity among documents
words. instance, vector space model, documents may represented term
vector, similarity may expressed terms cosine similarity (Turney, Pantel,
et al., 2010). word occurrences alone, could also additional signals
semantic similarity. instance, working Wikipedia corpus, categories
links also took account determine similarity among articles (Gabrilovich
& Markovitch, 2009; Ponzetto & Strube, 2007). work diers several
important respects. First, objective similarity value per se, rather
determining lower-dimensional embedding coordinates, would allow visualization
one application. Second, method based probabilistic modeling latent variables,
akin topic modeling, instead operating vector space model representation
documents.

3. Semantic Visualization
introduce problem formulation semantic visualization Section 3.1. focus
paper eects neighborhood graph structure semantic visualization
task. gure clearest way showcase eects design neighborhood
preservation framework existing generative process, PLSV (Iwata
et al., 2008), review Section 3.2. Section 3.3, describe innovation
semantic visualization model, abstraction mapping
topic space visualization space using radial basis function (RBF) kernels.
allows exploration various kernels, identify two exploration.
ease following discussion, include table notations Table 1.
3.1 Problem
task semantic visualization, input corpus documents = {d1 , . . . , dN }.
Every dn bag words, wnm denotes mth word dn . total number
words dn Mn . objective learn, dn , latent distribution Z
topics {P(z|dn )}Z
z=1 . topic z associated parameter z , probability
distribution {P(w|z )}wW words vocabulary W . words highest
probabilities given topic capture semantic topic.
1097

fiLe & Lauw

Notation
dn
xn
Mn
z
z
z
W
N
Z





Description
specic document
latent coordinate dn visualization space
number words document dn
specic topic
coordinate topic z visualization space
word distribution topic z
vocabulary (the set words lexicon)
total number documents corpus
total number topics (user-dened)
collection xn documents
collection z topics
collection z topics
collective set parameters {, , }

Table 1: Notations.
semantic visualization, additional objective semantic visualization,
learn, document dn , latent coordinate xn low-dimensionality
visualization space. Similarly, topic z associated latent coordinate z
visualization space. document dn topic distribution expressed terms
Euclidean distance coordinate xn dierent topic coordinates
= {z }Z
z=1 . Intuitively, closer xn topics z , higher P(z|dn )
probability topic z document dn .
following sections, systematically describe various components
solution. generative process links latent variables (coordinates) words
documents described Section 3.2. specic relationship documents
topics coordinates constitutes specic mapping function, model RBF
kernel Section 3.3. following Section 4, discuss incorporate neighborhood
structure semantic visualization.
3.2 Generative Process
describe generative process documents based topics visualization
coordinates. review PLSV whose graphical model shown Figure 1.
eventual complete model generalization model, involving enhancements
kernelization (Section 3.3) neighborhood structure preservation (Section 4).
generative process follows:
1. topic z = 1, . . . , Z:
(a) Draw zs word distribution: z Dirichlet()
(b) Draw zs coordinate: z Normal(0, 1 I)
2. document dn , n = 1, . . . , N :
(a) Draw dn coordinate: xn Normal(0, 1 I)
1098

fiSemantic Visualization Neighborhood Graph Regularization

N



Mn w

x

Z

z









Figure 1: Graphical model PLSV.
(b) word wnm dn :
i. Draw topic: z Multi({P(z|xn , )}Z
z=1 )
ii. Draw word: wnm Multi(z )
Here, Dirichlet prior, identity matrix, control variance
Z
Z
Normal distributions. parameters = {xn }N
n=1 , = {z }z=1 , = {z }z=1 , collectively
denoted = , , , learned documents based maximum posteriori
estimation. log likelihood function shown Equation 1.
L(|D) =

Mn
N

n=1 m=1

log

Z


P(z|xn , )P(wnm |z )

(1)

z=1

reiterate focus incorporating neighborhood graph structure
semantic visualization. building neighborhood graph regularization framework
existing generative process, i.e., PLSV, clearly observe improvement
PLSV arises neighborhood graph regularization. sense, work
tradition introducing neighborhood graph regularization probabilistic topic
modeling (Huh & Fienberg, 2012; Cai et al., 2008, 2009), contributions relate
neighborhood graph regularization, rather generative process. said,
one signicant dierence PLSV, exibility allowing various kernel
functions, discuss next.
3.3 RBF Kernels
Step 2(b)i generative process, topic z word drawn
distribution {P(z|xn , )}Z
z=1 . distribution relates coordinates topics
visualization space = {z }Z
z=1 coordinate xn document dn
documents topic distribution {P(z|dn )}Z
z=1 .
relationship formulated mapping problem want nd
function G maps point visualization space point topic space. However,
form G cannot known exactly visualization space topic space
latent spaces G may dierent across dierent domains. Therefore, compute
topic distributions, need way approximate G.
build function approximation unknown function G, use abstraction
Radial Basis Function (RBF) neural networks (Bishop, 1995) feedforward multilayered RBF neural networks one hidden layer serve universal approximator
1099

fiLe & Lauw

K nz




Z
Z










/xn

Figure 2: Topic distribution expressed function visualization coordinates using
Radial Basis Function (RBF) network.

arbitrary continuous functions (Park & Sandberg, 1991). property provides
condence model would ability approximate existing relationship
visualization space topic space arbitrary precision. Unlike PLSV (Iwata
et al., 2008) dened specic mapping function, approach generalizes semantic visualization model dening mapping problem terms kernelization,
admits several mapping functions within family RBF kernels.
context, Radial Basis Function (Buhmann, 2000) relate coordinate variables
based distances denes kernel function (||xn z ||) terms far data
point (e.g., xn ) center (e.g., z ). kernel function may take various forms,
e.g., Gaussian, multi-quadric, inverse quadratic, polyharmonic spline. express P(z|dn )
function xn , consider normalized architecture RBF network, three
layers shown Figure 2. input layer consists one input node (xn ). hidden
layer consists Z number normalized RBF activation functions. centered
z computes Z (||xn z ||) . linear output layer consists Z output nodes.
z =1

(||xn z ||)

output node yz (xn ) corresponds P(z|dn ), linear combination RBF
functions, shown Equation 2. Here, wz,z weight inuence RBF function

z P(z|dn ), constraint Z
z =1 wz,z = 1.
Z
P(z|dn ) = yz (xn ) =

z =1 wz,z (||xn z ||)
Z
z =1 (||xn z ||)

(2)

Equation 2 general form, instantiate specic mapping function,
need determine assignment wz,z form function . wz,z ,
experiment special case wz,z = 1 z = z 0 otherwise.
kernel function , one variation consider Gaussian, yields function Equation 3, refers collective set z s. Note set variance
Gaussian 1. However, true value really important dierent variance
value produces re-scaled visualization scaling factor equal variance.
1100

fiSemantic Visualization Neighborhood Graph Regularization

exp( 12 ||xn z ||2 )
P(z|dn )Gaussian = P(z|xn , )Gaussian = Z
1
2
z =1 exp( 2 ||xn z || )

(3)

Another variation considered Student-t. distribution also used
t-SNE (Van der Maaten & Hinton, 2008) context non-semantic, direct embedding mitigate eects crowding. Due mismatched dimensionalities, points
crunched together center visualization, prevents gaps forming
clusters. Therefore, hypothesize using Student-t radial basis function, yields function Equation 4, help improve performance
model crowding becomes issue. Note Student-t distribution one degree
freedom yields radial basis function form similar inverse quadratic.
(1 + ||xn z ||2 )1
P(z|dn )Studentt = P(z|xn , )Studentt = Z
2 1
z =1 (1 + ||xn z || )

(4)

Gaussian function (Equation 3) also used previously baseline PLSV
(Iwata et al., 2008) compare to. inclusion helps establish parity
comparative purposes, investigate eectiveness alternative Student-t
kernel (described above), well neighborhood regularization (described
next section).

4. Neighborhood Graph Regularization Framework
recent works (Cai et al., 2008, 2009; Huh & Fienberg, 2012) trying preserve
local neighborhood structure learning low-dimensional topic representations documents. works assume documents sampled nonlinear low-dimensional
subspace embedded high-dimensional space. Therefore, local neighborhood
structure important revealing hidden topics documents preserved
learning topic representations documents (Bai, Guo, Lan, & Cheng, 2014).
generative process semantic visualization described Section 3, document parameters sampled independently, may necessarily reect underlying local neighborhood structure. therefore seek realize assumption semantic visualization.
particular, assume two documents di dj close original space,
parameters j low-rank representation similar well. Coupled
kernelized semantic visualization model described Section 3, neighborhood
preservation approach described section constitutes proposed model, Semafore,
stands SEmantic visualization MAniFOld REgularization.
4.1 Neighborhood Regularization
neighborhood structure represented neighborhood graph. Given set
data points Euclidean space, neighborhood graph constructed input
data points vertices. denition, edges symmetric, i.e., ij = ji , weighted.
collection edge weights collectively denoted = {ij }.
moment, assume neighborhood graph, address
issue neighborhood graph may incorporated semantic visualiza1101

fiLe & Lauw

tion framework. actuality, neighborhood graph construction important
component, whose construction described detail Section 4.2.
One eective means incorporate neighborhood structure learning model
regularization framework (Belkin et al., 2006). leads re-design
log-likelihood function Equation 1 new regularized function L (Equation 5),
consists parameters (visualization coordinates topic distributions),
documents neighborhood structure.
L(|D, ) = L(|D) + R(|)

(5)

rst component L log-likelihood function Equation 1, reects
latent parameters observation D. second component R
regularization function, reects consistency latent parameters
neighboring documents neighborhood structure . regularization parameter,
commonly found neighborhood based algorithms (Belkin et al., 2006; Cai et al., 2008,
2009), controls extent regularization (we experiment dierent
experiments).
4.1.1 Proposed Regularization Function
turn denition R function. intuition data points
close high-dimensional space, also close low-rank representations, i.e., local consistency, also known smoothness. One function satises
R+ Equation 6. Here, F distance function operates low-rank space.
Minimizing R+ leads minimizing distance F(i , j ) neighbors (ij = 1).
R+ (|) =

N


ij F(i , j )

(6)

i,j=1;i=j

level local consistency still insucient, regulate
non-neighbors (i.e., ij = 0) behave. instance, prevent non-neighbors
similar low-rank representations. Another valid objective visualization keep
non-neighbors apart, satised another objective function R Equation 7. R
minimized two non-neighbors di dj (i.e., ij = 0) distant low-rank
representations. addition 1 F prevent division-by-zero error.
R (|) =

N

i,j=1;i=j;ij =0

1 ij
F(i , j ) + 1

(7)

hypothesize neither objective eective own. complete objective
would capture spirits keeping neighbors close, keeping non-neighbors apart.
Therefore, put Equation 6 Equation 7 together using summation maximize
objective function shown Equation 8. Note coecient 12 Equation 8
simplifying formula derivative R (|).
1
R (|) = (R+ (|) + R (|))
2
1102

(8)

fiSemantic Visualization Neighborhood Graph Regularization

2

d1

1

d2

I1

I2

0
-2

-1

0
-1

1

2

3

4

d3

-2

Figure 3: Example topic distribution may dierent visualization coordinates. points red line topic distributions.

Summation preserves absolute magnitude distance, helps improve
visualization task keeping non-neighbors separated visualizable Euclidean space.
Taking product unsuitable, constrains ratio distances neighbors distances non-neighbors. may result crowding eect,
many documents clustered together, relative ratio may maintained,
absolute distances visualization space could small.
proposed regularization function above, also possible consider
regularization functions. instance, also experimented modifying
regularization function adapted Discriminative Topic Model (DTM) (Huh & Fienberg,
2012), addressed topic modeling semantic visualization. Note
original DTM formulation, distance function F(i , j ) operates topic space,
adapt semantic visualization redening distance function F(i , j )
operate visualization space instead. modied DTM formulation shown
underperform proposed regularization function (Le & Lauw, 2014b).
4.1.2 Enforcing Neighborhood Structure: Visualization vs. Topic Space
turn denition F(1 , 2 ). neighborhood-based models (Belkin et al.,
2006; Cai et al., 2008, 2009), one low-rank representative space. semantic
visualization, two: topic visualization spaces. look
enforce neighborhood graph structure.
rst glance, seem equivalent. all, representations
documents. However, necessarily case. Consider simple example two
topics z1 z2 visualization coordinates 1 = (0, 0) 2 = (2, 0) respectively.
Meanwhile, three documents {d1 , d2 , d3 } coordinates x1 = (1, 1), x2 = (1, 1),
x3 = (1, 1). two documents coordinates, also
topic distributions. example, x1 x2 equidistant 1 2 ,
therefore according Equation 3, topic distribution P(z1 |d1 ) =
P(z1 |d2 ) = 0.5, P(z2 |d1 ) = P(z2 |d2 ) = 0.5. two documents topic
distributions, may necessarily coordinates. d3 also
1103

fiLe & Lauw

topic distribution d1 d2 , dierent coordinate. fact, coordinate
form (1, ?) topic distribution. example illustrated Figure 3.
suggests enforcing neighborhood structure topic space may necessarily lead data points closer visualization space. postulate
regularizing visualization space eective. also advantages computational eciency so, describe shortly. Therefore,
dene F(i , j ) squared Euclidean distance ||xi xj ||2 corresponding
visualization coordinates.
4.2 Neighborhood Graph
discuss neighborhood graph may approximated, concerns two
issues graph edges dened, well weighted. neighborhood graph constructed original data space represent document
tf-idf vector (Manning, Raghavan, Schutze, et al., 2008). also experiment dierent
vector representations, including word counts term frequencies, nd tf-idf give
best results. distance two document vectors measured using Euclidean
distance.
4.2.1 Graph Construction
research studies properties methods construction neighborhood graphs (Zemel & Carreira-Perpinan, 2004; Carey & Mahadevan, 2014). Since
construction neighborhood graph critical step may aect performance
various graph-based algorithms, problem research issue independent interest. scope exploring well-established graph construction techniques
may apply case semantic visualization. investigate various graph
construction methods empirically Section 6.
following, briey review two categories graph construction methods.
1. Neighborhood-based Graphs. formulation, edges formed data points
deemed suciently close other. admits dierent denitions
sucient closeness. common denitions found literature include
two below.
(a) -ball: neighborhood graph contains edge connecting two documents di
dj , di dj distance less threshold .
(b) k-nearest neighbors (k-NN) graph: neighborhood graph contains edge
connecting two documents di dj , di set Nk (dj ) knearest
neighbors dj , dj set Nk (di ).
-ball k-NN strongly data-dependent parameters (i.e., k)
straightforward choose best value parameters. Neither guarantees
graph would connected. also need carefully selected tuned,
extent also aect balance contribution neighbors
R+ non-neighbors R neighborhood regularization R Equation 8.
1104

fiSemantic Visualization Neighborhood Graph Regularization

Appendix A, explore empirically graph parameters help maintain
balance within neighborhood regularization function.
-ball suers another issue tends produce many edges points
located high-density regions, thus little restriction maximum degree
vertex. k-NN suer problem one commonly
used types graphs.
subsequent development experiments, experiment -ball
k-NN graph may variance performance dierent graph
construction techniques dierent datasets (Hein, Audibert, & Luxburg, 2007; Ting,
Huang, & Jordan, 2010; Coifman & Lafon, 2006).
2. Minimum Spanning Tree-based Graphs. -ball k-NN quite sensitive
noise sparsity, graph construction based combining multiple minimum
spanning trees help reduce sensitivity noise output graph (Zemel
& Carreira-Perpinan, 2004). two variations based approach.
(a) Perturbed Minimum Spanning Trees (PMST): PMST builds neighborhood
graph generating > 1 perturbed copies whole dataset according
local noise model tting MST perturbed copy. weight
eij [0, 1] assigned edge points xi xj equal
average number times edge appears trees.
(b) Disjoint Minimum Spanning Trees (DMST): DMST produces neighborhood
graph nding deterministic collection r minimum spanning trees
satises property tree collection uses edge trees.
neighborhood graph union edges trees contains r(N 1)
edges.
representative category, use DMST, deterministic easier
construct PMST showing similar ecacies.
4.2.2 Graph Weighting
next issue assign weights edges neighborhood graph.
respect, consider two variations edge weights.
1. Simple Minded :

ij =

1,
0,

di dj connected,
otherwise.

(9)

simplest approach use binary weighting assign weights
edges. However, approach assign uniform weights edges
sensitive errors, cli eect 1 immediately 0. Moreover,
since weights smoothed, could result loss information.
hypothesize among connected nodes, may still dierences
terms degrees similarity, expressed mutual distances.
motivates second approach below.
1105

fiLe & Lauw

2. Heat Kernel :

ij =

exp(
0,

||di dj ||2
),


di dj connected,
otherwise.

(10)

alternative approach using Heat Kernel function (Belkin & Niyogi, 2001;
Jebara, Wang, & Chang, 2009). Heat Kernel advantage Simple Minded
allowing smoother weights edges, helps address issues sensitivity
loss information. However, Simple Minded parameterized, Heat
Kernel one parameter needs determined (i.e., ). Note = ,
Heat Kernel degenerates Simple Minded, i.e., former general
formulation. exact value important model would
eectively absorbed regularization parameter. simplicity, set = 2.

5. Model Fitting
discuss parameters model described Sections 3 4
learned. One well-accepted framework learn model parameters using maximum posteriori (MAP) estimation Expectation-Maximization EM algorithm (Dempster,
Laird, & Rubin, 1977).
model, regularized conditional expectation complete-data log likelihood MAP estimation priors is:
Q(|) =
+

Mn
N
Z




P(z|n, m, ) log P(z|xn , )P(wnm |z )

n=1 m=1 z=1
N


Z


n=1

z=1

log(P(xn )) +

log(P(z )) +

Z


log(P(z ))

z=1

+ R(|),
current estimate. P(z|n, m, ) class posterior probability nth
document mth word current estimate. P(z ) symmetric Dirichlet prior
parameter word probability z . P(xn ) P(z ) Gaussian priors zero
mean spherical covariance document coordinates xn topic coordinates z .
set hyper-parameters = 0.01, = 0.1N = 0.1Z following PLSV (Iwata
et al., 2008).
E-step, P(z|n, m, ) updated follows:
P(z|n, m, ) = Z

P(z|xn , )P(wnm |z )

z =1 P(z

|x

n , )P(wnm |z )

.

M-step, maximizing Q(|) w.r.t zw , next estimate word probability
zw follows:
N Mn
m=1 I(wnm = w)P(z|n, m, ) +
n=1
zw = W
,
N Mn

m=1 I(wnm = w )P(z|n, m, ) + W
w =1
n=1
1106

fiSemantic Visualization Neighborhood Graph Regularization

I(.) indicator function. z xn cannot solved closed form,
estimated maximizing Q(|) using quasi-Newton (Liu & Nocedal, 1989).
computation fo gradients Q(|) w.r.t z xn depend specic
kernel used (see Section 3.3).
Gaussian kernel, following gradients:
n


Q(|)
=
P(z|xn , ) P(z|n, m, ) (z xn ) z ,
z

N

Q(|)
=
xn



n=1 m=1
Mn
Z




m=1 z=1


R(|)
.
P(z|xn , ) P(z|n, m, ) (xn z ) xn +
xn

Student-t kernel, following gradients:

N Mn
2 P(z|xn , ) P(z|n, m, ) (z xn )
Q(|)
=
z ,
z
1 + ||xn z ||2
n=1 m=1


Mn
Z

2 P(z|xn , ) P(z|n, m, ) (xn z )
Q(|)
R(|)
=
xn +
.
2
xn
1 + ||xn z ||
xn
m=1 z=1

gradient R(|) w.r.t. xn computed depending form regularization function R(|). use proposed regularization function R (|)
described Section 4.1.1, following gradient:
R (|)
R(|)
=
xn
xn



(xn xj )
1
=
4nj (xn xj )
4(1 nj )
.
2
2
(F(n , j ) + 1)
j=1;j=n

j=1;j=n

mentioned earlier, eciency advantage regularizing visualization space. R(|) contain variable z regularization visualization
O(N 2 ). contrast, regularizaspace. complexity computing R(|)
xn
tion topic space, take gradient R(|) w.r.t z . contributes
. Therefore, regularizatowards greater complexity O(Z 2 N 2 ) compute R(|)
z
tion topic space would run much slower visualization space.

6. Experiments
main objective experiments evaluate eectiveness neighborhood regularization semantic visualization model. describing experimental setup,
rst examine dierent design choices model relating kernel, graph construction, regularization function. Thereafter, compare Semafore baseline
methods also aim address visualization topic modeling, quantitatively
qualitatively, rst terms visualization terms topic modeling.
1107

fiLe & Lauw

6.1 Experimental Setup
section, give description benchmark datasets well suitable metrics
used evaluation.
6.1.1 Datasets
use three real-life, publicly available datasets (Cardoso-Cachopo, 2007) evaluation.
20N ews contains newsgroup articles (in English) 20 classes.
Reuters8 contains newswire articles (in English) 8 classes.
Cade12 contains web pages (in Brazilian Portuguese) classied 12 classes.
benchmark datasets used document classication. task fully
unsupervised, ground-truth class labels useful objective evaluation.
create balanced classes sampling fty documents class, following practice
PLSV (Iwata et al., 2008). results in, one sample, 1000 documents 20N ews,
400 Reuters8, 600 Cade12. vocabulary sizes 5.4K 20N ews, 1.9K
Reuters8, 7.6K Cade12. algorithms probabilistic, generate samples
dataset. sample, conduct independent runs. Therefore, result
reported setting average total 25 runs.
6.1.2 Metrics
suitable metric, return fundamental principle good visualization
preserve relationship documents (in high-dimensional space)
lower-dimensional visualization space. User studies, even well-designed, could
overly subjective may repeatable across dierent users reliably. Therefore,
objective evaluation, rely two types quantitative analysis:
Classication: evaluation relies ground-truth class labels found
datasets. well-established practice many clustering visualization
works machine learning. basis evaluation reasonable assumption
documents class related documents dierent classes.
Therefore good visualization would place documents class neighbors
visualization.
document dn , hide true class cn , generate prediction
class Ct (n) taking majority class among t-nearest neighbors, determined Euclidean distance visualization space. Classication accuracy
Classif ication Acc(t) dened fraction documents whose predicted class
Ct (n) matches true class cn . specically, have:
N
1
(Ct (n) = cn ),
Classif ication Acc(t) =
N
n=1

1108

fiSemantic Visualization Neighborhood Graph Regularization

delta function equals 1 prediction matches 0 otherwise.
metric used PLSV (Iwata et al., 2008). accuracy computed
based documents coordinates, trends produced computed based
topic distributions (due coupling kernels described Section 3.3).
Neighborhood Preservation: evaluation rely ground-truth class
labels local neighborhood structure input data. assumption
good visualization would able preserve local structure input
data much possible. two documents neighbors input data,
still neighbors visualization space.
every document dn , compute sets t-nearest neighbors Yt (n) Xt (n)
document dn input data visualization respectively. neighborhood
preservation accuracy P reservation Acc(t) dened average fraction
overlap size Yt (n) Xt (n) size Yt (n) (i.e. t), n = 1, . . . , N .
specically, have:

P reservation Acc(t) =

N
1 |Yt (n) Xt (n)|
,
N

n=1

|Yt (n) Xt (n)| size overlap set Yt (n) Xt (n).
similar measure found literature (Akkucuk & Carroll, 2006),
called rate agreement local structure agreement rate used
measure well local structure preserved input data
low dimensional embedding. also used tuning parameters non-linear
dimensionality reduction method (Chen & Buja, 2009).
subsequent experiments, let vary range [5, 50] step size
5 report accuracies. Since dierent methods may behave dierently dierent
ts, choosing specic comparison may unfair methods. Moreover,
method consistently well dierent ts would also smoother local
structure. Therefore, comparing various methods, present preservation
classication accuracies averaged across [5, 50], denoted P reservation Acc(Avg)
Classif ication Acc(Avg) respectively.
6.2 Parameter Study
section, study eects graph parameters model. Specically,
parameters concern graph construction, including number neighbors k k-NN
graph, distance threshold -ball graph, number minimum spanning trees
r DMST. type graph, use Simple Minded weight. following
gures, regularization function R = 10 number topics Z = 20.
use neighborhood preservation accuracy P reservation Acc(t) show eects
graph parameters metric need ground-truth class labels,
always available tuning graph parameters.
1109

fiLe & Lauw




















































W

W



W



E















Z















Figure 4: Preservation accuracy Semafore using k-NN graph dierent neighborhood size k (a) 20N ews, (b) Reuters8, (c) Cade12.











































W

W

W













E















Z















Figure 5: Preservation accuracy Semafore using DMST graph dierent number minimum spanning trees r (a) 20N ews, (b) Reuters8, (c) Cade12.





































E











W

W



W



















Z





















Figure 6: Preservation accuracy Semafore using -ball graph dierent values
distance threshold (a) 20N ews, (b) Reuters8, (c) Cade12.

1110

fiSemantic Visualization Neighborhood Graph Regularization

Figure 4, show performance model dierent neighborhood size k
k-NN graph dierent datasets. every k, vary plot P reservation Acc(t).
Figure 4 shows optimum k 20N ews, Reuters8, Cade12 10, 10, 5
respectively. compute average accuracy P reservation Acc(Avg) conrms
optima indeed k values. on, use k=10 20N ews
Reuters8, k=5 Cade12 k-NN graph used.
DMST graph, plot P reservation Acc(t) dierent number minimum
spanning trees r dierent datasets Figure 5. dicult see r best
gure dierences much. P reservation Acc(Avg)
computed shows three datasets, optimum r=5,6,7.
Subsequently, use r=6 DMST graphs three datasets.
-ball graph, Figure 6 plot P reservation Acc(t) dierent values
range [1.32, 1.40]. choose range =1.32 =1.40 roughly give
average number neighbors 5 100 respectively. P reservation Acc(Avg) shows
optimum 20N ews, Reuters8, Cade12 1.34, 1.35, 1.33 respectively.
6.3 Model Analysis
section, study various design choices involved designing Semafore
model, nally concluding eventual synthesis design choices used
comparison baselines. keep discussion focused organized,
following sub-section, vary single design choice, order isolate eects.
unvaried, model following setup default: number topics Z = 20,
graph construction method k-NN, graph weighting method simple minded,
RBF kernel Gaussian, regularization function R = 10.
6.3.1 Neighborhood Graph Construction
investigate three graph construction methods: k-NN, -ball DMST, representatives neighborhood-based minimum spanning tree-based methods respectively.
graph, parameter tuned shown Section 6.2. regularization
parameter , try dierent settings dataset. happens = 10
performs best graph construction methods across three datasets.
Figure 7, run Semafore dierent types graph three datasets
report P reservation Acc(Avg) dierent number topics Z. results show
dierent types graph behave dierently dierent datasets. 20N ews, -ball
DMST give model highest performance. Since dierence two
statistically signicant, choose use DMST subsequent experiments 20N ews.
Reuters8, since -ball outperforms others (signicant 0.05 level), going
default choice subsequent experiments. Cade12, choice DMST,
slightly better k-NN (statistically signicant Z = 10, 40, 50).
6.3.2 Neighborhood Graph Weighting
compare two variations graph weighting methods, namely: Simple Minded
Heat Kernel methods. experiment, use k-NN graph specic ks dierent
1111

fiLe & Lauw







D^d





W



W

W

EE











Ed
E











Ed



Z





Ed


Figure 7: eects dierent graph construction methods models performance.







,<t



W



W

W

^Dt











Ed
E












Ed
Z





Ed


Figure 8: eects dierent graph weighting schemes models performance.
graph used experiment k-NN graph specic ks dierent
datasets studied Section 6.2.











Ed
E

^<



W



W

W

'<











Ed
Z












Ed


Figure 9: eects Gaussian Student-t RBF kernels models performance.
1112

fiSemantic Visualization Neighborhood Graph Regularization

Regularization function
Graph construction
Graph weighting
RBF kernel

20N ews
R
DMST
Heat Kernel
Student-t

Reuters8
R
-ball
Heat Kernel
Student-t

Cade12
R
DMST
Simple Minded
Student-t

Table 2: Synthesized Model Dataset.
datasets studied Section 6.2. regularization parameter set 10 trying
various settings picking best one.
Figure 8, compare Simple Minded method Heat Kernel method see
inuences model dierent number topics Z. observe Heat Kernel
signicantly consistently better Simple Minded method across cases
20N ews Reuters8. dierence statistically signicant 0.01 level. One
explanation Heat Kernel assigns smoother weights graph edges, thus
robust Simple Minded. Cade12, Simple Minded slightly better, though
dierences statistically signicant 0.05 level Z = 40. Subsequently,
use Heat Kernel 20N ews Reuters8, Simple Minded Cade12 part
nal synthesis.
6.3.3 RBF Kernel
described Section 3.3, express topic distributions function visualization
coordinates using RBF network abstraction. section, show dierent
RBF kernels aect models performance. two kernels exploring Gaussian
(Equation 3) Student-t (Equation 4). tune regularization term kernel
see best one two kernels = 10.
Figure 9 shows results dierent number topics Z. Student-t kernel slight
edge Gaussian kernel consistently across dierent number topics. dierence
small, statistically signicant (at 0.05 level) majority cases (for 20N ews
Z = 10, 20, 30, 50, Reuters8 Z = 30, Cade12 Z = 10, 30, 50). slight
improvement could sign crowding problem exist model. Student-t
kernel would even useful extreme crowding issues,
number documents visualized even larger. Subsequently, due
slight edge, use Student-t part nal synthesis. see shortly, using
Student-t within synthesized model results signicant improvement overall.
6.3.4 Synthesised Semafore Model
Based model analysis preceding paragraphs, combine design choices
nal synthesis model called Semafore. synthesized model slightly dierent
dierent datasets, listed Table 2.
conduct another set experiments verify synthesized models
would produce noticeable improvement earlier version (kNN + Simple Minded
+ Gaussian Kernel) appeared earlier work (Le & Lauw, 2014b), underlining
1113

fiLe & Lauw

EE^D'<
,<^<













Ed










E





Ed
Z

EE^D'<
D^d^D^<

W


W


W


EE^D'<
D^d,<^<













Ed


Figure 10: synthesized models dierent properties compared earlier version
(kNN + Simple Minded + Gaussian Kernel) appeared earlier work
(Le & Lauw, 2014b).

utility subsequent enhancements. Figure 10 shows indeed case.
Based standard deviations shown gures, improvements clear
20N ews Reuters8 clear Cade12. Paired samples t-test indicate
improvement signicant 0.05 level lower cases, except cases
Z = 10, 20 Cade12. use synthesized models comparisons
baseline methods following section.
6.4 Comparison Visualizations
compare proposed model several baselines. First, outline set
comparative methods. Thereafter, discuss quantitative evaluation (in terms accuracy),
well qualitative evaluation (in terms example visualizations). Finally, show
gains visualization quality come expense topic modeling.
semantic visualization seeks ensure consistency topic model visualization, comparison focuses methods producing topics visualization coordinates
listed Table 3.
Semafore proposed method incorporates neighborhood structure
semantic visualization.
PLSV (Iwata et al., 2008) state-of-the-art, representing joint approach
without neighborhood structure preservation.
PE (LDA) represents pipeline approach involving topic modeling LDA (Blei
et al., 2003), followed visualizing documents topic distributions PE (Iwata
et al., 2007). pipeline better LDA/MDS appeared earlier
work (Le & Lauw, 2014b). pipeline methods, shown inferior PLSV
(Iwata et al., 2008), reproduced avoid duplication.
1114

fiSemantic Visualization Neighborhood Graph Regularization

Visualization

Topic model

Joint model

Neighborhood

Semafore
PLSV
PE (LDA)
t-SNE (LDA)
Table 3: Comparative Methods.
t-SNE (LDA) another pipeline approach rst uses LDA (Blei et al., 2003)
learn topic model use t-SNE (Van der Maaten & Hinton, 2008) visualize
documents topic distributions.
completeness, also conduct experiments comparing method t-SNE
Laplacian EigenMaps (LE) (Belkin & Niyogi, 2003) (direct visualization, without topic
modeling). keep discussion focused, show Appendix B,
consider t-SNE LE comparative baselines two methods model
visualization, topics.
6.4.1 Accuracy
section, compare model several baselines terms classication
accuracy (Figure 11) neighborhood preservation accuracy (Figure 12). two
gures, standard deviations Semafore shown.
Classcation Accuracy. Figure 11(a), 11(c) 11(e) show Classf ication Acc(t)
dierent ts Z = 20 20N ews, Reuters8, Cade12 respectively. t,
comparison shows outperformance Semafore baselines consistently. four
methods show behavior performances decrease increases.
increases, may lose accuracy predicting labels documents near border
cluster.
Now, vary number topics Z. Figure 11(b), show performance
Classf ication Acc(Avg) 20N ews. Figure 11(d) 11(f) show Reuters8
Cade12 respectively. gures, draw following observations
comparative methods:
Semafore performs best datasets across various numbers topics (Z).
Semafore beats PLSV 25% 51% 20N ews, 613% Reuters8,
2232% Cade12. margins performance respect PLSV statistically signicant 0.01 signicant level lower cases. eectively showcases
utility neighborhood regularization enhancing quality visualization.
preserving local consistency, Semafore achieves good accuracy even small
number topics (e.g., 10).
PLSV performs better PE (LDA) t-SNE (LDA), shows
utility joint, instead separate, modeling topics visualization.
PE (LDA) t-SNE (LDA) worse PLSV embeds documents
using two-step reductions optimize separately two dierent objective functions.
1115

fiLe & Lauw

Therefore, errors previous step may propagate next, without
opportunity correction. may cause distortions visualization.
cases, PLSV, PE (LDA) t-SNE (LDA) tend decreasing accuracies number topics increases. may number topic
increases, topic distributions word probabilities may overt data
thus accuracy reduced. contrast, Semafore shows quite stable performance across dierent numbers topics. may explained utility
neighborhood regularization, helps prevent overtting number
topics increases.
Neighborhood Preservation Accuracy. better classication accuracy,
Semafore also preserves well local structure input data visualization space.
P reservation Acc(t) results Figure 12(a), 12(c) 12(e) show Semafore
consistently better baselines terms neighborhood preservation across
dierent ts dierent datasets. Figure 12(b), 12(d) 12(f), vary number
topics Z report P reservation Acc(Avg) results. Semafore beats PLSV
41% 76% 20N ews, 2436% Reuters8, 2945% Cade12 terms
neighborhood preservation accuracy. improvements Semafore PLSV
statistically signicant 0.01 signicant level lower cases.
accuracy results based visualization coordinates. also computed accuracies based topic distributions, similar trends.
6.4.2 Visualizations
provide intuitive appreciation, briey describe qualitative comparison visualizations. method dataset, visualization shown scatterplot (best
seen color). document coordinate, assigned shape color based
class. topic also coordinate, drawn black, hollow circle. legend
provided, mapping symbol corresponding class label.
Note illustrative, rather comparative discussion, objective
evaluation rely eyeballing alone. However, shown quantitative
results preceding section, section, focus qualitative study
output visualizations.
20News. Figure 13 shows visualization 20N ews dataset. Semafores Figure 13(a)
shows dierent classes well separated. distinct clusters blue squares
purple diamonds top hockey baseball classes respectively, clusters
orange triangles pink asterisks bottom cryptography medicine, etc.
Beyond individual classes, visualization also places related classes nearby. Computerrelated classes found lower left. Politics religion lower right.
Comparatively, Figure 13(b) PLSV shows crowding center. instance,
motorcycle (green dashes) autos (red dashes) mixed center without good
separation. Figure 13(c) PE (LDA) worse. PE (LDA) give good separation
similar classes. mixes autos (red dashes) space (green circles) together
center. Medicine (pink asterisks) also mixed classes PE (LDA)
Semafore PLSV give good separation it. Figure 13(d) visualization tSNE (LDA). Although t-SNE (LDA) separate well hockey (blue squares) baseball
1116

fiSemantic Visualization Neighborhood Graph Regularization

W>^s


















W>




^









Es





W>











Zs

Zs







^





W>^s




















^E>




Ed





W>^s






Ed
Es



W>




^

^E>




^E>











Ed


Figure 11: Classication Accuracy Comparison.

1117





fiLe & Lauw

W>^s


















W>
W

W

^









Es

W

W>








Zs

Zs







^





W>^s



















^E>




Ed



W

W>^s











Ed
Es



W>
W

W

^

^E>




^E>










Ed


Figure 12: Preservation Accuracy Comparison.

1118





fiSemantic Visualization Neighborhood Graph Regularization

(purple diamonds) classes, able detect semantic similarities (as baseball
hockey sports). addition, still mixes documents dierent classes
together center upper right.
Reuters8. Figure 14 shows visualization outputs Reuters8 dataset. Semafore
Figure 14(a) better separating eight classes distinct clusters. anticlockwise direction top, navy blue diamonds (money-fx ), red dashes
(interest), red squares (crude), light blue pluses (earn), green triangles (acq), purple crosses
(ship), blue asterisks (grain), nally orange circles (trade).
comparison, PLSV Figure 14(b) shows several classes intermixed
center, including red dashes (interest), orange circles (trade), navy blue diamonds
(money-fx ). PE (LDA) Figure 14(c) also worse mixes dierentiated classes
red dashes (interest) navy blue diamonds (money-fx ) together. t-SNE (LDA)
Figure 14(d) seems better cluster separation still mix documents dierent
classes together red squares (crude) green triangles (acq) upper right.
Green triangles (acq) also mix light blue pluses (earn) left visualization
t-SNE (LDA).
Cade12. Figure 15 shows visualization outputs Cade12.
challenging dataset. Even so, Semafore Figure 15(a) still achieves better separation
classes, compared PLSV Figure 15(b). Particularly, Semafore gives
better separation esportes (green triangles) well compras-on-line (orange circles)
PLSV PE (LDA). t-SNE (LDA) shows quite good clusters esportes (green
triangles) well compras-on-line (orange circles) also merges many dierent
classes together clusters right upper right.
6.5 Comparison Topic Models
One question whether Semafores gain visualization quality closest baseline
PLSV expense quality topic model. investigate this,
compare topic models Semafore PLSV, share core generative process.
parity, comparison, include joint models, whereby visualization
coordinates aect topic models well.
metric use measure quality topic models pairwise mutual information
PMI. measures topic interpretability, based coocurrence frequencies top words
topic large external corpus. Although metrics perplexity heldout likelihood show generalization ability learned topic model unseen test
data, traditional metrics capture whether topics coherent (Chang, Gerrish,
Wang, Boyd-Graber, & Blei, 2009). Therefore, comparison, rely PMI,
measure quality topic words terms interpretability human.
human subjects, interpretability closely related coherence (Newman, Lau, Grieser, &
Baldwin, 2010), i.e., much top keywords topic associated
other. extensive study evaluation methods coherence, Newman et al. (2010)
identify Pointwise Mutual Information (PMI) best measure, terms
greatest correlation human judgments.
PMI based term cooccurrences. pair words wi wj , PMI dened
p(wi ,wj )
log p(wi )p(w
. topic, average pairwise PMIs among top 10 words
j)
1119

fiLe & Lauw

^

W>^s

W>

^E>
>











































Figure 13: Visualization documents 20N ews number topics Z = 20. point
represents document shape color represent document class.
topic drawn black, hollow circle.
1120

fiSemantic Visualization Neighborhood Graph Regularization

^

W>^s

W>

^E>
>



















Figure 14: Visualization documents Reuters8 number topics Z = 20.
point represents document shape color represent document class.
topic drawn black, hollow circle.

1121

fiLe & Lauw

^

W>^s

W>

^E>
>



























Figure 15: Visualization documents Cade12 number topics Z = 20. point
represents document shape color represent document class.
topic drawn black, hollow circle.

1122

fiSemantic Visualization Neighborhood Graph Regularization

W>^s

^






WD/^

WD/^

^






W>^s









Ed








Ed



Z

E

Figure 16: Topic Interpretability Semafore PLSV terms PMI Score (higher
better).

topic. topic model, average PMI across topics. Intuitively, PMI higher
(better), topic features words highly correlated one another.
Key PMI use external corpus estimate p(wi , wj ) p(wi ). Following
Newman et al. (2009), use Google Web 1T 5-gram Version 1 (Brants & Franz, 2006),
huge corpus n-grams generated 1 trillion word tokens. p(wi ) estimated
frequencies 1-grams. recommended Newman et al., p(wi , wj ) estimated
frequencies 5-grams. obtain PMI English-based 20N ews Reuters8,
Cade12 possess large-scale n-gram corpus specically
Brazilian Portuguese.
Figure 16, plot PMI score various number topics Z. Semafore performs
better PLSV across topics settings. Figure 16(a) 20News, except
case Z = 10, cases Semafores outperformance signicant 0.05 level
lower. Figure 16(b) Reuters8, cases Semafores outperformance signicant
0.05 level lower except Z = 30. results show Semafore improves
visualization sacricing topic interpretability learned topics.
greater appreciation quality output topic models, Appendix C,
show several examples topic models Z = 20, Semafore PLSV, terms
top keywords highest probabilities topic.

7. Conclusion
paper, address semantic visualization problem, jointly conducts topic
modeling visualization documents. propose new framework incorporate
neighborhood structure within probabilistic semantic visualization model called Semafore.
model carefully designed reect context semantic visualization, leading
number design choices related RBF kernel mapping topic visualiza1123

fiLe & Lauw

tion spaces, approximation neighborhood graph construction weighting,
well appropriate regularization functions spaces. Experiments real-life
datasets show Semafore signicantly outperforms baselines terms visualization quality accuracy, similar, slightly better topic model.
provides evidence neighborhood structure, together joint modeling topics
visualization, important semantic visualization.

Appendix A. Balancing Contributions Neighbors Non-neighbors
Regularization
mentioned Section 4.2, balance contribution neighbors R+
non-neighbors R neighborhood regularization R Equation 8 may require careful
tuning graph parameters (i.e., k). example, case using k-NN graph
N total number documents, would kN terms neighbor regularization
R+ , (N k)N terms non-neighbor regularization R . Supposing N increases
signicantly, might imbalance k remain unchanged. Therefore, N
changes, k also tuned accordingly maintain balance. simplistic
point, ratio kN (N k)N would remain roughly N
k grow similar factors. practice, recommend tuning k carefully.
run additional experiments validate argument 20N ews dataset.
basic point N changes, k tuned still show signicant improvement
due neighborhood graph regularization. closest baseline PLSV, empirically terms classication accuracy, well conceptually PLSV shares similar
generative process dierent kernel without neighborhood regularization.
Hence, compare performance method Semafore (with k-NN graph, heat
kernel weighting, Student-t kernel) PLSV various data sizes Z = 20 topics.
Figure 17(a) dataset size N = 500, Semafore runs k = 10.
Figure 17(b) dataset size N = 1000, Semafore runs k = 10.
Figure 17(c) dataset size N = 5000, Semafore runs k = 50.
note 10X dierence smallest largest datasets.
Yet relative outperformance Semafore PLSV around 15% 20% evident
across three datasets. supports case k tuned produce positive
eect using neighborhood graph regularization.

Appendix B. Additional Comparisons
mentioned Section 6.4, completeness, include additional comparisons
visualization methods also aim topic modeling. particular, include two
methods. First, include t-SNE (Van der Maaten & Hinton, 2008), also used
composite t-SNE (LDA). Second, include Laplacian EigenMaps (LE) (Belkin &
Niyogi, 2003), takes input neighborhood graph. Figure 18 Figure 19
show classication accuracy preservation accuracy Semafore , t-SNE LE
1124

fiSemantic Visualization Neighborhood Graph Regularization














^



W>^s

^


W>^s




^




























E

E

W>^s











E

Figure 17: Classication accuracy comparison 20N ews various data sizes (Z = 20).

^

^E
















>













































E













Z



Figure 18: Classication accuracy comparison.

^

^E







W


W

W



>
















E
























Z

Figure 19: Preservation accuracy comparison.

1125










fiLe & Lauw

varying t. Semafore outperforms LE cases. t-SNE, Semafore outperforms t-SNE Reuters8. However, 20N ews Cade12, dicult tell
whether Semafore t-SNE better. t-SNE tends decreasing accuracy increases. expected t-SNE known focus preserving local structure
(Van der Maaten & Hinton, 2008). small, basically consider local
structure visualization. increases, consider global structure
visualization Semafore outperforms t-SNE signicantly. Overall, Semafore
stable t-SNE changes, indicates Semafore tries balance preserving
local global structure better t-SNE. emphasize comparison
information purpose only, regard t-SNE LE comparative baselines.

Appendix C. Topic Model Examples
showcase topic models derived Semafore PLSV. 20N ews, Table 4 shows
topics Semafore, Table 5 shows topics PLSV. Reuters8, Table 6
shows topics Semafore, Table 7 shows topics PLSV. Cade12, Table 8
shows topics Semafore, Table 9 shows topics PLSV.
method, show list twenty topics. topic, produce
top ten words highest probabilities. shown top words, topics
correspond strongly classes. example, topic s19 Table 4 20N ews
Christianity, corresponds soc.religion.christian class. Topic s4
cars motorcycles, corresponding rec.autos rec.motorcycles. Topic s12 probably
concerning categories rec.sport.baseball and/or rec.sport.hockey.
Overall, observe quality topic words comparable across comparative methods. Note direct correspondence topics dierent
methods (e.g., rst topic Semafore may correspond rst topic PLSV).
manual inspection, see related topics, e.g., s4 p6 ,
s12 p7 . However, sets topics set keywords topic
identical. borne slight dierence terms PMI scores.
qualitative study helps show Semafore improves visualization quality,
still maintaining least quality topic words, better. supports
conclusion reached quantitative comparisons main manuscript.

1126

fiSemantic Visualization Neighborhood Graph Regularization

Table 4: Semafores Topic Model 20N ews (for 20 topics)
Topic ID
s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
s11
s12
s13
s14
s15
s16
s17
s18
s19

Top 10 Words
space, system, -rcb-, book, computer, university, list, post, price, science
article, year, good, write, guy, well, time, head, question, leave
gun, law, kuwait, people, death, fbus, article, control, weapon, child
window, le, program, widget, application, type, will, resource, call, function
car, bike, speed, engine, drive, lock, turn, mile, front, change
will, power, place, work, rate, write, sound, lead, good, interested
write, article, thing, time, people, better, start, problem, will, good
write, time, people, friend, pay, public, article, tax, opinion, money
people, claim, write, system, person, moral, evidence, objective, read, state
image, datum, graphic, send, le, format, package, software, mail, include
armenian, re, jew, child, kill, start, people, turkish, door, israel
system, board, will, datum, time, work, tape, test, copy, command
game, team, year, player, win, play, will, hit, season, hockey
will, post, space, good, time, include, cost, option, launch, people
drive, card, window, appear, disk, ram, driver, memory, work, color
mr., president, stephanopoulo, state, group, consider, party, question, issue, press
write, article, well, will, thing, work, point, include, time, help
key, article, chip, food, write, people, government, encryption, thing, algorithm
price, buy, apple, computer, dealer, t, model, problem, sell, monitor
god, jesus, will, christian, religion, faith, truth, bible, belief, church

Table 5: PLSVs Topic Model 20N ews (for 20 topics)
Topic ID
p0
p1
p2
p3
p4
p5
p6
p7
p8
p9
p10
p11
p12
p13
p14
p15
p16
p17
p18
p19

Top 10 Words
write, people, christian, belief, time, faith, god, religion, life,
god, will, jesus, kuwait, atheist, church, christian, man, religion, sin
armenian, appear, art, turkish, tartar, 1st, village, armenia, 1.40, genocide
will, key, write, time, article, government, system, thing, chip, hit
mr., stephanopoulo, president, will, party, state, door, time, meeting, open
write, re, article, gun, system, -rcb-, start, people, fbus, claim
car, will, bike, engine, drive, well, dealer, battery, change, front
game, win, year, will, team, play, season, good, goal, playo
player, team, write, hockey, game, fan, article, year, will, guy
space, system, datum, will, april, nasa, security, university, computer, list
graphic, image, le, ftp, send, format, package, system, datum, object
image, datum, program, window, version, le, software, tool, support, user
drive, jumper, master, ndet loop, slave, rate, gun, function, crime, set
window, le, card, will, program, color, driver, support, disk, bit
people, write, state, article, law, government, country, rights, jew,
write, article, thing, people, good, will, time, lot, year, day
work, drive, tape, scsus, problem, simm, controller, write, memory, article
widget, -rcb-, window, -lcb-, application, resource, set, visual, type, le
price, will, write, system, computer, article, apple, chip, monitor, board
will, vote, comp, newsgroup, suit, problem, os2, sco, post, mail

1127

fiLe & Lauw

Table 6: Semafores Topic Model Reuters8 (for 20 topics)
Topic ID
s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
s11
s12
s13
s14
s15
s16
s17
s18
s19

Top 10 Words
company, pipeline, raise, crude, march, spokesman, renery, capacity, corp, post
pct, bank, day, stg, today, reuter, money, market, mln, bill
oer, share, company, board, group, acquire, stock, dlr, acquisition, receive
exchange, currency, dollar, west, nance, baker, monetary, germany, continue, interest
share, reuter, dlr, mln, buy, company, corp, pay, stock, group
price, opec, market, bpd, ocial, february, month, output, saudus, january
rate, bank, pct, cut, fund, prime, point, reserve, issue, lower
billion, foreign, import, increase, dlr, trade, economic, export, will, country
bank, billion, market, government, fall, stock, economy, rise, surplus, decit
will, company, sell, pct, vessel, operation, week, billion, shipping, unit
strike, port, union, spokesman, cargo, employer, worker, sector, redundancy, court
oil, export, dlr, industry, year, pct, future, company, report, price
reuter, pct, report, national, week, brazil, today, increase, pay, april
trade, japan, japanese, reagan, state, tari, unite, market, washington, ocial
grain, mln, soviet, crop, tonne, year, usda, production, fall, analyst
trade, talk, gulf, gatt, bill, yeutter, round, reuter, call, negotiation
certicate, reuter, cost, government, program, agreement, agriculture, will, study, loan
year, ocial, import, will, state, price, government, china, land, rise
mln, ct, loss, net, shr, dlr, prot, qtr, reuter, year
oil, mln, will, barrel, dlr, crude, source, level, petroleum, day

Table 7: PLSVs Topic Model Reuters8 (for 20 topics)
Topic ID
p0
p1
p2
p3
p4
p5
p6
p7
p8
p9
p10
p11
p12
p13
p14
p15
p16
p17
p18
p19

Top 10 Words
will, oil, company, reuter, industry, canada, price, shell, raise, sell
rate, currency, dollar, exchange, baker, west, will, bank, reuter, treasury
bank, pct, day, import, year, rate, export, february, expect, reuter
share, company, corp, oer, stock, board, will, reuter, dlr, buy
rate, bank, pct, prime, cut, point, interest, market, lower, savings
market, bank, stock, price, japan, ministry, rise, ocial, gulf, bond
reuter, pct, week, report, year, march, mark, american, commission, gure
mln, ct, loss, net, dlr, shr, year, prot, qtr, reuter
mln, pct, billion, stg, dlr, reuter, market, january, revise, rise
billion, dlr, rate, market, surplus, currency, reserve, trading, dollar, foreign
oil, opec, price, bpd, pipeline, mln, crude, ocial, dlr, output
crude, dlr, barrel, corp, capacity, renery, oil, company, oer, group
reuter, ocial, state, cut, gulf, government, today, action, force, tell
oil, government, indonesium, price, foreign, bank, billion, reserve, company, industry
certicate, company, mln, year, grain, cooperative, program, dlr, government, cost
year, trade, agriculture, reuter, grain, agreement, gatt, yeutter, nancial, agricultural
strike, port, union, spokesman, employer, brazil, cargo, worker, redundancy, sector
trade, japan, japanese, reagan, tari, unite, washington, state, nakasone, semiconductor
grain, mln, crop, tonne, soviet, year, ocial, china, pct, oer
trade, country, minister, talk, state, meeting, economic, exchange, issue, baldrige

1128

fiSemantic Visualization Neighborhood Graph Regularization

Table 8: Semafores Topic Model Cade12 (for 20 topics)
Topic ID
s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
s11
s12
s13
s14
s15
s16
s17
s18
s19

Top 10 Words
sp, aulas, tecnologia, rj, sao, area, janeiro, particulares, areas, sica
terra, jun, gif, busca, virtual, brasil, forum, tempo, noticias, revistas
trabalho, seguranca, saude, medicina, ocupacional, prevencao, ppra, pcmso, imagem, imagens
peixes, cade, lazer, pesca, agua, rio, praia, hotel, sao, doce
agar, vida, personal, sica, base, tratamento, tem, pode, sistema, trainer
sao, br, rio, sul, criancas, www, escola, mail, http, atendimento
links, page, home, fotos, pagina, dicas, download, tenis, informacoes, jogos
internet, informatica, acesso, mg, br, servicos, provedor, mail, revista, horizonte
servicos, sao, paulo, entregas, entrega, sp, cesta, express, empresa, servico
pesca, sp, grupo, brasil, eventos, video, mg, informacoes, turismo, danca
astronomia, pagina, jose, foi, bem, espaco, tem, veja, losoa, correio
mp, banda, musicas, rock, musica, page, letras, bandas, pagina, site
historia, cultura, mundo, site, page, brasil, informacoes, rs, livro, arte
noticias, jornal, cidade, sp, sao, regiao, demolay, ordem, rio, capitulo
empresas, informacoes, informacao, dados, atraves, textos, mail, equipe, unicamp, centro
engenharia, servicos, projetos, empresa, consultoria, quimica, instituto, pesquisa, rio, manutencao
site, informacoes, brasil, associacao, educacao, pagina, organizacao, centro, brasileira, direitos
software, web, empresa, sistemas, sistema, br, marketing, desenvolvimento, windows, dados
virtual, online, venda, produtos, cade, shopping, internet, loja, compras, cursos
futebol, informacoes, fotos, clube, historia, paulo, sao, quake, pagina, cade

Topic ID
p0
p1
p2
p3
p4
p5
p6
p7
p8
p9
p10
p11
p12
p13
p14
p15
p16
p17
p18
p19

Top 10 Words
engenharia, projetos, servicos, trabalho, empresa, consultoria, seguranca, sp, medicina, sao
sao, ong, rio, instituto, personal, educacao, organizacao, sp, paulo, ns
sao, br, desenvolvimento, sistema, tratamento, mail, sistemas, clientes, informacoes, empresa
aulas, formula, quimica, particulares, informacoes, matematica, pilotos, fotos, sica, site
jornal, tenis, noticias, esportes, sp, informacoes, sao, esporte, fotos, links
musica, page, rock, bandas, links, home, pagina, musicas, music, fotos
pesca, demolay, sp, peixes, sao, fotos, ordem, capitulo, paulo, jitsu
mp, musicas, nacionais, agar, internacionais, rock, formato, site, page, pagina
pesquisa, tecnologia, informacoes, cade, ciencia, geograa, pesquisas, area, instituto, pagina
site, pagina, internet, mail, clique, veja, br, pode, foi, links
astronomia, informacoes, cultura, site, pagina, brasil, home, page, fotos, historia
banda, fotos, rock, letras, page, musicas, pagina, site, home, mp
internet, provedor, acesso, mg, informatica, software, servicos, belo, horizonte, manutencao
futebol, clube, sao, paulo, campeonato, historia, informacoes, pagina, turismo, tricolor
noticias, terra, internet, brasil, informatica, online, jornal, virtual, servicos, busca
links, page, quake, home, pagina, fotos, dicas, mp, download, informacoes
grupo, banda, karate, pagina, page, informacoes, fotos, home, rio, historia
produtos, virtual, shopping, cade, venda, online, sao, rio, loja, compras
br, sao, informacoes, marketing, mail, empresa, internet, www, fax, site
vida, dia, sao, foi, terra, panico, jose, tem, planetas, grande

Table 9: PLSVs Topic Model Cade12 (for 20 topics)

1129

fiLe & Lauw

References
Akkucuk, U., & Carroll, J. D. (2006). PARAMAP vs. Isomap: comparison two nonlinear
mapping algorithms. Journal Classication, 23 (2), 221254.
Bai, L., Guo, J., Lan, Y., & Cheng, X. (2014). Local Linear Matrix Factorization
Document Modeling. Advances Information Retrieval, pp. 398411. Springer.
Belkin, M., & Niyogi, P. (2001). Laplacian eigenmaps spectral techniques embedding clustering. Advances Neural Information Processing Systems (NIPS),
Vol. 14, pp. 585591.
Belkin, M., & Niyogi, P. (2003). Laplacian eigenmaps dimensionality reduction data
representation. Neural Computation, 15 (6), 13731396.
Belkin, M., Niyogi, P., & Sindhwani, V. (2006). Manifold regularization: geometric framework learning labeled unlabeled examples. Journal Machine Learning
Research (JMLR), 7, 23992434.
Bishop, C. M. (1995). Neural Networks Pattern Recognition. Oxford University Press.
Bishop, C. M., Svensen, M., & Williams, C. K. (1998). GTM: generative topographic
mapping. Neural Computation, 10 (1), 215234.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet Allocation. Journal
Machine Learning Research (JMLR), 3, 9931022.
Brants, T., & Franz, A. (2006). Web 1T 5-gram Version 1. Linguistic Data Consortium,
Philadelphia.
Buhmann, M. D. (2000). Radial basis functions. Acta Numerica 2000, 9.
Cai, D., Mei, Q., Han, J., & Zhai, C. (2008). Modeling hidden topics document manifold.
Proceedings ACM Conference Information Knowledge Management
(CIKM).
Cai, D., Wang, X., & He, X. (2009). Probabilistic dyadic data analysis local global
consistency. Proceedings International Conference Machine Learning
(ICML).
Cardoso-Cachopo, A. (2007). Improving Methods Single-label Text Categorization. PhD
Thesis, Instituto Superior Tecnico, Universidade Tecnica de Lisboa.
Carey, C., & Mahadevan, S. (2014). Manifold Spanning Graphs. Twenty-Eighth AAAI
Conference Articial Intelligence.
Chaney, A. J.-B., & Blei, D. M. (2012). Visualizing Topic Models. Proceedings
International AAAI Conference Web Social Media (ICWSM).
Chang, J., Gerrish, S., Wang, C., Boyd-Graber, J. L., & Blei, D. M. (2009). Reading
tea leaves: humans interpret topic models. Advances Neural Information
Processing Systems, pp. 288296.
Chen, L., & Buja, A. (2009). Local multidimensional scaling nonlinear dimension reduction, graph drawing, proximity analysis. Journal American Statistical
Association, 104 (485), 209219.
1130

fiSemantic Visualization Neighborhood Graph Regularization

Chi, E. H.-h. (2000). taxonomy visualization techniques using data state reference model. Proceedings IEEE Symposium Information Visualization
(InfoVis), pp. 6975.
Choo, J., Lee, C., Reddy, C. K., & Park, H. (2013). UTOPIAN: User-driven topic modeling based interactive nonnegative matrix factorization. IEEE Transactions
Visualization Computer Graphics, 19 (12), 19922001.
Chuang, J., Manning, C. D., & Heer, J. (2012). Termite: visualization techniques assessing textual topic models. Proceedings International Working Conference
Advanced Visual Interfaces (AVI), pp. 7477.
Coifman, R. R., & Lafon, S. (2006). Diusion maps. Applied Computational Harmonic
Analysis, 21 (1), 5 30.
Comon, P. (1994). Independent component analysis, new concept?. Signal Processing,
36 (3), 287314.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood incomplete
data via EM algorithm. Journal Royal Statistical Society, Series B, 39 (1),
138.
Dumais, S., Furnas, G., Landauer, T., Deerwester, S., Deerwester, S., et al. (1995). Latent
semantic indexing. Proceedings Text Retrieval Conference.
Fisher, R. A. (1936). use multiple measurements taxonomic problems. Annals
Eugenics, 7 (2), 179188.
Gabrilovich, E., & Markovitch, S. (2009). Wikipedia-based semantic interpretation
natural language processing. Journal Articial Intelligence Research (JAIR), 34 (2),
443.
Golub, G. H., & Van Loan, C. F. (2012). Matrix Computations, Vol. 3. JHU Press.
Gretarsson, B., Odonovan, J., Bostandjiev, S., Hollerer, T., Asuncion, A., Newman, D., &
Smyth, P. (2012). TopicNets: Visual analysis large text corpora topic modeling.
ACM Transactions Intelligent Systems Technology (TIST), 3 (2), 23.
Hein, M., Audibert, J.-y., & Luxburg, U. V. (2007). Graph Laplacians Convergence
Random Neighborhood Graphs. Journal Machine Learning Research, pp.
13251368.
Hinton, G. E., & Roweis, S. T. (2002). Stochastic neighbor embedding. Advances
Neural Information Processing Systems (NIPS), pp. 833840.
Hofmann, T. (1999). Probabilistic latent semantic indexing. Proceedings International ACM SIGIR Conference Research Development Information
Retrieval (SIGIR), pp. 5057.
Hu, Y., Boyd-Graber, J., Satino, B., & Smith, A. (2014). Interactive topic modeling.
Machine Learning, 95 (3), 423469.
Huh, S., & Fienberg, S. E. (2012). Discriminative topic modeling based manifold learning.
ACM Transactions Knowledge Discovery Data (TKDD), 5 (4), 20.
1131

fiLe & Lauw

Iwata, T., Saito, K., Ueda, N., Stromsten, S., Griths, T. L., & Tenenbaum, J. B. (2007).
Parametric embedding class visualization. Neural Computation, 19 (9), 25362556.
Iwata, T., Yamada, T., & Ueda, N. (2008). Probabilistic latent semantic visualization: topic
model visualizing documents. Proceedings ACM SIGKDD International
Conference Knowledge Discovery Data Mining (KDD), pp. 363371.
Jebara, T., Wang, J., & Chang, S.-F. (2009). Graph construction b-matching semisupervised learning. Proceedings 26th Annual International Conference
Machine Learning, pp. 441448. ACM.
Jollie, I. (2005). Principal Component Analysis. Wiley Online Library.
Kim, M., & Torre, F. (2010). Local minima embedding. Proceedings International
Conference Machine Learning (ICML), pp. 527534.
Kohonen, T. (1990). self-organizing map. Proceedings IEEE, 78 (9), 14641480.
Kruskal, J. B. (1964). Multidimensional scaling optimizing goodness nonmetric
hypothesis. Psychometrika, 29 (1), 127.
Laerty, J. D., & Wasserman, L. (2007). Statistical Analysis Semi-Supervised Regression.
Advances Neural Information Processing Systems (NIPS), pp. 801808.
Le, T., & Lauw, H. W. (2014a). Semantic visualization spherical representation.
Proceedings ACM SIGKDD International Conference Knowledge Discovery
Data Mining, pp. 10071016. ACM.
Le, T. M., & Lauw, H. W. (2014b). Manifold learning jointly modeling topic
visualization. Proceedings AAAI Conference Articial Intelligence.
Liu, D. C., & Nocedal, J. (1989). limited memory BFGS method large scale
optimization. Mathematical Programming, 45, 503528.
Manning, C. D., Raghavan, P., Schutze, H., et al. (2008). Introduction Information
Retrieval, Vol. 1. Cambridge University Press Cambridge.
McCallum, A., Wang, X., & Corrada-Emmanuel, A. (2007). Topic role discovery
social networks experiments enron academic email.. Journal Articial
Intelligence Research (JAIR), 30, 249272.
Millar, J. R., Peterson, G. L., & Mendenhall, M. J. (2009). Document Clustering
Visualization Latent Dirichlet Allocation Self-Organizing Maps. FLAIRS
Conference, Vol. 21, pp. 6974.
Newman, D., Karimi, S., & Cavedon, L. (2009). External evaluation topic models.
Australasian Document Computing Symposium (ADCS).
Newman, D., Lau, J. H., Grieser, K., & Baldwin, T. (2010). Automatic evaluation topic
coherence. Human Language Technologies: 2010 Annual Conference
North American Chapter Association Computational Linguistics, pp. 100
108.
Park, J., & Sandberg, I. W. (1991). Universal approximation using radial-basis-function
networks. Neural Computation, 3 (2), 246257.
1132

fiSemantic Visualization Neighborhood Graph Regularization

Ponzetto, S. P., & Strube, M. (2007). Knowledge derived Wikipedia computing
semantic relatedness.. Journal Articial Intelligence Research (JAIR), 30, 181212.
Reisinger, J., Waters, A., Silverthorn, B., & Mooney, R. J. (2010). Spherical topic models.
Proceedings International Conference Machine Learning (ICML), pp.
903910.
Roweis, S. T., & Saul, L. K. (2000). Nonlinear dimensionality reduction locally linear
embedding. Science, 290 (5500), 23232326.
Shaw, B., & Jebara, T. (2007). Minimum volume embedding. Proceedings International Conference Articial Intelligence Statistics (AISTATS), pp. 460467.
Shaw, B., & Jebara, T. (2009). Structure preserving embedding. Proceedings
International Conference Machine Learning (ICML), pp. 937944. ACM.
Tenenbaum, J. B., De Silva, V., & Langford, J. C. (2000). global geometric framework
nonlinear dimensionality reduction. Science, 290 (5500), 23192323.
Ting, D., Huang, L., & Jordan, M. I. (2010). Analysis Convergence Graph Laplacians. Proceedings International Conference Machine Learning (ICML).
Turney, P. D., Pantel, P., et al. (2010). frequency meaning: Vector space models
semantics. Journal Articial Intelligence Research (JAIR), 37 (1), 141188.
Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-SNE. Journal Machine
Learning Research (JMLR), 9 (2579-2605), 85.
Wei, F., Liu, S., Song, Y., Pan, S., Zhou, M. X., Qian, W., Shi, L., Tan, L., & Zhang,
Q. (2010). Tiara: visual exploratory text analytic system. Proceedings
ACM SIGKDD International Conference Knowledge Discovery Data Mining
(KDD), pp. 153162.
Wu, H., Bu, J., Chen, C., Zhu, J., Zhang, L., Liu, H., Wang, C., & Cai, D. (2012). Locally
discriminative topic modeling. Pattern Recognition, 45 (1), 617625.
Zemel, R. S., & Carreira-Perpinan, M. A. (2004). Proximity graphs clustering
manifold learning. Advances Neural Information Processing Systems (NIPS),
pp. 225232.
Zhou, D., Bousquet, O., Lal, T. N., Weston, J., & Scholkopf, B. (2004). Learning local
global consistency. Advances Neural Information Processing Systems (NIPS),
16 (16).
Zhu, X., Ghahramani, Z., Laerty, J., et al. (2003). Semi-supervised learning using Gaussian
elds harmonic functions. Proceedings International Conference
Machine Learning (ICML), Vol. 3, pp. 912919.

1133

fiJournal Artificial Intelligence Research 55 (2016) 835-887

Submitted 10/2015; published 04/2016

Parallel Model-Based Diagnosis Multi-Core Computers
Dietmar Jannach
Thomas Schmitz

dietmar.jannach@tu-dortmund.de
thomas.schmitz@tu-dortmund.de

TU Dortmund, Germany

Kostyantyn Shchekotykhin

kostyantyn.shchekotykhin@aau.at

Alpen-Adria University Klagenfurt, Austria

Abstract
Model-Based Diagnosis (MBD) principled domain-independent way analyzing system examination behaving expected. Given abstract
description (model) systems components behavior functioning normally, MBD techniques rely observations actual system behavior reason
possible causes discrepancies expected observed behavior. Due generality, MBD successfully applied variety application
domains last decades.
many application domains MBD, testing different hypotheses reasons
failure computationally costly, e.g., complex simulations system behavior performed. work, therefore propose different schemes
parallelizing diagnostic reasoning process order better exploit capabilities
modern multi-core computers. propose systematically evaluate parallelization
schemes Reiters hitting set algorithm finding leading minimal diagnoses using two different conflict detection techniques. Furthermore, perform initial
experiments basic depth-first search strategy assess potential parallelization
searching one single diagnosis. Finally, test effects parallelizing direct
encodings diagnosis problem constraint solver.

1. Introduction
Model-Based Diagnosis (MBD) subfield Artificial Intelligence concerned
automated determination possible causes system behaving expected.
early days MBD, diagnosed systems typically hardware artifacts like
electronic circuits. contrast earlier heuristic diagnosis approaches connected
symptoms possible causes, e.g., expert rules (Buchanan & Shortliffe, 1984),
MBD techniques rely abstract explicit representation (model) examined
system. models contain information systems structure, i.e., list
components connected, well information behavior
components functioning correctly. model available, expected behavior (outputs) system given inputs thus calculated. diagnosis problem
arises whenever expected behavior conflicts observed system behavior. MBD
techniques core construct test hypotheses faultiness individual
components system. Finally, diagnosis considered subset components
that, assumed faulty, explain observed behavior system.
Reiter (1987) suggests formal logical characterization diagnosis problem
first principles proposed breadth-first tree construction algorithm determine
c
2016
AI Access Foundation. rights reserved.

fiJannach, Schmitz, & Shchekotykhin

diagnoses given problem. Due generality used knowledge-representation
language suggested algorithms computation diagnoses, MBD
later applied variety application problems hardware. application
fields MBD, example, include diagnosis knowledge bases ontologies, process
specifications, feature models, user interface specifications user preference statements,
various types software artifacts including functional logic programs well
VHDL, Java spreadsheet programs (Felfernig, Friedrich, Jannach, & Stumptner, 2004;
Mateis, Stumptner, Wieland, & Wotawa, 2000; Jannach & Schmitz, 2014; Wotawa, 2001b;
Felfernig, Friedrich, Isak, Shchekotykhin, Teppan, & Jannach, 2009; Console, Friedrich,
& Dupre, 1993; Friedrich & Shchekotykhin, 2005; Stumptner & Wotawa, 1999; Friedrich,
Stumptner, & Wotawa, 1999; White, Benavides, Schmidt, Trinidad, Dougherty, & Cortes,
2010; Friedrich, Fugini, Mussi, Pernici, & Tagni, 2010).
several application fields, search diagnoses requires repeated computations based modified versions original model test different hypotheses
faultiness individual components. several works original problem
converted Constraint Satisfaction Problem (CSP) number relaxed versions
original CSP solved construct new node search tree (Felfernig
et al., 2004; Jannach & Schmitz, 2014; White et al., 2010). Depending application domain, computation CSP solutions check consistency can, however,
computationally intensive actually represents costly operation
construction search tree. Similar problems arise underlying reasoning
techniques, e.g., ontology debugging (Friedrich & Shchekotykhin, 2005), used.
Current MBD algorithms sequential nature generate one node time.
Therefore, exploit capabilities todays multi-core computer processors,
nowadays found even mobile devices. paper, propose new schemes
parallelize diagnostic reasoning process better exploit available computing
resources modern computer hardware. particular, work comprises following
algorithmic contributions insights based experimental evaluations:
propose two parallel versions Reiters (1987) sound complete Hitting Set
(HS) algorithm speed process finding diagnoses, common
problem setting above-described MBD applications. approaches
considered window-based parallelization schemes, means limited number search nodes processed parallel point time.
evaluate two different conflict detection techniques multi-core setting,
goal find leading diagnoses. set experiments, multiple conflicts computed construction tree node using novel
MergeXplain method (MXP) (Shchekotykhin, Jannach, & Schmitz, 2015)
processing time therefore implicitly allocated conflict generation.
demonstrate speedups also achieved parallelization scenarios search one single diagnosis, e.g., using basic parallel
depth-first strategy.
measure improvements achieved parallel constraint solving
using direct CSP-based encoding diagnosis problem. experiment
836

fiParallel Model-Based Diagnosis Multi-Core Computers

illustrates parallelization underlying solvers, particular using
direct encoding, advantageous.
evaluate proposed parallelization schemes extensive set experiments. following problem settings analyzed.
(i) Standard benchmark problems diagnosis research community;
(ii) Mutated CSPs Constraint Programming competition domain
CSP-based spreadsheet debugging (Jannach & Schmitz, 2014);
(iii) Faulty OWL ontologies used evaluation MBD-based debugging techniques
expressive ontologies (Shchekotykhin, Friedrich, Fleiss, & Rodler, 2012);
(iv) Synthetically generated problems allow us vary characteristics
underlying diagnosis problem.
results show using parallelization techniques help achieve substantial
speedups diagnosis process (a) across variety application scenarios, (b) without
exploiting specific knowledge structure underlying diagnosis problem,
(c) across different problem encodings, (d) also application problems like ontology
debugging cannot efficiently encoded SAT problems.
outline paper follows. next section, define main concepts
MBD introduce algorithm used compute diagnoses. Section 3, present
systematically evaluate parallelization schemes Reiters HS-tree method
goal find minimal diagnoses. Section 4, report results evaluations
implicitly allocate processing time conflict generation using MXP
conflict detection. Section 5 assess potential gains comparably simple
randomized depth-first strategy hybrid technique problem finding one
single diagnosis. results experiments direct CSP encoding reported
Section 6. Section 7 discuss previous works. paper ends summary
outlook Section 8.

2. Reiters Diagnosis Framework
section summarizes Reiters (1987) diagnosis framework use basis
work.
2.1 Definitions
Reiter (1987) formally characterized Model-Based Diagnosis using first-order logic.
main definitions summarized follows.
Definition 2.1. (Diagnosable System) diagnosable system described pair (SD,
Comps) SD system description (a set logical sentences) Comps represents
systems components (a finite set constants).
connections components normal behavior components
described terms logical sentences. normal behavior system components
837

fiJannach, Schmitz, & Shchekotykhin

usually described SD help distinguished negated unary predicate ab(.),
meaning abnormal.
diagnosis problem arises observation P Obs systems input-output
behavior (again expressed first-order sentences) deviates expected system behavior. diagnosis corresponds subset systems components
assume behave abnormally (be faulty) assumptions must consistent
observations. words, malfunctioning components
possible reason observations.
Definition 2.2. (Diagnosis) Given diagnosis problem (SD, Comps, Obs), diagnosis
subset minimal set Comps SD Obs tab(c)|c P u abpcq|c P
Compszu consistent.
According Definition 2.2, interested minimal diagnoses, i.e., diagnoses
contain superfluous elements thus supersets diagnoses. Whenever use term diagnosis remainder paper, therefore mean minimal
diagnosis. Whenever refer non-minimal diagnoses, explicitly mention fact.
Finding diagnoses theory done simply trying possible subsets
Comps checking consistency observations. Reiter (1987), however,
proposes efficient procedure based concept conflicts.
Definition 2.3. (Conflict) conflict (SD, Comps, Obs) set tc1 , ..., ck u Comps
SD Obs abpc1 q, ..., abpck qu inconsistent.
conflict corresponds subset components which, assumed behave normally,
consistent observations. conflict c considered minimal,
proper subset c exists also conflict.
2.2 Hitting Set Algorithm
Reiter (1987) discusses relationship conflicts diagnoses claims
Theorem 4.4 set diagnoses collection (minimal) conflicts F
equivalent set H minimal hitting sets 1 F .
determine minimal hitting sets therefore diagnoses, Reiter proposes
breadth-first search procedure construction hitting set tree (HS-tree), whose
construction guided conflicts. logic-based definition MBD problem
(Reiter, 1987), conflicts computed calls Theorem Prover (TP). TP
component considered black box assumptions made
conflicts determined. Depending application scenario problem encoding,
one can, however, also use specific algorithms like QuickXplain (Junker, 2004), Progression (Marques-Silva, Janota, & Belov, 2013) MergeXplain (Shchekotykhin et al., 2015),
guarantee computed conflict sets minimal.
main principle HS-tree algorithm create search tree node
either labeled conflict represents diagnosis. latter case node
expanded. Otherwise, child node generated element nodes
1. Given collection C subsets finite set S, hitting set C subset contains
least one element subset C. corresponds set cover problem.

838

fiParallel Model-Based Diagnosis Multi-Core Computers

conflict outgoing edge labeled one component nodes conflict.
subsequent expansions node components used label edges
path root tree current node assumed faulty. newly
generated child node either diagnosis labeled conflict
contain component already assumed faulty stage. conflict
found node, path labels represent diagnosis sense Definition 2.2.
2.2.1 Example
following example show HS-tree algorithm QuickXplain
(QXP) conflict detection technique combined locate fault specification
CSP. CSP instance defined tuple pV, D, Cq, V tv1 , . . . , vn u
set variables, tD1 , . . . , Dn u set domains variables V ,
C tC1 , . . . , Ck u set constraints. assignment subset X V set
pairs txv1 , d1 y, . . . , xvk , dm yu vi P X variable dj P Di value
domain variable. assignment comprises exactly one variable-value pair
variable X. constraint Ci P C defined list variables S, called scope,
forbids allows certain simultaneous assignments variables scope.
assignment satisfies constraint Ci comprises assignment allowed Ci .
assignment solution satisfies constraints C.
Consider CSP instance variables V ta, b, cu variable
domain t1, 2, 3u following set constraints defined:
C1 : b,

C2 : b c,

C3 : c a,

C4 : b c

Obviously, solution exists diagnosis problem consists finding subsets
constraints whose definition faulty. engineer modeled CSP could,
example, made mistake writing C2, b c.
Eventually, C4 added later correct problem, engineer forgot remove
C2. Given faulty definition I, two minimal conflicts exist, namely ttC1, C2, C3u,
tC2, C4uu, determined help QXP. Given two conflicts,
HS-tree algorithm finally determine three minimal hitting sets ttC2u, tC1, C4u,
tC3, C4uu, diagnoses problem instance. set diagnoses also contains
true cause error, definition C2.
Let us review detail HS-tree/QXP combination works example problem. illustrate tree construction Figure 1. logic-based definition
Reiter, HS-tree algorithm starts check observations Obs consistent
system description SD components Comps. application setting
corresponds check exists solution CSP instance.2 Since
case, QXP-call made, returns conflict tC1, C2, C3u, used
label root node ( 1 ) tree. element conflict, child node
created conflict element used path label. tree node, consistency SD, Obs, Comps tested; time, however, elements appear
2. Comps constraints tC1...C4u SD corresponds semantics/logic constraints
working correctly, e.g., ABpC1q _ pa bq. Obs empty example could partial value
assignment (test case) another scenario.

839

fiJannach, Schmitz, & Shchekotykhin

1
{C1,C2, C3}
C1

2

C2

C3

3

{C2,C4}
4

C2

{C2,C4}

C4

C4

C2
5

Figure 1: Example HS-tree construction.
labels path root node current node considered abnormal.
CSP diagnosis setting, means check solution modified
version original CSP remove constraints appear labels
path root current node.
node 2 , C1 correspondingly considered abnormal. removing C1
CSP is, however, sufficient solution exists relaxed problem, another call
QXP made, returns conflict tC2, C4u. tC1u therefore diagnosis
new conflict used label node 2 . algorithm proceeds breadth-first
style tests assuming tC2u tC3u individually faulty consistent
observations, case means solution relaxed CSP exists. Since tC2u
diagnosis least one solution exists C2 removed CSP definition
node marked 3 expanded. node 3 , correspond
diagnosis, already known conflict tC2, C4u reused overlap
nodes path label call P (QXP) required. last tree level,
nodes 4 5 expanded (closed marked 7) tC2u
already identified diagnosis previous level resulting diagnoses would
supersets tC2u. Finally, sets tC1, C4u tC3, C4u identified additional
diagnoses.
2.2.2 Discussion
Soundness Completeness According Reiter (1987), breadth-first construction scheme node closing rule ensure minimal diagnoses computed.
end HS-tree construction process, set edge labels path
root tree node marked 3 corresponds diagnosis.3
Greiner, Smith, Wilkerson (1989), later on, identified potential problem Reiters
algorithm cases conflicts returned P guaranteed minimal.
extension algorithm based HS-DAG (directed acyclic graph) structure
proposed solve problem.
context work, use methods return conflicts guaranteed minimal. example, according Theorem 1 work Junker (2004),
given set formulas sound complete consistency checker, QXP always returns
3. Reiter (1987) states Theorem 4.8 given set conflict sets F , HS-tree algorithm outputs
pruned tree set tHpnq|n node labeled 3u corresponds set H
minimal hitting sets F Hpnq set arc labels path node n root.

840

fiParallel Model-Based Diagnosis Multi-Core Computers

either minimal conflict conflict. minimality guarantee turn means
combination HS-tree algorithm QXP sound complete, i.e., returned
solutions actually (minimal) diagnoses diagnosis given set conflicts
missed. holds computing multiple conflicts time MXP
(Shchekotykhin et al., 2015).
simplify presentation parallelization approaches, therefore rely
Reiters original HS-tree formulation; extension deal HS-DAG structure
(Greiner et al., 1989) possible.
On-Demand Conflict Generation Complexity many above-mentioned
applications MBD practical problems, conflicts computed on-demand,
i.e., tree construction, cannot generally assume set minimal
conflicts given advance. Depending problem setting, finding conflicts
therefore computationally intensive part diagnosis process.
Generally, finding hitting sets collection sets known NP-hard problem
(Garey & Johnson, 1979). Moreover, deciding additional diagnosis exists conflicts
computed demand NP-complete even propositional Horn theories (Eiter &
Gottlob, 1995). Therefore, number heuristics-based, approximate thus incomplete,
well problem-specific diagnosis algorithms proposed years.
discuss approaches later sections. next section, we, however, focus
(worst-case) application scenarios goal find minimal diagnoses given
problem, i.e., focus complete algorithms.
Consider, example, problem debugging program specifications (e.g., constraint
programs, knowledge bases, ontologies, spreadsheets) MBD techniques mentioned
above. application domains, typically sufficient find one minimal diagnosis. work Jannach Schmitz (2014), example, spreadsheet developer
presented ranked list sets formulas (diagnoses) represent possible
reasons certain test case failed. developer either inspect
individually provide additional information (e.g., test cases) narrow set
candidates. one diagnosis computed presented, developer would
guarantee true cause problem, lead limited acceptance
diagnosis tool.

3. Parallel HS-Tree Construction
section present two sound complete parallelization strategies Reiters
HS-tree method determine minimal diagnoses.
3.1 Non-recursive HS-Tree Algorithm
use non-recursive version Reiters sequential HS-tree algorithm basis
implementation two parallelization strategies. Algorithm 1 shows main loop
breadth-first procedure, uses list open nodes expanded central data
structure.
algorithm takes diagnosis problem (DP) instance input returns set
diagnoses. DP given tuple (SD, Comps, Obs), SD system
841

fiJannach, Schmitz, & Shchekotykhin

Algorithm 1: diagnose: Main algorithm loop.
Input: diagnosis problem (SD, Comps, Obs)
Result: set diagnoses
1
2
3
4
5
6
7
8
9

H; paths H; conflicts H;
nodesToExpand = xgenerateRootNode(SD, Comps, Obs)y;
nodesToExpand x
newNodes = x y;
node = head(nodesToExpand) ;
foreach c P node.conflict
generateNode(node, c, , paths, conflicts, newNodes);
nodesToExpand = tail(nodesToExpand) newNodes;
return ;

Algorithm 2: generateNode: Node generation logic.
Input: existingNode expand, conflict element c P Comps,
sets , paths, conflicts, newNodes
1
2
3
4
5
6
7
8
9
10
11
12
13

newPathLabel = existingNode.pathLabel {c};
pE l P : l newPathLabelq ^ checkAndAddPathppaths, newPathLabelq
node = new Node(newPathLabel);
P conflicts : X newPathLabel H
node.conflict = S;
else
newConflicts = checkConsistency(SD, Comps, Obs, node.pathLabel);
node.conflict = head(newConflicts);
node.conflict H
newNodes = newNodes xnodey;
conflicts = conflicts newConflicts;
else
{node.pathLabel};

description, Comps set components potentially faulty Obs set
observations. method generateRootNode creates initial node, labeled
conflict empty path label. Within loop, first element firstin-first-out (FIFO) list open nodes nodesToExpand taken current element.
function generateNode (Algorithm 2) called element nodes conflict
adds new leaf nodes, still explored, global list. new
nodes appended () remaining list open nodes main loop,
842

fiParallel Model-Based Diagnosis Multi-Core Computers

continues elements remain expansion.4 Algorithm 2 (generateNode)
implements node generation logic, includes Reiters proposals conflict re-use,
tree pruning, management lists known conflicts, paths diagnoses.
method determines path label new node checks new path label
superset already found diagnosis.
Algorithm 3: checkAndAddPath: Adding new path label redundancy
check.
Input: previously explored paths, newPathLabel explored
Result: Boolean stating newPathLabel added paths

3

E l P paths : l newPathLabel
paths = paths newPathLabel;
return true;

4

return false;

1
2

function checkAndAddPath (Algorithm 3) used check node
already explored elsewhere tree. function returns true new path label
successfully inserted list known paths. Otherwise, list known paths
remains unchanged node closed.
new nodes, either existing conflict reused new one created call
consistency checker (Theorem Prover), tests new node diagnosis
returns set minimal conflicts otherwise. Depending outcome, new node
added list nodesToExpand diagnosis stored. Note Algorithm 2
return value instead modifies sets , paths, conflicts, newNodes,
passed parameters.
3.2 Level-Wise Parallelization
first parallelization scheme examines nodes one tree level parallel proceeds
next level elements level processed. example shown
Figure 1, would mean computations (consistency checks theorem prover
calls) required three first-level nodes labeled tC1u, tC2u, tC3u done
three parallel threads. nodes next level explored threads
previous level finished.
Using Level-Wise Parallelization (LWP) scheme, breadth-first character maintained. parallelization computations generally feasible consistency
checks node done independently done nodes
level. Synchronization required make sure thread starts exploring
path already examination another thread.
Algorithm 4 shows sequential Algorithm 1 adapted support
parallelization approach. Again, maintain list open nodes expanded.
difference run expansion nodes parallel collect
4. limitation regarding search depth number diagnoses find easily integrated
scheme.

843

fiJannach, Schmitz, & Shchekotykhin

Algorithm 4: diagnoseLW: Level-Wise Parallelization.
Input: diagnosis problem (SD, Comps, Obs)
Result: set diagnoses
1
2
3
4
5
6
7
8
9
10

H; conf licts H; paths = H;
nodesToExpand = xgenerateRootNode(SD, Comps, Obs)y;
nodesToExpand x
newNodes = x y;
foreach node P nodesToExpand
foreach c P node.conflict
// computations parallel
threads.execute(generateNode(node, c, , paths, conflicts, newNodes));
threads.await();
nodesToExpand = newNodes;

// Wait current level complete
// Prepare next level

return ;

nodes next level variable newNodes. current level finished,
overwrite list nodesToExpand list containing nodes next level.
Java-like API calls used pseudo-code Algorithm 4 interpreted
follows. statement threads.execute() takes function parameter schedules
execution pool threads given size. thread pool of, e.g., size 2,
generation first two nodes would done parallel next ones would
queued one threads finished. mechanism, ensure
number threads executed parallel less equal number hardware
threads CPUs.
statement threads.await() used synchronization blocks execution
subsequent code scheduled threads finished. guarantee
path explored twice, make sure two threads parallel add node
path label list known paths. achieved declaring
function checkAndAddPath critical section (Dijkstra, 1968), means
two threads execute function parallel. Furthermore, make access
global data structures (e.g., already known conflicts diagnoses) thread-safe,
i.e., ensure two threads simultanuously manipulate them.5
3.3 Full Parallelization
LWP, situations computation conflict specific node
takes particularly long. This, however, means even nodes current
level finished many threads idle, expansion HS-tree cannot proceed
level completed. Algorithm 5 shows proposed Full Parallelization (FP)
algorithm variant, immediately schedules every expandable node execution
thereby avoids potential CPU idle times end level.
5. Controlling concurrency aspects comparably simple modern programming languages like Java,
e.g., using synchronized keyword.

844

fiParallel Model-Based Diagnosis Multi-Core Computers

Algorithm 5: diagnoseFP: Full Parallelization.
Input: diagnosis problem (SD, Comps, Obs)
Result: set diagnoses
1
2
3
4
5
6
7
8

9
10
11
12

H; paths H; conflicts H;
nodesToExpand = xgenerateRootNode(SD, Comps, Obs)y;
size = 1; lastSize = 0;
psizelastSizeq _ pthreads.activeThreads 0q
1 size lastSize
node = nodesToExpand.get[lastSize + i];
foreach c P node.conflict
threads.execute(generateNodeFP(node, c, , paths, conflicts,
nodesToExpand));
lastSize = size;
wait();
size = nodesToExpand.length();
return ;

main loop algorithm slightly different basically monitors list
nodes expand. Whenever new entries list observed, i.e., last observed
list size different current one, retrieves recently added elements adds
thread queue execution. algorithm returns diagnoses new
elements added since last check threads active.6
FP, search necessarily follow breadth-first strategy anymore
non-minimal diagnoses found process. Therefore, whenever find new
diagnosis d, check set known diagnoses contains supersets
remove .
updated generateNode method listed Algorithm 6. updating shared
data structures (nodesToExpand, conflicts, ), make sure threads
interfere other. mutual exclusive section marked synchronized
keyword.
compared LWP, FP wait end level specific
node takes particularly long generate. hand, FP needs synchronization
threads, cases last nodes level finished
time, LWP could also advantageous. evaluate aspect Section 3.5.
3.4 Properties Algorithms
Algorithm 1 together Algorithms 2 3 corresponds implementation
HS-tree algorithm (Reiter, 1987). Algorithm 1 implements breadth-first search strategy
point (1) Reiters HS-tree algorithm since nodes stored list nodesToExpand
6. functions wait() notify() implement semantics pausing thread awaking paused
thread Java programming language used avoid active waiting loops.

845

fiJannach, Schmitz, & Shchekotykhin

Algorithm 6: generateNodeFP: Extended node generation logic.
Input: existingNode expand, c P Comps,
sets , paths, conflicts, nodesToExpand
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

17

newPathLabel = existingNode.pathLabel {c};
pE l P : l newPathLabelq ^ checkAndAddPathppaths, newPathLabelq
node = new Node(newPathLabel);
P conflicts : X newPathLabel H
node.conflict = S;
else
newConflicts = checkConsistency(SD, Comps, Obs, node.pathLabel);
node.conflict = head(newConflicts);
synchronized
node.conflict H
nodesToExpand = nodesToExpand xnodey;
conflicts = conflicts newConflicts;
else E P : newPathLabel
{node.pathLabel};
P : newPathLabel
z d;
notify();

processed iteratively first-in-first-out order (see lines 5 8). Algorithm 2 first
checks pruning rules (i) (ii) Reiter applied line 2. rules state
node pruned (i) exists diagnosis (ii) set labels
corresponding path tree subset set labels
path node. Pruning rule (ii) implemented Algorithm 3. Pruning rule (iii)
Reiters algorithm necessary since settings TP -call guarantees return
minimal conflicts.
Finally, point (2) Reiters HS-tree algorithm description implemented lines
4-8 Algorithm 2. Here, algorithm checks conflict reused
node label. case reuse possible, algorithm calls theorem prover TP find
another minimal conflict. conflict found, node added list open nodes
nodesToExpand . Otherwise, set node path labels added set diagnoses.
corresponds situation Reiters algorithm would mark node
HS-tree 3 symbol. Note label nodes 7 done Reiters
algorithms since simply store nodes expansion list.
Overall, conclude HS-tree algorithm implementation (Algorithm 1
3) properties Reiters original HS-tree algorithm. Namely, hitting set
returned algorithm minimal (soundness) existing minimal hitting sets
found (completeness).
846

fiParallel Model-Based Diagnosis Multi-Core Computers

3.4.1 Level-Wise Parallelization (LWP)
Theorem 3.1. Level-Wise Parallelization sound complete.
Proof. proof based fact LWP uses expansion pruning
techniques sequential algorithm (Algorithms 2 3). main loop line 3 applies
procedure original algorithm difference executions
Algorithm 2 done parallel level tree. Therefore, difference
sequential algorithm LWP lies order nodes one level
labeled generated.
Let us assume two nodes n1 n2 tree sequential
HS-tree algorithm process n1 n2 . Assuming neither n1 n2 correspond
diagnoses, sequential Algorithm 1 would correspondingly first add child nodes
n1 queue open nodes later append child nodes n2 .
parallelize computations needed generation n1 n2 LWP,
happen computations n1 need longer n2 . case
child nodes n2 placed queue first. order nodes
subsequently processed is, however, irrelevant computation minimal hitting
sets, since neither labeling pruning rules influenced it. fact,
labeling node n depends whether minimal conflict set f exists
Hpnq X f H, nodes level. pruning rules
state node n pruned exists node n1 labeled 3
Hpn1 q Hpnq, i.e., supersets already found diagnoses pruned. n n1
level, |Hpnq| |Hpn1 q|. Consequently, pruning rule applied
Hpnq Hpn1 q. Therefore, order nodes, i.e., nodes pruned, irrelevant
minimal hitting set lost. Consequently, LWP complete.
Soundness algorithm follows fact LWP constructs hitting sets
always order increasing cardinality. Therefore, LWP always return minimal hitting sets even scenarios stop k diagnoses found,
1 k N predefined constant N total number diagnoses
problem.
3.4.2 Full Parallelization (FP)
minimality hitting sets encountered search guaranteed FP,
since algorithm schedules node processing immediately generation (line 8
Algorithm 5). special treatment generateNodeFP function ensures
supersets already found hitting sets added supersets newly found
hitting set removed thread-safe manner (lines 13 16 Algorithm 6). Due
change generateNodeFP, analysis soundness completeness
done two distinct cases.
Theorem 3.2. Full Parallelization sound complete, applied find diagnoses
cardinality.
Proof. FP stops either (i) hitting set exists, i.e., leaf nodes tree
labeled either 3 7, (ii) predefined cardinality (tree-depth) reached.
latter case, every leaf node tree labeled either 3, 7, minimal conflict
847

fiJannach, Schmitz, & Shchekotykhin

set. Case (ii) reduced (i) removing branches tree labeled
minimal conflict. branches irrelevant since contribute
minimal hitting sets higher cardinality. Therefore, without loss generality, limit
discussion case (i).
According definition generateNodeFP, tree built using pruning
rule done sequential HS-tree algorithm. consequence, tree generated
FP must comprise least nodes tree generated sequential HStree procedure. Therefore, according Theorem 4.8 work Reiter (1987)
tree generated FP must comprise set leaf nodes labeled 3
set tHpnq|n node labeled 3u corresponds set H minimal hitting
sets. Moreover, result returned FP comprises minimal hitting sets,
generateNodeFP removes hitting sets H supersets hitting sets.
Consequently, FP sound complete, applied find diagnoses.
Theorem 3.3. Full Parallelization cannot guarantee completeness soundness
applied find first k diagnoses, i.e. 1 k N , N total number
diagnoses problem.
Proof. proof done constructing example FP returns least
one non-minimal hitting set set , thus violating Definition 2.2. instance,
situation might occur FP applied find one single diagnosis example problem
presented Section 2.2.1. Let us assume generation node corresponding
path C2 delayed, e.g., operating system scheduled another thread
execution first, node 4 correspondingly generated first. case, algorithm
would return non-minimal hitting set tC1, C2u diagnosis.
Note elements set returned FP case turned
diagnoses applying minimization algorithm like Inv-QuickXplain (Shchekotykhin,
Friedrich, Rodler, & Fleiss, 2014), algorithm adopts principles QuickXplain
applies divide-and-conquer strategy find one minimal diagnosis given set
inconsistent constraints.
Given hitting set H diagnosis problem, algorithm capable computing
minimal hitting set H 1 H requiring Op|H 1 |`|H 1 | logp|H|{|H 1 |qqq calls theorem
prover TP. first part, |H 1 |, reflects computational costs determining whether
H 1 minimal. second part represents number subproblems must
considered divide-and-conquer algorithm order find minimal hitting set H 1 .
3.5 Evaluation
determine performance improvements achieved various forms
parallelization proposed paper, conducted series experiments diagnosis
problems number different application domains. Specifically, used electronic
circuit benchmarks DX Competition 2011 Synthetic Track, faulty descriptions
Constraint Satisfaction Problems (CSPs), well problems domain ontology
debugging. addition, ran experiments synthetically created diagnosis problems
analyze impact varying different problem characteristics. diagnosis algorithms
848

fiParallel Model-Based Diagnosis Multi-Core Computers

evaluated paper implemented Java unless noted otherwise. Generally,
use wall clock times performance measure.
main part paper, focus results DX Competition
problems widely used benchmark. results problem
setups presented discussed appendix paper. cases,
results DX Competition problems follow similar trend achieved
experiments.
section compare HS-tree parallelization schemes LWP FP
sequential version algorithm, goal find diagnoses.
3.5.1 Dataset Procedure
set experiments, selected first five systems DX Competition 2011
Synthetic Track (see Table 1) (Kurtoglu & Feldman, 2011). system, competition specifies 20 scenarios injected faults resulting different faulty output values.
used system description given input output values diagnosis
process. additional information injected faults course ignored.
problems converted Constraint Satisfaction Problems. experiments used
Choco (Prudhomme, Fages, & Lorca, 2015) constraint solver QXP conflict
detection, returns one minimal conflict called node construction.
computation times required conflict identification strongly depend
order possibly faulty constraints, shuffled constraints test repeated
tests 100 times. report wall clock times actual diagnosis task; times
required input output independent HS-tree construction scheme
relevant benchmarks. parallel approaches, used thread pool size
four.7
Table 1 shows characteristics systems terms number constraints
(#C) problem variables (#V).8 numbers injected faults (#F)
numbers calculated diagnoses (#D) vary strongly different scenarios
system. columns show ranges values scenarios.
columns #D |D| indicate average number diagnoses average cardinality.
seen, search tree diagnosis become extremely broad
6,944 diagnoses average diagnosis size 3.38 system c432.
3.5.2 Results
Table 2 shows averaged results searching minimal diagnoses. first list
running times milliseconds sequential version (Seq.) improvements
LWP FP terms speedup efficiency respect sequential version.
Speedup Sp computed Sp T1 {Tp , T1 wall time using 1 thread (the
sequential algorithm) Tp wall time p parallel threads used. speedup
7. four hardware threads reasonable assumption standard desktop computers also mobile
devices. hardware used evaluation chapter laptop Intel i7-3632QM
CPU, 16GB RAM, running Windows 8 also four cores hyperthreading. results
evaluation server hardware 12 cores reported later Section.
8. systems marked *, search depth limited actual number faults ensure
sequential algorithm terminates within reasonable time frame.

849

fiJannach, Schmitz, & Shchekotykhin

System
74182
74L85
74283*
74181*
c432*

#C
21
35
38
67
162

#V
28
44
45
79
196

#F
4-5
1-3
2-4
3-6
2-5

#D
30 - 300
1 - 215
180 - 4,991
10 - 3,828
1 - 6,944

#D
139.0
66.4
1,232.7
877.8
1,069.3

|D|
4.66
3.13
4.42
4.53
3.38

Table 1: Characteristics selected DXC benchmarks.
2 would therefore mean needed computation times halved; speedup 4,
theoretical optimum using 4 threads, means time reduced
one quarter. efficiency Ep defined Sp {p compares speedup
theoretical optimum. fastest algorithm system highlighted bold.
System
74182
74L85
74283*
74181*
c432*

Seq.(QXP)
[ms]
65
209
371
21,695
85,024

LWP(QXP)
S4
E4
2.23
0.56
2.55
0.64
2.53
0.63
1.22
0.31
1.47
0.37

FP(QXP)
S4
E4
2.28 0.57
2.77 0.69
2.66 0.67
3.19 0.80
3.75 0.94

Table 2: Observed performance gains DXC benchmarks searching diagnoses.

tests, parallelization approaches outperform sequential algorithm. Furthermore, differences sequential algorithm one parallel approaches statistically significant (p 0.05) 95 100 tested scenarios.
systems, FP efficient LWP speedups range 2.28 3.75
(i.e., reduction running times 70%). 59 100 scenarios
differences LWP FP statistically significant. trend
observed efficiency FP higher complex problems. reason
problems time needed node generation much larger absolute
numbers additional overhead times required thread synchronization.
3.5.3 Adding Threads
use cases diagnosis process done powerful server architectures
often even CPU cores modern desktop computers. order assess
extent 4 threads help speed diagnosis process, tested
different benchmarks server machine 12 CPU cores. test compared
FP 4, 8, 10, 12 threads sequential algorithm.
results DXC benchmark problems shown Table 3. tested systems
diagnosis process faster using 8 instead 4 threads substantial speedups
5.20 could achieved compared sequential diagnosis, corresponds
850

fiParallel Model-Based Diagnosis Multi-Core Computers

runtime reduction 81%. one system, utilization 10 threads led
additional speedups. Using 12 threads fastest 3 5 tested systems.
efficiency, however, degrades threads used, time needed
synchronization threads. Using threads hardware actually cores
result additional speedups tested systems. reason
time threads busy conflict detection, e.g., finding solutions CSPs,
use almost 100% processing power assigned them.
System
74182
74L85
74283
74181*
c432*

Seq.(QXP)
[ms]
58
184
51,314
13,847
43,916

S4
2.09
2.53
3.04
3.45
3.43

E4
0.52
0.63
0.76
0.86
0.86

S8
2.43
3.29
4.38
5.20
4.77

FP(QXP)
E8
S10
E10
0.30 2.52 0.25
0.41 3.35 0.34
0.55 4.42 0.44
0.65 5.11 0.51
0.60 5.00 0.50

S12
2.54
3.38
4.50
5.19
4.74

E12
0.21
0.28
0.37
0.43
0.39

Table 3: Observed performance gains DXC benchmarks server 12 hardware
threads.

3.5.4 Additional Experiments
details additional experiments conducted compare proposed parallelization schemes sequential HS-Tree algorithm presented Section A.1
appendix. results show significant speedups also achieved Constraint Satisfaction Problems (Section A.1.1) ontologies (Section A.1.2). appendix
furthermore contains analysis effects adding threads benchmarks
CSPs ontologies (Section A.1.3) presents results simulation experiment
systematically varied different problem characteristics (Section A.1.4).
3.5.5 Discussion
Overall, results evaluations show parallelization approaches help improve performance diagnosis process, tested scenarios approaches
achieved speedups. cases FP faster LWP. However, depending
specifics given problem setting, using LWP advantageous situations,
e.g., time needed generate node small conflict generation time vary strongly. cases synchronization overhead needed
FP higher cost waiting threads finish. tested ontologies
Section A.1.2, case four tested scenarios.
Although FP average faster LWP significantly better sequential
HS-tree construction approach, tested scenarios efficiency still far
optimum 1. explained different effects. example, effect
false sharing happen memory two threads allocated block
(Bolosky & Scott, 1993). every access memory block synchronized although
two threads really share memory. Another possible effect called cache
851

fiJannach, Schmitz, & Shchekotykhin

contention (Chandra, Guo, Kim, & Solihin, 2005). threads work different computing
cores share memory, cache misses occur often depending
problem characteristics thus theoretical optimum cannot reached cases.

4. Parallel HS-Tree Construction Multiple Conflicts Per Node
sequential parallel version HS-tree algorithm, Theorem
Prover TP call corresponds invocation QXP. Whenever new node HS-tree
created, QXP searches exactly one new conflict case none already known
conflicts reused. strategy advantage call TP immediately
returns one conflict determined. turn means parallel
execution threads immediately see new conflict shared data structures
can, best case, reuse constructing new nodes.
disadvantage computing one conflict time QXP search
conflicts restarted invocation. recently proposed new conflict detection
technique called MergeXplain (MXP) (Shchekotykhin et al., 2015), capable
computing multiple conflicts one call. general idea MXP continue search
identification first conflict look additional conflicts remaining
constraints (or logical sentences) divide-and-conquer approach.
combined sequential HS-tree algorithm, effect tree construction time initially spent conflict detection construction continues
next node. exchange, chances conflict available reuse increase
next nodes. time, identification conflicts less timeintensive smaller sets constraints investigated due divide-and-conquer
approach MXP. experimental evaluation various benchmark problems shows
substantial performance improvements possible sequential HS-tree scenario
goal find leading diagnoses (Shchekotykhin et al., 2015).
section, explore benefits using MXP parallel HS-tree construction schemes proposed previous section. using MXP combination
multiple threads, implicit effect CPU processing power devoted conflict generation individual threads need time complete construction
new node. contrast sequential version, threads continue
work parallel.
next section, briefly review MXP algorithm report
results empirical evaluation benchmark datasets (Section 4.2).
4.1 Background QuickXplain MergeXplain
Algorithm 7 shows QXP conflict detection technique Junker (2004) applied
problem finding conflict diagnosis problem HS-tree construction.
QXP operates two sets constraints9 modified recursive calls.
background theory B comprises constraints considered anymore
part conflict current stage. beginning, set contains SD, Obs,
9. use term constraints original formulation. QXP independent
underlying reasoning technique, elements sets could general logical sentences well.

852

fiParallel Model-Based Diagnosis Multi-Core Computers

Algorithm 7: QuickXplain (QXP)
Input: diagnosis problem (SD, Comps, Obs), set visitedNodes elements
Output: set containing one minimal conflict CS C
1 B SD Obs {ab(c)|c P visitedNodes}; C abpcq|c P CompszvisitedNodesu;
2 isConsistent(B C) return conflict;
3 else C H return H;
4 return tc| abpcq P getConflictpB, B, Cqu;
5
6
7
8
9
10

function getConflict (B, D, C)
H ^ isConsistent(B) return H;
|C| 1 return C;
Split C disjoint, non-empty sets C1 C2
D2 getConflict (B C1 , C1 , C2 )
D1 getConflict (B D2 , D2 , C1 )
return D1 D2 ;

set nodes path current node HS-tree (visited nodes).
set C represents set constraints search conflict.
conflict C empty, algorithm immediately returns. Otherwise getConflict called, corresponds Junkers QXP method minor difference
getConflict require strict partial order set constraints C.
introduce variant QXP since cannot always assume prior fault information
available would allow us generate order.
rough idea QXP relax input set faulty constraints C partitioning
two sets C1 C2 . C1 conflict, algorithm continues partitioning C1
next recursive call. Otherwise, i.e., last partitioning split conflicts C
conflicts left C1 , algorithm extracts conflict sets C1
C2 . way, QXP finally identifies individual constraints inconsistent
remaining consistent set constraints background theory.
MXP builds ideas QXP computes multiple conflicts one call (if
exist). general procedure shown Algorithm 8. initial consistency checks,
method findConflicts called, returns tuple xC 1 , y, C 1 set
remaining consistent constraints set found conflicts. function recursively
splits set C constraints two halves. parts individually checked
consistency, allows us exclude larger consistent subsets C search process.
Besides potentially identified conflicts, calls findConflicts also return two sets
constraints consistent (C11 C21 q. union two sets consistent,
look conflict within C11 C11 (and background theory) style QXP.
details found earlier work, also results in-depth
experimental analysis reported (Shchekotykhin et al., 2015).
853

fiJannach, Schmitz, & Shchekotykhin

Algorithm 8: MergeXplain (MXP)
Input: diagnosis problem (SD, Comps, Obs), set visitedNodes elements
Output: , set minimal conflicts
1 B SD Obs {ab(c)|c P visitedNodes}; C abpcq|c P Compszu;
2
isConsistentpBq return solution;
3 isConsistentpB Cq return H;
4 x , findConflictspB, Cq
5 return tc| abpcq P };
6
7
8
9
10
11
12
13
14
15
16
17

function findConflicts (B, C) returns tuple xC 1 ,
isConsistent(B C) return xC, Hy;
|C| 1 return xH, tCuy;
Split C disjoint, non-empty sets C1 C2
xC11 , 1 findConflictspB, C1 q
xC21 , 2 findConflictspB, C2 q
1 2 ;
isConsistentpC11 C21 Bq
X getConflictpB C21 , C21 , C11 q
CS X getConflictpB X, X, C21 q
C11 C11 z tu P X
tCS u
return xC11 C21 , y;

4.2 Evaluation
section evaluate effects parallelizing diagnosis process use
MXP instead QXP calculate conflicts. (Shchekotykhin et al., 2015)
focus finding limited set (five) minimal diagnoses.
4.2.1 Implementation Variants
Using MXP parallel tree construction implicitly means time allocated
conflict generation using QXP proceeding next node.
analyze extent use MXP beneficial tested three different strategies
using MXP within full parallelization method FP.
Strategy (1): configuration simply called MXP instead QXP node
generation. Whenever MXP finds conflict, added global list known conflicts
(re-)used parallel threads. thread executes MXP node
generation continues next node MXP returns.
Strategy (2): strategy implements variant MXP slightly complex.
MXP finds first conflict, method immediately returns conflict
calling thread continue exploring additional nodes. time, new background thread started continues search additional conflicts, i.e., completes
work MXP call. addition, whenever MXP finds new conflict checks
already running node generation thread could reused conflict
854

fiParallel Model-Based Diagnosis Multi-Core Computers

available beforehand. case, search conflicts thread
stopped new conflict needed anymore. Strategy (2) could theory result
better CPU utilization, wait MXP call finish
continue building HS-tree. However, strategy also leads higher synchronization
costs threads, e.g., notify working threads newly identified conflicts.
Strategy (3): Finally, parallelized conflict detection procedure itself. Whenever
set C constraints split two parts, first recursive call findConflicts
queued execution thread pool second call executed current thread.
calls finished, algorithm continues.
experimentally evaluated three configurations benchmark datasets.
results showed Strategy (2) lead measurable performance improvements
compared Strategy (1). additional communication costs seem higher
saved executing conflict detection process background
thread. Strategy (3) applied combination strategies, similar
experiments reported sequential HS-tree construction (Shchekotykhin et al.,
2015), additional performance gains could observed due higher synchronization
costs. limited effectiveness Strategies (2) (3) principle caused
nature benchmark problems strategies might advantageous
different problem settings. following, therefore report results
applying Strategy (1).
4.2.2 Results DXC Benchmark Problems
results DXC benchmarks shown Table 4. left side table shows
results using QXP right hand side shows results MXP.
speedups shown FP columns refer respective sequential algorithms using
conflict detection technique.
Using MXP instead QXP favorable using sequential HS-tree algorithm
also reported work MXP (Shchekotykhin et al., 2015). reduction
running times ranges 17% 44%. speedups obtained FP using
MXP comparable FP using QXP range 1.33 2.10, i.e., lead
reduction running times 52%. speedups achieved addition
speedups sequential algorithm using MXP could already achieve QXP.
best results printed bold face Table 4 using MXP combination
FP consistently performs best. Overall, using FP combination MXP
38% 76% faster sequential algorithm using QXP. tests indicate
parallelization method works well also conflict detection techniques
complex QXP and, case, return one conflict call.
addition, investing time conflict detection situations goal find
leading diagnoses proves promising strategy.
4.2.3 Additional Experiments Discussion
ran additional experiments constraint problems ontology debugging problems. detailed results provided Section A.2.
855

fiJannach, Schmitz, & Shchekotykhin

System
74182
74L85
74283
74181
c432

Seq.(QXP)
[ms]
12
15
49
699
3,714

FP(QXP)
S4
E4
1.26 0.32
1.36 0.34
1.58 0.39
1.99 0.55
1.77 0.44

Seq.(MXP)
[ms]
10
12
35
394
2,888

FP(MXP)
S4
E4
1.52 0.38
1.33 0.33
1.48 0.37
2.10 0.53
1.72 0.43

Table 4: Observed performance gains DXC benchmarks (QXP vs MXP).

Overall, results obtained embedding MXP sequential algorithm confirm
results Shchekotykhin et al. (2015) using MXP favorable QXP
small problem instances. However, also observe allocating
time conflict detection MXP parallel processing setup help
speedup diagnosis process search number leading diagnoses. bestperforming configuration across experiments using Full Parallelization method
combination MXP setup led shortest computation times 20
25 tested scenarios (DX benchmarks, CSPs, ontologies).

5. Parallelized Depth-First Hybrid Search
application domains MBD, finding minimal diagnoses either required
simply possible computational complexity application-specific constraints allowed response times. settings, number algorithms
proposed years, example try find one minimal diagnoses
quickly find diagnoses certain cardinality (Metodi, Stern, Kalech, & Codish, 2014;
Feldman, Provan, & van Gemund, 2010b; de Kleer, 2011). cases, algorithms
principle extended used find diagnoses. are, however, optimized
task.
Instead analyzing various heuristic, stochastic approximative algorithms proposed literature individually respect potential parallelization,
analyze next section parallelization helpful already simple class
depth-first algorithms. context, also investigate measurable improvements
achieved without using (domain-specific) heuristic. Finally, propose
hybrid strategy combines depth-first full-parallel HS-tree construction
conduct additional experiments assess strategy advantageous task
quickly finding one minimal diagnosis.
5.1 Parallel Random Depth-First Search
section introduces parallelized depth-first search algorithm quickly find one single
diagnosis. different threads explore tree partially randomized form, call
scheme Parallel Random Depth-First Search (PRDFS).
856

fiParallel Model-Based Diagnosis Multi-Core Computers

5.1.1 Algorithm Description
Algorithm 9 shows main program recursive implementation PRDFS. Similar
HS-tree algorithm, search diagnoses guided conflicts. time, however,
algorithm greedily searches depth-first manner. diagnosis found,
checked minimality diagnosis contain redundant elements.
minimization non-minimal diagnosis achieved calling method like
Inv-QuickXplain (Shchekotykhin et al., 2014) simply trying remove one element
diagnosis checking resulting set still diagnosis.
Algorithm 9: diagnosePRDFS: Parallelized random depth-first search.
Input: diagnosis problem (SD, Comps, Obs),
number minDiags diagnoses find
Result: set diagnoses
1
2
3
4
5
6
7
8

H; conflicts H;
rootNode = getRootNode(SD, Comps, Obs);
1 nbThreads
threads.execute(expandPRDFS(rootNode, minDiags, , conflicts));
|| minDiags
wait();
threads.shutdownNow();
return ;

idea parallelization approach algorithm start multiple threads
root node. threads perform depth-first search parallel, pick
next conflict element explore randomized manner.
logic expanding node shown Algorithm 10. First, conflict
given node copied, changes set constraints affect
threads. Then, long enough diagnoses found, randomly chosen constraint
current nodes conflict used generate new node. expansion function
immediately called recursively new node, thereby implementing depth-first
strategy. identified diagnosis minimized added list known
diagnoses. Similar previous parallelization schemes, access global lists
known conflicts made thread-safe. specified number diagnoses
found threads finished, statement threads.shutdownNow() immediately stops
execution threads still running results returned. semantics
threads.execute(), wait(), notify() Section 3.
5.1.2 Example
Let us apply depth-first method example Section 2.2.1. Remember
two conflicts problem ttC1, C2, C3u, tC2, C4uu. partially expanded tree
problem seen Figure 2.
857

fiJannach, Schmitz, & Shchekotykhin

Algorithm 10: expandPRDFS: Parallel random depth-first node expansion.
Input: existingNode expand, number minDiags diagnoses find,
sets conflicts
1
2
3
4
5
6
7
8
9
10
11
12

13
14
15
16
17
18

C = existingNode.conflict.clone();
// Copy existingNodes conflict
|| minDiags ^|C| 0
Randomly pick constraint c C
C Cztcu;
newPathLabel = existingNode.pathLabel {c};
node = new Node(newPathlabel);
P conflicts : X newPathLabel H
node.conflict = S;
else
node.conflict = checkConsistency(SD, Comps, Obs, node.pathLabel);
node.conflict H
// New conflict found
conflicts = conflicts node.conflict;
// Recursive call implements depth-first search strategy
expandPRDFS(node, minDiags, , conflicts);
else
// Diagnosis found
diagnosis = minimize(node.pathLabel);
{diagnosis};
|| minDiags
notify();

example, first root node 1 created conflict tC1, C2, C3u
found. Next, random expansion would, example, pick conflict element C1
generate node 2 . node, conflict tC2, C4u computed tC1u
alone diagnosis. Since algorithm continues depth-first manner,
pick one label elements node 2 , e.g., C2 generate node 3 .
node, consistency check succeeds, conflict computed algorithm
found diagnosis. found diagnosis tC1, C2u is, however, minimal contains
redundant element C1. function Minimize, called end Algorithm
10, therefore remove redundant element obtain correct diagnosis tC2u.
used one thread example, one parallel threads would
probably started expanding root node using conflict element C2 (node 4 ).
case, single element diagnosis tC2u would identified already first
level. Adding parallel threads therefore help increase chances find one
hitting set faster different parts HS-tree explored parallel.
Instead random selection strategy, elaborate schemes pick next nodes
possible, e.g., based application-specific heuristics fault probabilities. One could
also better synchronize search efforts different threads avoid duplicate calculations. conducted experiments algorithm variant used shared
858

fiParallel Model-Based Diagnosis Multi-Core Computers

1
{C1,C2, C3}
C1

2

4

C2

C3

{C2,C4}
3

C2

{C2,C4}

C4

C2

C4

Figure 2: Example HS-tree construction PRDFS.
synchronized list open nodes avoid two threads generate identical sub-tree
parallel. did, however, observe significantly better results method
shown Algorithm 9 probably due synchronization overhead.
5.1.3 Discussion Soundness Completeness
Every single thread depth-first algorithm systematically explores full search space
based conflicts returned Theorem Prover. Therefore, existing diagnoses
found parameter minDiags equal higher number actually
existing diagnoses.
Whenever (potentially non-minimal) diagnosis encountered, minimization process ensures minimal diagnoses stored list diagnoses. duplicate
addition diagnosis one threads last lines algorithm
prevented consider diagnoses equal contain set elements
set definition cannot contain element twice.
Overall, algorithm designed find one diagnoses quickly. computation minimal diagnoses possible algorithm highly inefficient, e.g., due
computational costs minimizing diagnoses.
5.2 Hybrid Strategy
Let us consider problem finding one minimal diagnosis. One easily imagine
choice best parallelization strategy, i.e., breadth-first depth-first,
depend specifics given problem setting actual size existing
diagnoses. single-element diagnosis exists, exploring first level HS-tree
breadth-first approach might best choice (see Figure 3(a)). depth-first strategy
might eventually include element non-minimal diagnosis, would
number additional calculations ensure minimality diagnosis.
If, contrast, smallest actually existing diagnosis cardinality of, e.g., five,
breadth-first scheme would fully explore first four HS-tree levels
finding five-element diagnosis. depth-first scheme, contrast, might quickly find
859

fiDiagnosis detected

Jannach, Schmitz, & Shchekotykhin

superset five-element diagnosis, e.g., six elements, needs six
additional consistency checks remove redundant element diagnosis (Figure
3(b)).
Diagnosis detected

Diagnosis detected

(a) Breadth-first strategy advantageous.

(b) Depth-first strategy advantageous.

Figure 3: Two problem configurations different search strategies favorable.
Since cannot know cardinality diagnoses advance, propose hybrid
strategy, half threads adopt depth-first strategy half uses
fully parallelized breadth-first regime. implement strategy, Algorithms 5
(FP) 9 (PRDFS) started parallel algorithm allowed use one
half defined share available threads. coordination
two algorithms done help shared data structures contain known
conflicts diagnoses. enough diagnoses (e.g. one) found, running threads
terminated
results returned.
Diagnosis detected
5.3 Evaluation
evaluated different strategies efficiently finding one minimal diagnosis
set benchmark problems used previous sections. experiment
setup identical except goal find one arbitrary diagnosis
included additional depth-first algorithms. order measure potential benefits
parallelizing depth-first search, ran benchmarks PRDFS 4 threads
1 thread, latter setup corresponds Random Depth First Search
(RDFS) without parallelization.
5.3.1 Results DXC Benchmark Problems
results DXC benchmark problems shown Table 5. Overall, tested
systems, approaches proposed paper help speed process
finding one single diagnosis. 88 100 evaluated scenarios least one
tested approaches statistically significantly faster sequential algorithm.
12 scenarios, finding one single diagnosis simple modest
significant speedups compared sequential algorithm obtained.
comparing individual parallel algorithms, following observations
made:
860

fiParallel Model-Based Diagnosis Multi-Core Computers

examples, PRDFS method faster breadth-first search
implemented FP technique. one benchmark system, PRDFS approach
even achieve speedup 11 compared sequential algorithm, corresponds runtime reduction 91%.
compared non-parallel RDFS, PRDFS could achieve higher speedups
tested systems except simple one, took 16 ms even
sequential algorithm. Overall, parallelization therefore advantageous also
depth-first strategies.
performance Hybrid strategy lies performances components PRDFS FP 4 5 tested systems. systems, closer
faster one two. Adopting hybrid strategy therefore represent good
choice structure problem known advance, combines
ideas breadth-first depth-first search able quickly find diagnosis
problem settings unknown characteristics.

System
74182
74L85
74283
74181
c432

Seq.
[ms]
16
13
54
691
2,789

FP
S4
E4
1.37 0.34
1.34 0.33
1.67 0.42
2.08 0.52
1.89 0.47

RDFS
[ms]
9
11
25
74
1,435

PRDFS
S4
E4
0.84 0.21
1.06 0.27
1.22 0.31
1.23 0.31
2.96 0.74

Hybrid
S4
E4
0.84 0.21
1.05 0.26
1.06 0.26
1.04 0.26
1.81 0.45

Table 5: Observed performance gains DXC benchmarks finding one diagnosis.

5.3.2 Additional Experiments
detailed results obtained additional experiments provided
appendix. measurements include results CSPs (Section A.3.1) ontologies
(Section A.3.2), well results obtained systematically varying characteristics synthetic diagnosis problems (Section A.3.3). results indicate applying
depth-first parallelization strategy many cases advantageous CSP problems.
tests ontology problems simulation results however reveal depending problem structure cases breadth-first strategy
beneficial.
5.3.3 Discussion
experiments show parallelization depth-first search strategy (PRDFS)
help reduce computation times search one single diagnosis.
evaluated cases, PRDFS faster sequential counterpart.
cases, however, obtained improvements quite small virtually non-existent,
explained follows.
861

fiJannach, Schmitz, & Shchekotykhin

small scenarios, parallel depth-first search cannot significantly
faster non-parallel variant creation first node parallelized. Therefore major fraction tree construction process parallelized
all.
problem settings existing diagnoses size.
parallel depth-first searching threads therefore explore tree certain
depth none threads immediately return diagnosis much smaller
one determined another thread. E.g., given diagnosis problem,
diagnoses size 5, threads explore tree least level 5 find
diagnosis also likely find diagnosis level. Therefore,
setting thread much faster others.
Finally, suspect problems cache contention correspondingly increased number cache misses, leads general performance deterioration
overhead caused multiple threads.
Overall, obtained speedups depend problem structure. hybrid
technique represents good compromise cases faster sequential
breadth first search approach tested scenarios (including CSPs, ontologies, synthetically created diagnosis problems presented Section A.3). Also,
efficient PRDFS cases breadth first search better
depth first search.

6. Parallel Direct CSP Encodings
alternative conflict-guided diagnosis approaches like Reiters hitting set technique,
so-called direct encodings become popular research community recent
years (Feldman, Provan, de Kleer, Robert, & van Gemund, 2010a; Stern, Kalech, Feldman,
& Provan, 2012; Metodi et al., 2014; Mencia & Marques-Silva, 2014; Menca, Previti, &
Marques-Silva, 2015; Marques-Silva, Janota, Ignatiev, & Morgado, 2015).10
general idea direct encodings generate specific representation diagnosis
problem instance knowledge representation language use theorem
prover (e.g., SAT solver constraint engine) compute diagnoses directly.
methods support generation one multiple diagnoses calling theorem prover
once. Nica, Pill, Quaritsch, Wotawa (2013) made number experiments
compared conflict-directed search direct encodings showed
several problem settings, using direct encoding advantageous.
part paper, goal evaluate whether parallelization search
process case inside constraint engine help improve efficiency
diagnostic reasoning process. goal chapter therefore rather quantify
extent internal parallelization solver useful present new
algorithmic contribution.
10. direct encodings may always possible MBD settings discussed above.

862

fiParallel Model-Based Diagnosis Multi-Core Computers

6.1 Using Gecode Solver Direct Encodings
evaluation use Gecode constraint solver (Schulte, Lagerkvist, & Tack, 2016).
particular, use parallelization option Gecode test effects diagnosis
running times.11 chosen problem encoding similar one used Nica
Wotawa (2012). allows us make results comparable obtained
previous works. addition, provided encoding represented language
supported multiple solvers.
6.1.1 Example
Let us first show general idea small example. Consider following CSP12
consisting integer variables a1, a2, b1, b2, c1 constraints X1 , X2 , X3
defined as:
X1 : b1 a1 2, X2 : b2 a2 3, X3 : c1 b1 b2.
Let us assume programmer made mistake X3 actually c1
b1 ` b2. Given set expected observations (a test case) a1 1, a2 6, d1 20, MBD
applied considering constraints possibly faulty components.
direct encoding given CSP extended definition array AB
rab1 , ab2 , ab3 boolean (0/1) variables encode whether corresponding constraint
considered faulty not. constraints rewritten follows:
X11 : ab1 _ pb1 a1 2q,

X21 : ab2 _ pb2 a2 3q,

X31 : ab3 _ pc1 b1 b2q.

observations encoded equality constraints bind values
observed variables. example, constraints would be:
O1 : a1 1,

O2 : a2 6,

O3 : d1 20

order find diagnosis cardinality 1, additionally add constraint
ab1 ` ab2 ` ab3 1
let solver search solution. case, X3 would identified
possible diagnosis, i.e., ab3 would set 1 solver.
6.1.2 Parallelization Approach Gecode
using direct encoding, parallelization diagnosis process, shown
Reiters approach, cannot done embedded underlying search
procedure. However, modern constraint solvers, Gecode, or-tools many
solvers participated MiniZinc Challenge (Stuckey, Feydy, Schutt, Tack,
& Fischer, 2014), internally implement parallelization strategies better utilize todays
multi-core computer architectures (Michel, See, & Van Hentenryck, 2007; Chu, Schulte, &
11. state-of-the-art SAT solver capable parallelization could used analysis well.
12. Adapted earlier work (Jannach & Schmitz, 2014).

863

fiJannach, Schmitz, & Shchekotykhin

Stuckey, 2009). following, therefore evaluate set experiments,
solver-internal parallelization techniques help speed diagnosis process
direct encoding used.13
Gecode implements adaptive work stealing strategy (Chu et al., 2009) parallelization. general idea summarized follows. soon thread finishes
processing nodes search tree, steals nodes non-idle threads.
order decide thread work stolen, adaptive strategy uses
balancing heuristics estimate density solutions particular part
search tree. higher likelihood containing solution given branch,
work stolen branch.
6.2 Problem Encoding
evaluation use MiniZinc constraint modeling language. language
processed different solvers allows us model diagnosis problems CSPs
shown above.
6.2.1 Finding One Diagnosis
find single diagnosis given diagnosis problem (SD, Comps, Obs), generate
direct encoding MiniZinc follows.
(1) set components Comps generate array ab = [ab1 , . . . , abn ]
boolean variables.
(2) formula sdi P SD add constraint form
constraint abris _ psdi q;
observation oj P Obs model extended constraint
constraint oj ;
(3) Finally, add search goal output statement:
solve minimize sumpi 1..nqpbool2intpabrisqq;
output[show(ab)];
first statement last part (solve minimize), instructs solver search
(single) solution minimal number abnormal components, i.e., diagnosis
minimum cardinality. second statement (output) projects assignments set
abnormal variables, interested knowing components
faulty. assignments problem variables irrelevant.
6.2.2 Finding Diagnoses
problem encoding shown used quickly find one/all diagnoses minimum cardinality. is, however, sufficient scenarios goal find
diagnoses problem. therefore propose following sound complete algorithm
repeatedly modifies constraint problem systematically identify diagnoses.
13. contrast parallelization approaches presented previous sections, propose
new parallelization schemes rather rely existing ones implemented solver.

864

fiParallel Model-Based Diagnosis Multi-Core Computers

Technically, algorithm first searches diagnoses size 1 increases
desired cardinality diagnoses step step.

Algorithm 11: directDiag: Computation diagnoses using direct encoding.
Input: diagnosis problem (SD, Comps, Obs), maximum cardinality k
Result: set diagnoses
1
2
3
4
5
6
7
8
9
10

H; C H; card 1;
k |Comps| k |Comps|;
= generateModel (SD, Comps, Obs);
card k
= updateModel (M, card , C);
1 computeDiagnosespMq;
C C generateConstraintsp1 q;
1 ;
card card ` 1;
return ;

Procedure Algorithm 10 shows main components direct diagnosis method used
connection parallel constraint solver find diagnoses. algorithm starts
generation MiniZinc model (generateModel) described above.
difference search solutions given cardinality;
details encoding search goals given below.
iteration, algorithm modifies model updating cardinality
searched diagnoses furthermore adds new constraints corresponding already
found diagnoses (updateModel). updated model provided MiniZinc
interpreter (constraint solver), returns set solutions 1 . element P 1
corresponds diagnosis cardinality card .
order exclude supersets already found diagnoses 1 future iterations,
generate constraint P 1 formulas j l (generateConstraints):
constraint abrjs false _ _ abrls false;
constraints ensure already found diagnosis supersets cannot found
again. added model next iteration main loop. algorithm
continues diagnoses cardinalities k computed.
Changes Encoding calculate diagnoses given size, first instruct
solver search possible solutions provided constraint problem.14
addition, keeping steps (1) (2) Section 6.2.1 replace lines step (3)
14. achieved calling MiniZinc --all-solutions flag.

865

fiJannach, Schmitz, & Shchekotykhin

following statements:
constraint sumpi 1..nqpbool2intpabrisqq card ;
solve satisfy;
output[show(ab)];
first statement constrains number abnormal variables true
certain value, i.e., given cardinality card. second statement tells solver find
variable assignments satisfy constraints. last statement guarantees
solver considers solutions different different
respect assignments abnormal variables.
Soundness Completeness Algorithm 10 implements iterative deepening approach guarantees minimality diagnoses . Specifically, algorithm
constructs diagnoses order increasing cardinality limiting number ab
variables set true model. computation starts card 1,
means one ab variable true. Therefore, diagnoses cardinality 1,
i.e., comprising one abnormal variable, returned solver. found
diagnosis add constraint requires least one abnormal variables
diagnosis false. Therefore, neither diagnosis supersets found
subsequent iterations. constraints implement pruning rule HS-tree
algorithm. Finally, Algorithm 10 repeatedly increases cardinality parameter card
one continues next iteration. algorithm continues increment cardinality card becomes greater number components, corresponds
largest possible cardinality diagnosis. Consequently, given diagnosis problem
well sound complete constraint solver, Algorithm 10 returns diagnoses
problem.
6.3 Evaluation
evaluate speedups achieved parallelization also direct encoding,
used first five systems DXC Synthetic Track tested scenarios
using Gecode solver without parallelization 2 4 parallel threads.
6.3.1 Results
evaluated two different configurations. setup (A), task find one single
diagnosis minimum cardinality. setup (B), iterative deepening procedure
Section 6.2.2 used find diagnoses size actual error.
results setup (A) shown Table 6. observe using parallel
constraint solver pays except tiny problems overall search time
less 200 ms. Furthermore, adding worker threads also beneficial larger
problem sizes speedup 1.25 achieved complex test case
took 1.5 seconds solve.
pattern observed setup (B). detailed results listed Table
7. tiny problems, internal parallelization Gecode solver lead
performance improvements slightly slows whole process. soon
866

fiParallel Model-Based Diagnosis Multi-Core Computers

problems become complex, parallelization pays observe speedup
1.55 complex tested cases, corresponds runtime reduction
35%.
System
74182
74L85
74283
74181
c432

Direct Encoding
Abs. [ms]
S2
E2
S4
27 0.85 0.42 0.79
30 0.89 0.44 0.79
32 0.85 0.43 0.79
200 1.04 0.52 1.15
1,399 1.17 0.58 1.25

E4
0.20
0.20
0.20
0.29
0.31

Table 6: Observed performance gains DXC benchmarks finding one diagnosis
direct encoding using one (column Abs.), two, four threads.

System
74182
74L85
74283
74181
c432

Direct Encoding
Abs. [ms]
S2
E2
S4
136 0.84 0.42 0.80
60 0.83 0.41 0.77
158 0.93 0.47 0.92
1,670 1.19 0.59 1.33
229,869 1.22 0.61 1.55

E4
0.20
0.19
0.23
0.33
0.39

Table 7: Observed performance gains DXC benchmarks finding diagnoses
direct encoding using one (column Abs.), two, four threads.

6.3.2 Summary Remarks
Overall, experiments show parallelization beneficial direct encoding
diagnosis problem employed, particular problems non-trivial.
Comparing absolute running times Java implementation using open source
solver Choco optimized C++ implementation Gecode generally appropriate benchmark problems, Gecode works faster absolute scale.
Note, however, true cases. particular searching diagnoses size actual error complex system c432, even Reiters
non-parallelized Hitting Set algorithm much faster (85 seconds) using direct
encoding based iterative deepening (230 seconds). line observation
Nica et al. (2013) direct encodings always best choice searching
diagnoses.
first analysis run-time behavior Gecode shows larger problem is,
time spent solver iteration reconstruct internal structures,
lead measurable performance degradation. Note work relied
MiniZinc encoding diagnosis problem independent specifics
867

fiJannach, Schmitz, & Shchekotykhin

underlying constraint engine. implementation relies direct use API
specific CSP solver might help address certain performance issues. Nevertheless,
implementation must solver-specific allow us switch solvers easily
possible MiniZinc..

7. Relation Previous Works
section explore works related approach. First examine different
approaches computation diagnoses. focus general methods
parallelizing search algorithms.
7.1 Computation Diagnoses
Computing minimal hitting sets given set conflicts computationally hard problem
already discussed Section 2.2.2 several approaches proposed years
deal issue. approaches divided exhaustive approximate ones.
former perform sound complete search minimal diagnoses, whereas
latter improve computational efficiency exchange completeness, e.g., search
one small set diagnoses.
Approximate approaches example based stochastic search techniques like
genetic algorithms (Li & Yunfei, 2002) greedy stochastic search (Feldman et al., 2010b).
greedy method proposed Feldman et al. (2010b), example, uses two-step
approach. first phase, random possibly non-minimal diagnosis determined
modified DPLL15 algorithm. algorithm always finds one random diagnosis
invocation due random selection propositional variables assignments.
second step, algorithm minimizes diagnosis returned DPLL technique
repeatedly applying random modifications. randomly chooses negative literal
denotes corresponding component faulty flips value positive.
obtained candidate well diagnosis problem provided DPLL algorithm
check whether candidate diagnosis not. case success obtained
diagnosis kept another random flip done. Otherwise, negative literal labeled
failure another negative literal randomly selected. algorithm stops
number failures greater predefined constant returns best diagnosis
found far.
approach Li Yunfei (2002) genetic algorithm takes number conflict
sets input generates set bit-vectors (chromosomes), every bit encodes
truth value atom ab(.) predicate. iteration algorithm applies
genetic operations, mutation, crossover, etc., obtain new chromosomes. Subsequently, obtained bit-vectors evaluated hitting set fitting function
eliminates bad candidates. algorithm stops predefined number iterations
returns best diagnosis.
general, approximate approaches directly comparable LWP
FP techniques, since incomplete guarantee minimality returned
15. Davis-Putnam-Logemann-Loveland.

868

fiParallel Model-Based Diagnosis Multi-Core Computers

hitting sets. goal contrast improve performance time
maintaining completeness soundness property.
Another way finding approximate solutions use heuristic search approaches.
example, Abreu van Gemund (2009) proposed Staccato algorithm applies
number heuristics pruning search space. aggressive pruning techniques
result better performance search algorithms. However, also increase probability diagnoses found. approach aggressiveness
heuristics varied input parameters depending application goals.
recently, Cardoso Abreu (2013) suggested distributed version Staccato algorithm, based Map-Reduce scheme (Dean & Ghemawat, 2008)
therefore executed cluster servers. recent algorithms focus
efficient computation one minimum cardinality (minc) diagnoses (de Kleer,
2011). distributed approach minimum cardinality scenario, assumption (possibly incomplete) set conflicts already available input
beginning hitting-set construction process. application scenarios
address work, finding conflicts considered computationally
expensive part assume know minimal conflicts advance
compute on-demand also done works (Felfernig, Friedrich, Jannach,
Stumptner, et al., 2000; Friedrich & Shchekotykhin, 2005; Williams & Ragno, 2007); see also
work Pill, Quaritsch, Wotawa (2011) comparison conflict computation
approaches.
Exhaustive approaches often based HS-trees like work Wotawa (2001a)
tree construction algorithm reduces number pruning steps presence nonminimal conflicts. Alternatively, one use methods compute diagnoses without
explicit computation conflict sets, i.e., solving problem dual minimal hitting sets
(Satoh & Uno, 2005). Stern et al. (2012), example, suggest method explores
duality conflicts diagnoses uses symmetry guide search.
approaches exploit structure underlying problem, hierarchical (Autio
& Reiter, 1998), tree-structured (Stumptner & Wotawa, 2001), distributed (Wotawa &
Pill, 2013). algorithms similar HS-tree algorithm and, consequently,
parallelized similar way. example, consider Set-Enumeration Tree
(SE-tree) algorithm (Rymon, 1994). algorithm, similarly Reiters HS-tree approach,
uses breadth-first search specific expansion procedure implements pruning
node selection strategies. LWP FP parallelization variant
used SE-tree algorithm comparable speedups expected.
7.2 Parallelization Search Algorithms
Historically, parallelization search algorithms approached three different ways
(Burns, Lemons, Ruml, & Zhou, 2010):
(i) Parallelization node processing: applying type parallelization, tree
expanded one single process, computation labels evaluation
heuristics done parallel.
869

fiJannach, Schmitz, & Shchekotykhin

(ii) Window-based processing: approach, sets nodes, called windows, processed different threads parallel. windows formed search algorithm
according predefined criteria.
(iii) Tree decomposition approaches: Here, different sub-trees search tree assigned different processes (Ferguson & Korf, 1988; Brungger, Marzetta, Fukuda, &
Nievergelt, 1999).
principle, three types parallelization applied form HS-tree
generation problem.
Applying strategy (i) MBD problem setting would mean parallelize process
conflict computation, e.g., parallel variant QXP MXP. tested
partially parallelized version MXP, however lead performance
improvements compared single-threaded approach evaluated benchmark
problems (Shchekotykhin et al., 2015). experiments Section 4 however show
using MXP combination LWP FP thereby implicitly allocating CPU time
computation multiple conflicts construction single node advantageous. well-known conflict prime implicate computation algorithms (Junker,
2004; Marques-Silva et al., 2013; Previti, Ignatiev, Morgado, & Marques-Silva, 2015)
contrast designed parallel execution computation multiple conflicts.
Strategy (ii) computing sets nodes (windows) parallel example applied
Powley Korf (1991). work windows determined different thresholds
heuristic function Iterative Deepening A*. Applying strategy HS-tree
construction problem would mean categorize nodes expanded according
criterion, e.g., probability finding diagnosis, allocate different
groups individual threads. absence window criteria, LWP FP could
seen extreme cases window size one, open node allocated one thread
processor. experiments done throughout paper suggest independent
parallelization strategy (LWP FP) number parallel threads (windows)
exceed number physically available computing threads obtain best performance.
Finally (iii), strategy exploring different sub-trees search different
processes can, example, applied context MBD techniques using Binary
HS-Tree (BHS) algorithms (Pill & Quaritsch, 2012). Given set conflict sets, BHS
method generates root node labels input set conflicts. Then, selects
one components occurring conflicts generates two child nodes,
left node labeled conflicts comprising selected component right
node remaining ones. Consequently, diagnosis tree decomposed two subtrees processed parallel. main problem kind parallelization
conflicts often known advance computed search.
Anglano Portinale (1996) suggested another approach ultimately
parallelized diagnosis problem based structural problem characteristics.
work, first map given diagnosis problem Behavioral Petri Net (BPN). Then,
obtained BPN manually partitioned subnets every subnet provided
different Parallel Virtual Machine (PVM) parallel processing. relationship
work LWP FP parallelization schemes limited approaches also
require manual problem decomposition step.
870

fiParallel Model-Based Diagnosis Multi-Core Computers

general, parallelized versions domain-independent search algorithms like
applied MBD settings. However, MBD problem specifics make
application algorithms difficult. instance, PRA method
variant HDA discussed work Burns et al. (2010) use mechanism minimize
memory requirements retracting parts search tree. forgotten parts
later re-generated required. MBD setting, generation nodes however
costly part, applicability HDA seems limited. Similarly,
duplicate detection algorithms like PBNF (Burns et al., 2010) require existence
abstraction function partitions original search space blocks. general MBD
settings, however cannot assume function given.
order improve performance therefore avoid parallel generation
duplicate nodes different threads, plan investigate future work.
promising starting point research could work Phillips, Likhachev,
Koenig (2014). authors suggest variant A* algorithm generates
independent nodes order reduce costs node generation. Two nodes considered
independent generation one node lead change heuristic
function node. generation independent nodes done parallel
without risk repeated generation already known state. main difficulty
adopting algorithm MBD formulation admissible heuristic required
evaluate independence nodes arbitrary diagnosis problems. However,
specific problems encoded CSPs, Williams Ragno (2007) present
heuristic depends number unassigned variables particular search node.
Finally, parallelization also used literature speed processing
large search trees fit memory. Korf Schultze (2005), instance, suggest
extension hash-based delayed duplicate detection algorithm allows search
algorithm continue search parts search tree written read
hard drive. methods theory used combination LWP FP
parallelization schemes case complex diagnosis problems. plan explore use
(externally) saved search states context MBD part future works.

8. Summary
work, propose systematically evaluate various parallelization strategies
Model-Based Diagnosis better exploit capabilities multi-core computers. show
parallelization advantageous various problem settings diagnosis approaches. approaches include conflict-driven search minimal
diagnoses different conflict detection techniques (heuristic) depth-first search
order quickly determine single diagnosis. main benefits parallelization
approaches applied independent underlying reasoning engine
variety diagnostic problems cannot efficiently represented SAT CSP
problems. addition HS-tree based parallelization approaches, also show
parallelization beneficial settings direct problem encoding possible
modern parallel solver engines available.
evaluations furthermore shown speedups proposed parallelization methods vary according characteristics underlying diagnosis problem.
871

fiJannach, Schmitz, & Shchekotykhin

future work, plan explore techniques analyze characteristics order
predict advance parallelization method best suited find one single
diagnoses given problem.
Regarding algorithmic enhancements, furthermore plan investigate information underlying problem structure exploited achieve better distribution work parallel threads thereby avoid duplicate computations.
Furthermore, plan explore usage parallel solving schemes dual algorithms, i.e., algorithms compute diagnoses directly without computation minimal conflicts (Satoh & Uno, 2005; Felfernig, Schubert, & Zehentner, 2012; Stern et al.,
2012; Shchekotykhin et al., 2014).
presented algorithms designed use modern multi-core computers
today usually less dozen cores. results show additional performance improvements obtain proposed techniques become smaller
adding CPUs. part future works therefore plan develop
algorithms utilize specialized environments support massive parallelization.
context, future topic research could adaption parallel HS-tree
construction GPU architectures. GPUs, thousands computing cores,
proved superior tasks parallelized suitable way. Campeotto,
Palu, Dovier, Fioretto, Pontelli (2014) example used GPU parallelize constraint solver. However, yet fully clear whether tree construction techniques
efficiently parallelized GPU, many data structures shared across
nodes access synchronized.

Acknowledgements
paper significantly extends combines previous work (Jannach, Schmitz, &
Shchekotykhin, 2015; Shchekotykhin et al., 2015).
would like thank Hakan Kjellerstrand Gecode team support.
also thankful various helpful comments suggestions made anonymous
reviewers JAIR, DX14, DX15, AAAI15, IJCAI15.
work supported Carinthian Science Fund (KWF) contract KWF3520/26767/38701, Austrian Science Fund (FWF) German Research Foundation (DFG) contract numbers 2144 N-15 JA 2095/4-1 (Project Debugging
Spreadsheet Programs).

Appendix A.
appendix report results additional experiments made different
benchmark problems well results simulation experiments artificially created
problem instances.
Section A.1 contains results LWP FP parallelization schemes proposed
Section 3.
Section A.2 reports additional measurements regarding use MergeXplain
within parallel diagnosis process, see Section 4.
872

fiParallel Model-Based Diagnosis Multi-Core Computers

Section A.3 finally provides additional results parallelization depth-first
strategies discussed Section 5.
A.1 Additional Experiments LWP FP Parallelization Strategies
addition experiments DXC benchmark systems reported Section 3.5,
made additional experiments Constraint Satisfaction Problems, ontologies,
artificial Hitting Set construction problems. Furthermore, examined effects
increasing number available threads benchmarks CSPs ontologies.
A.1.1 Diagnosing Constraint Satisfaction Problems
Data Sets Procedure set experiments used number CSP instances
2008 CP solver competition (Lecoutre, Roussel, & van Dongen, 2008)
injected faults.16 diagnosis problems created follows. first generated
random solution using original CSP formulations. solution, randomly
picked 10% variables stored value assignments, served
test cases. stored variable assignments correspond expected outcomes
constraints formulated correctly. Next, manually inserted errors (mutations)
constraint problem formulations17 , e.g., changing less operator
operator, corresponds mutation-based approach software testing.
diagnosis task consists identifying possibly faulty constraints using partial
test cases. addition benchmark CSPs converted number spreadsheet
diagnosis problems (Jannach & Schmitz, 2014) CSPs test performance gains
realistic application settings.
Table 8 shows problem characteristics including number injected faults (#F),
number diagnoses (#D), average diagnosis size (|D|). general, selected
CSPs quite diverse respect size.
Results measurement results using 4 threads searching diagnoses given
Table 9. Improvements could achieved problem instances. exception
smallest problem mknap-1-5 speedups achieved LWP FP statistically
significant. problems, improvements strong (with running time
reduction 50%), whereas others improvements modest. average, FP
also faster LWP. However, FP consistently better LWP often
differences small.
observed results indicate performance gains depend number factors
including size conflicts, computation times conflict detection,
problem structure itself. average FP faster LWP, characteristics
problem settings seem considerable impact speedups obtained
different parallelization strategies.
16. able sufficient number repetitions, picked instances comparably small running
times.
17. mutated CSPs downloaded http://ls13-www.cs.tu-dortmund.de/homepage/hp_
downloads/jair/csps.zip.

873

fiJannach, Schmitz, & Shchekotykhin

Scenario
c8
costasArray-13
domino-100-100
gracefulK3-P2
mknap-1-5
queens-8
hospital payment
profit calculation
course planning
preservation model
revenue calculation

#C
523
87
100
60
7
28
38
28
457
701
93

#V
239
88
100
15
39
8
75
140
583
803
154

#F
8
2
3
4
1
15
4
5
2
1
4

#D
4
2
81
117
2
9
120
42
3024
22
1452

|D|
6.25
2.5
2
2.94
1
10.9
3.8
4.24
2
1
3

Table 8: Characteristics selected problem settings.
Scenario
c8
costasArray-13
domino-100-100
gracefulK3-P2
mknap-1-5
queens-8
hospital payment
profit calculation
course planning
preservation model
revenue calculation

Seq.(QXP)
[ms]
559
4,013
1,386
1,965
314
141
12,660
197
22,130
167
778

LWP(QXP)
S4
E4
1.10 0.27
2.16
0.54
3.08 0.77
2.75
0.69
1.03 0.26
1.57
0.39
1.64
0.41
1.71
0.43
2.58
0.65
1.46
0.37
2.81 0.70

FP(QXP)
S4
E4
1.07 0.27
2.58 0.65
3.05 0.76
2.99 0.75
1.02 0.25
1.65 0.41
1.73 0.43
2.00 0.50
2.61 0.65
1.48 0.37
2.58 0.64

Table 9: Results CSP benchmarks spreadsheets searching diagnoses.
A.1.2 Diagnosing Ontologies
Data Sets Procedure recent works, MBD techniques used locate faults
description logic ontologies (Friedrich & Shchekotykhin, 2005; Shchekotykhin et al., 2012;
Shchekotykhin & Friedrich, 2010), represented Web Ontology Language
(OWL) (Grau, Horrocks, Motik, Parsia, Patel-Schneider, & Sattler, 2008). testing
ontology, developer similarly earlier approach (Felfernig, Friedrich,
Jannach, Stumptner, & Zanker, 2001) specify set positive negative test cases.
test cases sets logical sentences must entailed ontology (positive)
entailed ontology (negative). addition, ontology itself, set
logical sentences, consistent coherent (Baader, Calvanese, McGuinness,
Nardi, & Patel-Schneider, 2010). diagnosis (debugging) problem context arises,
one requirements fulfilled.
work Shchekotykhin et al. (2012), two interactive debugging approaches
tested set faulty real-world ontologies (Kalyanpur, Parsia, Horridge, & Sirin, 2007)
874

fiParallel Model-Based Diagnosis Multi-Core Computers

two randomly modified large real-world ontologies. use dataset evaluate
performance gains applying parallelization schemes ontology debugging problem. details different tested ontologies given Table 10.
characteristics problems described terms description logic (DL) used
formulate ontology, number axioms (#A), concepts (#C), properties (#P),
individuals (#I). terms first-order logic, concepts properties correspond
unary binary predicates, whereas individuals correspond constants. Every letter
DL name, ALCHF pDq , corresponds syntactic feature language. E.g.,
ALCHF pDq Attributive concept Language Complement, properties Hierarchy,
Functional properties Datatypes. underlying description logic reasoner, used
Pellet (Sirin, Parsia, Grau, Kalyanpur, & Katz, 2007). manipulation knowledge bases diagnosis process accomplished OWL-API (Horridge &
Bechhofer, 2011).
Note considered ontology debugging problem different diagnosis settings discussed far cannot efficiently encoded CSP SAT problem.
reason decision problems, checking consistency concept
satisfiability, ontologies given Table 10 ExpTime-complete (Baader et al.,
2010). set experiments therefore helps us explore benefits parallelization
problem settings computation conflict sets hard. Furthermore,
application parallelization approaches ontology debugging problem demonstrates generality methods, i.e., show methods applicable
wide range diagnosis problems require existence sound complete
consistency checking procedure.
Due generality Reiters general approach and, correspondingly, implementation diagnosis procedures, technical integration OWL-DL reasoner
software framework relatively simple. difference CSP-based problems
instead calling Chocos solve() method inside Theorem Prover, make call
Pellet reasoner via OWL-API check consistency ontology.
Ontology
Chemical
Koala
Sweet-JPL
miniTambis
University
Economy
Transportation
Cton
Opengalen-no-propchains

DL
ALCHF pDq
ALCON pDq
ALCHOF pDq
ALCN
SOIN pDq
ALCHpDq
ALCHpDq
SHF
ALCHIF pDq

#A
144
44
2,579
173
49
1,781
1,300
33,203
9,664

#C/#P/#I
48/20/0
21/5/6
1,537/121/50
183/44/0
30/12/4
339/53/482
445/93/183
17,033/43/0
4,713/924/0

#D
6
10
13
48
90
864
1,782
15
110

|D|
1.67
2.3
1
3
3.67
7.17
8
4
4.13

Table 10: Characteristics tested ontologies.
Results obtained results using thread pool size four shown Table
11. Again, every case parallelization advantageous compared sequential
version cases obtained speedups substantial. Regarding comparison
875

fiJannach, Schmitz, & Shchekotykhin

LWP FP variants, clear winner across test cases. LWP seems
advantageous problems complex respect
computation times. problems easily solved, FP sometimes slightly
better. clear correlation problem characteristics like complexity
knowledge base terms size could identified within set benchmark
problems.
Ontology
Chemical
Koala
Sweet-JPL
miniTambis
University
Economy
Transportation
Cton
Opengalen-no-propchains

Seq.(QXP)
[ms]
237
16
7
135
85
355
1,696
203
11,044

LWP(QXP)
S4
E4
1.44 0.36
1.42 0.36
1.47
0.37
1.43
0.36
1.66
0.41
2.20 0.55
2.72 0.68
1.27 0.32
1.59
0.40

FP(QXP)
S4
E4
1.33 0.33
1.27 0.32
1.55 0.39
1.46 0.37
1.68 0.42
1.90 0.48
2.33 0.58
1.22 0.30
1.86 0.47

Table 11: Results ontologies searching diagnoses.

A.1.3 Adding Threads
Constraint Satisfaction Problems Table 12 shows results CSP benchmarks
spreadsheets using 12 threads. test utilizing 4 threads
advantageous one small scenario. However, 7 11 tested scenarios
computations 8 threads pay off. indicates
choosing right degree parallelization depend characteristics diagnosis
problem. diagnosis mknap-1-5 problem, example, cannot sped
parallelization contains one single conflict found root node.
contrast, graceful-K3-P2 problem benefits use 12 threads
could achieve speedup 4.21 scenario, corresponds runtime reduction
76%.
Ontologies results diagnosing ontologies 12 threads shown
Table 13. tested ontologies, comparably simple debugging cases, using
4 threads payed 3 7 cases. best results diagnosing
3 ontologies obtained 8 threads used. one ontology using
4 threads even slower sequential algorithm. indicates
effectiveness parallelization depends characteristics diagnosis problem
adding threads even slightly counterproductive.
A.1.4 Systematic Variation Problem Characteristics
Procedure better understand way problem characteristics influence
performance gains, used suite artificially created hitting set construction problems
876

fiParallel Model-Based Diagnosis Multi-Core Computers

Scenario

Seq.(QXP)
[ms] S4
E4
c8
444 1.05 0.26
costasArray-13
3,854 2.69 0.67
domino-100-100
213 2.04 0.51
gracefulK3-P2
1,743 3.03 0.76
mknap-1-5
4,141 1.00 0.25
queens-8
86 1.18 0.30
hospital payment
11,728 1.60 0.40
profit calculation
81 1.53 0.38
course planning
15,323 2.31 0.58
preservation model
127 1.34 0.34
revenue calculation
460 2.39 0.60

S8
1.07
2.88
2.30
4.12
1.00
1.30
1.70
1.59
2.85
1.41
2.17

FP(QXP)
E8
S10
E10
S12
E12
0.13 1.08 0.11 1.07 0.09
0.36 2.84 0.28 2.80 0.23
0.29 2.22 0.22 2.00 0.17
0.51 4.18 0.42 4.21 0.35
0.13 1.00 0.10 1.00 0.08
0.16 1.24 0.12 1.19 0.10
0.21 1.51 0.15 1.36 0.11
0.20 1.51 0.15 1.44 0.12
0.36 2.84 0.28 2.73 0.23
0.18 1.41 0.14 1.43 0.12
0.27 1.96 0.20 1.85 0.15

Table 12: Observed performance gains CSP benchmarks spreadsheets
server 12 hardware threads.
Ontology
Chemical
Koala
Sweet-JPL
miniTambis
University
Economy
Transportation

Seq.(QXP)
[ms]
246
21
6
134
88
352
1,448

S4
1.37
1.07
1.09
1.47
1.53
1.48
1.74

E4
0.34
0.27
0.27
0.37
0.38
0.37
0.43

S8
1.29
1.02
1.13
1.49
1.64
0.90
1.23

FP(QXP)
E8
S10
0.16 1.30
0.13 1.03
0.14 1.08
0.19 1.47
0.21 1.56
0.11 0.76
0.15 1.07

E10
0.13
0.10
0.11
0.15
0.16
0.08
0.11

S12
1.32
0.99
1.02
1.45
1.56
0.71
1.09

E12
0.11
0.08
0.09
0.12
0.13
0.06
0.09

Table 13: Observed performance gains ontologies server 12 hardware
threads.

following varying parameters: number components (#Cp), number conflicts
(#Cf), average size conflicts (|Cf|). Given parameters, used problem generator
produces set minimal conflicts desired characteristics. generator
first creates given number components uses components generate
requested number conflicts.
obtain realistic settings, generated conflicts equal size rather
varied according Gaussian distribution desired size mean. Similarly,
components equally likely part conflict used Gaussian
distribution assign component failure probabilities. probability distributions could
used generation process well, e.g., reflect specifics certain application
domain.
Since experiment conflicts known advance, conflict detection algorithm within consistency check return one suitable conflict upon request.
zero computation times unrealistic assumption conflict
877

fiJannach, Schmitz, & Shchekotykhin

detection actually costly part diagnosis process, varied assumed
conflict computation times analyze effect relative performance gains.
computation times simulated adding artificial active waiting times (Wt) inside
consistency check (shown ms Table 14). Note consistency check called
conflict reused current node; artificial waiting time applies
cases new conflict determined.
experiment repeated 100 times different variations problem setting
factor random effects. number diagnoses #D thus average well.
algorithms had, however, solve identical sets problems thus returned identical
sets diagnoses. limited search depth 4 experiments speed
benchmark process. average running times reported Table 14.
Results Varying Computation Times First, varied assumed conflict computation times quite small diagnosis problem using 4 parallel threads (Table 14).
first row assumed zero computation times shows long HS-tree construction
alone needs. improvements parallelization smaller case
overhead thread creation synchronization. However, soon add average
running time 10ms consistency check, parallelization approaches result
speedup 3, corresponds runtime reduction 67%. increasing
assumed computation time lead better relative improvements using pool
4 threads.
Results Varying Conflict Sizes average conflict size impacts breadth
HS-tree. Next, therefore varied average conflict size. hypothesis larger
conflicts correspondingly broader HS-trees better suited parallel processing.
results shown Table 14 confirm assumption. FP always slightly efficient
LWP. Average conflict sizes larger 9 did, however, lead strong additional
improvements using 4 threads.
Results Adding Threads larger conflicts, adding additional threads leads
improvements. Using 8 threads results improvements 7.27 (corresponding running time reduction 85%) larger conflict sizes
cases even higher levels parallelization achieved.
Results Adding Components Finally, varied problem complexity
adding components potentially faulty. Since left number
size conflicts unchanged, adding components led diagnoses included
different components. limited search depth 4 experiment, fewer
diagnoses found level search trees narrower. result,
relative performance gains lower fewer components (constraints).
Discussion simulation experiments demonstrate advantages parallelization.
tests, speedups LWP FP statistically significant. results also
confirm performance gains depend different characteristics underlying
problem. additional gains waiting end search level worker
threads finished typically led small improvements.
Redundant calculations can, however, still occur, particular conflicts
new nodes determined parallel two worker threads return conflict.
878

fiParallel Model-Based Diagnosis Multi-Core Computers

#Cp, #Cf, #D Wt Seq.
LWP
|Cf|
[ms] [ms]
S4
E4
Varying computation times Wt
50, 5, 4
25
0
23
2.26 0.56
50, 5, 4
25
10
483
2.98 0.75
50, 5, 4
25 100 3,223 2.83 0.71
Varying conflict sizes
50, 5, 6
99
10 1,672 3.62 0.91
50, 5, 9
214
10 3,531 3.80 0.95
50, 5, 12
278
10 4,605 3.83 0.96
Varying numbers components
50, 10, 9
201
10 3,516 3.79 0.95
75, 10, 9
105
10 2,223 3.52 0.88
100, 10, 9
97
10 2,419 3.13 0.78
#Cp, #Cf, #D Wt Seq.
LWP
I|Cf|
[ms] [ms]
S8
E8
Adding threads (8 instead 4)
50, 5, 6
99
10 1,672 6.40 0.80
50, 5, 9
214
10 3,531 7.10 0.89
50, 5, 12
278
10 4,605 7.25 0.91

FP
S4

E4

2.58
3.10
2.83

0.64
0.77
0.71

3.68
3.83
3.88

0.92
0.96
0.97

3.77 0.94
3.29 0.82
3.45 0.86
FP
S8
E8
6.50
7.15
7.27

0.81
0.89
0.91

Table 14: Simulation results.
Although without parallelization computing resources would left unused
anyway, redundant calculations lead overall longer computation times small
problems thread synchronization overheads.
A.2 Additional Experiments Using MXP Conflict Detection
section report additional results obtained using MergeXplain
instead QuickXplain conflict detection strategy described Section 4.2.
different experiments made using set CSPs ontology debugging problems. Remember set experiments goal identify set leading
diagnoses.
A.2.1 Diagnosing Constraint Satisfaction Problems
Table 15 shows results searching five diagnoses using CSP spreadsheet
benchmarks. MXP could help reduce running times tested
scenarios except smaller ones. tiny scenario mknap-1-5, simple
sequential algorithm using QXP fastest alternative. scenarios,
however, parallelization pays faster sequentially expanding search
tree. best result could achieved scenario costasArray-13, FP using
MXP reduced running times 83% compared sequential algorithm using QXP,
879

fiJannach, Schmitz, & Shchekotykhin

corresponds speedup 6. results indicate FP works well
QXP MXP.
Scenario
c8
costasArray-13
domino-100-100
gracefulK3-P2
mknap-1-5
queens-8
hospital payment
profit calculation
course planning
preservation model
revenue calculation

Seq.(QXP)
[ms]
455
2,601
53
528
19
75
1,885
33
1,522
411
48

FP(QXP)
S4
E4
1.03 0.26
3.66 0.91
1.26 0.32
2.67 0.67
0.99 0.25
1.55 0.39
1.17 0.29
1.92 0.48
0.99 0.25
1.50 0.37
1.21 0.30

Seq.(MXP)
[ms]
251
2,128
50
419
21
63
1,426
40
1,188
430
42

FP(MXP)
S4
E4
1.06 0.26
4.92 1.23
1.43 0.36
2.48 0.62
1.01 0.25
1.67 0.42
1.28 0.32
1.86 0.46
1.42 0.35
1.50 0.37
1.48 0.37

Table 15: Results CSP benchmarks spreadsheets (QXP vs MXP).
Note one case (costasArray-13) see efficiency value larger one,
means obtained speedup super-linear. happen special situations
search limited number diagnoses use FP method (see also
Section A.3.1). Assume generating one specific node takes particularly long, i.e.,
computation conflict set requires considerable amount time. case,
sequential algorithm stuck node time, FP method
continue generating nodes. nodes sufficient find (limited)
required number diagnoses, lead efficiency value greater
theoretical optimum.
A.2.2 Diagnosing Ontologies
results shown Table 16. Similar previous experiment, using MXP
combination FP pays cases except simple benchmark problems.
A.3 Additional Experiments Parallel Depth-First Search
section, report results additional experiments made assess
effects parallelizing depth-first search strategy described Section 5.3. set
experiments goal find one single minimal diagnosis. report results
obtained constraint problems ontology debugging problems discuss
findings simulation experiment systematically varied problem
characteristics.
A.3.1 Diagnosing Constraint Satisfaction Problems
results searching single diagnosis CSPs spreadsheets shown
Table 17. Again, parallelization generally shows good strategy speed
880

fiParallel Model-Based Diagnosis Multi-Core Computers

Ontology
Chemical
Koala
Sweet-JPL
miniTambis
University
Economy
Transportation
Cton
Opengalen-no-propchains

Seq.(QXP)
[ms]
187
15
5
68
33
19
71
174
2,145

FP(QXP)
S4
E4
2.10 0.53
1.49 0.37
1.27 0.32
1.04 0.26
1.05 0.26
1.10 0.27
1.08 0.27
1.36 0.34
1.22 0.30

Seq.(MXP)
[ms]
144
13
4
56
26
14
53
154
1,748

FP(MXP)
S4
E4
1.94 0.48
1.27 0.32
1.05 0.26
1.08 0.27
1.02 0.26
1.00 0.25
1.10 0.27
1.33 0.33
1.35 0.34

Table 16: Results Ontologies (QXP vs MXP).
diagnosis process. measured speedups except speedup RDFS first scenario
c8 statistically significant. specific problem setting, FP strategy
measurable effect strategies even modest performance deterioration
observed compared Reiters sequential algorithm. reason lies resulting
structure HS-tree narrow conflicts size one.
following detailed observations made comparing algorithms.
tested CSPs, FP advantageous compared RDFS PRDFS.
spreadsheets, contrast, RDFS PRDFS better breadth-first
approach FP three five cases.
comparing RDFS PRDFS, observe parallelization
advantageous also depth-first strategies.
Again, however, improvements seem depend underlying problem structure. case hospital payment scenario, speedup PRDFS high
3.1 compared sequential algorithm, corresponds runtime reduction
67%. parallel strategy is, however, consistently better
test cases.
performance Hybrid method lies performances
two components many, all, tested scenarios.

A.3.2 Diagnosing Ontologies
Next, evaluated search one diagnosis real-world ontologies (Table 18).
tested scenarios, applying depth-first strategy often pay compared
breadth-first methods. reason tested examples ontology debugging domain many cases single-element diagnoses exist, quickly
detected breadth-first strategy. Furthermore absolute running times often comparably small. Parallelizing depth-first strategy leads significant speedups
cases.
881

fiJannach, Schmitz, & Shchekotykhin

Scenario
c8
costasArray-13
domino-100-100
gracefulK3-P2
mknap-1-5
queens-8
hospital payment
profit calculation
course planning
preservation model
revenue calculation

Seq.
[ms]
462
1,996
57
372
166
72
263
99
3,072
182
152

FP
S4
E4
1.09 0.27
4.78 1.19
1.22 0.30
2.86 0.71
2.18 0.55
1.38 0.34
1.83 0.46
1.67 0.42
1.11 0.28
1.78 0.44
1.11 0.28

RDFS
[ms]
454
3,729
45
305
114
55
182
70
2,496
104
121

PRDFS
S4
E4
0.89 0.22
3.42 0.85
1.17 0.29
2.01 0.50
1.02 0.26
1.02 0.26
2.14 0.54
1.15 0.29
0.90 0.23
0.99 0.25
0.92 0.23

Hybrid
S4
E4
0.92 0.23
5.90 1.47
1.05 0.26
1.89 0.47
1.35 0.33
0.95 0.24
1.72 0.43
1.10 0.28
0.87 0.22
0.95 0.24
0.90 0.22

Table 17: Results CSP benchmarks spreadsheets finding one diagnosis.
Ontology
Chemical
Koala
Sweet-JPL
miniTambis
University
Economy
Transportation

Seq.
[ms]
73
10
3
58
29
17
65

FP
S4
E4
2.18 0.54
2.20 0.55
0.92 0.23
0.95 0.24
1.06 0.27
1.10 0.27
1.03 0.26

RDFS
[ms]
57
9
4
62
30
18
61

PRDFS
S4
E4
1.62 0.41
1.93 0.48
0.97 0.24
0.92 0.23
1.03 0.26
1.16 0.29
1.03 0.26

Hybrid
S4
E4
1.47 0.37
1.39 0.35
0.92 0.23
0.93 0.23
1.03 0.26
1.10 0.27
0.98 0.24

Table 18: Observed performance gains ontologies finding one diagnosis.
A.3.3 Systematic Variation Problem Characteristics
Table 19 finally shows simulation results searching one single diagnosis.
experiment used uniform probability distribution selecting components
conflicts obtain complex diagnosis problems. results summarized
follows.
FP expected better sequential version HS-tree algorithm
tested configurations.
small problems contain comparably small conflicts,
depth-first strategy work well. parallel sequential versions
even slower Reiters original proposal, except cases zero conflict
computation times assumed. indicates costs hitting set minimization high.
larger problem instances, relying depth-first strategy find one single
diagnosis advantageous also better FP. additional test even
882

fiParallel Model-Based Diagnosis Multi-Core Computers

#Cp, #Cf, I|D| Wt
Seq.
I|Cf|
[ms]
[ms]
Varying computation times Wt
50, 5, 4
3.40
0
11
50, 5, 4
3.40
10
89
50, 5, 4
3.40 100
572
Varying conflict sizes
50, 5, 6
2.86
10
90
50, 5, 9
2.36
10
86
50, 5, 12
2.11
10
83
Varying numbers components
50, 10, 9
3.47
10
229
75, 10, 9
3.97
10
570
100, 10, 9 4.34
10
1,467
conflicts
100, 12, 9
5.00
10 26,870

FP

RDFS
[ms]

PRDFS
S4
E4

Hybrid
S4
E4

S4

E4

2.61
1.50
1.50

0.65
0.37
0.37

2
155
1,052

1.01
1.28
1.30

0.25
0.32
0.33

0.85
2.24
2.26

0.21
0.56
0.56

1.57
1.55
1.61

0.39
0.39
0.40

143
138
124

1.26
1.34
1.23

0.31
0.33
0.31

2.12
2.04
1.95

0.53
0.51
0.49

2.36
3.09
2.37

0.59
0.77
0.59

202
228
240

1.35
1.37
1.34

0.34
0.34
0.33

1.65
1.42
1.26

0.41
0.36
0.31

1.28

0.32

280

1.39

0.35

1.24

0.31

Table 19: Simulation results finding one diagnosis.
larger problem shown last line Table 19 reveals potential depth-first
search approach.
problems larger, PRDFS help obtain runtime
improvements compared RDFS.
Hybrid method works well single case zero computation times.
Again, represents good choice problem structure known.
Overall, simulation experiments show speedups achieved
different methods depend underlying problem structure also search
one single diagnosis.

References
Abreu, R., & van Gemund, A. J. C. (2009). Low-Cost Approximate Minimal Hitting Set
Algorithm Application Model-Based Diagnosis. SARA09, pp. 29.
Anglano, C., & Portinale, L. (1996). Parallel model-based diagnosis using PVM. EuroPVM96, pp. 331334.
Autio, K., & Reiter, R. (1998). Structural Abstraction Model-Based Diagnosis.
ECAI98, pp. 269273.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. (2010).
Description Logic Handbook: Theory, Implementation Applications, Vol. 32.
Bolosky, W. J., & Scott, M. L. (1993). False Sharing Effect Shared Memory
Performance. SEDMS93, pp. 5771.
883

fiJannach, Schmitz, & Shchekotykhin

Brungger, A., Marzetta, A., Fukuda, K., & Nievergelt, J. (1999). parallel search bench
ZRAM applications. Annals Operations Research, 90 (0), 4563.
Buchanan, B., & Shortliffe, E. (Eds.). (1984). Rule-based Expert Systems: MYCIN Experiments Stanford Heuristic Programming Project. Addison-Wesley, Reading,
MA.
Burns, E., Lemons, S., Ruml, W., & Zhou, R. (2010). Best-First Heuristic Search
Multicore Machines. Journal Artificial Intelligence Research, 39, 689743.
Campeotto, F., Palu, A. D., Dovier, A., Fioretto, F., & Pontelli, E. (2014). Exploring
Use GPUs Constraint Solving. PADL14, pp. 152167.
Cardoso, N., & Abreu, R. (2013). Distributed Approach Diagnosis Candidate Generation. EPIA13, pp. 175186.
Chandra, D., Guo, F., Kim, S., & Solihin, Y. (2005). Predicting Inter-Thread Cache Contention Chip Multi-Processor Architecture. HPCA11, pp. 340351.
Chu, G., Schulte, C., & Stuckey, P. J. (2009). Confidence-Based Work Stealing Parallel
Constraint Programming. CP09, pp. 226241.
Console, L., Friedrich, G., & Dupre, D. T. (1993). Model-Based Diagnosis Meets Error
Diagnosis Logic Programs. IJCAI93, pp. 14941501.
de Kleer, J. (2011). Hitting set algorithms model-based diagnosis. DX11, pp. 100105.
Dean, J., & Ghemawat, S. (2008). MapReduce: Simplified Data Processing Large Clusters. Communications ACM, 51 (1), 107113.
Dijkstra, E. W. (1968). Structure THE-Multiprogramming System. Communications ACM, 11 (5), 341346.
Eiter, T., & Gottlob, G. (1995). Complexity Logic-Based Abduction. Journal
ACM, 42 (1), 342.
Feldman, A., Provan, G., de Kleer, J., Robert, S., & van Gemund, A. (2010a). Solving
model-based diagnosis problems max-sat solvers vice versa. DX10, pp.
185192.
Feldman, A., Provan, G., & van Gemund, A. (2010b). Approximate Model-Based Diagnosis
Using Greedy Stochastic Search. Journal Artifcial Intelligence Research, 38, 371
413.
Felfernig, A., Friedrich, G., Isak, K., Shchekotykhin, K. M., Teppan, E., & Jannach, D.
(2009). Automated debugging recommender user interface descriptions. Applied
Intelligence, 31 (1), 114.
Felfernig, A., Friedrich, G., Jannach, D., & Stumptner, M. (2004). Consistency-based diagnosis configuration knowledge bases. Artificial Intelligence, 152 (2), 213234.
Felfernig, A., Friedrich, G., Jannach, D., Stumptner, M., & Zanker, M. (2001). Hierarchical
diagnosis large configurator knowledge bases. KI01, pp. 185197.
Felfernig, A., Schubert, M., & Zehentner, C. (2012). efficient diagnosis algorithm
inconsistent constraint sets. Artificial Intelligence Engineering Design, Analysis
Manufacturing, 26 (1), 5362.
884

fiParallel Model-Based Diagnosis Multi-Core Computers

Felfernig, A., Friedrich, G., Jannach, D., Stumptner, M., et al. (2000). Consistency-based
diagnosis configuration knowledge bases. ECAI00, pp. 146150.
Ferguson, C., & Korf, R. E. (1988). Distributed tree search application alpha-beta
pruning. AAAI88, pp. 128132.
Friedrich, G., & Shchekotykhin, K. M. (2005). General Diagnosis Method Ontologies.
ISWC05, pp. 232246.
Friedrich, G., Stumptner, M., & Wotawa, F. (1999). Model-Based Diagnosis Hardware
Designs. Artificial Intelligence, 111 (1-2), 339.
Friedrich, G., Fugini, M., Mussi, E., Pernici, B., & Tagni, G. (2010). Exception handling
repair service-based processes. IEEE Transactions Software Engineering, 36 (2),
198215.
Friedrich, G., & Shchekotykhin, K. (2005). General Diagnosis Method Ontologies.
ISWC05, pp. 232246.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide Theory
NP-Completeness. W. H. Freeman & Co.
Grau, B. C., Horrocks, I., Motik, B., Parsia, B., Patel-Schneider, P., & Sattler, U. (2008).
OWL 2: next step OWL. Web Semantics: Science, Services Agents
World Wide Web, 6 (4), 309322.
Greiner, R., Smith, B. A., & Wilkerson, R. W. (1989). Correction Algorithm
Reiters Theory Diagnosis. Artificial Intelligence, 41 (1), 7988.
Horridge, M., & Bechhofer, S. (2011). OWL API: Java API OWL Ontologies.
Semantic Web Journal, 2 (1), 1121.
Jannach, D., & Schmitz, T. (2014). Model-based diagnosis spreadsheet programs:
constraint-based debugging approach. Automated Software Engineering, February
2014 (published online).
Jannach, D., Schmitz, T., & Shchekotykhin, K. (2015). Parallelized Hitting Set Computation
Model-Based Diagnosis. AAAI15, pp. 15031510.
Junker, U. (2004). QUICKXPLAIN: Preferred Explanations Relaxations OverConstrained Problems. AAAI04, pp. 167172.
Kalyanpur, A., Parsia, B., Horridge, M., & Sirin, E. (2007). Finding justifications
owl dl entailments. Semantic Web, Vol. 4825 Lecture Notes Computer
Science, pp. 267280.
Korf, R. E., & Schultze, P. (2005). Large-scale parallel breadth-first search. AAAI05,
pp. 13801385.
Kurtoglu, T., & Feldman, A. (2011). Third International Diagnostic Competition (DXC
11). https://sites.google.com/site/dxcompetition2011. Accessed: 2016-03-15.
Lecoutre, C., Roussel, O., & van Dongen, M. R. C. (2008). CPAI08 competition. http:
//www.cril.univ-artois.fr/CPAI08/. Accessed: 2016-03-15.
Li, L., & Yunfei, J. (2002). Computing Minimal Hitting Sets Genetic Algorithm.
DX02, pp. 14.
885

fiJannach, Schmitz, & Shchekotykhin

Marques-Silva, J., Janota, M., Ignatiev, A., & Morgado, A. (2015). Efficient Model Based
Diagnosis Maximum Satisfiability. IJCAI15, pp. 19661972.
Marques-Silva, J., Janota, M., & Belov, A. (2013). Minimal Sets Monotone Predicates
Boolean Formulae. Computer Aided Verification, pp. 592607.
Mateis, C., Stumptner, M., Wieland, D., & Wotawa, F. (2000). Model-Based Debugging
Java Programs. AADEBUG00.
Mencia, C., & Marques-Silva, J. (2014). Efficient Relaxations Over-constrained CSPs.
ICTAI14, pp. 725732.
Menca, C., Previti, A., & Marques-Silva, J. (2015). Literal-based MCS extraction.
IJCAI15, pp. 19731979.
Metodi, A., Stern, R., Kalech, M., & Codish, M. (2014). novel sat-based approach
model based diagnosis. Journal Artificial Intelligence Research, 51, 377411.
Michel, L., See, A., & Van Hentenryck, P. (2007). Parallelizing constraint programs transparently. CP07, pp. 514528.
Nica, I., Pill, I., Quaritsch, T., & Wotawa, F. (2013). route success: performance
comparison diagnosis algorithms. IJCAI13, pp. 10391045.
Nica, I., & Wotawa, F. (2012). ConDiag - computing minimal diagnoses using constraint
solver. DX12, pp. 185191.
Phillips, M., Likhachev, M., & Koenig, S. (2014). PA*SE: Parallel A* Slow Expansions.
ICAPS14.
Pill, I., Quaritsch, T., & Wotawa, F. (2011). conflicts diagnoses: empirical
evaluation minimal hitting set algorithms. DX11, pp. 203211.
Pill, I., & Quaritsch, T. (2012). Optimizations Boolean Approach Computing
Minimal Hitting Sets. ECAI12, pp. 648653.
Powley, C., & Korf, R. E. (1991). Single-agent parallel window search. IEEE Transactions
Pattern Analysis Machine Intelligence, 13 (5), 466477.
Previti, A., Ignatiev, A., Morgado, A., & Marques-Silva, J. (2015). Prime Compilation
Non-Clausal Formulae. IJCAI15, pp. 19801987.
Prudhomme, C., Fages, J.-G., & Lorca, X. (2015). Choco Documentation. TASC, INRIA
Rennes, LINA CNRS UMR 6241, COSLING S.A.S. http://www.choco-solver.org.
Reiter, R. (1987). Theory Diagnosis First Principles. Artificial Intelligence, 32 (1),
5795.
Rymon, R. (1994). SE-tree-based prime implicant generation algorithm. Annals
Mathematics Artificial Intelligence, 11 (1-4), 351365.
Satoh, K., & Uno, T. (2005). Enumerating Minimally Revised Specifications Using Dualization. JSAI05, pp. 182189.
Schulte, C., Lagerkvist, M., & Tack, G. (2016). GECODE - open, free, efficient constraint
solving toolkit. http://www.gecode.org. Accessed: 2016-03-15.
886

fiParallel Model-Based Diagnosis Multi-Core Computers

Shchekotykhin, K., Friedrich, G., Fleiss, P., & Rodler, P. (2012). Interactive ontology debugging: Two query strategies efficient fault localization. Journal Web Semantics,
1213, 88103.
Shchekotykhin, K. M., & Friedrich, G. (2010). Query strategy sequential ontology
debugging. ISWC10, pp. 696712.
Shchekotykhin, K., Jannach, D., & Schmitz, T. (2015). MergeXplain: Fast Computation
Multiple Conflicts Diagnosis. IJCAI15, pp. 32213228.
Shchekotykhin, K. M., Friedrich, G., Rodler, P., & Fleiss, P. (2014). Sequential diagnosis
high cardinality faults knowledge-bases direct diagnosis generation. ECAI14,
pp. 813818.
Sirin, E., Parsia, B., Grau, B. C., Kalyanpur, A., & Katz, Y. (2007). Pellet: Practical
OWL-DL Reasoner. Web Semantics: Science, Services Agents World Wide
Web, 5 (2), 51 53.
Stern, R., Kalech, M., Feldman, A., & Provan, G. (2012). Exploring Duality ConflictDirected Model-Based Diagnosis. AAAI12, pp. 828834.
Stuckey, P. J., Feydy, T., Schutt, A., Tack, G., & Fischer, J. (2014). MiniZinc Challenge
2008-2013. AI Magazine, 35 (2), 5560.
Stumptner, M., & Wotawa, F. (1999). Debugging functional programs. IJCAI99, pp.
10741079.
Stumptner, M., & Wotawa, F. (2001). Diagnosing Tree-Structured Systems. Artificial
Intelligence, 127 (1), 129.
White, J., Benavides, D., Schmidt, D. C., Trinidad, P., Dougherty, B., & Cortes, A. R.
(2010). Automated diagnosis feature model configurations. Journal Systems
Software, 83 (7), 10941107.
Williams, B. C., & Ragno, R. J. (2007). Conflict-directed A* role model-based
embedded systems. Discrete Applied Mathematics, 155 (12), 15621595.
Wotawa, F. (2001a). variant Reiters hitting-set algorithm. Information Processing
Letters, 79 (1), 4551.
Wotawa, F. (2001b). Debugging Hardware Designs Using Value-Based Model. Applied
Intelligence, 16 (1), 7192.
Wotawa, F., & Pill, I. (2013). classification modeling issues distributed modelbased diagnosis. AI Communications, 26 (1), 133143.

887

fiJournal Artificial Intelligence Research 55 (2016) 715-742

Submitted 10/15; published 3/16

Combining Two Three-Way Embedding Models Link
Prediction Knowledge Bases
Alberto Garca-Duran

alberto.garcia-duran@utc.fr

Sorbonne universites, Universite de technologie de Compiegne
CNRS, Heudiasyc UMR 7253
CS 60 319, 60 203 Compiegne cedex, France

Antoine Bordes

abordes@fb.com

Facebook AI Research
770 Broadway, New York, NY 10003. USA

Nicolas Usunier

usunier@fb.com

Facebook AI Research
112 Avenue de Wagram, 75017 Paris, France

Yves Grandvalet

yves.grandvalet@utc.fr

Sorbonne universites, Universite de technologie de Compiegne
CNRS, Heudiasyc UMR 7253
CS 60 319, 60 203 Compiegne cedex, France

Abstract
paper tackles problem endogenous link prediction knowledge base completion. Knowledge bases represented directed graphs whose nodes correspond
entities edges relationships. Previous attempts either consist powerful systems
high capacity model complex connectivity patterns, unfortunately usually
end overfitting rare relationships, approaches trade capacity simplicity order fairly model relationships, frequent not. paper, propose
Tatec, happy medium obtained complementing high-capacity model simpler one, pre-trained separately combined. present several variants
model different kinds regularization combination strategies show
approach outperforms existing methods different types relationships achieving
state-of-the-art results four benchmarks literature.

1. Introduction
Knowledge bases (KBs) crucial tools deal rise data, since provide
ways organize, manage retrieve digital knowledge. repositories cover
kind area, specific domains like biological processes, example GeneOntology (Ashburner, Ball, Blake, Botstein, Butler, Cherry, Davis, Dolinski, Dwight, Eppig,
et al., 2000), generic purposes. Freebase (Bollacker, Evans, Paritosh, Sturge, &
Taylor, 2008), huge collaborative KB belongs Google Knowledge Graph,
example latter kind provides expert/common-level knowledge capabilities
users. example knowledge engine WolframAlpha (Wolfram Research,
2009), engine answers natural language question, like far Saturn
sun?, human-readable answers (1,492 109 km) using internal KB.
KBs used question answering, also natural language process2016 AI Access Foundation. rights reserved.

fiGarca-Duran, Bordes, Usunier & Grandvalet

ing tasks like word-sense disambiguation (Navigli & Velardi, 2005), co-reference resolution
(Ponzetto & Strube, 2006) even machine translation (Knight & Luk, 1994).
KBs formalized directed multi-relational graphs, whose nodes correspond
entities connected edges encoding various kinds relationship. Hence, one
also refer multi-relational data. following denote connections among
entities via triples facts (head, label, tail ), entities head tail connected
relationship label. information KB represented via triple
concatenation several ones. Note multi-relational data present KBs
also recommender systems, nodes would correspond users products
edges different relationships them, social networks instance.
main issue KBs far complete. Freebase currently
contains thousands relationships 80 millions entities, leading billions
facts, remains small portion human knowledge, obviously. since question answering engines based KBs like WolframAlpha
capable generalizing acquired knowledge fill missing facts,
de facto limited: search matches question/query internal KB
information missing provide correct answer, even correctly
interpreted question. Consequently, huge efforts nowadays devoted towards
KB construction completion (Lao, Mitchell, & Cohen, 2011; Bordes, Glorot, Weston, &
Bengio, 2013a; Dong, Gabrilovich, Heitz, Horn, Lao, Murphy, Strohmann, Sun, & Zhang,
2014), via manual automatic processes, mix both. mainly divided two
tasks: entity creation extraction, consists adding new entities KB
link prediction, attempts add connections entities. paper focuses
latter case. Performing link prediction formalized filling incomplete triples
like (head, label, ?) (?, label, tail), predicting missing argument triple
triple exist KB, yet. instance, given small example KB
Figure 1, made 6 entities 2 different relationships, containing facts like (Jared
Leto, influenced by, Bono) (Michael Buble, profession, singer), would like
able predict new links (Frank Sinatra, profession, singer), using
fact influenced singer Michael Buble instance.
Link prediction KBs complex due several issues. entities homogeneously connected: lot links entities, whereas others
rarely connected. illustrate diverse characteristics present relationships
take look FB15k, subset Freebase introduced Bordes, Usunier, GarcaDuran, Weston, Yakhnenko (2013b). data set 14k entities 1k types
relationships, entities mean number triples 400, median 21 indicating
large number appear triples. Besides, roughly 25% connections type 1-to-1, is, head connected one tail, around 25%
type Many-to-Many, is, multiple heads linked tail vice versa.
result, diverse problems coexist database. Another property relationships
big impact performance typing arguments. FB15k,
relationships strongly typed like /sports/sports team/location, one
always expects football team head location tail, far less precise
/common/webpage/category one expects web page addresses tail
716

fiEmbedding Models Link Prediction KBs

ion


fes


pr

Actor
Singer

Jared
Leto

Bono
Michael
Buble

Frank
Sinatra

Figure 1: Example (incomplete) Knowledge Base 6 entities, 2 relationships
7 facts.
pretty much everything else head. link prediction algorithm able adapt
different settings.
Though exists (pseudo-) symbolic approaches link prediction based Markovlogic networks (Kok & Domingos, 2007) random walks (Lao et al., 2011), learning latent
features representations KB constituents - so-called embedding methods - recently
proved efficient performing link prediction KBs, (e.g. Bordes et al., 2013b;
Wang, Zhang, Feng, & Chen, 2014b; Lin, Liu, Sun, Liu, & Zhu, 2015; Chang, Yih, Yang,
& Meek, 2014; Wang, Zhang, Feng, & Chen, 2014a; Zhang, Salwen, Glass, & Gliozzo,
2014; Yang, Duan, Zhou, & Rim, 2014b). works, entities represented
low-dimensional vectors - embeddings - relationships act operators them:
embeddings operators define scoring function learned triples
observed KBs higher scores unobserved ones. embeddings meant
capture underlying features eventually allow create new links successfully.
scoring function used predict new links: higher score, likely
triple true. Representations relationships usually specific (except LFM
(Jenatton, Le Roux, Bordes, & Obozinski, 2012) sharing parameters
across relationships), embeddings entities shared relationships allow
transfer information across them. learning process considered multi-task,
one task concerns relationship, entities shared across tasks.
Embedding models classified according interactions use encode
validity triple scoring function. joint interaction head,
label tail used dealing 3-way model; binary
interactions head tail, head label, label
tail core model, 2-way model. kinds models represent
entities vectors, differ way model relationships: 3-way models
generally use matrices, whereas 2-way models use vectors. difference capacity
leads difference expressiveness models. larger capacity 3-way models
(due large number free parameters matrices) beneficial relationships
717

fiGarca-Duran, Bordes, Usunier & Grandvalet

appearing lot triples, detrimental rare ones even regularization applied.
Capacity difference 2- 3-way models, information encoded
two models also different: show Sections 3 5.3.2 kinds
models assess validity triple using different data patterns.
paper introduce Tatec encompass previous works combining wellcontrolled 2-way interactions high-capacity 3-way ones. aim capturing data
patterns approaches separately pre-training embeddings 2-way 3-way
models using different embedding spaces two them. demonstrate
following otherwise pre-training and/or use different embedding
spaces features cannot conveniently captured embeddings. Eventually,
pre-trained weights combined second stage, leading combination model
outperforms previous works conditions four benchmarks
literature, UMLS, Kinships, FB15k SVO. Tatec also carefully regularized since
systematically compared two different regularization schemes: adding penalty terms
loss function hard-normalizing embedding vectors constraining norms.
paper extension previous work (Garca-Duran, Bordes, & Usunier,
2014): added much thorough study regularization combination strategies Tatec. Besides propose experiments several new benchmarks
complete comparison proposed method w.r.t. state-of-the-art. also give examples predictions projections 2D obtained embeddings provide
insights behavior Tatec. paper organized follows. Section 2 discusses
previous works. Section 3 presents model justifies choices. Detailed explanations training procedure regularization schemes given Section 4.
Finally, present experimental results four benchmarks Section 5.

2. Related Work
section, discuss state-of-the-art modeling large multi-relational databases,
particular focus embedding methods knowledge base completion.
One simplest successful 2-way models TransE (Bordes et al., 2013b).
model, relationships represented translations embedding space:
(h, `, t) holds, embedding tail close embedding head
h plus vector depends label `. natural approach model
hierarchical asymmetric relationships, common knowledge bases
Freebase. Several modifications TransE proposed recently, TransH (Wang
et al., 2014b) TransR (Lin et al., 2015). TransH, embeddings entities
projected onto hyperplane depends ` translation. second
algorithm, TransR, follows idea, except projection operator matrix
general orthogonal projection hyperplane. shall see
next section, TransE corresponds Bigram model additional constraints
parameters.
2-way models shown good performances KB datasets,
limited expressiveness fail dramatically harder datasets.
contrast, 3-way models perform form low-rank tensor factorization,
respect extremely high expressiveness depending rank constraints.
718

fiEmbedding Models Link Prediction KBs

context link prediction multi-relational data, RESCAL (Nickel, Tresp, & Kriegel,
2011) follows natural modeling assumptions. Similarly TransE, RESCAL learns one
low-dimensional embedding entity. However, relationships represented
bilinear operator embedding space RESCAL, i.e. relationship corresponds
matrix, whereas TransE also represented vectors. Besides different
interactions across arguments triple explain validity, models also differ
training objective. training objective RESCAL Frobenius norm
original data tensor low-rank reconstruction, whereas Tatec uses margin
ranking criterion TransE. Another related 3-way model SME(bilinear) (Bordes et al.,
2013a). parameterization SME(bilinear) constrained version RESCAL,
also uses ranking criterion training objective.
Latent Factor Model (LFM) (Jenatton et al., 2012) Neural Tensor Networks
(NTN) (Socher, Chen, Manning, & Ng, 2013) use combinations 3-way model
constrained 2-way model, sense closer algorithm Tatec.
important differences algorithms Tatec, though. First, LFM
NTN share entity embeddings 2-way 3-way models, learn different
entity embeddings. use different embeddings 2-way 3-way models
increase model expressiveness, equivalent combination
shared embeddings higher dimensional embedding space, additional constrains
relation parameters. show experiments however, additional
constraints lead significant improvements. second main difference
approach LFM parameters relationships 2-way
3-way interaction terms also shared, case Tatec. Indeed, joint
parameterization might reduce expressiveness 2-way interaction terms which,
argue Section 3.3, left maximum degrees freedom. Lastly, LFM seeks
maximize likelihood function given set positive negative facts. NTN
general parameterization LFM, still uses entity embeddings
2-way 3-way interaction terms. Also, NTN two layers non-linearity
first layer, model add nonlinearity embedding step.
order precise overview differences approaches, give
Section 3 (Table 1) formulas scoring functions related works.
works ignore type-constraints present relationships (i.e.
entities legitimate arguments given relationship), approaches present
extensions making use side information. Hence, Chang et al. (2014) propose
modification RESCAL avoid incompatible entity-relation triples participate
loss function Krompass, Baier, Tresp (2015) propose similar framework
models optimized iterating small batches. Krompass et al. (2015) also propose
local closed-world assumption approximates type-information available
triples without requiring kind side information KB. Though
consider use side information, type-constraints could easily introduced
model either taking KB (when available), using approach
Krompass et al. (2015).
lot focus recently algorithms purely based learning
embeddings entities and/or relationships, many earlier alternatives proposed.
discuss works carried Bayesian clustering framework, well approaches
719

fiGarca-Duran, Bordes, Usunier & Grandvalet

Figure 2: Example RDF file Freebase
explicitly use graph structure data. Infinite Relational Model (Kemp,
Tenenbaum, Griffiths, Yamada, & Ueda, 2006), nonparametric extension
Stochastic Block Model (Wang & Wong, 1987), Bayesian clustering approach learns
clusters entities kind, i.e. groups entities similar relationships
entities. work followed Sutskever, Salakhutdinov, Tenenbaum
(2009), propose 3-way tensor factorization model based Bayesian clustering
entities within cluster share distribution embeddings.
Symbolic approaches, aforementioned Kok Domingos (2007),
also worth mentioning context. Though rule-based inference new links may
lead great expressiveness, usually limited quality coverage
handcrafted rules. Still, Path Ranking Algorithm (PRA) (Lao et al., 2011) presented
model able discover rules automatically performing random walks training
data, limitation connectivity nodes; i.e. shortenough path connecting two nodes, model able infer relation
them. Recently, Gardner, Talukdar, Krishnamurthy, Mitchell (2014) cover gap
combining model pre-trained embeddings. PRA used Knowledge
Vault project (Dong et al., 2014) conjunction embedding approach. Thus, even
though consider symbolic approaches here, could also combined
embedding model.
Even though present evaluate algorithms context knowledge bases,
work also applies broader context RDF data. RDF standard model
data interchange Web. core Linked Data initiative (Bizer, Heath,
Idehen, & Berners-Lee, 2008) aims extend linking structure Web use
URIs name relationship things well two ends link.
linking structure forms directed, labeled graph, edges represent named link
two objects. Thus data essentially made triples. RDF-terminology
triple defined (subject, predicate, object). Freebase dump available
format. Figure 2 shows example RDF file Freebase, m.02mjmr identifier resource representing Barack Obama. identifier several predicates
ns:influence.influence node.influenced ns:people.person.religion, whose
objects ns:m.01d1n (Reinhold Niebuhr) ns:m.01lp8 (Christianity), respectively.

3. TATEC
describe model motivations underlying parameterization. data
set relations entities fixed set entities E = {e1 , ..., eE }. Relations
represented triples (h, `, t) head h tail indexes entities (i.e.
720

fiEmbedding Models Link Prediction KBs

h, [[E]] = {1, ..., E}), label ` index relationship L = {l1 , ..., },
defines type relation entities eh et .
3.1 Scoring Function
goal learn discriminant scoring function set possible triples E LE
triples represent likely relations receive higher scores triples
represent unlikely ones. proposed model, Tatec, learns embeddings entities
low dimensional vector space, say Rd , parameters operators Rd Rd ,
operators associated single relationship. precisely, score given
Tatec triple (h, `, t), denoted s(h, `, t), defined as:
s(h, `, t) = s1 (h, `, t) + s2 (h, `, t)

(1)

s1 s2 following form:
(B) Bigram 2-way interaction term:

fi ff
fi ff
fi fi ff
s1 (h, `, t) = r`1 fieh1 + r`2 fiet1 + eh1 fiDfiet1 ,

(2)

eh1 , et1 embeddings Rd1 head tail entities (h, `, t) respectively,
r`1 r`2 vectors Rd1 depend relationship `, diagonal
matrix depend input triple.

fiff
fi

fi ff throughout section, . . canonical dot product,
ff

notation

fi figeneral
fi
fi
fi
x = x Ay x two vectors space
square matrix appropriate dimensions.
use two different relation vectors subject object order model
asymmetric relationships; instance, r1` = r2` , (Paris, capital of, France)
would score (France, capital of, Paris).
(T) Trigram 3-way interaction term:

fi fi ff
s2 (h, `, t) = eh2 fiR` fiet2 ,

(3)

R` matrix dimensions (d2 , d2 ), eh2 et2 embeddings Rd2
head tail entities respectively. embeddings entities term
2-way term; even different dimensions.
embedding dimensions d1 d2 hyperparameters model. vectors
matrices learned without additional parameter sharing.
2-way interaction term model similar Bordes et al. (2013a),
slightly general contain constraint relation-dependent
vectors r`1 r`2 . also seen relaxation translation model Bordes
et al. (2013b), special case r`1 = r`2 , identity matrix,
entity embeddings constrained lie unit sphere.
3-way term corresponds exactly model used collective factorization
method RESCAL (Nickel et al., 2011), chose high expressiveness
complex relationships. Indeed, said earlier introduction, 3-way models
721

fiGarca-Duran, Bordes, Usunier & Grandvalet

basically represent kind interaction among entities. combination 2- 3way terms already used Jenatton et al. (2012), Socher et al. (2013), but, besides
different parameterization, Tatec contrasts additional freedom brought
using different embeddings two interaction terms. LFM (Jenatton et al., 2012),
constraints imposed relation-dependent matrix 3-way terms (low rank
limited basis rank-one matrices), relation vectors r`1 r`2 constrained
image matrix (D = 0 work). global constraints severely
limited expressiveness 3-way model, act stringent regularization
reduces expressiveness 2-way model, which, explain Section 3.3,
left maximum degrees freedom. similar NTN (Socher et al., 2013)
respect share parameter relations. overall scoring
function similar model single layer, fundamental difference
use different embedding spaces use non-linear transfer function,
results facilitated training (for instance, gradients larger magnitude). Table
1 details scoring function aforementioned models.
3.2 Term Combination
study two strategies combining bigram trigram scores indicated Equation (1). cases, s1 s2 first trained separately detail Section 4
combined. difference two strategies depends whether
jointly update (or fine-tune) parameters s1 s2 second phase not.
3.2.1 Fine Tuning
first strategy, denoted Tatec-ft, simply consists summing scores following
Equation (1).

fi ff
fi ff
fi fi ff
fi fi ff
sF (h, `, t) = r`1 fieh1 + r`2 fiet1 + eh1 fiDfiet1 + eh2 fiR` fiet2
parameters s1 s2 (and hence s) fine-tuned second training phase
accommodate combination. version could trained directly without
pre-training s1 s2 separately show experiments detrimental.
3.2.2 Linear Combination
second strategy combines bigram trigram terms using linear combination,
without jointly fine-tuning parameters remain unchanged pre-training.
score hence defined follows:

fi ff

fi ff

fi fi ff

fi fi ff
sLC (h, `, t) = 1` r`1 fieh1 + 2` r`2 fiet1 + 3` eh1 fiDfiet1 + 4` eh2 fiR` fiet2
combination weights i` depend relationship learned optimizing
ranking loss (defined later (6)) using L-BFGS, additional quadratic penalization
P ||` ||22
term, ` P
, ` contains combination weights relation `, con` +
strained ` ` = ( hyperparameter). version Tatec denoted Tatec-lc
following.
722

fiEmbedding Models Link Prediction KBs

h-th relation

t-th entity

l-th entity

Figure 3: entry (h,l,t) tensor indicates relation l holds entities
h t.
3.3 Interpretation Motivation Model
section discusses motivations underlying parameterization Tatec,
particular choice 2-way model complement 3-way term.
3.3.1 2-Way Interactions Fiber Biases
first motivation 2-way 3-way model, use analogy
matrix factorization. common matrix factorization techniques collaborative
filtering add biases (also called offsets intercepts) model. instance, critical
step best-performing techniques Netflix prize add user item biases,
i.e. approximate user-rating Rui according (see e.g. Koren, Bell, & Volinsky, 2009):

fi ff
Rui Pu fiQi + u + +
(4)
P RU k , row Pu containing k-dimensional embedding user
(U number users), Q RIk containing embeddings items, u R
bias depending user R bias depending item ( constant
consider on).
2-way + 3-way interaction model propose seen 3-mode tensor version biased version matrix factorization: trigram term (T) collective
matrix factorization parameterization

offi
ff RESCAL algorithm (Nickel et al., 2011)
fi
plays role analogous term Pu Qi matrix factorization model collaborative filtering (4).
bigram term (B) plays role biases fiber tensor,1 i.e.
1
2
3
s1 (h, `, t) Bl,h
+ Bl,t
+ Bh,t

(5)

thus analogue tensors term u + matrix factorization model
(4). exact form s1 (h, `, t) given (B) corresponds specific form collective
1. Fibers higher order analogue matrix rows columns tensors defined fixing
every index one.

723

fiGarca-Duran, Bordes, Usunier & Grandvalet

h

1
factorization fiber-wise bias matrices B1 = Bl,h

l[[L]],h[[E]]

, B2 B3 Equation

(5). exactly learn one bias fiber many fibers little
data, while, argue following, specific form collective factorization
propose (B) allow share relevant information different biases. Note
whereas tensor dimensions n n (in general, problem set
entities considered head tail) 2mn + n2 biases, Tatec computes
biases means linear combinations n + 2m embeddings, allows learning
transfer across them.
3.3.2 Need Multiple Embeddings
key feature Tatec use different embedding spaces 2-way 3-way terms,
existing approaches types interactions use embedding
space (Jenatton et al., 2012; Socher et al., 2013). motivate choice section.
important notice biases matrix factorization model (4), bigram term overall scoring function (1) affect model expressiveness,
particular affect main modeling assumption embeddings low
rank. user/item-biases (4) boil adding two rank-1 matrices 1T
1 factorization model. Since rank matrix hyperparameter, one may
simply add 2 hyperparameter get slightly larger expressiveness before,
reasonably little impact since increase rank would remain small compared
original value (which usually 50 100 large collaborative filtering data sets).
critical feature biases collaborative filtering interfere capacity
control terms rank, namely 2-norm regularization: instance, Koren
et al. (2009) adjust
terms (4) using squared error measure fit regular
ization term kPu k22 + kQi k22 + u2 + i2 , > 0 regularization parameter.
kind regularization weighted trace norm regularization PQT (Salakhutdinov
& Srebro, 2010).
Leaving aside
weighted part, idea convergence,
P
P
2
2
quantity
kQi k2 equal 2 times sum singular values
u kPu k2 +
matrix PQT . However, kk22 , regularization applied user
biases,

2 times singular value rank-one matrix 1 , equal Ikk2 ,
much larger kk22 . Thus, pattern user+item biases exists data,
weakly hidden stronger factors, less regularized others
model able capture it. Biases, allowed fit data
factors, offer opportunity relaxing control capacity parts
model translates gains patterns capture indeed useful
patterns generalization. Otherwise, ends relaxing capacity lead
overfitting.

fi ff

fi ff
bigram terms closely related trigram term: terms r`1 fieh1 r`2 fiet1

fi fi ffto trigram term adding constant features entities embeddings,

added
eh1 fiDfiet1 directly appropriate quadratic form. Thus, way gain
addition bigram terms ensure capture useful patterns,
also capacity control terms less strict trigram terms. tensor
factorization models, especially 3-way interaction models parameterizations
724

fiEmbedding Models Link Prediction KBs

(T), capacity control regularization individual parameters still well
understood, sometimes turns detrimental effective experiments.
effective parameter admissible rank embeddings, leads
conclusion bigram term really useful addition trigram term
higher-dimensional embeddings used. Hence, absence clear concrete way
effectively controlling capacity trigram term, believe different embedding
spaces used.
3.3.3 2-Way Interactions Entity Types+Similarity
part model expressive, less regularized (see Subsection
4.2) part useful patterns learn meaningful
prediction task hand. section, give motivation 2-way interaction
term task modeling multi-relational data.
relationships multi-relational data, knowledge bases like FB15k particular, strongly typed, sense well-defined specific subsets entities either heads tails selected relationships. instance, relationship like
capital expects (big) city head country tail valid relation. Large
knowledge bases huge amounts entities, belong many different types.
Identifying expected types head tail entities relationships, appropriate
granularity types (e.g. person artist writer), likely filter
ff 95%

fi ffthe

fi
entity set prediction. exact form first two terms r`1 fieh1 + r`2 fiet1
2-way interaction model (B), corresponds low-rank factorization per
bias matrices (head, label) (tail, label) head tail entities
embeddings, based assumption types entities predicted based
(learned) features, features predicting head-types
predicting tail-types. such, natural share entities embeddings first two
terms (B).

fi fi ff
last term, eh1 fiDfiet1 , intended account global similarity entities.
instance, capital France easily predicted looking city
strongest overall connections France knowledge base. country city may
strongly linked geographical positions, independent respective
types. diagonal matrix allows re-weight features embedding space
account fact features used describe types may
describe similarity objects different types. use diagonal
matrix strictly equivalent using general symmetric matrix place D.2
reason using symmetric matrix comes intuition direction many
relationships arbitrary (i.e. choice triples Paris capital France
rather France capital Paris), model invariant arbitrary
inversions directions relationships (in case inversion direction,
relations vectors r`1 r`2 swapped, parameters unaffected).
2. see equivalence taking eigenvalue decomposition symmetric
change
fi apply

fi D:
ff
basis embeddings keep diagonal part term eh1 fiDfiet1 , apply
reverse transformation vectors r`1 r`2 . Note since rotations preserve Euclidean distances,
equivalence still holds 2-norm regularization embeddings.

725

fiGarca-Duran, Bordes, Usunier & Grandvalet

Table 1: Scoring function several models literature. Capitalized letters
denote matrices lower cased ones, vectors.
Model
TransE
TransH
TransR
RESCAL
LFM

Score (s(h, `, t))
h
`


` fi h ||e
ff + r ` e ||t 2
` fi ` ff 2
h
`
fi
fi
||(e w
e fiw ff) + r
(e fi w
ff e w )||2
hfi
`
tfi
|| e M`
+fir
fi ffe M` ||2
hfi `fi
e
R

fi ` fi 0 ff
h fi ` fi ff
efi ` fi ff
h fi ` fi ff
fi R fi + e fi R fi z + zfi R fi e + e fi R fi e

tasks invariance desirable, diagonal matrix could replaced
arbitrary matrix.

4. Training
Training Tatec carried using stochastic gradient descent objective function
comprising data-fitting term regularization term. two terms decribed
details section.
4.1 Ranking Objective
use ranking objective function, designed give higher scores positive
triples (facts express true verified information KB) negative
ones (facts supposed express false information). negative triples
provided KB, often not, need process turn positive triples
corrupted ones carry discriminative training. simple approach consists
creating negative examples replacing one argument positive triple random
element. way simple efficient practice may introduce noise creating
wrong negatives.
Let set positive triples provided KB, optimize following ranking
loss function:
X
X


s(h, `, t) + s(h0 , `0 , t0 ) +
(6)
(h,`,t)S (h0 ,`0 ,t0 )C(h,`,t)

[z]+ = max(z, 0) C(h, `, t) set corrupted triples. Depending
application, set defined 3 different ways:
1. C(h, `, t) = {(h0 , `0 , t0 ) [[E]] L [[E]]|h0 6= h `0 6= ` t0 6= t}
2. C(h, `, t) = {(h0 , `, t0 ) [[E]] L [[E]]|h0 6= h t0 6= t}
3. C(h, `, t) = {(h, `0 , t) [[E]] L [[E]]|`0 6= `}
margin hyperparameter defines minimum gap score
positive triple negative ones. stochastic gradient descent performed
minibatch setting. epoch data set shuffled split disjoint minibatches
triples 1 2 (see next section) negative triples created every positive one.
use two different learning rates 1 2 , one Bigram one Trigram
model; kept fixed whole training.
726

fiEmbedding Models Link Prediction KBs

Algorithm 1 Learning unregularized Tatec.
input Training set = {(h, l, t)}, margin , learning rates 1 2
1: initialization
2:
- Bigram: e1 uniform( 6d , 6d ) entity e
1
1
3:
r1 , r2 uniform( 6d , 6d ) `
1
1
4:
uniform( 6d , 6d )
1
1
5:
- Trigram: e2 uniform( 6d , 6d ) entity e
2
2
6:
R uniform( 6d , 6d ) `
2
2
7:
- Tatec-ft: pre-trained weights Bigram Trigram
8: embeddings normalized 2- Frobenius-norm equal 1.
9: loop
10:
Sbatch sample(S, m) // sample training minibatch size
11:
Tbatch // initialize set pairs examples
12:
(h, `, t) Sbatch
13:
(h0 , `0 , t0 ) sample
according selected strategy C(h, `, t)
negative triple
14:
Tbatch Tbatch (h, `, t), (h0 , `0 , t0 ) // record pairs examples
15:
end
X


16:
Update parameters using gradients
s(h, `, t) + s(h0 , `0 , t0 ) + :

(h,`,t),(h0 ,`0 ,t0 ) Tbatch

17:
18:
19:
20: end

- Bigram (Eq. 2): = s1
- Trigram (Eq. 3): = s2
- Tatec-ft (Eq. 1): = s1 + s2
loop

interested Bigram Trigram terms Tatec capture different
data patterns, using random initialization weights may lead bad local minima
thus poor solution. Hence, first pre-train separately s1 (h, `, t) s2 (h, `, t),
use learned weights initialize full model. Training Tatec
hence carried two phases: (disjoint) pre-training either (joint) fine-tuning
Tatec-ft learning combination weights Tatec-lc. pre-training
fine-tuning stopped using early stopping validation set, follow training
procedure summarized Algorithm 1, unregularized case. Training
linear combination weights Tatec-lc stopped convergence L-BFGS.
4.2 Regularization
Previous work embedding models used two different regularization strategies: either
constraining entity embeddings have, most, 2-norm value e (Garca-Duran
et al., 2014) adding 2-norm penalty weights (Wang et al., 2014b; Lin et al.,
2015) objective function (6). former, denote hard regularization,
regularization performed projecting entity embeddings minibatch onto
2-norm ball radius e . latter, denote soft regularization, penalization term form [||e||22 2e ]+ entity embeddings e added. soft scheme
allows 2-norm embeddings grow e , penalty.
control large capacity relation matrices Trigram model,
adapted two regularization schemes: hard scheme, force relation matrices
727

fiGarca-Duran, Bordes, Usunier & Grandvalet

Table 2: Statistics data sets used paper extracted four knowledge
bases: FB15k, SVO, Kinships UMLS.
Data set
Entities
Relationships
Training examples
Validation examples
Test examples

FB15k
14,951
1,345
483,142
50,000
59,071

SVO
30,605
4,547
1,000,000
50,000
250,000

Kinships
104
26
224,973
28,122
28,121

UMLS
135
49
102,612
89,302
89,302

have, most, Frobenius norm value l , soft one, include penalization
term form [||R||2F 2l ]+ loss function (6) . result, soft scheme
following regularization
term added loss function (6): C1 [||e1 ||22 2e ]+ +C2 [||e2 ||22

2e ]+ + [||R||2F 2l ]+ , C1 C2 hyperparameters weight importance
soft constraint. terms practicality, bigger flexibility soft version comes
one hyperparameter. following, suffixes soft hard used refer
either regularization scheme. Tatec also implicit regularization
factor since using entity representation entity regardless role
head tail.
sum up, hard regularization case, optimization problem Tatec-ft is:
X
X


min
s(h, `, t) + s(h0 , `0 , t0 ) +
(h,`,t)S (h0 ,`0 ,t0 )C(h,`,t)

s.t.

||ei1 ||2 e

[[E]]

||ei2 ||2 e
`

||R ||F l

[[E]]
` [[L]]

soft regularization case is:
X
X
X
min
[ s(h, `, t) + s(h0 , `0 , t0 )]+ + C1
[||ei1 ||22 2e ]+
(h,`,t)S (h0 ,`0 ,t0 )C(h,`,t)

+ C2

X

[||ei2 ||22 2e ]+ +

i[[E]]

i[[E]]

X

[||R` ||2F 2l ]+



`[[L]]


fi ff
fi ff
fi fi ff
fi fi ff
s(h, `, t) = r`1 fieh1 + r`2 fiet1 + eh1 fiDfiet1 + eh2 fiR` fiet2 cases.

5. Experiments
section presents various experiments illustrate competitive Tatec respect several state-of-the-art models 4 benchmarks literature: UMLS, Kinships, FB15k SVO. statistics data sets given Table 2. versions
Tatec components Bigram Trigram compared state-ofthe-art models database.
5.1 Experimental Setting
section details protocols used various experiments.
728

fiEmbedding Models Link Prediction KBs

5.1.1 Datasets Metrics
experimental settings evaluation metrics borrowed previous works,
allow result comparisons.
UMLS/Kinships Kinships (Denham, 1973) KB expressing relational structure
kinship system Australian tribe Alyawarra, UMLS (McCray, 2003)
KB biomedical high-level concepts like diseases symptoms connected verbs like
complicates, affects causes. data sets, whole set possible triples,
positive negative, observed. used area precision-recall curve metric.
dataset split 10-folds cross-validation: 8 training, 1 validation
last one test. Since number available negative triples much bigger
number positive triples, positive ones fold replicated match number
negative ones.3 negative triples correspond first setting negative examples
Section 4.1. number training epochs fixed 100. Bigram, Trigram
Tatec models validated every 10 epochs using AUC precision-recall
curve validation criterion 1,000 randomly chosen validation triples - keeping
proportion negative positive triples. TransE, ran baseline,
validated every 10 epochs well.
FB15k Introduced Bordes et al. (2013b), data set subset Freebase,
large database generic facts gathering 1.2 billion triples 80 million entities.
evaluation it, used ranking metric. head test triple replaced
entities dictionary turn, score computed them.
scores sorted descending order rank correct entity stored.
procedure repeated removing tail instead head. mean
ranks mean rank, proportion correct entities ranked top 10
hits@10. called raw setting. setting correct positive triples ranked
higher target one hence counted errors. order reduce noise
measure, thus granting clearer view ranking performance, remove
positive triples found either training, validation testing set, except
target one, ranking. setting called filtered (Garca-Duran et al., 2014).
Since FB15k made positive triples, negative ones generated.
that, epoch generate two negative triples per positive replacing single
unit positive triple random entity (once head tail).
corruption approach implements prior knowledge unobserved triples likely
invalid, widely used previous work learning embeddings knowledge
bases words context language models. negative triples correspond
second setting negative examples Section 4.1. ran 500 training epochs
TransE, Bigram, Trigram Tatec, using final filtered mean rank
validation criterion. several models statistically similar filtered mean ranks, take
hits@10 secondary validation criterion.4 Since dataset, training, validation
3. replication process carried training.
4. Garca-Duran et al. (2014) previously reported results FB15k SVO TransE
Tatec. However, preliminary work, hyperparameters validated smaller validation
set wide enough grid search, led suboptimal results. hence decided re-run
algorithms got major improvements.

729

fiGarca-Duran, Bordes, Usunier & Grandvalet

test sets fixed, give confidence interval results, randomly split
test set 4 subsets computing evaluation metrics. 5 times,
finally compute mean standard deviation 20 values mean rank
hits@10.
SVO SVO database nouns connected verbs subject-verb-direct object
relations extracted Wikipedia articles. introduced Jenatton et al.
(2012). database perform verb prediction task, one assign
correct verb given two nouns acting subject direct object; words,
present results ranking label given head tail. FB15k, two ranking metrics
computed, mean rank hits@5%, proportion predictions
correct verb ranked top 5% total number verbs, within
top 5% 4,547 227. use raw setting SVO. Due different kind
task (predicting label instead predicting head /tail ), negative triples
generated replacing label random verb. negative triples correspond
third setting negative examples Section 4.1. TransE, Bigram Trigram
number epochs fixed 500 validated every 10 epochs.
Tatec ran 10 epochs, validated each. mean rank chosen
validation criterion 1,000 random validation triples.
5.1.2 Implementation
pre-train Bigram Trigram models validated learning rate
stochastic gradient descent among {0.1, 0.01, 0.001, 0.0001} margin among {0.1, 0.25,
0.5, 1}. radius e determining value L2 -norm entity embeddings penalized fixed 1, radius l Trigram model
validated among {0, 1, 5, 10, 20}. Due different size KBs, embedding dimension validated different ranges. SVO selected
among {25, 50}, among {50, 75, 100} FB15k among {10, 20, 40} UMLS
Kinships. soft regularization applied, regularization parameter
validated among {0, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100}. fine-tuning Tatec, learning
rates selected among values learning Bigram Trigram models
isolation, independent values chosen pre-training, margin
penalization terms C1 C2 soft regularization used. configurations
model selected using performance validation set given Appendix A.
Training combination weights Tatec-lc carried iterative way,
alternating optimization parameters via L-BFGS, update parameters using
`
` = P||||||k2|| , stopping criterion reached. parameters initialized
k

2

1 value validated among {0.1, 1, 10, 50, 100, 200, 500, 1000}.
5.1.3 Baselines
Variants performed breakdown experiments 2 different versions Tatec
assess impact various aspects. variants are:
Tatec-ft-no-pretrain: Tatec-ft without pre-training s1 (h, l, t) s2 (h, l, t).
730

fiEmbedding Models Link Prediction KBs

Table 3: Test AUC precision-recall curve UMLS Kinships
models literature (top) Tatec (bottom). Best performing methods bold.
Model
SME(linear)
RESCAL
LFM
TransE-soft
TransE-hard
Bigram-hard
Trigram-hard
Tatec-ft-hard
Bigram-soft
Trigram-soft
Tatec-ft-soft
Tatec-lc-soft

UMLS
0.983 0.003
0.98
0.990 0.003
0.734 0.033
0.706 0.034
0.936 0.020
0.980 0.006
0.984 0.004
0.936 0.018
0.983 0.004
0.985 0.004
0.985 0.004

Kinships
0.907 0.008
0.95
0.946 0.005
0.135 0.005
0.134 0.005
0.140 0.004
0.943 0.009
0.876 0.012
0.141 0.003
0.948 0.008
0.919 0.008
0.941 0.009

Tatec-ft-shared: Tatec-ft sharing entities embeddings s1 (h, l, t)
s2 (h, l, t) without pre-training.
experiments 3 versions Tatec performed soft regularization setting. hyperparameters chosen using grid above.
Previous Models retrained TransE hyperparameter grid
Tatec used running baseline datasets, using either soft hard
regularization. addition, display results best performing methods
literature dataset, values extracted original papers.
UMLS Kinships, also report performance 3-way models RESCAL,
LFM 2-way SME(linear). FB15k, recent variants TransE, TransH,
TransR cTransR (Lin et al., 2015) chosen main baselines.
TransH TransR/cTransR, optimal values hyperparameters dimension, margin learning rate selected within similar ranges
Tatec. SVO, compare Tatec three different approaches: Counts,
2-way model SME(linear) 3-way LFM. Counts based direct estimation
probabilities triples (head, label, tail) using number occurrences pairs (head,
label) (label, tail) training set. results models extracted
(Jenatton et al., 2012), followed experimental setting. Since results
paper available raw setting, restricted experiments
configuration SVO well.
5.2 Results
recall suffixes soft hard refer regularization scheme used, suffixes
ft lc combination strategy Tatec.
731

fiGarca-Duran, Bordes, Usunier & Grandvalet

Table 4: Test results FB15k SVO models literature (top), Tatec
(middle) variants (bottom). Best performing methods bold. filtered setting used
FB15k raw setting SVO.

Model
Counts
SME(linear)
LFM
TransH
TransR
cTransR
TransE-soft
TransE-hard
Tatec-no-pretrain
Tatec-shared
Bigram-hard
Trigram-hard
Tatec-ft-hard
Bigram-soft
Trigram-soft
Tatec-ft-soft
Tatec-lc-soft

FB15k
Mean Rank
Hits@10
87
64.4
77
68.7
75
70.2
50.7 2.0
71.5 0.3
50.6 2.0
71.5 0.3
97.1 3.9
65.7 0.2
94.8 3.2
63.4 0.3
94.5 2.9
67.5 0.4
137.7 7.1
56.1 0.4
59.8 2.6
77.3 0.3
87.7 4.1
70.0 0.2
121.0 7.2
58.0 0.3
57.8 2.3
76.7 0.3
68.5 3.2
72.8 0.2

SVO
Mean Rank Hits@5%
517.4
72
199.6
77
195
78
282.5 1.7
70.6 0.2
282.8 2.3
70.6 0.2
219.2 1.9
77.6 0.1
187.9 1.2
79.5 0.1
188.5 1.9
79.8 0.1
211.9 1.8
77.8 0.1
189.2 2.1
79.5 0.2
185.4 1.5
80.0 0.1
182.6 1.2
80.1 0.1

5.2.1 UMLS Kinships
results two knowledge bases provided Table 3. UMLS, models
performing well. combination Bigram Trigram models slightly
better Trigram alone significant. seems constituents
Tatec, Bigram Trigram, encode complementary information
combination bring much improvement. Basically, dataset, many
methods somewhat efficient best one, LFM. difference TransE
Bigram dataset illustrates potential impact diagonal matrix D,
constrain embeddings head tail entities triple similar.
Regarding Kinships, big gap 2-way models like TransE 3way models like RESCAL. cause deterioration comes peculiarity
positive triples KB: entity appears 104 times number entities
KB head connected 104 entities even once.
words, conditional probabilities P (head|tail) P (tail|head) totally uninformative.
important consequence 2-way models since highly rely
information: Kinships, interaction head-tail is, best, irrelevant, though practice
interaction may even introduce noise.
Due poor performance Bigram model, combined
Trigram model combination turn detrimental w.r.t. performance
Trigram isolation: 2-way models quite noisy KB cannot take
advantage them. side Trigram model logically reaches similar
732

fiEmbedding Models Link Prediction KBs

Table 5: Test results FB15k. Proportion entities ranked Top 1.
Model
TransE-soft
Bigram-soft
Trigram-soft
Tatec-ft-soft

Hits@1
28.1
27.2
24.9
37.8

performance RESCAL, similar LFM well. Performance Tatec versions
based fine-tuning parameters (Tatec-ft) worse Trigram
Bigram degrades model. Tatec-lc, using potentially sparse linear combination
models, drawback since completely cancel influence
bigram model. conclusion experiments KB, one
components Tatec quite noisy, directly remove Tatec-lc
automatically. soft regularization setting seems slightly better also.
5.2.2 FB15k
Table 4 (left) displays results FB15k. Unlike Kinships, 2-way models
outperform 3-way models mean rank hits@10. simplicity 2-way
models seems advantage FB15k: something already observed
Yang, Yih, He, Gao, Deng (2014a). combination Bigram Trigram
models Tatec leads impressive improvement performance, means
KB information encoded 2 models complementary. Tatec
outperforms existing methods except TransE mean rank wide margin
hits@10. Bigram-soft performs roughly like cTransR, better counterpart
Bigram-hard. Though Trigram-soft better Trigram-hard well, Tatec-ft-soft
Tatec-ft-hard converge similar performances. Fine-tuning parameters
better simply using linear combination even Tatec-lc still performs well.
Tatec-ft outperforms variants Tatec-shared Tatec-no-pretrain
wide margin, confirms pre-training use different embeddings
spaces essential properly collect different data patterns Bigram Trigram models: sharing embeddings constrain much model, without
pre-training Tatec able encode complementary information constituents.
performance Tatec cases in-between performances soft version
Bigram Trigram models, indicates converge solution
even able reach best performance constituent models. Table 5
displays Hits@1 several models. Whereas differences performance
Bigram Trigram large ones shown hits@10, Tatec still
best model wide margin.
also broke results type relation, classifying relationship according
cardinality head tail arguments. relationship considered 1-to-1,
1-to-M, M-to-1 M-M regarding variety arguments head given tail vice versa.
average number different heads whole set unique pairs (label, tail) given
relationship 1.5 considered 1, way around.
number relations classified 1-to-1, 1-to-M, M-to-1 M-M 353, 305, 380
733

fiGarca-Duran, Bordes, Usunier & Grandvalet

Table 6: Detailed results category relationship. compare Bigrams,
Trigram Tatec models terms Hits@10 (in %) FB15k filtered setting
models literature. (M. stands Many).
Task
Rel. category
TransE-soft
TransH
TransR
cTransR
Bigram-soft
Trigram-soft
Tatec-ft-soft

1-to-1
76.2
66.8
78.8
81.5
76.2
56.4
79.3

Predicting head
1-to-M. M.-to-1 M.-to-M.
93.6
47.5
70.2
87.6
28.7
64.5
89.2
34.1
69.2
89
34.7
71.2
90.3
37.4
70.1
79.6
30.2
57
93.2
42.3
77.2

1-to-1
76.7
65.5
79.2
80.8
75.9
53.1
78.5

Predicting tail
1-to-M. M.-to-1 M.-to-M.
50.9
93.1
72.9
39.8
83.3
67.2
37.4
90.4
72.1
38.6
90.1
73.8
44.4
89.8
72.8
28.8
81.6
60.8
51.5
92.7
80.7

307, respectively. results displayed Table 6. Bigram Trigram models
cooperate constructive way types relationship predicting
head tail. Tatec-ft remarkably better M-to-M relationships.
5.2.3 SVO
Tatec achieves also good performance task since outperforms previous
methods metrics. before, regularization strategies lead similar
performances, soft setting slightly better. terms hits@5%, Tatec outperforms constituents, however terms mean rank Bigram model considerably
worse Trigram Tatec. performance LFM Trigram
Bigram models, confirms fact sharing embeddings 2-
3-way terms actually prevent make best use types interaction.
Kinships, since performance Bigram much worse
Trigram, Tatec-lc competitive. seems Bigram Trigram
perform well different types relationships (such FB15k), combining
via fine-tuning (i.e. Tatec-ft) allows get best both; however, one
consistently performing worse relationships seems happen Kinships
SVO, Tatec-lc good choice since cancel influence bad
model. Table 7 depicts training times various models FB15k, presenting relative
time w.r.t. TransE one training epoch. speedup training, could follow one
several following strategies:
use adaptive learning rates order make convergence faster;
train model GPUs, quite usual deep learning community
working large datasets;
parallellize training Hogwild (Recht, Re, Wright, & Niu, 2011).
could also speed validation. example, since entities relationships
usually strongly typed (i.e. given relationship, subset entities real candidates
subject object), might consider entities suitable type
given relationship role. Nevertheless, given scalability major issue
datasets used paper look speed optimization here.
734

fiEmbedding Models Link Prediction KBs

Table 7: Relative training times respect TransE FB15k running one
epoch single core.
Model
Bigram-soft
Trigram-soft
Tatec-ft-soft

Relative train. time
1.4
3.6
4.0

5.3 Illustrative Experiments
last experimental section provides illustrations insights performance
Tatec TransE.
5.3.1 TransE Symmetrical Relationships
TransE peculiar behavior: performs well FB15k quite poorly
datasets. Looking detail FB15k, noticed database
made lot pairs symmetrical relationships /film/film/subjects
/film/film subject/films, /music/album/genre /music/genre/albums.
simplicity translation model TransE works well when, predicting validity
unknown triple, model make use symmetrical counterpart
present training set. Specifically, 45,817 59,071 test triples FB15k
symmetrical triple training set. split test triples two subsets, one
containing test triples symmetrical triple used learning
stage containing ones symmetrical triple exist
training set, overall mean rank TransE 50.7 decomposed mean
rank 17.5 165.7, overall hits@10 71.5 decomposed 76.6 53.7,
respectively. TransE makes adequate use particular feature. original
TransE paper (Bordes et al., 2013b), algorithm shown perform well FB15k
dataset extracted KB WordNet (Miller, 1995): suspect
WordNet dataset also contains symmetrical counterparts test triples training set
(such hyperonym vs hyponym, meronym vs holonym).
Tatec also make use information is, expected, much better relations
symmetrical counterparts train: FB15k, mean rank Tatec-ft-soft
17.5 relations symmetrical counterparts 197.4 instead hits@10 84.4%
instead 50%. Yet, results datasets show, Tatec also able generalize
complex information needs taken account.
5.3.2 Anecdotal Examples
examples predictions Tatec FB15k displayed Table 8. first
row, want know answer question location polish
national football team?; among possible answers find locations,
specifically countries, makes sense national team. question
topic film Remember titans? top-10 candidates may potential film topics. answers question religion Noam Chomsky
belong to? typed religions. examples, sides re735

fiGarca-Duran, Bordes, Usunier & Grandvalet

Table 8: Examples predictions FB15k. Given entity relation type
test triple, Tatec fills missing slot. bold expected correct answer.
Triple
(poland national football team, /sports team/location, ?)

(?, /film/film subject/films , remember titans)

(noam chomsky, /people/person/religion, ?)

(?, /webpage/category, official website)

Top-10 predictions
Mexico, South Africa, Republic Poland
Belgium, Puerto Rico, Austria, Georgia
Uruguay, Colombia, Hong Kong
racism, vietnam war, aviation, capital punishment
television, filmmaking, Christmas
female, english language, korean war
atheism, agnosticism, catholicism, ashkenazi jews
buddhism, islam, protestantism
baptist, episcopal church, Hinduism
supreme court canada, butch hartman, robyn hitchcoc, mercer university
clancy brown, dana delany, hornets
grambling state university, dnipropetrovsk, juanes

60

Singers
Japanese singers
British MPNS
Glee casting
Attorneys USA

40

Singers
Japanese singers
British MPNS
Glee casting
Attorneys USA

60

40
20

20
0

0
20

20
40

40
60
60

40

20

0

20

40

60

80

40

(a) Embeddings Trigram

30

20

10

0

10

20

30

40

50

(b) Embeddings Bigram

Figure 4: Embeddings obtained Trigram Bigram models projected
2-D using t-SNE. MPNS stands Main Profession Singer.
lationship clearly typed: certain type entity expected head tail (country,
religion, person, movie, etc.). operators Tatec may operate specific regions
embedding space. contrary, relationship /webpage/category example non-typed relationship. one, could actually seen attribute rather
relationship, indicates entity head topic website official website.
Since many types entities webpage little correlation among
relationships, predicting left-hand side argument nearly impossible.
Figures 4a 4b show 2D projections embeddings selected entities Trigram Bigram models trained FB15k, respectively, obtained projecting
using t-SNE (Van der Maaten & Hinton, 2008). projection carried
Freebase entities whose profession either singer attorney USA.
observe Figure 4a attorneys clustered separated singers, except
one, corresponds multifaceted Fred Thompson5 . However, embeddings
singers clearly clustered: since singers appear multitude triples,
layout result compendium (sometimes heterogeneous) categories. illustrate
graphically different data patterns Bigram Trigram respond, focus
small cluster made Japanese singers seen Figure 4a (Trigram).
Figure 4b (Bigram) however, entities diluted whole set
5. Apart attorney, actor, radio personality, lawyer politician

736

fiEmbedding Models Link Prediction KBs

Table 9: Examples predictions SVO. Given two nouns acting subject direct
object test triple, Tatec predicts best fitting verb. bold expected correct answer.
Triple
(bus, ?

, service)

(emigrant, ?

, country)

(minister, ?, protest)
(vessel, ?, coal)
(tv channel, ?, video)
(great britain, ?, north america)

Top-10 predictions
use, provide, run, have, include
carry, offer, enter, make, take
flee, become, enter, leave, form
dominate, establish, make, move, join
lead, organize, join, involve, make
participate, conduct, stag, begin, attend
use, transport, carry, convert, send
make, provide, supply, sell, contain
feature, make, release, use, produce
have, include, call, base, show
include, become, found, establish, dominate
name, have, enter, form, run

singers. Looking neighboring embeddings Japanese singers entities Figure
4b, find entities highly connected japan like yoko ono born Japan, vic mignogna,
greg ayres, chris patton laura bailey worked dubbing industry
Japanese anime movies television series. shows impact interaction
heads tails Bigram model: tends push together entities connected
triples whatever relation. case, forms Japanese cluster.
Table 9 shows examples predictions SVO. first example, though run
target verb pair (bus, service), verbs like provide offer good
matches well. Similarly, non-target verbs like establish join, lead, participate
attend good matches second third examples ((emigrant, country)
(minister, protest)) respectively. fourth fifth instances show example
heterogeneous performance relationship (the target verb transport
cases) easily explained semantic point view: transport
good fit given pair (vessel, coal), whereas TV channel transports video
natural way express one watch videos TV channel, hence
leads poor performance target verb ranked #696. sixth example
particularly interesting, since even target verb, colonize, ranked far
list (#344), good candidates pair (Great Britain, North America) found
top-10. similar representation colonize,
almost synonyms, ranked much higher. effect verb frequency.
illustrated Figure 5a, frequent relationship is, higher Frobenius
norm is; hence, verbs similar meanings unbalanced frequencies ranked
differently, explains rare verb, colonize, ranked much worse
semantically similar words. consequence relation Frobenius
norm appearance frequency usual verbs tend highly ranked even though
sometimes good matches, due influence norm score.
figure, see Frobenius norm relation matrices larger
regularized (soft) case unregularized case. happens fixed
large value C2 l regularized case (e fixed 1). imposes
strong constraint norm entities relationship matrices
makes Frobenius norm matrices absorb whole impact norm
737

fiGarca-Duran, Bordes, Usunier & Grandvalet

250

5000

Regularized (soft)
Unregularized
Regularized (hard)

4000

Filtered Mean Rank

200

||Rl||2F

150

100

50

3000

2000

1000

0

0 0
10

1

2

10

10

3

10

4

10

1000 0
10

5

10

1

10

2

10

3

10

4

10

5

10

Number triples

Number triples

(a) Frobenius norm rel. matrices according (b) Test mean rank according number
training triples relationship.
number training triples rel.

Figure 5: Indicators behavior Tatec-ft FB15k according number
training triples relationship.
Table 10: Examples predictions SVO regularized unregularized
Trigram. bold expected correct answer.
Triple
(bus, ?

, service)

(emigrant, ?

, country)

(minister, ?

, protest)

(vessel, ?

, coal)

(tv channel, ?, video)
(great britain, ?, north america)

Top-10 predictions
Unregularized
Regularized (soft)
use, operate, offer, call, build,
provide, use, have, include, make,
include, have, know, make, create
offer, take, carry, serve, run
use, represent, save, flee, visit,
flee, become, come, enter, found,
come, make, leave, create, know
include, form, make, leave, join
bring, lead, reach, have, become, lead, organize, conduct, participate, join
say, include, help, leave, appoint
make, involve, support, suppress, raise
take, use, have, carry, make,
use, transport, make, carry, deliver,
hold, move, become, fill, serve
send, contain, supply, leave, provide
make, include, write, know, have,
release, make, feature, produce, have,
produce, use, play, give, become
include, use, take, show, base
have, use, include, make, leave,
include, found, become, run,name,
become, know, take, call, build
move, annex, form, establish, dominate

score, and, thus, impact verb frequency. could down-weight importance
verb frequency tuning parameters l C2 enforce stronger constraint.
Figure 10 shows effect verb frequency two models predicting
missing verb Table 9.
Breaking performance relationship, translated strong relation
performance relationship frequency (see Figure 5b). However,
relation 2-norm entities embeddings frequency
observed, explained given entity appear left right
argument unbalanced way.

6. Conclusion
paper presents Tatec, tensor factorization method satisfactorily combines 2and 3-way interaction terms obtain performance better best either constituent. Different data patterns properly encoded thanks use different embedding spaces two-phase training (pre-training fine-tuning/linear-combination).
Experiments four benchmarks different tasks different quality measures
738

fiEmbedding Models Link Prediction KBs

prove strength versatility model, whose scoring function, argue Section 3.1, tightly connected energy-based model literature TransE,
RESCAL LFM. experiments also allow us draw conclusions
two usual regularization schemes used far embedding-based models:
achieve similar performances, even soft regularization appears slightly efficient
one extra-hyperparameter.
work uses FB15k main testbed model tied KB schema;
long data formatted triples, Tatec applied. Hence, could directly
applied many Linked Data settings especially RDF format.

Acknowledgments
work carried framework Labex MS2T (ANR-11-IDEX-0004-02),
funded French National Agency Research (EVEREST-12-JS02-005-01).
Part work done Nicolas Usunier Sorbonne universites, Universite
de technologie de Compiegne, CNRS, Heudiasyc UMR 7253.

Appendix A. Optimal Hyperparameters
optimal configurations UMLS are:
- TransE-soft: = 40, = 0.01, = 0.5, C = 0;
- Bigram-soft: d1 = 40, 1 = 0.01, = 0.5, C = 0.1;
- Trigram-soft: d2 = 40, 2 = 0.01, = 1, C = 0.1, l = 5;
- Tatec-soft: d1 = 40, d2 = 40, 1 = 2 = 0.001, = 1, C1 = C2 = 0.01, l = 5;
- TransE-hard: = 40, = 0.01, = 0.1;
- Bigram-hard: d1 = 40, 1 = 0.01, = 0.5;
- Trigram-hard: d2 = 40, 2 = 0.01, = 1, l = 10;
- Tatec-hard: d1 = 40, d2 = 40, 1 = 2 = 0.001, = 1, l = 10.
- Tatec-linear-comb: d1 = 40, d2 = 40, = 0.5, = 50.
optimal configurations Kinships are:
- TransE-soft: = 40, = 0.01, = 1, C = 0;
- Bigram-soft: d1 = 40, 1 = 0.01, = 1, C = 1;
- Trigram-soft: d2 = 40, 2 = 0.01, = 0.5, C = 0.1, l = 5;
- Tatec-soft: d1 = 40, d2 = 40, 1 = 2 = 0.001, = 1, C1 = 100, C2 = 0.0001, l = 10;
- TransE-hard: = 40, = 0.01, = 1;
- Bigram-hard: d1 = 40, 1 = 0.01, = 1;
- Trigram-hard: d2 = 40, 2 = 0.01, = 0.5, l = 10;
- Tatec-hard d1 = 40, d2 = 40, 1 = 2 = 0.001, = 1, l = 10.
- Tatec-linear-comb: d1 = 40, d2 = 40, = 1, = 10.
optimal configurations FB15k are:
- TransE-soft: = 100, = 0.01, = 0.25, C = 0.1;
- Bigram-soft: d1 = 100, 1 = 0.01, = 1, C = 0;
739

fiGarca-Duran, Bordes, Usunier & Grandvalet

-

Trigram-soft: d2 = 50, 2 = 0.01, = 0.25, C = 0.001, l = 1;
Tatec-soft: d1 = 100, d2 = 50, 1 = 2 = 0.001, = 0.5, C1 = C2 = 0;
TransE-hard: = 100, = 0.01, = 0.25;
Bigram-hard: d1 = 100, 1 = 0.01, = 0.25;
Trigram-hard: d2 = 50, 2 = 0.01, = 0.25, l = 5;
Tatec-hard: d1 = 100, d2 = 50, 1 = 2 = 0.001, = 0.25, l = 5;
Tatec-no-pret: d1 = 100, d2 = 50, 1 = 2 = 0.01, = 0.25, C1 = 0, C2 = 0.001, l = 1;
Tatec-shared: d1 = d2 = 75, 1 = 2 = 0.01, = 0.25, C1 = C2 = 0.001, l = 5;
Tatec-linear-comb: d1 = 100, d2 = 50, = 0.25, = 200.

optimal configurations SVO are:
- TransE-soft: = 50, = 0.01, = 0.5, C = 1;
- Bigram-soft: d1 = 50, 1 = 0.01, = 1, C = 0.1;
- Trigram-soft: d2 = 50, 2 = 0.01, = 1, , C = 10, l = 20;
- Tatec-soft: d1 = 50, d2 = 50, 1 = 2 = 0.0001, = 1, C1 = 0.1, C2 = 1, l = 20;
- TransE-hard: = 50, = 0.01, = 0.5;
- Bigram-hard: d1 = 50, 1 = 0.01, = 1;
- Trigram-hard: d2 = 50, 2 = 0.01, = 1, l = 20;
- Tatec-hard: d1 = 50, d2 = 50, 1 = 2 = 0.0001, = 1, l = 20.
- Tatec-linear-comb: d1 = 50, d2 = 50, = 1, = 50.

References
Ashburner, M., Ball, C. A., Blake, J. A., Botstein, D., Butler, H., Cherry, J. M., Davis,
A. P., Dolinski, K., Dwight, S. S., Eppig, J. T., et al. (2000). Gene ontology: tool
unification biology. http://geneontology.org/.
Bizer, C., Heath, T., Idehen, K., & Berners-Lee, T. (2008). Linked data web
(ldow2008). http://linkeddata.org/.
Bollacker, K., Evans, C., Paritosh, P., Sturge, T., & Taylor, J. (2008). Freebase:
collaboratively created graph database structuring human knowledge. http:
//www.freebase.com.
Bordes, A., Glorot, X., Weston, J., & Bengio, Y. (2013a). semantic matching energy
function learning multi-relational data. Machine Learning, 94, 233259.
Bordes, A., Usunier, N., Garca-Duran, A., Weston, J., & Yakhnenko, O. (2013b). Translating embeddings modeling multi-relational data. Advances Neural Information
Processing Systems, pp. 27872795.
Chang, K.-W., Yih, W.-t., Yang, B., & Meek, C. (2014). Typed tensor decomposition
knowledge bases relation extraction. Proceedings 2014 Conference
Empirical Methods Natural Language Processing (EMNLP), pp. 15681579.
Denham, W. (1973). detection patterns Alyawarra nonverbal behavior. Ph.D.
thesis, University Washington.
Dong, X., Gabrilovich, E., Heitz, G., Horn, W., Lao, N., Murphy, K., Strohmann, T., Sun,
S., & Zhang, W. (2014). Knowledge vault: web-scale approach probabilistic
740

fiEmbedding Models Link Prediction KBs

knowledge fusion. Proceedings 20th ACM SIGKDD international conference
Knowledge discovery data mining, pp. 601610. ACM.
Garca-Duran, A., Bordes, A., & Usunier, N. (2014). Effective blending two threeway interactions modeling multi-relational data. ECML PKDD 2014. Springer
Berlin Heidelberg.
Gardner, M., Talukdar, P. P., Krishnamurthy, J., & Mitchell, T. (2014). Incorporating
vector space similarity random walk inference knowledge bases. Conference
Empirical Methods Natural Language Processing, EMNLP 2014, pp. 397406.
Jenatton, R., Le Roux, N., Bordes, A., & Obozinski, G. (2012). latent factor model
highly multi-relational data. NIPS 25.
Kemp, C., Tenenbaum, J. B., Griffiths, T. L., Yamada, T., & Ueda, N. (2006). Learning
systems concepts infinite relational model. Proc. 21st national
conf. Artif. Intel. (AAAI), pp. 381388.
Knight, K., & Luk, S. K. (1994). Building large-scale knowledge base machine translation. AAAI, Vol. 94, pp. 773778.
Kok, S., & Domingos, P. (2007). Statistical predicate invention. Proceedings 24th
international conference Machine learning, ICML 07, pp. 433440.
Koren, Y., Bell, R., & Volinsky, C. (2009). Matrix factorization techniques recommender
systems. Computer, 42 (8), 3037.
Krompass, D., Baier, S., & Tresp, V. (2015). Type-constrained representation learning
knowledge graphs. arXiv preprint arXiv:1508.02593.
Lao, N., Mitchell, T., & Cohen, W. W. (2011). Random walk inference learning
large scale knowledge base. Proceedings Conference Empirical Methods
Natural Language Processing, pp. 529539. Association Computational Linguistics.
Lin, Y., Liu, Z., Sun, M., Liu, Y., & Zhu, X. (2015). Learning entity relation embeddings
knowledge graph completion. Proceedings AAAI15.
McCray, A. T. (2003). upper level ontology biomedical domain. Comparative
Functional Genomics, 4, 8088.
Miller, G. (1995). WordNet: Lexical Database English. Communications ACM,
38 (11), 3941.
Navigli, R., & Velardi, P. (2005). Structural semantic interconnections: knowledge-based
approach word sense disambiguation. Pattern Analysis Machine Intelligence,
IEEE Transactions on, 27 (7), 10751086.
Nickel, M., Tresp, V., & Kriegel, H.-P. (2011). three-way model collective learning
multi-relational data. Proceedings 28th International Conference Machine
Learning (ICML-11), pp. 809816.
Ponzetto, S. P., & Strube, M. (2006). Exploiting semantic role labeling, wordnet
wikipedia coreference resolution. Proceedings main conference Human
Language Technology Conference North American Chapter Association
Computational Linguistics, pp. 192199. Association Computational Linguistics.
741

fiGarca-Duran, Bordes, Usunier & Grandvalet

Recht, B., Re, C., Wright, S., & Niu, F. (2011). Hogwild: lock-free approach parallelizing stochastic gradient descent. Advances Neural Information Processing
Systems 24, pp. 693701.
Salakhutdinov, R., & Srebro, N. (2010). Collaborative filtering non-uniform world:
Learning weighted trace norm. tc (X), 10, 2.
Socher, R., Chen, D., Manning, C. D., & Ng, A. Y. (2013). Reasoning Neural Tensor Networks Knowledge Base Completion. Advances Neural Information
Processing Systems 26.
Sutskever, I., Salakhutdinov, R., & Tenenbaum, J. (2009). Modelling relational data using
bayesian clustered tensor factorization. Adv. Neur. Inf. Proc. Syst. 22.
Van der Maaten, L., & Hinton, G. (2008). Visualizing data using t-sne. Journal Machine
Learning Research, 9 (2579-2605), 85.
Wang, Y. J., & Wong, G. Y. (1987). Stochastic blockmodels directed graphs. Journal
American Statistical Association, 82 (397).
Wang, Z., Zhang, J., Feng, J., & Chen, Z. (2014a). Knowledge graph text jointly
embedding. Proceedings 2014 Conference Empirical Methods Natural
Language Processing (EMNLP).
Wang, Z., Zhang, J., Feng, J., & Chen, Z. (2014b). Knowledge graph embedding translating hyperplanes. Proceedings Twenty-Eighth AAAI Conference
Artificial Intelligence, pp. 11121119.
Wolfram Research, I. (2009). Wolfram alpha. http://www.wolframalpha.com.
Yang, B., Yih, W.-t., He, X., Gao, J., & Deng, L. (2014a). Learning multi-relational semantics using neural-embedding models. CoRR, abs/1411.4072.
Yang, M.-C., Duan, N., Zhou, M., & Rim, H.-C. (2014b). Joint relational embeddings
knowledge-based question answering. Proceedings 2014 Conference
Empirical Methods Natural Language Processing (EMNLP), pp. 645650.
Zhang, J., Salwen, J., Glass, M., & Gliozzo, A. (2014). Word semantic representations using
bayesian probabilistic tensor factorization. Proceedings 2014 Conference
Empirical Methods Natural Language Processing (EMNLP).

742

fiJournal Artificial Intelligence Research 55 (2016) 11351178

Submitted 12/15; published 04/16

Exploiting Causality Selective Belief Filtering
Dynamic Bayesian Networks
Stefano V. Albrecht

svalb@cs.utexas.edu

Department Computer Science
University Texas Austin
Austin, TX 78712, USA

Subramanian Ramamoorthy

s.ramamoorthy@ed.ac.uk

School Informatics
University Edinburgh
Edinburgh, EH8 9AB, UK

Abstract
Dynamic Bayesian networks (DBNs) general model stochastic processes
partially observed states. Belief filtering DBNs task inferring belief state (i.e.
probability distribution process states) based incomplete noisy observations.
hard problem complex processes large state spaces. article,
explore idea accelerating filtering task automatically exploiting causality
process. consider specific type causal relation, called passivity, pertains
state variables cause changes variables. present Passivity-based
Selective Belief Filtering (PSBF) method, maintains factored belief representation
exploits passivity perform selective updates belief factors. PSBF produces
exact belief states certain assumptions approximate belief states otherwise,
approximation error bounded degree uncertainty process. show
empirically, synthetic processes varying sizes degrees passivity, PSBF
faster several alternative methods achieving competitive accuracy. Furthermore,
demonstrate passivity occurs naturally complex system multi-robot
warehouse, PSBF exploit accelerate filtering task.

1. Introduction
Dynamic Bayesian networks (DBNs) (Dean & Kanazawa, 1989) general model
stochastic processes partially observed states. topology DBN compact
specification variables process interact transitions (cf. Figure 1). Given
possible incompleteness noise observations, may generally possible
infer state process absolute certainty. Instead, may infer beliefs
process state based history observations, form probability distribution
state space process. often called belief state task calculating
belief states commonly referred belief filtering.
number exact approximate inference methods exist Bayesian networks (see,
e.g., Koller & Friedman, 2009; Pearl, 1988) used filtering DBNs,
applying unrolled DBN + 1 slice repeated observed
time step, via successive update current posterior (belief state) used
c
2016
AI Access Foundation. rights reserved.

fiAlbrecht & Ramamoorthy

xt1

xt+1
1

y1t+1

xt2

xt+1
2

y2t+1



t+1

Figure 1: Example dynamic Bayesian network (DBN) two state variables two
observation variables. xti xt+1
variables represent process states time

+ 1, respectively, yit+1 variables (shaded) represent observation time + 1.
arrows describe variables interact.

prior next time step (see also Murphy, 2002). However, clear
unrolled variant becomes intractable network grows unboundedly time. Even
successive update, exact methods become intractable high-dimensional process
states approximate methods may propagate growing errors time. Therefore, filtering
methods developed utilise special structure DBNs maintain errors
propagated time. (We defer detailed discussion methods Section 2.)
Often, key developing efficient filtering methods identify structure
process leveraged inference. article, interested application
DBNs representations actions partially observed decision processes,
POMDPs (Kaelbling, Littman, & Cassandra, 1998; Sondik, 1971) many variants.
DBNs used represent effects actions decision process, specifying
variables interact information decision maker observes. many cases,
decision processes exhibit high degrees causal structure (Pearl, 2000), mean
change one part process may cause change another part. experience
processes causal structure may used make filtering task
tractable, tell us beliefs need revised certain aspects
process state. example, variable x2 Figure 1 changes value variable x1
changed value (i.e. change x1 causes change x2 ), seems intuitive use
causal relation deciding whether revise ones belief x2 . Unfortunately,
current filtering methods take causal structure account.
refer type causal relation (between x1 x2 ) passivity. Intuitively,
say state variable xi passive given action if, executing action,
subset state variables directly affect xi (i.e. xi parents DBN)
xi may change value least one variables subset changed
value. worth pointing passivity occurs naturally frequently many
planning domains, especially robotic physical systems (Mainzer, 2010).
following example1 illustrates simple robot arm:

1. mark end example solid black square.

1136

fiExploiting Causality Selective Belief Filtering DBNs

3
2

2

1

3



XA

B

XB

1

(a) Robot arm gripper

C
(b) Holding blocks B

Figure 2: Robot arm three rotational joints gripper. variables represent
absolute orientations corresponding joints.
Example 1 (Robot arm). Consider robot arm three rotational joints gripper,
shown Figure 2a. joints denoted 1 , 2 , 3 may take values
discrete set {0 , 1 , ..., 359 } indicate absolute orientations (e.g. = 0 means
joint points exactly right, = 180 means points left).
joint i, let two actions CWi CCWi rotate joint 1 clockwise
counter-clockwise, respectively. uncertainty system could due stochastic
joint movements unreliable sensor readings joint orientations.
action CWi CCWi , variable passive value directly
modified action. However, variables j6=i passive change
values corresponding preceding variable j1 changed value, since changed
orientation joint j 1 causes changed orientation joint j (recall orientations
absolute). Note also accounts chains causal effects, indicated
arrows: orientation joint 3 changes orientation joint 1 changes, since joint
1 causes joint 2 change, turn causes joint 3 change.
examples passivity seen context object manipulation,
blocks planning domain (e.g. Pasula, Zettlemoyer, & Kaelbling, 2007). Figure 2b
shows arm holding blocks B A, top B. Here, position B (XB )
passive respect joint orientations since change orientations
changed. Furthermore, causal chain joint orientations position
block (XA ), since position change Bs position changes.

passivity exploited accelerate filtering task example?
fact state variables passive means aspects state may remain
unchanged, depending action choose. example, choose rotate joint
3, fact joints 1 2 passive means unaffected action.
Thus, seems redundant revise beliefs orientations joints 1 2. However,
precisely current filtering methods (cf. Section 2).
concretely, assume use factored belief representation P (1 , 2 , 3 ) = P (1 , 2 )
P (2 , 3 ) choose rotate 3 direction. Then, easy see need
update factor P (2 , 3 ), since 3 changes value, factor P (1 , 2 ), since
variables 1 , 2 passive. Since parents 1 , 2 (if any) change
values, know 1 , 2 change values either. show later, skipping
1137

fiAlbrecht & Ramamoorthy

P (1 , 2 ) result loss information cases, similarly chains
causal connections (cf. Example 1). complex example planning domain
involving passivity, exploited, discussed Section 6.2.
addition guiding belief revision, several features make passivity
interesting example causal relation: First all, passivity latent causal relation,
meaning readily extracted process dynamics without additional
annotation expert. (In Section 4, give procedure identifies passive variables
based conditional probability tables.) Furthermore, passivity deterministic
relation since passive variables may stochastic behaviour changing
values. Finally, passivity relatively simple example causal relation, idea
exploiting passivity order accelerate filtering task intuitive. Yet, best
knowledge, formalised explored rigorously before.
purpose present article formalise evaluate idea automatically
exploiting causal structure efficient belief filtering DBNs, using passivity concrete
example causal relation. Specifically, hypothesis large processes
high degrees passivity, structure exploited accelerate filtering task.
discussing related work Section 2 technical preliminaries Section 3,
contributions grouped following parts:
Section 4, give formally concise definition passivity discuss various
aspects definition. definition assumes decision process specified
set dynamic Bayesian networks (one action). also discuss nonexample passivity, mean variables appear passive really
passive. Finally, give simple procedure detect passive variables
based conditional probability tables.
Section 5, present Passivity-based Selective Belief Filtering (PSBF) method.
Following idea outlined above, PSBF uses factored belief representation
belief factors defined clusters correlated state variables. PSBF follows
2-step update procedure wherein belief state first propagated
process dynamics (the transition step) conditioned observation (the
observation step). interesting novelty PSBF way performs
transition step: rather updating belief factors, PSBF updates
factors whose variables suspects changed, possible exploiting
passivity (to made precise shortly). Similarly, observation step, PSBF updates
belief factors determines structurally connected
observation, uses parts observation relevant
belief factor, thus allowing efficient incorporation observations. PSBF
produces exact belief states certain assumptions approximate belief states
otherwise. also discuss computational complexity error bounds PSBF.
Section 6, evaluate PSBF two experimental domains: first evaluate PSBF
synthetic (i.e. randomly generated) processes varying sizes degrees passivity.
process sizes vary one thousand one trillion states, passivity
degrees vary 25% 100% passivity. results show PSBF faster
several alternative methods maintaining competitive accuracy. particular,
1138

fiExploiting Causality Selective Belief Filtering DBNs

results indicate computational gains grow significantly degree
passivity size process. evaluate PSBF complex simulation
multi-robot warehouse system style Kiva (Wurman, DAndrea, & Mountz,
2008). show passivity occurs system PSBF exploit
accelerate filtering task, outperforming alternative methods.
Finally, discuss strengths weaknesses PSBF Section 7, conclude
work Section 8. proofs found appendix.

2. Related Work
exists substantial body work belief filtering partially observed stochastic
processes. section, review filtering methods utilise special structure
DBNs situate work within related literature.
2.1 Approximate Belief Filtering DBNs
Several authors proposed filtering methods wherein belief state represented set
state samples. Specifically, probability process state normalised
frequency state samples correspond s. methods commonly
referred particle filters (PF); see work Doucet, de Freitas, Gordon (2001)
survey. common variant PF (Gordon, Salmond, & Smith, 1993), filtering
task consists propagating current state samples process dynamics
subsequent resampling step based probabilities new state samples
would produced observation. Two interesting features PF
applied processes discrete continuous variables, approximation
error converges zero increase number state samples.
known problem PF fact number samples needed acceptable
approximations grow drastically variance process dynamics (as shown
experiments; cf. Section 6). Rao-Blackwellised PF (RBPF) (Doucet, De Freitas,
Murphy, & Russell, 2000) developed address problem. RBPF assumes
state variables grouped sets R X distribution X
efficiently calculated R filtering. Hence, sample RBPF consists
sample R corresponding marginal distribution X. RBPF useful
variance R relatively low variance X high, since reduces number
samples needed acceptable approximations.
Boyen Koller (1999, 1998) recognised process consists several independent
weakly interacting subcomponents, belief state represented efficiently
product smaller beliefs individual subcomponents. seminal contribution show approximation error due factored representation essentially
bounded degree uncertainty (or mixing rates) process. precisely,
prove relative entropy (or KL divergence; Kullback & Leibler, 1951) two belief states contracts exponential rate propagated stochastic transition
process. Based observation, propose filtering method (BK) wherein belief
state represented factored form belief factors updated using exact inference method, junction tree algorithm (Lauritzen & Spiegelhalter, 1988). Since
1139

fiAlbrecht & Ramamoorthy

internal cliques used junction tree algorithm may correspond belief
state representation BK, final projection step typically performed
original factorisation restored. performance method depends crucially whether relevant correlations state variables captured small
clusters, whether projection step performed efficiently.
Factored particle filtering (FP) (Ng, Peshkin, & Pfeffer, 2002) addresses main drawbacks PF (many samples needed) BK (small clusters required) approximating
belief factors using set factored state samples. samples factored sense
assign values variables corresponding factor. allows FP
represent belief factors large BK, reduces number samples
needed due smaller number variables factor. authors provide different methods updating factored state samples, generic idea first perform
join operation full state samples reconstructed factored samples,
updated standard PF. updated samples projected
factored form using project operation. main drawback FP join
project operations essentially correspond standard relational database operations,
expensive.
Murphy Weiss (2001) propose filtering method called factored frontier (FF). FF
uses fully factored representation belief states; is, belief state product
marginals individual state variable. allows compact representation
beliefs. algorithm works moving set state variables (the frontier) forward
backward DBN topology. requires certain variable ordering,
difficult attain intra-correlations state variables (i.e. edges within + 1
slice DBN) allowed. authors show method equivalent single
iteration loopy belief propagation (LBP) (Pearl, 1988). Thus, similar LBP, FF
applied successive iterations improve approximation accuracy.
None works discussed explicitly address question causal relations
state variables exploited accelerate filtering task, or, alternatively,
filtering methods proposed therein implicitly benefit causal structure. method,
PSBF, related BK FP PSBF, too, uses factored belief representation,
belief factors defined clusters correlated state variables. Therefore,
analysis approximation errors Boyen Koller (1998) also applies PSBF,
show Section 5 well experiments. However, contrast BK FP, PSBF
perform inference complete factorisation, rather individual
factors. consequence, PSBF require join project operation, one
main disadvantages BK FP.
2.2 Belief Filtering Decision Processes
methods discussed preceding subsection used belief filtering decision
processes, including POMDPs (Kaelbling et al., 1998; Sondik, 1971). regard,
methods viewed pure filters concerned belief filtering
control decision process. contrast combined filtering
methods, interleave filtering control tasks decision processes make
specific assumptions regarding solutions thereof. exists large body literature
1140

fiExploiting Causality Selective Belief Filtering DBNs

combined methods, including reachability-based methods (Hauskrecht, 2000; Washington,
1997), grid-based methods (Zhou & Hansen, 2001; Brafman, 1997; Lovejoy, 1991), pointbased methods (Smith & Simmons, 2005; Pineau, Gordon, & Thrun, 2003), compression
methods (Roy, Gordon, & Thrun, 2005; Poupart & Boutilier, 2002).
potential advantage combined methods access additional
structure may, therefore, utilise synergies filtering control tasks. One
synergy use decision quality guide belief filtering, rather metrics
relative entropy. Poupart Boutilier (2001, 2000) propose filtering method, called
value-directed approximation, chooses different approximation schemes different
decisions minimise expected loss decision quality (i.e. accumulated rewards).
method assumes POMDP solved exactly value function
provided form -vectors represent available actions POMDP.
Based value function, algorithm computes switching set alternative
plans determine error bounds approximation schemes. used search
optimal approximation scheme tree-based manner, search traverses
approximate exact schemes.
idea using decision quality guide belief filtering appealing, method
involves series optimisation problems exhaustive tree search,
costly complex systems. advantage pure filtering methods, including proposed
method PSBF, filter processes complex combined methods,
multi-robot warehouse system studied Section 6. actual control task
done via domain-specific solutions (cf. Section 6.2.1).
2.3 Substructure Parameterisation
Bayesian networks, hence DBNs, allow compact parameterisation (i.e. specification
probabilities) efficient inference via conditional independence relations. addition,
considerable work identifying substructure parameterisation
simplify knowledge acquisition enhance inference (Koller & Friedman, 2009;
Boutilier, Dean, & Hanks, 1999). property studied work, passivity, one example
substructure parameterisation. notable examples include causal independence (e.g. Heckerman & Breese, 1994; Heckerman, 1993) context-specific independence
(Boutilier, Friedman, Goldszmidt, & Koller, 1996).
Causal independence assumption effects individual causes common
variable (i.e. parents variable) independent one another. allows
compact parameterisation via operators noisy-or (Srinivas, 1993; Pearl, 1988),
used enhance inference (Zhang & Poole, 1996). Note passivity
conceptually much simpler property causal independence, passivity neither
concerned strength individual causes extent depend
other. Moreover, passivity read directly parameterisation (cf. Section 4.3)
whereas causal independence usually imposed designer.
Context-specific independence (CSI) property states variable independent parents given certain assignment values (i.e. context)
parents. Non-local CSI statements follow similarly d-separation (Geiger, Verma,
& Pearl, 1989). allow reduction parameters (Boutilier et al., 1996)
1141

fiAlbrecht & Ramamoorthy

enhancement inference (Poole & Zhang, 2003). discuss Section 4, passivity viewed special kind CSI applied DBNs, parents respect
variable passive provide context CSI. However, contrast CSI,
passivity assume context actually observed.

3. Technical Preliminaries
section introduces basic concepts notation used work. begin
brief discussion decision processes provide context work, followed
discussion dynamic Bayesian networks model perform inference.
3.1 Decision Processes, Belief States, Exact Updates
consider stochastic decision process wherein, time t, process state
st decision maker, agent, choosing action . executing st ,

process transitions state st+1 probability (st , st+1 ) agent receives

observation ot+1 probability (st+1 , ot+1 ). assume factored representations
state space observation space O, = X1 ... Xn = Y1 ... Ym ,
domains Xi , Yj finite. notation si used denote value Xi
state S, analogously oj O. Moreover, assume process
time-invariant, meaning independent t. framework compatible
many decision models used artificial intelligence literature, including POMDPs
(Kaelbling et al., 1998; Sondik, 1971) many variants.
agent chooses action based belief state bt (also known information state),
represents agents beliefs likelihood states time t. Formally,
belief state probability distribution state space process. Belief filtering
task calculating belief state based history observations. Ideally,
resulting belief state exact retains relevant information past
observations (this sometimes referred sufficient statistic; cf. Astrom, 1965).
exact update rule simple procedure produces exact belief states:
Definition 1 (Exact update rule). exact update rule defined follows: taking
action observing ot+1 , belief state bt updated bt+1 via
bt+1 (s0 ) =

X



bt (s) (s, s0 )

(1)

sS


bt+1 (s0 ) = bt+1 (s0 ) (s0 , ot+1 )

(2)

normalisation constant.
sometimes refer step bt bt+1 transition step step bt+1 bt+1
observation step. Unfortunately, space complexity storing exact belief states
time complexity updating using exact update rule exponential
number state variables, making infeasible complex systems large state
spaces. Hence, efficient approximate methods required.
1142

fiExploiting Causality Selective Belief Filtering DBNs

3.2 Dynamic Bayesian Networks
dynamic Bayesian network (DBN) (Dean & Kanazawa, 1989) Bayesian network
special temporal semantics specifies stochastic process transitions one
state another. DBNs used model effects actions stochastic decision
process. Specifically, compact representation transition function
observation function Oa action a:
Definition 2 (DBN). dynamic Bayesian network action a, denoted , acyclic
directed graph consisting of:




t+1 xt , xt+1 X ,
State variables X = xt1 , ..., xtn X t+1 = xt+1


1 , ..., xn
representing states process time + 1, respectively.


t+1 t+1 , representing obser Observation variables t+1 = y1t+1 , ..., ym
j
j
vation received time + 1.




Directed edges Ea X X t+1 X t+1 X t+1 X t+1 t+1 t+1 t+1 ,
specifying network topology dependencies variables.
Conditional probability distributions Pa (z | paa (z)) variable z X t+1 t+1 ,
specifying probability z assumes certain value given specific assignment
parents paa (z) = {z 0 | (z 0 , z) Ea }. convenience, also define pata (Z) =
t+1 pa (Z), pa (Z) =
X paa (Z) pat+1


zZ paa (z).
(Z) = X
edges Ea distributions Pa define functions


0

(s, ) =

n


0
Pa xt+1
= s0i | paa (xt+1

) - (s, )



(3)

i=1

(s0 , o) =






Pa yjt+1 = oj | paa (yjt+1 ) - (s0 , o)

(4)

j=1
t+1
0
use notation paa (xt+1

) - (s, ) specify parents xi
t+1
0
X , respectively, assume corresponding values . Formally,
t+1
t+1

xtl pata (xit+1 ) xt+1
pat+1
= s0l0 . Similarly, use
(xi ), xl = sl xl0
l0
t+1
0
notation paa (yj ) - (s , o) specify parents yjt+1 X t+1 t+1 ,
respectively, assume corresponding values s0 o.

Xt

Example 2 (DBN representation robot arm). represent robot arm Example 1 set DBNs, one DBN action
{CW
, CCWi }.
at+1
t+1





state observation
variables
= 1 , 2t+1 , 3t+1 ,
n
DBNs X = 1 , 2 , 3 , X

t+1 = 1t+1 , 2t+1 , 3t+1 . make example realistic, let us assume
joint orientations bounded relative orientation immediately preceding joint
(e.g. form cone), first joint bounded relative ground.
means joint movement depends well preceding joint orientation, shown Figure 3. Moreover, joint orientations correlated (i.e. edges within
1143

fiAlbrecht & Ramamoorthy

1t

1t+1

1t+1

2t

2t+1

2t+1

3t

3t+1

3t+1

Xt

X t+1

t+1

Figure 3: DBN representation robot arm.
X t+1 ) joint exceed bound given preceding joint. Finally, observation variables depend solely corresponding joint variable. actions
example would differ variable distributions Pa .

3.3 Additional Definitions
useful define following:
xt+1

binary order defined X X t+1 xti xtj xt+1

j
t+1

1 < j n, xi xj 1 i, j n.
Given set Z X X t+1 , write Z denote tuple contains variables
Z, ordered .
Given ordered tuple Z = (zi1 , ..., zi|Z| ), define set S(Z) = Xi1 .... Xi|Z|
contain value tuples variables Z.
Given value tuple sZ = (si1 , ..., si|Z| ) S(Z), use notation Z - sZ
abbreviation zil = sil zil Z (i.e. variables Z assume
corresponding values sZ ).

4. Passivity
section introduces formal definition passivity, used basis
remainder article. also provide simple procedure detect passive
variables process dynamics.
4.1 Formal Definition
outlined Section 1, state variable xt+1
called passive action exists

(in DBN ) xt+1 may change value
subset xt+1

parents

X


1144

fiExploiting Causality Selective Belief Filtering DBNs

least one variables subset changed value. Conversely, xt+1


change variables subset change. Formally, define passivity follows:
t+1

Definition 3 (Passivity). Let action given DBN

. state variable xi
t+1


called passive exists
set a,i paa (xi ) \ xi that:

t+1
(i) xtj a,i : xt+1
Ea
j , xi

(ii) two states st st+1 (st , st+1 ) > 0 :


sti = st+1
xtj a,i : stj = st+1
j


(5)

state variable passive called active.
set a,i corresponds subset variables described above: contains
variables directly affect xit+1 (i.e. parents xt+1
X ) xt+1
may


change value variables a,i changed value. sometimes say
variable xt+1
passive respect another variable xtj case

xtj a,i . Furthermore, omit obvious context.
Clause (i) Definition 3 requires xt+1
intra-correlated variables a,i ;

specifically, edge xt+1

xt+1
xtj a,i . example, see
j

Figure 1 assumed variable xt+1
passive respect variable
2
xt1 . (We discuss purpose clause next subsection.) Clause (ii) defines
core semantics passivity requiring xt+1
remains unchanged variables

a,i remain unchanged. Note means distribution Pa xt+1
may specify

deterministic stochastic behaviour variables a,i change values.
includes xt+1
may change value all.

state variable xit+1 passive even parents X , none
xti . case, set a,i would empty clause (i) well premise (5)
would trivially hold true. However, variable passive change
value circumstances. words, would constant.
case, one consider removing variable state description order reduce
computational costs.
noted Section 2.3, passivity shown special kind context-specific
independence (CSI) (Boutilier et al., 1996) applied DBNs. Here, associated set a,i
passive variable xt+1
provides context: given assignment values xtj a,i (i.e.

t+1
context) xtj = xt+1
independent xtk , xt+1
xtk pata (xt+1
j , xi
) \ a,i
k
k 6= i. However, besides similarity, important difference passivity
CSI, passivity actually assume context observed. Thus,
passivity viewed kind CSI unobserved contexts. become clear
Section 5, describe filtering method exploits passivity.
4.2 Non-Example Passivity
purpose clause (i) definition passivity? all, discussed
previously, clause (ii) captures core idea passivity, variable may
change value variables respect passive changed value.
1145

fiAlbrecht & Ramamoorthy

xt1

xt+1
1

xt2

xt+1
2

Figure 4: Example process clause (ii) insufficient.
However, may seem intuitive clause (ii) sufficient passivity,
fact processes clause (ii) alone suffice. words, clause (ii)
necessary sufficient passivity. illustrate following example:
Example 3 (Non-example passivity). Consider process two binary state variables,
x1 , x2 , single action, a, shown Figure 4. (We omit observation variables
clarity.) dynamics process xt+1
takes value xt2 xt+1
takes
1
2
value xt1 (i.e. x1 x2 swap values time step). process,
state variables satisfy clause (ii) Definition 3: set x01 = x02 (i.e. initial values),
(st , st+1 ) positive states st = st+1 , hence (5) true. set x01 6= x02 ,
(st , st+1 ) positive states st , st+1 sti 6= st+1
, {1, 2}, hence (5)
trivially true since premise false.

Despite satisfying clause (ii), state variables xt+1
xt+1
Example 3
1
2
fact passive, following two reasons: Firstly, passivity causal relation
must imply causal order (Pearl, 2000). However, causal order
x1 x2 , edge xt+1
xt+1
1
2 . Secondly, passivity means
variable may change value another variable respect passive (a
variable a,i ) changed value. words, whether passive variable xt+1

may change value depends past values a,i (at time t) new values
a,i (at time + 1). However, variables Example 3 depend values
time t, hence values time + 1 predetermined depend whether
variables a,i change values.
first issue, namely causal order, addressed adding corresponding edges X t+1 . instance, Example 3 could add edge xt+1
xt+1
1
2
establish causal order. However, generally solve second issue,
every passive variable xit+1 must depend past new values variables a,i . words, xit+1 must inter-correlated well intra-correlated
variables a,i . former given definition (since every variable a,i
parent xt+1
) latter precisely required clause (i) Definition 3.
Therefore, clauses (i) (ii) together define formal meaning passivity.
4.3 Detecting Passive Variables
mentioned Section 1, passivity latent causal property sense
extracted process dynamics without additional information, additional
assumptions regarding representation variable distributions. order determine
1146

fiExploiting Causality Selective Belief Filtering DBNs


Algorithm 1 Passive(xt+1
, )

1:


Input: state variable xt+1
, DBN

Output: a,i xt+1
passive , else false


t+1
3: Q OrderedQueue P pata (xi ) \ xti
// ascending order |a,i |
2:

4:

Q 6=

5:

a,i NextElement(Q)

6:

Q Q \ {a,i }

7:

xtj a,i


t+1
xt+1
6 Ea
j , xi

8:
9:
10:
11:
12:
13:
14:
15:
16:

Go line 4 // clause (i) violated

a,i paa (xt+1
) \ a,i xti

n

t+1

t+1

x
|
x
a,i
j
a,i
j
S(a,i ), S(a,i ), si Xi


t+1
=s ,
Pa xt+1
=

|
x

,


,




a,i
a,i


< 1


a,i
Go line 4 // clause (ii) violated
return a,i
return false

variable xit+1 passive , one find set a,i clauses Definition 3
satisfied. simple procedure representation variable
distributions given Algorithm 1. algorithm takes inputs variable xt+1


t+1


DBN , checks whether xi passive searching set a,i satisfies
clauses Definition 3. Note power set P line 3 includes empty set ,
hence also accounts a,i = . Lines 7 9 check clause (i) satisfied lines
10 14 check clause (ii) satisfied. Line 13 essentially checks (5) holds true.
clauses satisfied, xt+1
passive respect variables a,i ,

algorithm returns set a,i . Otherwise, algorithm returns logical false.2
time complexity Algorithm 1 exponential worst case, xt+1


passive. Specifically, time requirements line 4 grow exponentially number
parents xt+1
X , time requirements line 12 grow exponentially

cardinality a,i a,i . However, time requirements reduced significantly
committing specific representations variable distributions Pa . example,
distributions represented tabular form, one utilise arrays indices
perform sweeping tests (5), i.e. line 13. Moreover, important realise
algorithm needs performed state variable, prior start
2. Strictly speaking, Algorithm 1 checks property stronger passivity
check (st , st+1 ) > 0 (cf. clause (ii)) line 12. However, algorithm modified include
check. omit exposition order highlight core ideas behind algorithm.

1147

fiAlbrecht & Ramamoorthy

process demand. since passivity invariant process states.
words, variable passive , always passive . Therefore, suffices
check advance passivity.
Note set a,i necessarily unique. example, consider
variable xt+1
1

passive respect variables xt2 xt3 , i.e. a,1 = xt2 , xt3 , assume
xt+1
changes
x3t+1 changes(i.e.
2

change time). Then,
0

00

easy verify a,1 = x2 a,1 = x3 also satisfy clauses (i) (ii), hence
a,1 , 0a,1 , 00a,1 valid sets definition passivity. guiding principle
cases Occams razor, which, intuitively speaking, states simplest explanation
suffices. case, means suffices use smallest set a,i terms
cardinality |a,i |. (Hence, line 3 Algorithm 1 sorts queue Q ascending order
|a,i |.) rationale exist multiple causal explanations passive variable
xt+1
, one involving fewest key variables favoured since reduces
(compared alternative explanations) number cases would
revise beliefs xit+1 . earlier example, accept a,1 causal explanation
t+1
xt+1
every time xt+1
xt+1
may
1 , would revise beliefs x1
2
3
0
changed values. However, accept a,1 causal explanation, would
revise belief x1t+1 xt+1
may changed value. difference
2
become obvious Section 5.2, explains passivity exploited
reduce computational costs.

5. Passivity-based Selective Belief Filtering
section presents Passivity-based Selective Belief Filtering (PSBF) method,
exploits passivity efficient filtering. discussed Section 3, assume process
specified set dynamic Bayesian networks contains one DBN
action A. Therefore, whenever refer action (e.g. , , Pa , paa ),
assumed context .
PSBF follows general two-step update procedure belief state first
propagated process dynamics (transition step) conditioned
observation (observation step). Thus, natural divide exposition PSBF
three parts: (1) belief state representation, (2) transition step, (3) observation
step. discussed Sections 5.1, 5.2, 5.3, respectively. summary PSBF
given Section 5.4. also discuss computational complexity error bounds
PSBF Sections 5.5 5.6, respectively.
5.1 Belief State Representation
Recall Section 1 principal idea behind PSBF maintain separate beliefs
individual aspects process, exploit passivity order perform selective
updates separate beliefs. union individual aspects constitutes complete
state description process. Therefore, belief state represented product
separate beliefs individual aspects.
capture informal notion individual aspects formally form clusters,
defined follows:
1148

fiExploiting Causality Selective Belief Filtering DBNs

C1
1t

1t+1

C1

C1
1t

1t+1

1t+1

1t+1

1t

1t+1

2t+1

2t

2t+1

2t+1

2t

2t+1

2t+1

3t+1

3t

3t+1

3t+1

3t

3t+1

3t+1

1t+1

C2
2t

2t+1
C3

3t

3t+1

C2
(a) C1 , C2 , C3

(b) C1

(c) C1 , C2

Figure 5: Three clusterings robot arm DBN.
Definition 4 (Cluster). clustering X t+1 set C = {C1 , ..., CK } satisfies
k : Ck X t+1 C1 ... CK = X t+1 . refer elements Ck C clusters.
underlying idea behind concept clusters variables cluster Ck
connected important sense. Specifically, two variables common
cluster, exists relation variables regarding likelihood
values may assume. words, variables correlated X t+1 .
number K concrete choice clusters Ck specified user
generated automatically. example, may specified manually domain expert
familiar structure modelled system, generated automatically using
methods ones described Section 6.1. stressed, however,
order reduce computational costs, advisable follow general rule small
possible, large necessary choosing clusters (see Section 5.5 discussion
computational complexity). Therefore, two variables strongly correlated,
presumably common cluster, whereas weakly
correlated (weakly meaning correlation ignored safely),
separate clusters order reduce computational costs. illustrated
following example:
Example 4 (Clusters robot DBN). Recall robot arm DBN Example 2, specifit+1 given three clusters
cally Figure
cluster
state
t+1 3. One way
t+1
t+1
variables X
C1 = 1
, C2 = 2
, C3 = 3
, shown Figure 5a. clustering
efficient since minimises size cluster. However, clusters fail capture
important correlation joint orientation restricted preceding joint
orientation
i1 . Another

way cluster state variables given single cluster
C1 = 1t+1 , 2t+1 , 3t+1 , shown Figure 5b. clustering captures correlations
variables. However, largest possible cluster

and, therefore,

least effi
cient one. compromise given two clusters C1 = 1t+1 , 2t+1 , C2 = 2t+1 , 3t+1 ,
shown Figure 5c. clustering captures correlation joint orientations immediately preceding joint orientations, efficient
previous clustering since smaller clusters.

1149

fiAlbrecht & Ramamoorthy

Given definition clusters, capture informal notion separate beliefs
form belief factors:
Definition 5 (Belief factor). Given cluster Ck , corresponding belief factor bk
probability distribution set S(Ck ).
Intuitively, belief factor bk represents agents beliefs likelihood values
variables corresponding cluster Ck . analogy view belief factor
smaller belief state, view b full belief state combination
smaller belief states. However, distinguish two, refer b simply belief
state bk belief factor.
Finally, given clusters Ck corresponding belief factors bk , belief state b
represented factored form
b(s) =

K


bk (sk )

k=1



use notation sk refer tuple (si )xt+1 Ck . (E.g., Ck = xt+1
, xt+1
2
3

= (s1 , s2 , s3 , s4 ), sk = (s2 , s3 ).)
5.2 Exploiting Passivity Transition Step
order perform selective updates belief factors bk , require procedure
performs transition step independently factor.3 obtain procedure
introducing two assumptions allow us modify transition step (1) exact
update rule. assumptions guarantee transition step performed exactly,
sense (1). However, discuss shortly, assumptions violated obtain
approximate belief states.
first assumption, (A1), states clusters must uncorrelated (i.e.
edges X t+1 clusters), second assumption, (A2), states clusters
must disjoint. Formally, defined follows:
t+1
(A1) : xt+1
Ck pat+1
(xi ) Ck


(A2) k 6= k 0 : Ck Ck0 =
Note neither assumption implies other. is, may case (A1)
satisfied (A2) violated, vice versa. Assuming (A1) (A2),
reformulate (1)
X


0
bt+1
Tka (s, s0k )
btk0 (sk0 )
(6)
k (sk ) = 1
S(pat (Ck ))


k0 :[xt+1
Ck0 : xti pat (Ck )]



1 normalisation constant


0
Tka (s, s0k ) =
Pa xt+1
= (s0k )i | paa (xt+1

) - (s, sk ) .
xt+1
Ck


3. also advantage belief factors updated parallel, useful feature
considering many platforms use parallel processing techniques.

1150

fiExploiting Causality Selective Belief Filtering DBNs

procedure performs transition step independently belief factor bk , hence
updated order parallel.
Assumption (A1) allows us bring (1) form updates belief
factors bk independently other. Specifically, (A1) allows us define cluster-based
transition function Tka , turn enables summation (6). Assumption (A2),
hand, guarantees product (6) correct. particular, may
case |sk0 | < |Ck0 | (i.e. fewer elements sk0 Ck0 ) variables
Ck0 patat (Ck ) (i.e. xt+1
Ck0 xti
/ patat (Ck )). cases, btk0

t+1
taken marginal distribution variables xi Ck0 xti patat (Ck ),
(A2) guarantees marginalisation introduces errors.
mentioned previously, assumption may violated obtain approximate belief
states. However, important distinction (A1) (A2) regard:
(A2) violated, (6) still well-defined sense still executed,
except product (6) may degrade accuracy results. contrast
(A1), structural requirement Tka sense Tka ill-defined without (A1).
since, (A1) violated, variables Ck may parents X t+1
0
Ck , case paa (xt+1
) - (s, sk ) would ill-defined. Thus, (A1) violated,
enforce modifying distributions Pa xt+1
Ck marginalise

t+1
variables pat+1
(x
)




C
,


clusters
C
. means

k
k


variable separate distribution every cluster contains variable, thereby
possibly introducing approximation error.
Given modified transition step (6), exploit passivity perform selective
updates belief factors bk . Recall Section 4.1 variable xt+1
passive

t+1

exists set a,i variables xi may change value
variables a,i changed value. causal connection used decide
whether values variables cluster Ck may changed, case
corresponding belief factor bk updated. Theorem 1 provides formal foundation:


Theorem 1. (A1) (A2) hold, xt+1
Ck passive ,


: bt+1
k (sk ) = bk (sk ).

Proof. Proof Appendix A.
Theorem 1 states clusters C1 , ..., CK disjoint uncorrelated,

variables cluster Ck passive , transition step corresponding
belief factor btk bt+1
omitted without loss information.
k
Theorem 1 translate situations (A1) (A2), both, violated?
key assumption (A1), states clusters must uncorrelated.
discussed earlier, enforce modifying variable distributions Pa cluster.
However, passive variable xit+1 Ck correlated (passive active) variable
t+1
t+1
xt+1
Ck0 , xt+1
pat+1
distribution Pa
(xi ), marginalising xj
j
j
t+1
t+1
xi typically cause xi lose passivity, sense would longer satisfy
clauses Definition 3. Consequently, would always perform transition
step Ck , even unmodified variables Ck passive. problematic
unnecessary computations, also modified distributions
introduce error every time transition step performed.
1151

fiAlbrecht & Ramamoorthy

1t

1t+1

1t+1
C1

2t

2t+1

2t+1
C2

3t

3t+1

3t+1

Xt

X t+1

t+1

Figure 6: Robot arm DBN implementing action CW3 . Dashed circles mark passive state
variables. coloured ellipses represent clusters C1 C2 .
alleviate effect, one check chance unmodified variables
cluster would change values. shown case whenever
causal path active variable variable cluster:
Definition 6 (Causal path). causal path , active variable xt+1
another

t+1
t+1 (Q)
t+1
(1)
(2)
(Q)
(1)
variable xj , sequence hx , x , ..., x x = xi , x
= xj ,
1 q < Q :
(i) x(q) X t+1
(ii) x(q) , x(q+1) Ea
(iii) x(q+1) passive respect x(q)
Intuitively, causal path defines chain causal effects (such joints 1 3
Example 1): since active variable x(1) may changed value x(2) passive
respect x(1) , x(2) may also changed value; since x(2) may changed
value x(3) passive respect x(2) , x(3) may also changed value, etc.
Hence, absence observing changes, mere existence causal path
x(1) x(Q) reason revise beliefs x(Q) . Therefore, general update rule,
omit transition step btk bt+1
unmodified variables cluster Ck passive
k


, causal path active variable variable Ck .
demonstrated following example:
Example 5 (PSBF update rule robot arm DBN). Let us consider robot arm
previous examples. Figure 6 shows DBN implements action CW3 .
action rotates joint 3 robot arm 1 clock-wise (i.e. joint orientation 3t+1
direct target action). Therefore, variable 3t+1 active variables
1t+1 2t+1 passive (shown dashed circles).

use clustering C1 = 1t+1 , 2t+1 , C2 = 2t+1 , 3t+1 reasons given Example 4. Since 1t+1 parent 2t+1 , PSBF enforce assumption (A1)
1152

fiExploiting Causality Selective Belief Filtering DBNs

Algorithm 2 SkippableClusters(C, )
1:

Input: clustering C = {C1 , ..., CK }, DBN

2:

Output: set clusters C C skipped transition step

3:

C C

4:

Q OrderedQueue(X t+1 )

5:

C 6= Q 6=

6:
7:
8:
9:
10:
11:
12:
13:
14:

xt+1
NextElement(Q)



Q Q \ xt+1


Passive(xt+1
, )


C C \ Ck C | xt+1
Ck


xt+1
Q
j
t+1

CausalPath(xt+1
, xj , )
n

C C \ Ck C | xt+1
Ck
j
n

Q Q \ xt+1
j

return C

marginalising 1t+1 variable distribution Pa 2t+1 cluster C2 . modified variable distribution loses passivity property (both clauses Definition 3
violated), unmodified distribution 1t+1 still passive.
performing transition step, PSBF update belief factor b2
corresponding cluster C2 contains active variable 3t+1 . However, since variables
cluster C1 passive (there modified variables C1 ), since causal
path 3t+1 variable C1 , PSBF omit update belief factor b1 .
Intuitively, makes sense since change orientation joint 3 cannot cause
change orientations preceding joints. Note corresponds saving
50% transition step.


Algorithm 2 defines procedure utilises rule find clusters
transition step skipped. algorithm takes inputs clustering C DBN
, returns set C skippable clusters. essentially searches active
variables xt+1
removes clusters Ck C contain variables

causal path xit+1 . function OrderedQueue(X t+1 ) returns ordered
queue Q variables X t+1 . performance Algorithm 2 depends order
queue. experiments, obtained good performance ordering variables
descending order number outgoing edges. function NextElement(Q) returns

next element queue; function Passive(xt+1
, ) defined Algorithm 1;
t+1 t+1

function CausalPath(xi , xj , ) returns logical true
1153

fiAlbrecht & Ramamoorthy

causal path xt+1
xjt+1 .4 Note that, given invariance passivity process

states (cf. Section 4.1), suffices call Algorithm 2 (in advance needed)
determine clusters omit transition step.
5.3 Efficient Incorporation Observations
PSBF perform observation step similarly exact update rule (2),
conditions propagated belief state bt+1 observation ot+1 obtain fully updated
belief state bt+1 . However, given factored belief state representation used PSBF,
require procedure respects factorisation observation step. Assuming
(A1) (A2) hold, bring (2) form updates belief factors bk
independently
X


t+1 0
0
0
bt+1
(s, ot+1 )
bt+1
(7)
k (sk ) = 2 bk (sk )
k0 (sk )
t+1 )) : = s0 k 0 6= k : C 0 pat+1 (Y t+1 ) 6=
S(pat+1
k
k
(Y

k




2 normalisation constant. Note that, analogously (6), variables
t+1 ), bt+1 taken marginal distribution
Ck0 pat+1
k0
(Y
t+1
t+1
Ck0 paat (Y
). Assumption (A2) guarantees marginalisation introduces
errors. (A1) (A2) hold, transition step (6) observation step (7)
produce exact belief states sense (1) (2), regardless many clusters
skipped transition step (cf. Theorem 1).
observation step (7) updates belief states uses observation variables
process. words, ignores internal structure observation variables.
However, clear variables cluster Ck marginally independent
observation variables t+1 (this determined using d-separation (Geiger et al., 1989),
simply checking directed path Ck t+1 ), need
perform observation step corresponding belief factor bk . expressed
formally Theorem 2:


Theorem 2. xt+1
Ck marginally independent yjt+1 t+1 ,

t+1
: bt+1
k (sk ) = bk (sk ).

Proof. Proof Appendix B.
Theorem 2 states variables Ck independent t+1 ,
observation step bk skipped. However, even Ck independent t+1 , may
case variables Ck depend subset Yk t+1 observation
variables. Clearly, cases, suffices use Yk rather t+1 observation
step. account this, first note variables t+1 may correlated
other. preserve correlations, subdivide t+1 clusters Cl t+1
introduce following assumptions:


(A3) : yjt+1 Cl paa (yjt+1 ) t+1 Cl
(A4) l 6= l0 : Cl Cl0 =
4. simple way implement function modify standard graph search method (such breath-first
search) check (iii) Definition 6, apply variables X t+1 edges Ea .

1154

fiExploiting Causality Selective Belief Filtering DBNs

Assumptions (A3) (A4) analogous (A1) (A2), respectively, essentially
serve purposes observation step. distinguish clusters Ck Cl ,
sometimes refer former state cluster latter observation cluster.
Assuming (A3) (A4) hold, redefine observation step
X


t+1 0
0

t+1
0
bt+1
(s
)
=

b
(s
)

(s,

)
bt+1
(8)
2
k
k
l
k
k
k0 (sk )
t+1
0
0
l: Cl Yk 6= S(pat+1
(Cl )) : sk = sk k 6= k : Ck0 pa (Cl ) 6=



al (s, ot+1
l ) =







t+1
t+1
Pa yjt+1 =(ot+1
)
|
pa
(y
)
(s,

)
j
j
l
l

yjt+1 Cl

Yk t+1 set observation variables marginally independent
variables Ck .
Given Theorem 2, one see (8) equivalent (7) observation variables
clustered (or, equivalently, single observation cluster Cl = t+1 ). However,
important note observation variables clustered (i.e. multiple
observation clusters Cl ), (8) notQnecessarily
equivalent
P
P(7).
QmTo see this, helpful
compare abstract formulations

(o
)
b

j=1
j

j=1 (oj ) bs ,
former corresponds (8) latter (7). Therein, (o1 , ..., om ) observation, bs
probability state S, (oj ) probability observing yj = oj
s. abstract formulations equivalent = 1 bs = 1 s,
cases may equivalent. Nonetheless, fix number observation
variables m, (8) approximates (7) closely increase number state variables
n. experiments indicate often suffices use state variables
observation variables order obtain good approximations.
Finally, show suffices perform observation step bk using
clusters Cl whose variables independent variables Ck , observe (8)
fact repeated application (7) every Cl , updated belief factor bt+1

k
t+1
used place bk subsequent application. Since every application
form (7) (with t+1 = Cl ), conclude Theorem 2 holds, hence observation
step skipped clusters Cl independent Ck .
5.4 Summary PSBF
preceding sections summarised follows:
Representation: belief state bt represented product K belief factors btk ,
Q


bt (s) = K
k=1 bk (s). belief factor bk probability distribution
set S(Ck ), Ck X t+1 cluster correlated state variables.
Transition step: transition step btk bt+1
performed using (6), clusters
k


Ck include active variables , causal path

active variable . clusters skipped.
Observation step: observation step bt+1
bt+1
performed using (8),
k
k
clusters Ck dependent observation variables t+1 , using
observation clusters Cl relevant Ck . clusters skipped.
1155

fiAlbrecht & Ramamoorthy

Algorithm 3 PSBF(at , ot+1 , (btk )Ck C | C, C, (a )aA )
1:

Input: action , observation ot+1 , belief factors (btk )Ck C

2:

Parameters: state clustering C, observation clustering C, DBNs (a )aA

3:

Output: updated belief factors (bt+1
k )Ck C

4:

// Transition step

5:

C SkippableClusters(C, )

6:

Ck C

7:
8:
9:
10:
11:



Ck C
bt+1
btk
k
else
s0k S(Ck )
X

0
bt+1
Tka (s, s0k )
btk0 (sk0 )
k (sk ) 1
S(patat (Ck ))

12:

k0 :[xt+1
Ck0 : xti patat (Ck )]


// Observation step

Ck C
n


14:
Yk yjt+1 t+1 | directed path Ck yjt+1
13:

15:

Yk =

16:

bt+1
bt+1
k
k

17:
18:
19:

else
s0k S(Ck )
t+1 0
0
bt+1
k (sk ) 2 bk (sk )



X



al (s, ot+1 )



0
bt+1
k0 (sk )

(Cl ) 6=
Cl C : Cl Yk 6= S(pat+1
(Cl )) : sk = s0k k0 6= k : Ck0 pat+1



20:

return

(bt+1
k )Ck C

Algorithm 3 provides procedural specification PSBF. algorithm takes inputs
action time t, , subsequent observation time + 1, ot+1 , belief factors
time t, btk . internal parameters state clustering C, observation clustering
C, set DBNs (a )aA define process. Lines 4 11 implement
transition step lines 12 19 implement observation step. Note suffices
execute lines 5 14 advance (or demand) remember results
future reference. algorithm returns updated belief factors bt+1
k .
1156

fiExploiting Causality Selective Belief Filtering DBNs

5.5 Space Time Complexity
belief factor bk one elementP
bk (sk ) sk S(Ck ).5 Thus, total space required
maintain K belief factors bk K
k=1 |S(Ck )|. Furthermore, size set S(Ck ) grows
exponentially number variables Ck , hence dominant growth factor
space requirement given largest cluster Ck |Ck | = maxk0 |Ck0 |. Therefore,
space complexity PSBF O(exp maxk |Ck |), hence representation feasible
reasonably small clusters Ck .
Similarly, numberPof operations required perform transition observation
steps order 2 K
k=1 |S(Ck )| worst case (i.e. clusters need updated
steps). Specifically, line 11 line 19 Algorithm 3 executed
every sk Ck . dominant growth factor given largest cluster Ck , hence
time complexity PSBF O(2 exp maxk |Ck |) = O(exp maxk |Ck |). Note
assumes analysis performed lines 5 14 Algorithm 3 done advance.
time complexity worst case, clusters need updated
transition observation steps. difficult derive time complexity
average case unclear average case terms passivity. Even
stipulate certain average degree passivity (e.g. 50% variables passive), would
still difficult make general statement time requirements since depends
crucially passive variables distributed across clusters. example, even
process average 90% passivity, one active variable cluster
every cluster would need updated transition step. Thus, general
statement make regards passivity time complexity PSBF
refined O(exp maxCk CT CO |Ck |), CT CO include clusters
need updated transition observation step, respectively.
5.6 Error Bounds
five possible sources approximation errors PSBF:
clusters correlated (i.e. (A1) (A3) violated)
clusters overlapping (i.e. (A2) (A4) violated)
Generally (8) multiple observation clusters Cl used
first two cases, approximation error depends amount correlation
overlap. little correlation overlap clusters,
approximation error expected small. Conversely, clusters strongly
correlated overlapping, approximation error expected large.
Boyen Koller (1998) provide useful analysis error bound filtering
method uses factored belief state representation. Since PSBF uses factored
representation, analysis applies directly PSBF. purpose section
restate main result analysis context work.
analysis uses concept relative entropy (Kullback & Leibler, 1951)
measure similarity belief states:
5. practice, suffices store |S(Ck )| 1 elements, irrelevant analysis.

1157

fiAlbrecht & Ramamoorthy

Definition 7 (Relative entropy). Let two probability distributions defined
set X. relative entropy defined
KL(||) =

X
xX

(x) ln

(x)
(x)

(x) > 0 (x) > 0.
Similar Boyen Koller (1998), define approximation error incurred PSBF
relative exact belief state. However, since consider decision process multiple
actions (represented DBNs ), define error action respectively:
Definition 8 (Approximation error). Let b exact belief state b approximation PSBF. taking action a, let b0 exact update b (using (1) (2))
b0 PSBF-update b (using (6) (8)). Furthermore, let b0 exact update
b (using (1) (2)). say PSBF incurs error relative b0
KL(b0 ||b0 ) KL(b0 ||b0 ) .
analysis also relies concept mixing rates. Intuitively, mixing rate
DBN quantifies degree stochasticity . depends mixing rates ka
individual clusters Ck :
Definition 9 (Mixing rate). mixing rate cluster Ck X t+1 defined
ka = 0min
00

,s

X



min Tka (s0 , s), Tka (s00 , s) .

sS(Ck )

Ck satisfy (A1) (A2), observation variables t+1 one observation
cluster, mixing rate given = (mink ka /r)q cluster Ck
depends r influences q clusters Ck0 6=k (Boyen & Koller, 1998).
worst case (that is, (A1A4) violated), minimal mixing rate given ka
single cluster Ck = X t+1 .
Finally, main result work Boyen Koller (1998), restated
context work Theorem 3, essentially states approximation error PSBF
(measured terms relative entropy) bounded mixing rates process:
Theorem 3 (Boyen & Koller, 1998). Let bt exact belief state bt approximation PSBF using clusters Ck . Then, states ~s = (s0 , ..., st ) actions
~a = (a0 , ..., at1 ),
h
max

~a
Eo1 ,...,ot KL(bt ||bt )
mina ~a
expectation E Q
taken possible sequences observations o1 , ..., ot
1

+1 , +1 ), defined above.
probabilities P (o , ..., ) = t1
=0 (s
1158

fiExploiting Causality Selective Belief Filtering DBNs

Process size

# x vars (n)

# vars (m)

# states (|S|)

# obs. (|O|)



10

3

> one thousand

8



20

6

> one million

64

L

30

9

> one billion

512

XL

40

12

> one trillion

4096

Table 1: Synthetic process sizes. variables binary.

6. Experimental Evaluation
evaluated PSBF two experimental domains: Section 6.1, evaluated PSBF
synthetic (i.e. randomly generated) processes varying sizes degrees passivity.
Section 6.2, evaluated PSBF simulation multi-robot warehouse system. brief
summary experimental results given Section 6.3.
6.1 Synthetic Processes
first evaluated PSBF series synthetic processes. PSBF compared selection
alternative methods, including PF (Gordon et al., 1993), RBPF (Doucet et al., 2000), BK
(Boyen & Koller, 1998), FF (Murphy & Weiss, 2001); see Section 2 discussion
methods. algorithms implemented Matlab 7.13, used Matlab
toolbox BNT (Murphy, 2001) implement BK FF.
6.1.1 Specification Synthetic Processes
generated synthetic processes four different sizes specified Table 1.
process generated follows:
First, variable xit+1 chosen passive probability p, case
also add edge (xti , xt+1
). refer p degree passivity. sample

t+1
edges X /X
X t+1 , generate mixture Gaussians G using Algorithm 4 (see
Appendix C). Figure 7 shows example G generated process size M. set
G used produce areas correlated variables (i.e. Gaussians),
constitute natural candidates state clusters.
Let vector maximum densities Gaussian G, let
vector densities value N. Then, every combination j, edge (xti , xt+1
j )
2
added probability equal maximum element j / , operators
point-wise. xt+1
chosen passive, edge (xti , xt+1

j ) added
t+1 t+1
t+1 t+1
< j. case, also add edge (xi , xj ). Edges (xi , xj ) added similarly
t+1
< j,6 also add edge (xti , xt+1
j ) passive xj . ensure every
variable effect generated process, xti connected least one xt+1
j
t+1
X t+1 (adding
(adding (xti , xt+1
)

necessary)


x


least
one
parent

X

j
6. condition < j cases ensure resulting DBN acyclic.

1159

fiAlbrecht & Ramamoorthy

0.35
0.3

Density

0.25
0.2
0.15
0.1
0.05
0

2

4

6

8

10

12

14

16

18

20

i, j

Figure 7: Example mixture Gaussians generated process size consisting
t/t+1
three Gaussians. closer two variables xi
xt+1
peak common
j
Gaussian, higher probability edge added them.
t+1 t+1
(xtj , xt+1
j ) necessary). Finally, edges (xi , yj ) added probability 0.1,
i, j, ensuring yjt+1 least one parent X t+1 .
variables process binary. Passive variables assumed passive
respect parents X . distributions Pa xt+1
X t+1 generated

t+1
uniformly randomly without bias. passive variables xi , modify Pa satisfy clause
(ii) Definition 3. distributions Pa yjt+1 t+1 generated probability
sampled uniformly either [0.0, 0.2] [0.8, 1.0], obtain meaningful observations.
Finally, every process consists two actions. obtained randomly choosing
one three variables xt+1
whose distributions Pa resampled

edges X added probability 0.1 (passive variables chosen way longer
passive). simulations, actions chosen uniformly randomly.
process starts random initial state, algorithms tested
sequence processes, initial states, chosen actions, random numbers.

6.1.2 Clustering Methods
used three different clustering methods, denoted hpci, hmorali, hmodisi. methods
applied variables X t+1 without edges involving X t+1 :
hpci drops directions edges (i.e. edge xt+1
xt+1
ads reverse

j
t+1
t+1
edge xj xi ) puts variables (undirected) path
one cluster. definition, resulting clusters satisfy assumptions (A1A4).
hmorali connects parents variable drops directions (it moralises
variables) extracts clusters fully connected variables (maximum cliques).
resulting clusters may satisfy assumptions (A1A4).
hmodisi similar hmorali truncates resulting clusters make disjoint
(clusters removed become subset another cluster). definition,
resulting clusters satisfy (A2/A4), necessarily (A1/A3).
example, consider Figure 5 Section 5.1. Here, hpci would produce cluster
C1 Figure 5b, since variables connected undirected path. Furthermore,
1160

fiExploiting Causality Selective Belief Filtering DBNs

hmorali would produce two clusters C1 C2 Figure 5c, correspond
two maximum cliques moralising variables X t+1 . Finally, hmodisi would produce
cluster C1 Figure 5c cluster C3 Figure 5a.
PSBF used clustering method generate clusters state variables (Ck )
observation variables (Cl ). Moreover, PSBF enforced (A1/A3) whenever necessary
modifying variable distributions described Section 5.1.
6.1.3 Accuracy
order compare accuracy tested algorithms, computed relative entropy
(cf. Definition 7) exact belief states obtained using exact update rule (cf. Definition 1)
approximate belief states produced tested algorithms. However, since exact
belief states relative entropy hard compute large processes, able
compare accuracy algorithms processes size only. algorithms initialised
uniform belief states, uniformly sampled particles.
first compared accuracy PSBF BK, since use factorisation
belief state representations. Figure 8 shows relative entropy PSBF BK averaged 1000 processes 0%, 20%, 40%, 60%, 80%, 100% passivity, respectively.
results show PSBF hpc/modisi produced lower relative entropy (i.e. higher accuracy) BK hpc/modisi, PSBF hmorali produced relative entropy comparable
BK hmorali. indicates violations (A2/A4) introduce smaller errors
violations (A1/A3). Note PSBF BK convergent behaviour
relative entropy, shows approximation error due factorisation
bounded, discussed Section 5.6. interesting since PSBF BK obtain approximation errors factorisation different ways: PSBF loses accuracy modifying
variable distributions ensure state clusters independent (cf. Section 5.2),
BK loses accuracy marginalising original factorisation inference (i.e.
projection step; cf. Section 2.1). Nevertheless, shown results, resulting
approximation errors bounded cases, similar convergence.
Note relative entropy methods increased degree passivity
process. explained fact higher passivity implies higher determinacy
and, therefore, lower mixing rates (cf. Definition 9), crucial factor error
bounds PSBF BK (cf. Theorem 3). Finally, note PSBF produce exact
belief states (i.e. zero relative entropy) using hpci clustering, despite fact
clusters generated hpci satisfy assumptions (A1A4). However, discussed detail
Sections 5.3 5.6, another possible source approximation errors multiple observation
clusters used, often case using hpci produce observation clusters.
compare accuracy PF/RBPF PSBF/BK, number samples used
PF/RBPF chosen automatically process required approximately
much time per belief update PSBF hmorali BK hmorali, respectively.
experiments, meant PF (RBPF) able process 100 300 (20
50) samples. However, since process 1000 states, nearly enough
represent uniform belief state. Hence, PF/RBPF produced much higher relative entropy
PSBF/BK. Moreover, fact processes high variance means
PF/RBPF would require many samples achieve accuracy PSBF/BK (as
1161

fiAlbrecht & Ramamoorthy

2

0.6
0.4

BK (pc)

PSBF (pc)

BK (moral)

PSBF (moral)

BK (modis)

PSBF (modis)

Relative entropy

Relative entropy

0.8

0.2
0

0

500

1000

1500
2000
Transition

2500

1.5
1
0.5
0

3000

0

500

(a) 0% passivity

1000

1500
2000
Transition

2500

3000

2500

3000

2500

3000

(b) 20% passivity

5
Relative entropy

Relative entropy

3

2

1

4
3
2
1

0

0

500

1000

1500
2000
Transition

2500

0

3000

0

500

8

6

4

2

0

1500
2000
Transition

(d) 60% passivity

Relative entropy

Relative entropy

(c) 40% passivity

1000

0

500

1000

1500
2000
Transition

2500

3000

(e) 80% passivity

6
4
2
0

0

500

1000

1500
2000
Transition

(f) 100% passivity

Figure 8: Accuracy results PSBF BK. Plots show relative entropy exact
algorithms belief states (lower better). Results averaged 1000 processes size
(n = 10, = 3), average 0%100% non-target variables passive (cf.
Section 6.1.1). PSBF/BK used clustering methods hpci, hmorali, hmodisi.
shown next section). One would expect latter issue alleviated use
exact inference RBPF (cf. Section 2.1). However, case much
variance process captured marginal distributions used particles
RBPF. contrast, synthetic processes exhibit high variance across variables,
1162

fiExploiting Causality Selective Belief Filtering DBNs

automatic grouping7 state variables sampled exact variables still contained
much variance sampled variables. Hence, RBPF required significantly samples
number could process time provided.
Finally, order compare accuracy FF PSBF/BK, number iterations
used FF (more precisely, number iterations loopy belief propagation; cf. Murphy &
Weiss, 2001) chosen automatically process FF required approximately
much time per belief update PSBF hmorali BK hmorali, respectively. However,
FF often able perform several iterations provided time, resulting
relative entropy substantially higher PSBF/BK. problem
FF designed specific class DBN topologies, namely containing
edges within X t+1 (called regular DBNs Murphy & Weiss, 2001). allows
FF use fully factored representation belief states, variable
belief factor. However, processes used experiments high intra-correlation
state variables (i.e. many edges X t+1 ), especially increasing passivity.
correlations cannot captured belief state representation FF, resulting
significantly higher relative entropy PSBF/BK.
6.1.4 Timing
measured computation times processes sizes S, M, L, XL passivities 25%,
50%, 75%, 100%, respectively. PSBF BK used hmorali clustering, seemed
appropriate fair comparison since produced consistently similar accuracy
algorithms. number samples used PF chosen automatically process
PF achieved average accuracy approximately good PSBF
BK, respectively, final 20% process. involved computing exact belief
states relative entropies, able use PF processes size only. omit
RBPF FF section shown previous section unsuitable
processes consider. PSBF tested 1, 2, 4 parallel processes,
allocated approximately number belief factors.
Figures 9a 9d show times 1000 transitions averaged 1000 processes,
Figure 9e shows average percentage belief factors updated transition
observation steps PSBF. timing reported PSBF includes time taken
modify variable distributions (in case overlapping clusters) detect skippable clusters
transition observation steps, done advance
action. results show PSBF able minimise time requirements significantly
exploiting passivity. First, note marginal gains 25% 50%
passivity, despite fact PSBF updated 14% fewer clusters transition step.
clusters mostly small. However, significant gains
50% 75% passivity average speed-ups 11% (S), 14% (M), 15% (L), 18% (XL),
7. open question group state variables sampled exact variables (Doucet et al.,
2000). used simple heuristic whereby set sampled variables contained variables xt+1


parents X t/t+1 none xti . remaining variables X t+1 constituted set
exact variables. ensure resulting grouping valid actions (i.e. DBNs) process,
considered edges involved DBNs; is, performed grouping union Ea
a. Moreover, improve efficiency, subdivided set exact variables clusters
variables connected undirected edges X t+1 without edges involving sampled variables.

1163

fi100
PF:BK
PF:PSBF
BK

50

0

25%

50%
75%
Passivity

400

200

150

PSBF1 100
PSBF2
PSBF4
50

100%

(a) (n=10, m=3)

0

25%

50%
75%
Passivity

Seconds 1000 transitions

150

Seconds 1000 transitions

Seconds 1000 transitions

Seconds 1000 transitions

Albrecht & Ramamoorthy

350
300
250
200
150
100
50

100%

0

(b) (n=20, m=6)

25%

100%

(c) L (n=30, m=9)

100
% updated belief factors

50%
75%
Passivity

700
600
500
400
300
200
100
0

25%

50%
75%
Passivity

100%

(d) XL (n=40, m=12)

(trans)
(obs)
(trans)
(obs)

80
60
40
L (trans)
L (obs)
XL (trans)
XL (obs)

20
0

25%

50%
75%
Passivity

100%

(e) Updated belief factors

Figure 9: Timing results. (ad) Average number seconds required 1000 transitions
UNIX dual-core machine 2.4 GHz, sizes S, M, L, XL. Passivity p% means
average p% non-target variables passive (cf. Section 6.1.1). PSBF BK
used hmorali clustering. PF optimised binary variables used number samples
achieve accuracy PSBF BK, respectively. PSBF run 1 (PSBF-1), 2
(PSBF-2), 4 (PSBF-4) parallel processes. (e) Average percentage belief factors
updated transition observation steps, respectively.
75% 100% passivity average speed-ups 11% (S), 33% (M), 46% (L),
49% (XL). shows computational gains grow significantly
degree passivity size process.
results show PSBF consistently outperformed BK process sizes.
two main computational savings PSBF relative BK: firstly, skipping belief
factors transition observation steps, secondly, perform
potentially expensive projection step restore original factorisation inference.
However, times algorithms grew exponentially size process,
note relative difference PSBF BK decreased significantly lower
degrees passivity. instance Free Lunch (see Section 7 discussion),
means PSBF performs best processes high passivity suffer
performance processes lack passivity. Specifically, computational overhead
modifying variable distributions detecting skippable belief factors amortise
1164

fiExploiting Causality Selective Belief Filtering DBNs

effectively large processes low passivity. Furthermore, low passivity, PSBF
often perform full transition observation steps (i.e. update belief factors
step), costly large processes.
BK PF affected passivity? surprisingly, performance BK
nearly unaffected increasing degrees passivity. junction tree algorithm used
BK benefited marginally increased sparsity process, computational
gains minimal. first unable use PF required many samples
(between 10k 200k) achieve comparable accuracy PSBF/BK, due
high variance processes. order investigate effect passivity PF,
implemented version PF strictly optimised binary variables. Interestingly,
found passivity adverse effect performance PF, requiring use
exponentially samples increased passivity (see Figure 9a). makes sense
view PF factored approximation method (such PSBF BK) means
analysis Section 5.6 applies. However, PF puts variables single cluster
(since actually factored method), mixing rate process much lower
PSBF BK (as discussed Section 5.6) and, thus, error bounds less
tight. compensate this, PF requires significantly samples increased passivity.
6.2 Multi-robot Warehouse System
section, demonstrate passivity occur naturally complex system
PSBF exploit accelerate filtering task. end, consider
multi-robot warehouse system style Kiva (Wurman et al., 2008),
robots task transport goods within warehouse (cf. Figure 10a).
6.2.1 Specification Warehouse System
Figure 10b shows initial state warehouse simulation. warehouse consists
2 workstations (W1, W2), 4 robots (R1R4), 16 inventory pods (I1I16). robot
move forward backward, turn left right, load unload inventory pod (if
positioned pod), nothing. Kiva, robots move inventory pods
unless carrying pod, case pods become obstacles. move
turn operations stochastic robot may move/turn far (3% chance)
nothing (2% chance). robot possesses two sensors, one telling inventory
pod loaded (if any) one direction facing. direction sensor noisy
random direction may reported (3% chance).
robot maintains list tasks form Bring inventory pod workstation
W (yellow area around W) Bring inventory pod position (x,y). tasks
executed depends control mode, use two simulations:8
8. control modes ad hoc often make suboptimal decisions. However, found current
solution techniques (DEC-)POMDPs, including approximate methods, infeasible setting.
Nonetheless, quality decisions made control modes largely depends accuracy
belief states, hence important belief states updated accurately. Therefore,
control modes sufficient purposes.

1165

fiAlbrecht & Ramamoorthy

(a) Kiva warehouse system

(b) Initial state simulation

Figure 10: (a) Kiva warehouse system (image reproduced DAndrea & Wurman, 2008).
Robots (orange coloured) transport shelfs goods workstations. (b) Initial
state warehouse simulation. warehouse consists 2 workstations (W1, W2), 4
robots (R1R4), 16 inventory pods (I1I16).
Centralised mode: central controller maintains belief state bt state
warehouse system. time t, samples 100 states bt removes

duplicate states, resulting set
P t= {s1 , s2 , ...}. resamples state



probabilities w(s ) = b (s )/ q b (sq ). Based current task
robot, performs search (Hart, Nilsson, & Raphael, 1968) (with Manhattan
distance) space joint actions find optimal action robot.
executing actions, robots send sensor readings controller,
controller updates belief state using sensor readings.
Decentralised mode: robot maintains belief state communication robots. knowledge robots
current tasks, communicated task allocation module. time t,
robot samples set state done centralised mode. Treating
robots static obstacles, performs search based current
task find action . repeated robot r states sq S,
resulting actions ar,q used
P obtain distributions r : [0, 1] (A
set actions) r (a) = q : ar,q =a w(sq ). robot executes action updates belief state using sensor readings distributions r
average robots actions.
tasks generated external scheduler time intervals sampled U [1, 10].
generated task assigned one robots sequential auction (Dias, Zlot,
Kalra, & Stentz, 2006). robots bids calculated total number steps needed
solve current tasks auctioned task (in simplified model
robots removed), averaged states S. robot lowest bid
assigned task.
1166

fiExploiting Causality Selective Belief Filtering DBNs

Figure 11: Example DBN smaller warehouse system consisting one inventory
pod (I1) two robots (R1, R2). DBN implements joint action R1 moves
R2 turns. Dashed circles mark passive state variables. coloured areas represent
state clusters C1 C8 .

6.2.2 DBN Topology Clustering
Figure 11 shows example DBN smaller warehouse one inventory pod two
robots. inventory pod represented two variables, I.x I.y, correspond
x position inventory pod. robot R represented four variables:
R.x/R.y x/y position, R.d direction, R.s status. status
robot R either R.s=0 (unloaded) R.s=I (loaded inventory pod I). Constants
size warehouse positions workstations omitted DBN.
four types clusters: I-clusters (C1C4) preserve correlation
R loaded I, must always position R (there two I-clusters
(I,R) pair); R-clusters (C5) S-clusters (C6), respectively, preserve
correlation two robots position carry inventory pod
(there one R/S-cluster (Ra,Rb) pair > b); And, finally, D-clusters (C7,
C8). PSBF uses singleton observation clusters (i.e. one cluster observation variable).
differences DBNs centralised decentralised modes
(Figure 11 uses centralised mode). centralised mode, one DBN
action combination robots. Since controller observes R.s noise-free, add
edges R.x/R.y I.x/I.y R.s=I remove otherwise simplify inference
(thus, Figure 11, R1 loaded I1 R2 unloaded). decentralised mode,
robot observes sensor readings, hence add remove edges
itself, edges robots must permanently added. also means
robots status variables (R.s) must linked I.x/I.y and, therefore, included
I-clusters (to preserve correlation must position R R
loaded I). Moreover, since robot knows action, one DBN
1167

fiSeconds per transition

Albrecht & Ramamoorthy

Centralised
180 Decentralised
160
140
120
100
80
60
BK

PSBF

PF

Figure 12: Results warehouse simulation, using centralised decentralised
control modes. Timing measured UNIX dual-core machine 2.4 GHz averaged
20 different simulations 100 transitions each.
actions, variables associated robots active (the
distributions r defined previous section used average actions).
6.2.3 Results
implemented PSBF, BK, PF C#, using framework Infer.NET (Minka, Winn,
Guiver, & Knowles, 2012) implement BK. allowed BK exploit sparsity
process offered improved memory handling. PSBF optimised sparsity (6)
(8), respectively, summing states btk0 / bt+1
k0 positive. PF naturally
benefits sparsity allows concentrate samples fewer states. number
samples used PF set way controller decisions invariant
random numbers used sampling process PF. done ensure
results repeatable. Finally, maintain sparsity process, probability
belief states lower 0.01 set 0. tested algorithms initialised exact
belief state, shown Figure 10b.
Figure 12 shows time per transition averaged 20 different simulations 100
transitions each. timing reported PSBF includes time needed modify variable
distributions (for overlapping clusters) detect skippable belief factors transition
observation steps, done demand every previously unseen
DBN. centralised mode, PSBF able outperform BK average 49%
PF 36%. PF needed 20,000 samples produce consistent (i.e. repeatable) results.
decentralised mode, PSBF outperformed BK average 17% PF 32%. PF
needed 45,000 samples produce consistent results, due increased variance
process. differences statistically significant, based paired t-tests 5%
significance level. Note PSBF BK slower decentralised mode since
corresponding DBNs much higher inter-connectivity. addition, PSBF updated
belief factors since active variables.
expected, PSBF able exploit high degree passivity process
accelerate filtering task. many cases, meant PSBF needed update less
half belief factors. Precisely many belief factors updated depends
1168

fiExploiting Causality Selective Belief Filtering DBNs

performed action. illustrate this, consider smaller warehouse DBN shown Figure 11
(for centralised mode), R1 moving R2 turning. Here, R1.x, R1.y,
R2.d active variables variables passive (dashed circles), corresponding
passivity 70%. DBN, PSBF updates belief factors corresponding clusters
C1, C2, C5, C8, since contain active variables, also updates belief
factors C3 C4, since directed paths active variables (R1.x R1.y)
them. Therefore, factors updated C6 C7.
consider full warehouse experiment, contains 16 inventory pods 4 robots,
resulting 48 variables 128 I-clusters, 6 R-clusters, 6 S-clusters, 4 D-clusters.
Assume similar situation one robot moves inventory pod, say R4 I1,
R13 turn. case, PSBF updates 3 6 R-clusters (those containing
R4), 0 6 S-clusters (since status change), 3 4 D-clusters (for R13), 38 128
I-clusters (32 I-clusters containing R4 plus 6 I-clusters R13 I1), amounting
total saving 69.44% belief factors need updated.
number states warehouse system (including invalid states) exceeded 1045
states. Therefore, unable compare accuracy tested algorithms terms
relative entropy. Instead, compared accuracy based results task
auctions number completed tasks end simulation. gives
good indication algorithms accuracy, since outcome auction
number completed tasks depend accuracy belief states. centralised
mode, algorithms generated 95% identical task auctions completed 15.7 (BK),
15.5 (PSBF), 15.2 (PF) tasks average. decentralised mode, generated
93% identical auctions completed 12.1 (BK), 12.2 (PSBF), 11.7 (PF) tasks
average. modes, none differences statistically significant. Therefore,
indicates PSBF achieved accuracy similar BK PF.
6.3 Summary Experimental Evaluation
experimental results show PSBF produces belief states competitive accuracy:
synthetic processes, PSBF achieved accuracy average better
comparable accuracy alternative methods. warehouse system, PSBF
able complete statistically equivalent number tasks compared
methods, indicates accuracy equivalent comparable.
Furthermore, experimental results show PSBF performed belief updates
significantly faster alternative methods: synthetic processes, PSBF using
parallel processes outperformed BK 64% largest process (XL), PF took
much time achieve accuracy comparable PSBF. particular, results show
computational gains grow significantly degree passivity
size process. warehouse system, PSBF outperformed alternative methods
49%, substantial saving considering size state space (more
1045 states). Furthermore, computational gains much higher centralised
control mode decentralised control mode, since latter significantly
lower degree passivity. Therefore, shows high degrees passivity bear
great potential filtering task.
1169

fiAlbrecht & Ramamoorthy

7. Free Lunch PSBF
view belief filtering method generally suited types processes.
Instead, method assumes certain structure process (explicitly implicitly)
attempts exploit order render filtering task tractable. Typically,
methods tailored way respect structure perform well
structure present process, suffer significant loss performance structure
absent. instance, PF works best processes low degrees uncertainty, since
means fewer state samples needed acceptable approximations.
hand, number samples needed acceptable approximations grow substantially
degree uncertainty process (as shown experiments). another
example, BK works best processes little correlation state variables, since
means belief factors small processed efficiently. However,
many variables strongly correlated, BK typically becomes infeasible.
Therefore, structural assumptions taken account choosing
filtering method specific process.
formal account view given Free Lunch theorems (Wolpert
& Macready, 1997, 1995) state that, intuitively speaking, two algorithms
equivalent performance averaged possible instances problem.
words, classes problem instances algorithm better performance
algorithm B, must classes problem instances
worse performance B. Then, question is: class problem instances (that
is, processes) PSBF expected achieve good performance? class essentially
described following three criteria:
Degree passivity PSBF attempts accelerate filtering task omitting
transition step many belief factors possible. depends passivity
variables state clusters. ideal case, process exhibits high degree
passivity PSBF omit transition step many belief factors.
worst case, process passive variables all, PSBF update
belief factors transition step. However, discussed Section 5.5, high degree
passivity necessarily sufficient infer many clusters skipped
transition step, since passive variables could distributed way
cluster skipped (e.g. passive variables distributed uniformly
amongst state clusters). Therefore, optimal case, passivity concentrated
correlated state variables passive variables end clusters.
Size state clusters space time complexity belief state representation
PSBF exponential size largest state cluster (cf. Section 5.5). Therefore,
ideal case, relevant variable correlations captured small state
clusters cost storing belief factors performing update procedures
small. worst case, large state clusters required retain variable
correlations cost storing updating belief factors large. Another reason
state clusters small way PSBF performs
transition step. One pre-requisite omitting transition step belief factor
variables corresponding cluster passive. many variables
1170

fiExploiting Causality Selective Belief Filtering DBNs

one cluster, less likely variables cluster passive, and,
therefore, less likely cluster skipped.
Structure observations third criterion, though arguably less important
criteria, structure observations (i.e. way observation
variables depend state variables) size observation clusters (Cl ).
PSBF attempts accelerate observation step skipping state
clusters whose variables structurally independent observation, and,
cluster cannot skipped, incorporating observation clusters
relevant update. Therefore, ideal case, fraction state clusters
depend observation, relevant correlations observation variables
captured small observation clusters. worst case, state clusters
depend observation sense, structure observation
allow efficient clustering.
Thus, summary, PSBF suitable processes high degrees passivity
relevant variable correlations captured small state observation
clusters. hand, PSBF may suitable low degrees
passivity, large state observation clusters necessary retain relevant
variable correlations process.
addition identifying class processes filtering method suitable,
also important justify practical relevance class. work, interested
robotic physical decision processes (as shown examples experiments).
systems typically exhibit number features: First all, robotic systems usually
causal structure (e.g. Mainzer, 2010; Pearl, 2000). Passivity, specific type
causality, observed many robotic systems, including robot arm used
examples multi-robot warehouse system Section 6.2. Furthermore, robotic systems
typically modular structure, module responsible specific
subtask may interact modules. modular structure often allows
efficient clustering, sense module corresponds cluster correlated state
variables. Finally, sensors used robotic systems typically provide information
certain aspects system, components system may benefit
sensor information. words, independencies state
observation variables. features correspond criteria (above) specify
class processes PSBF suitable filtering method. Therefore, believe
class practically justified.

8. Conclusion
Inferring state stochastic process difficult technical challenge complex
systems large state spaces. key developing efficient solutions identify special
structure process, e.g. topology parameterisation dynamic Bayesian
networks, leveraged render filtering task tractable.
end, present article explored idea automatically detecting exploiting
causal structure order accelerate belief filtering task. considered specific type
causal relation, termed passivity, pertains state variables cause changes
1171

fiAlbrecht & Ramamoorthy

state variables. demonstrate potential exploiting passivity, developed novel
filtering method, PSBF, uses factored belief state representation exploits passivity
perform selective updates belief factors. PSBF produces exact belief states
certain assumptions approximate belief states otherwise. showed empirically,
synthetic processes varying sizes degrees passivity well example
complex multi-robot system, PSBF faster several alternative methods
achieving competitive accuracy. particular, results showed computational
gains grow significantly size process degree passivity.
work demonstrates system exhibits much causal structure,
great potential exploiting structure render filtering task tractable.
particular, experiments support initial hypothesis factored beliefs passivity
useful combination large processes. insight relevant complex processes
high degrees causality, robots used homes, offices, industrial factories,
filtering task may constitute major impediment due often large state
space system.
several potential directions future work. example, would useful
know definition passivity could relaxed variables fall
definition, principal idea behind PSBF still applicable. One
relaxation could form approximate passivity, allows small probabilities
passive variables change values even relevant parents remain unchanged.
addition, would interesting know idea performing selective updates
belief factors (via passivity) could also applied existing methods use
factored belief state representation (cf. Section 2.1). Finally, another useful avenue future
work would formulate additional types causal relations exploited
ways similar PSBF exploits passivity, perhaps ways that.

Acknowledgements
article result long debate presented topic, process benefited
number discussions suggestions. particular, authors wish thank
anonymous reviewers NIPS12 UAI13 conferences well Journal AI
Research; attendees workshop Advances Causal Inference held UAI15;
colleagues School Informatics University Edinburgh. Furthermore,
authors acknowledge financial support German National Academic Foundation,
UK Engineering Physical Sciences Research Council (grant number EP/H012338/1),
European Commission (TOMSY Grant Agreement 270436).

1172

fiExploiting Causality Selective Belief Filtering DBNs

Appendix A. Proof Theorem 1
prove Theorem 1, useful first establish following lemma:
Lemma 1. (A1) holds xt+1
Ck passive ,

s, s0 : Tka (s, s0k ) = 1 sk = s0k .
Proof.
: fact (A1) means a,i Ck xt+1
Ck . Since xt+1
Ck


passive , follows xtj a,i passive , a,i . Therefore, given
Tka (s, s0k ) = 1 clause (ii) Definition 3, follows sk = s0k .
: Follows directly (A1) fact xt+1
Ck passive .


Using Lemma 1, give compact proof Theorem 1:


Theorem 2. (A1) (A2) hold, xt+1
Ck passive ,


: bt+1
k (sk ) = bk (sk ).

Proof.
0
bt+1
k (sk )

=

1

X



Tka (s, s0k )

S(pat (Ck ))

=

1

X

btk0 (sk0 )

k0 :[xt+1
Ck0 : xti pat (Ck )]




Lem1







Tka (s, s0k )



btk0 (sk0 )

S(pat (Ck )):sk =s0k k0 :[xt+1
Ck0 : xti pat (Ck )]




=

1 btk (sk )

X



Tka (s, s0k )



btk0 (sk0 )

S(pat (Ck )):sk =s0k k0 6= k:[xt+1
Ck0 : xti pat (Ck )]




{z

|

(A1)

= 1

=

1 btk (sk )

=

btk (sk ). (1 = 1 since btk normalised)

1173

}

fiAlbrecht & Ramamoorthy

Appendix B. Proof Theorem 2
prove Theorem 2, first note following proposition:



Proposition 1. xt+1
Ck marginally independent yjt+1 t+1 ,


s, s0 : k0 6=k sk0 = s0k0 (s, ot ) = (s0 , ot ).

proposition follows directly definition.

Using Proposition 1, give compact proof Theorem 2:


Theorem 2. xt+1
Ck marginally independent yjt+1 t+1 ,

t+1
: bt+1
k (sk ) = bk (sk ).

Proof.
X

t+1 0
0
bt+1
k (sk ) = 2 bk (sk )





(s, ot+1 )

0
bt+1
k0 (sk )

t+1 )) : = s0 k 0 6= k : C 0 pat+1 (Y t+1 ) 6=
S(pat+1
k
k

(Y
k




|

Prop1

{z

= constant , independent

=

bt+1 (s0k )
P k t+1 00
s00 bk (sk )
k

=

bt+1 (s0k )
P k t+1 00
s00 bk (sk )
k

0
= bt+1
k (sk ).

1174

}
s0k

fiExploiting Causality Selective Belief Filtering DBNs

Appendix C. Mixture Gaussians
Algorithm 4 provides simple procedure randomly generates mixture Gaussians
(i.e. set normal distributions) synthetic processes Section 6.1. algorithm
takes input number n state variables returns set G Gaussians whose means
set {1, ..., n}. number Gaussians, means, variances
chosen automatically achieve good coverage state variables minimising
(visual) overlap Gaussians. See Figure 7 example.

Algorithm 4 MixtureOfGaussians(n)
1:

Input: number state variables n

2:

Parameters: 4, min 5 , max

3:

Output: mixture Gaussians G

4:

G

5:

R {(1, ..., n)}

6:

R 6=

n
10

7:

R next element R

8:

R R \ {R}

9:

R(drand |R|e) // rand returns random number (0, 1)

10:

1 min[ R(1), R(|R|) ]

11:
12:

min[max , max[min , rand ]]


G G (, 2 ) // mean variance Gaussian

13:

R (R(1), R(2), ..., R(p)) R(p) <

14:

R+ (R(q), R(q + 1), ..., R(|R|)) R(q) > +

15:

R 6=

16:
17:
18:
19:

R R {R }
R+ 6=

R R {R+ }
return G

1175

fiAlbrecht & Ramamoorthy

References
Astrom, K. (1965). Optimal control Markov processes incomplete state information.
Journal Mathematical Analysis Applications, 10, 174205.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: structural assumptions computational leverage. Journal Artificial Intelligence Research, 11 (1),
194.
Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-specific independence Bayesian networks. Proceedings 12th Conference Uncertainty
Artificial Intelligence, pp. 115123.
Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes.
Proceedings 14th Conference Uncertainty Artificial Intelligence, pp. 3342.
Boyen, X., & Koller, D. (1999). Exploiting architecture dynamic systems. Proceedings
16th National Conference Artificial Intelligence, pp. 313320.
Brafman, R. (1997). heuristic variable grid solution method POMDPs. Proceedings
14th National Conference Artificial Intelligence, pp. 727733.
DAndrea, R., & Wurman, P. (2008). Future challenges coordinating hundreds autonomous vehicles distribution facilities. Proceedings IEEE International
Conference Technologies Practical Robot Applications, pp. 8083.
Dean, T., & Kanazawa, K. (1989). model reasoning persistence causation.
Computational Intelligence, 5, 142150.
Dias, M., Zlot, R., Kalra, N., & Stentz, A. (2006). Market-based multirobot coordination:
survey analysis. Proceedings IEEE, 94 (7), 12571270.
Doucet, A., de Freitas, N., & Gordon, N. (2001). Sequential Monte Carlo Methods Practice.
Springer Science & Business Media.
Doucet, A., De Freitas, N., Murphy, K., & Russell, S. (2000). Rao-Blackwellised particle
filtering dynamic Bayesian networks. Proceedings 16th Conference
Uncertainty Artificial Intelligence, pp. 176183.
Geiger, D., Verma, T., & Pearl, J. (1989). d-separation: theorems algorithms.
Proceedings 5th Conference Uncertainty Artificial Intelligence, pp. 139
148.
Gordon, N., Salmond, D., & Smith, A. (1993). Novel approach nonlinear/non-Gaussian
Bayesian state estimation. IEE Proceedings F (Radar Signal Processing), Vol.
140, pp. 107113.
Hart, P., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determination
minimum cost paths. IEEE Transactions Systems Science Cybernetics,
Vol. 4, pp. 100107.
Hauskrecht, M. (2000). Value-function approximations partially observable Markov
decision processes. Journal Artificial Intelligence Research, 13, 3394.
1176

fiExploiting Causality Selective Belief Filtering DBNs

Heckerman, D. (1993). Causal independence knowledge acquisition inference.
Proceedings 9th Conference Uncertainty Artificial Intelligence, pp. 122
127.
Heckerman, D., & Breese, J. (1994). new look causal independence. Proceedings
10th Conference Uncertainty Artificial Intelligence, pp. 286292.
Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning acting partially
observable stochastic domains. Artificial intelligence, 101 (1), 99134.
Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles Techniques.
MIT Press.
Kullback, S., & Leibler, R. (1951). information sufficiency. Annals Mathematical Statistics, 22 (1), 7986.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computations probabilities graphical
structures application expert systems. Journal Royal Statistical
Society. Series B (Methodological), 50 (2), 157224.
Lovejoy, W. (1991). Computationally feasible bounds partially observed Markov decision
processes. Operations Research, 39, 162175.
Mainzer, K. (2010). Causality natural, technical, social systems. European Review,
18, 433454.
Minka, T., Winn, J., Guiver, J., & Knowles, D. (2012). Infer.NET 2.5.. Microsoft Research
Cambridge. http://research.microsoft.com/infernet.
Murphy, K. (2001). Bayes net toolbox Matlab. Computing Science Statistics,
33 (2), 10241034. https://code.google.com/p/bnt/.
Murphy, K., & Weiss, Y. (2001). factored frontier algorithm approximate inference
DBNs. Proceedings 17th Conference Uncertainty Artificial Intelligence,
pp. 378385.
Murphy, K. (2002). Dynamic Bayesian Networks: Representation, Inference Learning.
Ph.D. thesis, University California, Berkeley.
Ng, B., Peshkin, L., & Pfeffer, A. (2002). Factored particles scalable monitoring.
Proceedings 18th Conference Uncertainty Artificial Intelligence, pp. 370
377.
Pasula, H., Zettlemoyer, L., & Kaelbling, L. (2007). Learning symbolic models stochastic
domains. Journal Artificial Intelligence Research, 29, 309352.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann.
Pearl, J. (2000). Causality: Models, Reasoning, Inference. Cambridge University Press.
Pineau, J., Gordon, G., & Thrun, S. (2003). Point-based value iteration: anytime algorithm
POMDPs. Proceedings 18th International Joint Conference Artificial
Intelligence, Vol. 18, pp. 10251032.
Poole, D., & Zhang, N. (2003). Exploiting contextual independence probabilistic inference.
Journal Artificial Intelligence Research, 18, 263313.
1177

fiAlbrecht & Ramamoorthy

Poupart, P., & Boutilier, C. (2000). Value-directed belief state approximation POMDPs.
Proceedings 16th Conference Uncertainty Artificial Intelligence, pp.
497506.
Poupart, P., & Boutilier, C. (2001). Vector-space analysis belief-state approximation
POMDPs. Proceedings 17th Conference Uncertainty Artificial
Intelligence, pp. 445452.
Poupart, P., & Boutilier, C. (2002). Value-directed compression POMDPs. Advances
Neural Information Processing Systems, pp. 15471554.
Roy, N., Gordon, G., & Thrun, S. (2005). Finding approximate POMDP solutions
belief compression. Journal Artificial Intelligence Research, 23, 140.
Smith, T., & Simmons, R. (2005). Point-based POMDP algorithms: improved analysis
implementation. Proceedings 21st Conference Uncertainty Artificial
Intelligence, pp. 542549.
Sondik, E. (1971). Optimal Control Partially Observable Markov Processes. Ph.D.
thesis, Stanford University.
Srinivas, S. (1993). generalization noisy-or model. Proceedings 9th Conference
Uncertainty Artificial Intelligence, pp. 208215.
Washington, R. (1997). BI-POMDP: bounded, incremental partially-observable Markovmodel planning. Recent Advances AI Planning, pp. 440451. Springer.
Wolpert, D., & Macready, W. (1995). free lunch theorems search. Tech. rep. SFI-TR95-02-010, Santa Fe Institute.
Wolpert, D., & Macready, W. (1997). free lunch theorems optimization. IEEE
Transactions Evolutionary Computation, 1 (1), 6782.
Wurman, P., DAndrea, R., & Mountz, M. (2008). Coordinating hundreds cooperative,
autonomous vehicles warehouses. AI Magazine, 29 (1), 9.
Zhang, N., & Poole, D. (1996). Exploiting causal independence Bayesian network inference.
Journal Artificial Intelligence Research, 5, 301328.
Zhou, R., & Hansen, E. (2001). improved grid-based approximation algorithm
POMDPs. Proceedings 17th International Joint Conference Artificial
Intelligence, pp. 707716.

1178

fiJournal Artificial Intelligence Research 55 (2016) 653-683

Submitted 06/15; published 03/16

Exact Algorithms MRE Inference
Xiaoyuan Zhu
Changhe Yuan

XIAOYUAN . ZHU @ QC . CUNY. EDU
CHANGHE . YUAN @ QC . CUNY. EDU

Queens College, City University New York
65-30 Kissena Blvd., Queens, NY 11367

Abstract
Relevant Explanation (MRE) inference task Bayesian networks finds
relevant partial instantiation target variables explanation given evidence maximizing
Generalized Bayes Factor (GBF). exact MRE algorithm developed previously
except exhaustive search. paper fills void introducing two Breadth-First Branch-andBound (BFBnB) algorithms solving MRE based novel upper bounds GBF. One upper
bound created decomposing computation GBF using target blanket decomposition
evidence variables. upper bound improves first bound two ways. One
split target blankets large converting auxiliary nodes pseudo-targets
scale large problems. perform summations instead maximizations
target variables target blanket. empirical evaluations show proposed
BFBnB algorithms make exact MRE inference tractable Bayesian networks could
solved previously.

1. Introduction
Bayesian networks probabilistic models capture conditional independencies
random variables directed acyclic graphs, provide principled approaches scientific explanation. Explanation tasks Bayesian networks classified three categories: explanation
reasoning, explanation model, explanation evidence (Lacave & Diez, 2002). goal
explanation reasoning Bayesian networks explain reasoning process used produce results credibility results established. goal explanation
model present knowledge encoded Bayesian network easily understandable forms
visual aids experts users examine even update knowledge. goal
explanation evidence explain observed variables particular states
using variables domain.
research focuses developing algorithms solving one methods explaining
evidence Bayesian networks, Relevant Explanation (MRE) (Yuan & Lu, 2007; Yuan,
Lim, & Lu, 2011b). idea MRE find partial instantiation target variables
maximizes Generalized Bayes Factor (GBF) (Fitelson, 2007; Good, 1985) explanation
evidence. GBF rational function probabilities suitable comparing explanations
different cardinalities. MRE shown theoretically empirically able prune
away independent less relevant variables final explanation (Yuan et al., 2011b; Yuan,
Liu, Lu, & Lim, 2009; Pacer, Lombrozo, Griffiths, Williams, & Chen, 2013).
Due difficulty finding meaningful upper bound GBF, exact algorithms
developed solve MRE except exhaustive search. local search Markov chain Monte
Carlo methods proposed previously (Yuan et al., 2009; Yuan, Lim, & Littman, 2011a).
c
2016
AI Access Foundation. rights reserved.

fiZ HU & UAN

paper, introduce first non-trivial exact MRE algorithms based Breadth-First Branchand-Bound (BFBnB) search. key idea proposed methods decompose whole
Bayesian network set overlapping subnetworks using target blanket decomposition
evidence variables. subnetwork characterized subset evidence variables target
blanket d-separates evidence variables target evidence variables. upper
bound GBF derived solving independent optimization problems subnetworks.
also show bound tightened merging target blankets share target variables.
propose another improved upper bound based two novel ideas. First,
decomposition may lead large target blankets prevent branch-and-bound algorithm
scaling large MRE problems. address problem, propose split large target blankets
converting auxiliary nodes pseudo-targets introduce additional decomposition. Second,
find upper bound tightened identifying summing enclosed-targets
target blanket. proposed upper bounds used BFBnB algorithms pruning
search space. evaluated algorithms set benchmark diagnostic Bayesian networks.
Experimental results show proposed algorithms make exact MRE inference tractable
Bayesian networks could solved previously.
rest paper organized follows. basics Bayesian networks MRE
problem introduced Section 2. Section 3, novel target blanket upper bound proposed.
improved upper bound discussed Section 4. proposed BFBnB algorithms introduced Section 5. Section 6, experimental results presented. Finally, discussions
conclusions provided Section 7.

2. Background
section, introduce basics Bayesian networks, explanation Bayesian networks,
MRE problem.
2.1 Bayesian Networks Moral Graph
Bayesian network (Pearl, 1988; Darwiche, 2009; Koller & Friedman, 2009) represented
directed acyclic graph (DAG). nodes DAG represent random variables. lack arcs
DAG define conditional independence relations among nodes. arc
node X, say parent X, X child . use upper-case letters
denote variables X variable sets X, lower-case letters values scalars x vectors
x. node ancestor node X directed path X. Let (X)
denote ancestors X, smallest ancestral set (X) node set X defined
(X) = X (Xi X (Xi )). directed graphs, d-separation describes conditional
independence relation two sets nodes X Y, given third set nodes Z, i.e.,
p(X|Z, Y) = p(X|Z). Markov blanket X smallest node set d-separates X
remaining
Q nodes network. network whole represents joint probability
distribution X p(X|PA(X)), PA(X) set parents X.
moral graph Gm DAG G undirected graph set nodes.
edge X Gm edge G
parents node G. undirected graph, Z separates X Y, Z intercepts
paths X Y. Moral graphs powerful construction explain d-separation.
654

fiE XACT LGORITHMS MRE NFERENCE

Lemma 1 (Lauritzen, Dawid, Larsen, & Leimer., 1990) links d-separation DAG separation
undirected graphs.
Lemma 1. Let X, Y, Z disjoint subsets nodes DAG G. Z d-separates X
Z separates X (GAN (XYZ) )m , (GAN (XYZ) )m moral graph
subgraph G node set (X Z).
2.2 Explanation Evidence Bayesian Networks
Numerous methods developed explain evidence Bayesian networks.
methods make simplifying assumptions focus singleton explanations (Heckerman, Breese,
& Rommelse, 1995; Jensen & Liang, 1994; Kalagnanam & Henrion, 1988). However, singleton
explanations may underspecified unable fully explain given evidence evidence
compound effect multiple causes.
domain multiple interdependent target variables, multivariate explanations often
appropriate explaining given evidence. Maximum Posteriori assignment (MAP) finds
complete instantiation set target variables maximizes joint posterior probability
given partial evidence variables. Recently Kwishthout (2013) extended MAP find
set joint assignments inforbable explanations. Probable Explanation (MPE) (Pearl,
1988) similar MAP except MPE defines target variables unobserved variables.
common drawback methods often produce hypotheses overspecified
may contain irrelevant variables explaining given evidence.
Everyday explanations necessarily partial explanations (Leake, 1995). Various pruning techniques used avoid overly complex explanations. methods grouped
two categories: pre-pruning post-pruning. Pre-pruning methods use context-specific independence relations represented Bayesian networks prune irrelevant variables (Pearl, 1988;
Shimony, 1993; van der Gaag & Wessels, 1993, 1995) applying methods MAP
generate explanations. contrast, post-pruning methods first generate explanations using methods
MAP MPE prune variables important. example method
proposed de Campos et al. (2001).
Several methods aim directly find appropriate explanations. likelihood evidence
used measure explanatory power explanation (Gardenfors, 1988). Chajewska
Halpern (1997) extend approach use value pair <likelihood, prior probability>
order explanations, forcing users make decisions clear order two
explanations. Henrion Druzdzel (1991) assume system set pre-defined explanation scenarios organized tree; use scenario highest posterior probability
explanation. Flores et al. (2005) propose automatically create explanation tree greedily
branching informative variable step maintaining probability
branch tree certain threshold. Nielsen et al. developed another method uses
causal information flow (Ay & Polani, 2008) select variables expand explanation tree.
2.3 Relevant Explanation
explanation evidence Bayesian networks, often classify nodes three categories:
target, evidence, auxiliary. target set represents variables interest inference.
evidence set E represents observed information. auxiliary set represents variables
655

fiZ HU & UAN

interest inference. MRE, finds partial instantiation explanation
given evidence e Bayesian network, formally defined follows (Yuan et al., 2011b).
Definition 1. Let set targets, e given evidence Bayesian network.
Relevant Explanation problem finding partial instantiation x maximum
generalized Bayes factor score GBF (x; e) explanation e, i.e.,
MRE (M; e) = argmax GBF (x; e),

(1)

x,XM

GBF defined
GBF (x; e) =

p(e|x) 1
,
p(e|x)

(2)

x joint value assignment (instantiation) subset X M, x represents
alternative explanations x.
commonly used measure selecting explanatory hypothesis probability explanation given evidence, used MAP MPE find likely configuration set
target variables. Probability-based methods, however, intrinsic capability prune
less relevant facts. Moreover, probability measure quite sensitive modeling choices,
e.g., simply refining model dramatically change best explanation. contrast, MRE maximizes rational function probabilities GBF Equation 2. makes possible compare
explanations different cardinalities prune less relevant variables automatically
principled way. search space MRE exponential number targets, complexity MRE conjectured N P P P -hard (Yuan et al., 2011a), makes naive brute force
algorithm impractical.
study properties generalized Bayes factor, reformulate GBF follows.
GBF (x; e) =
=

p(e|x)
p(x|e)p(x)
=
p(e|x)
p(x)p(x|e)
p(x|e)(1 p(x))
.
p(x)(1 p(x|e))

(3)

Therefore, need prior posterior probabilities hypothesis order compute
GBF. GBF hence able overcome drawback Bayes factor (Jeffreys, 1961)
pairwise comparisons multiple hypotheses.
Belief update ratio useful concept. belief update ratio X given e defined
follows (Yuan et al., 2011b).
p(X|e)
.
(4)
r(X; e) =
p(X)
GBF calculated belief update ratio follows.
p(x|e)(1 p(x))
r(x; e) p(x|e)
=
p(x)(1 p(x|e))
1 p(x|e)
r(x; e) 1
= 1+
.
1 p(x|e)

GBF (x; e) =

1. use p(x) shorthand p(X = x) paper.

656

(5)

fiE XACT LGORITHMS MRE NFERENCE

2.4 Existing Methods Solving MRE
Local search methods tabu search (Glover, 1990) applied solve MRE (Yuan
et al., 2011a). Tabu search starts empty solution set. step, generates neighbors
current solution adding, changing, deleting one target variable. tabu search selects
best neighbor highest GBF score visited before. tabu search,
best neighbor worse current solution. stop tabu search properly, upper bounds
set total number search steps number search steps since last
improvement L stopping criteria. Another Markov Chain Monte Carlo algorithm integrates
reversible-jump MCMC algorithm (Green, 1995) simulated annealing (Kirkpatrick, Gelatt,
& Vecchi, 1983) find solution simulating non-homogeneous Markov chain eventually
concentrates mass mode distribution GBF scores solutions. methods
provide approximate solutions whose quality unknown. Furthermore, accuracy
efficiency methods typically highly sensitive tunable parameters.
2.5 Branch Bound Algorithms Solving MPE/MAP
Branch-and-bound algorithms developed solving MAP MPE using upper
bounds derived based property optimization criterion structure Bayesian networks. mini-bucket upper bound used AND/OR tree search solving MPE (Dechter
& Rish, 2003; Marinescu & Dechter, 2009). Recently, improved mini-bucket upper bound (Marinescu, Dechter, & Ihler, 2014) proposed guide AND/OR search exact MAP inference.
work (Choi, Chavira, & Darwiche, 2007) showed mini-bucket upper bound
derived node splitting scheme. solve MAP exactly, upper bound proposed commuting order max sum operations MAP calculation (Park & Darwiche, 2003).
exact algorithm proposed solving MAP computing upper bounds arithmetic circuit
compiled Bayesian network (Huang, Chavira, & Darwiche, 2006).

3. Novel Upper Bound Based Target Blanket Decomposition Solving MRE
difficult solve MRE problems exactly exponential search space
need probabilistic inference search step. naive brute-force search method scale
Bayesian networks 15 targets. work, develop breadth-first branch-and-bound
algorithms use suite new upper bounds based target blanket decomposition prune
search space. algorithm makes possible solve MRE problems targets exactly.
3.1 Search Space Formulation
Assuming n targets, target states, search space MRE contains (d +
1)n 1 possible states (or solutions). organize search space search tree instantiating
targets according total order targets. state tree contains values
subset targets V. V defined expanded set targets. set variables U
yet considered expansion defined unexpanded set; i.e., U = {Ui |Ui M; Vj
V, Vj < Ui }; set variables P considered used defined
pruned set; i.e., P = \ {V U}. Figure 1 demonstrates different target sets state
{X1 , X4 , X6 } 9-target MRE problem.
657

fiZ HU & UAN

Pruned (P)

X1

X3

X2

X5

X4

X6 X7

Expanded (V)

X8

X9

Unexpanded (U)

Figure 1: Different types targets search state MRE, i.e., expanded (gray), pruned (white),
unexpanded (black).





ab

ab



ac

b

ac

ab

b

ab

ac

c

ac

bc

c

bc

bc

bc

abc abc abc abc abc abc abc abc

Figure 2: search tree MRE problem three targets, i.e., A, B, C. example
sub-tree rooted {b} marked gray.

search tree empty state root. non-leaf state tree number
children states instantiate one unexpanded target. Figure 2 shows example search tree
three targets = {a, a}, B = {b, b}, C = {c, c} order. Different
branches search tree may different numbers layers, states
cardinality appear layer.
possible use dynamic ordering expand targets improve GBF score
first. However, shown static ordering actually make computing upper bounds much
faster ultimately make search efficient (Yuan & Hansen, 2009). therefore simply
ordered targets according indices work.
3.2 Upper Bound Based Target Blanket Decomposition
MRE inference, upper bound state greater GBF score descendant states S. search, keep track highest-scoring state prune
whole subtree upper bound less GBF current best solution. following
introduce novel upper bound MRE inference.
first define concept called target blanket decomposition evidence variables.
Definition 2. target blanket decomposition evidence variables tuple < EvdList, TBList >
satisfies following properties: 1) EvdList set exclusive exhaustive subsets,
i.e.,EvdList = {Ei }, E = Ei Ei Ej = 6= j. 2) TBList set target blankets,
one blanket TB (Ei ) evidence subset Ei , TB (Ei ) minimal set targets
d-separates Ei targets evidence variables.
658

fiE XACT LGORITHMS MRE NFERENCE

target blanket decomposition naturally decomposes whole network overlapping
subnetworks, subnetwork containing evidence subset Ei target blanket TB (Ei ).
Target blankets named due resemblance Markov blankets, strictly speaking,
really Markov blankets evidence variables, may auxiliary
variables them.
given target set evidence set E, may exist multiple target blanket decompositions satisfy Definition 2. see that, note simply merge two evidence
subsets corresponding target blankets decomposition obtain another decomposition. However, among possible decompositions, minimal target blanket decomposition,
define follows.
Definition 3. minimal target blanket decomposition evidence variables target blanket
decomposition < EvdList, TBList > proper subset Ei EvdList valid
target blanket.
minimal target blanket decomposition must unique according following theorem.
Theorem 1. minimal target blanket decomposition evidence set E unique.
Proof. Proof contradiction: assume two minimal target blanket decompositions D1
D2 . Based property minimal target blanket decomposition, evidence subset D1
proper subset evidence subset D2 , vice versa. must exist two distinct
1
evidence subsets ED
D1 EjD2 D2 whose overlap Eij empty. must

true Eij d-separated target evidence variables given subset
2
TB (EiD1 ) TB (ED
j ), contradicting definition.
immediately follows non-minimal target blanket decompositions generated
performing merging minimal decomposition.
upper bound GBF derived multiplying upper bounds belief update ratios
calculated subnetworks. first derive upper bound belief update ratio
following theorem.
Theorem 2. Let = {X1 , X2 , . . . , Xn } set targets, e evidence, < EvdList,
TBList > target blanket decomposition E, EvdList = {Ei } TBList =
{TB (Ei )}. Then, subset X M, belief update ratio r(x; e) upper bounded
follows:
!
max
x,XM

C =

Q

p(ei )

r(x; e)




max
z,Z=TB (Ei )


p(e).

659

r(z; ei )

C,

(6)

fiZ HU & UAN

Proof.

formulation r(M; e),

r(M; e) = p(M|e) p(M)

= p(e|M) p(e)

=
p(ei |TB (Ei ))/p(e)


p(TB (Ei )|ei )p(ei )

=

p(TB (Ei ))



/p(e)
!



=

r(TB (Ei ); ei )p(ei ) /p(e).

(7)



third equality based property target blankets. Thus,
!

r(M; e) =
r(TB (Ei ); ei ) C,

(8)



C =
r(m; e),

Q


p(ei ) p(e). Equation 8, immediately following upper bound
!
max r(m; e)





max
z,Z=TB (Ei )

C.

r(z; ei )

(9)

X M, X = M\X, let Si = TB (Ei ) X denote subset targets
summed ith subnetwork, Si = TB (Ei ) X. Thus, separate TB (Ei )
two parts, i.e., related unrelated summation X, indexed = {i : Si 6= }
J = {j : Sj = } respectively. Based definition, have:
X
p(e|X) =
p(e|M)p(X|X)
X

!
X

=

iI

X



p(ei |TB (Ei )) p(X|X)

p(ej |TB (Ej ))

jJ

!




max p(ei |Si si )

iI

si






p(ej |TB (Ej )) .

(10)

jJ

last inequality derived using maximization instead averaging X. Since p(e|X) =
r(X; e)p(e), following upper bound r(x; e) performing maximization according
X sides Equation 10:
max r(x; e) = max
x

x




iI

=


iI

p(e|x)
p(e)
max max
si

si

max

p(ei |si si )p(ei )
p(ei )
!

r(z; ei )

z,Z=TB (Ei )

jJ

660

!






jJ

max

p(ej |z)p(ej ) 1

p(ej )
p(e)


z,Z=TB (Ej )

max
z,Z=TB (Ej )

r(z; ej ) C.

(11)

fiE XACT LGORITHMS MRE NFERENCE

Thus, X M, obtain final upper bound combining Equations 9 11.
!

max r(x; e)
max r(z; ei ) C.
x,XM



z,Z=TB (Ei )

MRE inference, evidence e given, thus C constant. Theorem 2 assumes V = ,
true beginning search. search V 6= , following
corollary derive upper bound belief update ratio.
Corollary 3. Let = {X1 , X2 , . . . , Xn } set targets, e evidence, < EvdList,
TBList > target blanket decomposition E, EvdList = {Ei } TBList =
{TB (Ei )}. Let U V unexpanded expanded target sets state S. Let Ti =
TB (Ei ) V, Ti = TB (Ei )\Ti . Then, subset X U, belief update ratio
r(x v; e) upper bounded follows.
!

max r(x v; e)
max r(z ti ; ei ) C,
(12)
x,XU

C =

Q

p(ei )



z,Z=Ti


p(e).

Based Corollary 3, derive upper bound GBF Theorem 4:
Theorem 4. Let = {X1 , X2 , . . . , Xn } set targets, e evidence, < EvdList,
TBList > target blanket decomposition E, EvdList = {Ei } TBList =
{TB (Ei )}. Let U V unexpanded expanded target sets state S. Let Ti =
TB (Ei ) V, Ti = TB (Ei )\Ti . Then, subset X U, generalized Bayesian
factor score GBF (x v; e) upper bounded follows.

max r(z ti ; ei ) C 1
GBF (x v; e) 1 +

max

z,Z=Ti



,

1 p(v|e)

x,XU

(13)


Q
C = p(ei ) p(e).
Proof. First, formulate GBF using belief update ratio Equation 5.
GBF (m; e) = 1 +

r(m; e) 1
.
1 p(m|e)

subset X U, p(x v|e) = p(x|v, e) p(v|e) p(v|e). Thus
r(x v; e) 1

max
max

GBF (x v; e) 1 +

x,XU

x,XU

.

1 p(v|e)

(14)

using Corollary 3, obtain following upper bound GBF:
!

max

GBF (x v; e) 1 +

x,XU

C =

Q

p(ei )


p(e).

661



max r(z ti ; ei )

z,Z=Ti

1 p(v|e)

C 1
,

fiZ HU & UAN

Moral graph

Directed subgraph

Splitted graph

TB(E2)

TB(E1,E2)


G

H
J

E1

E2
F
E3

K

(A)

B

G

C





H

L

J


E1

E2
F
E3

K

B

G

C





H

L

J

TB(E1)
B


E1

E2
F
E3


K

TB(E3)

TB(E3)

(B)

(C)

C

L

Figure 3: example compiling splitting target blanket decomposition. E1 , E2 , E3
evidence nodes. Gray nodes targets. Others auxiliary nodes. original target
blanket TB (E1 , E2 ) split TB (E1 ) TB (E2 ) converting auxiliary node
pseudo-target.

Using Equation 13, bound descendant states current state S. Equation 13
shows MRE problem (left), need search subsets targets find
best solution. However, calculate upper bound (right), need search fixed target
set TB (Ei ) subnetwork, usually small size easy compute.
instantiation z TB (Ei ), calculate r(z; ei ) store table called belief ratio table.
one table subnetwork. tables computed preprocessing step
looked step search compute upper bound state.
3.3 Compiling Minimal Target Blanket Decomposition
Theorems 2- 4 based factorizing
conditional joint distribution p(e|M) product
Q
set conditional distributions p(ei |TB (Ei )). Thus finding target blanket decomposition,
including evidence subsets target blankets, key part proposed methods.
proposed method, compile minimal target blanket decomposition based Lemma 1
first compiling moral graph. moral graph used prune irrelevant parts network
set network separation tests. compile moral graph, first generate smallest
ancestral set containing target set evidence set E, i.e., (ME). compile
moral graph (GAN (ME) )m . Figure 3(B) illustrates example moral graph compiled
Bayesian network three evidence nodes Figure 3(A). Using Lemma 1, minimal target blanket decomposition evidence variables achieved depth first graph traversal starting
unvisited evidence node moral graph (GAN (ME) )m . three scenarios
node visited.
Case 1: evidence node visited, add evidence current evidence subset
Ei , mark visited, continue visit unmarked neighbors.
Case 2: target visited, add target TB (Ei ) mark visited.
662

fiE XACT LGORITHMS MRE NFERENCE

Algorithm 1 Compiling Minimal Target Blanket Decomposition
Input: target set; E evidence set; MGraph moral graph (GAN (ME) )m .
Output: Target blanket decomposition < EvdList, TBList >, EvdList = {Ei },
TBList = {TB (Ei )}.
1: function C OMPILE INIMALTARGET B LANKETS(M, E, MGraph)
2:
EvdList ;
3:
TBList ;
4:
node Nd MGraph
5:
Nd E Nd visited
6:
EvdSeti ;
7:
TBSeti ;
8:
SearchStack Nd ;
. initialize stack Depth-first search
9:
SearchStack empty
10:
SearchNode SearchStack .pop();
11:
SearchNode visited
12:
continue;
13:
end
14:
mark SearchNode visited;
15:
SearchNode E
. Case 1
16:
EvdSeti .push(SearchNode);
17:
SearchStack .push(SearchNodes unvisited neighbors);
18:
else SearchNode
. Case 2
19:
TBSeti .push(SearchNode);
20:
else SearchNode auxiliary node
. Case 3
21:
SearchStack .push(SearchNodes unvisited neighbors);
22:
end
23:
end
24:
EvdList.push(EvdSeti );
25:
TBList.push(TBSeti );
26:
end
27:
mark targets auxiliary nodes MGraph unvisited;
28:
end
29: end function
Case 3: auxiliary node visited, mark visited continue visit
unmarked neighbors.
restarting search new evidence node, unmark targets auxiliary nodes,
targets may occur different target blankets, e.g., node F shared TB (E1 , E2 )
TB (E3 ) shown Figure 3(B). algorithm stops evidence nodes visited. algorithm summarized Algorithm 1. Furthermore, show Algorithm 1
guaranteed find minimal target blanket decomposition evidence set E.
Theorem 5. Algorithm 1 guaranteed find minimal target blanket decomposition evidence
variables.
663

fiZ HU & UAN

Proof. evidence E, Algorithm 1 stops search path target encountered.
evidence variables belong evidence subset Ei E minimal target blanket
decomposition must visited search paths terminated. Furthermore, since start
search one evidence E Ei , must true search stops minimal
target blanket TB (Ei ) fully visited.
3.4 Merging Target Blankets
computing upper bound, maximize belief update ratio r(TB (Ei ); ei )
TB (Ei ) independently. Thus common targets two different target blankets TB (Ei )
TB (Ej ) may set inconsistent values. much inconsistency may result loose bound.
tighten upper bound merging target blankets share targets. hand,
number targets individual target blanket large, make calculating
belief update ratio tables inefficient even infeasible due excessive memory consumption.
propose merge target blankets share targets constraint number targets
resulting target blanket cannot exceed constant K.
use undirected graph represent problem merging target blankets. nodes
graph denote target blankets. two target blankets share targets, edge
two corresponding nodes. weight edge number targets shared two target
blankets. formulation translates problem merging target blankets graph partition
problem. specifically, merging problem addressed recursively solving minimum bisection problem (Feige & Krauthgamer, 2002) undirected graph, partitions
vertices two equal halves minimize sum weights edges two
partitions.
minimum bisection problem NP-hard problem, however. cannot afford spend
much time computing upper bound. therefore use hierarchical clustering-like greedy
algorithm merging target blankets. first merge pair target blankets one
covers other. remaining target blankets, repeatedly merge two target blankets
share highest number targets long number targets resulting target
blanket exceed K. algorithm iterates two steps target blankets
merged. merge algorithm summarized Algorithm 2.

4. Improved Target Blanket Bound
target blanket upper bound two potential difficulties scaling large Bayesian networks many target variables. First, minimal target blanket decomposition (in Section 3.3)
lead large subnetworks belief ratio tables large build. Second, upper
bound still loose even merging target blankets. section, propose another upper bound based two new techniques improving scalability tightness previous
bound.
4.1 Splitting Large Target Blankets
decomposition method Section 3.3 may lead large target blankets prevent application BFBnB large scale MRE problems. address problem, notice MRE
problems, subnetwork contains three types nodes, i.e., targets TB (Ei ), evidence Ei ,
664

fiE XACT LGORITHMS MRE NFERENCE

Algorithm 2 Merging Target Blankets
Input: target blanket decomposition < EvdList, TBList >, EvdList = {Ei },
TBList = {TB (Ei )}; K maximum number targets target blanket.
Output: updated decomposition < EvdList, TBList >.
1: function ERGE TARGET B LANKETS(EvdList, TBList, K)
2:
MergeFlag True;
3:
MergeFlag
4:
TBListOld TBList;
. TBi denotes TB (Ei )
5:
EvdListOld EvdList;
6:
(TBi , TBj ) pair, TBi , TBj TBListOld
7:
TBi TBj TBList
. merge target blankets subset relation
8:
TBi TBj
9:
TBj .merge(TBi );
10:
TBList.remove(TBi );
11:
Ej .merge(Ei );
12:
EvdList.remove(Ei );
13:
else TBj TBi
14:
TBi .merge(TBj );
15:
TBList.remove(TBj );
16:
Ei .merge(Ej );
17:
EvdList.remove(Ej );
18:
end
19:
end
20:
end
21:
NumTargets 0;
22:
(TBi , TBj ) pair, TBi , TBj TBList
23:
(TBi TBj ).size() > NumTargets (TBi TBj ).size() < K
24:
TBPair (TBi , TBj );
. find TB pair merge
25:
NumTargets (TBi TBj ).size();
26:
EvdPair (Ei , Ej );
27:
end
28:
end
29:
NumTargets 0
30:
MergeFlag False;
31:
else
32:
TBPair .TBi .merge(TBPair .TBj );
33:
TBList.remove(TBPair .TBj );
34:
EvdPair .Ei .merge(EvdPair .Ej );
35:
EvdList.remove(EvdPair .Ej );
36:
end
37:
end
38: end function

665

fiZ HU & UAN

auxiliary nodes Ai , TB (Ei ) marks boundary subnetworks, Ei Ai conditionally independent outside nodes given TB (Ei ). idea split large target
blankets converting auxiliary nodes pseudo-targets act targets compiling
target blanket decomposition calculating belief ratio tables. pseudo-targets add additional d-separation Bayesian network, thus split large target blankets smaller ones.
Theorems 4 6 guarantee that, split, still find upper bound GBF.
Theorem 6. Let TB (Ei ) target blanket ith subset evidence Ei , Ai set
auxiliary nodes ith subnetwork target blanket decomposition. Assuming
converting subset Ai pseudo-targets, ith subnetwork decomposed
set target blankets TB (Eij ), TB (Ei ) Y=j TB (Eij ), Ei =j Eij , Eij Eik =
j 6= k. Then, belief update ratio r(x; ei ) upper bounded follows.

max
r(x; ei )
max
r(z; eij ) C,
(15)
x,X=TB (Ei )

C =

Q

j

j

z,Z=TB (Eij )


p(eij ) p(ei ).

Proof. Let X = TB (Ei ), Ai have,
X
p(ei |X) =
p(ei |X Y)p(Y|X)


=

XY





j

p(eij |TB (Eij ))p(Y|X)

j

max
z,Z=TB (Eij )Y

p(eij |TB (Eij )\Z, z).

(16)

second equality based property target blankets.
Since p(ei |X) = r(X; ei )p(ei ), following upper bound r(x; ei ),

max
r(x; ei )
max
r(z; eij ) C,
x,X=TB (Ei )

C =

Q

j

j

z,Z=TB (Eij )


p(eij ) p(ei ).

introduce greedy algorithm convert auxiliary nodes pseudo-targets incrementally
size resulting target blanket exceed K. algorithm, split target
blankets whose sizes exceed K using following steps.
Step 1: Calculate degree (i.e., number neighboring nodes) auxiliary node
Aij moral graph, sort according degrees descending order.
Step 2: Convert one auxiliary node pseudo-target according order, perform
depth-first search Section 3.3 compile minimal target blankets decomposition
subnetwork.
Step 3: Stop size resulting target blanket exceed K, repeat Step 2
auxiliary nodes converted.
666

fiE XACT LGORITHMS MRE NFERENCE

Algorithm 3 Splitting Target Blankets
Input: target blanket decomposition < EvdList, TBList >, EvdList = {Ei },
TBList = {TB (Ei )}; K maximum number targets target blanket. MGraph
moral graph (GAN (ME) )m .
Output: updated decomposition < EvdList, TBList >.
1: function PLIT TARGET B LANKETS(EvdList, TBList, K, MGraph)
2:
TBi TBList
3:
TBi .size() > K
4:
Aij Ai descending degree . Ai auxiliary node set TBi
5:
convert Aij pseudo-target;
6:
< EvdList , TBList > CompileMinimalTargetBlankets(TBi ,Ei ,MGraph);
7:
TBij .size() < K, TBij TBList
8:
break;
9:
end
10:
end
11:
end
12:
end
13: end function

algorithm summarized Algorithm 3. example splitting target blanket
illustrated Figure 3(C). original target blanket TB ({E1 , E2 }) = {I, F, B, D} split
TB (E1 ) = {A, B, D} TB (E2 ) = {A, F, I} converting auxiliary node pseudo-target.
4.2 Tightening Upper Bound
combining Theorems 4 6, see splitting process makes upper bound loose
maximizations pseudo-targets. order maintain even improve search
efficiency, need re-tighten bound. key idea comes Theorem 4, Ti
contains two types variables, i.e., pruned-targets unexpanded targets. Equation 13,
pruned-targets unexpanded targets state maximized derive upper bound. Since
pruned-targets occur subsequently generated states, maximizing pruned-targets
makes upper bound loose. However, directly summing pruned-targets subnetworks
change structure target blanket decomposition, thus makes upper bound invalid.
notice certain targets summed instead maximized without
affecting decomposition. First, need define another type targets called enclosed-target
set H target blanket following.
Definition 4. MRE inference, enclosed-targets H target blanket targets
conditionally independent variables outside target blanket given remaining variables target blanket.
words, enclosed-targets targets blocked (d-separated) outside
rest target blanket. Definition 4, see enclosed-target
occurs one target blanket. example Figure 3(B), J L enclosed-targets
TB (E3 )={F, J, L}.
667

fiZ HU & UAN

idea sum enclosed-targets individual subnetworks TB (Ei ) order
produce tighter upper bound. following theorem.
Theorem 7. Let = {X1 , X2 , . . . , Xn } set targets, e evidence, < EvdList,
TBList > target blanket decomposition E, EvdList = {Ei } TBList =
{TB (Ei )}. Let U, V, P unexpanded, expanded, pruned target sets state S.
Let Ti = TB (Ei ) V, Ti = TB (Ei )\Ti . Let Hi enclosed-target set TB (Ei )
= Hi P. Then, subset X U, belief update ratio r(x v; e) upper
bounded follows.

max r(x v; e)
max r(z ti ; ei ) C,
(17)
x,XU

C =

Q

p(ei )



z,Z=Ti \His


p(e).

Proof. X U, let W = X V W = M\W. Similar Theorem 2, let
Si = TB (Ei ) W, = {i : Si 6= }, J = {j : Sj = }, have,
X
p(e|W) =
p(e|M)p(W|W)
W

=

XY
W



p(ei |TB (Ei )) p(W|W)
p(ej |TB (Ej )).

iI

(18)

jJ

Equation 18 based property target blankets.
Assuming X = U\X, W = P X Si W. Since occurs
TB (Ei ), perform summation non-empty Equation 18 follows.
X
p(ei |TB (Ei ))p(His |M\His )


=

X p(ei , |TB (Ei )\His )


=

X

p(His |TB (Ei )\His )

p(His |M\His )

p(ei , |TB (Ei )\His )



= p(ei |TB (Ei )\His ),

(19)

p(His |TB (Ei )\His ) = p(His |M\His ), since conditionally independent
targets given TB (Ei )\His .
Based Equation 19, obtain upper bound summing Equation 18
replacing summation (Si \His ) maximization.


p(e|W)
max p(ei |TB (Ei )\Si , z)
p(ej |TB (Ej )).
(20)
iI

z,Z=Si \His

jJ

Since p(e|W) = r(W; e)p(e), following upper bound r(x v; e) re-organizing
partition TB (Ei ), i.e., Si based partition Ti based partition.


max r(x v; e)
max r(z ti ; ei )
max r(z tj ; ej ) C,
(21)
x,XU

iI

z,Z=Ti \His

jJ

668

z,Z=Tj

fiE XACT LGORITHMS MRE NFERENCE

Hi


B

B

C



C



C



C











Figure 4: Subproblem graph compiling belief ratio tables incremental algorithm.

Q
C = p(ei ) p(e).
Thus, X U, obtain tighter upper bound r(x v; e).

max r(x v; e)
max r(z ti ; ei ) C.
x,XU



z,Z=Ti \His

substituting Equation 17 14, tightened upper bound GBF Theorem 8.
Theorem 8. Let = {X1 , X2 , . . . , Xn } set targets, e evidence, < EvdList,
TBList > target blanket decomposition E, EvdList = {Ei } TBList =
{TB (Ei )}. Let U, V, P unexpanded, expanded, pruned target sets state S.
Let Ti = TB (Ei ) V, Ti = TB (Ei )\Ti . Let Hi enclosed-target set TB (Ei )
= Hi P. Then, subset X U, generalized Bayesian factor score
GBF (x v; e) upper bounded follows.

max r(z ti ; ei ) C 1
max

GBF (x v; e) 1 +

x,XU

C =

Q

p(ei )



z,Z=Ti \His

1 p(v|e)

,

(22)


p(e).

main difference Theorems 4 8 Theorem 4 maximizes upper
bound Theorem 8 sums results tighter upper bound.
4.3 Compiling Belief Ratio Tables
belief ratio tables contain belief update ratios configurations series target
sets generated based TB (Ei ), used calculate upper bounds MRE inference.
compile belief ratio tables, find set enclosed-targets Hi converting target
TB (Ei ) auxiliary node individually. add new targets evidence nodes
original subnetwork, add target Hi . subsets Hi used build
2|Hi | belief ratio tables TB (Ei ), |Hi | denotes size Hi . Let Hij subset Hi ,
669

fiZ HU & UAN

target set belief ratio table TB (Ei )\Hij . Since number belief ratio tables
increases exponentially according |Hi |, limit |Hi | larger N time
space reasons. Different enclosed-targets contribute different amount tightening upper bound,
thus select top N enclosed-targets Hi sorting enclosed-targets Hij according
belief update ratios maxhij r(hij ; ei ) descending order.
compile belief ratio tables, need calculate belief update ratios configurations target set TB (Ei )\Hij . straightforward method compile 2|Hi | belief ratio
tables independently. slow redundant computation. work, propose
incremental algorithm compiles belief ratio tables gradually smallest target set
Hi =TB (Ei )\Hi . Figure 4, assuming four enclosed-targets, i.e., Hi = {A, B, C, D},
organized alphabetical order, node subproblem graph represents target. first
compile belief ratio table Hi , traverse subproblem graph breadth-first order.
state (node), compile belief ratio table adding target complied belief ratio
table parent state. traversing subproblem graph, algorithm compiles belief
ratio tables. algorithm summarized Algorithm 4.
proposed algorithm, calculate hash key HK target blanket based Hi .
belief ratio tables target blanket stored hash table hash keys calculated
based corresponding subset enclosed-targets Hij .

5. Breadth-First Branch-and-Bound Algorithms
MRE inference, search nodes potential solutions except root node.
choose variety search methods explore search tree, e.g., depth-first search, best-first
search, breadth-first search. Since MRE prunes away independent less relevant targets,
usually number targets optimal solution large. Thus breadth-first search may
reach optimal solutions faster search strategies. choose breadthfirst search MRE inference. breadth-first search may require memory store
unexpanded states layer, however.
order utilize proposed upper bounds, BFBnB algorithms two major steps:
preprocessing search. preprocessing step includes compiling minimal target blanket decomposition, merging target blankets sharing targets, splitting large target blankets, creating
belief ratio tables TB (Ei ). suite upper bounds derived including different combinations preprocessing modules. empirical results show target
blanket upper bounds Theorems 4 8 show excellent performance.
search step BFBnB explores search tree layer layer keeping track
highest-scoring state, prunes state upper bound less current best GBF.
proposed upper bounds lead two versions BFBnB algorithms, MPBnd SPBnd. MPBnd
based upper bound Theorem 4, maximizes unexpanded targets
pruned-targets. Thus, MPBnd, target blanket TB (Ei ) one belief ratio table
computed calculating belief update ratios configurations TB (Ei ). state S,
search configuration belief ratio table consistent expanded targets
ti highest belief update ratio. calculate upper bound using Theorem 4.
SPBnd based upper bound Theorem 8, sums part pruned-targets
based structure Bayesian networks. Thus, SPBnd, target blanket TB (Ei )
set belief ratio tables derived calculating belief update ratios
670

fiE XACT LGORITHMS MRE NFERENCE

Algorithm 4 Compile Belief Ratio Tables
Input: target set; E evidence set; e given evidence; MGraph moral graph
(GAN (ME) )m ; target blanket decomposition < EvdList, TBList >, EvdList =
{Ei }, TBList = {TB (Ei )}; N maximum number enclosed-targets target
blanket.
Output: BeliefRatioTable belief ratio tables target blanket.
1: function C OMPILE B ELIEF R ATIOTABLE(M, E, e, MGraph, EvdList, TBList, N )
2:
BeliefRatioTable ;
3:
TBi TBList
4:
Hi ;
5:
X TBi
. generate enclosed-target set Hi
6:
change X auxiliary node;
7:
NewTBList CompileMinimalTargetBlankets(M, E, MGraph);
8:
NewTBi .push(X);
9:
NewTBi == TBi NewE == Ei
10:
Hi .push(X);
11:
end
12:
reset X target node;
13:
end
14:
Hi Hi .top(N );
. select top N nodes according maxhij r(hij ; ei )
15:
SearchQueue ;
16:
BeliefRatioTablei ;
17:
Hi = TB \Hi ;
18:
instantiation hi Hi
19:
BeliefRatioTablei [Hash()].push(GBF (hi ; e));
20:
end
21:
SearchQueue.push();
. contain Hij
22:
SearchQueue empty
. compile belief ratio table TB
23:
Hij SearchQueue.pop();
24:
X > Hij , X Hi
. order defined Hi
25:
instantiation x (X Hij Hi )
. calculate incrementally
26:
BeliefRatioTablei [Hash(X Hij )].push(GBF (x; e));
27:
end
28:
SearchQueue.push(Hij X);
29:
end
30:
end
31:
BeliefRatioTable.push(BeliefRatioTablei );
32:
end
33: end function

configurations TB (Ei )\Hij . algorithm calculating belief ratio tables SPBnd
summarized Algorithm 4, includes algorithm used MPBnd special case.
state S, calculate hash key SK based pruned-targets use SK &HK index
belief ratio table target blanket. Then, search configuration selected
671

fiZ HU & UAN

Algorithm 5 BFBnB Algorithm Based Target Blanket Upper Bounds
Input: target set; E evidence set; e given evidence; MGraph moral graph
(GAN (ME) )m ; K maximum number targets target blanket; N maximum
number enclosed-targets target blanket.
Output: BestExplanation arg maxx,XM GBF (x; e).
1: function BFB N BS EARCH(M, E, e, MGraph, K, N )
2:
< EvdList, TBList > CompileMinimalTargetBlankets(M, E, MGraph);
3:
MergeTargetBlankets(EvdList, TBList, K);
4:
SplitTargetBlankets(EvdList, TBList, K, MGraph);
5:
TBi .size() < K, TBi TBList
6:
return None;
. stop algorithm size TBi larger K
7:
end
8:
BeliefRatioTable CompileBeliefRatioTable(M,E,e,MGraph,EvdList,TBList,N );
9:
openList state x X M;
. initialize openList;
10:
maxGBF openList.top();
11:
openList empty
12:
x openList.pop();
13:
state U
. U unexpanded set current state x
14:
sucState {y} x;
15:
UpperBound CalcUpperBound (sucState, BeliefRatioTable);
16:
UpperBound maxGBF
17:
continue;
. use UpperBound prune expanded state sucState
18:
end
19:
maxGBF < GBF (sucState; e)
20:
maxGBF GBF (sucState; e);
21:
BestExplanation {y} x;
22:
end
23:
openList.push(sucState);
24:
end
25:
end
26: end function
27: function C ALC U PPER B OUND(sucState, BeliefRatioTable)
28:
BeliefRatioTable BeliefRatioTable
29:
BeliefRatioTable Hij BeliefRatioTable [Hash(P)&Hash(Hi )];
30:
MaxBeliefRatio maximum belief ratio consistent v sucState;
31:
end
Q
MaxBeliefRatio C1
32:
UpperBound 1 +
;
1p(v|e)
33: end function

belief ratio table consistent expanded targets ti highest belief update
ratio. Finally, calculate upper bound using Theorem 8.
BFBnB algorithm summarized Algorithm 5. main difference MPBnd
SPBnd methods used generate belief ratio table, i.e., Line 8 Algorithm 5.
speed search process, sort belief ratio table target blanket descending order,
672

fiE XACT LGORITHMS MRE NFERENCE

Networks
Alarm
Carpo
Hepar
Insurance
Emdec6h
CPCS179

Nodes
37
61
70
27
168
179

Leaves
11
43
41
6
117
151

States
2.84
2.23
2.31
3.30
2.00
2.29

Arcs
46
74
123
52
261
239

Table 1: Benchmark diagnostic Bayesian networks used evaluate proposed algorithms.
i.e., higher belief update ratios closer front table. Furthermore, search tree
MRE, expanded targets state guaranteed included descedant states. Thus
proposed method, record indices best belief update ratios, one belief
ratio table, expanded state. calculate upper bound current state, need
search best belief update ratio recorded indices parent.

6. Experiments
proposed algorithms evaluated six benchmark diagnostic Bayesian networks listed Table 1, i.e., Alarm (Ala), Carpo (Car), Hepar (Hep), Insurance (Ins), Emdec6h (Emd), CPCS179
(Cpc) (Beinlich, Suermondt, Chavez, & Cooper, 1989; Binder, Koller, Russell, & Kanazawa, 1997;
Onisko, 2003; Pradhan, Provan, Middleton, & Henrion, 1994). Among them, Alarm, Carpo, Hepar,
Insurance networks fewer 100 nodes. Emdec6h CPCS179 larger networks 100 nodes. listed number nodes (Nodes), number leaf nodes
(Leaves), average number node states (States), number arcs (Arcs) Bayesian
networks Table 1. experiments performed 2.67GHz Intel Xeon CPU E7 512G
RAM running 3.7.10 Linux kernel.
6.1 Experimental Design
Since proposed algorithms, MPBnd SPBnd, first nontrivial exact MRE algorithms,
use naive Breadth-First Brute-Force search algorithm (BFBF) baseline; basically
BFBF BFBnB bound set infinity. also included results tabu search
indicate difficulty MRE problems. MPBnd SPBnd, set maximum number
targets target blanket K 18. SPBnd, set maximum number enclosed-targets
target blanket N 7. BFBF search solve test cases fewer 15 targets.
compare performance among BFBF, MPBnd, SPBnd, perform experiments
two test settings, one exactly 12 targets (12-target setting) around 20 targets
(difficult-target setting). 12-target setting, randomly generated five test settings
network, setting consisting leaf nodes evidence, 12 remaining nodes targets,
others auxiliary nodes. setting, randomly generated 20 configurations
evidence (test cases) sampling prior distributions networks.
difficult-target setting, randomly generated five test settings network,
setting consisting leaf nodes evidence, around 20 remaining nodes targets,
others auxiliary nodes. number targets selected test cases challenging
673

fiZ HU & UAN

#Cases/Time
BFBF
MPBnd
SPBnd
T6400
T3200
T1600
T800
T400

Ala
1.7e3
17.0
2.6
92
17.3
85
9.1
79
4.7
76
2.4
72
1.2

Car
66.1
3.5
1.5
100
25.4
100
13.2
100
6.9
98
3.6
98
1.8

Hep
270.0
14.6
9.0
93
44.8
91
22.6
86
11.5
81
5.9
81
3.0

Ins
1.6e5
1.6e3
72.7
81
26.3
77
13.7
74
7.0
73
3.6
73
1.8

Emd
212.0
15.1
13.1
95
89.3
95
45.0
95
23.2
95
11.8
95
6.1

Cpc
1.3e4
815.0
377.0
90
108.0
90
55.1
90
28.0
90
14.1
90
7.0

Table 2: Comparison SPBnd, MPBnd, BFBF, tabu running time seconds (sec) well
accuracy tabu searches Bayesian networks 12-target setting.

BFBF still solvable MPBnd SPBnd. setting, randomly generated
20 configurations evidence (test cases) sampling prior distributions networks.
tabu search, set number search steps since last improvement L maximum
number search steps according different network settings. 12-target setting, set L
20 {400, 800, 1600, 3200, 6400}. difficult-target setting, set L 80
{12800, 25600, 51200}. evaluate search performance, compared solutions
tabu search SPBnd, counted number test cases tabu search achieved
optimal solutions.
6.2 Evaluation MPBnd SPBnd 12-Target Setting
Table 2, compared proposed MPBnd SPBnd algorithms BFBF tabu search
test cases 12-target setting. tabu search, listed number test cases solved
optimally (top) running time seconds (bottom) using different limits number
search steps. running time MPBnd SPBnd includes preprocessing time search
time. MPBnd, SPBnd BFBF able solve test cases exactly. MPBnd shown
significantly faster BFBF pruning upper bound. running time
SPBnd reduced using tightened upper bound. T400 fastest algorithm
worst accuracy. increase , running time tabu search increased significantly.
However, networks, tabu search could solve test cases optimally, even
using running time MPBnd SPBnd.
compare MPBnd, SPBnd BFBF detail, computed average running time
individual test settings MPBnd, SPBnd BFBF. illustrate logarithmic running
time pairs, BFBF vs MPBnd MPBnd vs SPBnd, points Figures 5 6 respectively.
also draw contour lines mark difference logarithmic running time
Figures 5 6. example, contour line marked -3 Figure 5 contains points
674

fiE XACT LGORITHMS MRE NFERENCE

7
Ala
Car
Hep
Ins
Emd
Cpc

MPBnd (log10ms)

6

0

-1
5
-2

4
-3

3

5

6

7

8

9

BFBF (log10ms)

Figure 5: Distributions logarithmic running time pairs milliseconds) MPBnd BFBF
Bayesian networks 12-target setting.
6
Ala
Car
Hep
Ins
Emd
Cpc

SPBnd (log10ms)

5

0

-1
4

-2
3
-3

2

3

4

5

6

7

MPBnd (log10ms)

Figure 6: Distributions logarithmic running time pairs milliseconds SPBnd MPBnd
Bayesian networks 12-target setting.

MPBnd 1000 times faster BFBF. results show although average running
time may change significantly, ratios running times BFBF MPBnd,
MPBnd SPBnd relatively stable. MPBnd roughly 10 100 times faster BFBF.
SPBnd roughly 3 4 times faster MPBnd.
6.3 Evaluation MPBnd SPBnd Difficult-Target Setting
Table 3, compared proposed SPBnd algorithm MPBnd tabu search test
cases difficult-target setting. list number targets network first row
675

fiZ HU & UAN

#Cases/Time
MPBnd
SPBnd
T51200
T25600
T12800

Ala
20
173
63
60
4.06
60
2.40
60
1.42

Car
15
0.98
0.13
100
5.55
100
2.89
100
1.50

Hep
22
478
106
72
17.12
65
9.38
63
5.15

Ins
17
43.91
58.72
76
0.96
76
0.76
76
0.59

Emd
20
225
96
86
34.14
86
17.48
86
8.93

Cpc
20
1,569.2
419
82
6.11
82
4.2
82
2.7

Table 3: Comparison SPBnd, MPBnd, tabu running time minutes (min) well
accuracy tabu searches Bayesian networks difficult-target setting.

table. tabu search, list number test cases solved optimally (top) running
time minutes (bottom) network. Increasing 12800 51200 helpful
preventing tabu search getting stuck local optima. Moreover, performance tabu
search varies greatly different test networks. SPBnd shown significantly faster
MPBnd networks due tightened upper bound. SPBnd, need compile
series belief ratio tables target blanket, may consume significant amount
running time. example, although running time 419 minutes CPCS179, search took
182.4 minutes. Also results, see running time MPBnd SPBnd
depends number targets, also tightness upper bound Bayesian
network structures, control number pruned states size belief table
target blanket TB (Ei ) respectively.
compare SPBnd MPBnd detail, computed average running time individual test settings them, illustrated running time pair logarithm point
Figure 7. also drew contour lines mark difference SPBnd MPBnd.
results showed data points network form cluster. SPBnd roughly 3 4
times faster MPBnd.
results Figures 6 7 showed test cases running times MPBnd
SPBnd close. two possible reasons behind observation. First,
network structures, enclosed-target sets may exist individual target blankets. Thus
cases SPBnd degenerate MPBnd. Second, running time test case consists
two parts, compiling belief ratio tables performing search. test cases, compiling
belief ratio tables may take significant amount time SPBnd. therefore make tradeoff
compiling time search time adjusting maximum number enclosed-targets N
target blanket.
6.4 Scalability SPBnd
also evaluated MPBnd SPBnd test cases increasing number targets 17
25 increment 2 three Bayesian networks, Hepar, Emdec6h, CPCS179.
target number i, randomly generated four test settings 5 test cases setting.
676

fiE XACT LGORITHMS MRE NFERENCE

8
Ala
Car
Hep
Ins
Emd
Cpc

SPBnd (log10ms)

7

6

0
-0.5
-1

5

-1.5

4

3

5

6

7

8

MPBnd (log10ms)

Figure 7: Distributions logarithmic running time (ms) pairs SPBnd MPBnd test cases
difficult-target setting.

Time
MPBnd

SPBnd

Hep
Emd
Cpc
Hep
Emd
Cpc

17
64.7
63.6
483.7
24.0
36.3
130.7

19
101.0
182.4
15.0
100.8
94.1

Targets
21
23
411.3
643.4
66.9 243.1
444.2
499.6
-

25
711.9
-

Table 4: Comparison SPBnd MPBnd running time (min) test cases increasing
number targets. dash indicates time (800m).

Bayesian networks, set leaf nodes evidence, remaining nodes targets, others
auxiliary nodes. set time limit running 800 minutes. results reported
Table 4. results show pruning upper bound slowed exponential growth
running time significantly. SPBnd handle complex MRE problems
reach MPBnd. results, also see running time SPBnd affected
tightness upper bound structures Bayesian networks, longer heavily depends
number targets.
6.5 Importance Splitting Large Target Blankets
show importance splitting large target blankets, calculated percentage test cases
need splitting operation various target settings three Bayesian networks, Hepar, Emdec6h,
CPCS179, increasing number targets. experiment, performed preprocessing step, handle 50 targets. target blankets test cases
677

fiZ HU & UAN

Emd
Cpc
Hep

70

Ratio (%)

60
50
40
30
20
10
0
15

20

25

30

35

40

45

50

Number tagets

Figure 8: Percentage cases using splitting operation different target settings.
Figure 8 split smaller ones size less K=18 using proposed splitting
algorithm. results Figure 8 show ratio test cases needs splitting increases,
decreases increasing numbers targets. reason initially increase
number targets leads large target blankets. number targets keeps increasing,
densely distributed targets tend introduce d-separation Bayesian network result
smaller target blankets. Figure 8, peaks curves located 22 approximately.
networks large number non-leaf nodes, e.g., Emdec6h, ratio higher 0.5
significant number target settings. means without splitting large target blankets
MRE problems cannot solved networks target settings.
6.6 Effect Summing Enclosed-Targets
section, take Hepar, Emdec6h, CPCS179 examples illustrate summing
enclosed-targets tightens target blanket upper bound. calculated log difference
upper bound UpBnd maximum GBF MaxGBF subtree rooted search state
test cases 12-target setting. Normalized histograms differences using
using tightening plotted Figure 9. x-axis shows log10 (UpBnd MaxGBF ),
y-axis shows normalized percentages. Thus smaller log10 (UpBnd MaxGBF ) indicates
tighter upper bound. clear graph summing enclosed-targets makes
bound much tighter.
6.7 Convergence Upper Bound
gain better perspective tightened upper bound Theorem 8 improves time,
calculated maximum upper bound MaxBound layer search tree
current maximum GBF CurrMax . also recorded running time finished search
layer. Figure 10 shows convergence curves MaxBound (dotted line) CurrMax (circled
line) search time one typical test case three networks, i.e., Hepar,
Emdec6h, CPCS179, 17-target setting Table 4. Figure 10(A) shows upper
bound dropped sharply beginning search decreased gradually search.
678

fiE XACT LGORITHMS MRE NFERENCE

0.25

0.35

0.16
Tightening
Tightening

Probability

Tightening
Tightening

Tightening
Tightening
0.12

0.25
0.15

0.08
0.15
0.04

0.05

0.05
6

2

2

5

6

5

15

25

0

15

Log10(UpBnd-MaxGBF)

Log10(UpBnd-MaxGBF)
(A)

5

5

15

25

Log10(UpBnd-MaxGBF)

(B)

(C)

Figure 9: Tightness upper bounds. evaluate effect summing enclosed-targets using
normalized histogram different Bayesian networks, i.e., Hepar (A), Emdec6h (B),
CPCS179 (C).

12

30

7

Bound
CurrMax

Bound
CurrMax

Bound
CurrMax

6

Log10GBF

10
20

5

8
4
10
6

3
1

2

3

4

5

Running time (log10ms)
(A)

1

2

3

4

Running time (log10ms)
(B)

5

1

2

3

4

5

Running time (log10ms)
(C)

Figure 10: Convergence upper bound current maximum GBF different Bayesian networks, i.e., Hepar (A), Emdec6h (B), CPCS179 (C).

Figure 10(B) shows upper bound dropped sharply end search became tight
quickly. Figure 10(C) shows upper bound dropped sharply beginning
end search. results demonstrate different behaviors bound tightening
search.

7. Discussions Conclusions
main contributions paper two BFBnB algorithms, i.e., MPBnd SPBnd, solving MRE exactly using upper bounds based target blanket decomposition evidence variables.
first non-trivial exact algorithms solving MRE Bayesian networks. key
idea proposed method decompose conditional joint probability p(E|M) set
marginal probabilities p(Ei |TB (Ei )). marginal probability related subnetwork characterized target blanket subset evidence variables. upper bound GBF derived
679

fiZ HU & UAN

maximizing belief update ratio fixed target set TB (Ei ) subnetwork separately.
upper bound tightened merging target blankets sharing set targets.
bound improved based two ideas. First, proposed split large
target blankets converting auxiliary nodes pseudo-targets, scales MRE inference
larger Bayesian networks targets. Second, tightened upper bound GBF
identifying summing enclosed-targets target blanket TB (Ei ). new upper
bound reduces search space dramatically. experimental results show SPBnd significantly faster MPBnd BFBF algorithms. proposed SPBnd MPBnd solve
MRE inference exactly Bayesian networks could solved previously.
proposed upper bounds calculated efficiently two reasons. First, target
blanket TB (Ei ) usually much smaller whole target set M. Second, original MRE
problem, need search subsets target set find best solution. However,
calculate upper bound, need search fixed target set TB (Ei ) subnetwork.
proposed MRE upper bound consists four sources relaxations, i.e., (1) relaxation
p(x, v|e) p(v|e) Equation 14, (2) bounding descendant states, (3) inconsistent
values sharing nodes different target blankets, (4) maximization prunedtargets individual search states. optimal upper bound achieved state equal
maximum GBF subtree rooted state. size expanded target set increase,
p(x, v|e) p(v|e) become much smaller 1, relaxation (1) becomes
tight. Thus, achieve optimal upper bound need minimize impact (3)
(4). work, minimized effect (3) merging target blankets sharing set
targets, minimized effect (4) identifying summing enclosed-target sets
target blanket.
Different brute-force algorithm, search time MPBnd SPBnd longer
mainly dependent size search space (i.e., number targets number states
target), also tightness upper bound structure Bayesian networks.
Bayesian networks large number targets, upper bound efficiently generated
long number targets target blanket small. SPBnd, tightness
upper bound depends number enclosed-targets quality enclosed-target Hij
measured belief update ratio maxhij r(hij ; ei ). enclosed-targets, proposed
method SPBnd degenerates MPBnd.
work, splitting algorithm designed split target blankets, whose sizes larger
K, based separation property undirected graphs converting auxiliary nodes
pseudo-targets. evidence node connected directly many targets, may decompose
using methods node splitting (Choi et al., 2007). also one future research
directions.

Acknowledgements
work supported NSF grants IIS-0953723, IIS-1219114, PSC-CUNY enhancement award. Part research previously presented AAAI-15 (Zhu & Yuan, 2015).
680

fiE XACT LGORITHMS MRE NFERENCE

References
Ay, N., & Polani, D. (2008). Information flows causal networks. Advances Complex Systems
(ACS), 11(01), 1741.
Beinlich, I., Suermondt, G., Chavez, R., & Cooper, G. (1989). alarm monitoring system:
case study two probabilistic inference techniques belief networks. Proceedings
2nd European Conference AI Medicine, pp. 247256.
Binder, J., Koller, D., Russell, S., & Kanazawa, K. (1997). Adaptive probabilistic networks
hidden variables. Machine Learning, 29, 213244.
Chajewska, U., & Halpern, J. Y. (1997). Defining explanation probabilistic systems. Proceedings Thirteenth Annual Conference Uncertainty Artificial Intelligence (UAI97),
pp. 6271, San Francisco, CA. Morgan Kaufmann Publishers.
Choi, A., Chavira, M., & Darwiche, A. (2007). Node splitting: scheme generating upper
bounds Bayesian networks. Proceedings 23rd Annual Conference Uncertainty
Artificial Intelligence (UAI-07), pp. 5766.
Darwiche, A. (2009). Modeling Reasoning Bayesian Networks. Cambridge University
Press.
de Campos, L. M., Gamez, J. A., & Moral, S. (2001). Simplifying explanations Bayesian belief
networks. International Journal Uncertainty, Fuzziness Knowledge-Based Systems, 9(4),
461489.
Dechter, R., & Rish, I. (2003). Mini-buckets: general scheme bounded inference. J. ACM,
50(2), 107153.
Feige, U., & Krauthgamer, R. (2002). polylogarithmic approximation minimum bisection.
SIAM J. Comput., 31(4), 10901118.
Fitelson, B. (2007). Likelihoodism, bayesianism, relational confirmation. Synthese, 156, 473
489.
Flores, J., Gamez, J. A., & Moral, S. (2005). Abductive inference Bayesian networks: finding
partition explanation space. Eighth European Conference Symbolic Quantitative Approaches Reasoning Uncertainty, ECSQARU05, pp. 6375. Springer Verlag.
Gardenfors, P. (1988). Knowledge Flux: Modeling Dynamics Epistemic States. MIT Press.
Glover, F. (1990). Tabu search: tutorial. Interfaces, 20, 7494.
Good, I. J. (1985). Weight evidence: brief survey. Bayesian statistics, 2, 249270.
Green, P. (1995). Reversible jump Markov chain Monte Carlo computation Bayesian model
determination. Biometrica, 82, 711732.
Heckerman, D., Breese, J., & Rommelse, K. (1995). Decision-theoretic troubleshooting. Communications ACM, 38, 4957.
Henrion, M., & Druzdzel, M. J. (1991). Qualitative propagation scenario-based schemes
explaining probabilistic reasoning. Bonissone, P., Henrion, M., Kanal, L., & Lemmer, J.
(Eds.), Uncertainty Artificial Intelligence 6, pp. 1732. Elsevier Science Publishing Company, Inc., New York, N. Y.
681

fiZ HU & UAN

Huang, J., Chavira, M., & Darwiche, A. (2006). Solving map exactly searching compiled
arithmetic circuits. Proceedings 21st National Conference Artificial intelligence,
Vol. 2, pp. 11431148. AAAI Press.
Jeffreys, H. (1961). Theory Probability. Oxford University Press.
Jensen, F. V., & Liang, J. (1994). drHugin: system value information Bayesian networks. Proceedings 1994 Conference Information Processing Management
Uncertainty Knowledge-Based Systems, pp. 178183.
Kalagnanam, J., & Henrion, M. (1988). comparison decision analysis expert rules
sequential diagnosis. Proceedings 4th Annual Conference Uncertainty Artificial
Intelligence (UAI-88), pp. 253270, New York, NY. Elsevier Science.
Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983). Optimization simulated annealing. Science,
pp. 671680.
Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models - Principles Techniques.
MIT Press.
Kwisthout, J. (2013). Inforbable Explanations: Finding explanations Bayesian networks
probable informative. van der Gaag, L. (Ed.), Symbolic Quantitative
Approaches Reasoning Uncertainty, Vol. 7958 Lecture Notes Computer Science,
pp. 328339. Springer Berlin Heidelberg.
Lacave, C., & Diez, F. (2002). review explanation methods Bayesian networks.
Knowledge Engineering Review, 17, 107127.
Lauritzen, S. L., Dawid, A. P., Larsen, B. N., & Leimer., H.-G. (1990). Independence properties
directed markov fields. Networks, 20(5), 491505.
Leake, D. B. (1995). Abduction, experience, goals: model everyday abductive explanation.
Journal Experimental Theoretical Artificial Intelligence, 7, 407428.
Marinescu, R., Dechter, R., & Ihler, A. (2014). AND/OR search marginal MAP. Proceedings
30th Annual Conference Uncertainty Artificial Intelligence (UAI-14), pp. 563
572.
Marinescu, R., & Dechter, R. (2009). AND/OR branch-and-bound search combinatorial optimization graphical models. Artif. Intell., 173(16-17), 14571491.
Onisko, A. (2003). Probabilistic Causal Models Medicine: Application Diagnosis Liver
Disorders. Ph.D. thesis, Institute Biocybernetics Biomedical Engineering, Polish
Academy Science.
Pacer, M., Lombrozo, T., Griffiths, T., Williams, J., & Chen, X. (2013). Evaluating computational
models explanation using human judgments. Proceedings 29th Annual Conference
Uncertainty Artificial Intelligence (UAI-13), pp. 498507.
Park, J. D., & Darwiche, A. (2003). Solving map exactly using systematic search. Proceedings
19th Annual Conference Uncertainty Artificial Intelligence (UAI-03), pp. 459468.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference.
Morgan Kaufmann.
682

fiE XACT LGORITHMS MRE NFERENCE

Pradhan, M., Provan, G., Middleton, B., & Henrion, M. (1994). Knowledge engineering large
belief networks. Proceedings Tenth Annual Conference Uncertainty Artificial
Intelligence, UAI-94, p. 484490. Morgan Kaufmann Publishers, Inc.
Shimony, S. (1993). role relevance explanation I: Irrelevance statistical independence.
International Journal Approximate Reasoning, 8(4), 281324.
van der Gaag, L., & Wessels, M. (1993). Selective evidence gathering diagnostic belief networks.
AISB Quarterly, pp. 2334.
van der Gaag, L., & Wessels, M. (1995). Efficient multiple-disorder diagnosis strategic focusing,
pp. 187204. UCL Press, London.
Yuan, C., & Hansen, E. A. (2009). Efficient computation jointree bounds systematic MAP
search. Proceedings 21st International Joint Conference Artificial Intelligence
(IJCAI-09), pp. 19821989.
Yuan, C., Lim, H., & Littman, M. L. (2011a). relevant explanation: computational complexity
approximation methods. Ann. Math. Artif. Intell., 61, 159183.
Yuan, C., Lim, H., & Lu, T.-C. (2011b). relevant explanation Bayesian networks. J. Artif.
Intell. Res., 42, 309352.
Yuan, C., Liu, X., Lu, T.-C., & Lim, H. (2009). relevant explanation: Properties, algorithms,
evaluations. Proceedings 25th Annual Conference Uncertainty Artificial Intelligence (UAI-09), pp. 631638.
Yuan, C., & Lu, T.-C. (2007). Finding explanations Bayesian networks. Proceedings
18th International Workshop Principles Diagnosis (DX-07), pp. 414419.
Zhu, X., & Yuan, C. (2015). exact algorithm solving relevant explanation Bayesian
networks. Proceedings 29th National Conference Artificial intelligence (AAAI15), pp. 36493655.

683

fiJournal Artificial Intelligence Research 55 (2016) 799-833

Submitted 08/15; published 03/16

Exact Algorithm Based MaxSAT Reasoning Maximum
Weight Clique Problem
Zhiwen Fang

ZHIWENF @ GMAIL . COM

State Key Lab. Software Development Environment
Beihang University, Beijing, 100083, P.R. China

Chu-Min Li

CHU - MIN . LI @ U - PICARDIE . FR

MIS, Universite de Picardie Jules Verne
Amiens 80039, France

Ke Xu

KEXU @ NLSDE . BUAA . EDU . CN

State Key Lab. Software Development Environment
Beihang University, Beijing, 100083, P.R. China

Abstract
Recently, MaxSAT reasoning shown effective computing tight upper bound
Maximum Clique (MC) (unweighted) graph. paper, apply MaxSAT reasoning
compute tight upper bound Maximum Weight Clique (MWC) wighted graph. first
study three usual encodings MWC weighted partial MaxSAT dealing hard clauses,
must satisfied solutions, soft clauses, weighted falsified.
drawbacks encodings motivate us propose encoding MWC special
weighted partial MaxSAT formalism, called LW (Literal-Weighted) encoding dedicated
upper bounding MWC, soft clauses literals soft clauses weighted.
optimal solution LW MaxSAT instance gives upper bound MWC, instead
optimal solution MWC. introduce two notions called Top-k literal failed clause
Top-k empty clause extend classical MaxSAT reasoning techniques, well two sound
transformation rules transform LW MaxSAT instance. Successive transformations LW
MaxSAT instance driven MaxSAT reasoning give tight upper bound encoded MWC.
approach implemented branch-and-bound algorithm called MWCLQ. Experimental
evaluations broadly used DIMACS benchmark, BHOSLIB benchmark, random graphs
benchmark winner determination problem show approach allows MWCLQ
reduce search space significantly solve MWC instances effectively. Consequently,
MWCLQ outperforms state-of-the-art exact algorithms vast majority instances. Moreover,
surprisingly effective solving hard dense instances.

1. Introduction
Consider undirected graph G = (V ,E), V set n vertices {v1 , v2 , ..., vn } E
set edges. density G computed 2m/(n(n 1)). clique G subset C V
every pair vertices adjacent. contrary, independent set G subset
V every pair vertices disconnected. vertex cover G subset V
every edge G least one endpoint S. maximum clique (MC) problem asks find
clique largest cardinality. MC problem prominent combinatorial optimization
problem tightly related two well-known graph problems, namely maximum
c
2016
AI Access Foundation. rights reserved.

fiFANG , L , & XU

independent set (MIS) problem minimum vertex cover (MVC) problem. Concretely,
maximum clique C G maximum independent set complement graph G G, V \C
minimum vertex cover G. Therefore, algorithms three problems directly
applied solve others practice. addition, exact MC solvers take advantage
following relation size maximum clique number independent sets. G
partitioned k independent sets, G cannot contain clique larger k,
independent set contribute one vertex clique.
MC problem NP-hard decision problem NP-complete (Karp, 1972), appears many applications social network analysis (e.g., Zhang, Nie, Jiang, Chen, & Liu,
2014b; Kibanov, Atzmueller, Scholz, & Stumme, 2014). fixed-parameter intractable (Downey
& Fellows, 1995). Moreover, proved approximating MC within |V |1 given > 0
NP-hard (Zuckerman, 2006). best polynomial-time approximation algorithm achieves
approximation ratio O(n(log log n)2 /(log n)3 ) (Feige, 2004). theoretical
practical importance MC problem, huge amount effort devoted solve
designing two types algorithms (also called solvers). One type heuristic algorithms mainly
including stochastic local search (e.g., Pullan & Hoos, 2006; Cai, Su, & Sattar, 2011; Cai, Su,
Luo, & Sattar, 2013; Fang, Chu, Qiao, Feng, & Xu, 2014a). Another exact algorithms including
branch-and-bound (BnB) search (e.g., Ostergard, 2002; Regin, 2003; Tomita & Seki, 2003; Konc &
Janezic, 2007; Li & Quan, 2010b; Tomita & Kameda, 2007; Li, Fang, & Xu, 2013). Heuristic algorithms able solve large-scale instances cannot guarantee optimality solutions.
Exact algorithms guarantee optimality solutions, worst-case time complexity
exponential unless P = N P .
tight upper bound size maximum clique graph essential BnB algorithm
solve MC problem efficiently. However, challenging obtain upper bound
reasonable time. state-of-the-art BnB algorithms apply approximation coloring independent set partition algorithms compute upper bound MC. instance, Fahle (2002) uses
constructive heuristic DSATUR color vertices one one according degrees. Konc
Janezic (2007), Tomita Kameda (2007), Li Quan (2010b) apply greedy strategy proposed Tomita Seki (2003) partition graph independent sets, use number
independent sets partition upper bound MC. MaxCLQ (Li & Quan, 2010b, 2010a)
encodes MC instance partial MaxSAT instance improves upper bound based
independent set partition making use MaxSAT reasoning. excellent performance MaxCLQ shows MaxSAT reasoning technologies allows compute tight upper bound MC
within reasonable time. IncMaxCLQ (Li et al., 2013) combines incremental upper bound
MaxSAT reasoning compute tight upper bound efficiently. addition independent
set partition MaxSAT reasoning, approaches, graph matching (Regin, 2003),
also used upper bounding MC.
One generalization MC problem associate vertex positive weight.
weight clique defined total weight vertices it. maximum weight clique
(MWC) problem consists finding clique largest weight. computationally equivalent
problems like weighted set packing problem. MWC problem appears variety realworld applications, protein structure predictions (Mascia, Cilia, Brunato, & Passerini, 2010),
coding theory (Zhian, Sabaei, Javan, & Tavallaie, 2013), combinatorial auctions (Wu & Hao, 2015),
computer vision (Ma & Latecki, 2012; Zhang, Javed, & Shah, 2014a), etc. example,
video object segmentation problem, challenging task select region high objectness
800

fiA N E XACT LGORITHM



AXIMUM W EIGHT C LIQUE

score sharing similar appearance, solved MWC algorithm (Ma & Latecki,
2012).
Compared MC problem, less work done solve MWC problem. Cliquer (Ostergard, 2002, 2001) one state-of-the-art exact solvers MC MWC,
deals MC MWC using similar methods. preprocessing, Cliquer partitions
graph independent sets determining one independent set time. long
vertices added current independent set, one largest degree added.
purpose preprocessing define vertex ordering v1 <v2 <. . .<vn , vi (1in)
vertex inserted independent set time i. Cliquer searches MC MWC
subgraph induced {vi , vi+1 , . . ., vn } successively i=n, n 1, . . ., 1 (in ordering).
MC MWC subgraph induced {vi , vi+1 , . . ., vn } associated vi
used prune subtrees subsequent search. Kumlander (2004, 2008b) inherits search strategy
Cliquer. addition MWC associated vi , Kumlander also partitions current
subgraph independent sets uses sum maximum weight independent set
upper bound. VCTable (Shimizu, Yamaguchi, Saitoh, & Masuda, 2012) improves Kumlanders algorithm new initial vertex order better implementation using bitwise operations.
Yamaguchi Masuda (2008) propose new upper bound based longest path directed
acyclic graph constructed original graph, improves bound based independent set partition. OTClique (Shimizu, Yamaguchi, Saitoh, & Masuda, 2013), also based
Cliquer, uses dynamic programming strategy calculate upper bounds small subproblems preprocessing, stores results table. stored upper bounds used
search. MinSatz (Li, Zhu, Manya, & Simon, 2012) exact solver MinSAT problem. important application MinSatz solve combinatorial optimization problems
MC MWC. MinSatz constructs weighted graph MinSAT instance uses clique
partition combined MaxSAT reasoning compute tight bound MinSAT instance
solve. addition exact algorithms, heuristic algorithms also proposed solve
MWC problem (Pullan, 2008; Wu, Hao, & Glover, 2012; Benlic & Hao, 2013).
paper, apply MaxSAT reasoning solve MWC. first study three usual encodings MWC MaxSAT. encodings intrinsic difficulties dealing vertex
weights. motivates us propose dedicated encoding MWC MaxSAT called LW
(Literal-Weighted) encoding, soft clauses literals soft clauses weighted.
optimal solution MaxSAT instance usual encodings MWC MaxSAT gives
MWC, optimal solution LW MaxSAT instance LW encoding gives upper
bound MWC. So, makes little sense run MaxSAT solver find optimal solution
LW MaxSAT instance. interest LW encoding transform LW MaxSAT
instance reduce optimal solution, tighter upper bound encoded MWC
obtained.
order transform LW MaxSAT instance, introduce two notions called Top-k literal
failed clause Top-k empty clause, two sound transformation rules. Then, implement
BnB algorithm MWC called MWCLQ. every search tree node, MWCLQ first encodes
current subgraph LW MaxSAT instance. Then, driven MaxSAT reasoning, MWCLQ
repeatedly transforms LW MaxSAT instance obtain tighter upper bound encoded
MWC. best knowledge, first time MaxSAT reasoning techniques
used specifically compute tight upper bound BnB algorithm MWC. Experimental results widely used DIMACS benchmark, BHOSLIB benchmark, random graphs realistic
801

fiFANG , L , & XU

benchmark winner determination problem show MWCLQ reduces search space
significantly outperforms state-of-the-art exact algorithms vast majority instances
benchmarks.
paper extended work Fang, Li, Qiao, Feng, Xu (2014b) by:
clearly motivating LW encoding illustrating weakness three classical encodings
MWC MaxSAT dealing vertex weights graph;
introducing notion Top-k empty clause exploiting MWCLQ;
formally proving transformation rules;
adding experimental results show effectiveness approach. algorithm
MWCLQ compared integer programming solver CPLEX, MinSAT
solver MinSatz, especially realistic instances winner determination problem.
State-of-the-art MaxSAT solvers using different encodings MWC MaxSAT also
compared MWCLQ. order evaluate LW encoding, state-ofthe-art MaxSAT solvers cannot use, compare different versions MWCLQ,
difference encoding MWC MaxSAT used compute upper bound.
paper organized follows. next section, introduce necessary notations
background knowledge. Section 3, present MWCLQ different encodings MWC
MaxSAT, extending MaxSAT reasoning weighted literals introducing notions
Top-k literal failed clause Top-k empty clause, two transformation rules. Experimental
results shown Section 4. Section 5 concludes paper.

2. Preliminaries
subgraph G induced subset V V G = (V , E ), E = {{vi ,vj } | vi ,vj V
{vi , vj } E}. vertex v, (v) = {u| {u,v}E} set neighbors v cardinality
|(v)| called degree v. use Gv denote subgraph induced (v) {v} G\v
denote subgraph induced V \{v}. maximal clique clique cannot extended
more. maximum clique maximal clique largest possible size. cardinality
maximum clique G usually denoted (G) called clique number G.
vertex v G, maximum clique G either Gv G\v. chromatic number
G, denoted (G), minimum number colors needed color vertices G
two adjacent vertices share color. vertices sharing color constitute
independent set. Therefore, graph coloring problem equivalent partitioning V
minimum number independent sets. Note (G) greater equal (G).
graph edge-weighted vertex-weighted. focus vertex-weighted graph
paper. Formally, vertex-weighted undirected graph G = (V, E, w) undirected graph G =
(V, E) combined weighting function w: V R+ every vertex v associated
positive weight w(v). sequel, use term weighted graph instead
Pof vertex-weighted
graph simplicity. weight clique C G defined w(C) = vC w(v). Given
weighted graph G, maximum weight clique problem asks find clique largest weight
G (Ostergard, 2001; Pullan, 2008; Wu et al., 2012) largest weight often denoted
802

fiA N E XACT LGORITHM



AXIMUM W EIGHT C LIQUE

v (G) literature. Note maximum weight clique necessarily clique containing
maximum number vertices, must maximal clique.
MC problem encoded partial MaxSAT problem. MaxSAT, variable x may
take value 0 (f alse) 1 (true). literal variable x negation x. clause c = 1 2
... l disjunction literals, also expressed set {1 , 2 , . . . , l }. clause
satisfied clause least one literal assigned true. length clause
c number literals contains, denoted length(c). unit clause clause containing
one literal. empty clause, denoted , contains literals cannot satisfied.
conjunctive normal form (CNF) formula = c1 c2 ... cm conjunction clauses. Given
CNF formula set variables {x1 , x2 , ..., xn }, satisfiability (SAT) problem test
assignment satisfying clauses , maximum satisfiability (MaxSAT)
problem find assignment satisfying maximum number clauses (Li & Manya, 2009).
minimum satisfiability (MinSAT) problem, contrary, find assignment minimizing
number satisfied clauses (Li et al., 2012). cases, clauses declared
hard must satisfied solutions, clauses soft falsified. partial
MaxSAT problem asks find assignment maximize number satisfied soft clauses
satisfying hard clauses. Note MaxSAT problem particular partial MaxSAT problem
without hard clauses. weighted clause pair (c, w), c soft clause w, positive
number, weight. weighted partial MaxSAT problem find assignment maximizing
total weight satisfied soft clauses satisfying hard clauses. weighted MaxSAT problem
weighted partial MaxSAT problem without hard clauses. optimal solution (weighted)
(partial) MaxSAT instance denoted opt() paper.
Two main types exact algorithms developed MaxSAT: SAT-based MaxSAT solvers (e.g.,
Morgado, Heras, Liffiton, Planes, & Marques-Silva, 2013; Ansotegui, Bonet, & Levy, 2013; Davies
& Bacchus, 2013a; Ansotegui & Gabas, 2013; Morgado, Dodaro, & Marques-Silva, 2014; Martins,
Joshi, Manquinho, & Lynce, 2014) solve MaxSAT instance repeatedly calling CDCL
(Conflict-Driven Clause Learning) based SAT solver solve sequence SAT problems,
BnB MaxSAT solvers (e.g., Li, Manya, & Planes, 2007; Kugel, 2010). SAT-based MaxSAT
solvers particularly efficient solve industrial MaxSAT problems, BnB MaxSAT
solvers particular efficient solve random MaxSAT problems. SAT-based solvers
MaxHS (Davies & Bacchus, 2013b) also exploit MIP (Mixed Integer Programming)
solving MaxSAT.
maximize number satisfied clauses equals minimize number falsified clauses.
Many algorithms based BnB scheme MaxSAT compute lower bound number
falsified clauses (Li, Manya, & Planes, 2006; Li et al., 2007; Larrosa, Heras, & de Givry, 2008;
Kugel, 2010) prune search space solving MaxSAT instance . Detecting disjoint inconsistent subsets soft clauses proved powerful computing bound,
subset soft clauses said inconsistent subset together hard clauses unsatisfiable.
Unit propagation effective technique widely used SAT MaxSAT solvers (Li & Anbulagan, 1997; Li, Manya, & Planes, 2005). pseudo-code allowing find inconsistent subset
soft clauses based unit propagation given Algorithm 1 (Li et al., 2005). algorithm
works follows. uses stack store unit clauses performs unit propagation
empty clause produced empty. empty clause produced, set clauses
used deriving empty clause, excluding hard clauses, returned. algorithm called
iteratively find many disjoint inconsistent subsets soft clauses possible. Note soft
803

fiFANG , L , & XU

clauses involved inconsistent subset removed detecting inconsistent subsets
ensure subsets disjoint. Also note solution empty set
returned, empty set means set hard clauses unsatisfiable.
Algorithm 1: ConflictDetectionByUP(, S), detect inconsistent subset soft clauses.
Input: MaxSAT instance stack storing unit clauses .
Output: Return inconsistent subset soft clauses unit propagation results empty
clause, otherwise return false.
1 begin
2
empty
3
pop unit clause u S;
4
literal u, record u reason ;
5
foreach clause c contains
6
satisfy c;
7
8
9
10
11
12
13
14
15
16
17
18
19

foreach clause c contains
remove c;
c unit clause
push c S;
c empty
push c empty queue Q, = {c};
Q empty
pop clause c Q
foreach removed literal c
reason r literal
push r Q, insert r I;
return set soft clauses I;
return false;

Failed literal detection (Freeman, 1995) used enhance unit propagation MaxSAT solving (Li et al., 2006). literal CNF formula called failed literal , unit propagation
{} results empty clause. Let IS() set soft clauses used unit propagation
derive empty clause assigning true . literals soft clause c = {1 , 2 , . . ., l }
failed, {c} IS(1 ) IS(2 ) IS(l ) inconsistent subset soft clauses (Li &
Quan, 2010b).

3. MWCLQ: Exact Algorithm MWC
section, propose exact algorithm based BnB scheme, namely MWCLQ,
MWC. Subsection 3.1, describe basic BnB algorithm. Subsection 3.2, introduce
novel encoding called LW (Literal-Weighted) encoding MWC MaxSAT discussing
three usual encodings. Given weighted graph G, LW MaxSAT encoding gives LW MaxSAT
instance lw represents upper bound v (G). Subsection 3.3, propose two transfor804

fiA N E XACT LGORITHM



AXIMUM W EIGHT C LIQUE

mation rules lw . Successive transformations lw driven MaxSAT reasoning give tight
upper bound v (G), presented Subsection 3.4.
3.1 Branch-and-Bound Search MWC
Algorithm 2 depicts pseudocode MWCLQ.
Algorithm 2: MWCLQ(G, C, LB), branch-and-bound algorithm MWC.
Input: weighted graph G=(V , E, w), clique C construction, lower bound
LB.
Output: clique weight greater LB, clique found.
1 begin
2
|V | = 0
3
return C;
4
5
6
7
8
9
10
11
12
13
14

U B overestimate(G)+w(C);
U B LB
return ;
v select(V );
C1 MWCLQ(Gv , C{v}, LB);
LB max(LB, w(C1 ));
C2 MWCLQ(G\v, C, LB);
w(C1 ) w(C2 )
return C2 ;
else
return C1 ;

MWCLQ searches clique, extended C, weight larger LB G = (V , E,
w). first computes upper bound UB calling overestimate(G) compares UB LB
test whether search necessary G. possible find better solution, MWCLQ
selects vertex calling select(V ). vertex v G, maximum weight clique G
either clique Gv containing v clique G\v containing v. Thus, MWCLQ searches
maximum weight clique Gv G\v successively.
MWCLQ form similar MaxCLQ, BnB algorithm MC (Li & Quan, 2010b),
overestimate function computing upper bound, select function choosing branching vertex, significantly different. two functions essential MWCLQ
MaxCLQ. high-quality upper bound allows solvers prune useless search, good vertex
ordering promises efficient search process. MaxCLQ computes base upper bound partitioning G independent sets, improves base upper bound MaxSAT reasoning.
Meanwhile, MaxCLQ orders vertices selecting first vertex minimum degree.
paper, use simple vertex ordering select v largest weight, breaking ties favor
vertex higher degree, focus efficiently compute tight upper bound using
MaxSAT reasoning.
805

fiFANG , L , & XU

3.2 Encoding MWC MaxSAT
MC MWC instance encoded MaxSAT follows.
Boolean variables: boolean variable xi added vertex vi , assigned
value true vi maximum clique construction;
Hard clauses: set hard clauses added require pair unconnected vertices
belong clique. Concretely, hard clause xi xj added pair
unconnected vertices vi vj ;
Soft clauses: exist different ways define set soft clauses. Given graph,
encodings MC MWC MaxSAT presented paper use set hard
clauses. differ soft clauses used, analyzed discussed
subsection.
MC instance also encoded MinSAT instance without hard clauses adopting
approach proposed Ignatiev, Morgado, Marques-Silva (2014). obtained MinSAT
instance turn encoded MaxSAT instance without hard clauses using approaches
work Kugel (2012), Zhu, Li, Manya and, Argelich (2012). encoding without hard
clauses provides new angle view MC MWC solving, awaits future research.
section, focus encodings MWC MaxSAT hard clauses different soft clauses used. Subsection 3.2.1 defines direct encoding MWC MaxSAT.
Subsection 3.2.2 defines split encoding iterative split encoding MWC MaxSAT. Subsection 3.2.3 proposes novel literal-weighted encoding MWC MaxSAT illustrates
advantages compared direct encoding, split encoding iterative split encoding
computing upper bound MWC.
3.2.1 IRECT E NCODING



MWC



AX SAT

1 v1

v2 2

4 v3

v4 5

Figure 1: weighted graph 4 vertices 1 edge. Numbers indicate vertex weights.
straightforward way define soft clauses associate unit soft clause xi vertex
vi , giving direct encoding MC MWC MaxSAT. example, MC instance
graph Fig. 1 (without considering vertex weights) encoded following partial MaxSAT
instance: (1) set variables {x1 , x2 , x3 , x4 }; (2) set hard clauses {x1 x3 , x1 x4 ,
x2 x3 , x2 x4 , x3 x4 }; (3) set soft clauses {x1 , x2 , x3 , x4 }.
assignment satisfying hard clauses gives rise clique, since variables assigned
value true correspond pairwise connected vertices. non-weighted case, assignment satisfying hard clauses maximizing number satisfied soft clauses gives rise
maximum clique.
806

fiA N E XACT LGORITHM



AXIMUM W EIGHT C LIQUE

weighted case, unit soft clause associated weight corresponding
vertex. example, MWC instance weighted graph Fig. 1 encoded
weighted partial MaxSAT instance boolean variables hard clauses
MC instance. set weighted soft clauses is: {(x1 ,1), (x2 ,2), (x3 ,4), (x4 ,5)}. assignment
satisfying hard clauses maximizing total weight satisfied soft clauses gives rise
maximum weight clique.
encoded MaxSAT, MaxSAT reasoning applied solve MC MWC
problem. Many MaxSAT algorithms obtain upper bound satisfied soft clauses computing
lower bound number falsified soft clauses. Inconsistent subsets soft clauses often
used computing lower bound. Recall subset soft clauses said inconsistent
subset together hard clauses satisfiable. non-weighted MaxSAT instance
soft clauses, r disjoint inconsistent subsets soft clauses detected, mr upper
bound opt() (Li et al., 2005, 2006).
weighted case, define weight subset soft clauses minimum
weight clauses S, namely,
w(S) = min w(c).
cS

Similar non-weighted case, following proposition.
Proposition 1. (Li et al., 2007) Given weighted partial
Pif disjoint inconsisPMaxSAT instance ,
tent subsets S1 , S1 , ..., Ss detected, opt() soft clause c w(c) 1is w(Si ).
Observe unit soft clauses direct encoding capture connection
vertices, set soft clauses used every graph n vertices, matter
vertices connected other. MC instance, number soft clauses
used base upper bound, MaxSAT reasoning applied improve base upper
bound detecting inconsistent soft clause subsets. Unfortunately, direct encoding
give trivial base upper bound, number vertices graph. Moreover, since
soft clauses direct encoding unit, inconsistent subset contains exactly two soft clauses,
limiting improvement upper bound half number vertices graph.
weighted case similar. base upper bound total weight vertices. use
following example illustrate direct encoding cannot compute tight upper bound even
simple MWC instance.
Example 1. total weight soft clauses direct encoding graph Fig. 1 12.
Using unit propagation, {(x3 , 4), (x4 , 5)} found inconsistent subset hard
clause x3 x4 , upper bound improved 4. left soft clauses are: (x1 , 1),
(x2 , 2), (x4 , 1), unit propagation detects {(x1 , 1), (x4 , 1)} also inconsistent,
improve upper bound 1. Finally, upper bound improved 6, larger
optimal solution 5.
drawbacks, direct encoding perform well, shown experimental results presented Section 4.3. One might want add at-most-one constraint
independent set remedy drawbacks direct encoding. is, independent set
= {v1 , v2 , . . . , vl }, add at-most-one constraint x1 +x2 +. . .+xl 1. However, at-most-one
constraint add anything new, subset hard clauses {xi xj | 1i<jl}
encoding already enforces at-most-one constraint (Chen, 2010). addition, encoding
807

fiFANG , L , & XU

at-most-one constraint using hard binary clauses efficient enough independent set
extremely large, usually case solving hard MC MWC instances. Therefore,
at-most-one constraint presumably useless encoding MC MWC MaxSAT.
3.2.2 PLIT E NCODING

TERATIVE

PLIT E NCODING



MWC



AX SAT

Another encoding introduced MC MaxCLQ (Li & Quan, 2010b) defining set soft
clauses based independent set partition G. Concretely, MaxCLQ first partitions G set
independent sets, creates soft clause independent set, disjunction
variables corresponding vertices independent set. independent set based encoding
shown substantially efficient direct encoding solve MC.
natural way extend independent set based MaxSAT encoding MWC split vertex
weights. independent set = {v1 , v2 , . . . , vl }, w(v1 ) w(v2 ) . . . w(vl ),
split vertex weights minimum weight w(vl ), thus obtain weighted soft clause
(x1 x2 . . . xl , w(vl )) l unit soft clauses: (x1 , w(v1 ) w(vl )), (x2 , w(v2 ) w(vl )), . . .,
(xl , w(vl ) w(vl )), l largest integer w(vl ) > w(vl ) I. encoding
called split encoding. total weight soft clauses split encoding less
direct encoding, giving better base upper bound, may still large.
Example 2. possible independent set partition graph Fig. 1 {v4 , v3 , v1 } {v2 }.
soft clauses split encoding based partition (x4 x3 x1 , 1), (x4 , 4), (x3 , 3)
(x2 , 2). total weight soft clauses 10, better direct encoding.
Using unit propagation, {(x4 , 4), (x3 , 3)} found inconsistent subset hard
clause x3 x4 , allows us improve upper bound 3. way, {(x4 , 1),
(x2 , 2)} found inconsistent, improves upper bound 1. Finally, remaining
soft clauses (x4 x3 x1 , 1) (x2 , 1) consistent. Thus, upper bound computed using
split encoding still 6.
split encoding improved so-called iterative split encoding, used
MinSatz (Li et al., 2012). idea iterative split encoding split vertex weights repeatedly
vertices independent set weight. Concretely, independent
set = {v1 , v2 , . . . , vl } w(v1 ) w(v2 ) . . . w(vl ), add soft clause (x1 x2
. . . xl , w(xl )), repeatedly find largest l w(xl ) > w(xl ) add soft clause
(x1 x2 . . . xl , w(xl ) w(xl )), l 1.
Example 3. possible independent set partition graph Fig. 1 {v4 , v3 , v1 }, {v2 }.
soft clauses iterative split encoding based partition (x4 x3 x1 , 1), (x4 x3 , 3),
(x4 , 1) (x2 , 2). total weight soft clauses 7. Starting unit propagation setting x4 = 1,
find {(x4 , 1), (x2 , 2)} inconsistent subset, improve upper bound
1 get new soft clause (x2 , 1). Then, find {(x4 x3 , 3), (x2 , 1)} inconsistent,
upper bound improved 5, tightest upper bound.
iterative split encoding gives non-trivial base upper bound, better
obtained direct encoding split encoding. key point iterative split encoding
vertex weights split advance, generating numerous soft clauses, variable
may appear several soft clauses clauses may differ one variable (e.g. clauses
(x4 x3 x1 , 1) (x4 x3 , 3) Example 3). Many splittings may useful upper
808

fiA N E XACT LGORITHM



AXIMUM W EIGHT C LIQUE

bound computation, MaxSAT reasoning may complicated instance
numerous soft clauses.
3.2.3 L ITERAL -W EIGHTED E NCODING



MWC



AX SAT

Recall MaxSAT encoding MC MaxCLQ guarantees variable appears
soft clauses number soft clauses equals number independent sets. MaxSAT
reasoning instance much simpler. order extend advantage MWC
split vertex weights advance, introduce Literal-Weighted encoding (LW encoding) MWC LW MaxSAT, literals soft clause also weighted. Concretely, weighted literal pair(, w), literal w weight. literal-weighted
soft clause (LW soft clause) disjunction weighted literals. Formally, given MWC instance
G = (V, E, w), encode G LW MaxSAT instance lw follows:
Associate vertex vi weighted literal (xi , w(xi )), w(xi ) = w(vi );
Add hard clause xi xj pair unconnected vertices vi vj ;
independent set = {v1 , v2 , ..., vl }, add LW soft clause c = (x1 , w(v1 )) (x2 , w(v2 ))
. . . (xl , w(vl )).
LW soft clause c also presented set {(x1 , w(x1 )), (x2 , w(x2 )),. . ., (xl , w(xl ))}.
weight c defined
w(c) = max (w(xj )).
1jl

LW clause c ordered w(x1 ) w(x2 ) ... w(xl ). sequel, LW soft clauses
always ordered. Therefore, weight LW soft clause always w(x1 ), representing cost
none vertices corresponding independent set included clique construction.
show optimal solution LW MaxSAT instance lw , denoted opt(lw ),
maximizes total weight satisfied soft clauses satisfies hard clauses, gives
upper bound v (G), differently usual encodings optimal solution
MaxSAT instance gives v (G).
MWC given assignment lw satisfies hard clauses maximizes
total weight satisfied literals. Example 4 suggests opt(lw ) v (G) different
MWC instance G.
1
v1
7
v2

4
v5

2 v3

6
v6

v4 3

Figure 2: weighted graph 6 vertices 6 edges. Numbers indicate vertex weights.

809

fiFANG , L , & XU

Example 4. possible independent set partition graph Fig. 2 {{v1 , v3 , v6 }, {v2 , v4 }
{v5 }}. set soft clauses LW MaxSAT instance lw based partition {{(x6 ,
6), (x3 , 2), (x1 , 1)}, {(x2 , 7), (x4 , 3)}, {(x5 , 4)}}. MWC graph {v5 , v6 }
v (G)=10. hand, {x1 =1, x2 =1} optimal solution lw opt(lw )=13.
Moreover, optimal solution lw correspond maximum weight clique
graph.
following proposition states relationship optimal solution LW MaxSAT
instance MWC encoded graph.
Proposition 2. Consider weighted graph G = (V, E, w), let lw LW instance based
independent set partition G, v (G) opt(lw ).
Proof. Suppose {vi1 , vi2 , . . . , vip } maximum weight clique G. Let cij LW clause
containing xij .
v (G) =

p
X
j=1

w(xij )

p
X

w(cij ) opt(lw ).

j=1

Since opt(lw ) upper bound v (G), makes little sense apply MaxSAT solver
find opt(lw ). However, transform lw , optimal solution new instance
tighter upper bound v (G), upper bound new optimal solution also
tightened. Example 5 illustrates this.
Example 5. possible independent set partition graph Fig. 1 {v4 , v3 , v1 } {v2 }.
LW soft clauses {(x4 , 5), (x3 , 4), (x1 , 1)} {(x2 , 2)}. total weight soft clauses 7,
giving base upper bound iterative split encoding. Unit propagation setting x2 = 1
makes x4 x3 false hard clauses. find split clause {(x4 , 5),
(x3 , 4), (x1 , 1)} {(x4 , 2), (x3 , 2)} {(x4 , 3), (x3 , 2), (x1 , 1)}, keep base upper
bound 7, splitting allows derive inconsistent subset soft clauses {{(x4 , 2),
(x3 , 2)}, {(x2 , 2)}} improve base upper bound 5, tightest possible upper bound.
Note unit propagation need derive empty clause improve upper bound
example. Furthermore, order obtain tightest upper bound, one unit propagation suffices, two inconsistent subsets need derived iterative split encoding Example 3.
key point Example 5 original LW MaxSAT instance transformed new
one, upper bound optimal solution improved 5 using MaxSAT reasoning.
tightest upper bound new LW MaxSAT instance also tightest upper bound
v (G).
next subsection, define two sound transformation rules transform LW
MaxSAT instance, allowing derive tight upper bound MWC general case. effective, application two rules driven MaxSAT reasoning,
presented Subsection 3.4.
810

fiA N E XACT LGORITHM



AXIMUM W EIGHT C LIQUE

3.3 Transformation Rules Literal-Weighted MaxSAT
propose two transformation rules split LW soft clause LW MaxSAT instance encoding
MWC instance G. soundness rules based Proposition 3, ensuring
splitting, optimal solution new LW MaxSAT instance tighter upper bound v (G).
Proposition 3. Let lw LW MaxSAT instance encoding MWC instance G.
LW soft clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . ., (xl , w(xl ))} lw 0<w(x1 ), split c
c = {(x1 , ), (x2 , min(w(x2 ), )), . . ., (xl , min(w(xl ), ))} c = {(x1 , w(x1 )), (x2 ,
max(w(x2 ) , 0)), . . ., (xl , max(w(xl ), 0))}, literals weight 0 removed,
get lw = (lw \{c}) {c ,c }, v (G) opt(lw ) opt(lw ).
Proof. First all, note c c ordered c. prove v (G) opt(lw ), first
show min(w(xj ), ) + max(w(xj ), 0) = w(xj ) j 1<jl. fact,
1. w(xj ) , min(w(xj ), ) + max(w(xj ), 0) = + w(xj ) = w(xj );
2. w(xj ) < , min(w(xj ), ) + max(w(xj ), 0) = w(xj ) + 0 = w(xj ).
words, total weight literal changed splitting c c c . Let C =
{vi1 , vi2 , . . . , vip } maximum weight clique G. {xi1 = 1, xi2 = 1, . . ., xip = 1}
variables 0, assignment lw satisfying hard clauses. Let S(xij ) = {c | xij
c} set soft clauses containing xij lw 1jp. clauses S(xij ) satisfied
xij . time,
X

w(c) =

cS(xij )

X

max w(x) w(xij ).

cS(xij )

xc

Hence, have,
opt(lw )

p
X
X
j=1 cS(xij )

w(c)

p
X
j=1

w(xij ) =

p
X

w(vij ) = v (G).

j=1

prove opt(lw ) opt(lw ), let assignment w(lw , A) (w(lw , A)) total
weight satisfied soft clauses lw (lw ).
1. c satisfied lw A, c c also satisfied lw A;
2. c satisfied A, c also satisfied since c literals c, c may
satisfied satisfied literal c may weight 0 c may thus removed
c .
note w(c) = w(c )+w(c ), w(lw , A) w(lw , A) assignment A,
implying opt(lw ) opt(lw ).
Given weighted graph G = (V, E, w), let lw LW MaxSAT instance based
independent set partition G, propose following two rules transform lw .
811

fiFANG , L , & XU

1. -Rule Given LW soft clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . ., (xl , w(xl ))} lw
weight 0<w(x1 ), split c c = {(x1 , ), (x2 , min(w(x2 ), )), . . ., (xl ,
min (w(xl ), ))} c = {(x1 , w(x1 )), (x2 , max(w(x2 ), 0)), . . ., (xl ,max(w(xl )
,0))}, i.e., lw = (lw \ {c}) {c , c }.
2. (k, )-Rule Given LW clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . . , (xl , w(xl ))} lw ,
integer 1k<l weight 0<w(x1 )w(xk+1 ), split c c = {(x1 , ), (x2 ,
max(w(x2 )+w(x1 ), 0)), . . ., (xk , max(w(xk )+w(x1 ), 0))}, c = {(x1 , w(x1 )),
(x2 , min(w(x2 ), w(x1 ))), . . ., (xk , min(w(xk ), w(x1 ))), (xk+1 , w(xk+1 )), . . ., (xl ,
w(xl ))}, i.e., lw = (lw \ {c}) {c , c }.
purpose -Rule (k, )-Rule split c clause c weight
clause c weight w(x1 ) without changing total weight literal. constraint
w(x1 )w(xk+1 ) ensure c remains ordered (i.e., w(x1 )min(w(x2 ), w(x1 )). . .
w(xk+1 )). . .w(xl )). use example illustrate (k, )-Rule. Let c LW soft
clause c = {(x1 , 5), (x2 , 3), (x3 , 2)}, (1) k=1 =2, c = {(x1 , 2)} c = {(x1 , 3), (x2 ,
3), (x3 , 2)}; (2) k=2 =3, c = {(x1 , 3), (x2 , 1)} c = {(x1 , 2), (x2 , 2), (x2 , 2)};
(3) k=2 =2, c = {(x1 , 2), (x2 , 0)} = {(x1 , 2)} c = {(x1 , 3), (x2 , 3), (x3 , 2)}.
soundness -Rule (k, )-Rule easily proved using Proposition 3.
two rules applied many possible ways generate many possible clauses. example,
special case, =w(xl ) -Rule, c = {(x1 , )), (x2 , )), . . . , (xl , ))} least
xl removed c since weight 0 c . words, transform lw
classical weighted MaxSAT instance repeatedly applying -Rule =w(xl ) clause
c containing literals different weights. way, obtain classical weighted partial
MaxSAT instance , literals soft clause weight whose optimal
solution gives maximum weight clique. Moreover, let MaxSAT instance obtained
applications -Rule, opt(lw ) opt(1 ) opt(2 ) . . . opt() = v (G).
fact, iterative split encoding G.
Observe application -Rule (k, )-Rule gives MaxSAT instance whose
optimal solution gives tighter upper bound v (G). Since unrestricted applications two
rules may effective, restrict applications cases inconsistent subset
soft clauses weight derived. Concretely, (k, )-Rule always applied clause
c k weighted literals failed falsified, allowing obtain c
inconsistent subset soft clauses derived, -Rule always applied clause
c inconsistent subset soft clauses. application allows improve upper bound
v (G) positive weight . Note rules strictly greater 0,
= 0, rules generate new empty clauses weight 0, cannot improve upper
bound. call extended MaxSAT reasoning application -Rule (k, )-Rule driven
MaxSAT reasoning LW MaxSAT instance. Details approach given afterwards.
3.4 Upper Bound Based MaxSAT Reasoning
section, show transformation LW MaxSAT instance driven MaxSAT reasoning. Subsection 3.4.1, apply -Rule transform inconsistent subset soft clauses.
Subsection 3.4.2 Subsection 3.4.3, introduce two notions, namely Top-k failed literal clause Top-k empty clause, deduce inconsistent subset soft clauses using
812

fiA N E XACT LGORITHM



AXIMUM W EIGHT C LIQUE

(k, )-Rule, transformed applying -Rule. Subsection 3.4.4, present
overestimating algorithm computes tight upper bound MWC successively transforming
LW MaxSAT instance.
3.4.1 RANSFORMING

NCONSISTENT

UBSET



OFT C LAUSES

First all, detect inconsistent subsets using unit propagation presented Algorithm 1.
Example 6. possible independent set partition graph Fig. 2 {{v1 , v4 , v6 }, {v2 ,
v5 }, {v3 }}, LW soft clauses based partition c1 = {(x6 , 6), (x4 , 3), (x1 , 1)}, c2 =
{(x2 , 7), (x5 , 4)} c3 = {(x3 , 2)}. set x3 =1 satisfy unit clause c3 , x3
removed hard clauses x1 x3 , x3 x5 , x3 x6 . Unit clauses x1 , x5 x6 imply
x1 =0, x5 =0 x6 =0, respectively. So, c1 c2 become unit clauses. Accordingly,
set x4 =1 x2 =1 satisfy them, falsifying hard clause x2 x4 .
Consequently, inconsistent soft clause subset = {c1 , c2 , c3 } detected weight
subset 2. Proposition 1 allows decrease upper bound 2, thus improved upper
bound 13. Since soft clauses involved inconsistent subset, cannot improve
upper bound more. However, split soft clauses using -Rule based
following proposition.
Proposition 4. Let = {c1 , c2 , . . ., ci } inconsistent subset LW soft clauses, -Rule
applied split every cj cj cj =w(S), = {c1 , c2 , ..., ci } still
inconsistent soft clause subset weight .
Proof. clearly inconsistent, clauses literals clauses
S. addition, every clause weight . Hence, inconsistent subset soft clauses
weight .
purpose splitting obtain = {c1 , c2 , . . ., ci } improve upper
bound.
Example 7. presented Example 6, {c1 , c2 , c3 } inconsistent subset weight 2. Applying -Rule =2, c1 = {(x6 , 2), (x4 , 2), (x1 , 1)}, c1 = {(x6 , 4), (x4 , 1)}, c2 = {(x2 , 2),
(x5 , 2)}, c2 = {(x2 , 5), (x5 , 2)}, c3 = {(x3 , 2)} c3 = {(x3 , 0)} (c3 removed).
easy see {c1 , c2 , c3 } inconsistent subset soft clauses weight 2,
remaining clauses {{(x6 , 4), (x4 , 1)}, {(x2 , 5), (x5 , 2)}} used improve
upper bound. However, unit propagation cannot used improve upper bound
unit clause exists. Moreover, failed literal detection work either every
soft clause contains least one literal failed. propose apply (k, )-Rule split
Top-k literal failed clause defined afterwards improve upper bound.
3.4.2 OP - K FAILED L ITERAL ETECTION
Definition 1. LW soft clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . ., (xl , w(xl ))} Top-k literal
failed x1 , x2 , . . ., xk failed literals, 1k<l.
define Top-k weight LW soft clause c wk (c) = w(x1 )w(xk+1 ), 1
k < length(c).
813

fiFANG , L , & XU

Proposition 5. Consider LW soft clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . ., (xl , w(xl ))}. c
Top-k literal failed k<l, IS(xj ) (1jk) set soft clauses making xj failed, split
c c c using (k, )-Rule = min(wk (c), w(IS(x1 )), w(IS(x2 )), ..., w(IS(xk ))),
{c } IS(x1 ) IS(x2 ) . . . IS(xk ) inconsistent subset soft clauses weight .
Proof. set clearly inconsistent, satisfaction literal c results empty
clause set. minimum weight clause set . inconsistent subset
soft clauses weight .
soon determine c Top-k literal failed, apply (k, )-Rule split c use
-Rule split clauses IS(x1 ) IS(x2 ) . . . IS(xk ). inconsistent subset weight
obtained way. Observe k=l, {c} IS(x1 ) IS(x2 ) . . . IS(xl ) classical
inconsistent subset soft clauses weight min(w(c), w(IS(x1 )), w(IS(x2 )), ..., w(IS(xl )))
clause split using -Rule.
Example 8. Consider soft clauses produced Example 7, c1 = {(x6 , 4), (x4 , 1)} c2 =
{(x2 , 5), (x5 , 2)}. test x2 c2 setting x2 =1. satisfy hard clauses x2 x6 x2 x4 ,
need set x6 =0 x4 =0. Then, c1 = {(x6 , 4), (x4 , 1)} becomes falsified, making x2 failed.
However, x5 failed. Therefore c2 Top-k literal failed k=1. Applying (k, )-Rule
k=1 =3 c2 , c2 = {(x2 , 3)} c2 = {(x2 , 2), (x5 , 2)}. Applying -Rule
=3 c1 , get c1 = {(x6 , 3), (x4 , 1)} c1 = {(x6 , 1)}. Consequently, get inconsistent
subset {c1 , c2 } weight 3.
result, detection Top-k literal failed clause allows improve upper bound
3, giving tightest upper bound 10.
3.4.3 OP - K E MPTY C LAUSE ETECTION
noteworthy literal declared failed, unit propagation literal falsifies literals clause. Sometimes, propagation literal makes weighted
literals soft clause c falsified, literals c. case, cannot declared
failed, cannot improve upper bound using approaches presented above. However,
split c using (k, )-Rule obtain falsified clause, declared failed.
1
v1
7
v2

4
v5

2 v3

6
v6

v4 3

Figure 3: weighted graph 6 vertices 9 edges. Numbers indicate vertex weight
Example 9. Consider weighted graph G Fig. 3, possible independent set partition G
{{v6 , v3 }, {v2 ,v5 }, {v4 , v1 }} LW soft clauses c1 ={(x6 , 6), (x3 , 2)}, c2 ={(x2 , 7),
814

fiA N E XACT LGORITHM



AXIMUM W EIGHT C LIQUE

(x5 , 4)} c3 ={(x4 , 3), (x1 , 1)}. total weight soft clauses LW MaxSAT instance
based partition 16. one hand, literals largest weight soft
clause failed. set x6 =1, need set x2 =0 satisfy hard clause x2 x6 .
literal x6 failed, unit propagation falsify clause. However,
weighted literal x2 c2 falsified. split c2 c2 = {(x2 , 3)} c2 = {(x2 , 4), (x5 , 4)}.
c2 falsified propagating x6 . So, x6 failed literal splitting c2 , c1 Top-k
(k=1) literal failed clause split c1 = {(x6 , 3)} c1 = {(x6 , 3), (x3 , 2)} using
(k, )-Rule. thus obtain inconsistent subset {c1 , c2 } weight =3. Consequently,
upper bound improved 3.
Example 9 suggests us define Top-k empty clause notion.
Definition 2. LW soft clause c = {(x1 , w(x1 )), (x2 , w(x2 )), . . ., (xl , w(xl ))} Top-k empty,
1k<l, literal assigned true, unit propagation falsifies
literals x1 , x2 , . . ., xk c.
straightforward show following proposition.
Proposition 6. literal declared failed splitting Top-k empty clause using
(k, )-Rule.
Proof. Splitting Top-k empty clause gives clause c = x1 x2 . . . xk ,
literal satisfaction makes c empty via unit propagation.
3.4.4 OVERESTIMATING LGORITHM



MWCLQ

Algorithm 3 formally describes successively transform LW MaxSAT instance applying
-Rule and/or (k, )-Rule obtain disjoint inconsistent subsets soft clauses every search
tree node MWCLQ. upper bound computed Algorithm 3 way tight,
shown experimental results.
Given weighted graph G, Algorithm 3 first encodes MWC instance LW MaxSAT
instance based independent set partition G. partitioning procedure works follows.
Vertices sorted decreasing order weights (ties broken favor vertices
higher degree) successively inserted independent set. Suppose current
independent sets I1 , I2 , . . ., Ii (in order, 0 beginning partitioning process).
current first vertex v inserted first Ij v unconnected vertices
already Ij . Ij exist, new independent set Ii+1 opened v inserted
Ii+1 . independent set encoded LW soft clause. total weights soft clauses
initial upper bound MWC G improved detecting disjoint inconsistent
subsets soft clauses using extended MaxSAT reasoning.
detection inconsistent subset soft clauses performed detecting failed literals
shortest available soft clause c. literal failed either unit propagation falsifies
literals another clause weighted literals another soft clause. case
literals c failed unit propagation falsifies literals another clause, usual
inconsistent subset soft clauses obtained. case k weighted literals c
falsified and/or unit propagation literal c falsifies weighted literals another
soft clause t, (k, )-Rule applied split c and/or obtain inconsistent subset
soft clauses. reason Top-k empty clauses Top-k literal failed clauses
815

fiFANG , L , & XU

Algorithm 3: overestimate(G), computing upper bound MWC MaxSAT reasoning

1
2
3
4
5
6
7

Input: weighted graph G=(V , E, w).
Output: upper bound maximum weight clique G.
begin
partition G independent sets I1 , I2 ,...,Ii ;
encode P
G LW MaxSAT instance lw mark soft clauses available;
/* base upper bound */
UB clw w(c);
lw contains available soft clause
c shortest available soft clause lw ;
mark c unavailable;
/* Let set soft clauses involved inconsistent
subset Stopk set top-k literal failed top-k
empty clauses */

8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34

, Stopk , k 0;
k < length(c)
k+1 failed empty clause
IS(k+1 );
else k+1 failed Top-kt empty clause wkt (t) > 0
(IS(k+1 ) \ {t});
Stopk Stopk {t};
else break;
k k + 1;
k > 0
k = length(c)
{c};

/* literals c failed */

else wk (c) > 0
Stopk Stopk {c};

/* c top-k literal failed */

else continue;
min(w(S), mintStopk (wkt (t)));
UB UB;
/* improve upper bound > 0 */

, Stopk
;
foreach clause cl
apply -Rule cl;
= {cl };
foreach Top-k literal failed clause Top-k empty clause Stopk
apply (, kt )-Rule t;


Stopk
= Stopk
{t };
;
lw (lw \ (S Stopk )) Stopk
mark clauses lw available;

return UB;

/* update soft clauses */

/* improved upper bound MaxSAT reasoning */

816

fiA N E XACT LGORITHM



AXIMUM W EIGHT C LIQUE

collected Stopk . Meanwhile, soft clauses involved inconsistent subset collected
S. case, computed = min(w(S), mintStopk (wkt (t))) (Line 23), w(S)
minimum clause weight S. Note Stopk empty usual inconsistent subset.
-Rule (k, )-Rule applied split clauses subset upper bound improved
weight subset.
Observe Algorithm 3 called every search tree node scratch graph
different search tree node different. Also observe detected inconsistent subsets soft
clauses disjoint, soft clause one subset used detect subset. fact,
inconsistent subset soft clauses removed Line 32 detecting another inconsistent
set. initial upper bound improved total weight inconsistent subsets.
approach might improved using MaxSAT resolution defined work Bonet, Levy
and, Manya (2006) Larrosa, Heras, Givry (2008), adapting approach Abrame
Habet (2014) transforms inconsistent subset clauses weighted empty clause
set new clauses used detection inconsistent subsets soft clauses.
Nevertheless, application MaxSAT resolution carefully driven approach
competitive (Abrame & Habet, 2015), MaxSAT resolution produces many intermediate
clauses.
note clauses produced using -Rule (k, )-Rule cannot obtained simply
applying MaxSAT resolution, MaxSAT resolution transforms MaxSAT instance
equivalent one, -Rule (k, )-Rule may change optimal solution LW
MaxSAT instance Proposition 3.

4. Empirical Evaluation
section, empirically evaluate MWCLQ extended MaxSAT reasoning using standard benchmarks, namely DIMACS benchmark, BHOSLIB benchmark, random graphs,
benchmark winner determination problem (WDP). conducted four experiments
study. first experiment evaluate performance MWCLQ comparing
state-of-the-art exact algorithms. second experiment compare different encodings
MWC MaxSAT. third experiment investigate impact splitting soft clauses
extended MaxSAT reasoning. forth experiment, applied MWC algorithms solve
WDP instances compare performances.
first introduce benchmarks used experiments describe experimental environment. present discuss experimental results detail.
4.1 Benchmarks Experimental Environment
Three types benchmarks used first three experiments.
1. DIMACS DIMACS benchmark taken Second DIMACS Implementation
Challenge, used widely benchmarking purposes literature algorithms MC, MWC, MVC, MIS on. 80 DIMACS instances generated
real-world applications coding theory, fault diagnosis, Kellers conjecture
Steiner Triple Problem, well random graphs generated different properties,
DSJC, brock p hat families. size instances ranges less 50
817

fiFANG , L , & XU

vertices 1,000 edges 4,000 vertices 5,000,000 edges. downloaded
instances website (ThanhVu & Thang, 2014).
2. BHOSLIB BHOSLIB (Xu, 2004)(Benchmarks Hidden Optimum Solutions Graph
Problems) instances based CSP model named RB (Xu & Li, 2000; Xu, Boussemart,
Hemery, & Lecoutre, 2007). Phase transitions exist model RB transition points
located exactly. BHOSLIB instances generated phase transition region
model RB appear extremely hard solve various algorithms, even
graph size small (Liu, Lin, Wang, Su, & Xu, 2011; Xu & Li, 2006). firstly used
SAT competition 2004, widely used evaluate algorithms MC,
MVC MIS.
3. Random random graph n vertices density p generated randomly selecting
edge probability p complete graph n vertices. experiments, n
ranges 150 700 p 0.5 0.95. Random graphs allow show asymptotic
behavior algorithm.
convert non-weighted graph weighted graph associating weight w(vi ) = mod
200+1 vertex vi . method initially proposed Pullan (2008) used
standard converting approach generate weighted graphs non-weighted instances (Wu
et al., 2012; Benlic & Hao, 2013).
solvers used experiments implemented C/C++. compile using
gcc/g++ 4.7.2 option -O3. experiments running machine Intel(R) Xeon(R)
CPU E5-2680 @ 2.70GHz, 8 cores 16G RAM Debian GNU/Linux 7.4. cut-off time
solver solve instance one hour (3600 seconds).
4.2 Comparison MWCLQ Algorithms
compare MWCLQ two state-of-the-art exact solvers specific MWC, MinSAT solver
CPLEX.
1. Cliquer state-of-the-art solver MC problem MWC problem. best
knowledge, almost recent exact algorithms MWC (e.g., Kumlander, 2004; Shimizu
et al., 2013, 2012) based Cliquer. used latest version Cliquer released
2010, available homepage (Ostergard, 2010).
2. DKum (Kumlander, 2004, 2008b) implemented VB. source code found
authors homepage (Kumlander, 2008a). execute experimental environment,
translated C.
3. MinSatz (Li et al., 2012) exact weighted partial MinSAT solver, achieves
state-of-the-art performance solving clique problems combinatorial auction problems.
4. CPLEX high-performance mathematical programming solver linear programming,
mixed integer programming, quadratic programming, quadratically constrained programming problems. Let G = (V, E, w) weighted graph, V = {v1 , v2 , . . . , vn },
818

fiA N E XACT LGORITHM



AXIMUM W EIGHT C LIQUE

integer programming formulation MWC instance G usually defined follows.
max

n
X

xi w(vi )

i=1

subject
xi + xj 1,

{vi , vj }
/E

xi {0, 1},

= 1, 2, . . . , n.

Observe at-most-one constraint enforced every independent set G. solution
integer programming problem corresponds maximum weight clique G. CPLEX
12.6 used experiments solve MWC encoding integer programming
problem way.
Table 1 shows runtimes different solvers DIMACS BHOSLIB benchmarks. 80
instances DIMACS benchmark used experiment. MWCLQ solves 61 instances
within cut-off time, Cliquer, DKum, MinSatz CPLEX solves 52, 48, 58 44 instances, respectively. simplicity, exclude instances solved within 100 seconds
solvers solved solver within 3600 seconds. 39 instances displayed Table 1,
MWCLQ outperforms Cliquer 32 instances comparable Cliquer instances.
MWCLQ significantly outperforms DKum instances except hamming10-2,
solved within 20 seconds. MWCLQ dominates MinSatz 32 instances. particular,
8 instances brock family, MWCLQ 20X faster MinSatz instances 400 vertices solves four instances 800 vertices, cannot solved
MinSatz. MWCLQ outperforms CPLEX 28 instances. Particularly, CPLEX cannot solve
instance brock p hat families, MWCLQ solve efficiently.
interesting MinSatz CPLEX solve MANN a27 MANN a45 within cut-off
time, cannot solved specific MWC solver. Note MANN a27 MANN a45
also hard heuristic algorithms. instance, BLS (Benlic & Hao, 2013) find
approximating solution 12281 success rate 16% within 396.58 seconds MANN a27,
PLS (Pullan, 2008) find approximating solution 12264.
also used 40 instances BHOSLIB benchmark. instances extremely
hard exact MWC solvers. evaluated solvers solve 5 smallest instances
450 vertices. MWCLQ DKum solve 5 instances, CPLEX, Cliquer MinSatz solves 3,
1 0 instances, respectively. Moreover, MWCLQ achieves best performance 4
5 instances.
Table 2 shows mean runtimes random graphs. generate 50 graphs point. MWCLQ algorithm solves 700 graphs experiment within cutoff time,
Cliquer, Dkum, MinSatz CPLEX solves 595, 548, 548 387 instances, respectively.
MWCLQ significantly outperforms Dkum MinSatz instances dominates CPLEX
points except (200, 0.95) algorithms solve instances within 30 seconds. MWCLQ
dominates Cliquer completely random graphs density D0.7, comparable Cliquer D<0.7. MWCLQ solver solve instances point (700, 0.7).
experimental results suggest particular MWCLQ effective solving graphs
819

fiFANG , L , & XU

Table 1: Runtimes (in seconds) DIMACS BHOSLIB benchmarks. cut-off time 3600
seconds. |V | stands number vertices, density graph v
optimal solution MWC. solver cannot solve instance within 3600
seconds, runtime marked -. Instances solved within 100 seconds solvers
solved solver within cut-off time omitted.
Graph
|V |
brock400 1
400
brock400 2
400
brock400 3
400
brock400 4
400
brock800 1
800
brock800 2
800
brock800 3
800
brock800 4
800
C250.9
250
DSJC1000.5
1000
DSJC500.5
500
gen200 p0.9 44
200
gen200 p0.9 55
200
gen400 p0.9 75
400
hamming10-2
1024
johnson32-2-4
496
MANN a27
378
MANN a45
1035
p hat1000-1
1000
p hat1000-2
1000
p hat1500-1
1500
p hat500-1
500
p hat500-2
500
p hat500-3
500
p hat700-1
700
p hat700-2
700
san1000
1000
san200 0.7 2
200
san200 0.9 1
200
san200 0.9 2
200
san200 0.9 3
200
san400 0.7 1
400
san400 0.7 2
400
san400 0.7 3
400
san400 0.9 1
400
sanr200 0.7
200
sanr200 0.9
200
sanr400 0.5
400
sanr400 0.7
400
Total: 80
rb30-15-1
450
frb30-15-2
450
frb30-15-3
450
frb30-15-4
450
frb30-15-5
450
Total: 40


74
75
74
75
65
65
65
65
89
50
50
89
89
90
99
88
98
99
24
49
25
25
50
75
25
50
50
69
89
89
89
70
70
70
90
69
89
50
70

v
3422
3350
3471
3626
3121
3043
3076
2971
5092
2186
1725
5043
5416
8006
50512
2033
12283
34265
1514
5777
1619
1231
3920
5375
1441
5290
1716
2422
6825
6082
4748
3941
3110
2771
9776
2325
5126
1835
2992

82
82
82
82
82

2990
3006
2995
3032
3011

Cliquer
260.2
366.1
290.7
287.4
1548
1603
1702
1990
32.50
0.97
678.6
1718
1469
0.12
1.03
0.02
5.59
1.03
169.5
403.9
223.3
2329
0.20
1150
0.15
29.98
52
1065
1

820

DKum
607.2
524.5
573.6
606.3
2215
91.88
0.97
119.8
362.2
7.47
0.35
2.72
0.02
3.86
0.17
213.4
23.28
166.9
15.11
470.3
215.3
74.48
48
177.7
44.98
423.4
287.5
154.0
5

MinSatz
2046
2737
2363
1609
640.6
3527
32.80
72.40
35.19
4.45
3487
21.75
1628
252.3
1.52
10.41
1009
4.42
39.93
46.90
0.11
0.10
16.37
118.2
12.62
34.74
96.12
1849
2.73
68.97
9.44
429.0
58
0

CPLEX
30.41
2.44
1.81
238.7
0.06
0.43
2.07
32.61
0.70
0.31
0.83
3.54
24.45
87.75
43.53
13.30
10.81
11.01
44
193.5
258.2
118.3
3

MWCLQ
129.3
127.7
107.2
81.81
1427
2002
1545
2039
39.99
81.30
0.92
7.00
2.76
15.46
0.53
2501
4.12
0.01
2.34
916.7
0.13
47.88
183.8
0
0.24
1.64
16.19
3.44
4.98
6.37
1257
0.13
6.40
0.31
25.57
61
244.9
30.14
131.7
181.8
57.31
5

fiA N E XACT LGORITHM



AXIMUM W EIGHT C LIQUE

Table 2: Mean runtimes seconds random graphs, obtained solving 50 graphs point.
cut-off time 3600 seconds. |V | stands number vertices, density
graph v weight optimal solution, averaged solved instances. mean runtime marked - instance solved point within
cut-off time. # stands number instances solved solver within cut-off
time
|V |
150
150
200
200
200
300
300
300
500
500
600
600
700
700

Benchmark

v
0.90
3394
0.95
4766
0.80
3249
0.90
5095
0.95
7372
0.70
2441
0.80
3334
0.90
5351
0.60
2285
0.70
2969
0.60
2496
0.70
3293
0.60
2510
0.70
3283
Total 700

Cliquer
Time
#
21.06
50
1006
50
4.02
50
974.9
50
0
1.76
50
81.45
50
0
3.94
50
119.9
50
23.51
50
1256
50
46.01
50
3006
45
595

DKum
Time
#
7.92
50
35.42
50
5.05
50
373.0
50
2464
19
3.01
50
107.1
50
0
9.45
50
302.1
50
62.46
50
2884
29
132.8
50
0
548

MinSatz
Time
#
3.57
50
2.21
50
12.65
50
94.63
50
111.1
50
32.36
50
379.6
50
0
208.5
50
2994
48
1006
50
0
2757
50
0
548

CPLEX
Time
#
2.58
50
1.99
50
54.83
50
23.45
50
3.91
50
353.5
50
303.7
47
263.5
40
0
0
0
0
0
0
387

MWCLQ
Time
#
0.58
50
0.42
50
0.99
50
14.30
50
28.77
50
1.13
50
14.87
50
845.4
50
6.03
50
93.36
50
34.79
50
869.9
50
78.24
50
2434
50
700

Table 3: Lower bounds given different solvers DIMACS instances cannot solved
solver within cut-off time.
Instance
C1000.9
C2000.5
C2000.9
C4000.5
C500.9
MANN a81
gen400 p0.9 55
gen400 p0.9 65
hamming10-4
keller5
keller6
p hat1000-3
p hat1500-2
p hat1500-3
p hat700-3

|V |
1000
2000
2000
4000
500
3321
400
400
1024
776
3361
1000
1500
1500
700


90
50
90
50
90
100
90
90
83
75
82
74
51
75
75

Cliquer
1181
2466
534
1284
1868
195
2501
2855
738
2860
511
2417
2897
1521
2822

DKum
1846
1186
782
863
2070
1201
3037
3179
1798
2139
1637
2893
3127
2067
5156

821

MinSatz
6385
2198
6747
2263
5594
100970
5988
6180
4062
3317
5595
6779
6100
7050
7340

CPLEX
8066
1358
7362
1854
6520
111386
6611
6720
5042
3317
6937
7258
5954
9058
7400

MWCLQ
8471
2466
10034
2698
6672
111033
6676
6832
4614
3317
6316
7588
7104
8449
7565

fiFANG , L , & XU

104

Time

103

102

101

MWCLQ
Cliquer
DKum

san200_0.9_2
gen200_p0.9_44
san400_0.7_3
san400_0.7_2
san200_0.9_3
san400_0.7_1
brock400_3
p_hat700-2
c250.9
hamming10-2
san1000
brock800_3
brock400_4
p_hat500-3
brock400_1
brock800_1
brock800_2
p_hat1000-2
brock800_4
san400_0.9_1

100

Figure 4: Runtimes find optimal solution. Instances solvers find optimal
solution within 100 seconds MWCLQ finds optimal solution less one second
omitted

high density. CPLEX also effective dense graphs, cannot solve random instance
500 vertices.
15 DIMACS instances cannot solved algorithm within cut-off time,
report largest weight clique found solver algorithm terminates,
lower bound optimal solution. Table 3 shows MWCLQ computes best lower bounds
11 instances, CPLEX gives best lower bounds 5 instances. MWCLQ give
significantly better lower bound MWC solvers instances except C2000.5,
MWCLQ Cliquer share bound. result suggests MWCLQ often compute
better approximate solution exact solvers within given time.
exact algorithm MC MWC usually solves instance two phases. first phase,
algorithm finds optimal solution. second phase, algorithm proves
solution indeed optimal showing better solution exists. Fig. 4, compare
runtimes Cliquer, Dkum MWCLQ need find optimal solution DIMACS instance.
822

fiA N E XACT LGORITHM



AXIMUM W EIGHT C LIQUE

simplicity, instances solvers find optimal solution within 100 seconds
MWCLQ finds optimal solution less one second omitted. MWCLQ always finds
optimal solution much faster solvers instances except hamming10-2.
instances brock800 2 brock800 4, although MWCLQ spends time Cliquer
exactly solving (see Table 1), finds optimal solution 10 times faster Cliquer.
summary, Table 3 Fig. 4 show MWCLQ generally finds optimal solution
much faster solvers, although sometimes may spend time prove optimality.
4.3 Comparison Different Encodings MWC MaxSAT
presented four encodings MWC MaxSAT, namely direct encoding, split encoding, iterative split encoding LW encoding. MWC instance encoded
MaxSAT instance using first three encodings, MaxSAT solver used search
optimal solution MWC instance. However, MWC instance encoded MaxSAT
instance using LW encoding, optimal solution MaxSAT instance MWC,
upper bound MWC. Therefore, MaxSAT solver used solve MWC instance,
first three encodings used.
Different MaxSAT solvers, MWCLQ uses MaxSAT reasoning upper bounding procedure search tree node, current subgraph dynamically encoded
MaxSAT instance. So, four encodings could used MWCLQ encode current
subgraph.
Recall MWCLQ based LW encoding. implemented three versions
MWCLQ, i.e., MWCLQdir , MWCLQsp MWCLQit, identical MWCLQ,
based direct encoding, split encoding iterative split encoding, respectively.
MaxSAT instance obtained using direct encoding, inconsistent subsets contain two soft
clauses, soft clauses unit. MWCLQdir detects inconsistent subset
{(x1 , w(x1 )), (x2 , w(x2 ))}, weighted clause subset, say, (x2 , w(x2 )), split
(x2 , w(x1 )) (x2 , w(x2 )w(x1 )). upper bound improved w(x1 ),
clause (x2 , w(x2 )w(x1 )) used detection. MaxSAT instance obtained using
split iterative split encoding, vertices soft clause weight.
Observe Top-k literal failed clause Top-k empty clause make sense,
(k, )-Rule needed three encoding.
experiment, ran following four different kinds state-of-the-art MaxSAT solvers
solve MWC instances encoded MaxSAT using direct encoding, split encoding
iterative split encoding.
1. akMaxSat (Kugel, 2010) branch-and-bound MaxSAT solver computes lower
bound combination MaxSAT resolution detection disjoint inconsistent subsets. One authors sent us source code submitted MaxSAT evaluation 2012.
2. MaxSatz (Li et al., 2007) branch-and-bound MaxSAT solver incorporates SAT
technologies. used latest version MaxSatz2013, one best solvers
MaxSAT evaluation 2013.
3. WPM1-2013 (Ansotegui, Bonet, & Levy, 2009) MaxSAT solver based successive calls
SAT solvers. One authors provided us executable file WPM used
MaxSAT evaluation 2013.
823

fiFANG , L , & XU

4. MaxHS (Davies & Bacchus, 2013b) hybrid Maxsat solver exploits SAT
integer programming technologies.
report number instances solved MWCLQdir , MWCLQsp, MWCLQit, MWCLQ,
akMaxSat, MaxSatz MaxHS within 3600 seconds. results WPM reported, solves 3 DIMACS instances solve BHOSLIB instance. addition,
always runs memory solving random instances.

Table 4: Number instances solved within 3600 seconds MaxSAT solvers using three different
encodings four versions MWCLQ. Direct stands direct encoding, Split
split encoding Iter iterative split encoding.
Instance
Benchmark
#
brock
12
c-fat
7
C
7
DSJC
2
gen
5
hamming
6
johnson
4
keller
3
MANN
4
p hat
15
san
15
DIMACS: 80
BHOSLIB: 40
(150,0.9)
50
(150,0.95)
50
(200,0.8)
50
(200,0.9)
50
(200,0.95)
50
(300,0.7)
50
(300,0.8)
50
(300,0.9)
50
(500,0.6)
50
(500,0.7)
50
(600,0.6)
50
(600,0.7)
50
(700,0.6)
50
(700,0.7)
50
RAND: 700

akMaxSat
Direct
Split
Iter
4
4
4
7
7
7
2
2
2
0
0
0
2
2
2
5
5
5
3
3
3
1
1
1
1
1
1
5
4
5
7
9
9
37
38
39
0
0
1
43
50
50
46
50
50
38
50
50
44
50
50
44
50
50
30
48
50
14
7
9
8
8
8
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
267
313
317

Direct
4
7
2
1
2
5
3
1
2
7
8
42
0
50
49
49
49
49
50
47
2
0
0
0
0
0
1
346

MaxSatz
Split
4
7
2
1
2
5
3
1
2
5
8
40
0
50
50
50
50
50
50
19
0
0
0
0
0
0
0
319

Iter
4
7
2
1
2
5
3
1
2
7
8
42
5
50
50
50
50
50
50
18
0
0
0
0
0
0
0
318

Direct
4
7
2
0
3
5
4
1
3
4
13
46
3
50
50
50
50
50
50
41
35
0
0
0
0
0
0
376

MaxHS
Split
0
7
1
0
0
4
3
0
2
1
4
22
0
7
50
0
0
14
0
0
0
0
0
0
0
0
0
71

Iter
0
7
1
0
0
3
2
0
2
0
3
18
0
7
50
0
0
14
0
0
0
0
0
0
0
0
0
71

MWCLQdir
4
7
1
2
0
3
3
1
1
8
6
36
0
50
0
50
0
0
50
50
0
50
49
50
0
50
0
399

MWCLQsp
8
7
2
2
2
4
3
1
1
8
10
48
0
50
50
50
41
25
50
50
0
50
41
50
0
50
0
507

MWCLQit
8
7
2
2
2
4
3
1
1
9
14
53
5
50
50
50
50
50
50
50
29
50
50
50
39
50
0
618

MWCLQ
12
7
2
2
2
5
3
1
1
11
15
61
5
50
50
50
50
50
50
50
50
50
50
50
50
50
50
700

Experimental results Table 4 suggest encoding approaches affect performances
MaxSAT solvers solve MWC instance. However, effectiveness direct encoding,
split encoding iterative encoding different MaxSAT solvers clear. Concretely,
iterative split encoding makes akMaxSat little faster encodings do. MaxHS using
direct encoding dominates MaxHS using encodings. iterative split encoding better
choice MaxSatz solve BHOSLIB instances, direct encoding better solve random
graphs. results also show MWCLQ significantly outperforms MaxSAT solvers solve
MWC instance. observe although MaxSAT reasoning powerful improving upper
bound BnB algorithm MC MWC, using MaxSAT solver solve MWC instance
effective.
824

fiA N E XACT LGORITHM



AXIMUM W EIGHT C LIQUE

Thanks extended MaxSAT reasoning, MWCLQ significantly outperforms MWCLQdir ,
MWCLQsp MWCLQit . MWCLQdir MWCLQsp slow solving MWC instances
cannot capture graph structure well. Although iterative split encoding
MWCLQit capture graph structure, MWCLQit exploit power MaxSAT
reasoning efficiently, clause splittings MWCLQit driven MaxSAT reasoning.
advantages MWCLQ described follows: (1) BnB algorithm MWC
able exploit graph structure, especially ordering vertices partitioning graph
independent sets search tree node, MaxSAT solver do; (2) exploits
power MaxSAT reasoning upper bounding procedure derive tight upper bound,
classical BnB algorithm MWC do. Nevertheless, make MaxSAT reasoning
beneficial, relevant encoding MWC MaxSAT necessary, shown Table 4.
encoding obvious impact performance MWCLQ.
4.4 Effectiveness Upper Bound Based Extended MaxSAT Reasoning
study impact extended MaxSAT reasoning soft clause splitting, implemented
two derived versions MWCLQ, namely, MWCLQ-- MWCLQ-. MWCLQ-- identical
MWCLQ except uses trivial bound based independent partition graph,
equal sum largest weight independent set. MWCLQ- identical
MWCLQ except detects inconsistent subsets soft clauses, use -Rule
(k, )-Rule split clauses subset.
Table 5 shows runtimes (in seconds) search tree sizes (in thousands) MWCLQ--, MWCLQand MWCLQ solving DIMACS BHOSLIB instances, well gain ratio
MWCLQ- MWCLQ compared MWCLQ-- terms runtime search tree size computed MWCLQ--/MWCLQ- MWCLQ--/MWCLQ, respectively. MaxSAT reasoning without
soft clause splitting MWCLQ- makes MWCLQ- better MWCLQ-- terms search tree
size. reduction search tree size enough general compensate overhead
MaxSAT reasoning, gain MWCLQ- compared MWCLQ-- terms runtime
clear. However, soft clause splitting using -Rule (k, )-Rule allows MaxSAT reasoning prune much search space, making MWCLQ substantially (from 1.08 4.05 times)
faster MWCLQ--. fact, MWCLQ solves 3 instances MWCLQ-- MWCLQ-,
significantly outperforms instances except san1000. Observe MWCLQ--
MWCLQ- solve number instances Minsatz (58) DIMACS benchmark,
solve instances Cliquer (52), Dkum (48) CPLEX (44).
Table 6 shows mean runtimes (in seconds) mean search tree sizes (in thousands) different
versions MWCLQ random instances, averaged solved instances point,
well gain ratio MWCLQ MWCLQ- compared MWCLQ--. MWCLQ solves
instances. However, neither MWCLQ-- MWCLQ- solve graphs points (300, 0.90)
(700, 0.7). MaxSAT reasoning allows MWCLQ- prune search space MWCLQ-on instances, overhead makes MWCLQ- slower instances 300 vertices. However, similarly DIMACS BHOSLIB cases, MWCLQ 1.2 14.3 times
faster MWCLQ--, soft clause splittings using -Rule (k, )-Rule allow
MWCLQ substantially reduce search space instances. Furthermore, gain terms
runtime search tree size increases density graph.
825

fiFANG , L , & XU

Table 5: Runtimes (in seconds) search tree sizes (in thousands) different versions MWCLQ solving DIMACS BHOSLIB instances, well gain ratio MWCLQ MWCLQ- compared MWCLQ--. cut-off time 3600 seconds. -
means solver cannot solve instance within cut-off time. Instances solved
within 10 seconds solvers solved within cut-off time omitted.
Benchmark
Graph
brock400 1
brock400 2
brock400 3
brock400 4
brock800 1
brock800 2
brock800 3
brock800 4
C250.9
DSJC1000.5
gen200 p0.9 44
gen200 p0.9 55
hamming10-2
p hat1000-2
p hat500-3
p hat700-2
san1000
san200 0.9 3
san400 0.7 3
san400 0.9 1
sanr200 0.9
sanr400 0.7
frb30-15-1
frb30-15-2
frb30-15-3
frb30-15-4
frb30-15-5

MWCLQ-Time
Size
209.0
63153
203.5
58770
173.4
50079
131.5
37420
1813
421259
2563
648486
1968
470594
2623
675790
159.4
31542
87.73
26154
27.83
7560
10.41
2530
3009
457787
113.4
17715
135.9
31173
56.31
15955
9.79
2936
25.90
6690
35.71
12228
411.3
156311
42.28
13416
171.8
67246
267.9
83139
85.27
22362

Time
234.3
227.7
194.2
146.3
1990
2695
2201
2931
160.5
91.87
23.12
8.70
2813
117.5
137.2
48.47
10.53
20.10
39.60
394.8
39.17
167.0
257.3
82.23

MWCLQGain
Size
0.89
54505
0.89
51244
0.89
43616
0.90
32853
0.91
396425
0.95
605433
0.89
442518
0.89
633367
0.99
20114
0.95
25207
1.20
3692
1.20
1310
1.07
238949
0.97
10571
0.99
30680
1.16
7946
0.93
2403
1.29
3001
0.90
10797
1.04
108476
1.08
9287
1.03
48401
1.04
63411
1.04
17705

Gain
1.16
1.15
1.15
1.14
1.06
1.07
1.06
1.07
1.57
1.04
2.05
1.93
1.92
1.68
1.02
2.01
1.22
2.23
1.13
1.44
1.44
1.39
1.31
1.26

Time
129.3
127.7
107.2
81.81
1427
2002
1545
2039
39.99
81.30
7.00
2.76
15.46
2501
916.7
47.88
183.8
16.19
6.37
1257
6.40
25.57
244.9
30.14
131.7
181.8
57.31

MWCLQ
Gain
Size
1.62
25367
1.59
23788
1.62
19830
1.61
15133
1.27
223715
1.28
339910
1.27
248218
1.29
352700
3.99
4472
1.08
17032
3.98
979
3.77
344
41758
176892
3.28
77115
2.37
4186
0.74
21694
3.48
2180
1.54
1213
63106
4.05
850
1.40
5705
1.68
27928
1.40
2597
1.30
14589
1.47
16336
1.49
5575

Gain
2.49
2.47
2.53
2.47
1.88
1.91
1.90
1.92
7.05
1.54
7.72
7.34
5.94
4.23
1.44
7.32
2.42
7.87
2.14
5.60
5.17
4.61
5.09
4.01

Table 6: Runtimes (in seconds) search tree sizes (in thousands) different versions MWCLQ random instances, averaged solved instances point, well
gain ratio MWCLQ MWCLQ- compared MWCLQ--. cut-off time
3600 seconds solver solve one instance.
Graph
|V |

150
0.90
150
0.95
200
0.80
200
0.90
200
0.95
300
0.70
300
0.80
300
0.90
500
0.60
500
0.70
600
0.60
600
0.70
700
0.60
700
0.70

#
50
50
50
50
50
50
50
31
50
50
50
50
50
31

MWCLQ-Time
Size
1.85
715
5.23
1440
1.71
725
49.41
14030
405.8
81772
1.58
552
27.09
7655
2261
399832
7.53
2310
133.8
34848
42.85
13483
1229
320607
94.10
24897
3177
682683

#
50
50
50
50
50
50
50
33
50
50
50
50
50
15

Time
1.34
1.28
1.89
40.13
117.1
1.80
30.98
2261
8.12
149.5
46.40
1390
104.8
3343

MWCLQGain
Size
1.38
255
4.09
123
0.90
539
1.23
6109
3.47
8583
0.88
493
0.87
6265
1.00
245004
0.93
2176
0.89
31905
0.92
12661
0.88
292627
0.90
23624
0.95
575774

826

Gain
2.81
11.69
1.34
2.30
9.53
1.12
1.22
1.63
1.06
1.09
1.06
1.10
1.05
1.19

#
50
50
50
50
50
50
50
50
50
50
50
50
50
50

Time
0.58
0.42
0.99
14.30
28.77
1.13
14.87
845.4
6.03
93.36
34.79
869.9
78.24
2434

MWCLQ
Gain
Size
3.19
108
12.45
46
1.73
245
3.46
2031
14.10
2454
1.40
263
1.82
2611
2.67
85676
1.25
1314
1.43
16326
1.23
7667
1.41
149522
1.20
14219
1.31
351894

Gain
6.58
30.69
2.96
6.91
33.32
2.10
2.93
4.67
1.76
2.13
1.76
2.14
1.75
1.94

fiA N E XACT LGORITHM



AXIMUM W EIGHT C LIQUE

4.5 Application MWCLQ Winner Determination Problem
important application MWC solve winner determination problem (WDP) combinatorial auctions. auctioneer set items, = {1, 2, . . . , m}, sell, buyers
submit set n bids, B = {B1 , B2 , . . . , Bn }. bid pair Bi = (Si , Pi ), Si
subset items Pi 0 price items Si . WDP problem determine
bids winning losing maximum auctioneers revenue, item
allocated one bidder. define MWC instance G = (V, E, w) WDP instance
follows.
bid Bi B, define vertex vi weight w(vi ) = Pi , i.e., V = {v1 , v2 , . . . , vn }
w(vi ) = Pi ;
Add edge {vi , vj } G Bi Bj share common items, i.e., E =
{{vi , vj } | Si Sj = , 1 < j n}.
maximum weight clique G corresponds feasible subset bids maximum revenue.
experiment, compared MWCLQ Cliquer, Dkum, MinSatz CPLEX realistic WDP instances. also selected MaxHS using direct encoding,
effective MaxSAT solver solve MWC instance, comparison. used benchmark
provided Lau Goh (2002), widely used benchmark purpose test WDP
algorithms (Guo, Lim, Rodrigues, & Zhu, 2006; Sghir, Hao, Jaafar, & Ghedira, 2014; Wu & Hao,
2015). Instances benchmark generated incorporating following factors, i.e., pricing factor models bidders acceptable price range bid, preference factor
takes account bidders preferences among bids, fairness factor measures fairness distributing items among bidders. benchmark contains 500 instances 1500
items 1500 bids, divided 5 groups item number bid number.
group contains 100 instances labeled REL-m-n, number items n
number bids.
Table 7: Mean runtimes seconds WDP instances, obtained solving 100 graphs
group. cut-off time 3600 seconds. mean runtime marked - instance
solved group within cut-off time. # stands number instances solved
solver.
Benchmark
Group
REL-500-1000
REL-1000-1000
REL-1000-500
REL-1000-1500
REL-1500-1500

#
100
100
100
100
100

Cliquer
Time
#
809.9
100
3.73
100
0.03
100
2.84
100
3.38
100

DKum
Time
#
628.3
100
4.52
100
0.03
100
3.55
100
5.30
100

MinSatz
Time
#
552.5
100
25.39
100
0.81
100
67.90
100
78.19
100

MaxHS
Time
#
0
0
1003.7
100
0
0

CPLEX
Time
#
0
0
266.7
100
0
0

MWCLQ
Time
#
55.61
100
1.45
100
0.05
100
1.56
100
2.42
100

Table 7 summarizes mean runtimes numbers instances solved within cut-off time
group. Results show MWCLQ outperforms solvers groups except REL-1000500, Cliquer, Dkum, MinSatz MWCLQ comparable. MWCLQ achieves least
10X speedup instances hardest group REL-500-1000. MaxHS CPLEX
effective solving instances. experiment, transformed WDP MWC first
827

fiFANG , L , & XU

formulated integer programming. Another method used Wu Hao (2015)
formulate WDP directly integer programming. method appears slightly
effective transformation experiment, affect comparison,
instances CPLEX able solve within 3600 seconds also REL-1000-500
transformation.
Table 8 reports detailed comparative results first 10 instances group. MWCLQ outperforms solvers instances except 10 instances easiest group
(in401, in402, ..., in410), solved Cliquer, DKum, MinSatz MWCLQ within
less one second. results suggest MWCLQ effective solving MWC instances WDP specific MWC algorithms, MinSatz, MaxHS CPLEX. Moreover,
MWCLQ even efficient state-of-the-art heuristic algorithms relatively
hard instances. example, MWCLQ solves in108 101.04 seconds, heuristic algorithm (Wu & Hao, 2015), based tabu search takes, 113.53 seconds find solution
probability 0.73. Ignoring difference running environments, MWCLQ faster tabu
search algorithm in108. Note heuristic algorithms give feasible solution,
cannot guarantee optimality.

5. Conclusion
MaxSAT reasoning proved effective MC problem, based partition
graph independent sets. However, MaxSAT reasoning cannot naturally extended solve
MWC problem literal weights, shown relatively poor performance
MWCLQ- , MWCLQdir , MWCLQsp MWCLQit. MWCLQ- exploits MaxSAT reasoning
deal literal weights. encodings MWC MaxSAT used MWCLQdir
MWCLQsp capture well graph structure. Although MWCLQit exploit graph
structure, many useless splits difficulties take advantage MaxSAT reasoning
efficiently. thus propose encode MWC instance literal-weighted MaxSAT instance,
soft clauses literals soft clauses weighted. optimal solution
LW MaxSAT instance MWC, upper bound MWC. interest LW
encoding transform LW MaxSAT instance optimal solution new
instance tighter upper bound MWC.
Concretely, every search tree node BnB algorithm MWC, partition current
subgraph independent sets obtain LW MaxSAT instance, soft clause
corresponds independent set. successively transform LW MaxSAT instance
identifying Top-k literal failed clause Top-k empty clause using -Rule
(k, )-Rule. Consequently, obtain tight upper bound MWC prune search
space. approach implemented MWCLQ, substantially better MWCLQ-,
MWCLQdir , MWCLQsp MWCLQit, confirming effectiveness approach. MWCLQ
also favorably compared state-of-the-art MWC solvers Cliquer, DKum, MinSatz
CPLEX, well several state-of-the-art MaxSAT solvers using different encodings, standard
benchmarks instances realistic applications.
future, plan study impact vertex ordering unit clause ordering MWCLQ. also interesting use MaxSAT reasoning solve combinatorial optimization
problems, especially weighted version, using dedicated encoding.
828

fiA N E XACT LGORITHM



AXIMUM W EIGHT C LIQUE

Table 8: Runtimes seconds first 10 instances group benchmark Table 7.
cut-off time 3600 seconds. |V | stands number vertices, density
graph transforming WDP MWC v optimal solution MWC.
solver cannot solve instance within 3600 seconds, runtime marked -
Graph
in101
in102
in103
in104
in105
in106
in107
in108
in109
in110
in201
in202
in203
in204
in205
in206
in207
in208
in209
in210
in401
in402
in403
in404
in405
in406
in407
in408
in409
in410
in501
in502
in503
in504
in505
in506
in507
in508
in509
in510
in601
in602
in603
in604
in605
in606
in607
in608
in609
in610

|V |
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
1000
500
500
500
500
500
500
500
500
500
500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500
1500


0.31
0.30
0.31
0.30
0.30
0.30
0.30
0.31
0.30
0.30
0.15
0.16
0.16
0.17
0.16
0.16
0.17
0.16
0.16
0.16
0.15
0.15
0.16
0.17
0.17
0.14
0.17
0.16
0.14
0.17
0.09
0.08
0.08
0.08
0.08
0.08
0.08
0.07
0.08
0.08
0.09
0.09
0.09
0.10
0.10
0.10
0.10
0.10
0.10
0.11

v
72724.617
72518.222
72129.500
72709.646
75646.127
71258.613
69713.403
75813.205
69475.895
68295.289
81557.742
90708.127
86239.214
87075.428
86515.951
91518.964
93129.248
94904.679
87268.965
89962.396
77417.482
76273.336
74843.957
78761.690
75915.900
72863.324
76365.717
77018.833
73188.619
73791.658
88656.958
86236.911
87812.377
85600.001
84860.165
84623.414
90288.472
86853.500
88316.087
89014.137
108800.445
105611.476
105121.021
107733.805
109840.984
107113.067
113180.284
105266.107
109472.332
113716.965

Cliquer
819.6
414.9
551.4
330.8
840.3
272.7
595.9
1321
247.3
308.5
1.80
3.73
3.63
6.13
3.11
2.37
3.95
4.07
2.13
3.54
0.02
0.02
0.03
0.05
0.05
0.02
0.05
0.04
0.01
0.04
4.64
1.69
3.44
2.37
2.16
1.41
2.82
1.24
3.59
1.29
4.52
2.01
1.84
3.86
3.72
2.72
4.20
2.21
2.77
6.14

DKum
616.1
289.0
455.5
218.1
547.3
346.4
460.1
1121
238.8
307.7
2.47
4.98
6.65
7.20
4.85
3.37
5.69
5.18
3.38
4.78
0.03
0.02
0.04
0.08
0.08
0.02
0.07
0.04
0.02
0.06
5.68
2.30
4.42
3.80
2.82
2.32
3.73
1.60
4.37
2.19
5.51
2.98
2.56
5.71
5.75
4.37
6.65
3.72
3.78
10.03

829

MinSatz
610.8
308.8
479.6
353.5
367.7
270.7
736.9
924.9
305.0
411.7
22.09
25.90
26.66
30.47
27.42
22.85
28.18
22.09
26.66
22.09
0.69
0.72
0.76
0.89
0.88
0.75
0.79
0.82
0.71
0.85
81.25
65.16
72.06
61.32
62.09
65.92
68.99
58.26
64.39
64.39
76.04
70.61
65.96
84.58
75.27
77.60
78.37
77.60
69.84
94.67

MaxHS
923.01
1107.2
891.23
1082.9
1300.6
992.34
1569.1
1187.2
846.56
1132.1
-

CPLEX
181.70
198.12
229.0
208.97
224.98
175.15
364.58
199.39
217.31
243.42
-

MWCLQ
53.87
34.40
44.94
37.00
37.33
23.56
68.07
101.04
29.10
43.16
1.01
1.37
1.57
2.01
1.64
1.11
1.69
1.05
1.54
1.21
0.04
0.04
0.04
0.06
0.06
0.05
0.06
0.06
0.04
0.06
2.30
1.45
1.72
1.56
1.56
1.34
1.32
1.11
1.80
1.15
2.39
1.75
1.35
3.09
2.22
2.13
2.38
2.17
1.86
3.70

fiFANG , L , & XU

6. Acknowledgments
would like thank anonymous reviewers helpful comments suggestions. also
thank Jichang Zhao, Qiao Kan, Xu Feng Shaowei Cai proofreads suggestions. Part
work done first author joint Ph.D. student Universite de Picardie Jules
Verne. research partly supported NSFC (Grant No. 61421003), fund State
Key Lab Software Development Environment (Grant No. SKLSDE-2015ZX-05), Chinese
State Key Laboratory Software Development Environment Open Fund (Grant No. SKLSDE2012KF-07), MeCS platform Universite de Picardie Jules Verne.

References
Abrame, A., & Habet, D. (2014). Efficient application max-sat resolution inconsistent subsets.
Proc. CP-2014, pp. 92107. Springer.
Abrame, A., & Habet, D. (2015). resiliency unit propagation max-resolution. Proc.
AAAI-2015, pp. 268274. AAAI Press.
Ansotegui, C., Bonet, M. L., & Levy, J. (2009). Solving (weighted) partial maxsat satisfiability testing. Theory Applications Satisfiability Testing-SAT 2009, pp. 427440.
Ansotegui, C., Bonet, M. L., & Levy, J. (2013). Sat-based maxsat algorithms. Artificial Intelligence,
196, 77105.
Ansotegui, C., & Gabas, J. (2013). Solving (weighted) partial maxsat ILP. Proc. CPAIOR2013, pp. 403409.
Benlic, U., & Hao, J. K. (2013). Breakout local search maximum clique problems. Computers
& Operations Research, 40, 192206.
Bonet, M. L., Levy, J., & Manya, F. (2006). complete calculus max-sat. Theory
Applications Satisfiability Testing-SAT 2006, pp. 240251. Springer.
Cai, S., Su, K., Luo, C., & Sattar, A. (2013). NuMVC: efficient local search algorithm
minimum vertex cover. Journal Artificial Intelligence Research, 46, 687716.
Cai, S., Su, K., & Sattar, A. (2011). Local search edge weighting configuration checking
heuristics minimum vertex cover. Artificial Intelligence, 175(9), 16721696.
Chen, J. (2010). new SAT encoding at-most-one constraint. International Workshop
Modelling Reformulating Constraint Satisfaction Problems.
Davies, J., & Bacchus, F. (2013a). Exploiting power mip solvers maxsat. Theory
Applications Satisfiability TestingSAT 2013, pp. 166181. Springer.
Davies, J., & Bacchus, F. (2013b). Postponing optimization speed MAXSAT solving. Proc.
CP-2013, pp. 247262. Springer.
Downey, R. G., & Fellows, M. R. (1995). Fixed-parameter tractability completeness I: Basic
results. SIAM Journal Computing, 24(4), 873921.
Fahle, T. (2002). Simple fast: Improving branch-and-bound algorithm maximum clique.
Proc. ESA-2002, pp. 485498.
Fang, Z., Chu, Y., Qiao, K., Feng, X., & Xu, K. (2014a). Combining edge weight vertex weight
minimum vertex cover problem. Frontiers Algorithmics, pp. 7181. Springer.
830

fiA N E XACT LGORITHM



AXIMUM W EIGHT C LIQUE

Fang, Z., Li, C. M., Qiao, K., Feng, X., & Xu, K. (2014b). Solving maximum weight clique using
maximum satisfiability reasoning. Proc. ECAI-2014, Vol. 263, pp. 303 308.
Feige, U. (2004). Approximating maximum clique removing subgraphs. SIAM Journal
Discrete Mathematics, 18(2), 219225.
Freeman, J. W. (1995). Improvements propositional satisfiability search algorithms. Ph.D. thesis.
Guo, Y., Lim, A., Rodrigues, B., & Zhu, Y. (2006). Heuristics bidding problem. Computers &
operations research, 33(8), 21792188.
Ignatiev, A., Morgado, A., & Marques-Silva, J. (2014). reducing maximum independent set
minimum satisfiability. Theory Applications Satisfiability TestingSAT 2014, pp.
103120. Springer.
Karp, R. M. (1972). Reducibility among combinatorial problems. Complexity Computer
Computations, pp. pp 85103. Springer.
Kibanov, M., Atzmueller, M., Scholz, C., & Stumme, G. (2014). Temporal evolution contacts
communities networks face-to-face human interactions. Science China Information
Sciences, 57(3), 117.
Konc, J., & Janezic, D. (2007). improved branch bound algorithm maximum clique
problem. Communications Mathematical Computer Chemistry, 58, 569590.
Kugel, A. (2010). Improved exact solver weighted Max-SAT problem. Workshop Pragmatics SAT, Vol. 436.
Kugel, A. (2012). Natural Max-SAT encoding Min-SAT. Learning Intelligent Optimization, pp. 431436. Springer.
Kumlander, D. (2004). new exact algorithm maximum-weight clique problem based
heuristic vertex-coloring backtrack search. Proc. MOC-2004, pp. 202208.
Kumlander, D. (2008a) http://www.kumlander.eu/graph/index.html.
Kumlander, D. (2008b). importance special sorting maximum-weight clique algorithm
based colour classes. Modelling, computation optimization information systems
management sciences, pp. 165174. Springer.
Larrosa, J., Heras, F., & de Givry, S. (2008). logical approach efficient max-sat solving.
Artificial Intelligence, 172(2), 204233.
Lau, H. C., & Goh, Y. G. (2002). intelligent brokering system support multi-agent web-based
4 th-party logistics. Proc. ICTAI-2002, pp. 154161. IEEE.
Li, C. M., & Anbulagan, A. (1997). Heuristics based unit propagation satisfiability problems.
Proc. IJCAI-1997, pp. 366371. Morgan Kaufmann Publishers Inc.
Li, C. M., Fang, Z., & Xu, K. (2013). Combining MaxSAT reasoning incremental upper bound
maximum clique problem. Proc. ICTAI-2013, pp. 939946. IEEE.
Li, C. M., Manya, F., & Planes, J. (2005). Exploiting unit propagation compute lower bounds
branch bound max-sat solvers. Proc. CP-2005, Vol. 3709, pp. 403414. Springer.
Li, C. M., Manya, F., & Planes, J. (2007). New inference rules Max-SAT. Journal Artificial
Intelligence Research, 30, 321359.
831

fiFANG , L , & XU

Li, C. M., & Manya, F. (2009). Maxsat, hard soft constraints.. Handbook satisfiability, 185,
613631.
Li, C. M., Manya, F., & Planes, J. (2006). Detecting disjoint inconsistent subformulas computing
lower bounds max-sat. Proc. AAAI-2006, Vol. 6, pp. 8691.
Li, C. M., & Quan, Z. (2010a). Combining graph structure exploitation propositional reasoning
maximum clique problem. Proc. ICTAI-2010, Vol. 1, pp. 344351. IEEE.
Li, C. M., & Quan, Z. (2010b). efficient branch-and-bound algorithm based MaxSAT
maximum clique problem. Proc. AAAI-2010, pp. 128133.
Li, C. M., Zhu, Z., Manya, F., & Simon, L. (2012). Optimizing minimum satisfiability. Artificial Intelligence, 190, 3244.
Liu, T., Lin, X., Wang, C., Su, K., & Xu, K. (2011). Large hinge width sparse random hypergraphs. Proc. IJCAI-2011, Vol. 2011, pp. 611616.
Ma, T., & Latecki, L. J. (2012). Maximum weight cliques mutex constraints video object
segmentation. Proc. CVPR-2012, pp. 670677. IEEE.
Martins, R., Joshi, S., Manquinho, V., & Lynce, I. (2014). Incremental cardinality constraints
maxsat. Proc. CP-2014, pp. 531548. Springer.
Mascia, F., Cilia, E., Brunato, M., & Passerini, A. (2010). Predicting structural functional sites
proteins searching maximum-weight cliques. Proc. AAAI-2010, pp. 12741279.
AAAI.
Morgado, A., Dodaro, C., & Marques-Silva, J. (2014). Core-guided maxsat soft cardinality
constraints. Proc. CP-2014, pp. 564573. Springer.
Morgado, A., Heras, F., Liffiton, M., Planes, J., & Marques-Silva, J. (2013). Iterative coreguided maxsat solving: survey assessment. Constraints, 18(4), 478534.
Ostergard, P. (2001). new algorithm maximum-weight clique problem. Nordic Journal
Computing, 8, 424436.
Ostergard, P. (2002). fast algorithm maximum clique problem. Discrete Applied Mathematics, 120, 197207.
Ostergard, P. (2010). Cliquer source code. http://users.tkk.fi/pat/cliquer.html.
Pullan, W. (2008). Approximating maximum vertex/edge weighted clique using local search.
Journal Heuristics, 19, 117134.
Pullan, W., & Hoos, H. H. (2006). Dynamic local search maximum clique problem. Journal
Artificial Intelligence Research, 25, 159185.
Regin, J. C. (2003). Solving maximum clique problem constraint programming. Proc.
CPAIOR-2003, pp. 634648.
Sghir, I., Hao, J.-K., Jaafar, I. B., & Ghedira, K. (2014). recombination-based tabu search algorithm winner determination problem. Artificial Evolution, pp. 157167. Springer.
Shimizu, S., Yamaguchi, K., Saitoh, T., & Masuda, S. (2012). improvements Kumlanders
maximum weight clique extraction algorithm. Proc. International Conference
Electrical, Computer, Electronics Communication Engineering, pp. 307311.
832

fiA N E XACT LGORITHM



AXIMUM W EIGHT C LIQUE

Shimizu, S., Yamaguchi, K., Saitoh, T., & Masuda, S. (2013). Optimal table method finding
maximum weight clique. Proc. 13th International Conference Applied Computer
Science, No. 12. WSEAS.
ThanhVu, H. N., & Thang, B. (2014). DIMACS benchmark. https://turing.cs.hbg.psu.
edu/txn131/clique.html.
Tomita, E., & Kameda, T. (2007). efficient branch-and-bound algorithm finding maximum
clique computational experiments. Journal Global Optimization, 37, 95111.
Tomita, E., & Seki, T. (2003). efficient branch-and-bound algorithm finding maximum
clique. Proc. Discrete Mathematics Theoretical Computer Science, Vol. 2731, pp.
278289.
Wu, Q., Hao, J. K., & Glover, F. (2012). Multi-neighborhood tabu search maximum weight
clique problem. Annals Operations Research, 196, 611634.
Wu, Q., & Hao, J.-K. (2015). Solving winner determination problem via weighted maximum
clique heuristic. Expert Systems Applications, 42(1), 355365.
Xu, K., Boussemart, F., Hemery, F., & Lecoutre, C. (2007). Random constraint satisfaction: Easy
generation hard (satisfiable) instances. Artificial Intelligence, 171, 514534.
Xu, K., & Li, W. (2000). Exact phase transitions random constraint satisfaction problems. Journal
Artificial Intelligence Research, 12, 93103.
Xu, K. (2004). BHOSLIB: Benchmarks hidden optimum solutions graph problems. http:
//www.nlsde.buaa.edu.cn/kexu/benchmarks/graph-benchmarks.htm.
Xu, K., & Li, W. (2006). Many hard examples exact phase transitions. Theoretical Computer
Science, 355(3), 291302.
Yamaguchi, K., & Masuda, S. (2008). new exact algorithm maximum weight clique
problem. Proc. ITC-CSCC-2008, pp. 317320.
Zhang, D., Javed, O., & Shah, M. (2014a). Video object co-segmentation regulated maximum
weight cliques. Proc. ECCV-2014, pp. 551566. Springer.
Zhang, W., Nie, L., Jiang, H., Chen, Z., & Liu, J. (2014b). Developer social networks software
engineering: construction, analysis, applications. Science China Information Sciences,
57(12), 123.
Zhian, H., Sabaei, M., Javan, N. T., & Tavallaie, O. (2013). Increasing coding opportunities using maximum-weight clique. Proc. Computer Science Electronic Engineering
Conference-2013, pp. 168173. IEEE.
Zhu, Z., Li, C. M., Manya, F., & Argelich, J. (2012). new encoding MinSAT MaxSAT.
Proc. CP-2012, pp. 455463. Springer.
Zuckerman, D. (2006). Linear degree extractors inapproximability max clique chromatic number. Proceedings 38th annual ACM symposium Theory computing,
pp. 681690. ACM.

833

fiJournal Artificial Intelligence Research 55 (2016) 743-798

Submitted 06/15; published 03/16

Knowledge Representation Probabilistic
Spatio-Temporal Knowledge Bases
Francesco Parisi

FPARISI @ DIMES . UNICAL .

Department Informatics, Modeling,
Electronics System Engineering
University Calabria, Rende, Italy

John Grant

GRANT @ CS . UMD . EDU

Department Computer Science UMIACS
University Maryland, College Park, USA

Abstract
represent knowledge integrity constraints formalization probabilistic spatiotemporal knowledge bases. start defining syntax semantics formalization called
PST knowledge bases. definition generalizes earlier version, called SPOT,
declarative framework representation processing probabilistic spatio-temporal data
probability represented interval exact value unknown. augment
previous definition adding type non-atomic formula expresses integrity constraints.
result highly expressive formalism knowledge representation dealing probabilistic spatio-temporal data. obtain complexity results checking consistency PST
knowledge bases answering queries PST knowledge bases, also specify tractable
cases. domains PST framework finite, extend results also arbitrarily
large finite domains.

1. Introduction
Recent years seen great deal interest tracking moving objects. fundamental
issue many applications providing location-based context-aware services, emergency call-out assistance, live traffic reports, food drink finder, location-based advertising, mobile tourist guidance, pervasive healthcare, analysis animal behavior (Ahson & Ilyas, 2010;
Petrova & Wang, 2011; Karimi, 2013). innovative services becoming widely diffused
MarketsandMarkets forecasts location-based services market grow $8.12
billion 2014 $39.87 billion 2019 (MarketsandMarkets, 2014).
important aspect systems providing location-based context-aware services
need manage spatial temporal data together. reason, researchers investigated detail representation processing spatio-temporal data, AI (Cohn &
Hazarika, 2001; Gabelaia, Kontchakov, Kurucz, Wolter, & Zakharyaschev, 2005; Yaman, Nau,
& Subrahmanian, 2004, 2005a; Knapp, Merz, Wirsing, & Zappe, 2006) databases (Agarwal,
Arge, & Erickson, 2003; Pelanis, Saltenis, & Jensen, 2006). However, many cases location
objects uncertain: cases handled using probabilities (Parker, Yaman, Nau, &
Subrahmanian, 2007b; Tao, Cheng, Xiao, Ngai, Kao, & Prabhakar, 2005). Sometimes probabilities known exactly. Indeed, position object given time estimated
means location estimation method proximity (where location object
derived vicinity one antennas), fingerprinting (where radio signal strength meac
2016
AI Access Foundation. rights reserved.

fiPARISI & G RANT

surements produced moving object matched radio map built
system working), dead reckoning (where position object derived last
known position, assuming direction motion either speed travelled distance
known) (Ahson & Ilyas, 2010; Karimi, 2013). However, since location estimation methods
limited accuracy precision, asserted object given position
given time probability whose value belongs interval. SPOT (Spatial PrObabilistic Temporal) framework introduced Parker, Subrahmanian, Grant (2007a) provide
declarative framework representation processing probabilistic spatio-temporal data
probabilities known exactly.
SPOT framework able represent atomic statements form object id is/was/will
inside region r time probability interval [`, u]. allows representation
information concerning moving objects several application domains. cell phone provider
interested knowing cell phones range towers given time
probability (Bayir, Demirbas, & Eagle, 2010). transportation company interested
predicting vehicles given road given time (and probability)
order avoid congestion (Karbassi & Barth, 2003). Finally, retailer interested knowing
positions shoppers moving shopping mall order offer suitable customized coupons
discounts (Kurkovsky & Harihar, 2006).
framework introduced Parker et al. (2007a) extended Parker, Infantes, Subrahmanian, Grant (2008) Grant, Parisi, Parker, Subrahmanian (2010) include
specific integrity constraint that, given moving object, points reachable
given starting point one time unit. captures scenario objects speed limits
points reachable objects depending distance points. However,
even extended SPOT framework yet general enough represent additional knowledge concerning movements objects. Examples knowledge may aware cannot
represent SPOT framework are, instance, fact
(i) cannot two distinct objects given region given time interval (as happens
airport passenger screening);
(ii) object cannot reach given region starting given location less given
amount time (as happens vehicles whose route options well speed
limited);
(iii) object go away given region stayed least given amount
time (as happens production lines assembling several parts requires given amount
time).
overcome limitation allow kind knowledge represented, define probabilistic spatio-temporal (PST) knowledge bases (KBs) consisting atomic statements,
representable SPOT framework spatio-temporal denial (abbreviated std) formulas, general class formulas account three cases above, many (including
reachability constraint Parker et al., 2008; Grant et al., 2010).
focus paper systematic study knowledge representation probabilistic
spatio-temporal data. start defining concept PST KB provide formal semantics, given terms worlds, interpretations, models (Section 2). define
concept consistent PST KB, characterize complexity checking consistency, showing
744

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

NP-complete general, even presence binary std-formulas (Section 3).
present sufficient condition checking consistency relies feasibility
set mixed-binary linear inequalities (Section 3.2), necessary condition using instead set
linear inequalities (Section 3.3). showing special case unary std-formulas
tractable (Section 3.4), deal restricted expressive class binary std-formulas
identify cases consistency checking problem tractable (Section 3.5). address
problem answering selection queries PST KBs optimistic cautious
semantics (Section 4). show checking consistency exploited answer kinds
queries PST KBs, characterize complexity query answering problem (Section 4.2).
that, derive several sets linear inequalities answering queries (Section 4.3). Finally,
extend framework case time, space, number objects increased
arbitrarily large finite domains, show PST KB either eventually consistent eventually inconsistent (Section 5). discuss related work (Section 6). Section 7 summarizes
paper. also suggest research projects Section 8.

2. PST Framework
section introduces syntax semantics PST KBs generalizing SPOT framework introduced Parker et al. (2007a) extended Parker et al. (2008) Grant et al. (2010).
Basically, define PST KB augmenting previous framework non-atomic formulas
(i.e., spatio-temporal denial formulas) represent integrity constraints. way make
statements whose meaning certain object trajectories cannot occur.
2.1 Syntax
assume existence three types constant symbols: object symbols, time value symbols,
spatial region symbols. constants ID = {id1 , . . . , idm }, = [0, 1, . . . , tmax]
(where tmax integer), set r Space = {p1 , . . . , pn }. r region Space.
apply unique name assumption; so, instance idi idj 6= j different objects;
similarly, pi pj 6= j different points. also use variables type: object
variables, time variables, spatial variables.
spatio-temporal atom (st-atom, short) expression form loc(X, Y, Z), where:
(i) X object variable constant id ID,
(ii) space 1 variable constant r Space,
(iii) Z time variable constant .
say st-atom loc(X, Y, Z) ground arguments X, Y, Z constants. instance, loc(id, r, t), id ID, r Space, ground st-atom. intuitive
meaning loc(id, r, t) object id is/was/will inside region r time t.
Definition 1 (PST atom). PST atom ground st-atom loc(id, r, t) annotated probability
interval [`, u] [0, 1] (with ` u rational numbers), denoted loc(id, r, t)[`, u].
1. write Space refer set points used PST KB. write space refer spatial aspect
probabilistic spatio-temporal knowledge.

745

fiPARISI & G RANT

7

loc(id1 , c, 9)[.9, 1]
loc(id1 , a, 1)[.4, .7]
loc(id1 , b, 1)[.4, .9]
loc(id1 , d, 15)[.6, 1]
loc(id1 , e, 18)[.7, 1]
loc(id2 , b, 2)[.5, .9]
loc(id2 , c, 12)[.9, 1]
loc(id2 , d, 18)[.6, .9]
loc(id2 , d, 20)[.2, .9]

6
5

e
c

4



3
2

b

1



0
0

1

2

3

4

5

6

7

(a)

(b)

Figure 1: (a) map airport area (names regions bottom-right corner);
PST atoms.

(b)

Intuitively, PST atom loc(id, r, t)[`, u] says object id is/was/will inside region r
time probability interval [`, u]. Hence, PST atoms represent information
past present, also information future, methods predicting
destination moving objects (Mittu & Ross, 2003; Hammel, Rogers, & Yetso, 2003; Southey, Loh,
& Wilkinson, 2007), querying predictive databases (Akdere, Cetintemel, Riondato, Upfal,
& Zdonik, 2011; Parisi, Sliva, & Subrahmanian, 2013).
original SPOT definition, ease implementation, Space grid within
rectangular regions considered; however, general framework, Space arbitrary
region nonempty subset Space. Still, convenience use rectangular
regions running example.
Example 1. Consider airport security system collects data biometric sensors
well Bluetooth WiFi enabled devices. Biometric data faces recognized sensors (Li & Jain, 2011) matched given profiles (such checked-in passports,
wanted criminals). Similarly, device identifiers (e.g., MAC addresses) recognized areas
covered network antennas matched profiles collected airport hotspots (such
logins, possibly associated passport numbers). simplified plan airport area reported
Figure 1(a), regions a, b, c, d, e covered sensors and/or antennas highlighted.
entered area, passengers typically move path delimited queue dividers (represented dotted lines figure, overlapping regions b), reach room
upper-half right side security checks performed (region c included room).
Next, passengers spend time hall room (overlapping region d), finally go
towards exit (near region e).
Suppose security system uses SPOT framework represent information
every PST atom consists profile id resulting matching phase, region
sensor/antenna recognizing profile operating, time point profile recognized,
lower upper probability bounds recognizing process. instance, PST atom
loc(id1 , c, 9)[.9, 1] says profile id id1 region c time 9 probability
746

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

interval [.9, 1] (the high-accuracy sensors used security check points located region c entail
narrow probability interval upper bound equal 1). Atom loc(id1 , a, 1)[.4, .7] says
id1 recognized region earlier time 1 probability [.4, .7]. Assume
information represented set atoms Figure 1(b), includes two atoms above.
2
PST atoms used represent output process aimed tracking objects basis
sensor measurements. Generally, sensors characterized likelihood function providing
conditional probability obtaining measurement given value parameter ,
distance tracked object sensor. instance, likelihood function l()
represent probability detecting object distance meters sensor
position. However, likelihood function generally probability distribution viewed
function . may l(1 ) = .9, l(2 ) = .4, l(3 ) = .1, l( 3 ) = 0, distances
1 < 2 < 3 . information encoded using PST atoms loc(id, ri , t)[l(i ), l(i )]
region ri determined distance (more general probability intervals used
likelihood values know exactly).2 However, several object tracking techniques combine
information likelihood function prior position distribution obtain probability
distribution Space. PST atoms represent kind information defining PST atom
single probability point Space. 3 refer reader Related Work section
detailed discussion object tracking techniques relationship framework.
Although PST atoms express much useful information, cannot express additional knowledge integrity constraints provide. paper add integrity constraints original
PST framework form PST KBs. integrity constraints form spatio-temporal denial formulas (std formulas short). soon see formulas expressive enough
capture large set conditions. Basically, std formula universally quantified negation
conjunctions st-atoms built-in predicates. note std formulas related subclass
first-order formulas introduced Doder, Grant, Ognjanovic (2013), except
(the std formulas) allow built-in predicates well. case, focus Doder et al.
axiomatization various probabilistic spatio-temporal logics.
Definition 2 (Std-formula). std-formula expression form
k


^
loc(Xi , Yi , Zi ) (X) (Y) (Z)
X, Y, Z
i=1

where:
X set object variables, set space variables, Z set time variables;
loc(Xi , Yi , Zi ), [1..k], st-atoms, Xi , Yi , Zi may variables constants appropriate type, that, Xi (resp., Yi , Zi ) variable, occurs
X (resp, Y, Z). Moreover, variable X, Y, Z occurs least one st-atom
loc(Xi , Yi , Zi ), [1..k];
2. note PST KBs resulting PST atoms encoding information provided likelihood function may
inconsistent fact likelihood function need probability distribution. turn
clearer introducing formal semantics PST KBs Section 2.2.
3. PST KBs resulting PST atoms encoding output tracking turn consistent.

747

fiPARISI & G RANT

(X) conjunction built-in predicates form Xi Xj , Xi Xj either
variables occurring X ids ID, operator {=, 6=};
(Y) conjunction built-in predicates form Yi Yj , Yi Yj either
variables occurring regions (i.e., non-empty subsets Space), comparison
operator {=, 6=, ov, nov} (where ov stands overlaps nov stands
overlap);
(Z) conjunction built-in predicates form Zi Zj Zi Zj either
time value variable Z may followed +n n positive integer
operator {=, 6=, <, }.
Example 2. running example, region c security checks one individual time
performed. constraint cannot two distinct objects region c time 1
20 expressed following std-formula:
f1 = X1 , X2 , Z1 [loc(X1 , c, Z1 ) loc(X2 , c, Z1 ) X1 6= X2 Z1 1 20 Z1 ].
Due distance several obstacles entrance exit, also
constraint object reach region e starting region less 10 time units,
expressed as:
f2 = X1 , Z1 , Z2 [loc(X1 , a, Z1 ) loc(X1 , e, Z2 ) Z1 < Z2 Z2 < Z1 + 10].
Moreover, security check individual takes least 2 time units, know
object id go away region c stayed least 2 time units,
expressed as:
f3 = Y1 , Y2 , Z1 , Z2 , Z3 [loc(id, Y1 , Z1 ) loc(id, c, Z2 ) loc(id, Y2 , Z3 ) Y1 nov c Y2 nov c
Z2 = Z1 + 1 Z2 < Z3 Z2 + 2 Z3 ].
2
work later useful distinguish std-formulas based number (k) statoms them. particular, unary std-formulas k = 1 binary std-formulas k = 2.
Example 2 f1 f2 binary std-formulas f3 ternary std-formula.
initial SPOT framework (Parker et al., 2007a) PST atoms considered. Moreover, assumed points Space reachable points objects.
overcome limitation, Grant et al. (2010) extended SPOT framework introducing reachability definitions. reachability atom written reachableid (p, q) id ID object
id, p, q Space. Intuitively, reachability atom says possible object id
reach location q location p one unit time. Hence, reachable one time unit
depends locations p q, also object id. show, reachability
expressed formalism integrity constraint. However, order formulate reachability
framework denial formulas, need deal reachable, rather
reachable.
Example 3. Let r region consisting points q reachable p one time
unit. corresponding std-formula is:
X1 , Z1 , Z2 [loc(X1 , {p}, Z1 ) loc(X1 , r, Z2 ) Z2 = Z1 + 1].
2
integrity constraint used points reachable p one time unit.
also express points reached p number time units, 1,
changing Z1 + 1 Z1 + i.
748

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

Example 4. running example, following std-formula states points region
r = {(x, y)|0 x 5 = 3} (i.e., close upper-side wall dividing hall
room one queue dividers) reachable less 3 time units
point r0 = {(x, y)|0 x 5 = 2} (i.e., points close side wall):
f4 = X1 , Z1 , Z2 [loc(X1 , r0 , Z1 ) loc(X1 , r, Z2 ) Z1 < Z2 Z2 < Z1 + 3].
2
ready formally define PST KBs.
Definition 3 (PST KB). Given sets ID, , Space, PST KB K pair hA, Fi,
finite set PST atoms F finite set std-formulas using object symbols ID, time
values , spatial regions consisting sets points Space.
Example 5. running example, ID = {id1 , id2 }, = [0, 20], Space set points (x, y)
0 x 7 0 7, PST KB Kex pair hAex , Fex i, Aex
set consisting PST atoms Figure 1(b), Fex set {f1 , f2 , f3 , f4 } std-formulas
defined Examples 2 4.
2
2.2 Semantics
semantics PST KB defined concept worlds. introducing
concept, define ground std-formulas.
Given std-formula f form Definition 2, denote f set substitutions variables X, Y, Z constants ID, S, , respectively,
set subsets Space contain single point. 4 Moreover, given substitution
f , denote (f ) ground std-formula resulting applying f : (f ) =


Vk

i=1 loc((Xi ), (Yi ), (Zi )) ((X)) ((Y)) ((Z)) . ground conjunction
built-in predicates ((X)) ((Y)) ((Z)) evaluates either true false. true
omit it. (f ) either negation conjunction ground st-atoms truth value true
(when conjunction built-in predicates evaluates false).
Example 6. Consider formula f1 = X1 , X2 , Z1 [loc(X1 , c, Z1 )loc(X2 , c, Z1 )X1 6= X2
Z1 1 20 Z1 ] introduced Example 2, substitution = {X1 /id1 , X2 /id2 , Z1 /6},
id1 , id2 ID 6 tmax. Thus, (f1 ) = [loc(id1 , c, 6) loc(id2 , c, 6)],
conjunction ground built-in predicates id1 6= id2 6 1 6 20, evaluating true,
reported (f1 ).
2
Definition 4 (World). world w function, w : ID Space.
Basically, world w specifies trajectory id ID. is, id ID, w says
Space object id was/is/will time . particular, means object
one location time.5 However, location may contain multiple objects. easy
see world w represented set {loc(id, {p}, t)| w(id, t) = p} ground st-atoms.
4. use singleton subsets Space order reduce number possible instantiations variables
exponential linear size Space, without serious effect meanings std-formulas.
5. examples may useful allow objects enter leave space consideration.
accomplished, instance, one external points outside space objects may located.
simplify matters assume Space contains points.

749

fiPARISI & G RANT

Example 7. World w1 describing trajectories id1 id2 time units [0, 20]
w1 (id1 , t) = (4, 1) [0, 5], w1 (id1 , t) = (7, 2) [6, 7], w1 (id1 , t) = (7, 4)
[8, 10], w1 (id1 , t) = (4, 4) [11, 16], w1 (id1 , t) = (1, 6) [17, 20], w1 (id2 , t) =
(4, 1) [0, 11], w1 (id2 , t) = (7, 5) [12, 15], w1 (id2 , t) = (7, 7) [16, 16],
w1 (id2 , t) = (4, 5) [17, 20].
2
Definition 5 (Satisfaction). Given world w ground st-atom = loc(id, r, t), say w
satisfies (denoted
r. Moreover, say w satisfies conjunction
Vk w |= a) iff w(id, t) V
ground st-atoms i=1 ai (denoted w |= ki=1 ai ) iff w |= ai [1..k]. Finally, world w satisfies
std-formula f (denoted w |= f ) iff substitution f , w |= (f ).
Note that, negation front f , w |= (f ) iff w satisfy ground st-atom
(f ) conjunction ground built-in predicates (f ) evaluates false.
Example 8. World w1 Example 7 satisfies st-atom loc(id1 , b, 0), w1 (id1 , 0) = (4, 1)
belongs region b (see Figure 1(a)). Moreover, w1 |= [loc(id1 , b, 0) loc(id1 , e, 15)] w1 6|=
loc(id1 , e, 15), since w1 (id1 , 15) = (4, 4) 6 e.
2
following, denote W(K) set worlds PST KB K. Moreover,
order simplify formulas, assume w ranges W(K).
interpretation PST KB K probability distribution function (PDF) W(K),
is, function assigning probability value world W(K). I(w) probability
w describes actual trajectories objects.6 interpretations models K
case write instead I.
Definition 6 (Model). model PST KB K = hA, Fi interpretation K that:
P
(w) [`, u];
loc(id, r, t)[`, u] A,
w | w|=loc(id,r,t)

f F,

P

(w) = 0.

w | w6|=f

first condition definition means that, atom = loc(id, r, t)[`, u] A,
sum probabilities assigned worlds satisfying st-atom loc(id, r, t)
belong probability interval [`, u] specified a. second condition means every world
satisfying formula f F must assigned probability equal 0.
Example 9. Let w1 world introduced Example 7. Let w2 w1 except w2 (id1 , 1) =
(3, 2), let w3 w2 except w3 (id2 , 2) = (2, 2), w3 (id2 , t) = (0, 3) [18..20]. Let
(w1 ) = .7 (w2 ) = .2 (w3 ) = .1, (w) = 0 worlds
W(Kex ). checked satisfies conditions DefinitionP6 PST KB Kex
running example. instance, atom loc(id1 , a, 1)[.4, .7] Aex , w|w|=loc(id1 ,a,1) (w) =
(w1 ) = .7 [.4, .7] (note that, time 1, w2 (id1 , 1) = w3 (id1 , 1) = (3, 2) region a).
Moreover, easy check w1 , w2 , w3 satisfy every std-formula Fex . Thus, model
Kex .
2
say PST KB K consistent iff model it. set models K
denoted M(K).
6. PDF, I(w) non-negative sums 1 worlds.

750

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

Definition 7 (Consistency). PST KB K consistent iff M(K) 6= .
Example 10. PST KB Kex running example consistent, exists model
Example 9 it.
2

3. Checking Consistency PST KBs
section, address fundamental problem checking consistency PST KBs.
Given PST KB K = hA, Fi, consistency checking problem deciding whether M(K) 6= ,
is, whether model K.
Section 3.1 show consistency checking problem NP-complete. goal
rest section find efficient ways determine consistency. Section 3.2 find
sufficient condition using set mixed-binary linear inequalities. Then, Section 3.3 find
necessary condition using different set linear inequalities. deal special case
std-formulas unary Section 3.4. Finally investigate detail case
std-formulas either unary binary Section 3.5.
complexity analysis take size PST KB K = hA, Fi, whose PST atoms
std-formulas built constants ID, , Space, number PST atoms
std-formulas K plus number items ID, , Space, is, |K| = |A| + |F| + |ID| +
|T | + |Space|.
3.1 Checking Consistency NP-Complete
considering case general PST KBs, first note consistency checking problem addressed initial SPOT framework Parker et al. (2007a) PST atoms
considered. special case PST KB concept F = . shown
consistency PST KB K = hA, (using notation) checked polynomial
time w.r.t. size K solving set linear inequalities whose variables vid,t,p represent
probability object id point p time t.
reason presenting result Parker et al. twofold: first, compare complexity
consistency checking problem general PST KBs initial SPOT framework;
second, use prove tractability results PST KBs.
Fact 1 (Parker et al.). Let K = hA, PST KB (where set std-formulas empty).
K consistent iff feasible solution CC(K), CC(K) consists following
(in)equalities:
P
(1) loc(id, r, t)[`, u] A: `
vid,t,p u;
pr

(2) id ID, :

P

vid,t,p = 1;

pSpace

(3) id ID, T, p Space: vid,t,p 0.
Basically, inequalities (1) ensure solution CC(K) places object r probability ` u, required atom (id, r, t, [`, u]). Inequalities (2) (3) ensure
id t, vid,t,p variables jointly represent probability distribution. Fact 1 correct
every model K corresponds solution CC(K) sum
751

fiPARISI & G RANT

probabilities assigned worlds K satisfying st-atom loc(id, {p}, t) equal
value assigned variable vid,t,p .
state first result: consistency checking problem NP-complete.
Theorem 1. Given PST KB K = hA, Fi, deciding whether K consistent NP-complete.
Proof. (Membership). show checking consistency K reduced deciding
instance K (an extension to) Probabilistic Satisfiability (PSAT) problem (Hailperin, 1984;
Nilsson, 1986), NP (Georgakopoulos, Kavvadias, & Papadimitriou, 1988). Given set
clauses C1 , . . . , Cm , consisting disjunction one literals constructed
propositional variables x1 , . . . , xn , probability values Pr(C1 ), . . . , Pr(Cm )
clause, PSAT problem deciding whether probability distribution set
2n truth assignments propositional variables x1 , . . . , xn clause Ci ,
sum probabilities assigned truth assignments satisfying Ci equal Pr(Ci ),
[1..m]. PSAT generalization SAT, obtained PSAT assigning probability
equal one clause. Georgakopoulos et al. formulated PSAT terms feasibility
system + 1 linear equations using 2n variables corresponding probabilities assigned
truth assignments. show existence polynomial-size witness, following result
linear programming theory exploited Georgakopoulos et al.: system linear
equalities feasible solution, admits least one feasible solution nonzero variables (Papadimitriou & Steiglitz, 1982). follows consider extension
PSAT clause Ci associated probability interval [Pr` (Ci ), Pru (Ci )], instead
single value. membership NP extension straightforwardly follows
membership proof provided Georgakopoulos et al. PSAT, using probability intervals
still formulated linear system introduced Georgakopoulos et al. reducing doublesided inequalities equalities single bounded slack variables (Jaumard, Hansen, & de Aragao,
1991).
Given PST KB K = hA, Fi, define instance K PSAT clause associated probability interval. Let U set propositional variables xid,p,t
id ID, p Space, (i.e., st-atom loc(id, {p}, t) corresponds propositional
variable xid,p,t U ). conjunction K clauses associated probability intervals defined
follows:
W
PST atom = loc(id, r, t)[`, u] A, K consists clause Ca =
xid,p,t .
pr

probability interval [Pr` (Ca ), Pru (Ca )] equal [`, u].
f F f (f ) = [

k
V

loc((Xi ), (Yi ), (Zi ))], K consists

i=1

clause C(f ) =

k
W
i=1

x(Xi ),(Yi ),(Zi ) , whose probability interval [1, 1].

id ID , K consists clause Cid,t =

W

xid,p,t , whose

pSpace

probability interval [1, 1].
id ID, , pi , pj Space, pi 6= pj , K consists clause
Cid,t,pi ,pj = xid,pi ,t xid,pj ,t whose probability interval [1, 1].
752

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

easy see world w W(K), truth assignment w variables
U w (xid,p,t ) true iff w(id, t) = p. However, truth assignments
correspond world W(K) (for instance, (xid,pi ,t ) (xid,pj ,t ),
pi 6= pj , true). show K consistent iff K satisfiable.
() Given model K, show PDF set truth assignments
clause C K , sum probabilities assigned truth assignments
satisfying C belongs [Pr` (C), Pru (C)]. Let truth assignment w corresponding world w W(K), (w ) = (w) ( ) = 0 truth assignments
corresponding world. easy check conditions Definition 6 entail clauses
form Ca C(f ) satisfied , clauses form Cid,t Cid,t,pi ,pj satisfied
well since w1 , . . . w|W(K)| may assigned probability different 0
every world w W(K) definition assigns exactly one point Space id, pair.
() Let PDF set truth assignments clause C K , sum
probabilities assigned truth assignments satisfying C belongs [Pr` (C), Pru (C)].
model K defined (w) = (w ) truth assignment corresponding world w W(K). Since clauses form Cid,t (resp., Cid,t,pi ,pj ) satisfied
, truth assignments (xid,p,t )=false p Space (resp., (xid,pi ,t )=true
(xid,pj ,t )=true, pi , pj Space, pi 6= pj ) assigned probability 0. Hence,
truth assignments correspond world, fact clauses form Ca C(f )
satisfied entails conditions Definition 6 hold.
(Hardness). show reduction problem NP-hard Hamiltonian path problem (Papadimitriou, 1994), is, problem checking whether path directed graph G
visits vertex G exactly once.
Given directed graph G = hV, Ei, V = {v0 , . . . , vk } set vertices, E
set pairs (vi , vj ) vi , vj V , construct instance problem follows. Let
ID = {id}, Space = V , = [0, . . . , k]. K pair hA, Fi consists PST
atom loc(id, {v0 }, 0)[1, 1] F consists std-formulas f1i (with [0..k]) f2 that:
f1i = Z1 , Z2 [loc(id, {vi }, Z1 ) loc(id, Space\V 0 , Z2 ) Z2 = Z1 + 1] V 0
set vertices vj s.t. (vi , vj ) E. formula says points id reach starting
vi one time step V 0 . (f1i exist V 0 = Space.)
f2 = Y1 , Z1 , Z2 [loc(id, Y1 , Z1 ) loc(id, Y1 , Z2 ) Z1 6= Z2 ], saying id
location distinct time values.
show K consistent iff Hamiltonian path G.
() one id A, every world w W(K) w places id vertex
V time value . K consistent, model M(K)
assigns probability greater zero worlds w f F, w |= f . particular,
let w one world. fact w |= f1i entails [0, k 1], w(id, t) = vi
w(id, + 1) = vj iff (vi , vj ) E. Moreover, fact w |= f2 entails t, t0 [0, k], 6= t0 ,
w(id, t) 6= w(id, t0 ), meaning id never placed w vertex different time
units. Since loc(id, v0 , 0)[1, 1] A, every world assigned probability greater zero
w(id, 0) = v0 . follows every world w W(K) assigned
M(K) probability greater zero encodes Hamiltonian path G whose first vertex
v0 . fact, w W(K) (w) > 0 following properties hold: (i) w(id, 0) = v0 ,
753

fiPARISI & G RANT

(ii) [0, k 1], w(id, t) = vi , w(id, + 1) = vj iff (vi , vj ) E. (iii) t, t0 [0, k], 6= t0 ,
w(id, t) 6= w(id, t0 ). Conditions (i) (ii) entail = w(id, 0), w(id, 1), . . . , w(id, k)
path G starting vertex v0 , condition (iii) entails vertex v V occurs exactly
.
() Let Hamiltonian path G. denote [i] (with [0..k]) i-th vertex .
W.l.o.g. assume first vertex v0 , is, [0] = v0 . show K
consistent finding model it. Let function W worlds w W,
(w) = 0, except world w that: w (id, 0) = [0] = v0 , [1, k],
w (id, t) = [t]. easy see w |= F. fact, [0..k], f1i satisfied w , since
fact path G entails [0, k 1], w (id, t) = vi w (id, t+1) = vj
edge (vi , vj ) edge G. Moreover, f2 satisfied w , since fact Hamiltonian
path entails w places id different locations (i.e., vertices G) different times. Since

w
aP
probability different 0. Let (w ) = 1. Therefore,
P |= F, assigned

w|w|=loc(id,v0 ,0) (w) = (w )+ w|w6=w w|=loc(id,v0 ,0) (w) = 1, condition required
atom loc(id, v0 , 0)[1, 1] holds too. Thus, model K.
note NP-hardness holds already binary std-formulas. Section 3.5 find
conditions make consistency checking problem tractable binary std-formulas. reduction shown membership proof, consistency checking problem PSAT, would
allow us define additional tractable cases PSAT instances resulting reduction
tractable. However, discussed Section 6, tractable cases identified PSAT (Georgakopoulos et al., 1988; Andersen & Pretolani, 2001) carry
framework.
3.2 Sufficient Condition Checking Consistency
present set mixed-binary linear inequalities whose feasibility entails consistency PST
KB K = hA, Fi. explained Section 3.1, Parker et al. (2007a) showed consistency
PST KB K = hA, (using notation) checked polynomial time w.r.t. size
K solving set linear inequalities whose variables vid,t,p represent probability object
id point p time t. Here, start set linear inequalities augment
inequalities ensuring so-obtained set linear inequalities feasible solution
every ground std-formula derived F satisfied. achieve this, need introduce
binary variables , thus obtaining set mixed-binary linear inequalities.
Definition 8 (MBL(K)). Let K = hA, Fi. MBL(K) consists following (in)equalities:
P
(1) loc(id, r, t)[`, u] A: `
vid,t,p u;
pr

(2) id ID, :

P

vid,t,p = 1;

pSpace

(3) id ID, T, p Space: vid,t,p 0;
(4) f F f (f ) = [

k
V
i=1

number st-atoms f , (in)equalities:
754

loc((Xi ), (Yi ), (Zi ))], k

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

P
(a) [1..k] :
p(Yi ) v(Xi ),(Zi ),p ;
Pk
(b)
i=1 = k 1;
(c) [1..k] : {0, 1}.
Basically, inequalities (1) ensure solution MBL(K) places object r probability ` u, required atom (id, r, t, [`, u]). Inequalities (2) (3) ensure
id t, vid,t,p variables jointly represent probability distribution. Moreover,
ground st-atom loc((Xi ), (Yi ), (Zi )) ground std-formula (f ), inequalities (4)(a)
(4)(c) entail probability v(Xi ),(Zi ),p object (Xi ) point p region (Yi )
time (Zi ) either constrained 0 free take value greater 1. Intuitively
enough, v(Xi ),(Zi ),p enforced zero (i.e., = 0), object (Xi ) region
(Yi ) time (Zi ). hand, v(Xi ),(Zi ),p left free take value less
equal one (i.e., = 1), (Xi ) may may region (Yi ) time (Zi ). Finally,
equality (4)(b) entails least one k ground st-atoms loc((Xi ), (Yi ), (Zi ))
(f ) (Xi ) placed point (Yi ) time (Zi ).
Example 11. Consider ground std-formula (f1 ) = [loc(id1 , c, 6) loc(id2 , c, 6)] Example 6. Then, inequalities MBL(K) corresponding (f1 ) are:
P
P
(4b) 1 + 2 = 1;
(4c) 1 , 2 {0, 1}.
2
(4a) pc vid1 ,6,p 1 ;
pc vid2 ,6,p 2 ;
following theorem states MBL(K) used check K consistent.
Theorem 2. feasible solution MBL(K) K consistent.
Proof. Let solution MBL(K), (vid,t,p ) value assigned variable vid,t,p
.
Q define function W(K) that, world w W(K), (w) =
idID,tT,w(id,t)=p (vid,t,p ), (w) product values assigned solution
variables vid,t,p w(id, t) = p. shown that, (in)equalities (2) (3)
definition
MBL(K) entail PDF W(K). Moreover,
since (vid,t,p )
P
P

(w),


atom
loc(id,
r,
t)[`,
u]

A,
equal

w|w|=loc(id,r,t) (w) =
P P w|w|=loc(id,t,p)
P
pr
w|w|=loc(id,t,p) (w) =
pr (vid,t,p ) [`, u]. Given f F f (f )
Vk
logically equivalent negation conjunction st-atoms
i=1 loc((Xi ), (Yi ),
P
(Zi )), inequalities (4)(a-c) entail [1..k] p(Yi ) (v(Xi ),(Zi ),p ) =
0. Thus p (Yi ), (v(Xi ),(Zi ),p ) = 0. Hence, world w W(K) w((Xi ),
(Zi )) = p, (w) = 0 due presence factor
P (v(Xi ),(Zi ),p ) = 0 product defining (w). Therefore, std-formula f F, w | w6|=f (w) = 0; hence model
K K consistent.
consequence Theorem 2 well-known techniques solving linear optimization
problems adopted address consistency checking problem, thus taking advantage
results 50 years research integer linear programming (Junger et al., 2010).
following example shows converse Theorem 2 hold (K may consistent even feasible solution MBL(K)).
Example 12. Let ID = {id}, = [0, 1], Space = {p0 , p1 }, K = hA, Fi where:
= {loc(id, {p0 }, 0)[0.5, 0.5], loc(id, {p1 }, 1)[0.5, 0.5]}
F = {[loc(id, {p0 }, 0) loc(id, {p1 }, 1)}.
755

fiPARISI & G RANT

Thus, W = {w1 , w2 , w3 , w4 } where: w1 (id, 0) = p0 , w1 (id, 1) = p0 , w2 (id, 0) = p0 , w2 (id, 1) =
p1 , w3 (id, 0) = p1 , w3 (id, 1) = p0 , w4 (id, 0) = p1 , w4 (id, 1) = p1 .
easy check (w1 ) = 0.5, (w2 ) = 0, (w3 ) = 0, (w4 ) = 0.5
model K. MBL(K) includes following inequalities:
0.5 vid,0,p0 0.5; 0.5 vid,1,p1 0.5; vid,0,p0 + vid,0,p1 = 1; vid,1,p0 + vid,1,p1 = 1;
vid,0,p0 1 ; vid,1,p1 2 ; 1 + 2 = 1; 1 , 2 {0, 1};
vid,0,p0 0, vid,0,p1 0, vid,1,p0 0, vid,1,p1 0.
first two inequalities force vid,0,p0 vid,1,p1 0.5. second line 1
2 must equal 1. contradicts 1 + 2 = 1. Hence MBL(K) feasible solution.
2
3.3 Necessary Condition Checking Consistency
following, given PST KB K, introduce set NC(K) linear inequalities K
consistent feasible solution NC(K). is, existence feasible solution
NC(K) turns necessary condition consistency K.
MBL(K) (see Definition 8), NC(K) uses rational variables vid,t,p representing probability
object id point p time t. kinds variables used definition
NC(K), case obtain pure system linear inequalities.
Definition 9 (NC(K)). Let K = hA, Fi PST KB. NC(K) consists following (in)equalities:
P
vid,t,p u;
(1) loc(id, r, t)[`, u] A: `
pr

(2) id ID, :

P

vid,t,p = 1;

pSpace

(3) id ID, T, p Space: vid,t,p 0;
V
(4) f F (with k conjuncts) f s.t. (f ) = [ ki=1 loc((Xi ), (Yi ), (Zi ))],
inequalities:
Pk
p1 (Y1 ), p2 (Y2 ), . . . , pk (Yk ),
i=1 v(Xi ),(Zi ),pi k 1.
Herein, (in)equalities (1)-(3) MBL(K) Definition 8,
meaning. addition,
V NC(K) contains inequalities (4) impose that, ground stdformula form ki=1 loc(idi , ri , ti ) k-tuple points hp1 , p2 , . . . , pk belonging
respectively regions hr1 , r2 , . . . , rk i, sum probabilities vidi ,pi ,ti object idi
point pi time ti , [1..k], greater k 1. stated following theorem,
set consisting inequalities (4) along inequalities (1)-(3) turns feasible solution
corresponding PST KB inconsistent.
Theorem 3. feasible solution NC(K), K consistent.
Proof. Suppose NC(K) feasible solution. due fact feasible
solution CC(hA, (introduced Fact 1), PST KB hA, consistent, thus K =
hA, Fi consistent well (since set M(hA, Fi) models K = hA, Fi subset
set M(hA, i) models K0 = hA, i).
Otherwise, feasible solution CC(hA, i) thus NC(K) feasible solution
due fact least one inequalities item (4) Definition 9 satisfied every
756

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

solution
V CC(hA, i). is, solution CC(hA, i), ground std- formula
(f ) = ki=1 loc((Xi ), (Yi ), (Zi )) exist p1 (Y1 ), p2 (Y2 ), . . . , pk
P
(Yk ), ki=1 (v(Xi ),(Zi ),pi ) > k 1.
Since every model 0 K0 = hA, corresponds solution CC(hA, i)
sum probabilities assigned
0 worlds satisfying st-atom loc(id, {p}, t) equal
Pk
(vid,t,p ), fact i=1 (v(Xi ),(Zi ),pi ) > k 1 holds entails that, model 0
K0 , sum probabilities assigned 0 worlds satisfying least one st-atoms
(f ) greater k 1, is,
0 M(K0 ),

k
X

X

0 (w) > k 1

(1)

i=1 w|=loc((Xi ),(Yi ),(Zi ))

recall use following well-known result minimum probability conjunction probabilistic events among correlation known. Given n probabilistic events
e1 , . . . , en whose (marginal)
probabilities Pr(e1 ), . . . , Pr(en ) respectively, Pr(e1 en )
P
max (0, 1 n + ni=1 Pr(ei )). one Frechet inequalities (the one provides
upper bound maximum probability) implicitly reported already Booles work (1854).
setting, viewing st-atoms probabilistic events, Frechet inequality entails
model 0 K0 , probability set st-atoms together satisfy world greater
equal maximum zero 1 |S| plus sum probabilities
st-atom according 0 . is,


X
W(K0 )

w
S, w |=


X

0 (w) max 0, 1 |S| +


X
W(K0 ),

w
w |=



0 (w)


(2)

Equation (1) entails model 0 K0 , term right-hand side Equation (2) evaluates value greater zero set st-atoms (f ) considered.
Therefore, model 0 K0 , sum probabilities worlds satisfying
st-atoms (f ) greater zero. Since set M(hA, Fi) models K = hA, Fi
subset set M(hA, i) models K0 = hA, i, property also holds model
K0 . Thus, 0 model K. Hence K inconsistent.
example usage Theorem 3 given below, checking NC(K)
feasible solution conclude PST KB K consistent.
Example 13. Let ID = {id}, = [0, 1, 2], Space = {p0 , p1 , p2 }. Let K = hA, Fi
consists PST atoms loc(id, {p0 }, 0)[0.4, 1], loc(id, {p1 }, 0)[0.5, 1], loc(id, {p0 }, 1)
[0.8, 1], loc(id, {p0 }, 2) [0.8, 1], meaning that, id p0 p1 time 0 probability greater equal 0.4 0.5, respectively, p0 times 1 2
probability greater equal 0.8.
F consists std-formula: [loc(id, {p0 , p1 }, 0) loc(id, {p0 }, 1) loc(id, {p0 }, 2)],
saying id cannot p0 times 1 2 region consisting
points p0 p1 time point 0.
757

fiPARISI & G RANT

easy see NC(K) contains, among others, following inequalities:
0.4 vid,0,p0 1
0.5 vid,0,p1 1
0.8 vid,1,p0 1
0.8 vid,2,p0 1
vid,0,p0 + vid,1,p0 + vid,2,p0 2
vid,0,p1 + vid,1,p0 + vid,2,p0 2
last two inequalities derive item (4) Definition 9. Clearly, inequalities
cannot satisfied given 0.5 vid,0,p1 , 0.8 vid,1,p0 , 0.8 vid,2,p0 . Thus, since NC(K)
feasible solution, conclude K inconsistent.
2
However, cannot say anything consistency K feasible solution
NC(K). following examples show case NC(K) feasible solution K
consistent, case NC(K) feasible solution K inconsistent.
Example 14. Consider PST KB Example 13 modified probability
id p0 times 1 2 greater equal 0.5, instead 0.8. easy see
feasible solution NC(K) case. Theorem 3 cannot used decide whether K
consistent not. matter fact, K consistent shown follows. Let w1 , w2 w3
worlds K
w1 (id, 0) = p1 , w1 (id, 1) = p1 , w1 (id, 2) = p0 ,
w2 (id, 0) = p0 , w2 (id, 1) = p0 , w2 (id, 2) = p1 ,
w3 (id, 0) = p2 , w3 (id, 1) = p0 , w3 (id, 2) = p2 ,
let PDF W(K) (w1 ) = 0.5, (w2 ) = 0.4, (w3 ) = 0.1,
(w) = 0 world w W(K). straightforward check model
K.
2
Example 15. Again, let ID = {id}, = [0, 1, 2], Space = {p0 , p1 , p2 }. Let K = hA, Fi
= {loc(id, {p0 }, 0)[0.5, 1], loc(id, {p0 }, 1)[0.5, 1], loc(id, {p0 }, 2)[0.5, 1]}, i.e., id
p0 time probability greater 0.5, F consists std-formulas:
[loc(id, {p0 }, 0) loc(id, {p0 }, 1)],
[loc(id, {p0 }, 1) loc(id, {p0 }, 2)],
[loc(id, {p0 }, 0) loc(id, {p0 }, 2)],
saying id cannot p0 time 1 2 already previous time value.
solution NC(K) obtained assigning variables vid,t,p0 vid,t,p1 value 0.5
(where [0, 2]), variables value 0. NC(K) feasible solution, Theorem 3
says nothing fact K consistent not. However, checked K
consistent. Let Pt set worlds W(K) placing id p0 time t, .
std-formulas F entail every world belonging two three sets must assigned
probability equal 0 model K. is, F entails P0 , P1 , P2 pairwise disjoint
sets consider worlds assigned probability greater zero model.
observe PST atoms require sum probabilities worlds
three sets least equal 0.5. Therefore, overall sum probabilities assigned
worlds three sets would greater equal 1.5, entails
model K.
2
Theorem 3 shows consistency K implies existence feasible solution
NC(K) previous examples show relationship two
758

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

concepts. However, make stronger statement special case. PST KBs F
consists single ground std-formula (of arity) constructed st-atoms referring singlepoint regions, NC(K) feasible solution iff KB consistent.

Theorem 4. Let K = hA, Fi F = { loc(id1 , {p1 }, t1 ) loc(idn , {pn }, tn )]}. Then,
K consistent iff feasible solution NC(K).
Proof. Using Theorem 3 need prove feasible solution NC(K) K
consistent.
Let f std-formula F. first observe f contains pair st-atoms referring
id time value different points Space, K consistent. Indeed, case,
f satisfied every world, thus impose restriction interpretations
K (see Definition 4). Given this, following w.l.o.g. assume every distinct pair
st-atoms loc(idi , {pi }, ti ), loc(idj , {pj }, tj ) f , idi = idj ti 6= tj .
Let solution NC(K), K0 = hA, i. Then, solution CC(K0 ) (from Fact
1) corresponds model 0 K0 (i) sum probabilities assigned 0
worlds satisfying st-atom
loc(id, {p}, t) (i.e., marginal probability loc(id, {p}, t))
P
equal (vid,t,p ), (ii) ki=1 (vidi ,ti ,pi ) k 1. Viewing st-atom loc(idi , {pi }, ti ) f
probabilistic event whose (marginal) probability (vidi ,ti ,pi ), Frechet inequality (recalled
proof Theorem 3) entails thatPthe minimum probability st-atoms f occur
together equal max(0, 1 k + ki=1 (vidi ,ti ,pi )). equal zero since second
argument function max greater zero due fact (ii) holds. fact
minimum probability st-atoms f simultaneously occur equal zero suffices ensure
least one model 00 K0 00 assigns probability equal zero
worlds K0 satisfy f . f std-formula F, every world satisfying
f assigned probability equal zero 00 , follows 00 model K too.
following example shows considering even binary std-formula containing st-atom
referring region consisting two points, may happen feasible solution
NC(K) even K consistent.
Example 16. Let ID = {id}, = [0, 1], Space = {p0 , p1 , p2 }. Let K = hA, Fi
= {loc(id, {p0 }, 0)[0.4, 1], loc(id, {p1 }, 0)[0.4, 1], loc(id, {p0 }, 1)[0.4, 1], }, F stdformula [loc(id, {p0 , p1 }, 0) loc(id, {p2 }, 1)] saying id cannot move point p2 time 1
either p0 p1 time 0. easy see NC(K) feasible K consistent. 2
Section 3.5, present method deciding polynomial time consistency
PST KBs binary std-formulas satisfying acyclicity conditions used. turns
consistency PST KBs Examples 15 16 decided polynomial
time using approach.
3.4 Unary Std-Formulas Tractable
start identifying tractable case consistency checking problem: std-formulas
unary, is, formula F consists one st-atom possibly conjunction
built-in predicates (i.e., Definition 2, k = 1).
759

fiPARISI & G RANT

Example 17. constraint object region r time 5 10
expressed following unary std-formula: X1 , Z1 [loc(X1 , r, Z1 ) Z1 5 10 Z1 ].
constraint object id always region r expressed as:
Y1 , Z1 [loc(id, Y1 , Z1 ) Y1 nov r]. 7
2
following theorem states checking consistency tractable unary std-formulas
considered.
Theorem 5. Let K = hA, Fi PST KB F consists unary std-formulas only. Then,
deciding whether K consistent P IM E.
Proof. statement follows fact F consists unary std-formulas only, K =
hA, Fi equivalent (i.e., exactly set models as) K0 = hA0 , i, A0
consists atoms plus atom loc((Xi ), (Yi ), (Zi ))[0, 0]
ground std-formula
(f ) = [loc((Xi ), (Yi ), (Zi ))], f F f . Since, f F f polynomial
w.r.t. size K, size A0 (and thus K0 ) increases polynomial number atoms.
Hence, apply Fact 1, entails consistency PST KBs F =
decided P IM E.
3.5 Tractable Binary Std-Formulas
following focus PST KBs std-formulas binary. restricted
expressive class std-formulas allow us impose several practical constraints many
application contexts. matter fact, f1 f2 Example 2 well f4 Example 4
binary std-formulas. Furthermore, using approach suggested proof Theorem 5,
assume unary std-formulas encoded PST atoms. Thus, results stated
section straightforwardly apply case unary binary std-formulas PST
KB.
start noting consistency checking proved feasible Grant et al. (2010)
case reachability definitions (but integrity constraints) allowed.
showed Example 3 reachability definition expressed binary std-formula. Hence,
special case binary std-formulas represent reachability definitions, consistency
checking tractable.
general case PST KB K = hA, Fi F consists binary std-formulas,
define undirected graph, called std-graph, maximal independent set represents
world K satisfying std-formulas F. 8 later use graph characterize binary
std-formulas consistency checking problem turns tractable.
Definition 10 (std-graph). Given PST KB K = hA, Fi F consists binary stdformulas, std-graph G(K) undirected graph hV, Ei whose sets vertices V edges
E that:
1) V consists set ground st-atoms form loc(id, {p}, t) id ID, p
Space, ;
7. Recall every substitution Y1 must region containing single point.
8. maximal independent set std-graph maximal independent set undirected graph whose vertices
st-atoms. formal relationship maximal independent sets std-graph worlds PST KB
given Proposition 1.

760

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

2) E consists
i) edge every pair ground st-atoms V referring object
time value, is, p1 , p2 Space, p1 6= p2 , id ID, , hloc(id, {p1 }, t),
loc(id, {p2 }, t)i E;
ii) edge hloc(id1 , {p1 }, t1 ), loc(id2 , {p2 }, t2 )i every pair ground st-atoms
V loc(id1 , r1 , t1 ) loc(id2 , r2 , t2 )] p1 r1 p2 r2 belongs
set ground std-formulas derived F.
write G instead G(K) K known.
Basically, edge G connects pair st-atoms cannot belong together world
satisfying std-formulas F. particular, edge type i) connects two st-atoms representing
fact object two places time admitted according
definition world (see Definition 4). edge type ii) connects two st-atoms representing fact
(i.e., object id1 point p1 time t1 object id2 point p2 time t2 ) consistent
ground std-formula entailed F.
structure G follows. hid, ti pair, G contains clique 9 size |Space|
consists vertex loc(id, {p}, t) point Space edges type i)
following, refer
clique clique hid, ti pair hid, ti clique. ground
std-formula f = loc(id1 , r1 , t1 ) loc(id2 , r2 , t2 )] derived F, G contains
clique size |r1 | + |r2 | consists vertex loc(id1 , {p1 }, t1 ) point r1 , vertex
loc(id2 , {p2 }, t2 ) point r2 , edges types i) ii) refer clique
clique std-formula f .
Example 18. Let ID = {id1 , id2 }, = [0, 1, 2], Space = {p1 , p2 , p3 , p4 }. Assume F
consists following std-formulas:
f1 = X1 , X2 , Z1 [loc(X1 , {p2 , p4 }, Z1 ) loc(X2 , {p2 , p4 }, Z1 ) X1 6= X2 0 Z1
Z1 1], saying cannot two distinct objects region consisting points
{p2 , p4 } times 0 1;
f2 = Z1 [loc(id1 , {p3 , p4 }, 0) loc(id1 , {p1 }, Z1 ) 1 Z1 Z1 2], saying object
id1 cannot reach point p1 starting region {p3 , p4 } time 0 1 2 time units.
std-graph G shown Figure 2(a), where, sake readability, vertices labelled
names points Space refer, id time value vertex
reported column row belongs (for instance, vertex loc(id1 , {p1 }, 0)
represented circle upper-left corner). Observe G consists 10 (maximal) cliques,
one 6 hid, ti pairs, one 4 ground std-formulas derived F.
Specifically, hid, ti-clique, id {id1 , id2 } [1..2], consists four vertices
loc(id, {pk }, t) k [1..4], cliques ground std-formulas following sets:
{loc(id1 , {p2 }, 0), loc(id2 , {p2 }, 0), loc(id1 , {p4 }, 0), loc(id2 , {p4 }, 0)},
{loc(id1 , {p2 }, 1), loc(id2 , {p2 }, 1), loc(id1 , {p4 }, 1), loc(id2 , {p4 }, 1)},
{loc(id1 , {p3 }, 0), loc(id1 , {p4 }, 0), loc(id1 , {p1 }, 1)},
{loc(id1 , {p3 }, 0), loc(id1 , {p4 }, 0), loc(id1 , {p1 }, 2)}.
2
761

fiPARISI & G RANT

id1

id2

p1

p2

p2

p1

p3

p4

p4

p3

p1

p2

p2

p1

p3

p4

p4

p3

id1,0

id2,0

p1

p2

p2

p1

id1,1

id2,1

p3

p4

p4

p3

id1,2

id2,2

t=0

t=1

t=2

(a)

(b)

Figure 2: (a) Std-graph G; (b) Auxiliary-graph AG.
worth noting cliques hid, ti pairs well std-formulas
maximal cliques general, shown following.
Example 19. Continuing Example 18, assume F augmented following (ground)
std-formulas:
f3 = [loc(id2 , {p3 }, 1) loc(id2 , {p2 }, 2)]
f4 = [loc(id2 , {p4 }, 1) loc(id2 , {p2 }, 2)]
Thus, clique std-formula f3 consists set {loc(id2 , {p3 }, 1), loc(id2 , {p2 }, 2)},
f4 consists set {loc(id2 , {p4 }, 1), loc(id2 , {p2 }, 2)}. cliques maximal ones, included clique consisting set vertices
{loc(id2 , {p3 }, 1), loc(id2 , {p4 }, 1), loc(id2 , {p2 }, 2)}. basically due fact
constraint imposed f3 f4 expressed succinctly, [loc(id2 , {p3 , p4 }, 1)
loc(id2 , {p2 }, 2)] whose associated clique maximal.
2
following proposition follows definition std-graph fact every object
must unique point time value.
Proposition 1. Let K = hA, Fi PST KB F consists binary std-formulas only. Every
maximal independent set G consisting vertex hid, ti clique one-to-one
correspondence world w W w |= F.
9. Note use terminology clique complete subgraph G, maximal clique clique contained
clique. point since maximal cliques often called simply cliques.

762

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

Observe maximal independent set G property stated Proposition 1, G must contain maximal clique including least two hid, ti cliques (this happens
instance F contains std-formula [loc(id, Space, t1 ) loc(id, Space, t2 )]). case,
world satisfies F PST KB trivially inconsistent. general case, may exponential number maximal independent sets G property stated Proposition 1,
fact PST KB consistent decided using G explained follows.
3.5.1 C LIQUE -ACYCLIC TD -G RAPHS
characterization tractable cases consistency checking problem focuses KBs
binary std-formulas std-graphs property, call clique-acyclic. start
preliminary definitions.
Definition 11 (Binary maximal clique). std-graph G call clique binary maximal iff
contains vertices two hid, ti pairs properly included clique contains
vertices hid, ti pairs.
particular, std-graph G Figure 2(a) 4 binary maximal cliques involving id1 .
Definition 12 (Clique-subgraph). call subgraph std-graph G clique subgraph iff contains
vertices G, one edge hid, ti clique well one new distinct edge
binary maximal clique.
clique-subgraph std-graph graph G Figure 2(a) 10 edges, one
6 hid, ti pairs one binary maximal clique.
Definition 13 (Clique-acyclic std-graph). Std-graph G said clique-acyclic iff cliquesubgraphs acyclic graphs (that is, forests). G called clique-cyclic clique-acyclic.
Basically, clique-acyclicity means cycle found std-graph compressing
binary maximal cliques single edges using one edge hid, ti-clique. easy
see std-graph shown Figure 2(a) clique-acyclic. stated next proposition,
clique-acyclicity checked using following auxiliary graph basically compresses
clique-subgraph essential structure. clear definition auxiliary
graph obtained clique-subgraphs graph G.
Definition 14 (Auxiliary graph). auxiliary graph G undirected graph AG = hV 0 , E 0
that:
V 0 consists vertex hid, ti pair, id ID ;
E 0 consists edges binary maximal cliques clique-subgraph
previous vertex loc(id, {p}, t) replaced corresponding hid, ti pair. denote
C(e) binary maximal clique C edge e originated.
auxiliary graph std-graph Figure 2(a) shown Figure 2(b). example,
C(e) edge e = h(id1 , 0), (id1 , 1)i consists following set vertices
{loc(id1 , {p3 }, 0), loc(id1 , {p4 }, 0), loc(id1 , {p1 }, 1)}.
following proposition follows Definitions 13 14.
763

fiPARISI & G RANT

Proposition 2. Std-graph G clique-acyclic iff auxiliary graph AG acyclic (that is, AG
forest).
following, introduce set linear inequalities used check consistency PST KBs std-formulas binary auxiliary graph acyclic.
next subsection special case cyclic auxiliary graph. cases
working single connected component auxiliary graph. Suppose AG
n connected components C1 , . . . , Cn let Ki PST KB corresponding Ci
i. K model show obtain model Mi Ki . Let wi world appropriate
hid, ti pairs Ki .PLet Wi worlds K extend wi hid, ti values
Ki . Define Mi (wi ) = wWi (w). Going direction, suppose M1 , . . . , Mn
models PST KBs corresponding Ci , . . . , Cn respectively. world w K, let
w1 , . . . , wn restrictions w C1 , . . . , Cn respectively. Defining (w) = ni=1 (wi )
model K. shown following result.
Proposition 3. K consistent iff PST KBs corresponding connected components
G (and hence AG) consistent.
Hence proofs suffices assume AG single connected component.10
Definition 15 (BC(K)). Let K = hA, Fi PST KB F consists binary std-formulas
only. BC(K) consists following (in)equalities:
P
vid,t,p u;
(1) loc(id, r, t)[`, u] A: `
pr

(2) id ID, :

P

vid,t,p = 1;

pSpace

(3) id ID, T, p Space: vid,t,p 0;
P
vid,t,p 1.
(4) edge e AG:
loc(id,{p},t)C(e)

following theorem states checking whether BC(K) feasible solution equivalent
deciding consistency PST KBs K std-formulas binary generate acyclic
auxiliary graph. intuition behind result follows. feasible solution
BC(K), inequalities (1)-(3) entail hid, ti pair, events object id time
point p Space arranged fit whole probability space without overlapping. Roughly
speaking would suffice define model PST KB without std-formulas combining
distributions obtained hid, ti pair. Considering std-formulas means events
cannot occur together, is, cannot coexist portion probability space.
Intuitively, fact inequality (4) BC(K) satisfied edge e entails events
corresponding st-atoms C(e) arranged distinct portions probability space
(avoiding overlaps). auxiliary graph AG acyclic, reasoning inductively repeated
edge AG without ever reconsidering arrangement events corresponding
st-atoms considered previously. satisfiability inequality (4) BC(K) also necessary
10. indicate statement following theorem AG several connected components,
proof works one Proposition 3 applied.

764

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

condition consistency K fact satisfied edge e AG intuitively
means events corresponding st-atoms C(e), even taken alone, cannot arranged
probability space without overlapping.
Theorem 6. Let K = hA, Fi PST KB F consists binary std-formulas only. AG
acyclic, K consistent iff feasible solution BC(K).
Proof. use notation K0 K F removed, is, K0 = hA, i. Then, M(K)
subset M(K0 ).
() prove contrapositive: feasible solution BC(K), K inconsistent. simplest case BC(K) feasible solution due fact BC(K0 )
feasible solution. inequalities (4), hence K0 consistent, thus K
consistent well.
Otherwise, feasible solution BC(K0 ) thus BC(K) feasible solution due
fact least one inequalities
item (4) Definition 15 satisfied. Therefore,
P
edge e AG loc(id,{p},t)C(e) vid,t,p > 1. show K inconsistent
show interpretation model K. suppose interpretation
model K. Since pair A1 , A2 st-atoms belong C(e) iff (a) [A1 A2 ] belongs set
ground formulas derived F, (b) A1 = loc(id, {p1 }, t) A2 = loc(id, {p2 }, t)
p2 6= p1 , world w W(K) (w) > 0, w |= A1 , w |= A2 (in
particular, case (b) world w W(K) w |= A1 , w |= A2 , whether
(w) > 0). is, every world w K assigned non-zero probability
satisfies one st-atom C(e). Hence, W(K) partitioned |C(e)| + 2
pairwise disjoint sets follows:
(i) set worlds w W(K) (w) = 0 (this set includes worlds satisfying
least two st-atoms C(e))
(ii) set worlds w W(K) (w) > 0 w satisfy st-atom C(e);
(iii) st-atom Ai C(e), set worlds w W(K) (w) > 0, w |= Ai ,
w 6|= Aj Aj C(e) (and different Ai ).
Therefore, sum probabilities worlds satisfying least one st-atom C(e) equal
sum, st-atom C(e), sum probabilities worlds satisfying atom
C(e) st-atoms C(e),
X
w W(K)
w |= loc(id, {p}, t)
loc(id, {p}, t) C(e)

(w) =

X

X

loc(id,{p},t)C(e)

w W(K)
w |= loc(id, {p}, t)
w 6|= loc(id0 , {p0 }, t0 )
loc(id0 , {p0 }, t0 ) C(e)

(w).

recall model K also model K0 . above-mentioned partitioning
W(K) also holds W(K0 ). Therefore, solution BC(K0 ), one-to-one corresponding
model K0 , sum probabilities assigned worlds K0 satisfying st-atom loc(id, {p}, t) equal value assigned variable vid,t,p . particular,
765

fiPARISI & G RANT

given st-atom loc(id, {p}, t) C(e),
X
X
(w) =
W(K0 )

(w) = vid,t,p .

W(K0 )

w
w |= loc(id, {p}, t)

w
w |= loc(id, {p}, t)
loc(id, {p}, t) C(e)
w 6|= loc(id0 , {p0 }, t0 )
loc(id0 , {p0 }, t0 ) C(e)

Considering st-atoms C(e) obtain
X
X
(w) =
W(K0 )

X

loc(id,{p},t)C(e)

w
w |= loc(id, {p}, t)
loc(id, {p}, t) C(e)

=

X

(w) =

W(K0 )

w
w |= loc(id, {p}, t)
w 6|= loc(id0 , {p0 }, t0 )
loc(id0 , {p0 }, t0 ) C(e)

vid,t,p > 1.

loc(id,{p},t)C(e)

inequality holds since inequality (4) Definition 15 satisfied edge e.
Finally, since following inequality holds due definition model
X
X
(w)
(w),
wW(K0 )

w W(K0 )
w |= loc(id, {p}, t)

latter strictly greater one, follows PDF W(K0 ), meaning
model K0 . Hence interpretation model K K inconsistent.
() prove auxiliary graph AG acyclic BC(K) feasible solution,
K consistent. prove mathematical induction number edges AG.
Base case: number edges 0, AG consists set isolated vertices. fact
AG contains edges means set ground std-formulas derived F
empty. Thus feasible solution BC(K) iff feasible solution CC(K0 ). Using
Fact 1, obtain K consistent.
Inductive step: prove statement holds edges AG, holds + 1
edges. given acyclic graph AG + 1 edges, write AG +1 .
acyclicity choose subgraph, AG edges new edge connects vertex
isolated vertex AG AG clearly acyclic. Let K = hA, F PST KB
F consists subset ground std-formulas derived F std-graph G
auxiliary graph AG . induction hypothesis feasible solution BC(K ),
K consistent.
write K+1 = hA, F +1 F +1 consists ground std-formulas F plus
corresponding new edge e, AG +1 turns auxiliary graph G(K+1 ).
Assuming induction hypothesis, show feasible solution BC(K+1 ),
K+1 consistent.
Let solution BC(K+1 ). Obviously, also solution BC(K ), consists
subset (in)equalities BC(K+1 ). fact solution BC(K ) means
model K sum probabilities assigned worlds
766

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

K satisfying st-atom loc(id, {p}, t) equal value (vid,t,p ) assigned variable vid,t,p
. Since solution also BC(K+1 ), thus inequality (4) Definition 15 holds, follows

X
X
X
(vid,t,p ) 1.
(w) =
loc(id,{p},t)C(e)

w W(K ),
w |= loc(id, {p}, t)

loc(id,{p},t)C(e)

is, model K property sum probabilities worlds
satisfying least one st-atom C(e) less equal one.
assigns probability equal zero every world W(K ) containing
least two st-atoms C(e), done, since turns model K+1 too.
Otherwise, starting , show model +1 K+1 constructed
reasoning follows.
recall new edge e added AG obtain AG +1 form h(id1 , t1 ),
(id2 , t2 )i. Let C1 (e) C2 (e) sets st-atoms C(e) form loc(id1 , {p}, t1 )
loc(id2 , {p}, t2 ), respectively. Hence, C(e) = C1 (e) C2 (e) C1 (e) C2 (e) = . Let
= hw1 , w2 , . . . , wn permutation worlds w W(K ) assigned
probability greater zero (i.e., (w) > 0) first k worlds satisfy statom C1 (e). Note that, W(K ) = W(K+1 ), world W(K ) consists one
atom C1 (e), function whose domain ID (see Definition 4) st-atoms
C1 (e) refer id andP
time value. Let us denote sum probabilities
first worlds , is, = ij=1 (wj ), wj . Thus, k equal sum
probabilities assigned worlds satisfying st-atom C1 (e), i.e.,
X

k =

X

(w) =

w W(K ),
w |= loc(id, {p}, t),
loc(id, {p}, t) C1 (e)

(vid,t,p ).

loc(id,{p},t)C1 (e)

Let subset Space consisting points p variables vid2 ,t2 ,p assigned
value greater zero , is, = {p | p Space, (vid2 ,t2 ,p ) > 0}. Observe
variables correspond st-atoms referring endpoint (id2 , t2 ) edge e. Let = hp1 , p2 , . . . , pm
permutation points first h points p correspond variables vid2 ,t2 ,p
loc(id2 , {p}, t2 ) 6 C2 (e), subsequent points (with index [h + 1, .., m])
correspond variables vid2 ,t2 ,p loc(id2 , {p}, t2 ) C2 (e). Let sum
Pi
values assigned first points , is, =
j=1 (vid2 ,t2 ,pj ) pi .
Observe thatPm = 1 since equality (2) Definition 15 holds BC(K+1 ), h =
1
(vid,t,p ). fact inequality (4) Definition 15 holds BC(K+1 )
loc(id2 ,{p},t2 )C2 (e)

entails that,
1 h + k =

X

(vid,t,p ) +

loc(id,{p},t)C2 (e)

X

(vid,t,p ) 1.

loc(id,{p},t)C1 (e)

Therefore, obtain k h . Intuitively, means possible define PDF
worlds K+1 world satisfies two st-atoms C(e), keeping satisfied
PST atoms std-formulas satisfied . formally described below.
767

fiPARISI & G RANT

define model +1 K+1 follows. Let U = hu1 , u2 , . . . , uz sequence
consisting values {1 , . . . , n } {1 , . . . , } ordered ascending order. define
following z non-zero probability worlds K+1 . ui U , let wi world that:
id ID \ {id2 }, \ {t2 }, wi (id, t) = wj (id, t), wj W(K ) j
smallest subscript j ui .
wi (id2 , t2 ) = pj , pj j smallest subscript j ui .
define +1 PDF W(K+1 ) (i) +1 (w1 ) = v1 , (ii) +1 (wi ) =
vi vi1 [1..z], (iii) worlds w W(K+1 ), +1 (w) = 0.
show +1 model K+1 . First, observe +1 model K .
fact, id ID \ {id2 }, id \ {t2 }, p Space, probability object id
time value point p according +1 equal probability id time value
point p according ,
X
X
id ID \ {id2 }, \ {t2 }, p Space,
+1 (w) =
(w).
w W(K ),
w(id, t) = p

w W(K+1 ),
w(id, t) = p

Moreover, hid2 , t2 pair,
X
p Space,

+1 (w) = (vid2 ,t2 ,p ) =

X

(w).

w W(K ),
w(id, t) = p

w W(K+1 ),
w(id, t) = p

ensures that, PST atom = loc(id, r, t)[`, u] A, sum probabilities
assigned +1 worlds satisfying st-atom loc(id, r, t) belong probability interval
[`, u] specified a, since hold . Moreover, none worlds assigned
probability equal zero assigned probability greater zero +1 ,
consists worlds assigned non-zero probability . Therefore, +1 model
K .
Finally, show +1 model K+1 , suffices observe k h entails
world wi K+1 (with [1..z]) assigned probability greater zero
satisfies two atoms C(e). completes proof.
following example shows usage result Theorem 7 check consistency
PST KB.
Example 20. Continuing Example 18, assume K PST KB F set stdformulas given Example 18 consist following set PST atoms:
loc(id1 , {p3 , p4 }, 0)[.7, 1], loc(id1 , {p1 }, 1)[.2, .5], loc(id1 , {p1 }, 2)[.3, .8], loc(id2 , {p2 }, t)[.7, 1]
[0..2]. checked PST KB corresponding set linear inequalities BC(K) feasible solution. Thus, since auxiliary graph AG(K) shown Figure 2(b)
acyclic, follows K consistent.
Consider PST KB K0 obtained K replacing atom loc(id1 , {p1 }, 2)[.3, .8]
loc(id1 , {p1 }, 2)[.8, .8], lower probability .8 instead .3. case, BC(K0 )
includes inequalities 0.7 vid1 ,0,p3 + vid1 ,0,p4 1 0.8 vid1 ,2,p1 0.8 (due PST
atoms), vid1 ,0,p3 + vid1 ,0,p4 + vid1 ,1,p1 1 (due edge h(id1 , 0), (id1 , 2)i AG(K)).
Clearly, feasible solution BC(K0 ), thus K0 consistent.
2
768

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

note fact acyclicity AG used left-to-right proof Theorem 6.
Therefore, whether AG(F) acyclic, PST KBs std-formulas binary,
necessary condition stated following theorem used checking consistency.
Theorem 7. Let K = hA, Fi PST KB F consists binary std-formulas only.
feasible solution BC(K), K consistent.
condition BC(K) feasible solution stronger NC(K) feasible
solution, used Theorem 3 general std-formulas, since easily checked every
solution BC(K) solution NC(K), converse hold general. Thus,
PST KBs K, using Theorem 3 cannot conclude anything consistency
K, NC(K) feasible solution, show K inconsistent using result
Theorem 7.
Example 21. already observed fact PST KB K Example 16
consistent cannot concluded checking whether feasible solution NC(K). However,
easy see BC(K) feasible solution case, thus conclude
PST KB Example 16 consistent applying result Theorem 7.
2
Although necessary condition Theorem 6 still used check consistency
auxiliary graph cyclic (see Theorem 7), sufficient condition Theorem 6 entails
PST KB K consistent corresponding set BC(K) linear inequalities feasible
solution auxiliary graph acyclic. following, identify class std-graphs
consistency checking problem remains tractable even std-graph clique-cyclic (and
thus auxiliary graph cyclic).
3.5.2 IMPLE C LIQUE -C YCLIC TD -G RAPHS
section provide set linear inequalities checking consistency PST KB
whose std-graph satisfies following property.
Definition 16 (Simple clique-cyclic std-graph). Std-graph G said simple clique-cyclic iff
auxiliary graph AG simple graph 11 cyclic connected component G contains
single maximal clique hid, ti clique.
following example simple clique-cyclic std-graph. provide std-graph
auxiliary graph.
Example 22. PST KB K = hA, Fi Example 15, obtain std-graph G shown
Figure 3(a). auxiliary graph AG shown Figure 3(b). G consists single connected component one maximal clique Cl hid, ti clique, namely
Cl = {loc(id, {p0 }, 0), loc(id, {p0 }, 1), loc(id, {p0 }, 2)}. easy see G simple cliquecyclic.
2
following theorem states checking consistency PST KB whose std-graph
simple clique-cyclic accomplished checking whether feasible solution set
linear inequalities.
start defining new linear system: CL(K).
11. loops multiple edges vertices.

769

fiPARISI & G RANT

id
p1

t=0
p2

p0

p1

id1,0

t=1
p2

p0

id1,1
p1

id

t=2

id1,2

p2

p0

(a)

(b)

Figure 3: (a) Std-graph (b) auxiliary graph PST KB Example 15.
Definition 17 (CL(K)). Let K = hA, Fi PST KB F consists binary std-formulas
only. CL(K) consists following (in)equalities:
(1) loc(id, r, t)[`, u] A: `

P

vid,t,p u;

pr

(2) id ID, :

P

vid,t,p = 1;

pSpace

(3) p Space, id ID, : vid,t,p 0;
(4) (a)
P acyclic connected component G (and hence AG), edge e AG:
vid,t,p 1.
loc(id,{p},t)C(e)

(b) cyclic connected component
G (and hence AG), maximal clique Cl
P
hid, ti clique:
vid,t,p 1.
loc(id,{p},t)Cl

intuition behind result stated following theorem similar given
tractable case Theorem 6. Indeed, G simple clique-cyclic CL(K) feasible solution,
events corresponding st-atoms edge e acyclic connected component AG,
well events corresponding st-atoms maximal clique connected component AG, arranged probability space avoiding overlaps. basically suffices
define model K.
Theorem 8. Let K = hA, Fi PST KB F consists binary std-formulas only. G
simple clique-cyclic, K consistent iff feasible solution CL(K).
770

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

Proof. explained earlier deal single cyclic connected component G.
() proof follows reasoning left-to-right proof Theorem 6.
() Let solution CL(K). Obviously, alsoP
solution CC(K0 ) K0 =
0
0
0
hA,
P i, model K (vid,t,p ) = wW(K0 ),w|=loc(id,{p},t) (w)
loc(id,{p},t)Cl (vid,t,p ) 1. Given this, show starting define new model
K0 also model K.
hid, ti pair, define S(id, t) subset Space consisting points p
vid,t,p assigned value greater zero , i.e., S(id, t) = {p | p Space, (vid,t,p ) > 0}.
distinguish following two sets hid, ti pairs: IDT1 (resp. IDT2 ) set
hid, ti pairs (resp. is) point p S(id, t) loc(id, {p}, t) Cl.
separately consider pairs IDT1 IDT2 , hid, ti pair belonging one
sets, define sequence points S(id, t) along sequence cumulative probability
values used build model .
start set IDT1 . Let hidi , ti i, [1..|IDT1 |] i-th pair IDT1 (after

ordering pairs IDT1 according fixed order). Let = hp1i , . . . , pm
permutation
points S(idi , tP
), let (k) sum values assigned first k points
, is, (k) = kj=1 (vidi ,ti ,pj ). Thus, hi (1), . . . , (mi )i sequence (cumulative)



probability values associated sequence points hp1i , . . . , pm
i-th pair IDT1 .
consider pairs IDT2 , i.e, occurring st-atom Cl. Denote
hidi , ti i, [1..|IDT2 |], i-th pair IDT2 (according fixed order). first
1
pair IDT2 , let 1 = hp11 , . . . , ph1 1 , ph1 1 +1 , . . . , pm
1 permutation points S(id1 , t1 )
first h1 points p correspond variables vid1 ,t1 ,p loc(id1 , {p}, t1 ) Cl
1
(consequently, points p {ph1 1 +1 , . . . , pm
1 } correspond variables vid1 ,t1 ,p
loc(id1 , {p}, t1 ) 6 Cl).

hidi , ti [2..|IDT2 |], let = hp1i , . . . , pgi , pigi +1 , . . . , pigi +hi , pigi +hi +1 , . . . , pm

g
+h
+1
g

sequence points S(idi , ti ) (i) point p {p1i , . . . , pi , pi , . . . , pi }
(resp. p {pgi +1 , . . . , pgi +hi }) corresponds variable vidi ,ti ,p loc(idi , {p}, ti ) 6 Cl
(resp. loc(idi , {p}, ti ) Cl); (ii) points sequence distinct except pgi
pgi +hi +1 may refer point S(idi , ti ). Denoting (k) sum values
P
assigned first k points (i.e., (k) = kj=1 (vidi ,ti ,pj )), choose sequences

2 , . . . , |IDT2 | that, [2..|IDT2 |], (gi ) = i1 (gi1 +hi1 ) (assume g1 = 0).
Note make choice since following holds:

|IDT2 | hi
X X

|IDT2 |

(vidi ,ti ,pk ) =

X



i=1 k=1

|IDT2 |

(gi + hi ) (gi ) =

i=1

X

X

(vidi ,ti ,p ) 1.

i=1 loc(idi ,{p},ti )Cl

way sequences h1 , . . . , |IDT2 | hi (1), . . . , (mi )i defined allow us build
model K0 world assigned probability greater zero satisfies two
distinct st-atoms Cl. ensure model K too.

define model . LetSV = hv1 , v2 , . . . , vz sequence consisting
values iIDT1 {i (1), . . . , (mi )} iIDT2 {i (1), . . . , (mi )} ordered ascending order.
define following z non-zero probability worlds K0 (note world K0 also
world K). vj V , let wj world that:
771

fiPARISI & G RANT

hidi , ti IDT1 , define wj (idi , ti ) = pki , pki left-most value
hi (1), . . . , (mi )i (k) vj .
hidi , ti IDT2 , define wj (idi , ti ) = pki , pki left-most value
hi (1), . . . , (mi )i (k) vj .
Finally, define PDF W(K0 ) (i) (w1 ) = v1 , (ii) (wj ) = vj vj1
j [1..z], (iii) (w) = 0 worlds w W(K0 ). easy check
0
0
model
P K corresponds solution CC(K ), is,
(vid,t,p ) = wW(K0 ),w|=loc(id,{p},t) (w). Moreover, construction shown ensures
non-zero probability world K0 satisfies two distinct st-atoms Cl, follows
model K.
Theorem 8 used, instance, decide PST KB Example 15 consistent.
Example 23. PST KB K = hA, Fi Example 15, CL(K) linear system obtained
CC(hA, i) augmenting inequality vid,0,p0 + vid,1,p0 + vid,2,p0 1. Since CL(K)
also contains inequality 0.5 vid,t,p0 [0..2] (due presence PST atom
loc(id, {p0 }, t)[0.5, 1] A), follows CL(K) feasible solution, conclude
K consistent (here G clique-cyclic std-graph shown Figure 3(a)).
2
conclude section following theorem stating checking consistency
PST KBs identified Section 3.5 tractable.
Theorem 9. Let K = hA, Fi PST KB F consists binary std-formulas only. G
clique-acyclic simple clique-cyclic, checking consistency K P IM E.
Proof. G consists one connected component, statement follows facts building
G, well checking whether G acyclic (resp. simple clique-cyclic), accomplished
polynomial time w.r.t. size K checking whether feasible solution BC(K)
(resp., CL(K)) polynomial too. G consists one connected component, statement follows facts finding connected components G done polynomial
time, and, using Proposition 3, need check consistency PST KBs corresponding connected components order decide consistency whole PST
KB.

4. Querying PST Knowledge Bases
section investigates complexity checking answer queries. Section 4.1 contains
basic definitions. Section 4.2 contains major result complexity checking answers
queries. Finally, Section 4.3 gives sufficient necessary conditions answering queries
tractable cases.
4.1 Optimistic Cautious Answers
problem querying SPOT data investigated Parker et al. (2007a, 2009), Parisi,
Parker, Grant, Subrahmanian (2010), Grant, Molinaro, Parisi (2013) specific
772

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

frameworks corresponding PST KBs (of form hA, i), PST atoms considered. section, address problem answering selection queries general (consistent) PST KBs. kinds queries considered Parker et al. (2007a, 2009), Parisi
et al. (2010), Grant et al. (2013) focused count queries.
selection query expression form (?id, q, ?t, [`, u]), q region [`, u]
probability interval. Intuitively, selection query says: Given region q probability interval
[`, u], find objects id times id inside q time probability interval
[`, u]. two semantics interpreting statement, leading two types answers
selection queries. Optimistic answers objects time values may query region
probability specified interval, whereas cautious answers consist objects
time values guaranteed region probability given interval. Thus,
cautious answers subset optimistic ones.
following definition extends original definition optimistic cautious selection
query answers general case consistent PST knowledge bases.
Definition 18 (Optimistic/Cautious Query Answers). Let K consistent PST KB, Q =
(?id, q, ?t, [`, u]) selection query. Then,
hid, ti
Pis optimistic answer Q w.r.t. K iff exists model M(K) s.t.
(w) [`, u].
w|=loc(id,q,t)

hid, ti
Pis cautious answer Q w.r.t. K iff every model M(K),
(w) [`, u].
w|=loc(id,q,t)

Example 24. Let q1 = {(7, 3), (7, 4)} (q1 overlaps region c, see Figure 1). Model
Example 9 entails hid1 , 9i optimistic answer Q = (?id, q1 , ?t, [.7, 1]), w1 (id1 , 9) =
w2 (id1 , 9) = w3 (id1 , 9) = (7, 4) q1 (w1 ) + (w2 ) + (w3 ) = 1 [.7, 1].
Let q2 region including region c. hid1 , 9i cautious answer Q0 = (?id, q2 , ?t, [.7, 1]),
according model Kex , id1 must region c (and thus q2 ) time 9 probability
[.9, 1] (due loc(id1 , c, 9)[.9, 1] Aex ). Clearly, hid1 , 9i also optimistic answer Q0 . 2
4.2 Complexity Query Answering
following theorem shows consistency checking used answer selection queries
optimistic cautious semantics.
Theorem 10. Let K = hA, Fi consistent PST KB, Q = (?id, q, ?t, [`, u]). Then,
1) hid, ti optimistic answer Q w.r.t. K iff hA {loc(id, q, t)[`, u]}, Fi consistent.
2) hid, ti cautious answer Q w.r.t. K iff hA {loc(id, q, t)[0, ` ]}, Fi hA
{loc(id, q, t)[u + , 1]}, Fi consistent, smallest rational number
precision m3 , = |A| + |F| |ID| |T | |Space| + |ID| |T | + |ID| |T | |Space|.
Proof. Statement 1) easily follows Definitions 6 18.
Statement 2) follows fact hid, ti cautious answer Q w.r.t. K iff i) hid, ti
optimistic answer Q0 = (?id, q, ?t, [0, ` ]) w.r.t. K, ii) hid, ti optimistic
773

fiPARISI & G RANT

answer Q00 = (?id, q, ?t, [u + , 1]) w.r.t. K, small non-zero constant whose size
polynomial w.r.t. size K. show existence , observe (i) checking
consistency K reduced deciding PSAT instance K consisting clauses,
= |A| + |F| |ID| |T | |Space| + |ID| |T | + |ID| |T | |Space|, shown membership
proof Theorem 1; (ii) satisfying probability distribution instance PSAT
clauses exists, one + 1 non-zero probabilities, entries
consisting rational numbers precision O(m2 ) (Georgakopoulos et al., 1988; Papadimitriou
& Steiglitz, 1982); Then, choosing equal smallest rational number precision m3 suffices
obtain sufficiently small non-zero constant, whose size polynomial w.r.t. size K,
hA {loc(id, q, t)[0, ` ]}, Fi (resp., hA {loc(id, q, t)[u + , 1]}, Fi) consistent iff
hA {loc(id, q, t)[0, ` ]}, Fi (resp., hA {loc(id, q, t)[u + , 1]}, Fi) consistent,
infinitely small value greater zero.
following corollary follows Theorems 1 10.
Corollary 1. Let K = hA, Fi consistent PST KB. Given query Q = (?id, q, ?t, [`, u]),
1) deciding whether hid, ti optimistic answer Q w.r.t. K NP.
2) deciding whether hid, ti cautious answer Q w.r.t. K coNP.
problem deciding whether pair hid, ti optimistic/cautious answer selection
query Q solved polynomial time PST KBs whose F component empty (Parker et al.,
2007a, 2009; Parisi et al., 2010). However, presence integrity constraints problem
becomes hard.
Theorem 11. Let K = hA, Fi consistent PST KB. Given query Q = (?id, q, ?t, [`, u]),
1) deciding whether hid, ti optimistic answer Q w.r.t. K NP-hard.
2) deciding whether hid, ti cautious answer Q w.r.t. K coNP-hard.
Proof. first prove item 1). show reduction problem NP-hard Hamiltonian
path problem (Papadimitriou, 1994). Given directed graph G = hV, Ei, V = {v0 , . . . , vk }
set k +1 vertexes G E set edges, construct instance problem
follows. Let ID = {id}, Space = V {p0 , . . . , pk1 } {pT , pF }, = [0, . . . , 2k + 1].
K pair hA, Fi consist PST atom loc(id, v0 , 0)[1, 1] F consists
following formulas:
[0..k 1], f1i = Z1 , Z2 [loc(id, {vi }, Z1 ) loc(id, Space \ V 0 , Z2 ) Z2 = Z1 + 1]
V 0 = {vj |(vi , vj ) E}{pi , pT , pF }. is, locations id reach starting
vi one time step locations vj s.t. (vi , vj ) E point {pi , pT , pF }.
f1k = Z1 , Z2 [loc(id, {vk }, Z1 ) loc(id, Space \ V 00 , Z2 ) Z2 = Z1 + 1] V 00 =
{vj |(vk , vj ) E} {pT , pF }. formula says locations id reach starting
vk one time step locations vj (vk , vj ) E {pT , pF }.
[0..k1], f2i = Z1 , Z2 [loc(id, {pi }, Z1 )loc(id, Space\{vi+1 }, Z2 )Z2 = Z1 +1],
saying location id reach starting pi one time step vi+1 .
774

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

f3 = Y1 , Z1 , Z2 [loc(id, Y1 , Z1 ) loc(id, Y1 , Z2 ) Z1 6= Z2 Y1 6= {pT } Y1 6= {pF }],
saying id location distinct time values, location different
pT pF .
f4 = Z1 , Z2 [loc(id, {p0 , . . . , pk1 }, Z1 ) loc(id, {pT }, Z2 )], saying id
pT was/is/will location {p0 , . . . , pk1 }.
f5 = Y2 , Z1 , Z2 [loc(id, {pT }, Z1 ) loc(id, Y2 , Z2 ) Y2 6= {pT } Z1 < Z2 ], saying
id go away pT reaching location.
f6 = Y2 , Z1 , Z2 [loc(id, {pF }, Z1 ) loc(id, Y2 , Z2 ) Y2 6= {pF } Z1 < Z2 ], saying
id go away pF reaching location.
Finally, let Q = (?id, {pT }, ?t, [1, 1]), let pair checked optimistic answer
Q hid, 2k + 1i.
First all, observe M(K) 6= . fact, consider world w W(K) w(id, 0) =
v0 , w(id, 1) = p0 , w(id, 2) = v1 , w(id, 3) = p1 , . . . , w(id, t) = vt/2 , w(id, + 1) = pt/2+1 , (in
cases even) . . . , w(id, 2k2) = vk1 , w(id, 2k1) = pk , w(id, 2k) = vk , w(id, 2k+1) =
pF . easy see w |= F. fact, [0..2k], id moves towards reachable locations
one time point: id vi , [0..k 1], moves pi (that is, f1i satisfied w);
id pi , [0..k 1], moves vi+1 (that is, f2i satisfied w); vk
moves pF (that is, f1k satisfied w). Moreover, id placed w location
different times (that is, f3 satisfied), neither location {p1 , . . . , pk1 } pT (that
is, f4 satisfied too). Moreover, id move pF reaching location (that is, f6
satisfied), reach location pT (thus, also f5 satisfied w). Now, consider PDF
W(K) assigning probability equal 1 w, probability equal 0 every world
W(K). Clearly, model K: assigns non-zero probability worlds satisfying F,
ensures atom loc(id, v0 , 0)[1, 1] satisfied w assigned probability equal
1.
show hid, 2k + 1i optimistic answer Q w.r.t. K iff Hamiltonian
path G.
()
P hid, 2k + 1i optimistic answer Q w.r.t. K, model M(K) s.t.
(w) = 1. Let W W(K) set worldsP
w w(id, 2k + 1) =
w|w|=loc(id,pT ,2k+1) P
pT (w) > 0. wW (w) = 1 model (thus, w|w|=loc(id,v0 ,0) (w) = 1
due loc(id, v0 , 0)[1, 1]) A), follows every world w W w(id, 0) = v0 .
Moreover, since model, thus f4 satisfied every world w W , holds
[0..2k + 1], w(id, t) 6 {p0 , . . . , pk 1}, meaning id never placed location
{p0 , . . . , pk1 } w. Moreover, since w W , w(id, 0) = v0 , since f1i f2i
(with [1..k]), f3 satisfied every world W , follows (i) [0, k],
w(id, t) V (i.e., id placed vertex G time point [0, k]), (ii)
t, t0 [0, k], w(id, t) 6= w(id, t0 ) (id placed vertex G different times
[0, k]). Given this, time = k + 1, f1k f3 entail id placed world W location
pT pF . f5 (resp., f6 ) entails id go away pT (resp., pF ) reaching
location, since w(id, 2k + 1) = pT , follows every world w W
[k + 1, 2k + 1], w(id, t) = pT . Hence, w W (w) > 0, w(id, 0) = v0 ,
[1, k], w(id, t) V , t, t0 [0, k], w(id, t) 6= w(id, t0 ), [k + 1, 2k + 1], w(id, t) = pT .
Therefore, w(id, 0), w(id, 1), . . . , w(id, k) Hamiltonian path G.
775

fiPARISI & G RANT

() Let Hamiltonian path G. denote [i] (with [0..k]) i-th vertex
. W.l.o.g. assume first
Pvertex v0 , is, [0] = v0 . show
model M(K) w|w|=loc(id,pT ,2k+1) (w) = 1, is, hid, 2k + 1i
optimistic answer Q w.r.t. K. Let function W worlds w W,
(w) = 0 except world w that: w (id, 0) = [0] = v0 , [1, k],
w (id, t) = [t], [k + 1, 2k + 1], w (id, t) = pT . easy see w |= F. fact,
[1..k], f1i well f2i satisfied w , since fact path G entails
[0, k 1], w (id, t) = vi w (id, + 1) = vj edge (vi , vj ) edge G
w (id, k + 1) = pT reachable location V . Moreover, f3 satisfied w ,
since fact Hamiltonian path entails w places id different locations different
times, except location pT . Formula f4 satisfied w , since w place id
location {p0 , . . . , pk1 }. Formula f5 satisfied w , since w place id location
different pT time point k + 1, id reaches pT . Finally, f6 satisfied w well,
since w place id location pF . Since wP|= F, assigned probability
different 0. Let (w ) = 1. Moreover, w|w|=loc(id,v0 ,0) (w) = (w ) = 1,
condition required loc(id, v0 , 0)[1, 1] holds too, proving model K.
prove item 2). show reduction problem complement
Hamiltonian path problem. Given directed graph G, construct instance problem
proof 1), except Q = (?id, {pT }, ?t, [0, 1 ]), > 0 stated
Theorem 10. shown proof 1), K consistent. Moreover, Hamiltonian
path G iff pair hid, 2k + 1i optimistic answer Q0 = (?id, {pT }, ?t, [1, 1]).
observe hid, 2k + 1i optimistic answer Q0 iff hid, 2k + 1i cautious
answer Q = (?id, {pT }, ?t, [0, 1 ]) > 0 sufficiently small. reasoning
proof Theorem 10, choose
P = , ensures M(K)
P

(w)
<
1,

w|w|=loc(id,pT ,t) (w) 1 .
w|w|=loc(id,pT ,t)

query answering problem related decision version entailment problem
probabilistic logic (Nilsson, 1986; Georgakopoulos et al., 1988), follows. Given conjunction clauses, associated probability, additional clause C
given lower upper bounds admissible values probability, decide whether
probability distribution satisfying probability clauses lower upper
bounds C. probabilistic logic, entailment problem reduced PSAT (Nilsson, 1986; Georgakopoulos et al., 1988), shown query answering problem PST
KBs reduced consistency checking problem. However, important difference
query answering problem probabilistic entailment problem: assume
input PST KBs consistent, entailment problem defined (even
satisfiable). consequence, coNP-hardness probabilistic entailment problem straightforwardly follows NP-hardness PSAT ( unsatisfiable iff contradiction C
lower upper bounds entailed ). is, satisfiability source complexity
probabilistic entailment problem. However, setting, given consistent PST KB
thus checking consistency cannot source complexity query answering problem.
fact, Theorem 11 viewed strengthening coNP-hardness probabilistic entailment
problem, proved specific setting corresponding case input formula
known satisfiable.
776

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

4.3 Sets Linear Inequalities Answering Queries
following corollary Theorems 2 10 states set mixed-binary linear inequalities
MBL() introduced Definition 8 also exploited answering queries.
Corollary 2. Let K = hA, Fi consistent PST KB, Q = (?id, q, ?t, [`, u]) query. Then,
1) feasible solution MBL(hA{loc(id, q, t)[`, u]}, Fi), hid, ti optimistic
answer Q w.r.t. K.
2) MBL(hA {loc(id, q, t)[0, ` ]}, Fi) MBL(hA {loc(id, q, t)[u + , 1]}, Fi)
feasible solution, hid, ti cautious answer Q w.r.t. K (where given
Theorem 10).
Similarly, Theorems 3 10 following corollary follows, stating linear system
NC() introduced Definition 9 also used answering queries.
Corollary 3. Let K = hA, Fi consistent PST KB, Q = (?id, q, ?t, [`, u]) query. Then,
1) feasible solution NC(hA {loc(id, q, t)[`, u]}, Fi), hid, ti
optimistic answer Q w.r.t. K.
2) NC(hA {loc(id, q, t)[0, ` ]}, Fi) NC(hA {loc(id, q, t)[u + , 1]}, Fi)
feasible solution, hid, ti cautious answer Q w.r.t. K (where given
Theorem 10).
Moreover, obtain following corollary Theorems 4 10.

Corollary 4. Let K = hA, Fi consistent PST KB F = { loc(id1 , {p1 }, t1 )
loc(id2 , {p2 }, t2 ) loc(idn , {pn }, tn )]}, Q = (?id, q, ?t, [`, u]) query. Then,
1) hid, ti optimistic answer Q w.r.t. K iff feasible solution NC(hA
{loc(id, q, t)[`, u]}, Fi).
2) hid, ti cautious answer Q w.r.t. K iff NC(hA {loc(id, q, t)[0, ` ]}, Fi)
NC(hA {loc(id, q, t)[u + , 1]}, Fi) feasible solution, given Theorem 10.
Theorems 6 10 obtain following result.
Corollary 5. Let K = hA, Fi consistent PST KB F consists binary std-formulas
only, Q = (?id, q, ?t, [`, u]) query. AG acyclic,
1) hid, ti optimistic answer Q w.r.t. K iff feasible solution BC(hA
{loc(id, q, t)[`, u]}, Fi).
2) hid, ti cautious answer Q w.r.t. K iff BC(hA {loc(id, q, t)[0, ` ]}, Fi)
BC(hA {loc(id, q, t)[u + , 1]}, Fi) feasible solution, given Theorem 10.
Moreover, results Theorem 7, shape auxiliary graph considered,
Theorem 10 entail following.
777

fiPARISI & G RANT

Corollary 6. Let K = hA, Fi consistent PST KB F consists binary std-formulas
only, Q = (?id, q, ?t, [`, u]) query.
1) feasible solution BC(hA {loc(id, q, t)[`, u]}, Fi), hid, ti
optimistic answer Q w.r.t. K
2) BC(hA {loc(id, q, t)[0, ` ]}, Fi) BC(hA {loc(id, q, t)[u + , 1]}, Fi)
feasible solution, hid, ti cautious answer Q w.r.t. K (where given
Theorem 10).
Still, Theorems 8 10 obtain following corollary.
Corollary 7. Let K = hA, Fi consistent PST KB F consists binary std-formulas
only, Q = (?id, q, ?t, [`, u]) query. AG simple-clique cyclic,
1) hid, ti optimistic answer Q w.r.t. K iff feasible solution CL(hA
{loc(id, q, t)[`, u]}, Fi).
2) hid, ti cautious answer Q w.r.t. K iff CL(hA {loc(id, q, t)[0, ` ]}, Fi)
CL(hA {loc(id, q, t)[u + , 1]}, Fi) feasible solution, given Theorem 10.
Finally, state following corollary Theorems 9 10 concerning cases computing selection queries tractable. Although result stated assuming binary stdformulas PST KB, holds general case unary binary stdformulas PST KB since, using approach suggested proof Theorem 5,
assume unary std-formulas encoded PST atoms.
Corollary 8. Let K = hA, Fi PST KB F consists binary std-formulas only.
G clique-acyclic simple clique-cyclic, deciding whether hid, ti optimistic/cautious
answer selection query Q = (?id, q, ?t, [`, u]) PTIME.

5. Domain Enlargement
point assumed three domains ID = {id1 , . . . , idm }, = [0, 1, . . . ,
tmax], Space = {p1 , . . . , pn } fixed. context investigated consistency
PST KB. section investigate happens PST KB one domains
modified. modification consists possibly adding new time values, spatial points,
objects. fact, interested happens add arbitrarily many (but finite number
of) entities. Section 5.1 consider case deal longer time period add
additional time values beyond tmax. Section 5.2 deal time case
allow finer division time values. Section 5.3 deals case Space enlarged,
is, new points added. Finally, Section 5.4 deal additional objects well
combinations domain enlargement.
5.1 Extending Time Beyond tmax
start PST KB K using ID, , Space. Suppose extend beyond tmax;
write 0 = [0, 1, . . . , tmax, . . . , t0 ]. means syntax must also extended
778

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

constants tmax + 1, . . . , t0 ; however, K use new constants. semantics
different world w0 must function w0 : ID 0 Space. write W
set worlds using W 0 set worlds using 0 . connection W
W 0 every w W extended many worlds W 0 choosing point Space
new time value object. one world w0 W 0 unique restriction
world W . Satisfaction worlds W 0 defined way satisfaction
worlds W (with W 0 substituted W ).
interest semantics due concept interpretation interpretation
assigns world probability. Let W = {w1 , . . . , wz } z = nm(tmax+1) let W 0 =
0
{w10 , . . . , wy0 } = nm(t +1) . every wi W , define set worlds Wi0 =
{wj0 |wj0 W 0 wj0 extension wi }. means restriction every wj0 Wi0
wi . Next, letPI1 interpretation W . call interpretation I10 extension I1 i,
1 z, w0 W 0 I10 (w0 ) = I1 (wi ). is, probability value I1 (wi ) distributed among

extensions wi i. also go direction. I20 interpretation

P
0
0
W , define restriction W , I2 , using formula, is, I2 (wi ) = w0 W 0 I2 (w0 )

i, 1 z.
framework allows us investigate happens consistency inconsistency
K go 0 . start inconsistency. state next theorem two different
forms useful various contexts.
Theorem 12. Let K contain time values .
a) K inconsistent time , K remains inconsistent expanded 0 .
b) 0 expansion K consistent 0 consistent .
Proof. second part contrapositive first, suffices prove one part.
prove second one. assume K, uses time values , consistent
expanded 0 . means interpretation 0 K (using W 0 ) model
K. WeP
obtain interpretation (for W ) 0 using formula given above, is,
(wi ) = w0 W 0 0 (w0 ) i, 1 z. need show model K.

Consider first atomic formulas
wi , wi |= loc(id, r, t) iff w0 |=
PA. Clearly, worldP
0
0
0
0
loc(id, r, t) w Wi . Hence, w|w|=loc(id,r,t) (w) =
w0 |w0 |=loc(id,r,t) (w ).
takes care A.
P
P
Next, let f F. w0 |w0 6|=f 0 (w0 ) = 0. need show also w|w6|=f (w) = 0.
Let wi W world wi 6|= f . means substitution using time
values wi 6|= (f ). remains substitutionPwhen 0 used time.
wj0 Wi0 , wj0 6|= (f ). 0 model K hence F, w0 |w0 6|=f 0 (w0 ) = 0 result
follows definition .
Next consider case PST KB defined using consistent. turns
consistency need preserved expanded 0 . Consider case following
single integrity constraint:
X1 X2 Y1 Z1 [loc(X1 , Y1 , Z1 ) loc(X2 , Y1 , Z1 ) X1 6= X2 tmax < Z1 ] suppose
|ID| > |Space|. std-formula states two different objects cannot location
time tmax. condition tmax < Z1 false substitutions Z1 ; hence integrity
constraint automatically true. enlarged 0 (> ), say t0 = tmax + 1,
779

fiPARISI & G RANT

inconsistency enough points Space objects occupy distinct
points. show given consistent K always extend = [0, . . . , tmax]
big = [0, . . . , tmax, . . . , tbig] K consistent big K remains consistent
0 0 = [0, . . . , tmax, . . . , t0 ]. Essentially, must make sure every time variable
substitution makes conjunct true f . example, previous example
modified
X1 X2 Y1 Z1 Z2 [loc(X1 , Y1 , Z1 ) loc(X2 , Y1 , Z2 ) X1 6= X2 tmax < Z1 Z2 = Z1 + 4]
must enough time values include tmax + 5. show systematically
proof next theorem.
Theorem 13. consistent PST KB K defined time values = [0, . . . , tmax], always finite time value tbig tmax computed linear time, K consistent
using big = [0, . . . , tmax, . . . , tbig] K consistent 0 = [0, . . . , tmax, . . . , t0 ].
Proof. obtain time value tf integrity constraint f F. f initialize
tf = tmax + 1. f contains one time variable Z done f . suppose f
least one conjunct two distinct time variables. must one following four
forms (where +0 omitted): 1) Zi + = Zj + n, 2) Zi + 6= Zj + n, 3) Zi + < Zj + n,
4) Zi + Zj + n. process involves adding certain number tf conjunct.
type 1) add |m n|, type 2) add 1, type 3) add max(0, n + 1), type 4) add
max(0, n m). Then, adding numbers conjuncts obtain tf value
f F. Let tbig = maxf F {tf }. process linear size F.
must show K consistent big consistent 0 = [0, . . . , tmax,
. . . , t0 ]. t0 tbig consistency follows Theorem 12b). assume tbig t0
K consistent big . means interpretation big model K.
proof Theorem 12, issue concerns constraints F. write W big

worlds using big write wb arbitrary world W big . f F,
P set big
(wb ) = 0. obtain 0 big arbitrary way long formula
b
b
Pw |w 6|=f 0 0
(w ) = big (wib ) satisfied. show 0 model K 0 , need
w0 Wi0
P
show w0 |w0 6|=f (w0 ) = 0. let wi0 W 0 wi0 6|= f . substitution
0 wi0 6|= 0 (f ). 0 may include time values 0 . construction assures
already substitution b using values big world wib
restriction wi0 big wib 6|= b (f ). Hence 0 model K 0 .
results allow us consider case arbitrarily large (finite) time.
Definition 19. call K eventually consistent (resp. inconsistent) time integer
L K consistent (resp. inconsistent) = [0, . . . , L, . . . , ].
Corollary 9. Every K either eventually consistent time eventually inconsistent time.
Proof. K inconsistent time Theorem 12a) eventually inconsistent time.
K consistent time two cases. K still consistent time big
computed proof Theorem 13 eventually consistent time. Otherwise
conclude Theorem 12a) eventually inconsistent time.
Consider results used query answering expand time
= [0, . . . , tmax] 0 = [0, . . . , tmax, . . . , t0 ]. order avoid confusion write K
780

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

time K0 time 0 . First, K inconsistent, according Theorem 12a),
K0 also inconsistent. consider case K consistent (recall queries evaluated
consistent PST KBs). Note check pair hid, ti answer Q
expansion 0 , considers also time values tmax < t0 . Using Theorem 10
obtain:
hid, ti optimistic answer Q w.r.t. K0 iff hA {loc(id, q, t)[`, u]}, Fi consistent
using 0 . need check separately K0 consistent K0 hA
{loc(id, q, t)[`, u]}, Fi.
check cautious answer, first check K0 consistent, so,
hid, ti cautious answer Q w.r.t. K0 iff hA {loc(id, q, t)[0, ` ]}, Fi hA
{loc(id, q, t)[u + , 1]}, Fi consistent using 0 , given Theorem 10.
similar vein, Theorem 11 carries also case extended time.
5.2 Extending Time Frequent Time Values
second type time extension consider frequent time values,
tmax + 1 time values 0 tmax. illustration, use 0 = [0, 0.5, 1, 1.5,
. . . , tmax]. Again, syntax must changed well include new constants 0.5, 1.5,
on; however, original K defined using = [0, 1, . . . , tmax] contain new
constant. situation worlds similar happened previous subsection: every
w : ID Space unique restriction many worlds, ws, w0 : ID 0 Space. Also,
satisfaction worlds W 0 using 0 defined way worlds W using .
first result type time extension previous one.
Theorem 14. Let K contain time values .
a) K inconsistent time , K remains inconsistent expanded 0 .
b) 0 expansion K consistent 0 consistent .
Proof. Analogous proof Theorem 12.
Next consider case K consistent (using ). Again, consistency need
preserved going 0 . Consider case following std-formula:
Y1 Z1 [loc(id1 , Y1 , Z1 ) Z1 > 0 Z1 < 1).
formula states id1 cannot location 0 1. condition 0 < Z1 Z1 <
1 false substitutions , hence integrity constraint true. expand 0
substitution 0.5 Z1 makes formula state id1 cannot location time 0.5,
making inconsistent. However, extend frequent time values f r K
consistent f r consistent subdivision original time values.
Theorem 15. consistent PST KB K defined time values = [0, . . . , tmax],
always integer value computed linear time, K consistent using
f r = [0, 1 , 2 , . . . , 1, +1
, . . . , tmax] K remains consistent every subdivision .
781

fiPARISI & G RANT

Proof. proof analogous proof Theorem 13. calculate integer f
integrity constraint f F let = maxf F {f }. idea make sure
enough time values allow (Z) become true subdivision time intervals
become true substitution. worst case every time variable may require new subdivision
time values. Hence choose f = 2k k number time variables f .
rest proof analogous proof Theorem 13.
previous section wanted consider arbitrarily large time values,
consider dividing time intervals arbitrarily many times.
Definition 20. call K divisionally consistent (resp. inconsistent) time integer
L K consistent (resp. inconsistent) = [0, 1 , 2 , . . . , tmax] > L.
Corollary 10. Every K either divisionally consistent time divisionally inconsistent time.
Proof. Analogous proof Corollary 9.
Results analogous time expansion done Section 5.1 hold also query answering
complexity.
5.3 Space Enlargement
subsection consider happens consistency inconsistency K Space
enlarged, say Space = {p1 , . . . , pn } Space0 = {p1 , . . . , pn , . . . , pv }. change
semantics different case expanded . w : ID Space world
(for Space), remains world Space0 . writing W set worlds using Space
W 0 set worlds using Space0 , obtain W W 0 . change definition
satisfaction, number interpretations becomes greatly enlarged. Still, every interpretation
using Space extended unique 0 using Space0 assigning 0 (w0 ) = 0 w0
W0 \ W.
case time, start case K inconsistent. unlike time,
Space enlarged K may become consistent. Consider simple example K consists
single atom: = loc(id1 , {p1 , . . . , pn }, 1)[.2, .7]. K inconsistent. add single point
Space Space0 = {p1 , . . . , pn , pn+1 } K becomes consistent: instance w10 world
w10 (id1 , 1) = p1 values arbitrary, w20 world w20 (id1 , 1) =
pn+1 values arbitrary, assigning 0 (w10 ) = 0.5 0 (w20 ) = 0.5 0 (w0 ) =
0 worlds makes 0 model. similar situation may occur integrity constraints.
Consider K contains single std-formula: f : X1 Y1 Z1 [loc(X1 , Y1 , Z1 )Y1 ov {p1 , . . . , pn }]
Since every region overlaps Space, f , itself, inconsistent. again, enlarge Space
one point pn+1 Space0 find model 0 follows. Let 0 (w0 ) = 0 every world
0
w0 w0 (id, t)
PSpace for0 hid, ti-pair, (w0 ) = 1 w0 (id, t) = pn+1
hid, ti pairs. w0 |w0 6|=f (w) = 0; hence inconsistency removed. However, suppose
addition f , K also contains atom = loc(id1 , {p1 }, 0)[1, 1] A, is, = {a}
F = {f }. PST atom states id1 must p1 time 0. case cannot make
K consistent adding number points Space f conflict. Hence
general statement happens inconsistent PST KB Space extended
Space0 .
782

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

Next consider case K consistent (using Space). case show
remains consistent Space expanded Space0 .
Theorem 16. K consistent Space, K remains consistent Space enlarged
Space0 .
Proof. Let model K (using Space). assigns probability (w) world w.
Space extended Space0 , many new worlds added. define 0 (w) = (w) w
world Space used 0 (w) = 0 otherwise. Thus basically excluding
worlds using points Space sense given probability 0, 0 model
K using Space0 . Hence consistency preserved.
Again, case time, interested happens arbitrarily large finite
spaces.
Definition 21. call K eventually consistent (resp. inconsistent) space integer
L K consistent (resp. inconsistent) Space = {p1 , . . . , pn , . . . , pL , . . . , p }.
Corollary 11. Every K either eventually consistent space eventually inconsistent space.
Proof. two cases. start first case K consistent Space. Then,
Theorem 16 remains consistent larger Space0 . Hence eventually consistent
space. Consider second case K inconsistent Space. two subcases.
First, suppose K becomes consistent larger Space0 . Then, first case,
Theorem 16 K eventually consistent space. second subcase K never becomes
consistent matter Space extended Space0 . means K eventually inconsistent
space.
showed earlier examples inconsistent K (with Space) may become consistent
Space enlarged. calculate bound size needed space
bound reached consistency inconsistency change spatial enlargement.
Theorem 17. every inconsistent K using Space = {p1 , . . . , pn }, explicit bound L
tractable compute K using Space0 = {p1 , . . . , pL } inconsistent, remains
inconsistent enlargement Space0 .
Proof. already know corollary L exists. show
compute it. First all, enlargement Space resolves inconsistency K K
eventually inconsistent space choose L = n. Thus need deal detail
case enlargement Space resolves inconsistency K, is, K
eventually consistent space. consider adding points Space make K consistent.
three cases:
1. inconsistency due alone,
2. inconsistency due combination elements F,
3. inconsistency due F only.
783

fiPARISI & G RANT

Consider first Case 1 inconsistency must due atoms fixed id
values. assume dealing atoms using specific pair values: id
t. Adding point points Space resolve inconsistency atoms using id
give Space probability less 1. instance, let = {loc(id, r1 , t, [0, 0.4]), loc(id, r2 , t, [0.1,
0.3])} r1 r2 = Space. Then, enlarging Space Space0 = Space {pn+1 } resolves
inconsistency. true general, example, pn+1 added
Space, {p1 , . . . , pn } becomes proper subset relevant Space0 may consistently
probability less 1. Now, consider may inconsistencies involving multiple pairs
id values. matter addition single point pn+1 Space
resolves inconsistencies. Case 1 choose L = n + 1.
Case 2 inconsistency K due combination elements F. every
atom refers specific region r specific object id time value t, inconsistency
occur f F, substitution f must act atom (with probability interval
[0, 0]). instance, let = loc(id, r1 , t, [0, 0.4]) f = X1 [loc(X1 , r2 , t)] r1 r2 =
Space. instance f causes inconsistency [loc(id, r2 , t)] effect
semantics atom loc(id, r2 , t, [0, 0]). true general, example.
way Case 2 reduces Case 1 choose L = n + 1.
last case, Case 3, inconsistency due F. problem case
due fact F requires points Space. need consider
expressed number spatial points std-formulas. cannot done writing many
spatial variables formula f spatial variables, say Y1 , . . . , Ym , way
express must refer different points. However, express Space
enough points writing f0 = X1 Y1 Z1 [loc(X1 , Y1 , Z1 ) Y1 ov Space]. Again, adding
single point Space resolves inconsistency. also write std-formulas require least
certain number points. express constraint point occupied
one object one time using following 3 std-formulas:
f1 = X1 Y1 Z1 Z2 [loc(X1 , Y1 , Z1 ) loc(X1 , Y1 , Z2 ) Z1 6= Z2 ]
f2 = X1 X2 Y1 Z1 [loc(X1 , Y1 , Z1 ) loc(X2 , Y1 , Z1 ) X1 6= X2 ]
f3 = X1 X2 Y1 Z1 Z2 [loc(X1 , Y1 , Z1 ) loc(X2 , Y1 , Z2 ) X1 6= X2 Z1 6= Z2 ]
f1 states object point two different times; f2 states time
two objects point; f3 states two different objects cannot point
two different times. Hence using std-formulas reference number objects times
number time points. Let L = n + |ID| (tmax + 1). Thus Space0 = {p1 , . . . , pL } consists
point object time point addition original n points Space,
place objects Space0 even none points Space used object
needs new point time point. Clearly, L > n + 1, L works Cases 1 2
well. value L choose tractable explicit bound. Thus K using
Space0 = {p1 , . . . , pL } still inconsistent, points added Space0 make consistent.
5.4 Extending Number Objects Several Entities
last case number constants may increased ID. Since world w
function w : ID Space, expansion objects similar expansion time,
784

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

expansion space. extension restriction interpretations going ID
larger ID0 analogous addition time values . Without getting details
analogous cases time given earlier, state K inconsistent ID, remains
inconsistent ID expanded ID0 . show consistency need preserved, consider f =
X1 X2 . . . Xm+1 [loc(X1 , Y1 , Z1 )loc(X2 , Y2 , Z2 ). . .loc(Xm+1 , Ym+1 , Zm+1 )X1 6= X2
X1 6= X3 . . . Xi 6= Xj . . . Xm 6= Xm+1 ] 6= j. Recalling ID = {id1 , . . . , idm }
find K = {f } consistent ID states cannot distinct
objects. becomes inconsistent ID enlarged ID0 = {id1 , . . . , idm , idm+1 }.
always find q linear time K consistent ID0 = {id1 , . . . , idm , . . . , idq }
K consistent enlargement ID0 . Obtaining value q easier obtaining
tbig time. matter counting number X variables f F
taking maximum value, bigger m. Hence define eventually consistent (resp.
inconsistent) objects way time, obtain result every K either
eventually consistent objects eventually inconsistent objects.
far considered individual enlargements either time values points space
objects. may interested also combining several types extensions. expansion
time objects similar, start combining them.
Definition 22. call K using ID, , Space eventually consistent (resp. inconsistent)
objects general time integers L1 , L2 , L3 K consistent (resp.
inconsistent) ID = {id1 , . . . , idm , . . . , idL1 , . . . , id }, Space = {p1 , . . . , pn }, =
tmax+1
[0, 1 , 2 , . . . , 1, +1
, . . . , . . . , ] > L2 > L3 .
, . . . tmax,

combination expansion objects time magnitude divisionally works
essentially way expansion one items. is, set
worlds original K W set worlds expansion ID W 0 ,
every world W set extensions W 0 every world W 0
unique world W restriction. key issue Space remains unchanged.
let K contain time values , regions Space objects ID. Suppose
expanded 0 expansion may magnitude division ID expanded
ID0 find K, context 0 ID0 Space unchanged, consistent.
proof Theorem 12 start model 0 K using 0 ID0 obtain corresponding
model K using starting ID. Thus, contrapositive, K inconsistent
, Space ID, remains inconsistent 0 , Space ID0 . shows inconsistent
K eventually inconsistent objects general time. case K consistent , Space
ID becomes inconsistent expansion 0 , Space ID0 , eventually
inconsistent objects general time. alternative K remain consistent
matter ID expanded. Hence earlier results put together following
result.
Theorem 18. Every PST KB K either eventually consistent objects general time
eventually inconsistent objects general time.
However, situation different cases combine space time objects
show using example.12
12. example suggested one reviewers.

785

fiPARISI & G RANT

Example 25. Consider PST KB K = h, {f }i defined using ID = {id1 , . . . , idm }, =
[0, 1, . . . , tmax], Space = {p1 , . . . , pn } n f std-formula f2 used
proof Theorem 17 states two different objects cannot point
time. condition n, K consistent. Expand ID ID0 =
{id1 , . . . , idm , . . . , idn+1 } leaving Space unchanged call new KB K0 .
many objects number points; hence K0 inconsistent. Next, expand Space
Space00 = {p1 , . . . , pn , pn+1 } leaving ID0 unchanged call new KB K00 . K00
consistent.
2
example shows cannot claim eventual (in)consistency number objects
number points may increase continue process indefinitely. analogous result holds case number time values number points may increase.

6. Related Work
first discuss related works classical probabilistic logic explicit spatial
temporal components. discuss relationship work spatio-temporal
approaches. Finally, relate framework object tracking.
6.1 Probabilistic Logic
discussed Section 3, PST KB expressed classical propositional logic (Hailperin,
1984; Nilsson, 1986; Paris, 1994), particular consistency checking problem
formulated terms Probabilistic Satisfiability (PSAT), whose first formulation attributed
Boole (1854). presentation AI community Nilsson (1986), study PSAT
point view efficient algorithms computational complexity first addressed
Georgakopoulos et al. (1988), showed PSAT NP NP-hard even binary
clauses. tractable results identified Georgakopoulos et al. concern special case
clause involves two literals (2PSAT) graph clauses outerplanar,13
graph clauses contains vertex literal two kinds edges: i) edge
pair literals built propositional variable, (ii) edge pair
literals appearing clause. note PSAT formula K encoding PST KB K
(see proof Theorem 1) contains two literals per clause even focus binary
std-formulas (K becomes 2PSAT formula assume Space consists two
points).
result provided Georgakopoulos et al. relies reducing 2PSAT tractable instance
2MAXSAT (weighted maximum satisfiability problem two literals per clause). Using reduction result Conforti Cornuejols (1992) tractability problems
formulated integer program whose matrix balanced, following general result
provided Conforti Cornuejols: PSAT tractable balanced set clauses, is,
set clauses whose corresponding {0, 1} clause-variable matrix balanced. However,
result also doesnt help finding tractable cases PST KBs K reduction PSAT K ,
considering three points Space suffices make matrix corresponding K balanced (it
entails presence odd cycle graph clauses that, observed Andersen & Pre13. graph said outerplanar embedded plane vertices lie face.
is, drawn plane without crossings way vertex totally surrounded edges.

786

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

tolani, 2001, characterizes non-balanced matrices). Motivated fact tractable cases
PSAT identified Georgakopoulos et al. Conforti Cornuejols rely using polynomial
time algorithm whose complexity characterized high polynomial degree (specifically,
complexity O(n6 log n) n number propositional variables), Andersen Pretolani identified efficient algorithms classes balanced sets clauses
represented either hypertrees (where hyperarc corresponds set literals clause)
co-occurrence graph partial k-tree (Bodlaender, 1998).
worth noting none tractable cases identified paper derived
results PSAT described reducing K PSAT formula K . fact, tractable
cases derive specific structure PST KBs. hand, tractability results
entail tractability PSAT instances reduced tractable instance consistency
checking problem. particular, decide polynomial time PSAT instances K
structure specified reduction consistency checking problem PSAT,
consistency checking problem corresponding PST KB K turns tractable.
principle, fact reduction consistency checking problem PSAT
enables well-known techniques solving (general) instances PSAT based column generation (Kavvadias & Papadimitriou, 1990; Jaumard et al., 1991) used addressing consistency PST KBs. holds problem answering selection queries PST
KBs that, shown Section 4, addressed solving suitable instances consistency
checking problem. Recent approaches solve PSAT using SAT (Finger & Bona, 2011) Integer
Linear Programming (Cozman & di Ianni, 2015) column selection report experiments showing
phase transition behaviour (first observed Finger Bona PSAT) depending fraction number clauses propositional variables well number probability
assignments. Using techniques PSAT instances hundreds propositional variables
clauses solved reasonable time. However, believe reducing consistency checking (or query answering) problem PSAT applying techniques would
successful approach, number propositional variables clauses would
generated would huge even small-size PST KBs. avoid problem, conjecture
specific structure (PSAT formulas encoding) PST KBs could exploited devise
efficient techniques solving consistency checking query answering problems.
regard, would interesting investigate connection framework emerging field lifted probabilistic inference (Kersting, 2012), structure FOL-constructs
(such indistinguishable individuals) exploited speed reasoning process, see
results carry PST KBs.
use integrity constraints encode domain knowledge studied Lukasiewicz
(1999, 2001) Flesca, Furfaro, Parisi (2014), probabilistic frameworks however
explicitly deal space time. problem probabilistic deduction presence
conditional constraints basic events addressed Lukasiewicz (1999), identified
tractable instances probabilistic KBs, whose conditional constraints define conditional constraint
trees, support deduction paths premise/conclusions basic events. problem checking
consistency relational probabilistic databases (where tuples viewed basic events)
presence denial constraints addressed Flesca et al., provided tractability results constraints whose conflict hypergraph (Chomicki, Marcinkowski, & Staworko, 2004) acyclic (Fagin, 1983) well special kind cyclic hypergraphs, encode neither
clique-acyclic std-graphs simple clique-cyclic std-graphs. important probabilistic logic pro787

fiPARISI & G RANT

gramming approach conditional constraints proposed Lukasiewicz (2001), studied
complexity satisfiability entailment problems several types formulas
without identifying tractable cases. Differently above-cited papers, atomic information
framework structure involving objects, space, time, thus atoms may also
intrinsically related object, space, time value.
6.2 Spatio-Temporal Approaches
Substantial work done spatio-temporal logics (Gabelaia et al., 2005; Knapp et al.,
2006) combine spatial temporal formalisms. includes important contributions
qualitative spatio-temporal representation reasoning (Muller, 1998; Wolter & Zakharyaschev,
2000; Cohn & Hazarika, 2001), focus describing entities qualitative relationships
dealing discrete time. Cohn, Li, Liu, Renz (2014) provided upto-date overview work done field qualitative spatial reasoning, recently
important problem combining topological directional information extended spatial objects
addressed. However, works intended reasoning moving objects
whose location given time uncertain (they put probabilities mix). Yaman
et al. (2004, 2005a, 2005b) focused spatio-temporal logical theories describe known plans
moving objects sets go atoms, stating object go location
L1 L2 , leaving L1 reaching L2 time intervals, travelling speed
given interval. Later, Parker et al. (2007b) extended logic include probabilistic
information plans. SPOT framework Parker et al. (2007a) extended
work uncertainty objects might given time.
Past work SPOT framework investigated efficient algorithms computing optimistic
cautious answers selection queries (Parker et al., 2009; Parisi et al., 2010). initial SPOT
framework build adding integrity constraints implemented tested
real US Navy databases containing ship location data (Parker et al., 2009; Parisi et al., 2010). Aggregate queries recently investigated Grant et al. (2013), proposed three semantics
along computational methods evaluating them. SPOT databases provide information
moving objects, one important aspect addressed Parker et al. (2008) investigated Grant et al. (2010) revising SPOT data information objects may
changed objects move. Grant et al. (2010) proposed several strategies revising SPOT data
finding maximal consistent subsets, minimally modifying spatial, temporal, object,
probability components PST atoms. full logic including negation, disjunction quantifiers
managing SPOT data recently proposed Doder et al. (2013), focused finding
sound complete sets axioms several fragments logic. Grant, Parisi, Subrahmanian (2013) provided comprehensive survey SPOT framework related research
also reviewed.
much work spatio-temporal databases (Agarwal et al., 2003; Pelanis et al.,
2006) probabilistic spatio-temporal databases (Tao et al., 2005; Zhang, Chen, Jensen, Ooi, &
Zhang, 2009; Zheng, Trajcevski, Zhou, & Scheuermann, 2011), works mainly focus devising indexing mechanisms scaling query computation, instead representing knowledge
declarative fashion. particular, Chung, Lee, Chen (2009) use indexing speed computation range queries derive PDF location object moving one-dimensional
space using past moving behavior moving velocity distribution. Zhang et al. (2009)
788

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

provide B x -tree index variant B + -tree applicable moving objects whose location
velocity uncertain. Two types pruning introduced Yang, Lu, Jensen (2010)
efficiently solve queries asking sets k objects least threshold probability
containing k nearest objects given object. Dealing similar problem, Chen, Qin,
Liu (2010) propose TPR-tree indexing. Finally, Zheng et al. (2011) deal primarily
objects moving along road networks, introduce indexing mechanism efficiently processing probabilistic range queries. However, none works systematically addresses issue
considering integrity constraints probabilistic spatio-temporal data.
6.3 Object Tracking
Object tracking one important problems computer vision (Szeliski, 2010)
consecutive positions tracked object estimated moves different frames
video. Numerous approaches object tracking proposed, mainly differing
type object representation used (e.g., centroid, primitive geometric shapes), image
features selected (e.g., colour, optical flow), object detection method adopted (e.g., background
subtraction, segmentation). However, tracking algorithm chosen given application
strongly depends application domain (Yilmaz, Javed, & Shah, 2006). Moreover, object
tracking algorithms may incur errors, due instance loss information caused projection
3D world 2D image, noise images, partial full object occlusions, estimation
position tracked moving objects inherently uncertain (even camera focuses fixed,
specific area). Several important statistical methods object tracking computer vision (e.g., see
Broida & Chellappa, 1986; Beymer & Konolige, 1999; Rosales & Sclaroff, 1999) based
well-known Kalman filter (1960) extensions deal non-linear case, well
particle filtering (Kitagawa, 1987).
Filtering techniques extensively used object tracking presence sensors,
cameras. matter fact, object tracking extensively addressed
general setting position (i.e., state) one objects estimated recursive
Bayesian filter given measurements time coming different kinds sensors
(including, instance, radar, sonar, infrared, types sensors possibly along visual
sensors) (Stone, Corwin, & Barlow, 1999). Basically, observed time point t, output
filter probability distribution (i.e., posterior) position target object,
computed combining motion updated time prior likelihood
observation received time t, likelihood represents probability sensor
measurement conditioned object position (Bar-Shalom, Kirubarajan, & Li, 2002).
Kalman filter used discrete-time estimation continuous spatial positions
objects whose movement equations assumed linear Gaussian noise. also
used successfully non-linear systems applying linearization unscented transformation (Julier, Jeffrey, & Uhlmann, 2004). discrete space non-linear systems, particle filtering
successfully used, providing solution applied state-space model
generalizes traditional Kalman filtering methods (Arulampalam, Maskell, Gordon, &
Clapp, 2002). general framework particle filtering based Sequential Importance Sampling Resampling proposed Liu Chen (1998), though number different types
particle filters exist shown outperform others used particular
applications (Arulampalam et al., 2002).
789

fiPARISI & G RANT

Differently Kalman filtering, estimated position object observed
time point represented continuous distribution, particle filters based histogram representation probability density, approximated finite number particles (i.e.,
samples): particle represents position space, weights associated particles (or
proportion number particles) define histogram probability distribution space.
fits representation paradigm PST KBs: PST KBs allow us represent,
object time point, PDF Space defining PST atom single valued probability
interval (that is, ` = u) point Space, used easily represent output object tracking techniques based particle filtering. filtering techniques returning continuous
distribution Space, discretization step applied.
output object tracking techniques represented using PST KBs,
important aspects techniques deal PST KBs cannot do. particular, filtering techniques use conditional independence represent PDF objects positions conditional
positions previous time. PST KBs encode output inference process,
lack expressive power kind inference. instance, tracking techniques
represent knowledge object region r1 time t1 probably region
r2 time t2 probability depending time elapsed t1 t2 . Indeed trackers
rely motion model according distribution objects location spreads
elapsed time since last measurement: distribution objects locations
narrowly focused locations near measured position t1 t2 close t1 , diffuse
t2 faraway t1 . tracking techniques sort things quite naturally, PST KBs
capture aspects behavior. instance, express fact object
probably region r1 t1 , use integrity constraint imposing travel
distance units 1 time point. would increase probability finding object
time t2 region less units away r1 , would decrease probability finding
object region farther away units. However, different inferred
tracking techniques using conditional independence.
hand, tracking techniques combine well interval probabilities.
fact typically return PDF objects position observed time point. contrast, using general PST atoms (with probability intervals), object time point,
(possibly infinite number) PDFs compatible probability intervals specified
atoms succinctly represented. instance, assuming Space = {p1 , p2 } PST atom
loc(id, {p1 }, t)[0, 0.5], PDFs f Space assigning probability f (p1 ) [0, 0.5]
f (p2 ) = 1 f (p1 ) represented. However, set PDFs represented restricted using
single valued probability intervals adding integrity constraints using std-formulas.
note PST formalism allows us impose integrity constraints KBs
obtained integrating position data coming different sources. Consider instance
integration several PST KBs, consisting PST atoms encoding output
autonomous tracking system. so-obtained integrated KB still PST KB, integrity
constraints used express knowledge overall system, could expressed
considering tracking systems separately. instance, suppose integrated PST KB
consisting position data monitored cars collected using black-box tracking systems
installed cars insurance companies. std-formulas used express correlations
among monitored cars. instance, knowing region r licensed inspection station
able inspect k cars time, impose constraint cannot
790

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

k cars r time. constraint would meaningless considered KB
car tracking system separately, useful restricting set consistent interpretations
PST KB obtained integrating several sources.

7. Summary
believe first comprehensive paper focuses systematically knowledge representation form integrity constraints probabilistic spatio-temporal data.14 knowledge
represented form spatio-temporal atoms describing location objects time
probability interval well spatio-temporal denial formulas describing integrity constraints system must satisfy. Within framework investigated consistency checking
problem well problem answering selection queries consistent PST KBs. Although
problems turned hard general case, devised several sets linear
inequalities allow us decide consistency well answer queries checking feasibility. addition, identified different classes spatio-temporal denial formulas
checking consistency answering queries tractable. Finally, discussed extension
framework arbitrarily large finite numbers objects, time values, points space
showed behavior consistency inconsistency uniform.

8. Future Work Conclusion
issues investigated. Following Parisi Grant (2014b),
studied problem restoring consistency PST KBs form KhA, (where set
std-formulas empty), consider problem repairing inconsistent PST KB
inconsistency due presence std-formulas satisfied. regard would
interesting devise methods answering queries inconsistent PST KBs. Recently
research probabilistic reasoning inconsistency (Picado-Muino, 2011; Thimm,
2013; Potyka & Thimm, 2014) help regard. would also interesting look
possibility semantic query optimization PST KBs, study use previous
knowledge efficiently check consistency process queries updates.
Another direction future work investigation probabilistic std-formulas expressing
constraints hold probability given interval. Intuitively, would allow us state
instance two objects region time probability greater
given threshold, instead stating cannot situation. kind
probabilistic constraint could expressed using pstd-formulas form f [`, 1] f
std-formula (i.e., f [1, 1] captures meaning std-formula). change semantics
means changing
definition model. second part Definition 6 modified
P
f F, w|w|=f (w) [`, 1]. Clearly, lower bounds complexity consistency checking
query answering problems still hold extension. easy check upper bound
provided Theorem 1 holds since reduction PSAT still provided mapping pstdformulas clauses associated probability interval. regards tractable cases, conjecture
results Theorems 6 8 still hold pstd-formulas form f [`, 1] right-hand
side inequality (4) Definitions 15 17, respectively, replaced lower probability
bound ` ground pstd-formula f [`, 1] generating inequality. However, allowing general
14. substantially revised expanded version work Parisi Grant (2014a).

791

fiPARISI & G RANT

probability intervals associated std-formulas introduces new issues semantics: f [`, u]
would entail f [1 u, 1 `] holds. Providing clear intuitive semantics kind
std-formula, well probabilistic std-formulas allowing probability intervals associated
conjunct (instead whole formula), deferred future work.
PST formalism propositional even though atoms substantial content.
added std-formulas integrity constraints; special class first-order logic formulas. would interesting consider works general first-order probabilistic logics (Halpern, 1990; Lukasiewicz & Kern-Isberner, 1999; Kern-Isberner & Thimm, 2010).
logics developed different purpose attempt could made represent spatiotemporal information them. works well could enhanced spatiotemporal information, Markov Logic (Richardson & Domingos, 2006), Bayesian Logic
Programs (Kersting & Raedt, 2007). particular, Milch et al. (2005) introduces first-order language called BLOG (Bayesian LOGic) defining probability models worlds unknown
objects identity uncertainty, finds natural application object tracking unknown
objects. may possible find generalization PST formalism includes
concepts. so, aspect need take care fact Markov logic
Bayesian logic programs deal unique probability distribution, deal
probability distributions compatible PST atoms std-formulas.
Researchers AI studying spatial temporal reasoning many years (Allen,
1984; Randell, Cui, & Cohn, 1992; Galton, 2009). interesting project incorporation
concepts PST framework. new syntax semantics include adding rules
language. additions allow adding types information well new
integrity constraints. instance, using concepts qualitative direction orientation proposed spatial reasoning (Galton, 2009) would allow us explicitly represent knowledge
region toward object moving. Another important concept needed many applications explicit representation qualitative quantitative distance objects well
information speed (i.e., maximum average) objects. Additional structured information objects type (e.g., vessel, vehicle, person, etc.) would general useful
exhaustively reason moving objects. However, depending addition made increase
expressive power extended framework, important consequences complexity
consistency checking problem may arise. Spatial temporal aspects formalisms qualitative
spatial temporal reasoning expressive framework (Gabelaia
et al., 2005; Knapp et al., 2006). trade-off expressiveness complexity within
hierarchy formalisms obtained combining well-known spatial temporal logics analyzed
Gabelaia et al. (2005), shown complexity satisfiability problem
spatio-temporal logics (not dealing probabilities) vary NP-complete undecidable.
Using formalisms PST framework may drastically increase computational complexity problems studied paper. Nevertheless, believe attempt
later made include even simpler concepts qualitative spatio-temporal reasoning PST
framework, particularly trying exploit restrictions recently studied Huang, Li, Renz (2013)
identify tractable fragments.
paper, proposed framework four features moving objects taken
account: spatial component, temporal component, inherent uncertainty acquired data,
integrity constraints application domain. expressiveness features could
improved represent additional knowledge may interest practical applications, partic792

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

ularly using ideas papers dealing spatio-temporal reasoning. endeavor concrete
implementation framework, ideas PostGIS spatial database system extends
PostgreSQL would useful. believe worthwhile later incorporate concepts
PST framework. took first step extending PST framework addition
integrity constraints, hope researchers use work starting point investigations
important role integrity constraints probabilistic spatio-temporal knowledge bases.

Acknowledgments
wish thank referees numerous helpful comments helped us substantially improving paper.

References
Agarwal, P. K., Arge, L., & Erickson, J. (2003). Indexing moving points. J. Comput. Syst. Sci.,
66(1), 207243.
Ahson, S. A., & Ilyas, M. (2010). Location-Based Services Handbook: Applications, Technologies,
Security. CRC Press, Hoboken, NJ.
Akdere, M., Cetintemel, U., Riondato, M., Upfal, E., & Zdonik, S. B. (2011). case predictive database systems: Opportunities challenges. Proceedings 5th Biennial
Conference Innovative Data Systems Research (CIDR), pp. 167174.
Allen, J. F. (1984). Towards general theory action time. Artif. Intell., 23(2), 123154.
Andersen, K. A., & Pretolani, D. (2001). Easy cases probabilistic satisfiability. Ann. Math. Artif.
Intell., 33(1), 6991.
Arulampalam, M. S., Maskell, S., Gordon, N. J., & Clapp, T. (2002). tutorial particle filters
online nonlinear/non-gaussian bayesian tracking. IEEE Transactions Signal Processing,
50(2), 174188.
Bar-Shalom, Y., Kirubarajan, T., & Li, X.-R. (2002). Estimation Applications Tracking
Navigation. John Wiley & Sons, Inc., New York, NY, USA.
Bayir, M. A., Demirbas, M., & Eagle, N. (2010). Mobility profiler: framework discovering
mobility profiles cell phone users. Pervasive Mobile Computing, 6(4), 435 454.
Beymer, D., & Konolige, K. (1999). Real-time tracking multiple people using continuous detection. Proceedings Workshop Frame-rate Applications, Methods Experiences
Regularly Available Technology Equipment (FRAME-RATE), conjunction
7th IEEE International Conference Computer Vision (ICCV).
Bodlaender, H. L. (1998). partial k-arboretum graphs bounded treewidth. Theor. Comput.
Sci., 209(1-2), 145.
Boole, G. (1854). Investigation Laws Thought Founded Mathematical
Theories Logic Probabilities. Macmillan, London.
Broida, T. J., & Chellappa, R. (1986). Estimation object motion parameters noisy images.
IEEE Trans. Pattern Anal. Mach. Intell., 8(1), 9099.
793

fiPARISI & G RANT

Chen, Y.-F., Qin, X.-L., & Liu, L. (2010). Uncertain distance-based range queries uncertain
moving objects. J. Comput. Sci. Technol., 25(5), 982998.
Chomicki, J., Marcinkowski, J., & Staworko, S. (2004). Computing consistent query answers using
conflict hypergraphs. Proceedings 2004 ACM CIKM International Conference
Information Knowledge Management (CIKM), pp. 417426.
Chung, B. S. E., Lee, W.-C., & Chen, A. L. P. (2009). Processing probabilistic spatio-temporal
range queries moving objects uncertainty. Proceedings 12th International
Conference Extending Database Technology (EDBT), pp. 6071.
Cohn, A. G., & Hazarika, S. M. (2001). Qualitative spatial representation reasoning:
overview. Fundam. Inform., 46(1-2), 129.
Cohn, A. G., Li, S., Liu, W., & Renz, J. (2014). Reasoning topological cardinal direction
relations 2-dimensional spatial objects. J. Artif. Intell. Res. (JAIR), 51, 493532.
Conforti, M., & Cornuejols, G. (1992). class logic problems solvable linear programming.
Proceedings 33rd Annual Symposium Foundations Computer Science (FOCS),
pp. 670675.
Cozman, F. G., & di Ianni, L. F. (2015). Probabilistic satisfiability coherence checking
integer programming. Int. J. Approx. Reasoning, 58, 5770.
Doder, D., Grant, J., & Ognjanovic, Z. (2013). Probabilistic logics objects located space
time. J. Logic Computation, 23(3), 487515.
Fagin, R. (1983). Degrees acyclicity hypergraphs relational database schemes. Journal
ACM, 30(3).
Finger, M., & Bona, G. D. (2011). Probabilistic satisfiability: Logic-based algorithms phase
transition. Proceedings 22nd International Joint Conference Artificial Intelligence
(IJCAI), pp. 528533.
Flesca, S., Furfaro, F., & Parisi, F. (2014). Consistency checking querying probabilistic
databases integrity constraints. J. Comput. Syst. Sci., 80(7), 14481489.
Gabelaia, D., Kontchakov, R., Kurucz, A., Wolter, F., & Zakharyaschev, M. (2005). Combining
spatial temporal logics: Expressiveness vs. complexity. J. Artif. Intell. Res., 23, 167243.
Galton, A. (2009). Spatial temporal knowledge representation. Earth Science Informatics, 2(3),
169187.
Georgakopoulos, G. F., Kavvadias, D. J., & Papadimitriou, C. H. (1988). Probabilistic satisfiability.
J. Complexity, 4(1), 111.
Grant, J., Molinaro, C., & Parisi, F. (2013). Aggregate count queries probabilistic spatio-temporal
databases. Proceedings 7th International Conference Scalable Uncertainty Management (SUM), pp. 255268.
Grant, J., Parisi, F., Parker, A., & Subrahmanian, V. S. (2010). agm-style belief revision mechanism probabilistic spatio-temporal logics. Artif. Intell., 174(1), 72104.
Grant, J., Parisi, F., & Subrahmanian, V. S. (2013). Research probabilistic spatiotemporal
databases: SPOT framework. Advances Probabilistic Databases Uncertain
Information Management, Vol. 304 Studies Fuzziness Soft Computing, pp. 122.
Springer.
794

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

Hailperin, T. (1984). Probability logic. Notre Dame Journal Formal Logic, 25(3), 198212.
Halpern, J. Y. (1990). analysis first-order logics probability. Artif. Intell., 46(3), 311350.
Hammel, T., Rogers, T. J., & Yetso, B. (2003). Fusing live sensor data situational multimedia
views. Proceedings 9th International Workshop Multimedia Information Systems
(MIS), pp. 145156.
Huang, J., Li, J. J., & Renz, J. (2013). Decomposition tractability qualitative spatial
temporal reasoning. Artif. Intell., 195, 140164.
Jaumard, B., Hansen, P., & de Aragao, M. P. (1991). Column generation methods probabilistic
logic. ORSA Journal Computing, 3(2), 135148.
Julier, S. J., Jeffrey, & Uhlmann, K. (2004). Unscented filtering nonlinear estimation. Proceedings IEEE, 92, 401422.
Junger, M., Liebling, T., Naddef, D., Nemhauser, G., Pulleyblank, W., Reinelt, G., Rinaldi, G., &
Wolsey, L. (Eds.). (2010). 50 Years Integer Programming 1958-2008: Early Years
State-of-the-Art. Springer, Heidelberg.
Kalman, R. E. (1960). new approach linear filtering prediction problems. Transactions
ASMEJournal Basic Engineering, 82(Series D), 3545.
Karbassi, A., & Barth, M. (2003). Vehicle route prediction time arrival estimation techniques
improved transportation system management. Proceedings 2013 IEEE Intelligent
Vehicles Symposium, pp. 511516.
Karimi, H. A. (2013). Advanced location-based technologies services. CRC Press, Hoboken,
NJ.
Kavvadias, D. J., & Papadimitriou, C. H. (1990). linear programming approach reasoning
probabilities. Ann. Math. Artif. Intell., 1, 189205.
Kern-Isberner, G., & Thimm, M. (2010). Novel semantical approaches relational probabilistic
conditionals. Proceedings 12th International Conference Principles Knowledge
Representation Reasoning (KR).
Kersting, K., & Raedt, L. D. (2007). Bayesian logic programming: Theory tool. Getoor, L.,
& Taskar, B. (Eds.), Introduction Statistical Relational Learning. MIT Press.
Kersting, K. (2012). Lifted probabilistic inference. Proceedings 20th European Conference
Artificial Intelligence (ECAI), pp. 3338.
Kitagawa, G. (1987). Non-gaussian state-space modeling nonstationary time series. Journal
American Statistical Association, 82(400), 10321041.
Knapp, A., Merz, S., Wirsing, M., & Zappe, J. (2006). Specification refinement mobile
systems mtla mobile uml. Theor. Comput. Sci., 351(2), 184202.
Kurkovsky, S., & Harihar, K. (2006). Using ubiquitous computing interactive mobile marketing.
Personal Ubiquitous Comput., 10(4), 227240.
Li, S. Z., & Jain, A. K. (Eds.). (2011). Handbook Face Recognition, 2nd Edition. Springer.
Liu, J. S., & Chen, R. (1998). Sequential monte carlo methods dynamic systems. Journal
American Statistical Association, 93, 10321044.
795

fiPARISI & G RANT

Lukasiewicz, T. (2001). Probabilistic logic programming conditional constraints. ACM Trans.
Computational Logic, 2(3), 289339.
Lukasiewicz, T. (1999). Probabilistic deduction conditional constraints basic events. J.
Artif. Intell. Res. (JAIR), 10, 199241.
Lukasiewicz, T., & Kern-Isberner, G. (1999). Probalilistic logic programming maximum
entropy. Proceedings 5th European Conference Symbolic Quantitative Approaches Reasoning Uncertainty (ECSQARU), pp. 279292.
MarketsandMarkets (2014). Location Based Services (LBS) Market (Mapping, Discovery
Infotainment, Location Analytics, Leisure Social Networking, Location Based Advertising, Augmented Reality Gaming, Tracking) - Worldwide Forecasts Analysis (2014 - 2019). http://www.marketsandmarkets.com/Market-Reports/
location-based-service-market-96994431.html.
Milch, B., Marthi, B., Russell, S. J., Sontag, D., Ong, D. L., & Kolobov, A. (2005). BLOG: probabilistic models unknown objects. Proceedings 19th International Joint Conference Artificial Intelligence (IJCAI), pp. 13521359.
Mittu, R., & Ross, R. (2003). Building upon coalitions agent experiment (coax) - integration
multimedia information gccs-m using impact. Proceedings 9th International
Workshop Multimedia Information Systems (MIS), pp. 3544.
Muller, P. (1998). qualitative theory motion based spatio-temporal primitives. Proceedings 6th International Conference Principles Knowledge Representation
Reasoning (KR), pp. 131143.
Nilsson, N. J. (1986). Probabilistic logic. Artif. Intell., 28(1), 7187.
Papadimitriou, C. H., & Steiglitz, K. (1982). Combinatorial optimization: algorithms complexity. Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
Papadimitriou, C. M. (1994). Computational complexity. Addison-Wesley, Reading, Massachusetts.
Paris, J. (1994). Uncertain Reasoners Companion: Mathematical Perspective. Cambridge
University Press.
Parisi, F., & Grant, J. (2014a). Integrity constraints probabilistic spatio-temporal knowledgebases. Proceedings 8th International Conference Scalable Uncertainty Management (SUM), pp. 251264.
Parisi, F., & Grant, J. (2014b). Repairs consistent answers inconsistent probabilistic spatiotemporal databases. Proceedings 8th International Conference Scalable Uncertainty Management (SUM), pp. 265279.
Parisi, F., Parker, A., Grant, J., & Subrahmanian, V. S. (2010). Scaling cautious selection spatial
probabilistic temporal databases. Methods Handling Imperfect Spatial Information,
Vol. 256 Studies Fuzziness Soft Computing, pp. 307340. Springer.
Parisi, F., Sliva, A., & Subrahmanian, V. S. (2013). temporal database forecasting algebra. Int. J.
Approximate Reasoning, 54(7), 827860.
Parker, A., Infantes, G., Grant, J., & Subrahmanian, V. S. (2009). SPOT databases: Efficient consistency checking optimistic selection probabilistic spatial databases. IEEE Transactions
Knowledge Data Engineering (TKDE), 21(1), 92107.
796

fiK NOWLEDGE R EPRESENTATION P ROBABILISTIC PATIO -T EMPORAL K NOWLEDGE BASES

Parker, A., Infantes, G., Subrahmanian, V. S., & Grant, J. (2008). AGM-based belief revision
mechanism probabilistic spatio-temporal logics. Proceedings 23rd AAAI Conference Artificial Intelligence (AAAI), pp. 511516.
Parker, A., Subrahmanian, V. S., & Grant, J. (2007a). logical formulation probabilistic spatial
databases. IEEE Transactions Knowledge Data Engineering (TKDE), 19(11), 1541
1556.
Parker, A., Yaman, F., Nau, D. S., & Subrahmanian, V. S. (2007b). Probabilistic go theories.
Proceedings 20th International Joint Conference Artificial Intelligence (IJCAI), pp.
501506.
Pelanis, M., Saltenis, S., & Jensen, C. S. (2006). Indexing past, present, anticipated future
positions moving objects. ACM Trans. Database Syst., 31(1), 255298.
Petrova, K., & Wang, B. (2011). Location-based services deployment demand: aroadmap
model. Electronic Commerce Research, 11(1), 529.
Picado-Muino, D. (2011). Measuring repairing inconsistency probabilistic knowledge bases.
Int. J. Approx. Reasoning, 52(6), 828840.
Potyka, N., & Thimm, M. (2014). Consolidation probabilistic knowledge bases inconsistency
minimization. Proceedings 21st European Conference Artificial Intelligence
(ECAI), pp. 729734.
Randell, D. A., Cui, Z., & Cohn, A. G. (1992). spatial logic based regions connection.
Proceedings 3rd International Conference Principles Knowledge Representation
Reasoning (KR), pp. 165176.
Richardson, M., & Domingos, P. (2006). Markov logic networks. Mach. Learn., 62(1-2), 107136.
Rosales, R., & Sclaroff, S. (1999). 3D trajectory recovery tracking multiple objects trajectory
guided recognition actions. Proceedings 6th Conference Computer Vision
Pattern Recognition (CVPR, pp. 21172123.
Southey, F., Loh, W., & Wilkinson, D. F. (2007). Inferring complex agent motions partial trajectory observations. Proceedings 20th International Joint Conference Artificial
Intelligence (IJCAI), pp. 26312637.
Stone, L. D., Corwin, T. L., & Barlow, C. A. (1999). Bayesian Multiple Target Tracking (1st edition).
Artech House, Inc., Norwood, MA, USA.
Szeliski, R. (2010). Computer Vision: Algorithms Applications. Springer-Verlag New York,
Inc., New York, NY, USA.
Tao, Y., Cheng, R., Xiao, X., Ngai, W. K., Kao, B., & Prabhakar, S. (2005). Indexing multidimensional uncertain data arbitrary probability density functions. Proceedings
31st International Conference Large Data Bases (VLDB), pp. 922933.
Thimm, M. (2013). Inconsistency measures probabilistic logics. Artif. Intell., 197, 124.
Wolter, F., & Zakharyaschev, M. (2000). Spatio-temporal representation reasoning based
rcc-8. Proceedings 7th International Conference Principles Knowledge Representation Reasoning (KR), pp. 314.
797

fiPARISI & G RANT

Yaman, F., Nau, D. S., & Subrahmanian, V. S. (2004). logic motion. Proceedings 9th
International Conference Principles Knowledge Representation Reasoning (KR),
pp. 8594.
Yaman, F., Nau, D. S., & Subrahmanian, V. S. (2005a). Going far, logically. Proceedings
19th International Joint Conference Artificial Intelligence (IJCAI), pp. 615620.
Yaman, F., Nau, D. S., & Subrahmanian, V. (2005b). motion closed world assumption. Proceedings 19th International Joint Conference Artificial Intelligence (IJCAI), pp.
621626.
Yang, B., Lu, H., & Jensen, C. S. (2010). Probabilistic threshold k nearest neighbor queries
moving objects symbolic indoor space. Proceedings 13th International Conference Extending Database Technology (EDBT), pp. 335346.
Yilmaz, A., Javed, O., & Shah, M. (2006). Object tracking: survey. ACM Comput. Surv., 38(4).
Zhang, M., Chen, S., Jensen, C. S., Ooi, B. C., & Zhang, Z. (2009). Effectively indexing uncertain
moving objects predictive queries. Proceedings VLDB Endowment (PVLDB), 2(1),
11981209.
Zheng, K., Trajcevski, G., Zhou, X., & Scheuermann, P. (2011). Probabilistic range queries
uncertain trajectories road networks. Proceedings 14th International Conference
Extending Database Technology (EDBT), pp. 283294.

798

fiJournal Artificial Intelligence Research 55 (2016) 283-316

Submitted 03/24; published 01/16

News Across Languages - Cross-Lingual Document
Similarity Event Tracking
Jan Rupnik
Andrej Muhic
Gregor Leban
Primoz Skraba
Blaz Fortuna
Marko Grobelnik

jan.rupnik@ijs.si
andrej.muhic@ijs.si
gregor.leban@ijs.si
primoz.skraba@ijs.si
blaz.fortuna@ijs.si
marko.grobelnik@ijs.si

Artificial Intelligence Laboratory, Jozef Stefan Institute,
Jamova cesta 39, 1000 Ljubljana, Slovenia

Abstract
todays world, follow news distributed globally. Significant events
reported different sources different languages. work, address
problem tracking events large multilingual stream. Within recently developed
system Event Registry examine two aspects problem: compare articles
different languages link collections articles different languages refer
event. Taking multilingual stream clusters articles language,
compare different cross-lingual document similarity measures based Wikipedia.
allows us compute similarity two articles regardless language. Building
previous work, show methods scale well compute meaningful
similarity articles languages little direct overlap training
data. Using capability, propose approach link clusters articles across
languages represent event. provide extensive evaluation
system whole, well evaluation quality robustness similarity
measure linking algorithm.

1. Introduction
Content Internet becoming increasingly multilingual. prime example Wikipedia. 2001, majority pages written English, 2015, percentage
English articles dropped 14%. time, online news begun dominate reporting current events. However, machine translation remains relatively rudimentary. allows people understand simple phrases web pages, remains inadequate
advanced understanding text. paper consider intersection
developments: track events reported multiple languages.
term event vague ambiguous, practical purposes, define
significant happening reported media. Examples events
would include shooting Malaysia Airlines plane Ukraine July 18th, 2014
(see Figure 1) HSBCs admittance aiding clients tax evasion February
9th, 2015. Events covered many articles question find
articles different languages describing single event. Generally, events
c
2016
AI Access Foundation. rights reserved.

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

Figure 1: Events represented collections articles event, case
Malaysian airliner shot Ukraine. results shown
figure obtained using query http://eventregistry.org/event/
997350#?lang=eng&tab=articles. content presented part Event
Registry system, developed authors.

specific general themes time component plays important role
example, two wars Iraq would considered separate events.
input, consider stream articles different languages list events.
goal assign articles corresponding events. priori, know
coverage articles, is, events may covered know
articles necessarily fit one events. task divided two parts:
detecting events within language linking events across languages.
paper address second step.
consider high volume articles different languages. using language
detector, stream split separate monolingual streams. Within monolingual
284

fiCross-Lingual Document Similarity Event Tracking

stream, online clustering approach employed, tracked clusters correspond
definition events - based Event Registry system (Leban, Fortuna,
Brank, & Grobelnik, 2014b, 2014a). main goal paper connect clusters
(representations events) across languages, is, detect set articles
language reports event set articles language B.
approach link clusters across languages combines two ingredients: cross-lingual
document similarity measure, interpreted language independent topic
model, semantic annotation documents, enables alternative way comparing documents. Since work represents complicated pipeline, concentrate
two specific elements. Overall, approach considered systems
perspective (considering system whole) rather considering problems
isolation.
first ingredient approach link clusters across languages represents continuation previous work (Rupnik, Muhic, & Skraba, 2011a, 2012, 2011b; Muhic, Rupnik,
& Skraba, 2012) explored representations documents valid
multiple languages. representations could interpreted multilingual topics,
used proxies compute cross-lingual similarities documents. learn
representations, use Wikipedia training corpus. Significantly,
consider major hub languages English, German, French, etc.
significant overlap article coverage, also smaller languages (in terms number
Wikipedia articles) Slovenian Hindi, may negligible overlap
article coverage. define similarity two articles regardless language, allows us cluster articles according topic. underlying assumption
articles describing event similar therefore put
cluster.
Using similarity function, propose novel algorithm linking events/clusters
across languages. pose task classification problem based several sets
features. addition features, cross-lingual similarity also used quickly identify
small list potential linking candidates cluster. greatly increases
scalability system.
paper organized follows: first provide overview system whole
Section 2, includes subsection summarizes main system requirements.
present related work Section 3. related work covers work cross-lingual
document similarity well work cross-lingual cluster linking. Section 4, introduce problem cross-lingual document similarity computation describe several
approaches problem, notably new approach based hub languages. Section 5, introduce central problem cross-lingual linking clusters news articles
approach combines cross-lingual similarity functions knowledge extraction based techniques. Finally, present interpret experimental results
Section 6 discuss conclusions point several promising future directions.

2. Pipeline
base techniques cross-lingual event linking online system detection
world events, called Event Registry (Leban et al., 2014b, 2014a). Event Registry
285

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

Data
collec-on

Main-
stream
news

Ar-cle-level
processing

Event
construc-on

Seman&c
annota&on

Ar&cle clustering

Extrac&on
date references

Event forma&on

Manual event
administra&on

Event info.
extrac&on

Detec&on
ar&cle loca&on

Cross-lingual
cluster
matching

Cross-lingual
ar-cle matching

Detec&on
ar&cle duplicates

Event storage &
maintenance

Filling event
template
Iden&fying
related events

Frontend
interface

API
access

Figure 2: Event Registry pipeline. new articles collected, first analyzed individually (Article-level processing). next step, groups articles
event identified relevant information event
extracted (Event construction phase). Although pipeline contains several
components, focus two highlighted image.

repository events, events automatically identified analyzing news articles
collected numerous news outlets world. important components
pipeline Event Registry shown Figure 2. briefly describe
main components.
collection news articles performed using Newsfeed service (Trampus
& Novak, 2012). service monitors RSS feeds around 100,000 mainstream news
outlets available globally. Whenever new article detected RSS feed, service
downloads available information article sends article
pipeline. Newsfeed downloads daily average around 200,000 news articles various
languages, English, Spanish German common.
Collected articles first semantically annotated identifying mentions relevant
concepts either entities important keywords. disambiguation entity linking
concepts done using Wikipedia main knowledge base. algorithm
semantic annotation uses machine learning detect significant terms within unstructured
text link appropriate Wikipedia articles. approach models link probability combines prior word sense distributions context based sense distributions.
286

fiCross-Lingual Document Similarity Event Tracking

details reported Milne Witten (2008) Zhang Rettinger (2014a).
part semantic annotation also analyze dateline article identify
location described event well extract dates mentioned article using
set regular expressions. Since articles frequently revised also detect collected
article revision previous one use information next phases
pipeline. last important processing step document level efficiently
identify articles available languages similar article.
methodology task one main contributions paper explained
details Section 4.
next step, online clustering algorithm (Brank, Leban, & Grobelnik, 2014)
applied articles order identify groups articles discussing
event. new article, clustering algorithm determines article
assigned existing cluster new cluster. underlying assumption
articles describing event similar enough therefore put
cluster. clustering, new article first tokenized, stop words
removed remaining words stemmed. remaining tokens represented
vector-space model normalized using TF-IDF1 (see Section 4.1 definition).
Cosine similarity used find similar existing cluster, comparing documents vector centroid vector cluster. user-defined threshold used
determine article similar enough existing clusters (0.4 used
experiments). highest similarity threshold, article assigned
corresponding cluster, otherwise new cluster created, initially containing single
article. Whenever article assigned cluster, clusters centroid vector also
updated. Since articles event commonly written short period
time, remove clusters oldest article cluster becomes 4 days
old. housekeeping mechanism prevents clustering becoming slow also
ensures articles assigned obsolete clusters.
number articles cluster reaches threshold (which language dependent parameter), assume articles cluster describing event.
point, new event unique ID created Event Registry, cluster
articles assigned it. analyzing articles, extract main information
event, event location, date, relevant entities keywords, etc.
Since articles cluster single language, also want identify
existing clusters report event languages join clusters
event. task performed using classification approach
second major contribution paper. described detail Section 5.
cluster identified information event extracted, available
data stored custom-built database system. data accessible
API web interface (http://eventregistry.org/), provide numerous search
visualization options.

1. IDF weights dynamically computed new article news articles within 10 day
window.

287

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

2.1 System Requirements
goal build system monitors global media analyzes events
reported on. approach consists two steps: tracking events separately language
(based language detection online clustering approach) connecting them.
pipeline must able process millions articles per day perform billions
similarity computations day. steps rely heavily similarity computation,
therefore highly scalable.
Therefore, focus implementations run single shared memory machine,
opposed clusters machines. simplifies implementation system maintenance.
summarize, following properties desirable:
Training - training (building cross-lingual models) scale many languages robust quality training resources. system
able take advantage comparable corpora (as opposed parallel translationbased corpora), missing data.
Operation efficiency - similarity computation fast - system must
able handle billions similarity computations per day. Computing similarity
new document set known documents efficient (the main
application linking documents two monolingual streams).
Operation cost - system run strong shared machine server
rely paid services.
Implementation - system straightforward implement, parameters
tune.
believe cross-lingual similarity component meets requirements
desirable commercial setting, several different costs taken consideration.

3. Related Work
section, describe previous work described literature. Since two
distinctive tasks tackle paper (computing cross-lingual document similarity
cross-lingual cluster linking), separated related work two corresponding
parts.
3.1 Cross-Lingual Document Similarity
four main families approaches cross-lingual similarity.
3.1.1 Translation Dictionary Based Approaches
obvious way compare documents written different languages use machine
translation perform monolingual similarity (see Peters & Braschler, 2012; Potthast,
Barron-Cedeno, Stein, & Rosso, 2011). One use free tools Moses (Koehn et al.,
2007) translation services, Google Translate (https://translate.google.com/).
288

fiCross-Lingual Document Similarity Event Tracking

two issues approaches: solve harder problem needs
solved less robust training resource quality - large sets translated sentences typically needed. Training Moses languages scarce linguistic resources
thus problematic. issue using online services Google Translate
APIs limited free. operation efficiency cost requirements make
translation-based approaches less suited system. Closely related works CrossLingual Vector Space Model (CL-VSM) (Potthast et al., 2011) approach presented
Pouliquen, Steinberger, Deguernel (2008) compare documents using
dictionaries, cases EuroVoc dictionaries (Rodrguez, Azcona, & Paredes,
2008). generality approaches limited quality available linguistic
resources, may scarce non-existent certain language pairs.
3.1.2 Probabilistic Topic Model Based Approaches
exist many variants modelling documents language independent way using probabilistic graphical models. models include: Joint Probabilistic Latent Semantic Analysis (JPLSA) (Platt, Toutanova, & Yih, 2010), Coupled Probabilistic LSA
(CPLSA) (Platt et al., 2010), Probabilistic Cross-Lingual LSA (PCLLSA) (Zhang, Mei, &
Zhai, 2010) Polylingual Topic Models (PLTM) (Mimno, Wallach, Naradowsky, Smith,
& McCallum, 2009) Bayesian version PCLLSA. methods (except
CPLSA) describe multilingual document collections samples generative probabilistic models, variations assumptions model structure. topics
represent latent variables used generate observed variables (words), process
specific language. parameter estimation posed inference problem
typically intractable one usually solves using approximate techniques. variants solutions based Gibbs sampling Variational Inference, nontrivial
implement may require experienced practitioner applied. Furthermore,
representing new document mixture topics another potentially hard inference
problem must solved.
3.1.3 Matrix Factorization Base Approaches
Several matrix factorization based approaches exist literature. models include:
Non-negative matrix factorization based (Xiao & Guo, 2013), Cross-Lingual Latent Semantic Indexing CL-LSI (Dumais, Letsche, Littman, & Landauer, 1997; Peters & Braschler,
2012), Canonical Correlation Analysis (CCA) (Hotelling, 1935), Oriented Principal Component Analysis (OPCA) (Platt et al., 2010). quadratic time space dependency
OPCA method makes impractical large scale purposes. addition, OPCA forces
vocabulary sizes languages same, less intuitive. setting, method Xiao Guo (2013) prohibitively high computational cost
building models (it uses dense matrices whose dimensions product training set
size vocabulary size). proposed approach combines CCA CL-LSI. Another
closely related method Cross-Lingual Explicit Semantic Analysis (CL-ESA) (Potthast,
Stein, & Anderka, 2008), uses Wikipedia (as current work) compare
documents. interpreted using sample covariance matrix features
two languages define dot product used compute similarities. authors
289

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

CL-ESA compare CL-LSI find CL-LSI outperform CL-ESA
information retrieval, costlier optimize large corpus (CL-ESA requires
training). find scalability argument apply case: based advances numerical linear algebra solve large CL-LSI problems involve millions
documents opposed 10,000 document limit reported Potthast et al. (2008).
addition, CL-ESA less suited computing similarities two large monolingual
streams. example, day compute similarities 500,000 English
500,000 German news articles. Comparing German news article 500,000
English news articles either prohibitively slow (involves projecting English articles
Wikipedia) consumes much memory (involves storing projected English articles,
Wikipedia size 1,000,000 500,000 1,000,000 non-sparse matrix).
3.1.4 Monolingual Approaches
Finally, related work includes monolingual approaches treat document written different languages monolingual fashion. intuition named entities (for example, Obama) cognate words (for example, tsunami) written
similar fashion many languages. example, Cross-Language Character n-Gram
Model (CL-CNG) (Potthast et al., 2011) represents documents bags character ngrams. Another approach use language dependent keyword lists based cognate
words (Pouliquen et al., 2008). approaches may suitable comparing documents written languages share writing system, apply case
global news tracking.
Based requirements Section 2.1, chose focus methods based vector
space models linear embeddings. propose method efficient
popular alternatives (a clustering-based approach latent semantic indexing), still
simple optimize use.
3.2 Cross-Lingual Cluster Linking
Although number services aggregate news identifying clusters
similar articles, almost services provide linking clusters different
languages. Google News well Yahoo! News able identify clusters articles
event, offer linking clusters across languages. service
found, provides cross-lingual cluster linking, European Media Monitor
(EMM) (Pouliquen et al., 2008; Steinberger, Pouliquen, & Ignat, 2005). EMM clusters
articles 60 languages tries determine clusters articles different
languages describe event. achieve cluster linking, EMM uses three different
language independent vector representations cluster. first vector contains
weighted list references countries mentioned articles, second
vector contains weighted list mentioned people organizations. last vector
contains weighted list Eurovoc subject domain descriptors. descriptors
topics, air transport, EC agreement, competition pollution control
articles automatically categorized (Pouliquen, Steinberger, & Ignat, 2006). Similarity
clusters computed using linear combination cosine similarities
computed three vectors. similarity threshold, clusters
290

fiCross-Lingual Document Similarity Event Tracking

linked. Compared EMM, approach uses document similarities obtain small set
potentially equivalent clusters. Additionally, decide two clusters equivalent
based hand-set threshold similarity value instead use classification model
uses larger set features related tested pair clusters.
system, significantly different worth mentioning, GDELT
project (Leetaru & Schrodt, 2013). GDELT, events also extracted articles,
case, event specified form triple containing two actors
relation. project contains extensive vocabulary possible relations, mostly related political events. order identify events, GDELT collects articles
65 languages uses machine translation translate English. information
extraction done translated article.

4. Cross-Lingual Document Similarity
Document similarity important component techniques text mining natural
language processing. Many techniques use similarity black box, e.g., kernel
Support Vector Machines. Comparison documents (or types text snippets)
monolingual setting well-studied problem field information retrieval (Salton
& Buckley, 1988). first formally introduce problem followed description
approach.
4.1 Problem Definition
first describe documents represented vectors compare documents mono-lingual setting. define way measure cross-lingual similarity
natural models consider.
4.1.1 Document Representation
standard vector space model (Salton & Buckley, 1988) represents documents vectors, term corresponds word phrase fixed vocabulary. Formally,
document represented vector x Rn , n corresponds size
vocabulary, vector elements xk correspond number times term k occurred
document, also called term frequency Fk (d).
also used term re-weighting scheme adjusts fact words
occur frequently general. term weight correspond importance
term given corpus. common weighting scheme called Term Frequency
Inverse Document Frequency (T F IDF ) weighting.
Document Frequency (IDF )
Inverse

N
weight dictionary term k defined log DFk , DFk number
documents corpus contain term k N total number documents
corpus. building cross-lingual models, IDF scores computed
respect Wikipedia corpus. part system, computed TFIDF
vectors streams news articles multiple languages. IDF scores
language changed dynamically - new document computed IDF news
articles within 10 day window.
291

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

Therefore define documents F IDF
xij :=

term frequency document
.
inverse document frequency term j

F IDF weighted vector space model document representation corresponds map
: text Rn defined by:


N
.
(d)k = F k (d) log
DF k
4.1.2 Mono-Lingual Similarity
common way computing similarity documents cosine similarity,
sim(d1 , d2 ) =

h(d1 ), (d2 )i
,
k(d1 )kk(d2 )k

h, kk standard inner product Euclidean norm. dealing two
languages, one could ignore language information build vector space using
union tokens languages. cosine similarity function space
useful extent, example Internet Obama may appear Spanish
English texts presence terms English Spanish document would contribute similarity. general however, large parts vocabularies
may intersect. means given language pair, many words languages
cannot contribute similarity score. cases make similarity function
insensitive data.
4.1.3 Cross-Lingual Similarity
Processing multilingual dataset results several vector spaces varying dimensionality, one language. dimensionality vector space corresponding i-th
language denoted ni vector space model mapping denoted : text Rni .
similarity documents language language j defined bilinear operator represented matrix Si,j Rni nj :
simi,j (d1 , d2 ) =

hi (d1 ), Si,j j (d2 )i
,
ki (d1 )kkj (d2 )k

d1 d2 documents written i-th j-th language respectively.
maximal singular value Si,j bounded 1, similarity scores lie
interval [1, 1]. provide overview models Section 4.2 introduce
additional notation 4.3. Starting Section 4.4 ending Section 4.7
describe approaches compute Si,j given training data.
4.2 Cross-Lingual Models
section, describe several approaches problem computing multilingual similarities introduced Section 4.1. present four approaches: simple approach
based k-means clustering Section 4.4, standard approach based singular value decomposition Section 4.5, related approach called Canonical Correlation Analysis (CCA)
292

fiCross-Lingual Document Similarity Event Tracking

Section 4.6 finally new method, extension CCA two languages Section 4.7. CCA used find correlated patterns pair languages,
whereas extended method optimizes Sum Squared Correlations (SSCOR)
several language pairs, introduced Kettenring (1971). SSCOR problem
difficult solve setting (hundreds thousands features, hundreds thousands
examples). tackle this, propose method consists two ingredients.
first one based observation certain datasets (such Wikipedia) biased
towards one language (English Wikipedia), exploited reformulate
difficult optimization problem eigenvector problem. second ingredient dimensionality reduction using CL-LSI, makes eigenvector problem computationally
numerically tractable.
concentrate approaches based linear maps rather alternatives,
machine translation probabilistic models, discussed section related
work. start introducing notation.
4.3 Notation
cross-lingual similarity models presented paper based comparable corpora.
comparable corpus collection documents multiple languages, alignment
documents topic, even rough translation other.
Wikipedia example comparable corpus, specific entry described
multiple languages (e.g., Berlin currently described 222 languages). News articles
represent another example, event described newspapers several
languages.
formally, multilingual document = (u1 , . . . um ) tuple documents
topic (comparable), ui document written language i. Note
individual document ui empty document (missing resource)
must contain least two nonempty documents. means analysis
discard strictly monolingual documents cross-lingual information available.
comparable corpus = d1 , . . . , ds collection multilingual documents. using
vector space model, represent set matrices X1 , . . . , Xm ,
Xi Rni matrix corresponding language ni vocabulary size
language i. Furthermore, let Xi` denote `-th column matrix Xi matrices
respect document alignment - vector Xi` corresponds TFIDF vector
i-th component P
multilingual document d` . use N denote total row dimension
X, i.e., N :=
i=1 ni . See Figure 3 illustration introduced notation.
describe four models cross-lingual similarity computation next
sub-sections.
4.4 k-Means
k-means algorithm perhaps well-known widely-used clustering algorithm. Here, present application compute cross-lingual similarities. idea
based concatenating corpus matrices, running standard k-means clustering obtain
matrix centroids, reversing concatenation step obtain set aligned bases,
finally used compute cross-lingual similarities. See Figure 4 overview
293

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

= {d1 , d2 , . . . , ds }
d2
d3

d1

X

{

{X
X{X
X {X
X1

1
1

X12 X13

X1s

2

1
2

X22 X23

X2s

3

1
3

X32 X33

X3s

{


Xm

{n
{n
{n

1

{

2

3

{n

N



{

1
2 X3
Xm Xm
Xm


ds



Figure 3: Multilingual corpora matrix representations using vector space
model.

294

fiCross-Lingual Document Similarity Event Tracking

k-means:
X1
X2
X3


=

C1

Q

C2
C3

new vector i-th language,
x 2 Rni mapped new
coordinates minimize:
||Xi Ci ||
solution
= (CiT Ci )

1

CiT x

{

Columns Q
indicator vectors

Pi

Figure 4: k-means algorithm coordinate change.
procedure. left side Figure 4 illustrates decomposition right side
summarizes coordinate change.
order apply algorithm, first merge term-document matrices
single matrix X stacking individual term-document matrices (as seen Figure 3):


,
X := X1T , X2T , , Xm
columns respect alignment documents (here MATLAB notation
concatenating matrices used). Therefore, document represented long vector
indexed terms languages.
run k-means algorithm (Hartigan, 1975) obtain centroid matrix
C RN k , k columns represent centroid vectors. centroid matrix
split vertically blocks:

C = [C1T Cm
] ,
according number dimensions language, i.e., Ci Rni k . reiterate,
matrices Ci computed using multilingual corpus matrix X (based Wikipedia
example).
compute cross-lingual document similarities new documents, note matrix
Ci represents vector space basis used map points Rni k-dimensional
space, new coordinates vector x Rni expressed as:
(CiT Ci )1 CiT xi .
resulting matrix similarity computation language language j
defined scaling factor as:
Ci (CiT Ci )1 (CjT Cj )1 Cj .
matrix result mapping documents language independent space using
pseudo-inverses centroid matrices Pi = (CiT Ci )1 Ci comparing using
standard inner product, results matrix PiT Pj . sake presentation,
assumed centroid vectors linearly independent. (An independent subspace
could obtained using additional Gram-Schmidt step (Golub & Van Loan, 2012)
matrix C, case.)
295

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

LSI:
X1
X2
X3


=

U1



VT

U2
U3

X = U SV
UT U =

U 2 RN k

V TV =

V 2 Rsk

Figure 5: LSI multilingual corpus matrix decomposition.

4.5 Cross-Lingual Latent Semantic Indexing
second approach consider Cross-Lingual Latent Semantic Indexing (CL-LSI) (Dumais et al., 1997) variant LSI (Deerwester, Dumais, Landauer, Furnas, &
Harshman, 1990) one language. approach similar k-means,
first concatenate corpus matrices, compute decomposition, case
CL-LSI truncated Singular Value Decomposition (SVD), decouple column space matrix use blocks compute linear maps common vector space, standard
cosine similarity used compare documents.
method based computing truncated singular value decomposition
concatenated corpus matrix X U SV . See Figure 5 decomposition. Representing
documents topic coordinates done way k-means case (see
Figure 4), describe compute coordinate change functions.
cross-lingual similarity functions based rank-k truncated SVD: X U V ,
U RN k basis vectors interest Rkk truncated diagonal matrix
singular eigenvalues. aligned basis obtained first splitting U vertically according
]T . Then,
number dimensions language: U = [U1T Um
k-means clustering, compute pseudoinverses Pi = (UiT Ui )1 UiT . matrices Pi
used change basis standard basis Rni basis spanned
columns Ui .
4.5.1 Implementation Note
Since matrix X large could use iterative method like Lanczos algorithm reorthogonalization (Golub & Van Loan, 2012) find left singular vectors
(columns U ) corresponding largest singular values. turns Lanczos
method converges slowly gap leading singular values small. Moreover,
Lanczos method hard parallelize. Instead, use randomized version
SVD (Halko, Martinsson, & Tropp, 2011) viewed block Lanczos method.
enables us use parallelization speeds computation considerably.
compute matrices Pi used QR algorithm (Golub & Van Loan, 2012)
factorize Ui Ui = Qi Ri , QTi Qi = Ri triangular matrix. Pi
obtained solving Ri Pi = Qi .
296

fiCross-Lingual Document Similarity Event Tracking

4.6 Canonical Correlation Analysis
present statistical technique analyze data two sources, extension
presented next section.
Canonical Correlation Analysis (CCA) (Hotelling, 1935) dimensionality reduction
technique similar Principal Component Analysis (PCA) (Pearson, 1901), additional assumption data consists feature vectors arose two sources (two
views) share information. Examples include: bilingual document collection (Fortuna, Cristianini, & Shawe-Taylor, 2006) collection images captions (Hardoon,
Mourao-Miranda, Brammer, & Shawe-Taylor, 2008). Instead looking linear combinations features maximize variance (PCA) look linear combination
feature vectors first view linear combination second view,
maximally correlated.
Interpreting columns Xi observation vectors sampled underlying distribution Xi Rni , idea find two weight vectors wi Rni wj Rnj
random variables wiT Xi wjT Xj maximally correlated (wi wj used
map random vectors random variables, computing weighted sums vector components). Let (x, y) denote sample-based correlation coefficient two vectors
observations x y. using sample matrix notation Xi Xj (assuming
data missing simplify exposition), problem formulated following
optimization problem:
maximizen

wi Rni ,wj R

j

wiT Ci,j wj
(wiT Xi , wjT Xj ) = q
,
q
wiT Ci,i wi wjT Cj,j wj

Ci,i Cj,j empirical estimates variances Xi Xj respectively Ci,j
estimate covariance matrix. Assuming observation vectors centered
(only purposes presentation), matrices computed following way:
1
Xi XjT , similarly Ci,i Cj,j . optimization problem reduced
Ci,j = n1
eigenvalue problem includes inverting variance matrices Ci,i Cj,j .
matrices invertible, one use regularization technique replacing Ci,i
(1 )Ci,i + I, [0, 1] regularization coefficient identity
matrix. (The applied Cj,j .) single canonical variable usually inadequate
representing original random vector typically one looks k projection pairs
(wi1 , wj1 ), . . . , (wik , wjk ), (wiu )T Xi (wju )T Xj highly correlated (wiu )T Xi
uncorrelated (wiv )T Xi u 6= v analogously wju vectors.
Note method original form applicable two languages
aligned set observations available. next section describe scalable extension
CCA two languages.
4.7 Hub Language Based CCA Extension
Building cross-lingual similarity models based comparable corpora challenging two
main reasons. first problem related missing alignment data: number
languages large, dataset documents cover languages small (or may even
empty). Even two languages considered, set aligned documents
297

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

small (an extreme example given Piedmontese Hindi Wikipedias
inter-language links available), case none methods presented far
applicable. second challenge scale - data high-dimensional (many languages
hundreds thousands features per language) number multilingual documents may large (over one million case Wikipedia). optimization problem
posed CCA trivial solve: covariance matrices prohibitively
large fit memory (even storing 100,000 100,000 element matrix requires 80GB
memory) iterative matrix-multiplication based approaches solving generalized eigenvalue problems required (the covariance matrices expressed products sparse
matrices, means fast matrix-vector multiplication).
describe extension CCA two languages, trained
large comparable corpora handle missing data. extension consider
based generalization CCA two views, introduced Kettenring (1971),
namely Sum Squared Correlations SSCOR, state formally later
section. approach exploits certain characteristic data, namely hub language
characteristic (see below) two ways: reduce dimensionality data
simplify optimization problem.
4.7.1 Hub Language Characteristic
case Wikipedia, observed even though training resources scarce
certain language pairs, often exists indirect training data. considering third language, training data languages pair, use composition
learned maps proxy. refer third language hub language.
hub language language high proportion non-empty documents =
{d1 , ..., d` }. mentioned, focus multilingual documents include
least two languages. prototypical example case Wikipedia English.
notion hub language could interpreted following way. non-English
Wikipedia page contains one links variants page languages,
English likely one them. makes English hub language.
use following notation define subsets multilingual comparable corpus:
let a(i, j) denote index set multilingual documents non-missing data
i-th j-th language:
a(i, j) = {k | dk = (u1 , ..., um ), ui 6= , uj 6= } ,
let a(i) denote index set multilingual documents non missing data
i-th language.
describe two step approach building cross-lingual similarity matrix.
first part related LSI reduces dimensionality data. second step
refines linear mappings optimizes linear dependence data.
4.7.2 Step 1: Hub Language Based Dimensionality Reduction
first step method project X1 , . . . , Xm lower-dimensional spaces without
destroying cross-lingual structure. Treating nonzero columns Xi observation
vectors sampled underlying distribution Xi Vi = Rni , analyze empirical
298

fiCross-Lingual Document Similarity Event Tracking

cross-covariance matrices:
Ci,j =

X
1
(Xi` ci ) (Xj` cj )T ,
|a(i, j)| 1
`a(i,j)

ci = a1i `a(i) Xi` . finding low-rank approximations Ci,j identify
subspaces Vi Vj relevant extracting linear patterns Xi
Xj . Let X1 represent hub language corpus matrix. LSI approach finding
subspaces perform singular value decomposition full N N covariance
matrix composed blocks Ci,j . |a(i, j)| small many language pairs (as
case Wikipedia), many empirical estimates Ci,j unreliable, result
overfitting. reason, perform truncated singular value decomposition

Pm
matrix C = [C1,2 C1,m ] U SV , U Rn1 k , Rkk , V R( i=2 ni )k .
split matrix V vertically blocks n2 , . . . , nm rows: V = [V2T VmT ]T . Note
columns U orthogonal columns Vi (columns V orthogonal).
Let V1 := U . proceed reducing dimensionality Xi setting: Yi = ViT Xi ,
Yi RkN . summarize, first step reduces dimensionality data
based CL-LSI, optimizes hub language related cross-covariance blocks.
P

4.7.3 Step 2: Simplifying Solving SSCOR.
second step involves solving generalized version canonical correlation analysis
matrices Yi order find mappings Pi . approach based sum squares
correlations formulation Kettenring (1971), consider correlations
pairs (Y1 , Yi ), > 1 due hub language problem characteristic. present
original unconstrained optimization problem, constrained formulation based
hub language problem characteristic. simplify constraints reformulate
problem eigenvalue problem using Lagrange multipliers.
original sum squared correlations formulated unconstrained problem:
maximize
wi Rk


X

(wiT Yi , wjT Yj )2 .

i<j

solve similar problem restricting = 1 omitting optimization non-hub
language pairs. Let Di,i Rkk denote empirical covariance Yi Di,j denote
empirical cross-covariance computed based Yi Yj . solve following constrained
(unit variance constraints) optimization problem:
maximize
wi

Rk


X

w1T D1,i wi

2

subject wiT Di,i wi = 1,

= 1, . . . , m.

(1)

i=2

constraints wiT Di,i wi simplified using Cholesky decomposition Di,i =
KiT Ki substitution: yi := Ki wi . inverting Ki matrices defining Gi :=
K1T D1,i Ki1 , problem reformulated:
maximize
yi Rk


X

y1T Gi yi

2

subject yiT yi = 1,

i=2

299

= 1, . . . , m.

(2)

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

necessary condition optimality derivatives Lagrangian vanish.
Lagrangian (2) expressed as:
L(y1 , . . . , ym , 1 , . . . , ) =


X

y1T Gi yi

2

+

i=2


X


yiT yi 1 .

i=1

Stationarity conditions give us:


X


L=0
y1T Gi yi Gi yi + 1 y1 = 0,
y1

(3)



L = 0 y1T Gi yi GTi y1 + yi = 0, > 1.
yi

(4)

i=2

Multiplying equations (4) yiT applying constraints, eliminate
gives us:

GTi y1 = y1T Gi yi yi , > 1.
(5)
Plugging (3), obtain eigenvalue problem:
!

X
Gi GTi y1 + 1 y1 = 0.
i=2

eigenvectors

Pm




i=2 Gi Gi

yi obtained (5): yi :=
wi :=

Ki1 yi .

solve problem first language. solutions
GT
y1
.
kGT
y1 k

Note solution (1) recovered by:

linear transformation w variables thus expressed as:
Y1 := eigenvectors


X

Gi GTi ,

i=2

N diagonal matrix

W1 = K11 Y1
Wi = Ki1 GTi Y1 N,
normalizes GTi Y1 ,

N (j, j) :=

1
kG(i Y1 (:,j)k .

4.7.4 Remark
technique related Generalization Canonical Correlation Analysis (GCCA)
Carroll (1968), unknown group configuration variable defined objective
maximize sum squared correlations group variable others.
problem reformulated eigenvalue problem. difference lies fact
set unknown group configuration variable hub language, simplifies
solution. complexity method O(k 3 ), k reduced dimension
LSI preprocessing step, whereas solving GCCA method scales O(s3 ),
number samples (see Gifi, 1990). Another issue GCCA cannot
directly applied case missing documents.
summarize, first reduced dimensionality data k-dimensional features
found new representation (via linear transformation) maximizes directions
linear dependence languages. final projections enable mappings
common space defined as: Pi (x) = WiT ViT x.
300

fiCross-Lingual Document Similarity Event Tracking

English articles

Spanish articles

candidate
clusters

Figure 6: Clusters composed English Spanish news articles. Arrows link English
articles Spanish k-nearest neighbor matches according crosslingual similarity.

5. Cross-Lingual Event Linking
main application test cross-lingual similarity cross-lingual event
linking. online media streams particularly news articles often duplication
reporting, different viewpoints opinions, centering around single event.
events covered many articles question address find
articles different languages describing single event. paper consider
problem matching events different languages. address problem
detection events instead base evaluation online system detection
world events, Event Registry. events represented clusters articles
ultimately problem reduces finding suitable matchings clusters articles
different languages.
5.1 Problem Definition
problem cross-lingual event linking match monolingual clusters news articles
describe event across languages. example, want match cluster
Spanish news articles cluster English news articles describe
earthquake.
article written language `, ` L = {`1 , `2 , ..., `m }.
language `, obtain set monolingual clusters C` . precisely, articles
corresponding cluster c C` written language `. Given pair languages
`a L, `b L `a 6= `b , would like identify cluster pairs (ci , cj ) C`a C`b
ci cj describe event.
Matching clusters generalized matching problem. cannot assume
one cluster per language per event, assume complete coverage i.e.,
exists least one cluster per event every language. partly due news
coverage might granular languages, partly due noise errors
301

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

event detection process. implies cannot make assumptions matching
(e.g., one-to-one complete matching) excludes use standard weighted bipartite
matching type algorithms problem. example shown Figure 6,
cluster may contain articles closely matched many clusters different
language.
also seek algorithm exhaustive comparison clusters,
since become prohibitively expensive working real-time setting.
specifically, wish avoid testing cluster ci clusters
languages. Performing exhaustive comparison would result O(|C|2 ) tests, |C|
number clusters (over languages), feasible number
clusters order tens thousands. address testing clusters
connected least one k-nearest neighbor (marked candidate clusters
Figure 6).
5.2 Algorithm
order identify clusters equivalent cluster ci , developed two-stage
algorithm. cluster ci , first efficiently identify small set candidate clusters
find clusters among candidates, equivalent ci . example
shown Figure 6.
details first step described Algorithm 1. algorithm begins
individually inspecting news article ai cluster ci . Using chosen method
computing cross-lingual document similarity (see Section 4.2), identifies 10
similar news articles ai language ` L. similar article aj , identify
corresponding cluster cj add set candidates. set candidate clusters
obtained way several orders magnitude smaller number clusters,
linear respect number news articles cluster ci . practice,
clusters contain highly related articles similar articles languages
mostly fall candidate clusters.
Although computed document similarities approximate, assumption articles different languages describing event generally higher similarity
articles different events. assumption always hold, redundancy data mitigates false positives. Since compute 10 similar
articles article ci , likely identify relevant candidates cluster
ci .
second stage algorithm determines (if any) candidate clusters
equivalent ci . treat task supervised learning problem. candidate
cluster cj C, compute vector learning features indicative whether
ci cj equivalent apply binary classification model predicts
clusters equivalent not. classification algorithm used train model
linear Support Vector Machine (SVM) method (Shawe-Taylor & Cristianini, 2004).
use three groups features describe cluster pair (ci , cj ). first group based
cross-lingual article links, derived using cross-lingual similarity: news
article ai linked 10-nearest neighbors articles languages (10 per
language). group contains following features:
302

fiCross-Lingual Document Similarity Event Tracking

input: test cluster ci , set clusters C` language ` L
output: set clusters C potentially equivalent ci
C {};
article ai ci
language ` L
/* use hub CCA find 10 similar articles article ai
language `
*/
SimilarArticles = getCCASimilarArticles(ai , `);
article aj SimilarArticles
/* find cluster cj article aj assigned
*/
cj c, c C` aj c;
/* add cluster cj set candidates C
*/
C C {cj };
end
end
end
Algorithm 1: Algorithm identifying candidate clusters C potentially equivalent ci
linkCount number times news articles cj among 10-nearest
neighbors articles ci . words, number times article
ci similar article (i.e., among 10 similar) cj .
avgSimScore average similarity score links, identified linkCount,
two clusters.
second group concept-related features. Articles imported Event
Registry annotated disambiguating mentioned entities keywords corresponding Wikipedia pages (Zhang & Rettinger, 2014b). Whenever Barack Obama is,
example, mentioned article, article annotated link Wikipedia page.
way, mentions entities (people, locations, organizations) ordinary keywords (e.g., bank, tax, ebola, plane, company) annotated. Although Spanish article
Obama annotated Spanish version Wikipedia page, many
cases link Wikipedia pages English versions. done since
Wikipedia provides information regarding pages different languages represent
concept/entity. Using approach, word avion Spanish article
annotated concept word plane English article. Although
articles different languages, annotations therefore provide languageindependent vocabulary used compare articles/clusters. analyzing
articles clusters ci cj , identify relevant entities keywords
cluster. Additionally, also assign weights concepts based
frequently occur articles cluster. list relevant concepts
corresponding weights, consider following features:
entityCosSim cosine similarity vectors entities clusters ci
cj .
303

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

keywordCosSim cosine similarity vectors keywords clusters ci
cj .
entityJaccardSim Jaccard similarity coefficient (Levandowsky & Winter, 1971)
sets entities clusters ci cj .
keywordJaccardSim Jaccard similarity coefficient sets keywords
clusters ci cj .
last group features contains three miscellaneous features seem discriminative unrelated previous two groups:
hasSameLocation feature boolean variable true location
event clusters same. location events estimated considering
locations mentioned articles form cluster provided Event
Registry.
timeDiff absolute difference hours two events. publication
time date events computed average publication time date
articles provided Event Registry.
sharedDates determined Jaccard similarity coefficient sets date
mentions extracted articles. use extracted mentions dates provided
Event Registry, uses extensive set regular expressions detect normalize mentions dates different forms.

6. Evaluation
describe main dataset building cross-lingual models based
Wikipedia present two sets experiments. first set experiments establishes
hub based approach deal language pairs little training data
available. second set experiments compares main approaches presented
task mate retrieval task event linking. mate retrieval task
given test set document pairs, pair consists document
translation. Given query document test set, goal retrieve translation
language, also referred mate document. Finally, examine
different choices features impact event linking performance.
6.1 Wikipedia Comparable Corpus
investigate empirical performance low-rank approximations test algorithms large-scale, real-world multilingual dataset extracted Wikipedia
using inter-language links alignment. results large number weakly comparable documents 200 languages. Wikipedia large source multilingual
data especially important languages translation tools, multilingual dictionaries Eurovoc (Rodrguez et al., 2008), strongly aligned multilingual
corpora Europarl (Koehn, 2005) available. Documents different languages
related inter-language links found left Wikipedia page.
304

fiCross-Lingual Document Similarity Event Tracking

Wikipedia constantly growing. currently 12 Wikipedias 1 million articles, 52 100k articles, 129 10k articles, 236
1, 000 articles.
Wikipedia page embedded page tag. First, check title
page starts Wikipedia namespace (which includes categories discussion pages)
process page does. Then, check redirection page
store redirect link inter-language links point redirection links also.
none applies, extract text parse Wikipedia markup. Currently,
markup removed.
get inter-language link matrix using previously stored redirection links interlanguage links. inter-language link points redirection replace
redirection target link. turns obtain matrix symmetric,
consequently underlying graph symmetric. means existence
inter-language link one way (i.e., English German) guarantee
inter-language link reverse direction (German English). correct
transform matrix symmetric computing + obtaining undirected
graph. rare case symmetrization multiple links pointing
document, pick first one encountered. matrix enables us build
alignment across Wikipedia2 languages.
6.2 Experiments Missing Alignment Data
subsection, investigate empirical performance hub CCA approach.
demonstrate approach successfully applied even case
fully missing alignment information. purpose, select subset Wikipedia
languages containing three major languages, English (4,212k articles)en (hub language),
Spanish (9,686k articles)es, Russian (9,662k articles)ru, five minority (in terms
Wikipedia sizes) languages, Slovenian (136k articles)sl, Piedmontese (59k articles)pms,
Waray-Waray (112k articles)war (all 2 million native speakers), Creole (54k
articles)ht (8 million native speakers), Hindi (97k articles)hi (180 million native
speakers). preprocessing, remove documents contain less 20 different
words (referred stubs3 ) remove words occurring less 50 documents
well top 100 frequent words (in language separately). represent
documents normalized TFIDF (Salton & Buckley, 1988) weighted vectors. IDF
scores computed language based aligned documents English
Wikipedia. English language IDF scores based English documents
aligned Spanish documents exist.
evaluation based splitting data training test sets. select
test set documents multilingual documents least one nonempty alignment
list: (hi, ht), (hi, pms), (war, ht), (war, pms). guarantees cover
languages. Moreover test set suitable testing retrieval
hub chosen pairs empty alignments. remaining documents used
2. dataset based Wikipedia dumps available 2013.
3. documents typically low value linguistic resource. Examples include titles
columns table, remains parsing process, Wikipedia articles little information contained one two sentences.

305

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

training. Table 1, display corresponding sizes training test documents
language pair.
training set, perform two step procedure obtain common document representation set mappings Pi . test set language pair, testi,j =
{(x` , y` )|` = 1 : n(i, j)}, consists comparable document pairs (linked Wikipedia pages),
n(i, j) test set size. evaluate representation measuring mate retrieval
quality test sets: `, rank projected documents Pj (y1 ), . . . , Pj (yn(i,j) )
according similarity Pi (x` ) compute rank mate document
r(`) = rank(P
j (y` )). final
retrieval score (between -100 100) computed as:
Pn(i,j) n(i,j)r(`)
100
`=1
n(i,j)
n(i,j)1 0.5 . score less 0 means method performs worse random retrieval score 100 indicates perfect mate retrieval.
mate retrieval results included Table 2.
observe method performs well pairs languages, least 50,000
training documents available(en, es, ru, sl ). note taking k = 500 k = 1, 000
multilingual topics usually results similar performance, notable exceptions:
case (ht, war ) additional topics result increase performance, opposed
(ht, pms) performance drops, suggests overfitting. languages
method performs poorly ht war, explained quality
data (see Table 3 explanation follows). case pms, demonstrate solid
performance achieved language pairs (pms, sl ) (pms, hi ), 2,000
training documents shared pms sl training documents available
pms hi. Also observe case (pms, ht) method still obtains
score 62, even though training set intersection zero ht data corrupted,
show next paragraph.

Table 1: Training test sizes (in thousands). first row represents size training sets used construct mappings low-dimensional language independent
space using en hub. diagonal elements represent number unique
training documents test documents language.
en
es
ru
sl
en 671 - 4.6 463 - 4.3 369 - 3.2 50.3 - 2.0
es
463 - 4.3 187 - 2.9 28.2 - 2.0
ru
369 - 3.2 29.6 - 1.9
sl
50.3 - 2
hi
war
ht
pms

hi
14.4 - 2.8
8.7 - 2.5
9.2 - 2.7
3.8 - 1.6
14.4 - 2.8

war
8.58 - 2.4
6.9 - 2.4
2.9 - 1.1
1.2 - 0.99
0.58 - 0.8
8.6 - 2.4

ht
17 - 2.3
13.2 - 2
3.2 - 2.2
0.95 - 1.2
0.0 - 2.1
0.04 - 0.5
17 - 2.3

pms
16.6 - 2.7
13.8 - 2.6
10.2 - 1.3
1.8 - 1.0
0.0 - 0.8
0.0 - 2.0
0.0 - 0.4
16.6 - 2.7

inspect properties training sets roughly estimating fraction
rank(A)
min(rows(A), cols(A)) English training matrix corresponding mate matrix,
rows(A) cols(A) denote number rows columns respectively.
denominator represents theoretically highest possible rank matrix could have.
306

fiCross-Lingual Document Similarity Event Tracking

Table 2: Pairwise retrieval, 500 topics left 1,000
en
es
ru
sl
hi
war
en
98 - 98 95 - 97 97 - 98 82 - 84 76 - 74
es 97 - 98
94 - 96 97 - 98 85 - 84 76 - 77
ru 96 - 97 94 - 95
97 - 97 81 - 82 73 - 74
sl 96 - 97 95 - 95 95 - 95
91 - 91 68 - 68
hi 81 - 82 82 - 81 80 - 80 91 - 91
68 - 67
war 68 - 63 71 - 68 72 - 71 68 - 68 66 - 62
ht 52 - 58 63 - 66 66 - 62 61 - 71 44 - 55 16 - 50
pms 95 - 96 96 - 96 94 - 94 93 - 93 85 - 85 23 - 26

topics right
ht
pms
53 - 55 96 - 97
56 - 57 96 - 96
55 - 56 96 - 96
59 - 69 93 - 93
50 - 55 87 - 86
28 - 48 24 - 21
62 - 49
66 - 54

Ideally, two fractions approximately - aligned spaces
reasonably similar dimensionality. display numbers pairs Table 3.

Table 3: Dimensionality drift. column corresponds pair aligned corpus matrices
English another language. numbers represent ratio
numerical rank highest possible rank. example, column en
ht tells us English-Creole pairwise-aligned corpus matrix pair,
English counterpart full rank, Creole counterpart far full
rank.
en es
0.81 0.89

en ru
0.8 0.89

en sl
0.98 0.96

en hi
11

en war
0.74 0.56

en ht
1 0.22

en pms
0.89 0.38

clear case Creole language 22% documents
unique suitable training. Though removed stub documents, many
remaining documents nearly same, quality smaller Wikipedias low.
confirmed Creole, Waray-Waray, Piedmontese languages manual
inspection. low quality documents correspond templates year, person,
town, etc. contain unique words.
also problem quality test data. example, look
test pair (war, ht) 386/534 Waray-Waray test documents unique
side almost Creole test documents (523/534) unique. indicates poor
alignment leads poor performance.
6.3 Evaluation Cross-Lingual Event Linking
order determine accurately predict cluster equivalence, performed two
experiments multilingual setting using English, German Spanish languages
labelled data evaluate linking performance. first experiment,
tested well individual approaches cross-lingual article linking perform
used linking clusters event. second experiment tested
307

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

accurate prediction model trained different subsets learning features.
evaluate prediction accuracy given dataset used 10-fold cross validation.
created manually labelled dataset order evaluate cross-lingual event linking
using two human annotators. annotators provided interface listing
articles, content top concepts pair clusters. task determine
clusters equivalent (i.e., discuss event). obtain pair clusters
(ci , cj ) annotate, first randomly chose cluster ci , used Algorithm 1 compute
set potentially equivalent clusters C randomly chose cluster cj C. dataset
provided annotators contains 808 examples, 402 equivalent clusters
pairs 406 not. Clusters learning example either English, Spanish
German. Although Event Registry imports articles languages well, restricted
experiments three languages. chose three languages since
large number articles clusters per day makes cluster linking
problem hard due large number possible links.
Section 4.2, described three main algorithms identifying similar articles
different languages. algorithms k-means, LSI hub CCA. training
set, used common Wikipedia alignment three languages. test
algorithms performed best, made following test. three algorithms,
analyzed articles Event Registry article computed similar
articles languages. test informative identified similar articles
cluster linking trained three classifiers described Section 5.2 one
algorithm. classifier allowed use learning features cross-lingual
article linking features values determined based selected algorithm
(k-means, LSI hub CCA). results trained models shown Table 4.
also show number topics (the dimensions latent space) influences
quality, except case k-means algorithm, performance 500
topic vectors reported, due higher computational cost.
observe that, task cluster linking, LSI hub CCA perform comparably
outperform k-means.
also compared proposed approaches task Wikipedia mate retrieval
(the task Section 6.2). computed Average (over language pairs) Mean
Reciprocal Rank (AMRR) (Voorhees et al., 1999) performance different approaches
Wikipedia data holding 15, 000 aligned test documents using 300, 000
aligned documents training set. Figure 7 shows AMRR score function
number feature vectors. clear hub CCA outperforms LSI approach
k-means lags far behind testing Wikipedia data. hub CCA approach 500
topic vectors manages perform comparably LSI-based approach 1, 000 topic
vectors, shows CCA method improve model memory footprint
well similarity computation time.
Furthermore, inspected number topics influences accuracy cluster
linking. see Table 4 choosing number features larger 500 barely
affects linking performance, contrast fact additional topics helped
improve AMMR, see Figure 7. differences may arisen due different domains
training testing (Wikipedia pages versus news articles).
308

fiCross-Lingual Document Similarity Event Tracking

0.9
0.8

AMRR

0.7
0.6
0.5
Hub CCA
LSI
k-means

0.4
0.3
0.2
100

200

300

400
500
600
700
Number feature vectors

800

900

1,000

Figure 7: Average mean reciprocal ranks

also analyzed cluster size influences accuracy cluster linking. intuition
linking large cluster pairs easier linking clusters articles.
reasoning large clusters would provide document linking information (more
articles mean links similar articles) well accurately aggregated
semantic information. case smaller clusters, errors similarity models
greater impact decrease performance classifier, too. validate
hypothesis split learning examples two datasets one containing cluster
pairs combined number articles clusters 20 one dataset
combined number 20 more. results experiment seen
Table 5. seen, results confirm expectations: smaller clusters
indeed harder correctly predict cluster pair merged not.
hub CCA attains higher precision classification accuracy task linking
small cluster pairs methods, LSI slightly better linking large
cluster pairs. gain precision LSI hub CCA linking large clusters much
smaller gain precision hub CCA LSI linking small clusters.
reason decided use hub CCA similarity computation component system.
second experiment, evaluate relevant individual groups features
correctly determine cluster equivalence. purpose, computed accuracy using
individual groups features, well using different combination groups. Since hub
CCA best performance three algorithms, used compute values
cross-lingual article linking features. results evaluation shown Table 6.
see using single group features, highest prediction accuracy
achieved using concept-related features. classification accuracy case 88.5%.
additionally including also cross-lingual article linking features, classification
309

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

Table 4: Accuracy cluster linking 500/800/1,000 topic vectors obtained different cross-lingual similarity algorithms. table shows algorithms
obtained classification accuracy, precision recall.
Models
hub CCA
LSI
k-means

Accuracy %
78.2/79.6/80.3
78.9/78.7/80.6
73.9/-/-

Precision %
76.3/78.0/80.5
76.8/77.0/78.7
69.5/-/-

Recall %
81.6/82.1/79.9
83.3/80.6/83.6
84.6/-/-

F1 %
78.9/80.0/80.2
79.9/78.8/81.1
76.3/-/-

Table 5: Accuracy cluster linking using 500 topic vectors two datasets containing
large (left number) small (right number) clusters. dataset small
clusters contained subset learning examples combined number
articles clusters cluster pair 20. remaining
learning examples put dataset large clusters.
Models
hub CCA
LSI
k-means

Accuracy %
81.2 - 77.8
82.8 - 76.4
75.5 - 71.2

Precision %
80.5 - 74.5
81.3 - 70.9
72.8 - 70.8

Recall %
91.3 - 57.5
93.1 - 57.5
95.3 - 36.2

F1 %
85.6 - 64.9
86.8 - 63.5
82.5 - 47.9

accuracy rises slightly 89.4%. Using three groups features, achieved accuracy
89.2%.
test accuracy predictions language dependent also performed
evaluations separately individual language pairs. experiment split
annotated learning examples three datasets, dataset contained
examples one language pair. training classifier three groups features
available. results shown Table 7. see performance cluster
linking English-German dataset highest terms accuracy, precision, recall
F1 . performance English-Spanish dataset comparable performance
English-German dataset, former achieves higher recall (and slightly higher
F1 score), latter achieves higher precision. possible explanation results
higher quantity quality English-German language resources leads
accurate cross-lingual article similarity measure well extensive semantic
annotation articles.
Based performed experiments, make following conclusions.
cross-lingual similarity algorithms provide valuable information used identify
clusters describe event different languages. task cluster linking,
cross-lingual article linking features however significantly less informative compared
concept-related features extracted semantic annotations. Never310

fiCross-Lingual Document Similarity Event Tracking

theless, cross-lingual article similarity features important two reasons.
first allow us identify given cluster limited set candidate clusters
potentially equivalent. important feature since reduces search
space several orders magnitude. second reason features important
concept annotations available articles annotation news articles
computationally intensive done subset collected articles.
prediction accuracies individual language pairs comparable although seems
achievable accuracy correlates amount available language resources.

Table 6: accuracy classifier story linking using different sets learning
features.
Features
hub CCA
Concepts
Misc
hub CCA + Concepts
hub CCA + Misc
Concepts + Misc


Accuracy %
78.3 5.9
88.5 2.7
54.8 6.7
89.4 2.5
78.8 5.0
88.7 2.6
89.2 2.6

Precision %
78.2 7.0
88.6 4.8
61.8 16.5
89.4 4.6
78.9 7.1
88.8 4.6
88.8 4.9

Recall %
78.9 5.2
88.6 2.2
58.2 30.2
89.6 2.4
79.4 4.6
88.8 2.2
90.1 1.9

F1 %
78.4 5.5
88.5 2.4
52.4 13.0
89.4 2.3
79.0 4.5
88.7 2.3
89.3 2.3

Table 7: accuracy classifier story linking training data language
pair separately using learning features.
Language pair
en, de
en, es
es, de

Accuracy %
91.8 5.5
87.7 5.4
88.6 4.3

Precision %
91.7 6.3
87.7 7.4
89.7 9.1

Recall %
93.7 6.3
88.5 9.8
84.3 11.9

F1 %
92.5 5.1
87.6 5.9
85.9 6.0

6.4 Remarks Scalability Implementation
One main advantages approach highly scalable. fast,
robust quality training data, easily extendable, simple implement relatively
small hardware requirements. similarity pipeline computationally intensive
part currently runs machine two Intel Xeon E5-2667 v2, 3.30GHz processors
256GB RAM. sufficient similarity computation large number
languages needed. currently uses Wikipedia freely available knowledge base
experiments show similarity pipeline dramatically reduces search space
linking clusters.
311

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

Currently, compute similarities 24 languages tags: eng, spa, deu, zho, ita,
fra, rus, swe, nld, tur, jpn, por, ara, fin, ron, kor, hrv, tam, hun, slv, pol, srp, cat, ukr
support language top 100 Wikipedia languages. data stream Newsfeed
(http://newsfeed.ijs.si/) provides 430k unique articles per day. system currently
computes 2 million similarities per second, means compute 16 1010 similarities
per day. store one day buffer language requires 1.5 GB memory
documents stored 500-dimensional vectors. note time complexity
similarity computations scales linearly dimension feature space
depend number languages. article, compute top 10 similar ones
every language.
linear algebra matrix vector operations, use high performance numerical
linear algebra libraries BLAS, OPENBLAS Intel MKL, currently allows us
process one million articles per day. current implementation, use
variation hub approach. projector matrices size 500 300, 000, every
projector takes 1.1 GB RAM. Moreover, need proxy matrices size 500 500
every language pair. 0.5 GB 24 languages 9.2 GB 100 languages.
together need around 135 GB RAM system 100 languages. Usage
proxy matrices enables projection input documents common space
handling language pairs missing low alignment. enables us block-wise
similarity computations improving system efficiency. code therefore
easily parallelized using matrix multiplication rather performing matrix - vector
multiplications. speeds code factor around 4. way, obtain
caching gains ability use vectorization. system also easily extendable.
Adding new language requires computation projector matrix proxy matrices
already available languages.
6.5 Remarks Reproducibility Experiments
made code data used experiments publicly available
https://github.com/rupnikj/jair_paper.git. manually labelled dataset used
evaluation event linking available dataset subfolder github
repository. included archive contains two folders: positive negative,
first folder includes examples cluster pairs two languages represent
event second folder contains pairs clusters two languages represent
different events. example JSON file contains top level information
pair clusters (including text articles) well set meta attributes,
correspond features described Section 5.2.
code folder includes MATLAB scripts building cross-lingual similarity models
introduced 4.2, used publicly available Wikipedia corpus reproduce cross-lingual similarity evaluation. also made available similarity
computation 100 languages service xling.ijs.si.
addition, Event Registry system (http://eventregistry.org/) comes
API, documented https://github.com/gregorleban/event-registry-python,
used download events articles.
312

fiCross-Lingual Document Similarity Event Tracking

7. Discussion Future Work
paper presented cross-lingual system linking events different languages. Building existing system, Event Registry, present evaluate several
approaches compute cross-lingual similarity function. also present approach
link events evaluate effectiveness various features. final pipeline scalable
terms number articles number languages, accurately linking events.
task mate retrieval, observe refining LSI-based projections
hub CCA leads improved retrieval precision, methods perform comparably
task event linking. inspection showed CCA-based approach reached
higher precision smaller clusters. interpretation linking features
highly aggregated large clusters, compensates lower per-document precision
LSI. Another possible reason advantage show Wikipedia lost
news domain. hypothesis could validated testing approach documents
different domain.
experiments show hub CCA-based features present good baseline,
greatly benefit additional semantic-based features. Even though experiments addition CCA-based features semantic features lead great performance improvements, two important benefits approach. First, linking
process sped using smaller set candidate clusters. Second, approach
robust languages semantic extraction available, due scarce linguistic
resources.
7.1 Future Work
Currently system loosely-coupled language component built independently
rest system, particular linking component. possible better
embeddings obtained methods jointly optimize classification task
embedding.
Another point interest evaluate system languages scarce linguistic
resources, semantic annotation might available. purpose, labelled
dataset linked clusters extended first. mate retrieval evaluation showed
even language pairs training set overlap, hub CCA recovers signal.
order improve performance classifier cluster linking, additional
features also extracted articles clusters checked increase
accuracy classification. Since amount linguistic resources vary significantly
language language would also make sense build separate classifier
language pair. Intuitively, improve performance since weights individual
learning features could adapted tested pair languages.

Acknowledgments
authors gratefully acknowledge funding work provided
projects X-LIKE (ICT-257790-STREP), MultilingualWeb (PSP-2009.5.2 Agr.# 250500),
313

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

TransLectures (FP7-ICT-2011-7), PlanetData (ICT-257641-NoE), RENDER (ICT-257790STREP), XLime (FP7-ICT-611346), META-NET (ICT-249119-NoE).

References
Brank, J., Leban, G., & Grobelnik, M. (2014). high-performance multithreaded approach
clustering stream documents. Proceedings 17th International Multiconference Information Society 2014, Volume E, Ljubljana, Slovenia, pp. 58.
Carroll, J. D. (1968). Generalization canonical correlation analysis three sets
variables. Proceedings American Psychological Association, 227228.
Deerwester, S., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A. (1990).
Indexing latent semantic analysis. Journal American Society Information
Science, 41(6), 391407.
Dumais, S., Letsche, T., Littman, M., & Landauer, T. (1997). Automatic Cross-Language
Retrieval Using Latent Semantic Indexing. AAAI spring symposium crosslanguage text speech retrieval. American Association Artificial Intelligence,
vol. 16. 1997, p. 21.
Fortuna, B., Cristianini, N., & Shawe-Taylor, J. (2006). Kernel methods bioengineering,
communications image processing, chap. Kernel Canonical Correlation Analysis
Learning Semantics Text, pp. 263282. Idea Group Publishing.
Gifi, A. (1990). Nonlinear Multivariate Analysis. Wiley Series Probability Statistics.
Golub, G. H., & Van Loan, C. F. (2012). Matrix computations, Vol. 3. Johns Hopkins
University Press.
Halko, N., Martinsson, P. G., & Tropp, J. A. (2011). Finding Structure Randomness:
Probabilistic Algorithms Constructing Approximate Matrix Decompositions. Society Industrial Applied Mathematics Review, 53 (2), 217288.
Hardoon, D. R., Mourao-Miranda, J., Brammer, M., & Shawe-Taylor, J. (2008). Using
image stimuli drive fMRI analysis. Neural Information Processing, pp. 477486.
Springer.
Hartigan, J. (1975). Clustering algorithms. John Wiley & Sons Inc, New York.
Hotelling, H. (1935). predictable criterion.. Journal educational Psychology,
26 (2), 139.
Kettenring, J. R. (1971). Canonical analysis several sets variables. Biometrika, 58,
43345.
Koehn, P. (2005). Europarl: Parallel Corpus Statistical Machine Translation.
Tenth Machine Translation Summit, Vol. 5, pp. 7986 Phuket, Thailand.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan,
B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., & Herbst, E.
(2007). Moses: Open Source Toolkit Statistical Machine Translation. Proceedings
45th Annual Meeting ACL Interactive Poster Demonstration
Sessions, ACL 07, pp. 177180 Stroudsburg, PA, USA. Association Computational
Linguistics.
314

fiCross-Lingual Document Similarity Event Tracking

Leban, G., Fortuna, B., Brank, J., & Grobelnik, M. (2014a). Cross-lingual detection
world events news articles. Proceedings 13th International Semantic
Web Conference, pp. 2124 Riva del Garda - Trentino, Italy.
Leban, G., Fortuna, B., Brank, J., & Grobelnik, M. (2014b). Event Registry: Learning
World Events News. Proceedings Companion Publication
23rd International Conference World Wide Web Companion, WWW Companion
14, pp. 107110 Seoul, Republic Korea. International World Wide Web Conferences
Steering Committee.
Leetaru, K., & Schrodt, P. A. (2013). GDELT: Global data events, location, tone,
19792012. International Studies Association (ISA) Annual Convention, Vol. 2,
p. 4 San Francisco, California, USA.
Levandowsky, M., & Winter, D. (1971). Distance Sets. Nature, 234 (5323), 3435.
Milne, D., & Witten, I. H. (2008). Learning Link Wikipedia. Proceedings
17th ACM Conference Information Knowledge Management, CIKM 08, pp.
509518 New York, NY, USA. ACM.
Mimno, D., Wallach, H. M., Naradowsky, J., Smith, D. A., & McCallum, A. (2009). Polylingual Topic Models. Proceedings 2009 Conference Empirical Methods
Natural Language Processing: Volume 2 - Volume 2, EMNLP 09, pp. 880889 Singapore. Association Computational Linguistics.
Muhic, A., Rupnik, J., & Skraba, P. (2012). Cross-lingual document similarity. Information Technology Interfaces (ITI), Proceedings ITI 2012 34th International
Conference on, pp. 387392 Cavtat / Dubrovnik, Croatia. IEEE.
Pearson, K. (1901). lines planes closest fit systems points space. Philosophical Magazine, 2 (6), 559572.
Peters, C., & Braschler, M. (2012). Multilingual Information Retrieval. Springer Berlin
Heidelberg, Berlin, Heidelberg.
Platt, J. C., Toutanova, K., & Yih, W.-t. (2010). Translingual document representations
discriminative projections. Proceedings 2010 Conference Empirical
Methods Natural Language Processing, pp. 251261 Massachusetts, USA. Association Computational Linguistics.
Potthast, M., Barron-Cedeno, A., Stein, B., & Rosso, P. (2011). Cross-language Plagiarism
Detection. Language Resources Evaluation, 45 (1), 4562.
Potthast, M., Stein, B., & Anderka, M. (2008). Wikipedia-Based Multilingual Retrieval
Model. Advances Information Retrieval , 30th European Conference Information Retrieval Research (ECIR), pp. 522530 Glasgow, UK.
Pouliquen, B., Steinberger, R., & Deguernel, O. (2008). Story tracking: linking similar news
time across languages. Proceedings Workshop Multi-source Multilingual Information Extraction Summarization, pp. 4956 Manchester, United
Kingdom. Association Computational Linguistics.
Pouliquen, B., Steinberger, R., & Ignat, C. (2006). Automatic annotation multilingual
text collections conceptual thesaurus. arXiv preprint cs/0609059.
315

fiRupnik, Muhic, Leban, Fortuna, Skraba & Grobelnik

Rodrguez, J. M. A., Azcona, E. R., & Paredes, L. P. (2008). Promoting Government
Controlled Vocabularies Semantic Web: EUROVOC Thesaurus
CPV Product Classification System. Semantic Interoperability European Digital
Library, 111.
Rupnik, J., Muhic, A., & Skraba, P. (2011a). Low-rank approximations large, multilingual data. Low Rank Approximation Sparse Representation, Neural Information Processing Systems 2011 Workshop.
Rupnik, J., Muhic, A., & Skraba, P. (2011b). Spanning Spaces: Learning Cross-Lingual Similarities. Beyond Mahalanobis: Supervised Large-Scale Learning Similarity, Neural
Information Processing Systems 2011 Workshop.
Rupnik, J., Muhic, A., & Skraba, P. (2012). Multilingual Document Retrieval Hub
Languages. Proceedings 15th Multiconference Information Society 2012
(IS-2012), pp. 201204 Ljubljana, Slovenia.
Salton, G., & Buckley, C. (1988). Term-weighting approaches automatic text retrieval..
Vol. 24, pp. 513523. Elsevier.
Shawe-Taylor, J., & Cristianini, N. (2004). Kernel Methods Pattern Analysis. Cambridge
University Press.
Steinberger, R., Pouliquen, B., & Ignat, C. (2005). NewsExplorer: Multilingual News Analysis Cross-Lingual Linking. Information Technology Interfaces.
Trampus, M., & Novak, B. (2012). Internals Aggregated Web News Feed.
Proceedings 15th Multiconference Information Society 2012 (IS-2012), pp.
221224 Ljubljana, Slovenia.
Voorhees, E. M., et al. (1999). TREC-8 Question Answering Track Report. Proceedings 8th Text Retrieval Conference (TREC-8), Vol. 99, pp. 7782 Gaithersburg,
MD, USA.
Xiao, M., & Guo, Y. (2013). novel two-step method cross language representation
learning. Advances Neural Information Processing Systems, pp. 12591267 Sateline, NV, USA.
Zhang, D., Mei, Q., & Zhai, C. (2010). Cross-lingual latent topic extraction. Proceedings
48th Annual Meeting Association Computational Linguistics, pp.
11281137 Uppsala, Sweden. Association Computational Linguistics.
Zhang, L., & Rettinger, A. (2014a). Semantic Annotation, Analysis Comparison:
Multilingual Cross-lingual Text Analytics Toolkit. Proceedings Demonstrations 14th Conference European Chapter Association Computational Linguistics (EACL 2014), pp. 1316 Gothenburg, Sweden. Association
Computational Linguistics.
Zhang, L., & Rettinger, A. (2014b). X-LiSA: Cross-lingual Semantic Annotation. Proceedings Large Data Bases (VLDB) Endowment, 7 (13), 16931696.

316

fiJournal Artificial Intelligence Research 55 (2016) 63-93

Submitted 03/15; published 01/16

Cross-Lingual Bridges Models Lexical Borrowing
Yulia Tsvetkov
Chris Dyer

ytsvetko@cs.cmu.edu
cdyer@cs.cmu.edu

Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA

Abstract
Linguistic borrowing phenomenon transferring linguistic constructions (lexical,
phonological, morphological, syntactic) donor language recipient language result contacts communities speaking different languages. Borrowed
words found languages, andin contrast cognate relationshipsborrowing
relationships may exist across unrelated languages (for example, 40% Swahilis
vocabulary borrowed unrelated language Arabic). work, develop
model morpho-phonological transformations across languages. features based
universal constraints Optimality Theory (OT), show compared
several standardbut linguistically navebaselines, OT-inspired model obtains
good performance predicting donor forms borrowed forms dozen
training examples, making cost-effective strategy sharing lexical information
across languages. demonstrate applications lexical borrowing model machine
translation, using resource-rich donor language obtain translations out-of-vocabulary
loanwords lower resource language. framework obtains substantial improvements
(up 1.6 BLEU) standard baselines.

1. Introduction
State-of-the-art natural language processing (NLP) tools, text parsing, speech
recognition synthesis, text speech translation, semantic analysis inference, rely
availability language-specific data resources exist resource-rich
languages. make NLP tools available languages, techniques developed
projecting resources resource-rich languages using parallel (translated) data
bridge cross-lingual part-of-speech tagging (Yarowsky, Ngai, & Wicentowski, 2001;
Das & Petrov, 2011; Li, Graa, & Taskar, 2012; Tckstrm, Das, Petrov, McDonald, &
Nivre, 2013), syntactic parsing (Wu, 1997; Kuhn, 2004; Smith & Smith, 2004; Hwa, Resnik,
Weinberg, Cabezas, & Kolak, 2005; Xi & Hwa, 2005; Burkett & Klein, 2008; Snyder, Naseem,
& Barzilay, 2009; Ganchev, Gillenwater, & Taskar, 2009; Tiedemann, 2014), word sense
tagging (Diab & Resnik, 2002), semantic role labeling (Pad & Lapata, 2009; Kozhevnikov &
Titov, 2013), metaphor identification (Tsvetkov, Boytsov, Gershman, Nyberg, & Dyer, 2014),
others. limiting reagent methods parallel data. small parallel
corpora exist many languages (Smith, Saint-Amand, Plamada, Koehn, Callison-Burch,
& Lopez, 2013), suitably large parallel corpora expensive, typically exist
English geopolitically economically important language pairs.
Furthermore, English high-resource language, linguistically typological
outlier number respects (e.g., relatively simple morphology, complex system verbal
c
2016
AI Access Foundation. rights reserved.

fiTsvetkov & Dyer





pippal

falafel

parpaare

Hebrew

Gawwada

Sanskrit





pilpil

falAfil

pilipili

Persian

Arabic

Swahili

Figure 1: example multilingual borrowing Sanskrit typologically diverse, lowand high-resource languages (Haspelmath & Tadmor, 2009).

auxiliaries, large lexicon, etc.), assumption construction-level parallelism
projection techniques depend thus questionable. Given state affairs,
urgent need methods establishing lexical links across languages rely
large-scale parallel corpora. Without new strategies, 7,000+ languages
worldmany millions speakerswill remain resource-poor standpoint
NLP.
advocate novel approach automatically constructing language-specific resources,
even languages resources raw text corpora. main motivation
research linguistic borrowingthe phenomenon transferring linguistic constructions
(lexical, phonological, morphological, syntactic) donor language recipient
language result contacts communities speaking different languages (Thomason
& Kaufman, 2001). Borrowed words (also called loanwords, e.g., Figure 1) lexical
items adopted another language integrated (nativized) recipient language.
Borrowing occurs typically part minority language speakers, language
wider communication minority language (Sankoff, 2002); one reason
donor languages often bridge resource-rich resource-limited languages.
Borrowing distinctive pervasive phenomenon: languages borrowed
languages point lifetime, borrowed words constitute large fraction
(1070%) language lexicons (Haspelmath, 2009).
Loanword nativization primarily phonological process. Donor words undergo phonological repairs adapt foreign word segmental, phonotactic, suprasegmental
morpho-phonological constraints recipient language (Holden, 1976; Van Coetsem, 1988;
Ahn & Iverson, 2004; Kawahara, 2008; Hock & Joseph, 2009; Calabrese & Wetzels, 2009;
Kang, 2011, inter alia). Common phonological repair strategies include feature/phoneme
epenthesis, elision, degemination, assimilation. speakers encounter foreign
word (either lemma inflected form), analyze morphologically stem,
morphological loanword integration thus amounts selecting appropriate donor
surface form (out existing inflections lemma), applying recipient
language morphology (Repetti, 2006). Adapted loanwords freely undergo recipient
language inflectional derivational processes. Nouns borrowed preferentially,
parts speech, affixes, inflections, phonemes (Whitney, 1881; Moravcsik,
1978; Myers-Scotton, 2002, p. 240).
Although borrowing pervasive topic enduring interest historical
theoretical linguists (Haugen, 1950; Weinreich, 1979), limited work computational
64

fiCross-Lingual Bridges Models Lexical Borrowing

modeling addressed phenomenon. However, topic well-suited computational
models (e.g., systematic phonological changes occur borrowing modeled
using established computational primitives finite state transducers), models
borrowing useful applications. work summarized development
computational model lexical borrowing exploration applications
augment language resources computational approaches NLP resource-limited
languages. Specifically, demonstrate multilingual dictionaries extracted using models
borrowing improve resource-limited statistical machine translation (MT), using pivoting
paradigm borrowing pair translation pair single language
common.
problem address identification plausible donor words (in donor
language) given loanword (in recipient language), vice versa. example, given

Swahili loanword safari journey, model identifies Arabic donor K
Q (sfryh)1
journey (3). Although high level, instance well-known problem
modeling string transductions, interest able identify correspondences across
languages minimal supervision, make technique applicable low-resource
settings. reduce supervision burden, model includes awareness morphophonological repair strategies native speakers language subconsciously employ
adapt loanword phonological constraints recipient language (3.3). end,
use constraint-based theories phonology, exemplified Optimality Theory (OT)
(Prince & Smolensky, 2008; McCarthy, 2009), non-computational linguistic work
demonstrated particularly well suited account phonologically complex borrowing
processes (Kang, 2011). operationalize OT constraints features borrowing
model (3.4). conduct case study Arabic Swahili, two phylogenetically unrelated
languages long history contact; apply model additional language
pairs (3.5). employ models lexical borrowing obtain cross-lingual bridges
loanwords low-resource language donors resource-rich language.
donor language used pivot obtain translations via triangulation out-of-vocabulary
loanwords (4). conduct translation experiments three resource-poor setups:
SwahiliEnglish pivoting via Arabic, MalteseEnglish pivoting via Italic, Romanian
English2 pivoting via French. intrinsic evaluation, ArabicSwahili, ItalianMaltese,
FrenchRomanian borrowing models significantly outperform transliteration cognate
discovery models (5.1). provide systematic quantitative qualitative analysis
contribution integrated translations, relative baselines oracles, corpora
varying sizes (5.2). proposed pivoting approach yields substantial improvements
(up +1.6 BLEU) SwahiliArabicEnglish translation, moderate improvement (up
+0.8 BLEU) MalteseItalianEnglish translation, small (+0.2 BLEU) statistically
significant improvements RomanianFrenchEnglish.
contributions twofold. software implementations
OT (Hayes, Tesar, & Zuraw, 2013), used chiefly facilitate linguistic
analysis; show use OT formulate model learned less
supervision linguistically nave models. best knowledge, first
1. use Buckwalter notation write Arabic glosses.
2. Romanian resource-poor MT perspective, work simulate resource-poor
scenario.

65

fiTsvetkov & Dyer

computational model lexical borrowing used downstream NLP task. Second,
show lexical correspondences induced using model project resourcesnamely,
translationsleading improved performance downstream translation system.3

2. Motivation
task modeling borrowing under-explored computational linguistics, although
important practical applications lends modeling variety
established computational techniques. section first situate task respect
two closely related research directions: modeling transliteration modeling cognate
forms. motivate new line research proposed work: modeling borrowing.
2.1 Borrowing vs. Transliteration
Borrowing transliteration. Transliteration refers writing different orthography,
whereas borrowing refers expanding language include words adapted another
language. Unlike borrowing, transliteration amenable orthographicrather
morpho-phonologicalfeatures, although transliteration also prone phonetic
adaptation (Knight & Graehl, 1998). Borrowed words might begun transliterations,
characteristic borrowed words become assimilated linguistic system
recipient language, became regular content words, example, orange

sugar English words borrowed Arabic l. ' PAK (nArnj) Q@ (Alskr), respectively.
Whatever historical origins, synchronically, words indistinguishable
speakers words native ancestral forms language. Thus, morphophonological processes must accounted borrowing models complex
required transliteration models.
2.2 Borrowing vs. Inheritance
Cognates words related languages inherited single word common
ancestral language (the proto-language). Loanwords, hand, occur
languages, either related not, historically came contact. modeling
perspective, cognates borrowed words require separate investigation loanwords
likely display marginal phonotactic (and phonological) patterns inherited
lexical items. Theoretical analysis cognates tended concerned diachronic
point view, modeling word changes across time. immense scientific
interest, language processing applications arguably better served models synchronic
processes, peculiar loanword analysis.

3. article thoroughly revised extended version work Tsvetkov, Ammar, Dyer
(2015), Tsvetkov Dyer (2015). provide detailed linguistic background lexical
borrowing OT. demonstrate results new language, Maltese, emphasize generality
method. Additional extensions include detailed error analysis complete literature
survey.

66

fiCross-Lingual Bridges Models Lexical Borrowing

2.3 Borrowing?
Borrowing distinctive pervasive phenomenon: languages borrowed
languages point lifetime, borrowed words constitute large fraction
language lexicons. Another important property borrowing adaptation
borrowed items, changes words systematic, knowledge morphological
phonological patterns language used predict borrowings realized
language, without list all. Therefore, modeling borrowing task
well-suited computational approaches.
suggestion work identify borrowing relations resourcerich donor languages (such English, French, Spanish, Arabic, Chinese, Russian)
resource-limited recipient languages. example, 3070% vocabulary Vietnamese,
Cantonese, Thairelatively resource-limited languages spoken hundreds millions
peopleare borrowed Chinese English, languages numerous data
resources created. Similarly, African languages greatly influenced
Arabic, Spanish, English, Frenchwidely spoken languages Swahili, Zulu,
Malagasy, Hausa, Tarifit, Yoruba contain 40% loanwords. Indo-Iranian languages
Hindustani, Hindi, Urdu, Bengali, Persian, Pashtospoken 860 million, also extensively
borrowed Arabic English (Haspelmath & Tadmor, 2009). short, least
billion people speaking resource-scarce languages whose lexicons heavily borrowed
resource-rich languages.
important? Lexical translations alignments extracted large parallel
corpora widely used project annotations high- low-resource languages
(Hwa et al., 2005; Tckstrm et al., 2013; Ganchev et al., 2009, inter alia). Unfortunately,
large-scale parallel resources unavailable majority resource-limited languages.
Loanwords used source cross-lingual links complementary lexical alignments
obtained parallel data bilingual lexicons. holds promise applying existing
cross-lingual methods bootstrapping linguistic resources languages parallel
data available.

3. Constraint-Based Models Lexical Borrowing
task identify plausible donorloan word pairs language pair. modeling
string transductions well-studied problem NLP, wish able learn crosslingual patterns minimal training data. therefore propose model whose features
motivated linguistic knowledgerather overparameterized numerous weakly
correlated features practical large amounts training data available.
features scoring model inspired Optimality Theory (OT; 3.1),
borrowing candidates ranked universal constraints posited underly human
faculty language, candidates determined transduction processes articulated
prior studies contact linguistics.
illustrated Figure 2, model conceptually divided three main parts: (1)
mapping orthographic word forms two languages common phonetic space; (2)
generation loanword pronunciation candidates donor word; (3) ranking
generated loanword candidates, based linguistic constraints donor recipient
67

fiTsvetkov & Dyer

1

donor
word IPA

2

2

Syllabification

2

Phonological
adaptation

1

3

Morphological
adaptation

GEN - generate loanword candidates

Ranking
OT constraints


IPA

loan
word

EVAL - rank candidates

Figure 2: morpho-phonological borrowing model conceptually three main parts: (1)
conversion orthographic word forms pronunciations International Phonetic Alphabet format;
(2) generation loanword pronunciation candidates; (3) ranking generated candidates using
Optimality-Theoretic constraints. Part (1) (2) rule-based, (1) uses pronunciation dictionaries,
(2) based prior linguistic studies; part (3) learned. (3) learn OT constraint weights
dozen automatically extracted training examples.

languages. proposed system, parts (1) (2) rule-based; whereas (3) learned.
component model discussed detail rest section.
model implemented cascade finite-state transducers. Parts (1) (2)
amount unweighted string transformation operations. (1), convert orthographic
word forms pronunciations International Phonetic Alphabet (IPA),
pronunciation transducers. (2) syllabify donor pronunciations, perform insertion,
deletion, substitution phonemes morphemes (affixes), generate multiple
loanword candidates donor word. Although string transformation transducers (2)
generate loanword candidates found recipient language vocabulary,
candidates filtered due composition recipient language lexicon acceptor.
model performs string transformations donor recipient (recapitulating
historical process). However, resulting relation (i.e., final composed transducer)
bidirectional model well used reason underlying donor forms
given recipient forms. probabilistic cascade, Bayes rule could used reverse
direction infer underlying donor forms given loanword. However, instead opt
train model discriminatively find likely underlying form, given loanword.
part (3), candidates evaluated (i.e., scored) weighted sum universal constraint
violations. non-negative weights, call cost vector, constitute model
parameters learned using small training set donorrecipient pairs. use
shortest path algorithm find path minimal cost.
3.1 OT: Constraint-Based Evaluation
Borrowing relations may result quite complex transformations surface.
decision evaluate borrowing candidates weighting counts constraint violations
based Optimality Theory, shown complex surface phenomena
well-explained interaction constraints form outputs relationships
inputs outputs (Kager, 1999).
OT posits surface phonetic words language emerge underlying phonological
forms according two-stage process: first, various candidates surface form
enumerated consideration (the generation gen phase); then, candidates
weighed one another see closely conforms toor equivalently, least
egregiously violatesthe phonological preferences language. preferences
68

fiCross-Lingual Bridges Models Lexical Borrowing

/Eg/
a.

+

dep-io

max-io

onset

no-coda





Eg

b.

Eg@

c.

E

d.

Eg

!


!



!



Table 1: constraint tableau. dep-io max-io onset no-coda ranked OT constraints
according phonological system English. /Eg/ underlying phonological form, (a),
(b) (c), (d) output candidates consideration. actual surface form (a),
incurs lower ranked violations candidates.

[Sarr]
a.

Sarr

b.

Sar.ri

c.
d.

+

*complex

no-coda

!


!

!

dep-io




Sa.ri
Sa.rri

max-io




Table 2: example OT analysis adapted account borrowing. OT constraints ranked
according phonological system recipient language (here, Swahili). donor (Arabic)
($r~) evil considered underlying form. winning surface form (c) Swahili
word Q
loanword shari evil.

correctly characterized, actual surface form selected optimal
realization underlying form. preferences expressed violable constraints
(violable many cases may candidate satisfies them).
two types OT constraints: markedness faithfulness constraints. Markedness constraints (McCarthy & Prince, 1995) describe unnatural (dispreferred) patterns
language. Faithfulness constraints (Prince & Smolensky, 2008) reward correspondences
underlying form surface candidates. clarify distinction
faithfulness markedness constraint groups NLP readership, draw
following analogy components machine translation speech recognition: faithfulness constraints analogical translation model acoustic model (reflecting
well output candidate appropriate input), markedness constraints
analogical language model (requiring well-formedness output candidate).
Without faithfulness constraints, optimal surface form could differ arbitrarily
underlying form. originally proposed, OT holds set constraints universal,
ranking language-specific.
OT, then, grammar set universal constraints language-specific
ranking, derivation surface form consists underlying form, surface
candidates, constraint violations candidates (under surface form
correctly chosen). example OT analysis shown Table 1; OT constraints
explained later, Tables 3 4.
OT adapted account borrowing treating donor language word
underlying form recipient language; is, phonological system
69

fiTsvetkov & Dyer

recipient language encoded system constraints, constraints account
donor word adapted borrowed. show example Table 2.
substantial prior work linguistics borrowing OT paradigm (Yip, 1993;
Davidson & Noyer, 1997; Jacobs & Gussenhoven, 2000; Kang, 2003; Broselow, 2004; Adler,
2006; Rose & Demuth, 2006; Kenstowicz & Suchato, 2006; Kenstowicz, 2007; Mwita, 2009),
none led computational realizations.
OT assumes ordinal constraint ranking strict dominance rather constraint
weighting. that, OT-inspired model departs OTs standard evaluation assumptions: following Goldwater Johnson (2003), use linear scoring scheme.
3.2 Case Study: ArabicSwahili Borrowing
section, use ArabicSwahili4 language-pair describe prototypical
linguistic adaptation processes words undergo borrowed. Then, describe
model processes general terms.
Swahili lexicon influenced Arabic result prolonged period
language contact due Indian Ocean trading (800 ce1920), well influence
Islam (Rothman, 2002). According several independent studies, Arabic loanwords
constitute 18% (Hurskainen, 2004b) 40% (Johnson, 1939) Swahili word types.
Despite strong susceptibility Swahili borrowing large fraction Swahili
words originating Arabic, two languages typologically distinct profoundly
dissimilar phonological morpho-syntactic systems. survey systems briefly
since illustrate Arabic loanwords substantially adapted conform
Swahili phonotactics. First, Arabic five syllable patterns:5 CV, CVV, CVC, CVCC,
CVVC (McCarthy, 1985, pp. 2328), whereas Swahili (like Bantu languages)
open syllables form CV V. segment level, Swahili loanword adaptation thus
involves extensive vowel epenthesis consonant clusters syllable final position

syllable ends consonant, example, H
. AJ (ktAb) kitabu book (Polom, 1967;
Schadeberg, 2009; Mwita, 2009). Second, phonological adaptation Swahili loanwords
includes shortening vowels (unlike Arabic, Swahili phonemic length);
substitution consonants found Arabic Swahili (e.g., emphatic
(pharyngealized) /tQ //t/, voiceless velar fricative /x//k/, dental fricatives /T//s/,
/D//z/, voiced velar fricative /G//g/); adoption Arabic phonemes

originally present Swahili /T/, /D/, /G/ (e.g., QK
Ym' (tH*yr) tahadhari warning);
($r~) shari evil). Finally, adapted
degemination Arabic geminate consonants (e.g., Q
loanwords freely undergo Swahili inflectional derivational processes, example,
(Alwzyr) waziri minister, mawaziri ministers, kiuwaziri ministerial (Zawawi,
QK
P@
1979; Schadeberg, 2009).

4. simplicity, subsume Omani Arabic historical dialects Arabic label Arabic;
data examples Modern Standard Arabic. Similarly, subsume Swahili, dialects
protolanguages Swahili.
5. C stands consonant, V vowel.

70

fiCross-Lingual Bridges Models Lexical Borrowing

3.3 ArabicSwahili Borrowing Transducers
use unweighted transducers pronunciation, syllabification, morphological
phonological adaptation describe below. example illustrates
possible string transformations individual components model shown Figure 3.
goal transducers minimally overgenerate Swahili adapted forms Arabic
words, based adaptations described above.
Arabic word
IPA
kuttaba



kitaba
...

Syllabification
ku.tta.ba.
ku.t.ta.ba.
...
ki.ta.ba.
ki.ta.b.
...

Phonological
adaptation

Morphological
adaptation

ku.ta.ba. [degemination]
ku.tata.ba. [epenthesis]
ku.ta.bu. [final vowel subst.]
ki.ta.bu. [final vowel subst.]
ki.ta.bu. [epenthesis]
...

Figure 3: example Arabic word
Swahili loanword kitabu.

ku.tata.ba.li.
ku.tata.ba.
vi.ki.ta.bu.
ki.ta.bu.
ki.ta.bu.
...

Ranking
OT constraints

IPA
Swahili word

ku.ta<DEP-V>ta<PEAK>.ba.li<DEP-MORPH>.
ku.ta<DEP-V>ta<PEAK>.ba.li.
kitabu
ku.tta<*COMPLEX>.ba.
ki.ta.bu<IDENT-IO-V>.
ki.ta.bu<DEP-V>.
vi<DEP-MORPH>.ki.ta.bu<IDENT-IO-V>.

AK. AJ (ktAbA) book.sg.indef transformed model

3.3.1 Pronunciation
Based IPA, assign shared symbols sounds exist sound systems
Arabic Swahili (e.g., nasals /n/, /m/; voiced stops /b/, /d/), language-specific
unique symbols sounds unique phonemic inventory Arabic (e.g.,
pharyngeal voiced voiceless fricatives //, /Q/) Swahili (e.g., velar nasal /N/).
Swahili, construct pronunciation dictionary based Omniglot grapheme-to-IPA
mapping.6 Arabic, use CMU Arabic vowelized pronunciation dictionary containing
700K types average four pronunciations per unvowelized input word
type (Metze, Hsiao, Jin, Nallasamy, & Schultz, 2010).7 design four transducers
Arabic Swahili word-to-IPA IPA-to-word transducerseach union linear
chain transducers, well one acceptor per pronunciation dictionary listing.
3.3.2 Syllabification
Arabic words borrowed Swahili undergo repair violations Swahili segmental
phonotactic constraints, example via vowel epenthesis consonant cluster. Importantly,
repair depends upon syllabification. simulate plausible phonological repair processes,
generate multiple syllabification variants input pronunciations. syllabification
transducer optionally inserts syllable separators phones. example,
input phonetic sequence /kuttAbA/, output strings include /ku.t.tA.bA/, /kut.tA.bA/,
/ku.ttA.bA/ syllabification variants; variant violates different constraints
consequently triggers different phonological adaptations.
6. www.omniglot.com
7. Since working level word types context, cannot disambiguate
intended form, include options. example, input word AK. AJ (ktAbA) book.sg.indef,
use pronunciations /kitAbA/ /kuttAbA/.

71

fiTsvetkov & Dyer

3.3.3 Phonological Adaptation
Phonological adaptation syllabified phone sequences crux loanword adaptation
process. implement phonological adaptation transducers composition plausible
context-dependent insertions, deletions, substitutions phone subsets, based prior
studies summarized 3.2. follows, list phonological adaptation components
order transducer composition borrowing model. vowel deletion transducer
shortens Arabic long vowels vowel clusters. consonant degemination transducer
shortens Arabic geminate consonants, example, degeminates /tt/ /ku.ttA.bA/,
outputting /ku.tA.bA/. substitution similar phonemes transducer substitutes
similar phonemes phonemes found Arabic Swahili (Polom,
1967, p. 45). example, emphatic /tQ /, /dQ /, /sQ / replaced corresponding
non-emphatic segments [t], [d], [s]. vowel epenthesis transducer inserts vowel
pairs consonants (/ku.ttA.bA/ /ku.tatA.bA/), end syllable,
syllable ends consonant (/ku.t.tA.bA/ /ku.ta.tA.bA/). Sometimes possible
predict final vowel word, depending word-final coda consonant
Arabic counterpart: /u/ /o/ added Arabic donor ends labial, /i/
/e/ added coronals dorsals (Mwita, 2009). Following rules, final vowel
substitution transducer complements inventory final vowels loanword candidates.
3.3.4 Morphological Adaptation
Arabic Swahili significant morphological processes alter appearance
lemmas. deal morphological variants, construct morphological adaptation
transducers optionally strip Arabic concatenative affixes clitics, optionally
append Swahili affixes, generating superset possible loanword hypotheses.
obtain list Arabic affixes Arabic morphological analyzer SAMA (Maamouri,
Graff, Bouziri, Krouna, & Kulick, 2010); Swahili affixes taken hand-crafted
Swahili morphological analyzer (Littell, Price, & Levin, 2014). sake simplicity
implementation, strip one Arabic prefix one suffix per
word; Swahili concatenate two Swahili prefixes one suffix.
3.4 Learning Constraint Weights
Due computational problems working OT (Eisner, 1997, 2002), make
simplifying assumptions (1) bounding theoretically infinite set underlying forms
small linguistically-motivated subset allowed transformations donor pronunciations,
described 3.3; (2) imposing priori restrictions set surface realizations
intersecting candidate set recipient pronunciation lexicon; (3) assuming
set constraints finite regular (Ellison, 1994); (4) assigning linear weights
constraints, rather learning ordinal constraint ranking strict dominance
(Boersma & Hayes, 2001; Goldwater & Johnson, 2003).
discussed 3.1, OT distinguishes markedness constraints detect dispreferred
phonetic patterns language, faithfulness constraints, ensure correspondences
underlying form surface candidates. implemented constraints
listed Tables 3 4. Faithfulness constraints integrated phonological transformation
components transitions following insertion, deletion, substitution. Markedness
72

fiCross-Lingual Bridges Models Lexical Borrowing

Faithfulness constraints
max-io-morph (donor) affix deletion
max-io-c
consonant deletion
max-io-v
vowel deletion
dep-io-morph (recipient) affix epenthesis
dep-io-v
vowel epenthesis
ident-io-c
consonant substitution
ident-io-c-m
substitution manner pronunciation
ident-io-c-a
substitution place articulation
ident-io-c-s
substitution sonority
ident-io-c-p
pharyngeal consonant substitution
ident-io-c-g
glottal consonant substitution
ident-io-c-e
emphatic consonant substitution
ident-io-v
vowel substitution
ident-io-v-o
substitution vowel openness
ident-io-v-r
substitution vowel roundness
ident-io-v-f
substitution vowel frontness
ident-io-v-fin final vowel substitution
Table 3: Faithfulness constraints prefer pronounced realizations completely congruent
underlying forms.

Markedness constraints
no-coda
syllables must coda
onset
syllables must onsets
peak
one syllabic peak
ssp
complex onsets rise sonority,
complex codas fall sonority
*complex-s consonant clusters syllable margins
*complex-c consonant clusters within syllable
*complex-v vowel clusters
Table 4: Markedness constraints impose language-specific structural well-formedness surface
realizations.

constraints implemented standalone identity transducers: inputs equal outputs,
path weights representing candidate evaluation respect violated constraints
different.
final loanword transducer composition transducers described 3.3
OT constraint transducers. path transducer represents syllabified phonemic
sequence along (weighted) OT constraints violates, shortest path outputs
those, whose cumulative weight violated constraints minimal.
OT constraints realized features linear model, feature weights
learned discriminative training maximize accuracy obtained loanword
transducer small development set donorrecipient pairs. parameter estimation,
employ NelderMead algorithm (Nelder & Mead, 1965), heuristic derivative-free
73

fiTsvetkov & Dyer

method iteratively optimizes, based objective function evaluation, convex
hull n + 1 simplex vertices.8 objective function used work soft accuracy
development set, defined proportion correctly identified donor words
total set 1-best outputs.
3.5 Adapting Model New Language
ArabicSwahili case study shows that, principle, borrowing model constructed.
reasonable question ask is: much work required build similar system
new language pair? claim design permits rapid development new
language pairs. First, string transformation operations, well OT constraints
language-universal. adaptation required linguistic analysis identify plausible
morpho-phonological repair strategies new language pair (i.e., subset allowed
insertions, deletions, substitutions phonemes morphemes). Since need
overgenerate candidates (the OT constraints filter bad outputs), effort
minimal relative many grammar engineering exercises. second language-specific
component grapheme-to-IPA converter. non-trivial problem
cases, problem well studied, many under-resourced languages (e.g., Swahili),
phonographic systems orthography corresponds phonology. tendency
explained fact that, many cases, lower-resource languages developed
orthography relatively recently, rather organically evolved written forms
preserve archaic idiosyncratic spellings distantly related current
phonology language see English.
illustrate ease language pair engineered, applied
borrowing model ItalianMaltese FrenchRomanian language pairs. Maltese
Romanian, like Swahili, large number borrowed words lexicons (Tadmor,
2009). Maltese (a phylogenetically Semitic language) 35.1%30.3% loanwords Romance
(Italian/Sicilian) origin (Comrie & Spagnol, 2015). Although French Romanian
sister languages (both descending Latin), 12% Romanian types French
borrowings came language past centuries (Schulte, 2009).
language pairs manually define set allowed insertions, deletions, substitutions
phonemes morphemes, based training sets. set Maltese affixes
defined based linguistic survey Fabri, Gasser, Habash, Kiraz, Wintner (2014).
employ GlobalPhone pronunciation dictionary French (Schultz & Schlippe,
2014), converted IPA, automatically constructed Italian, Romanian, Maltese
pronunciation dictionaries using Omniglot grapheme-to-IPA conversion rules
languages.

4. Models Lexical Borrowing Statistical Machine Translation
turning experimental verification analysis borrowing model,
introduce external application borrowing model used component
machine translation. rely borrowing model project translation information
8. decision use NelderMead rather conventional gradient-based optimization algorithms
motivated purely practical limitations finite-state toolkit used made computing
derivatives latent structure impractical engineering standpoint.

74

fiCross-Lingual Bridges Models Lexical Borrowing

high-resource donor language low-resource recipient language, thus mitigating
deleterious effects out-of-vocabulary (OOV) words.
OOVs ubiquitous difficult problem MT. translation system encounters
OOVa word observed training data, trained system thus
lacks translation variantsit usually outputs word source language,
producing erroneous disfluent translations. MT systems, even trained billionsentence-size parallel corpora, encounter OOVs test time. Often, named
entities neologisms. However, OOV problem much acute morphologicallyrich low-resource scenarios: there, OOVs primarily lexicon-peripheral items
names specialized/technical terms, also regular content words. Since borrowed
words component regular lexical content language, projecting translations
onto recipient language identifying borrowed lexical material plausible strategy
solving problem.
Procuring translations OOVs subject active research decades.
Translation named entities usually generated using transliteration techniques (AlOnaizan & Knight, 2002; Hermjakob, Knight, & Daum III, 2008; Habash, 2008). Extracting
translation lexicon recovering OOV content words phrases done mining
bi-lingual monolingual resources (Rapp, 1995; Callison-Burch, Koehn, & Osborne, 2006;
Haghighi, Liang, Berg-Kirkpatrick, & Klein, 2008; Marton, Callison-Burch, & Resnik, 2009;
Razmara, Siahbani, Haffari, & Sarkar, 2013; Saluja, Hassan, Toutanova, & Quirk, 2014).
addition, OOV content words recovered exploiting cognates, transliterating
pivoting via closely-related resource-richer language, language
exists (Haji, Hric, & Kubo, 2000; Mann & Yarowsky, 2001; Kondrak, Marcu, & Knight,
2003; De Gispert & Marino, 2006; Habash & Hu, 2009; Durrani, Sajjad, Fraser, & Schmid,
2010; Wang, Nakov, & Ng, 2012; Nakov & Ng, 2012; Dholakia & Sarkar, 2014). work
similar spirit latter pivoting approach, show obtain translations
OOV content words pivoting via unrelated, often typologically distant resource-rich
language.
solution depicted, high level, Figure 4. Given OOV word resource-poor
MT, use borrowing system identify list likely donor words donor
language. Then, using MT system resource-rich language, translate donor
words target language resource-poor MT (here, English). Finally,
integrate translation candidates resource-poor system.
discuss integrating translation candidates acquired via borrowing plus resourcerich translation.
Briefly, phrase-based translation works follows. set candidate translations
input sentence created matching contiguous spans input inventory
phrasal translations, reordering target-language appropriate order, choosing
best one according model combines features phrases used, reordering
patterns, target language model (Koehn, Och, & Marcu, 2003). limitation
approach generate input/output phrase pairs directly observed
training corpus. resource-limited languages, standard phrasal inventory
generally incomplete due limited parallel data. Thus, decoders hope
producing good output find fluent, meaning-preserving translation using incomplete
translation lexicons. Synthetic phrases strategy integrating translated phrases
75

fiSWAHILIENGLISH

safari |||
kituruki |||

*OOV*
*OOV*

ARABICENGLISH

( ysAfr) ||| travel

TRANSLATION
CANDIDATES

ARABIC -to- SWAHILI
BORROWING

Tsvetkov & Dyer

( trky) ||| turkish

Figure 4: improve resource-poor SwahiliEnglish MT system, extract translation candidates
OOV Swahili words borrowed Arabic using Swahili-to-Arabic borrowing system
ArabicEnglish resource-rich MT.

directly MT translation model, rather via pre- post-processing MT inputs
outputs (Tsvetkov, Dyer, Levin, & Bhatia, 2013; Chahuneau, Schlinger, Smith, & Dyer, 2013;
Schlinger, Chahuneau, & Dyer, 2013; Ammar, Chahuneau, Denkowski, Hanneman, Ling,
Matthews, Murray, Segall, Tsvetkov, Lavie, & Dyer, 2013; Tsvetkov, Metze, & Dyer, 2014;
Tsvetkov & Dyer, 2015). Synthetic phrases phrasal translations directly
extractable training data, generated auxiliary translation postediting
processes (for example, extracted borrowing model). important advantage
synthetic phrases process often benefits phrase synthesizers high
recall (relative precision) since global translation model still final say
whether synthesized phrase used.
OOV, borrowing system produces n-best list plausible donors;
donor extract k-best list translations.9 Then, pair OOV
resulting n k translation candidates. translation candidates noisy:
generated donors may erroneous, errors propagated translation.10
allow low-resource translation system leverage good translations missing
default phrase inventory, able learn trustworthy are,
integrate borrowing-model acquired translation candidates synthetic phrases.
let translation model learn whether trust phrases, translation options
obtained borrowing model augmented indicator feature indicating
phrase generated externally (i.e., rather extracted parallel
data). Additional features assess properties donorloan words relation; goal
provide indication plausibility pair (to mark possible errors outputs
borrowing system). employ two types features: phonetic semantic. Since borrowing
primarily phonological phenomenon, phonetic features provide indication
9. set n k 5; experiment values.
10. give input borrowing system OOV words, although, clearly, OOVs loanwords,
loanword OOVs borrowed donor language. However, important property
borrowing model operations general, specific language-pair reduced
small set plausible changes donor word undergo process assimilation
recipient language. Thus, borrowing system minimally overgenerates set output
candidates given input. borrowing system encounters input word borrowed
target donor language, usually (but always) produces empty output.

76

fiCross-Lingual Bridges Models Lexical Borrowing

typical (or atypical) pronunciation word language; loanwords expected
less typical core vocabulary words. goal semantic features measure
semantic similarity donor loan words: erroneous candidates borrowed
words changed meaning time expected different meaning
OOV.
4.1 Phonetic Features
compute phonetic features first train (5-gram) language model (LM) IPA pronunciations donor/recipient language vocabulary (p ). Then, re-score pronunciations
donor loanword candidates using LMs. hypothesize donorloanword
pairs donor loanword phone LM score high. capture intuition
three features: f1 = p (donor), f2 = p (loanword), harmonic mean two
1 f2
scores f3 = f2f1 +f
(the harmonic mean set values high values
2
high).
4.2 Semantic Features
compute semantic similarity feature candidate donor OOV
loanword follows. first train, using large monolingual corpora, 100-dimensional word
vector representations donor recipient language vocabularies.11 Then, employ
canonical correlation analysis (CCA) small donorloanword dictionaries (training sets
borrowing models) project word embeddings 50-dimensional vectors
maximized correlation dimensions. semantic feature annotating
synthetic translation candidates cosine distance resulting donor loanword
vectors. use word2vec Skip-gram model (Mikolov, Sutskever, Chen, Corrado, &
Dean, 2013) train monolingual vectors,12 CCA-based tool (Faruqui & Dyer, 2014)
projecting word vectors.13

5. Experiments
turn problem empirically validating model proposed.
evaluation consists two parts. First, perform intrinsic assessment models
ability learn borrowing correspondences compare similar approaches
use less linguistic knowledge used solve similar string mapping
problems. Second, show effect borrowing-augmented translations translation
systems, exploring effects features proposed above.
5.1 Intrinsic Evaluation Models Lexical Borrowing
experimental setup defined follows. input borrowing model
loanword candidate Swahili/Maltese/Romanian, outputs plausible donor words
Arabic/Italian/French monolingual lexicon (i.e., word pronunciation dictionary).
11. assume parallel data limited recipient language, monolingual data available.
12. code.google.com/p/word2vec
13. github.com/mfaruqui/eacl14-cca

77

fiTsvetkov & Dyer

train borrowing model using small set training examples, evaluate
using held-out test set. rest section describe detail datasets, tools,
experimental results.
5.1.1 Resources
employ ArabicEnglish SwahiliEnglish bitexts extract training set (corpora
sizes 5.4M 14K sentence pairs, respectively), using cognate discovery technique
(Kondrak, 2001). Phonetically semantically similar strings classified cognates;
phonetic similarity string similarity phonetic representations, semantic
similarly approximated translation.14 thereby extract Arabic Swahili pairs
(a,s)
ha, si phonetically similar ( min(|a|,|s|)
< 0.5) (a, s) Levenshtein
distance aligned English word e. FastAlign (Dyer,
Chahuneau, & Smith, 2013) used word alignments. Given extracted word pair
ha, si, also extract word pairs {ha0 , si} proper Arabic words a0 share
lemma producing average 33 Arabic types per Swahili type. use MADA
(Habash, Rambow, & Roth, 2009) Arabic morphological expansion.
resulting dataset 490 extracted ArabicSwahili borrowing examples,15
set aside randomly sampled 73 examples (15%) evaluation,16 use remaining 417
examples model parameter optimization. ItalianMaltese language pair, use
technique extract 425 training 75 (15%) randomly sampled test examples.
FrenchRomanian language pair, use existing small annotated set borrowing
examples,17 282 training 50 (15%) randomly sampled test examples.
use pyfsta Python interface OpenFst (Allauzen, Riley, Schalkwyk, Skut, &
Mohri, 2007)for borrowing model implementation.18
5.1.2 Baselines
compare model several baselines. Levenshtein distance baselines chose
closest word (either surface pronunciation-based). cognates baselines,
evaluate variant Levenshtein distance tuned identify cognates (Mann & Yarowsky,
2001; Kondrak & Sherif, 2006); method identified Kondrak Sherif (2006)
among top three cognate identification methods. transliteration baselines
generate plausible transliterations input Swahili (or Romanian) words donor
lexicon using model Ammar, Dyer, Smith (2012), multiple references
lattice without reranking. CRF transliteration model linear-chain CRF
label source character sequence target characters. features label
unigrams, label bigrams, label conjoined moving window source characters.
OT-uniform baselines, evaluate accuracy borrowing model uniform
14. cognate discovery technique sufficient extract small training set, generally applicable,
requires parallel corpora manually constructed dictionaries measure semantic similarity. Large
parallel corpora unavailable language pairs, including SwahiliEnglish.
15. training/test example one Swahili word corresponds extracted Arabic donor words.
16. manually verified test set contains clear ArabicSwahili borrowings. example, extract



Swahili kusafiri, safari Arabic Q@, Q
, Q (Alsfr, ysAfr, sfr) aligned travel.
17. http://wold.clld.org/vocabulary/8
18. https://github.com/vchahun/pyfst

78

fiCross-Lingual Bridges Models Lexical Borrowing

Reachability
Ambiguity

arsw
87.7%
857

itmt
92.7
11

frro
82.0%
12

Table 5: evaluation borrowing model design. Reachability percentage donor
recipient pairs reachable donor recipient language. Ambiguity average
number outputs model generates per one input.

weights, thus shortest path loanwords transducer forms violate
fewest constraints.
5.1.3 Evaluation
addition predictive accuracy models (if model produces multiple hypotheses
1-best weight, count proportion correct outputs set),
evaluate two particular aspects proposed model: (1) appropriateness model
family, (2) quality learned OT constraint weights. first aspect designed
evaluate whether morpho-phonological transformations implemented model
required sufficient generate loanwords donor inputs. report two
evaluation measures: model reachability ambiguity. Reachability percentage
test samples reachable (i.e., path input test example correct
output) loanword transducer. nave model generates possible strings
would score 100% reachability; however, inference may expensive discriminative
component greater burden. order capture trade-off, also report
inherent ambiguity model, average number outputs potentially
generated per input. generic ArabicSwahili transducer, example, ambiguity
786,998the size Arabic pronunciation lexicon.19
5.1.4 Results
reachability ambiguity borrowing model listed Table 5. Briefly,
model obtains high reachability, significantly reducing average number possible
outputs per input: Arabic 787K 857 words, Maltese 129K 11, French
62K 12. result shows loanword transducer design, based prior
linguistic analysis, plausible model word borrowing. Yet, average 33
correct Arabic words possible 857 outputs, thus second part modelOT
constraint weights optimizationis crucial.
accuracy results Table 6 show challenging task modeling lexical
borrowing two distinct languages is, importantly, orthographic
phonetic baselines including state-of-the-art generative model transliteration
suitable task. Phonetic baselines ArabicSwahili perform better orthographic
ones, substantially worse OT-based models, even OT constraints weighted.
Crucially, performance borrowing model learned OT weights corroborates
19. measure ambiguity equivalent perplexity assuming uniform distribution output forms.

79

fiTsvetkov & Dyer

Orthographic baselines
Phonetic baselines

OT

Levenshtein-orthographic
Transliteration
Levenshtein-pronunciation
Cognates
OT-uniform constraint weights
OT-learned constraint weights

Accuracy (%)
arsw itmt frro
8.9
61.5
38.0
16.4
61.3
36.0
19.8
64.4
26.3
19.7
63.7
30.7
29.3
65.6
58.5
48.4
83.3
75.6

Table 6: evaluation borrowing model accuracy. compare following setups:
orthographic (surface) phonetic (based pronunciation lexicon) Levenshtein distance, cognate
identification model uses heuristic Levenshtein distance lower penalty vowel updates
similar letter/phone substitutions, CRF transliteration model, model uniform
learned OT constraint weights assignment.

assumption made numerous linguistic accounts OT adequate analysis
lexical borrowing phenomenon.
5.1.5 Qualitative Evaluation
constraint ranking learned borrowing model (constraints listed Tables 3, 4)
line prior linguistic analysis. Swahili no-coda dominates markedness
constraints. *complex-s *complex-c, restricting consonant clusters, dominate
*complex-v, confirming Swahili permissive vowel clusters. sspsonority-based
constraintcaptures common pattern consonant clustering, found across languages,
also learned model undominated competitors Swahili,
dominating markedness constraint Romanian. Morphologically-motivated constraints
also comply tendencies discussed linguistic literature: donor words may remain
unmodified treated stem, reinfected according recipient
morphology, thus dep-io-morph dominated easily max-io-morph. Finally,
vowel epenthesis dep-io-v common strategy Arabic loanword adaptation,
ranked lower according model; however, ranked highly FrenchRomanian
model, vowel insertion rare.
second interesting by-product model inferred syllabification.
conduct systematic quantitative evaluation, higher-ranked Swahili outputs tend
contain linguistically plausible syllabifications, although syllabification transducer inserts
optional syllable boundaries every pair phones. result attests
plausible constraint ranking learned model. Example Swahili syllabifications20 along
constraint violations produced borrowing model depicted Table 7.
5.2 Extrinsic Evaluation Pivoting via Borrowing MT
turn extrinsic evaluation, looking two low-resource translation tasks: Swahili
English translation (resource-rich donor language: Arabic), MalteseEnglish translation
20. chose examples ArabicSwahili system challenging case due
linguistic discrepancies.

80

fiCross-Lingual Bridges Models Lexical Borrowing

en
book
palace

ar orth.
ktAb
AlqSr

ar pron.
kitAb
AlqaSr

sw syl.
ki.ta.bu.
ka.sri

wage

Ajrh

Aujrah

u.ji.ra.

Violated constraints
ident-io-c-ghA, ai, dep-io-vh, ui
max-io-morphhAl, i, ident-io-c-shq, ki,
ident-io-c-ehS, si, *complex-chsri, dep-io-vh, ii
max-io-vhA, i, onsethui ,
dep-io-vh, ii, max-io-chh,

Table 7: Examples inferred syllabification corresponding constraint violations produced
borrowing model.

(resource-rich donor language: Italian), RomanianEnglish translation (resource-rich
donor language: French). begin reviewing datasets used, discuss two
oracle experiments attempt quantify much value could obtain perfect
borrowing model (since mistakes made MT systems involve borrowed words).
Armed understanding, explore much improvement obtained
using system.
5.2.1 Datasets Software
SwahiliEnglish parallel corpus crawled Global Voices project website21 .
MalteseEnglish language pair, sample parallel corpus size
EUbookshop corpus OPUS collection (Tiedemann, 2012). Similarly, simulate
resource-poor scenario RomanianEnglish language pair, sample corpus
transcribed TED talks (Cettolo, Girardi, & Federico, 2012). evaluate translation
improvement corpora different sizes conduct experiments sub-sampled 4,000,
8,000, 14,000 parallel sentences training corpora (the smaller training
corpus, OOVs has). Corpora sizes along statistics source-side OOV
tokens types given Table 8. Statistics held-out dev test sets used
translation experiments given Table 9.
ArabicEnglish pivot translation system trained parallel corpus 5.4
million sentences available Linguistic Data Consortium (LDC), optimized
standard NIST MTEval dataset year 2005 (MT05). ItalianEnglish system
trained 11 million sentences OPUS corpus. FrenchEnglish pivot system
trained 400,000 sentences transcribed TED talks, optimized
dev talks RomanianEnglish system; test talks RomanianEnglish
system removed FrenchEnglish training corpus.
MT experiments, use cdec22 translation toolkit (Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, & Resnik, 2010), optimize parameters
MERT (Och, 2003). English 4-gram language models Kneser-Ney smoothing
(Kneser & Ney, 1995) trained using KenLM (Heafield, 2011) target side
parallel training corpora Gigaword corpus (Parker, Graff, Kong, Chen, & Maeda,
2009). Results reported using case-insensitive BLEU single reference (Papineni,
Roukos, Ward, & Zhu, 2002). verify improvements consistent
21. sw.globalvoicesonline.org
22. www.cdec-decoder.org

81

fiTsvetkov & Dyer

swen

mten

roen

Tokens
Types
OOV tokens
OOV types
Tokens
Types
OOV tokens
OOV types
Tokens
Types
OOV tokens
OOV types

4K
84,764
14,554
4,465 (12.7%)
3,610 (50.3%)
104,181
14,605
4,735(8.7%)
4,171 (44.0%)
35,978
7,210
3,268 (16.6%)
2,382 (55.0%)

8K
170,493
23,134
3,509 (10.0%)
2,950 (41.1%)
206,781
22,407
3,497 (6.4%)
3,236 (34.2%)
71,584
11,144
2,585 (13.1%)
1,922 (44.4%)

14K
300,648
33,288
2,965 (8.4%)
2,523 (35.1%)
358,373
31,176
2,840 (5.2%)
2,673 (28.2%)
121,718
15,112
2,177 (11.1%)
1,649 (38.1%)

Table 8: Statistics SwahiliEnglish, MalteseEnglish, RomanianEnglish corpora
source-side OOV rates 4K, 8K, 14K parallel training sentences.

Sentences
Tokens
Types

swen
dev
test
1,552
1,732
33,446 35,057
7,008
7,180

mten
dev
test
2,000
2,000
54,628 54,272
9,508
9,471

roen
dev
test
2,687
2,265
24,754 19,659
5,141
4,328

Table 9: Dev test corpora sizes.

effect optimizer instability, train three systems MT setup; reported
BLEU scores averaged systems.
5.2.2 Upper Bounds
goal experiments evaluate contribution OOV dictionaries
extract pivoting via borrowing, also understand potential contribution exploiting borrowing. overall improvement would achieved
could correctly translate OOVs borrowed another language?
overall improvement achieved correctly translate OOVs? answer
question defining upper bound experiments. upper bound experiments
word-align available parallel corpora, including dev test sets, extract
alignments oracle translations OOV words. Then, append extracted OOV
dictionaries training corpora re-train SMT setups without OOVs. Translation
scores resulting system provide upper bound improvement correctly
translating OOVs. append oracle translations subset OOV dictionaries,
particular translations OOVs output borrowing system
empty, obtain upper bound achieved using method (if borrowing
system provided perfect outputs relative reference translations). Understanding
82

fiCross-Lingual Bridges Models Lexical Borrowing

4K
5,050
10,138
347

Loan OOVs swen
Loan OOVs mten
Loan OOVs roen

8K
4,219
6,456
271

14K
3,577
4,883
216

Table 10: size dictionaries extracted using pivoting via borrowing integrated translation
models.

Transliteration OOVs swen
Transliteration OOVs mten
Transliteration OOVs roen

4K
49
26,734
906

8K
32
19,049
714

14K
22
15,008
578

Table 11: size translated lexicons extracted using pivoting via transliteration integrated
translation models.

upper bounds relevant experiments, experiments involve
augmenting translation dictionaries; however, aware prior work providing
similar analysis upper bounds, recommend calibrating procedure
future work OOV mitigation strategies.
5.2.3 Borrowing-Augmented Setups
described 4, integrate translations OOV loanwords translation model using
synthetic phrase paradigm. Due data sparsity, conjecture non-OOVs
occur times training corpus also lack appropriate translation candidates,
target-language OOVs. therefore plug borrowing system OOVs
non-OOV words occur less 3 times training corpus. list Table 10
size resulting borrowed lexicons integrate translation tables.23
5.2.4 Transliteration-Augmented Setups
addition standard baselines, evaluate transliteration baselines, replace
borrowing model baselines described 5.1. borrowing system,
transliteration outputs filtered contain target language lexicons. list
Table 11 size obtained translated lexicons.
5.2.5 Results
Translation results shown Table 12. evaluate separately contribution
integrated OOV translations, translations annotated phonetic
semantic features. also provide upper bound scores integrated loanword dictionaries
well recovering OOVs.
23. Differences statistics stem differences types corpora, genre, domain, morphological
richness source language.

83

fiTsvetkov & Dyer

swen

mten

roen

Baseline
+ Transliteration OOVs
+ Loan OOVs
+ Features
Upper bound loan
Upper bound OOVs
Baseline
+ Transliteration OOVs
+ Loan OOVs
+ Features
Upper bound loan
Upper bound OOVs
Baseline
+ Transliteration OOVs
+ Loan OOVs
+ Features
Upper bound loan
Upper bound OOVs

4K
13.2
13.4
14.3
14.8
18.9
19.2
26.4
26.5
27.2
26.9
28.5
31.6
15.8
15.8
16.0
16.0
16.6
28.0

8K
15.1
15.3
15.7
16.4
19.1
20.4
31.4
30.8
31.7
31.9
32.2
35.6
18.5
18.7
18.7
18.6
19.4
28.8

14K
17.1
17.2
18.2
18.4
20.7
21.1
35.2
34.9
35.3
34.5
35.7
38.0
20.7
20.8
20.7
20.6
20.9
30.4

Table 12: BLUE scores SwahiliEnglish, MalteseEnglish, RomanianEnglish MT
experiments.

SwahiliEnglish MT performance improved +1.6 BLEU augment
translated OOV loanwords leveraged ArabicSwahili borrowing
ArabicEnglish MT. contribution borrowing dictionaries +0.61.1 BLEU,
phonetic semantic features contribute additional half BLEU. importantly,
upper bound results show system improved substantially better
dictionaries OOV loanwords. result confirms OOV borrowed words
important type OOVs, proper modeling potential improve translation
large margin. MalteseEnglish system also improved substantially, +0.8
BLEU, contribution additional features less pronounced. RomanianEnglish
systems obtain small significant improvement 4K 8K, p < .01 (Clark, Dyer,
Lavie, & Smith, 2011). However, expected rate borrowing French
Romanian smaller, and, result, integrated loanword dictionaries small.
Transliteration baseline, conversely, effective RomanianFrench language pair,
languages related typologically, common cognates addition loanwords.
Still, even dictionaries translations pivoting via borrowing/transliteration
improve, even almost approach upper bounds results.
5.2.6 Error Analysis
augmented MT systems combine three main components: translation system itself,
borrowing system, pivot translation system. step application errors may
occur lead erroneous translations. identify main sources errors Swahili
84

fiCross-Lingual Bridges Models Lexical Borrowing

Error source
Reachability borrowing system
Loanword production errors
ArabicEnglish translation errors
SwahiliEnglish translation errors

#
113
191
20
29

%
32.0
54.1
5.7
8.2

Table 13: Sources errors.

English end-to-end system, conducted manual analysis errors translations OOV
types produced SwahiliEnglish 4K translation systems. gold standard corpus
use Helsinki Corpus Swahili24 (Hurskainen, 2004a, HCS). HCS morphologically,
syntactically, semantically annotated corpus 580K sentences (12.7M tokens).
corpus 52,351 surface forms (1.5M tokens) marked Arabic loanwords.
3,610 OOV types SwahiliEnglish 4K translation systems, 481 word types
annotated HCS. manually annotated 481 words identified 353 errors;
remaining 128 words translated correctly end-to-end system. analysis
reveals error sources detailed below. Table 13 summarize statistics error
sources.
1. Reachability borrowing system.
368 481 input words produced loanword candidates. main reason
unreachable paths complex morphology Swahili OOVs, taken
account borrowing system. example, atakayehusika involved,
lemma husika involve.
2. Loanword production errors.
half errors due incorrect outputs borrowing system.
line ArabicSwahili borrowing system accuracy reported Table 6.
example, morphological variants lemma wahi never (hayajawahi, halijawahi,
hazijawahi), incorrectly produced Arabic donor word Ag. (jAwh) java. Additional examples include variants lemma saidia help (isaidie, kimewasaidia)
produced Arabic donor candidates variants proper name Saidia.
3. ArabicEnglish translation errors.
frequent source errors ArabicEnglish MT system, identified OOV Arabic words. example, although Swahili loanword awashukuru

thank borrowing system correctly produced plausible donor word P
(w$kwr) thank (rarely used), translation variant produced
ArabicEnglish MT kochkor.
4. SwahiliEnglish translation errors.
cases, although borrowing system produced correct donor candidate,
ArabicEnglish translation also correct, translation variants different
reference translations SwahiliEnglish MT system. example,
24. www.aakkl.helsinki.fi/cameel/corpus/intro.htm

85

fiTsvetkov & Dyer

word alihuzunika grieved correctly produced Arabic donor Qm '@ (AlHzn) grief.
Translation variants produced ArabicEnglish MT sadness, grief, saddened,
sorrow, sad, mourning, grieved, saddening, mourn, distressed, whereas expected
translation SwahiliEnglish reference translations disappointed. Another
source errors occurred despite correct outputs borrowing translation
systems historical meaning change words. interesting example semantic

shift word sakafu floor, borrowed Arabic word (sqf )
ceiling.
Complex morphology Swahili Arabic frequent source errors
steps application. Concatenation several prefixes Swahili affects reachability
borrowing system. Swahili prefixes flip meaning words, example
kutoadhibiwa impunity, produces lemma adhibiwa punishment, consequently
translations torture, torturing, tortured. Finally, derivational processes languages
handled system, example, verb aliyorithi inherited, produces

Arabic noun KP@@ (AlwArvp) heiress, English translations heiress. Jointly
reasoning morphological processes donor recipient languages suggests
possible avenue remedying issues.

6. Additional Related Work
exception study conducted Blair Ingram (2003) generation
borrowed phonemes EnglishJapanese language pair (the method generalize
borrowed phonemes borrowed words, rely linguistic insights),
aware prior work computational modeling lexical borrowing.
papers mention tangentially address borrowing, briefly list here. Daum III
(2009) focuses areal effects linguistic typology, broader phenomenon includes
borrowing genetic relations across languages. study aimed discovering language
areas based typological features languages. Garley Hockenmaier (2012) train
maximum entropy classifier character n-gram morphological features identify
anglicisms (which compare loanwords) online community German hip hop
fans. Finally, List Moran (2013) published toolkit computational tasks
historical linguistics remark Automatic approaches borrowing detection
still infancy historical linguistics.

7. Conclusion
Given loanword, model identifies plausible donor words contact language.
show discriminative model Optimality Theoretic features effectively models
systematic phonological changes ArabicSwahili loanwords. also found model
methodology generally applicable language pairs minimal engineering
effort. translation results substantially improve baseline confirm OOV
loanwords important merit investigation.
numerous research questions would like explore further. possible
monolingually identify borrowed words language? automatically identify
donor language (or phonological properties) borrowed word? Since languages may
86

fiCross-Lingual Bridges Models Lexical Borrowing

borrow many sources, jointly modeling process lead better performance?
reduce amount language-specific engineering required deploy model?
integrate knowledge borrowing additional downstream NLP applications?
intend address questions future work.

Acknowledgments
thank Nathan Schneider, Shuly Wintner, Llus Mrquez, anonymous reviewers
help constructive feedback. also thank Waleed Ammar help
Arabic. work supported part U.S. Army Research Laboratory
U.S. Army Research Office contract/grant number W911NF-10-1-0533, part
National Science Foundation award IIS-1526745. Computational resources
provided Google form Google Cloud Computing grant NSF
XSEDE program TG-CCR110017.

References
Adler, A. N. (2006). Faithfulness perception loanword adaptation: case study
Hawaiian. Lingua, 116 (7), 10241045.
Ahn, S.-C., & Iverson, G. K. (2004). Dimensions Korean laryngeal phonology. Journal
East Asian Linguistics, 13 (4), 345379.
Al-Onaizan, Y., & Knight, K. (2002). Machine transliteration names Arabic text.
Proc. ACL workshop Computational Approaches Semitic Languages, pp. 113.
Association Computational Linguistics.
Allauzen, C., Riley, M., Schalkwyk, J., Skut, W., & Mohri, M. (2007). OpenFst: general
efficient weighted finite-state transducer library. Implementation Application
Automata, pp. 1123. Springer.
Ammar, W., Chahuneau, V., Denkowski, M., Hanneman, G., Ling, W., Matthews, A., Murray,
K., Segall, N., Tsvetkov, Y., Lavie, A., & Dyer, C. (2013). cmu machine translation
systems WMT 2013: Syntax, synthetic translation options, pseudo-references.
Proc. WMT.
Ammar, W., Dyer, C., & Smith, N. A. (2012). Transliteration sequence labeling
lattice encodings reranking. Proc. NEWS workshop ACL.
Blair, A. D., & Ingram, J. (2003). Learning predict phonological structure English
loanwords Japanese. Applied Intelligence, 19 (1-2), 101108.
Boersma, P., & Hayes, B. (2001). Empirical tests gradual learning algorithm. Linguistic
inquiry, 32 (1), 4586.
Broselow, E. (2004). Language contact phonology: richness stimulus, poverty
base. Proc. NELS, Vol. 34, pp. 122.
Burkett, D., & Klein, D. (2008). Two languages better one (for syntactic parsing).
Proc. EMNLP, pp. 877886.
87

fiTsvetkov & Dyer

Calabrese, A., & Wetzels, W. L. (2009). Loan phonology, Vol. 307. John Benjamins
Publishing.
Callison-Burch, C., Koehn, P., & Osborne, M. (2006). Improved statistical machine translation using paraphrases. Proc. ACL.
Cettolo, M., Girardi, C., & Federico, M. (2012). WIT3 : Web inventory transcribed
translated talks. Proc. EAMT, pp. 261268.
Chahuneau, V., Schlinger, E., Smith, N. A., & Dyer, C. (2013). Translating morphologically rich languages synthetic phrases. Proc. EMNLP, pp. 16771687.
Clark, J. H., Dyer, C., Lavie, A., & Smith, N. A. (2011). Better hypothesis testing
statistical machine translation: Controlling optimizer instability. Proc. ACL, pp.
176181.
Comrie, B., & Spagnol, M. (2015). Maltese loanword typology. Submitted.
Das, D., & Petrov, S. (2011). Unsupervised part-of-speech tagging bilingual graph-based
projections. Proc. ACL, pp. 600609. Association Computational Linguistics.
Daum III, H. (2009). Non-parametric Bayesian areal linguistics. Proc. NAACL, pp.
593601. Association Computational Linguistics.
Davidson, L., & Noyer, R. (1997). Loan phonology Huave: nativization ranking
faithfulness constraints. Proc. WCCFL, Vol. 15, pp. 6579.
De Gispert, A., & Marino, J. B. (2006). Catalan-English statistical machine translation
without parallel corpus: bridging Spanish. Proc. LREC, pp. 6568.
Dholakia, R., & Sarkar, A. (2014). Pivot-based triangulation low-resource languages.
Proc. AMTA.
Diab, M., & Resnik, P. (2002). unsupervised method word sense tagging using parallel
corpora. Proc. ACL.
Durrani, N., Sajjad, H., Fraser, A., & Schmid, H. (2010). Hindi-to-Urdu machine translation
transliteration. Pro. ACL, pp. 465474.
Dyer, C., Chahuneau, V., & Smith, N. A. (2013). simple, fast, effective reparameterization IBM Model 2. Proc. NAACL.
Dyer, C., Lopez, A., Ganitkevitch, J., Weese, J., Ture, F., Blunsom, P., Setiawan, H.,
Eidelman, V., & Resnik, P. (2010). cdec: decoder, alignment, learning framework
finite-state context-free translation models. Proc. ACL.
Eisner, J. (1997). Efficient generation primitive Optimality Theory. Proc. EACL, pp.
313320.
Eisner, J. (2002). Comprehension compilation Optimality Theory. Proc. ACL, pp.
5663.
Ellison, T. M. (1994). Phonological derivation Optimality Theory. Proc. CICLing, pp.
10071013.
Fabri, R., Gasser, M., Habash, N., Kiraz, G., & Wintner, S. (2014). Linguistic introduction:
orthography, morphology syntax Semitic languages. Natural Language
Processing Semitic Languages, pp. 341. Springer.
88

fiCross-Lingual Bridges Models Lexical Borrowing

Faruqui, M., & Dyer, C. (2014). Improving vector space word representations using multilingual correlation. Proc. EACL.
Ganchev, K., Gillenwater, J., & Taskar, B. (2009). Dependency grammar induction via bitext
projection constraints. Proc. ACL, pp. 369377. Association Computational
Linguistics.
Garley, M., & Hockenmaier, J. (2012). Beefmoves: dissemination, diversity, dynamics
English borrowings German hip hop forum. Proc. ACL, pp. 135139.
Goldwater, S., & Johnson, M. (2003). Learning OT constraint rankings using maximum
entropy model. Proc. Stockholm workshop variation within Optimality Theory,
pp. 111120.
Habash, N. (2008). Four techniques online handling out-of-vocabulary words
Arabic-English statistical machine translation. Proc. ACL, pp. 5760.
Habash, N., & Hu, J. (2009). Improving Arabic-Chinese statistical machine translation using
English pivot language. Proc. WMT, pp. 173181.
Habash, N., Rambow, O., & Roth, R. (2009). MADA+TOKAN: toolkit Arabic
tokenization, diacritization, morphological disambiguation, POS tagging, stemming
lemmatization. Proc. MEDAR, pp. 102109.
Haghighi, A., Liang, P., Berg-Kirkpatrick, T., & Klein, D. (2008). Learning bilingual lexicons
monolingual corpora. Proc. ACL, pp. 771779.
Haji, J., Hric, J., & Kubo, V. (2000). Machine translation close languages.
Proc. ANLP, pp. 712.
Haspelmath, M. (2009). Lexical borrowing: concepts issues. Loanwords Worlds
Languages: comparative handbook, 3554.
Haspelmath, M., & Tadmor, U. (Eds.). (2009). Loanwords Worlds Languages:
Comparative Handbook. Max Planck Institute Evolutionary Anthropology, Leipzig.
Haugen, E. (1950). analysis linguistic borrowing. Language, 210231.
Hayes, B., Tesar, B., & Zuraw, K. (2013). OTSoft 2.3.2..
Heafield, K. (2011). KenLM: Faster smaller language model queries. Proc. WMT.
Hermjakob, U., Knight, K., & Daum III, H. (2008). Name translation statistical machine
translation-learning transliterate. Proc. ACL, pp. 389397.
Hock, H. H., & Joseph, B. D. (2009). Language history, language change, language
relationship: introduction historical comparative linguistics, Vol. 218. Walter
de Gruyter.
Holden, K. (1976). Assimilation rates borrowings phonological productivity. Language,
131147.
Hurskainen, A. (2004a). HCS 2004Helsinki corpus Swahili. Tech. rep., Compilers:
Institute Asian African Studies (University Helsinki) CSC.
Hurskainen, A. (2004b). Loan words Swahili. Bromber, K., & Smieja, B. (Eds.),
Globalisation African Languages, pp. 199218. Walter de Gruyter.
89

fiTsvetkov & Dyer

Hwa, R., Resnik, P., Weinberg, A., Cabezas, C., & Kolak, O. (2005). Bootstrapping parsers
via syntactic projection across parallel texts. Natural Language Engineering, 11 (3).
Jacobs, H., & Gussenhoven, C. (2000). Loan phonology: perception, salience, lexicon
OT. Optimality Theory: Phonology, syntax, acquisition, 193209.
Johnson, F. (1939). Standard Swahili-English dictionary. Oxford University Press.
Kager, R. (1999). Optimality Theory. Cambridge University Press.
Kang, Y. (2003). Perceptual similarity loanword adaptation: English postvocalic word-final
stops Korean. Phonology, 20 (2), 219274.
Kang, Y. (2011). Loanword phonology. van Oostendorp, M., Ewen, C., Hume, E., & Rice,
K. (Eds.), Companion Phonology. WileyBlackwell.
Kawahara, S. (2008). Phonetic naturalness unnaturalness Japanese loanword phonology. Journal East Asian Linguistics, 17 (4), 317330.
Kenstowicz, M. (2007). Salience similarity loanword adaptation: case study
Fijian. Language Sciences, 29 (2), 316340.
Kenstowicz, M., & Suchato, A. (2006). Issues loanword adaptation: case study
Thai. Lingua, 116 (7), 921949.
Kneser, R., & Ney, H. (1995). Improved backing-off m-gram language modeling. Proc.
ICASSP, Vol. 1, pp. 181184. IEEE.
Knight, K., & Graehl, J. (1998). Machine transliteration. Computational Linguistics, 24 (4),
599612.
Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation. Proc.
NAACL-HLT, pp. 4854.
Kondrak, G. (2001). Identifying cognates phonetic semantic similarity. Proc.
NAACL, pp. 18. Association Computational Linguistics.
Kondrak, G., Marcu, D., & Knight, K. (2003). Cognates improve statistical translation
models. Proc. HLT-NAACL, pp. 4648. Association Computational Linguistics.
Kondrak, G., & Sherif, T. (2006). Evaluation several phonetic similarity algorithms
task cognate identification. Proc. Workshop Linguistic Distances, pp.
4350. Association Computational Linguistics.
Kozhevnikov, M., & Titov, I. (2013). Cross-lingual transfer semantic role labeling models.
Proc. ACL, pp. 11901200.
Kuhn, J. (2004). Experiments parallel-text based grammar induction. Proc. ACL, p.
470.
Li, S., Graa, J. V., & Taskar, B. (2012). Wiki-ly supervised part-of-speech tagging. Proc.
EMNLP, pp. 13891398.
List, J.-M., & Moran, S. (2013). open source toolkit quantitative historical linguistics.
Proc. ACL (System Demonstrations), pp. 1318.
Littell, P., Price, K., & Levin, L. (2014). Morphological parsing Swahili using crowdsourced
lexical resources. Proc. LREC.
90

fiCross-Lingual Bridges Models Lexical Borrowing

Maamouri, M., Graff, D., Bouziri, B., Krouna, S., & Kulick, S. (2010). LDC Standard Arabic
morphological analyzer (SAMA) v. 3.1..
Mann, G. S., & Yarowsky, D. (2001). Multipath translation lexicon induction via bridge
languages. Proc. HLT-NAACL, pp. 18.
Marton, Y., Callison-Burch, C., & Resnik, P. (2009). Improved statistical machine translation
using monolingually-derived paraphrases. Proc. EMNLP, pp. 381390.
McCarthy, J. J. (1985). Formal problems Semitic phonology morphology. Ph.D. thesis,
MIT.
McCarthy, J. J. (2009). Optimality Theory: Applying theory data. John Wiley &
Sons.
McCarthy, J. J., & Prince, A. (1995). Faithfulness reduplicative identity. Beckman et
al. (Eds.), 249384.
Metze, F., Hsiao, R., Jin, Q., Nallasamy, U., & Schultz, T. (2010). 2010 CMU GALE
speech-to-text system. Proc. INTERSPEECH, pp. 15011504.
Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed
representations words phrases compositionality. Proc. NIPS, pp.
31113119.
Moravcsik, E. (1978). Language contact. Universals human language, 1, 93122.
Mwita, L. C. (2009). adaptation Swahili loanwords Arabic: constraint-based
analysis. Journal Pan African Studies.
Myers-Scotton, C. (2002). Contact linguistics: Bilingual encounters grammatical outcomes. Oxford University Press Oxford.
Nakov, P., & Ng, H. T. (2012). Improving statistical machine translation resourcepoor language using related resource-rich languages. Journal Artificial Intelligence
Research, 179222.
Nelder, J. A., & Mead, R. (1965). simplex method function minimization. Computer
journal, 7 (4), 308313.
Och, F. J. (2003). Minimum error rate training statistical machine translation. Proc.
ACL, pp. 160167.
Pad, S., & Lapata, M. (2009). Cross-lingual annotation projection semantic roles.
Journal Artificial Intelligence Research, 36 (1), 307340.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). bleu: method automatic
evaluation machine translation. Proc. ACL, pp. 311318.
Parker, R., Graff, D., Kong, J., Chen, K., & Maeda, K. (2009). English Gigaword fourth
edition..
Polom, E. C. (1967). Swahili Language Handbook. ERIC.
Prince, A., & Smolensky, P. (2008). Optimality Theory: Constraint interaction generative
grammar. John Wiley & Sons.
91

fiTsvetkov & Dyer

Rapp, R. (1995). Identifying word translations non-parallel texts. Pro. ACL, pp.
320322.
Razmara, M., Siahbani, M., Haffari, R., & Sarkar, A. (2013). Graph propagation
paraphrasing out-of-vocabulary words statistical machine translation. Proc. ACL,
pp. 11051115.
Repetti, L. (2006). emergence marked structures integration loans Italian.
Amsterdam Studies Theory History Linguistic Science Series 4, 274, 209.
Rose, Y., & Demuth, K. (2006). Vowel epenthesis loanword adaptation: Representational
phonetic considerations. Lingua, 116 (7), 11121139.
Rothman, N. C. (2002). Indian Ocean trading links: Swahili experience. Comparative
Civilizations Review, 46, 7990.
Saluja, A., Hassan, H., Toutanova, K., & Quirk, C. (2014). Graph-based semi-supervised
learning translation models monolingual data. Proc. ACL, pp. 676686.
Sankoff, G. (2002). Linguistic outcomes language contact. Chambers, J., Trudgill, P.,
& Schilling-Estes, N. (Eds.), Handbook Sociolinguistics, pp. 638668. Blackwell.
Schadeberg, T. C. (2009). Loanwords Swahili. Haspelmath, M., & Tadmor, U. (Eds.),
Loanwords Worlds Languages: Comparative Handbook, pp. 76102. Max
Planck Institute Evolutionary Anthropology.
Schlinger, E., Chahuneau, V., & Dyer, C. (2013). morphogen: Translation morphologically
rich languages synthetic phrases. Prague Bulletin Mathematical Linguistics,
100, 5162.
Schulte, K. (2009). Loanwords Romanian. Haspelmath, M., & Tadmor, U. (Eds.),
Loanwords Worlds Languages: Comparative Handbook, pp. 230259. Max
Planck Institute Evolutionary Anthropology.
Schultz, T., & Schlippe, T. (2014). GlobalPhone: Pronunciation dictionaries 20 languages.
Proc. LREC.
Smith, D. A., & Smith, N. A. (2004). Bilingual parsing factored estimation: Using
english parse korean.. Proc. EMNLP, pp. 4956.
Smith, J. R., Saint-Amand, H., Plamada, M., Koehn, P., Callison-Burch, C., & Lopez, A.
(2013). Dirt cheap web-scale parallel text Common Crawl. Proc. ACL, pp.
13741383.
Snyder, B., Naseem, T., & Barzilay, R. (2009). Unsupervised multilingual grammar induction.
Proc. ACL/AFNLP, pp. 7381.
Tckstrm, O., Das, D., Petrov, S., McDonald, R., & Nivre, J. (2013). Token type
constraints cross-lingual part-of-speech tagging.. Transactions Association
Computational Linguistics, 1, 112.
Tadmor, U. (2009). Loanwords worlds languages: Findings results. Haspelmath,
M., & Tadmor, U. (Eds.), Loanwords Worlds Languages: Comparative
Handbook, pp. 5575. Max Planck Institute Evolutionary Anthropology.
92

fiCross-Lingual Bridges Models Lexical Borrowing

Thomason, S. G., & Kaufman, T. (2001). Language contact. Edinburgh University Press
Edinburgh.
Tiedemann, J. (2012). Parallel data, tools interfaces OPUS. Proc. LREC, pp.
22142218.
Tiedemann, J. (2014). Rediscovering annotation projection cross-lingual parser induction.
Proc. COLING.
Tsvetkov, Y., Ammar, W., & Dyer, C. (2015). Constraint-based models lexical borrowing.
Proc. NAACL, pp. 598608.
Tsvetkov, Y., Boytsov, L., Gershman, A., Nyberg, E., & Dyer, C. (2014). Metaphor detection
cross-lingual model transfer. Proc. ACL, pp. 248258.
Tsvetkov, Y., & Dyer, C. (2015). Lexicon stratification translating out-of-vocabulary
words. Proc. ACL.
Tsvetkov, Y., Dyer, C., Levin, L., & Bhatia, A. (2013). Generating English determiners
phrase-based translation synthetic translation options. Proc. WMT.
Tsvetkov, Y., Metze, F., & Dyer, C. (2014). Augmenting translation models simulated
acoustic confusions improved spoken language translation. Proc. EACL, pp.
616625.
Van Coetsem, F. (1988). Loan phonology two transfer types language contact.
Walter de Gruyter.
Wang, P., Nakov, P., & Ng, H. T. (2012). Source language adaptation resource-poor
machine translation. Proc. EMNLP, pp. 286296.
Weinreich, U. (1979). Languages contact: Findings problems. Walter de Gruyter.
Whitney, W. D. (1881). mixture language. Transactions American Philological
Association (1870), 526.
Wu, D. (1997). Stochastic inversion transduction grammars bilingual parsing parallel
corpora. Computational linguistics, 23 (3), 377403.
Xi, C., & Hwa, R. (2005). backoff model bootstrapping resources non-English
languages. Proc. EMNLP, pp. 851858.
Yarowsky, D., Ngai, G., & Wicentowski, R. (2001). Inducing multilingual text analysis tools
via robust projection across aligned corpora. Proc. HLT, pp. 18.
Yip, M. (1993). Cantonese loanword phonology Optimality Theory. Journal East
Asian Linguistics, 2 (3), 261291.
Zawawi, S. (1979). Loan words effect classification Swahili nominals.
Leiden: E.J. Brill.

93

fiJournal Artificial Intelligence Research 55 (2016) 889-952

Submitted 9/15; published 4/16

Searching Best Solutions Graphical Models
Natalia Flerova

NFLEROVA @ UCI . EDU

University California, Irvine
Irvine, CA 92697, USA

Radu Marinescu

RADU . MARINESCU @ IE . IBM . COM

IBM Research Ireland

Rina Dechter

DECHTER @ UCI . EDU

University California, Irvine
Irvine, CA 92697, USA

Abstract
paper focuses finding best solutions combinatorial optimization problems
using best-first depth-first branch bound search. Specifically, present new algorithm mA*, extending well-known A* m-best task, first time prove desirable
properties, including soundness, completeness optimal efficiency, maintained. Since bestfirst algorithms require extensive memory, also extend memory-efficient depth-first branch
bound m-best task.
adapt algorithms optimization tasks graphical models (e.g., Weighted CSP
MPE Bayesian networks), provide complexity analysis empirical evaluation. experiments confirm theory best-first approach largely superior memory available,
depth-first branch bound robust. also show algorithms competitive
related schemes recently developed m-best task.

1. Introduction
usual aim combinatorial optimization find optimal solution, minimum maximum,
objective function. However, many applications desirable obtain single
optimal solution, set first best solutions integer m. motivated
many real-life domains, task arises. instance, problem finding likely
haplotype pedigree presented finding probable assignment Bayesian
network encodes genetic information (Fishelson, Dovgolevsky, & Geiger, 2005). practice data often corrupted missing, makes single optimal solution unreliable.
possible increase confidence answer finding set best solutions
choosing final solution expert help obtaining additional genetic data.
examples m-best tasks arise procurement auction problems probabilistic expert systems, certain constraints often cannot directly incorporated model, either
make problem infeasibly complex vague formalize (e.g. idiosyncratic
preferences human user). Thus domains may practical first find several
good solutions relaxed problem pick one satisfies additional constraints
post-processing manner. Additionally, sometimes set diverse assignments approximately
cost required, reliable communication network design. Finally, context
summation problem graphical models, probability evidence partition function,
approximation derived summing likely tuples.
c
2016
AI Access Foundation. rights reserved.

fiF LEROVA , ARINESCU , & ECHTER

problem finding best solutions well studied. One earliest
influential works belongs Lawler (1972). provided general scheme extends
optimization algorithm m-best task. idea compute next best solution successively
finding single optimal solution slightly different reformulation original problem
excludes solutions generated far. approach extended improved
years still one primary strategies finding best solutions. approaches
direct, trying avoid repeated computation inherent Lawlers scheme. Two earlier
works relevant provide highest challenge work Nilsson (1998)
Aljazzar Leue (2011).
Nilsson proposed junction-tree based message-passing scheme iteratively finds
best solutions. claimed best runtime complexity among m-best schemes
graphical models. analysis (Section 6) shows indeed Nilssons scheme second
best worst case time complexity algorithm BE+m-BF (Section 5.3). However,
practice scheme feasible problems large induced width.
recent work Aljazzar Leue proposed algorithm called K*, A* search-style
scheme finding k shortest paths interleaved breadth-first search. used
specialized data structure unclear approach straightforwardly extended
graphical models, point leave future work.
One popular approximate approaches solving optimization problems based
LP-relaxation problem (Wainwright & Jordan, 2003). m-best extension approach
(Fromer & Globerson, 2009) guarantee exact solutions, quite efficient practice.
discuss previous works Section 6.
main focus lies optimization context graphical models, Bayesian networks, Markov networks constraint networks. However, algorithms developed
used general purpose tasks, finding shortest paths graph. Various
graph-exploiting algorithms solving optimization tasks graphical models developed
past decades. algorithms often characterized either inference type
(e.g., message-passing schemes, variable elimination) search type (e.g., AND/OR search
recursive-conditioning). earlier works, (e.g., Flerova, Dechter, & Rollon, 2011), extended
inference schemes represented bucket elimination algorithm (BE) (Dechter, 1999, 2013)
task finding best solutions. However, due large memory requirements, variable
elimination algorithms, including bucket elimination, cannot used practice finding exact
solutions combinatorial optimization tasks problems graph dense. Depth-first branch
bound (DFBnB) best-first search (BFS) flexible trade space time.
work explores question solving best solutions task using heuristic search schemes.
contribution lies extending heuristic algorithms best solutions task. describe general purpose m-best variants depth-first branch bound best-first search,
specifically A*, yielding algorithms m-BB m-A* respectively, analyze properties. show m-A* inherits A*s desirable properties (Dechter & Pearl, 1985),
significantly optimally efficient compared alternative exact search-based scheme.
also discuss size search space explored m-BB. extend new m-best algorithms graphical models exploring AND/OR search space.
evaluate resulting algorithms 6 benchmarks 300 instances total,
examine impact number solutions algorithms behaviour. particular,
890

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

observe runtime schemes (except depth-first branch bound
exploring AND/OR tree) scales much better worst case theoretical analysis
suggests.
also show m-A* search using exact bucket elimination heuristic (a scheme
call BE+m-BF) highly efficient easier problems suffers severely memory issues
denser graphs, far A*-based schemes using approximate mini-bucket heuristics. Finally, compare schemes efficient algorithms based
LP-relaxation (Fromer & Globerson, 2009; Batra, 2012), showing competitiveness even superiority large values (m 10), providing optimality guarantees.
paper organized follows. Section 2 provide relevant background. Section 3
presents extension best-first search m-best task. particular, define m-A*,
extension A* algorithm finding best solutions (3.1), prove main properties (3.2).
Section 4 describes algorithm m-BB, extension depth-first branch bound algorithm
solving m-best task. Section 5 discuss adaptation two newly proposed m-best
search algorithms AND/OR search spaces graphical models, including hybrid method
BE+m-BF incorporates variable elimination heuristic search. Section 6 elaborates
related work contrasts methods. Section 7 presents empirical evaluation
m-best schemes Section 8 concludes.

2. Background
begin formally defining graphical models framework providing background
heuristic search.
2.1 Graphical Models
denote variables upper-case letters (e.g., X, Y, Z) values variables lower-case
letters (e.g., x, y, z). Sets variables denoted upper-case letters bold (e.g. X, Y, Z).
assignment (X1 = x1 , . . . , Xn = xn ) abbreviated x = (x1 , . . . , xn ).
denote functions letters f, g, h etc., set functions byPF. function f
scope S1 = {X1 , . . . , Xr } denoted fS1 . P
summation
P operator xX defines sum
possible values variables X, namely x1 X1 , . . . , xn Xn . Minimization minxX
maximization maxxX operators defined similar manner. Note use terms elimination
P
marginalization interchangeably.
convenience sometimes use minx (maxx , x )
P
denote minxX (maxxX , xX ).
graphical model collection local functions subsets variables conveys probabilistic, deterministic, preferential information, whose structure described graph.
graph captures independencies irrelevance information inherent model, useful
interpreting modeled data and, significantly, exploited reasoning algorithms.
set local functions combined variety ways generate global function, whose
scope set variables.
N
EFINITION 1 (Graphical model). graphical model 4-tuple = hX, D, F, i:
1. X = {X1 , . . . , Xn } finite set variables;

2. = {D1 , . . . , Dn } set respective finite domains values;
891

fiF LEROVA , ARINESCU , & ECHTER

3. F = {f1 , . . . , fr } set non-negative real-valued discrete functions, defined scopes
variables Si X. called local functions.
N
N
Q P
4.
combination operator, e.g.,
{ , } (product, sum)
graphical model represents
global function, whose scope X combination
N
local functions: rj=1 fj .
N
P
N
= Qand fi : DSi N Weighted Constraint Satisfaction Problems (WCSPs).
=
fi = Pi (Xi | pai ) Bayesian network. probabilities P
defined relative directed acyclic graph G X, set Xi1 , . . . , Xik parents
pai Xi , i.e. Xij edge pointing Xij Xi . illustration, consider
Bayesian network 5 variables whose directed acyclic graph (DAG) given Figure 1(a).
common optimization task Bayesian network probable explanation
(MPE) also known maximum posteriori hypothesis (MAP),1 goal compute
optimal value
r


C = max
fj (xSj )
x

j=1

optimizing configuration


x = argmax
x

r


fj (xSj )

j=1

related task, typical WCSP,
min-sum, namely computing minimal cost
P
P assignment (min-sum): C = minx j fj (x) optimizing configuration x = argminx j fj (x).
Historically task also sometimes referred energy Q
minimization. equivalent

MPE/MAP task following sense: Cmax
= maxx j fj (x) solution MPE




problem,
P Cmax = exp (Cmin ), Cmin solution min-sum problem Cmin =
minx j gj (x) j, gj (x) = log (fj (x)).
graphical model defines primal graph captures dependencies problems
variables. variables vertices. edge connects two vertices whose variables appear
scope function. important property graphical model, characterizing
complexity reasoning tasks induced width. ordered graph pair (G, o)
G undirected graph, = (X1 , . . . , Xn ) ordering nodes. width node
number nodes neighbors precede ordering. width graph along
ordering maximum width nodes. induced ordered graph obtained
ordered graph follows: nodes processed last first based o; node Xj
processed, preceding neighbors connected. width ordered induced graph along
ordering called induced width along denoted w (o). induced width
graph, denoted w , minimal induced width orderings. Abusing notation
sometimes use w denote induced width along particular ordering, meaning
clear context.
Figure 1(b) depicts primal graph Bayesian network Figure 1(a). Figures 1(c)
1(d) show induced graphs primal graph Figure 1(a) respectively along orderings
1. communities MAP also refers task optimizing partial assignment variables. However,
paper use MAP MPE interchangeable, referring optimal full variable assignment.

892

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

P (A)





B

E

C





C

E

B





(c)

(d)

P (B|A)

B

C

B

C

P (C|A)

E

E

P (E|B, C)





P (D|A, B)

(a)

(b)

Figure 1: (a) DAG Bayesian network, (b) primal graph (also called moral graph),
(c) induced graph along = (A, E, D, C, B), (d) induced graph along
= (A, B, C, D, E), example Gogate (2009).

= (A, E, D, C, B) o0 = (A, B, C, D, E). dashed lines Figure 1(c) represent
induced edges, namely edges absent moral graph, introduced
induced graph. see induced width along ordering w (o) = 4 induced
width along ordering o0 w (o0 ) = 2, respectively.
2.2 Heuristic Search
analysis focuses best-first search (BFS), whose behaviour task finding single
optimal solution well understood. Assuming minimization task, best-first search always expands
node best (i.e., smallest) value heuristic evaluation function. maintains graph
explored paths, list CLOSED expanded nodes frontier OPEN nodes. BFS chooses
OPEN node n smallest value heuristic evaluation function f (n), expands
generating successors succ(n), places CLOSED, places succ(n) OPEN.
popular variant best-first search, A*, uses heuristic evaluation function f (n) = g(n) + h(n),
g(n) cost path root n, h(n) heuristic function estimates
optimal cost go h (n) n goal node. heuristic function called admissible
never overestimates (for minimization) true minimal cost reach goal h (n). Namely,
n h(n) h (n). heuristic called consistent monotonic, every node n every
successor n0 n following inequality holds: h(n) c(n, n0 ) + h(n0 ). h(n) consistent,
values evaluation function f (n) along path non-decreasing. known
regardless tie-breaking rule A* expands node n reachable strictly C -bounded path
root, node referred surely expanded A* (Dechter & Pearl, 1985).
path C -bounded relative f , n : f (n) < C , C cost optimal
solution.
A* search number attractive properties (Nillson, 1980; Pearl, 1984; Dechter & Pearl,
1985):
893

fiF LEROVA , ARINESCU , & ECHTER

Soundness completeness: A* terminates optimal solution.
h consistent, A* explores set nodes = {n|f (n) C } surely
expands nodes = {n|f (n) < C }.
Optimal efficiency consistent heuristic: h consistent, node surely expanded A* must expanded sound complete search algorithm using
heuristic information.
Optimal efficiency node expansions: heuristic function consistent, A*,
searching graph, expands node once, time nodes expansion
A* found shortest path it.
Dominance: Given two heuristic functions h1 h2 , s.t. n h1 (n) < h2 (n), A1 expand
every node surely expanded A2 , Ai uses heuristic hi .
Although best-first search known best algorithm terms number nodes expanded (Dechter & Pearl, 1985), requires exponential memory worst-case.
popular alternative depth-first branch bound (DFBnB), whose attractive feature, compared best-first search, executed linear memory. Yet,
search space graph, exploit memory improve performance flexibly trading space
time. Depth-first branch bound expands nodes depth-first manner, maintaining cost
best solution found far upper bound U B cost optimal solution.
heuristic evaluation function current node n greater equal upper bound,
node pruned subtree never explored. worst case depth-first branch
bound explores entire search space. best case first solution found optimal,
case performance good BFS. However, solution depth unbound, depth-first
search might follow infinite branch never terminate. Also, search space graph,
DFBnB may expand nodes numerous time, unless uses caching checks duplicates.
2.3 Search Graphical Models
Search algorithms provide way systematically enumerate possible assignments given
graphical model. Optimization problems graphical models naturally presented
task finding optimal cost path appropriate search space.
simplest variant search space so-called search tree. level corresponds
variable original problem. nodes correspond partial variable assignments
arc weights derived problems input functions. size search tree bounded
O(k n ), n number variables k maximum domain size.
Throughout section going illustrate concepts using example problem
six variables {A, B, C, D, E, F } six pairwise functions. primal graph shown Figure
2(a). Figure 2(b) displays search tree corresponding lexicographical ordering.
2.3.1 AND/OR EARCH PACES
search trees blind problem decomposition encoded graphical models therefore inefficient. exploit independencies model. AND/OR search spaces
894

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

(a) Primal
graph

(b) search tree along ordering A, B, C, D, E, F

Figure 2: example problem 6 variables {A, B, C, D, E, F } 5 pairwise functions.
graphical models introduced better capture problem structure (Dechter & Mateescu, 2007). AND/OR search space defined relative pseudo tree primal graph
captures problem decomposition. Figure 3(a) shows pseudo tree example problem.
EFINITION 2. pseudo tree undirected graph G = (V, E) directed rooted tree =
(V, E 0 ), every arc G included E 0 back-arc , namely connects node
ancestor . arcs E 0 may included E.
Given graphical model = hX, D, Fi primal graph G pseudo tree G,
AND/OR search tree ST contains alternating levels nodes. structure based
underlying pseudo tree . root node ST node labelled variable
root . children node Xi nodes labelled value assignments hXi , xi
(or simply hxi i). children node hXi , xi nodes labelled children
Xi , representing conditionally independent subproblems. AND/OR tree corresponding
pseudo tree Figure 3(a) shown Figure 3(b). arcs nodes Xi hXi , xi
AND/OR search tree annotated weights derived cost functions F:
EFINITION 3 (arc weight). weight w(Xi , xi ) arc (Xi , hXi , xi i) combination (i.e.
sum WCSP product MPE) functions, whose scope includes Xi fully
assigned along path root node corresponding hXi , xi i, evaluated values
along path.
identical subproblems identified context (namely, partial instantiation
ancestors separates subproblem rest problem graph), merged,
yielding AND/OR search graph (Dechter & Mateescu, 2007). Merging context-mergeable
nodes yields context-minimal AND/OR search graph, denoted CT . example seen
Figure 3(c). size context-minimal AND/OR search graph shown exponential
induced width G along pseudo tree (Dechter & Mateescu, 2007).
solution tree CT subtree that: (1) contains root node CT ; (2)
internal node n , children ; (3) internal node n ,
exactly one children ; (4) every tip node (i.e., nodes children)
terminal node. cost solution tree product, MPE sum WCSP, weights
associated arcs.
node n CT associated value v(n) capturing optimal solution cost
conditioned subproblem rooted n. Assuming MPE/MAP problem, shown v(n)
895

fiF LEROVA , ARINESCU , & ECHTER






0

1



B

B



0

1

0

1



C



B
C

E



F



0



C

E
1

0

1



F

F

0

0 1 0 1 0 1 0 1

E

C

1

0

1



F

F

0

0 1 0 1 0 1 0 1

(a) Pseudo tree

1

0

1



F

F

0

E
1

0

1



F

F

0 1 0 1 0 1 0 1

0 1 0 1 0 1 0 1

(b) AND/OR search tree






0

1



B

B



0

1

C

C




C

E

0

1

0

0

C

C
1

0

1

0

1

E
1

0

E
1

0

E

E
1

0

1

0

1











F

F

F

F



0 1

0 1

0 1

0 1

0 1

0 1

0 1

0 1

(c) Context-minimal AND/OR search graph

Figure 3: AND/OR search spaces graphical models.

computed recursively based values ns successors: nodes maximization,
nodes multiplication. WCSPs, v(n) nodes updated minimization
summation, respectively (Dechter & Mateescu, 2007).
next provide overview depth-first branch bound best-first search algorithms,
explore AND/OR search spaces (Marinescu & Dechter, 2009b, 2009a; Otten & Dechter, 2011).
schemes use heuristics generated either mini-bucket elimination scheme (2.3.4)
soft arc-consistency schemes (Marinescu & Dechter, 2009a, 2009b; Schiex, 2000; Darwiche, Dechter, Choi, Gogate, & Otten, 2008) composite (Ihler, Flerova, Dechter, & Otten,
2012). customary heuristic search literature, defining search algorithms
assume without loss generality minimization task (i.e., min-sum optimization problem).
896

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

Algorithm 1: AOBF exploring AND/OR search tree (Marinescu & Dechter, 2009b)

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

24
25
26
27
28
29
30
31
32

Input: graphical model = hX, D, Fi, pseudo tree rooted X1 , heuristic function h()
Output: Optimal solution
Create root node labelled X1 let G (explored search space) = {s};
Initialize v(s) = h(s) best partial solution tree G;
SOLVED
Select non-terminal tip node n . node break;
n node labeled Xi
forall xi D(Xi )
Create child n0 = hXi , xi i;
n TERMINAL
Mark n0 SOLVED;
succ(n) succ(n) n0 ;
else n node labeled hXi , xi
forall successor Xj Xi
Create child n0 = Xj ;
succ(n) succ(n) n0 ;
Initialize v(n0 ) = h(n0 ) new nodes;
Add new nodes explores search space G G {succ(n)};
Let {n};
6=
Let p node descendants G still S;
{p};
p node
v(p) = minksucc(p) (w(p, k) + v(k));
Mark best successor k ancestors p, k = arg minksucc(p) (w(p, k) + v(k))
(maintaining previously marked successor still best);
Mark p SOLVED best marked successor solved;
else p
P node
v(p) = ksucc(p) v(k);
Mark arcs successors;
Mark p SOLVED children SOLVED;
p changes value p marked SOLVED
Add parents p p one successors marked arc;
Recompute following marked arcs root s;
return hv(s), i;

2.3.2 B EST-F IRST AND/OR EARCH
state-of-the-art version best-first search AND/OR search spaces graphical models
Best-First AND/OR search algorithm (AOBF) (Marinescu & Dechter, 2009b). AOBF
variant AO* (Nillson, 1980) explores context-minimal AND/OR search graph.
AOBF described Algorithm 1. simplicity, present algorithm traversing
AND/OR search tree. AOBF maintains explicated part search space G also keeps
track current best partial solution tree . interleaves iteratively top-down node expansion
step (lines 4-16), selects non-terminal tip node generates children G,
bottom-up cost revision step (lines 17-30), updates values internal nodes based
childrens values. newly generated child node terminal marked solved (line 9).
897

fiF LEROVA , ARINESCU , & ECHTER

bottom-up phase, nodes least one solved child nodes
children solved also marked solved. algorithm also marks arc best
child node minimum achieved (line 23). Following backward
step, new best partial solution tree recomputed (line 31). AOBF terminates root
node marked solved. heuristic used admissible, point termination
optimal solution cost v(s), root node search space.
Extending algorithm explore context-minimal AND/OR search graph straightforward done follows. expanding non-terminal node lines 11-14, AOBF
generate corresponding children already present explicated search
space G rather links them. identical nodes G easily recognized based
contexts (Marinescu & Dechter, 2009b).
HEOREM 1 (complexity, Marinescu & Dechter, 2009b). Algorithm AOBF traversing context
minimal AND/OR graph time space complexity O(n k w ), n number
variable problem, w induced width pseudo tree k bounds domain size.
2.3.3 EPTH -F IRST AND/OR B RANCH B OUND
depth-first AND/OR Branch Bound (AOBB) (Marinescu & Dechter, 2009a) algorithm
traverses AND/OR search space depth-first rather best-first manner, keeping
track current upper bound minimal solution cost.
simplicity, present variant algorithm explores AND/OR
search tree. AOBB described Algorithm 2 interleaves forward node expansion (lines 4-17)
backward cost revision (or propagation) step (lines 19-29) updates node values (capturing
current best solution subproblem rooted node), search terminates
optimal solution found. node n pruned (lines 12-13) current upper bound
higher nodes heuristic lower bound, computed recursively using procedure described
Algorithm 3.

worst case, AOBB explores entire search space, namely O(n k w ) nodes (assuming
context-minimal AND/OR search graph). practice, however, AOBB likely expand
nodes AOBF using heuristic, empirical performance AOBB depends heavily
order solutions encountered, namely quickly algorithm finds
close optimal solution use upper bound pruning.
2.3.4 INI -B UCKET H EURISTICS
AND/OR search algorithms presented (AOBF AOBB) often use mini-bucket
(also known MBE) heuristic h(n). Mini-Bucket Elimination MBE (Dechter & Rish, 2003)
approximate version exact variable elimination algorithm called bucket elimination (BE)
(Dechter, 1999). MBE (Algorithm 4) bounds space time complexity full bucket elimination (which exponential induced width w ). Given variable ordering, algorithm
associates variable Xi bucket contains functions defined variable,
higher index variables. Large buckets partitioned smaller subsets, called minibuckets, containing distinct variables. parameter called i-bound.
algorithm processes buckets last first (lines 2-10 Algorithm 4). mini-buckets
variable processed separately. Assuming min-sum problem, MBE calculates sum
898

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

Algorithm 2: AOBB exploring AND/OR search tree (Marinescu & Dechter, 2009b)

1
2
3
4
5
6
7
8
9
10
11

12
13

Input: graphical model = hX, D, Fi, pseudo tree rooted X1 , heuristic function h();
Output: Optimal solution
Create root node labelled X1 let stack created expanded nodes OP EN {s};
Initialize v(s) best partial solution tree rooted (s) ; U B ;
OP EN 6=
Select top node n OPEN;
n node labeled Xi
foreach xi D(Xi )
Add child n0 labeled hXi , xi list successors n;
Initialize v(n0 ) = 0, best partial solution tree rooted n (n0 ) = ;
n node labelled hXi , xi
foreach ancestor k n
Recursively evaluate cost partial solution tree rooted k, based heuristic function
h(), assign cost f (k); // see Algorithm 3
evaluated partial solution better current upper bound k (e.g. f (k) v(k)
Prune subtree current tip node n;
else

14

foreach successor Xj Xi
Add child n0 labeled Xj list successors n;
Initialize v(n0 ) , best partial solution tree rooted n, (n0 ) ;

15
16
17

18
19
20
21
22
23
24
25
26
27

28
29

Add successors n top OPEN;
list successors node n empty
node n root node
return solution: v(n), (n) ;
else
p node
v(p) v(p) + v(n), (p) (p) (n);
else p node
new value better old one, e.g. v(p) > (c(p, n) + v(n)) minimization
v(p) w(p, n) + v(n), (p) (p) hxi , Xi i;
Remove n list successors p;
Move one level up: n p;

functions mini-bucket eliminates variable using min operator (line 9).
new function placed appropriate lower bucket (line 10). MBE generates bound (lower
minimization upper maximization) optimal value. Higher values take computational resources, yield accurate bounds. large enough (i.e., w ), MBE
coincides full Bucket Elimination.
HEOREM 2 (complexity, Dechter & Rish, 2003). Given graphical model variable ordering
induced width w (o) i-bound parameter i, time mini-bucket algorithm


MBE(i) O(nk min(i,w (o))+1 ) space complexity O(nk min(i,w (o)) ), n number
problem variables k maximum domains size.
Mini-bucket elimination viewed message passing leaves root along minibucket tree. mini-bucket tree graphical model mini-buckets nodes. BucketX
899

fiF LEROVA , ARINESCU , & ECHTER

Algorithm 3: Recursive computation heuristic evaluation function
1

2
3
4

function evalPartialSolutionTree(T (n), h(n))
Input: Partial solution subtree (n) rooted node n, heuristic function h(n);
Output: Heuristic evaluation function f (T (n));
succ(n) ==
n node
return 0;
else

5

return h(n);

6
7
8
9
10
11
12
13

else
n node
let k1 , . . . , kl children n;
P
return li=1 evalPartialSolutionTree(T (ki ), h(ki ));
else n node
let k child n;
return w(n, k) + evalPartialSolutionTree(T (k), h(k));

Algorithm 4: Mini-Bucket Elimination

1

2
3
4
5
6
7
8
9
10

11

12

Input: model = hX, D, Fi, ordering o, parameter
Output: Approximate solution M, ordered augmented buckets
Initialize: Partition functions F Bucket1 , . . . , Bucketn , Bucketi contains functions
whose highest variable Xi .
// Backward pass
p n downto 1
Let h1 , . . . , hj functions (original intermediate) Bucketp ; let S1 , . . . , Sj scopes;
Xp instantiated (Xp = xp )
Assign Xp = xp hi put resulting function appropriate bucket;
else
Generate i-partitioning;
foreach Qk Q0
P
Generate message function hkb : hkb = minxp Xp ji=1 hi ;
Add hkb bucket Xb , largest-index variable scope(hkb );
// Forward pass
Assign value variable ordering combination functions bucket
minimal;
return function computed bucket first variable corresponding assignment;

child BucketY function hXY generated BucketX variable X eliminated,
placed BucketY . Therefore, every vertex root one parent possibly
several child vertices. Note mini-bucket tree corresponds pseudo tree, minibuckets variables combined form call augmented buckets, corresponding
variable nodes (Dechter & Mateescu, 2007).
Mini-bucket elimination often used generate heuristics search algorithms graphical
models, formulated search spaces Kask Dechter (1999a, 1999b) extended
AND/OR search Marinescu Dechter (2005).
900

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

EFINITION 4 (MBE heuristic AND/OR search space, Marinescu & Dechter, 2005). Given
ordered set augmented buckets {B(X1 ), . . . , B(Xn )} generated Mini-Bucket algorithm
BE(i) along bucket tree , given node n AND/OR search tree, static minibucket heuristic function h(n) computed follows:
1. n node, labeled hXp , xp i, then:
X
h(n) =
hkj
hkj {B(Xp )B(Xp1 ...Xpq )}

Namely, sum intermediate functions hkj satisfy following two properties:
generated buckets B(Xk ), Xk descendant Xp bucket tree
reside bucket B(Xp ) bucket B(Xp1 . . . Xpq ) = {B(Xp1 ), . . . , B(Xpq )} correspond ancestors {Xp1 ), . . . , Xpq } Xp
2. n node, labeled Xp , then:
h(n) =

min

msucc(p)

(w(n, m) + h(m))

children n labeled values xp Xp .
established necessary background, turn main part paper,
presenting contributions, beginning extension best-first search m-best task.
customary heuristic search literature without loss generality, assume
remaining paper min-sum optimization problem.

3. Best-First Search Finding Best Solutions
Extending best-first search (Section 2.2) particular popular version, A*, mbest task fairly straightforward suggested, example, Charniak Shimony (1994).
Instead stopping finding optimal solution, algorithm continues exploring search
space, reporting next discovered solutions obtained. show
solutions indeed best found decreasing order optimality.
particular, second solution reported second best solution and, general, ith solution
discovered ith best.
3.1 m-A*: Definition
m-best tree-search variant A* denoted m-A* (Algorithm 5, assumes consistent heuristic)
solves m-best optimization problem general search graph. show later
extended general admissible heuristics.
scheme expands nodes order increasing value f usual A* manner.
keeps lists created nodes OPEN expanded nodes CLOSED, usual, maintaining
search tree, denoted r. Beginning start node s, m-A* picks node smallest
evaluation function f (n) OPEN puts CLOSED (line 7). node goal, new
solution reported (lines 8-13). Otherwise, node expanded children created (lines
15-23). algorithm may encounter node multiple times maintain
901

fiF LEROVA , ARINESCU , & ECHTER

Algorithm 5: m-A* exploring graph, assuming consistent heuristic

1
2
3
4
5
6
7

8
9

10
11

Input: implicit directed search graph G = (N, E), start node set goal nodes Goals,
consistent heuristic evaluation function h(n), parameter
Output: best solutions
Initialize: OPEN=, CLOSED=, tree r = , = 1 (i counts current solution searched for)
OPEN {s}; f (s) = h(s);
Make root r;

OPEN empty
return solutions found far;
Remove node, denoted n, OPEN minimum f (break ties arbitrarily, favour goal nodes
deeper nodes) put CLOSED;
n goal node
Output current solution obtained tracing back pointers n (pointers assigned step
22); denote solution Soli ;
=
return;
else

12

+ 1;

13
14
15
16
17
18
19
20
21
22
23

24

else
Expand node n, generating children Ch ;
foreach n0 Ch
n0 already appears OPEN CLOSED times
Discard node n0 ;
else
Compute current path cost g(n0 ) = g(n) + c(n, n0 );
Compute evaluation function f (n0 ) = g(n0 ) + h(n0 ) ;
Attach pointer n0 back n r;
Insert n0 right place OPEN based f (n0 );
return set best solutions found

copies OPEN CLOSED lists combined (line 17), separate paths copy
explored search tree (lines 22-23). Nodes encountered beyond times discarded (line 18).
denote Ci ith best solution cost, fi (n) cost ith best solution going node
n, fi (n) heuristic evaluation function estimating fi (n) gi (n) hi (n) estimates
ith best costs n n goal, respectively.
heuristic consistent, whenever algorithm reaches node seen (if
search space graph tree), exists possibility new path improving
previously discovered ones. Therefore, lines 17-18 revised following way
account possibility better path n0 discovered:
17
n0 appears already times union OPEN CLOSED
18
g(n0 ) strictly smaller gm (n0 ), current m-best path n0
19
Keep n0 pointer n put n back OPEN
20
Discard earlier subtree rooted n
Figure 4 shows example m-A* finding = 3 shortest paths toy problem.
left hand side Figure 4 shows problem graph 7 variables 8 edges, together
902

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS


3

h(A) = 5

2

h(B) = 4



order
nodes expanded

#1

h(C) = 2

B

#5

C

B f =6

h(D) = 3

C2
C3

1

2

#8



D2

4

#2

m=3
C1 = 8

C f =4

= 10
= 10

#3
D1 f = 6

f =8

2
h(F ) = 1

h(E) = 1

E

F
3

#10 f = 9 #9
E2
F2 f = 8

3
G

h(G) = 0

G4
f = 12

#12
G3

#7 f = 6 #4
F1

f = 8 E1

#11
G2

f = 10

f = 10

#6
G1
f =8

nodes CLOSED

(a) Problem graph

(b) Trace m-A*

Figure 4: Example problem. left: problem graph heuristic values h(n) node.
right: trace m-A* finding = 3 best solutions evaluation function f (n)
node. White nodes CLOSED, grey one created, discarded.

admissible heuristic functions node. Note heuristic consistent. example,
h(A) > h(C) + c(A, C). start node, G goal node. right side Figure
present trace m-A*, evaluation function copy nodes created
time 3rd solution found. white nodes CLOSED, grey one (node G4 )
created, never put OPEN. algorithm expands nodes OPEN increasing order
evaluation functions. assume ties broken favour deeper nodes. First, m-A*
discovers solution C F G cost C1 = 8, next solution C E G
cost C1 = 10 found. third solutions B F G cost C1 = 10. Note
two copies node D, E F four copies G created. goal node G4
discarded, bound total number copies particular node = 3.
N
HEOREM 3. Given graphical model = hX, D, F, n variables whose domain size
bounded k, worst case time space complexity m-A* exploring search tree
O(k n ).
Proof. worst case m-A* would explore entire search tree, whose size O(k n ) (Section 2.3). Since underlying search space tree, algorithm never encounter
nodes once, thus nodes duplicated.
903

fiF LEROVA , ARINESCU , & ECHTER

3.2 Properties m-A*
section extend desirable properties A*, listed Section 2.2, m-best case.
simplicity without loss generality, assume throughout search graph accommodates
least distinct solutions.
HEOREM 4. Given optimization task, implicit directed search graph G integer
parameter 1, m-A* guided admissible heuristic following properties:
1. Soundness completeness: m-A* terminates best solutions generated order
costs.
2. Optimal efficiency consistent heuristic: node surely expanded2 m-A*
must expanded search algorithm traversing G guaranteed find
best solutions heuristic information.
3. Optimal efficiency node expansions: m-A* expands node times
heuristic consistent. ith path found node ith best path.
4. Dominance: Given two heuristic functions h1 h2 , every n h1 (n) < h2 (n),
m-A*1 expand every node surely expanded m-A*2 , m-A*i using heuristic hi .
prove properties m-A* Sections 3.2.1-3.2.2.
3.2.1 OUNDNESS C OMPLETENESS
Algorithm m-A* maintains copies node discards rest. next show
restriction compromise completeness.
P ROPOSITION 1. node discarded m-A* lead m-best solutions.
Proof. Consider consistent heuristic first (as described Algorithm 5). moment
m-A* discovered node n (m + 1)th time, copies n reside OPEN CLOSED
algorithm maintains distinct paths each. Let (m + 1)th path.
prove Theorem 10, node n discovered (m + 1)th time, cost Cnew newly
discovered path new (m + 1)th best, namely better costs already discovered:
Cnew Cm . Therefore, eliminated (m + 1)th path node n guaranteed worse
remaining ones thus part potential m-best optimal solutions
might passing node n.
heuristic consistent, m-A* modified replace worst previously
discovered paths newly found new , cost latter better place new
copy OPEN. Thus, again, safe bound number copies m.
clear along particular solution path evaluation function nodes
bounded paths cost C(), heuristic admissible.
P ROPOSITION 2. following true regarding m-A*:
1. solution path , forall n , f (n) C().
2. precisely defined Section 3.2.3

904

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

2. Unless already discovered m-A*, always node n resides
OPEN.
3. Therefore, long m-A* discover must node OPEN f (n)
C().
Proof. 1. f (n) = g (n) + h(n) since h(n) c (n, t) due admissibility, c (n, t)
actual cost n goal node along , conclude f (n) g (n) + h(n) = C().
2. reachable path root always leaf OPEN unless nodes along path
expanded CLOSED.
3. Follows easily 1 2.
follows immediately Proposition 2 (stated similarly Nilsson, 1982) that:
P ROPOSITION 3. [Necessary condition node expansion] node n expanded m-A*
searching ith best solution (1 m) satisfies f (n) Ci .
also clear
P ROPOSITION 4. [Sufficient condition node expansion] Every node n OPEN,
f (n) < Ci , must expanded m-A* ith best solution found.
Soundness completeness m-A* follows quite immediately.
HEOREM 5 (soundness completeness). Algorithm m-A* generates m-best solutions
order, namely, ith solution generated ith best solution.
Proof. Let us assume contradiction case. Let ith generated solution path
first one generated according best-first order. Namely ith solution
generated cost C C > Ci . However, algorithm selected goal ti along
, evaluation function f (ti ) = gi (ti ) = C, while, based Proposition 2,
node n0 OPEN whose evaluation function Ci . Thus n0 selected
expansion instead ti . contradiction therefore result follows.
3.2.2 MPACT H EURISTIC TRENGTH
Like A*, performance m-A* improves accurate heuristic.
P ROPOSITION 5. Consider two heuristic functions h1 h2 . Let us denote m-A*1 algorithm uses heuristic h1 m-A*2 one using heuristic h2 . heuristic h1
informed h2 , namely every node n, h2 (n) < h1 (n), algorithm m-A*2 expand
every node expanded algorithm m-A*1 finding j th solution
j {1, 2, . . . , m}, assuming tie-breaking rule.
Proof. Since h1 informed h2 , h1 (n) > h2 (n) every non-goal node n. Let us
assume m-A*1 expands non-terminal node n finding j th best solution
cost Cj . node n expanded, means (a) point OPEN (b) evaluation
function satisfies f1 (n) = g(n) + h1 (n) Cj (Proposition 3). Consider current path
start node n. node n0 path selected point expansion thus
905

fiF LEROVA , ARINESCU , & ECHTER

m-A search space

search space
C1 Cm
algorithm
Ci


|Cm
C1 |

search space
explored m-A
compared

Figure 5: schematic representation search spaces explored m-A* algorithm, de
pending cost Cm

evaluation functions nodes also bounded cost j th best solution:
f1 (n0 ) Cj . Since h1 (n0 ) > h2 (n0 ) every node n0 along path , evaluation functions
according heuristic h2 (n) obey:
f2 (n0 ) = g(n0 ) + h2 (n0 ) < g(n0 ) + h1 (n0 ) < Cj

(1)

thus node n0 must also expanded m-A*2 .
Consider case exact heuristic. easy show that:
HEOREM 6. h = h exact heuristic, m-A* generates solutions j-optimal
paths 1 j m.
Proof. Since h exact, f values OPEN expanded sequence values C1 C2
. generated nodes evaluation function f = C definition
. . . Ci . . . Cm
1
optimal paths (since h = h ), f = C2 must paths second
best on. Notice solutions costs.
h = h , m-A*s complexity clearly linear number nodes evaluation
. However, cost function small range values, may
function f Cm
. avoid exponential frontier
exponential number solution paths cost Cm
chose tie-breaking rule expanding deeper nodes first, yielding number node expansions
bounded n, n bounds solution length. Clearly:

HEOREM 7. m-A* access
favour deeper
P h = h , then, using tie-breaking rule
th
nodes, expands #N = #Ni nodes, #Ni length optimal solution
path. Clearly, #N n.

906

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

Figure 6: graph G0 represents new problem instance constructed appending branch leading new goal node node n.

3.2.3 -A* C ONSISTENT H EURISTIC
m-A* uses consistent heuristic, several useful properties.
Optimal efficiency consistent heuristic. Algorithm A* known optimally efficient
consistent heuristic (Dechter & Pearl, 1985). Namely, algorithm extends search
paths root uses heuristic information A* expand every node
surely expanded A*, i.e., expand every n, f (n) < C . extend notion
nodes surely expanded A* m-best case:
-bounded
P ROPOSITION 6. Algorithm m-A* expand node n reachable strictly Cm
path root, regardless tie-breaking rule. set nodes referred surely
expanded m-A*.
-bounded path = {s, n , n , . . . n}. start node
Proof. Let us consider strictly Cm
1 2
clearly expanded beginning search children, including node n1 , placed
, node n must expanded m-A* finding mth best
OPEN. Since f (n1 ) < Cm
1
solution (Proposition 4), children, including n2 , turn placed OPEN. true
nodes , including n.

HEOREM 8 (m-optimal efficiency). search algorithm, guaranteed find m-best
solutions explores search space m-A* consistent heuristic,
expand node surely expanded m-A*. Namely expand every node
, i.e. f (n0 ) < C , n0 .
lies path dominated Cm

proof idea similar work Dechter Pearl (1985). Namely show
algorithm expand node n, surely expanded m-A*, miss one m-best
solutions, applied slightly modified problem:
Proof. Let us consider problem search graph G consistent heuristic h. Assume
node n surely expanded m-A* finding j th best solution. Let B algorithm
uses heuristic h guaranteed find best solutions. Let also assume
node n expanded B. consistency heuristic also allows us better characterize
nodes expanded m-A*.
create new problem graph G0 (see Figure 6) adding new goal node
h(t) = 0, connecting n edge cost c = h(n) + , = 0.5(Cj D)
907

fiF LEROVA , ARINESCU , & ECHTER

= maxn0 Sj f (n0 ), Sj set nodes surely expanded m-A* finding
j th solution. possible show heuristic h admissible graph G0 (Dechter &
Pearl, 1985). Since = 0.5(Cj D), C = 2. construction, evaluation function
new goal node is:

f (t) = g(t) + h(t) = g(n) + c = g(n) + h(n) + = f (n) + + = Cj < Cj

(2)

means reachable path whose cost strictly bounded Cj .
guarantees m-A* expand (Proposition 6), discovering solution cost Cj .
hand, algorithm B, expand node n original problem, still expand
thus reach node discover solution cost Cj , returning true
set best solutions modified problem. contradiction theorem follows.
P ROPOSITION 7. heuristic function employed m-A* consistent, values evaluation function f sequence expanded nodes non-decreasing.
proof straightforward extension result Nilsson (1980).
Proof. Let node n2 expanded immediately n1 . n2 already OPEN time
n1 expanded, node selection rule follows f (n1 ) f (n2 ). n2
OPEN, must added result expansion n1 , i.e., child n1 .
case cost getting n2 start node g(n2 ) = g(n1 ) + c(n1 , n2 )
evaluation function node n2 f (n2 ) = g(n2 ) + h(n2 ) = g(n1 ) + c(n1 , n2 ) + h(n2 ). Since h(n)
consistent, h(n1 ) c(n1 , n2 )+h(n2 ) f (n2 ) g(n1 )+h(n1 ). Namely, f (n2 ) f (n1 ).
heuristic function consistent, stronger condition Proposition 4:
HEOREM 9. Algorithm m-A* using consistent heuristic function:
;
1. expands nodes n f (n) < Cm
;
2. never expands nodes evaluation function f (n) > Cm

3. expands nodes f (n) = Cm , subject tie-breaking rule.
node n never expanded
Proof. 1. Assume exists node n f (n) < Cm
m-A*. situation arise node n never OPEN list, otherwise would
expanded, according Proposition 4. implies parent node n search
space (let us denote node p) never expanded. However, similarly done
. Thus
proof Proposition 7, easy show f (p) f (n) and, consequently f (p) < Cm
node p must also never OPEN, otherwise would expanded. Clearly, true
ancestors n, start node s. Since node clearly OPEN beginning
search, initial assumption incorrect property follows.
2. 3. Follow directly Proposition 3.

Figure 7 provides schematic summary search space explored m-A* consistent heuristic.
908

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS


{n|f(n) < Cm
}

search space
explored m-A

*

*

{n|f (n) > Cm
}


{n|f (n) = Cm
}

Figure 7: nodes explored m-A* algorithm consistent heuristic.
Optimal efficiency node expansions. Whenever node n selected expansion
first time m-A*, algorithm already found shortest path node. extend
property follows:
HEOREM 10. Given consistent heuristic h, m-A* selects node n expansion
ith time, g(n) = gi (n), namely found ith best path start node n.
Proof. induction. = 1 (basic step) theorem holds (Nillson, 1980). Assume also
holds (i 1)th expansion node n. Let us consider ith case, > 1 (inductive step).
already expanded node n (i 1) times due inductive hypothesis already
found (i 1) distinct best paths node n. Let us assume cost newly found
solution path greater ith optimal one, i.e. gi (n) > gi (n). Then, exists different,
undiscovered path n cost g (n) = gi (n) < gi (n). Proposition 2 exists
OPEN node n0 . Obviously, node n0 must located start node node
n. Denoting C (n0 , n) = c(n0 , n1 ) + + c(nk , n), heuristic consistency easily
follows h(n0 ) < C (n0 , n) + h(n) evaluation function node n0 along path
f (n0 ) = g (n0 ) + h(n0 ) < g (n0 ) + C (n0 , n) + h(n). Seeing cost path
n g (n) = g (n0 ) + C , conclude f (n0 ) < f (n). However, contradicts
assumption node n expanded ith time node n0 . theorem follows.
3.2.4 MPACT R EQUIRED B EST OLUTIONS
sequence sizes search spaces explored m-A* function obviously monotonically increasing m. Denoting j-A* i-A* versions m-A* algorithm
search respectively j best solutions, make following straightforward characterization:
P ROPOSITION 8. Given search graph consistent heuristic,
1. node expanded i-A* expanded j-A* < j use tie-breaking
rule.
909

fiF LEROVA , ARINESCU , & ECHTER

2. set S(i, j) nodes defined S(i, j) = {n|Ci < f (n) < Cj } surely expanded
j-A* surely expanded i-A*.
3. Cj = Ci , difference number nodes expanded i-A* j-A* determined
tie-breaking rule.
proof follows trivially Theorem 9. result, larger discrepancy respective costs Cj Ci yields larger difference search spaces explored j-A* i-A*.
difference, however, also depends granularity values sequence
observed evaluation functions increase, related arc costs (or weights) search
graph. Ci = Cj = C, search space explored i-A* j-A* differ
frontier nodes satisfying f (n) = C. Figure 5 represents schematically search spaces explored
i-A* algorithm.

4. Depth-First Branch Bound Finding Best Solutions
Along valuable properties, m-A* inherits also disadvantages A*: exponential space
complexity, makes algorithm infeasible many applications. alternative approach
searching using depth-first branch bound (DFBnB), implemented linear
space necessary therefore often practical. DFBnB finds optimal solution
exploring search space depth-first manner. algorithm maintains cost U best
solution encountered far prunes search nodes whose lower-bounding evaluation function
f (n) = g(n) + h(n) larger U . Extending DFBnB m-best task straightforward,
describe next.
4.1 m-BB Algorithm
Algorithm m-BB, depth-first branch bound extension m-best task, explores
search tree presented Algorithm 6. usual, algorithm maintains lists OPEN
CLOSED nodes. also maintains sorted list CANDIDATE nodes contains best
solutions found far. Nodes OPEN organized last - first manner order
facilitate depth-first exploration search space (i.e., OPEN stack). step, m-BB
expands next node n OPEN (line 5). goal node, new complete solution found
(line 6) stored CANDIDATE list (line 7-9), re-sorted (line 10).
best solutions maintained (lines 11-13).
main modification depth-first branch bound, extended m-best task,
pruning condition. Let U1 U2 . . . Um denote costs best solutions
encountered thus far. Um upper bound used pruning. solutions discovered, pruning takes place. Algorithm m-BB expands current node n, generates children
(lines 15-17) computes evaluation function (line 18-19). prunes subproblem n
iff f (n) Um (lines 20-23). easy see algorithm terminates, outputs
m-best solutions problem.
HEOREM 11. Algorithm m-BB sound complete m-best solutions task.
Proof. Algorithm m-BB explores search space systematically. solutions
, C
skipped ones satisfying f (n) Um (see step 22). Since Um Cm

910

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

Algorithm 6: m-BB exploring graph, assuming consistent heuristic

1

2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

24

Input: implicit directed search graph G = (N, E) start node n0 set goal nodes Goals.
heuristic function h(n). Parameter (the number desired solutions).
Output: best solutions
Initialize: OPEN=, CLOSED=, tree r = , sorted list CANDIDATE = , UpperBound = , = 1 (i
counts current solution searched for);
Put start node n0 OPEN, g(n0 ) = 0, f (n0 ) = h(n0 );
Assign n0 root r;
OPEN empty
Remove top node OPEN, denoted n, put CLOSED;
n goal node
soli solution obtained tracing back pointers n n0 (pointers assigned step 17);
Ci cost soli ;
Place solution soli CANDIDATE;
Sort CANDIDATE increasing order solution costs;
size CANDIDATE list
Um cost mth element CANDIDATE;
Keep first elements CANDIDATE, discard rest;
else
Expand node n, generating children succ(n);
forall n0 succ(n)
Attach pointer n0 back n r;
g(n0 ) = g(n) + c(n, n0 );
f (n0 ) = g(n0 ) + h(n0 );
f (n0 ) < Um
Place n0 OPEN;
else
Discard n0 ;
return solutions CANDIDATE list

therefore path cannot lead newly discovered
best solution cost, implies f (n) Cm
m-best cost.

N
HEOREM 12. Given graphical model = hX, D, F, i, worst case time complexity
m-BB explores search tree O(k n + log m), n number variables,
k domain size number required solutions. Space complexity O(n).
Proof. worst case m-BB would explore entire search tree size O(k n ). maintaining
CANDIDATE list introduces additional time overhead O(log m). Since search tree
yields caching, m-BB uses space linear number variables.
4.2 Characterization Search Space Explored m-BB
already shown m-A* superior exact search algorithm finding mbest solutions heuristic consistent (Theorem 8). particular, m-BB must expand
}.
nodes surely expanded m-A*, namely set nodes {n|f (n) < Cm
Theorem 8 pruning condition clear that:
911

fiF LEROVA , ARINESCU , & ECHTER

P ROPOSITION 9. Given consistent heuristic m-BB must expand node set {n|f (n) <
}. Also, instances m-BB expands nodes satisfying f (n) > C .
Cm

Several sources overhead m-BB discussed next.
4.2.1 -BB VS . BB
Pruning m-BB occur upper bound current mth best solution assigned
valid value, i.e., solutions found. absence determinism, solutions
consistent, time takes find arbitrary solutions depth-first manner O(m n),
n length solution (for graphical models n coincides number variables).
problem contains determinism may difficult find even single solution. means
m-BB search may exhaustive quite time.
4.2.2 MPACT OLUTION RDER
difference number nodes expanded BB m-BB depends greatly variance
solution costs. solutions cost, U1 = Um . However,
situation unlikely therefore conditions m-BBs node expansions impacted
1 , . . . , U j } non-increasing sequence
order solutions discovered. Let {Um

th
upper bounds best solution, point m-BB uncovered j th solution.
j
Initially Um
= , j {1, . . . , 1}.
P ROPOSITION 10. discovery (j 1)th j th solutions set nodes
j1
U j U j1 .
expanded m-BB included Sj = {n | f (n) Um
}, Cm


Proof. discovering (j 1)th j th solutions m-BB expands nodes satisfying
j1
j1
{n | f (n) Um
}, hence j : Cj Um
. j th solution found, either replaces
j
th
previous bound solution Um = Cj k th upper bound, k {1, . . . , 1}, yielding
j1
j
U j U j1 .
. Either way, Cm
= Um1
Um


4.2.3 RDERING OVERHEAD
need keep list sorted solutions (the CANDIDATE list) implies O(log m) overhead
new solution discovered. total number solutions encountered termination
hard characterize.
4.2.4 C ACHING OVERHEAD
overhead related caching arises m-BB explores search graph uses caching.
version algorithm (not explicitly presented) stores best partial solutions
fully explored subproblems (and subset partial set discovered) re-uses
results whenever subproblem encountered again. order implement caching, mBB requires store list length node cached. Moreover, cached partial
solutions need sorted, yields O(m log m) time overhead per cached node.
912

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

5. Adapting m-A* m-BB Graphical Models
main task find best solutions optimization tasks graphical models. Therefore,
adapt m-best search algorithms m-A* m-BB explore AND/OR search space
graphical models, yielding algorithms m-AOBF m-AOBB, respectively. also describe
hybrid algorithm BE+m-BF, combining Bucket Elimination m-A*.
5.1 m-AOBF: Best-First AND/OR Search Best Solutions Graphical Models
extension algorithm AOBF (Section 2.3.2) m-best task seems fairly straightforward,
principle. m-AOBF AOBF continues searching discovering first solution,
required number best solutions obtained. actual implementation requires several
modifications discuss next.
easy extend AOBFs bottom-up node values updates corresponding arc marking
mechanism m-best task. Therefore, order keep track current best partial solution
tree searching ith best solution adopt naive approach maintains explicitly
list OPEN containing entire partial solution trees (not nodes), sorted ascending order
heuristic evaluation costs. Algorithm 7 presents pseudo-code simple scheme explores
AND/OR search tree generates solutions one one order costs. step,
algorithm removes next partial solution tree 0 OPEN (line 4). 0 complete
solution, added list solutions along cost (lines 5-8), otherwise algorithm
expands tip node n 0 , generating successors (line 10-17). newly generated node
n0 added 0 separately, yielding new partial solution tree 00 (lines 19-23), whose cost
recursively evaluated using Algorithm 3, AOBB (line 28). new partial trees
placed OPEN (line 29). Search stops solutions found.
note maintenance OPEN list containing explicit partial solution subtrees
source significant additional overhead become apparent empirical evaluation
Section 7. Thus, question whether performance m-AOBF improved open
therefore rich topic future work.
m-A* properties (Section 3.2) extended m-AOBF. particular, algorithm mAOBF admissible heuristic sound complete, terminating best solutions
generated order costs. m-AOBF also optimal terms number nodes expanded
compared algorithm explores AND/OR search space
consistent heuristic function.
HEOREM 13 (m-AOBF complexity). complexity algorithm m-AOBF traversing either
h1
AND/OR search tree context minimal AND/OR search graph time space O(k deg ),
h depth underlying pseudo tree, k maximum domain size, deg bounds
degree nodes pseudo tree. pseudo tree balanced (each internal node
exactly deg child nodes), time space complexity O(k n ), n number
variables.
real complexity bound m-AOBF comes cost function. appears however
maintaining OPEN list brute force manner lend easily effective way
enumerating partial solution subtrees therefore search space partial solution
subtrees actually exponential n. detailed proof Theorem 13 given Appendix.
913

fiF LEROVA , ARINESCU , & ECHTER

Algorithm 7: m-AOBF exploring AND/OR search tree

1
2
3
4
5
6
7
8
9

10
11
12
13
14
15
16
17
18

19
20
21
22
23
24
25
26
27
28

29
30

Input: graphical model = hX, D, Fi, pseudo tree rooted X1 , heuristic function h(), parameter m;
Output: best solutions
Create root node labelled X1 , let G = {s} (explored search space) = {s} (partial solution tree);
Initialize ; OP EN {T }; = 1; (i counts current solution searched for);
OP EN 6=
Select top partial solution tree 0 remove OPEN;
0 complete solution
{hf (T 0 ), 0 i};
+ 1;
continue;
Select non-terminal tip node n 0 ;
// Expand node n
n node labeled Xi
forall xi D(Xi )
Create child n0 labeled hXi , xi i;
succ(n) succ(n) {n0 };
else n node labeled hXi , xi
forall successor Xj Xi
Create child n0 labeled Xj ;
succ(n) succ(n) {n0 };
G G {succ(n)};
// Generate new partial solution trees
L ;
forall n0 succ(n) Initialize v(n0 ) = h(n0 );
n node
forall n0 succ(n)
Create new partial solution tree 00 0 {n0 };
L L {T 00 };
else n node
Create new partial solution tree 00 0 {succ(n)};
forall 00 L
Recursively evaluate assign f (T 00 ) cost partial solution tree 00 , based heuristic
function h(); // see Algorithm 3
Place 00 OPEN, keeping sorted ascending order costs f (T 00 );
return best solutions found S;

5.2 m-AOBB: AND/OR Branch Bound Best Solutions Graphical Models
Algorithm m-AOBB extends AND/OR Branch Bound search (AOBB, Section 2.3.3)
m-best task. main difference AOBB m-AOBB value function computed
node. m-AOBB tracks costs best partial solutions solved subproblem. Thus extends node value v(n) solution tree (n) rooted n AOBB ordered
sets length m, denoted v(n) (n), respectively, v(n) = {v1 (n), . . . , vm (n)}
ordered set costs best solutions subproblem rooted n, (n) =
(n)} set corresponding solution trees. extension arises due
{T1 (n), . . . , Tm
depth-first manner search space exploration m-AOBB conjunction AND/OR decomposition. Therefore, due AND/OR decomposition m-AOBB needs completely solve
914

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

Algorithm 8: m-AOBB exploring AND/OR search tree

1
2

3
4
5
6
7
8
9
10
11
12
13
14

Input: graphical model = hX, D, Fi, pseudo tree rooted X1 , heuristic function h(), parameter m;
Output: best solutions
// INITIALIZE
Create root node labeled X1 let stack created expanded nodes OP EN = {s};
Initialize v(s) = (a set bounds best solutions s) set best partial solution trees rooted
(s) = ; U B = , sorted list DIDAT E = ;
OP EN 6=
Select top node n OPEN;
// EXPAND
n node labeled Xi
foreach xi D(Xi )
Add child n0 labeled hXi , xi list succ(n) containing successors n;
Initialize v(n0 ) = 0, set best partial solution trees rooted n (n0 ) = ;
n node labeled hXi , xi
Let p ancestor n;
Recursively evaluate assign f (p) cost partial solution tree rooted p, based heuristic h(); //
see Algorithm 3
vm (p) < f (p) vm (p)
Prune subtree current tip node n;
else
foreach successor Xj Xi
Add child n0 labeled Xj list succ(n) containing successors n;
Initialize v(n0 ) = , set best partial solution trees rooted n (n0 ) = ;

15
16
17

18
19
20
21
22
23
24
25

26

27
28
29

30

31
32
33
34

Remove n OPEN add succ(n) top OPEN;
// PROPAGATE
list successors node n empty
n root node
return set solutions rooted n costs: (n), v(n) ;
else
Update ancestors n, nodes p, bottom up:
p node
Combine set partial solution trees subproblem rooted p (p) set partial
solution trees rooted n (n) costs v(p) v(n); // see Algorithm 9
Assign resulting set costs set best partial solution trees respectively v(p)
(p);
else p node
foreach solution cost vi (n) set v(n)
Update cost weight arc, creating new set costs v 0 (n):
vi0 (n) = c(p, n) + vi (n);
Merge sets partial solutions v(n) v(p) sets partial solution trees rooted p n:
(p) (n), keeping best elements; //Algorithm 10
Assign results merging respectively v(p) (p);
Remove n list successors p;
Move one level up: n p;
return v(s) (s)

subproblems rooted children n0 node n, even single solution
subproblem n acquired (unlike m-BB case). Consequently, bottom-up phase
sets costs propagated updated. m-AOBF hand, maintains
set partial solution trees.
915

fiF LEROVA , ARINESCU , & ECHTER

Algorithm 9: Combining sets costs partial solution trees
1

2
3
4

5
6
7
8
9
10
11

function Combine(v(n), v(p), (n),T (n))
Input: Input sorted sets costs v(n), v(p), corresponding partial solution trees (n), (p), number
required solutions
Output: set costs best combined solutions v 0 (p), corresponding partial solution trees 0 (p)
// INITIALIZE
Sorted list OPEN, initially empty; //contains potential cost combinations
v 0 (p) ; 0 (p) ;
k = 1; //number partial solutions already assembled, total
// Search possible combinations
OPEN v1 (n) + v1 (p);
k < OP EN empty
Remove top node V OPEN, V = Svi (n) + vj (p);
vk0 (p) V ;
0
(p) Ti (n) Tj (p);
vi+1 (n) + vj (p) OP EN
Put vi+1 (n) + vj (p) OP EN ;

13

vi (n) + vj+1 (p) OP EN
Put vi (n) + vj+1 (p) OP EN ;

14

k k + 1;

15

return v 0 (p), (p);

12

Unlike m-AOBF discovers solutions one one order costs, m-AOBB (pseudocode Algorithm 8) reports entire set solutions once, termination. m-AOBB interleaves forward node expansion (lines 5-18) backward propagation (or cost revision) step
(lines 19-33) updates node values search terminates. node n pruned (lines 12-13)
current upper bound mth solution n, vm (n), lower nodes evaluation
functions f (n), computed recursively AOBB (Algorithm 3). bottom-up
propagation phase node partial solutions subproblems rooted nodes
children combined (line 24-26, Algorithm 9). parent node p v(p) (p)
updated incorporate new possibly better partial solutions rooted child node n (lines
27-31, Algorithm 10).
5.2.1 C HARACTERIZING N ODE P ROCESSING OVERHEAD
addition increase explored search space m-BB experiences compared BB
due reduced pruning (Section 4.2), AND/OR search introduces additional overhead mAOBB. propagation set costs partial solution trees leads increase
memory factor per node. Processing partial solutions nodes
introduces additional overhead.
HEOREM 14. Algorithm m-AOBB exploring AND/OR search tree time overhead
O(m deg log m) per node O(m k) per node, deg bounds degree
pseudo tree k largest domain size. Assuming k < deg log(m), total worst case time
complexity O(n k h deg log(m)) space complexity O(mn). time complexity

m-AOBB exploring AND/OR search graph O(n k w deg log(m)), space complexity

O(mn k w ).
916

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

Algorithm 10: Merging sets costs partial solution trees
1

2
3
4
5

6
7
8
9
10
11
12
13
14
15
16

17

function Merge(v(n),v(p),T (n),T (p))
Input: Input sorted cost sets v(n) v(p), sets corresponding partial solution trees (n) (p),
number required solutions
Output: v 0 (p), merged set best solution costs, 0 (p) set corresponding partial solution trees
// INITIALIZE
v 0 (p) ;
0 (p) ;
i, j 1; //indices cost sets
k 1; //index resulting array
// Merge two sorted sets
k
vi (p) vj (n)
vk0 (p) vi (p);
0
Tk (p) Ti (p);
+ 1;
k k + 1;
else
vk0 (p) vj (n);
0
Tk (p) Tj (n);
j j + 1;
k k + 1;
return v 0 (p) 0 (p);

Proof. Combining sets current m-best partial solutions (Algorithm 9) introduces overheard
O(m log(m)). resulting time overhead per node O(deg log(m)). Merging two
sorted sets costs (Algorithm 10) done O(m) steps. node O(k) children,
resulting overhead O(m k). Assuming k < deg log(m), complexity dominated
processing nodes. worst case, tree version m-AOBB, called m-AOBB-tree,
would explore complete search space size O(n k h ), h bounds depth pseudo

tree, graph version, called m-AOBB-graph, would visit space size O(n k w ),
w induced width pseudo tree. space complexity m-AOBB-tree follows
need propagate sets O(m) partial solutions length O(n). time overhead
m-AOBB AND/OR trees AND/OR graphs. space complexity m-AOBBgraph explained need store partial solutions cached node.

5.3 Algorithm BE+m-BF
known exact heuristics graphical models generated Bucket Elimination
(BE) algorithm described Section 2.3.4. therefore first compile exact heuristics along
ordering using apply m-A* (or m-AOBF, work point),
using exact heuristics. resulting algorithm called BE+m-BF. Worst-case analysis
algorithm show yields best worst-case complexity compared known
m-best algorithm graphical models.
917

fiF LEROVA , ARINESCU , & ECHTER

HEOREM 15. time complexity BE+m-BF O(nk w+1 + nm) n number
variables, k largest domain size, w induced width problem desired
number solutions. space complexity O(nk w + nm).
Proof. BEs time complexity O(nk w+1 ) space complexity O(nk w ) (Dechter, 1999).
Since compiles exact heuristic function, m-A* exact heuristic expands nodes
f (n) = Cj searching ith solution. algorithm breaks ties favour
deeper nodes, expand nodes solution paths. path length n, yielding total
time space complexity step algorithm equal O(n m).

6. Related Work
distinguish several primary approaches employed earlier m-best exact algorithms,
mentioned already Introduction. Note original works include space
complexity analysis bounds provided often own.
first influential approach introduced Lawler (1972). aimed use of-theshelf optimization schemes best solutions. Lawler showed extend given optimization
algorithm m-best task. step, algorithm seeks best solution re-formulation
original problem excludes solutions already discovered. scheme improved years still one primary strategies finding m-best solutions.
time space complexity Lawlers scheme O(nmT (n)) O(S(n)) respectively,
(n) S(n) time space complexity finding single best solution. example,
use AOBF underlying optimization algorithm, use Lawlers method yields time


complexity O(n2 mk w log n ) space complexity O(nk w log n ).
Hamacher Queyranne (1985) built upon Lawlers work used building blocks algorithms find first second best solutions. two best solutions generated,
new problem formulated second best solution best solution new problem.
Then, second best solution new problem becomes overall third best solution
procedure repeated. algorithm time complexity O(m T2 (n)) space complexity
O(S2 (n)), T2 (n) S2 (n) respectively time space finding second best
solution. complexity method always bounded Lawler, seeing
Lawlers scheme used algorithm finding second best task. Using m-AOBF

find two best solutions, obtain time complexity O(2mnk w log n ) space complexity

O(2nk w log n ).
Nilsson (1998) applied Lawlers method using join-tree algorithm. top algorithm reuses computations previous iterations. scheme, called max-flow algorithm, uses
message-passing junction tree calculate initial max-marginal functions cluster
(e.g. probability values probable assignments task) yielding best solution. Note
step equivalent running bucket-elimination algorithm. Subsequent solutions recovered conditioning search consult generated function. time complexity analysis
Nilsson (1998) O(2p|C| + 2mp|R| + pm log (pm)), p number cliques joint
tree, |C| size largest clique |R| size largest residual (i.e. number
variables cluster neighbouring clusters). space complexity bounded
O(p|C| + p(|S|)), |S| size separator clusters. applied buckettree, Nilssons scheme time space complexity O(2nk w+1 + mn(2k + log(mn))


O(nk w +1 + nk w ) respectively, since bucket tree p = n cliques, whose size bounded
918

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS



|C| = k w +1 residual cluster |R| = k (the domain single variable).
Thus algorithm better time complexity schemes mentioned far, except
BE+m-BF.
Two works de Campos et al. build upon Nilssons approach, extending solving mbest MAP task using genetic algorithms (de Campos, Gamez, & Moral, 1999) probability trees
(de Campos, Gamez, & Moral, 2004), respectively. schemes approximate authors
provide theoretical analysis complexity.
Fairly recently, Yanover Weiss (2004) developed iterative scheme based belief propagation, called BMMF. iteration BMMF uses Loopy Belief Propagation solve two new
problems obtained restricting values certain variables. applied junction tree

induced width w (whose largest cluster size bounded k w +1 ) exact algorithm
time complexity O(2mnk w+1 ) space complexity O(nk w + mnk). applied
loopy graph, BMMF guaranteed find exact solutions.
Another approach based Lawlers idea uses optimization via LP-relaxation (Wainwright
& Jordan, 2003), formulated Fromer Globerson (2009). method, called Spanning TRee
Inequalities Partitioning Enumerating Solutions, STRIPES, also partitions search
space, systematically excluding previously determined assignments. step new
constraints added LP optimization problem, solved via off-the-shelf LP-solver.
general, algorithm approximate. However, trees junction-trees exact
underlying LP solver reports solutions within time limit. PESTEELARS extension
scheme Batra (2012) solves LP relaxation using message-passing approach that,
unlike conventional LP solvers, exploits structure problems graph. complexity
LP-based algorithm hard characterize using usual graph parameters.
Another approach extends variable elimination (or dynamic programming) schemes directly
obtain best solutions. recent paper (Flerova et al., 2011) extended bucket elimination mini-bucket elimination m-best solutions task, yielding exact scheme called
elim-m-opt approximate version called mbe-m-opt, respectively. work also embeds
m-best optimization task within semi-ring framework. time space complexities


algorithm elim-m-opt bounded O(m log mnk w +1 ) O(mnk w ), respectively.
Two related dynamic programming based ideas Seroussi Golmard (1994) Elliot
(2007). Seroussi Golmard extract solutions directly, propagating best partial
solutions along junction tree. Given junction tree p cliques, largest cluster size |C|, separator
size bounded |S| branching degree deg, time complexity algorithm O(m2 p
|C| deg) space complexity O(m p |S|). Adapted bucket tree, algorithm


time complexity O(m2 nk w +1 deg) space complexity O(mnk w ). Elliot propagates
best partial solutions along representation called Valued And-Or Acyclic Graph, also known
smooth deterministic decomposable negation normal form (sd-DNNF) (Darwiche, 2001).

time complexity Elliots algorithm O(nk w +1 log (m deg)) space complexity

O(mnk w +1 ).
Several methods focus search schemes obtaining multiple optimal solution k shortest
paths task (KSP). survey see paper Eppstein (1994). majority algorithms
assume entire search graph available memory thus directly applicable.
recent exception Aljazzar Leue (2011), whose K algorithm finds k shortest paths
search on-the-fly thus potentially useful graphical models. algorithm
interleaves A* search problems implicit graph G Dijkstras algorithm (1959) spe919

fiF LEROVA , ARINESCU , & ECHTER

BE+m-BF


O(nk w

m-AOBF


O(nk w

log n

+1

+ mn)

O(2nk w

)



Nilsson 1998

+1

+ mn(2 log(mn) + 2k))

elim-m-opt


Gosh et al. 2012

Elliot
2007


Yanover Weiss 2004

m-AOBB


Hamacher Queyranne

O(mnk w

O(mnk w

+1

+1



O(mnk w )

log m)

log (m deg))

O(mnk w

O(mnk w deg log m)



O(2mnk w

+1

log n

Aljazzar Leue 2011


O(nk w w log (nk) + m)

)

)

Seroussi Golmard 1994
O(m2 nk w+1 deg)

Lawler 1972

O(n2 mk w )

m-A*-tree
O(k n )

m-BB-tree

O(k n + log m)

Figure 8: Time complexity comparison exact m-best algorithms specified bucket tree.
parent node graph better complexity children. Problem parameters:
n - number variables, k - largest domain size, w - induced width, deg - degree
join (bucket) tree. algorithms highlighted.

cific path graph structure denoted P (G). P (G) directed graph, vertices correspond
edges problem graph G. Given consistent heuristic, K , applied AND/OR

search graph time space O(nk w w log(n k) + m).
recently, Gosh, et al., (2012) introduced best-first search algorithm generating ordered solutions explicit AND/OR trees graphs. time complexity algorithm
bounded O(mnk w ), applied context-minimal AND/OR search graph. space
complexity bounded O(s nk w+1 ), number candidate solutions generated
stored algorithm, hard quantify using usual graph parameters. However, approach,
explores space complete solutions, seem practical graphical models
requires entire AND/OR search space fully explicated memory attempting generate even second best solution. contrast, algorithms generate best
solutions traversing space partial solutions.
920

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

Figure 8 provides visual comparison worst-case time complexity bounds
discussed schemes form directed graph, node corresponds algorithm
parent graph better complexity child. assume analysis
n >> > k > 2.
see best emerging scheme, far worst-case performance goes BE+m-BF.
However, since requires compiling exact heuristics, often infeasible. see also
algorithm elim-m-opt appears relatively good time complexity superior, example, mAOBB search. However, showed previous work (Flerova et al., 2011), quite limited
empirically. Note worst-case analysis often fails capture practical performance, either algorithms good worst-case performance require much memory,
ignores power cost function bounding performance.

7. Experimental Evaluation
experiments consist two parts: evaluation m-best search algorithms benchmarks
recent UAI Pascal2 competitions comparison schemes previously developed algorithms randomly generated networks, whose parameters structure
restricted due limitations available implementations competing schemes.
defer discussion second part experiments Section 7.5, concentrating
evaluation m-best search schemes only.
7.1 Overview Methodology
used 6 benchmarks, all, except binary grids, came real-world domains:
1. Pedigrees
2. Binary grids
3. WCSP
4. Promedas
5. Proteins
6. Segmentation
pedigrees benchmark (pedigree*) used UAI 2008 competition.3 arise
domain genetic linkage analysis associated task haplotyping.
haplotype sequence alleles different loci inherited individual one parent,
two haplotypes (maternal paternal) individual constitute individuals genotype.
genotypes measured standard procedures, result list unordered pairs
alleles, one pair locus. maximum likelihood haplotype problem consists finding
joint haplotype configuration members pedigree maximizes probability
data. shown that, given pedigree data, haplotyping problem equivalent
computing probable explanation Bayesian network represents pedigree (see
paper Fishelson Geiger (2002) details).
3. http://graphmod.ics.uci.edu/group/Repository

921

fiF LEROVA , ARINESCU , & ECHTER

Benchmark
Pedigrees
Grids
WCSP
Promedas
Proteins
Segmentation

# inst
13
32
61
86
72
47

n
581-1006
144-2500
25-1057
197-2113
15-242
222-234

k
3-7
2
2-100
2
18-81
2-21

w
16-39
15-90
5-287
5-120
5-16
15-18

hT
52-104
48-283
11-337
34-187
7-44
47-67

Table 1: Benchmark parameters: # inst - number instances, n - number variables, k - domain
size, w - induced width, hT - pseudo tree height.

binary grid networks (50-*, 75-* 90-*)4 nodes corresponding
binary variables arranged N N square functions defined pairs
variables generated uniformly randomly.
WCSP (*.wcsp) benchmark includes random binary WCSPs, scheduling problems
SPOT5 benchmark, radio link frequency assignment problems, providing large variety
problem parameters.
Protein side-chain prediction (pdb*) networks correspond side-chain conformation prediction tasks protein folding problem (Yanover, Schueler-Furman, & Weiss, 2008).
resulting instances relatively nodes, large variable domains, generally rendering
instances complex.
Promedas (or chain *) segmentation (* s.binary) probabilistic networks
come set problems used 2011 Probabilistic Inference Challenge.5 Promedas instances based Bayesian network model developed expert systems medical diagnosis
(Wemmenhove, Mooij, Wiegerinck, Leisink, Kappen, & Neijt, 2007). Segmentation common
benchmark used computer vision, modeling task image segmentation MPE problem,
namely assigning label every pixel image, pixels label share
certain characteristics.
Table 1 describes benchmark parameters: # inst - number instances, n - number variables, k - maximum domain size, w - induced width ordering used, hT - pseudo-tree height.
induced width one crucial parameters indicating difficulty problem,
difference induced width mini-bucket i-bound signifies strength
heuristic. i-bound considerably smaller induced width, heuristic
weak, i-bound equal greater induced width yields exact heuristic,
turn yields much faster search. Clearly, large number variables, high domain size large
pseudo tree height suggest harder problems.
7.1.1 LGORITHMS
distinguish 6 algorithms: BE+m-BF, m-A*-tree m-BB-tree exploring regular
search tree modifications explore AND/OR search tree, denoted m-AOBF-tree
m-AOBB-tree. also consider variant m-AOBF explores AND/OR search graph
m-AOBF-graph. implement m-AOBB AND/OR search graph,
4. http://graphmod.ics.uci.edu/repos/mpe/grids/
5. http://www.cs.huji.ac.il/project/PASCAL/archive/mpe.tgz

922

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

overhead due book-keeping looked prohibitive. used m-AOBF representative AND/OR
graph search, see proved indeed cost-effective. algorithms
guided pre-compiled mini-bucket heuristics, described Section 2.3.4. used 10 i-bounds,
ranging 2 22. However, hard problems computing mini-bucket heuristic
larger i-bounds proved infeasible, actual range i-bounds varies among benchmarks
among instances within benchmark. algorithms restricted static variable ordering
computed using min-fill heuristic (Kjrulff, 1990). AND/OR schemes used pseudo
tree. implementation algorithms m-BB, m-BF m-AOBF break ties lexicographically,
algorithm m-AOBB solves independent subproblems rooted node increasing
order lower bound heuristic estimates.
algorithms implemented C++ (32-bit) experiments run 2.6GHz
quad-core processor. memory limit set 4 GB per problem, time limit 3 hours.
report CPU time (in seconds) number nodes expanded search. uniformity
consider task throughout maximization-product problem, also known
Probable Explanation task (MPE MAP). focus complete exact solutions thus
report results algorithm found less solutions (for best-first schemes)
optimality solutions proved (for branch bound schemes).
7.1.2 G OALS E MPIRICAL E VALUATION
address following aspects:
1. Comparing best-first depth-first branch bound approaches
2. impact AND/OR decomposition search performance
3. Scalability algorithms number required solutions
4. Comparison earlier proposed algorithms
7.2 Main Trends Behavior Algorithms
Tables 2, 4, 6, 8, 10, 12 present algorithms raw results form runtime
seconds number expanded nodes select instances benchmark, selected
best illustrate prevailing trends. benchmark show results two values
i-bound, corresponding, cases, relatively weak strong heuristics. Note ibound impact BE+m-BF, since always calculates exact heuristic. show three
values number solutions m, equal 1 (ordinary optimization problem), 10 100.
order see bigger picture, Figures 9-14 show bar charts representing
benchmark median runtime number instances solved algorithm particular
strength heuristic (i-bound) {1, 2, 5, 10, 100}. y-axis logarithmic scale.
numbers bars indicate actual values median time seconds number
solved instances, respectively. important note figures account harder
instances, i-bound yield exact heuristic. acknowledge median
times strictly comparable since calculated varied number instances solved
algorithm. However, metric robust outliers gives us intuition
algorithms relative success. addition, Tables 3, 5, 7, 9, 11 13 show benchmark
number instances, given algorithm best terms runtime terms
number expanded nodes. several algorithms show best result, counts towards
score them.
923

fiF LEROVA , ARINESCU , & ECHTER

instance
(n,k,w ,h)

i-bound

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
Timeout
0.05
6647
OOM
OOM
OOM
1269.0
348648825
22.99
2320223
0.05
6647

number solutions
m=10
nodes
OOM
OOM
OOM
Timeout
Timeout
0.06
6671
OOM
OOM
OOM
1275.65
348648869
164.72
12110559
0.06
6671

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
OOT
1461.76
46419482
OOM
OOM
OOM
OOM
151.49
33563300
107.03
4274313
OOM

OOM
OOM
OOM
OOT
2389.32
74629839
OOM
OOM
OOM
OOM
152.27
33609110
185.66
7245553
OOM

OOM
OOM
OOM
OOT
3321.47
83802828
OOM
OOM
OOM
OOM
148.08
36255491
251.98
8319419
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
5760.25
14260410
OOM
OOM
OOM
793.56
2579416
Timeout
484.69
1530768
OOM

OOM
OOM
OOM
Timeout
OOT
OOM
OOM
OOM
OOM
Timeout
551.67
1995114
OOM

OOM
OOM
OOM
Timeout
OOT
OOM
OOM
OOM
OOM
Timeout
858.29
3507104
OOM

10.22

10.29

algorithm

m=1
time

503.wcsp

4

(144, 4, 9, 44)

8

myciel5g 3.wcsp

4

(47,2, 19, 46)

8

satellite01ac.wcsp

4

(79, 8, 19, 56)

8

29.wcsp

4

(83, 4, 18, 58)

8

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

10.23
11.59
12.96
1.81
0.0
0.05
0.09
0.02
0.17
0.02
0.0

nodes

464134
OOM
938812
2243619
87717
111
2347
1401
2098
37629
1577
111

time

11.58
12.89
2.63
0.0
0.05
0.09
0.02
0.17
0.33
0.0

464182
OOM
938869
2245137
147851
168
2395
1447
2155
38463
24239
168

m=100
time

nodes
OOM
OOM
OOM
Timeout
Timeout
0.06
6984
OOM
OOM
OOM
1255.46
348651775
8010.42
568148386
0.06
6984

11.57
12.77
115.3
0.01
0.08
0.13
0.02
0.25
79.38
0.01

464698
OOM
939508
2279587
9189667
739
2899
1482
2724
55125
6731546
739

Table 2: WCSP: CPU time (in seconds) number nodes expanded. Timeout stands
exceeding time limit 3 hours. OOM indicates 4GB memory. bold
highlight best time number nodes m. Parameters: n - number
variables, k - domain size, w - induced width, h - pseudo tree height.

next provide elaboration interpretation results.
7.2.1 WCSP
Table 2 shows results two values i-bound select instances chosen best illustrate
common trends seen across WCSP benchmark. Figure 9 presents median time
number solved instances algorithm i=16. Table 3 shows i-bound (i=16)
number instances schemes best runtime best number
expanded nodes. many problem instances benchmark mini-bucket elimination
large i-bounds infeasible, thus present results small medium i-bounds.
924

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

BE+m-BF
m-AOBF tree

m-AOBF graph
m-A* tree

1.94
0.01
0.04
0.08
0.05

1.71

10

100

6
3
3
2

3
3
2

3
3
2

3
3
2

2

Solved instances

WCSPs, i=16

3
3

4

5

6

6

m-BB tree
m-AOBB tree

5
5

5
5

5

6

m-AOBF graph
m-A* tree

6

BE+m-BF
m-AOBF tree
6

0.0
0.01
0.03
0.05

1.7
0.0
0.01
0.02
0.05

5


2

5
5

1

10-1

2.8

2.79

2.71
1.71
0.0
0.01
0.02
0.04

1.71
0.1

0.0
0.01
0.02
0.05

Median time
WCSPs, i=16

100

m-BB tree
m-AOBB tree
6.33

101

100

1

2

5



10

100

Figure 9: Median time number solved instances (out 49) select values WCSPs,
i-bound=16. Numbers bars - actual values time (sec) # instances. Total
instances benchmark: 61, discarded instances due exact heuristic: 12.

BE+m-BF. suggested theory, whenever BE+m-BF run memory,
efficient scheme. See example Table 2, 503.wcsp 29.wcsp. However, calculation
exact heuristic feasible easier instances and, Figure 9 shows, solve
925

fiF LEROVA , ARINESCU , & ECHTER

algorithm

solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

WCSPs: # inst=61, n=14-1058
k=2-100, w =6-287, hT =8-585, i-bound=16
m=1
m=2
m=5
m=10
m=100
#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN
43
43
43
43
43
1/2
1/1
0/1
1/1
0/0
1/2
0/2
0/2
0/2
0/3
5/1
4/3
5/3
4/3
5/2
1/0
2/0
1/0
1/0
0/0
1/2
2/0
0/0
0/0
0/0
2/1
2/1
2/1
2/1
2/1

Table 3: Number instances, algorithm best runtime (#BT) best number
expanded nodes (#BN), WCSPs. 61 instances 12 exact heuristics. table
accounts remaining 49, i-bound= 16 .

2 WCSP instances. seen Table 3, two instances BE+m-BF demonstrated best
runtime among schemes.
m-AOBB-tree. number problems small values m, m-AOBB-tree superior mBB-tree terms runtime number expanded nodes. example, 29.wcsp,
i=4, m=10 m-AOBB-tree requires 2.63 seconds solve problem expands 147851 nodes
runtime m-BB-tree 12.89 seconds expands 2245137 nodes. However,
majority instances m-AOBB-tree slower schemes, seen Figure 9. Moreover,
m-AOBB-tree scales poorly number solutions. = 100 often
worst runtime largest explored search space among schemes, e.g. i=8, 503.wcsp.
striking decrease performance grows consistent across various benchmarks
explained need combine sets partial solutions nodes described
earlier. overhead connected AND/OR decomposition also accounts larger time per
node ratio m-AOBB-tree, compared schemes. example, Table 2 instance
myciel5g 3.wcsp, i=8, m=10 m=100 m-AOBB-tree expands less nodes m-BB-tree,
runtime larger. Nevertheless, m-AOBB-tree benefits. Since space efficient
algorithms, often scheme able find solutions harder instances,
especially heuristic weak, see myciel5g 3.wcsp i=4 satellite01ac.wcsp
i=4 i=8, respectively.
m-BB-tree. Figure 9 see m-BB-tree solves almost number problems
m-AOBB-tree considerably better median time.
m-AOBF-tree m-AOBF-graph. Unsurprisingly, best-first search algorithms often run
space problems feasible branch bound, 503.wcsp myciel5g 3.wcsp i=8.
m-AOBF-based schemes overall inferior algorithms, solving, Figure 9 shows,
least number problems. schemes run memory much often m-A*-tree.
believe due overhead maintaining OPEN list partial solution trees, opposed
OPEN list individual nodes m-A*-tree does. Whenever m-AOBF schemes manage
find solutions, example instance 29.wcsp, i=8, m-AOBF-graph explores smallest
926

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

search space among schemes, except BE+m-BF. time m-AOBF-tree sometimes
expands nodes compared m-AOBF-graph, also m-A*-tree,
would normally expect, since m-A*-tree traverses search space, inherently larger
AND/OR one. However, important remember though better space efficiency
AND/OR schemes often observed, guaranteed. Many factors, tie breaking
nodes value evaluation function, impact performance mAOBF-tree. m-AOBF-tree m-AOBF-graph almost median time number
solved problems, seen Figure 9.
m-A*-tree. three best first algorithms m-A*-tree overall best. Figure 9 see
solves instances schemes values median runtime
close BE+m-BF. Table 3 proves i-bound=16 scheme fastest among
schemes largest number instances, showing best runtime 4-5 instances, depending
m. explained part relatively reduced overhead maintaining search space
OPEN list memory, compared example m-AOBF schemes.
7.2.2 P EDIGREES
Table 4 displays results select instances Pedigree benchmark two i-bounds each.
Overall, difference results algorithms greatly diminishes heuristic
strength increases. Figure 10 shows median time number solved instances select
values i=16. number instances schemes best runtime
best number expanded nodes i-bound presented Table 5.
BE+m-BF. BE+m-BF often superior algorithms, especially
schemes use lower values i-bound, e.g. pedigree23, i=12, ms. large i-bounds thus
accurate heuristics difference much smaller. Moreover, sometimes BE+m-BF
slower schemes, due time required calculate exact heuristic, e.g. pedigree23,
i=16. Table 5 shows BE+m-BF overall fastest. see Pedigree benchmark
algorithm quite successful, evident many instances solved (see Figure 10).
m-AOBB-tree. low values m-AOBB-tree slightly superior algorithms,
solving number instances, (see Figure 10). hand, median time
largest. fails solve instances m=100. Table 4 see m-AOBB-tree
slowest, (e.g., pedigree23, i=16, ms). Yet, instance pedigree33, i=12, m=1, scheme
one find solution.
m-BB-tree. expected, m-BB-tree inferior best-first search schemes unless latter
run memory. case WCSP, scheme often faster m-AOBB-tree,
example, pedigree30, i=16, values m. bar charts show m-BB-tree second
worst median time values m, solves number problems m=100.
m-AOBF schemes. m-AOBF algorithms unsuccessful Pedigree benchmark.
often run memory even = 1 (e.g. pedigree33, i=22). instances
report solution m-AOBF-tree faster m-AOBF-graph, though difference usually
large.
m-A*-tree. saw WCSPs, pedigree instances m-A*-tree faster two
m-AOBF schemes, seen Figure 10, values m. Moreover, superior harder instances
927

fiF LEROVA , ARINESCU , & ECHTER

instance
(n,k,w ,h)

i-bound

pedigree33

12

algorithm

m=1
time

(798, 4, 24, 132)
22

pedigree30

12

(1290, 5, 20, 105)

16

pedigree23

12

(403, 5, 21, 64)

16

pedigree20

12

(438, 5, 20, 65)
16

nodes

number solutions
m=10
time
nodes
OOM
OOM
OOM
Timeout
Timeout
OOM
OOM
OOM
1.55
77138
4.15
177397
Timeout
OOM

m=100
nodes
OOM
OOM
OOM
Timeout
Timeout
OOM
OOM
OOM
3.76
112422
21.48
655141
Timeout
OOM
time

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
7814.77
145203641
OOM
OOM
OOM
1.32
73625
2.98
145717
2.88
70644
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
2510.59
33453995
7.90
6423
OOM
OOM
65.43
4866388
84.28
12243789
594.36
6907399
7.90
6423

OOM
OOM
OOM
Timeout
OOT
7.99
7611
OOM
OOM
65.86
4867551
85.72
12298570
Timeout
7.99
7611

OOM
OOM
OOM
Timeout
OOT
9.22
23028
OOM
OOM
67.01
4882985
127.25
13027245
Timeout
9.22
23028

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
8.44
713664
23.0
4446224
32.11
831096
7.11
630
OOM
OOM
0.52
53862
4.5
837288
13.39
346145
7.11
630

OOM
OOM
715729
4676953
75355901
2482
OOM
OOM
55927
959931
50252107
2482

OOM
OOM
11.15
904802
35.46
6179124
Timeout
7.68
19297
OOM
OOM
1.17
85300
14.14
1641751
OOM
7.68
19297

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

27.0
34.23
88.85
24.95

24.94
27.32
54.46
24.95

OOM
OOM
2321986
7239379
4365855
491
OOM
OOM
2279192
5857301
2531322
491

8.46
24.56
1077.9
7.24

0.58
5.61
713.73
7.24

26.66
37.63
1019.76
24.99

24.93
29.14
970.61
24.99

OOM
OOM
2324701
7434961
63940515
3482
OOM
OOM
2281907
5946423
59333828
3482

OOM
OOM
2353927
9155747
Timeout
26.16
32643
OOM
OOM
25.97
2311133
40.2
6617200
Timeout
26.16
32643
27.47
66.81

Table 4: Pedigrees: CPU time (in seconds) number nodes expanded. Timeout stands
exceeding time limit 3 hours. OOM indicates 4GB memory. bold
highlight best time number nodes m.Parameters: n - number
variables, k - domain size, w - induced width, h - pseudo tree height.

infeasible m-AOBF schemes BE+m-BF, e.g. pedigree23, i=16. shown Figure 10,
solves 5 instances i=16, ms, best second best results, depending
number solutions. However, median time m-A*-tree considerably larger
BE+m-BF, i-bound latter solves single instance less.
7.2.3 B INARY G RIDS
Table 6 shows results select instances grid networks domain. Figure 11 shows
median runtime number solved instances i=18, Table 7 presents number
instances, algorithm best, i-bound. trends algorithms
928

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree

842.17

BE+m-BF
m-AOBF tree

6.66

5.62

5.65

28.13

29.14

40.2

98.29

m-BB tree
m-AOBB tree
5
5
4

5
5
2

4

5
5
5

6
4

5
5

6

0.0

0.0
0.0

100

m-AOBF graph
m-A* tree

5
5
4

10



BE+m-BF
m-AOBF tree

10

100

0.0

5



0.0
0.0

2

0.0
0.0

1

0.0
0.0

10-1

0.0
0.0

100

0.0
0.0

Solved instances
Pedigrees, i=16

0.32
0.0
0.0

5

2

4

1

0.27
0.0
0.0

0.23
0.0
0.0

100

0.23
0.0
0.0

1.39

5.6

101

5.58

27.32

102

27.41

62.91

Median time
Pedigrees, i=16

280.27

103

Figure 10: Median time number solved instances (out 12) select values Pedigrees, i-bound=16. Numbers bars - actual values time (sec) # instances.
Total instances benchmark: 13 , discarded instances due exact heuristic: 1.

behavior observed WCSP Pedigree benchmarks also noticed Grid benchmark
well. particular, m-AOBB-tree successful small, even solving
instances, seen Figure 11. shows worse results m=100 number
solutions largest median time. m-BB-tree smaller median time ms, still
929

fiF LEROVA , ARINESCU , & ECHTER

algorithm

solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

Pedigrees: # inst=13, n=335-1290
k=3-7, w =15-47, hT =52-204, i-bound=16
m=1
m=2
m=5
m=10
m=100
#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN
6
6
6
6
6
0/0
0/0
0/0
0/0
0/0
0/0
0/0
0/0
0/0
0/0
1/1
1/1
1/1
1/1
1/1
0/0
0/0
1/1
1/1
1/1
1/1
1/1
0/0
0/0
0/0
4/4
4/4
4/4
4/4
4/4

Table 5: Number instances, algorithm best runtime (#BT) best number
expanded nodes (#BN), Pedigrees. 13 instances 1 exact heuristics. table
accounts remaining 12, i-bound= 16 .

considerably slower best-first schemes. m-A*-tree presents best compromise
small medium running time relatively large number solved instances. Table 7
shows majority grid instances fastest algorithm. two m-AOBF schemes
results quite similar other, solving almost number instances ms little
difference median runtimes, shown Figure 11. consistently inferior
schemes except BE+m-BF, often runs memory. main difference
Grid benchmark compared previously discussed domains lies behaviour BE+mBF i-bound high. Even though expands less nodes, many problems BE+m-BF
slower schemes due large time required compute exact heuristic.
example, grid 75-19-5, i=18, m=10 runtime BE+m-BF 143.11 seconds, even
m-AOBB-tree, known slow, terminates 94.0 seconds. time, instance
BE+m-BF explores smallest search space values m.
7.2.4 P ROMEDAS
Table 8 shows results Promedas benchmark. Figure 12 presents median time number solved instances benchmark i=16. Table 9 shows i-bound number
instances schemes best runtime best number expanded nodes.
significant fraction instances solved algorithms, especially low
medium i-bounds. Unlike benchmarks, m-AOBB-tree solves instances
small ms, also quite successful m=100, solving one instance less best
scheme value m, m-BB-tree. Moreover, sometimes m-AOBB-tree scheme
report solutions, especially weak heuristic, e.g. chain 50.fg chain 212.fg, i=12.
BE+m-BF runs memory instances, seen Table 8. Overall, variance
algorithms performance significant Promedas previously discussed benchmarks. example, see Figure 12, i=16 m-A*-tree, m-BB-tree m-AOBB-tree solve
25 33 instances [1, 10], BE+m-BF m-AOBF-based schemes
solve 4 8 instances. Table 9 demonstrates m-A*-tree often fastest
algorithms.
930

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

instance
(n,k,w ,h)

50-15-5

i-bound

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
8.83
866865
8.75
1967152
3.29
251502
OOM
OOM
OOM
OOM
OOM
Timeout
347.24
17332742
OOM
OOM

number solutions
m=10
time
nodes
OOM
OOM
11.97
1177549
11.91
2647393
34.28
2485393
OOM
OOM
OOM
OOM
OOM
Timeout
692.59
28676212
OOM
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
3290939
289
OOM
OOM
51205
355700
85289
289

OOM
OOM
OOM
Timeout
368.18
16431707
18.47
1220
OOM
OOM
0.39
55783
1.83
421798
116.77
7505310
18.47
1220

OOM
OOM
OOM
Timeout
Timeout
18.67
9534
OOM
OOM
0.89
104621
4.92
892065
Timeout
18.67
9534

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
347.24
17332742
OOM
OOM
OOM
OOM
1.3
153399
74.83
14968683
46.53
2450725
OOM
OOM

OOM
OOM
OOM
Timeout
692.59
28676212
OOM
OOM
OOM
OOM
1.88
211547
76.93
15403354
118.26
4940247
OOM
OOM

OOM
OOM
OOM
Timeout
2277.92
75442102
OOM
OOM
OOM
OOM
3.53
362344
85.42
16631321
563.22
18306275
OOM
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
3591.1
119431966
143.11
361
OOM
OOM
14.3
1609506
16.27
4005082
39.66
1367955
143.11
361

OOM
OOM
OOM
Timeout
Timeout
143.11
2330
OOM
OOM
18.76
2029844
22.28
5320573
94.0
3480629
143.11
2330

OOM
OOM
OOM
Timeout
Timeout
144.11
16897
OOM
OOM
28.04
2995437
37.26
8191215
Timeout
144.11
16897

algorithm

m=1
time

10

(400, 2, 27, 99)
18

50-17-5
10

(289, 2, 22, 84)
18

90-20-5

10

(400, 2, 27, 99)

18

75-19-5

10

(361, 2, 25, 89)
18

82.95
18.45

0.35
1.39
1.79
18.45

nodes

m=100
time

nodes
OOM
OOM
20.22
1931039
22.06
4708311
Timeout
OOM
OOM
OOM
OOM
OOM
Timeout
2277.92
75442102
OOM
OOM

Table 6: Grids: CPU time (in seconds) number nodes expanded. Timeout stands
exceeding time limit 3 hours. OOM indicates 4GB memory. bold
highlight best time number nodes m. Parameters: n - number
variables, k - domain size, w - induced width, h - pseudo tree height.

7.2.5 P ROTEIN
Table 10 shows select Protein instances i=4 i=8, respectively. Figure 13 Table 11 show
summary results i=4. benchmark fairly difficult due large domain size
(up 81). heuristic calculation feasible higher i-bounds. particular, BE+m-BF
considerable problems calculating exact heuristic. Even low i-bounds relatively easy
instances solved. Note instances pdb1ctk pdb1dlw i-bound=8 yields exact heuristic.
m-AOBF-tree m-AOBF-graph fail find solutions within memory limit
majority instances, e.g., pdb1b2v pdb1cxy, i=4. much difference
runtimes algorithms, exception m-AOBB-tree. example, pdb1b2v, i=8,
931

fiF LEROVA , ARINESCU , & ECHTER

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree
440.64

BE+m-BF
m-AOBF tree

37.26
0.16

10



100

m-BB tree
m-AOBB tree
13
15

13
15
18

13
15

13
15

19

m-AOBF graph
m-A* tree

19

BE+m-BF
m-AOBF tree

0.39

1.04
1.23
0.02

0.38

0.9
1.3
0.01

5

2

2.14
3.23
1.0

22.28

20.16
45.35

17.49
28.04
0.36

1

13
15
18

10-1

0.0

0.35

0.01

100

0.83
0.78

101

1.43
0.88

16.27
20.68

Median time
Grids, i=18

102

91.79

103

6
5
3
3

4

5
5

5
5
4

5

6
4

5

5

Solved instances

Grids, i=18

6

101

100

1

2

5



10

100

Figure 11: Median time number solved instances (out 31) select values
Grids, i-bound=18. Numbers bars - actual values time (sec) # instances.
Total instances benchmark: 32, discarded instances due exact heuristic: 1.

m-AOBB-tree requires 6.46 seconds find m=10 solutions, runtimes algorithms
range 0.03 0.09 seconds (except BE+m-BF runs memory). However,
slow performance m-AOBB-tree easier problems feasible algorithms
compensated fact many instances scheme report solution, solving
932

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

algorithm

solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

Grids: # inst=32, n=144-2500
k=2-2, w =15-74, hT =48-312, i-bound=18
m=1
m=2
m=5
m=10
m=100
#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN
12
12
13
13
14
0/0
0/0
0/0
0/0
0/0
0/0
0/1
0/2
0/1
0/3
8/2
8/2
8/6
9/6
7/6
1/0
1/0
0/0
1/0
2/2
7 / 10
7 / 10
5/5
5/5
2/2
5/7
5/6
6/5
6/6
6/4

Table 7: Number instances, algorithm best runtime (#BT) best number
expanded nodes (#BN), Grids. 32 instances 1 exact heuristics. table
accounts remaining 31, i-bound= 18 .

instances considerable amount [1, 10] (Figure 13). Table 11 shows m-AOBBtree best terms time space overwhelming majority problems
values except = 100.
7.2.6 EGMENTATION
Table 12 shows results select instances Segmentation benchmark two i-bounds,
namely i=4 i=12, Figure 14 Table 13 present summary results i=12.
Unlike WCSP, benchmark chose display relatively low i-bounds calculating heuristic larger infeasible, problems low induced width
wished avoid displaying results obtained exact heuristics. main peculiarity
benchmark striking success BE+m-BF. Overall solves many instances usually
superior m-A*-tree m-BB-tree, seen Figure 14. Moreover, runtime superior
schemes, true instances Table 12 also illustrated results
Table 13. heuristic weak, m-AOBB-tree fairly successful, example, finding
solutions values 12 4 s.binary, i=4, infeasible scheme except
BE+m-BF. However, usual, m-AOBB-tree overall slowest schemes.
7.3 Best-first vs Depth-First Branch Bound Best Solutions
Let us consider data presented Tables 2-13 Figures 9-14 order summarize
observations contrast performance best-first depth-first branch bound schemes.
Among best-first search schemes m-A*-tree successful. often effective,
armed good heuristic, requires less space best-first schemes.
already noted, BE+m-BF shows good results Segmentation benchmark, best
algorithm terms median runtime, solving least number problems
schemes. However, benchmarks calculation exact heuristic often
infeasible.
933

fiF LEROVA , ARINESCU , & ECHTER

instance
(n,k,w ,h)

i-bound

16

(620, 2, 30, 64)

22

(676, 2, 30, 70)

chain 212.fg

12

16

12

(773, 2, 33, 79)
22

chain 50.fg

m=1
time

chain 107.fg

chain 141.fg

algorithm

12

661, 2, 36, 76)
22

nodes

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
7.89
919865
14.58
3139711
67.95
1398364
OOM
OOM
OOM
9.2
1093564
17.0
3861414
122.01
3214924
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
9553.91
1276222668
272.0
9878480
OOM
OOM
OOM
14.16
1261489
279.61
56821714
140.9
6490042
OOM

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

number solutions
m=10
time
nodes
OOM
OOM
18.89
2108122
35.15
7051974
229.49
5134280
OOM
OOM
OOM
20.83
2465364
42.36
9205755
418.46
11123810
OOM

m=100
time

44.61
102.6
627.94

56.59
100.45
855.84

nodes
OOM
OOM
4641627
18494630
13594667
OOM
OOM
OOM
6356871
19217427
21388619
OOM

OOM
OOM
OOM
OOT
25481595
OOM
OOM
OOM
3379926
87947802
14103095
OOM

OOM
OOM
OOM
OOT
2091.26
64400241
OOM
OOM
OOM
OOM
885.72
160581726
909.48
33842266
OOM

OOM
OOM
OOM
Timeout
49808550
OOM
OOM
OOM
1118792
15922806
11336657
OOM

OOM
OOM
OOM
Timeout
4206.07
111853485
OOM
OOM
OOM
33.87
3669711
141.66
27615033
1239.88
24717964
OOM

OOM
OOM
OOM
Timeout
Timeout
OOM
OOM
OOM
78.37
8186757
342.51
58246101
5032.11
86444575
OOM

OOM
OOM
OOM
Timeout
1404.27
33495406
OOM
OOM
OOM
53.87
5673948
91.15
18515503
Timeout
OOM

OOM
OOM
OOM
Timeout
3748.85
93992107
OOM
OOM
OOM
OOM
176.14
34915510
Timeout
OOM

OOM
OOM
OOM
Timeout
10070.0
245628104
OOM
OOM
OOM
OOM
447.46
85945673
Timeout
OOM

1772.8

9.91
78.08
584.83

721.67

38.48
460.15
315.2

Table 8: Promedas: CPU time (in seconds) number nodes expanded. Timeout stands
exceeding time limit 3 hours. OOM indicates 4GB memory. bold
highlight best time number nodes m. Parameters: n - number
variables, k - domain size, w - induced width, h - pseudo tree height.

two m-AOBF-based schemes overall inferior due prohibitively large memory, solving
fewer instances algorithms. believe non-trivial extension AOBF
single solution m-best task straightforward, hard represent multiple
partial solution trees efficient manner. order efficient m-AOBF implementation,
one needs quickly identify partial solution subtree select extend next, searching (k + 1)th solution finding k th best solution. AOBF (for 1 solution) uses
arc-marking mechanism efficiently represent current best partial solution subtree
search, easy extend case searching best solutions. Therefore,
shown Section 5.1, m-AOBF implements naive mechanism par934

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

BE+m-BF
m-AOBF tree

105

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree

10



100

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree
27
26
8

8

8

8

2

3

4
4

5

6

6

7

7
7

Solved instances

Promedas, i=16

101

8

17

26
27
30

28
27
32

28
27
33

BE+m-BF
m-AOBF tree

1.36
2.97
4.45
3.62

60.1
7.47
0.15
1.55
2.0

3.43
1.95
7.08
0.08

5

2

185.53

325.85

267.69
51.79

146.85
0.03

1

25
27
31

10-1

2.03
1.37

5.19
0.01
0.48
0.62

101

100

7.64

33.68

102

45.83

109.68

Median time
Promedas, i=16

103

1268.88

104

100

1

2

5



10

100

Figure 12: Median time number solved instances (out 86) summary select values
Promedas, i-bound=16. Numbers bars - actual values time (sec) #
instances. Total instances benchmark: 75, discarded instances due exact heuristic:
11.

tial solution trees represented explicitly memory. simple representation, however, incurs
considerable computational overhead searching best solutions, indeed
revealed experiments. efficient implementation m-AOBF left future work.
935

fiF LEROVA , ARINESCU , & ECHTER

algorithm

solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

Promedas: # inst=86, n=197-2113
k=2-2, w =5-120, hT =34-187, i-bound=16
m=1
m=2
m=5
m=10
m=100
#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN
42
42
45
44
46
0/0
0/0
0/0
0/0
0/0
0/1
0/2
0/2
0/3
0/2
22 / 17
21 / 17
18 / 17
18 / 15
9/9
1/0
1/0
1/0
2/0
10 / 5
4/8
4/8
3/5
4/8
2/7
8/7
8/6
8/6
8/5
8/6

Table 9: Number instances, algorithm best runtime (#BT) best number
expanded nodes (#BN), Promedas. 86 instances 11 exact heuristics.
table accounts remaining 75, i-bound= 16 .

Unsurprisingly, branch bound algorithms robust terms memory also
dominate m-A*-tree best-first schemes many benchmarks terms number
instances solved. However, tend considerably larger median time expand
nodes. particular, m-AOBB-tree scale well number solutions large
values runtime increases drastically. Unlike m-AOBF, whose inferior performance
attributed specifics implementation, depth-first m-AOBB suffers issues inherent
solving m-best problem depth-first manner. Algorithm 10 describes, m-AOBB needs
merge best partial solution internal node, hurts performance significantly,
cannot avoided, unless algorithmic approach fundamentally changed.
see way overcome limitation.
Overall, whenever calculation exact heuristic feasible, BE+m-BF
algorithm choice. Otherwise, m-A*-tree superior relatively easy problems, mAOBB-tree best scheme hard memory intensive instances. superiority best-first
approach, whenever memory available, expected, based, one hand, intuition derived
knowledge task finding single solution, hand, theoretical
results Section 3.2.
7.4 Scalability Algorithms Number Required Solutions
Figures 15-17 present plots showing runtime seconds number expanded nodes
function number solutions (on log scale) two instances benchmark.
Figure 15 displays results WCSP Pedigree benchmarks, Figure 16 - Grids Promedas,
Figure 17 - Proteins Segmentation. Lower values (on y-axis) preferable. row
contains two instances benchmarks specific value i-bound, runtime plots
shown ones containing expanded nodes. examples chosen best
illustrate prevailing tendencies.
Note theoretical analysis suggests runtime BE+m-BF, best among

algorithms, scale since worst case complexity O(nk w + mn). theoretical
complexity best-first search schemes m-AOBF-tree m-A*-tree linear number
936

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

instance
(n,k,w ,h)

pdb1b2v

(133, 36, 13, 33)

pdb1cxy

(70, 81, 9, 19)

i-bound

m=1
time

4

8

4

8

pdb1ctj

4

(62, 81, 8, 21)

8

pdb1dlw

4

(84, 81, 8, 29)

algorithm

8

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

nodes
OOM
OOM

0.12
10.23
6.51
0.02
0.03
0.0
0.01
0.29

2508
948337
100584
OOM
95
95
139
2401
6563
OOM

number solutions
m=10
nodes
OOM
OOM
0.17
3186
11.09
1034645
35.14
827365
OOM
0.06
294
0.09
108
0.03
597
0.07
8861
6.46
256588
OOM
time

OOM
OOM
0.38
0.4

3708
51020
OOM
OOM
OOM
0.01
121
0.03
5791
0.66
7029
OOM

10.43
844.08
5.64
0.01
0.0
0.02
0.01
0.01
0.22
0.01

46.17
47.27
187.38
0.01

0.14
1.06
18.53
0.01

OOM
OOM
35400
76609260
74833
62
49
45
62
1118
3098
62
OOM
OOM
579108
6380302
1451906
294
OOM
OOM
6900
162913
154850
294

m=100
time

nodes
OOM
OOM

0.34
14.4
4462.0
0.42
0.64
0.15
0.5

6249
1404370
230849005
OOM
1956
135
3051
67330
Timeout
OOM

OOM
OOM
0.48
0.6

OOM
OOM
4434
73849

OOM

10854
191203
OOM
OOM
OOM
0.1
2870
0.27
53702
44.28
1335157
OOM

OOM
OOM

OOM
OOM

OOM
OOM
OOM
0.04
0.07
2.04

13.23
1039.96
18.29
0.02
0.07
0.11
0.03
0.03
2.32
0.02

46.26
47.33
544.55
0.05

0.18
1.09
157.01
0.05

480
11429
34567

43538
94422614
306054
265
302
74
265
5385
54324
265
OOM
OOM
579405
6391107
12759004
635
OOM
OOM
7240
167037
8632114
635

0.94
1.45

19.74
1325.84
157.43
0.07
0.42
0.75
0.08
0.15
71.86
0.07

46.49
50.72
0.39

0.52
1.86
0.39

65340
120786833
5307198
1050
1825
95
1057
31066
3273324
1050
OOM
OOM
582375
6762911
OOT
4265
OOM
OOM
10855
280189
OOT
4265

Table 10: Protein: CPU time (in seconds) number nodes expanded. Timeout stands
exceeding time limit 3 hours. OOM indicates 4GB memory. bold
highlight best time number nodes m. Parameters: n - number
variables, k - domain size, w - induced width, h - pseudo tree height.

solutions, m-BB-tree overhead due m-best task factor (m log m)
m-AOBB-tree (m log deg), deg degree pseudo tree. observed
compared schemes runtime BE+m-BF indeed rises quite slowly number
solutions increases, even reaches 100. runtime m-A*-tree also scales well m.
behaviour m-BB-tree depends lot benchmarks. Pedigrees Protein runtime
changes little instances number solutions grows, benchmarks,
runtime m=100 tends significantly larger m=1. m-AOBF-tree m-AOBFgraph often provide solutions even m=1 or, alternatively, run memory
slightly increases (m [2, 10]). algorithms clearly successful practice.
937

fiF LEROVA , ARINESCU , & ECHTER

BE+m-BF
m-AOBF tree

105

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree

1457.61

45.28

41.91

190.28
39.45

177.3
37.46

102

220.64

103

50.72

597.38

Median time
Protein, i=4

104

5

2

10



100

10
10

13
13

13
13

13
13

13
13

Solved instances
Protein, i=4

20

25

35

42
35
25

35

42

m-BB tree
m-AOBB tree

25

35
25

25

35

44

m-AOBF graph
m-A* tree

44

BE+m-BF
m-AOBF tree

0.14
0.38
0.67

2.95

2.89

0.03
0.19
0.39

1

0.01
0.17
0.32

2.87
0.01
0.14
0.27

2.84
0.01
0.13
0.25

100

3.39

101

100

1

2

5



6

6

6

6

6

101

10

100

Figure 13: Median time number solved instances (out 72) select values
Protein, i-bound=4. Numbers bars - actual values time (sec) # instances.
Total instances benchmark: 72, discarded instances due exact heuristic: 0.

discussed before, runtime number expanded nodes m-AOBB-tree increase
drastically gets larger.
938

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

algorithm

solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

m=1
#BT / #BN
26
8/3
1 / 10
11 / 9
8/1
21 / 22
6/4

Protein: # inst=72, n=15-242
k=18-81, w =5-16, hT =7-44, i-bound=4
m=2
m=5
m=10
#BT / #BN #BT / #BN #BT / #BN
26
27
27
7/1
7/1
7/0
2 / 12
2 / 12
1 / 13
9/9
12 / 9
14 / 10
9/2
8/3
6/3
21 / 21
18 / 19
17 / 17
6/2
5/2
5/2

m=100
#BT / #BN
34
5/0
1 / 10
17 / 13
11 / 10
3/3
6/2

Table 11: Number instances, algorithm best runtime (#BT) best
number expanded nodes (#BN), Protein. 72 instances 0 exact heuristics.
table accounts remaining 72, i-bound= 4 .

7.5 Comparison Competing Algorithms
compare methods number previously developed schemes described details
Section 6: STRIPES, PESTEELARS Nilssons algorithm. implementations
schemes provided Dhruv Batra. first two approaches based ideas LP relaxations approximate, known often find exact solutions, though provide
guarantees optimality. Nilssons algorithm exact message-passing scheme operating
junction tree. first set experiments (on tree benchmark) also show results
STILARS algorithm, older version PESTEELARS algorithm. However, scheme
consistently inferior two LP-based schemes considered two
benchmarks. following, collectively refer 4 algorithms competing schemes.
7.5.1 R ANDOMLY G ENERATED B ENCHMARKS
available us code LP-based Nilssons approaches developed run restricted inputs only, could applied benchmarks used bulk evaluation described above. concluded re-implementing competing codes work general
input would time consuming would provide additional insights. Thus chose
compare algorithms competitors using benchmarks acceptable
competing schemes.
Specifically, comparison performed following three benchmarks: random trees,
random binary grids random graphs submodular potentials, call submodular
graphs remainder section. Table 14 shows parameters benchmarks.
instances generated following manner. First, vector 12 logarithmically spaced integers 10 103.5 generated, serving number variables instances.
binary grids benchmarks value used generate two problems number
variables. edges variables generated uniformly randomly, making
sure end graph tree, grid loopy graph, depending benchmark. edge
define binary potential vertex unary potential exponential form: f = e ,
939

fiF LEROVA , ARINESCU , & ECHTER

instance
(n,k,w ,h)

i-bound

4

(225, 2, 16, 48)

12

(227, 2, 16, 57)

7 9 s.binary

(234, 2, 16, 53)

OOM
OOM
OOM
Timeout
164.91
5653312
0.0
225
7.31
103327
10.47
1843
0.03
3754
0.04
8251
0.08
4158
0.0
225

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
71.71
2733703
0.01
227
0.23
3338
0.33
799
0.01
585
0.05
10687
0.21
11076
0.01
227

OOM
OOM
OOM
Timeout
360.14
14906212
0.02
1365
3.75
46121
5.72
1827
0.09
9103
0.19
30119
14.28
1054628
0.02
1365

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
127.17
3337949
0.01
234
8.85
122663
OOM
0.02
1978
0.03
4415
0.05
2750
0.01
234

OOM
OOM
OOM
Timeout
505.08
17976200
0.03
1337
OOM
OOM
0.06
4170
0.11
13357
10.54
806490
0.03
1337

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

OOM
OOM
OOM
Timeout
110.19
4227437
0.01
231
OOM
OOM
1.02
102671
2.07
428791
0.75
39170
0.01
231

OOM
OOM
OOM
Timeout
555.6
23302165
0.03
1615
OOM
OOM
1.17
115407
2.9
527967
11.99
809403
0.03
1615

m=1
time

12 4 s.binary

16 16 s.binary

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

number solutions
m=10
nodes
OOM
OOM
OOM
Timeout
505.82
18888321
0.02
1619
10.36
143333
OOM
0.06
5692
0.21
24349
1.62
118074
0.02
1619

algorithm

4

12

4

12

11 4 s.binary

4

(231, 2, 16, 57)

12

nodes

time

m=100
time

nodes
OOM
OOM
OOM
Timeout
4371.05
189179726
0.21
11194
OOM
OOM
0.3
18616
1.32
131571
489.57
40961080
0.21
11194

0.19

0.38
1.2
0.19

0.21

0.28
0.95
0.21

OOM
OOM
OOM
Timeout
OOT
11157
OOM
OOM
30542
141591
OOT
11157
OOM
OOM
OOM
Timeout
OOT
10212
OOM
OOM
15807
89675
OOT
10212

OOM
OOM
OOM
Timeout
OOT
0.28
14241
OOM
OOM
1.86
167983
7.1
1010155
8497.93
617227854
0.28
14241

Table 12: Segmentation: CPU time (in seconds) number nodes expanded. Timeout
stands exceeding time limit 3 hours. OOM indicates 4GB memory.
bold highlight best time number nodes m. Parameters: n - number
variables, k - domain size, w - induced width, h-pseudo tree height.

real number sampled uniform distribution. third benchmark potentials modified submodular. random trees m-best optimization LP
problem guaranteed tight, graphs submodular potentials LP optimization
problem tight, m-best extension not, arbitrary loopy graphs, including grids,
algorithms provide guarantees.
7.5.2 C OMPETING LGORITHMS P ERFORMANCE
Table 15 shows runtimes select instances random tree benchmark 5 mbest search schemes competing LP schemes STILARS, PESTEELARS STRIPES.
940

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

BE+m-BF
m-AOBF tree

m-AOBF graph
m-A* tree

m-BB tree
m-AOBB tree

105

1949.3

103

5

100

13

14
14

18

19

20

24

24
24
24

m-BB tree
m-AOBB tree

24

24
24
24

24
20

24

24
24
24

m-AOBF graph
m-A* tree

20
20

21
20

Solved instances
Segmentation, i=12

24

24
24
24

BE+m-BF
m-AOBF tree

101

10



24
24

2

0.02
0.31
0.4
0.03
0.08

1

10-1

0.01
0.22
0.26
0.02
0.04
0.7

0.01
0.15
0.2
0.01
0.03
0.04

100

3.94

101

0.17
1.1
1.55
0.17
0.65

102

0.0
0.08
0.1
0.01
0.01
0.03

Median time
Segmentation, i=12

104

1

2

5



10

100

Figure 14: Median time number solved instances (out 47) select values
Segmentation, i-bound=12. Numbers bars - actual values time (sec) #
instances. Total instances benchmark: 47, discarded due exact heuristic: 0.

observed benchmarks STILARS always inferior two schemes
therefore excluded remainder evaluation. Instead, Tables 16 17,
show results random binary grids submodular graphs benchmarks, added
comparison Nilssons max-flow algorithm. Table 15-17 time limit set 1 hour,
memory limit 3 GB. schemes behavior quite consistent across instances.
941

fiF LEROVA , ARINESCU , & ECHTER

Time vs m. WCSPs: bwt3ac.wcsp

Time vs m. WCSPs: queen5_5_3.wcsp

(45, 11, 16, 27), i=4

(25, 3, 18, 21), i=16

7

15

6

Time, sec

Time, sec

5
10

5

4
3
2
1

0

0

1.0

10.0

Number solutions

m-AOBF-tree
m-AOBF-graph

2.0

1e6

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

10.0

m-AOBF-tree
m-AOBF-graph

Nodes vs m. WCSPs: bwt3ac.wcsp
(45, 11, 16, 27), i=4
1.4

1e6

100.0

Number solutions
m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

Nodes vs m. WCSPs: queen5_5_3.wcsp
(25, 3, 18, 21), i=16

1.2
1.5

1.0

Nodes

Nodes

0.8

1.0

0.6

0.5

0.4

0.0

0.0

0.2

1.0

10.0

Number solutions

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

m-AOBB-tree
BE+m-BF

Time vs m. Pedigrees: pedigree9

Time vs m. Pedigrees: pedigree30

(1290, 5, 20, 105), i=16

(1119, 7, 25, 123), i=22

1400

2000

1200

1500

Time, sec

1000

Time, sec

10.0

Number solutions

800

1000

600
400
200
0

500
0

1.0

10.0

Number solutions

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

m-AOBF-tree
m-AOBF-graph

Nodes vs m. Pedigrees: pedigree30
1e7

10.0

Number solutions
m-A*-tree
m-BB-tree

100.0

m-AOBB-tree
BE+m-BF

Nodes vs m. Pedigrees: pedigree9

(1290, 5, 20, 105), i=16

5

3

(1119, 7, 25, 123), i=22

Nodes

4

1.5

Nodes

2.0

1e7

1.0

2

0.5

1

0.0

0
1.0

10.0

Number solutions

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

10.0

Number solutions

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

m-AOBB-tree
BE+m-BF

Figure 15: CPU time seconds number expanded nodes function number solutions. WCSP Pedigrees, 4 GB, 3 hours.

942

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

Time vs m. Grids: 50-19-5

Time vs m. Grids: 75-18-5

(361, 2, 25, 93), i=18

(324, 2, 24, 85), i=18

40

3500
3000
2500
2000
1500
1000
500
0

Time, sec

Time, sec

30
20
10
0
1.0

10.0

100.0

Number solutions

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

m-AOBF-tree
m-AOBF-graph

Nodes vs m. Grids: 50-19-5
6

7
6
5
4
3
2
1
0

m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

(324, 2, 24, 85), i=18

1e5

5

Nodes

Nodes

4
3
2
1
0

1.0

10.0

100.0

Number solutions

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

10.0

100.0

Number solutions

m-AOBF-tree
m-AOBF-graph

Time vs m. Promedas: or_chain_17.fg

m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

Time vs m. Promedas: or_chain_212.fg
(773, 2, 33, 79), i=22

(531, 2, 19, 51), i=16

5000

10

4000

8

Time, sec

Time, sec

100.0

Nodes vs m. Grids: 75-18-5

(361, 2, 25, 93), i=18

8 1e8

10.0

Number solutions

3000

6

2000

4

1000

2
0

0
1.0

10.0

m-AOBF-tree
m-AOBF-graph

1e5

100.0

Number solutions
m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

10.0

m-AOBF-tree
m-AOBF-graph

Nodes vs m. Promedas: or_chain_17.fg
(531, 2, 19, 51), i=16

1e8

0.6

m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

Nodes vs m. Promedas: or_chain_212.fg
(773, 2, 33, 79), i=22

Nodes

0.8

3

Nodes

4

100.0

Number solutions

0.4

2
1

0.2

0

0.0
1.0

10.0

Number solutions

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

10.0

Number solutions

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

m-AOBB-tree
BE+m-BF

Figure 16: CPU time seconds number expanded nodes function number solutions. Grids Promedas, 4 GB, 3 hours.

943

fiF LEROVA , ARINESCU , & ECHTER

Time vs m. Protein: pdb1at0

Time vs m. Protein: pdb1b2v

(122, 81, 8, 25), i=4

(133, 36, 13, 33), i=8

800
700
600
500
400
300
200
100
0

7
6

Time, sec

Time, sec

5
4
3
2
1
0
1.0

10.0

100.0

Number solutions

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

m-AOBF-tree
m-AOBF-graph

Nodes vs m. Protein: pdb1at0

m-AOBB-tree
BE+m-BF

2.5
2.0

0.6

Nodes

Nodes

1.5

0.4

1.0

0.2

0.5

0.0

0.0
1.0

10.0

100.0

Number solutions

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

10.0

100.0

Number solutions

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

Time vs m. Segmentation: 10_16_s.binary

Time vs m. Segmentation: 7_29_s.binary

(230, 2, 15, 52), i=4

(234, 2, 15, 63), i=12

10000

8

8000

6

Time, sec

Time, sec

m-A*-tree
m-BB-tree

(133, 36, 13, 33), i=8

1e5

0.8

6000

4

4000

2

2000

0

0

1.0

10.0

100.0

Number solutions

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

1.0

m-AOBB-tree
BE+m-BF

10.0

100.0

Number solutions

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

m-AOBB-tree
BE+m-BF

Time vs m. Segmentation: 10_16_s.binary

Time vs m. Segmentation: 7_29_s.binary

(230, 2, 15, 52), i=4

(234, 2, 15, 63), i=12

10000

8

8000

6

Time, sec

Time, sec

100.0

Nodes vs m. Protein: pdb1b2v

(122, 81, 8, 25), i=4

1e7

10.0

Number solutions

6000

4

4000
2000

2

0

0
1.0

10.0

Number solutions

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

1.0

m-AOBB-tree
BE+m-BF

10.0

Number solutions

m-AOBF-tree
m-AOBF-graph

m-A*-tree
m-BB-tree

100.0

m-AOBB-tree
BE+m-BF

Figure 17: CPU time seconds number expanded nodes function number solutions. Protein Segmentation, 4 GB, 3 hours.

944

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

algorithm

solved
m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
BE+m-BF

Segmentation: # inst=47, n=222-234
k=2-21, w =15-18, hT =47-67, i-bound=12
m=1
m=2
m=5
m=10
m=100
#BT / #BN #BT / #BN #BT / #BN #BT / #BN #BT / #BN
23
23
23
23
23
0/5
0/0
0/0
0/0
0/0
0/5
0/7
0 / 11
0 / 15
0 / 14
15 / 0
11 / 0
12 / 0
11 / 0
3/0
7/0
5/0
3/0
2/0
0/0
0/0
3/0
0/0
0/0
0/0
21 / 19
20 / 17
22 / 13
24 / 9
24 / 10

Table 13: Number instances, algorithm best runtime (#BT) best number expanded nodes (#BN), Segmentation. 47 instances 0 exact heuristics.
table accounts remaining 47, i-bound= 12 .

Benchmark
Random trees
Random Binary Grids
Random submodular graphs

# inst
12
24
12

n
10-5994
16-3192
16-3192

k
2-4
2
2

w
1
6-79
4-74

hT
5-132
9-221
9-208

Table 14: Benchmark parameters: # inst - number instances, n - number variables, k - domain
size, w - induced width, hT - pseudo tree height.

STILARS Nilssons schemes always dominated two competing schemes
terms runtime. STRIPES PESTEELARS sometimes faster schemes
m=1, e.g. tree nnodes880 ps1 k4, however, three benchmark scale rather poorly
m. 5 almost always inferior algorithms, provided latter report
results, occasional exception m-AOBB-tree, also tends slow large m.
problems PESTEELARS STRIPES superior search schemes
largest networks 1000 variables, grid nnodes3192 ps2 k2,
infeasible algorithms. Overall, five m-best algorithms proved superiority
considered competing schemes majority instances, often better runtime, especially
> 2, guaranteeing solution optimality.

8. Conclusion
work finding best solutions graphical models focused either iterative
schemes based Lawlers idea dynamic programming (e.g., variable-elimination treeclustering). showed first time combinatorial optimization defined graphical
models traditional heuristic search paradigms directly applicable, often superior.
Specifically, extended best-first depth-first branch bound search algorithms solve
m-best optimization task, presenting m-A* m-BB, respectively. showed properties A* extend m-A* algorithm and, particular, proved m-A* superior
945

fiF LEROVA , ARINESCU , & ECHTER

instance

tree nnodes245 ps1 k2

(245, 2, 2, 32)

tree nnodes880 ps1 k4

(880, 4, 2, 52)

tree nnodes5994 ps1 k4

(5994, 4, 2, 189)

algorithm

i-bound=4. k=2
m=5
m=10
time
time
0.05
0.09
0.08
0.13
0.01
0.02
0.02
0.03
0.06
14.14
7.93
33.3
0.4
0.88
0.51
1.32

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
STILARS
STRIPES
PESTEELARS

m=1
time
0.02
0.02
0.0
0.0
0.02
0.0
0.09
0.0

m=2
time
0.02
0.03
0.0
0.0
0.02
0.04
0.17
0.13

m=100
time
0.61
0.95
0.12
0.37
3045.25
1757.41
13.88
47.32

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
STILARS
STRIPES
PESTEELARS

0.3
0.48
0.08
0.1
1.17
0.0
5.67
0.0

0.46
0.76
0.17
0.3
1.37
0.11
11.26
0.87

1.06
1.8
0.24
0.54
52.36
28.19
28.09
6.13

1.9
3.28
0.48
1.23
927.12
81.21
56.41
9.26

OOM
OOM
3.67
14.38
Timeout
2440.22
607.01
79.0

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
STILARS
STRIPES
PESTEELARS

OOM
OOM
5.44
5.77
851.48
0.05
248.53
0.05

OOM
OOM
10.68
29.04
922.19
2.72
506.25
18.28

OOM
OOM
18.31
36.49
Timeout
64.48
1279.87
91.17

OOM
OOM
37.21
97.73
Timeout
250.36
2576.87
169.39

OOM
OOM
206.26
1112.2
Timeout
7325.4
Timeout
Timeout

Table 15: Random trees, i-bound=4. Timeout - time, OOM - memory. 3 GB, 1 hour.
search scheme m-best task. also analyzed overhead algorithms caused
need find multiple solutions. introduced BE+m-BF, hybrid variable elimination
best-first search scheme showed best worst-case time complexity among
m-best algorithms graphical models known us.
evaluated schemes empirically. observed AND/OR decomposition
search space, significantly boosts performance traditional heuristic search schemes,
cost-effective m-best search algorithms, least current implementation.
expected, best-first schemes dominate branch bound algorithms whenever sufficient
space available, fail memory-intensive problems. compared schemes 4
previously developed algorithms: three approximate schemes based LP-relaxation problem algorithm performing message passing junction tree. showed schemes
often dominate competing schemes, known efficient, terms runtime, especially
required number solutions large. Moreover, scheme guarantee solution optimality.

Acknowledgement
work sponsored part NSF grants IIS-1065618 IIS-1254071, United
States Air Force Contract No. FA8750-14-C-0011 DARPA PPAML program.

946

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

instance

grid nnodes380 ps1 k2

(380, 2, 25, 379)

grid nnodes380 ps2 k2

(380, 2, 25, 61)

grid nnodes3192 ps2 k2

(3192, 2, 75, 217)

algorithm

m=2
time
Random binary grid

i-bound=20
m=5
m=10
time
time

m=25
time

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

OOM
OOM
0.49
0.56
55.93
112.5
5.06
4.63

OOM
OOM
0.53
0.64
106.34
772.49
46.57
13.4

OOM
OOM
0.58
0.71
202.65
1860.46
172.95
28.95

OOM
OOM
0.67
0.91
2027.66
5026.68
361.04
75.28

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

OOM
OOM
0.2
0.32
7.62
110.4
2.23
3.98

OOM
OOM
0.23
0.36
12.82
757.14
19.41
11.5

OOM
OOM
0.26
0.58
67.59
1820.45
38.54
24.34

OOM
OOM
0.36
0.95
1964.18
4985.0
Timeout
Timeout

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

OOM
OOM
OOM
Timeout
Timeout
OOM
123.45
26.86

OOM
OOM
OOM
Timeout
Timeout
OOM
658.05
81.27

OOM
OOM
OOM
Timeout
Timeout
OOM
3035.29
172.35

OOM
OOM
OOM
Timeout
Timeout
OOM
Timeout
Timeout

Table 16: Random binary grids, i-bound=20. Timeout - time, OOM - memory. 3 GB,
1 hour.

947

fiF LEROVA , ARINESCU , & ECHTER

instance

gen nnodes132 ps1 k2

(132, 2, 13, 34)

gen nnodes380 ps1 k2

(380, 2, 25, 61)

gen nnodes1122 ps1 k2

(1122, 2, 43, 112)

algorithm

i-bound=20
m=5
m=10
time
time
0.02
0.03
0.03
0.06
0.01
0.02
0.0
0.03
0.09
5.44
60.81
144.93
1.32
3.13
8.52
18.56

m=25
time
0.05
0.09
0.02
0.05
120.67
394.26
13.24
48.76

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

m=2
time
0.01
0.01
0.0
0.0
0.03
9.34
0.5
2.9

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

OOM
OOM
0.47
0.54
51.77
105.58
2.07
4.38

OOM
OOM
0.51
0.61
110.96
728.0
6.2
14.09

OOM
OOM
0.57
0.73
141.68
1753.98
13.21
29.96

OOM
OOM
0.72
1.03
2027.05
4817.09
76.0
75.04

m-AOBF tree
m-AOBF graph
m-A* tree
m-BB tree
m-AOBB tree
Nilsson
STRIPES
PESTEELARS

OOM
OOM
OOM
Timeout
Timeout
OOM
16.46
9.69

OOM
OOM
OOM
Timeout
Timeout
OOM
57.96
28.84

OOM
OOM
OOM
Timeout
Timeout
OOM
107.73
61.04

OOM
OOM
OOM
Timeout
Timeout
OOM
282.4
158.7

Table 17: Random loopy graphs submodular potentials, i-bound=20. Timeout - time,
OOM - memory. 3 GB, 1 hour.

948

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

Figure 18: Example AND/OR search tree 3 layers nodes.

Appendix A. Proof Theorem 13
Let ST AND/OR search tree relative pseudo tree depth h, n number
variables, k maximum domain size, deg maximum degree nodes .
Define partial solution subtree 0 subtree ST that: (1) 0 contains root
ST ; (2) non-terminal node n 0 , 0 contains exactly one child node n0
n; (3) non-terminal node n 0 0 contains child nodes n01 , . . . , n0j n;
(4) leaf tip node 0 doesnt successors 0 .
nodes ST grouped layers. h layers ith layer, denoted
Li , 1 h, contains nodes whose variables depth , together
children. assume root depth 1. illustration, Figure 18 depicts
AND/OR search tree 3 layers, example L1 = {A, hA, 0i, hA, 1i}.
denote TiOR set partial solution subtrees whose leaf nodes nodes Li .
Similarly, TiAN set partial solution subtrees whose leaf nodes nodes Li .
partial solution subtree 0 T2OR whose leaf nodes nodes belonging 2nd layer
highlighted Figure 18, namely 0 = {A, hA, 0i, B, C}.
L EMMA 1. Given 0 TiOR 00 TiAN 00 extension 0 , 0 00
number leaf nodes.
Proof. Let number leaf nodes 0 . definition, nodes
extended exactly one child node 00 . follows 00 also leaf nodes.
L EMMA 2. Given 0 TiOR , number leaf nodes 0 , denoted mi , deg i1 .
Proof. show induction mi = deg i1 . = 1 m1 = 1. Assume = p 1,
. first extend 0 00 . Lemma 1, 00 0
mp1 = deg p2 , let 0 Tp1
p1
number leaf nodes, namely mp1 . Next, extend 00 000 TpOR . Since
mp1 leaf nodes 00 deg child nodes 000 , follows mp ,
number leaf nodes 000 mp = mp1 deg = deg p2 deg = deg p1 .
Proof Theorem 13 Consider number partial solution subtrees N contained ST :
949

fiF LEROVA , ARINESCU , & ECHTER

N=

h
X

(NiOR + NiAN )

(3)

i=1

NiOR


= |TiOR | NiAN = |TiAN |, respectively.
0
, easy see 0 extended single partial solution subtree
Given Ti1
00

Ti leaf nodes 0 deg child nodes 00 . Therefore:

NiOR = Ni1

(4)

Given 0 TiOR , 0 extended k partial solution subtrees 00 TiAN
leaf nodes 0 exactly one child node 00 k
bounds domain size. Lemmas 1 2, that:
NiAN = NiOR k deg

i1

(5)

Using Equations 4 5, well N1OR = 1, rewrite Equation 3 follows:
N = (1 + k)
+ (k + k deg+1 )
+ (k deg+1 + k deg

2 +deg+1

)
(6)

+ ...
+ (k deg
O(k

h2 +deg h3 +...+1

deg h 1
deg1

+ k deg

h1 +deg h2 +...+1

)

)

Thus, worst-case number partial solution subtrees need stored OPEN
h1
h1
N O(k deg ). Therefore, time space complexity m-AOBF follows O(k deg ).
pseudo tree balanced, namely internal node exactly deg child nodes,
time space complexity bound O(k n ), since n O(deg h1 ).

References
Aljazzar, H., & Leue, S. (2011). K : heuristic search algorithm finding k shortest paths.
Artificial Intelligence, 175(18), 21292154.
Batra, D. (2012). efficient message-passing algorithm M-best MAP problem. Uncertainty
Artificial Intelligence.
Charniak, E., & Shimony, S. (1994). Cost-based abduction MAP explanation. Artificial Intelligence, 66(2), 345374.
Darwiche, A. (2001). Decomposable negation normal form. Journal ACM (JACM), 48(4),
608647.
Darwiche, A., Dechter, R., Choi, A., Gogate, V., & Otten, L. (2008).
Results probablistic inference evaluation UAI08, web-report
http://graphmod.ics.uci.edu/uai08/Evaluation/Report.
In: Uncertainty Artificial Intelligence applications workshop.
950

fiS EARCHING B EST OLUTIONS G RAPHICAL ODELS

de Campos, L. M., Gamez, J. A., & Moral, S. (1999). Partial abductive inference bayesian belief
networks using genetic algorithm. Pattern Recognition Letters, 20(11), 12111217.
de Campos, L. M., Gamez, J. A., & Moral, S. (2004). Partial abductive inference bayesian networks using probability trees. Enterprise Information Systems V, pp. 146154. Springer.
Dechter, R. (1999). Bucket elimination: unifying framework reasoning. Artificial Intelligence,
113(1), 4185.
Dechter, R., & Mateescu, R. (2007). AND/OR search spaces graphical models. Artificial Intelligence, 171(2-3), 73106.
Dechter, R., & Rish, I. (2003). Mini-buckets: general scheme bounded inference. Journal
ACM, 50(2), 107153.
Dechter, R. (2013). Reasoning probabilistic deterministic graphical models: Exact algorithms. Synthesis Lectures Artificial Intelligence Machine Learning, 7(3), 1191.
Dechter, R., & Pearl, J. (1985). Generalized best-first search strategies optimality A*.
Journal ACM (JACM), 32(3), 505536.
Dijkstra, E. W. (1959). note two problems connexion graphs. Numerische mathematik,
1(1), 269271.
Elliott, P. (2007). Extracting K Best Solutions Valued And-Or Acyclic Graph. Masters
thesis, Massachusetts Institute Technology.
Eppstein, D. (1994). Finding k shortest paths. Proceedings 35th Symposium Foundations Computer Science, pp. 154165. IEEE Comput. Soc. Press.
Fishelson, M., & Geiger, D. (2002). Exact genetic linkage computations general pedigrees.
International Conference Intelligent Systems Molecular Biology (ISMB), pp. 189198.
Fishelson, M. a., Dovgolevsky, N., & Geiger, D. (2005). Maximum likelihood haplotyping
general pedigrees. Human Heredity, 59(1), 4160.
Flerova, N., Dechter, R., & Rollon, E. (2011). Bucket mini-bucket schemes best solutions
graphical models. Graph structures knowledge representation reasoning
workshop.
Fromer, M., & Globerson, A. (2009). lp view m-best map problem. Advances Neural
Information Processing Systems, 22, 567575.
Ghosh, P., Sharma, A., Chakrabarti, P., & Dasgupta, P. (2012). Algorithms generating ordered
solutions explicit AND/OR structures. Journal Artificial Intelligence (JAIR), 44(1),
275333.
Gogate, V. G. (2009). Sampling Algorithms Probabilistic Graphical Models Determinism
DISSERTATION. Ph.D. thesis, University California, Irvine.
Hamacher, H., & Queyranne, M. (1985). K best solutions combinatorial optimization problems.
Annals Operations Research, 4(1), 123143.
Ihler, A. T., Flerova, N., Dechter, R., & Otten, L. (2012). Join-graph based cost-shifting schemes.
arXiv preprint arXiv:1210.4878.
Kask, K., & Dechter, R. (1999a). Branch bound mini-bucket heuristics. IJCAI, Vol. 99,
pp. 426433.
951

fiF LEROVA , ARINESCU , & ECHTER

Kask, K., & Dechter, R. (1999b). Mini-bucket heuristics improved search. Proceedings
Fifteenth conference Uncertainty artificial intelligence, pp. 314323. Morgan
Kaufmann Publishers Inc.
Kjrulff, U. (1990). Triangulation graphsalgorithms giving small total state space. Tech. Report
R-90-09.
Lawler, E. (1972). procedure computing k best solutions discrete optimization problems
application shortest path problem. Management Science, 18(7), 401405.
Marinescu, R., & Dechter, R. (2009a). AND/OR Branch-and-Bound search combinatorial optimization graphical models. Artificial Intelligence, 173(16-17), 14571491.
Marinescu, R., & Dechter, R. (2009b). Memory intensive AND/OR search combinatorial optimization graphical models. Artificial Intelligence, 173(16-17), 14921524.
Marinescu, R., & Dechter, R. (2005). AND/OR branch-and-bound graphical models. International Joint Conference Artificial Intelligence, Vol. 19, p. 224. Lawrence Erlbaum
Associates Ltd.
Nillson, N. J. (1980). Principles Artificial Intelligence. Tioga, Palo Alto, CA.
Nilsson, D. (1998). efficient algorithm finding probable configurations probabilistic expert systems. Statistics Computing, 8(2), 159173.
Nilsson, N. (1982). Principles artificial intelligence. Springer Verlag.
Otten, L., & Dechter, R. (2011). Anytime AND/OR depth first search combinatorial optimization. SOCS.
Pearl, J. (1984). Heuristics: Intelligent Search Strategies. Addison-Wesley.
Schiex, T. (2000). Arc consistency soft constraints. International Conference Principles
Practice Constraint Programming (CP), 411424.
Seroussi, B., & Golmard, J. (1994). algorithm directly finding K probable configurations Bayesian networks. International Journal Approximate Reasoning, 11(3), 205
233.
Wainwright, M. J., & Jordan, M. I. (2003). Variational inference graphical models: view
marginal polytope. Proceedings Annual Allerton congerence communication
control computing, Vol. 41, pp. 961971. Citeseer.
Wemmenhove, B., Mooij, J. M., Wiegerinck, W., Leisink, M., Kappen, H. J., & Neijt, J. P. (2007).
Inference promedas medical expert system. Artificial intelligence medicine, pp.
456460. Springer.
Yanover, C., & Weiss, Y. (2004). Finding Probable Configurations Using Loopy Belief
Propagation. Advances Neural Information Processing Systems 16. MIT Press.
Yanover, C., Schueler-Furman, O., & Weiss, Y. (2008). Minimizing learning energy functions
side-chain prediction. Journal Computational Biology, 15(7), 899911.

952

fiJournal Artificial Intelligence Research 55 (2016) 165-208

Submitted 03/15; published 01/16

Effectiveness Automatic Translations
Cross-Lingual Ontology Mapping
Mamoun Abu Helou

mamoun.abuhelou@disco.unimib.it

Department Informatics,
Systems Communication
University Milan-Bicocca

Matteo Palmonari

matteo.palmonari@disco.unimib.it

Department Informatics,
Systems Communication
University Milan-Bicocca

Mustafa Jarrar

mjarrar@birzeit.edu

Department Computer Science
Birzeit University

Abstract
Accessing integrating data lexicalized different languages challenge. Multilingual lexical resources play fundamental role reducing language barriers map
concepts lexicalized different languages. paper present large-scale study
effectiveness automatic translations support two key cross-lingual ontology
mapping tasks: retrieval candidate matches selection correct matches
inclusion final alignment. conduct experiments using four different large
gold standards, one consisting pair mapped wordnets, cover four different
families languages. categorize concepts based lexicalization (type words,
synonym richness, position subconcept graph) analyze distributions
gold standards. Leveraging categorization, measure several aspects translation
effectiveness, word-translation correctness, word sense coverage, synset synonym coverage. Finally, thoroughly discuss several findings study,
believe helpful design sophisticated cross-lingual mapping algorithms.

1. Introduction
Different ontology representation models proposed ease data exchange
integration across applications. Axiomatic ontologies represented logic-based languages, e.g., OWL (2004), define concepts means logical axioms. Lexical ontologies define meaning concepts taking account words used
express (Hirst, 2004): concept defined one synonym words (Miller,
1995), refer lexicalization concept, connected concepts
semantic relations. Several hybridizations two approaches also proposed (Vossen et al., 2010).
data sources using different ontologies integrated, mappings
concepts described ontologies established. task also called
ontology mapping. Automatic ontology mapping methods introduced ease task
finding potential mappings determining ones included final
alignment. Ontology mapping methods perform two main sub tasks: candidate match
c
2016
AI Access Foundation. rights reserved.

fiAbu Helou, Palmonari, & Jarrar

retrieval, first set potential matches found; mapping selection, subset
potential matches included final alignment.
aspects concept modelling common lexical logical ontologies, despite
differences: concepts lexicalization organized subconcept graphs.
Synonymful lexicalizations, i.e., lexicalizations contain one synonym words,
frequently found lexical ontologies axiomatic ontologies. However,
enriching lexicalization concepts axiomatic ontologies set synonyms
well-established practice ontology mapping (Sorrentino, Bergamaschi, Gawinecki, & Po,
2010; Shvaiko & Euzenat, 2013; Faria, Martins, Nanavaty, Taheri, Pesquita, Santos, Cruz,
& Couto, 2014).
Cross-lingual ontology mapping task establishing mappings concepts
source ontology lexicalized language concepts target ontology lexicalized different language (Spohr, Hollink, & Cimiano, 2011). consider
million datasets published online linked open data 24 different languages (LOGD, 2015), cross-lingual ontology mapping currently considered important
challenge (Gracia, Montiel-Ponsoda, Cimiano, Gomez-Perez, Buitelaar, & McCrae, 2012).
instance, COMSODE project (2015), several tables lexicalized different languages published RDF (2014) annotated using domain ontologies.
Data publishers would like annotate data using ontologies lexicalized native language well English. Annotations native language publishers
facilitate access local citizens, annotations English support integration data published different countries large amount data published
English. cross-lingual ontology mapping system may help facilitating bilingual
data annotation.
Cross-lingual ontology mapping methods also helpful construction multilingual large lexical ontologies (De Melo & Weikum, 2009; Abu Helou, Palmonari, Jarrar, &
Fellbaum, 2014). example, Arabic Ontology project (Birzeit, 2011; Jarrar, 2011,
2006; Jarrar et al., 2014), kernel core concepts could extended mapping new
concepts defined synsets glosses English WordNet (Miller, 1995; Fellbaum,
1998) derive novel semantic relations.
cross-lingual ontology mapping methods include step concepts
lexicalizations one ontology automatically translated language
ontology (Pianta, Bentivogli, & Girardi, 2002; Vossen, 2004). frequently adopted
approach obtain automatic translations use multilingual lexical resources,
machine translation tools bilingual dictionaries. quality translations used
mapping method major impact performance. However, found
systematic large-scale analysis effectiveness automatic translations
context cross-lingual mapping missing. study presented paper aims
providing significant contribution fill gap.
study, use two multilingual lexical resources sources translations: Google
Translate (2015) BabelNet 1 (Navigli & Ponzetto, 2012). Google Translate machine
translation tool frequently used cross-lingual ontology mapping (Shvaiko,
Euzenat, Mao, Jimenez-Ruiz, Li, & Ngonga, 2014). Previous work suggested
1. used BabelNet version 2.5. Recent versions released writing paper (BabelNet, 2012).

166

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

Google Translate performs better Web translation services context concept mapping (Al-Kabi, Hailat, Al-Shawakfa, & Alsmadi, 2013; Oliver & Climent, 2012).
addition, service configured obtain reverse translations,
increase number words automatically translated. BabelNet largest
multilingual knowledge resource available today. concepts derived
fusion English WordNet - largest lexical ontology - large source encyclopedic knowledge Wikipedia (2015b). Multilingual lexicalizations created
using inter-lingual links Wikipedia, different translation strategies several bilingual
dictionaries collaboratively created Web, explain detail Sections 3.3
5. one hand, expect translations obtained BabelNet cover large
number words. hand, evaluating translations obtained BabelNet
indirectly evaluating different sources translations, used
individually several cross-lingual mapping approaches. Another reason choosing
resources study large number languages covered Google Translate
BabelNet (respectively, 90 272), compared resources kind.
study organized follows. focusing concepts lexicalizations, consider
concepts synsets, i.e., sets words equivalent meaning given context (Miller,
1995). definition used classify concepts (synsets) different categories, based
different characteristics: word ambiguity (e.g., monosemous vs. polysemous), number
synonyms (e.g., synonymful vs. synonymless), position concept hierarchy (e.g.,
leaves vs. intermediate concepts). Using classifications, evaluate effectiveness
translations obtained multilingual lexical resources studying performance
cross-lingual mapping tasks executed using automatic translations different categories
synsets. first analyze coverage translations impact candidate match
retrieval task. analyze difficulty mapping selection task using baseline
mapping selection method. analyses based different measures introduced
evaluate translation effectiveness terms coverage correctness, based
comparison translations considered perfect according gold standard, i.e.,
set cross-lingual mappings deemed correct.
gold standards, use cross-lingual mappings manually established (or validated)
lexicographers four wordnets (Arabic, Italian, Slovene Spanish)
English WordNet. Using gold standards based wordnets two main advantages.
contain large number mapped concepts, much larger, e.g., gold standards used evaluate cross-lingual ontology mapping systems Ontology Alignment
Evaluation Initiative (OAEI) (Shvaiko et al., 2014; OAEI, 2015), leverage
lexical characterization concepts different categories provide in-depth analysis. wordnets used experiments also representative different families
languages different ontology sizes.
best knowledge, first attempt carry systematic largescale study effectiveness multilingual lexical resources sources translations
context cross-lingual ontology mapping. previous work, resources
mostly evaluated context specific algorithms (Fu, Brennan, & OSullivan,
2012; Spohr et al., 2011), limited number gold standards, limited
number languages. experiments lead interesting findings, discussed
(numbered) list observations summarized lessons learned section. Overall,
167

fiAbu Helou, Palmonari, & Jarrar

believe findings study useful definition accurate
flexible mapping algorithms, based characterization concepts lexicalization.
paper structured follows. Section 2, introduce preliminary definitions used rest paper. Section 3, overview related work, goal
discussing role automatic translations cross-lingual ontology mapping related
fields. evaluation measures multilingual lexical resources used study
obtain translations presented respectively sections 4 5. section 6, present
experiments. Conclusions future work end paper.

2. Preliminaries
section introduce definitions used study, cover concept lexicalizations, cross-lingual mapping translation tasks.
2.1 Lexicalization Concepts
consider general definition ontologies, focusing lexical characterization
concepts, relations natural language words used concepts.
choice motivated observation even ontology matching systems look
semantics axiomatic ontologies, e.g., LogMap (Jimenez-Ruiz & Grau, 2011), use
concept lexicalizations retrieve candidate matches concepts source ontology.
reason, borrow several definitions lexical ontologies like WordNet (Miller, 1995)
use terminology throughout paper.
Slightly abusing terminology (but coherently WordNet), words lexemes
associated concept. word called simple contains one token, e.g, table,
called collection 2 contains tokens, e.g., tabular array.
Wordnet organizes natural language words synonym sets, so-called synsets.
synset represents one underlying concept, i.e., set words (synonyms) share
meaning given context. W set words represented wordnet,
synset P(W ) set words = {w1 , ..., wn }.
synset contain one word (synonymless) many words (synonymful ). use
concept synset interchangeably rest paper. Depending specific
case, use two notations concepts: set notation {w1 , ..., wn } used need
make explicit reference words contained synset, symbol notation
used reference needed. also use set notation w state
word w contained synset s. set words contained concept also called
lexicalization. use superscript specify natural language used concept
lexicalizations needed, i.e., wL , sL , W N L represent word, synset wordnet
respectively lexicalized language L.
addition lexical relations, link individual words (e.g., synonymy, antonymy),
wordnets support semantic relations, link concepts. Hypernymy hyponymy important semantic relations wordnets. defined one
2. alternative name used instead collection multiword expression (MWE), frequently used
particular literature machine translation tool evaluation (Sag, Baldwin, Bond, Copestake,
& Flickinger, 2002); use collection coherent WordNet terminology used throughout
paper.

168

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

inverse one determine subconcept graph wordnets (see Section 3.1). example, synset {table, tabular array} hyponym synset {array},
synset {array} hypernym synset {table, tabular array}.
word polysemous, i.e., many meanings (or senses) member many
synsets. paper superscript + right-hand corner word, e.g., board+ ,
indicates polysemous word. word monosemous, i.e. one meaning
member one synset. example, English WordNet3 eight senses
word table+ ; one senses means set data arranged rows columns,
word tabular array synonym word. Another sense means food
meals general, word board+ synonym word.
Given set words W set synsets defined wordnet W N ,
function senses : W 7 P(S) returns set synsets word belong to, defined
senses(w) = {s|w s}. define set word senses wordnet,
W = {< w, > |s senses(w)}, i.e., set couples < w, >, sense
w (in given context). Observe number word senses higher number
synsets consider associations words synsets.
Example 1. word table+ eight senses English WordNet, senseEn (table)
= {{ table+ , tabular array },{table+ },{table+ },{mesa+ , table+ },{table+ },{board+ , table+ },
{postpone, prorogue+ , hold over+ , put over+ , table+ , shelve+ , set back+ , defer+ , remit+ ,
put off+ }, {table+ , tabularize, tabularise, tabulate+ }}4 .
2.2 Cross-Lingual Mapping
ontology matching field, cross-lingual mapping defined task finding
establishing mappings concepts source ontology lexicalized language L1
concepts target ontology lexicalized language L2 (Spohr et al., 2011). Mappings
represent different relations source target concepts. consider specific
mapping relation R, source concept target concept t, output mapping
task set couples < s, >, also called alignment. cross-lingual mapping task
mapping relation R composed two main steps (or, sub tasks):
candidate match retrieval: find, source concept lexicalized L1 , set
target concepts lexicalized L2 . call concepts found task candidate
matches.
mapping selection: given set candidate matches = {t1 , ..., tn } (lexicalized
L2 ) source concept (lexicalized L1 ), select set concepts 0
that, 0 , R(s, t) holds. R(s, t) holds, say correct
match s, < s, > correct mapping.
depth analysis semantics cross-lingual mappings, refer
previous work (Abu Helou et al., 2014). gold standard alignment (or, gold standard
short), denoted gs, alignment synsets (concepts) two wordnets
mappings alignment believed correct. paper, consider
equivalence mappings, i.e., mappings specify source target concepts
3. following use WordNet version 3.0.
4. senses definitions found online http://wordnetweb.princeton.edu/perl/webwn?s=table

169

fiAbu Helou, Palmonari, & Jarrar

equivalent meaning. often assumed cardinality equivalence mappings
1:1, meaning source concept one correct match.
gold standard alignment cardinality 1:1, synset source language
one equivalent synset target language. use predicate symbol
indicate two synsets equivalent (express meaning) gold standard.
Using synset mappings gold standard gs, define possible senses word wL1
L1
2
target language L2 , denoted sensesL
gs (w ), senses L2 equivalent
senses wL1 native language L1 :
L2
sensesgs
(wL1 ) = {sL2 | sL1 (wL1 sL1 sL1 sL2 )}

(1)

Observe candidate match retrieval step define upper bound mapping
selection step: correct mapping selected target mapping
retrieved candidate match. addition, mapping selection form disambiguation
task : correct meaning concept (the lexicalization concept), target
language chosen among different possible meanings. larger number candidate
matches little evidence preferring one candidate another likely make
selection problem difficult.
2.3 Translation Tasks
Translating words one language words another language crucial context
cross-lingual concept mapping, and, particular, candidate match retrieval step.
seek clarity consider two translation tasks: translations single words
translations synsets. Translations based external multilingual lexical resources,
e.g., machine translation tool dictionary built using multilingual lexical resources.
define word-translation word wL1 target language L2 resource
L1 7 P(W L2 ) maps word w L1 sets words
2
D, function wT ransL
: W
target language L2 .
define synset-translation synset sL1 target language L2
L1 7 P(P(W L2 )) maps synset sets
2
resource D, function sT ransL
:
sets words, output word-translation w s.
synset-translation function defined follows:
L2
L1
L1
L1
2
sT ransL
(s ) = {wT ransD (w ) | w }

(2)

Example 2. synset-translation Italian synset {tavola+ , tabella}It
+

En
+,It ),
English given follow: sTransEn
({tavola , tabella} ) = {wTransD (tavola
En

wTransD (tabella )} = {{table, board, plank, panel, diner, slab}, {table, list}}.
Observe definition synset-translation function make set
union outputs every word-translation applied words synset. Instead,
using Eq.2, write output synset-translation function multiset union
sets returned every word-translation. instance, Example 2, sTransEn

({tavola+ , tabella}It ) = {table(2) , board(1) , plank(1) , panel(1) , diner(1) , slab(1) }, superscript
numbers brackets indicate frequency count words translation set.
Similarly, table(2) means word table appears two subsets, i.e., word
table resulted translation two synonym words source synset,
170

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

tavola tabella. way count number word-translations
produced one word target language given synset-translation. counts
helpful use results synset-translation perform mapping selection
step. example, counts used weigh candidate matches majority
voting approach, like one used experiments Section 6.3.2.

3. Automatic Translations Cross-Lingual Mapping Tasks
section review use automatically generated translations cross-lingual
ontology mapping related tasks enrichment multilingual knowledge
resources cross-lingual word-sense disambiguation.
enrichment multilingual knowledge resources related cross-lingual ontology
mapping findings study several reasons. First, multilingual knowledge resources used sources translations cross-lingual ontology mapping
approaches. Second, wordnets mapped English WordNet use gold
standards multilingual knowledge resources, mappings represent interlingual links concepts. Third, two frequently adopted approaches
enrich multilingual knowledge resources based either mapping concepts lexicalized
different languages translating concepts lexicalizations. Since evaluate
correctness coverage translations ontology concepts, findings relevant also
approaches intend use translations ontology mapping methods enrich
multilingual knowledge resources.
Cross-lingual word sense disambiguation another research field translations
used solve mapping problem, related to, also quite different
from, mapping tasks considered study.
discussing related work different research areas discuss usage
term concept lexical axiomatic ontologies. conclude section presenting
contributions study evaluation automatically generated translations
cross-lingual ontology mapping related tasks.
3.1 Concepts Lexical Axiomatic Ontologies
Concepts constituents thoughts (Margolis & Laurence, 2014). relation natural language thought much debated. example, maintain
concepts independent language (Fodor, 1975; Pinker, 1994) others believe
concepts require natural language exist (Carruthers, 2002; Spelke, 2003). However,
natural language plays major role expressing concepts many computational knowledge representation systems proposed support natural language processing, information
retrieval data integration tasks. Ontologies among computational knowledge
representation systems. distinguish two different kinds ontologies.
lexical ontologies, meaning concepts primarily defined relation
words used express them. example, order represent concept
table, reference object used eat meal, set words used refer
concept specified. Lexical ontologies include domain thesauri, wordnets,
popular English WordNet (Miller, 1995; Fellbaum, 1998). axiomatic
ontologies (or, logical ontologies) meaning concepts defined axioms specified
171

fiAbu Helou, Palmonari, & Jarrar

logical language, e.g., First Order Logic, interpreted constraints
mathematical structures support automated reasoning (Horrocks, 2008). Examples
logical ontologies include web ontologies defined RDFS (2014) OWL, annotated
database schema spreadsheet also considered ontology based broad
definition (Zhuge, Xing, & Shi, 2008; Po & Sorrentino, 2011; Mulwad, Finin, & Joshi, 2013;
Zhang, 2014). example, represent afore-mentioned concept table, define
piece furniture smooth flat top usually supported one
vertical legs logical language. intended interpretation concept every
table ever existed world, or, specifically, list products type
table described spreadsheet (Mulwad et al., 2013; Zhang, 2014).
Many hybrid approaches also exist. example, efforts assure certain logical properties relations represented lexical ontologies found KYOTO (Vossen et al.,
2010). YAGO logical ontology integrates many concepts English WordNet (Suchanek, Kasneci, & Weikum, 2008). WordNet concepts used annotate database
schema given formal interpretation used support database integration (Sorrentino et al., 2010).
matter fact, despite several differences, concepts modelled lexical, axiomatic,
hybrid ontologies share two important features. First, concepts organized subconcept
graphs, i.e., hierarchies, partially ordered sets, lattices define relations
concepts based generality. relations referred subconcept relations
axiomatic ontologies, different relations represented lexical ontologies,
e.g., hyponymy/hypernymy. Second, every ontology concepts lexical descriptions
may include set synonym words. course, synonyms first class citizens
lexical ontologies available large number concepts, availability
limited axiomatic ontologies. However, step enrich concept lexicalizations
logical ontologies synonyms extracted dictionaries lexical resources
introduced many ontology mapping approaches exploit lexical matching algorithms (Shvaiko & Euzenat, 2013; Otero-Cerdeira, Rodrguez-Martnez, & Gmez-Rodrguez,
2015; Sorrentino et al., 2010; Faria et al., 2014).
3.2 Translations Cross-Lingual Ontology Mapping
majority ontology mapping methods proposed literature addressed
problem mapping ontological resources lexicalized natural language, called
mono-lingual ontology mapping. Since mono-lingual matching systems cannot directly access semantic information ontologies lexicalized different natural languages (Fu
et al., 2012), techniques reconcile ontologies lexicalized different natural languages
proposed (Gracia et al., 2012; Trojahn, Fu, Zamazal, & Ritze, 2014).
Translation-driven approaches used overcome natural language barriers
transforming cross-lingual mapping problem mono-lingual one (Fu et al., 2012).
Different multilingual lexical resources used perform translation tasks,
including manual translations, machine translation tools, bilingual dictionaries built
Web-based multilingual resources. rich classification comparison crosslingual mapping systems refer work Trojahn et al. (2014).
172

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

Liang Sini (2006) manually mapped English thesaurus AGROVOC (2014)
Chinese thesaurus CAT (2014). mappings generated approaches likely
accurate reliable. However, time resource consuming process
specially maintaining large complex ontologies.
Machine translation tools widely adopted cross-lingual ontology mapping. Spohr
et al. (2011) translate ontology labels pivot language (English) using machine
translation tool (Bing, 2016). Then, define feature vector based combination
string-based structural-based similarity metrics learn matching function using
support vector machine. Like approaches based supervised machine learning algorithms, approach disadvantage requiring significant number training
samples well-designed features achieve good performance. Fu et al. (2012) translate
ontology labels using Google Translate, match translated labels combining different similarity measures. approach leverages structural information
ontology concepts considering neighbours matching process. approaches proposed also apply string-based, lexical structural matching
methods ontology labels translated machine translation tools, like Google Translate
Bing (Faria et al., 2014; Jimenez-Ruiz, Grau, Xia, Solimando, Chen, Cross, Gong, Zhang,
& Chennai-Thiagarajan, 2014; Djeddi & Khadir, 2014).
Multilingual knowledge resources available web also exploited translate concepts labels (Hovy, Navigli, & Ponzetto, 2012). Wiktionary (2015) used
generate translations match English French ontologies (Lin & Krizhanovsky, 2011).
First, bilingual English-French lexicon built using Wiktionary used translate
labels ontologies. Then, monolingual ontology matching system COMS
used (Lin, Butters, Sandkuhl, & Ciravegna, 2010). COMS uses set string-based, lexical
structural matching techniques find appropriate mappings. similar approach
uses Wikipedia inter-lingual links retrieve candidate matches source concepts (Bouma,
2010; Hertling & Paulheim, 2012). However, used alone, Wiktionary Wikipedia
inter-lingual links may limited coverage, particular resource-poor languages.
spite efforts, cross-lingual mapping systems still perform significantly worse
mono-lingual mapping systems according recent results OAEI contest (Shvaiko
et al., 2014), suggest cross-lingual ontology mapping still challenging
problem (Trojahn et al., 2014). datasets used evaluate cross-lingual mapping
OAEI, i.e., datasets multifarm track (Meilicke et al., 2012), consist alignments
established axiomatic ontologies relatively small size specific domain
conference organization. Since study want investigate translations obtained
different multilingual lexical resources large scale specific domain,
decided use different larger gold standards experiments.
3.3 Translations Enrichment Multilingual Knowledge Resources
Several multilingual wordnets (lexical ontologies) developed manually automatically translating concepts English WordNet new languages (Pianta et al., 2002;
Vossen, 2004; Tufis, Cristea, & Stamou, 2004; Gonzalez-Agirre, Laparra, & Rigau, 2012;
Tomaz & Fiser, 2006). expand merge models (Vossen, 2004) main approaches used development multilingual wordnets. merge model, synsets
173

fiAbu Helou, Palmonari, & Jarrar

pre-existing resource one language (e.g., thesaurus, even unstructured lexical resource like dictionary) aligned equivalent synset English. expand
model, English synsets translated respective languages. main advantage
two approaches avoid expensive manual elaboration semantic hierarchy
new languages. English WordNet (Fellbaum, 1998) hierarchy used reference
wordnets. Moreover, ontology built following approaches also
automatically mapped English WordNet.
several wordnets, English concepts manually translated human lexicographers using external lexical resources dictionaries, thesauri taxonomies.
approach applied build example Arabic wordnet (Rodrguez et al.,
2008), Italian wordnet (Pianta et al., 2002), Spanish wordnet (Gonzalez-Agirre
et al., 2012) core Slovene wordnet, used experiments. However,
manual approach construct ontologies aim cover natural languages lexicons
often effort-intensive time-consuming task (De Melo & Weikum, 2009). Automatic
approaches therefore proposed reduce lexicographers workload.
Parallel corpora used building wordnets languages English.
basic assumption underlying methods translations words real texts
offer insights semantics (Resnik & Yarowsky, 1999). Slovene wordnet
enriched using word alignments generated sentence-aligned multilingual corpus (Fiser,
2007). wordnet extended using bilingual dictionaries inter-lingual
links Wikipedia. similar approach also followed building French wordnet (Sagot
& Fiser, 2008). monosemous words English WordNet automatically translated using bilingual French-English dictionaries built various multilingual resources,
Wikipedia inter-lingual links, Wiktionary, Wikispecies (2015), EUROVOC
thesaurus (2015).
Sentence-aligned parallel corpora may available pair natural languages. addition, specific tools needed perform sentence and/or word alignment across corpora, bilingual dictionaries extracted corpora
biased towards domains cover. overcome limitations, Macedonian
wordnet (Saveski & Trajkovski, 2010), machine translation tool used create parallel corpora. Monosemous English words directly translated using bilingual
English-Macedonian dictionary. polysemous words, English WordNet sense-tagged
glosses (WordNet-Princeton, 2015) automatically translated Macedonian using
Google Translate.
supervised method automatically enrich English synsets lexicalizations
languages also proposed (De Melo & Weikum, 2012). method learns determine best translation English synsets taking account bilingual dictionaries,
structural information English WordNet, corpus frequency information.
approaches enrich multilingual knowledge resources proposed
build Universal WordNet (UWN, De Melo & Weikum, 2009), WikiNet (Nastase, Strube,
Boerschinger, Zirn, & Elghafari, 2010), BabelNet (Navigli & Ponzetto, 2012),
integrate multilingual encyclopedic knowledge Wikipedia English WordNet.
paper, focus BabelNet, largest multilingual knowledge resource today,
use study build bilingual dictionaries use translation (explained
174

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

Section 5). comprehensive comparison amongst afore-mentioned three resources
found work Navigli Ponzetto (2012).
BabelNet (Navigli & Ponzetto, 2012) built integrating English WordNet
Wikipedia. two resources mapped using unsupervised approach.
result, BabelNet covers approximately 83% WordNets nominal synsets. Synsets
English WordNet cover particular (but only) classes objects, e.g., University5 , Wikipedia entries cover particular (but only) named entities, e.g.,
University Milano-Bicocca6,7 . Synsets English WordNet BabelNet
entries enriched lexicalizations languages using variety lexical resources. first set lexicalizations languages English obtained using
inter-lingual links Wikipedia. Synsets Wikipedia entries cannot found
enriched using automatic translations English senses-tagged sentences, extracted
Wikipedia SemCor corpus (Miller, Leacock, Tengi, & Bunker, 1993).
frequent translation given language detected included variant lexicalization language; approach named context-translation. Translations
monosemous English words collected using Google Translate directly included expanded lexicalizations; approach named contextless-translation.
Observe contextless translations based heuristics, i.e., monosemous
words correctly translated (also referred monosemous word heuristics). core
BabelNet consists lexicalizations obtained approaches, also named BabelNet synsets. Later, BabelNet synsets lexicalizations expanded multilingual
lexical resources: Wiktionary, WikiData (2015), OmegaWiki (2015), several wordnets
mapped English wordnet, available Open Multilingual
Wordnet (OMWN, 2015; Bond & Foster, 2013).
BabelNet lexicalizations (synsets) evaluated manually mapped wordnets, also use experiments gold standards. also performed manual
evaluation randomly sampled set concepts. limit evaluation consists
making explicit sampled senses uniformly cover polysemous monosemous
senses. Otherwise distinction important evaluate different translations, also vast number translations obtained using contextless approach,
based monosemous word heuristics. experiments (Section 6)
specifically analyze effectiveness monosemous word heuristics context
ontology mapping.
observe expand model used substantially merge model
approaches automate enrichment multilingual wordnets knowledge resources.
One may attempt enrich existing wordnet via merge approach mapping
unstructured weakly structured lexicon, e.g., dictionary, structured reference
ontology, e.g., English WordNet. example, Arabic Ontology Project (Jarrar, 2011; Abu Helou et al., 2014), authors plan use approach extend core
ontology manually created mapped English WordNet. However, mapping
task incorporated approach particularly challenging (Abu Helou, 2014): lack
semantic relations synsets unstructured lexicon makes difficult
5. http://wordnetweb.princeton.edu/perl/webwn?s=university
6. https://en.wikipedia.org/wiki/University Milano-Bicocca
7. http://babelnet.org/synset?word=University Milano-Bicocca

175

fiAbu Helou, Palmonari, & Jarrar

disambiguate meaning translation matching steps (Shvaiko & Euzenat, 2013; Trojahn et al., 2014). effective cross-lingual ontology mapping method
support application merge model large scale, thus supporting construction enrichment multilingual knowledge resources. example, recent work
suggests approach, despite difficulty task, return multilingual concept lexicalizations richer ones obtained automatically translating
concepts labels (Abu Helou & Palmonari, 2015).
3.4 Translations Cross-Lingual Word Sense Disambiguation
Cross-lingual ontology mapping also related Cross-lingual Word Sense Disambiguation problem (CL-WSD), studied recent past addressed
SemEval 2010 2013 challenges (Els & Veronique, 2010; Lefever & Hoste, 2013).
goal CL-WSD predict semantically correct translations ambiguous words
context (Resnik & Yarowsky, 1999).
CL-WSD, lexical disambiguation task performed word translation task,
called lexical substitution task (McCarthy & Navigli, 2009). Given source word
sentence (e.g., Italian word), system tries translate word different
language (e.g., English). translation considered correct preserves sense
word context also target language.
CL-WSD systems rely parallel corpora (Gale, Church, & Yarowsky, 1992;
Resnik & Yarowsky, 1999; Apidianaki, 2009), including exploit existing multilingual wordnets (Ide, Erjavec, & Tufis, 2002). However, success coverage
methods highly depends nature parallel corpora way
extracted information used select appropriate senses. Corpora known
domain-orientated coverage, i.e., fine-grained senses different domains might
found specific parallel corpora (Navigli, 2009). importantly, parallel corpora may
available language couples specific domains (Apidianaki, 2009; Saveski &
Trajkovski, 2010).
One fundamental difference CL-WSD task cross-lingual ontology mapping CL-WSD context always available defined sentence word
occurs in. cross-lingual ontology mapping context defined neighbours translated concept, may limited (Mulwad et al., 2013; Zhang, 2014), may
even available, e.g., unstructured lexicon matched structured
ontology (Abu Helou et al., 2014).
3.5 Scope Contribution Study
Even approaches cross-lingual ontology mapping based transforming
cross-lingual mapping problem monolingual one leveraging translations obtained
machine translations tool multilingual lexical resources (Trojahn et al., 2014),
efforts dedicated systematically study effectiveness translations
cross-lingual ontology mapping.
Fu, Brennan, OSullivan (2009) studied limitations translation-based ontology mapping approaches, particular, extent inadequate translations introduce noise subsequent mapping step fail cover adequate number
176

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

concepts. performed two experiments, examined mappings independent,
domain specific, small-scale ontologies labeled English Chinese:
Semantic Web Research Communities ontology ISWC ontology. ontologies
lexically enriched. Fu colleagues classified translation errors
introduced machine translation tools three main categories. Inadequate translation,
translation source concept returns word, belongs concept
target ontology specific/generic equivalent concept; synonymic
translation, translation source concept returns word, synonym
word used target ontology denote equivalent concept (but different
one used target ontology); incorrect translation, translation
simply wrong. addition, study showed translating ontology labels isolation
leads poorly translated ontologies yields low-quality matching results, thus,
label translations conducted within context. context characterized
surrounding ontology concepts.
Spohr et al. (2011) observed that, target ontology lexicalized
one language, convenient translate source concepts languages
merge evidence provided translations. However, applicable
multilingual labels available target ontology, case
several cross-lingual mapping scenarios. addition, obtain better translations study
suggested translating source target ontologies labels pivot language
improve, extent, quality translation. However, authors stated
evidence experiments several language pairs needed support
claim, quality machine translations depends significantly pair considered
languages.
paper analyze effectiveness automatic translations cross-lingual concept mapping using large scale, general domain, lexically rich ontologies (wordnets).
ontologies used studies cover four different families languages besides English.
study effectiveness translations conducting large number experiments
address candidate match retrieval mapping selection steps ontology
mapping process. Overall, believe none previous work cross-lingual ontology
mapping provided systematic study, compared terms scale (size considered concepts), number considered languages, level detail analysis (concept
categorization).
analyses discussed paper also related studies automatic
translation strategies conducted evaluate BabelNet, one two multilingual
lexical resources used source translation study. work, quantitatively
evaluate correctness coverage translation strategies used BabelNet
means support cross-lingual mapping tasks (using mappings wordnets comparison; see Section 6.3.1). studies conducted evaluate BabelNet aimed, instead,
evaluating translation strategies means enrich multilingual lexicalizations
concepts. introduce two new measures evaluate correctness coverage
translations obtained multilingual resources, i.e., translation correctness synonym
coverage (see Section 4.3.2). addition, coverage correctness automatic translations study evaluated considering different categories synsets (defined
177

fiAbu Helou, Palmonari, & Jarrar

{tavolo+, tavola+} {asse+, tavola+}

{tavola+}

axle tree
board
axis
plank

table
desk

{plate+}

{table+}

board
plank

{tavola+, tabella}

table
panel

list
table

Italian Synsets

wTransDEn(wiIt)

diner slab

{board+, plank+} {table+, tabular array}

English Synsets

: translation.

{tavola+, tabella}

: mapped synsets.
: synset.

Figure 1: Example: Synset-translation

Section 6.2). Finally, analyze effectiveness monosemous word heuristic,
used several mapping systems BabelNet (contextless translation).

4. Evaluation Measures
study, want estimate effectiveness translations obtained multilingual
lexical resources (hereafter referred resources) finding candidate matches large
set concepts. also want estimate difficulty selecting one correct mapping
among set candidate matches, based information provided translations.
first objective, define four measures use evaluate translation
correctness coverage. first two measures, translation correctness word
sense coverage, used evaluate effectiveness word-translations given
word independently meaning, i.e., sense word given.
two measures, synset coverage synonym coverage, used evaluate
effectiveness synset-translation given synset focusing lexicalization
synsets target language. Word sense coverage synset coverage two measures
proposed previous work Navigli Ponzetto (2012), rewrite definitions
according notation introduced Section 2.3. Translation correctness synonym
coverage introduced study. facilitate definition measures first
introduce definition perfect translations respect gold standard.
measures derive several measures, e.g., averaging values across one wordnet,
present results experiments. second objective, use measure
straightforwardly derived well-known Precision measure (Shvaiko & Euzenat,
2013) explained, directly, Section 6.3.2.
178

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

4.1 Perfect Translations Gold Standard
perfect word-translation word wL1 target language L2 w.r.t gold
standard gs set every synonym words possible senses wL1 target
language L2 :
[

n
L2
L1
L2
L1
L2 L2
L2
L1
L2
wT ransgs (w ) =
wi | (s sensesgs (w ) wi )
(3)
i=1

Example 3. Figure 1 illustrates synset-translation tasks four Italian synsets
English. synset mapped equivalent synset English specified gold
standard gs. translations also obtained mappings Italian
English wordnets represented gs. instance, four (Italian English)
synsets mappings are: {tavola+ , tabella+ } {table+ , tabular array}, {asse+ , tavola+ }
{board+ , plank+ }, {tavolo+ , tavola+ } {table+ }, {tavola+ } {plate+ }. Figure 1,
perfect word-translation Italian word tavola English given follow:
+,It ) = {table+ , tabular array, board+ , plank+ , plate+ }En .
wTransEn
gs (tavola
Observe perfect word-translation function returns every word every possible
sense target language, i.e., word translation perfect returns complete
lexicalization every possible senses input word target language. definition
motivated scope analysis, evaluates effectiveness automatic
translations settings domain determined a-priori. individual
input word considered outside specific context, e.g., specific sentence, specialised
domain concept hierarchy, meaning word cannot disambiguated, unless
word monosemous. Otherwise, observe domain-specific machine translation
system, e.g., specialised financial domain, could determine correct meaning (and
translation) word, even word considered individually, implicit
interpretation context system. Thus, consideration polysemous words
absence context specification, defined translation word (i.e., set
words returned source translation) perfect contains, every possible
usage word, possible lexicalizations target language. one considers wordtranslations specialized domain, he/she may need adapt definition perfect
word-translation consequently.
perfect synset-translation synset sL1 target language L2 w.r.t gold
standard gs defined set every synonym words synset L2 mapped sL1
gs. perfect synset-translations defined follows:
[

n
L2
L2
L2
L2
L1
L2
L2 L1
(4)
sT ransgs (s ) =
wi | (wi )
i=1

Example 4. Figure 1, perfect synset-translation Italian synset {tavola+ ,
+

+
En .
tabella} given follow: sTransEn
gs ({tavola , tabella} ) = {table , tabular array}
4.2 Evaluation Word-Translations
section introduce translation correctness word sense coverage measures.
179

fiAbu Helou, Palmonari, & Jarrar

4.2.1 Translation Correctness
input word, measure evaluates extent resource returns precise
complete translations compared perfect word translations defined gold standard, consider every possible sense word target language.
define measure, need specify word returned resource
correct. word wL2 correct-translation word wL1 w.r.t gold standard
gs, wL2 belongs set perfect word-translations wL1 w.r.t gs (denoted
L1
L2
2
wT ransL
gs (w )). principle captured function correctTwL1 ,D (w ) defined
following equation:

L1
2
1
wL2 {wT ransL
L2
gs (w )}.
correctTwL1 ,D (w ) =
0
otherwise.
Example 5. Figure 1 English words table, board, plank correct
translations Italian word tavola, e.g., correctTtavoalIt (tableEn ) =1. English
words diner, panel, slab incorrect translations Italian word tavola,
e.g., correctTtavoalIt (slabEn )=0.
measure correctness translations returned resource word wL1
translation-correctness defined Eq. 5. measure computed harmonic
mean, i.e., F1 -measure, two measures: 1) Precision (Pr), defined number correct
translations returned resource total number translations returned
D; 2) Recall (R) 8 , defined number correct translations returned translation
resource total number perfect word translations. use Recall, Precision
F1 -measure (computed standard range), normalized range [0..100].
translation returned resource D, Precision set zero.

Pr =

|{wL2 |correctTwL1 ,D (wL2 )}|
2
L1
|{wT ransL
(w )}|

100, R =

|{wL2 |correctTwL1 ,D (wL2 )}|
2
L
|{wT ransL
gs (w 1 )}|

100

Pr R
100
(5)
Pr + R
Example 6. example shown Figure 1, correctness English translation
Italian word tavola computed follows: recall R = 60.0, precision P r = 50.0,
translation-correctness TransCorrectnessEn (tavolaIt ) = 55.0.
L2
ransCorrectnessD
(wL1 ) = F1 (P r, R) 100 = 2

4.2.2 Word Sense Coverage
input word, measure evaluates many possible word senses target
language covered least word translation (as defined Navigli & Ponzetto, 2012).
translation covers sense sL2 input word wL1 different language
resource returns least one word sL2 . use binary predicate cov(x, y) state
word-translation x covers sense y. Word senses coverage tells extent
polysemy word covered translation resource. Ideally, resource effective
8. remark Recall also named translation accuracy W SD literature (Navigli, 2009).

180

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

translating word wL1 able return correct-translations every possible
sense wL1 L2 .
Given word wL1 translated target language L2 resource D, word
senses coverage wL1 defined follows:

L2
wsCoverageD
(wL1 )



L2
L1
L1
L2
2
| sL2 | sL2 sensesL
gs (w ) cov(wT ransD (w ), ) |
=


2
L
| sL2 | sL2 sensesL
gs (w 1 ) |

(6)

Example 7. Figure 1, polysemous Italian word tavola four senses,
one mapped equivalent synset English. Using translation resource
three four senses covered (Eq.6). instance, senses {table+ }
+
covered, cov(wT ransEn
(tavola), {table}) = 1, sense {plate } covered,
cov(wTransEn
(tavola), {plate}) =0.
4.3 Evaluation Synset-Translations
section introduce synset synonym coverage measures.
4.3.1 Synset Coverage
measure defined boolean function applied input synset (Navigli & Ponzetto,
2012). synset sL1 covered translation, i.e., multi set union translation
constituent words, returns least one word equivalent synset target
language. measure useful computed set source synsets presented
Navigli Ponzetto. example, computing percentage source synsets
mapped gold standard covered translations obtained lexical resource,
evaluate number mappings discovered using translation
resource.
formally define synset coverage compact way, use concept perfect
L1
2
synset translation synset sL1 target language LL2 , denoted sT ransL
gs (s ).
L
1 synset translated target language L2 resource D, synset coverage
defined follows:

L1
L2 sT ransL2 (sL1 ))
2
1
wL2 (wL2 sT ransL
L2 L1
gs
(s ) w
sCoverageD (s ) =
(7)
0
otherwise.
Example 8. Consider Italian equivalent English synsets depicted Figure 1. Three four Italian synsets covered translation returns least
one word equivalent English synsets. instance, mapping {tavolo+ , tavola+ }
{table+ } covered, mapping {tavola+ } {plate+ } covered.

4.3.2 Synonyms Coverage
input synset sL1 , measure evaluates number words sL1
word-translation covers equivalent synset target language. measure tells
many synonyms concept lexicalization covered correct translations.
181

fiAbu Helou, Palmonari, & Jarrar

Give synset sL1 equivalent synset sL2 target language; sL1 translated
using resource D, synonym coverage sL1 defined follows:

L2 L1
synonymsCoverageD
(s )



L1
L2
2
| wL1 | wL1 sL1 cov(wT ransL
(w ), ) |
=
|sL1 |

(8)

Example 9. Figure 1 Italian synsets {tavola+ , tabella}, {asse+ , tavola+ },
{tavolo+ , tavola+ } full synonym words coverage (Eq.8). Whereas, synset {tavola+ }
covered word covered.
Synonym coverage valuable measure evaluate translations obtained lexical
resources field cross-lingual concept mapping. Consider, example, input
synset sL1 translation resource returns many (synonym) words
equivalent synsets sL2 target language. one hand, synonym words
useful increase probability finding sL2 among candidate matches sL1 .
hand, synonym words used evidence selecting sL2 best match
sL1 , e.g., compared candidate matches little evidence collected
via translation9 . Finally, observe synonym words coverage complementary
indication word senses coverage measure effectives translation resource,
i.e., coverage synonym words tool disambiguate polysemy translations
returned translation resource.
Throughout paper, order quantify overall coverage measures correctness word-translation tasks across dataset (wordnet), compute Macroaverage measure (Vincent, 2013). reported coverage measures normalized
range [0..100].

5. Multilingual Lexical Resources Translation
Automatic translations obtained using different kinds multilingual machinereadable lexical resources. selection resources depends level information encode, instance, quality (accuracy) translations provide, lexical domains cover. resources include: dictionaries, thesauri, wordnets, machine
translation tools, Web-based collaborative multilingual knowledge resources (resources
lexical knowledge manually collaboratively generated, e.g., Wikipedia).
study two multilingual lexical resources used sources translations: Google
Translate BabelNet. Google Translate statistical machine translation tool. Different machine translation systems exist could used; instance, rule-based systems,
e.g., Apertium (Apertium, 2015), statistical-based systems, e.g., UPC (Marioo et al.,
2006). used Google Translate previous work suggested performs better
Web translation services context concept mapping (Al-Kabi et al., 2013;
Oliver & Climent, 2012), adopted several matching systems including
ones evaluated OAEI (Shvaiko et al., 2014). Moreover, Google Translate generic
statistical machine translation, domain-independent system, covers large number
languages, including ones considered study. common evaluation measure
9. intuition used, example, cross-lingual similarity measure proposed support
matching lexical ontologies lexicalized different languages (Abu Helou & Palmonari, 2015).

182

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

Figure 2: Google Translate response Italian word tavola translated English
machine translation quality BLEU (Bilingual Evaluation Understudy) (Papineni,
Roukos, Ward, & Zhu, 2002), based n-gram precision model. Thus,
measure fit context word-to-word translation, case considering. comparison different machine translation tools scope study.
rich comprehensive comparison different machine translation tools refer
work Costa-jussa, Farrus, Marino, Fonollosa (2012).
BabelNet arguably largest state-of-the-art multilingual knowledge resource. BabelNet integrated several Web-based collaborative multilingual knowledge resources
(see Section 3.3). addition, makes different translation strategies available,
want evaluate indirectly study: sense-tagged sentence translation, direct machine
translation monosemous words, translations obtained Wikipedia Wordnet
mappings.
used Google Translate BabelNet construct bilingual dictionaries every
pairs non-English English languages considered study (see Section 6.2).
Google Translate service accessible API used translate
sentence-to-sentence word-to-word many pairs languages. Figure 2 shows Googles
word-to-word translation response JSON (2015) format Italian word tavola
translated English. Google returns preferable (common) translation trans
item. list possible translations also given dict item, part-of-speech
(P oS) tagged. translation word dict item reverse translation set
score. reverse translation set potential synonym words input word.
score estimates translation usage (e.g., common, uncommon, rare translations).
translation directions (e.g., It-to-En, En-to-It) machine translation tools
said different performance applied cross-lingual information retrieval
tasks (McCarley, 1999). ensure largest possible coverage compiled three bilingual
dictionaries using Google Translate taking account translation direction.
also collect translations provided reverse translation sets. best
knowledge available matching systems consider translations returned trans
item. pair non-English English languages considered gold standard
build following bilingual dictionaries: f romEn uses translations collected
English non-English words; toEn uses translations collected nonEnglish English words; merges translations collected two dictionaries
ensure largest possible coverage (with Google Translate). Observe f romEn
toEn subsets .
183

fiAbu Helou, Palmonari, & Jarrar

Table 1: Translation settings
Bilingual Dictionary
f romEn
toEn
MT
BN
BNcore
&BNcore
&BN

Description
translations English non-English words using Google Translate
translations non-English English words using Google Translate
union f romEn toEn
translations encoded BabelNet except translation Open Multilingual WordNet
BabelNet core synsets translations
union BNcore
union BN

BabelNet structured graph nodes. Nodes, called BabelNet synsets, represent
concepts named entities, lexicalized several languages. instance,
Italian lexicalizations node represent Italian synset, represents equivalent
synset corresponding English lexicalization, synset English WordNet.
translation given word source language (e.g., It) target language (e.g.,
En) BabelNet given every word target language, localizes
nodes lexicalized input word. example, Italian word tavola
lexicalization 15 nodes10 (14 concepts, 1 named entity). nodes provide 25
possible translations (lexicalization) English: {board, correlation table, place setting,
plank, setting, table, tablet, gang plank, wood plank, plate, table setting, stretcher bar,
Panel, Panel cartoon, Oil panel, ..., etc}. word lexicalization may derive
one many different lexical resources integrated BabelNet.
analyze impact different lexical resources integrated BabelNet,
extracted, every pair non-English English languages used study (see
Section 6.2), two bilingual dictionaries BabelNet synsets. first dictionary
extracted BabelNet core synsets (called BNcore ), contain multilingual lexicalizations built from: sense-tagged sentences, monosemous word translation using Google
Translate (monosemous words heuristic), Wikipedia inter-lingual links. second dictionary extracted BabelNet synsets (called BN ), synsets BabelNet, contain
multilingual lexicalizations built from: BNcore , lexicalization obtained WikiData,
Wikitionary, OmegaWiki, Wikipedia redirection links. Observe BNcore subset BN . excluded translations obtained Open Multilingual WordNet
(Bond & Foster, 2013), adopt gold standards study.
also merged translations BNcore dictionaries (called &BNcore ),
translations BN dictionaries (called &BN ). way
compare evaluate impact different Web-based multilingual resources, BabelNet
core synsets, machine translation tools cross-lingual mapping tasks.
bilingual dictionaries use study summarized Table 1.

6. Experiments
Three experiments conducted study coverage, correctness, impact two
multilingual lexical resources used sources translation mapping concepts
lexicalized different languages. Four non-English wordnets, mapped
English WordNet, used gold standards.
10. http://babelnet.org/search?word=tavola&lang=IT

184

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

Table 2: Size wordnets (gold standards) used experiments
Words
Word senses
Synsets

English
147306
206941
117659

Arabic
13866
23481
10349

Italian
40178
61588
33731

Slovene
39985
70947
42583

Spanish
36880
57989
38702

First, Section 6.2 describe details wordnets profile concepts
based lexicalizations. Then, Section 6.2, move perform experiments.
organize discussion experiments follows. Section 6.3.1 evaluate coverage correctness translations obtained different lexical resources
discuss impact retrieving candidate matches concept mapping tasks.
Section 6.3.2, evidence collected translations used baseline mapping selection
approach, i.e., majority voting, evaluate difficulty mapping selection task.
Section 6.3.3, analyze coverage translations relation position
concepts semantic hierarchies.
Finally, Section 6.4 summarize observations draw potential future
directions.
6.1 Experimental Setup
English, Arabic, Italian, Slovene, Spanish wordnets imported database.
wordnets database includes words, synsets, semantic relations, mappings
non-English wordnet English Wordnet. compiled different bilingual dictionaries (Table 1) Google Translate API BabelNet described Section
5. stored dictionaries database efficiently execute experiments.
6.2 Mapped Wordnets Used Gold Standards
study use wordnets English (Miller, 1995; Fellbaum, 1998), Arabic (Rodrguez
et al., 2008), Italian (Pianta et al., 2002), Slovene (Fiser, 2007) Spanish (GonzalezAgirre et al., 2012). wordnets provide high quality cross-lingual mappings contain
large inventories concepts. size terms words, word senses synsets
reported Table 211 . wordnets built using different approaches
cover different families languages: Germanic languages (e.g., English), Romance
languages (e.g., Italian Spanish), Slavic languages (e.g., Slovene), Semitic
languages (e.g., Arabic). Spanish, English, Arabic also among top five spoken
languages world (Wikipedia, 2015a), processing gathered significant
interest research community. Italian Slovene represent two minority languages.
Table 3 show distribution words wordnet disaggregated several
categories: considering word ambiguity, distinguish Monosemous words (M ),
words one sense (meaning), Polysemous words (P ), words
two senses. considering word complexity, distinguish Single words
(S), strings (lexemes) spaces hyphens, Collection words (C), strings
consist two simple words, connected spaces hyphens. also
11. Arabic, Italian, Slovene wordnets obtained OMWN (2015), Spanish wordnet
obtained MCR (2012). lexical gaps (synsets lexicalization) (Vossen, 2004) excluded.

185

fiAbu Helou, Palmonari, & Jarrar

Table 3: Word distribution gold standards category: quantity (percentage)
Words
onosemous(M )
P olysemous(P )
Simple(S)
Collection(C)
&S
&C
P &S
P &C

English
120433
(81.8)
26873
(18.2)
83118
(56.4)
64188
(43.6)
59021
(40.1)
61412
(41.6)
24097
(16.4)
2776
(01.9)

Arabic
10025
(72.3)
3841
(27.7)
8953
(64.6)
4913
(35.4)
5361
(38.5)
4664
(33.6)
3592
(26.0)
249
(01.8)

Italian
29816
(74.2)
10362
(25.8)
33133
(82.5)
7045
(17.5)
22987
(57.2)
6827
(17.0)
10146
(25.3)
218
(00.5)

Slovene
28635
(71.6)
11350
(28.4)
29943
(74.9)
10042
(25.1)
19223
(48.1)
9412
(23.5)
10720
(26.8)
630
(01.6)

Spanish
30106
(81.6)
6774
(18.4)
22630
(61.4)
14250
(38.6)
16212
(44.0)
13894
(37.7)
6418
(17.4)
356
(00.9)

Table 4: Synsets categories
Category

P
OW
MW
&OW
&M W
IX
P &OW
P &M W

Synset name
words Monosemous
words Polysemous
One-Word
Many-Words
Monosemous OW
Monosemous W
MIXed
Polysemous OW
Polysemous W

Definition synsets have...
monosemous words
polysemous words
one word (synonymless synset)
two synonym words (synonymful synset)
one word, also monosemous word
two synonym words, monosemous words
monosemous polysemous synonym words
one word, also polysemous word
two synonym words, polysemous words

consider four categories derived combining word ambiguity complexity
categories. example, tourism monosemous simple word (M &S), tabular
array monosemous collection word (M &C), table+ polysmouse simple
word (P &S), break up+ polysemous collection word (P &C).
Observation 1. vast majority collection words monosemous words:
average 1.3% words polysemous collection words across wordnets. means
word used concept label less likely ambiguous composite word
likely ambiguous simple word.
classify synsets based ambiguity number words (respectively first second, third fourth categories synsets described upper part
Table 4). combining orthogonal classifications, consider five categories
synsets described lower part Table 4. One observe &OW
&M W subsets . P &OW P &M W subsets
P , IX subsets W S. Examples synsets English
WordNet category shown Table 5. Table 6 describes, every wordnet,
total number percentage synsets grouped category.
Table 5: Synset examples categories English
Category
&OW

Example
{desk}

&M W
IX
P &OW
P &OW
P &OW

{tourism, touristry}
{table+ , tabular array}
{cocktail+ }
{cocktail+ }
{table+ }

P &M W

{board+ , table+ }

Definition
piece furniture writing surface usually drawers
compartments
business providing services tourists
set data arranged rows columns
short mixed drink
appetizer served first course meal
piece furniture smooth flat top usually supported
one vertical legs
food meals general

186

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

Table 6: Synset category-wise distribution gold standards: quantity (percentage)
Synsets

P
MWS
OWS
M&OWS
M&MWS
MIX
P&OWS
P&MWS

English
57415
(48.8)
41568
(35.3)
53784
(45.7)
63875
(54.3)
33596
(28.6)
23819
(20.2)
18676
(15.9)
30279
(25.7)
11289
(9.60)

Arabic
3381
(32.7)
4409
(42.6)
6162
(59.5)
4197
(40.5)
1995
(19.3)
1386
(13.4)
2559
(24.7)
2194
(21.2)
2215
(21.4)

Italian
14393
(42.7)
14641
(43.4)
13644
(40.4)
21084
(59.6)
10492
(31.1)
3901
(11.6)
5691
(16.9)
9609
(28.5)
4046
(12.0)

Slovene
17615
(41.4)
19609
(46.0)
14994
(35.2)
27589
(64.8)
14848
(34.9)
2767
(06.5)
5359
(12.6)
12741
(29.9)
6868
(16.1)

Spanish
19020
(49.1)
16269
(42.1)
14994
(38.7)
27589
(71.3)
14120
(36.5)
4900
(12.7)
3413
(08.8)
12005
(31.0)
4264
(11.0)

Table 7: Examples mappings Italian English synsets category
Synsets

M&OWS
{art
school}

M&MWS

M&OWS

{scuola
darte}

{radiostazione,
stazione
radio}

{radio
station}

M&MWS

{turismo} {tourism,
touristry}

{accoppiata,{exacta,
abperbinata}
fecta}

MIX

{minorit} {minority+
, nonage}

{biforcarsi,
ramificarsi,
diramarsi}

P&OWS

{forchetta}{fork+ }

{stretto,
vicino}

P&MWS

{chiudersi}{close+ ,
shut+ }

{inquietarsi, {care+ ,
allarworry+ }
marsi}

{branch,
fork+ ,
furcate,
ramify,
separate}
{close+ }

MIX

P&OWS

P&MWS

{tavolino,
banco+ ,
scrivania}
{docente+ ,
cattedratico,
professore}
{tavola+ ,
tabella}

{desk}

{ordinario+ {full
}
professor}

{entita+ , {entity}
cosa+ }

{prof,
professor}

{viaggiatore+{traveler,
}
traveller}

{classe+ ,
aula+ }

{table+ ,
tabular
array}

{contribuire+{conduce,
}
contribute,
lead+ }

{cibo+ ,
{repast,
pasto+ ,
meal+ }
+
mangiare
}

{poltrona+ ,
seggiola,
sedia}
{segnare+ ,
scalfire}

{chair +
}

{cosa+ }

{tavola+ ,
tavolo+ }

{score+ ,
mark+ ,
nock+ }

{moderare+ {chair+ ,
{cibo+ ,
}
moderate+ , vitto+ }
lead+ }

{thing + }

{classroom,
schoolroom}

{table+ }

{board+ ,
table+ }

Observation 2. Wordnets synonymless synsets (OW S) synonymful
synsets (MWS), 58.1% synsets being, average, synonymless. Arabic,
less OW W S, represents exception among considered wordnets.
particular, Arabic polysemous synsets (all P ) equally distributed OW
W S.
gold standards exist mappings synsets every category. Examples
mappings couple categories synsets Italian English shown
Table 7. percentage mapped synsets non-English wordnets
English WordNet, grouped category, reported Table 8.
results confirm languages cover number words noticed
Hirst (2004), and, hence, concepts shared different languages different ways
express meanings (i.e., belong different lexical categories). instance, 57%
Italian &OW synsets mapped monosemous synsets English (M &OW
&M W S). hand, 25% Italian &OW mapped polysemous
synsets English (P &OW P &M W S). percentage monosemous non-English
synsets mapped polysemous English synsets ranges 10% (Slovene)
30% (Arabic). percentage monosemous English synsets mapped
polysemous non-English synsets ranges 6% (Arabic) 14% (Italian). instance,
&OW Italian synsets {fotografare} {azioni ordinarie} mapped {shoot+ ,
snap+ , photograph+ } {common shares, common stock, ordinary shares}, respectively
P &M W &M W English synset.
187

fiAbu Helou, Palmonari, & Jarrar

Table 8: Distribution mapping category: percentage
M&OWS M&MWS MIX
English
Arabic
M&OWS 32.9
19.2
5.1
M&MWS 15.1
28.6
5.1
MIX
17.2
28.7
37.7
P&OWS 27.4
14.8
21.7
P&MWS 7.3
8.7
30.4
Slovene
M&OWS 23.4
25.2
14.2
M&MWS 47.8
39.7
13.0
MIX
18.1
27.5
48.7
P&OWS 7.1
4.0
8.4
P&MWS 3.5
3.7
15.7

P&OWS P&MWS M&OWS M&MWS MIX
Italian
5.4
2.3
36.2
20.9
10.6
2.5
1.5
21.2
34.9
10.3
15.5
22.6
17.8
27.2
38.7
57.3
29.5
17.9
10.7
18.4
19.4
44.2
6.9
6.3
22.0
Spanish
9.0
6.8
42.6
10.7
7.8
4.4
4.3
22.2
63.1
7.7
20.2
27.1
14.5
17.1
44.1
45.3
25.7
17.8
5.4
15.1
21.1
36.1
2.9
3.8
25.3

P&OWS P&MWS
9.4
4.6
22.5
43.0
20.5

4.1
2.8
26.8
29.0
37.4

8.4
3.3
19.4
48.5
20.4

3.1
1.9
24.2
25.9
44.9

Observation 3. Synsets different languages, equivalent meaning,
fall different synset categories. example, Italian monosemous synonymless synset
{forchetta} mapped polysemous synomymless synset {fork+ } English.
indicates monosemous word heuristic, adopted approaches
concept mapping multilingual knowledge construction, e.g., work presented Navigli
Ponzetto (2012), successful large number concepts fails still relevant
number concepts. average 19.3% non-English monosemous synsets mapped
English polysemous synsets gold standards, average 8.9% English monosemous synsets mapped non-English polysemous synsets gold standards.
details impact monosemous word heuristics provided Section 6.3.1,
translation correctness analyzed.
6.3 Results Discussion
section describe details three experiments presented study.
6.3.1 Experiment 1: Coverage Correctness Translations
Candidate Match Retrieval
order evaluate coverage translations obtained different lexical resources, use two measures. compute average word sense coverage across
words wordnet, word sense coverage defined individual word Eq.6.
compute average synset coverage across synsets wordnet, synset coverage (a boolean value) defined individual word Eq. 7. values normalized
range [0..100]. sake clarity simply refer measures word sense
synset coverage (at wordnet level).
Table 9 reports, wordnet, word sense synset coverage different translation settings. Synsets higher coverage word senses translation settings.
explained observation synset covered translation returns
least one word lexicalization equivalent synset target language (see
Eq.7).
observe machine translations non-English English (M toEn) achieve
higher word sense synset coverage machine translation English non-English
(M f romEn). instance, word sense coverage toEn 5.2 (Italian) 10.9
188

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

Table 9: Word sense synset coverages different translation settings
Translation
BNcore
BN
f romEn
toEn
MT
&BNcore
&BN

Arabic
Senses
Synsets
19.9
37.4
30.8
51.3
51.3
69.9
57.9
76.1
59.2
77.7
60.8
79.2
62.5
80.2

Italian
Senses
Synsets
40.0
62.5
51.7
72.8
60.2
81.9
65.4
83.9
68.1
87.6
69.8
89.0
72.2
89.9

Slovene
Senses
Synsets
28.8
44.2
35.9
52.0
40.2
60.0
49.6
67.2
53.8
72.4
55.8
74.2
57.5
75.2

Spanish
Senses
Synsets
33.9
44.7
39.8
49.0
56.1
67.8
67.0
77.0
69.4
79.7
71.5
81.3
72.3
81.7

(Spanish) percentage points higher f romEn, synset coverage toEn
2.0 (Italian) 9.2 (Spanish) percentage points higher f romEn.
Observation 4. Machine translation tools perform asymmetrically: toEn achieves
higher word sense synset coverage f romEn.
machine translation bilingual dictionary (M ), built union
machine translation directions (see Section 5), performs better dictionaries
built considering direction alone (i.e., f romEn toEn). Word
sense coverage average 2.7 8.2 percentage points higher toEn
f romEn, respectively. Synset coverage average 3.5 7.0 percentage
points higher toEn f romEn, respectively.
BNcore BN translation settings, based BabelNet, obtain lower coverage every machine translation setting wordnets. explained
limited coverage words occur non-English wordnets Wikipedia concepts
(which mostly cover named entities), incompleteness mappings used construct BabelNet (Navigli & Ponzetto, 2012). However, remarked that,
several languages, BabelNet also includes lexicalizations Open Multilingual
WordNet excluded study part gold standard (see
Section 6.2). means several well-known languages French, Germany,
Spanish, Italian12 expect much higher translation coverage BabelNet.
Still, best results obtained combining available translations, i.e.,
machine translation tool BabelNet, &BN . instance, &BN word sense
coverage average 3.5 percentage points higher . &BN synset coverage
average 2.4 percentage points higher .
also observe BN achieves considerably higher coverage BNcore ,
average difference word sense synset coverage 10.4 10.1 percentage points
respectively (BNcore subset BN - see Section 5). However, additional
coverage lost combining BNcore BN translations: &BN word
sense coverage average 1.7 percentage points higher &BNcore ,
&BN synset coverage average 0.8 percentage points higher &BNcore .
Observation 5. results highlight machine translation tools achieve higher
coverage BabelNet, integrates several Web-based multilingual resources (i.e.,
Wikitionary, OmegaWiki, WikiData, Wikipeida redirection links). However, integrating BabelNet machine translation tools still yields significant gain coverage, mostly
12. See Languages Coverage Statistics BabelNet (2012).

189

fiAbu Helou, Palmonari, & Jarrar

Table 10: Average synset coverage category
Synsets
BN

35.4
P
58.8
OW
44.5
MW
56.1
&OW 32.2
&M W 40.3
IX
59.8
P &OW 55.8
P &M W 61.9

Arabic
MT MT&BN
64.2
67.3
82.9
85.4
67.0
70.5
85.0
86.8
58.0
61.5
73.4
76.0
86.9
88.6
75.4
79.0
90.5
92.0

BN
68.6
69.0
64.1
81.0
63.8
81.6
80.7
71.0
80.8

Italian
MT MT&BN
86.0
88.4
80.5
83.0
79.5
82.2
93.6
95.2
83.5
86.1
92.6
94.5
94.3
95.8
83.3
86.4
93.7
95.1

BN
59.8
44.8
52.8
50.7
60.8
54.3
53.1
43.4
47.3

Slovene
MT MT&BN
78.8
80.8
65.5
69.1
70.1
73.0
76.6
79.1
78.3
80.3
81.4
83.2
76.5
79.0
60.5
64.5
74.7
77.6

BN
34.4
62.5
44.1
59.0
32.4
40.1
65.5
57.9
75.5

Spanish
MT MT&BN
78.1
79.5
80.2
83.0
75.8
78.0
87.7
89.3
74.7
76.2
88.0
89.0
85.8
87.6
77.1
80.2
88.8
90.9

Table 11: Average number candidate matches
Synsets
synonymless (OW S)
synonymful (M W S)

Arabic
48
124

Italian
17
49

Slovene
11
21

Spanish
27
75

BNcore (Wikipedia inter-lingual links, context based translations).
Table 10 reports average coverage synset category using BN, MT
&BN translation settings (the settings achieving highest coverage). results show
synonymful synsets (M W S) covered synonymless synsets (OW S)
every wordnet almost every translation setting. confirms intuition richer
concept lexicalizations help find least one correct translation using machine translation tools. Polysemous synsets (all P ) covered monosemous synsets
(all ) Arabic Spanish, less monosemous synsets (all ) Italian
Slovene. explained distribution polysemous monosemous synsets
synonymless synonymful synsets: monosemous synsets (all )
synonymless synsets, polysemous synsets (all P ) synonymful synsets.
IXed synsets covered synsets, since synonymful synsets,
combine monosemous polysemous words.
Observation 6. Synonymful synsets (M W S) covered synonymless
synsets (OW S) (see Table 10). However, higher coverage comes price larger
number candidate matches, thus making mapping selection task challenging
(see Table 11).
Observation 6 supported figures shown Table 11, reports average
number candidate matches synonymless vs. synonymful synsets. addition,
synonymful synsets contain least one polysemous word (see Table 6 ). Thus, one
expect sets candidate matches returned translations synonymful synsets
larger size, also noisier, translation polysemous words.
in-depth analysis difficulty mapping selection task different synset
categories provided Section 6.3.2. analysis confirm mapping selection
problem difficult synsets contain polysemous words, represent
majority synonymful synsets. time, joint translation synonym words
support mapping selection many synsets (e.g., synsets contain
190

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

Table 12: Average recall word-translation correctness category
Words

P

C
M&S
M&C
P&S
P&C

Words

20.2
53.1
38.1
13.3
26.7
12.7
55.0
25.3
29.3

BN
(63.6)
(38.0)
(48.3)
(63.4)
(63.0)
(65.2)
(37.7)
(46.6)
(50.8)


P

C
M&S
M&C
P&S
P&C


45.2
42.6
43.7
46.7
43.7
48.3
43.7
22.9
44.5

BN
(66.1)
(39.6)
(56.3)
(66.0)
(65.8)
(66.5)
(39.2)
(50.3)
(58.9)

Arabic
MT
45.1
(36.9)
83.3
(22.8)
67.0
(27.3)
35.1
(43.9)
54.8
(32.6)
34.0
(44.9)
85.2
(22.3)
55.4
(32.0)
55.7
(31.0)
Slovene
MT
63.6
(47.8)
73.0
(30.1)
67.0
(41.4)
64.3
(45.0)
62.5
(49.0)
66.0
(45.5)
75.0
(30.0)
38.9
(31.3)
66.3
(42.3)

MT&BN
48.0
(56.4)
85.2
(40.9)
70.0
(49.1)
37.0
(53.8)
58.6
(57.2)
35.8
(55.0)
87.0
(40.9)
58.2
(40.3)
58.3
(50.2)

49.0
71.5
54.4
56.9
46.7
56.9
71.8
56.0
54.8

BN
(65.6)
(44.9)
(57.0)
(65.8)
(65.3)
(66.3)
(44.8)
(48.6)
(58.6)

MT&BN
66.6
(60.8)
75.4
(33.7)
69.7
(49.9)
67.3
(60.3)
65.4
(60.7)
69.1
(61.1)
77.4
(33.5)
41.1
(39.9)
69.1
(52.4)

28.1
74.8
48.9
17.4
37.6
17.0
77.3
30.6
36.7

BN
(61.6)
(38.9)
(51.0)
(62.3)
(61.1)
(63.0)
(38.7)
(47.5)
(53.1)

Italian
MT
65.8
(47.0)
89.8
(31.9)
73.0
(41.0)
67.3
(47.5)
65.4
(46.7)
66.9
(47.9)
90.0
(31.7)
80.3
(39.0)
72.0
(42.1)
Spanish
MT
68.4
(48.1)
92.1
(28.4)
78.8
(41.0)
63.1
(48.4)
73.2
(47.6)
62.8
(48.6)
93.0
(27.9)
75.6
(38.8)
72.8
(43.5)

MT&BN
69.8
(62.1)
91.3
(45.1)
75.9
(55.5)
72.6
(62.8)
69.1
(61.6)
72.2
(63.5)
91.4
(45.1)
84.9
(44.1)
75.3
(56.8)
MT&BN
70.8
(56.9)
93.7
(41.4)
81.0
(53.3)
65.7
(53.5)
75.5
(59.1)
65.4
(53.9)
94.7
(41.5)
76.7
(40.3)
75.0
(53.3)

polysemous words, e.g., IXed synsets), means collect evidence deciding
mapping.
order evaluate correctness translations obtained different resources, use two measures. compute average word-translation correctness across
words wordnet; word-translation correctness defined individual word
Eq.5. addition, report average word-translation recall (recall, short), using
subformula Eq.513 .
Average recall word-translation correctness BN , &BN dictionaries, disaggregated word category, reported Table 12.
results show word-translation correct monosemous collection
words polysemous simple words. contrast, recall word-translation
higher polysemous words (P ) monosemous words (M ) every source
translation every wordnets, exception BN dictionary Slovene
wordnet. Recall word-translation also higher simple words (S) collection
words (C) every setting. observations explained monosemous words
usually less frequent domain-specific polysemous words. addition,
collection words also monosemous words - remarked Observation 1 -,
polysemous words simple words: recall correctness translations
simple words affected translation simple polysemous words. Translations
polysemous simple words return average larger word sets. sets likely
contain richer lexicalizations target language, also contain words
belong sense input words target language.
Observation 7. translation monosemous collection words average
correct translation polysemous simple words, achieves lower recall.
13. Word-translation correctness defined using formula based F1 -measure.

191

fiAbu Helou, Palmonari, & Jarrar

Focusing performance different sources translation, notice recall
higher BN , correctness BN higher . &BN
combines strengths dictionaries, i.e., higher recall word-translation
, higher correctness BN . instance, Table 12 notice
correctness word-translation improved 9.8 19.2 percentage points
Spanish Arabic respectively, add translations derived BN . recall
word-translation improved much 20.4 38.3 percentage points Italian
Spanish respectively, add BN translations derived . best results
thus obtained &BN , obtain recall (correctness) scores range
58.3%(50.2%) (Arabic) 75.3% (56.8%)(Italian). low recall Arabic
explained low recall translations monosemous collection words.
Observation 8. combination machine translation tools Web-based multilingual resources context-based sentence translations, like ones incorporated
BabelNet, improves recall, also correctness word-translations.
6.3.2 Experiment 2: Mapping Selection Difficulty
one hand, translations returned given synset used evidence
select mapping synset target language. hand, translations
many, polysemous words synset return several candidate matches,
incorrect, thus making mapping selection task difficult solve. experiment
analyzes difficulty mapping selection task performed candidate matches
retrieved translations obtained different lexical resources.
experiment use translations returned machine translation tool,
sake simplicity (with exception analysis synonym word coverage,
also include BN ). focus because, shown previous section,
higher coverage BN , widely used previous work ontology matching. addition, slight increase coverage obtained &BN ,
compared , ignored particular experiment.
perform analysis use greedy baseline method candidate mapping selection compare quality alignment computed method gold
standard alignment. baseline mapping selection method use majority voting top
evidence collected synset translations.
Mapping Selection Majority Voting. Every source synset translated using
synset translation function defined Eq.2. output represented multi set
union returned translations. word w(i) multi set, (i) word
frequency count, represents votes candidate matches contain w. Therefore,
candidate match source synset s, contains many words returned
translation s, receive votes likely target
selected mapping. Candidate matches ranked votes mapping containing
top-voted match selected.
happen several candidate matches receive equal number votes,
results tie. case, source synset mapping selection task undecidable;
contrast say mapping decidable unique candidate match receives
highest number votes. However, tie occurs among set top-voted candidate
192

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

100

TopOne

90

100

TopSet

Arabic
Slovene

90

80

80

70

70

60

60

50

50

40

40

30

30

20

20

10

10

0

Italian
Spanish

0

Arabic

Italian

Slovene

Spanish

(a) correct mappings found opOne
opSet settings

M-MWS

M-OWS

MIX

P-MWS

P-OWS

(b) correct mappings distinguishable candidate matches category

Figure 3: Correct mappings found baseline selection strategy
matches, valuable know set contains also correct mapping (w.r.t gold
standard) number candidate matches tie. fact, set top-voted
candidate matches also contains correct match, correct mapping could found
via user interaction relatively low effort. reasons use two settings
experiments majority voting candidate selection approach:
TopOne: exists unique top-voted candidate match source synset,
mapping containing match selected included alignment. tie
occurs, mapping selected.
TopSet: correct mapping selected oracle set top-voted
matches (no matter cardinality) included alignment.
quantify quality alignment compute (selection) correctness
percentage correct mappings returned selection setting set covered
mappings, i.e., mappings set candidate matches contains correct
mapping14 . words, TopOne setting, mapping considered correct source
synset, correct match synset (according gold standard)
unique top-voted candidate match; TopSet setting, mapping considered correct
source synset, whenever correct match synset included set
top-voted candidate matches. Observe every mapping counted correct
TopOne setting, also counted correct TopSet setting.
comparison performance terms correct mappings returned
opOne opSet selection settings wordnet shown Figure 3(a). average
correct mappings obtained opOne opSet settings 28% 50% respectively.
Based performance simple baseline methods, suggest translations
helpful mapping selection, although sophisticated methods make use
evidence devised. addition, number correct mappings increased
average 30 points case assume user select correct
mapping among set top-voted matches returned mapping selection method,
14. equivalent compute relative precision measure: Precision interpreted usual ontology
matching (Shvaiko & Euzenat, 2013) normalized range [0..100], evaluated
restricted subset gold standard. restricted subset consists mappings containing
source concepts covered translations

193

fiAbu Helou, Palmonari, & Jarrar

100
90
80
70
60
50
40
30
20
10
0

Arabic
Slovene

M-MWS

M-OWS

MIX

100
90
80
70
60
50
40
30
20
10
0

Italian
Spanish

P-MWS

Arabic
Slovene

M-MWS

P-OWS

(a)

M-OWS

MIX

Italian
Spanish

P-MWS

P-OWS

(b)

Figure 4: Percentage correct mappings synset category (a) opOne selection
(b) opSet selection

e.g., interactive ontology matching approach (Cruz, Loprete, Palmonari, Stroe, &
Taheri, 2014; Sarasua, Simperl, & Noy, 2012). However, average cardinality sets
top-voted matches (T opSet) high 49 synsets, makes difficult users
make decision.
Figure 3(b) shows, every wordnet every category source synset, percentage
correct mappings found using opOne selection total synset decidable
mappings. baseline opOne mapping selection strategy achieves remarkable performance monosemous synsets (i.e., &OW &M W S) poor performance
polysemous synsets. average, opOne selection capable select correct matches
much 88.2% monosemous synsets.
Figure 4(a) 4(b) show, every wordnet every category target synset,
percentage correct mappings found respectively opOne opSet selection
settings. figured mappings synsets polysemous words, particular
polysemous synonymless synsets (P &OW S), much likely undecidable, i.e.,
set many top-voted candidate matches found. fact, target synsets
P &OW S, mapping almost always undecidable opOne selection.
Observation 8. Evidence provided machine translation tools valuable successfully decide upon correct mappings monosemous synsets, fails support
decision polysemous synsets.
Observation 9. Mappings polysemous synonymless target synsets (P &OW S)
cannot successfully selected leveraging evidence translations
simple selection strategy like majority voting translations assign equal number
votes several candidate matches.
Observation 10. set top-voted candidate matches validated, e.g.,
opSet selection settings, possible find correct mapping vast majority
monosemous synsets (on average, 85%).
want investigate correct mappings likely found larger
small number top-voted mappings selected (with opSet selection). end,
analyze distribution correct mappings found opSet selection among topvoted candidate matches different size every wordnet. Correct mappings found
194

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

40

Arabic

Correct synsets (%)

35

Italian

Slovene

Spanish

30
25

20
15
10
5

Arabic
Italian

0

TopOne

2

3

4

5

6

7

8

9

10

Number candidate matches

Slovakian
Spanish

Figure 5: Percentage correct mappings vs. size top-voted candidate matches
opSet selection

Table 13: Synonym words coverage (%) synonymful synsets (MWS)
Translation
BN
MT
MT&BN

Arabic
51.9
68.9
71.3

Italian
59.8
68.5
72.6

Slovene
56.7
61.5
65.4

Spanish
61.2
74.4
77.5

sets top-voted candidate matches size ranges 1 238 candidates.
distribution plotted Figure 5: x-axis represents number selected top-voted
candidate matches (up size equal ten), y-axis represents percentage
found correct mappings. average, 28% correct mappings found unique
top-voted candidate match exists, i.e., like opOne selection settings (see Figure 3(a)).
instance, 4% correct mappings found sets top-voted mappings
contain four candidate matches, percentage represents absolute number
317, 1455, 991, 1328 synsets Arabic, Italian, Slovene, Spanish wordnets,
respectively.
Observation 11. Synsets occur targets mappings found opOne selection (decidable mappings) safely filtered candidate matches
source synsets, error estimated low 0.2% removing correct match.
Finally, analyze impact synonyms mapping selection task. Synonymful
synsets (i.e., &M W S, MIX, P &M W S) likely correctly mapped
opOne selection (Figure 4(a)) synonymless synsets (i.e., &OW P &OW S),
even average number candidate matches greater synonymful synsets
synonymless synsets (see Table 11). results confirm synonyms helpful
retrieving candidate matches - previously observed Observation 6 - also
selecting correct mappings: translation different words express
concept provide evidence decide best mapping concept.
Table 13 reports, every wordnet, synonym words coverage synonymful synsets
(M W S) using BN , &BN dictionaries (synonym words coverage defined
Eq.8). best results obtained &BN , synonym words coverage ranging 65.4% (Slovene) 77.5% (Spanish). Thus, average, two synonyms
translated correctly synonymful synsets.
195

fiAbu Helou, Palmonari, & Jarrar

50

BN

45

MT

40

MT&BN

MT

35

40

MT&BN

30

35
30

25

25

20

20

15

15

10

10

5

5

0

0

Arabic

Italian

Slovene

Arabic

Spanish

(a) Percentage W synsets fully covered
different translation settings

Italian

Slovene

Spanish

(b) Percentage correct mappings W
synsets found opOne selection

Figure 6: Synonymful synsets (M W S) whose synonym words fully covered

Figure 6(a) shows percentage synonymful synsets fully covered, i.e.,
synsets contain words correctly translated. average, dictionary fully covers greater percentage synonymful synsets BN , gain 18
points. best results obtained &BN average gain 6 points
respect . Although BN dictionary limited impact overall synsets coverage
(with gain 2.4 points, shown Experiment 2 ), BN improves synonym words coverage average 6 points, significant impact mapping selection
majority voting. instance, compared , &BN dictionary improves
percentage correct mappings opOne selection synonymful synsets
fully covered 4.6 points, shown Figure 6(b). Covering synonym words
belonging synonymful synset, improves synsets coverage, also makes
mapping selection step easier. Thus, integrating lexical resources translation
advantageous mapping selection tasks well.
Observation 12. synonymful synsets, larger number synonym words
covered translations, easier mapping selection task is.
6.3.3 Experiment 3: Coverage Correctness Translations vs.
Concept Specialization
recall synset covered none words equivalent synset
target language returned translation. words, synset
covered, correct match cannot found among set candidate matches found
translation. analysis helps exploration problem synset coverage
investigating 1) impact domain specificity synset coverage, 2) possibility
improving coverage expanding set found candidate matches synsets
similar ones retrieved translations.
investigate non covered synsets characterized extent based
specificity, use two different methods characterize specificity: domain labels
associated synsets WordNet Domains (Bentivogli, Forner, Magnini, & Pianta, 2004),
e.g., biology, animals, on; position synsets occupy semantic hierarchies,
e.g., synsets occur leaf nodes hypernym hierarchies.
196

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

70
60

Percentage

50
40
30
20
10
0

Arabic

Italian

Slovene

Spanish

Figure 7: Percentage domain specific synsets covered
consider synset associated domain label Wordnet domains
domain-specific, i.e., every label different Factoum (i.e., general, non specified
domain). every wordnet, percentage domain specific synsets covered
dictionary shown Figure 7. example, found that, average, 36%
non covered synsets dictionary labeled Factoum. rest
non covered synsets (64%) distributed different domains (with biology, animals, person, plants, geography frequent ones). findings consolidate
ones discussed Experiment 1 : monosemous words, often express specific
concepts, found less covered polysemous words, often express
general concepts.
Observation 13. Domain-specific concepts less coverage, machine translation
tools, general concepts.
intent, consider synsets covered translations distributed semantic hierarchy defined hypernym/hyponym relation.
context, consider leaf synsets (called Lsynsets) specific synsets, intermediate synsets (called Isynsets), i.e., synsets occurring positions hierarchy,
considered generic. consider subset synsets, i.e., nominal
synsets, whose hierarchical structure well-established English wordnet. particular, determine position source synset consider position equivalent
synset English WordNet, using mappings existing wordnets.
Figure 8(a) reports percentage Lsynsets Isynsets every wordnet.
notice wordnets leaf synsets intermediate synsets,
exception Arabic wordnet. exception explained strategy used
construction wordnet relatively small size. construction
Arabic wordnet (Rodrguez et al., 2008), based expand model paradigm
introduced EuroWordNet project (Vossen, 2004), initiated translation
core concepts English WordNet (Boyd-Graber, Osherson, & Schapire, 2006),
was, thereafter, extended concepts. core concepts (over 5 thousands) often
assumed common across different cultures languages, often intermediate
synsets.
Figure 8(b) reports percentage Lsynsets Isynsets covered
dictionary wordnet. average percentage nominal Lsynsets Isynsets
covered dictionary 21.1% 16.6%, respectively. Table 14 reports,
every wordnet, distribution nominal Lsynsets vs. Isynsets, grouped synset
197

fiAbu Helou, Palmonari, & Jarrar

100.0

30.0
25.0

Percentage

Percentage

80.0
60.0

L synsets
synsets

40.0
20.0

20.0

Arabic
Italian

15.0

Slovene

10.0

Spanish

5.0
0.0

0.0
English

Arabic

Italian

Slovene

L synsets

Spanish

(a)

synsets

(b)

Figure 8: Percentage Leaf synsets (Lsynsets) Intermediate synsets (Isynsets) (a)
gold standards (b) non-covered synsets

30.0

Percentage

25.0
20.0
15.0
10.0

5.0
0.0
Arabic

Italian

Slovene

Spanish

Figure 9: Neighbour synset coverage non-covered synset
category. notice Lsynsets likely covered Isynsets,
large number non-covered synsets consists synonymless synsets.
Moreover, would like evaluate if, non covered synsets, translations return
candidate matches least semantically similar equivalent synsets
target language. Neighbor synsets (i.e., hypernyms, hyponyms, siblings) usually
considered similar many wordnet graph based similarity measures (Navigli, 2009). Inspired work presented Resnik Yarowsky (1999), one could consider establishing
weighted mapping synset source language synsets target language,
weight represents degree similarity source target
synset. experiment define similar synsets one either hyponym/hypernym
siblings one. shown Figure 9, average percentage synsets
covered least one synset similar equivalent synset found
among candidate matches 20.1%. consistent intuition machine
translation systems provide translations (implicitly) capture coarse-grained
sense specification fine-grained sense specification encoded wordnets.
fact, observed WordNet sometimes fine-grained even human judges
agree (Hovy, Marcus, Palmer, Ramshaw, & Weischede, 2006).
Observation 14. significant percentage non covered synsets (20.1%, average), machine translation tools return synsets least similar equivalent
synsets target language.

198

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

Table 14: Distribution leaf intermediate (non-)covered synsets category
Synsets

M-MWS
M-OWS
MIX
P-MWS
P-OWS

Synsets

M-MWS
M-OWS
MIX
P-MWS
P-OWS


Arabic
Non-covered
Covered
Leaf
Inter
Leaf
Inter
9.1
16.0
35.0
39.9
14.3
27.1
27.0
31.6
3.7
8.5
19.6
68.2
2.7
5.2
17.9
74.2
8.8
13.1
19.7
58.4
7.8
14.4
23.5
54.3
Slovene
Non-covered
Covered
Leaf
Inter
Leaf
Inter
10.9
4.7
60.0
24.3
13.2
4.6
70.4
11.9
11.3
7.1
38.1
43.5
13.0
8.3
28.1
50.6
22.3
10.2
35.3
32.2
15.2
6.7
52.4
25.7

Italian
Non-Covered
Covered
Leaf
Inter
Leaf
Inter
5.3
1.5
71.6
21.5
11.4
4.0
60.3
24.3
4.1
2.1
44.6
49.3
4.0
2.6
34.4
59.0
11.3
5.5
44.1
39.0
8.6
3.6
52.1
35.7
Spanish
Non-Covered
Covered
Leaf
Inter
Leaf
Inter
8.0
1.1
82.5
8.4
17.8
4.2
64.9
13.1
9.6
2.4
48.7
39.3
7.1
3.3
28.3
61.3
14.1
7.0
37.0
41.8
13.8
4.2
56.9
25.0

Based observation, candidate match retrieval step modified
include among candidate matches also synsets similar ones retrieved translation. approach followed several cross-lingual ontology matching systems (Fu et al., 2012; Cruz, Palmonari, Caimi, & Stroe, 2013; Faria et al., 2014). However,
expanding set considered candidate matches disadvantage increasing
difficulty mapping selection task. results analyses suggest expansion
candidate matches set technique could applied particular categories source synsets, e.g., synonymless leaf synsets. could provide system (or
user, interactive matching settings) greater ability map synsets less
likely covered translations, without increasing number candidate matches
every source synset, e.g., synsets distinguishable monosemous candidate
matches (see Observation 8).
6.4 Lessons Learned & Future Works
section summarize main results findings study highlight
potential future directions.
general conclusion draw study machine translation tools
multilingual knowledge resources return useful translations large number
concepts. Thus, translations provide valuable support candidate match retrieval
cross-lingual ontology matching, covering minimum 75.2% maximum 89.9%
synsets four languages English considered study. consider
BabelNet also incorporates translations derived mappings Open Multilingual
Wordnet (Bond & Foster, 2013) (which excluded study
used gold standards), coverage expected even increase several
resource-rich languages covered wordnet. addition, experiments suggest
translations helpful, limited extent, selected categories synsets,
also mapping selection task.
Concisely, main results experiments suggest that:
199

fiAbu Helou, Palmonari, & Jarrar

monosemous concepts (i.e., concepts monosemous words) considered domain-specific;
combining lexical resources improves quality results;
machine translation tools perform poorer domain-specific concepts domainindependent ones;
synonymful synsets higher coverage synomymless synsets;
most, all, monosemous concepts mapped confidently even simple
selection methods (e.g., translation-based majority voting);
mappings involving polysemous synonymless synsets harder filter within
mapping selection task;
coverage synonym words (in synonymful synsets), easier
mapping selection task.
Compared previous systems, used machine translation tools considering
one translation direction, study built dictionaries cover translation
directions including reverse translations. technique shown significantly
improve coverage translations. practice, candidate matches found
larger number input concepts, thus increasing upper-bound recall cross-lingual
ontology matching systems. promising future research direction, one may try
improve coverage considering additional information available machine translation
tools like Google Translate (e.g., reverse translation synonym-like sets, part-of-speech tagged
translations, translation scores). additional information increase
upper-bound recall, also precision, adequately used matching selection step.
example, one may compare words returned reverse translations input
source synset, e.g., using translation-correctness measure (Eq.5). translation
higher translation-correctness could given higher weight selection step.
selection correct mapping set candidate matches still remains difficult
task, particular contextual knowledge cannot used disambiguate meaning
concepts. However, findings paper suggest several research directions
mitigate problem.
one hand, simple baseline selection method based majority voting used
experiments overcome sophisticated methods. example, recent
work, define lexical similarity measure based evidence collected translations
run local similarity optimization algorithm improve assignments
source target concepts (Abu Helou & Palmonari, 2015). future work, would
like leverage analysis mapping selection difficulty dependent lexical
characterisation source target concepts (e.g., polysemous vs. monosemous concepts,
synonymless vs. synonymful synsets) discussed paper. plan investigate
matching algorithms could adapt behavior based category source
synset candidate matches.
hand, cross-lingual mappings may still hard decide upon using
fully automatic approach. Thus, would like investigate cross-lingual ontology
200

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

matching domain, adoption semi-automatic matching methods. web application
could used solve difficult cross-lingual matching tasks, one proposed match
short service descriptions different languages (Narducci, Palmonari, & Semeraro, 2013).
Beyond this, interactive matching processes aggregate inputs given multiplicity
users, either experts (Cruz et al., 2014) crowd workers (Sarasua et al., 2012) seem
particularly promising large cross-lingual matching tasks. findings paper
particularly useful similar approaches help decide mappings
user inputs valuable (e.g., polysemous synonymless concepts). Overall
plan follow latter research directions use map model ease construction
lexical-semantic ontology context Arabic Ontology Project (Abu Helou
et al., 2014), also motivated study presented paper.

7. Conclusions
study investigated effectiveness automatic translations derived
state-of-the-art machine translation tool (Google Translate) state-of-the-art multilingual knowledge resource (BabelNet) support cross-lingual ontology mapping. perform
analysis used four large repositories cross-lingual mappings, include
mappings wordnets four different languages English WordNet. Effectiveness
automatic translations analyzed terms coverage correctness. One key contribution study, besides scale experiments, analysis effectiveness
automatic translations specific categories synsets.
example, found automatic translations achieve lower coverage domain
specific concepts. another example, found amount monosemous words
correctly translated polysemous words another language negligible:
cross-lingual ontology mapping methods use monosemous word heuristic may lead
include several wrong mappings alignment. coarse grain, analyses
suggest automatic translations capable covering large number word senses,
particular multilingual lexical resources (e.g., Google Translate BabelNet)
translation strategies (i.e., reverse translations Google Translate) integrated.
hand, automatic translations correct limited extent, least
compared translations derived manually mapped wordnets.
analyses discussed paper inspired definition cross-lingual similarity
measure lexical ontologies (Abu Helou & Palmonari, 2015). natural subsequent step
utilize study outcomes cross-lingual mapping systems. One promising
research direction define adaptive mapping methods different strategies
used depending lexical characterization source concepts. example, one
could integrate interactive mapping methods crowdsourcing approaches decide
subset mappings, estimated particularly difficult map. Another
research direction plan investigate method estimate concept ambiguity
small ontologies explicitly contain synonyms, e.g., matching
wordnets. method would help us use adaptive cross-lingual mapping methods
axiomatic ontologies lexically-poor data sources, e.g., web tables.
201

fiAbu Helou, Palmonari, & Jarrar

Acknowledgments
authors would like thank anonymous reviewers helpful comments
valuable suggestions, Pikakshi Manchanda help proofreading. work
supported part COMSODE project (FP7-ICT-611358) SIERA project (FP7INCO-295006). Corresponding author: Mamoun Abu Helou, E-mail: mamoun.abuhelou@
disco.unimib.it.

References
Abu Helou, M. (2014). Towards constructing linguistic ontologies: Mapping framework
preliminary experimental analysis. Proceedings Second Doctoral Workshop
Artificial Intelligence, Pisa, Italy. CEUR-WS.
Abu Helou, M., & Palmonari, M. (2015). Cross-lingual lexical matching word translation local similarity optimization. Proceedings 10th International
Conference Semantic Systems, SEMANTiCS 2015, Vienna, Austria, September.
Abu Helou, M., Palmonari, M., Jarrar, M., & Fellbaum, C. (2014). Towards building linguistic ontology via cross-language matching. Proceedings 7th International
Conference Global WordNet.
AGROVOC (2014).
Multilingual agricultural thesaurus.
vest-registry/vocabularies/agrovoc.

http://aims.fao.org/

Al-Kabi, M. N., Hailat, T. M., Al-Shawakfa, E. M., & Alsmadi, I. M. (2013). Evaluating English Arabic Machine Translation Using BLEU. International Journal Advanced
Computer Science Applications(IJACSA), 4 (1).
Apertium (2015). Open-source machine translation platform. http://www.apertium.org.
Apidianaki, M. (2009). Data-driven semantic analysis multilingual wsd lexical selection translation. Proceedings 12th Conference European Chapter
Association Computational Linguistics (ACL), EACL 09, pp. 7785, Stroudsburg, PA, USA. Association Computational Linguistics (ACL).
BabelNet (2012). large multilingual semantic network. http://babelnet.org.
Bentivogli, L., Forner, P., Magnini, B., & Pianta, E. (2004). Revising wordnet domains
hierarchy: Semantics, coverage balancing. Proceedings Workshop
Multilingual Linguistic Ressources, MLR 04, pp. 101108, Stroudsburg, PA, USA.
Association Computational Linguistics (ACL).
Bing (2016). Bing translate. http://www.bing.com/translator.
Birzeit (2011). Arabic Ontology. http://sina.birzeit.edu/ArabicOntology/.
Bond, F., & Foster, R. (2013). Linking extending open multilingual wordnet.
ACL (1), pp. 13521362. Association Computer Linguistics (ACL).
Bouma, G. (2010). Cross-lingual ontology alignment using eurowordnet wikipedia.
LREC. European Language Resources Association (ACL).
202

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

Boyd-Graber, J., Osherson, & Schapire, R. (2006). Adding dense, weighted connections
WordNet connections wordnet. Proceedings Third Global WordNet
Meeting.
Carruthers, P. (2002). cognitive functions language. Behavioral Brain Sciences,
25 (6), 657674.
CAT (2014).
Chinese Agricultural Thesaurus.
http://www.ciard.net/partners/
labof-chinese-agricultural-ontology-services.
COMSODE (2015). Components supporting open data exploitation. http://www.
comsode.eu/.
Costa-jussa, M. R., Farrus, M., Marino, J. B., & Fonollosa, J. A. R. (2012). Study
comparison rule-based statistical catalan-spanish machine translation systems.
Computing informatics, 31 (2), 245270.
Cruz, I. F., Palmonari, M., Caimi, F., & Stroe, C. (2013). Building linked ontologies
high precision using subclass mapping discovery. Artif. Intell. Rev., 40 (2), 127145.
Cruz, I., Loprete, F., Palmonari, M., Stroe, C., & Taheri, A. (2014). Pay-as-you-go multiuser feedback model ontology matching. Knowledge Engineering Knowledge
Management, Vol. 8876 Lecture Notes Computer Science, pp. 8096. Springer.
De Melo, G., & Weikum, G. (2009). Towards universal wordnet learning combined
evidence. Proceedings 18th ACM Conference Information Knowledge
Management, CIKM 2009, Hong Kong, China, November, pp. 513522. ACM.
De Melo, G., & Weikum, G. (2012). Constructing utilizing wordnets using statistical
methods. Language Resources Evaluation, 46 (2), 287311.
Djeddi, W. E., & Khadir, M. T. (2014). Xmap++: Results oaei 2014. Proceedings
9th International Workshop Ontology Matching co-located 13th
International Semantic Web Conference (ISWC 2014), October, Vol. 20, pp. 163169.
Els, L., & Veronique, H. (2010). Semeval-2010 task 3: Cross-lingual word sense disambiguation. Proceedings 5th International Workshop Semantic Evaluation.
Association Computational Linguistics (ACL).
EUROVOC (2015). EUs multilingual thesaurus. http://europa.eu/eurovoc.
Faria, D., Martins, C., Nanavaty, A., Taheri, A., Pesquita, C., Santos, E., Cruz, I. F., &
Couto, F. M. (2014). Agreementmakerlight results oaei 2014. Proceedings
9th International Workshop Ontology Matching co-located 13th
International Semantic Web Conference (ISWC 2014), October, Vol. 20, pp. 126134.
Fellbaum, C. (Ed.). (1998). WordNet Electronic Lexical Database. MIT Press,
Cambridge, MA; London.
Fiser, D. (2007). Leveraging parallel corpora existing wordnets automatic construction slovene wordnet. LTC, Vol. 5603 Lecture Notes Computer Science,
pp. 359368. Springer.
Fodor, J. (1975). Language Thought. Cambridge, MA: Harvard University Press.
203

fiAbu Helou, Palmonari, & Jarrar

Fu, B., Brennan, R., & OSullivan, D. (2009). Cross-lingual ontology mapping - investigation impact machine translation. ASWC, pp. 115.
Fu, B., Brennan, R., & OSullivan, D. (2012). configurable translation-based cross-lingual
ontology mapping system adjust mapping outcomes. J. Web Semantic, 15, 1536.
Gale, W. A., Church, K. W., & Yarowsky, D. (1992). Using bilingual materials develop
word sense disambiguation methods. Proceedings International Conference
Theoretical Methodological Issues Machine Translation.
Gonzalez-Agirre, A., Laparra, E., & Rigau, G. (2012). Multilingual central repository
version 3.0. LREC, pp. 25252529. European Language Resources Association
(ELRA).
Google (2015). Google Translate. https://translate.google.com/.
Gracia, J., Montiel-Ponsoda, E., Cimiano, P., Gomez-Perez, A., Buitelaar, P., & McCrae,
J. (2012). Challenges multilingual web data. Web Semantic, 11, 6371.
Hertling, S., & Paulheim, H. (2012). Wikimatch - using wikipedia ontology matching.
Ontology Matching, Vol. 946 CEUR Workshop Proceedings. CEUR-WS.org.
Hirst, G. (2004). Ontology lexicon. Handbook Ontologies Information
Systems. Springer.
Horrocks, I. (2008). Ontologies semantic web. Communications ACM, 51 (12),
5867.
Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., & Weischede, R. (2006). Ontonotes:
90% solution. Proceedings Human Language Technology Conference
NAACL. Companion Volume, Short Papers XX, NAACL 06. Association
Computational Linguistics (ACL), Morristown, NJ, USA.
Hovy, E., Navigli, R., & Ponzetto, S. P. (2012). Collaboratively built semi-structured content
artificial intelligence: story far. Artificial Intelligence.
Ide, N., Erjavec, T., & Tufis, D. (2002). Sense discrimination parallel corpora. Proceedings ACL-02 Workshop Word Sense Disambiguation: Recent Successes
Future Directions - Volume 8, WSD 02, pp. 6166, Stroudsburg, PA, USA. Association Computational Linguistics (ACL).
Jarrar, M. (2006). Position paper: Towards notion gloss, adoption linguistic
resources formal ontology engineering. Proceedings 15th International
Conference World Wide Web, pp. 497503, NY, USA. ACM.
Jarrar, M. (2011). Building formal arabic ontology (invited paper). Proceedings
Experts Meeting Arabic Ontologies Semantic Networks. Alecso, Arab League.
26-28 July.Tunis.
Jarrar, M., Yahya, A., Salhi, A., Abu Helou, M., Sayrafi, B., Arar, M., Daher, J., Hicks, A.,
Fellbaum, C., Bortoli, S., Bouquet, P., Costa, R., Roche, C., & Palmonari, M. (2014).
Arabization multilingual knowledge sharing- final report research setup.
SIERA Project 2.3 Deliverable.
204

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

Jimenez-Ruiz, E., Grau, B. C., Xia, W., Solimando, A., Chen, X., Cross, V., Gong, Y.,
Zhang, S., & Chennai-Thiagarajan, A. (2014). Logmap family results oaei 2014?.
Proceedings 9th International Workshop Ontology Matching co-located
13th International Semantic Web Conference (ISWC 2014), October, Vol. 20,
pp. 126134.
Jimenez-Ruiz, E., & Grau, B. C. (2011). Logmap: Logic-based scalable ontology matching. Semantic WebISWC 2011, pp. 273288. Springer.
Json-W3schools (2015). Javascript object notation. http://www.w3schools.com/json/.
Lefever, E., & Hoste, V. (2013). Semeval-2013 task 10: Cross-lingual word sense disambiguation. Proc. SemEval, 158166.
Liang, A. C., & Sini, M. (2006). Mapping agrovoc chinese agricultural thesaurus:
Definitions, tools, procedures. New Review Hypermedia Multimedia, 12 (1),
5162.
Lin, F., Butters, J., Sandkuhl, K., & Ciravegna, F. (2010). Context-based ontology matching: Concept application cases. 10th IEEE International Conference Computer Information Technology, CIT 2010, Bradford, West Yorkshire, UK, June
29-July 1, 2010, pp. 12921298.
Lin, F., & Krizhanovsky, A. (2011). Multilingual ontology matching based wiktionary
data accessible via sparql endpoint. CoRR, abs/1109.0732.
LOGD (2015).
Linking Open Government Data.
http://logd.tw.rpi.edu/
iogds-data-analytics. [Online; accessed March-2015].
Margolis, E., & Laurence, S. (2014). Concepts. Zalta, E. N. (Ed.), Stanford Encyclopedia Philosophy (Spring edition).
Marioo, J. B., Banchs, R. E., Crego, J. M., de Gispert, A., Lambert, P., Fonollosa, J. A. R.,
& Costa-jussa, M. R. (2006). N-gram-based machine translation. Comput. Linguist.,
32 (4), 527549.
McCarley, J. S. (1999). translate documents queries cross-language
information retrieval?. Proceedings 37th Annual Meeting Association
Computational Linguistics Computational Linguistics (ACL), ACL 99, pp.
208214, Stroudsburg, PA, USA. Association Computational Linguistics (ACL).
McCarthy, D., & Navigli, R. (2009). english lexical substitution task. Language Resources Evaluation, 43 (2), 139159.
MCR (2012). Multilingual Central Repository. http://adimen.si.ehu.es/web/MCR.
Meilicke, C., Garca-Castro, R., Freitas, F., van Hage, W. R., Montiel-Ponsoda, E.,
de Azevedo, R. R., Stuckenschmidt, H., Svab-Zamazal, O., Svatek, V., Tamilin, A.,
Trojahn, C., & Wang, S. (2012). Multifarm: benchmark multilingual ontology
matching. Web Semantics: Science, Services Agents World Wide Web,
15 (3).
Miller, G. A. (1995). Wordnet: lexical database english. Commun. ACM, 38 (11),
3941.
205

fiAbu Helou, Palmonari, & Jarrar

Miller, G. A., Leacock, C., Tengi, R., & Bunker, R. T. (1993). semantic concordance.
Proceedings workshop Human Language Technology, HLT 93, pp. 303308,
Stroudsburg, PA, USA. Association Computational Linguistics (ACL).
Mulwad, V., Finin, T., & Joshi, A. (2013). Semantic message passing generating linked
data tables. International Semantic Web Conference (1), Vol. 8218 Lecture
Notes Computer Science, pp. 363378. Springer.
Narducci, F., Palmonari, M., & Semeraro, G. (2013). Cross-language semantic matching
discovering links e-gov services lod cloud. KNOW@LOD, Vol. 992
CEUR Workshop Proceedings, pp. 2132. CEUR-WS.org.
Nastase, V., Strube, M., Boerschinger, B., Zirn, C., & Elghafari, A. (2010). Wikinet:
large scale multi-lingual concept network. LREC. European Language Resources
Association (ACL).
Navigli, R. (2009). Word sense disambiguation: survey. ACM Comput. Surv., 41 (2),
10:110:69.
Navigli, R., & Ponzetto, S. P. (2012). Babelnet: automatic construction, evaluation
application wide-coverage multilingual semantic network. Artificial Intelligence,
193 (0), 217 250.
OAEI (2015).
ontology alignment evaluation initiative.
ontologymatching.org.

http://oaei.

Oliver, A., & Climent, S. (2012). Building wordnets machine translation sense tagged
corpora. Proceedings 6th International Conference Global WordNet.
Omegawiki (2015). Wikimedia. http://www.omegawiki.org.
OMWN (2015). open multilingual wordnet. http://compling.hss.ntu.edu.sg/omw/.
Otero-Cerdeira, L., Rodrguez-Martnez, F. J., & Gmez-Rodrguez, A. (2015). Ontology
matching: literature review. Expert Systems Applications, 42 (2), 949 971.
OWL (2004). Web ontology language. http://www.w3.org/TR/owl-features/.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). Bleu: method automatic
evaluation machine translation. Proceedings 40th Annual Meeting
Association Computational Linguistics (ACL), ACL 02, pp. 311318, Stroudsburg,
PA, USA. Association Computational Linguistics (ACL).
Pianta, E., Bentivogli, L., & Girardi, C. (2002). Multiwordnet: developing aligned multilingual database. Proceedings First International Conference Global
WordNet.
Pinker, S. (1994). Language Instinct. Harper Perennial Modern Classics, New York.
Po, L., & Sorrentino, S. (2011). Automatic generation probabilistic relationships
improving schema matching. Information Systems, 36 (2), 192 208. Special Issue:
Semantic Integration Data, Multimedia, Services.
RDF (2014). Resource Description Framework. http://www.w3.org/RDF/.
RDFS (2014). RDF Schema. http://www.w3.org/TR/rdf-schema.
206

fiEffectiveness Automatic Translations Cross-Lingual Ontology Mapping

Resnik, P., & Yarowsky, D. (1999). Distinguishing systems distinguishing senses: new
evaluation methods word sense disambiguation. Natural Language Engineering,
5 (2), 113133.
Rodrguez, H., Farwell, D., Farreres, J., Bertran, M., Mart, M. A., Black, W., Elkateb, S.,
Kirk, J., Vossen, P., & Fellbaum, C. (2008). Arabic wordnet: Current state future
extensions. Proceedings Forth International Conference Global WordNet.
Sag, I., Baldwin, T., Bond, F., Copestake, A., & Flickinger, D. (2002). Multiword expressions: pain neck nlp. Computational Linguistics Intelligent Text
Processing, Vol. 2276 Lecture Notes Computer Science, pp. 115. Springer.
Sagot, B., & Fiser, D. (2008). Building free french wordnet multilingual resources.
Proceedings Sixth International Language Resources Evaluation (LREC),
Marrakech, Maroc.
Sarasua, C., Simperl, E., & Noy, N. (2012). Crowdmap: Crowdsourcing ontology alignment
microtasks. Semantic Web ISWC 2012, Vol. 7649 Lecture Notes
Computer Science, pp. 525541. Springer.
Saveski, M., & Trajkovski, I. (2010). Automatic construction wordnets using machine
translation language modeling. Proceedings Seventh Language Technologies Conference, 13th International Multiconference Information Society, volume C.
Shvaiko, P., & Euzenat, J. (2013). Ontology matching: State art future challenges.
IEEE Trans. Knowl. Data Eng., 25 (1), 158176.
Shvaiko, P., Euzenat, J., Mao, M., Jimenez-Ruiz, E., Li, J., & Ngonga, A. (Eds.). (2014).
Proceedings 9th International Workshop Ontology Matching collocated
13th International Semantic Web Conference (ISWC 2014), Riva del Garda,
Trentino, Italy, October 20, 2014, Vol. 1317 CEUR Workshop Proceedings. CEURWS.org.
Sorrentino, S., Bergamaschi, S., Gawinecki, M., & Po, L. (2010). Schema label normalization
improving schema matching. Data & Knowledge Engineering, 69 (12), 1254 1273.
Special issue 28th International Conference Conceptual Modeling (ER 2009).
Spelke, S. E. (2003). makes us smart? core knowledge natural language.
Language Mind: Advances Study Language Thought, pp. 277311.
Mit Press.
Spohr, D., Hollink, L., & Cimiano, P. (2011). machine learning approach multilingual cross-lingual ontology matching. Proceedings 10th International
Conference Semantic Web - Volume Part I, ISWC11, pp. 665680. Springer.
Suchanek, F. M., Kasneci, G., & Weikum, G. (2008). Yago: large ontology wikipedia
wordnet. Web Semantics: Science, Services Agents World Wide Web,
6 (3), 203217.
Tomaz, E., & Fiser, D. (2006). Building slovene wordnet. Proceedings 5th International Conference Language Resources Evaluation (LREC06), Genoa,
Italy.
207

fiAbu Helou, Palmonari, & Jarrar

Trojahn, C., Fu, B., Zamazal, O., & Ritze, D. (2014). State-of-the-art multilingual
cross-lingual ontology matching. Towards Multilingual Semantic Web, pp. 119
135. Springer.
Tufis, D., Cristea, D., & Stamou, S. (2004). Balkanet: Aims, methods, results perspectives. general overview. Special Issue BalkaNet. Romanian Journal Science
Technology Information.
Vincent, V. A. (2013). Macro- micro-averaged evaluation measures [[basic draft]].
Technical report.
Vossen, P. (2004). Eurowordnet: multilingual database autonomous languagespecific wordnets connected via inter-lingualindex. International Journal Lexicography, 17 (2), 161173.
Vossen, P., Eneko, A., Francis, B., Wauter, B., Axel, H., Amanda, H., Shu-Kai, H., Hitoshi,
I., Chu-Ren, H., Kyoko, K., Andrea, M., German, R., Francesco, R., Roxane, S., &
Maurizio, T. (2010). KYOTO: Wiki Establishing Semantic Interoperability
Knowledge Sharing Across Languages Cultures, pp. 265294. Springer.
Wikidata (2015). Wikimedia. http://www.wikidata.org.
Wikipedia (2015a). List languages number native speakers. http://en.wikipedia.
org/wiki/List-of-languages-by-number-of-native-speakers.
Wikipedia (2015b). Wikipedia. https://www.wikipedia.org.
Wikispecies (2015). Wikimedia. https://species.wikimedia.org.
Wiktionary (2015). Wikimedia. https://www.wiktionary.org.
WordNet-Princeton (2015). Semantically tagged glosses. http://wordnet.princeton.
edu/glosstag.shtml.
Zhang, Z. (2014). Towards efficient effective semantic table interpretation.
Semantic Web - ISWC 2014 - 13th International Semantic Web Conference, Riva del
Garda, Italy, October 19-23, 2014. Proceedings, Part I, pp. 487502.
Zhuge, H., Xing, Y., & Shi, P. (2008). Resource space model, owl database: Mapping
integration. ACM Transactions Internet Technology (TOIT), 8 (4), 20.

208

fiJournal Artificial Intelligence Research 55 (2016) 389-408

Submitted 07/15; published 02/16

Predicting Twitter User Demographics using Distant
Supervision Website Traffic Data
Aron Culotta
Nirmal Kumar Ravi

aculotta@iit.edu
nravi@hawk.iit.edu

Department Computer Science, Illinois Institute Technology
Chicago, IL 60616

Jennifer Cutler

jcutler2@stuart.iit.edu

Stuart School Business, Illinois Institute Technology
Chicago, IL 60616

Abstract
Understanding demographics users online social networks important applications health, marketing, public messaging. Whereas prior approaches
rely supervised learning approach, individual users labeled demographics training, instead create distantly labeled dataset collecting audience
measurement data 1,500 websites (e.g., 50% visitors gizmodo.com estimated
bachelors degree). fit regression model predict demographics
information followers website Twitter. Using patterns derived
textual content social network user, final model produces
average held-out correlation .77 across seven different variables (age, gender, education,
ethnicity, income, parental status, political preference). apply model
classify individual Twitter users ethnicity, gender, political preference, finding
performance surprisingly competitive fully supervised approach.

1. Introduction
Social media increasingly used make inferences real world, application politics (OConnor, Balasubramanyan, Routledge, & Smith, 2010), health (Dredze,
2012), marketing (Gopinath, Thomas, & Krishnamurthi, 2014). Understanding demographic makeup sample social media users critical progress
area, allows researchers overcome considerable selection bias uncontrolled
data. Additionally, capability help public messaging campaigns ensure
target demographic reached.
common approach demographic inference supervised classification
training set annotated users, model fit predict user attributes content
writings (Argamon, Dhawle, Koppel, & Pennebaker, 2005; Schler, Koppel, Argamon,
& Pennebaker, 2006; Rao, Yarowsky, Shreevats, & Gupta, 2010; Pennacchiotti & Popescu,
2011; Burger, Henderson, Kim, & Zarrella, 2011; Rao, Paul, Fink, Yarowsky, Oates, &
Coppersmith, 2011; Al Zamal, Liu, & Ruths, 2012). approach number limitations: collecting human annotations costly error-prone; many demographic variables
interest cannot easily labeled inspecting profile (e.g., income, education level);
restricting learning small set labeled profiles, generalizability classifier
c
2016
AI Access Foundation. rights reserved.

fiCulotta, Ravi, & Cutler

limited. Additionally, past work focused text primary source evidence,
making little use network evidence.
paper, fit regression models predict seven demographic variables Twitter
users (age, gender, education, ethnicity, income, parental status, political preference)
based follow content tweets. Rather using
standard supervised approach, construct distantly labeled dataset consisting web
traffic demographic data Quantcast.com. pairing web traffic demographics
site followers site Twitter.com, fit regression model set
Twitter users expected demographic profile. evaluate accuracy
recovering Quantcast statistics well classifying individual Twitter users.
experiments investigate several research questions:
RQ1. demographics set Twitter users inferred network
information alone? find across seven demographic variables average heldout correlation .73 web traffic demographics website
predicted regression model based sites Twitter followers. learn,
example, high-income users likely follow Economist young
users likely follow PlayStation.
RQ2. revealing demographics: social network features linguistic features? Overall, find text-based features slightly predictive
social network features (.79 vs. .73), particularly income age variables.
RQ3. regression model extended classify individual users? Using
hand-labeled validation set users annotated gender, ethnicity, political
preference, find distantly trained regression model provides classification
accuracy competitive fully-supervised approach. Averaged across three
classification tasks, approaches obtain F1 score 81%.
RQ4. much follower linguistic information needed prediction?
find identities 10 friends per user, chosen random, sufficient
achieve 90% accuracy obtained using 200 friends. Accuracy using linguistic
features begins plateau 2,000 unique terms observed per user.
remainder paper, first review related work, describe data
collected Twitter QuantCast feature representation used task; next,
present regression classification results; finally, conclude outline directions
future work.1

2. Related Work
Predicting attributes social media users growing area interest, recent work
focusing age (Schler et al., 2006; Rosenthal & McKeown, 2011; Nguyen, Smith, & Ros,
2011; Al Zamal et al., 2012), gender (Rao et al., 2010; Burger et al., 2011; Liu & Ruths,
1. Replication code data available here: https://github.com/tapilab/jair-2016-demographics.

390

fiPredicting Twitter User Demographics

2013), race/ethnicity (Pennacchiotti & Popescu, 2011; Rao et al., 2011), personality (Argamon et al., 2005; Schwartz, Eichstaedt, Kern, Dziurzynski, Ramones, Agrawal, Shah, Kosinski, Stillwell, Seligman, et al., 2013), political affiliation (Conover, Goncalves, Ratkiewicz,
Flammini, & Menczer, 2011; Barbera, 2013; Volkova & Van Durme, 2015), occupation (Preotiuc-Pietro, Lampos, & Aletras, 2015). work predicts demographics
web browsing histories (Goel, Hofman, & Sirer, 2012). majority approaches
rely hand-annotated training data, require explicit self-identification user,
limited coarse attribute values (e.g., 25-years-old).
Distantly supervised learning (also called lightly weakly supervised learning) provides alternative standard supervised learning relies less individual annotated
examples, instead bootstrapping models declarative constraints. Previous work
developed methods train classifiers prior knowledge label proportions (Jin & Liu,
2005; Musicant, Christensen, & Olson, 2007; Quadrianto, Petterson, & Smola, 2009a; Liang,
Jordan, & Klein, 2009; Ganchev, Graca, Gillenwater, & Taskar, 2010; Mann & McCallum,
2010; Zhu, Chen, & Xing, 2014) prior knowledge features-label associations (Schapire,
Rochery, Rahim, & Gupta, 2002; Druck, Mann, & McCallum, 2008; Melville, Gryc, &
Lawrence, 2009). addition standard document categorization tasks, lightly supervised approaches applied named-entity recognition (Mann & McCallum, 2010;
Ganchev & Das, 2013; Wang & Manning, 2014), dependency parsing (Druck, Mann, &
McCallum, 2009; Ganchev, Gillenwater, & Taskar, 2009), language identification (King &
Abney, 2013), sentiment analysis (Melville et al., 2009).
Chang, Rosenn, Backstrom, Marlow (2010) propose related distantly supervised
approach demographic inference, inferring user-level ethnicity using name/ethnicity distributions provided U.S. Census; however, approach uses evidence first
last names, often available, thus appropriate populationlevel estimates. Oktay, Firat, Ertem (2014) extend work Chang et al. (2010)
also include statistics first names. Rao et al. (2011) take similar approach, also
including evidence linguistic features infer gender ethnicity Facebook
users; evaluate fine-grained ethnicity classes Nigeria use limited
training data. recently, Mohammady Culotta (2014) trained ethnicity model
Twitter using county-level supervision, which, like approach, uses distant source
supervision build model individual demographics.
several studies predicting population-level statistics social media.
Eisenstein, Smith, Xing (2011) use geolocated tweets predict zip-code statistics
race/ethnicity, income, variables using Census data; Schwartz et al. (2013)
Culotta (2014) similarly predict county health statistics Twitter. However, none
prior work attempts predict evaluate user level.
primary methodological novelties present work use web traffic data
form weak supervision use follower information primary source
evidence. Additionally, work considers larger set demographic variables
prior work, predicts much fine-grained set categories (e.g., six different age
brackets instead two three used previously).
paper extends initial version work (Culotta, Kumar, & Cutler, 2015).
novel contributions include following: (1) added additional demographic variable (political preference); (2) added additional labeled dataset
391

fiCulotta, Ravi, & Cutler

evaluation (also political preference); (3) whereas initial version used friend
features, introduced textual features 9M tweets; (4) included
new analysis accuracy varies number terms collected per user.
also reproduced prior results using newly collected data QuantCast.com;
since QuantCast demographic statistics changed time, overall correlations
reported deviate slightly reported original version, trends
qualitative conclusions remain same.

3. Data
section describe various data collected experiments.
3.1 Quantcast
Quantcast.com audience measurement company tracks demographics visitors millions websites. accomplished part using cookies track
browsing activity large panel respondents (Kamerer, 2013). writing,
estimated demographics large number websites publicly accessible
searchable web interface.
sampled 1,532 websites Quantcast downloaded statistics seven demographic variables:
Gender: Male, Female
Age: 18-24, 25-34, 35-44, 45-54, 55-64, 65+
Income: $0-50k, $50-100k, $100-150k, $150k+
Education: College, College, Grad School
Children: Kids, Kids
Ethnicity: Caucasian, Hispanic, African American, Asian
Political Preference: Democrat, Republican
variable, Quantcast reports estimated percentage visitors website
given demographic.
3.2 Twitter
website collected previous step, executed script search Twitter
account, manually verified it; 1,066 accounts original set 1,532 found.
assumption work demographic profiles followers website
Twitter correlated demographic profiles visitors website.
undoubtedly biases introduced (e.g., Twitter users may skew younger web
traffic panel), aggregate differences limited impact final model.
represent 1,066 Twitter accounts feature vectors derived
information followers. Below, describe features based social
network linguistic content followers tweets.
392

fiPredicting Twitter User Demographics

h+p://www.lifehacker.com*
73%*Male*
19%*Graduate*School*
30%*$50A100k*
12%*Hispanic*

lifehacker*

A*

B*

lifehackers*Neighbor*Vector*
C*
D*
2*/*2*=*1.0*

1*/*2*=*0.5*

C*

D*
Neighbors*

Figure 1: Data model. collect QuantCast demographic data website,
construct Neighbor Vector Twitter connections website,
based proportion websites followers friends
neighbor.

3.2.1 Friend Features
Recall Twitter users allowed follow accounts, introduces asymmetric relationship users. X follows , say friend X (though
reverse may true). account, queried Twitter REST API sample 300 followers, using followers/ids request. sample necessarily
uniform. Twitter API documentation states time, results ordered
recent following first however, ordering subject unannounced
change eventual consistency issues.
followers, collected 5,000 accounts follow,
called friends, using friends/ids API request. Thus, original accounts
Quantcast, (300 5K = 1.5M ) additional accounts two hops
original account (the friend follower). refer discovered accounts
neighbors original Quantcast account. course, many accounts
duplicates, two different followers follow many accounts (i.e., triadic
closure) indeed, core assumption number duplicates represents
strength similarity neighbors.
393

fiCulotta, Ravi, & Cutler

105

number unique neighbors

107

number neighbor links

count

count

106
104

105
104

103 0
10

101

rank

102

103

103 0
10

101

rank

102

103

Figure 2: Rank-order frequency plots number neighbors per account number links neighbors per account.

original accounts, compute fraction followers friends
neighbors store neighbor vector. Figure 1 shows example.
Suppose Quantcast account LifeHacker two followers B; follows
C, B follows C D. neighbor vector LifeHacker {(C, 1), (D, .5)}.
suggests LifeHacker stronger relationship C D.2 example,
data top neighbors LifeHacker EA (the video game developer), GTASeries (a
video game fan site), PlayStation.
resulting dataset consists 1.7M unique neighbors original 1,066 accounts.
reduce dimensionality, removed neighbors fewer 100 followers, leaving
46,649 unique neighbors total 178M incoming links. Figure 2 plots number
unique neighbors per account well number neighbor links per account.
3.2.2 Text Features
addition neighbor vector, created analogous vector based tweets
followers account. collected recent 200 tweets 300
followers 1,066 accounts using statuses/user timeline API request.
tweet, perform standard tokenization, removing non-internal punctuation,
converting lower case, maintaining hashtag mentions. URLs collapsed
single feature type, digits (e.g., 12 mapped 99; 123 mapped 999).
Characters repeated twice converted single occurrence. Terms used
fewer 20 different users removed.
resulting dataset consists 9,427,489 tweets containing 112,642 unique terms written 59,431 users. original 1,066 accounts, create text vector similar
previous section. value represents proportion followers account
use term. E.g., xij = .1 indicates 10% 300 followers account use
term j.
2. Note use friends rather followers, since friend links created thus
likely indicate interests.

394

fiPredicting Twitter User Demographics

4. Analysis
section, report results predicting demographics aggregate level (i.e.,
demographics accounts followers) user level (i.e., individual Twitter
users demographic profile).
4.1 Regression
Quantcast site, pair demographic variables friend text feature
vectors construct regression problem. Thus, attempt predict demographic
profile followers Twitter account based friends followers
content tweets.
Due high dimensionality (46,649 friend features 112,642 text features)
small number examples (1,066), use elastic net regularization (Zou & Hastie, 2005),
combines L1 L2 penalties. Furthermore, since output variable consists
dependent categories (e.g., age brackets), use multi-task variant elastic net
ensure features selected L1 regularizer category. use
implementation MultiTaskElasticNet scikit-learn (Pedregosa et al., 2011).
Recall standard linear regression selects coefficients minimize squared error
list training instances {xi , yi }N
i=1 , feature vector xi expected output yi .
argmin

N
1 X
(yi xi )2
N i=1

Lasso imposes L1 regularizer , ridge regression imposes L2 regularizer
. Elastic net combines penalties:
argmin

N
1 X
(yi xi )2 + 1 ||||1 + 2 ||||22
N i=1

1 2 control strength L1 L2 regularizers, respectively. L1
regularizer encourages sparsity (i.e., many 0 values ), L2 regularizer prevents
values becoming large.
Multi-task elastic net extends standard elastic net groups related regression problems (Obozinski & Taskar, 2006). E.g., case, would like account fact
regressions College, College, Grad School related; thus,
would like sparse solutions similar across tasks (that is, L1 select
features task).
(1)
(M )
Let (j) coefficients task j, let k = (k . . . k )T vector
coefficients formed concatenating coefficients kth feature across tasks.
multi-task elastic net objective enforces similar features selected across tasks:
argmin

Nj

X
1 X

N
j=1 j

(j)

(j)

(yi (j)T xi )2 +

i=1

1

p
X
k=1

395

||k ||1 + 2 ||||22

fiCulotta, Ravi, & Cutler

Democrat
60

Politics

r =0.84

r =0.85

40

40

Predicted Value (%)

20 40 60

10 20 30 40 50

18-24

20
10
10 20 30 40
Caucasian
r =0.86

20 40 60 80

0

20 40 60 80
35-44

20

Age

r =0.61

20

Hispanic

African American
r =0.89

20 40

60
40
20
0

45-54

55-64

15
10
5
0

r =0.68

0

0 10 20 30 40

True Value (%)

r =0.74

20

10

10
10

r =0.78

r =0.69

15
10
5
0
0

0

10 20

50

20

40

r =0.64

30 40 50 60

r =0.76

10 20 30 40
Kids

70
60
50
40
30

r =0.72

20 40 60

College

60

$150k+

Family

Education

40
20 40 60

20

65+

College
60

30

20

0 5 10 15 20

Asian

20
10

20 40 60

r =0.77

10 20

Ethnicity

r =0.79

20 30 40

r =0.77

$100-150k

r =0.69

20 40 60 80

10
10 20 30 40

40

20

20

40

Income

$50-100k

30

40

25
20
15

40
30
20
10

80
60
40
20

r =0.80

60

r =0.75

40
30
20
10

$0-50k

r =0.91

100

25-34

r =0.81

30

Male

50

20

20

Gender

Republican

Grad School
40
30
20
10

r =0.78

10 20 30 40 50

Figure 3: Scatter plots true demographic variables Quantcast versus predicted using elastic net regression fit friend text features Twitter.
predictions computed using five fold cross-validation; panel also
reports held-out correlation coefficient (r).

Nj number instances task j p number features.
fit three versions model using three different feature sets: Friends, Text,
Friends+Text. tuned regularization parameters held-out set 200 accounts
Gender prediction, setting scikit-learn parameters l1 ratio=0.5 model,
alpha=1e5 Friends model, alpha=1e2 Text Friends+Text models.
4.1.1 Regression Results
perform five-fold cross-validation report held-out correlation coefficient (r)
predicted true demographic variables. Figure 3 displays resulting
scatter plots 21 categories 7 demographic variables.
see overall correlation strong: .77 average, ranging .61
35-44 age bracket .91 Male. correlation coefficients significant
using two-tailed t-test (p < 0.01), Bonferroni adjustment 21 comparisons.
results indicate neighbor text vectors provides reliable signal
demographics group Twitter users. put correlation value context,
indirect comparison, Eisenstein et al. (2011) predict ethnicity proportions ZIP
Code using Twitter data obtain maximum correlation .337 (though data
experimental setup differ considerably).
396

fiPredicting Twitter User Demographics

Category
Gender
Age

Income

Politics

Value
Male
Female
18-24
25-34
35-44
45-54
55-64
65+
$0-50k
$50-100k
$100-150k
$150k+
Democrat
Republican

Education

Children

Ethnicity

College
College
Grad School
Kids
Kids
Caucasian
Hispanic
Afr. Amer.
Asian

Top Accounts
AdamSchefter, SportsCenter, espn, mortreport, WIRED
TheEllenShow, Oprah, MarthaStewart, Pinterest, Etsy
IGN, PlayStation, RockstarGames, Ubisoft, steam games
azizansari, lenadunham, mindykaling, WIRED
cnnbrk, BarackObama, AP, TMZ, espn
FoxNews, cnnbrk, AP, WSJ, CNN
FoxNews, cnnbrk, AP, ABC, WSJ
FoxNews, AP, WSJ, cnnbrk, DRUDGE REPORT
YouTube, PlayStation, IGN, RockstarGames, Drake
AdamSchefter, cnnbrk, SportsCenter, espn, ErinAndrews
WSJ, espn, AdamSchefter, SportsCenter, ErinAndrews
WSJ, TheEconomist, Forbes, nytimes, business
BarackObama,
Oprah,
NewYorker,
UncleRUSH,
MichelleObama
FoxNews,
michellemalkin,
seanhannity,
megynkelly,
DRUDGE REPORT
YouTube, PlayStation, RockstarGames, Xbox, IGN
StephenAtHome, WIRED, ConanOBrien, mashable
nytimes, WSJ, NewYorker, TheEconomist, washingtonpost
NewYorker, StephenAtHome, nytimes, maddow, pitchfork
parenting,
parentsmagazine,
HuffPostParents,
TheEllenShow, thepioneerwoman
jimmyfallon,
FoxNews,
blakeshelton,
TheEllenShow,
TheOnion
latimes, Lakers, ABC7, Dodgers, KTLA
KevinHart4real, Drake, Tip, iamdiddy, UncleRUSH
TechCrunch, WIRED, BillGates, TheEconomist, SFGate

Table 1: Accounts highest estimated coefficients category.
examine results, Table 1 displays features 5 largest coefficients per class according regression model fit using friend features. Many
results match common stereotypes: sports accounts predictive men (AdamShefter
MortReport ESPN reporters), video game accounts predictive younger users
(IGN video gaming media company), financial news accounts predictive greater
income, parenting magazines predictive users children. also
appear geographic effects, California-related accounts highly weighted
Hispanic Asian categories. seems good city-level resolution
Los Angeles accounts (latimes, Lakers) strongly correlated Hispanic users,
whereas San Francisco accounts (SFGate, SFist, SFWeekly) strongly correlated
Asian users. seem selection bias, one must use caution
interpreting results. example, BillGates predictive Asian users,
part California many Asian-Americans part California
strong technology sector.
397

fiCulotta, Ravi, & Cutler

Category
Gender
Age

Value
Male
Female
18-24
25-34
35-44
45-54
55-64
65+

Income

$0-50k
$50-100k
$100-150k

Politics

$150k+
Democrat
Republican

Education

College
College
Grad School

Children

Kids
Kids

Ethnicity

Caucasian
Hispanic
Afr. Amer.
Asian

Top Terms
film, guy, gay, man, fuck, game, team, internet, review, guys
hair, her, omg, family, girl, she, girls, cute, beautiful, thinking
d, haha, album, x, xd, :, actually, stream, wanna, im
super, dc, baby, definitely, nba, pregnancy, wedding, even,
entire, nyc
star, fans, kids, tv, bike, mind, store, awesome, screen, son
wow, vote, american, comes, ca, santa, county, boys, nice,
high
vote, golf, red, american, country, north, county, holiday,
smile, 99,999
vote, golf, @foxnews, holiday, may, american, he, family,
north, national
lol, games, @youtube, damn, black, ps9, side, d, community,
god
great, seattle, he, performance, lose, usa, kansas, iphone,
wow, cold
santa, flight, nice, looks, practice, congrats, bike, dc, retweet,
ride
dc, nyc, market, @wsj, congrats, beach, san, york, ca, looks
women, u, aint, nyc, equality, la, voice, seattle, dc, @nytimes
@foxnews, christmas, #tcot, football, county, morning,
family, christians, country, obamas
lol, games, put, @youtube, county, made, ps9, xbox, videos,
found
our, youre, seattle, photo, @mashable, la, apple, fashion,
probably, san
dc, @nytimes, market, which, review, excellent, boston, also,
congrats, @washingtonpost
care, street, gay, years, health, drink, dc, white, ht, album
kids, school, child, family, kid, daughter, children, utah,
moms, parents
christmas, fun, dog, country, st, could, luck, guy, florida, john
la, los, san, el, angeles, california, ca, lol, l.a, lakers
black, lol, bout, aint, brown, lil, african, blessed, smh, atlanta
chinese, la, sf, san, china, korea, india, bay, vs, hi

Table 2: Terms highest estimated coefficients category.

Additionally, Table 2 shows top 10 terms category text-only model.
text features follow many trends friend features, perhaps
greater level granularity. terms self-evident, highlight here:
xd emoticon laughing used among young users, often separated spaces
(thus tokens x appearing separately 18-24 bracket); smh stands
398

fiPredicting Twitter User Demographics

Model
multi-task elastic net
elastic net
ridge

Friends
.73
.72
.62

Text
.79
.78
.79

Friends + Text
.77
.76
.78

Average
.76
.75
.73

Table 3: Average held-out correlation across demographic variables three competing
regression models.

shaking head, expression disbelief predictive African American users;
hi abbreviation state Hawaii, large Asian population.
Finally, compare multi-task elastic net single-task variant elastic net
ridge regression (with regularization parameters tuned before). Table 3 shows
three methods mostly produce comparable accuracy, exception friends
features, ridge regression performs substantially worse others.
4.2 Classification
regression results suggest proposed model accurately characterize
demographics group Twitter accounts. section, provide additional validation manually annotated Twitter accounts investigate whether model
accurately predict demographics individual users.
4.2.1 Labeled Data
Many demographic variables difficult label individual level e.g., income education level rarely explicitly mentioned either profile tweet. Indeed,
advantage approach aggregate statistics readily available
many demographics interest difficult label individual level. validation purposes, focus three variables fairly reliably labeled individuals:
gender, ethnicity, political preference.
gender ethnicity data originally collected Mohammady Culotta
(2014) follows: First, used Twitter Streaming API obtain random sample
users, filtered United States (using time zone place country code
profile). six days worth data (December 6-12, 2013), sampled 1,000 profiles
random categorized analyzing profile, tweets, profile image
user. categorized 770 Twitter profiles one four ethnicities (Asian, African
American, Hispanic, Caucasian). ethnicity could determined
discarded (230/1,000; 23%).3 category frequency Asian (22), African American
(263), Hispanic (158), Caucasian (327). estimate inter-annotator agreement, second
annotator sampled categorized 120 users. Among users annotators
selected one four categories, 74/76 labels agreed (97%). disagreement
category could determined: 21/120 labels (17.5%), one annotator
indicated category could determined, selected category. Gender
3. introduces bias towards accounts identifiable ethnicity; leave investigation
future work.

399

fiCulotta, Ravi, & Cutler

Gender

# friends

104

Ethnicity

Politics

103
102

# unique terms

101
104
103
102
101 0
10

101

102

103 100

101
102
user rank

103 100

101

102

103

Figure 4: Rank-order frequency plots number friends per user number
unique terms per user labeled datasets (gender, ethnicity, politics).
friends terms restricted one 46,649 accounts 112,642
terms used regression experiments.

annotation done automatically comparing first name provided user profile
U.S. Census list names gender (Census, 1990). Ambiguous names
removed.
user, collected 200 friends using Twitter API. removed
accounts restricted access friend information; also removed Asian users due
small sample size, leaving total 615 users. classification, user represented
identity friends (up 200). friend accounts contained
46,649 accounts used regression experiments retained. additionally collected
3,200 tweets user constructed binary term vector, using
tokenization regression model.
political preference data comes Volkova, Coppersmith, Van Durme (2014),
turn builds labeled data Pennacchiotti Popescu (2011) Al Zamal
et al. (2012). Volkova (2014) provides detailed description data. use
geo-centric portion data, contains Twitter users Maryland, Virginia,
Delaware report political affiliation Twitter profile description (e.g., Im
father, husband, Republican). Note feature representation consider
tokens user profile. contains 183 Republican users 230 Democratic users.
(We consider related political datasets annotated based user
follows, may give unfair advantage friend features.) user
5,000 friends 200 tweets. Figure 4 shows number friends number
unique terms per user dataset.
400

fiPredicting Twitter User Demographics

Gender
Ethnicity
Politics
Average

Friends
distant full
.75
.66
.60
.68
.80
.83
.72
.72

Text
distant
.86
.86
.56
.76

full
.84
.86
.73
.81

Friends + Text
distant
full
.87
.84
.81
.86
.74
.73
.81
.81

Table 4: F1 results Twitter user classification manually annotated data. consider three different feature sets (Friends, Text, Friends+Text), well two
classification models: full fully-supervised logistic regression classifier fit
manually labeled Twitter users; distant proposed distantly-supervised regression model, fit QuantCast data, using manually annotated Twitter
users. largest values row bold.

4.2.2 Classification Models
model initially trained regression task, make modifications
apply classification task. represent user labeled data binary
vector friend text features, using tokenization regression results.
example, user follows accounts B, feature values 1
corresponding accounts; similarly, user mentions terms X Y, feature
values 1. repurpose regression model perform classification, must modify
coefficients returned regression. first compute z-score coefficient
respect coefficients category value. E.g., coefficients
Male class adjusted mean 0 unit variance. makes coefficients
comparable across labels. classify user, compute dot product
coefficients binary feature vector, selecting class maximum value.
regression model fit combined Friend+Text feature set performed poorly
initial classification experiments. Upon investigation, determined coefficients
two types feature tended differ order magnitude. Rather use
model directly, instead adopted ensemble approach combining outputs
two models trained separately text friend features. classify set users,
computed feature-coefficient dot product separately text friend models,
computed z-score resulting values class label (e.g., dot-products produced
text model Male class standardized zero mean unit variance).
put predicted values model range. finally summed
outputs models returned class maximum value user.
also compared fully-supervised baseline. trained logistic regression
classifier L2 regularization, using feature representation above. perform
three-fold cross-validation compare accuracy distantly supervised approach.
401

fiCulotta, Ravi, & Cutler

Friends

Ethnicity

Politics

Macro F1

Gender

Text

Friends+Text

0.9
0.8
0.7
0.6
0.5
0.4
0.9
0.8
0.7
0.6
0.5
0.4
0.9
0.8
0.7
0.6
distant supervision
0.5
full supervision
0.4
10 20 30 40 50 60 70 80 90 100 10 20 30 40 50 60 70 80 90 100 10 20 30 40 50 60 70 80 90 100

% labeled training data

Figure 5: Twitter user classification results comparing standard logistic regression classifier (full supervision), trained using cross-validation, versus proposed
approach (distant supervision), fit solely statistics Quantcast, individually labeled data. Distant supervision comparable full
supervision, even 100% available training data provided.

4.2.3 Classification Results
Table 4 compares F1 scores distantly supervised approach (distant) well
fully supervised baseline (full). report results using three feature sets
three labeled datasets.
Overall, distantly supervised approach comparable fully supervised approach. two three tasks, F1 score distant meets exceeds full.
third task (politics), best distant method within 3% best full method (.80
vs .83). Averaging three tasks, best distant method indistinguishable
best full method (both produce F1 scores 81%).
primary result distant supervision performs poorly political classification
using text features. speculate part due small number tweets per
user available dataset (at 200 tweets per user). Additionally, text features
used distantly supervised approach collected several years tweets
contained political dataset. Given rapid topical changes political dialogue,
likely many highly-weighted terms distant text model less relevant
older data. results suggest friend features may less susceptible
data drift friend-based distant model performs much better text-based
model (.80 vs .56).
number labeled data relatively small (fewer 1,000 users), examined accuracy fully supervised approach number labeled data increase
(Figure 5). appears supervised classification accuracy mostly plateaued
402

fiPredicting Twitter User Demographics

Macro F1

Gender
Ethnicity
Politics

0.9
0.8
0.7
0.6
0.5
0.4
0
0.9
0.8
0.7
0.6
0.5
0.4
0
0.9
0.8
0.7
0.6
0.5
0.4
0

Friends

Text

10 20 30 40 50

0

2000 4000 6000 8000

10 20 30 40 50

0

2000 4000 6000 8000

10 20 30 40 50

0

friends per user

500

1000

terms per user

Figure 6: Classification F1 scores distantly supervised approach number
friends number unique terms collected per user increase (with standard
deviations computed five random trials).

task (with possible exception Friends+Text features gender classification).
ethnicity, distant outperforms full half labeled data used fit
classification approach, full dominates. Thus, appears distantly
supervised approach comparable fully supervised learning across range sizes
labeled data.
4.2.4 Sensitivity Number Features
Finally, investigate much information need user make
accurate prediction demographics. so, perform experiment
randomly sample subset friends terms user, report
F1 number selected features increases. friends, consider subsets size
{1, 3, 5, 10, 20, 30, 40, 50} (values greater 50 significantly increase accuracy).
terms, consider subsets size {10, 100, 1000, 2000, 8029} (8,029 maximum
number unique terms used single user labeled data).
Figure 6 displays results. see accuracy plateaus quickly using friend
features: three tasks, F1 score using 10 friends within 5% score using
200 friends. text features, accuracy begins plateau around 2K unique terms
Gender Ethnicity tasks. lower accuracy using Text features Politics
likely due part simple fact Politics data fewer tweets per user (a
maximum 200 tweets per user, compared 3,200 Gender Ethnicity
tasks).
results implications scalability Twitter API rate limits make difficult
collect complete social graph tweets set users. Additionally,
403

fiCulotta, Ravi, & Cutler

important privacy implications; revealing even small amount social information may
also reveal considerable amount demographic information. Twitter users concerned
privacy may wish disable setting makes friend identity information public.

5. Conclusions Future Work
paper, shown pairing web traffic demographic data Twitter data
provides simple effective way train demographic inference model without annotation individual profiles. validated approach aggregate (by comparing Quantcast data) individual level (by comparing hand-labeled
annotations), finding high accuracy cases. Somewhat surprisingly, approach
outperforms fully-supervised approach gender classification, competitive
ethnicity political classification.
short-term future work, test generalizability approach new
groups Twitter users. example, collect users city county compare
predictions Census demographics geographic location. Additionally, investigate ways combine labeled unlabeled data using semi-supervised
learning (Quadrianto, Smola, Caetano, & Le, 2009b; Ganchev et al., 2010; Mann & McCallum, 2010). Finally, fully validate across demographic variables, consider
administering surveys Twitter users compare predictions self-reported survey
responses.
Additional future work may investigate sophisticated types distant supervision.
example, homophily constraints imposed encourage neighbors similar demographics; location constraints use used learn county demographic
data. Also, multi-task model captures interaction demographic variables training time, also use collective inference reflect correlations among
demographic variables. Finally, considered simple bag-of-words feature
representations; future work may investigate low-dimensional embeddings non-linear
models.

Acknowledgments
research funded part support IIT Educational Research Initiative Fund. Culotta supported part National Science Foundation grant
#IIS-1526674. opinions, findings conclusions recommendations expressed
material authors necessarily reflect sponsor.

References
Al Zamal, F., Liu, W., & Ruths, D. (2012). Homophily latent attribute inference:
Inferring latent attributes twitter users neighbors. ICWSM.
Argamon, S., Dhawle, S., Koppel, M., & Pennebaker, J. W. (2005). Lexical predictors
personality type. proceedings Joint Annual Meeting Interface
Classification Society North America.
404

fiPredicting Twitter User Demographics

Barbera, P. (2013). Birds feather tweet together. bayesian ideal point estimation
using twitter data. Proceedings Social Media Political Participation,
Florence, Italy, pp. 1011.
Burger, J. D., Henderson, J., Kim, G., & Zarrella, G. (2011). Discriminating gender
twitter. Proceedings Conference Empirical Methods Natural Language
Processing, EMNLP 11, pp. 13011309, Stroudsburg, PA, USA. Association Computational Linguistics.
Census (1990).
List surnames.
http://www2.census.gov/topics/genealogy/
1990surnames. Accessed: 2015-06-01.
Chang, J., Rosenn, I., Backstrom, L., & Marlow, C. (2010). ePluribus: ethnicity social
networks. Fourth International AAAI Conference Weblogs Social Media.
Conover, M. D., Goncalves, B., Ratkiewicz, J., Flammini, A., & Menczer, F. (2011). Predicting political alignment twitter users. IEEE Third international conference
social computing (SOCIALCOM), pp. 192199. IEEE.
Culotta, A. (2014). Estimating county health statistics twitter. CHI.
Culotta, A., Kumar, N. R., & Cutler, J. (2015). Predicting demographics twitter
users website traffic data. Twenty-ninth National Conference Artificial
Intelligence (AAAI).
Dredze, M. (2012). social media change public health. IEEE Intelligent Systems,
27 (4), 8184.
Druck, G., Mann, G., & McCallum, A. (2008). Learning labeled features using generalized expectation criteria. Proceedings 31st Annual International ACM SIGIR
Conference Research Development Information Retrieval, pp. 595602.
Druck, G., Mann, G., & McCallum, A. (2009). Semi-supervised learning dependency
parsers using generalized expectation criteria. ACL.
Eisenstein, J., Smith, N. A., & Xing, E. P. (2011). Discovering sociolinguistic associations
structured sparsity. Proceedings 49th Annual Meeting Association
Computational Linguistics: Human Language Technologies - Volume 1, HLT 11,
pp. 13651374, Stroudsburg, PA, USA. Association Computational Linguistics.
Ganchev, K., & Das, D. (2013). Cross-lingual discriminative learning sequence models
posterior regularization.. EMNLP, pp. 19962006.
Ganchev, K., Gillenwater, J., & Taskar, B. (2009). Dependency grammar induction via
bitext projection constraints. Proceedings Joint Conference 47th
Annual Meeting ACL 4th International Joint Conference Natural
Language Processing AFNLP: Volume 1-Volume 1, pp. 369377. Association
Computational Linguistics.
Ganchev, K., Graca, J., Gillenwater, J., & Taskar, B. (2010). Posterior regularization
structured latent variable models. J. Mach. Learn. Res., 11, 20012049.
Goel, S., Hofman, J. M., & Sirer, M. I. (2012). web: large-scale
study browsing behavior.. ICWSM.
405

fiCulotta, Ravi, & Cutler

Gopinath, S., Thomas, J. S., & Krishnamurthi, L. (2014). Investigating relationship
content online word mouth, advertising, brand performance.
Marketing Science, 33 (2), 241258.
Jin, R., & Liu, Y. (2005). framework incorporating class priors discriminative
classification. PAKDD.
Kamerer, D. (2013). Estimating online audiences: Understanding limitations competitive intelligence services. First Monday, 18 (5).
King, B., & Abney, S. (2013). Labeling languages words mixed-language documents
using weakly supervised methods. Proceedings NAACL-HLT, pp. 11101119.
Liang, P., Jordan, M. I., & Klein, D. (2009). Learning measurements exponential
families. Proceedings 26th Annual International Conference Machine
Learning, ICML 09, p. 641648, New York, NY, USA. ACM.
Liu, W., & Ruths, D. (2013). Whats name? using first names features gender
inference twitter. AAAI Spring Symposium Analyzing Microtext.
Mann, G. S., & McCallum, A. (2010). Generalized expectation criteria semi-supervised
learning weakly labeled data. J. Mach. Learn. Res., 11, 955984.
Melville, P., Gryc, W., & Lawrence, R. D. (2009). Sentiment analysis blogs combining
lexical knowledge text classification. Proceedings 15th ACM SIGKDD
International Conference Knowledge Discovery Data Mining, KDD 09, p.
12751284, New York, NY, USA. ACM.
Mohammady, E., & Culotta, A. (2014). Using county demographics infer attributes
twitter users. ACL Joint Workshop Social Dynamics Personal Attributes
Social Media.
Musicant, D., Christensen, J., & Olson, J. (2007). Supervised learning training
aggregate outputs. Seventh IEEE International Conference Data Mining, 2007.
ICDM 2007, pp. 252261.
Nguyen, D., Smith, N. A., & Ros, C. P. (2011). Author age prediction text using linear
regression. Proceedings 5th ACL-HLT Workshop Language Technology
Cultural Heritage, Social Scie nces, Humanities, LaTeCH 11, pp. 115123,
Stroudsburg, PA, USA. Association Computational Linguistics.
Obozinski, G., & Taskar, B. (2006). Multi-task feature selection. workshop structural Knowledge Transfer Machine Learning 23rd International Conference
Machine Learning (ICML.
OConnor, B., Balasubramanyan, R., Routledge, B. R., & Smith, N. A. (2010). tweets
polls: Linking text sentiment public opinion time series.. ICWSM, 11, 122129.
Oktay, H., Firat, A., & Ertem, Z. (2014). Demographic breakdown twitter users:
analysis based names. Academy Science Engineering (ASE).
Pedregosa, F., et al. (2011). Scikit-learn: Machine learning Python. Machine Learning
Research, 12, 28252830.
406

fiPredicting Twitter User Demographics

Pennacchiotti, M., & Popescu, A.-M. (2011). machine learning approach twitter user
classification.. Adamic, L. A., Baeza-Yates, R. A., & Counts, S. (Eds.), ICWSM.
AAAI Press.
Preotiuc-Pietro, D., Lampos, V., & Aletras, N. (2015). analysis user occupational
class twitter content. ACL.
Quadrianto, N., Petterson, J., & Smola, A. J. (2009a). Distribution matching transduction. Advances Neural Information Processing Systems 22, p. 15001508. MIT
Press.
Quadrianto, N., Smola, A. J., Caetano, T. S., & Le, Q. V. (2009b). Estimating labels
label proportions. J. Mach. Learn. Res., 10, 23492374.
Rao, D., Paul, M. J., Fink, C., Yarowsky, D., Oates, T., & Coppersmith, G. (2011). Hierarchical bayesian models latent attribute detection social media. ICWSM.
Rao, D., Yarowsky, D., Shreevats, A., & Gupta, M. (2010). Classifying latent user attributes
twitter. Proceedings 2nd International Workshop Search Mining
User-generated Contents, SMUC 10, pp. 3744, New York, NY, USA. ACM.
Rosenthal, S., & McKeown, K. (2011). Age prediction blogs: study style, content,
online behavior pre- post-social media generations. Proceedings 49th
Annual Meeting Association Computational Linguistics: Human Language
Technologies - Volume 1, HLT 11, pp. 763772, Stroudsburg, PA, USA. Association
Computational Linguistics.
Schapire, R. E., Rochery, M., Rahim, M. G., & Gupta, N. K. (2002). Incorporating prior
knowledge boosting. Proceedings Nineteenth International Conference,
pp. 538545.
Schler, J., Koppel, M., Argamon, S., & Pennebaker, J. W. (2006). Effects age
gender blogging. AAAI 2006 Spring Symposium Computational Approaches
Analysing Weblogs (AAAI-CAAW), pp. 0603.
Schwartz, H. A., Eichstaedt, J. C., Kern, M. L., Dziurzynski, L., Ramones, S. M., Agrawal,
M., Shah, A., Kosinski, M., Stillwell, D., Seligman, M. E., et al. (2013). Characterizing geographic variation well-being using tweets. Seventh International AAAI
Conference Weblogs Social Media.
Volkova, S. (2014). Twitter data collection: Crawling users, neighbors communication personal attribute prediction social media. Tech. rep., Johns Hopkins
University.
Volkova, S., Coppersmith, G., & Van Durme, B. (2014). Inferring user political preferences
streaming communications. Proceedings Association Computational
Linguistics (ACL).
Volkova, S., & Van Durme, B. (2015). Online bayesian models personal analytics
social media. Proceedings Twenty-Ninth Conference Artificial Intelligence
(AAAI), Austin, TX.
Wang, M., & Manning, C. D. (2014). Cross-lingual projected expectation regularization
weakly supervised learning. TACL, 2, 5566.
407

fiCulotta, Ravi, & Cutler

Zhu, J., Chen, N., & Xing, E. P. (2014). Bayesian inference posterior regularization
applications infinite latent svms. Journal Machine Learning Research, 15,
17991847.
Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net.
Journal Royal Statistical Society: Series B (Statistical Methodology), 67 (2),
301320.

408

fiJournal Artificial Intelligence Research 55 (2016) 249-281

Submitted 03/15; published 01/16

Utilisation Metadata Fields Query Expansion
Cross-Lingual Search User-Generated Internet Video
Ahmad Khwileh
Debasis Ganguly
Gareth J. F. Jones

ahmad.khwileh2@mail.dcu.ie
dganguly@computing.dcu.ie
gjones@computing.dcu.ie

ADAPT Centre, School Computing
Dublin City University
Dublin 9, Ireland

Abstract
Recent years seen significant efforts area Cross Language Information
Retrieval (CLIR) text retrieval. work initially focused formally published
content, recently research begun concentrate CLIR informal social
media content. However, despite current expansion online multimedia archives,
little work CLIR content. limited work
Cross-Language Video Retrieval (CLVR) professional videos, documentaries
TV news broadcasts, date, significant investigation CLVR
rapidly growing archives informal user generated (UGC) content. Key differences
UGC professionally produced content nature structure
textual UGC metadata associated it, well form quality
content itself. setting, retrieval effectiveness may suffer translation
errors common CLIR tasks, also recognition errors associated automatic
speech recognition (ASR) systems used transcribe spoken content video
informality inconsistency associated user-created metadata
video. work proposes evaluates techniques improve CLIR effectiveness
noisy UGC content. experimental investigation shows different sources
evidence, e.g. content different fields structured metadata, significantly
affect CLIR effectiveness. Results experiments also show metadata field
varying robustness query expansion (QE) hence negative impact
CLIR effectiveness. work proposes novel adaptive QE technique predicts
reliable source expansion shows technique effective
improving CLIR effectiveness UGC content.

1. Introduction
Increasing amounts user generated multilingual video content (UGC) uploaded
social video-sharing websites Youtube (2015), Facebook (Facebook video, 2015),
BlipTv (2015) many others. 2015, YouTube, predominant online video sharing
site, reported 300 hours video content uploaded every minute 61
different languages (YouTube Press, 2015). ease flexibility video content
production, coupled low cost publishing wide potential reach, resulting
exponential growth number videos available Web.
time, due increasing user demands accessing viewing content,
important manage way facilitate effective efficient access
c
2016
AI Access Foundation. rights reserved.

fiKhwileh, Ganguly, & Jones

it. large amounts content creating need development
sophisticated video retrieval systems, presenting new challenges exciting opportunities
Information Retrieval (IR) research (Bendersky, Garcia-Pueyo, Harmsen, Josifovski, &
Lepikhin, 2014; Naaman, 2012).
One key challenges effective exploitation UGC content multilingual setting effective search languages user queries content
metadata. multilingual perspective, quality UGC depends solely
characteristics individuals actually produce upload videos language.
lack formal editorial control means uploaded videos typically varied across languages terms audio, visual metadata quality. Moreover, quantity
topical coverage content across different languages uneven, often meaning satisfying information need user one language achieved
providing relevant content another language. example, bilingual Arabic speakers
frequently enter Arabic queries relevant content English,
even likely video material little UGC Arabic content currently available
certain topics cultural historical topics. fact classical use-case
Cross-Language Information Retrieval (CLIR) seeks enable users enter search
queries one language retrieve relevant content another one. Translation technologies
key successfully bridging language gap users query relevant
content (Oard & Diekema, 1998; Herbert, Szarvas, & Gurevych, 2011).
quality monolingual search UGC content dependent effective utilization available metadata. CLIR search effectiveness depend translation
quality query content languages. many potential choices
design robust CLIR framework Internet video search task, current lack
detailed investigation means lack understanding specific challenges
represents thus little guidance available choices made
developing framework.
paper, investigate CLIR search effectiveness archive user-generated
Internet video content originally used MediaEval 2012 Search Hyperlinking
task (Eskevich, Jones, Chen, Aly, Ordelman, & Larson, 2012a), extend
CLIR task. examine retrieval effectiveness using title description metadata
provided video uploader automatic speech recognition (ASR) transcripts
content. investigate application automatic query expansion source
improving CLIR retrieval performance. Retrieval query expansion carried
using Divergence Randomness (DFR) IR model, automatic translation
carried using Google Translate (2015). understand task better, undertake
detailed performance analysis examining impact different source metadata information CLIR behaviour. However, current investigation limited application
state-of-the-art Machine Translation (MT) information retrieval (IR) methods
task, order establish basis investigations.
remainder paper structured follows: Section 2 gives general
background CLIR, Section 3 reviews related work, Section 4 describes test set
used experiments evaluation metric, Section 5 describes initial retrieval
experiments examining relative CLIR effectiveness source evidence (ASR,
Title Description), Section 6 describes approach improving CLIR effectiveness
250

fiCross-Lingual Search User-Generated Internet Video

using careful adjustment retrieval algorithm setting, Section7 describes approach
improving CLIR effectiveness using automatic query expansion techniques, Section 8
concludes paper provides directions work.

2. Cross Language Information Retrieval
stated previously, goal CLIR satisfy user information need expressed
query one language using content another language. CLIR techniques use
translation bridge language barrier query indexed content.
techniques differ mainly translation module placed, either
query processing document indexing stage. Figure 1 shows CLIR techniques
utilise translation technologies bridge barrier query language (L2)
document language (L1). Query Translation approach (QT CLIR) common

Figure 1: Document Query based CLIR Techniques.
CLIR technique (Oard & Diekema, 1998; Herbert et al., 2011; Sokolov, Hieber, & Riezler,
2014); query translated match index language (L1). technique
known low cost (per translated query) easy implement, since translation
tool used online retrieval time translate query document language.
Yet approach dependent sensitive quality query translation
retrieval. queries may lack context semantic content, makes
harder interpret translate. Previous literature explored multiple techniques
overcome issues either improving translation quality using various translation
techniques (Chen, Hueng, Ding, & Tsai, 1998; Gao, Nie, Xun, Zhang, Zhou, & Huang,
2001; Varshney & Bajpai, 2014; Lee, Chen, Kao, & Cheng, 2010) improving query
using query reformulation techniques Query Expansion (QE) Relevance
Feedback (RF) process (Carpineto & Romano, 2012).
251

fiKhwileh, Ganguly, & Jones

QE RF operates selecting terms documents marked
relevant user adding original query. absence user created
relevance data, top ranked documents assumed relevant process often referred
pseudo relevance feedback (PRF). Although noisy, PRF shown effective
improving overall retrieval effectiveness (Bellaachia & Amor-Tijani, 2008). crosslingual settings, query expanded translation provide effective
query translation using pre-translation QE technique (Ballesteros & Croft, 1997). QE
expansion also applied translation (post-translation QE), using combination
pre-translation post-translation QE shown Figure 1, order alleviate
impact IR effectiveness arising translation problems (Ballesteros & Croft, 1998;
Rogati & Yang, 2002).
alternative QT CLIR Document Translation (DT CLIR) documents
collection translated query language (Oard & Hackett, 1998; Lee & Croft,
2014). Several arguments suggest DT competitive superior QT CLIR
tasks, due fact less sensitive translation errors. DT
advantage translation carried offline prior retrieval, allows
possibility tuned accurate translation. Another advantage DT CLIR
require result translation shown Figure 1, since documents
already translated index time. However, DT CLIR shown effective
several tasks, application CLIR settings often impractical due
large amount time resources required document translation. Particularly,
document collection large search carried across multiple language
pairs. less common CLIR technique shown effective, Hybrid CLIR
approach utilises document query translation approaches, thus allowing
relative advantages approaches complement (McCarley, 1999; Kishida
& Kando, 2006; Parton, McKeown, Allan, & Henestroza, 2008).
Several approaches proposed carry translation process CLIR
framework. commonly used ones bilingual dictionaries machine translation (MT) (Zhou, Truran, Brailsford, Wade, & Ashman, 2012). Bilingual dictionaries
perform word-by-word translation using machine-readable dictionary sets
entries words possible translations language (Pirkola, Hedlund,
Keskustalo, & Jarvelin, 2001). approach suffer issues coverage, since
words may contained machine-readable dictionary, ambiguity since
relies dictionary many words multiple possible translations selecting correct translation among non-trivial task. Machine translation (MT)
techniques use trained system perform automatic translation free-text one
natural language another (Nikoulina, Kovachev, Lagos, & Monz, 2012; Magdy & Jones,
2014). MT also similar dictionary coverage problems, creation
single best translation addresses translation ambiguity issues. recent years, MT
become commonly used technique CLIR due increasing availability
high quality off-the-shelf MT systems. CLIR research dealt translation
module black-box without control translation process, rather used
one freely available online translation tools Google Translate (2015), Bing
translate (2015) others, proven effective. example, CLEF
evaluation campaigns 2009 (CLEF, 2015a), best performing non-Google MT system
252

fiCross-Lingual Search User-Generated Internet Video

achieved 70% performance achieved Google Translate tool (Leveling, Zhou,
Jones, & Wade, 2010; Zhou et al., 2012).
experimental investigation, choose default common CLIR
settings, QT CLIR technique utilises Google MT translate tool.
nevertheless, plan explore CLIR settings (e.g. DT-CLIR open-box MT system)
future work.

3. Related Work
Several CLIR tasks explored across different domains document types (Peters,
Braschler, & Clough, 2012). closely related CLIR work examined
research carried tasks within CLEF evaluation campaigns professionally
generated video content (2015b).
2002-2004 Cross-Language Spoken Document Retrieval (CL-SDR) task investigated news story document retrieval using data NIST TREC 8-9 Spoken
Document Retrieval (SR) manually translated queries (Federico & Jones, 2004; Federico, Bertoldi, Levow, & Jones, 2005). aim tasks evaluate CLIR
systems noisy automatic transcripts spoken documents known story boundaries
involved retrieval American English news broadcasts unsegmented
segmented transcripts taken radio TV news. CLIR tasks done using
topics several European languages. metadata provided tasks,
interesting findings indicate even manually translated queries, best CLIR
performance resulted 15% reduction monolingual ones (Federico & Jones, 2004),
using dictionary term-by-term translation, reduction increased
40% 60%, highlights challenge CLIR video collections (Federico et al.,
2005).
ambitious Cross-Language Speech Retrieval (CL-SR) task ran within CLEF
2005-2007 (White, Oard, Jones, Soergel, & Huang, 2006; Oard, Wang, Jones, White,
Pecina, Soergel, Huang, & Shafran, 2007; Pecina, Hoffmannova, Jones, Zhang, & Oard,
2008). examined CLIR spontaneous conversational speech oral history collection
content English Czech. tasks provided ASR transcripts, automatically
manually generated metadata interviews. goal Czech English
tasks create systems could help monolingual cross lingual searchers identify
sections interview wish listen Czech English interviews.
reported results tasks showed use manual metadata yielded substantial
statistically significant improvement retrieval effectiveness ASR transcripts
automatically created metadata. investigation carried Inkpen, Alzghool, Jones,
Oard (2006) CL-SR standard collection showed retrieval effectiveness could
improved careful selection term weighting scheme ASR
manual metadata. Alzghool Inkpen (2008) also used test collection CLEF
2007 CL-SR task present method combining results different retrieval models
order improve overall retrieval effectiveness. also provided comparison
ASR manual metadata, indicating superiority manual metadata
maintaining retrieval effectiveness. Another interesting follow study, reported
Jones, Zhang, Newman, Lam-Adesina (2007), examined compared CLIR
253

fiKhwileh, Ganguly, & Jones

effectiveness source evidence included collection. Results work
indicate searching manually generated metadata gives higher performance terms
recall precision search noisy ASR transcripts.
VideoCLEF task introduced CLEF 2008 CLEF 2009. task
provided Dutch TV content featuring English-speaking experts studio guests. VideoCLEF piloted tasks involved performing classification, translation keyword extraction
dual language video using either machine learning techniques treating IR
task. Participants provided Dutch archival metadata, Dutch speech transcripts,
English speech transcripts (Larson, Newman, & Jones, 2009, 2010).
previous work CLVR focused running CLIR tasks professional video
broadcast whether documentaries, TV shows interviews high quality recording
consistency length, visual audio quality across collections. collections
included manually automatically created metadata. example, domain experts following carefully prescribed format wrote manually created metadata CLEF
2005-2007 consistent speech quality word error rate 25% across collections
used (White et al., 2006; Oard et al., 2007; Pecina et al., 2008).
CLEF tasks followed establishment MediaEval benchmarking
campaign 2010 (MediaEval, 2015). Activities MediaEval focused various
multimedia search tasks, included CLIR elements.
emergence user-generated video content web introduced new search
opportunities challenges exploitation user-generated metadata (Eickhoff, Li,
& de Vries, 2013; Filippova & Hall, 2011; Toderici, Aradhye, Pasca, Sbaiz, & Yagnik, 2010).
CLIR published text explored wide variety language pairs
many years, recent research begun explore CLIR user-generated informal text.
One example work one done Bagdouri, Oard, Castelli (2014)
explored retrieval questions posed formal English across user generated documents
Arabic collected forum posts. employed DT CLIR approach
translated Arabic informal text English. results show retrieval precision
enhanced applying informal text classifier help translation informal
content. Lee Croft (2014) also experimented CLIR task informal documents.
developed CLIR task large collection Chinese forum posts demonstrated
translation noise increased informal text used discussion forums.
retrieval approach proposed use PRF approach improve retrieval effectiveness.
results showed PRF approaches useful reducing impact translation
errors retrieval effectiveness tasks.
UGC begun attract considerable research interest video retrieval indexing recent years. none work far included element CLIR,
much addressed main issues user-generated content video retrieval.
example, work focused quality user-generated metadata video retrieval (Eickhoff et al., 2013; Filippova & Hall, 2011; Toderici et al., 2010), work
focused quality visual/audio features within scale dynamics UGC
content (Bendersky et al., 2014; Chelba, Bikel, Shugrina, Nguyen, & Kumar, 2012; Langlois, Chambel, Oliveira, Carvalho, Marques, & Falcao, 2010). Moreover, 2010,
TREC Video Retrieval Evaluation (TRECVID) (2015), main video retrieval benchmark
multimedia community, provided collection Internet videos used several
254

fiCross-Lingual Search User-Generated Internet Video

Table 1: Length statistics indexed blip10000 fields.

Stan.Dev
Avg.Length
Median
Max
Min

Title
3.0
5.3
5.0
22.0
0.0

Desc
106.9
47.7
24.0
3197.0
1.0

ASR
2399.5
703.0
1674.8
20451.0
0.0

tasks. However, design TRECVID tasks mainly focused exploiting visual information applications shot level (concept detection), short video clips (event
detection) others. One task relevant work known-item search
task (KIS) (Over, Awad, Fiscus, Antonishek, Michel, Smeaton, Kraaij, & Quenot, 2011)
TRECVID, task aimed explore retrieval visual queries included
TRECVID annually 2010 2012. Results participants rather inconsistent year year terms retrieval effectiveness different search approaches,
one conclusion difficulty actually setting evaluation task Internet
collections.
work, focus studying retrieval challenges Internet-based UGC multimedia collections audio data highly variable many aspects including audio
conditions recording, microphones used, fluency informality language used speaker. challenges produce ASR errors affect
retrieval effectiveness monolingual retrieval, reported Eskevich, Jones,
Wartena, Larson, Aly, Verschoor, Ordelman (2012b) Eskevich (2014), also
cross-lingual settings combined query translation.
best knowledge, work first effort explore issues CLIR
video collected user-contributed source Internet. Thus creators varied
backgrounds differing motivations interests generated content without
central editorial control style, format quality. makes uploaded videos
varied terms amount quality manually added metadata descriptions,
thus challenging multiple retrieval perspectives. particular relevance
investigation following aspects data:
Distribution document lengths: restriction document length
found highly variable. length variability poses challenge
retrieval task, particularly significant within CLIR due presence
translation errors. breakdown details various fields blip10000
test collection shown Table 1.
High variability ASR quality video transcripts: Even ASR system
used, variation audio quality, speaking styles speakers, generally
leads significant variability accuracy transcripts.
Inconsistencies sparseness associated user contributed metadata: titles
may short one two terms, descriptions generic,
informal sometimes incomplete, making utility retrieval varied.
255

fiKhwileh, Ganguly, & Jones

4. Experimental Test Set Evaluation
blip10000 collection used experiments crawl Internet video sharing
platform Blip.tv (Schmiedeke, Xu, Ferrane, Eskevich, Kofler, Larson, Esteve, Lamel, Jones,
& Sikora, 2013). originally used content dataset MediaEval 2012
Search Hyperlinking task (Eskevich et al., 2012a). blip10000 collection contains
crawled videos together associated metadata. metadata composed
titles descriptions video provided video uploader.
addition, associated ASR transcripts also provided videos. collection
consists 14,838 videos total running time ca. 3,288 hours, total size
862 GB1 .
length statistics fields shown Table 1. noted
huge variation length distributions across different fields. Table 1 also
highlights variations individual fields videos. example, one
video may ASR, another may contain 20K terms. experiments
indexed metadata fields separately, combination shown Figure 2.

Figure 2: Example combined-field document.

1. Blip10000 Data Collection obtained from:
http://skuld.cs.umass.edu/traces/mmsys/2013/blip/Blip10000.html

256

fiCross-Lingual Search User-Generated Internet Video

Table 2: Monolingual English query vs Arabic-English translated query example.
Monolingual (MN) Query :
<top>
<num>37 </num>
<Mn-Lg>the video features recent USA sanctioned
clean energy Act.</Mn-Lg>
<Mn-Sh>clean energy legislation USA</Mn-Sh>
</top>
Machine Translated (CL) Query :
<top>
<num>37</num>
<CL-AR-Lg>Video displays Identify measures United
States toward alternative Energy</CL-AR-Lg>
<CL-AR-Sh>Rules United States
toward alternative energy</CL-AR-Sh>
</top>
4.1 Query Construction CLIR Task
MediaEval 2012 Search Hyperlinking task (Eskevich et al., 2012a) knownitem search task, search single previously seen relevant video (the known-item),
provided 60 English queries collected using Amazon Mechanical Turk (MTurk)
crowd-sourcing platform (2015). query contains full query statement (long query)
terse web type search query (short query). investigation, explored
short long queries give better understanding query-length independent
retrieval behaviour monolingual CLIR tasks. create CLIR test set,
extended original monolingual English queries giving Arabic, Italian
French native speakers, asking translate natural queries
native language. short long queries translated Arabic. order
explore CLIR effectiveness across multiple language pairs, short query set
also expressed Italian, long query set constructed French.
types queries expressed two languages (long queries expressed Arabic
French, short queries expressed Arabic Italian) allowed us
draw better conclusions CLIR performance task. used Google
translate API2 translate query sets back English. would expected.
MT translation produced different version original monolingual ones; addition
expected deletion/insertion edits shown example Table 2, also
Named Entity Errors (NEEs) Out-Of-Vocabulary (OOV) items Google translation
could translate correctly. translation edits errors pose challenge
retrieval effectiveness MT translated queries compared monolingual ones.
monolingual English query sets originally provided MediaEval
2012 Search Hyperlinking task. investigation query sets labelled
follows:
2. https://developers.google.com/translate

257

fiKhwileh, Ganguly, & Jones

Mn-Sh: 60 EN short queries (monolingual)
Mn-Lg: 60 EN long queries (monolingual)
CLIR query sets labelled follows:
CL-AR-Sh: 60 AR short queries translated EN
CL-AR-Lg: 60 AR long queries translated EN
CL-IT-Sh: 60 long queries translated EN
CL-FR-Lg: 60 FR long queries translated EN
4.2 Mean Reciprocal Rank (MRR) Evaluation Metric
Since retrieval problem addressing known-item search
seeking retrieve single known relevant item, evaluate investigations using
standard metric task Mean Reciprocal Rank (MRR) metric computed
shown Equation 1 ranki indicates rank ground truth known item
ith query intended find.
RR =

n
1
1X
n i=1 ranki

(1)

Similar known-item experiments, also chose define recall number
times relevant-item found across set queries (Buttcher, Clarke, & Cormack,
2010). Moreover, recall reported default standard TREC1000 results cut
off, also report cutoff points 10, 50, 100 documents experiments.

5. Single Field Retrieval
first part investigation examines behaviour separate document information fields CLIR framework. particularly interested impact
translation errors inconsistencies retrieval effectiveness given noise ASR
transcripts, shortness title field, inconsistencies description field.
examine question evaluating CLIR robustness field measure
retrieval effectiveness behaves CLIR framework. Throughout investigation
paper, define CLIR Robustness well field source evidence performs
CLIR framework. observe computing significance change
CLIR monolingual performance using setting across query sets.
run CLIR robustness evaluation experiment, compare CLIR effectiveness
field monolingual baseline:
ASR index contains ASR transcript fields.
Title index contains title fields.
Desc index contains description fields.
258

fiCross-Lingual Search User-Generated Internet Video

Table 3: Mono vs. CLIR performance per index

Title index
ASR index
Desc index

Mn-Sh
0.239
0.4275
0.2154

CL-AR-Sh
0.2288
0.2748
0.1943

CL-IT-Sh
0.2383
0.3873
0.2102

Mn-Lg
0.2827
0.4513
0.2432

CL-AR-Lg
0.2244
0.3487
0.2285

CL-FR-Lg
0.2239
0.3833
0.2316

5.1 Retrieval Model
single field retrieval experiments carried using Terrier retrieval engine3 .
Terrier standard open source IR toolkit providing many best established retrieval
algorithms widely used IR research community. Stop-words removed based
standard Terrier list, stemming performed using Terrier implementation
Porter stemming. used PL2 model4 , probabilistic retrieval model
Divergence Randomness (DFR) framework (Gianni, 2003). reason selected
model available retrieval models characteristics data collection.
Previous studies as(Amati & Van Rijsbergen, 2002) shown PL2 less
sensitivity length distribution compared retrieval models works better
experiments seek early precision, aligns known-item experiment. PL2
thus suitable since Internet based data collection huge variation lengths, whether
field level document level, shown Table 1. PL2 document scoring
model defined shown Equation 2, Score(d, Q) retrieval matching score
document query term Poisson distribution F/N , F query
term frequency whole collection N total number documents
collection. qtw query term weight given qtf /qtf max; qtf query term
frequency qtf max maximum query term frequency among query terms.
tfn normalized term frequency defined Equation 3, l length
document d. avgl average length documents, c free parameter
normalization. set parameter c, followed empirically determined standard
settings recommended Amati Van Rijsbergen (2002) Ounis (2007b),
c = 1 short queries c = 7 long queries.

Score(d, Q) =

X
tQ

qtw .

1
tfn
(tfn log2
+ ( tfn ). log2 e + 0.5 log2 (2.tfn ))
1 + tfn


tfn =

X

(tf. log2 (1 + c.



avgl
)), (c > 0)
l

(2)

(3)

5.2 Experimental Results Discussion
results index shown Table 3, show MRR lower
cases CLIR task. Thus retrieval effectiveness fields negatively impacted
3. http://www.terrier.org/
4. Terrier implementation model found :
http://terrier.org/docs/v4.0/javadoc/org/terrier/matching/models/PL2.html

259

fiKhwileh, Ganguly, & Jones

Table 4: AR CLIR - t-values according % MRR reduction index
CL-AR-Sh CL-AR-Lg
Title index
-1.69
-1.73
ASR index
-1.94*
-2.50*
Desc index
-0.829
-0.44
*Statistically significant values p-value < 0.05.
Table 5: FR CLIR - t-values according % MRR reduction index
CL-IT-Sh CL-FR-Lg
Title index
-0.05
-1.77
ASR index
-1.58
-2.04*
Desc index
-0.32
-0.47
*Statistically significant values p-value < 0.05.
CLIR. confirms expected additional retrieval challenge arises
imperfect query translation. MRR Arabic queries reduced higher degree
French Italian queries. likely due relative difficulty Arabic
MT (Alqudsi, Omar, & Shaker, 2012). One significant challenge Arabic English
MT relates named entities. instance, query including word dreamweaver (the
proprietary web development tool) expressed dreamweaver FR IT,

AR, represented Q
P Y@ resulted OOV term
Google Translate transliterated completely different word Aldirimovr
useful retrieval using English language metadata.
Further, looking reduction MRR index indicates different responses query translation; notably impact greatest index ASR
transcript field across languages using short long queries.
better understand significance CLIR reductions MRR, computed
statistical significance reduction. calculated t-value difference
95% confidence level representing monolingual CLIR MRRs pairs every
query level. significance test results terms t-values indexes searched
Arabic CLIR queries shown Table 4 French Italian CLIR queries
Table 5. Looking t-values, observe queries less challenging
others since performance significantly different monolingual.
Furthermore, Tables 4 5 indicate ASR transcripts indeed
lowest robustness CLIR setting. searching single-field indexes,
long short queries, ASR index least robustness statistically significant
negative reduction Arabic French (p<0.05). Italian short queries,
MRR reduction rates ASR index (ASR index) statistically significant,
still highest negative impact fields.
conclude experiment even incomplete, short and/or sometimes unreliable, user-uploaded titles meta descriptions robust CLIR
setting ASR fields. noted earlier, degree ASR recognition errors may
vary one video another Internet, due wide variation audio quality.
260

fiCross-Lingual Search User-Generated Internet Video

interaction recognition error rate, document length retrieval behaviour
highly complex, observed Eskevich Jones (2014), plan explore
effect detail future work view improving CLIR robustness
ASR transcript field.

6. Retrieval Combined Metadata Fields
examined effectiveness three separate fields monolingual retrieval
CLIR, section explore potential combining improving retrieval
effectiveness. investigation, carried another set experiments combined
evidence individual fields. this, first combine fields pairs,
shown Figure 1, integrate three fields varied field weighting.
6.1 Retrieval Model
combined field experiments use DFR PL2F model5 (Macdonald, Plachouras,
He, Lioma, & Ounis, 2006). modified version PL2 model used
previous section. PL2F model designed adopt per-field weighting combining
multiple evidence fields single index search. term frequencies document
fields normalised separately combined weighted sum. PL2F uses
document scoring function PL2, shown Equation 2, tf n weighted sum
normalised term frequencies normalised term frequencies tfX field x,
case x (ASR, title, desc) indicated Equation 4. lx length
field x document d. avglx average length field x across documents,
cx , wx per-field normalization parameters. per-field normalization feature
PL2 modifies standard PL2 document scoring function include weighted sum
normalised term frequencies tfx .
tfn =

X

(wx .tfx . log2 (1 + cx .

x

avglx
)), (cx > 0)
lx

(4)

tfx also needs two parameters wx , cx set. Hence, scoring indexed document
need set parameters:
Cx set per-field length normalization parameters cx need set
every field Cx ={ c asr, c title, c desc}, Wx set per-field boost factors wx
need set field Wx ={ w asr, w title, w desc}.
6.2 Two Field Combinations
Table 6 shows MRR values fields combined pairs indexed using
PL2F retrieval model. interested potential improved retrieval using
fields combination. Comparing results Table 6 earlier results shown
Table 3, see field combination effective monolingual CLIR
tasks. improvement could probably obtained weighting fields differently.
5. Terrier implementation model found http://terrier.org/docs/v4.0/javadoc/org/
terrier/matching/models/PL2F.html

261

fiKhwileh, Ganguly, & Jones

Table 6: Mono vs. CLIR performance field pair combinations

TitleDesc index
ASRDesc index
ASRTitle index

Mn-Sh
0.2503
0.4394
0.4295

CL-AR-Sh
0.2421
0.3624
0.3676

CL-IT-Sh
0.2463
0.3951
0.3820

Mn-Lg
0.3020
0.5245
0.4527

CL-AR-Lg
0.2795
0.3905
0.3451

CL-FR-Lg
0.2614
0.4326
0.3768

Table 7: Weighting scheme Wx single-weighted retrieval models
PL2ASR
PL2Title
PL2Desc

ASR
wx
1
1

Title
1
wx
1

Desc
1
1
wx

However, main goal investigation potential combining three fields,
explore detail next section.
6.3 Three Field Combinations
section describe investigation retrieval effectiveness combination
three fields. explore giving higher weight specific field others.
set values proposed single-weighted retrieval models adopted
following steps:
Construct model based using PL2F document scoring targets single field
x (ASR, title, desc): PL2FASR, PL2Title, PL2Desc.
Assign equal cx value fields allow full-length normalization term
frequency field Cx = {1,1,1} short queries, Cx ={7,7,7} long
queries. also followed empirically standard settings recommended Amati
Van Rijsbergen (2002), Ounis (2007b).
Wx , set wx value targeted field, rest fixed 1,
give priority field x others, Wx = {wx ,1,1}. reason
chose 1 allow presence term frequencies,
normal (is boosted) weights.
combination weighting schemes shown Table 7, case one field
weight boost wx . examine retrieval behaviour, vary wx boost parameters
model range 1 60 using increments 1. first weighting iteration
weighting point wx = 1 models Wx = {1,1,1}.
6.3.1 Experimental Results Discussion
Figure 3 shows MRR performance weighting point long queries (CL-ARLg CL-FR-Lg query sets), short queries (CL-AR-Sh CL-IT-Sh query sets).
seen Figure 3, fields behave differently weight boosting. best CLIR
262

fiCross-Lingual Search User-Generated Internet Video

Figure 3: MRR CLIR performance single weighted models across weighting points
(wx) using short long query sets.
precision performance always achieved giving higher weight title field
AR, FR query sets. Across weighting points languages pairs,
PL2Title model shows higher performance fields short long
query sets.
Moreover, also seen figures, get lower performance
give progressively higher weights ASR Desc fields. strong CLIR performance
PL2Title model indicates stability title fields Internet videos
fields. Also, fact titles may written video uploader
attention descriptions could referred following reasons:
uploader thought important high quality title video since
would help promoting video-sharing site.
uploader believed importance since shown header
video, description generally shown video may
examined viewer.
263

fiKhwileh, Ganguly, & Jones

could also case known-item queries, users wrote queries
viewed videos might likely include titles videos
query find intended video, believe would easier find
using title video. However, noted MTurk task
used create queries Search Hyperlinking Mediaeval task display
video title user writing query created intention
suitable re-find known-item video.
Table 8: Single index Mono vs. CLIR Recall performance represented number
found documents cut-off values 10, 50, 100.

Title index
ASR index
Desc index
TitleDesc index
ASRDesc index
ASRTitle index
Index

Mn-Sh
19
35
17
20
35
35
37

Title index
ASR index
Desc index
TitleDesc index
ASRDesc index
ASRTitle index
Index

Mn-Sh
25
42
29
33
47
42
47

Title index
ASR index
Desc index
TitleDesc index
ASRDesc index
ASRTitle index
Index

25
46
34
38
50
46
50

10 - cut
CL-AR-Sh CL-IT-Sh
16
19
31
34
15
15
17
19
30
35
31
34
33
37
50 - cut
CL-AR-Sh CL-IT-Sh
23
23
38
42
23
27
28
30
41
44
38
42
40
47
100 - cut
23
24
41
45
27
31
32
35
47
49
41
45
45
50

Mn-Lg
23
34
23
25
40
34
40

CL-AR-Lg
18
29
21
21
32
28
33

CL-FR-Lg
19
29
20
22
33
38
34

Mn-Lg
30
43
31
37
46
43
48

CL-AR-Lg
28
37
27
29
40
36
41

CL-FR-Lg
26
39
28
31
43
40
45

32
47
34
42
50
46
52

29
40
32
35
43
39
43

29
43
32
38
47
42
49

Comparing MRR PL2Title values shown Table 3, also seen
performance PL2Title almost double one obtained independent
Title field (Title index). MRR values ASR Desc fields similar
two experiments. wx increases Title field, see
improvement, optimal weight depending query length
language pair. attempt better understand field combination improves
retrieval effectiveness, examined Recall individual fields combinations.
264

fiCross-Lingual Search User-Generated Internet Video

Table 8 shows total number known-items retrieved top 10, 50 100
field set. seen Title field lowest recall isolation,
boost Recall fields used combination. results Figure 3
suggest title field brings additional evidence without bringing noise,
case Desc ASR fields degrade effectiveness weight increased.

7. Query Expansion Using Combined Metadata Fields
Sections 5 6, analyzed effectiveness data source (ASR, Title
description) CLIR framework showed overall performance robust
fields combined together optimal way. also showed adjusting
retrieval settings give higher weight reliable data source, i.e. Title,
comparison less reliable fields, benefit overall CLIR performance.
section, seek modify query using field information improve
retrieval effectiveness. query modification strategy explore based query
expansion (QE) techniques (Carpineto & Romano, 2012), use different sources
evidence (fields) enrich original query. underlying motivation behind applying
QE expanding query include important terms make effective
identifying relevant items. Ideally, QE alleviate impact translation
errors adding terms top ranked videos either relevant related
query. QE effectiveness relies quality informativeness top ranked
documents (Amati, Carpineto, & Romano, 2004). Top ranked documents taken
external resources WordNet Wikipedia (Pal, Mitra, & Datta, 2013)
local document collection considering top ranking documents relevant
query. consider interesting/challenging approach here, local
collection expansion approach since aim investigate noisy collection web
videos utilized improve query effectiveness. We, nevertheless, plan consider
use external collections QE future investigation.
Existing research shown QE techniques useful improving monolingual CLIR effectiveness many languages (Bellaachia & Amor-Tijani, 2008). However, research focused primarily collections professionally written
formal text none minimum amount noise. some,
limited, work applying QE techniques noisy data, work OCR
Data (Tong, Zhai, Milic-Frayling, & Evans, 1996; Lam-Adesina & Jones, 2006),
recently, user-generated informal text (Lee & Croft, 2014). section,
interested taking challenge applying QE CLIR settings focus avoiding
problems may arise noise presented source evidence; particular
task, translation errors query, transcription errors ASR well
inconsistency errors user generated textual metadata. explore expansion
queries based top ranked documents. Expansion terms intended make
query reliable robust find intended relevant item. interest
section summarized help following research questions:
expand query using top ranked documents, might affect
overall CLIR effectiveness? effective QE setting noisy data
collected Internet videos be?
265

fiKhwileh, Ganguly, & Jones

different UGC information sources useful QE? Since
different sources (fields) different relative characteristics behaviour retrieval, observed empirically Sections 5 6.
describe approach addressing questions following sections.
investigate reliability single field QE challenges Section 7.1.
propose adaptive approach improve overall QE robustness6 selecting best
source expansion Section 7.2.
7.1 Query Expansion Fields
employ Divergence Randomness (DFR) QE mechanism proposed Gianni
(2003). technique computes weight rank terms top ranking documents. DFR QE generalizes Rocchios method (Salton & Buckley, 1997) implement
several term weighting models measure informativeness term pseudo
relevant set.
DFR QE two stages. First, applies DFR term weighting model measure
informativeness top terms top ranking document. main concept
DFR term weighting model infer informativeness term divergence
distribution top documents random distribution. use DFR weighting
model called Bo1, parameter-free DFR model uses BoseEinstein statistics weight
term based informativeness. parameter free model widely used
proven effective (He & Ounis, 2007a; Plachouras, He, & Ounis, 2004; Gianni,
2003). weight w term top ranked documents using DFR Bo1 model
shown Equation 5, tfx frequency term pseudo-relevant set (top
n ranked documents). Pn given F/N ; F term frequency query term
whole collection N number documents whole collection.
w(t) = tfx . log2 (

1 + Pn
) + log2 (1 + Pn )
Pn

(5)

Secondly, query term weight qtw , obtained single-pass retrieval (as
described Equation 2), adjusted according newly obtained weighting
values w(t) newly extracted terms original ones using Equation 6,
wmax (t) indicated maximum w(t) values among expanded query terms.
qtw = qtw +

w(t)
wmax (t)

(6)

illustrate QE approach used experiments, provide QE example
CLIR-AR query :
EEE PC 900 Troubleshooting laptop
terms pc, laptop, mac, us, classrooms generated running DFR QE
take top 5 terms top-5 documents. Note two expansion terms pc
laptop also appear original query, therefore, weight terms
6. QE robustness interpreted context likely improve retrieval performance
baseline, baseline single-pass retrieval using original query.

266

fiCross-Lingual Search User-Generated Internet Video

boosted greater 17 . new expansion terms (mac, us classrooms)
added original query weights adjusted based informativeness
uniqueness top n documents versus whole collection. term mac
predicted informative unique gets weight greater 0 since appears
top n documents terms (classrooms, us) assigned low weights
close 0 since also appear documents (non top-n). Using method,
final expanding reweighing query explained follows :
eee1.000, pc1.9211, 9001.0000 ,troubleshoot1.0000, laptop1.2988, mac0.2195,
us0.0000, classroom0.0000.
original query terms may appear top-terms (900, EEE
troubleshoot), formula Equation 6 would give
weight single-pass retrieval settings. cases might common
CLIR settings due presence named entity translation errors, example
Aldirimovr described Section 5. Since NEEs produced translation,
never appear top ranked documents since present
collection, therefore weight always remain expansions.
pose extra challenge overall QE effectiveness try handle
following sections designing post-translation QE tuned task. main
reasons us pick post-translation QE approach experimental investigation
:
study impact translation errors/translation quality QE effectiveness.
investigated running post-translation analysis query expansion
performance.
investigate whether adding new informative terms query reduce
impact translation errors improve retrieval effectiveness.
use default QE parameter settings experiment set
extract 10 informative terms top 3 returned documents. settings
suggested Gianni (2003) conducting extensive experiments several test
collections. However, case, task much challenging,
one relevant document find, also extend settings explore possible
parameter combinations. parameter settings proposed QE runs tuned
explore top (3, 5, 10) terms top (3, 5, 7) documents. QE runs
parameters combinations follows:
Taking top 3 terms, includes QE runs take top 3 terms top
3 documents, top 5 top 10 documents.
Taking top 5 terms, includes QE runs take top 5 terms top
3 documents, top 5 top 10 documents.
Taking top 10 terms, includes QE runs take top 10 terms
top 3 documents, top 5 top 10 documents.
7. 1 normal weight term appears original query

267

fiKhwileh, Ganguly, & Jones

Table 9: Optimized parameters (top-terms top-doc) selected QE run

exp-ASR
exp-Title
exp-Desc
exp-All

Terms
5
3
3
3

Docs
5
3
3
3

study best parameters field expansion, explored parameter variations. chose best performing setting QE run. optimized settings
shown Table 9.
QE framework using fields monolingual text retrieval proposed
Ounis (2007a). suggests improved term-weighting method based field statistics
achieve better retrieval performance. investigation, adopt approach
CLIR QE tune single-field QE technique allows us assess
effectiveness field QE. method, QE performed follows:
top n terms extracted top n documents retrieved response
executing query separate field type (title, description ASR) order
investigate individual effectiveness QE.
Retrieval carried similarly setting experiment Section 6,
combine fields together (see Figure 2), give equal weight 1
field. use PL2F model (described Section 6) retrieval experiment.
Since query sets (long/short) led similar conclusions previous Sections
regarding field effectiveness, chose run QE short queries (the CLIR
queries CL-AR-Sh CL-IT-Sh described Section 4.1).
conducted several QE runs based using individual field combinations.
reason tuned runs assess effectiveness field combination
QE. proposed field-based QE runs follows.
exp-ASR: Queries expanded using ASR field only, taking top
ranking terms top documents retrieved ASR index.
exp-Title: Queries expanded using Title field only, taking top ranking
terms top documents retrieved Title index.
exp-Desc: Queries expanded using Desc field only, taking top ranking
terms top documents retrieved Desc index.
exp-All : Queries expanded using combination fields, i.e. ASR, Title
Desc.
exp-Non : run skips expansion queries single pass
retrieval using original query. Note use baseline since want
assess effective QE challenging task.
268

fiCross-Lingual Search User-Generated Internet Video

Table 10: MRR performance QE run.

exp ASR
exp Title
exp Desc
exp
exp Non

CL-AR-Sh
0.3502
0.3820
0.3470
0.3571
0.3726

CL-IT-Sh
0.3735
0.4060
0.4090
0.4069
.4081

Table 11: Overall Recall (total found known-items) QE run.

exp ASR
exp Title
exp Desc
exp
exp Non

CL-AR-Sh
53
52
51
53
52

CL-IT-Sh
57
56
56
56
56

run field-based QE experiments explore MRR performance field.
MRR performance across different runs shown Table 10, overall recall
results shown Table 11. seen MRR proposed runs (exp-All,
exp-ASR, exp-Title, exp-Desc) improve baseline run (exp-Non). expTitle exp-Desc get less similar MRR performance, exp-ASR achieves
significantly lower MRR. fact multiple sources noise coming either
translation fields themselves, together fact known-item
task, one relevant item may highly ranked initial
search, justify ineffectiveness QE runs.
better understand robustness QE run compare baseline,
study difference MRR (MRR) query level proposed runs
baseline run (exp-Non). MRR particular query level QE run (exp-x)
indicated MRR = (MRR(exp-x) - MRR(exp-Non)). MRR results
CL-AR-Sh query across runs shown Figure 4, Figure 5 shows
MRR results CL-AR-Sh queries.
Since DFR QE model (see Equation 5 6) uses informativeness measure
weight extracted top terms, decreases increases MRR values
shown Figures 4 5 explained follows.
MRR = 0, means top ranked terms predicted less informative,
reason QE runs minimal effect baseline (exp-Non). Based
DFR definition informativeness (see Equation 5 6), indicates
terms added particular query given low weight
common top ranked documents, also whole document collection.
words, QE run could find helpful terms might potentially
improve overall performance. MRR performance run suggests
situation occasionally occurs QE runs across several queries, particularly
269

fiKhwileh, Ganguly, & Jones

exp-Title exp-Desc. probably arise due recall problem
two fields have, shown Table 8 (see also single-field retrieval experiments
Section 5). fact, queries exp-Desc exp-Title runs,
application QE low zero effect MRR compared
single-pass baseline retrieval (exp-Non).
MRR > 0, means top ranked terms predicted highly informative
relevant query, reason QE shows positive
increase baseline. turn suggests exp-ASR runs able
improve queries (more positive MRR points) terms ranking knownitem runs. also expected due higher recall performance
fields.

1.0

1.0

0.0 0.5 1.0

expASR

0.0 0.5 1.0

expAll

0

10

20

30

40

50

60

0

10

30

40

50

60

50

60

1.0

1.0

0.0 0.5 1.0

expDesc

0.0 0.5 1.0

expTitle

20

0

10

20

30

40

50

60

0

10

20

30

40

Figure 4: MRR across QE runs CL-AR-Sh.
Moreover, runs positive MRR queries. Particularly interesting fact combined QE run (exp-All) yield best results.
MRR < 0, means top ranked terms predicted highly informative
relevant query, reason QE negative
effect baseline run. MRR values show incorrect prediction
significantly impacted runs. particular, exp-ASR run affected
most, seen negative MRR values across several queries
CL-AR-Sh CL-IT-Sh.
conclude experiments setting expansion based Title
Desc fields may help improve MRR improving ranking queries.
However, improvement covers limited number queries due coverage
270

fiCross-Lingual Search User-Generated Internet Video

1.0

1.0

0.0 0.5 1.0

expASR

0.0 0.5 1.0

expAll

0

10

20

30

40

50

60

0

10

30

40

50

60

50

60

1.0

1.0

0.0 0.5 1.0

expDesc

0.0 0.5 1.0

expTitle

20

0

10

20

30

40

50

60

0

10

20

30

40

Figure 5: MRR across QE runs CL-AR-Sh.
recall issues two fields (see Section 5). ASR fields seem better coverage
terms improving performance queries. However, coverage
negative sometimes QE model fails pick right terms.
Even ASR combined fields using exp-All approach, overall
performance still improve MRR respect baseline (exp-Non).
shown experiments, QE run uses single field shows positive MRR
values queries. However, per-field improvements decrease average
fields combined together (for example, exp-All MRR values shown Table
10 lower exp-Title CL-AR-Sh, exp-Desc CL-IT-Sh). combination
approach sufficiently effective overall retrieval performance still
negatively impacted noise present fields. next section, propose
novel approach alleviating issue selecting best source expansion.
technique adaptively predicts performance QE run, chooses source
likely achieve positive impact, elaborated next section.
7.2 Selecting Best Source Expansion
section introduce proposed approach improve QE effectiveness
CLIR task predicting whether QE needed not, is, select best source
expansion. particular, aim design robust QE approach prevent
minimize negative MRR changes exp-ASR runs have. argue
reductions caused two reasons:
query performance intiail run perfect known-item task.
case happen known relevant document ranked first position.
271

fiKhwileh, Ganguly, & Jones

added new expansion terms potentially disturb query performance
negatively impact retrieval effectiveness.
mentioned literature (Mitra, Singhal, & Buckley, 1998; Terra & Warren,
2005), QE effectiveness challenged query drift issue expansion terms informative, belong topic original
query. shown Table 1, ASR transcripts 20K length, Desc
long 3K length. long descriptive fields may cover many different topics
noise, may relevant topic query. common
dataset used task, since using user-generated videos length,
topic ASR quality video may specific consistent theme.
address issues propose modified QE technique use pre-retrieval
prediction technique decide whether QE likely beneficial source
reliable expansion. describe prediction technique used approach
Section 7.2.1. Section 7.2.2 explains proposed Adaptive QE algorithm. Section 7.2.3
reports experiments conduct investigate effectiveness approach.
7.2.1 Predicting QE Effectiveness
Predicting QE effectiveness proposed within concept selective QE framework Amati et al. (2004), widely used improve QE effectiveness.
main concept behind technique disable QE predicted negative impact first-pass retrieval performance. prediction based pre-retrieval
metrics assess application QE make decision whether apply not.
predictions based capturing query statistics collection,
query difficulty (Gianni, 2003), query clarity score (Cronen-Townsend, Zhou, &
Croft, 2002), query length (Amati et al., 2004). Full analysis effectiveness every
prediction techniques effectiveness contained work Ounis (2006).
chose use one successful predictors, Average Inverse Collection Term
Frequency (AvICTF) (He & Ounis, 2006). use prediction metric proposed
Adaptive QE technique predict whether QE needed field combination
reliable expansion.
AvICTF predictor previously tested field-based query expansion
several previous studies (He & Ounis, 2007a; Macdonald, He, Plachouras, & Ounis,
2005). reason choose predictor others definition higher
potential work well CLIR addresses term frequency aspect query.
principle, term frequency good indicator predicting translation errors.
example NEE error like word Aldirimovr would zero term frequency
used give indication overall query performance. AvICTF also low
cost metric uses local collection statistics make prediction, AvICTF
value higher tuned threshold, query predicted preform well
using first-pass retrieval therefore query expansion disabled. AvICTF
defined follows.
Q
coll
log2 Q ( token
)
F
AvICT F =
(7)
ql
272

fiCross-Lingual Search User-Generated Internet Video

tokencoll number tokens whole collection. ql query length, F
query term frequency whole collection. next sections, describe
adaptive QE algorithm implemented experimental investigation.
7.2.2 Adaptive Field-Based QE Technique
section propose adaptive QE technique, adaptivity concept inspired
work Ounis (2006). idea behind adaptive method automatically
set weights fields query running time. redesign concept
auto-select field combination best source expansion. adaptive
field-based QE algorithm implemented explained below:
indexing, algorithm processes possible fields combination separate
indexes, index one, two n fields, n total number fields
available collection.
retrieval query Q:
1. algorithm calculates prediction value V across every possible index,
selects index axindex highest prediction value axV .
2. algorithm makes decision whether apply QE comparing prediction value axV trained threshold. predicted
value less threshold, algorithm skips QE moves straight
retrieval.
3. prediction decision proceed QE, algorithm expands
query Q taking top terms top ranked documents axindex
run retrieval.
adaptive QE algorithm predicts effectiveness possible QE field run
chooses one likely useful query. words, adaptive
QE make use (exp-ASR, exp-Title, exp-Desc, exp-Non, exp-All) well
possible QE runs based two fields combination (exp-ASRDesc, expTitleASR, exp-TitleDesc) produce adaptive QE runs follows.
exp-Adapt-AR: Expands AR queries (CL-AR-Sh) using proposed adaptive
QE technique.
exp-Adapt-IT: Expands queries (CL-IT-Sh) using proposed adaptive QE
technique.
training prediction tuning threshold value, conduct two-fold holdout
evaluation query sets. divide CL-AR-Sh CL-IT-Sh sets training
testing query sets described below:
AR-train queries set: Contains 30 queries picked randomly CL-AR-Sh set.
IT-train queries set: Contains 30 queries picked randomly CL-IT-Sh set.
273

fiKhwileh, Ganguly, & Jones

Test queries set: Contains remaining 30 queries CL-AR-Sh query set
remaining 30 queries CL-IT-Sh query set. two sets used evaluate
proposed adaptive QE runs (exp-Adapt-AR exp-Adapt-IT).
training, similar work Ounis (2007a), perform manual
data sweeping range [3, 15] interval 1 training queries.
best threshold chosen exp-adapt-AR AR-train queries 6. best threshold
found exp-adapt-IT IT-train queries 9. difference threshold
values attributed distinct level translation qualities query
sets. discussed previously Section 5, queries better CLIR performance
AR ones, thus QE threshold different here.
7.2.3 Experimental Results Discussion
ran proposed adaptive QE using two runs: exp-Adapt-AR exp-Adapt-IT
explained previously. overall performance results two runs test queries
shown Table 12.
explained before, adaptive QE runs involve possible QE runs onesingle run, use one predicted better performance, selection
statistics run indicated Table 13. seen Table 12, overall MRR
Table 12: Results adaptive QE runs terms MRR Recall.

exp-Adapt-AR
exp-Adapt-IT

exp-Adapt-AR
exp-Adapt-IT

MRR
Adaptive
0.43614
0.4867
Recall
Exp-Adapt
22
22

(Baseline exp-Non)
0.3785
0.4580
Exp-Non (Baseline)
22
22

Table 13: selection statistics adaptive QE runs.

exp-ASRDesc
exp-TitleASR
exp-TitleDesc
exp-Title
exp-Desc
exp-ASR
exp-Non
exp-All

exp-Adapt-AR
4
7
4
1
2
3
5
4

exp-Adapt-IT
6
7
5
2
0
2
8
0

performance improves using proposed adaptive QE technique. Also, looking
recall performance Table 12, appears technique successfully improves
274

fiCross-Lingual Search User-Generated Internet Video

0.5
0.0
0.5
1.0

1.0

0.5

0.0

0.5

1.0

expAdaptAR

1.0

expAdaptIT

0

5

10

15

20

25

30

0

5

10

15

20

25

30

Figure 6: MRR (Baseline Vs Adaptive QE runs) languages using test queries

overall ranking maintaining recall level. attributed fact
runs able selectively make use best performance individual
run.
better understand improvement baseline, similar previous experiment Section 7.1, also calculate MRR values test query
AR-train IT-train query tests Figure 6. MRR values Figure 6 show
technique reduces chance significant reduction MRR baseline. Overall, appears QE approach robustness improve overall CLIR
performance selecting reliable source expansion individually query.

8. Conclusions Research
paper examined CLVR based text metadata fields Arabic-English, FrenchEnglish Italian-English known-item search task based blip10000 collection.
studied retrieval effectiveness challenges three different sources information:
ASR transcripts, challenged recognition errors, video titles,
short lack content, video descriptions, generic incomplete.
first set experiments analysed behaviour sources CLIR examining
CLIR robustness. found ASR transcript field lowest robustness across
fields performance drop significantly CLIR. explored field
combination retrieval explore performance together, investigation showed
giving higher weight titles fields gives improved CLIR performance.
general experiments show tuning retrieval settings give higher weight
towards fields lower CLIR robustness degrades retrieval effectiveness.
275

fiKhwileh, Ganguly, & Jones

work also investigated effectiveness automatic query expansion CLIR
setting task. found information sources varying reliability
query expansion, negative impact retrieval effectiveness
CLIR framework combined together. proposed adaptive query expansion technique automatically selects reliable source expansion based
well established query performance prediction technique. results experimental investigation show technique better robustness maintain retrieval
performance CLIR setting.
general, found noisy CLIR setting, translation errors,
transcription errors, incorrect incomplete UGC metadata varied document
lengths, might single best solution recommended used
answering queries even expanding/ improving them, rather adaptive approach
relies trained heuristics tuned specifically task collection.
concept adaptivity studied next investigations problem.
analysis CLIR effectiveness UGC video gives us suggestions
investigation many areas. One potential direction work automatically
assess quality ASR transcripts Description information assign weights
based quality measures, also explore task dependent tuning machine translation process. Studying CLIR effectiveness UGC sources evidence (such
visual information social interaction data include tweets, personal profile data)
would interesting follow investigation. Another area future study improve
document representation within framework developing document expansion
technique tuned improve overall CLIR robustness effectiveness
source evidence. Also, plan expand work studying effectiveness
available CLIR techniques Hybrid DT CLIR task.

References
Alqudsi, A., Omar, N., & Shaker, K. (2012). Arabic machine translation: survey. Artificial
Intelligence Review, 124.
Alzghool, M., & Inkpen, D. (2008). Cluster-based model fusion spontaneous speech
retrieval. Proceedings ACM SIGIR Workshop Searching Spontaneous
Conversational Speech, pp. 410. Citeseer.
Amati, G., Carpineto, C., & Romano, G. (2004). Query difficulty, robustness, selective
application query expansion. Advances information retrieval, pp. 127137.
Springer.
Amati, G., & Van Rijsbergen, C. J. (2002). Probabilistic models information retrieval
based measuring divergence randomness. ACM Transactions Information Systems (TOIS), 20 (4), 357389.
Amazon (2015). Amazon mechanical turk - welcome. https://www.mturk.com/. Retrieved:
2015-03-30.
Bagdouri, M., Oard, D. W., & Castelli, V. (2014). Clir informal content arabic forum
posts. Proceedings 23rd ACM International Conference Conference
Information Knowledge Management, pp. 18111814. ACM.
276

fiCross-Lingual Search User-Generated Internet Video

Ballesteros, L., & Croft, W. B. (1997). Phrasal translation query expansion techniques
cross-language information retrieval. ACM SIGIR Forum, Vol. 31, pp. 8491.
ACM.
Ballesteros, L., & Croft, W. B. (1998). Resolving ambiguity cross-language retrieval.
Proceedings 21st annual international ACM SIGIR conference Research
development information retrieval, pp. 6471. ACM.
Bellaachia, A., & Amor-Tijani, G. (2008). Enhanced query expansion english-arabic
clir. Database Expert Systems Application, 2008. DEXA08. 19th International
Workshop on, pp. 6166. IEEE.
Bendersky, M., Garcia-Pueyo, L., Harmsen, J., Josifovski, V., & Lepikhin, D. (2014).
next: retrieval methods large scale related video suggestion. Proceedings
20th ACM SIGKDD international conference Knowledge discovery data
mining, pp. 17691778. ACM.
Bing (2015). Bing translator. http://www.bing.com/translator/. Retrieved: 2015-03-30.
BlipTV (2015). Bliptv. https://www.blip.tv. Retrieved: 2015-03-30.
Buttcher, S., Clarke, C. L., & Cormack, G. V. (2010). Information retrieval: Implementing
evaluating search engines. Mit Press.
Carpineto, C., & Romano, G. (2012). survey automatic query expansion information
retrieval. ACM Computing Surveys (CSUR), 44 (1), 1.
Chelba, C., Bikel, D., Shugrina, M., Nguyen, P., & Kumar, S. (2012). Large scale language
modeling automatic speech recognition. arXiv preprint arXiv:1210.8440.
Chen, H.-H., Hueng, S.-J., Ding, Y.-W., & Tsai, S.-C. (1998). Proper name translation
cross-language information retrieval. Proceedings 17th international conference Computational linguistics-Volume 1, pp. 232236. Association Computational Linguistics.
CLEF (2015a). clef initiative (conference labs evaluation forum) - clef2009.
http://www.clef-initiative.eu/edition/clef2009. Retrieved: 2015-03-30.
CLEF (2015b). clef initiative (conference labs evaluation forum) - homepage.
http://www.clef-initiative.eu/. Retrieved: 2015-03-30.
Cronen-Townsend, S., Zhou, Y., & Croft, W. B. (2002). Predicting query performance.
Proceedings 25th annual international ACM SIGIR conference Research
development information retrieval, pp. 299306. ACM.
Eickhoff, C., Li, W., & de Vries, A. P. (2013). Exploiting user comments audio-visual
content indexing retrieval. Advances Information Retrieval, pp. 3849.
Springer.
Eskevich, M. (2014). Towards effective retrieval spontaneous conversational spoken content. Ph.D. thesis, Dublin City University.
Eskevich, M., & Jones, G. J. F. (2014). Exploring speech retrieval meetings using
ami corpus. Computer Speech & Language.
277

fiKhwileh, Ganguly, & Jones

Eskevich, M., Jones, G. J. F., Chen, S., Aly, R., Ordelman, R., & Larson, M. (2012a).
Search hyperlinking task mediaeval 2012..
Eskevich, M., Jones, G. J., Wartena, C., Larson, M., Aly, R., Verschoor, T., & Ordelman,
R. (2012b). Comparing retrieval effectiveness alternative content segmentation
methods internet video search. Content-Based Multimedia Indexing (CBMI),
2012 10th International Workshop on, pp. 16. IEEE.
Facebook video (2015). Facebook. https://www.facebook.com/facebook/videos. Retrieved: 2015-03-30.
Federico, M., Bertoldi, N., Levow, G.-A., & Jones, G. J. F. (2005). Clef 2004 cross-language
spoken document retrieval track. Multilingual Information Access Text, Speech
Images, pp. 816820. Springer.
Federico, M., & Jones, G. J. F. (2004). clef 2003 cross-language spoken document retrieval track. Comparative Evaluation Multilingual Information Access Systems,
pp. 646652. Springer.
Filippova, K., & Hall, K. B. (2011). Improved video categorization text metadata
user comments. Proceedings 34th international ACM SIGIR conference
Research development Information Retrieval, pp. 835842. ACM.
Gao, J., Nie, J.-Y., Xun, E., Zhang, J., Zhou, M., & Huang, C. (2001). Improving query
translation cross-language information retrieval using statistical models. Proceedings 24th annual international ACM SIGIR conference Research
development information retrieval, pp. 96104. ACM.
Gianni, A. (2003). Probabilistic Models Information Retrieval based Divergence
Randomness. Ph.D. thesis, Department Computing Science, University Glasgow.
Google (2015). Google translate. https://translate.google.com/. Retrieved: 2015-0330.
He, B., & Ounis, I. (2006). Query performance prediction. Information Systems, 31 (7),
585594.
He, B., & Ounis, I. (2007a). Combining fields query expansion adaptive query
expansion. Information processing & management, 43 (5), 12941307.
He, B., & Ounis, I. (2007b). setting hyper-parameters term frequency normalization information retrieval. ACM Transactions Information Systems (TOIS),
25 (3), 13.
Herbert, B., Szarvas, G., & Gurevych, I. (2011). Combining query translation techniques
improve cross-language information retrieval. Advances Information Retrieval,
pp. 712715. Springer.
Inkpen, D., Alzghool, M., Jones, G. J. F., & Oard, D. W. (2006). Investigating crosslanguage speech retrieval spontaneous conversational speech collection. Proceedings Human Language Technology Conference NAACL, Companion
Volume: Short Papers, pp. 6164. Association Computational Linguistics.
278

fiCross-Lingual Search User-Generated Internet Video

Jones, G. J., Zhang, K., Newman, E., & Lam-Adesina, A. M. (2007). Examining
contributions automatic speech transcriptions metadata sources searching
spontaneous conversational speech..
Kishida, K., & Kando, N. (2006). hybrid approach query document translation
using pivot language cross-language information retrieval. Springer.
Lam-Adesina, A. M., & Jones, G. J. (2006). Using string comparison context improved
relevance feedback different text media. String Processing Information
Retrieval, pp. 229241. Springer.
Langlois, T., Chambel, T., Oliveira, E., Carvalho, P., Marques, G., & Falcao, A. (2010).
Virus: video information retrieval using subtitles. Proceedings 14th International Academic MindTrek Conference: Envisioning Future Media Environments, pp.
197200. ACM.
Larson, M., Newman, E., & Jones, G. J. F. (2009). Overview videoclef 2008: Automatic
generation topic-based feeds dual language audio-visual content. Evaluating
Systems Multilingual Multimodal Information Access, pp. 906917. Springer.
Larson, M., Newman, E., & Jones, G. J. F. (2010). Overview videoclef 2009: New perspectives speech-based multimedia content enrichment. Multilingual Information
Access Evaluation II. Multimedia Experiments, pp. 354368. Springer.
Lee, C.-J., Chen, C.-H., Kao, S.-H., & Cheng, P.-J. (2010). translate translate?.
Proceedings 33rd international ACM SIGIR conference Research
development information retrieval, pp. 651658. ACM.
Lee, C.-J., & Croft, W. B. (2014). Cross-language pseudo-relevance feedback techniques
informal text. Advances Information Retrieval, pp. 260272. Springer.
Leveling, J., Zhou, D., Jones, G. J., & Wade, V. (2010). Document expansion, query
translation language modeling ad-hoc ir. Multilingual Information Access
Evaluation I. Text Retrieval Experiments, pp. 5861. Springer.
Macdonald, C., He, B., Plachouras, V., & Ounis, I. (2005). University glasgow trec
2005: Experiments terabyte enterprise tracks terrier.. TREC.
Macdonald, C., Plachouras, V., He, B., Lioma, C., & Ounis, I. (2006). University Glasgow WebCLEF 2005: Experiments per-field normalisation language specific
stemming. Springer.
Magdy, W., & Jones, G. J. (2014). Studying machine translation technologies largedata clir tasks: patent prior-art search case study. Information Retrieval, 17 (5-6),
492519.
McCarley, J. S. (1999). translate documents queries cross-language
information retrieval?. Proceedings 37th annual meeting Association
Computational Linguistics Computational Linguistics, pp. 208214. Association
Computational Linguistics.
MediaEval (2015). MediaEval Benchmarking Initiative Multimedia Evaluation. http:
//www.multimediaeval.org/. Retrieved: 2015-03-30.
279

fiKhwileh, Ganguly, & Jones

Mitra, M., Singhal, A., & Buckley, C. (1998). Improving automatic query expansion.
Proceedings 21st annual international ACM SIGIR conference Research
development information retrieval, pp. 206214. ACM.
Naaman, M. (2012). Social multimedia: highlighting opportunities search mining
multimedia data social media applications. Multimedia Tools Applications,
56 (1), 934.
Nikoulina, V., Kovachev, B., Lagos, N., & Monz, C. (2012). Adaptation statistical machine translation model cross-lingual information retrieval service context.
Proceedings 13th Conference European Chapter Association
Computational Linguistics, pp. 109119. Association Computational Linguistics.
Oard, D. W., & Diekema, A. R. (1998). Cross-language information retrieval. Annual review
information science technology, 33, 223256.
Oard, D. W., & Hackett, P. G. (1998). Document translation cross-language text retrieval university maryland. Information Technology: Sixth Text
REtrieval Conference (TREC-6), pp. 687696. US Dept. Commerce, Technology
Administration, National Institute Standards Technology.
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., Pecina, P., Soergel, D., Huang,
X., & Shafran, I. (2007). Overview clef-2006 cross-language speech retrieval
track. Evaluation multilingual multi-modal information retrieval, pp. 744
758. Springer.
Over, P., Awad, G. M., Fiscus, J., Antonishek, B., Michel, M., Smeaton, A. F., Kraaij, W.,
& Quenot, G. (2011). Trecvid 2010an overview goals, tasks, data, evaluation
mechanisms, metrics..
Pal, D., Mitra, M., & Datta, K. (2013). Improving query expansion using wordnet. CoRR,
abs/1309.4938.
Parton, K., McKeown, K. R., Allan, J., & Henestroza, E. (2008). Simultaneous multilingual search translingual information retrieval. Proceedings 17th ACM
conference Information knowledge management, pp. 719728. ACM.
Pecina, P., Hoffmannova, P., Jones, G. J., Zhang, Y., & Oard, D. W. (2008). Overview
clef-2007 cross-language speech retrieval track. Advances Multilingual
Multimodal Information Retrieval, pp. 674686. Springer.
Peters, C., Braschler, M., & Clough, P. (2012). Multilingual information retrieval:
research practice. Springer Science & Business Media.
Pirkola, A., Hedlund, T., Keskustalo, H., & Jarvelin, K. (2001). Dictionary-based crosslanguage information retrieval: Problems, methods, research findings. Information
retrieval, 4 (3-4), 209230.
Plachouras, V., He, B., & Ounis, I. (2004). University glasgow trec 2004: Experiments
web, robust, terabyte tracks terrier.. TREC.
Rogati, M., & Yang, Y. (2002). Cross-lingual pseudo-relevance feedback using comparable
corpus. Evaluation Cross-Language Information Retrieval Systems, pp. 151157.
Springer.
280

fiCross-Lingual Search User-Generated Internet Video

Salton, G., & Buckley, C. (1997). Improving retrieval performance relevance feedback.
Readings information retrieval, 24 (5), 355363.
Schmiedeke, S., Xu, P., Ferrane, I., Eskevich, M., Kofler, C., Larson, M. A., Esteve, Y.,
Lamel, L., Jones, G. J. F., & Sikora, T. (2013). Blip10000: social video dataset
containing spug content tagging retrieval. Proceedings 4th ACM
Multimedia Systems Conference, pp. 96101. ACM.
Sokolov, A., Hieber, F., & Riezler, S. (2014). Learning translate queries clir. Proceedings 37th international ACM SIGIR conference Research & development
information retrieval, pp. 11791182. ACM.
Terra, E., & Warren, R. (2005). Poison pills: harmful relevant documents feedback.
Proceedings 14th ACM international conference Information knowledge
management, pp. 319320. ACM.
Toderici, G., Aradhye, H., Pasca, M., Sbaiz, L., & Yagnik, J. (2010). Finding meaning
youtube: Tag recommendation category discovery. Computer Vision
Pattern Recognition (CVPR), 2010 IEEE Conference on, pp. 34473454. IEEE.
Tong, X., Zhai, C., Milic-Frayling, N., & Evans, D. A. (1996). Ocr correction query
expansion retrieval ocr dataclarit trec-5 confusion track report.. TREC.
TRECVID (2015). Trec video retrieval evaluation home page. http://trecvid.nist.gov/.
Retrieved: 2015-03-30.
Varshney, S., & Bajpai, J. (2014). Improving performance english-hindi cross language information retrieval using transliteration query terms. arXiv preprint
arXiv:1401.3510.
White, R. W., Oard, D. W., Jones, G. J. F., Soergel, D., & Huang, X. (2006). Overview
CLEF-2005 cross-language speech retrieval track. Springer.
Youtube (2015). Youtube. http://www.youtube.com/. Retrieved: 2015-03-30.
YouTube Press (2015). Statistics - YouTube.
statistics.html. Retrieved: 2015-03-30.

http://www.youtube.com/yt/press/

Zhou, D., Truran, M., Brailsford, T., Wade, V., & Ashman, H. (2012). Translation techniques cross-language information retrieval. ACM Computing Surveys (CSUR),
45 (1), 1.

281

fiJournal Artificial Intelligence Research 55 (2016) 95-130

Submitted 03/15; published 01/16

Translation Alters Sentiment
Saif M. Mohammad

saif.mohammad@nrc-cnrc.gc.ca

National Research Council Canada

Mohammad Salameh

msalameh@ualberta.ca

University Alberta

Svetlana Kiritchenko

svetlana.kiritchenko@nrc-cnrc.gc.ca

National Research Council Canada

Abstract
Sentiment analysis research predominantly English texts. Thus exist
many sentiment resources English, less languages. Approaches
improve sentiment analysis resource-poor focus language include: (a) translate
focus language text resource-rich language English, apply powerful
English sentiment analysis system text, (b) translate resources sentiment
labeled corpora sentiment lexicons English focus language, use
additional resources focus-language sentiment analysis system. paper
systematically examine options. use Arabic social media posts standin focus language text. show sentiment analysis English translations
Arabic texts produces competitive results, w.r.t. Arabic sentiment analysis. show
Arabic sentiment analysis systems benefit use automatically translated
English sentiment lexicons. also conduct manual annotation studies examine
sentiment translation different sentiment source word text.
especially relevant building better automatic translation systems. process,
create state-of-the-art Arabic sentiment analysis system, new dialectal Arabic sentiment
lexicon, first ArabicEnglish parallel corpus independently annotated
sentiment Arabic English speakers.

1. Introduction
term sentiment analysis commonly used refer goal determining
valence polarity piece text, whether positive, negative, neutral. However,
generally refer determining ones attitude towards particular target
topic. Automatic sentiment analysis text, especially social media posts, number
applications commerce, public health, public policy development. past two
decades, vast majority research English texts. Furthermore, many sentiment resources essential automatic sentiment analysis (e.g., sentiment lexicons) exist
English. Thus growing need effective methods analyzing text
languages Arabic Chinese, especially posts social media. improvements
statistical machine translation systems last decade, longer rely
strictly monolingual sentiment analysis systemsat least two alternatives may
viable:
(a) Run English sentiment analysis system, using English resources, English translations focus language text.

c
2016
National Research Council Canada. rights reserved.

fiMohammad, Salameh, & Kiritchenko

(b) Use focus-language sentiment analysis system employs focus-language resources
translations English resources focus language.
paper systematically examine options. use Arabic social media posts
specific instance focus language text. use state-of-the-art Arabic English
sentiment analysis systems well state-of-the-art Arabic-to-English English-toArabic translation systems. outline advantages disadvantages
methods listed above, conduct quantitative qualitative experiments determine
impact translation sentiment. benchmarks use manually determined sentiment
labels Arabic posts.
results help users determine methods best suited particular needs.
Along way, answer several research questions as:
1. sentiment prediction accuracy expected Arabic blog posts tweets
translated English (using current state-of-art techniques), run
state-of-the-art English sentiment analysis system?
2. performance compare current state-of-the-art Arabic
sentiment system?
3. loss sentiment predictability translating Arabic text English
automatically vs. manually?
4. difficult humans determine sentiment text automatically translated
another language native language?
5. dealing translated text, accurate determining sentiment Arabic text: (1) automatic sentiment analysis translated text, (2)
human annotation translated text sentiment?
6. Arabic posts sentiment analysis systems benefit additional training data
automatic translation sentiment-labeled English tweets additional
sentiment lexicons automatic translations existing English lexicons?
7. automatic translations words sentiment associations original source words (as listed source language lexicons, say)? not,
different reasons lead discrepancies?
inferences drawn experiments necessarily apply language pairs
ArabicEnglish. Languages differ significantly terms characteristics
impact sentiment. However, similar set experiments used language
pairs well determine impact translation sentiment.
experiments two different datasets, show sentiment analysis
English translations Arabic texts produces competitive results, w.r.t. Arabic sentiment
analysis. also show translation (both manual automatic) introduces marked
changes sentiment carried text; positive negative texts often translated
texts neutral. also find certain attributes automatically translated
text mislead humans regards true sentiment source text, seem
affect automatic sentiment analysis system.
show difficult obtain improvement Arabic sentiment analysis
systems simply adding training data translation existing labeled English
96

fiHow Translation Alters Sentiment

corpus, systems benefit use automatically translated English sentiment
lexicons. examining subset translated lexicon entries show close 90%
entries valid even focus language. word automatic translation may
convey sentiment poor translation quality word
translation used differently two languages.
process developing experiments study translation impacts sentiment, created new dialectal Arabic sentiment lexicon state-of-the-art Arabic
sentiment analysis system porting NRC-Canadas competition winning system (Mohammad, Kiritchenko, & Zhu, 2013; Kiritchenko, Zhu, & Mohammad, 2014b) Arabic.
also created substantial amount sentiment labeled data pertaining Arabic social
media texts English translations. first resource text
one language translations another language (both manually automatically
produced) manually labeled sentiment. sentiment lexicons
sentiment-labeled corpora made freely available.1
begin survey related work sentiment analysis English, sentiment analysis Arabic, work cross-lingual sentiment analysis (Section 2). Section 3
present core method systematically study impact translation sentiment.
Section 4 describe developed components needed experiments:
translations Arabic texts English, translations English resources Arabic,
sentiment-labeled data Arabic English, English sentiment analysis system,
Arabic sentiment analysis system. Section 5, present results experiments
translating focus language text English application English sentiment analysis
system. also conduct qualitative quantitative studies investigate
reasons sentiment impacted translation. example, find sentiment
expressions often mistranslated neutral expressions, however automatic sentiment
analysis systems able recover extent errors. Section 6,
present results experiments translating English resources Arabic using
features drawn Arabic sentiment analysis. also describe manual annotation study extent automatic Arabic translations sentiment
source English words. Finally, present conclusions future directions Section 7.2

2. Related Work
last decade, explosion work exploring various aspects
sentiment analysis English texts: detecting subjective objective sentences; classifying
sentences positive, negative, neutral; detecting person expressing sentiment
target sentiment; applying sentiment analysis health, commerce,
disaster management. Surveys Pang Lee (2008), Liu Zhang (2012),
Mohammad (2016) give details many approaches. However, less work
Arabic texts. sub-sections below, briefly outline relevant sentiment analysis
research English texts, Arabic texts, texts one language using resources
another (multilingual sentiment analysis).
1. http://www.purl.org/net/ArabicSA
2. early findings work first presented Salameh, Mohammad, Kiritchenko (2015).

97

fiMohammad, Salameh, & Kiritchenko

2.1 Sentiment Analysis English Social Media
English sentiment analysis systems applied many different kinds texts including customer reviews, newspaper headlines (Bellegarda, 2010), novels (Boucouvalas,
2002; John, Boucouvalas, & Xu, 2006; Mohammad & Yang, 2011), emails (Liu, Lieberman,
& Selker, 2003; Mohammad & Yang, 2011), blogs (Neviarouskaya, Prendinger, & Ishizuka,
2011; Genereux & Evans, 2006; Mihalcea & Liu, 2006), tweets (Mohammad, 2012).
Often systems cater specific needs text formality versus
informality, length utterances, etc. Sentiment analysis systems developed specifically
tweets include Go, Bhayani, Huang (2009), Pak Paroubek (2010), Agarwal,
Xie, Vovsha, Rambow, Passonneau (2011), Thelwall, Buckley, Paltoglou (2011),
Brody Diakopoulos (2011), Aisopos, Papadakis, Tserpes, Varvarigou (2012), Bakliwal, Arora, Madhappan, Kapre, Singh, Varma (2012). survey Martnez-Camara,
Martn-Valdivia, Urenalopez, Montejoraez (2012) provides overview research
sentiment analysis tweets. last two years, several shared tasks sentiment
analysis organized Conference Semantic Evaluation Exercises (SemEval),
allowed comparison different approaches common datasets different
domains (Wilson, Kozareva, Nakov, Rosenthal, Stoyanov, & Ritter, 2013; Rosenthal, Ritter,
Nakov, & Stoyanov, 2014; Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos,
& Manandhar, 2014). NRC-Canada system (Kiritchenko et al., 2014b) ranked first
competitions, use experiments. Notably, system makes extensive
use sentiment lexicons handles negation appropriately.3 summarize system
Section 4.3.
2.2 Sentiment Analysis Arabic Social Media
Sentiment analysis Arabic social media texts several challenges. text often
regional Arabic dialect rather Modern Standard Arabic (MSA). Unlike MSA
standardized form Arabic, dialectal Arabic spoken form Arabic lacks
strict writing standards. text often includes words languages Arabic
multiple scripts may used express Arabic foreign words. addition, Arabic
morphologically complex language. Negation MSA expressed negation
particles, dialects (Egyptian) expressed using circumfix.
studies tackling sentiment analysis Arabic texts (Ahmad,
Cheng, & Almas, 2006; Farra, Challita, Assi, & Hajj, 2010; Abdul-Mageed, Diab, & Korayem, 2011; Badaro, Baly, Hajj, Habash, & El-Hajj, 2014). also shared task
detecting sentiment intensity Arabic phrases (Kiritchenko, Mohammad, & Salameh,
2016).4 works closely related studies sentiment analysis Arabic social media (Al-Kabi, Gigieh, Alsmadi, Wahsheh, & Haidar, 2013; Ahmed, Pasquier,
& Qadah, 2013; El-Beltagy & Ali, 2013; Mourad & Darwish, 2013; Abdul-Mageed, Diab,
& Kubler, 2014). review existing Arabic sentiment analysis systems designed specifically Arabic social media datasets. Abdul-Mageed et al. (2014) trained
SVM classifier manually labeled dataset applied two-stage classification first
3. Zhu, Guo, Mohammad, Kiritchenko (2014a) show impact negation cannot properly
captured simply reversing polarity scope.
4. http://alt.qcri.org/semeval2016/task7/

98

fiHow Translation Alters Sentiment

separates subjective objective sentences classifies subjective positive
negative instances. authors compiled several datasets multiple social media
resources included chatroom messages, tweets, forum posts, Wikipedia Talk pages.
datasets manually labeled two native Arabic speakers. However, resources
made publicly available yet. Abdul-Mageed Diab (2014) also used data
several resources compile build SANA, large-scale, multi-genre, multidialect
lexical resource. SANA covers Egyptian Levantine dialects well MSA. Abbasi,
Chen, Salem (2008) deployed Arabic morphological, syntactic stylistic features
sentiment analysis Arabic web forums. efficient feature selection, adopted
Entropy Weighted Genetic Algorithm (EWGA). Mourad Darwish (2013) trained SVM
Naive Bayes classifiers Arabic tweets annotated two native Arabic speakers.
compare systems performance Section 4.4.2.
Refaee Rieser (2014b) manually annotated tweets sentiment two native Arabic
speakers. used SVM classify tweets two-stage approach: polar vs. neutral,
positive vs. negative. test system dataset. However, dataset
provided superset data originally used experiments (Refaee &
Rieser, 2014a). Thus, performances automatic sentiment analysis systems applied
two sets directly comparable.
2.3 Multilingual Sentiment Analysis
Work multilingual sentiment analysis mainly addressed mapping sentiment resources
English morphologically complex languages. Mihalcea, Banea, Wiebe (2007)
used English resources automatically generate Romanian subjectivity lexicon using
EnglishRomanian dictionary. generated lexicon used classify Romanian
text. Balahur Turchi (2014) conducted study assess performance statistical
sentiment analysis techniques machine-translated texts. Opinion-bearing English phrases
New York Times Text (20022005) corpus split training test datasets.
English sentiment analysis system trained training dataset prediction
accuracy test set found 68%. Next, training test datasets
automatically translated German, Spanish, French using publicly available
machine-translation engines (Google, Bing, Moses). translated test sets
manually corrected errors. German, Spanish, French, sentiment analysis
system trained translated training set language tested
translated-and-corrected test set. authors observe German, Spanish,
French sentiment analysis systems obtain accuracies low sixties (and thus
much lower 68%). Contrary work, study uses original text focus
language, manual automatic translations, well manual automatic
sentiment assignments systematically examine effect translation sentiment.
Further, use several external sentiment resources well translations within
state-of-the-art sentiment systems. Also, German, Spanish, French much closer
English, Dialectal Arabic English. Finally, deal noisy social media texts
opposed polished news media texts. also exists research using sentiment
analysis improve machine translation, work Chen Zhu (2014),
beyond scope paper.
99

fiMohammad, Salameh, & Kiritchenko

3. Method Determining Impact Translation Sentiment
systematically study impact translation sentiment analysis, propose two
experimental setups corresponding (a) (b) described Introduction:
Setup A: Translate Arabic text English (manually automatically) annotate English text sentiment (manually automatically). Compare
sentiment labels assigned translated English text manual sentiment annotations Arabic text. similar sentiment annotations are, less
impact translation.
Setup B: Translate sentiment annotated corpora lexicons English Arabic
(automatically), use additional resources supervised Arabic sentiment
classification. Compare sentiment labels assigned system manual
sentiment annotations Arabic text. similar sentiment annotations
are, less impact translation.
3.1 Impact Translation Sentiment - Setup A: Translating Focus
Language Text English
Setup explore translation text Arabic English impacts sentiment. Specifically, analyze performance English sentiment analysis system,
using English resources, automatic translations Arabic social media texts. setup
outlined below:
Identify compile Arabic social media dataset. refer Ar. [Ar
comes first two letters Arabic.]
Manually translate Ar English. refer English translations
En(Manl.Trans.). [Manl. manual, Trans. translations.]
Automatically translate Ar English. refer English translations
En(Auto.Trans.). [Auto. automatic.]
Manually annotate Ar sentiment. refer sentiment-labeled dataset
Ar(Manl.Sent.).
Manually annotate English datasets [En(Manl.Trans.) En(Auto.Trans.)]
sentiment, creating En(Manl.Trans., Manl.Sent.) En(Auto.Trans., Manl.Sent.),
respectively.
Run state-of-the-art Arabic sentiment analysis system Ar, creating Ar(Auto.Sent.).
acts baseline system.
Run state-of-the-art English sentiment analysis system English datasets
[En(Manl.Trans.) En(Auto.Trans.)], creating En(Manl.Trans., Auto.Sent.)
En(Auto.Trans., Auto.Sent.), respectively.
100

fiHow Translation Alters Sentiment

Figure 1: Setup A: Translating focus language text English. compare sentiment
labels Ar(Manl.Sent.) (shown shaded box) datasets shown
right side figure. Ar(Manl.Sent.) original Arabic text manually
annotated sentiment.

Figure 1 depicts setup. various sentiment-labeled datasets created,
compare pairs datasets draw inferences. example, comparing labels
Ar(Manl.Sent.) En(Manl.Trans., Manl.Sent.) show different sentiment
labels tend text manually translated Arabic English. comparison also show, example, whether positive tweets tend translated
neutral tweets, extent. Furthermore, results demonstrate feasible
first translate Arabic text English use automatic sentiment analysis
(Ar(Manl.Sent.) vs. En(Auto.Trans., Auto.Sent.)). Section 5, provide analysis
several comparisons two different Arabic social media datasets.
DATA RESOURCES: list corpora lexicons used Setup
shown Table 1. Since manual translation text Arabic English costly
exercise, chose, experiments, existing Arabic social media dataset
already translated BBN Arabic-DialectEnglish Parallel Text (Zbib, Malchiodi,
Devlin, Stallard, Matsoukas, Schwartz, Makhoul, Zaidan, & Callison-Burch, 2012).5
contains 3.5 million tokens Arabic dialect sentences English translations.
use randomly chosen subset 1200 Levantine dialectal sentences, refer
BBN posts BBN dataset, experiments.
also conduct experiments dataset 2000 tweets originating Syria (a
country Levantine dialectal Arabic commonly spoken). tweets collected
May 2014 polling Twitter API. refer dataset Syrian tweets
Syria dataset.6 Note, however, manual translations Syrian tweets
5. https://catalog.ldc.upenn.edu/LDC2012T09
6. number instances chosen BBN Syria datasets somewhat arbitrary; however,
constrained funds available manual sentiment annotations datasets
translations.

101

fiMohammad, Salameh, & Kiritchenko

Resource
positive
a. Focus language (Arabic) corpora
(and English translations)
BBN posts
498
Syrian tweets
448

Number instances
negative
neutral

575
1,350

126
202

total

1,199
2,000

b. Resources explored baseline Arabic sentiment system
Automatic lexicons:
Arabic Emoticon Lexicon
Arabic Hashtag Lexicon
Arabic Hashtag Lexicon (dialectal)

22,962
13,118
11,941

20,342
8,846
8,179

-

43,304
21,964
20,128

c. Resources explored English sentiment system
Manual lexicons:
Bing Lius Lexicon
MPQA Subjectivity Lexicon
NRC Emotion Lexicon
Automatic lexicons:
NRC Emoticon Lexicon
NRC Hashtag Lexicon

2,006
2,718
2,317

4,783
4,911
3,338

570
8,527

6,789
8,199
14,182

38,312
32,048

24,156
22,081

-

62,468
54,129

Table 1: Resources used Setup A. (Note 1: focus language corpora split test
training folds part cross-validation experiments. Note 2: NRC Emotion
Lexicon NRC Emoticon Lexicon similar spelling,
two different lexicons.)

available. automatic sentiment analysis experiments, focus language corpora
(BBN dataset Syria dataset) split test training folds part crossvalidation experiments.
use number manually automatically created English sentiment lexicons
English sentiment analysis system (as shown row c. Table 1). compare
accuracies obtained English sentiment analysis system Arabic sentiment
analysis system, create new Arabic wordsentiment association lexicons
described Section 4.4.1. lexicons called Arabic Hashtag Lexicon,
Dialectal Arabic Hashtag Lexicon, Arabic Emoticon Lexicon.
3.2 Impact Translation Sentiment - Setup B: Translating English
Sentiment Resources Focus Language
Setup B explore translation text English Arabic impacts sentiment. Specifically, analyze change performance Arabic sentiment analysis
system allowed also make use automatic translations English sentiment
lexicons corpora. setup outlined below:
Identify Arabic social media dataset. Manually annotate sentiment split
corpus test training subsets. refer test corpus Ar
sentiment-labeled test corpus Ar(Manl.Sent.).
102

fiHow Translation Alters Sentiment

Identify create suitable Arabic sentiment lexicons.
Identify suitable English sentiment lexicon(s) corpus English tweets labeled
sentiment.
Automatically translate English corpus lexicon Arabic. refer
Arabic translation corpus Ar(Auto.Trans.) Arabic translation
lexicon ArLex(Auto.Trans.) [Auto. automatic; Lex lexicon.]
Train separate Arabic sentiment analysis systems using following sets
resources:
1. Arabic training corpus only;
2. Arabic training corpus Arabic translation English corpus;
3. Arabic training corpus Arabic sentiment lexicon;
4. Arabic training corpus, Arabic sentiment lexicon, Arabic translation English lexicon.
Apply Arabic sentiment analysis systems test set Ar.
Figure 2 depicts setup. various sentiment-labeled datasets created,
compare automatically labeled sets manual sentiment annotations
Ar(Manl.Sent.), calculate accuracies automatic labeling. accuracies
help answer questions as: useful automatically translated English sentiment resources Arabic sentiment analysis. also perform manual annotation study
subset automatically translated resources determine different kinds errors
result automatic translations. Section 6, provide analysis
experiments different English resources.
DATA RESOURCES: list corpora lexicons used Setup B
shown Table 2. chose Arabic portion BBN Arabic-DialectEnglish Parallel
Text primary Arabic social media dataset Setup B. Specifically, use
subset 1200 Levantine dialectal sentences, refer BBN posts BBN
dataset. English corpus, choose SemEval-2013 Task 2 (Sentiment Analysis
Twitter) training dataset (Wilson et al., 2013) experiments BBN
dataset, dataset social media posts. Further, already manually annotated
sentiment.
several sentiment lexicons English. chose four manually created lexicons experiments: NRC Emotion Lexicon (Mohammad & Turney, 2010; Mohammad
& Yang, 2011), Bing Liu Lexicon (Hu & Liu, 2004), MPQA Subjectivity Lexicon (Wilson,
Wiebe, & Hoffmann, 2005), AFINN (Nielsen, 2011). also experiment
lexicons automatically generated tweets Kiritchenko et al. (2014b): NRC Emoticon Lexicon (a.k.a. Sentiment140 lexicon) NRC Hashtag Sentiment Lexicon.
lexicons helped obtain best results sentiment analysis shared task competitions (Mohammad et al., 2013; Kiritchenko, Zhu, Cherry, & Mohammad, 2014a; Zhu, Kiritchenko, &
Mohammad, 2014b).
103

fiMohammad, Salameh, & Kiritchenko

Manual

Arabic'Test'
Set'
Sen7ment''
Assignment'

Manual)

Automatic:!
Baseline

Arabic'Test'
Set'
Arabic'Training'
Set'

Sen7ment''
Assignment'

Arabic'Test'
Set'
Arabic'Training'
Set'
English'Training'
Set'

Automa'c))
Transla'on)

Ar(Auto.Trans)'
addi7onal'training'data'

Sen7ment''
Assignment'

Arabic'Test'
Set'
Arabic'Training'
Set'

Sen7ment''
Assignment'

Ar(Manl.Sent.)'
gold'data'

Automa'c)

Ar(Auto.Sent.)'

Automatic: !
Baseline + Translated Corpus
Automa'c)

Ar(Auto.Sent.)'

Automatic:!
Baseline + Arabic lexicon
Automa'c)

Ar(Auto.Sent.)'

Arabic'Lexicon'

Arabic'Test'
Set'

Arabic'Training'
Set'

Sen7ment''
Assignment'

Arabic'Lexicon'
English'
Lexicon'

Automa'c))
Transla'on)

Automatic:!
Baseline + Arabic lexicon +
Translated lexicon
Automa'c)

Ar(Auto.Sent.)'

ArLex(Auto.Trans)'
addi7onal'lexicon'

Figure 2: Setup B: Translating English sentiment resources focus language.
compare sentiment labels Ar(Manl.Sent.) (shown shaded box)
datasets shown right side figure. Ar(Manl.Sent.) original
Arabic text manually annotated sentiment.

104

fiHow Translation Alters Sentiment

Resource

Number instances
positive negative neutral

a. Focus language (Arabic) corpora
BBN posts

498

total

575

126

1,199

8,179

-

20,128

b. Resources used Arabic sentiment system
Automatic lexicons:
Arabic Hashtag Lexicon (dialectal)

11,941

c. English resources translated Arabic
Sentiment-labeled corpus:
SemEval-2013 Task 2 corpus

3,620

1,549

4,743

9,912

Manual lexicons:
AFINN
Bing Lius Lexicon
MPQA Subjectivity Lexicon
NRC Emotion Lexicon

878
2,006
2,718
2,317

1,598
4,783
4,911
3,338

570
8,527

2,476
6,789
8,199
14,182

15,210
18,341

11,530
14,241

-

26,740
32,582

Automatic lexicons:
NRC Emoticon Lexicon
NRC Hashtag Lexicon

Table 2: Resources used Setup B. (Note 1: automatic sentiment analysis experiments, focus language corpora split test training folds part
cross-validation experiments. Note 2: Automatic translations English resources Arabic done using Google Translate. entries, especially
automatic lexicons, left untranslated Google Translate
information them.)

4. Capabilities Needed Performing Experiments
experimental setups described involve several component tasks: generating
translations manually automatically (Section 4.1), manually annotating Arabic
English texts sentiment (Section 4.2), automatic sentiment analysis English texts
(Section 4.3), automatic sentiment analysis Arabic texts (Section 4.4). describe
sub-sections below.
4.1 Generating Translations
Setup requires certain Arabic corpora translated English, whereas Setup B requires
English resources (corpus lexicon) translated Arabic. two subsections describe obtained translations.
4.1.1 Generating English Translations
BBN dialectal Arabic dataset comes manual translations English. generate automatic English translations BBN posts Syrian tweets employing
in-house multi-stack phrase-based machine translation (MT) system, Portage (Cherry
& Foster, 2012). statistical machine translation (SMT) system trained data
105

fiMohammad, Salameh, & Kiritchenko

OpenMT 2012. preprocess training data segmenting Arabic source side
training data MADA 3.2 (Habash, Rambow, & Roth, 2009), using Penn Arabic
Treebank (PATB) segmentation scheme recommended El Kholy Habash (2012).


@ @ @) Ya (
) used interchangeably, normalize characters bare Alif @ dotless Ya , respectively.

Since different forms Arabic characters Alif ( @

normalization decreases sparcity Arabic tokens improves translation.
English side training data lower-cased tokenized stripping punctuation
marks. set decoders stack size 10000 distortion limit 7. replace
out-of-vocabulary words translated text UNKNOWN token (which shown
annotators). decoders log-linear model tuned MIRA (Chiang, Marton, &
Resnik, 2008; Cherry & Foster, 2012). KN-smoothed 5-gram language model trained
English Gigaword target side parallel data.
4.1.2 Generating Arabic Translations
Setup B, run SemEval-2013 English tweets dataset (Wilson et al., 2013)
Google Translate obtain Arabic translations.7 Even though Google Translate phrasebased statistical MT system primarily designed translate sentences, also
provide one-word translations. translations often word representing predominant sense word source language. Thus also use Google Translate
translate Arabic words English sentiment lexicons listed Table 2.
Note Google Translate unable translate words lexicons. Table 2
gives number words translated well break sentiment category (positive, negative, neutral). translated lexicons made freely available.8
generate manual translations lexicons, Section 6.1, describe study
automatic translations examined Arabic speaker.
4.2 Creating Sentiment Labeled Data Arabic English
Manual sentiment annotations performed crowdsourcing platform CrowdFlower9
three BBN datasets two Syria datasets:
1. Original Arabic posts (the BBN Syria datasets), annotated Arabic speakers.
2. Manual English translations Arabic posts (available BBN dataset),
annotated English speakers.
3. Automatic English translations Arabic posts (the BBN Syria datasets), annotated English speakers.
Questionnaire 3. shown below. questionnaire 2. similar, except
states text created manual translation Arabic posts. questionnaire 1. also similar 3., except Arabic states
7. Since in-house system, Portage, designed translate text Arabic English,
way round, use publicly available Google Translate experiments Setup B.
Google Translate: https://translate.google.com
8. http://www.purl.com/net/lexicons
9. http://www.crowdflower.com

106

fiHow Translation Alters Sentiment

target texts posts social media (no mention translations questionnaire).

Questionnaire 3: Judge sentiment posts
General Instructions:
- Attempt HITs native speaker English.
- responses confidential.
- possible occasional post may swear word express something offensive.
text different something one might find public forum.
Task-Specific Instructions:
given English sentences translated Arabic using automatic machine
translation system. translations may ungrammatical hard understand. translation system unable translate word, shows word UNK symbol, representing
unknown.
Select option best captures sentiment conveyed sentence:
- positive
- negative
- neutral
- uncertain positive negative
Select positive sentence shows positive attitude (possibly toward object event).
example:
- hope every year good shape
- honest dont know say story, nicest sensation
Select negative sentence shows negative attitude (possibly toward object event).
example:
- new Spiderman movie terrible
- government make us bankrupt
Select neutral sentence shows neutral attitude (possibly toward object event).
example:
- Add spices, onion sauce
- expresses truly relation Israel
Select Uncertain positive negative sentence shows uncertain attitude
sentence expresses positive negative attitude. example:
- strange forward glass car broken yet
- like ice cream hate chocolate chips
Actual HIT
sentence translated English Arabic computer algorithm. sentence
may ungrammatical hard follow. Additionally, system unable translate
Arabic words. words shown UNK symbol.
Sentence: Especially companies acknowledge soft target UNK penetrate serwer commercial network .

107

fiMohammad, Salameh, & Kiritchenko

Select option best captures sentiment conveyed sentence:
- positive
- negative
- neutral
- uncertain positive negative

post annotated least ten annotators majority sentiment label
chosen. small number instances annotated label uncertain
positive negative. instances set aside included analysis.
Table 3 shows class distribution sentiment labels various datasets. Observe
rows a. d. neutral tweets constitute 10% original data
BBN Syria datasets. Syrian tweets much higher percentage negative posts,
whereas BBN data, percentages positive negative posts comparable.
Rows b., c., e. show translated texts tend lose sentiment information
relatively higher percentage neutral instances translated text
original text.
post, determine count frequent annotation divided
total number annotations. score averaged posts determine interannotator agreement shown last column Table 3. use agreement score
benchmark compare performance automatic sentiment systems (described below).
4.3 English Sentiment Analysis
use English-language sentiment analysis system developed NRC-Canada (Kiritchenko et al., 2014b) experiments. system obtained highest scores two
recent international competitions sentiment analysis tweets SemEval-2013 Task 2
(Wilson et al., 2013) SemEval-2014 Task 9 (Rosenthal et al., 2014). briefly describe
system below; details, refer reader work Kiritchenko, Zhu,
Mohammad (2014b).
linear-kernel Support Vector Machine (Chang & Lin, 2011) classifier trained
available training data. classifier leverages variety surface-form, semantic,
sentiment lexicon features described below. sentiment lexicon features derived
existing, general-purpose, manual lexicons, namely NRC Emotion Lexicon (Mohammad &
Turney, 2010, 2013), Bing Liu Lexicon (Hu & Liu, 2004), MPQA Subjectivity Lexicon
(Wilson et al., 2005).
NRC Emotion Lexicon sentiment emotion labels 14,000 words
(Mohammad & Turney, 2010; Mohammad & Yang, 2011). labels compiled
Mechanical Turk annotations.10 Bing Liu Lexicon 6,800 words
sentiment labels (Hu & Liu, 2004). lexicon originally used detecting
sentiment customer reviews. MPQA Subjectivity Lexicon, draws
General Inquirer sources, sentiment labels 8,000 words (Wilson
et al., 2005).
10. https://www.mturk.com/mturk/welcome

108

fiHow Translation Alters Sentiment

BBN dataset
a. Ar(Manl.Sent)
b. En(Manl.Trans., Manl.Sent)
c. En(Auto.Trans., Manl.Sent)
Syria dataset
d. Ar(Manl.Sent)
e. En(Auto.Trans., Manl.Sent)

positive

negative

neutral

agreement

41.50
35.00
36.17

47.92
43.25
36.50

10.58
21.75
27.34

73.82
68.00
65.70

22.40
14.25

67.50
66.15

10.10
19.60

79.05
76.10

Table 3: Class distribution (in percentage) sentiment annotated datasets.
also used automatically generated, tweet-specific lexicons NRC Hashtag Sentiment Lexicon NRC Emoticon Lexicon (Kiritchenko et al., 2014b).11 sub-section
gives details lexicons generated.
4.3.1 Generating English Sentiment Lexicons
ablation experiments study Mohammad et al. (2013) showed NRCCanada sentiment analysis system benefited use NRC Hashtag Sentiment Lexicon NRC Emoticon Lexicon. NRC Hashtag Sentiment Lexicon
created follows. list 77 seed words, synonyms positive negative,
compiled Rogets Thesaurus. Then, Twitter API polled collect tweets
words hashtags. tweets positive hashtag emoticon
express positive sentiment. similarly tweets negative hashtag
emoticon express negative sentiment. Hashtags emoticons used tweets
complex ways, example sarcastic tweets. Nonetheless, majority tweets
positive hashtag emoticon shown positive (and similarly negative
hashtags emoticons). Thus algorithm extract positive negative terms
tweets considers tweet positive positive hashtag negative
negative hashtag. term tweet set, sentiment score computed measuring PMI (pointwise mutual information) term positive negative
category:
SenScore (w) = P I(w, pos) P I(w, neg)

(1)

w term lexicon. P I(w, pos) PMI score w positive
class, P I(w, neg) PMI score w negative class. positive
SenScore (w) implies word tends co-occur positive seeds
negative seeds. Thus, likely associated positive sentiment. Similarly,
negative score suggests word tends co-occur negative seeds
positive seeds. Thus, likely associated negative sentiment. magnitude
SenScore (w) indicates strength association.
NRC Emoticon Lexicon (aka Sentiment140 Lexicon) created similar fashion
using emoticons :) :( seeds.
11. http://www.purl.com/net/lexicons

109

fiMohammad, Salameh, & Kiritchenko

4.3.2 Pre-processing Feature Generation
following pre-processing steps performed. URLs user mentions normalized
http://someurl @someuser, respectively. Tweets tokenized part-of-speech
tagged CMU Twitter NLP tool (Gimpel, Schneider, OConnor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, & Smith, 2011). Then, tweet represented
feature vector.
features:
Word character ngrams;
POS: number occurrences part-of-speech tag;
Negation: number negated contexts. Negation also affects ngram features:
word w becomes w NEG negated context;
Automatic sentiment lexicons: token w occurring sentence present
lexicon, sentiment score score(w) used compute:
- number tokens score(w) 6= 0;
P
- total score = wtweet score(w);
- maximal score = max wtweet score(w);
- score last token tweet.
features calculated lexicon separately.
Manually created sentiment lexicons: three manual sentiment lexicons,
following features computed:
- sum positive scores tweet tokens;
- sum negative scores tweet tokens.
Style features: presence/absence all-cap words, hashtags, punctuation marks,
emoticons, elongated words.
4.4 Arabic Sentiment Analysis
build Arabic sentiment analysis system reconstructing NRC-Canada English
system deal Arabic text. extracts feature set described Section 4.3.2. also generated word-sentiment association lexicons using process
described Section 4.3.1, Arabic words Arabic tweets (more details subsection below). preprocess Arabic text tokenizing CMU Twitter NLP tool
deal specific tokens URLs, usernames, emoticons. use MADA
generate lemmas. Finally, normalize different forms Alif Ya bare Alif
dotless Ya.
4.4.1 Generating Arabic Sentiment Lexicons
emoticons hashtag words tweet often act sentiment labels rest
tweet. use idea, commonly referred distant supervision (Go et al., 2009),
generate three different Arabic sentiment lexicons:
110

fiHow Translation Alters Sentiment

Lexicon
Arabic Emoticon Lexicon
Arabic Hashtag Lexicon
Arabic Hashtag Lexicon (dialectal)

# seeds
pos neg
12
11
109 121
135 348

# tweets
pos
neg
520,000 455,282
209,784
37,209
177,556
34,705

# entries lexicon
unigram bigram trigram
43,309 229,747 325,366
22,007 128,814 233,481
20,128
93,613 159,986

Table 4: Details Arabic Hashtag Lexicon, Arabic Emoticon Lexicon,
Dialectal Arabic Hashtag Lexicon.

Arabic Emoticon Lexicon: collected close one million Arabic tweets
emoticons (:) :(). purposes generating sentiment lexicon, :)
considered positive label (pos) :( considered negative label (neg).
word w, occurred least five times tweets, sentiment score
calculated using formula shown (same described Section 4.3.1,
proposed first Mohammad et al., 2013):
SentimentScore(w) = P I(w, pos) P I(w, neg)

(2)

PMI stands Pointwise Mutual Information. refer resulting entries
Arabic Emoticon Lexicon.
Arabic Hashtag Lexicon: NRC-Canada system used 77 positive negative
seed words generate English NRC Hashtag Sentiment Lexicon (Mohammad
et al., 2013; Kiritchenko et al., 2014b). translated English seeds Arabic
using Google Translate. Among translations provided, chose words
less ambiguous tended strong sentiment Arabic texts. increase
coverage seed list, manually added different inflections translations.
polled Twitter API period June August 2014 collected tweets
included seed words hashtags. filtering duplicate tweets
retweets, ended 209,784 positive unique tweets 37,209 negative unique
tweets. unigram, bigram, trigram, w, occurred least five times
tweets, SenScore (w) calculated described Section 4.3.1.
refer lexicon Arabic Hashtag Lexicon.
Arabic Hashtag Lexicon (Dialectal): Refaee Rieser (2014a) manually created
small sentiment lexicon 483 dialectal Arabic sentiment words tweets. used
words seeds collect tweets contain them, generated PMI-based
sentiment lexicon described above. refer lexicon Dialectal
Arabic Hashtag Lexicon Arabic Hashtag Lexicon (dialectal).
number seeds tweets used create Arabic Hashtag Lexicon, Arabic
Emoticon Lexicon, Dialectal Arabic Hashtag Lexicon shown Table 4.
table also shows number unigram, bigram, trigram entries lexicons.
111

fiMohammad, Salameh, & Kiritchenko

Dataset
Sentiment classes
Number instances
frequent class baseline
Human agreement benchmark
system, using non-lexicon features
a. Arabic Emoticon Lexicon features
b. Arabic Hashtag Lexicon features
c. Dialectal Arabic Hashtag Lexicon features
d. lexicon features a., b., c.

BBN posts
pos, neg, neu
1199
47.95
73.82

Syrian tweets
pos, neg, neu
2000
67.50
79.05

62.40
62.97
65.31
63.47

78.35
78.96
79.35
79.00

Table 5: Accuracy obtained using features different automatically generated Arabic
sentiment lexicons. highest scores shown bold.

Arabic Sentiment Labeled Dataset
sentiment classes
number instances
frequent class baseline
Human agreement benchmark
Mourad Darwish Arabic SA system
Arabic SA system

MD
pos, neg
1111
66.06
72.50
74.62

RR
pos,neg
2681
68.92
85.23

BBN posts
pos, neg, neu
1199
47.95
73.82
65.31

Syrian tweets
pos, neg, neu
2000
67.50
79.05
79.35

Table 6: Accuracy (in percentage) sentiment analysis (SA) systems various Arabic
social media datasets.

4.4.2 Evaluation
Table 5 shows ten-fold cross-validation accuracies obtained BBN Syria datasets
using various Arabic sentiment lexicons discussed above. Observe best results obtained using Dialectal Arabic Hashtag Lexicon. Both, Arabic
Hashtag Lexicon Arabic Emoticon Lexicon features, added Dialectal
Arabic Hashtag Lexicon features result improvement classification accuracy.
Henceforth paper, use Dialectal Arabic Hashtag Lexicon Arabic
sentiment lexicon.
Existing sentiment-labeled Arabic datasets include one described Mourad
Darwish (2013), refer MD, one described Refaee Rieser
(2014a), refer RR. tested Arabic sentiment system MD
RR, well two newly sentiment-annotated Arabic datasetsBBN posts
Syrian tweets. Table 6 shows results ten-fold cross-validation experiments
datasets. MD RR, presented results two-class problem (positive
vs. negative) allow comparison prior published results. BBN Syria
datasets, results shown case system identify one three
classes: positive, negative, neutral. Human agreement scores shown available.
Note accuracy system higher previously published results
MD dataset. previously published results RR dataset small
112

fiHow Translation Alters Sentiment

Dataset
Sentiment classes
Number instances
frequent class baseline
Human agreement benchmark
System
a. Features
b. - lexicon features
c. - ngram features
c1. - word ngram features
c2. - char. ngram features
d. - style features
e. - ngram features - style features
f. - lexicon features - style features

BBN posts
pos, neg, neu
1199
47.95
73.82

Syrian tweets
pos, neg, neu
2000
67.50
79.05

65.31
61.98
63.07
64.72
63.31
65.23
62.23
61.90

79.35
79.35
66.45
77.71
78.40
79.20
67.35
79.45

Table 7: Ablation experiments showing accuracy BBN Syria datasets.
larger drop performance removing feature set, useful
feature set classification.

subset (about 1000 instances) Refaee Rieser (2014a) obtained accuracy
87%. results Table 6 larger dataset directly comparable.
determine impact different feature sets performance conducting ablation
experiments, remove one set features time observe change
performance. larger drop accuracy, useful removed feature set.
Table 7 shows ablation results BBN Syria datasets.
Observe BBN dataset largest drop performance occurs
remove sentiment lexicon features. shows method producing sentiment lexicon effective generating useful wordsentiment association entries. Ngrams
helpful sentiment classification, especially Syria dataset. Removing
style features (features based hashtags, exclamations, etc.) result large
drop performance datasets, likely character ngram features
subsume much discerning power. Row e. shows performance use
sentiment lexicon features (no ngram features style features) row f. shows performance use ngram features (no lexicon features style features).
Observe performance using lexicon features alone rather competitive
BBN dataset, suggesting automatically generated sentiment lexicons able
capture termsentiment association similar extent supervised algorithm
learn ngram features training data. Syria dataset, ngrams alone produce
results reaching human agreement levels. believe may markedly
lower type token ratio (lexical diversity) Syrian tweets due skew
dataset towards negative class. (Table 15 Section 5.2 shows type token ratios
various datasets.)

113

fiMohammad, Salameh, & Kiritchenko

BBN dataset
a. Ar(Auto.Sent)
b. En(Manl.Trans., Auto.Sent)
c. En(Auto.Trans., Auto.Sent)
Syria dataset
d. Ar(Auto.Sent)
e. En(Auto.Trans., Auto.Sent)

pos

neg

neu

39.78
43.12
42.87

60.05
55.63
56.05

0.17
1.25
1.08

20.60
24.75

75.30
69.75

4.10
5.50

Table 8: Class distribution (in percentage) resulting automatic sentiment analysis.
Data Pair
a. Ar(Manl.Sent) - Ar(Auto.Sent)
b. Ar(Manl.Sent) - En(Manl.Trans., Manl.Sent)
c. Ar(Manl.Sent) - En(Manl.Trans., Auto.Sent)
d. Ar(Manl.Sent) - En(Auto.Trans., Manl.Sent)
e. Ar(Manl.Sent) - En(Auto.Trans., Auto.Sent)
f. En(Manl.Trans., Manl.Sent) - En(Auto.Trans., Manl.Sent)
g. En(Manl.Trans., Manl.Sent) - En(Manl.Trans., Auto.Sent)
h. En(Auto.Trans., Manl.Sent) - En(Auto.Trans., Auto.Sent)

Match %
65.31
71.31
67.73
57.21
62.08
60.08
63.11
69.58

Table 9: Setup A: Match percentage pairs sentiment labeled BBN datasets.

5. Experiments Sentiment Translation - Setup A: Translating
Focus Language Text English
Setup (as described Section 3.1) analyze performance English sentiment analysis system, using English resources, automatic translations Arabic social
media texts. Using methods systems described Sections 4.1, 4.2, 4.3, 4.4,
generated translations manually automatically sentiment labeled
datasets mentioned Section 3.1s Experimental Setup (also shown Figure 1). Table 8
shows distribution positive, negative, neutral classes various datasets
automatically labeled sentiment. percentages compared
Table 3 (rows a. d.) show true sentiment distribution BBN
Syria datasets. Observe automatic system difficulty assigning neutral class
posts. probably small percentage (about 10%) neutral tweets
training data. Also notice system predominantly guesses negative,
also reflection distribution training data. strong bias negatives
lessened English translations.
Main Result: Tables 9 10 show similar sentiment labels across various
pairs datasets BBN posts Syrian tweets, respectively. example, row a.
Table 9 shows comparison Arabic tweets manually annotated
sentiment automatically labeled sentiment Arabic sentiment
analysis system. Column 2 shows percentage instances sentiment labels
match across two datasets compared. row a. match percentage 65.31%
114

fiHow Translation Alters Sentiment

Data Pair
a. Ar(Manl.Sent) - Ar(Auto.Sent)
b. Ar(Manl.Sent) - En(Auto.Trans., Manl.Sent)
c. Ar(Manl.Sent) - En(Auto.Trans., Auto.Sent)
d. En(Auto.Trans, Manl.Sent) - En(Auto.Trans., Auto.Sent)

Match %
79.35
71.05
79.16
76.80

Table 10: Setup A: Match percentage pairs sentiment labeled Syria datasets.

represents accuracy automatic sentiment analysis system Arabic BBN
posts.
Row b. shows difference labels text manually translated Arabic
English, even though sentiment labeling Arabic English done manually.
Observe two labels match 71.31% time. However, agreement among
human sentiment annotators original Arabic texts 73.82%. So, English
translation affect sentiment, dramatically.
Row c. shows results manually translated text run English
sentiment analysis system labels compared Ar(Manl.Sent.) Observe
match pair 67.73%, much lower 71.31% obtained
manual sentiment labeling. shows English sentiment system performing
rather well. (One would expect get match greater 71.31%.) importantly, English sentiment system shows competitive result 62.08% run
automatically translated text (row e.), makes choice viable option sentiment
analysis non-English texts. result inline previous findings cross-lingual
information retrieval (Nie, Simard, Isabelle, & Durand, 1999) text classification (Amini
& Goutte, 2010).
Rows d. e. compare Ar(Manl.Sent.) manual automatic sentiment labeling
automatic translations. Since automatic translation Arabic English fairly
difficult, expect match percentages lower rows b. c.,
exactly observe. However, unexpected find number row e.
higher row d. find pattern corresponding data pairs
Syrian tweets well (rows b. c. Table 10). suggests certain attributes
automatically translated text mislead humans regards true sentiment
source text. However, attributes seem affect automatic sentiment
analysis system much. conduct experiments explore reasons behind
Section 5.1.
Row f. shows manual automatic translation lead 60% match
manually annotated sentiment labels other. Row g. shows accuracy
English automatic sentiment analysis system manually translated text (assuming
English sentiment labels gold). Row h. shows accuracy English automatic sentiment analysis system automatically translated text (assuming English sentiment
labels gold). case, systems accuracy 69.58% higher human
agreement automatically translated text (65.7%), shows automatic
translation greatly impacts sentiment perceived humans.

115

fiMohammad, Salameh, & Kiritchenko

5.1 Qualitative Analysis Translations Differ Sentiment
Source Text
seen results experiments previous section, translations
text often preserve original sentiment. Further, exist number instances
manual sentiment annotations automatic translations differ sentiments
original Arabic text, automatic English sentiment analysis system correctly
predicts sentiment original Arabic text. describe study conducted
determine main reasons are, frequently
reasons come play.
started creating dataset manual annotations Arabic texts disagreed
manual annotations translations. Specifically, BBN dataset, created
instances composed of:
a. original Arabic tweet,
b. manually determined sentiment Arabic tweet (positive, negative,
neutral),
c. manual English translation Arabic tweet,
d. manually determined sentiment translation (positive, negative,
neutral).
kept instances b. differed d. filtered set keeping
instances automatic English sentiment analysis system correctly predicted b. instances arranged decreasing order inter-annotator agreement
sentiment annotation Arabic texts. Since annotation task time intensive,
wanted annotate instances high confidence sentiments
original Arabic texts. top 100 instances presented judge spoke
English Arabic fluently. refer dataset BBN Manl.Trans.
Disagreement Pairs. 100 instances, judge asked
opinion b. d. differ. precise directions shown below:
Annotation Guidelines
instance (row), tell us think manually annotated sentiment
English translation differs original sentiment Arabic post.
Codes:
1. Bad translation
a.
b.
c.
d.

sentiment words disappear
sentiment words added
sentiment words replaced opposite sentiment words
something sentiment words (also) caused disagreement
(may ill-formed text, may grammar, may position
negators like never, tense, auxiliaries, etc.)

2. Translation reasonable (sentiment-wise), sentence
viewed one sentiment Arabic speaking population
different sentiment English-speaking population due cultural
life-style differences.
116

fiHow Translation Alters Sentiment

3. know.
Note:
codes sub-categories. enter 1b, 1c, etc.
even enter 1 none sub-categories apply.
annotate, discover new categories, add
list codes, use new codes well.
one code applies instance, separate comma.
example, say 1a, 1d 1b, 1c, 2.

repeated annotation procedure, instances involving automatically translated texts:
a. original Arabic tweet,
b. manually determined sentiment Arabic tweet (positive, negative,
neutral),
c. automatic English translation Arabic tweet,
d. manually determined sentiment translation (positive, negative,
neutral).
before, kept instances b. differed d., instances automatic English sentiment analysis system correctly predicted b.
100 instances highest inter-annotator agreement Arabic sentiment annotation
presented judge. refer dataset BBN Auto.Trans. Disagreement Pairs. judge asked opinion b. d. differ. Since automatic
translations exist BBN Syria datasets, annotation done
100 instances Syria dataset well. refer dataset Syria
Auto.Trans. Disagreement Pairs.
5.1.1 Annotation Results
took human judge 12 hours annotate three sets (100 instances each) described
above. distribution reasons disagreement sentiment
original text sentiment translation 100 instances dataset
shown Table 11. Since total number instances study 100, number
reason (code) also corresponding percentage. Note since instance
annotated belong one reason, percentages sum 100%. Also,
since annotator could choose broad reason category code (for example, 1.), none
sub-categories apply (for example, 1a. 1b.), sum entries subcategories
need equal number entries subsuming reason category.
judge marked handful instances know category
add new reason categories. shows judge largely able determine
reason disagreement two manual sentiment annotations involved (one
original Arabic post one English translation), terms reasons
pre-specified.
117

fiMohammad, Salameh, & Kiritchenko

Reason Disagreement
1. Bad Translations
1a. sentiment words disappear
1b. sentiment words added
1c. sentiment words replaced
words opposite sentiment
1d. sentiment changed due
ill-formed text, grammar, etc
2. Cultural differences
3. know

Percentage Disagreement Pairs
BBN dataset
Syria dataset
Manl.Trans. Auto.Trans.
Auto.Trans.
41
95
91
11
58
80
0
1
1
9

37

8

26
73
2

13
10
3

12
26
4

Table 11: Class distribution reasons disagreement sentiment
original text sentiment translation 100 instances three
datasets. Note entries represent number percentage
instances, since subset 100 instances all.

large percentage instances manually translated disagreement sets affected judge thought cultural differences. However, bad translation
still significant cause disagreement. cursory examination BBN dataset
also gave us impression manual translations could better.
Nonetheless, contrast manually translated disagreement sets, automatically
translated disagreement sets markedly high proportion instances (more 90%)
bad translation led directly disagreement sentiment. specifically,
automatic translations seem often mistranslate sentiment expressions either
appear translation appear neutral terms translation
(58% instances BBN Auto.Trans. 80% Syria Auto.Trans.).
instances pertaining 1b. found data. surprising since
one would expect translator add sentiment none.
5.1.2 Discussion
Table 12 shows examples disagreement categories resulting manual translation BBN subset. (We show examples 1b. lack data
sub-category.) sub-categories present, table shows original Arabic post, BBN-provided manual translation, comments judge. Table
13 shows examples disagreement categories resulting automatic
translation BBN subset.
Discussions judge revealed following phenomenon commonly led
1a., 1c., 1d., 2.:
Ambiguous words: Often word many meanings, one sense certain
sentiment (positive, negative, neutral) another sense different sentiment,
mistranslated wrong sense leading sentiment disagreement.
common automatically translated text, occurs sometimes even manually
118

fiHow Translation Alters Sentiment

1a. Sentiment words disappear

@ ,,, QK.
Post
? K

D.k

negative

what,
bolded text Arabic expression literally means let
loved Gecko. expresses disgust anger. part left
Comments
untranslated human translator.
1c. Sentiment words replaced words opposite sentiment
. GA K
KBAK
Post


neutral

Manl.Trans.

Manl.Trans.
Comments







secondly, still refreshment room?
ambiguous Arabic word mistranslated refreshment instead
recovery room. surprising human translator made mistake.

negative
neutral

1d. Sentiment changed due ill-formed text, grammar, tense, etc
Post
Manl.Trans.


Q@ PAm .' @ AK
@
Xk.
@ H. Q @Y Q
'@

negative

good still coming, water leak day project
post supposed shows sarcasm saying expect good come,
meaning worse yet come. expression widely used
Comments
Arabic conversations mean something negative. also example 2.
2. Cultural Differences
Q
Post
.. J
K B .. k@
.

neutral

Manl.Trans.

honestly... comment...
Although post seem literally negative, many Arabic
conversations used express negative opinionsimilar speechless.

neutral

although didnt see crescent home
post associate negative sentiment observing crescent moon
Islam associated beginning month holiday

neutral

Comments
Post
Manl.Trans.
Comments





G@
C@ IJ
? AJ J
K.



negative

negative

Table 12: Examples different reasons disagreement sentiment original text sentiment manual translation.

translated textespecially translation done crowdsourcing, quality
control checks stringent. Even human translators occasion
tempted use Google Translate.
Sarcasm: Sarcasm sometimes hard detect, even humans, even
detected, upon translation, differences cultural language norms mean
translation longer appears sarcastic. See example 1d. Table 12.
Metaphors: Metaphors, example 1c. Table 13, also hard
translateagain automatic system, extent even humans. often translated neutral opposite sentiment expressions,
contributing 1a. 1c.
Word-reordering: Automatic translations often lead poor word-reordering
target language, sentiment implications original post
negation terms. Missing misplaced negation term lead different sentiment.
Sarcasm also greatly affected word-reordering. See example 1d. Table 13.
current statistical machine translation systems evaluated using ngram-based
evaluation metric (BLEU). However, metric often misses (or penalize enough)
119

fiMohammad, Salameh, & Kiritchenko

1a. Sentiment words disappear
Q
@@AK PAJ
Post
k@
.
Auto.Trans. match UNK frankly .
bolded word dialectal Arabic typo translated system.
Comments
meant sleeeeeping i.e., match boring
1c. Sentiment words replaced words opposite sentiment
H PAB@
Q @ @ J AJKY@
Post
H. PA


.




Auto.Trans.

negative
neutral

negative

minimum taught relatives clock
two meanings: scorpions clock arms. Also AJKY@
H. PA

means either
word lower. post supposed metaphorically state world
Comments
taught relatives hurt like scorpion bites. post mistranslated,
leading neutral (instead negative) sentiment.
1d. Sentiment changed due ill-formed text, grammar, tense, etc
JK

Q
Post
K
.

neutral

Auto.Trans.

positive




know , mean , cleanliness
correct translation know cleanliness means.
Word reordering missing negation led text seemingly positive
Comments
sentiment.
2. Cultural Differences

AJK@
Post
.. AD

Ag ADJ
J
C@ PA
Auto.Trans. dont know putting place .
post perceived Arab annotators said conversation
Comments
express negative attitude toward object
@ @ AKA K kP@ @
Post
AKA Q
Auto.Trans. God mercy dead God cure patients
Supplications Arabic annotated positive, although
contains lots negative phrases. tweet annotated
Comments
positive confidence 1.0, automatic translation negative
confidence 0.7, thus showing cultural differences perceiving tweet

negative

negative
neutral

positive
negative

Table 13: Examples different reasons disagreement sentiment original text sentiment automatic translation.

mistranslations caused many phenomenon listed above. Thus, evident here,
means automatically generated translations often carry misleading sentiment.
5.2 Quantitative Analysis Features Translations Impact Sentiment
previous section, qualitatively analyzed human annotation sentiment
translations difficult. section quantitatively explore:
causes automatic translations inferior manual translations terms
preserving sentiment. (Recall Table 9 b. c. higher scores d.
e.)
II automatic translation, error prone may be, offers advantages
automatic sentiment analysis system compared human annotations. (Recall
Table 9 row d. lower score row e.)
Although hard prove causation, hope experiments shed light
features translated texts impact sentiment.
120

fiHow Translation Alters Sentiment

Dataset
System
a. features
b. - lexicon features

BBN Manl.Trans.

BBN Auto.Trans.

67.73
63.73

62.08
60.74

Table 14: Accuracy sentiment analysis system manual automatic translations, without sentiment lexicons.
Dataset
BBN posts (Arabic)
BBN posts (Manl.Trans.)
BBN posts (Auto.Trans.)
Syrian tweets (Arabic)
Syrian tweets (Auto.Trans.)

#types
6,054
3,592
3,108

#tokens
11,928
16,609
16,660

#types/#tokens
0.5075
0.2163
0.1866

11,667
6,731

35,983
57,153

0.3242
0.1178

Table 15: Lexical diversity datasets.
qualitative analysis previous section suggests one main reasons
may fact sentiment words original text translated
neutral terms. test quantitatively ablation experiments English
translations (manual automatic), observing effect removing sentiment lexicon
features. Table 14 shows results. Observe sentiment lexicon features
helpful manual translations (improve results 4 percentage points)
automatic translations (improves results 1.34 points).
hypothesis automatic sentiment analysis system correctly annotates several automatically translated instances manual annotations translation may
fail (II), sentiment system learn appropriate model even mistranslated text especially automatic translation makes consistent errors. example,
Q@ @ (Oh God grant victory to) consistently translated God forsake.
tweets phrase correctly annotated positive system, marked
negative human annotators.
true, surmise automatic translations lower lexical
diversity manual translations. is, automatic translations lower word type
(unique term) token ratio manual translations. Table 15 shows number types
tokens original Arabic BBN Syria datasets also translations.
automatic translations removed UNK tokens determining counts.
First, note even human translations lower type token ratio
original source text. Additionally, observe hypothesized, type token
ratio markedly higher manual translations compared automatic translations
text. supports hypothesis SMT system translates source
tokens consistently. Since automatic sentiment analysis system trained
consistently translated text original sentiment labels source text, still
able determine true sentiment. However, since human sentiment annotators see many
instances sentiment terms mistranslated neutral terms, unable
determine true sentiment.
121

fiMohammad, Salameh, & Kiritchenko

System

Accuracy
(in percentage)
a. Baseline (uses word ngram style features training fold)
61.98
b. Baseline + Arabic translation English corpus
English corpus: SemEval task 2 training corpus
42.78
c. Baseline + Arabic translation English lexicon
i. English lexicon: AFINN
63.41
ii. English lexicon: Bing Liu Lexicon
62.99
iii. English lexicon: MPQA
61.91
iv. English lexicon: NRC Emotion Lexicon
63.48
v. English lexicon: NRC Emoticon Lexicon
62.40
vi. English lexicon: NRC Hashtag Lexicon
61.73
d. Baseline + Arabic lexicon
i. Arabic lexicon: Arabic Emoticon Lexicon
62.40
ii. Arabic lexicon: Arabic Hashtag Lexicon
62.97
iii. Arabic lexicon: Arabic Hashtag Lexicon (dialectal)
65.31
e. Baseline + Arabic lexicon + Arabic translation English lexicon
Arabic lexicon: Arabic Hashtag Lexicon (dialectal)
i. English lexicon: AFINN
65.73
ii. English lexicon: Bing Liu Lexicon
66.15
iii. English lexicon: MPQA
65.15
iv. English lexicon: NRC Emotion Lexicon
66.23
v. English lexicon: NRC Emoticon Lexicon
66.15
vi. English lexicon: NRC Hashtag Lexicon
64.22
f. Baseline + Arabic Hashtag Lexicon (dialectal)
+ Arabic translation NRC Emotion Lexicon
66.57
Table 16: Setup B: Cross-validation experiments BBN dataset. highest score
overall highest scores c., d., e. shown bold.

6. Experiments Sentiment Translation - Setup B: Translating
Sentiment Resources English Focus Language
describe sentiment analysis experiments automatically translate resources
English focus language (Arabic) improve accuracy sentiment analysis
system operating texts focus language (Setup B). implemented Setup B
described Section 3.2 Figure 2, using capabilities described Section 4.
translations obtained using Google Translate. Arabic portion BBN dataset
used primary focus language text. baseline system performs ten-fold crossvalidation dataset using word ngrams style features described Section 4.4.
systems use additional resourcessome originally created Arabic sources
translations English resources.
Table 16 shows accuracy automatic sentiment analysis systems. (Generated
sentiment labels compared manual annotation Arabic speakers.) Comparing rows
a. b. infer simply translating sentiment-labeled tweets English
Arabic using additional training data Arabic sentiment analysis leads
122

fiHow Translation Alters Sentiment

poor results. saw Section 5, language produced machine translation
system differs substantially corresponding natural language. Therefore, sentiment analysis system cannot fully benefit additional labeled machine-translated
corpus asked annotate natural language text test time. Similar observations
reported work Balahur Turchi (2014). possible better results
may obtained using one many domain-adaptation techniques proposed
literature, however, leave future work.12
also conducted experiments adding Arabic translations English lexicons
baseline system row a.see results rows c.i. c.vi. table. best
results obtained using Arabic translations NRC Emotion Lexicon (row c.iv.).
Row d. shows results obtained using baseline system additional features
Arabic sentiment lexicon. shown Section 4.4, Dialectal Arabic Hashtag
Lexicon outperformed Arabic lexiconsthose results shown completeness convenience. compare accuracies obtained rows e. f. d.
determine using additional features Arabic translations English sentiment lexicons beneficial. Observe translated English lexicons help obtain higher
accuracies row d.iii. (65.31%). best results obtained translations
English sentiment lexicon using NRC Emotion Lexicon. Using NRC Emotion Lexicon along Dialectal Arabic Hashtag Lexicon addition baseline
system (row f.) gives slight improvement (66.57%).
Thus sentiment analysis Arabic social media posts, difficult extract
benefit automatic translations English sentiment labeled sentences. However, improvements obtained using automatic translations English sentiment lexicons.
6.1 Manual Examination Automatically Translated Entries
Sentiment Lexicon
shown above, lexicons created translating existing ones languages beneficial automatic sentiment analysis, even one good lexicons focus language
(such Dialectal Arabic Hashtag Lexicon Arabic). However, experiments
explicitly quantify extent translated entries appropriate,
translation alters sentiment source word. conducted manual annotation
study 300 entries NRC Emotion Lexicon determine percentage entries
appropriate even automatic translation focus language (Arabic).
appropriate entry Arabic translation sentiment association
English source word. Additionally, translated entries deemed incorrect
focus language classified coarse error categories. list pre-decided error
categories presented annotator, annotator also encouraged create
new error categories, required. error categories shown below:
1. word completely mistranslated.
2. translation perfect, English word translated word related
correct translation. Arabic word provided different sentiment
English source word.
12. Sampling English corpus obtain similar class distribution Arabic dataset led
small improvements.

123

fiMohammad, Salameh, & Kiritchenko

positive
negative
neutral


Translation
# English Entries
100
100
100
300

# positive
85
4
5
94

Translation
# negative # neutral
9
6
92
4
7
88
108
98

# changed
15 (15.0%)
8 (08.0%)
12 (12.0%)
35 (11.7%)

Table 17: Annotations NRC Emotion Lexicons sentiment association entries automatic translation Arabic.

Error categories
1. Mistranslated
2. Translated related word
3. Translation correct, 3a., 3b., 3c.
3a. Different dominant sense
3b. Cultural differences
3c. reasons

Percentage
total errors
9.7
38.7
51.6
29.0
22.6
0.0

Table 18: Percentage erroneous entries translated NRC Emotion Lexicon
assigned error category.

3. translation correct, Arabic word different sentiment
English source word.
(a) dominant sense Arabic word different dominant sense
English source word, different sentiments.
(b) Cultural life style differences Arabic English speakers lead
different sentiment associations English word translation.
(c) reason (give reason can).
annotator native speaker Arabic, also fluent English.
chose NRC Emotion Lexicon study manually created
led best results experiments (Table 16). Since manual annotation
tedious, study, randomly selected 100 positive words, 100 negative words,
100 neutral words lexicon.
Table 17 shows results human annotation study. 100 positive entries
examined, 85 marked appropriate Arabic well. Nine translations
marked negative Arabic, six marked neutral. Similarly, 92%
translated negative entries 88% translated neutral entries marked appropriate Arabic. Overall, 11.67% translated entries deemed incorrect
Arabic.
Table 18 gives percentage erroneous entries assigned error category. Observe close 10% errors caused gross mistranslations, close 40%
errors caused translations related word, 50% errors
caused, bad translation, differences word used Arabiceither
different sense distributions (29%) cultural differences (22.6%).
124

fiHow Translation Alters Sentiment

7. Conclusions
Much work sentiment analysis focused English texts. Thus,
languages limited sentiment resources. paper conducted several experiments
exploring two broad approaches improving sentiment analysis Arabic social media text
help English resources state-of-the-art translation systems: (a) translate
focus language text resource-rich language English, apply powerful
English sentiment analysis system translation, (b) translate resources
sentiment labeled corpora sentiment lexicons English focus language,
use additional resources focus-language sentiment analysis system. goal
systematically study impact translation (manual automatic) sentiment.
experiments show automatic sentiment analysis English translations (even
automatic translations) Arabic texts lead competitive resultsresults
similar obtained current state-of-the-art Arabic sentiment analysis systems.
Similar findings reported tasks, Information Retrieval (Nie
et al., 1999) Text Classification (Amini & Goutte, 2010). Surprisingly, results also
show automatic sentiment analysis automatic translations outperforms manual
sentiment annotations automatically translated text. suggests SMT errors
impact human perception sentiment markedly automatic sentiment systems.
Furthermore, conduct qualitative quantitative studies investigate observe results. find sentiment expressions often mistranslated neutral
expressions translated. Additionally, automatic translation often makes consistent
errors translating terms, since automatic system learns termsentiment associations training data, learn mistranslated word cue true
sentiment, thus recovering error. Sarcasm, metaphoric expressions, incorrect
word-reordering common reasons translations fail preserve sentiment.
Finally, observe even correctly translated texts sometimes marked
different sentiment speakers source language believe be. Thus,
sentiment, least extent dependent cultural context annotator.
also conducted experiments translating English resources Arabic help
improve Arabic sentiment analysis systems. Specifically, found using automatic
Arabic translations many freely available English sentiment lexicons improved accuracy.
However, experiments simply added translated, sentiment-labeled, English tweets data
existing Arabic training data resulted drop accuracy. manual examination
subset automatic translations English lexicon entries, native speaker
Arabic marked close 90% appropriate (that is, Arabic word
sentiment association English source word). annotator, fluent English
well, categorized remaining 10% entries different error classesreasons
entries valid Arabic. Mistranslation, cultural differences,
different sense distributions Arabic English, primary reasons errors
automatic translation entries sentiment lexicon.
Caveats: automatic systems employed experiments, i.e., Arabic sentiment
analysis, English sentiment analysis, machine translation, exhibit state-of-the-art per125

fiMohammad, Salameh, & Kiritchenko

formance; nevertheless, improvements possible. Arabic sentiment analysis
system possibly benefit features derived specifically Arabic language.
English sentiment analysis system adapted peculiarities machinetranslated texts, notably different regular English. current machine
translation system trained non-tweet data results high percentage
out-of-vocabulary words datasets. Tweets mixture dialects even
mixture languages (e.g., Arabic English). Addressing factors future work
give even insight sentiment altered translation, specific contexts.
Data: resources created part project (Arabic sentiment lexicons,
Arabic sentiment annotations social media posts, English sentiment annotations
translations) made freely available.13

Acknowledgments
Thanks Kareem Darwish Eshrag Refaee sharing data. Thanks Norah
Alkharashi annotating translations reasons sentiment preserved.
thank Colin Cherry, Samuel Larkin, Marine Carpuat helpful discussions.

References
Abbasi, A., Chen, H., & Salem, A. (2008). Sentiment analysis multiple languages: Feature
selection opinion classification web forums. ACM Transactions Information
Systems, 26 (3), 12:112:34.
Abdul-Mageed, M., & Diab, M. (2014). SANA: large scale multi-genre, multi-dialect
lexicon Arabic subjectivity sentiment analysis. Proceedings 9th International Conference Language Resources Evaluation, LREC 14. European
Language Resources Association.
Abdul-Mageed, M., Diab, M., & Kubler, S. (2014). SAMAR: Subjectivity sentiment
analysis Arabic social media. Computer Speech & Language, 28 (1), 20 37.
Abdul-Mageed, M., Diab, M. T., & Korayem, M. (2011). Subjectivity sentiment analysis modern standard Arabic. Proceedings 49th Annual Meeting
Association Computational Linguistics: Human Language Technologies, pp. 587
591.
Agarwal, A., Xie, B., Vovsha, I., Rambow, O., & Passonneau, R. (2011). Sentiment analysis
Twitter data. Proceedings Workshop Languages Social Media, LSM
11, pp. 3038, Portland, Oregon.
Ahmad, K., Cheng, D., & Almas, Y. (2006). Multi-lingual sentiment analysis financial
news streams. Proceedings 1st International Conference Grid Finance.
Ahmed, S., Pasquier, M., & Qadah, G. (2013). Key issues conducting sentiment analysis
Arabic social media text. Proceedings 9th International Conference
Innovations Information Technology, pp. 7277. IEEE.
13. http://www.purl.org/net/ArabicSA

126

fiHow Translation Alters Sentiment

Aisopos, F., Papadakis, G., Tserpes, K., & Varvarigou, T. (2012). Textual contextual
patterns sentiment analysis microblogs. Proceedings 21st International Conference World Wide Web Companion, WWW 12 Companion, pp.
453454, New York, NY, USA.
Al-Kabi, M., Gigieh, A., Alsmadi, I., Wahsheh, H., & Haidar, M. (2013). opinion analysis tool colloquial standard Arabic. Proceedings 4th International
Conference Information Communication Systems, ICICS 13.
Amini, M.-R., & Goutte, C. (2010). co-classification approach learning multilingual corpora. Machine learning, 79 (1-2), 105121.
Badaro, G., Baly, R., Hajj, H., Habash, N., & El-Hajj, W. (2014). large scale Arabic
sentiment lexicon Arabic opinion mining. Proceedings EMNLP Workshop
Arabic Natural Language Processing (ANLP), pp. 165173, Doha, Qatar.
Bakliwal, A., Arora, P., Madhappan, S., Kapre, N., Singh, M., & Varma, V. (2012). Mining sentiments tweets. Proceedings 3rd Workshop Computational
Approaches Subjectivity Sentiment Analysis, WASSA 12, pp. 1118, Jeju, Republic Korea.
Balahur, A., & Turchi, M. (2014). Comparative experiments using supervised learning
machine translation multilingual sentiment analysis. Computer Speech &
Language, 28 (1), 5675.
Bellegarda, J. (2010). Emotion analysis using latent affective folding embedding.
Proceedings NAACL-HLT Workshop Computational Approaches Analysis
Generation Emotion Text, pp. 19, Los Angeles, California.
Boucouvalas, A. C. (2002). Real time text-to-emotion engine expressive Internet communication. Emerging Communication: Studies New Technologies Practices
Communication, 5, 305318.
Brody, S., & Diakopoulos, N. (2011). Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using word
lengthening detect sentiment microblogs. Proceedings Conference
Empirical Methods Natural Language Processing, EMNLP 11, pp. 562570,
Stroudsburg, PA, USA.
Chang, C.-C., & Lin, C.-J. (2011). LIBSVM: library support vector machines. ACM
Transactions Intelligent Systems Technology, 2 (3), 27:127:27.
Chen, B., & Zhu, X. (2014). Bilingual sentiment consistency statistical machine translation. Proceedings 14th Conference European Chapter Association Computational Linguistics, pp. 607615, Gothenburg, Sweden. Association
Computational Linguistics.
Cherry, C., & Foster, G. (2012). Batch tuning strategies statistical machine translation.
Proceedings Conference North American Chapter Association
Computational Linguistics: Human Language Technologies, pp. 427436.
Chiang, D., Marton, Y., & Resnik, P. (2008). Online large-margin training syntactic
structural translation features. Proceedings Conference Empirical
Methods Natural Language Processing, EMNLP 08, pp. 224233.
127

fiMohammad, Salameh, & Kiritchenko

El-Beltagy, S. R., & Ali, A. (2013). Open issues sentiment analysis Arabic social
media: case study. Proceedings 9th International Conference Innovations Information Technology, pp. 215220. IEEE.
El Kholy, A., & Habash, N. (2012). Orthographic morphological processing
EnglishArabic statistical machine translation. Machine Translation, 26 (1-2), 2545.
Farra, N., Challita, E., Assi, R. A., & Hajj, H. (2010). Sentence-level documentlevel sentiment mining Arabic texts. Proceedings IEEE International
Conference Data Mining Workshops, pp. 11141119. IEEE.
Genereux, M., & Evans, R. P. (2006). Distinguishing affective states weblogs. Proceedings AAAI Spring Symposium Computational Approaches Analysing
Weblogs, pp. 2729, Stanford, California.
Gimpel, K., Schneider, N., OConnor, B., Das, D., Mills, D., Eisenstein, J., Heilman, M.,
Yogatama, D., Flanigan, J., & Smith, N. A. (2011). Part-of-speech tagging Twitter:
Annotation, features, experiments. Proceedings Annual Meeting
Association Computational Linguistics, ACL 11, pp. 4247.
Go, A., Bhayani, R., & Huang, L. (2009). Twitter sentiment classification using distant
supervision. Tech. rep., Stanford University.
Habash, N., Rambow, O., & Roth, R. (2009). MADA+TOKAN: toolkit Arabic
tokenization, diacritization, morphological disambiguation, POS tagging, stemming
lemmatization. Proceedings 2nd International Conference Arabic
Language Resources Tools, pp. 102109, Cairo, Egypt. MEDAR Consortium.
Hu, M., & Liu, B. (2004). Mining summarizing customer reviews. Proceedings
10th ACM SIGKDD International Conference Knowledge Discovery Data
Mining, KDD 04, pp. 168177, New York, NY, USA. ACM.
John, D., Boucouvalas, A. C., & Xu, Z. (2006). Representing emotional momentum within
expressive Internet communication. Proceedings 24th International Conference Internet Multimedia Systems Applications, pp. 183188, Anaheim,
CA. ACTA Press.
Kiritchenko, S., Mohammad, S., & Salameh, M. (2016). SemEval-2016 Task 7: Determining
sentiment intensity english arabic phrases. Proceedings International
Workshop Semantic Evaluation, SemEval 16.
Kiritchenko, S., Zhu, X., Cherry, C., & Mohammad, S. (2014a). NRC-Canada-2014: Detecting aspects sentiment customer reviews. Proceedings 8th International
Workshop Semantic Evaluation (SemEval 2014), pp. 437442, Dublin, Ireland.
Kiritchenko, S., Zhu, X., & Mohammad, S. M. (2014b). Sentiment analysis short informal
texts. Journal Artificial Intelligence Research, 50, 723762.
Liu, B., & Zhang, L. (2012). survey opinion mining sentiment analysis.
Aggarwal, C. C., & Zhai, C. (Eds.), Mining Text Data, pp. 415463. Springer US.
Liu, H., Lieberman, H., & Selker, T. (2003). model textual affect sensing using realworld knowledge. Proceedings 8th International Conference Intelligent
User Interfaces, IUI 03, pp. 125132, New York, NY. ACM.
128

fiHow Translation Alters Sentiment

Martnez-Camara, E., Martn-Valdivia, M. T., Urenalopez, L. A., & Montejoraez, A. R.
(2012). Sentiment analysis Twitter. Natural Language Engineering, 128.
Mihalcea, R., Banea, C., & Wiebe, J. (2007). Learning multilingual subjective language via
cross-lingual projections. Proceedings 45th Annual Meeting Association
Computational Linguistics, p. 976.
Mihalcea, R., & Liu, H. (2006). corpus-based approach finding happiness. Proceedings AAAI Spring Symposium Computational Approaches Analysing
Weblogs, pp. 139144. AAAI Press.
Mohammad, S. M. (2012). #Emotional tweets. Proceedings First Joint Conference
Lexical Computational Semantics, *SEM 12, pp. 246255, Montreal, Canada.
Mohammad, S. M. (2016). Sentiment analysis: Detecting valence, emotions,
affectual states text. Emotion Measurement.
Mohammad, S. M., Kiritchenko, S., & Zhu, X. (2013). NRC-Canada: Building stateof-the-art sentiment analysis tweets. Proceedings 7th International
Workshop Semantic Evaluation Exercises, SemEval 13, Atlanta, Georgia, USA.
Mohammad, S. M., & Turney, P. D. (2010). Emotions evoked common words
phrases: Using Mechanical Turk create emotion lexicon. Proceedings
NAACL-HLT Workshop Computational Approaches Analysis Generation
Emotion Text, pp. 2634, LA, California.
Mohammad, S. M., & Turney, P. D. (2013). Crowdsourcing word-emotion association
lexicon. Computational Intelligence, 29 (3), 436465.
Mohammad, S. M., & Yang, T. W. (2011). Tracking sentiment mail: genders differ
emotional axes. Proceedings ACL Workshop Computational Approaches
Subjectivity Sentiment Analysis, WASSA 11, pp. 7079, Portland, OR, USA.
Mourad, A., & Darwish, K. (2013). Subjectivity sentiment analysis modern standard
Arabic Arabic microblogs. Proceedings 4th Workshop Computational
Approaches Subjectivity, Sentiment Social Media Analysis, WASSA 13, pp.
5564.
Neviarouskaya, A., Prendinger, H., & Ishizuka, M. (2011). Affect analysis model: novel
rule-based approach affect sensing text. Natural Language Engineering, 17,
95135.
Nie, J.-Y., Simard, M., Isabelle, P., & Durand, R. (1999). Cross-language information
retrieval based parallel texts automatic mining parallel texts Web.
Proceedings 22nd Annual International ACM SIGIR Conference Research
Development Information Retrieval, pp. 7481. ACM.
Nielsen, F. A. (2011). new ANEW: Evaluation word list sentiment analysis
microblogs. Proceedings ESWC2011 Workshop Making Sense Microposts: Big things come small packages, pp. 9398.
Pak, A., & Paroubek, P. (2010). Twitter corpus sentiment analysis opinion
mining. Proceedings 7th Conference International Language Resources
Evaluation, LREC 10, pp. 13201326, Valletta, Malta.
129

fiMohammad, Salameh, & Kiritchenko

Pang, B., & Lee, L. (2008). Opinion mining sentiment analysis. Foundations Trends
Information Retrieval, 2 (12), 1135.
Pontiki, M., Galanis, D., Pavlopoulos, J., Papageorgiou, H., Androutsopoulos, I., & Manandhar, S. (2014). SemEval-2014 Task 4: Aspect based sentiment analysis. Proceedings International Workshop Semantic Evaluation, SemEval 14, pp. 2735,
Dublin, Ireland.
Refaee, E., & Rieser, V. (2014a). Arabic Twitter corpus subjectivity sentiment
analysis. Proceedings 9th International Conference Language Resources
Evaluation, LREC 14, Reykjavik, Iceland. European Language Resources Association.
Refaee, E., & Rieser, V. (2014b). Subjectivity sentiment analysis Arabic Twitter
feeds limited resources. Proceedings Workshop Free/Open-Source
Arabic Corpora Corpora Processing Tools, p. 16.
Rosenthal, S., Ritter, A., Nakov, P., & Stoyanov, V. (2014). SemEval-2014 Task 9: Sentiment
analysis Twitter. Proceedings 8th International Workshop Semantic
Evaluation, SemEval 14, pp. 7380, Dublin, Ireland.
Salameh, M., Mohammad, S., & Kiritchenko, S. (2015). Sentiment translation:
case-study arabic social media posts. Proceedings 2015 Conference
North American Chapter Association Computational Linguistics: Human
Language Technologies, pp. 767777, Denver, Colorado.
Thelwall, M., Buckley, K., & Paltoglou, G. (2011). Sentiment Twitter events. Journal
American Society Information Science Technology, 62 (2), 406418.
Wilson, T., Kozareva, Z., Nakov, P., Rosenthal, S., Stoyanov, V., & Ritter, A. (2013).
SemEval-2013 Task 2: Sentiment analysis Twitter. Proceedings International Workshop Semantic Evaluation, SemEval 13, Atlanta, Georgia, USA.
Wilson, T., Wiebe, J., & Hoffmann, P. (2005). Recognizing contextual polarity phraselevel sentiment analysis. Proceedings Conference Human Language Technology Empirical Methods Natural Language Processing, HLT 05, pp. 347354,
Stroudsburg, PA, USA.
Zbib, R., Malchiodi, E., Devlin, J., Stallard, D., Matsoukas, S., Schwartz, R., Makhoul, J.,
Zaidan, O. F., & Callison-Burch, C. (2012). Machine translation Arabic dialects.
Proceedings Conference North American Chapter Association
Computational Linguistics: Human Language Technologies, pp. 4959. Association
Computational Linguistics.
Zhu, X., Guo, H., Mohammad, S., & Kiritchenko, S. (2014a). empirical study
effect negation words sentiment. Proceedings Annual Meeting
Association Computational Linguistics, ACL, Vol. 14.
Zhu, X., Kiritchenko, S., & Mohammad, S. (2014b). NRC-Canada-2014: Recent improvements sentiment analysis tweets. Proceedings 8th International
Workshop Semantic Evaluation (SemEval 2014), pp. 443447, Dublin, Ireland.

130

fiJournal Artificial Intelligence Research 55 (2016) 17-61

Submitted 03/15; published 01/16

Integrating Rules Dictionaries
Shallow-Transfer Machine Translation
Phrase-Based Statistical Machine Translation
Vctor M. Sanchez-Cartagena

vmsanchez@dlsi.ua.es

Prompsit Language Engineering
Av. Universitat s/n. Edifici Quorum III
E-03202 Elx, Spain

Juan Antonio Perez-Ortiz
Felipe Sanchez-Martnez

japerez@dlsi.ua.es
fsanchez@dlsi.ua.es

Dep. de Llenguatges Sistemes Informatics
Universitat dAlacant
E-03071, Alacant, Spain

Abstract
describe hybridisation strategy whose objective integrate linguistic resources
shallow-transfer rule-based machine translation (RBMT) phrase-based statistical machine translation (PBSMT). basically consists enriching phrase table
PBSMT system bilingual phrase pairs matching transfer rules dictionary entries
shallow-transfer RBMT system. new strategy takes advantage linguistic resources used RBMT system segment source-language sentences
translated, overcomes limitations existing hybrid approaches treat
RBMT systems black box. Experimental results confirm approach delivers
translations higher quality existing ones, specially useful
parallel corpus available training SMT system small translating outof-domain texts well covered RBMT dictionaries. combination
approach recently proposed unsupervised shallow-transfer rule inference algorithm
results significantly greater translation quality baseline PBSMT;
case, hand-crafted resource used dictionaries commonly used RBMT.
Moreover, translation quality achieved hybrid system built automatically
inferred rules similar obtained built hand-crafted rules.

1. Introduction
Statistical machine translation (SMT) (Koehn, 2010) currently leading paradigm
machine translation (MT) research. SMT systems attractive may
built little human effort enough monolingual parallel corpora available.
However, parallel corpora always easy harvest, may even exist
(under-resourced) language pairs. contrary, rule-based machine translation
(RBMT) systems (Hutchins & Somers, 1992) may built without parallel corpus;
however, need explicit representation linguistic information, whose coding
human experts requires considerable amount time.
Even large parallel corpus available, SMT systems may still limitations result (i) data sparseness problem makes difficult collect enough
c
2016
AI Access Foundation. rights reserved.

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

phrase pairs covering inflected word forms highly inflected languages, (ii)
domain problem caused training parallel corpus belongs domain different
texts translated. One potential solution follow hybrid approach
(Thurmair, 2009) combine RBMT system SMT system order mitigate
limitations. approach follow paper, linguistic resources
shallow-transfer RBMT used enrich phrase table phrase-based SMT
(PBSMT) system.
Like transfer-based RBMT system, shallow-transfer RBMT systems carry
translation process three steps: analysis source-language (SL) sentence
produce SL intermediate representation (IR), transfer SL IR targetlanguage (TL) IR, generation final translation TL IR. Shallow-transfer
RBMT systems perform complete syntactic analysis input sentences
work simple IRs consisting sequence lexical forms. lexical form comprises
lemma, lexical category morphological inflection information word.
shallow-transfer RBMT, Apertium system (Forcada et al., 2011) used
experiments, analysis step, SL sentence split chunks. chunk
translated shallow-transfer rule translations concatenated order
build TL sentence. process similar process carried PBSMT
decoder, builds translation hypotheses segmenting SL sentence phrases
translating SL phrase according phrase table. systems work
flat sub-segments easy integrate chunks RBMT SMT phrase table
scored feature functions commonly used PBSMT. Moreover,
use RBMT dictionaries shallow-transfer rules allows PBSMT decoder
choose phrase pairs go beyond word-for-word translation words RBMT
dictionaries, well translating inflected word forms contain; thus alleviating
data sparseness problem. addition, data general-purpose RBMT system
help reduce bias SMT system towards domain training corpus.
Additionally, even rules RBMT system yet created,
automatically inferred small fragment training parallel corpus means
(unsupervised) rule inference approach proposed Sanchez-Cartagena, Perez-Ortiz,
Sanchez-Martnez (2015). better use therefore made training parallel corpus
RBMT dictionaries existing approaches (Schwenk, Abdul-Rauf, Barrault, &
Senellart, 2009) simply add dictionaries phrase table. combining rule
inference algorithm hybridisation approach, translation knowledge contained
parallel corpus generalised sequences words observed
corpus, share lexical category morphological inflection information words
observed.
enrichment PBSMT models RBMT linguistic data already explored
authors (see Section 2.1); however, approach presented paper first
one specifically designed use shallow-transfer RBMT takes advantage
way linguistic resources used RBMT system. best
knowledge, general approach Eisele et al. (2008), described Section 2.1,
hybrid approach literature applied shallow-transfer RBMT systems.
experimental results show hybrid approach outperforms strategy developed Eisele et al. (2008). Moreover, performance hybrid system built using
18

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

automatically inferred rules par hybrid system built hand-crafted
rules. also worth pointing system (Sanchez-Cartagena, Sanchez-Martnez, &
Perez-Ortiz, 2011b) built approach using hand-crafted rules Apertium
project (Forcada et al., 2011) one winners1 pairwise manual evaluation
WMT 2011 shared translation task (Callison-Burch, Koehn, Monz, & Zaidan, 2011)
SpanishEnglish language pair. hybridisation approach presented paper,
together aforementioned rule inference algorithm, contribute alleviating
data sparseness problem SMT systems highly inflected languages
involved reducing corpus size requirements regards building PBSMT systems.
remainder paper organised follows. Section 2 reviews related work
hybrid machine translation, including description limitations general hybridisation approach proposed Eisele et al. (2008). Section 3 describes hybridisation
strategy set different alternatives scoring phrase pairs generated
linguistic resources RBMT system. Two different sets experiments, integrate data Apertium RBMT platform (Forcada et al., 2011), described
order evaluate hybridisation strategy (Section 4) assess whether automatically inferred rules replace hand-crafted ones hybrid system (Section 5).
paper ends human evaluation error analysis (Section 6) concluding
remarks (Section 7).

2. Related Work
Hybrid approaches related presented paper split integrate RBMT elements SMT system (sections 2.1 2.2) integrate
SMT elements RBMT architecture (Section 2.3).2 Approaches first group
turn split two groups: use linguistic information existing
RBMT system (Section 2.1) use linguistic resources inferred parallel
corpus SMT models estimated (Section 2.2).
2.1 Integrating Hand-Crafted Linguistic Resources SMT
Bilingual dictionaries frequently reused resource RBMT;
added SMT systems since early days (Brown et al., 1993). One simplest
strategies, already put practice Apertium bilingual dictionaries (Tyers, 2009), consists adding dictionary entries directly training parallel
corpus. addition obvious increase lexical coverage, Schwenk et al. (2009) state
quality alignments obtained also improved words bilingual
dictionary appear sentences parallel corpus. However, guaranteed
that, following strategy, multi-word expressions bilingual dictionary ap1. system found statistically significantly better using sign test p 0.10.
2. meant strict classification: approaches listed section could
included groups. Moreover, approaches outputs different MT systems
combined without making modification inner workings systems involved,
system combination (Rosti, Matsoukas, & Schwartz, 2007) listed review because, unlike
approach, involve creation new MT architectures combine elements
SMT RBMT.

19

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

pear SL sentences translated may split smaller units
phrase-extraction algorithm. Dictionaries also added SMT systems
together rule-based enhancements, work Popovic Ney (2006),
propose combining dictionaries use hand-crafted rules order reorder
SL sentences match structure TL.
approaches take advantage full RBMT system. Eisele et al. (2008) present
strategy based augmentation phrase table include information provided
RBMT system. approach treats RBMT system black box, i.e.,
algorithm concerned inner workings RBMT system. sentences
translated hybrid system first translated RBMT system small
phrase table obtained resulting parallel corpus (from on, synthetic corpus).
new phrase table directly added original phrase table obtained
training parallel corpus. approach following limitations, overcome
hybrid approach described paper:
Deficient segment alignment. phrase pairs extracted synthetic
corpus usual procedure followed PBSMT (Koehn, 2010, 5.2.3), unaligned words included multiple phrase pairs, since evidence
correspondence language, phrase pairs made solely unaligned
words extracted. word alignments incorrect, phrase pairs
mutual translation may extracted correct phrase pairs present parallel
sentences may obtained.3 less reliable word alignments are,
severe problem becomes.
word alignment synthetic corpus obtained Eisele et al. (2008) may
unreliable owing vocabulary mismatch text translated
alignment models, inferred training corpus.4 limitation
becomes evident text translated share domain
training corpus, actually data RBMT system
useful.5
Relying word alignments reasonable strategy extracting phrase pairs
parallel corpus know built. However,
know RBMT system used generate TL side corpus,
3. Consider following segment EnglishSpanish parallel sentence: Barcelona City Council Ayuntamiento de Barcelona. word alignment segments link Barcelona
languages, incorrect phrase pairs Barcelona City Council Barcelona would extracted,
whereas correct phrase pair City Council Ayuntamiento would extracted.
4. Alignment models contain information words test corpus present
training corpus, words therefore aligned likely phrase pairs
mutual translation extracted them.
5. problem could alleviated building alignment models concatenation synthetic
corpus training corpus, incrementally training (Gao, Lewis, Quirk, & Hwang, 2011)
word alignment models. former would computationally expensive, since process would
carried time new text translated resulting hybrid system (e.g. building
word alignment models EnglishSpanish parallel corpus 600 000 sentences described
Section 4.1 took around 6 hours AMD Opteron 2 Ghz processor). latter likely cause
alignment errors infrequent words synthetic corpus found training corpus
involved.

20

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

precise phrase extraction mechanism takes advantage RBMT
system uses dictionaries shallow-transfer rules segment SL sentences
used.
Inadequate balance different types phrase pairs obtained.
probabilities derived Eisele et al. (2008) phrase pairs extracted
synthetic corpus added phrase table consistent
independently estimated two different corpora. one hand, SL
phrase translated way training corpus RBMT system,
probability corresponding phrase pair increased comparison
phrase pairs SL phrase translated different
way. hand, translations SL phrase differ
produced RBMT system, frequency training corpus taken
account scoring corresponding phrase pairs, noise may consequently
introduced case SL phrases low frequency training corpus.
instance, phrase pair extracted training corpus whose SL phrase appears
less reliable receive lower score phrase pair whose
SL phrase appears 10 000 times (see Section 3.2). overcome limitation
following sophisticated scoring scheme joins synthetic phrase pairs
phrase pairs obtained training corpus single list computing
phrase translation probabilities (see Section 3.2.3).
Another interesting approach Enache, Espana-Bonet, Ranta, Marquez
(2012), interlingua RBMT system developed limited domain patent
translation integrated PBSMT architecture generating synthetic phrase pairs
chunks extracted SL sentences parsed RBMT system.6
philosophy behind hybrid approach synthetic phrase pairs generated
chunks matched shallow-transfer rules. However, significant differences exist
method used score phrase pairs generated RBMT system. Enache et al.
use pre-defined single value source-to-target target-to-source phrase translation
probabilities lexical weightings synthetic phrase pairs. consequence
synthetic phrase pairs equiprobable relative weight (compared phrase
pairs extracted training parallel corpus) optimised tuning step
SMT training process. proposal, however, relative weight synthetic phrase
pairs optimised tuning process thanks use binary feature function,
phrases translated way parallel corpus RBMT system receive
higher scores, lexical translation probabilities synthetic phrase pairs
computed based principles SMT: taking account translations
individual words make phrases.
Finally, Rosa, Marecek, Dusek (2012) create set rules applied
output SMT system order fix common errors. main difference
proposal lies fact that, although rules similar
transfer rules, operate TL side, syntactic analysis performed
applying them.
6. parse tree may obtained sentences follow usual structure
restricted domain. occurs case 66.7% sentences test set.

21

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

2.2 Adding Morphological Information SMT
hybridisation approach combined rule inference approach described
Sanchez-Cartagena et al. (2015) order integrate set structural transfer rules
inferred SMT training parallel corpus, thereby extending PBSMT models
new linguistic information. Since shallow-transfer rules operate lexical forms made
lemma, lexical category morphological inflection information, combination
two approaches seen novel way extending PBSMT morphological
features.
manner, resulting approach related factored translation models (Koehn
& Hoang, 2007), extension PBSMT word replaced set
factors represent lemma, lexical category, morphological inflection information.
phrase-based translation model inferred lemmas independent one lexical
categories morphology. word-based generation model, inferred
additional monolingual data, maps combinations lemmas, lexical category morphological inflection information inflected word forms. main differences
factored models hybrid approach follows:
factored models, translation lemmas morphological information completely independent. types translations combined order generate
final sequence surface forms (running words), combinatorial explosion likely
produced (too many combinations lemmas morphological information
need scored). combinations cannot explored, correct translation
hypotheses may pruned (Bojar & Hajic, 2008; Graham & van Genabith, 2010).
Moreover, idiomatic translations follow general morphological rules
TL may assigned low probability translation model, even
though would high probability phrase table built surface forms.
strategy differs one followed combination two
approaches, translation hypotheses built surface-form-based models
(like usually used PBSMT) enriched synthetic phrase
pairs generated rules inferred training corpus. complexity dealing translations lemmas morphological inflection information moved
decoding training time, rule inference algorithm deals it.7
hybrid approach works existing bilingual dictionaries, factored models
use bilingual dictionaries all. consequence, translate morphological inflection information different way. factored models probability
TL morphological inflection factors depends solely morphological inflection
factors SL sentence. contrast, transfer rules used method obtain morphological inflection attributes TL words either SL words
translation according bilingual dictionary. makes formalism
expressive eases treatment certain linguistic phenomena. Consider,
instance, case morphological inflection attribute
7. worth noting Graham van Genabith (2010) proposed strategy partially mitigating
issues caused fact factored models treat lemmas morphological information totally
independent elements: extraction training parallel corpus factored templates,
phrases decomposed lemma morphological information translation.

22

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

exists TL (such gender translating English Spanish French).
hybrid approach, structural transfer rule gender number agreement
noun adjective would assign gender translation
TL according bilingual dictionary SL noun TL noun adjective.
type rule inferred small parallel corpus. factored models,
however, translation model would presumably assign similar probabilities TL
noun-adjective sequences genders, success agreement would
depend solely ability TL model differentiate them.
relevant approaches morphological attributes integrated
translation model SMT system found literature. Green DeNero (2012)
define new feature function models morpho-syntactic agreements, factored
language models (Kirchhoff & Yang, 2005) assign probabilities TL sentences depending
sequences word forms morphological features, among factors.
approaches differ strategy presented paper mainly
perform generalisation enriches translation model translations sequences
SL words unseen training corpus.
Riezler Maxwell III (2006) went also added syntactic information
SMT. developed hybrid RBMT-SMT system works follows. SL sentence parsed lexical functional grammar (Riezler et al., 2002) obtain SL
intermediate representation (IR). SL IR transferred TL IR applying
set probabilistic rules obtained parallel corpus. rule contains set scores
inspired present phrase table PBSMT system. Finally, TL sentence
generated TL IR. Since SL sentence parsed many different ways
many different TL IRs generated applying different rules, TL model also
used addition aforementioned phrase-table-like features. features
finally combined means log-linear model, weights optimised means
minimum error rate training (Och, 2003) SMT. results show grammar
used able completely parse half sentences test set (partial parse trees
obtained instead, resulting translation much worse translation
fully parsed sentences), considering sentences could fully parsed,
statistically significant improvement PBSMT system trained using
data. However, human evaluation showed improvement grammaticality
translations. main differences proposal following:
first, approach Riezler Maxwell III use existing bilingual dictionaries;
second, uses syntactic information allows system perform deeper linguistic analysis expense able fully parse input sentences,
results drop translation performance. contrast, approach works lexical
categories morphological inflection information robust ungrammatical
input.
2.3 Integrating Statistical Elements RBMT
Regarding enhancement RBMT systems statistical elements, worth noting
RBMT systems often use statistical methods part-of-speech tagging (Cutting, Kupiec, Pedersen, & Sibun, 1992) parsing (Federmann & Hunsicker, 2011). Besides
23

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

components, elements SMT integrated RBMT, causing greater
changes RBMT architecture. instance, multiple hypotheses generated
transfer step, probable one chosen according TL
model (Lavie, 2008; Carl, 2007). Another option use phrase pairs instead transfer
rules transfer step, keep using RBMT analysis generation modules (Crego, 2014). approach Riezler Maxwell III (2006), discussed previously,
also uses TL model order choose among translations generated applying rules,
integrates elements SMT, feature functions usually encoded
SMT phrase table.
different alternative consists taking advantage full syntactic analysis performed syntactic-transfer RBMT systems create structure TL sentence,
insert phrase translations PBSMT phrase table nodes TL
parse tree (Labaka, Espana-Bonet, Marquez, & Sarasola, 2014). SMT, final translation maximum probability according TL model scores
phrase table phrases inserted tree obtained. However,
phrase reordering allowed, since structure TL sentences guided
parse tree. set-up also followed systems proposed authors (Federmann et al., 2010; Zbib et al., 2012).

3. Enhancement Phrase-Based SMT Shallow-Transfer Linguistic
Resources
access inner working RBMT system, correspondence
SL segments input sentence translations computed without relying
statistical word alignments. fact, even necessary translate whole
sentence RBMT system. individual translation according bilingual
dictionary word, translation segment matches shallow-transfer
rule constitute minimum set bilingual phrases ensures linguistic
information RBMT system extracted. Another advantage method
approach Eisele et al. (2008) lies fact rules match SL segment
would applied shallow-transfer RBMT system greedy
operating mode also taken account.8 Thus, hybrid strategy first generates
synthetic phrase pairs RBMT linguistic data SL text translated,
integrates PBSMT models without decomposition.
8. Consider, instance, English sentence visited Bob Alices dog sleeping translated
Spanish shallow-transfer RBMT system. Let us suppose following segments
sentence match shallow-transfer rule: visited matches rule removes personal pronoun (it
omitted Spanish), adds corresponding preposition generates visite a; Bob Alices
dog matches rule processes Saxon genitive, adds preposition determiner needed
Spanish generates el perro de Bob Alice; Alices dog also matches rule processes
Saxon genitive noun phrase acting owner contains single proper noun, generates el
perro de Alice. RBMT engine chooses rules applied left-to-right, longest match
manner, produces visite al perro de Bob Alice estaba durmiendo, means visited Bobs dog
Alice sleeping. right translation, visite Bob el perro de Alice estaba durmiendo,
obtained rule matches Alices dog applied. Eisele et al.s (2008) method applied
build hybrid system, phrase pairs correct translation visited Bob visite Bob
Alices dog sleeping el perro de Alice estaba durmiendo would extracted.

24

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

3.1 Generation Synthetic Phrase Pairs
way synthetic phrase pairs generated differs depending linguistic resources bilingual dictionaries shallow-transfer rules used. generate
bilingual phrase pairs bilingual dictionary, SL surface forms recognised
shallow-transfer RBMT system corresponding SL IRs listed; then,
SL IRs translated bilingual dictionary obtain corresponding TL IRs;
finally, corresponding TL word forms obtained means RBMT generation
module.9 instance, generation phrase pairs EnglishSpanish bilingual dictionary Apertium RBMT platform, mappings SL surface forms
lexical forms houses house N-num:pl however however ADV generated.
translated TL bilingual dictionary: resulting phrase pairs
houses casas however sin embargo. Since dictionaries may contain multi-word
units, phrase pairs generated may contain one word (SL TL)
sides. Note that, unlike method Eisele et al. (2008), sentences translated
used. Thus, generation phrase pairs bilingual dictionary needs
performed rather time new text translated.
Bilingual phrase pairs matching structural transfer rules generated similar way,
using SL text translated. Thus, process repeated time new text
translated hybrid system.10 First, SL sentences analysed order
obtain SL IRs, sequences lexical forms match structural
transfer rule passed rest RBMT pipeline obtain translations.
sequence SL lexical forms matched one structural transfer rule,
used generate many bilingual phrase pairs different rules matches.
differs way Apertium translates, since cases longest rule
would applied.
Let us suppose English sentence little dogs run fast translated Spanish.
analysed Apertium following sequence lexical forms: POSP-p:1.num:pl,
little ADJ, dog N-num:pl, run VERB-t:inf, fast ADV.11 RBMT system contained
two rules, one performs swapping number gender agreement
adjective noun it, another matches determiner followed
adjective noun, swaps adjective noun makes three words agree
gender number, segments little ADJ dog N-num:pl POSP-p:1.num:pl
little ADJ dog N-num:pl would used generate following bilingual phrase pairs:
little dogs perros pequenos little dogs mis perros pequenos.
9. TL IR contains missing values morphological inflection attributes, different TL phrase
possible value attribute generated. instance, mapping SL (English)
word form beautiful SL lexical form beautiful ADJ-num:sg two EnglishSpanish phrase pairs
generated: beautiful bonito beautiful bonita; first phrase adjective beautiful
translated masculine, whereas second case translated feminine.
10. step carried without dramatically reducing decoding efficiency thanks fact
many steps Apertium translation pipeline implemented partial finite-state transducers (Roche & Schabes, 1997) able process tens thousands words per second average
desktop computer (Forcada et al., 2011, 4.1).
11. meaning abbreviations used represent lexical categories are: POSP = possessive pronoun;
ADJ = adjective; N = common noun; VERB = verb; ADV = adverb. Regarding morphological
inflection information, p:1 = first person, num:pl = plural number t:inf = infinitive mood.

25

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

Note that, unlike generation bilingual phrases bilingual dictionary,
generation bilingual phrase pairs shallow-transfer rules guided text
translated.12 decided order make approach computationally
feasible avoid meaningless phrases. Consider, instance, rule triggered
determiner followed adjective noun English. Generating possible
phrase pairs virtually matching rule would involve combining determiners
dictionary adjectives nouns, causing generation many
meaningless phrases, wireless boy el nino inalambrico.
phrase pairs generated assigned frequency 1, since
generated actual parallel corpus. frequencies used score phrase
pairs, described next section.
3.2 Scoring Synthetic Phrase Pairs
PBSMT systems usually attach 4 scores (Koehn, 2010, Sec. 5.3) every phrase pair
phrase table (translation model): source-to-target target-to-source phrase translation
probabilities source-to-target target-to-source lexical weightings. source-totarget translation probability (t|s) phrase pair (s, t) usually computed means
Eq. (1), count() stands frequency phrase pair list phrase pairs
extracted training parallel corpus.
count(s, t)
ti count(s, ti )

(t|s) = P

(1)

purpose lexical weightings act back-off scoring phrase pairs
low frequency (Koehn, 2010, Sec. 5.3.3). lexical weighting score phrase pair
usually computed product lexical translation probability source word
target word aligned. Lexical translation probabilities obtained
lexical translation model estimated maximum likelihood word alignments
parallel corpus.
values four scores synthetic phrase pairs calculated different
ways may affect scores phrase pairs extracted original training
corpus. respect, desirable scoring method applied synthetic
corpus-extracted phrase pairs increases probability phrase pairs whose SL
phrase translated way training corpus RBMT system.
addition, scoring method also consider frequency parallel corpus
SL phrases translation performed RBMT system agree
found training corpus. Finally, also desirable addition synthetic
phrase pairs statistical models involve big computational effort, since
executed every text translated.
12. bilingual phrase pairs generated segments training corpus match rule,
method would less effective dealing data sparseness, since synthetic phrases generated
rules would available sequences words present training corpus.

26

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

section, propose method13 integrating set synthetic phrase pairs obtained RBMT data PBSMT system meets aforementioned requirements. remainder section contains, addition method, description
phrase scoring approaches found literature limitations.14
strategies presented evaluated described Section 4.
3.2.1 Creating Additional Phrase Table
One simple strategy integrating synthetic phrase pairs hybrid SMT system
putting different (synthetic) phrase table, Koehn Schroeder
(2007) propose context domain adaptation. decoder builds hypotheses,
looks phrase pairs phrase tables phrase pair found both,
one instance phrase table used build hypotheses. reason
authors refer approach alternative decoding paths. score
phrase table receives different weight tuning process, help
hybrid system obtain appropriate relative weighting sources phrase pairs.
scoring strategy used integrate synthetic phrase pairs PBSMT models, phrase translation probabilities synthetic phrase table computed
means Eq. (1), done phrase pairs extracted parallel corpus,
using counts within set synthetic phrase pairs. lexical weighting scores
phrase pair computed set word alignments lexical translation
model described Koehn (2010, 5.3.3). lexical translation model used
estimated synthetic corpus generated RBMT bilingual dictionary,
described Section 3.1; word alignments used obtained tracing back
operations carried RBMT engine.15
Since phrase tables computed totally independent way, phrase translation probabilities phrase pairs appear phrase tables increased
comparison phrase pairs appear one them. Consider,
instance, SL phrase two different translations according RBMT system:
b c. source-to-target phrase translation probabilities synthetic phrase table
13. method already described Sanchez-Cartagena, Sanchez-Martnez, Perez-Ortiz
(2011a); however, first time systematically compared scoring methods
found literature evaluated automatically inferred rules.
14. Methods relevance phrase tables combined must defined advance (i.e.,
primary secondary phrase table), fill-up (Bisazza, Ruiz, & Federico, 2011),
described section evaluated. leave responsibility adapting
relative relevance types phrase pairs type texts translated tuning step
SMT training process.
15. Apertium engine keeps track step translation pipeline input word
output word obtained. path starting input SL surface form followed
order obtain TL surface form aligned it. exception made step pipeline
converts input word multiple output words vice-versa. case, words involved
left unaligned; done avoid generating many word alignments could incorrect. Let us
suppose Spanish sentence Por otra parte mis amigos americanos han decidido venir translated
English hand American friends decided come Apertium. Spanish
phrase Por otra parte analysed Apertium single lexical form. translated
English, produces segment hand generation step. exception
made, SL word por would aligned four TL words on, the, hand SL
words otra parte would also aligned set TL words.

27

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

resulting phrase pairs would synth (b|a) = 0.5 synth (c|a) = 0.5. Let us also
suppose that, extracting phrase pairs parallel corpus, phrase pairs (a, b)
(a, d) frequency, phrase pairs source.
resulting source-to-target phrase translation probabilities would corpus (b|a) = 0.5
corpus (d|a) = 0.5. Although evidence suggests b likely
translation c d, three translations probability.
3.2.2 Phrase Table Linear Interpolation
Alternatively, two phrase tables built, linearly interpolated
single one (Sennrich, 2012, 2.1). scores attached phrase pair
resulting phrase table obtained linear interpolation value corresponding score corpus-extracted phrase table synthetic phrase table.
instance, source-to-target phrase translation probability computed shown Eq. (2)
below, countsynth () frequency phrase pair list phrase pairs
generated RBMT system, countcorpus () frequency phrase pair list
phrase pairs extracted parallel corpus corpus synth weights
phrase tables; obviously corpus + synth = 1. weights optimised means
perplexity minimisation phrase table built development set (Sennrich, 2012,
2.4).
countsynth (s, t)
countcorpus (s, t)
+ synth P
count
(s,

)
corpus

ti
ti countsynth (s, ti )

(t|s) = corpus P

(2)

method, unlike uses two independent phrase tables described
Section 3.2.1, increases phrase translation probability phrase pairs appear
phrase tables present one them. phrase
pairs (a, b), (a, c) (a, d) mentioned above, resulting probabilities would (b|a) =
0.5synth + 0.5corpus = 0.5; (c|a) = 0.5synth ; (d|a) = 0.5corpus . However,
method use frequency source phrases training corpus
interpolating phrase tables. source phrase x found training
corpus, aligned y, possible translation according RBMT
system z, source-to-target phrase translation probabilities phrase pairs would
(y|x) = corpus (z|x) = synth , respectively. x found 10 000 times
training corpus, always translated y, probabilities would exactly
weights corpus synth phrase pairs. However,
phrase pair (x, y) much reliable found training corpus 10 000 times
found once. probabilities resulting phrase table reflected
difference, decoder would presumably able choose better phrase pairs
produce reliable translations.
3.2.3 Proposed Strategy: Directly Expanding Phrase Table
One way taking account absolute frequency different phrases training
corpus join synthetic phrase pairs corpus-extracted phrase pairs calculate
phrase translation probabilities means relative frequency usual. source-totarget phrase translation probabilities resulting phrase table therefore computed
28

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

follows:
countcorpus (s, t) + countsynth (s, t)
ti (countcorpus (s, ti ) + countsynth (s, ti ))

(t|s) = P

(3)

Since countsynth () = 1 synthetic phrase pairs, synthetic phrase pair
share SL side corpus-extracted phrase pair, source-to-target phrase translation
probability synthetic phrase pair may small compared phrase pair
extracted training corpus.16 Depending texts translated, may
desirable synthetic phrase pair higher phrase translation probability
corpus-extracted phrase pair SL side. order adapt relative weight
texts translated, additional binary feature function flags synthetic
phrase pairs added phrase table.17
lexical weighting scores phrase table built combination method
obtained using lexical translation model types phrase pairs.
model (actually, one model source-to-target another model target-to-source
lexical weighting) obtained concatenation training parallel corpus
synthetic phrase pairs generated RBMT bilingual dictionary. lexical weighting
scores computed using word alignments obtained statistical methods
corpus-extracted phrase pairs, usual (Koehn, 2010, 5.2.1), obtained
tracing back operations carried different translation steps Apertium
synthetic phrase pairs (see Section 3.2.1).
3.2.4 Augmenting Training Corpus
Finally, simplest approach involves appending RBMT-generated phrase pairs
training corpus running usual PBSMT training algorithm. Unlike previous
approaches, improves alignments original training corpus enriches
lexicalised reordering model (Koehn, 2010, 5.4.2), addition phrase table.
phrase extraction algorithm (Koehn, 2010, 5.2.3) may, however, split resulting bilingual phrase pairs smaller units may signify multi-word expressions
translated way appear RBMT bilingual dictionary.
16. applies phrase pairs share TL side target-to-source phrase translation
probability.
17. order take account absolute frequencies parallel corpora two phrase
tables combined obtained, Sennrich (2012, 4.2) proposes weighted counts interpolation method, similar presented paper. two main differences
approaches. Firstly, order adapt weight types phrases texts
translated, weighted counts approach multiplies frequency phrase pair factor
building phrase table; depending origin phrase, different factor used.
contrary, method adds binary feature function phrase table. secondly, weighted
counts approach optimises factors determine relative weight type phrase pair
means perplexity minimisation phrase table built development set (Sennrich, 2012,
2.4) isolation, i.e., connection rest elements present log-linear model.
contrast, new method optimises weight binary feature function together rest
elements log-linear model tuning process. Given poor results obtained
phrase table interpolation method weights also optimised means perplexity
minimisation experiments reported Section 4.2, weighted counts included
experimental setup.

29

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

Although strategy feasible real-world environment computational cost word aligning whole training corpus document translated,18 worth evaluating strategy enriches data
lexicalised reordering model obtained.

4. Evaluation Hand-Crafted Resources
set experiments whose objective evaluating feasibility hybridisation
strategy described Section 3 using hand-crafted linguistic resources Apertium RBMT platform conducted. compare, different language pairs, training corpus sizes text domains, translation quality achieved baseline PBSMT
system, RBMT system data extracted, Eisele et al.s (2008)
approach set hybrid systems using phrase scoring alternatives described
Section 3.2.
4.1 Experimental Setup
language pairs used evaluation BretonFrench19 EnglishSpanish.20
BretonFrench chosen problem resource scarceness:
around 60 000 parallel sentences available pair (Tyers, 2009; Tiedemann,
2012). EnglishSpanish chosen wide range parallel corpora available allows us perform in-domain out-of-domain evaluations.
Moreover, Spanish highly inflected language English not, results
directions EnglishSpanish language pair allow us evaluate detail impact
hybrid strategy translation highly inflected languages.
translation model PBSMT systems EnglishSpanish trained
Europarl parallel corpus (Koehn, 2005) version 5; 21 TL model trained
corpus. cases, Q4/2000 portion set aside evaluation
purposes. Different subsets parallel corpus different number sentences
used build systems; however, cases language model trained
whole TL side Europarl corpus. subsets randomly chosen
way larger corpora include sentences smaller ones. different subcorpora
contain 10 000, 40 000, 160 000, 600 000 1 272 260 sentences; latter corresponds
whole training corpus.
Regarding BretonFrench, translation model built using freelyavailable parallel corpus language pair (Tyers, 2009; Tiedemann, 2012),
contains short sentences tourism computer localisation domains. Different
training corpus sizes used too, namely 10 000, 25 000 54 196 parallel sentences.
latter corresponds whole corpus except subsets reserved tuning
testing. EnglishSpanish language pair, sentences randomly chosen
way larger corpora include sentences smaller ones. TL model
18. Recall different set synthetic phrase pairs generated SL text translated.
19. FrenchBreton RBMT system Apertium platform.
20. symbol means first language acts SL second one TL.
symbol means evaluation performed translation directions.
21. http://www.statmt.org/europarl/archives.html#v5

30

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

learnt monolingual corpus built concatenating target side
whole parallel training corpus French Europarl corpus provided WMT 2011
shared translation task.22
Although larger monolingual corpora available target languages included evaluation setup, used experiments focused evaluating impact RBMT data PBSMT translation model.
learning TL model monolingual corpus exceed size
biggest parallel corpus used experiments, risk huge language model
overshadow impact RBMT data SMT translation model reduced. Note
that, real-world environment, size TL model may need limited
hybrid MT system required reduced memory footprint, example,
going executed handheld device.23
BretonFrench systems tuned using 3 000 parallel sentences randomly chosen
available parallel corpus evaluated using another randomly chosen subset
size; obviously subsets used training. in-domain evaluation
could performed language pair. Regarding EnglishSpanish, in-domain
out-of-domain evaluations carried out. former performed tuning
systems 2 000 parallel sentences randomly chosen Q4/2000 portion Europarl
v5 corpus (Koehn, 2005) evaluating 2 000 random parallel sentences
portion corpus; special care taken avoid overlapping test
tuning sets. out-of-domain evaluation performed using newstest2008
set tuning newstest2010 test testing; sets belong news domain
distributed part WMT 2010 shared translation task.24 Table 1 summarises
data concerning corpora used experiments. Sentences contain
40 tokens removed parallel corpora, customary, order avoid
problems word alignment tool GIZA++ (Och & Ney, 2003).25
experiments carried release 2.1 free/open-source PBSMT
system Moses (Koehn et al., 2007) together SRILM language modelling toolkit
(Stolcke, 2002), used train 5-gram language model using interpolated KneserNey discounting (Goodman & Chen, 1998). Word alignments computed means
GIZA++ (Och & Ney, 2003). weights different feature functions optimised
means minimum error rate training (Och, 2003). parallel corpora lowercased
tokenised training, test sets used evaluate systems.
hand-crafted shallow-transfer rules dictionaries borrowed Apertium platform (Forcada et al., 2011). particular, engine linguistic resources
EnglishSpanish, BretonFrench downloaded Apertium Subversion
22. http://www.statmt.org/wmt11/translation-task.html
23. also EnglishSpanish parallel corpora available, used
experiments one main objectives hybrid approach presented paper, pointed
introduction, alleviate data sparseness problem SMT.
24. http://www.statmt.org/wmt10/translation-task.html
25. Preliminary experiments showed that, sentences contained 40 tokens, GIZA++
able align them. Sentences 40 tokens also removed tuning
test sets order ensure approach Eisele et al. (2008) able extract phrase
pairs needed. Recall method needs align sentences test set RBMT
translations.

31

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

Corpus
Language model (English)
Language model (Spanish)

training

Europarl tuning
Europarl testing
newstest2012 tuning
newstest2013 testing

Source
#words #voc
209 562 11 561
836 194 20 883
3 341 577 36 798
12 546 758 61 654
26 595 542 82 585
42 642
5 157
42 114
5 080
34 878
6 209
48 367
7 701

#sentences
1 650 152
1 650 152
10 000
40 000
160 000
600 000
1 272 260
2 000
2 000
1 732
2 215

Target
#words
#voc
45 712 294 110 018
47 734 244 165 896
216 187
15 884
862 789
30 583
3 452 067
55 584
12 971 035
94 315
27 496 270 125 813
43 348
6 411
42 661
6 289
36 410
7 085
50 745
9 277

(a) EnglishSpanish

Corpus
Language model (French)
training
tuning
testing

#sentences
2 041 625
10 000
25 000
54 196
3 000
3 000

Source
#words #voc
146 255 16 711
365 856 27 606
795 045 41 157
44 586
8 340
43 276
8 119

Target
#words
#voc
60 356 583 155 028
146 556
17 588
369 396
28 333
801 780
40 279
45 086
8 907
43 419
8 832

(b) BretonFrench

Table 1: Number sentences, words, size vocabulary training, tuning
test sets used experiments.

repository.26 Apertium linguistic data contains 326 228 entries EnglishSpanish
bilingual dictionary, 284 EnglishSpanish shallow-transfer rules 138 SpanishEnglish
shallow-transfer rules. Regarding BretonFrench, bilingual dictionary contains 21 593
entries 254 shallow-transfer rules.27
language pair, domain, training corpus size, following systems
built evaluated:
baseline: standard PBSMT system.28
26. Revisions 24177, 22150 28674, respectively.
27. transfer phase split Apertium three steps (Forcada et al., 2011) language pairs
used, step works set rules. Specifically, Apertium linguistic data contains
216 chunker rules, 60 interchunk rules, 7 postchunk rules EnglishSpanish; 106 chunker rules,
31 interchunk rules, 7 postchunk rules SpanishEnglish; 169 chunker rules, 79 interchunk
rules 6 postchunk rules BretonFrench.
28. features baseline system WMT 2011 shared translation task: http://www.
statmt.org/wmt11/baseline.html.

32

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

Apertium: Apertium shallow-transfer RBMT engine, dictionaries
transfer rules borrowed.
extended-phrase: hybrid system described Section 3 following strategy
scoring phrase pairs generated RBMT data described Section 3.2.3.
extended-phrase-dict: above, using dictionaries RBMT
system (without shallow-transfer rules). comparison system
extended-phrase permits evaluation impact use shallow-transfer
rules.
extended-corpus: hybrid system described Section 3 following strategy used
score synthetic phrase pairs simply involves adding synthetic phrase
pairs training corpus (see Section 3.2.4).
two-phrase-tables: hybrid system described Section 3 following strategy used
score synthetic phrase pairs based two independent phrase tables (Koehn &
Schroeder, 2007) (see Section 3.2.1).
interpolation: hybrid system described Section 3 following strategy used
score synthetic phrase pairs based linear interpolation two phrase
tables (Sennrich, 2012, 2.1) (see Section 3.2.2). interpolation weights obtained means perplexity minimisation phrase table built tuning
set.
Eisele: approach Eisele et al. (2008), using alignment model learnt
training corpus obtain word alignments source sentences
RBMT-translated sentences.
4.2 Results Discussion
Figures 15 show BLEU (Papineni, Roukos, Ward, & Zhu, 2002) automatic evaluation
score systems evaluated; TER (Snover, Dorr, Schwartz, Micciulla, & Makhoul, 2006)
METEOR (Banerjee & Lavie, 2005) behave similar manner. addition, statistical significance difference BLEU, TER METEOR scores obtained
hybridisation approach extended-phrase (see Section 3.2.3) obtained
systems computed means paired bootstrap resampling (Koehn,
2004) (p 0.05; 1 000 iterations).29 results pair-wise comparison reported
table, included figure, cell represents reference system
approach extended-phrase compared training corpus size; table
contains results three evaluation metrics: BLEU (B), TER (T) METEOR
(M). arrow pointing upwards () means extended-phrase outperforms reference
system, arrow pointing downwards () means reference system outperforms
extended-phrase, equal sign (=) means difference systems
statistically significant.
29. extended-phrase compared systems expected achieve highest
translation quality different hybrid approaches, theory overcomes limitations
approaches (see Section 3.2).

33

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

0.32
0.3

BLEU score

0.28
0.26

0.16
0.14

baseline
extended-phrase
extended-phrase-dict
extended-corpus

10000

two-phrase-tables
interpolation
Eisele
Apertium

40000
160000
600000
Size training corpus (in sentences)

1272260

(a) BLEU scores.

system
metric
baseline
Apertium
extended-phrase-dict
extended-corpus
two-phrase-tables
interpolation
Eisele

10 000
BTM



===
===
===


40 000
BTM


==
===
===
===
==

160 000
BTM
=

===
=
=
===


600 000
BTM
===

===
=
=
===


1 272 260
BTM
==

==
===
==
===


(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phrase
methods evaluated (a method per row). Columns represent training corpus sizes
evaluation metrics: BLEU (B), TER (T) METEOR (M). means extended-phrase
outperforms reference method statistically significant margin, means opposite,
= means statistically significant difference them.

Figure 1: EnglishSpanish in-domain evaluation, automatic evaluation scores obtained baseline PBSMT system, Apertium, hybrid approaches described
Section 3.2, hybrid approach Eisele et al. (2008). table shows pair-wise
comparison system extended-phrase (see Section 3.2.3).

results show hybrid approach described Section 3 (extended-phrase)
outperforms RBMT baseline PBSMT system statistically significant
margin different scenarios. Namely, translating out-of-domain texts (texts whose
domain different domain parallel corpus used; occurs training
corpus sizes language pairs) translating in-domain texts SMT system
trained relatively small parallel corpus. Thus, found literature (see Section 2),
34

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

0.28

0.26

BLEU score

0.24

0.22

0.2

0.18

0.16

0.14
10000

baseline
extended-phrase
extended-phrase-dict
extended-corpus

two-phrase-tables
interpolation
Eisele
Apertium

40000
160000
600000
Size training corpus (in sentences)

1272260

(a) BLEU scores.

system
metric
baseline
Apertium
extended-phrase-dict
extended-corpus
two-phrase-tables
interpolation
Eisele

10 000
BTM



==

===


40 000
BTM




==
==


160 000
BTM



==
=



600 000
BTM



==




1 272 260
BTM



==




(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phrase
methods evaluated (a method per row). Columns represent training corpus sizes
evaluation metrics: BLEU (B), TER (T) METEOR (M). means extended-phrase
outperforms reference method statistically significant margin, means opposite,
= means statistically significant difference them.

Figure 2: EnglishSpanish out-of-domain evaluation, automatic evaluation scores
obtained baseline PBSMT system, Apertium, hybrid approaches described
Section 3.2, hybrid approach Eisele et al. (2008). table shows pair-wise
comparison system extended-phrase (see Section 3.2.3).

possible confirm shallow-transfer RBMT PBSMT systems combined
hybrid system outperforms them.
regard differences observed results in-domain out-ofdomain evaluations, important state that, EnglishSpanish, out-of-domain
tuning test sets come general (news) domain RBMT data
developed bearing mind translation general texts (mainly news). case,
Apertium-generated (synthetic) phrase pairs, contain hand-crafted knowledge
35

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

0.32
0.3

BLEU score

0.28
0.26
0.24

0.16
0.14
0.12
10000

baseline
extended-phrase
extended-phrase-dict
extended-corpus

two-phrase-tables
interpolation
Eisele
Apertium

40000
160000
600000
Size training corpus (in sentences)

1272260

(a) BLEU scores.

system
metric
baseline
Apertium
extended-phrase-dict
extended-corpus
two-phrase-tables
interpolation
Eisele

10 000
BTM


==
==
===
==


40 000
BTM


===
=
===
===


160 000
BTM


===
===
==
===
=

600 000
BTM
===

==
===
===
===
=

1 272 260
BTM
===

==

===
===
=

(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phrase
methods evaluated (a method per row). Columns represent training corpus sizes
evaluation metrics: BLEU (B), TER (T) METEOR (M). means extended-phrase
outperforms reference method statistically significant margin, means opposite,
= means statistically significant difference them.

Figure 3: SpanishEnglish in-domain evaluation, automatic evaluation scores obtained baseline PBSMT system, Apertium, hybrid approaches described
Section 3.2, hybrid approach Eisele et al. (2008). table shows pair-wise
comparison system extended-phrase (see Section 3.2.3).

general domain, cover sequences words input text covered,
sparsely found, original training corpus. Contrarily, in-domain tests reveal that,
soon PBSMT system able learn reliable information parallel
corpus, synthetic RBMT phrase pairs become useless in-domain test sets
come specialised domain parliament speeches. BretonFrench, given
small size corpus available, hybrid approach outperforms pure RBMT
PBSMT approaches experiments performed.
36

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

0.28

0.26

BLEU score

0.24

0.22

0.2

0.18

0.16

0.14
10000

baseline
extended-phrase
extended-phrase-dict
extended-corpus

two-phrase-tables
interpolation
Eisele
Apertium

40000
160000
600000
Size training corpus (in sentences)

1272260

(a) BLEU scores.

system
metric
baseline
Apertium
extended-phrase-dict
extended-corpus
two-phrase-tables
interpolation
Eisele

10 000
BTM



=
===
===


40 000
BTM



==
==
=


160 000
BTM



=
=
==


600 000
BTM


==
===
=



1 272 260
BTM



==
===



(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phrase
methods evaluated (a method per row). Columns represent training corpus sizes
evaluation metrics: BLEU (B), TER (T) METEOR (M). means extended-phrase
outperforms reference method statistically significant margin, means opposite,
= means statistically significant difference them.

Figure 4: SpanishEnglish out-of-domain evaluation, automatic evaluation scores
obtained baseline PBSMT system, Apertium, hybrid approaches described
Section 3.2, hybrid approach Eisele et al. (2008). table shows pair-wise
comparison system extended-phrase (see Section 3.2.3).

analysis proportion synthetic phrase pairs included decoder
final translation30 different evaluation scenarios, depicted figures 68, confirms reason differences in-domain out-of-domain results.
EnglishSpanish training corpus size hybrid system, proportion synthetic
phrases higher out-of-domain evaluation.
30. synthetic phrase pair also obtained parallel corpus, considered synthetic
figures 68.

37

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

0.26
0.24

BLEU score

0.22
0.2
0.18
0.16
0.14
baseline
extended-phrase
extended-phrase-dict
extended-corpus

0.12
0.1
10000

two-phrase-tables
interpolation
Eisele
Apertium

25000
Size training corpus (in sentences)

54196

(a) BLEU scores.

system
metric
baseline
Apertium
extended-phrase-dict
extended-corpus
two-phrase-tables
interpolation
Eisele

10 000
BTM


===
==
==
==


25 000
BTM


===
=
===
==


54 196
BTM
=

===
=
===
===


(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phrase
methods evaluated (a method per row). Columns represent training corpus sizes
evaluation metrics: BLEU (B), TER (T) METEOR (M). means extended-phrase
outperforms reference method statistically significant margin, means opposite,
= means statistically significant difference them.

Figure 5: BretonFrench in-domain evaluation, automatic evaluation scores obtained baseline PBSMT system, Apertium, hybrid approaches described
Section 3.2, hybrid approach Eisele et al. (2008). table shows pair-wise
comparison system extended-phrase (see Section 3.2.3).

Regarding difference hybrid systems enriched RBMT resources (extended-phrase) include dictionary (extended-phrase-dict),
patterns detected. EnglishSpanish, impact shallow-transfer
rules higher translating out-of-domain texts decreases training corpus
grows. impact therefore higher decoder chooses high proportion
Apertium phrases (see figures 6 7). Moreover, systems including shallow-transfer
rules outperform counterparts include dictionary wider margin
38

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

1

two-phrase-tables
interpolation
Eisele

extended-phrase
extended-phrase-dict
extended-corpus

Proportion synthetic phrases

0.8

0.6

0.4

0.2

0
10000

40000
160000
600000
Size training corpus (in sentences)

1272260

(a) In-domain evaluation.
1

two-phrase-tables
interpolation
Eisele

extended-phrase
extended-phrase-dict
extended-corpus

Proportion synthetic phrases

0.8

0.6

0.4

0.2

0
10000

40000
160000
600000
Size training corpus (in sentences)

1272260

(b) Out-of-domain evaluation.

Figure 6: EnglishSpanish, proportion phrase pairs generated RBMT data
chosen decoder translating test set different hybrid approaches
described Section 3.2 hybrid approach Eisele et al. (2008).

translating out-of-domain texts English Spanish way round.
Spanish morphology richer, transfer rules help perform agreement operations
translating Spanish. contrary, Spanish source language, one
main limitations suffered baseline PBSMT system high number outof-vocabulary (OOV) words, already mitigated integrating dictionaries
39

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

1

two-phrase-tables
interpolation
Eisele

extended-phrase
extended-phrase-dict
extended-corpus

Proportion synthetic phrases

0.8

0.6

0.4

0.2

0
10000

40000
160000
600000
Size training corpus (in sentences)

1272260

(a) In-domain evaluation.
1

two-phrase-tables
interpolation
Eisele

extended-phrase
extended-phrase-dict
extended-corpus

Proportion synthetic phrases

0.8

0.6

0.4

0.2

0
10000

40000
160000
600000
Size training corpus (in sentences)

1272260

(b) Out-of-domain evaluation.

Figure 7: SpanishEnglish, proportion phrase pairs generated RBMT data
chosen decoder translating test set different hybrid approaches
described Section 3.2 hybrid approach Eisele et al. (2008).

phrase table extended-phrase-dict approach, shown figures 911.31
figures show amount OOV words much higher baseline system
31. approach Eisele et al. (2008) number OOV words always 0 phrase table
contains phrase pairs obtained translating test set RBMT system, RBMT system
copies verbatim output words appear dictionaries.

40

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

1

two-phrase-tables
interpolation
Eisele

extended-phrase
extended-phrase-dict
extended-corpus

Proportion synthetic phrases

0.8

0.6

0.4

0.2

0
10000

25000
Size training corpus (in sentences)

54196

Figure 8: BretonFrench, proportion phrase pairs generated RBMT data
chosen decoder translating test set different hybrid approaches
described Section 3.2 hybrid approach Eisele et al. (2008).

SL Spanish SL English reduction amount
OOVs adding RBMT dictionaries consequently also higher first case.
contrast, positive impact rules limited EnglishSpanish
in-domain evaluation, statistically significant improvement hybrid system
enriched solely dictionaries (according three evaluation metrics)
observed smallest EnglishSpanish training corpus. fact, training
corpus sizes, inclusion shallow-transfer rules hybrid system produces
statistically significant drop translation quality according one three evaluation
metrics (METEOR case EnglishSpanish in-domain evaluation TER
case SpanishEnglish). training parallel corpus belongs domain
test corpus, corpus-extracted phrase pairs likely contain accurate fluent
translations compared mechanical regular translations provided
RBMT shallow-transfer rules. One possible explanation fact degradation
caused rules measured TER METEOR used BLEU
tuning (Och, 2003). Consequently, weight feature function flags whether
phrase pairs comes parallel corpus RBMT system set
inclusion shallow-transfer rules penalise translation quality measured
BLEU. effect using evaluation metrics tuning yet studied.
regard BretonFrench, impact shallow-transfer rules also limited:
difference hybrid system enriched shallow-transfer rules system
enriched dictionaries statistically significant training corpus
sizes evaluated. reason probably sentences test set
complex grammatical structure: average sentence length 9 words (Tyers, 2009)
contains many sentences simply noun phrases. Another possible reason
41

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

2500

Number OOVs

2000

two-phrase-tables
interpolation
Eisele

baseline
extended-phrase
extended-phrase-dict
extended-corpus

1500

1000

500

0

-500
10000

40000
160000
600000
Size training corpus (in sentences)

1272260

(a) In-domain evaluation.
9000
8000
7000

two-phrase-tables
interpolation
Eisele

baseline
extended-phrase
extended-phrase-dict
extended-corpus

Number OOVs

6000
5000
4000
3000
2000
1000
0
-1000
10000

40000
160000
600000
Size training corpus (in sentences)

1272260

(b) Out-of-domain evaluation.

Figure 9: EnglishSpanish, number out-of-vocabulary words test set
different hybrid approaches described Section 3.2 hybrid approach Eisele et al.
(2008).

may fact quality BretonFrench shallow-transfer rules may lower
quality rules used language pairs, since effort spent
development smaller.
regards different phrase scoring approaches defined Section 3.2, differences observed. remarkable differences show inclusion
synthetic phrase pairs great impact, is, EnglishSpanish out-of-domain eval42

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

3500

two-phrase-tables
interpolation
Eisele

baseline
extended-phrase
extended-phrase-dict
extended-corpus

3000

Number OOVs

2500
2000
1500
1000
500
0
-500
10000

40000
160000
600000
Size training corpus (in sentences)

1272260

(a) In-domain evaluation.
12000

10000

two-phrase-tables
interpolation
Eisele

baseline
extended-phrase
extended-phrase-dict
extended-corpus

Number OOVs

8000

6000

4000

2000

0

-2000
10000

40000
160000
600000
Size training corpus (in sentences)

1272260

(b) Out-of-domain evaluation.

Figure 10: SpanishEnglish, number out-of-vocabulary words test set
different hybrid approaches described Section 3.2 hybrid approach Eisele
et al. (2008).

uations. Firstly, interpolation strategy frequently outperformed strategies,
hybrid systems built usually choose relatively small proportion synthetic phrase pairs. theory, outperform two-phrase-tables strategy
assigns higher probabilities synthetic phrase pairs also found training parallel corpus, actually two-phrase-tables approach generally achieves higher
translation quality. One possible reason result may fact that,
43

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

6000

two-phrase-tables
interpolation
Eisele

baseline
extended-phrase
extended-phrase-dict
extended-corpus

5000

Number OOVs

4000

3000

2000

1000

0

-1000
10000

25000
Size training corpus (in sentences)

54196

Figure 11: BretonFrench, number out-of-vocabulary words test set
different hybrid approaches described Section 3.2 hybrid approach Eisele et al.
(2008).

interpolation method relative weights two types phrase pairs optimised
minimise perplexity set phrase pairs extracted tuning corpus,
two-phrase-tables strategy relative weights optimised maximise translation
quality minimum error rate tuning algorithm. latter case, interaction
phrase pairs rest elements PBSMT system taken account
tuning process. Nevertheless, additional experiments whose objective
carry in-depth evaluation impact method used optimise relative
weight types phrase pairs need carried out. Concerning extendedcorpus strategy, consistently outperform strategies, probably
synthetic phrase pairs short subphrases clearly improve reordering model. However, already stated, strategy could used real-world
setting high computational cost aligning synthetic phrase pairs
training corpus together every document translated. Finally, two-phrasetables strategy outperformed extended-phrase strategy experiments carried
EnglishSpanish language pair (except smallest training corpus size,
effect increasing probability phrase pairs appear phrase
tables, described Section 3.2.1, less relevant). reverse language pair,
two-phrase-tables strategy sometimes better, three evaluation metrics never agree
difference strategies small compared EnglishSpanish.
results suggest that, least evaluation scenario shallow-transfer
rules highest impact, phrase scoring strategy defined Section 3.2.3 able
achieve better balance two sources phrase pairs.
Finally, hybridisation strategy defined Section 3, together phrase scoring
strategy defined Section 3.2.3, outperforms approach Eisele et al. (2008)
44

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

language pairs, training corpus sizes domains. biggest difference
approaches observed small corpora used training. anticipated
Section 2.1, circumstances, reliable alignment models learnt
training corpus therefore reliable phrase pairs obtained input text
RBMT translation. approach presented work, contrarily, affected
issue rely word alignments order generate phrase pairs
RBMT system. addition, significant difference even training
corpus relatively big (more one million parallel sentences). high proportion
synthetic phrase pairs used compared hybrid approaches (see figures 68)
suggests approach Eisele et al. able find adequate balance
types phrase pairs. may synthetic phrase pairs even extracted
SL segments match transfer rule straightforward
scoring method used, simply consists concatenating phrase table obtained
training parallel corpus obtained RBMT system.

5. Evaluation Automatically Inferred Rules
empirically proved previous section, shallow-transfer rules improve
performance PBSMT. However, considerable human effort high level
linguistic knowledge needed create them. order reduce degree human
effort required achieve improvement, algorithm proposed Sanchez-Cartagena
et al. (2015) used infer set shallow-transfer rules training parallel
corpus PBSMT models built, set rules, together
bilingual dictionary, used enlarge phrase table previously described.
significant boost translation quality could thus achieved sole addition
RBMT dictionaries. section, set experiments whose objective assess
viability approach presented.
method proposed Sanchez-Cartagena et al. (2015) uses parallel corpora infer
shallow-transfer rules compatible formalism used Apertium (Forcada
et al., 2011). approach inspired method Sanchez-Martnez Forcada
(2009), uses generalisation alignment template formalism (Och & Ney, 2004)
encode transfer rules, overcomes important limitations method SanchezMartnez Forcada (2009). refer reader paper Sanchez-Cartagena et al.
thorough description limitations.
approach Sanchez-Cartagena et al. (2015) first literature
problem automatically inferring transfer rules reduced finding optimal value
minimisation problem. prove translation quality achieved automatically inferred rules generally close obtained hand-crafted rules. Moreover,
language pairs, automatically inferred rules even able outperform
hand-crafted ones.
5.1 Experimental Setup
Two considerations borne mind inferring set shallow-transfer rules
integrated PBSMT system. Firstly, experiments conducted SanchezCartagena et al. (2015) concluded one features rule inference algorithm,
45

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

generalisation alignment templates combinations values morphological inflection
attributes observed training corpus, one causes vast complexity
aforementioned minimisation problem brings significant translation quality
boost training corpus small (below 1 000 parallel sentences). Given
fact parallel corpus sizes SMT system starts competitive
much bigger, generalisation morphological inflection attributes skipped
inferring shallow-transfer rules integrated PBSMT. Moreover, preliminary
experiments showed that, even disabling generalisation non-observed combinations values morphological inflection attributes, global minimisation algorithm
still needs huge amount processing time order infer set rules parallel
corpus contains hundreds thousands parallel sentences.
Secondly, rule inference algorithm Sanchez-Cartagena et al. (2015) filters rules
generated ensure that, applied shallow-transfer RBMT
system greedy, left-to-right, longest-match way, groups words need
processed together translated rule. on, shall refer
process optimising rules chunking. Since, principle, SMT decoder splits
input sentences possible ways, process might needed. Shallow-transfer
rules sequences SL lexical categories present corpus would therefore
generated.
ran preliminary experiments results showed consistent
differences systems whose rules optimised chunking
systems whose rules not: statistically significant differences found
evaluation metrics. SpanishEnglish, optimising rules chunking
brings tiny improvement, EnglishSpanish, effect opposite. Since
impact rules higher translation out-of-domain texts, effect
optimisation also noticeable scenario.
optimisation rules chunking affects resulting hybrid system two ways.
one hand, prevents inclusion phrase table multiple noisy phrase pairs
generated shallow-transfer rules match sequences lexical categories
need processed together translating languages involved.
Owing fact decoder cannot evaluate translation hypotheses,
useless phrase pairs may prevent other, important phrase pairs included
final translation. may also occur language model enough
information properly score synthetic phrase pairs built noisy rules.
point view, optimisation rules chunking positive impact
translation quality. Furthermore, since SMT system perform greedy
segmentation input sentence, rules discarded optimisation
chunking RBMT may still useful included PBSMT system. Rules
would prevent application important rule RBMT engine
prevent application rule hybrid system because, principle,
possible segmentations taken account. light preliminary results,
seems former relevant SpanishEnglish, latter higher
positive impact EnglishSpanish. Since Spanish morphologically complex,
rules needed correctly perform agreements, rules discarded
46

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

optimisation chunking probably useful. Nevertheless, differences yet
studied greater depth.
Bearing considerations mind, experiments carried follows.
language pairs, corpora RBMT dictionaries used previous section,
new system, extended-phrase-learnt, built; system, rule inference algorithm described Sanchez-Cartagena et al. (2015) applied training corpus
optimisation rules chunking performed. rules inferred,
together dictionaries, used enrich PBSMT system following
hybridisation strategy described Section 3. time complexity minimisation problem solved rule inference approach, first 160 000 sentences
training corpus used rule inference cases corpus
larger 160 000 sentences. words, systems built 160 000, 600 000,
whole set parallel sentences use exactly set shallow-transfer rules.32
compare new system pure PBSMT baseline built data,
hybrid system built Apertium hand-crafted rules dictionaries, hybrid system
built strategy Apertium dictionaries, two different
versions RBMT system Apertium: one version using hand-crafted rules another
version automatically inferred rules. hybrid systems, scoring method
described Section 3.2.3 used, since scoring method proved
perform best experiments described previous section.
5.2 Results Discussion
comparison hybrid approach extended-phrase-learnt approaches
considered section presented figures 1216. results show BLEU
(Papineni et al., 2002) automatic evaluation score different systems evaluated; TER
METEOR behavie similar way. addition, statistical significance33
difference extended-phrase-learnt systems also presented table,
way depicted previous section.
comparison PBSMT baseline pure RBMT system shows
hybrid approach automatically inferred rules behaves way handcrafted rules used: outperforms baselines training corpus small
out-of-domain text translated. comparison performed hybrid system
uses dictionaries, hybrid approach also outperforms dictionary-based
approach almost cases hybrid approach hand-crafted rules: outof-domain evaluation in-domain evaluation smallest parallel corpus size,
although three evaluation metrics agree latter case. words,
automatic inference shallow-transfer rules, statistically significant improvement
approach uses dictionaries achieved without using additional
linguistic resources.
32. addition, part training corpus used rule inference split two parts:
first 4/5 corpus used actual rule inference, last 1/5 employed
development corpus order optimise threshold , experiments described SanchezCartagena et al. (2015). training corpora bigger 10 000 sentences, 2 000 sentences
used optimising , remaining part corpus used rule inference.
33. obtained paired bootstrap resampling (Koehn, 2004) (p 0.05; 1 000 iterations).

47

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

0.32

0.3

BLEU score

0.28

0.26

0.16

0.14

baseline
extended-phrase
extended-phrase-dict

10000

extended-phrase-learnt
Apertium-learnt
Apertium

40000
160000
600000
Size training corpus (in sentences)

1272260

(a) BLEU scores.

system
metric
baseline
Apertium
Apertium-learnt
extended-phrase-dict
extended-phrase

10 000
BTM



==
===

40 000
BTM



===
==

160 000
BTM
===


=
=

600 000
BTM
===


===
===

1 272 260
BTM
==


==
===

(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phraselearnt methods evaluated (a method per row). Columns represent training corpus
sizes evaluation metrics: BLEU (B), TER (T) METEOR (M). means extendedphrase-learnt outperforms reference method statistically significant margin, means
opposite, = means statistically significant difference them.

Figure 12: EnglishSpanish in-domain evaluation, automatic evaluation scores
obtained baseline PBSMT system, Apertium hand-crafted rules (Apertiumlearnt), Apertium learnt rules (Apertium-learnt), hybrid approach (described
Section 3.2.3) using hand-crafted shallow-transfer rules (extended-phrase), set rules
inferred training corpus (extended-phrase-learnt) rules (extendedphrase-dict). table shows pair-wise comparison system extended-phraselearnt.

cases statistically significant difference hybrid system
hand-crafted rules hybrid system automatically inferred rules.
occurs, instance, EnglishSpanish out-of-domain evaluation training
corpus contains 600 000 sentence pairs. translation quality similar obtained
hand-crafted rules therefore attained without intervention human ex48

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

0.26

0.24

BLEU score

0.22

0.2

0.18

0.16

0.14
10000

baseline
extended-phrase
extended-phrase-dict

extended-phrase-learnt
Apertium-learnt
Apertium

40000
160000
600000
Size training corpus (in sentences)

1272260

(a) BLEU scores.

system
metric
baseline
Apertium
Apertium-learnt
extended-phrase-dict
extended-phrase

10 000
BTM






40 000
BTM






160 000
BTM






600 000
BTM




===

1 272 260
BTM




=

(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phraselearnt methods evaluated (a method per row). Columns represent training corpus
sizes evaluation metrics: BLEU (B), TER (T) METEOR (M). means extendedphrase-learnt outperforms reference method statistically significant margin, means
opposite, = means statistically significant difference them.

Figure 13: EnglishSpanish out-of-domain evaluation, automatic evaluation scores
obtained baseline PBSMT system, Apertium hand-crafted rules (Apertiumlearnt), Apertium learnt rules (Apertium-learnt), hybrid approach (described
Section 3.2.3) using hand-crafted shallow-transfer rules (extended-phrase), set rules
inferred training corpus (extended-phrase-learnt) rules (extendedphrase-dict). table shows pair-wise comparison system extended-phraselearnt.
perts usually create them.34 rest cases, hybrid system
34. Although translation quality systems similar according automatic evaluation metrics,
differences amount rules used case. set hand-crafted rules
Apertium platform contains hundred rules language pair, number inferred
rules ranges 2 000 75 000, depending language pair size training parallel
corpus. figures directly comparable, since rule formalism used hand-crafted
rules expressive automatically inferred rules (Sanchez-Cartagena et al., 2015,

49

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

0.32
0.3

BLEU score

0.28
0.26
0.24
0.18
0.16
0.14

baseline
extended-phrase
extended-phrase-dict

10000

extended-phrase-learnt
Apertium-learnt
Apertium

40000
160000
600000
Size training corpus (in sentences)

1272260

(a) BLEU scores.

system
metric
baseline
Apertium
Apertium-learnt
extended-phrase-dict
extended-phrase

10 000
BTM



===
=

40 000
BTM



==
==

160 000
BTM



==
===

600 000
BTM
===


==
===

1 272 260
BTM
===


=
===

(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phraselearnt methods evaluated (a method per row). Columns represent training corpus
sizes evaluation metrics: BLEU (B), TER (T) METEOR (M). means extendedphrase-learnt outperforms reference method statistically significant margin, means
opposite, = means statistically significant difference them.

Figure 14: SpanishEnglish in-domain evaluation, automatic evaluation scores
obtained baseline PBSMT system, Apertium hand-crafted rules (Apertiumlearnt), Apertium learnt rules (Apertium-learnt), hybrid approach (described
Section 3.2.3) using hand-crafted shallow-transfer rules (extended-phrase), set rules
inferred training corpus (extended-phrase-learnt) rules (extendedphrase-dict). table shows pair-wise comparison system extended-phraselearnt.

hand-crafted rules outperforms hybrid system dictionaries, translation quality
achieved hybrid system automatically inferred rules (extended-phrase-learnt)
lies in-between.
3). Nevertheless, error analysis described Section 6.2 shows automatically inferred rules
contain many exceptions applied particular words included hand-crafted ones.

50

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

0.26

0.24

BLEU score

0.22

0.2

0.18

0.16

0.14
10000

baseline
extended-phrase
extended-phrase-dict

extended-phrase-learnt
Apertium-learnt
Apertium

40000
160000
600000
Size training corpus (in sentences)

1272260

(a) BLEU scores.

system
metric
baseline
Apertium
Apertium-learnt
extended-phrase-dict
extended-phrase

10 000
BTM



=


40 000
BTM



=
=

160 000
BTM



==
===

600 000
BTM



===
==

1 272 260
BTM



=
==

(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phraselearnt methods evaluated (a method per row). Columns represent training corpus
sizes evaluation metrics: BLEU (B), TER (T) METEOR (M). means extendedphrase-learnt outperforms reference method statistically significant margin, means
opposite, = means statistically significant difference them.

Figure 15: SpanishEnglish out-of-domain evaluation, automatic evaluation scores
obtained baseline PBSMT system, Apertium hand-crafted rules (Apertiumlearnt), Apertium learnt rules (Apertium-learnt), hybrid approach (described
Section 3.2.3) using hand-crafted shallow-transfer rules (extended-phrase), set rules
inferred training corpus (extended-phrase-learnt) rules (extendedphrase-dict). table shows pair-wise comparison system extended-phraselearnt.

addition, worth noting translation quality approach extendedphrase-learnt drop size training corpus exceeds 160 000 sentences
full training corpus used rule inference. fact, circumstances (600 000 parallel sentences) significant differences use
automatically inferred rules hand-crafted rules hybrid systems (EnglishSpanish,
out-of-domain evaluation). observation probably related fact trans51

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

0.26
0.24
0.22

BLEU score

0.2
0.18
0.16
0.14
0.12
0.1
0.08

baseline
extended-phrase
extended-phrase-dict
10000

extended-phrase-learnt
Apertium-learnt
Apertium

25000
Size training corpus (in sentences)

54196

(a) BLEU scores.

system
metric
baseline
Apertium
Apertium-learnt
extended-phrase-dict
extended-phrase

10 000
BTM



==
===

25 000
BTM



==
==

54 196
BTM



===
===

(b) Paired bootstrap resampling comparison (p 0.05; 1 000 iterations) extended-phraselearnt methods evaluated (a method per row). Columns represent training corpus
sizes evaluation metrics: BLEU (B), TER (T) METEOR (M). means extendedphrase-learnt outperforms reference method statistically significant margin, means
opposite, = means statistically significant difference them.

Figure 16: BretonFrench in-domain evaluation, automatic evaluation scores
obtained baseline PBSMT system, Apertium hand-crafted rules (Apertiumlearnt), Apertium learnt rules (Apertium-learnt), hybrid approach (described
Section 3.2.3) using hand-crafted shallow-transfer rules (extended-phrase), set rules
inferred training corpus (extended-phrase-learnt) rules (extendedphrase-dict). table shows pair-wise comparison system extended-phraselearnt.

lation performance automatically inferred rules grows slowly size
training corpus, rules obtained bigger parallel corpora would probably
similar obtained fragment 160 000 sentences. Nevertheless, exact
impact proportion training corpus used rule inference different training
corpus sizes, language pairs domains merits research.
52

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

Finally, also worth noting difference hand-crafted rules (Apertium)
automatically inferred rules (Apertium-learnt) used RBMT
system: cases (BretonFrench EnglishSpanish out-of-domain evaluation)
difference translation performance considerably higher difference
hybrid systems enriched hand-crafted rules automatically inferred rules
(see figures 13 16). occurs RBMT translation completely led
shallow-transfer rules, possible errors encoded automatically inferred
rules direct impact output.

6. Human Evaluation Error Analysis
section reports, one hand, results obtained out-of-domain human
evaluation performed EnglishSpanish largest training parallel corpus used,
and, other, analysis translation errors performed different systems
evaluated Section 5.
6.1 Human Evaluation
order confirm results obtained automatic evaluation metrics, performed human evaluation EnglishSpanish out-of-domain texts. systems
included human evaluation described previous section trained
largest parallel corpus used.
asked 15 users rank (allowing ties) translations produced baseline
PBSMT system (baseline), Apertium hand-crafted rules, hybrid approach using
dictionaries (extended-phrase-dict), hybrid approach using automatically inferred
rules (extended-phrase-learnt) hybrid approach using hand-crafted rules (extendedphrase). user ranked translations 50 SL sentences test set.
users split 5 groups, users group ranked exactly set SL
sentences, thus allowing us compute inter-annotator agreement. total, translations
250 sentences test set ranked. evaluation method similar
followed WMT 2012 shared translation task (Callison-Burch et al., 2012).
computed ratio wins system (Callison-Burch et al., 2012, Eq. 4)
proportion times system ranked better system. score
allows us sort systems best worst, shown last row Table 2.
resulting ordering exactly obtained automatic evaluation metrics
(see Figure 13).
Table 2 also shows results pairwise comparison systems:
cell represents proportion sentences system named row label
outperforms system named column label. score shown bold type means
difference statistically significant.35 results entirely confirm results
obtained automatic evaluation measures: hybrid systems outperform RBMT
PBSMT systems, automatically inferred rule allow us build better hybrid sys35. According Sign Test, p 0.10. chose relatively high p-value small amount
human rankings available.

53

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

extended-phrase
extended-phrase
extended-phrase-l.
extended-phrase-d.
baseline
Apertium
>

extended-phrase-l.
0.55

0.45
0.40
0.40
0.32
0.61

0.46
0.45
0.35
0.56

extended-phrase-d.
0.60
0.55
0.51
0.35
0.51

baseline
0.60
0.55
0.49
0.37
0.51

Apertium
0.68
0.65
0.65
0.63
0.35

Table 2: Results human evaluation; extended-phrase-l. abbreviation extendedphrase-learnt extended-phrase-d. abbreviation extended-phrase-dict. last
row represents proportion times system outperforms system,
remaining cells show results pairwise evaluation: represent proportion
sentences system named row label outperforms system named
column label. score shown bold type system named
row label wins often system named column label means
difference statistically significant.

tems using dictionaries external resource, i.e.extended-phrase-learnt outperforms
extended-phrase-dict.
Finally, inter-annotator agreement computed described Callison-Burch et al.
(2012, Sec. 3.2) = 0.503, usually interpreted fair agreement.
6.2 Error Analysis
addition assessing translation quality means automatic evaluation metrics
human ranking, also interesting compare different types errors made
systems evaluated section. compared translations performed different
systems used human evaluation found interesting trends summarise
below. focused analysis EnglishSpanish language pair
rules highest impact (see Section 4.2). Table 3 shows seven examples
translations refer throughout section.
comparison pure RBMT system Apertium, baseline PBSMT system
hybrid system extended-phrase shows two pure systems complementary
combined, number errors reduced. comparing pure
statistical system hybrid one, reduction number OOV words observed
(e.g. word patterned example #1). also words whose translation
specific domain parliament speeches performed pure PBSMT
system, translated appropriate way news domain
hybrid system. example word feel example #2. differences
systems lexical: hybrid system produces better agreement
determiners, nouns adjectives (see example #3)36 correctly translates noun phrases
made adjacent nouns (see example #4),37 among grammatical improvements.
36. grammatically correct translation Spanish specialised category categora especializada
37. correct translation Spanish adjacent nouns Brno socialists socialistas de Brno;
literally means socialists Brno.

54

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

compared Apertium RBMT system, hybrid system produces
fluent translations TL, probably thanks use TL model. instance,
hybrid system deals better sentences regular grammatical structure
(see translation example #4).38 Preposition choices also generally
better hybrid system (for instance, preposition correctly removed
hybrid system example #4), translation phrasal verbs (see closing
translated different systems example #5).
results evaluation show translation performance hybrid system
built automatically inferred rules (extended-phrase-learnt) close hybrid
system built hand-crafted rules (extended-phrase; see Figure 13). manual inspection
translations produced reveals hand-crafted rules automatically inferred
rules produce similar translations. one hand, automatically inferred rules
encode many exceptions general translation rules, makes outperform
hand-crafted ones case sentences. One common example phenomenon
swapping adjectivenoun sequence. adjectives (prepositive adjectives)
must swapped translating Spanish automatically inferred
rules able learn (for instance, adjective best example #6).
hand, hand-crafted rules encode long-range grammatical operations, subjectpredicate agreement example #7 invaded translated invadieron,
agrees person number translation 150 drivers, could
automatically inferred rule inference algorithm considers segments
5 tokens.

7. Concluding Remarks
paper, hybridisation approach enrich PBSMT models
data shallow-transfer RBMT systems presented. confirmed
data shallow-transfer RBMT improve PBSMT systems also resulting
hybrid system outperforms pure PBSMT RBMT systems built
data.
hybridisation approach overcomes limitations general-purpose strategy
attempts improve PBSMT models data MT systems (Eisele et al.,
2008) thanks fact takes advantage way shallow-transfer
RBMT system uses linguistic resources segment SL sentences. experiments
carried shown hybrid approach outperforms strategy Eisele et al.
statistically significant margin wide range situations. fact, system (SanchezCartagena et al., 2011b) built hybridisation approach described work
one winners pair-wise manual evaluation WMT 2011 shared translation
task (Callison-Burch et al., 2011) SpanishEnglish.39 effectiveness hybrid
38. reference sentence, relatively free translation SL sentence, contain
word Debera, although appropriate translation context.
39. evaluation performed asking users rank translations produced different systems.
Users iteratively ranked (from best worst) translations SL sentence produced 5
different systems. refer reader description task Callison-Burch et al. (2011)
details evaluation. human evaluation described Section 6 carried
similar way.

55

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

#

1

2

3

system
source
Apertium
baseline
extended-phrase
reference
source
Apertium
baseline
extended-phrase
reference
source
Apertium
baseline
extended-phrase
reference
source
Apertium

4

baseline
extended-phrase
reference

5

source
Apertium
baseline
extended-phrase
reference
source
extended-phrase-l.

6

extended-phrase
reference

7

source
extended-phrase-l.
extended-phrase
reference

sentence
inauguration thick lace, Oslo patterned velvet.
Si la inauguracion era sobre encaje grueso, en Oslo sea terciopelo estampado.
Si la inauguracion de centro encaje en Oslo, fue patterned.
Si la inauguracion era sobre encaje grueso, en Oslo, fue terciopelo estampado
Si para la toma de posesion de trataba de un encaje gordo,
en Oslo es un terciopelo con estampado.
feels kick.
Siente un chut.
El considera una patada.
El siente una patada.
El siente una patada
specialised category nobody year physics.
La categora solo especializada que nadie gano este ano era fsica.
La unica categora especializados que nadie gano este ano ha sido fsica
La unica categora especializada que nadie gano este ano ha sido fsica.
La unica categora especializada, sin una mujer premiada, fue fsica.
should, ecology aesthetics dont seem interest
Brno socialists much.
Tenga que, pero la ecologa la estetica parecen para interesar
los socialistas de Brno mucho.
Debera, pero la ecologa la estetica parecen interes
el Brno socialistas mucho.
Debera, pero la ecologa la estetica parecen interes
los socialistas de Brno mucho.
Lo que pasa es que la ecologa la estetica parecen interesar demasiado
los socialistas de Brno.
opposed principle closing parties.
Somos opposed encima principio al encierro abajo de partidos.
Nos oponemos por principio la clausura de partidos.
Nos oponemos por principio la clausura de los partidos.
Por principio nos oponemos la clausura de partidos.
Theres best skiers snow borders
county - real talent, added.
hay algunos de los mejores esquiadores fronteras de nieve
en el condado aqu - un verdadero talento, anadio.
hay algunos de los esquiadores mejores fronteras de nieve
en el condado aqu - algunos verdadero talento, anadio.
Aqu se encuentran algunos de los mejores esquiadores snowboarders
del condado, talento verdadero, anadio.
150 drivers invaded works council meeting [...]
Unos 150 conductores invadido una reunion del consejo de empresa [...]
Unos 150 conductores invadieron una reunion de consejo de los trabajos [...]
Unos 150 conductores invadieron un comite [...]

Table 3: Translations Spanish different English sentences extracted out-ofdomain evaluation corpus produced systems evaluated Section 6.
remarkable differences highlighted. extended-phrase-l. abbreviation extendedphrase-learnt, hybrid system automatically inferred rules.

56

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

approach thereby confirmed automatic human evaluation (results WMT
2011 human evaluation compatible human evaluation described
Section 6: experiments, hybrid system built method outperforms pure
PBSMT system).
Moreover, proved rule inference algorithm presented SanchezCartagena et al. (2015) successfully combined hybrid approach, thus allowing hybrid system built using dictionaries hand-crafted linguistic
resource. improvement translation quality also achieved way
hand-crafted shallow-transfer rules used. hybrid system automatically
inferred rules able attain translation quality achieved hybrid system
hand-crafted rules and, even not, often obtains better results hybrid
system uses dictionaries enrich PBSMT models. Additionally, need
human expert write rules avoided.
According results obtained, hybrid approach especially recommended
training parallel corpus (for translation model) monolingual corpus (for
language model) moderate size domain training corpus different
domain texts translated.40 use moderate-sized training corpora
may necessary order limit size phrase table TL model
hybrid system must executed mobile device limited memory. Moreover,
hybrid approach presented work also safely applied scenarios, since
drops translation quality comparison PBSMT baseline detected.
good enough hand-crafted rules available, worth using instead inferring
rules parallel training corpus, not, applying rule inference
algorithm significantly degrade translation quality.41
hybridisation method described paper implemented software tool
called rule2Phrase (Sanchez-Cartagena, Sanchez-Martnez, & Perez-Ortiz, 2012)
released GNU GPL v3 free software license. source code freely
downloaded http://www.dlsi.ua.es/~vmsanchez/Rule2Phrase.tar.gz. tool
includes phrase scoring strategies described sections 3.2.3 3.2.4
paper.
40. EnglishSpanish out-of-domain evaluations described Section 5 repeated using TL model
estimated much bigger monolingual corpora. particular, portion News Crawl monolingual corpus provided WMT 2011 shared translation task (http://www.statmt.org/wmt11/
translation-task.html) concatenated Europarl corpus. result, English Spanish
monolingual corpora around 6 200 000 sentences obtained. results evaluation
showed parallel corpus contains around 26 000 000 words used together
monolingual corpora, difference hybrid system built automatically inferred rules
baseline SMT system statistically significant evaluation metrics.
41. Manually creating transfer rules involves huge human effort. Rule writers must first identify
grammatical divergences languages involved need treated rules sort
frequency texts translated RBMT system. operation called contrastive
analysis. write rules deal divergencies, starting frequent ones
choosing case possible conflicts rules. Rules written humans may
good enough grammatical divergencies identified contrastive analysis,
frequency correctly estimated enough time invested writing rules
dealing important grammatical divergencies.

57

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

Acknowledgments
Research funded Spanish Ministry Economy Competitiveness projects
TIN2009-14009-C02-01 TIN2012-32615, Generalitat Valenciana grant ACIF
2010/174, European Union Seventh Framework Programme FP7/2007-2013
grant agreement PIAP-GA-2012-324414 (Abu-MaTran).

References
Banerjee, S., & Lavie, A. (2005). Meteor: automatic metric mt evaluation improved correlation human judgments. Proceedings ACL Workshop
Intrinsic Extrinsic Evaluation Measures Machine Translation and/or Summarization, pp. 6572, Ann Arbor, Michigan, USA.
Bisazza, A., Ruiz, N., & Federico, M. (2011). Fill-up versus interpolation methods
phrase-based smt adaptation. Proceedings 8th International Workshop
Spoken Language Translation, San Francisco, California, USA.
Bojar, O., & Hajic, J. (2008). Phrase-based deep syntactic English-to-Czech statistical
machine translation. Proceedings third Workshop Statistical Machine
translation, pp. 143146, Columbus, Ohio, USA.
Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., Goldsmith, M. J., Hajic, J., Mercer,
R. L., & Mohanty, S. (1993). dictionaries data too. Proceedings
Workshop Human Language Technology, pp. 202205, Princeton, New Jersey.
Callison-Burch, C., Koehn, P., Monz, C., & Zaidan, O. (2011). Findings 2011 workshop statistical machine translation. Proceedings Sixth Workshop
Statistical Machine Translation, pp. 2264, Edinburgh, Scotland.
Callison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut, R., & Specia, L. (2012). Findings
2012 workshop statistical machine translation. Proceedings Seventh
Workshop Statistical Machine Translation, pp. 1051, Montreal, Canada.
Carl, M. (2007). METIS-II: German English MT system. Proceedings XI
Machine Translation Summit, pp. 6573, Copenhagen, Denmark.
Crego, J. (2014). SYSTRAN RBMT engine: hybridization experiments. 3rd Workshop
Hybrid Approaches Machine Translation (HyTra), Gothenburg, Sweden.
Cutting, D., Kupiec, J., Pedersen, J., & Sibun, P. (1992). practical part-of-speech tagger.
Proceedings Third Conference Applied Natural Language Processing, pp.
133140, Trento, Italy.
Eisele, A., Federmann, C., Saint-Amand, H., Jellinghaus, M., Herrmann, T., & Chen, Y.
(2008). Using Moses integrate multiple rule-based machine translation engines
hybrid system. Proceedings Third Workshop Statistical Machine
Translation, pp. 179182, Columbus, Ohio, USA.
Enache, R., Espana-Bonet, C., Ranta, A., & Marquez, L. (2012). hybrid system
patent translation. Proceedings 16th Annual Conference European
Association Machine Translation, pp. 269276, Trento, Italy.
58

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

Federmann, C., Eisele, A., Uszkoreit, H., Chen, Y., Hunsicker, S., & Xu, J. (2010).
experiments shallow hybrid mt systems. Proceedings Joint Fifth Workshop Statistical Machine Translation Metrics, pp. 7781, Uppsala, Sweden.
Federmann, C., & Hunsicker, S. (2011). Stochastic parse tree selection existing rbmt
system. Proceedings Sixth Workshop Statistical Machine Translation, pp.
351357, Edinburgh, Scotland.
Forcada, M. L., Ginest-Rosell, M., Nordfalk, J., ORegan, J., Ortiz-Rojas, S., Perez-Ortiz,
J. A., F. Sanchez-Martnez, G. R.-S., & Tyers, F. M. (2011). Apertium: free/opensource platform rule-based machine translation. Machine Translation, 25 (2), 127
144. Special Issue: Free/Open-Source Machine Translation.
Gao, Q., Lewis, W., Quirk, C., & Hwang, M.-Y. (2011). Incremental Training Intentional Over-fitting Word Alignment. Proceedings XIII Machine Translation
Summit, pp. 106113, Xiamen, China.
Goodman, J., & Chen, S. F. (1998). empirical study smoothing techniques language
modeling. Tech. rep. TR-10-98, Harvard University.
Graham, Y., & van Genabith, J. (2010). Factor templates factored machine translation models. Proceedings 7th International Workshop Spoken Language
Translation, pp. 275283, Paris, France.
Green, S., & DeNero, J. (2012). class-based agreement model generating accurately
inflected translations. Proceedings 50th Annual Meeting Association
Computational Linguistics: Long Papers - Volume 1, pp. 146155, Jeju Island,
Korea.
Hutchins, W. J., & Somers, H. L. (1992). introduction machine translation, Vol. 362.
Academic Press New York.
Kirchhoff, K., & Yang, M. (2005). Improved language modeling statistical machine
translation. Proceedings ACL Workshop Building Using Parallel
Texts, pp. 125128, Ann Arbor, Michigan, USA.
Koehn, P. (2004). Statistical significance tests machine translation evaluation. Proceedings Conference Empirical Methods Natural Language Processing,
Vol. 4, pp. 388395, Barcelona, Spain.
Koehn, P. (2005). Europarl: parallel corpus statistical machine translation. Proceedings X Machine Translation Summit, pp. 1216, Phuket, Thailand.
Koehn, P. (2010). Statistical Machine Translation. Cambridge University Press.
Koehn, P., & Hoang, H. (2007). Factored translation models. Proceedings 2007
Joint Conference Empirical Methods Natural Language Processing Computational Natural Language Learning (EMNLP-CoNLL), pp. 868876, Prague.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B.,
Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., & Herbst, E.
(2007). Moses: Open source toolkit statistical machine translation. Proceedings
45th Annual Meeting ACL Interactive Poster Demonstration
Sessions, pp. 177180, Prague, Czech Republic.
59

fiSanchez-Cartagena, Perez-Ortiz, & Sanchez-Martnez

Koehn, P., & Schroeder, J. (2007). Experiments domain adaptation statistical machine translation. Proceedings Second Workshop Statistical Machine
Translation, pp. 224227, Prague, Czech Republic.
Labaka, G., Espana-Bonet, C., Marquez, L., & Sarasola, K. (2014). hybrid machine
translation architecture guided syntax. Machine Translation, 28 (2), 91125.
Lavie, A. (2008). Stat-XFER: General Search-Based Syntax-Driven Framework Machine Translation. Gelbukh, A. (Ed.), Computational Linguistics Intelligent
Text Processing, Vol. 4919 Lecture Notes Computer Science, pp. 362375.
Och, F. J., & Ney, H. (2003). systematic comparison various statistical alignment
models. Computational Linguistics, 29 (1), 1951.
Och, F. J., & Ney, H. (2004). alignment template approach statistical machine
translation. Computational Linguistics, 30 (4), 417449.
Och, F. J. (2003). Minimum error rate training statistical machine translation. Proceedings 41st Annual Meeting Association Computational Linguistics,
pp. 160167, Sapporo, Japan.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). Bleu: method automatic
evaluation machine translation. Proceedings 40th Annual Meeting
Association Computational Linguistics, pp. 311318, Philadelphia, Pennsylvania,
USA.
Popovic, M., & Ney, H. (2006). Statistical machine translation small amount
bilingual training data. 5th LREC SALTMIL Workshop Minority Languages,
p. 2529, Genoa, Italy.
Riezler, S., King, T. H., Kaplan, R. M., Crouch, R., Maxwell, III, J. T., & Johnson, M.
(2002). Parsing wall street journal using lexical-functional grammar discriminative estimation techniques. Proceedings 40th Annual Meeting
Association Computational Linguistics, ACL 02, pp. 271278, Stroudsburg, PA,
USA. Association Computational Linguistics.
Riezler, S., & Maxwell III, J. T. (2006). Grammatical machine translation. Proceedings
Human Language Technology Conference NAACL, Main Conference, pp.
248255, New York City, New York, USA.
Roche, E., & Schabes, Y. (1997). Introduction. Roche, E., & Schabes, Y. (Eds.), Finitestate language processing, pp. 165. MIT, Cambridge, Massachusetts, USA.
Rosa, R., Marecek, D., & Dusek, O. (2012). Depfix: system automatic correction
czech mt outputs. Proceedings Seventh Workshop Statistical Machine
Translation, pp. 362368, Montreal, Canada.
Rosti, A.-V., Matsoukas, S., & Schwartz, R. (2007). Improved word-level system combination machine translation. Proceedings 45th Annual Meeting
Association Computational Linguistics, pp. 312319, Prague, Czech Republic.
Sanchez-Cartagena, V. M., Perez-Ortiz, J. A., & Sanchez-Martnez, F. (2015). generalised
alignment template formalism application inference shallow-transfer
60

fiIntegrating Shallow-Transfer Rules Statistical Machine Translation

machine translation rules scarce bilingual corpora. Computer Speech & Language,
32 (1), 4690.
Sanchez-Cartagena, V. M., Sanchez-Martnez, F., & Perez-Ortiz, J. A. (2011a). Integrating
shallow-transfer rules phrase-based statistical machine translation. Proceedings
XIII Machine Translation Summit, pp. 562569, Xiamen, China.
Sanchez-Cartagena, V. M., Sanchez-Martnez, F., & Perez-Ortiz, J. A. (2011b). Universitat dAlacant hybrid machine translation system WMT 2011. Proceedings
Sixth Workshop Statistical Machine Translation, pp. 457463, Edinburgh,
Scotland.
Sanchez-Cartagena, V. M., Sanchez-Martnez, F., & Perez-Ortiz, J. A. (2012). opensource toolkit integrating shallow-transfer rules phrase-based satistical machine translation. Proceedings Third International Workshop Free/OpenSource Rule-Based Machine Translation, pp. 4154, Gothenburg, Sweden.
Sanchez-Martnez, F., & Forcada, M. L. (2009). Inferring shallow-transfer machine translation rules small parallel corpora. Journal Artificial Intelligence Research,
34 (1), 605635.
Schwenk, H., Abdul-Rauf, S., Barrault, L., & Senellart, J. (2009). SMT SPE machine
translation systems WMT09. Proceedings Fourth Workshop Statistical
Machine Translation, pp. 130134, Athens, Greece.
Sennrich, R. (2012). Perplexity minimization translation model domain adaptation
statistical machine translation. Proceedings 13th Conference European Chapter Association Computational Linguistics, pp. 539549, Avignon,
France.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L., & Makhoul, J. (2006). study translation edit rate targeted human annotation. Proceedings 7th biennial
conference Association Machine Translation Americas, pp. 223231,
Cambridge, Massachusetts, USA.
Stolcke, A. (2002). SRILM extensible language modeling toolkit. Proceedings
7th International Conference Spoken Language Processing, pp. 901904, Denver,
Colorado, USA.
Thurmair, G. (2009). Comparing different architectures hybrid Machine Translation
systems. Proceedings XII Machine Translation Summit, Ottawa, Canada.
Tiedemann, J. (2012). Parallel Data, Tools Interfaces OPUS. Proceedings
Eight International Conference Language Resources Evaluation, pp. 2214
2218, Istanbul, Turkey.
Tyers, F. M. (2009). Rule-based augmentation training data BretonFrench statistical
machine translation. Proceedings 13th Annual Conference European
Association Machine Translation, pp. 213217, Barcelona, Spain.
Zbib, R., Kayser, M., Matsoukas, S., Makhoul, J., Nader, H., Soliman, H., & Safadi, R.
(2012). Methods integrating rule-based statistical systems Arabic English machine translation. Machine Translation, 26 (1-2), 6783.

61

fi
